# The Impact of Model Scaling on Seen and Unseen Language Performance

*Rhitabrat Pokharel; Sina Bagheri Nezhad; Ameeta Agrawal; Suresh Singh*

---

> ### üìã Quick Facts & Metrics
> *   **Quality Score:** 8/10
> *   **Languages Analyzed:** 204 (Seen & Unseen)
> *   **Model Families:** 3
> *   **Key Tasks:** Text Classification, Machine Translation
> *   **Settings:** Zero-shot vs. Two-shot
> *   **References:** 8 Citations

---

> ### üìù Executive Summary
>
> As Large Language Models (LLMs) scale rapidly in size, a critical knowledge gap remains regarding how this parameter growth affects cross-lingual capabilities, particularly for languages completely absent during pretraining ("unseen" languages). While scaling laws suggest performance improvements with size in high-resource contexts, it is unclear whether this trend extends to zero-shot generalization across diverse linguistic families. Understanding this relationship is essential for efficient resource allocation and for developing truly universal models, as simply increasing model size is computationally expensive and may not yield proportional benefits for underrepresented languages.
>
> This study provides a comprehensive, systematic analysis of scaling laws across 204 languages by evaluating three families of multilingual LLMs of varying parameter sizes. The technical approach isolates specific variables by contrasting Base Pretraining against Instruction-Tuning and evaluating performance under Zero-shot versus Two-shot prompting scenarios. Crucially, the methodology decouples the impact of total pretraining resources from the specific proportional distribution of languages, allowing the researchers to strictly categorize and compare performance on "seen" versus "unseen" language groups across text classification and machine translation tasks.
>
> The evaluation reveals a divergent scaling behavior dependent on the task and prompting strategy: increased model scale showed negligible correlation with performance in zero-shot text classification, yet resulted in linear improvements when shifted to two-shot settings. For machine translation, scaling benefits were exclusive to instruction-tuned models, with base models seeing no advantage from increased size. The data indicates that overall resource volume is a stronger predictor of multilingual performance than specific language class proportions. However, a statistically significant performance gap between seen and unseen languages persists across all evaluated model scales and prompting settings, indicating that scale alone cannot bridge this disparity.
>
> These findings significantly influence the field by demystifying the drivers of multilingual performance, shifting the focus from balancing fine-grained language distributions to maximizing total training resources. The research highlights the limitations of relying solely on parameter scaling for zero-shot capabilities, establishing instead that few-shot prompting and instruction tuning are critical prerequisites for unlocking scaling benefits, particularly in translation tasks. By proving that the "seen versus unseen" gap is resistant to scaling, the paper directs future research toward architectural and training optimizations rather than simple parameter expansion to achieve true language universality.

---

## üîë Key Findings

*   **Divergent Scaling Behavior:** Model scale has a negligible impact on zero-shot performance but shows linear improvements in two-shot text classification.
*   **Task-Specific Scaling Efficacy:** Scaling benefits differ by task; machine translation sees benefits exclusively in instruction-tuned models.
*   **Resource Level vs. Proportion:** Overall resource levels predict performance better than specific pretraining language proportions.
*   **Seen vs. Unseen Disparity:** A significant performance gap exists between seen and unseen languages regardless of model scale or setting.

---

## üõ† Methodology

The study systematically examined three families of multilingual Large Language Models (LLMs) of varying parameter sizes. Performance was evaluated on text classification and machine translation tasks across 204 languages, categorized as seen or unseen, using zero-shot and two-shot learning scenarios.

---

## ‚öôÔ∏è Technical Details

The technical approach evaluates the following variables and configurations:

*   **Model Scale:** Evaluation across varying parameter counts.
*   **Training Paradigm:** Contrast between Base Pretraining and Instruction-Tuning.
*   **Evaluation Settings:** Zero-shot vs. Two-shot prompting.
*   **Resource Configuration:** Manipulation of overall resource levels vs. pretraining language proportions.
*   **Language Categorization:** Strict grouping into Seen and Unseen languages.

---

## üìä Results

**Text Classification**
*   **Zero-shot:** Increased model scale shows negligible correlation with performance.
*   **Two-shot:** Increased model scale results in linear improvements.

**Machine Translation**
*   Scaling benefits are exclusive to instruction-tuned models; base models show no advantage from increased size.

**General Trends**
*   **Predictors:** Overall resource levels are a stronger predictor of performance than pretraining language proportions.
*   **The Gap:** A statistically significant performance gap between seen and unseen languages persists and cannot be closed by increased model scale or altered prompting settings.

---

## üöÄ Contributions

*   **Comprehensive Scaling Analysis:** Provides a large-scale analysis of scaling laws for multilingual capabilities across 204 languages.
*   **Demystifying Performance Drivers:** Identifies total resource volume, rather than language distribution, as the primary driver of multilingual effectiveness.
*   **Contextualizing Prompt Engineering:** Clarifies the limitations of zero-shot scaling versus few-shot prompting for classification and highlights the necessity of instruction tuning for translation scaling benefits.

---

**Rating:** 8/10 | **References:** 8 citations