# RSQ: Learning from Important Tokens Leads to Better Quantized LLMs

*Yi-Lin Sung; Prateek Yadav; Jialu Li; Jaehong Yoon; Mohit Bansal*

---

> ### ðŸ“Š Quick Facts
>
> *   **Focus:** Post-Training Quantization (PTQ) for Large Language Models.
> *   **Target Configs:** W3A16 & W2A16 (3-bit & 2-bit weights).
> *   **Novelty:** Importance-weighted reconstruction loss using token attention.
> *   **Best Result (3-bit):** Wiki PPL of **9.27** (LLaMA3-8B).
> *   **Best Result (2-bit):** Wiki PPL of **16.26** vs baseline 22.71.
> *   **Avg Accuracy Gain:** **+1.6%** for LLaMA3-8B over baselines.
> *   **Quality Score:** 9/10

---

## Executive Summary

Deploying Large Language Models (LLMs) on resource-constrained hardware necessitates aggressive post-training quantization (PTQ), particularly to W3A16 (3-bit weights, 16-bit activations). However, current state-of-the-art methods like GPTQ and QuaRot minimize reconstruction errors uniformly across all calibration tokens. This "one-size-fits-all" approach fails to preserve the semantic significance of high-value tokens, leading to disproportionate accuracy degradation in complex reasoning tasks despite maintaining overall perplexity. The core challenge lies in identifying which features are most critical for model fidelity during the quantization process, as treating all inputs equally ignores the varying semantic density of the calibration data.

The authors propose **RSQ (Rotate, Scale, then Quantize)**, a framework that integrates token importance weighting directly into the PTQ optimization objective. RSQ operates in three stages: first, "Rotate" applies a randomized Hadamard transformation and converts LayerNorm to RMSNorm to smooth distributions and mitigate outliers; second, "Scale" minimizes a weighted reconstruction loss where the error term is scaled by a token importance factor $R$; and third, "Quantize" adapts the GPTQ Hessian matrix to reflect these weights. Crucially, RSQ determines importance via the "**AttnCon**" strategy, which leverages attention coefficients to identify salient tokens, rather than relying solely on heuristics like First-N.

Experimental evaluation demonstrates RSQâ€™s superiority over baselines across W3A16 and W2A16 configurations. For LLaMA3-8B (W3A16), RSQ achieved a WikiText Perplexity of 9.27. More significantly, RSQ improved average accuracy on standard zero-shot reasoning benchmarksâ€”including PIQA, ARC-Easy, ARC-Challenge, HellaSwag, and Winograndeâ€”by +1.6% for LLaMA3-8B, +0.9% for Mistral-NeMo-12B, and +0.4% for Qwen-2.5-7B. Performance gains were most pronounced at W2A16 (2-bit weights), where RSQ reduced Wiki PPL to 16.26 compared to QuaRotâ€™s 22.71. This research establishes a paradigm shift in PTQ by demonstrating that semantic-aware error minimization is more effective than uniform reconstruction.

---

## Key Findings

*   **Superior Performance over Baselines:** RSQ consistently outperforms state-of-the-art methods like QuaRot and GPTQ across multiple model architectures and bit-precisions.
*   **Importance of Token Selection:** Using only the first 25% of tokens for reconstruction (a proxy for importance) lowered WikiText Perplexity in LLaMA3-8B (3-bit) to **9.27**, compared to 9.51 when using all tokens.
*   **Accuracy Gains in Reasoning Tasks:** RSQ achieved significant improvements in zero-shot reasoning accuracy:
    *   **+1.6%** for LLaMA3-8B
    *   **+0.9%** for Mistral-NeMo-12B
    *   **+0.4%** for Qwen-2.5-7B
*   **Enhanced Low-Bit Performance:** The benefits of RSQ are magnified at lower bit-precisions. At 2-bit, RSQ achieved a Wiki PPL of **16.26**, a drastic improvement over QuaRot's 22.71.
*   **Versatility:** The approach extends effectively to Vector Quantization (VQ) scenarios, reducing PPL from 24.69 to 20.08.

---

## Methodology

The RSQ framework addresses the limitation of uniform reconstruction in standard PTQ by introducing a token-importance-aware optimization process. Instead of treating all calibration tokens equally, RSQ identifies and prioritizes semantically significant tokens.

The methodology is built upon a three-stage pipeline designed to optimize weight quantization:

1.  **Rotation & Normalization:** The method first applies a randomized Hadamard matrix to smooth activation distributions and mitigate outliers. Concurrently, it converts LayerNorm to RMSNorm prior to rotation to further stabilize the quantization process.
2.  **Importance Weighting:** A scaling factor ($R$) is derived from the calibration data. This factor represents the importance of specific tokens, allowing the model to focus preservation efforts on the most critical parts of the input.
3.  **Weighted Optimization:** The reconstruction loss and Hessian matrix used in quantization are modified to incorporate the scaling factor, ensuring that quantization errors on important tokens are penalized more heavily than those on less important tokens.

---

## Contributions

*   **Proposal of RSQ:** Introduced a novel quantization framework, "Rotate, Scale, then Quantize," which integrates token importance directly into the PTQ optimization loop.
*   **Semantic-Aware Quantization:** Shifted the paradigm from uniform reconstruction error minimization to semantic-aware error minimization, proving that preserving critical tokens is vital for maintaining reasoning capabilities.
*   **AttnCon Strategy:** Developed and validated a strategy to compute token importance per layer using attention coefficients, offering a more robust alternative to simple heuristics like First-N.
*   **Empirical Validation:** Provided extensive experimental evidence across various model sizes (LLaMA, Mistral, Qwen) and precisions (W3A16, W2A16, VQ), demonstrating consistent gains in both perplexity and zero-shot accuracy.

---

## Technical Details

The technical implementation of RSQ modifies standard GPTQ frameworks through specific mathematical adjustments to the loss function and Hessian matrix.

### The Three-Step Process

1.  **Rotate**
    *   **Action:** Applies a randomized Hadamard matrix.
    *   **Purpose:** Smooths activation distributions and mitigates outliers.
    *   **Normalization:** Converts LayerNorm to RMSNorm before rotation to maintain consistency.

2.  **Scale**
    *   **Objective Mod:** Modifies the standard reconstruction loss to include a token importance scaling factor, $R$.
    *   **Loss Function:** Minimizes the term:
        $$ ||(WX - \tilde{W}X)R||^2_2 $$
    *   Where $W$ is the weight, $X$ is the input, and $\tilde{W}$ is the quantized weight.

3.  **Quantize**
    *   **Framework:** Utilizes the GPTQ framework.
    *   **Hessian Modification:** Updates the Hessian matrix to account for the importance scaling:
        $$ H_{RSQ} = 2XR^2X^\top $$

### Token Importance Strategies

RSQ relies on computing token importance per layer. The paper explored several methods:
*   **AttnCon (Primary Strategy):** Leverages attention coefficients to identify salient tokens.
*   **Heuristics:** Explored "First-N" (using the first N tokens).
*   **Dynamic Methods:** Explored "TokenSim" for similarity-based importance.

*Note: Computing these importance metrics introduces overhead during the calibration phase, but this cost is amortized during inference time.*

---

**Citations:** 40 references
**Analysis Quality Score:** 9/10