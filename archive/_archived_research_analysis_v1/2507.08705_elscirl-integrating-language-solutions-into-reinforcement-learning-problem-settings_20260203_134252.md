---
title: 'elsciRL: Integrating Language Solutions into Reinforcement Learning Problem
  Settings'
arxiv_id: '2507.08705'
source_url: https://arxiv.org/abs/2507.08705
generated_at: '2026-02-03T13:42:52'
quality_score: 8
citation_count: 19
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# elsciRL: Integrating Language Solutions into Reinforcement Learning Problem Settings

*Philip Osborne; Danilo S. Carvalho; AndrÃ© Freitas*

---

> ### ðŸ“Š Quick Facts
> ---
> *   **Quality Score:** 8/10
> *   **References:** 19 Citations
> *   **Core Framework:** `elsciRL` (extends HELIOS)
> *   **LLM Model:** Llama3.2
> *   **Embedding Model:** MiniLMv6
> *   **Environments Tested:** Classroom, FrozenLake, Maze

---

## Executive Summary

This paper addresses the challenge of effectively integrating natural language solutions, particularly those powered by Large Language Models (LLMs), into reinforcement learning (RL) environments. While RL agents are proficient at sequential decision-making, they often lack the interpretability and high-level guidance that language can provide. The authors identified a gap in the software infrastructure required to bridge these modalities, noting that existing methods often lack a unified, accessible framework. This integration matters because it promises to enhance agent performance through human-readable instructions and accelerates the broader scientific exploration of language-augmented artificial intelligence.

The key innovation is **`elsciRL`**, an open-source Python library that extends the HELIOS framework by implementing the "Language Adapter with Self-Completing Instruction Following" (LASIF) model. Technically, the system utilizes an LLM Adapter (specifically Llama3.2) to transform environmental states into language observations ($f: s \to o$), incorporating caching to handle partial observability. The instruction-following mechanism relies on pre-computed states and matches user inputs using Cosine Similarity with MiniLMv6 embeddings, including processes for instruction decomposition and validation. Additionally, the authors introduced a Python Flask-based graphical user interface that allows users to input text prompts, which the system uses to generate and self-complete instructions with minimal setup.

Empirical evaluation was conducted across Classroom, FrozenLake, and Maze environments using a "Single Setting" methodology (training with rewards, testing without). The results indicated that Q-Learning generally outperformed Deep Q-Networks (DQN) due to the simplicity of the tested problems. While the instruction-following mechanism successfully improved early training rewards and overall testing performance, the integration of LLM-generated descriptions did not generally aid performance and, in the Maze environment, resulted in a local optima trap. The authors explicitly note that these findings are subject to potential statistical limitations given the low sample sizes, which ranged between 5 and 10 runs.

---

## Key Findings

*   **Performance Enhancement:** Empirical results demonstrate that instructions generated via the proposed framework have the potential to improve the performance of reinforcement learning (RL) agents.
*   **Framework Extensibility:** The Language Adapter with Self-Completing Instruction framework can be successfully extended and implemented using Large Language Models (LLMs).
*   **Ease of Application:** The approach allows for the integration of language solutions into RL problems with minimal setup requirements, making it easily adaptable to new applications.
*   **Effective User Interaction:** The system successfully enables users to generate and self-complete instructions through text input provided via a GUI.

---

## Methodology

The authors developed `elsciRL`, an open-source Python library designed to bridge language solutions with reinforcement learning environments. The specific technical approach involves extending the "Language Adapter with Self-Completing Instruction" framework by incorporating Large Language Models (LLMs).

To facilitate this interaction, they implemented a novel Graphical User Interface (GUI) that allows users to input text prompts. The LLM processes these inputs to generate instructions, which are subsequently self-completed by the system. The methodology prioritizes a software-centric approach that requires minimal setup to apply to various applications.

---

## Technical Details

| Component | Specification |
| :--- | :--- |
| **Core Framework** | Extension of HELIOS using LASIF (Language Adapter with Self-Completing Instruction Following). |
| **LLM Adapter** | Utilizes **Llama3.2** to transform states to language observations ($f: s \to o$). Includes caching mechanisms to handle partial observability. |
| **Instruction Following** | Uses pre-computed states and **Cosine Similarity** (MiniLMv6 embeddings) for matching. Includes instruction decomposition and validation processes. |
| **User Interface** | Built with **Python Flask**; provides configuration and visualization capabilities. |

---

## Results

The framework was evaluated via a **'Single Setting'** methodology (training with rewards, testing without) across three distinct environments:

*   **Classroom**
*   **FrozenLake**
*   **Maze**

**Performance Observations:**
*   **Algorithm Comparison:** Q-Learning outperformed DQN due to the simplicity of the problems.
*   **Instruction Following:** Improved early training rewards and overall testing performance.
*   **LLM Descriptions:** Generated descriptions did not generally aid performance. In the Maze environment, this actually led to a local optima trap.
*   **Limitations:** Authors noted potential statistical limitations due to low sample sizes (5-10 runs).

---

## Contributions

*   **Software Infrastructure:** The release of `elsciRL`, an open-source Python library, providing a new tool for researchers to integrate language solutions into RL settings.
*   **Technical Framework Implementation:** The successful extension of the Language Adapter with Self-Completing Instruction framework using LLMs within a reward-based environment.
*   **User Interface Innovation:** The introduction of a novel GUI that streamlines the process of providing text input for LLMs to generate and self-complete instructions.
*   **Research Acceleration:** A contribution towards accelerating the evaluation of language solutions in RL environments, thereby fostering new opportunities for scientific discovery.