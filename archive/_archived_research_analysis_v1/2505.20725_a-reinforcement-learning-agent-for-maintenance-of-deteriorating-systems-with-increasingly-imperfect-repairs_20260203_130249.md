---
title: A reinforcement learning agent for maintenance of deteriorating systems with
  increasingly imperfect repairs
arxiv_id: '2505.20725'
source_url: https://arxiv.org/abs/2505.20725
generated_at: '2026-02-03T13:02:49'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# A reinforcement learning agent for maintenance of deteriorating systems with increasingly imperfect repairs

*Alberto Pliego MarugÃ¡n; JesÃºs M. Pinar-PÃ©rez; Fausto Pedro GarcÃ­a MÃ¡rquez*

***

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **References:** 40 citations
> *   **Cost Improvement:** ~14.5% reduction vs. traditional Limit Policy
> *   **Core Architecture:** Double Deep Q-Network (DDQN)
> *   **Key Innovation:** Handling non-stationary aging via imperfect repair modeling

***

## Executive Summary

This research addresses the optimization of maintenance strategies for deteriorating systems subject to "increasingly imperfect repairs," a specific type of aging where repair effectiveness diminishes cumulatively with each intervention. This creates a non-stationary stochastic environment where the systemâ€™s physical properties evolve over time, rendering traditional maintenance models that rely on fixed preventive thresholds or perfect repair assumptions ineffective. The challenge lies in managing the continuous degradation process (modeled as a gamma process) without manual intervention, a critical requirement for Industry 4.0 applications that demand automated, adaptive decision-making frameworks capable of handling dynamic system states.

The key innovation is the development of a **Double Deep Q-Network (DDQN)** reinforcement learning agent designed to handle continuous state spaces without relying on predefined thresholds. Technically, the authors define the state vector as $s_t = [X_t, N_t]$, where $X_t$ represents the current continuous degradation level and $N_t$ represents the cumulative number of repairs performed. This formulation allows a neural network function approximator to map the continuous degradation state to optimal maintenance actions. The architecture utilizes the DDQN mechanism to decouple target action selection from evaluation, effectively mitigating the overestimation bias common in standard Q-learning. This allows the agent to navigate the non-stationary transition functions inherent in the model and learn a dynamic policy that adapts to the system's specific aging profile.

Evaluating the agent using long-run maintenance cost as the primary metric, the study demonstrates that the DDQN approach achieves substantial economic benefits over standard benchmarks. Specifically, the agent realized a cost reduction of approximately **14.5%** compared to the traditional "Limit Policy" (a fixed-threshold preventive strategy) and significantly outperformed corrective-only maintenance strategies. The agent exhibited high convergence speed and stability, successfully identifying optimal intervention thresholds that dynamically adjusted to the system's degradation history. These results confirm that the DDQN agent can effectively minimize operational costs in complex, continuous environments without human tuning of intervention parameters.

This work advances the field of maintenance engineering by bridging the gap between theoretical reinforcement learning models and the physical realities of non-stationary system aging. By successfully applying deep RL to a framework that accounts for the diminishing returns of repairs, the authors provide a realistic, automated tool for asset management that eliminates reliance on expert heuristics. The ability to optimize maintenance without predefined thresholds offers a scalable, flexible solution for Industry 4.0, enabling more responsive and cost-effective operations. This research sets a precedent for utilizing advanced machine learning architectures to solve complex reliability problems where system dynamics evolve over time.

***

## Key Findings

*   **Significant Cost Improvement:** The developed Double Deep Q-Network (DDQN) agent significantly improves long-run maintenance costs compared to common maintenance strategies.
*   **Handling Imperfect Repairs:** The agent successfully operates within a novel maintenance framework where repairs are "increasingly imperfect," meaning system repair effectiveness diminishes as more repairs are performed.
*   **Elimination of Predefined Thresholds:** The proposed approach eliminates the need for a predefined preventive threshold, allowing the agent to function effectively within a continuous degradation state space.
*   **High Flexibility:** The agent demonstrates high flexibility and adaptability, learning optimal behaviors across various scenarios and environmental parameter changes.

***

## Methodology

The research employs a Reinforcement Learning (RL) approach with the following characteristics:

*   **Core Architecture:** Centered on a Double Deep Q-Network (DDQN) architecture to address decision-making in complex environments.
*   **Environment Modeling:** The environment was modeled using a **gamma degradation process** combined with a novel maintenance policy that simulates real-world deterioration through "increasingly imperfect repairs."
*   **Training Process:** The RL agent was trained to generate maintenance policies by interacting with this environment.
*   **State Space Handling:** Specifically designed to handle continuous degradation states without relying on fixed preventive thresholds.

***

## Technical Details

*   **Network Architecture:** Utilizes a Double Deep Q-Network (DDQN) to address overestimation bias by decoupling target action selection and evaluation.
*   **State Space:** Operates within a continuous degradation state space using a neural network function approximator.
*   **Imperfect Repair Framework:** Introduces a framework for 'increasingly imperfect repairs' where repair effectiveness diminishes with cumulative repairs, requiring handling of a non-stationary transition function.
*   **Dynamic Policy Learning:** Eliminates the need for predefined preventive thresholds, learning a dynamic policy that maps continuous degradation states to optimal maintenance actions.

***

## Contributions

*   **Novel Degradation Modeling:** Introduction of a maintenance model that integrates a gamma degradation process with the concept of increasingly imperfect repairs, offering a more realistic representation of physical system aging.
*   **Advancement in RL for Maintenance:** Development of a DDQN-based agent capable of handling continuous state spaces and optimizing maintenance without predefined thresholds, addressing specific limitations of traditional optimization methods.
*   **Industry 4.0 Application:** Provision of a flexible, machine-learning-based solution that addresses the new paradigms of maintenance optimization required by modern industrial challenges, demonstrating both cost-efficiency and adaptability.

***

## Results

*   **Cost Efficiency:** The DDQN agent achieved significant improvements in long-run costs compared to common maintenance strategies such as corrective or fixed-threshold preventive maintenance.
*   **Quantitative Performance:** Achieved a cost reduction of approximately **14.5%** compared to the traditional "Limit Policy."
*   **Adaptability:** The agent demonstrated high adaptability, stability across various scenarios, and robustness to environmental parameter changes.
*   **Autonomy:** It operated successfully within a complex continuous state space without human intervention for threshold tuning, validating the practical applicability of the RL approach.