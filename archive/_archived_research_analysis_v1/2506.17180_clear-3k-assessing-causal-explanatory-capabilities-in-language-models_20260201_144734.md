# CLEAR-3K: Assessing Causal Explanatory Capabilities in Language Models
*Naiming Liu; Richard Baraniuk; Shashank Sonkar*

---

> ### ðŸ“Š Quick Facts
>
> *   **Dataset:** CLEAR-3K (3,008 questions)
> *   **Model Scale:** 0.5B to 72B parameters
> *   **Models Evaluated:** 21 state-of-the-art LLMs
> *   **Primary Metric:** Matthews Correlation Coefficient (MCC)
> *   **Performance Ceiling:** MCC ~0.55 (Plateau)
> *   **Domain Distribution:** 84% STEM, 16% Humanities
> *   **Quality Score:** 9/10

---

## Executive Summary

This research addresses a fundamental limitation in current Large Language Models (LLMs): the inability to reliably distinguish between true causal explanatory relationships and mere semantic relatedness. While LLMs demonstrate impressive capabilities in pattern matching and fluency, it is unclear whether these systems possess a genuine understanding of causalityâ€”specifically, if they can identify *why* an assertion is trueâ€”or if they simply rely on surface-level lexical associations. This failure is critical for the deployment of LLMs in scientific, medical, and legal domains where reasoning through cause-and-effect is required, rather than simply retrieving correlated information.

The key innovation is the introduction of **CLEAR-3K**, a benchmark specifically designed to isolate causal reasoning from semantic fluency. The dataset comprises 3,008 human-curated assertion-reason questions, predominantly in STEM fields, distinguished by a 4-category annotation scheme. Technically, the authors formulate a binary "Causal Explanation Task," requiring models to determine if a provided Reason (R) serves as a valid causal explanation for an Assertion (A). This design explicitly penalizes models attempting to solve the problem through shallow heuristics like keyword overlap or semantic proximity, ensuring that correct answers require inference regarding the causal link rather than just textual similarity.

An evaluation of 21 state-of-the-art models (ranging from 0.5B to 72B parameters) revealed significant constraints in current architectural approaches. Regardless of model scale, performance plateaued at a Matthews Correlation Coefficient (MCC) of just 0.55, a score indicating weak correlation and substantial room for improvement. Furthermore, the analysis identified a distinct behavioral shift induced by scaling: smaller models tend to be "skeptical," frequently rejecting true causal relationships, whereas larger models become "permissive," erroneously accepting assertions based on semantic relatedness alone. This indicates that increasing parameters merely shifts the decision threshold from false negatives to false positives without resolving the underlying reasoning deficit.

This work delivers a critical empirical finding: simply scaling model parameters does not resolve the core issue of causal discrimination. By demonstrating that larger models become more gullible to semantic correlations rather than more discerning of causality, the paper challenges the prevailing "bigger is better" narrative regarding reasoning capabilities. CLEAR-3K provides the field with a rigorous tool to measure this specific gap, signaling to the community that future advances in reasoning will likely require novel training objectives or architectural innovations rather than continued brute-force scaling.

---

## Key Findings

*   **Semantic vs. Causal Confusion:** Language models frequently fail to distinguish between genuine causal explanatory relationships and mere semantic relatedness, often relying on lexical and semantic overlap as a proxy for causality.
*   **Scaling Shifts Behavior:** As parameter count increases, models exhibit a distinct behavioral shift, moving from being **overly skeptical** about causal relationships to becoming **excessively permissive** in accepting them.
*   **Performance Plateau:** Despite the increase in model size and shifts in decision thresholds, the performance of even the best-performing models plateaus at a Matthews Correlation Coefficient (MCC) of just **0.55**, indicating significant limitations in current causal reasoning capabilities.

---

## Methodology

The methodology involved constructing **CLEAR-3K**, a dataset comprising 3,000 assertion-reasoning questions. The evaluation was framed as a classification challenge requiring models to distinguish between semantic relatedness and true causal explanatory relationships. A comprehensive assessment was conducted on **21 state-of-the-art language models**, covering parameter sizes from **0.5 billion to 72 billion**.

---

## Technical Details

The technical implementation of the study is characterized by the following specifications:

| Component | Specification |
| :--- | :--- |
| **Dataset Name** | CLEAR-3K |
| **Data Volume** | 3,008 human-curated questions |
| **Subject Breakdown** | 8 subjects (84% STEM, 16% Humanities) |
| **Annotation Scheme** | 4-category annotation |
| **Task Formulation** | Binary 'Causal Explanation Task' (Output: 'yes' or 'no') |
| **Contexts Analyzed** | True-True and False contexts |
| **Models Tested** | 21 models (ranging 0.5B â€“ 72B parameters) |

---

## Results

The evaluation results highlight specific trends and limitations in model performance:

*   **Metric:** The primary metric used was the Matthews Correlation Coefficient (MCC).
*   **The Plateau:** Models plateau at an MCC of **0.55** regardless of parameter size (up to 72B).
*   **Behavioral Shift:**
    *   **Small Models:** Tend to be "skeptical" (frequent rejection of causal claims).
    *   **Large Models:** Tend to be "permissive" (frequent acceptance of causal claims).
*   **Heuristic Reliance:** Evidence suggests models rely on **semantic overlap heuristics** rather than distinguishing genuine causality.

---

## Contributions

*   **Benchmark Introduction:** Introduced CLEAR-3K as a specialized, large-scale benchmark for evaluating causal explanatory capabilities.
*   **Empirical Evidence:** Provided empirical evidence that current LLMs rely on shallow heuristics (like semantic similarity) rather than deep causal inference.
*   **Scaling Analysis:** Offered critical analysis of how model scaling affects causal reasoning, demonstrating that simply increasing parameters does not resolve the fundamental issue of distinguishing causality from correlation/semantic similarity.

---
*References: 38 citations*