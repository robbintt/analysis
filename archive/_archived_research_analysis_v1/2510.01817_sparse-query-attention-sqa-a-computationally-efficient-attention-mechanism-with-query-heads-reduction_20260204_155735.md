---
title: 'Sparse Query Attention (SQA): A Computationally Efficient Attention Mechanism
  with Query Heads Reduction'
arxiv_id: '2510.01817'
source_url: https://arxiv.org/abs/2510.01817
generated_at: '2026-02-04T15:57:35'
quality_score: 8
citation_count: 29
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Sparse Query Attention (SQA): A Computationally Efficient Attention Mechanism with Query Heads Reduction

*Adam Filipek*

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **"Performance Gain"** | Up to **3x** throughput improvement |
| **"Sequence Length"** | Effective on **32k ‚Äì 200k** tokens |
| **"Optimization Target"** | Compute Bottleneck (FLOPs) |
| **"Core Mechanism"** | Query Head Reduction ($H_q$) |
| **"Quality Impact"** | Minimal impact on accuracy |
| **"Paper Quality"** | **8/10** |
| **"References"** | 29 Citations |

---

### üìù Executive Summary

Standard Transformer architectures face significant computational bottlenecks when processing long sequences, primarily due to the quadratic complexity of the attention mechanism. While existing optimizations like Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) successfully address memory bandwidth limitations by reducing Key and Value heads, they do not resolve the fundamental computational (FLOPs) bottleneck associated with attention score calculation. This paper addresses the critical need for reducing the arithmetic intensity of the $QK^T$ operation in compute-bound scenarios‚Äîsuch as pre-training, fine-tuning, and long-context inference‚Äîwhere memory bandwidth is not the primary constraint.

The research introduces **Sparse Query Attention (SQA)**, a novel architectural modification that reduces the number of Query heads ($H_q$) rather than the Key/Value heads. Mathematically, this reduces the computational cost of the attention matrix multiplication by a factor of $H/H_q$. Unlike sliding window approaches that limit context, SQA preserves the full attention context and maintains the dimensionality of K/V projections to ensure high capacity for context representation.

Empirical benchmarks indicate that SQA achieves throughput improvements of up to **3x** in compute-bound scenarios, outperforming traditional methods that primarily target memory bandwidth. The mechanism demonstrated high efficacy on very long sequences ranging from **32k to 200k tokens**, offering distinct speed advantages during training and encoding phases where MQA and GQA typically offer none. Preliminary experiments regarding model retention showed minimal impact on overall quality, confirming that the theoretical FLOPs reduction translates directly to practical performance gains without significant accuracy degradation.

---

## üîë Key Findings

*   **Computational Efficiency:** SQA reduces computational complexity (FLOPs) by specifically decreasing the number of Query heads, a distinct approach from methods that reduce Key/Value heads.
*   **Throughput Gains:** Achieves throughput improvements of **up to 3x** in computation-bound scenarios such as pre-training and fine-tuning.
*   **Long-Sequence Efficacy:** Demonstrates high performance on long sequences ranging from **32k to 200k tokens**.
*   **Quality Retention:** Preliminary experiments indicate minimal impact on overall model quality despite the reduction in computational complexity.
*   **Targeted Optimization:** Unlike MQA and GQA, SQA targets the compute bottleneck of attention score calculation ($QK^T$) rather than memory bandwidth limitations.

---

## üõ†Ô∏è Methodology

The research proposes **Sparse Query Attention (SQA)**, a modification to the standard Multi-Head Attention (MHA) mechanism. The core methodology involves:

1.  **Architectural Modification:** Reducing the number of Query heads instead of sharing Key and Value projections (as seen in MQA/GQA).
2.  **Theoretical Foundation:** Establishing the mathematical formulation for SQA to prove the reduction in complexity.
3.  **Variant Analysis:** Examining a family of architectural variants to determine optimal configurations.
4.  **Empirical Validation:** Conducting benchmarks on long sequences (32k‚Äì200k tokens) to assess both computational throughput and the model's ability to retain quality.

---

## üß¨ Technical Details

SQA optimizes the attention mechanism by focusing on the arithmetic intensity required for score calculation.

*   **Target Operation:** Focuses on the $QK^T$ operation, the primary computational bottleneck.
*   **Mechanism:** Reduces the number of Query heads ($H_q$) while keeping Key/Value heads ($H$) unchanged.
*   **Complexity Reduction:** Computational cost is reduced by a factor of $H/H_q$.
*   **Context Preservation:** Maintains K/V dimensions to ensure high-capacity context representation without relying on sliding windows.
*   **Architectural Variants:**
    *   **Symmetric SQA (sSQA)**
    *   **Extreme SQA (xSQA)**
*   **Compatibility:** Maintains standard Transformer compatibility and full attention context.
*   **Differentiation:**
    *   **vs. MQA/GQA:** SQA targets compute (FLOPs); MQA/GQA target memory bandwidth.
    *   **vs. Sliding Window:** SQA maintains full context; Sliding Window limits it.

---

## ‚úÖ Contributions

*   **New Optimization Path:** Identification of a novel optimization focusing on Query head reduction to lower FLOPs, addressing a gap left by methods like MQA and GQA.
*   **Mathematical Foundation:** Establishment of the mathematical formulation and theoretical underpinnings for reducing Query heads to decrease attention complexity.
*   **Practical Solution:** Provision of a viable solution to the quadratic computational complexity barrier in Transformers for long-context applications.
*   **Empirical Validation:** Provision of evidence demonstrating substantial improvements in training and processing efficiency without significant loss in model accuracy.

---

## üìà Results

*   **Throughput:** Achieved **up to 3x** improvement in compute-bound scenarios.
*   **Sequence Handling:** Demonstrated high efficacy on long sequences (**32k to 200k tokens**).
*   **Training Speed:** Provides speed advantages in training and encoding where MQA and GQA typically offer no benefit.
*   **Accuracy:** Preliminary experiments indicate minimal impact on model quality, with theoretical FLOPs reduction proportional to the reduction in query heads.
