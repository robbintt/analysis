---
title: 'AirQA: A Comprehensive QA Dataset for AI Research with Instance-Level Evaluation'
arxiv_id: '2509.16952'
source_url: https://arxiv.org/abs/2509.16952
generated_at: '2026-02-03T13:39:54'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# AirQA: A Comprehensive QA Dataset for AI Research with Instance-Level Evaluation

*Tiancheng Huang; Ruisheng Cao; Yuxin Zhang; Zhangyi Kang; Zijian Wang; Chenrun Wang; Yijie Luo; Hang Zheng; Lirong Qian; Lu Chen; Kai Yu*

---

### ðŸ“Š Quick Facts

| Metric | Value |
| :--- | :--- |
| **Total Papers** | 13,948 |
| **Total Questions** | 1,246 |
| **Task Categories** | 4 (Single-doc, Multi-doc, Retrieval, Comprehensive) |
| **Evaluation Method** | Instance-level (Objective checks + GPT-4o-mini) |
| **Avg Papers/Question** | 1.63 |
| **Quality Score** | 9/10 |

---

## Executive Summary

Evaluating AI agents on scientific literature is hindered by the lack of comprehensive, realistic benchmarks capable of assessing complex reasoning and tool-use, alongside a critical scarcity of high-quality interaction trajectory data needed to train smaller models. Existing datasets often fail to cover the necessary breadth of task typesâ€”such as multi-document analysis or cross-modal interpretationâ€”or rely on expensive, subjective evaluation methods. This paper addresses these dual challenges by establishing a rigorous standard for scientific QA and providing a mechanism to overcome data limitations for model training.

The authors introduce **AirQA**, a human-annotated dataset comprising 1,246 questions grounded in 13,948 AI research papers, alongside the **ExTrActor** automated synthesis framework. AirQA is technically distinct in its coverage of four task types and five modalities. It employs a rigorous instance-level evaluation system using 19 Python functions for objective checks and GPT-4o-mini for subjective logic, ensuring strict, binary scoring based on specific "scoring points."

The AirQA dataset presents a challenging workload with a balanced distribution of questions. Benchmarks reveal that most current state-of-the-art LLMs significantly underperform on this dataset; for instance, GPT-4o achieved only approximately **41.7% accuracy** under the strict evaluation protocol. However, small language models fine-tuned using data synthesized by the ExTrActor framework demonstrated marked improvement, achieving accuracy scores nearing **40%**, which makes them comparable to large, resource-intensive models.

This work significantly advances the field by providing a cost-effective, objective benchmark that fills a critical gap in the evaluation of scientific AI agents. By demonstrating that automated instruction tuning via ExTrActor can bridge the performance gap between small and large models, the research offers a scalable path toward deploying efficient, capable AI agents in research settings.

---

## Key Findings

*   **LLM Underperformance:** Evaluations using the AirQA dataset reveal that most current open-source and proprietary Large Language Models (LLMs) underperform, validating the dataset's quality and the complexity of scientific QA tasks.
*   **ExTrActor Efficacy:** The ExTrActor framework significantly improves the multi-turn tool-use capabilities of smaller language models.
*   **Parity Achieved:** Through the use of ExTrActor-generated data, small models are able to achieve performance levels comparable to larger, more resource-intensive models in scientific paper QA workflows.

---

## Methodology

The research employs a dual-methodology approach consisting of dataset construction and an automated synthesis framework.

### 1. Dataset Construction (AirQA)
*   **Scope:** A human-annotated QA dataset for Artificial Intelligence encompassing 13,948 papers.
*   **Volume:** 1,246 questions designed with multi-task, multi-modal capabilities.
*   **Metrics:** Incorporates instance-level evaluation metrics to ensure rigorous assessment.

### 2. Automated Synthesis Framework (ExTrActor)
*   **Purpose:** Developed to address training data shortages for interactive agents.
*   **Mechanism:** Utilizes three distinct LLM-based agents to automatically perform example generation and the collection of interaction trajectories.
*   **Efficiency:** Operates without human intervention, solving the bottleneck of high-quality interaction trajectory scarcity.

---

## Technical Details

### Task Categorization
The AirQA approach categorizes tasks into four distinct types:
*   Single-doc Detail
*   Multiple-doc Analysis
*   Paper Retrieval
*   Comprehensive QA

### Multi-Modal Elements
The dataset incorporates a diverse range of data modalities:
*   **Text**
*   **Table**
*   **Image**
*   **Formula**
*   **Metadata**

### Evaluation Methodology
*   **Approach:** Utilizes an instance-level evaluation methodology based on objective 'scoring points' and strict output formatting.
*   **Grading:** Graded by 19 Python functions comprising objective checks and GPT-4o-mini for subjective logic.
*   **Output:** Results in binary scores (pass/fail).

### Dataset Construction Pipeline
*   **Annotators:** Constructed by 26 AI experts.
*   **Verification:** Uses an automated inspection pipeline.
*   **Sources:** Papers are sourced from arXiv and major conferences.

### ExTrActor Framework
*   Generates training data specifically to enhance multi-turn tool-use capabilities.
*   Enables smaller language models to match the performance of larger models.

---

## Results

*   **Dataset Composition:** The AirQA dataset comprises 1,246 examples across 13,948 papers, with an average involvement of **1.63 papers** per example.
*   **Question Distribution:**
    *   **28%** Single-doc
    *   **26%** Multiple-doc
    *   **23%** Retrieval
    *   **23%** Comprehensive
*   **Modality Split:**
    *   **50%** Text-only
    *   **50%** Multi-modal (Tables and Images each account for 17%).
*   **Question Complexity:** The average question length is **34.84 words**.
*   **Dataset Uniqueness:** Compared to similar datasets, AirQA is unique in its coverage of all four task types and five element categories and is cost-effective due to predominantly objective evaluation.
*   **Model Performance:**
    *   Most LLMs underperform on the dataset.
    *   Smaller models trained with data generated by the ExTrActor framework achieve performance comparable to larger, resource-intensive models.

---

## Contributions

1.  **AirQA Dataset:** Introduction of a comprehensive, realistic benchmark for evaluating AI research agents, filling a critical gap in high-quality benchmarks for scientific QA.
2.  **ExTrActor Framework:** A novel solution to the bottleneck of high-quality interaction trajectory scarcity, enabling the automated generation of training data for interactive agents.
3.  **Advancement of Small Models:** Empirical evidence that automated instruction tuning can bridge the performance gap between small and large models in complex, multi-turn tool-use scenarios.

---

**Quality Score:** 9/10  
**References:** 40 citations