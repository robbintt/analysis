# VLURes: Benchmarking VLM Visual and Linguistic Understanding in Low-Resource Languages

*Jesse Atuhurra; Iqra Ali; Tomoya Iwakura; Hidetaka Kamigaito; Tatsuya Hiraoka*

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Top Model** | GPT-4o (90.8% accuracy) |
| **Human Baseline** | ~97.5% accuracy |
| **Languages Covered** | English, Japanese, Swahili, Urdu |
| **Total Tasks** | 8 Vision-Language Tasks + 1 Unrelatedness Task |
| **Dataset Volume** | ~1,000 image-text pairs per language |
| **Avg. Text Length** | Range from 270 words (En) to 392 words (Sw) |

---

### üìã Executive Summary

#### **Problem**
Current benchmarks for Vision-Language Models (VLMs) are predominantly constrained to English and short-context scenarios. This creates a significant blind spot regarding model performance on low-resource languages (data scarcity, distinct structures) and fails to evaluate fine-grained, long-content understanding. As VLMs are deployed globally, there is a critical need for robust evaluation frameworks that go beyond English-centric environments.

#### **Innovation**
The authors introduce **VLURes**, the first multilingual benchmark for fine-grained visual and linguistic understanding under **long-text conditions**.
*   **Data Source:** Curated from web resources across four languages.
*   **Alignment:** Uses CLIP ViT-L/14 with a 0.15 cosine similarity threshold and bipartite assignment.
*   **Taxonomy:** 8 tasks split into *Image-Only* (Object/Scene/Relation understanding) and *Image+Text* (Matching, VQA).
*   **Differentiation:** Includes a specialized **'Unrelatedness' task** to test detection of mismatched pairings, ensuring true comprehension.

#### **Results**
Evaluation of ten state-of-the-art VLMs revealed:
*   **Performance Gap:** GPT-4o leads at 90.8% but lags behind human performance (~97.5%) by **6.7%**.
*   **Data Specs:** ~1,000 pairs per language with substantial context lengths (e.g., 392 words for Swahili).
*   **Hierarchy:** Proprietary models significantly outperformed open-source counterparts.
*   **Validation:** Long-text settings introduce unique challenges in fine-grained understanding that short-text benchmarks fail to capture.

#### **Impact**
VLURes shifts the research focus from generic English tasks to complex, multilingual scenarios. By providing novel resources for low-resource languages (Swahili and Urdu), the benchmark enables the community to identify specific architectural weaknesses, paving the way for globally inclusive models.

---

### üîç Key Findings

*   **Human vs. AI Gap:** Even the best-performing model (GPT-4o) trails human performance by a margin of **6.7%** (90.8% vs ~97.5%).
*   **Proprietary Dominance:** A substantial performance gap exists between proprietary models and open-source VLMs.
*   **Language Disparities:** Significant accuracy variations were observed across the four evaluated languages (English, Japanese, Swahili, Urdu).
*   **The Long-Text Challenge:** The study confirms that long-text settings reveal unique difficulties in fine-grained understanding that are invisible in short-text benchmarks.

---

### üß™ Methodology

The researchers adopted a comprehensive approach to benchmark construction and model evaluation:

*   **Benchmark Construction:** Introduced VLURes, a novel benchmark comprising eight vision-and-language tasks plus one "unrelatedness" task.
*   **Data Curation:** Datasets were curated from web resources in target languages, covering **ten image categories** with rich text contexts tailored for low-resource languages.
*   **Model Evaluation:** Ten state-of-the-art VLMs were evaluated.
*   **Assessment Protocol:** Models were prompted for responses and rationales. Assessment utilized a hybrid approach:
    *   Automatic metrics.
    *   Manual evaluation by native speakers.

---

### ‚öôÔ∏è Technical Details

#### Dataset Construction & Alignment
*   **Source:** Article-level context from web resources in **English, Japanese, Swahili, and Urdu**.
*   **Image-Text Alignment:** Utilized CLIP ViT-L/14.
    *   Cosine Similarity Threshold: 0.15
    *   Assignment Approach: Bipartite assignment.
*   **Content Safety:** Spans 10 categories with strict sanitization; excludes NSFW content.

#### Task Taxonomy
The benchmark is divided into two primary categories with specific sub-tasks:

1.  **Image-Only Tasks**
    *   Object Recognition
    *   Scene Understanding
    *   Relationship Understanding
    *   Semantic Segmentation
    *   Image Captioning
2.  **Image + Text Tasks**
    *   Image-Text Matching
    *   Unrelatedness (Specialized task)
    *   Visual Question Answering (VQA)

#### Dataset Statistics (Language Context)

| Language | Average Text Length |
| :--- | :--- |
| **English** | 270 words |
| **Japanese** | 447 characters |
| **Swahili** | 392 words |
| **Urdu** | 373 words |

#### Evaluation Framework
*   **Scoring Method:** Hybrid scoring combining **LLM-as-a-Judge** with manual native speaker assessment.
*   **Input:** Models are evaluated via generated rationale prompts to assess reasoning capabilities.

---

### üìà Benchmark Results

The evaluation provided quantitative insights into model capabilities and dataset complexity:

*   **Leader:** **GPT-4o** achieved the highest accuracy at **90.8%**.
*   **Performance Gap:** A clear gap remains between proprietary models and open-source alternatives.
*   **Language Variance:** Accuracy fluctuated significantly across the tested languages, highlighting bias in English-centric training.
*   **Contextual Difficulty:** Experiments demonstrated that the long-text setting (avg. ~300-400 words/chars) creates distinct hurdles for fine-grained understanding compared to standard short-context benchmarks.

---

### üèÅ Contributions

1.  **New Benchmark Standard:** Established VLURes as the first benchmark to evaluate fine-grained visual and linguistic understanding in a multilingual context under **long-text conditions**.
2.  **Resource Expansion:** Provided new vision-language resources and datasets for **low-resource languages** (Swahili and Urdu), which are historically underrepresented.
3.  **Comprehensive Analysis:** Delivered a rigorous evaluation of ten VLMs, highlighting specific limitations in open-source models and emphasizing the need for further development to close the gap with human performance.

---
**Document Metadata**
*   **Quality Score:** 8/10
*   **References:** 40 citations