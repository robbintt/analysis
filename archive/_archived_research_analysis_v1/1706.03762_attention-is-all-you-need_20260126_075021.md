---
title: Attention Is All You Need
arxiv_id: '1706.03762'
source_url: https://arxiv.org/abs/1706.03762
generated_at: '2026-01-26T07:50:21'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

***

# Attention Is All You Need

*Noam Shazeer, Attention Is, Google Brain, All You, Google Research, Niki Parmar, Ashish Vaswani, Jakob Uszkoreit*

---

> ### ðŸ“Š Quick Facts Sidebar
>
> *   **Quality Score:** 9/10
> *   **Total Citations:** 40
> *   **Training Time:** 3.5 days (on 8 GPUs)
> *   **Top BLEU Score:** 41.8 (En-to-Fr)
> *   **Architecture:** Encoder-Decoder (Self-Attention only)

---

## Executive Summary

### **Problem**
Prior to this work, state-of-the-art sequence transduction models relied heavily on RNNs, LSTMs, and CNNs, which suffered from inherent computational inefficiencies. RNNs and LSTMs processed data sequentially, preventing parallelization and significantly slowing training, while CNNs, though more parallelizable, struggled to capture long-range dependencies effectively. This research addressed the critical need for a novel architecture that maximizes computational efficiency through massive parallelization while effectively modeling complex dependencies between distant sequence elements without the constraints of recurrence.

### **Innovation**
The core innovation is the "Transformer," a neural network architecture that relies entirely on attention mechanisms, dispensing with recurrence and convolution completely. The system utilizes an encoder-decoder structure composed of stacks of identical layers, leveraging "self-attention" to aggregate information from the entire sequence simultaneously regardless of positional distance. Key technical features include Scaled Dot-Product Attention, multi-head attention to capture diverse representation subspaces, and positional encodings to handle token order, all integrated within a framework of residual connections and LayerNorm.

### **Results**
The Transformer established new state-of-the-art benchmarks in machine translation, with the "Transformer Big" model achieving BLEU scores of 28.4 on the WMT 2014 English-to-German task and 41.8 on English-to-French. Notably, these superior results were achieved with high efficiency; while the base model could reach competitive performance in as little as 12 hours on 8 NVIDIA P100 GPUs, the Big model required only 3.5 days on the same hardware to attain these record-breaking scores. Additionally, the architecture proved versatile, generalizing effectively to English constituency parsing with strong performance.

### **Impact**
This research represents a paradigm shift in natural language processing, proving that attention mechanisms are sufficient to achieve state-of-the-art results without recurrence or convolution. The Transformerâ€™s ability to train significantly faster while resolving long-range dependency issues has established it as the dominant architecture for sequence modeling tasks. Its design has paved the way for modern large-scale language models (LLMs), fundamentally influencing subsequent research across machine translation, text generation, and comprehension.

---

## Key Findings

*   **Performance:** The Transformer architecture outperformed existing state-of-the-art models on WMT 2014 English-to-German and English-to-French tasks, achieving BLEU scores of **28.4** and **41.8** respectively.
*   **Efficiency:** It requires significantly less training time, with specific runs completed in **3.5 days on 8 GPUs**, and offers superior parallelization capabilities compared to previous models.
*   **Versatility:** The model generalizes well to other tasks, demonstrating strong performance in English constituency parsing.

---

## Methodology

The researchers proposed a novel neural network architecture called the **Transformer**, which relies solely on attention mechanisms and eliminates the need for recurrent and convolutional neural networks.

The evaluation process consisted of:
1.  **Primary Tasks:** Testing on two machine translation benchmarks (WMT 2014 English-to-German and English-to-French).
2.  **Secondary Tasks:** Application to English constituency parsing to validate generalization capabilities beyond translation.

---

## Contributions

The main contributions of this paper include:

*   **New Architecture:** Introduction of a new network architecture based entirely on attention mechanisms.
*   **Benchmarking:** Establishment of new state-of-the-art performance benchmarks in machine translation.
*   **Efficiency:** Demonstration of efficiency breakthroughs regarding training speed and parallelization.
*   **Validation:** Proof of the model's versatility across different tasks, such as constituency parsing.

---

## Technical Details

### **Core Architecture**
*   **Type:** Encoder-decoder architecture relying entirely on self-attention mechanisms.
*   **Design:** Eschews recurrence and convolution to enable parallelization.
*   **Path Efficiency:** Path length between positions is constant.

### **Stacks & Composition**
*   **Layers:** Both encoder and decoder have **N=6** identical layers with dimensionality **d_model=512**.
*   **Encoder:** Composed of Multi-head self-attention and a position-wise fully connected feed-forward network.
*   **Decoder:** Composed of Masked multi-head self-attention, multi-head attention over encoder output, and a position-wise feed-forward network.

### **Add-ons & Normalization**
*   **Residual Connections:** Implemented around each sub-layer.
*   **Normalization:** Followed by LayerNorm.

### **Attention Mechanism**
*   **Formula:** Uses Scaled Dot-Product Attention computed via `softmax(QK^T / sqrt(d_k))V`.
*   **Multi-head:** Projects Q, K, V h times to capture diverse information.

### **Feed-Forward Networks**
*   **Structure:** Two linear transformations with ReLU activation.
*   **Dimensions:** Input/output dimension is 512; inner-layer dimension is 2048.

---

## Results

### **Translation Performance (WMT 2014)**
*   **English-to-German:** BLEU Score of **28.4** (New State-of-the-Art).
*   **English-to-French:** BLEU Score of **41.8** (New State-of-the-Art).

### **Training Efficiency**
*   **Base Model:** Reached competitive translation quality in as little as **12 hours** (8 NVIDIA P100 GPUs).
*   **Big Model:** Achieved record-breaking scores in **3.5 days** on the same hardware.

### **Generalization & Comparison**
*   **Parsing:** Demonstrated the ability to generalize effectively to English constituency parsing.
*   **vs. RNN/LSTM:** Offers significantly higher parallelization than recurrent models.
*   **vs. CNNs:** Reduces long-range dependency operations from linear/logarithmic to constant compared to convolutional models (e.g., ConvS2S, ByteNet).

---

**REFERENCES:** 40 citations