# CoDeQ: End-to-End Joint Model Compression with Dead-Zone Quantizer for High-Sparsity and Low-Precision Networks

*Authors: Jonathan Wensh√∏j; Tong Chen; Bob Pepin; Raghavendra Selvan*

---

## üóÇÔ∏è Quick Facts Sidebar

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Total Citations** | 23 |
| **Core Innovation** | Unified Pruning & Quantization |
| **Primary Metric** | Bit-Operations (BOPs) |
| **Best BOP Reduction** | ~2.62% of baseline (ResNet-50) |
| **Optimization** | Single-loop, fully differentiable |

---

## Executive Summary

Deep neural network compression is critical for efficient deployment on resource-constrained hardware, yet existing methods often treat pruning (removing weights) and quantization (reducing bit-width) as disjoint, sequential processes. This separation necessitates cumbersome multi-stage pipelines, auxiliary training loops, and intricate hyperparameter tuning, resulting in significant computational overhead and engineering complexity. Consequently, there is a need for a unified framework that can simultaneously determine optimal sparsity patterns and precision levels within a standard training regimen to achieve extreme compression without compromising model accuracy.

The key innovation is **CoDeQ**, a fully differentiable, end-to-end framework that theoretically unifies pruning and quantization into a single operator. The core technical breakthrough leverages the mathematical equivalence between a scalar quantizer's "dead-zone" and magnitude pruning; by parameterizing the width of this dead-zone, the method can control sparsity directly. CoDeQ utilizes a **single global hyperparameter** to explicitly regulate sparsity, alongside an additional parameter for bit-width control, enabling the method to operate effectively in **both fixed-precision and mixed-precision quantization regimes**. This approach optimizes these parameters simultaneously via backpropagation within a single training loop, eliminating the need for auxiliary regularization or iterative search procedures.

Evaluating performance using Bit-Operations (BOPs) to measure theoretical throughput, CoDeQ achieves state-of-the-art efficiency across various architectures. On CIFAR-10, the **fixed 4-bit variant** of CoDeQ notably outperformed leading methods like QST. On ImageNet, ResNet-18 reduced BOPs to approximately **5%** of the original requirement while matching QST's accuracy levels. More significantly, ResNet-50 achieved a reduction to just **2.62%** of the baseline BOPs‚Äîoutperforming QST's 4.5%‚Äîwith a minimal Top-1 accuracy drop of only 0.81%. Additionally, the method demonstrated generalizability by achieving effective results on transformer architectures like TinyViT.

CoDeQ represents a significant shift in model compression strategies by proving that extreme sparsity and low precision can be achieved through a simplified, gradient-based approach rather than complex multi-stage optimization. The framework's architecture-agnostic nature, combined with its reduction of hyperparameter tuning complexity via the use of global control knobs, lowers the barrier to entry for deploying highly efficient models. By successfully decoupling sparsity and bit-width selection within a unified loss function across different precision regimes, this research establishes a new standard for operational efficiency in joint compression optimization.

---

## Key Findings

*   **Extreme Compression Efficiency:** CoDeQ reduces bit operations (BOPs) to approximately **5%** of the original requirement on the ImageNet dataset using ResNet-18.
*   **Accuracy Retention:** Despite the high level of compression, the method maintains accuracy close to that of full-precision models.
*   **Operational Versatility:** The approach achieves these results in both **fixed-precision** and **mixed-precision** quantization regimes without sacrificing performance.
*   **Optimization Efficiency:** Unlike existing methods, CoDeQ determines sparsity patterns and quantization parameters simultaneously within a single training loop, avoiding the computational overhead of auxiliary procedures.
*   **Decoupled Control:** The method successfully decouples the selection of sparsity from the selection of bit-width, allowing for more granular control over the compression process.

---

## Methodology

CoDeQ is a fully differentiable, end-to-end optimization framework for joint pruning and quantization. Its core mechanism relies on the theoretical observation that the **dead-zone** of a scalar quantizer is mathematically equivalent to magnitude pruning.

Instead of treating pruning and quantization as separate steps, CoDeQ integrates them by parameterizing the width of the quantizer's dead-zone. This parameter is learned directly via backpropagation alongside standard quantization parameters.

*   **Unified Framework:** Integrates pruning and quantization into a single operator.
*   **Learnable Parameters:** Uses a single global hyperparameter to explicitly regulate sparsity.
*   **Mixed-Precision Support:** An optional second hyperparameter controls mixed-precision settings.
*   **Simultaneous Learning:** Parameters are learned within the training loop without external heuristics.

---

## Technical Details

CoDeQ performs end-to-end joint model compression by simultaneously determining sparsity patterns and quantization parameters within a single training loop.

### Core Mechanism
*   **Learnable Dead-Zone Quantizer:** Width is parameterized by $\delta_{dz}$ to control sparsity.
*   **Bit-width Control:** A parameter $\delta_{bit}$ governs bit-width for mixed-precision support.

### Loss Function
The total loss is defined as:
$$L(q_w; s(\delta_{bit}), d(\delta_{dz})) + \lambda_{bit}||\delta_{bit}||^2_2 + \lambda_{dz}||\delta_{dz}||^2_2$$

### Implementation Specifications
*   **Granularity:** Layer-wise.
*   **Scale Factors:** Based on the 99th quantile of absmax (with $\epsilon = 10^{-8}$).
*   **Hyperparameters (Example):**
    *   **ResNet-18:** $\lambda_{bit}=0.1$, $\lambda_{dz}=0.02$

---

## Results

The primary efficiency metric used is **Bit-Operations (BOPs)**, assuming idealized support for unstructured weight sparsity.

*   **CIFAR-10:**
    *   **ResNet-20:** Achieved the highest accuracy and lowest BOPs among compared methods (QST, SQL, etc.).
    *   **Performance:** The fixed 4-bit variant outperformed QST.
*   **Transformer Architectures (TinyViT):**
    *   Demonstrated the method's generalizability with minimal accuracy loss.
*   **ImageNet:**
    *   **ResNet-18:** Reduced BOPs to approximately **5%** of the original requirement while matching QST's accuracy.
    *   **ResNet-50:** Achieved a BOP reduction to **2.62%** of the baseline (compared to QST's 4.5%) with a Top-1 accuracy drop of only **0.81%**.

---

## Contributions

*   **Novel Integration of Pruning and Quantization:** Introduction of CoDeQ, a simple, fully differentiable method that unifies pruning and quantization into a single operator, removing the theoretical and practical divide found in sequential approaches.
*   **Dead-Zone Parameterization:** Establishing that learning the dead-zone width of a quantizer provides a direct, data-driven gradient signal for inducing sparsity, eliminating the need for auxiliary regularization procedures.
*   **Reduction of Engineering Complexity:** Providing an architecture-agnostic solution that reduces hyperparameter tuning and engineering complexity by removing reliance on external search methods or pre-training steps.
*   **State-of-the-Art Performance:** Demonstrating that a simplified, gradient-based approach can achieve high-sparsity (95% BOP reduction) and low-precision networks while maintaining competitive accuracy on large-scale datasets like ImageNet.