---
title: 'SCA-LLM: Spectral-Attentive LLM-Based Wireless World Modeling for Agentic
  Communications'
arxiv_id: '2509.08139'
source_url: https://arxiv.org/abs/2509.08139
generated_at: '2026-01-27T23:10:01'
quality_score: 8
citation_count: 39
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# SCA-LLM: Spectral-Attentive LLM-Based Wireless World Modeling for Agentic Communications

*Xianfu Lei, Lisheng Fan, Ke He, Eand Symeon, George K. Karagiannidis, Le He, Thang X. Vu*

> ### **Quick Facts**
> - **Quality Score:** 8/10
> - **Total Citations:** 39
> - **Base Architecture:** Pre-trained GPT-2
> - **Key Metric:** Up to **-2.4 dB** NMSE advantage
> - **Dataset:** QuaDRiGa (3GPP Standards)
> - **Training Efficiency:** Majority of LLM parameters frozen

---

## Executive Summary

This research addresses the critical challenge of enabling Large Language Models (LLMs) to perform high-fidelity Wireless World Modeling for the emerging era of Agentic Communications. As 6G networks evolve toward AI-native architectures, autonomous agents require a deep semantic understanding of the radio environment to manage communication links effectively. However, LLMs are inherently designed for discrete text tokens, creating a significant domain mismatch when processing the continuous, complex-valued data inherent to Radio Frequency (RF) signals. Bridging this gap is essential for moving beyond traditional signal processing to create intelligent systems that can model and predict the physical layer of the wireless world with the same reasoning capabilities applied to natural language.

The paper introduces SCA-LLM, a novel framework that adapts a pre-trained GPT-2 model for wireless channel prediction through a specialized Spectral-Attentive Adapter (SCA). To translate RF characteristics into a format suitable for the LLM, the framework converts complex-valued Channel State Information (CSI) into real-valued sequences by concatenating real and imaginary parts. The core innovation lies in the SCA module, which utilizes a Multi-Spectral Channel Attention Layer and 2-D Discrete Cosine Transform (DCT) to capture spectral components and frequency indices. This design effectively mitigates domain mismatch while maintaining parameter efficiency; the majority of the LLM parameters remain frozen during training, with only the learnable embeddings, normalization layers, and the SCA module being optimized.

Evaluations conducted using the QuaDRiGa channel simulator in a 4x4 MIMO-OFDM configuration demonstrated that SCA-LLM achieves state-of-the-art performance, surpassing traditional temporal models and existing architectures. Specifically, the model outperformed RNN, LSTM, GRU, and standard Transformer baselines, as well as the prior LLM4CP method, achieving an NMSE advantage of up to -2.4 dB. Beyond raw accuracy, the framework exhibited exceptional robustness to SNR variations ranging from 0 to 20 dB. Furthermore, SCA-LLM successfully validated zero-shot generalization capabilities, accurately predicting channels in Urban Micro (UMi) environments after being trained exclusively on Urban Macro (UMa) data, a feat that traditional models typically cannot accomplish without retraining.

The significance of this work lies in its validation of LLMs as viable tools for physical layer modeling and digital twinning of the RF environment. By demonstrating that semantic reasoning architectures can effectively model the wireless world, SCA-LLM paves the way for Agentic Communications, where AI agents can autonomously interpret and react to complex channel dynamics. This approach reduces the computational overhead associated with training from scratch and facilitates the deployment of AI-native wireless systems. Ultimately, the ability to transfer knowledge across different environments (e.g., from macro to micro cells) signals a paradigm shift toward more flexible, efficient, and intelligent 6G networks.

---

## Key Findings

*   **Core Topic:** Bridging Large Language Models (LLMs) with wireless communication systems.
*   **Primary Application:** Enabling **Agentic Communications**, where AI agents autonomously manage communication links.
*   **Secondary Application:** **Wireless World Modeling**, utilizing predictive models or digital twins of the RF environment.

---

## Technical Methodology

The SCA-LLM framework proposes a parameter-efficient approach to adapting LLMs for the wireless domain.

### Framework Architecture
*   **Backbone:** Utilizes a pre-trained **GPT-2** model as the sequence modeling backbone.
*   **Pipeline:** Comprises four distinct stages:
    1.  Normalization Layer
    2.  Spectral-Attentive Adapter (SCA)
    3.  LLM (GPT-2)
    4.  Output Head

### Data Processing
*   **Input Representation:** Processes complex-valued Channel State Information (CSI) matrices.
*   **Conversion:** Converts complex values to real-valued representations by concatenating real and imaginary parts.
*   **Processing Mode:** Employs parallel processing for OFDM subcarriers.

### Spectral-Attentive Adapter (SCA)
The SCA module is designed to bridge the gap between RF data and LLM capabilities:
*   **Multi-Spectral Channel Attention Layer:** Captures essential spectral components.
*   **2-D Discrete Cosine Transform (DCT):** Analyzes frequency indices to mitigate domain mismatch.
*   **Configuration:** Uses an embedding size of 64, 4 attention layers, and a 7x7 DCT analysis.

### Training Strategy
To maintain parameter efficiency:
*   **Frozen Parameters:** The majority of the LLM, including Multi-Head Self-Attention and Feed-Forward Networks, remains frozen.
*   **Trainable Components:** Only learnable embeddings, Layer Normalization, the SCA module, and the output head are trained.

---

## Evaluation & Results

### Simulation Setup
*   **Simulator:** QuaDRiGa (based on 3GPP standards).
*   **Configuration:** 4x4 MIMO-OFDM with 12 subcarriers.
*   **Mobility:** User velocities ranging from 0 to 60 km/h.

### Datasets
*   **Training:** 10,400 samples (Urban Macro - UMa NLOS).
*   **Testing - In-Distribution:** UMa NLOS.
*   **Testing - Zero-Shot:** Urban Micro (UMi) NLOS to test generalization.

### Performance Metrics
*   **Metric:** Normalized Mean Squared Error (NMSE) in dB.
*   **Comparison:** Benchmarked against RNN, LSTM, GRU, Transformer, and prior LLM4CP method.

### Outcomes
*   **Accuracy:** SCA-LLM achieved state-of-the-art results with an NMSE advantage of up to **-2.4 dB** over competitors.
*   **Robustness:** Demonstrated high resilience to SNR variations (0-20 dB).
*   **Generalization:** Successfully performed zero-shot prediction in UMi environments after training only on UMa data.