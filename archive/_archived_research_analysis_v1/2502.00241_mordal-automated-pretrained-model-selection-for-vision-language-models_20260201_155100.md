# Mordal: Automated Pretrained Model Selection for Vision Language Models
*Shiqi He; Insu Jang; Mosharaf Chowdhury*

---

> ### ðŸ“Š Quick Facts & Key Metrics
> 
> *   **Efficiency Gain:** 8.9Ã— to 11.6Ã— reduction in GPU hours compared to grid search.
> *   **Search Space:** Evaluated 49 candidates against a potential market of >150,000 models.
> *   **Top Visual/Doc QA Model:** SigLIP-Vicuna (SigLIP-so400m + Vicuna-1.5-7B).
> *   **Top Knowledge Task Model:** SigLIP-Llama (SigLIP-so400m + Llama-3-8B).
> *   **Baseline Cost:** Training a single 7B parameter candidate requires >100 GPU hours.

---

## Executive Summary

Selecting the optimal Vision Language Model (VLM) for specific tasks has become a computationally prohibitive challenge due to the rapid expansion of available pretrained components. With thousands of potential architecture combinations, identifying the most effective model via traditional exhaustive grid searches is intractable and consumes massive resources. This paper addresses the "Pretrained Model Selection Problem" to mitigate the prohibitive costs associated with discovering high-performing multimodal systems in a saturated market.

The authors introduce **Mordal**, the first automated framework designed to identify task-specific multimodal models without manual design. The framework optimizes the search process through two primary strategies:
1.  **Candidate Reduction:** Utilizes clustering algorithms to filter and shrink the pool of viable architectures.
2.  **Evaluation Minimization:** Accelerates assessment using early stopping mechanisms and data-driven performance extrapolation.

This approach allows Mordal to navigate the vast search space efficiently, identifying optimal pairings of vision encoders and language models tailored to specific user-defined tasks. Mordal demonstrates significant efficiency gains, achieving an **8.9Ã— to 11.6Ã— reduction** in required GPU hours compared to standard grid search methods.

Crucially, the evaluation process discovered novel VLM architectures that explicitly outperform current state-of-the-art models. The framework successfully identified distinct optimal pairings for different tasks, such as **SigLIP-Vicuna** for Visual QA tasks and **SigLIP-Llama** for Knowledge-heavy tasks. The significance of this work lies in transforming VLM selection into a scalable, automated pipeline that democratizes access to high-performance systems. By proving that automated, task-specific selection yields higher returns than simply relying on larger, general-purpose models, Mordal challenges the "bigger is better" paradigm.

---

## Key Findings

*   **Massive Cost Reduction:** Mordal reduces the computational cost of finding the optimal VLM by utilizing **8.9Ã— to 11.6Ã— lower GPU hours** compared to traditional grid search methods.
*   **Novel High-Performance Architectures:** The evaluation process discovered novel VLM architectures that outperform current state-of-the-art models (SOTA).
*   **Automated Task Specialization:** The framework proves that task-specific multimodal models can be identified automatically without manual, handcrafted design.
*   **Optimization Strategy:** Efficiency is achieved through shrinking the pool of candidate models and accelerating evaluation time.
*   **"No Silver Bullet":** No single VLM architecture dominates all tasks; upgrading the Language Model does not guarantee better performance in all categories.

---

## Methodology

The authors propose **Mordal**, an automated multimodal model search framework designed to identify the best VLM for a specific user-defined task. The methodology centers on efficiency optimization through two primary strategies:

1.  **Candidate Reduction:** This strategy uses algorithms to reduce the total number of model candidates, preventing the need for exhaustive search.
2.  **Evaluation Minimization:** This employs techniques to minimize the time required to evaluate each remaining candidate model, ensuring rapid iteration.

---

## Technical Details

### VLM Background Architecture

| Component | Function | Options |
| :--- | :--- | :--- |
| **Vision Encoder** | Processes input images to extract raw feature embeddings. | CLIP (ViT-L/14), SigLIP (so400m-patch14), InternViT, DINOv2 |
| **Feature Projector** | Facilitates cross-modal alignment by mapping raw vision embeddings into the text token embedding space. | Linear layer, MLP |
| **Language Model** | A decoder-only LLM that processes mixed embeddings (text + aligned vision) to generate text outputs. | Llama (1.5-7B, 3-8B), Vicuna (1.5-7B), Mistral |

### Mordal Optimization Strategy

| Strategy | Description |
| :--- | :--- |
| **Goal** | Addresses the "Pretrained Model Selection Problem" to reduce the prohibitive cost of exhaustive grid search. |
| **Search Space Reduction** | Shrinks the pool of candidates by clustering based on similarity and implementing a two-step evaluation process (inter-cluster and intra-cluster). |
| **Evaluation Acceleration** | Minimizes evaluation time via an early stopping mechanism and the use of observational scaling laws. |

---

## Results

### Computational Efficiency

| Metric | Value |
| :--- | :--- |
| **Mordal Efficiency** | Achieves 8.9Ã— to 11.6Ã— lower GPU hours vs. grid search. |
| **Single Candidate Cost** | Training a single VLM candidate (7B parameter scale) requires >100 GPU hours. |
| **Grid Search Burden** | An exhaustive grid search on 49 candidates cost >1,000 GPU hours. |

### VLM Performance

*   **General Finding:** No single VLM architecture dominates all tasks ("No Silver Bullet"). Upgrading the Language Model does not guarantee better performance across the board.

**Top Performing Configurations by Task**

| Task Category | Top Performer (Architecture) | Metrics Breakdown |
| :--- | :--- | :--- |
| **Visual & Doc QA** | **SigLIP-Vicuna**<br>*(SigLIP-so400m + Vicuna-1.5-7B)* | â€¢ GQA: 66.4<br>â€¢ VizWiz: 44.8<br>â€¢ ChartQA: 18.4<br>â€¢ DocVQA: 24.1 |
| **Knowledge Tasks** | **SigLIP-Llama**<br>*(SigLIP-so400m + Llama-3-8B)* | â€¢ ScienceQA: 78.5<br>â€¢ AI2D: 60.1 |

### Search Space Scope

*   **Motivating Experiment:** 49 candidates (7 encoders Ã— 7 LLMs).
*   **Market Context:** HuggingFace lists >150,000 LLMs as of January 2025, highlighting the necessity for automated selection.

---

## Contributions

*   **Framework Introduction:** Introduced the first automated framework (**Mordal**) to create task-specific multimodal models.
*   **Resource Efficiency:** Demonstrated a method for performing model selection that is an order of magnitude more resource-efficient than standard grid search.
*   **Empirical Performance:** Empirical contribution of new VLM configurations that exceed the performance benchmarks of existing state-of-the-art counterparts.

---

**Paper Quality Score:** 9/10  
**References:** 40 citations