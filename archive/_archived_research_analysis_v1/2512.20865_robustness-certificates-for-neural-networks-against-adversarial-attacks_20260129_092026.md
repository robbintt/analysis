# Robustness Certificates for Neural Networks against Adversarial Attacks

*Sara Taheri; Mahalakshmi Sabanayagam; Debarghya Ghoshdastidar; Majid Zamani*

---

> ### ðŸ“Š Quick Facts
>
> *   **Datasets:** MNIST, SVHN, CIFAR-10
> *   **Core Innovation:** Unified framework for both training-time (poisoning) and test-time robustness
> *   **Methodology:** Barrier Certificates (Control Theory) + PAC Learning
> *   **Agnosticism:** Model-agnostic; no prior attack knowledge needed
> *   **Performance:** 13sâ€“53s runtime; outperforms RAB baseline on certified radii
> *   **Quality Score:** 8/10 | **References:** 40 citations

---

## Executive Summary

Neural networks are highly susceptible to adversarial attacks, which occur both during training (data poisoning) and at inference time (evasion). The core challenge addressed in this paper is the lack of formal, unified guarantees for model safety under these worst-case conditions. Existing defensive strategies often fail because they rely on restrictive assumptions, such as prior knowledge of the specific attack vector or the contamination level of the training dataset.

The authors introduce a novel framework that bridges **control theory** and **machine learning** by modeling gradient-based neural network training as a discrete-time dynamical system (dt-DS). To certify robustness, they adapt the concept of "**Barrier Certificates**" (BC) from control theory, using them to mathematically verify that the model remains within a safety region despite perturbations. Specifically, they employ a Neural Network-Based Barrier Certificate (NNBC), trained as a classifier on poisoned trajectories, to identify the largest admissible perturbation.

To ensure these certificates generalize, the authors derive **Probably Approximately Correct (PAC)** bounds by solving a Scenario Convex Program (SCP). The proposed framework was evaluated across standard datasets, demonstrating the ability to certify non-trivial robust radii against Backdoor attacks (BDA) with high efficiency. When compared to the RAB baseline, the framework consistently achieved larger Certified Robust Radii. This research establishes the first unified framework capable of providing formal guarantees for both training-time and test-time attacks, setting a new standard for rigorous defense mechanisms.

---

## Key Findings

*   **Unified Safety Certification:** The proposed framework successfully certifies a robust radius for neural networks, ensuring safety against worst-case $l_p$-norm based data poisoning attacks without requiring prior knowledge of the attack type or contamination level.
*   **Proven Empirical Success:** Experiments on standard datasets (MNIST, SVHN, and CIFAR-10) demonstrate the approach's ability to certify non-trivial perturbation budgets while remaining model-agnostic.
*   **Dual-Phase Guarantee:** The framework is the first of its kind to provide formal guarantees for both **training-time (poisoning)** and **test-time adversarial attacks** within a unified structure.
*   **Statistical Rigor:** By utilizing Probably Approximately Correct (PAC) bounds, the method provides a rigorous statistical confidence lower bound on the certified robustness radius.

---

## Methodology

The authors model gradient-based neural network training as a **discrete-time dynamical system (dt-DS)** and formulate the problem of poisoning robustness as a formal safety verification problem.

1.  **Barrier Certificates (BC):** To solve the verification problem, the authors adapt the BC concept from control theory, providing sufficient conditions to certify that a model remains safe (within a robust radius) despite poisoning.
2.  **NN-BC Implementation:** Practically, BCs are parameterized as neural networks trained on finite sets of poisoned trajectories.
3.  **Generalization via PAC:** To ensure results generalize beyond observed data, the authors derive PAC bounds by solving a **Scenario Convex Program (SCP)**, which statistically validates the certified robustness radius.

---

## Contributions

*   **Novel Formal Framework:** Introduces a principled framework that bridges control theory (barrier certificates) and machine learning to provide formal robustness guarantees, moving away from restrictive assumptions common in existing defenses.
*   **Dual-Phase Certification:** Establishes the first unified framework capable of providing formal guarantees for both training (data poisoning) and test-time attack settings.
*   **Generalization Guarantees:** Derivation of PAC bounds via scenario convex programs, offering a statistical confidence measure for the certified robustness that applies to data distributions beyond the finite training set.
*   **Practical Applicability:** A model-agnostic solution that requires no prior knowledge of the specific attack vectors or the extent of data contamination, addressing significant limitations in current defensive literature.

---

## Technical Details

**Core Concept**
The paper proposes a framework for certifying robustness against training-time (data poisoning) and test-time (evasion) attacks by modeling gradient-based machine learning training as a **discrete-time dynamical system**.

**Safety Verification**
*   Robustness certification is cast as a formal safety verification problem using **Barrier Certificates (BC)**.
*   Specifically uses a **Neural Network-Based BC (NNBC)** parameterized as $B_{\phi}$.
*   The NNBC is trained to find the **Certified Robust Radius ($\delta^*_{cert}$)**: the largest admissible perturbation ($\ell_p$) where test accuracy degradation is bounded.

**Optimization Approach**
*   The optimization involves relaxing a Robust Constrained Program (RCP) to a **Chance-Constrained Problem (CCP)**.
*   Solved as a Scenario Convex Program (SCP) using finite scenarios.
*   The method provides PAC guarantees with a sample complexity bound $\hat{N} \ge \frac{\ln \beta}{\ln (1 - \epsilon)}$.

**Scope**
*   **Model Agnostic:** Applicable across different neural network architectures.
*   **Unified:** Valid for both perturbation types (poisoning and evasion).

---

## Results

The framework was evaluated on MNIST, SVHN, and CIFAR-10 datasets against Backdoor attacks (BDA), comparing Certified Robust Radii ($\delta^*_{cert}$) against a RAB baseline.

### Performance Comparison (Method vs RAB)

*   **MNIST ($\ell_\infty$):**
    *   Target Acc 0.90: **0.08** (RAB: NA)
    *   Target Acc 0.60: **0.19** *(RAB: 0.14)*
*   **SVHN:**
    *   Target Acc 0.80: **0.06** (RAB: NA)
    *   Target Acc 0.40: **0.14** *(RAB: 0.08)*
*   **CIFAR-10:**
    *   Target Acc 0.30: **0.12** *(RAB: 0.05)*

### Efficiency & Accuracy
*   The NNBC framework consistently yielded certified radii **tighter to empirical upper bounds** than RAB.
*   **Runtime:** 13 to 53 seconds using NNBC configurations of 4-7 layers and 800-4000 units.

---

*References: 40 citations*