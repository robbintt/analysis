---
title: 'TD-M(PC)$^2$: Improving Temporal Difference MPC Through Policy Constraint'
arxiv_id: '2502.03550'
source_url: https://arxiv.org/abs/2502.03550
generated_at: '2026-02-06T05:41:36'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# TD-M(PC)$^2$: Improving Temporal Difference MPC Through Policy Constraint

*Haotian Lin; Pengcheng Wang; Jeff Schneider; Guanya Shi*

---

> ### ðŸ“Š Quick Facts
>
> *   **Focus:** Model-Based Reinforcement Learning (MBRL) & Temporal Difference MPC
> *   **Core Problem:** Structural policy mismatch causing persistent value overestimation
> *   **Key Innovation:** Policy Constraint (TD-M(PC)$^2$) via regularization
> *   **Primary Benchmark:** 61-Degree-of-Freedom (DoF) Humanoid Control
> *   **Computational Cost:** Zero additional overhead
> *   **Impact:** 2,159% overestimation reduction in high-complexity tasks

---

## Executive Summary

This research addresses a critical instability in model-based reinforcement learning (MBRL) algorithms that combine model-predictive control (MPC) with learned value functions or policy priors, specifically the TD-MPC2 framework. The authors identify that these methods suffer from persistent value overestimation, a problem that escalates severely with task complexity. The root cause is identified as "structural policy mismatch": a distributional shift occurs because the data generation policy (the planner) differs from the learned policy prior used for training. This discrepancy forces the value function to evaluate out-of-distribution (OOD) actions, leading to erroneously high value estimates that destabilize learning, particularly in high-dimensional control tasks like 61-DoF humanoid locomotion.

The authors propose **TD-M(PC)$^2$**, a minimalist modification to existing MBRL frameworks that introduces a policy constraint via regularization. Technically, the method adds a regularization term to the learning objective that penalizes deviations between the learned policy prior and the data-generating planning policy. By constraining the policy to reduce OOD queries, the method aligns the learned prior more closely with the planner's behavior, thereby mitigating the distributional shift. The authors provide theoretical backing (Theorem 3.1) bounding the H-step policy suboptimality, linking performance gaps directly to value approximation error. Crucially, this innovation is computationally free, requiring minimal code changes without adding overhead to the planning or learning loops.

Experimental results demonstrate that TD-M(PC)$^2$ effectively eliminates the extreme value overestimation seen in baseline methods. In a 4-DoF Hopper task, overestimation was relatively low (15%), but in high-complexity 61-DoF humanoid tasks (h1hand-run-v0), TD-MPC2 overestimated value by 2,159%. TD-M(PC)$^2$ corrected these errors and significantly outperformed the TD-MPC2 baseline across high-DoF environments. Specifically, in the 61-DoF humanoid tasks (run and slide), the proposed method achieved substantially higher true values ($V^\pi$) and more accurate function estimation ($\hat{V}$) than the strong baseline, establishing new performance benchmarks for continuous control without increasing computational cost.

The significance of this work lies in its dual contribution of rigorous diagnosis and an accessible solution. By theoretically framing structural policy mismatch as the fundamental cause of value overestimation in temporal difference MPC, the authors provide clarity on a widespread failure mode in MBRL. The proposed regularization solution is easily integrated into existing state-of-the-art methods, offering a "plug-and-play" improvement that enhances robustness in high-dimensional control problems.

---

## Key Findings

*   **Value Overestimation Diagnosis:** Existing MBRL algorithms combining model-based planning with learned value/policy priors suffer from persistent value overestimation.
*   **Root Cause:** This overestimation is theoretically and experimentally linked to a structural policy mismatch between the data generation policy and the learned policy prior.
*   **Mitigation Strategy:** Reducing out-of-distribution (OOD) queries through policy regularization effectively mitigates this mismatch and stabilizes value learning.
*   **Performance Superiority:** The method significantly outperforms strong baselines like TD-MPC2, particularly in high-complexity 61-DoF humanoid tasks.
*   **Computational Efficiency:** The proposed solution achieves improvements without adding computational overhead.

---

## Methodology

The authors propose a minimalist modification to existing model-based reinforcement learning frameworks, specifically building upon TD-MPC2.

**Core Methodological Innovation:**
The introduction of a **policy regularization term** designed to constrain the policy and reduce out-of-distribution (OOD) queries. This aligns the learned policy prior more closely with the data generation policy produced by the planner.

**Implementation:**
*   **Lightweight:** Requires minimal changes to the existing codebase.
*   **Efficient:** Incurs no additional computational cost.

---

## Contributions

*   **Theoretical Diagnosis:** Provides a theoretical and empirical diagnosis of structural policy mismatch as the fundamental cause of value overestimation in temporal difference MPC methods.
*   **Proposed Solution:** Introduces a computationally free, regularization-based solution (TD-M(PC)$^2$) that can be easily integrated into existing state-of-the-art methods.
*   **Benchmarking:** Establishes new performance benchmarks for continuous control, demonstrating robustness and effectiveness in high-dimensional control problems such as 61-DoF humanoid control.

---

## Technical Details

### Core Architecture
Builds on TD-MPC2 combining model-based planning with TD learning.
*   **Components:** Encoder ($h$), Latent Dynamics Model ($d$), Reward Model ($R$), Action-Value Function ($\hat{Q}$), and Nominal Policy ($\pi$).

### Planning
*   Utilizes local trajectory optimization (Model Predictive Path Integral control).
*   Maximizes expected return over a horizon $H$.
*   Appends a learned value function as terminal cost.

### Problem Identification
*   **Policy Mismatch:** A structural issue where off-policy training on the nominal policy differs from data collection via the planning policy.
*   **Consequence:** Causes distributional shift and persistent value overestimation.

### Proposed Solution: TD-M(PC)$^2$
*   Introduces a **Policy Constraint** (policy regularization) to reduce OOD queries.
*   Implements a conservative exploitation strategy.

### Theoretical Analysis
*   **Theorem 3.1:** Provides a bound on H-step Policy Suboptimality.
*   **Insight:** If model error is low, the performance gap is dominated by the value approximation error term $\gamma^H(1+\gamma^2)\epsilon_k / (1-\gamma)^2$.

---

## Results

### Quantitative Findings on Value Overestimation
The study measured Value Approximation Error ($E_{\rho_o}[\hat{V} - V^\pi]$). The baseline showed extreme overestimation in high-dimensional tasks:

| Task | Complexity | Overestimation Rate |
| :--- | :--- | :--- |
| **Hopper-Stand** | 4-DoF | 15% |
| **Dog-Trot** | 36-DoF | 231% |
| **h1hand-run-v0** | 61-DoF | 2,159% |
| **h1hand-slide-v0** | 61-DoF | 746% |

### Performance Metrics
*   **Metrics:** Value Approximation Error, True Value ($V^\pi$), and Function Estimation ($\hat{V}$).
*   **TD-M(PC)$^2$ Performance:** Significantly outperforms the TD-MPC2 baseline, particularly in high-complexity 61-DoF humanoid tasks.
*   **Efficiency:** Performance improvements are achieved without adding computational overhead.

---

## Evaluation

**Quality Score:** 9/10
**References:** 40 citations