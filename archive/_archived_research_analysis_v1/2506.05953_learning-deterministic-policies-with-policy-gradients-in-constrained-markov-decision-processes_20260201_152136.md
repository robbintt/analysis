# Learning Deterministic Policies with Policy Gradients in Constrained Markov Decision Processes

*Alessandro Montenegro; Leonardo Cesani; Marco Mussi; Matteo Papini; Alberto Maria Metelli*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Core Algorithm** | C-PG (Constrained Policy Gradient) |
| **Variants** | C-PGAE (Action-based), C-PGPE (Parameter-based) |
| **Frameworks** | CMDP, Continuous Control |
| **Batch Size** | 100 (vs 600 for baselines) |
| **Environments Tested** | DGWW, CostLQR, RobotWorld |
| **Citations** | 3 |

---

> ### ðŸ’¡ Executive Summary
>
> Constrained Reinforcement Learning (CRL) is critical for deploying autonomous systems in safety-critical environments where agents must maximize rewards while strictly adhering to safety constraints. However, a significant theoretical gap exists in guaranteeing global last-iterate convergence to optimal deterministic policies within the Constrained Markov Decision Process (CMDP) framework. Previous methods often struggle to maintain performance when transitioning from the stochastic policies required for exploration during training to the deterministic policies needed for reliable deployment. This lack of convergence guarantees hinders the application of reinforcement learning in real-world scenarios where constraint satisfaction is paramount.
>
> The authors introduce **C-PG (Constrained Policy Gradient)**, an exploration-agnostic algorithm designed to guarantee global last-iterate convergence in CRL settings. The core innovation lies in decoupling the exploration mechanism from the convergence proofs, allowing the algorithm to operate irrespective of how stochasticity is injected. The framework establishes specific noise models that rigorously bridge stochastic training with deterministic deployment. The authors propose two distinct variants: **C-PGAE**, which utilizes action-based exploration, and **C-PGPE**, which employs parameter-based exploration using a stochastic hyperpolicy.
>
> Experimental evaluations on DGWW, CostLQR, and RobotWorld environments demonstrate that C-PG variants significantly outperform state-of-the-art baselines (NPG-PD2 and RPG-PD2) in both performance and sample efficiency. The proposed methods achieved convergence with a batch size of 100, whereas baselines required a batch size of 600 to function. In the CostLQR environment, the deterministic deployment of C-PGPE (DC-PGPE) achieved a return of approximately 0.25 with a low cost (0.05â€“0.1), while the stochastic training version converged to a return of only -4. Furthermore, DC-PGAE satisfied cost constraints more effectively than stochastic variants in the DGWW environment. The results also highlighted the robustness of C-PGPE, which remained stable across varying regularization parameters, whereas baselines exhibited high sensitivity to hyperparameter changes.

---

## Key Findings

*   **Exploration-Agnostic Convergence:** Introduction of C-PG, an exploration-agnostic algorithm for Constrained Reinforcement Learning (CRL) that guarantees global last-iterate convergence.
*   **Deterministic Policy Deployment:** The algorithm converges to optimal deterministic policies when stochasticity is removed after training, bridging the gap between training and deployment.
*   **Dual Exploration Support:** The approach is validated for both action-based (**C-PGAE**) and parameter-based (**C-PGPE**) exploration strategies.
*   **Superior Performance:** In experiments, the proposed variants outperform state-of-the-art baselines on constrained control tasks, particularly when deploying deterministic policies.
*   **Sample Efficiency:** C-PG methods solve problems with significantly smaller batch sizes compared to baselines.

---

## Methodology

The research is grounded in the **Constrained Markov Decision Processes (CMDPs)** framework, utilizing policy-based methods tailored for continuous control. The methodology supports two primary exploration mechanisms:

1.  **Action-based Exploration:** Learning parameters of a stochastic policy directly.
2.  **Parameter-based Exploration:** Learning parameters of a stochastic hyperpolicy (which in turn samples parameters for the policy).

The core design utilizes the **C-PG algorithm**, which operates independently of specific exploration strategies. The protocol involves:
*   Training a stochastic policy or hyperpolicy.
*   "Switching off" the stochasticity post-training.
*   Deploying the underlying deterministic policy, which is guaranteed to be optimal.

---

## Contributions

*   **Rigorous Theoretical Proofs:** The paper provides the first rigorous proofs for global last-iterate convergence in constrained settings.
*   **Noise Model Definition:** It establishes specific noise models that bridge stochastic training with deterministic deployment, enabling effective convergence to optimal deterministic policies.
*   **Novel Framework Proposal:** Introduces the C-PG framework and its variants (C-PGAE and C-PGPE), offering theoretically grounded tools for constrained decision-making.

---

## Technical Details

*   **Core Algorithm:**
    *   **Name:** C-PG (Constrained Policy Gradient)
    *   **Type:** Exploration-agnostic algorithm for Constrained Reinforcement Learning.
    *   **Property:** Guarantees global last-iterate convergence.

*   **Architecture:**
    *   Uses a stochastic hyperpolicy to sample parameters for a deterministic linear policy.
    *   Stochasticity is removed post-training for final deployment.

*   **Variants:**
    *   **C-PGAE:** Action-based Exploration.
    *   **C-PGPE:** Parameter-based Exploration.
    *   **Counterparts:** DC-PGAE and DC-PGPE (Deterministic versions).

*   **Configuration:**
    *   **Hyperpolicy Variance:** $10^{-3}$
    *   **Optimizer:** Adam
    *   **Learning Rate (Policy):** 0.001
    *   **Learning Rate (Dual):** 0.01
    *   **Iterations:** 6000
    *   **Batch Size:** 100
    *   **Horizon:** 100
    *   **Discount Factor:** 1

*   **Baselines:**
    *   **Models:** NPG-PD2 and RPG-PD2
    *   **Config:** Infinite-horizon settings ($T=1000$, $\\gamma=0.98$, batch size 600).

---

## Results

Experiments were conducted on **DGWW**, **CostLQR**, and **RobotWorld** environments, measuring return, cost, Lagrangian value, and sample efficiency.

*   **Sample Efficiency:** C-PG methods solved problems with a batch size of **100**, whereas baselines required a batch size of **600**.
*   **Cost Constraint Satisfaction (DGWW):** The deterministic variant **DC-PGAE** satisfied cost constraints better than stochastic variants in the DGWW environment.
*   **Performance in CostLQR:**
    *   **DC-PGPE (Deterministic):** Converged to a return of **~0.25** with low cost (**0.05â€“0.1**).
    *   **C-PGPE (Stochastic):** Converged to a return of only **~-4**.
*   **Robustness:** C-PGPE remained stable across tested regularization parameters. In contrast, baselines showed high sensitivity to hyperparameter changes.

---

**Quality Score:** 8/10  
**References:** 3 citations