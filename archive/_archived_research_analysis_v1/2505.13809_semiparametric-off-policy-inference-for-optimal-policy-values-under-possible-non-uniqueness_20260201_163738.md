# Semiparametric Off-Policy Inference for Optimal Policy Values under Possible Non-Uniqueness

*Haoyu Wei*

---

> ### **Quick Facts**
> **Quality Score:** 7/10  
> **References:** 40 Citations  
> **Core Method:** NSAVE (Nonparametric SequentiAl Value Evaluation)  
> **Primary Domain:** Reinforcement Learning & Causal Inference  
> **Application:** Mobile Health (OhioT1DM dataset)

---

## Executive Summary

Standard Off-Policy Evaluation (OPE) frameworks are designed to evaluate fixed, pre-specified policies, creating a significant reliability gap when applied to optimal policies estimated from data. This paper addresses a critical failure in semiparametric inference: when an optimal policy is non-unique or nearly deterministic, the inference problem becomes non-regular, meaning the parameter of interest changes discontinuously with small data fluctuations. This non-regularity invalidates standard asymptotic theories, rendering the characterization of the standard efficient influence function (EIF) impossible and leading to invalid confidence intervals. Resolving this is essential for high-stakes domains like personalized medicine, where understanding the statistical uncertainty of an "optimal" strategy is as vital as the strategy itself.

The key innovation is **NSAVE** (Nonparametric Sequential Value Evaluation), a method designed to handle non-regularity through a smoothing-based approach that transforms the problem into a regular one, enabling valid asymptotic approximations. Unlike previous methods such as SAVE, which relied on linear Q-function approximations, NSAVE operates nonparametrically and employs a sequential, offline estimation strategy to handle data. Technically, the algorithm randomly permutes data trajectories and sequentially updates nuisance parameters—including Q-functions, Marginal Importance Sampling (MIS) ratios, and Value Functions—within a loop. This process updates the optimal policy via the argmax of the Q-function and calculates trajectory estimating functions, culminating in a final estimator that is a weighted average of trajectory scores, thereby avoiding plug-in instability without requiring live environmental interaction.

The study’s primary findings are theoretical, establishing rigorous conditions for valid inference through proofs rather than standard simulation metrics like MSE. **Theorem 3.1** proves that if the optimal policy is unique and deterministic, the EIF exists and Regular Asymptotically Linear (RAL) estimators are valid. Conversely, **Theorem 3.2** demonstrates that under non-uniqueness, the functional possesses no influence function, meaning standard RAL estimators cannot exist. While explicit numerical performance metrics are not the focus of the text, NSAVE demonstrated qualitative numerical stability in deterministic regimes where the SAVE baseline typically fails, successfully maintaining double robustness. The framework’s practical utility was further confirmed through application to the OhioT1DM mobile health dataset.

This work significantly advances reinforcement learning and causal inference by formally characterizing the non-regularity inherent in optimal policy inference. By shifting the focus from fixed target policies to estimated optimal policies, NSAVE provides the first robust method for enabling valid confidence intervals in complex, data-selected scenarios. The paper sets a new theoretical foundation for future research into the asymptotic properties of value estimation in degenerate data regimes, ensuring that decision-making algorithms in complex fields are not only effective but also statistically reliable.

---

## Key Findings

*   **Failure of Existing Methods:** Existing Off-Policy Evaluation (OPE) methods often fail for estimated optimal policies, particularly when the optimal policy is non-unique or nearly deterministic.
*   **Non-Regularity Issues:** Non-regularity arises in the inference process specifically under conditions of policy non-uniqueness, complicating the characterization of the efficient influence function.
*   **NSAVE Performance:** The proposed NSAVE method achieves semiparametric efficiency and double robustness in cases where the optimal policy is unique.
*   **Stability in Degenerate Regimes:** NSAVE demonstrates stability in degenerate regimes that lie outside the scope of standard asymptotic theories used by current methods.
*   **Restoring Valid Inference:** Valid inference can be restored for non-unique optimal policies using a smoothing-based approach, while post-selection procedures allow for uniform coverage when policies are selected by data.

---

## Methodology

The research introduces a comprehensive framework for analyzing inference for optimal policy values in Markov decision processes:

1.  **Efficient Influence Function (EIF) Characterization:** The study characterizes the EIF to analyze inference, distinguishing between regular and non-regular scenarios.
2.  **NSAVE Introduction:** The authors introduce **NSAVE** (Nonparametric Sequential Value Evaluation), a novel semiparametric method designed to handle the complexities of optimal policy inference.
3.  **Smoothing and Post-Selection:** To address non-regularity and non-uniqueness, the methodology incorporates a smoothing-based approach and a post-selection procedure to ensure uniform coverage for data-selected policies.
4.  **Validation:** The approach is validated through simulation studies and a specific application to the **OhioT1DM mobile health dataset**.

---

## Technical Details

**Method Name:** NSAVE (Nonparametric SequentiAl Value Evaluation)

**Objective:** Off-policy evaluation (OPE) of the optimal policy value in Markov Decision Processes (MDPs), specifically targeting scenarios where the optimal policy is non-unique or deterministic.

**Framework:**
*   Utilizes semiparametric inference via Efficient Influence Functions (EIF).
*   **Nuisance Parameters:** Includes the Q-function, Marginal Importance Sampling (MIS) Ratio, and Value Function.

**Algorithm Strategy:**
The algorithm employs a sequential, online estimation strategy to avoid plug-in instability:
1.  **Initialization:** Randomly permutes data and estimates an initial optimal policy.
2.  **Sequential Loop:**
    *   Updates the optimal policy via argmax of the Q-function.
    *   Trains nuisance functions.
    *   Calculates trajectory estimating functions.
    *   Estimates variance.
3.  **Final Estimator:** A weighted average of trajectory scores using inverse standard deviations as weights.

---

## Results

The analysis focuses primarily on theoretical findings and qualitative stability:

*   **Theorem 3.1:** Establishes that if the optimal policy is unique and deterministic, the EIF exists and Regular Asymptotically Linear (RAL) estimators exist.
*   **Theorem 3.2:** States that if the optimal policy is non-unique, the functional has no influence function and standard RAL estimators do not exist.
*   **Comparison with SAVE:**
    *   **Nonparametric:** NSAVE removes the need for linear Q-function approximations (unlike SAVE).
    *   **Stability:** Maintains numerical stability in deterministic regimes where SAVE fails.
    *   **Robustness:** Retains the double robustness property.
*   **Inference Capabilities:** NSAVE achieves semiparametric efficiency for unique policies and uses smoothing-based approaches for valid inference when policies are data-selected or non-unique.

---

## Contributions

*   **Scope Extension:** Identifies and addresses the limitation of current OPE methods which focus on fixed target policies, extending the scope to estimated optimal policies.
*   **Formal Characterization:** Provides a formal characterization of non-regularity arising from policy non-uniqueness in the context of the efficient influence function.
*   **NSAVE Method:** Contributes a method that successfully combines semiparametric efficiency with the double robustness property while maintaining stability in degenerate asymptotic regimes.
*   **Technical Solutions:** Offers specific technical solutions (smoothing-based approach and post-selection procedures) to ensure valid confidence intervals for complex scenarios involving non-unique or data-selected optimal policies.