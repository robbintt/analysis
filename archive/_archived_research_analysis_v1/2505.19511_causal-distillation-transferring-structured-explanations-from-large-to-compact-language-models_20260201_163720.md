# Causal Distillation: Transferring Structured Explanations from Large to Compact Language Models

*Aggrey Muhebwa; Khalid K. Osman*

---

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Teacher Model** | GPT-4 (Proprietary) |
| **Student Models** | TinyLlama (1.1B), Phi-2 (~2B), Gemma-2B (2.2B) |
| **Dataset** | Climate-FEVER (1,535 claims) |
| **Optimization** | LoRA, 4-bit Quantization, LR 2e-5 |
| **Performance** | CEC Score ~0.52 |
| **Training Epochs** | 10 |

---

## üìù Executive Summary

A significant performance gap exists between large proprietary language models (LLMs) and smaller, open-source counterparts regarding causal reasoning capabilities. While large models like GPT-4 excel at generating structured cause-and-effect explanations, compact models often struggle to produce coherent and logically consistent reasoning chains. This disparity limits the deployment of trustworthy, high-performance AI in resource-constrained environments or scenarios requiring data privacy, as smaller models fail to provide the transparent explanations necessary for verifying their decisions.

To address this, the authors propose "Causal Distillation," a novel teacher-student framework designed to transfer implicit causal reasoning skills from GPT-4 to compact open-source models (TinyLlama, Phi-2, and Gemma-2B). The methodology involves supervised fine-tuning on a Climate-FEVER dataset augmented with synthetic structured explanations and utilizes optimization techniques like LoRA and 4-bit quantization. Crucially, the paper introduces a precise dual-component objective function that minimizes "Causal Explanation Divergence" by combining token-level cross-entropy loss with sentence-level semantic distance (cosine loss). Additionally, the authors introduce the Causal Explanation Coherence (CEC) metric, a principled evaluation method utilizing bidirectional cosine similarity to measure semantic alignment, ensuring robustness against paraphrasing variations while assessing both faithfulness and coverage.

Experiments conducted on the Climate-FEVER dataset demonstrated the framework's efficacy in enhancing the reasoning capabilities of student models. All distilled student models achieved strong quantitative results, with Causal Explanation Coherence (CEC) scores reaching approximately 0.52, indicating a successful transfer of structural and logical consistency from the teacher. Furthermore, baseline comparisons confirmed that the distilled models significantly outperformed non-distilled counterparts in generating logically consistent explanations.

This research establishes a vital pipeline for developing smaller, accessible models capable of robust causal reasoning, thereby democratizing access to advanced, explainable AI. By bridging the performance gap in reasoning tasks, the Causal Distillation framework facilitates the deployment of sophisticated language models on edge devices without sacrificing the ability to generate transparent explanations.

---

## üîç Key Findings

*   **Performance Gap:** Large proprietary language models currently possess superior causal reasoning abilities compared to smaller, open-source models.
*   **Successful Transfer:** The proposed framework successfully transfers causal reasoning capabilities from a large teacher model to a compact student model through the distillation of structured cause-and-effect explanations.
*   **New Evaluation Metric:** The new Causal Explanation Coherence (CEC) metric effectively measures the quality of student-generated explanations by assessing structural and logical consistency.
*   **Metric Robustness:** The CEC metric captures both faithfulness and coverage of the causal chain by utilizing sentence-level semantic alignment against the teacher's reference.

---

## üõ†Ô∏è Methodology

The authors introduce a 'Causal Distillation' framework based on a teacher-student architecture. The methodology involves training a compact student model to generate structured cause-and-effect explanations that are consistent with those produced by a larger, proprietary teacher model.

To evaluate the efficacy of this training, the authors developed the **Causal Explanation Coherence (CEC) metric**. This metric assesses the student's output by measuring sentence-level semantic alignment relative to the teacher's reference, ensuring that the generated reasoning adheres to the underlying logic and structure of the causal chain.

---

## ‚öôÔ∏è Technical Details

**Framework & Architecture**
*   **Approach:** Causal Knowledge Distillation via supervised fine-tuning.
*   **Transfer Target:** Implicit causal reasoning from GPT-4 (Teacher) to open-source compact models.
*   **Student Models:** TinyLlama (1.1B), Phi-2 (~2B), and Gemma-2B (2.2B).

**Objective Function**
*   **Goal:** Minimize "Causal Explanation Divergence".
*   **Loss Components:** Combination of token-level loss (cross-entropy) and cosine distance.

**Data & Inputs**
*   **Dataset:** Climate-FEVER (1,535 claims), augmented with synthetic explanations from GPT-4.
*   **Input Format:** Concatenated `[Claim]; [Evidence]; [Label]` followed by an explanation prompt.

**Optimization & Training**
*   **Quantization:** 4-bit
*   **Techniques:** LoRA (Low-Rank Adaptation)
*   **Hyperparameters:** 10 epochs, Learning rate of 2e-5, Batch size of 8.

**Evaluation Metric**
*   **Name:** Causal Explanation Coherence (CEC).
*   **Mechanism:** Assesses semantic alignment at the sentence level using bidirectional cosine similarity.
*   **Characteristics:** Robust to paraphrasing and order-invariant.

---

## üöÄ Contributions

1.  **Causal Distillation Framework**
    A novel approach for transferring complex causal reasoning skills from large proprietary models to smaller, accessible open-source models, bridging the performance gap in reasoning tasks.

2.  **Causal Explanation Coherence (CEC)**
    A new, principled metric designed to systematically evaluate the coherence of language model outputs, specifically targeting the structural and logical consistency of causal explanations.

3.  **Foundation for Robust Reasoning**
    The establishment of a training and evaluation pipeline that enables the development of smaller models capable of robust causal reasoning, addressing the challenge of explanation faithfulness and coverage.

---

## üìà Results

Experiments on the Climate-FEVER dataset demonstrated that all student models achieved high CEC scores (~0.52). Baseline comparisons confirmed that the distilled models significantly outperformed non-distilled counterparts in generating logically consistent explanations, validating the utility of the Causal Distillation framework over standard training approaches.

---

**Document Quality Score:** 8/10  
**References:** 30 citations