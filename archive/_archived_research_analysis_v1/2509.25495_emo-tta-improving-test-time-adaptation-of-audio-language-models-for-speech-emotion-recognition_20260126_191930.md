---
title: 'EMO-TTA: Improving Test-Time Adaptation of Audio-Language Models for Speech
  Emotion Recognition'
arxiv_id: '2509.25495'
source_url: https://arxiv.org/abs/2509.25495
generated_at: '2026-01-26T19:19:30'
quality_score: 6
citation_count: 0
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# EMO-TTA: Improving Test-Time Adaptation of Audio-Language Models for Speech Emotion Recognition

*Alicia Hong, Hongfei Du, George Mason, Index Terms, Jiacheng Shi, Ye Gao*

---

## ‚ö° Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Core Methodology** | Test-Time Adaptation (TTA) using Entropy Minimization |
| **Base Architecture** | Frozen Pre-trained Audio-Language Models (e.g., CLAP) |
| **Update Mechanism** | Learnable Text Prompts & Lightweight Adapters |
| **Key Benchmarks** | IEMOCAP, MELD |
| **Evaluation Metrics** | Weighted Accuracy (WA), Unweighted Accuracy (UA) |
| **Key Advantage** | Privacy-preserving (No source data required) |

---

## üìù Executive Summary

Speech Emotion Recognition (SER) models frequently face performance degradation due to domain shift‚Äîthe discrepancy between the acoustic conditions of training data and real-world deployment environments. This challenge is exacerbated by standard unsupervised domain adaptation methods that demand access to source data or labels during the adaptation phase. These dependencies impose significant resource constraints and privacy risks, limiting the practical applicability of SER systems in dynamic settings where source information is unavailable or restricted.

To address these limitations, the authors introduce **EMO-TTA**, a test-time adaptation framework designed for frozen pre-trained Audio-Language Models (ALMs), specifically utilizing architectures like CLAP. Operating strictly at inference time, the technique minimizes entropy to optimize model confidence on unlabeled target data. Rather than fine-tuning the entire backbone, EMO-TTA updates only learnable text prompts or lightweight adapters. This mechanism aligns audio features with the semantic space of emotion descriptions, bridging the distribution gap between training and testing domains without altering the model's core weights.

Evaluation on the IEMOCAP and MELD benchmarks provides quantitative evidence of the framework's efficacy, measured via Weighted Accuracy (WA) and Unweighted Accuracy (UA). The results demonstrate that EMO-TTA achieves State-of-the-Art (SOTA) performance, delivering superior metric scores compared to standard zero-shot Audio-Language Models and audio-only baselines. Specifically, the method significantly outperformed existing approaches in both zero-shot and cross-corpus settings, validating that the integration of semantic text knowledge effectively mitigates the accuracy losses typically associated with domain shift.

This work establishes a practical advancement for SER deployment by offering a privacy-preserving solution that requires only unlabeled test streams. By decoupling the adaptation process from source data dependencies, EMO-TTA enables real-time robustness in diverse acoustic environments. Furthermore, the parameter-efficient nature of updating prompts or adapters ensures minimal computational overhead, presenting a scalable paradigm for leveraging multi-modal semantic context in resource-constrained applications.

---

## üîç Key Findings

*   **Superior Cross-Domain Performance:** Significantly outperforms standard Audio-Language Models and baselines in zero-shot and cross-corpus Speech Emotion Recognition scenarios.
*   **Effective Domain Mitigation:** Successfully mitigates distribution shift between source and target domains without requiring access to source data or labels.
*   **Semantic Robustness:** Leveraging semantic knowledge from the text modality leads to more robust recognition compared to audio-only methods.
*   **Benchmark Success:** Achieved state-of-the-art results on major benchmarks (**IEMOCAP** and **MELD**) regarding both Weighted Accuracy (WA) and Unweighted Accuracy (UA).

---

## üõ†Ô∏è Methodology

The researchers propose **EMO-TTA**, a test-time adaptation framework built upon frozen pre-trained Audio-Language Models (such as CLAP).

*   **Operation Mode:** The method functions exclusively at inference time.
*   **Optimization Strategy:** It employs **entropy minimization** to optimize model confidence on the target test data.
*   **Parameter Efficiency:** Instead of full fine-tuning, the framework updates only learnable text prompts or lightweight adapters. This aligns audio features with the semantic space of emotion text descriptions without modifying the pre-trained backbone.

---

## ‚öôÔ∏è Technical Details

**System Architecture**
*   **Core Components:** Audio-Language Models (ALMs) combined with Test-Time Adaptation (TTA).
*   **Modality:** Multi-modal system utilizing both semantic knowledge (text) and acoustic signals (audio).
*   **Contextual Technologies:** PANNs (Pre-trained Audio Neural Networks) and HTS-AT (Hierarchical Token-Semantic Audio Transformer).

**Optimization & Adaptation**
*   **Objective:** Enable robust recognition for Zero-shot and Cross-corpus SER.
*   **Constraint:** Mitigates distribution shift without requiring source data or source labels during the adaptation phase.

---

## üìä Contributions

*   **Novel Formulation:** Introduces a novel Test-Time Adaptation formulation specifically tailored for Audio-Language Models in the context of speech emotion recognition.
*   **Semantic Guidance:** Demonstrates how the semantic context of language descriptions can effectively guide acoustic feature adaptation.
*   **Privacy-Preserving Solution:** Provides a practical solution for real-world deployment that relies solely on unlabeled test streams, removing dependency on private source data.
*   **Parameter Efficiency:** Offers an adaptation approach with minimal computational overhead, suitable for resource-constrained environments.

---

## üìà Evaluation & Results

*   **Benchmarks:** IEMOCAP and MELD.
*   **Metrics:** Weighted Accuracy (WA) and Unweighted Accuracy (UA).
*   **Performance:** The method claimed to significantly outperform standard Audio-Language Models and audio-only baselines, achieving **State-of-the-Art (SOTA)** results on the specified benchmarks.

---

*Document generated from research analysis. Quality Score: 6/10 | Citations: 0*