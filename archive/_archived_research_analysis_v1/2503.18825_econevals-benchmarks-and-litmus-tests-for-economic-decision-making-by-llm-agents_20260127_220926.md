---
title: 'EconEvals: Benchmarks and Litmus Tests for Economic Decision-Making by LLM
  Agents'
arxiv_id: '2503.18825'
source_url: https://arxiv.org/abs/2503.18825
generated_at: '2026-01-27T22:09:26'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# EconEvals: Benchmarks and Litmus Tests for Economic Decision-Making by LLM Agents

*Julia Shephard, Minkai Li, Ran Shorrer, Sara Fish (Harvard University, Penn State)*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Citations:** 40
> *   **Key Domains:** Economic Decision, Procurement, Scheduling, Pricing
> *   **Core Metrics:** Litmus Score, Reliability Score, Competency Score
> *   **Methodology:** Dual-component (Benchmarks + Litmus Tests) over 100-period sessions

---

## Executive Summary

As Large Language Models (LLMs) transition from passive text generators to autonomous agents, the lack of rigorous evaluation frameworks for their economic rationality poses a significant barrier to real-world deployment. General-purpose benchmarks fail to capture the specific decision-making capabilities required for economic tasks, such as optimizing under budget constraints, handling dynamic environments, and managing tradeoffs between conflicting objectives. This paper addresses the critical gap between general LLM performance and economic utility, emphasizing the need to understand how these models navigate complex scenarios involving risk, uncertainty, and multi-step reasoning.

The authors introduce **EconEvals**, a dual-component evaluation framework comprising domain-specific economic benchmarks and a novel behavioral "litmus test" protocol. Technically, the system utilizes a lightweight, tool-based interaction architecture where agents employ getter and action tools, supported by persistent memory mechanisms, over extended 100-period chat sessions. The environments are categorized as **Stationary** (Procurement, Scheduling) or **Non-Stationary** (Pricing) to test in-context learning. A key technical innovation is the Litmus Test framework and its associated quantitative metrics: the **Litmus Score**, the **Reliability Score**, and the **Competency Score**.

Evaluation across frontier LLMs reveals a significant temporal evolution in economic capabilities, with newer generations outperforming predecessors in complex scenarios. Behavioral analysis of chain-of-thought reasoning confirmed that LLMs effectively utilize "internal discount factors" during decision-making. Conversely, early-generation models displayed a strong tendency for underexploration in multi-armed bandit environments. The study successfully validated the Litmus Test framework as robust and self-consistent; however, it identified a limitation in current models: while they demonstrate high Competency Scores when optimizing single goals, they struggle significantly with tasks involving multiple conflicting objectives.

This research establishes a foundational methodology for evaluating LLM agents specifically within economic contexts, providing a standardized approach to dissectingå†³ç­–-making beyond simple accuracy. These findings are pivotal for the future development of autonomous economic agents, signaling that while models are becoming capable of dynamic in-context learning over 100-period sessions, their ability to handle the nuance of conflicting incentives remains a critical area for further research and engineering.

---

## Key Findings

*   **Temporal Evolution of Capabilities**
    The study reveals significant insights into how the economic decision-making capabilities and behavioral tendencies of frontier LLMs have changed over time, noting marked improvements in newer model generations.

*   **Behavioral Analysis**
    By analyzing chain-of-thought reasoning and choice behavior, the authors derived economically meaningful insights regarding how models navigate complex scenarios, specifically confirming the presence of 'internal discount factors'.

*   **Framework Validation**
    The proposed "litmus test" framework was successfully validated, exhibiting strong self-consistency, robustness, and generalizability across different models.

*   **Performance on Conflicting Objectives**
    The research highlights the specific ability (or inability) of LLMs to handle tasks with multiple conflicting objectives. Models generally excel at single goals (Competency) but struggle with strategic tradeoffs (Litmus/Reliability).

---

## Methodology

The research employs a two-pronged evaluation strategy combining domain-specific benchmarks with a novel behavioral framework:

### 1. Economic Benchmarks
The authors designed benchmarks based on fundamental economic problemsâ€”specifically **procurement**, **scheduling**, and **pricing**.
*   **Focus:** Tests an LLM's ability to perform in-context learning, adapting to the environment dynamically.
*   **Context:** Simulates real-world economic constraints and optimization problems.

### 2. Litmus Tests Framework
This involves stylized decision-making tasks characterized by multiple conflicting objectives. The framework introduces three distinct quantitative metrics to dissect agent behavior:
*   **Litmus Score:** Quantifies the model's tradeoff response.
*   **Reliability Score:** Measures the coherence of the model's choice behavior.
*   **Competency Score:** Assesses capability when conflicting objectives are replaced by a single, well-defined goal.

### 3. Evaluation Scope
The methodology was applied to a broad array of 'frontier' (state-of-the-art) LLMs to assess current industry standards and temporal progress.

---

## Technical Details

### System Architecture
*   **Interaction Protocol:** Lightweight, tool-based interaction protocol (function calling) utilizing getter and action tools.
*   **Session Length:** Single 100-period chat session.
*   **Memory Mechanism:** Persistent memory across periods using `write_notes` and `read_notes` tools.

### Environment Categories
*   **Stationary Environments:** Procurement, Scheduling.
*   **Non-Stationary Environments:** Pricing.
*   **Difficulty Scaling:** Scales from Basic to Hard.

### Procurement Benchmark Specifications
*   **Objective:** Maximize the quantity of workers within budget B.
*   **Mathematical Formulation:**
    $$f(z_1,...,z_n) := \prod_{i=1}^{k} (\sum_{a_j \in A_i} e_j z_j)^{1/k}$$
*   **Variables:** Involves substitutes and complements.
*   **Deal Types:** Simulates Simple, Bulk Only, and Two-Part Tariff deal types.

---

## Contributions

*   **Novel Evaluation Metrics**
    Introduction of a standardized scoring system (Litmus, Reliability, and Competency scores) to dissect LLM decision-making beyond simple accuracy.

*   **Domain-Specific Benchmarks**
    Creation of rigorous testing protocols derived from core economic disciplines (procurement, scheduling, pricing) to assess in-context learning.

*   **Foundation for Economic Agents**
    The paper establishes a foundational methodology for evaluating LLM agents specifically as they are integrated into real-world economic decision-making roles, addressing the gap between general LLM evaluation and economic utility.

---

## Results Analysis

*   **Agent Performance:** Frontier LLMs exhibit a significant temporal evolution in economic decision-making capabilities across model generations.
*   **Strategic Reasoning:** Chain-of-thought analysis confirms that LLMs possess 'internal discount factors' in decision-making contexts.
*   **Exploration Issues:** Early-generation LLMs display a tendency for underexploration in multi-armed bandit environments.
*   **Framework Robustness:** The Litmus Test framework was validated as robust and self-consistent, generalizable across different models.
*   **Limitations Identified:** The research identified limitations in LLMs' ability to handle tasks with multiple conflicting objectives, distinguishing this from their capability to optimize single, well-specified goals.