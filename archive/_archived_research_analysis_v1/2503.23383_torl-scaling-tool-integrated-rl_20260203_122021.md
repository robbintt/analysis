---
title: 'ToRL: Scaling Tool-Integrated RL'
arxiv_id: '2503.23383'
source_url: https://arxiv.org/abs/2503.23383
generated_at: '2026-02-03T12:20:21'
quality_score: 9
citation_count: 24
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# ToRL: Scaling Tool-Integrated RL
*Xuefeng Li; Haoyang Zou; Pengfei Liu*

> ### ðŸ“Œ Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Quality Score** | 9/10 |
> | **References** | 24 Citations |
> | **Base Models** | Qwen2.5-Math (1.5B & 7B) |
> | **Dataset Size** | 28,740 distilled questions |
> | **Key Benchmark** | AIME 24: **43.3%** Accuracy |

---

## Executive Summary
Current approaches to tool integration in Large Language Models (LLMs) primarily rely on Supervised Fine-Tuning (SFT), which depends on static, pre-collected demonstrations of tool usage. This method is fundamentally limited because it restricts models to mimicking specific patterns found in training data rather than learning optimal strategies for tool interaction. Consequently, models often fail to generalize to novel scenarios or determine the most effective moments to utilize external computational resources.

This paper addresses this critical need by introducing **ToRL (Tool-Integrated Reinforcement Learning)**, a novel framework that shifts the training paradigm from SFT to reinforcement learning. Unlike traditional methods, ToRL scales RL directly from base modelsâ€”specifically Qwen2.5-Math architecturesâ€”without requiring prior SFT on tool-use trajectories. The framework employs an iterative, reward-driven loop consisting of natural language reasoning, code generation, and tool execution. This allows the model to update its trajectory dynamically based on computational feedback, learning optimal policies through trial and error.

The significance of this research lies in its validation of reinforcement learning as a superior method for tool integration compared to supervised approaches. By establishing new state-of-the-art benchmarks on mathematical reasoning tasks, ToRL provides a robust blueprint for developing agentic systems capable of sophisticated tool use. Furthermore, the study offers critical insights into emergent capabilities, demonstrating that complex behaviorsâ€”such as strategic tool invocation, self-regulation, and dynamic switching between computational and analytical reasoningâ€”can arise spontaneously from reward maximization.

---

## Key Findings
*   **Benchmark Performance:** The ToRL-7B model achieved **43.3%** accuracy on the AIME 24 benchmark, significantly outperforming baseline methods.
*   **Superiority Over Baselines:**
    *   Surpassed standard reinforcement learning (without tools) by **14%**.
    *   Outperformed the best existing Tool-Integrated Reasoning (TIR) model by **17%**.
*   **Model Efficiency:** The 7B model's performance is comparable to some 32B models, demonstrating high parameter efficiency.
*   **Emergent Behaviors:** The model demonstrated autonomous strategic behaviors, including:
    *   Learning *when* to invoke tools.
    *   Self-regulation by dynamically switching between computational and analytical reasoning.
    *   Iterative verification to resolve inconsistencies.

---

## Methodology
The researchers developed the **ToRL Framework**, designed to train Large Language Models (LLMs) to autonomously utilize computational tools.

*   **Paradigm Shift:** Unlike traditional Supervised Fine-Tuning (SFT) that relies on static demonstrations, ToRL employs a reinforcement learning paradigm.
*   **Exploration & Learning:** This allows the model to actively explore its environment and discover optimal tool-use strategies through reward-driven learning.
*   **Experimental Models:** Experiments were conducted using Qwen2.5-Math models (1.5B and 7B parameters).

---

## Technical Details
ToRL is a framework that scales reinforcement learning from base models without requiring prior Supervised Fine-Tuning on tool-use trajectories.

**System Architecture & Process**
*   **Iterative Loop:** The system employs a cycle involving:
    1.  Natural language reasoning ($r$)
    2.  Code generation ($c$)
    3.  Tool execution ($o$)
*   **Dynamic Trajectory Updates:** The trajectory $s_k$ is updated dynamically based on computational feedback from a code interpreter.

**Data Processing**
*   **Sources:** Training data was sourced from Numina-MATH, MATH, and DeepScaleR.
*   **Filtering:** Data was filtered to **75,149** verifiable questions.
*   **Distillation:** The final set was distilled using LIMR to **28,740** questions.

---

## Results
The ToRL framework demonstrated substantial improvements across different model sizes, particularly in mathematical reasoning tasks.

**Performance Metrics (AIME 24)**
*   **ToRL-7B:** Achieved **43.3%** accuracy.
*   **Comparison to TIR:** Outperformed Qwen-2.5-Math-Instruct-TIR by **14%**.
*   **Comparison to Standard RL:** Outperformed standard RL (no tools) by **14%**.

**Model Size Analysis**
*   **1.5B Model:** Showed improvements of up to **12%** over baselines without tools.
*   **7B Model:** Showed improvements of up to **14%** over baselines without tools.

**Behavioral Observations**
The models exhibited the ability to self-correct and verify results iteratively, resolving conflicts between their own reasoning traces and the output provided by external tools.

---

## Contributions
*   **Novel Framework:** Introduces the ToRL Framework, representing a significant shift from supervised fine-tuning to reinforcement learning for tool integration.
*   **Benchmark Advancement:** Advances the field of Tool-Integrated Reasoning by establishing new performance benchmarks on mathematical reasoning tasks (AIME 24).
*   **Insights into Emergence:** Provides evidence that complex behaviors (dynamic adaptation and self-correction) can emerge spontaneously from reward maximization without explicit supervised instructions.