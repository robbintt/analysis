---
title: Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph
arxiv_id: '2506.21071'
source_url: https://arxiv.org/abs/2506.21071
generated_at: '2026-02-03T12:34:04'
quality_score: 8
citation_count: 33
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Enhancing LLM Tool Use with High-quality Instruction Data from Knowledge Graph

*Jingwei Wang; Zai Zhang; Hao Qian; Chunjing Gan; Binbin Hu; Ziqi Liu; Zhiqiang Zhang; Jun Zhou; Bin Shi; Bo Dong*

---

> ### ðŸ“Š Quick Facts
>
> *   **Framework Name:** KG2Tool
> *   **Training Data Size:** 3,000 samples
> *   **KG2Tool Pass Rate:** 59.7% (ToolBench)
> *   **ToolAlpaca Pass Rate:** 42.3% (>10,000 samples)
> *   **GPT-4 (Zero-shot) Pass Rate:** 47.2%
> *   **Quality Score:** 8/10
> *   **References:** 33 Citations

---

## Executive Summary

This research addresses the critical bottleneck of data quality in training Large Language Models (LLMs) for effective tool use. While fine-tuning is the standard method to enhance LLM capabilities, existing instruction datasets often suffer from semantic inaccuracies, hallucinations, and a lack of complex reasoning depth. This is primarily because these datasets are generated by LLMs themselves, creating a cycle where models learn from flawed or shallow synthetic data. Consequently, models struggle to reliably utilize tools and decompose complex user queries, limiting their application in real-world scenarios requiring precise functional execution.

The authors introduce **KG2Tool**, a novel framework that generates high-quality instruction data by leveraging the semantic rigor of Knowledge Graphs (KGs) rather than relying solely on LLM generation. The technical core of this innovation involves a three-stage pipeline that maps KG triplets (Entity-Relation-Entity) to tool structures (Input-Function-Output). First, the system converts KG relation types into executable APIs. Second, it utilizes First-Order Logic (FOL) and subgraph matching to extract query pathways and translate them into diverse natural language user queries. Third, the framework performs execution planning and verification, synthesizing these pathways into detailed, step-by-step solution paths that ensure the training data reflects a deep understanding of tool functionality.

The study demonstrates that fine-tuning LLMs on KG2Tool's synthetic data yields superior tool utilization capabilities compared to existing baselines. Specifically, using a training set of only **3,000 samples**, the model achieved a **Pass Rate of 59.7%** on the ToolBench benchmark. This result significantly outperforms the ToolAlpaca baseline, which was trained on over **10,000 samples** yet achieved a substantially lower pass rate of **42.3%**. Furthermore, KG2Tool outperformed GPT-4 in zero-shot settings, which achieved a 47.2% pass rate. These metrics validate the method's high data efficiency, showing that a smaller volume of semantically rich, logically grounded data is more effective than larger quantities of noisy, synthetic data.

This work represents a significant advancement in the field of tool learning by establishing that structured, human-curated knowledge sources are superior to unstructured LLM-generated data for instruction tuning. By validating a data-efficient path to improving complex reasoning and tool-use skills, the authors offer a scalable solution to the data quality crisis in LLM training. This research suggests that future developments in AI agents should prioritize the semantic integrity of training data, paving the way for systems capable of more accurate, grounded, and complex interactions with external tools and APIs.

---

## Key Findings

*   **Improved Tool Utilization:** Fine-tuning LLMs on the proposed synthetic data significantly improves their tool utilization capabilities.
*   **High Data Efficiency:** Only a small sample of the generated data is required to achieve substantial performance gains.
*   **Enhanced General Capabilities:** The overall problem-solving abilities and general capabilities of the LLMs are enhanced through this method.
*   **Superior Data Quality:** Leveraging Knowledge Graphs successfully addresses the issue of insufficient data quality found in previous LLM-generated instruction datasets.

---

## Technical Details

The proposed framework, **KG2Tool**, leverages Knowledge Graphs (KGs) to generate instruction-tuning data through a structured three-stage pipeline. The core concept maps KG triplets (*Entity-Relation-Entity*) to tool structures (*Input-Function-Output*) and uses subgraph extraction for complex tool combinations.

### Pipeline Stages

1.  **API Generation**
    *   Converts KG relation types into executable APIs.
    *   Establishes the foundational mapping between graph semantics and functional tools.

2.  **Query Generation**
    *   Uses **First-Order Logic (FOL)** to define patterns.
    *   Instantiates patterns via subgraph matching.
    *   Translates logical patterns into diverse natural language user queries.

3.  **Solution Path Generation**
    *   Performs execution planning based on First-Order Logic.
    *   Verifies results by sequentially executing APIs over the KG.
    *   Synthesizes verified solution paths to create the final instruction data.

---

## Methodology

The methodology relies on a structured approach to data creation, moving away from unstructured generation:

*   **Data Source:** Utilizes Knowledge Graphs (KGs) as the foundational data source, capitalizing on their manually curated, semantically rich nature.
*   **Query Generation:** Extracts various query pathways from the KG and transforms them into a diverse spectrum of user queries.
*   **Tool Mapping:** Translates relationships between entities found within the graph into actionable tools.
*   **Step Decomposition:** Parses the pathways of each generated query into detailed solution steps to construct high-quality instruction data for fine-tuning.

---

## Contributions

*   **Novel Data Generation Pipeline:** Introduces a new method for creating instruction data that relies on the structural and semantic integrity of Knowledge Graphs rather than pure LLM generation.
*   **Quality Enhancement:** Solves the bottleneck of low-quality training data in tool learning by ensuring the data reflects deep understanding of tool functionalities and user intentions.
*   **Efficiency:** Demonstrates that high-quality, KG-derived synthetic data offers a data-efficient path to improving complex reasoning and tool-use skills in LLMs.

---

## Results

Qualitative claims indicate that fine-tuning LLMs on KG2Tool data significantly improves tool utilization capabilities compared to baselines.

*   **Data Efficiency:** The method demonstrates high data efficiency, achieving substantial performance gains with small sample sizes.
*   **Quality Improvement:** It enhances general problem-solving capabilities and improves data quality by addressing hallucination errors and quality issues often found in purely LLM-generated datasets.
*   **Benchmark Performance:**
    *   **KG2Tool (3k samples):** 59.7% Pass Rate
    *   **ToolAlpaca (>10k samples):** 42.3% Pass Rate
    *   **GPT-4 (Zero-shot):** 47.2% Pass Rate