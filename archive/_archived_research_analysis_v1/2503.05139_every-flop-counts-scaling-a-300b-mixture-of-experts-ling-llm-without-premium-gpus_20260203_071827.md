---
title: 'Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium
  GPUs'
arxiv_id: '2503.05139'
source_url: https://arxiv.org/abs/2503.05139
generated_at: '2026-02-03T07:18:27'
quality_score: 8
citation_count: 14
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium GPUs

*Ling Team; Binwei Zeng; Chao Huang; Chao Zhang; Changxin Tian; Cong Chen; Dingnan Jin; Feng Yu; Feng Zhu; Feng Yuan; Fakang Wang; Gangshan Wang; Guangyao Zhai; Haitao Zhang; Huizhong Li; Jun Zhou; Jia Liu; Junpeng Fang; Junjie Ou; Jun Hu; Ji Luo; Ji Zhang; Jian Liu; Jian Sha; Jianxue Qian; Jiewei Wu; Junping Zhao; Jianguo Li; Jubao Feng; Jingchao Di; Junming Xu; Jinghua Yao; Kuan Xu; Kewei Du; Longfei Li; Lei Liang; Lu Yu; Li Tang; Lin Ju; Peng Xu; Qing Cui; Song Liu; Shicheng Li; Shun Song; Song Yan; Tengwei Cai; Tianyi Chen; Ting Guo; Ting Huang; Tao Feng; Tao Wu; Wei Wu; Xiaolu Zhang; Xueming Yang; Xin Zhao; Xiaobo Hu; Xin Lin; Yao Zhao; Yilong Wang; Yongzhen Guo; Yuanyuan Wang; Yue Yang; Yang Cao; Yuhao Fu; Yi Xiong; Yanzhe Li; Zhe Li; Zhiqiang Zhang; Ziqi Liu; Zhaoxin Huan; Zujie Wen; Zhenhang Sun; Zhuoxuan Du; Zhengyu He*

---

### ⚡ Quick Facts

| Metric | Details |
| :--- | :--- |
| **Model Variants** | **Ling-Lite**: 16.8B Total / 2.75B Active<br>**Ling-Plus**: 290B Total / 28.8B Active |
| **Cost Efficiency** | ~20% savings on computing costs vs. premium clusters. |
| **Infrastructure** | DLRover, XPUTimer (90% memory reduction), EDiT (66.1% time reduction). |
| **Top Benchmark** | **Ling-Plus** scored **87.1%** on C-Eval. |
| **Tool Use** | Superior capability via Knowledge Graph data (BFCL_v2: 75.39%). |

---

## Executive Summary

Training state-of-the-art Large Language Models (LLMs) at the scale of hundreds of billions of parameters has traditionally been restricted to well-funded organizations due to the prohibitive cost and limited accessibility of premium, high-performance GPU clusters. This paper addresses the critical barrier to entry in AI development by investigating whether massive scale can be achieved using commodity, lower-performance hardware. The authors challenge the prevailing assumption that top-tier infrastructure is a prerequisite for developing competitive models, highlighting the need for more cost-efficient and sustainable methodologies in the face of escalating computational demands.

The research introduces a holistic optimization stack centered on the Ling LLM family, which utilizes a Mixture-of-Experts (MoE) architecture to maximize the utility of every FLOP on resource-constrained devices. The authors developed two variants: Ling-Lite (16.8B total parameters, 2.75B activated) and Ling-Plus (290B total parameters, 28.8B activated), optimizing for inference efficiency by drastically reducing the number of active parameters. Technically, the solution integrates a custom infrastructure featuring DLRover for unified backend management and EDiT, a system for elastic distributed training that adapts to the volatility of lower-spec hardware. To ensure robustness, the team implemented advanced anomaly detection and auto-checkpoint recovery protocols. The system also incorporates storage optimizations such as Device Multi-tenancy, which allows multiple tasks to share GPU resources, alongside FUSE and PCache to minimize I/O overhead. Additionally, a novel data strategy leverages knowledge graphs to specifically enhance the model's tool-use capabilities and reasoning depth.

The study demonstrates that training the 290-billion-parameter Ling-Plus on non-premium hardware achieved approximately 20% savings in computing costs compared to high-performance clusters, without sacrificing model quality. The infrastructure optimizations yielded substantial efficiency gains: XPUTimer reduced memory usage by 90%, EDiT decreased training time by up to 66.1%, and I/O optimization cut overhead by 50%. In benchmark evaluations, Ling-Plus achieved performance parity with industry leaders, scoring 67.74% on MMLU-Pro and 87.1% on C-Eval. Notably, the implementation of the knowledge graph data strategy successfully translated into superior tool-use performance, evidenced by a score of 75.39% on BFCL_v2. Furthermore, the Flood inference framework ensured consistent evaluation with less than 0.5% deviation across clusters.

This work represents a significant shift toward the democratization of AI development by providing a proof-of-concept that state-of-the-art LLMs can be trained without access to elite hardware. By validating specific technical strategies for architecture optimization and resource-constrained training, the authors lower the barrier to entry for high-performance AI, enabling a wider range of institutions to contribute to large-scale model research. The findings promote a more sustainable paradigm for future AI scaling, suggesting that algorithmic efficiency and infrastructure optimization can effectively substitute for expensive capital expenditures in GPU clusters.

---

## Key Findings

*   **Model Architecture and Scale:** Successfully developed two Mixture-of-Experts (MoE) LLMs—Ling-Lite (16.8B total parameters, 2.75B activated) and Ling-Plus (290B total parameters, 28.8B activated)—demonstrating that massive scale is achievable without premium hardware.
*   **Cost-Efficiency:** Training a 300B-parameter MoE model on lower-performance devices resulted in approximately **20% savings** in computing costs compared to high-performance device clusters, without significant performance degradation.
*   **Benchmark Parity:** Both Ling-Lite and Ling-Plus achieved performance levels comparable to leading industry benchmarks, including both dense and MoE models of similar scale.
*   **Enhanced Capabilities:** By leveraging high-quality data generated from knowledge graphs, the models exhibited superior capabilities in tool use compared to other existing models.

---

## Methodology

The research focuses on maximizing computational efficiency and stability within resource-constrained environments. The methodology includes:

*   **Optimization Implementation:** Implementing innovative optimizations for model architecture and the underlying training processes to maximize the utility of every FLOP.
*   **Anomaly Handling:** Refining protocols for training anomaly handling to ensure robust training on lower-specification hardware.
*   **Evaluation Efficiency:** Enhancing model evaluation efficiency to reduce the overhead associated with assessing large-scale models.
*   **Data Strategy:** Utilizing high-quality data derived from knowledge graphs to specifically improve tool-use capabilities.

---

## Technical Details

**Architecture & Design**
*   **Type:** Mixture-of-Experts (MoE)
*   **Variants:**
    *   **Ling-Lite:** 16.8B Total / 2.75B Active Parameters
    *   **Ling-Plus:** 290B Total / 28.8B Active Parameters
*   **Basis:** Scaling law analysis and knowledge graph data strategy for tool-use.

**Infrastructure Stack**
*   **DLRover:** Unified backend integration.
*   **XPUTimer:** Runtime performance analysis (achieved 90% memory usage reduction).
*   **EDiT:** Elastic distributed training (reduced training time by up to 66.1%).

**Storage & I/O**
*   **Optimizations:** Device Multi-tenancy, FUSE, PCache, cross-cluster synchronization.
*   **Impact:** I/O optimization cut overhead by 50%.

**Robustness & Evaluation**
*   **Stability:** Anomaly detection and auto checkpoint recovery.
*   **Framework:** Flood offline inference framework for consistent cross-cluster evaluation (<0.5% deviation).

---

## Results

**Infrastructure Efficiency**
*   **Computing Cost:** ~20% savings on lower-performance devices.
*   **Memory Usage:** XPUTimer reduction of 90%.
*   **Training Speed:** EDiT reduction of up to 66.1%.
*   **I/O Overhead:** 50% reduction.

**Benchmark Performance**

| Benchmark | Ling-Lite | Ling-Plus |
| :--- | :---: | :---: |
| **MMLU-Pro** | 49.12% | **67.74%** |
| **GPQA** | 28.28% | **42.55%** |
| **LiveCodeBench** | 18.75% | **27.68%** |
| **AIME 2024** | 13.33% | **20.0%** |
| **BFCL_v2 (Tool Use)** | 67.53% | **75.39%** |
| **C-Eval** | 73.06% | **87.1%** |

Both models demonstrated performance parity with industry benchmarks and superior tool-use capabilities.

---

## Contributions

*   **Democratization of AI Development:** The report provides actionable insights and a proof-of-concept that training state-of-the-art, 300B-scale MoE LLMs is feasible on non-premium hardware, significantly lowering the barrier to entry for high-performance AI development.
*   **Resource-Constrained Training Protocols:** The work contributes specific technical strategies for architecture optimization, anomaly handling, and evaluation that address the prevalent cost inefficiencies and resource limitations in training large MoE systems.
*   **Sustainable Scaling:** By demonstrating comparable performance at reduced costs, the research promotes a more scalable and sustainable paradigm for future large language model training.

---

**References:** 14 citations  
**Quality Score:** 8/10