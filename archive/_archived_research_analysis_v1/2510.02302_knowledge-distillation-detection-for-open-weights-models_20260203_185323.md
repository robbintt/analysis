---
title: Knowledge Distillation Detection for Open-weights Models
arxiv_id: '2510.02302'
source_url: https://arxiv.org/abs/2510.02302
generated_at: '2026-02-03T18:53:23'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Knowledge Distillation Detection for Open-weights Models

*Qin Shi; Amber Yijia Zheng; Qifan Song; Raymond A. Yeh*

---

> ### üìä Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Citations:** 40
> *   **CIFAR-10 Accuracy:** 59.6%
> *   **ImageNet Accuracy:** 71.2%
> *   **Text-to-Image Accuracy:** 20.0%
> *   **Framework Type:** Data-free, Model-agnostic
> *   **Availability:** Open-source implementation released

---

## Executive Summary

**Problem**
As the prevalence of open-weight AI models grows, ensuring model provenance and protecting intellectual property (IP) becomes increasingly critical. This paper addresses the novel challenge of Knowledge Distillation Detection (KDD): determining whether a specific "teacher" model was used to distill knowledge into a target "student" model. This issue is of paramount importance because distillation is frequently used for both legitimate model compression and malicious model theft; however, detecting this lineage is historically difficult without access to the original training data or the internal training logs.

**Innovation**
The authors introduce a model-agnostic framework designed to operate in a practical, data-free setting, requiring only access to the student's weights and the teacher's black-box API. The core innovation is a three-stage pipeline consisting of Input Construction, Score Computation, and Prediction. Technically, the method employs a Mixup-based strategy to synthesize inputs that maximize student confidence while aligning with the student‚Äôs Batch Normalization Statistics (BNS). Detection is formulated as a multiple-choice task, utilizing a Point-wise Score (based on the inverse of KL Divergence) and a Set-level Score (Aligned Cosine Similarity using SVD on centered logit matrices) to measure statistical dependencies between the student and potential teachers.

**Results**
The proposed method demonstrates significant performance improvements over existing baselines, achieving state-of-the-art accuracy across multiple benchmarks. In image classification tasks, the framework achieved **59.6%** accuracy on CIFAR-10 and **71.2%** accuracy on ImageNet. Furthermore, the approach exhibits broad applicability beyond discriminative tasks, achieving **20.0%** accuracy in detecting distillation within text-to-image generation scenarios. These results validate the method's ability to reliably identify the source teacher model from a pool of candidates.

**Impact**
This research formally defines the KDD task and provides the first comprehensive, practical solution for detecting distillation lineage without training data. By establishing a feasible detection mechanism in a resource-constrained environment (using only student weights and teacher APIs), the work offers a vital tool for auditing the AI supply chain and protecting model IP. The release of open-source implementation code further ensures that the findings will serve as a benchmark for future research in model security and provenance for both discriminative and generative architectures.

---

## Key Findings

*   ‚úÖ **Significant Performance Improvements:** Achieved **59.6%** accuracy on CIFAR-10, **71.2%** on ImageNet, and **20.0%** for text-to-image generation, surpassing existing baselines.
*   ‚úÖ **Broad Applicability:** Validated across both discriminative (image classification) and generative (text-to-image) tasks.
*   ‚úÖ **Data-Free Feasibility:** The method is practically feasible without access to original training data, relying solely on student weights and the teacher API.

---

## Methodology

The researchers introduce a **model-agnostic framework** designed to detect knowledge distillation without access to training datasets. The methodology relies on two core components:

1.  **Data-free Input Synthesis:** Probing the models to generate synthetic inputs that maximize the student model's confidence.
2.  **Statistical Score Computation:** Analyzing the interactions between the synthesized inputs, the student weights, and the outputs from the teacher API to determine lineage.

---

## Technical Details

The approach formulates Knowledge Distillation Detection (KDD) as a multiple-choice task to identify if a specific teacher model distilled a student model. It operates under strict constraints: open-weight student access, black-box teacher API access, and data-free availability.

### Pipeline Architecture

The framework utilizes a three-stage pipeline:

1.  **Input Construction**
2.  **Score Computation**
3.  **Prediction**

### Input Synthesis Strategy
*   **Technique:** Mixup-based strategy.
*   **Optimization:** A generator is trained to maximize student confidence.
*   **Alignment:** Aligns Batch Normalization Statistics (BNS) with the student's internal data distribution.

### Scoring Metrics
Detection relies on two primary metrics to measure statistical dependencies:

*   **Point-wise Score:** Calculated as the inverse of KL Divergence.
*   **Set-level Score:** Calculated using Aligned Cosine Similarity (ACS), applying Singular Value Decomposition (SVD) on centered logit matrices.

---

## Contributions

*   üìù **Formal Definition:** Provided the first formal definition of the 'knowledge distillation detection' (KDD) task.
*   üõ†Ô∏è **Framework Introduction:** Introduced a practical, model-agnostic framework utilizing data-free synthesis and statistical analysis.
*   üèÜ **Empirical Validation:** Conducted comprehensive empirical validation establishing state-of-the-art performance.
*   üíª **Open Source:** Released the implementation code as open-source to facilitate future research.

---

## Results Breakdown

| Domain | Dataset / Task | Metric | Result |
| :--- | :--- | :--- | :--- |
| **Image Classification** | CIFAR-10 | Accuracy | **59.6%** |
| **Image Classification** | ImageNet | Accuracy | **71.2%** |
| **Generative AI** | Text-to-Image | Accuracy | **20.0%** |