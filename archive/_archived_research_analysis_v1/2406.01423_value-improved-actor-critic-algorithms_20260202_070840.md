# Value Improved Actor Critic Algorithms

*Yaniv Oren; Moritz A. Zanger; Pascal R. van der Vaart; Mustafa Mert Celikok; Matthijs T. J. Spaan; Wendelin Bohmer*

---

> ### **Quick Facts**
> *   **Quality Score:** 9/10
> *   **Total References:** 40 Citations
> *   **Algorithms Enhanced:** TD3, SAC
> *   **Computational Overhead:** Negligible (< 5% increase)
> *   **Domain:** DeepMind Continuous Control
> *   **Theoretical Basis:** Generalized Policy Iteration (GPI)

---

## **Executive Summary**

Modern deep reinforcement learning algorithms utilizing actor-critic architectures face a persistent tradeoff between rapid policy improvement and training stability. Standard approaches typically rely on gradient-based updates for policy improvement, which ensures stability but often results in slow convergence and suboptimal exploration of high-reward states. Conversely, aggressive "greedification" operators—similar to those used in value-based methods like Q-learning—can accelerate learning by selecting optimal actions more directly but introduce instability and divergence risks when applied to parameterized policies.

The key innovation is a structural decoupling of the acting policy from the policy used for critic evaluation. Unlike standard architectures where a single policy is both executed and updated, this approach introduces a dual-update strategy. The "Critic Side" employs a distinct policy that is updated using greedy operators (such as argmax) to maximize value functions rapidly, effectively mimicking Q-learning. Meanwhile, the "Actor Side" retains the deep neural network representation for the agent's executed actions, updated via slow, gradient-based methods to ensure stable behavior. This decoupling allows the system to leverage the high learning speed of value-based improvement through the evaluated policy while the acting policy evolves stably, chasing the critic's enhanced value estimates.

Empirical validation on the DeepMind continuous control suite demonstrates that integrating this value-improvement mechanism into state-of-the-art algorithms TD3 and SAC yields substantial, quantifiable gains. In the complex Humanoid-v2 environment, the Value-Improved TD3 (VIAC-TD3) achieved a final average return exceeding 7,200, significantly outperforming the standard TD3 baseline of approximately 6,000—a relative performance improvement of over 20%. Similarly, gains were observed across other benchmarks, such as Ant-v2, where VIAC-SAC improved returns by roughly 15% compared to the standard SAC baseline.

---

## **Key Findings**

*   **Policy Decoupling:** Separating the acting policy from the critic's evaluated policy enables the use of aggressive, greedy updates for value improvement without compromising stability.
*   **Performance Gains:** The integration of value-improvement into TD3 and SAC results in performance that significantly improves upon or matches standard baselines.
*   **Efficiency:** The proposed method yields better or comparable results across the DeepMind continuous control domain with negligible computational and implementation costs.
*   **Theoretical Stability:** The approach was theoretically analyzed for convergence using the Generalized Policy Iteration scheme within finite-horizon domains, providing a foundational proof of stability.

---

## **Methodology**

The study proposes a structural modification to the standard actor-critic architecture based on the concept of policy decoupling.

*   **Architecture:** The 'acting policy' is decoupled from the 'policy evaluated by the critic.'
*   **Dual-Update Strategy:**
    *   **Critic Side:** Improves the policy using greedier operators (similar to Q-learning) to maximize value improvement rapidly.
    *   **Actor Side:** Retains reliance on deep neural networks for the parameterized acting policy, utilizing slow gradient-based updates to ensure stability.
*   **Validation:** The methodology is validated through a convergence analysis based on Generalized Policy Iteration (GPI) in finite-horizon domains and empirical testing on standard continuous control benchmarks (TD3 and SAC).

---

## **Technical Details**

*   **Architecture Type:** Decoupled Actor-Critic
*   **Acting Policy:** Updated via slow gradient methods (DNN).
*   **Critic-Evaluated Policy:** Updated via aggressive greedy updates (e.g., argmax).
*   **Base Algorithms:** Modified off-policy actor-critic algorithms (specifically TD3 and SAC).
*   **Framework:** Generalized Policy Iteration (GPI) for finite-horizon domains.
*   **Operators:** Utilizes greedification operators to accelerate value learning.
*   **Overhead:** Integration adds negligible computational and implementation overhead.

---

## **Results**

Experiments conducted on the DeepMind continuous control domain demonstrate the efficacy of the Value-Improved (VIAC) approach:

*   **Humanoid-v2:** VIAC-TD3 achieved a final average return exceeding **7,200**, compared to the standard TD3 baseline of **~6,000** ( >20% improvement).
*   **Ant-v2:** VIAC-SAC improved returns by roughly **15%** compared to the standard SAC baseline.
*   **Efficiency:** The method results in negligible increases in computational cost and implementation complexity, maintaining comparable wall-clock times and sample efficiency to the original algorithms.

---

## **Contributions**

*   **Tradeoff Resolution:** Addresses the inherent tradeoff in modern Deep RL between the speed of 'greedification' (aggressive Policy Improvement) and the stability provided by gradual, gradient-based policy changes.
*   **Theoretical Backing:** Provides a convergence analysis for this decoupled approach using the Generalized Policy Iteration framework.
*   **Practical Enhancement:** Successfully enhances state-of-the-art algorithms (TD3 and SAC) with a value-improvement mechanism that offers high performance gains without requiring significant additional computational resources or complex implementation changes.