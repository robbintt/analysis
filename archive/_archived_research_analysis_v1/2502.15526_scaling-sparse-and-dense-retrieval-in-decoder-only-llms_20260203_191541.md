---
title: Scaling Sparse and Dense Retrieval in Decoder-Only LLMs
arxiv_id: '2502.15526'
source_url: https://arxiv.org/abs/2502.15526
generated_at: '2026-02-03T19:15:41'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Scaling Sparse and Dense Retrieval in Decoder-Only LLMs
*Hansi Zeng; Julian Killingback; Hamed Zamani*

> ### üìä Quick Facts
> *   **Model Architectures:** Llama-3 (1B, 3B, 8B parameters)
> *   **Training Corpus:** MS MARCO passages
> *   **Key Benchmarks:** MSMARCO, TREC DL 19/20 (In-domain); BEIR (Out-of-domain)
> *   **Top Achievement:** SOTA results on all evaluation sets using 8B Sparse Model (CL + KD)
> *   **Quality Score:** 9/10

---

## üìù Executive Summary

This research addresses the critical gap in understanding how sparse retrieval models and Knowledge Distillation (KD) scale within modern decoder-only Large Language Models (LLMs). While the field has extensively studied the scaling behavior of dense retrieval and Contrastive Loss (CL), the efficacy of sparse methods at larger parameter scales has remained underexplored. This problem is significant because resolving these scaling laws is essential for developing efficient, high-performance Retrieval-Augmented Generation (RAG) systems. If sparse retrieval cannot maintain performance parity with dense models as size increases, the field risks over-relying on computationally expensive dense paradigms.

The authors introduce a systematic comparative study utilizing the Llama-3 family (1B, 3B, and 8B parameters) adapted for retrieval tasks by replacing causal attention masks with bidirectional ones. The core technical innovation involves the implementation of sparse retrieval within a decoder-only architecture: hidden states are projected to the vocabulary space, processed via max-pooling and ReLU activation, and regulated using FLOP-based constraints to induce sparsity. This approach is contrasted against dense retrieval (using mean pooling) across three distinct fine-tuning objectives: Contrastive Loss (CL), Knowledge Distillation (KD) via external cross-encoders, and a hybrid combination, all trained on the MS MARCO corpus under a fixed compute budget.

The study reveals markedly divergent scaling behaviors: Contrastive Loss exhibits strong positive scaling as model size grows from 1B to 8B, whereas Knowledge Distillation shows diminishing returns with increased scale. Sparse retrieval consistently outperformed dense retrieval across both in-domain benchmarks (MSMARCO, TREC DL 19/20) and out-of-domain generalization tests (BEIR). Furthermore, sparse retrieval demonstrated superior robustness to imperfect supervised signals. The combination of CL and KD proved optimal, enabling the 8B sparse retrieval model to achieve State-of-the-Art (SOTA) results across all evaluation sets, measured by MRR@10 and Average nDCG@10.

---

## üîë Key Findings

*   **Divergent Scaling Behaviors:** Contrastive Loss (CL) enables significant performance gains as model size increases, whereas Knowledge Distillation (KD) yields minimal improvements across 1B, 3B, and 8B parameter scales.
*   **Superiority of Sparse Retrieval:** Sparse retrieval models consistently outperform dense retrieval models across both in-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks.
*   **Robustness to Data Quality:** Sparse retrieval demonstrates greater robustness to imperfect supervised signals compared to dense retrieval paradigms.
*   **Achievement of SOTA:** By combining Contrastive Loss and Knowledge Distillation, the authors successfully scaled sparse retrieval to the 8B parameter level, achieving State-of-the-Art (SOTA) results on all evaluation sets.

---

## üõ†Ô∏è Methodology

The researchers conducted a systematic comparative study using decoder-only LLMs from the Llama-3 series (specifically 1B, 3B, and 8B parameter models). The study utilized MSMARCO passages as the training dataset within a fixed compute budget.

The methodology involved evaluating various configurations of:
*   **Retrieval Paradigms:** Sparse vs. Dense
*   **Fine-tuning Objectives:** Contrastive Loss (CL), Knowledge Distillation (KD), and their combination

Performance was measured using both in-domain benchmarks (MSMARCO, TREC DL) and out-of-domain benchmarks (BEIR).

---

## ‚öôÔ∏è Technical Details

**Architecture & Pre-training**
*   **Models:** Llama 3 decoder-only LLMs (1B, 3B, 8B parameters).
*   **Attention Modification:** Replaced the causal attention mask with a bidirectional one to enable retrieval.
*   **Objective:** Masked Next-Token Prediction (MNTP) on the MS MARCO corpus.
*   **Configuration:** 20% token masking; loss calculated using logits from the previous position (max 10,000 steps).

**Retrieval Mechanisms**
*   **Dense Retrieval:** Utilizes mean pooling over hidden states with dot product scoring.
*   **Sparse Retrieval:** Projects hidden states to vocabulary space, applies max-pooling, ReLU, and log rescaling. FLOP regularization is applied to induce sparsity. Uses dot product scoring.

**Fine-Tuning Objectives**
*   **Contrastive Loss (CL):** Standard contrastive learning.
*   **Knowledge Distillation (KD):** Utilizes external cross-encoders (MarginMSE or KL-Divergence).
*   **Combined Objective:** A hybrid approach calculated as `0.5 * (CL + KD)`.

---

## üìà Results

*   **Scaling Dynamics:** Contrastive Loss (CL) exhibits strong positive scaling behavior as model size increases (1B to 8B), whereas Knowledge Distillation (KD) shows diminishing returns with scaling.
*   **Performance Comparison:** Sparse retrieval consistently outperforms dense retrieval both in-domain (MSMARCO, TREC DL 19/20) and out-of-domain (BEIR), demonstrating higher robustness.
*   **SOTA Achievement:** By combining CL and KD, the 8B sparse retrieval model achieved State-of-the-Art (SOTA) results on all evaluated sets.
*   **Evaluation Metrics:**
    *   MS MARCO Dev: MRR@10
    *   TREC DL 19 & 20: nDCG@10
    *   BEIR: Average nDCG@10

---

## üèÜ Contributions

*   **Bridging the Research Gap:** Addressed the lack of research regarding the scaling behavior of sparse retrieval and Knowledge Distillation (KD), shifting focus from the predominantly studied dense retrieval and Contrastive Loss (CL) methods.
*   **Empirical Analysis of Scaling Laws:** Provided empirical evidence that scaling laws (performance improvements with increased model size) manifest clearly under CL training but are largely absent under KD training.
*   **Validation of Sparse Paradigms:** Validated the efficacy of sparse retrieval in a decoder-only LLM context, proving its generalization capability and robustness surpass that of dense retrieval.
*   **Performance Benchmarking:** Established a new performance standard (SOTA) by demonstrating that a hybrid CL and KD approach enables effective scaling of sparse retrieval models to 8B parameters.

---

**Quality Score:** 9/10
**References:** 40 citations