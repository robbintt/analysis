# Adversarial Robustness of Vision in Open Foundation Models

*Jonathon Fox; William J Buchanan; Pavlos Papadopoulos*

---

## ðŸ“‘ Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 28 Citations |
| **Models Analyzed** | LLaVA-1.5-13B, Llama 3.2 Vision-8B-2 |
| **Dataset** | VQA v2 |
| **Attack Method** | Untargeted Projected Gradient Descent (PGD) |
| **Modality Targeted** | Visual Input |
| **Key Insight** | Accuracy does not correlate with robustness |

---

## Executive Summary

**Problem**
This paper addresses the critical security vulnerabilities inherent in open-weight Vision-Language Models (VLMs), specifically focusing on the susceptibility of the visual modality to adversarial exploitation. As these models are increasingly deployed in sensitive applications, the prevailing assumption that superior performance on standard benchmarks correlates with security remains a significant and unverified risk. This research highlights that while textual prompt injections are often scrutinized, the vision component of multimodal models presents a viable and potent attack vector that has been comparatively under-assessed in the landscape of open foundation models.

**Innovation**
The study presents a comparative empirical analysis between two distinct architectures, Meta's Llama 3.2 Vision-8B-2 and LLaVA-1.5-13B, to evaluate how structural differences impact security. Technically, the authors employ untargeted Projected Gradient Descent (PGD) attacks specifically confined to the visual input modality, using a subset of the VQA v2 dataset as the evaluation benchmark. By isolating the image component from the text prompt, the methodology rigorously quantifies the degradation in Visual Question Answering performance, isolating robustness as a function of the model's visual processing capabilities rather than its language understanding.

**Results**
The empirical findings reveal a distinct decoupling of accuracy and robustness across the tested models, illustrated by concrete performance metrics. While LLaVA-1.5-13B demonstrated a higher baseline accuracy on clean data (81.6%), it suffered a catastrophic drop in performance under adversarial attack, falling to 13.7%. In contrast, Llama 3.2 Vision-8B-2, despite a slightly lower baseline (79.6%), exhibited superior adversarial robustness, maintaining a significantly higher accuracy of 41.6% under the same perturbation conditions. Crucially, the results indicate that the disparity in robustness between the two models becomes increasingly pronounced at higher perturbation levels, confirming that Llama 3.2 Vision is better able to maintain stability against escalating adversarial noise compared to the higher-scoring LLaVA model.

**Impact**
The significance of this research lies in its challenge to the industry reliance on standard leaderboards as proxies for model security. By demonstrating that a model's proficiency on clean data is not a predictive indicator of its resilience against adversarial inputsâ€”evidenced by the vast disparity in robustness between the two architecturesâ€”the paper establishes that raw accuracy does not equate to safety. These findings contribute a vital reference point for the field, urging researchers and developers to prioritize architectural robustness and visual security auditing alongside performance optimization in the development of future open foundation models.

---

## Key Findings

*   **Llama 3.2 Vision Superior Robustness**: Despite having a lower baseline accuracy on the VQA v2 dataset compared to LLaVA-1.5-13B, Llama 3.2 Vision exhibited a smaller drop in performance when subjected to adversarial attacks.
*   **Impact of Perturbation Levels**: The disparity in robustness between the two models became particularly pronounced at higher perturbation levels.
*   **Vision Modality as an Attack Vector**: The study confirms that the visual input modality is a viable and effective attack vector for degrading the performance of contemporary open-weight Vision-Language Models (VLMs).
*   **Decoupling of Accuracy and Robustness**: Adversarial robustness does not necessarily correlate directly with standard benchmark performance, indicating that a model's accuracy on clean data does not predict its resilience against attacks.

---

## Methodology

The study focused on evaluating two specific open-weight Vision-Language Models: **LLaVA-1.5-13B** and **Meta's Llama 3.2 Vision-8B-2**.

*   **Attack Strategy**: Untargeted Projected Gradient Descent (PGD) attacks were employed specifically against the visual input modality to induce adversarial perturbations.
*   **Evaluation Dataset**: The models were empirically evaluated on a subset of the **Visual Question Answering (VQA) v2** dataset.
*   **Metrics**: Performance was quantified using the standard VQA accuracy metric, with a specific focus on calculating the accuracy degradation (accuracy drop) relative to the baseline performance.

---

## Technical Details

*   **Target Models**: Llama 3.2 Vision and LLaVA-1.5-13B.
*   **Attack Mechanism**: Projected Gradient Descent (PGD) targeting visual inputs.
*   **Evaluation Benchmark**: VQA v2 dataset.
*   **Architecture**: Transformer-based with multimodal integration.
*   **Perturbation Focus**: Visual inputs (pixel-level noise) rather than textual prompt injections.

---

## Results

LLaVA-1.5-13B demonstrated higher baseline accuracy on clean data, but Llama 3.2 Vision exhibited superior adversarial robustness with significantly smaller performance drops under attack, a trend that became more pronounced at higher perturbation levels. The results confirm a decoupling of accuracy and robustness, where high standard performance does not predict resilience against adversarial inputs.

---

## Contributions

*   **Security Assessment**: Provided an empirical assessment of the security vulnerabilities in modern, large-scale open foundation models, specifically highlighting the risks associated with vision inputs.
*   **Theoretical Insight**: Contributed to the understanding that architectural and training factors influence robustness independently of standard benchmark results, challenging the assumption that higher-performing models are inherently more secure.
*   **Comparative Analysis**: Offered a direct comparative analysis of robustness between distinct model architectures (LLaVA and Llama 3.2 Vision), establishing a reference point for future research into VLM security.

---
**Report generated based on 28 references.**