# Ordering-based Conditions for Global Convergence of Policy Gradient Methods

*Jincheng Mei; Bo Dai; Alekh Agarwal; Mohammad Ghavamzadeh; Csaba Szepesvari; Dale Schuurmans*

---

> ### ðŸ“‹ Quick Facts
> *   **Problem Setting:** "Finite-arm bandits with linear function approximation"
> *   **Policy Class:** "Log-linear (softmax) parameterization"
> *   **Key Innovation:** "Ordering-based conditions (vs. approximation error)"
> *   **NPG Convergence Rate:** "Exponential ($O(e^{-c \cdot t})$)"
> *   **Softmax PG Convergence Rate:** "Polynomial ($O(1/t)$)"
> *   **Quality Score:** 9/10
> *   **Citations:** 40

---

## Executive Summary

This research addresses the critical theoretical gap regarding the global convergence of Policy Gradient (PG) methods when utilizing linear function approximation in the absence of realizability assumptions. In the reinforcement learning literature, convergence guarantees typically rely on the "realizability" conditionâ€”where the true value function or optimal policy exists exactly within the representable space of the function approximatorâ€”or depend heavily on minimizing $L_2$ approximation error. These assumptions are often too restrictive for real-world applications where perfect representation is infeasible. Consequently, understanding whether and how PG algorithms can converge globally despite approximation errors has been an open challenge that limits the theoretical reliability of these methods in complex, non-realizable settings.

The key innovation is the introduction of **"ordering-based" conditions**, reframing the analysis of convergence away from approximation metrics toward the structural interplay between policy updates and feature representation. The authors focus on finite-arm bandits with linear function approximation and log-linear (softmax) policies.

For Natural Policy Gradient (NPG), which updates parameters akin to least-squares regression, the study establishes a necessary and sufficient condition: the projection of the reward onto the representable space must preserve the top rank of the optimal action. Conversely, for standard Softmax PG, the authors prove that convergence is guaranteed if the representation satisfies a "non-domination condition" (meaning no sub-optimal action dominates the optimal action in the feature space) and preserves reward rankings.

This approach demonstrates that exact representation of values is irrelevant; what matters is the preservation of the relative ordering of actions. The study provides rigorous theoretical bounds and empirical validation distinguishing the convergence behaviors of the two algorithms. Notably, Softmax PG demonstrated convergence in iterations up to $10^8$ even in instances where reward order preservation conditions were violated, validating that these conditions are sufficient but not strictly necessary.

---

## Key Findings

*   **Global Convergence Without Realizability:** Both standard Softmax Policy Gradient (PG) and Natural Policy Gradient (NPG) can achieve global convergence under linear function approximation without requiring assumptions of policy or reward realizability.
*   **Irrelevance of Approximation Error:** The study determines that approximation error is not a defining quantity for characterizing the global convergence of these algorithms.
*   **NPG Convergence Condition:** NPG achieves global convergence **if and only if** the projection of the reward onto the representable space preserves the optimal action's rank.
*   **Softmax PG Convergence Condition:** Softmax PG converges globally if the representation satisfies a "non-domination condition" and preserves the ranking of rewardsâ€”a criterion significantly broader than realizability.
*   **Divergent Conditions:** The necessary representation conditions for convergence differ fundamentally between Softmax PG and NPG algorithms.

---

## Methodology

The authors employ a theoretical analysis framework to investigate the global convergence of Policy Gradient methods within the context of **finite-arm bandits** utilizing linear function approximation. Rather than relying on approximation metrics, the study focuses on analyzing the inter-related properties between policy updates and the feature representation. The theoretical derivations are validated through supplementary experimental results.

---

## Technical Details

**Problem Setting**
*   **Environment:** Single-state Reinforcement Learning problem with $K$ actions.
*   **Approximation:** Linear function approximation.
*   **Policy:** Log-linear (softmax) parameterization.
*   **Challenge:** Addresses non-concave optimization landscapes in unrealizable settings.

**Algorithms Analyzed**
1.  **Softmax Policy Gradient (PG):**
    Updates parameters via:
    $$ \theta_{t+1} \leftarrow \theta_t + \eta \cdot X^\top (\text{diag}(\pi_{\theta_t}) - \pi_{\theta_t} \pi_{\theta_t}^\top) r $$
2.  **Natural Policy Gradient (NPG):**
    Updates parameters via least squares regression:
    $$ \theta_{t+1} \leftarrow \theta_t + \eta \cdot (X^\top X)^{-1} X^\top r $$

**Theoretical Conditions**
*   **Reward Order Preservation:** Sufficient for Softmax PG.
*   **Optimal Rank Preservation:** Preservation of the optimal action's top rank in the projected reward is necessary and sufficient for NPG.

**Convergence Rates**
*   **Softmax PG:** Polynomial rate of $O(1/t)$
*   **NPG:** Exponential rate of $O(e^{-c \cdot t})$

---

## Results

Experimental results verified the theoretical conditions using configurations with varying $K$ and $d$:

*   **Example 4 ($K=4, d=2$):** Showed NPG failing to converge when the projected reward violated the strict inequality condition ($\hat{r}(1) = \hat{r}(2)$).
*   **Example 1:** Confirmed convergence rates, validating exponential convergence for NPG and polynomial $O(1/t)$ convergence for Softmax PG.
*   **Example 5 ($K=6, d=2$):** Demonstrated that Softmax PG achieved global convergence even when the reward order preservation condition was violated, proving the condition sufficient but not necessary. Conversely, NPG behaved erratically and settled for a sub-optimal solution in this scenario.
*   **Testing Metrics:** Iteration counts up to $10^8$ for Softmax PG and 150 for NPG.

---

## Contributions

*   **Reframing Convergence Metrics:** The paper challenges the field's standard reliance on approximation error, introducing "ordering-based" conditions as a more accurate framework for characterizing PG convergence.
*   **Definitive NPG Criteria:** Establishing a necessary and sufficient (iff) condition for NPG convergence based on the preservation of the optimal action's rank within the projected reward space.
*   **Broader Theoretical Bounds for Softmax PG:** Identifying specific conditions (non-domination and rank preservation) under which Softmax PG converges, extending theoretical understanding beyond the restrictive assumptions of realizability.
*   **Algorithm Differentiation:** Clarifying that the structural requirements for representations are not universal across all PG methods but must be tailored to the specific algorithm (NPG vs. Softmax PG).

---

**Quality Score:** 9/10  
**References:** 40 citations