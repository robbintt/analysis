---
title: 'Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video
  Reasoning'
arxiv_id: '2601.21037'
source_url: https://arxiv.org/abs/2601.21037
generated_at: '2026-02-03T12:37:58'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning

*Chengzu Li; Zanyi Wang; Jiaang Li; Yi Xu; Han Zhou; Huanyu Zhang; Ruichuan An; Dengyang Jiang; Zhaochong An; Ivan VuliÄ‡; Serge Belongie; Anna Korhonen*

***

> ### ðŸ“Š Quick Facts
> ---
> *   **Base Architecture:** Wan 2.2 TI2V 5B
> *   **Quality Score:** 7/10
> *   **Total Citations:** 40
> *   **Core Mechanism:** Visual Test-Time Scaling
> *   **Key Domains:** Maze Navigation (Discrete), Tangram Puzzles (Continuous)
> *   **Primary Advantage:** Robust Zero-Shot Generalization

***

## Executive Summary

Current Vision-Language Models (VLMs) are fundamentally limited in their capacity for fine-grained spatial understanding and continuous action planning, often failing to simulate the physical dynamics necessary for complex environments. This creates a critical bottleneck for embodied AI and robotics, where agents must achieve robust zero-shot generalizationâ€”adapting to unseen data distributions without task-specific fine-tuningâ€”relying solely on visual cues and high-level instructions to navigate novel physical spaces.

The authors introduce a novel paradigm that formulates visual reasoning as a conditional video generation problem, shifting from textual reasoning chains to generating sequences of visual frames as intermediate reasoning steps. Utilizing a fine-tuned Wan 2.2 TI2V 5B model, the system learns a conditional policy $P(\text{Trajectory} | \text{Icon}, \text{Layout})$. A key technical innovation is the integration of **"Visual Context,"** which employs explicit visual controlsâ€”such as agent icons and tangram shapesâ€”to maintain visual consistency.

Furthermore, the paper establishes a **"Visual Test-Time Scaling"** strategy that manipulates the inference budget by increasing total frame count and adjusting a scaling factor ($\kappa$) for frames per step, allowing the model to dynamically allocate more computation to complex planning tasks. The methodology was evaluated across Maze Navigation (sequential discrete planning) and Tangram Puzzles (continuous manipulation).

In Maze Navigation, the study demonstrated a clear Visual Test-Time Scaling Law; increasing the inference budget significantly enhanced Out-of-Distribution (OOD) generalization. Conversely, the Tangram task maintained robustness but exhibited no scaling trend due to rigid geometric consistency constraints. This research shifts the perception of video generation models from media creation tools to scalable, generalizable engines for visual reasoning.

***

## Key Findings

*   **Robust Zero-Shot Generalization:** The video generation model demonstrates high performance on unseen data distributions in both Maze Navigation and Tangram Puzzle tasks without requiring specific fine-tuning.
*   **Utility of Visual Context:** The model effectively utilizes explicit visual controls, such as agent icons and tangram shapes, to maintain visual consistency and robustly adapt planning capabilities to new patterns.
*   **Visual Test-Time Scaling Law:** A scaling law was observed in sequential planning where increasing the generated video length (the visual inference budget) significantly enhances zero-shot generalization, particularly for spatially and temporally complex paths.
*   **Efficacy Across Visual Regimes:** The approach proves effective in two distinct environments:
    *   **Maze Navigation:** Sequential discrete planning with low visual change.
    *   **Tangram Puzzle:** Continuous manipulation with high visual change.

***

## Methodology

The study posits that generated frames serve as intermediate reasoning steps bridging the gap between an initial state and a final solution. To validate this, the authors focused on assessing the model's ability to handle fine-grained spatial understanding and continuous action planningâ€”areas where traditional VLMs typically struggle.

### Task Selection
*   **Maze Navigation:** Used to assess sequential discrete planning characterized by low visual change.
*   **Tangram Puzzle:** Used to assess continuous manipulation characterized by high visual change.

***

## Technical Details

### System Architecture & Controls
*   **Base Model:** Fine-tuned Wan 2.2 TI2V 5B.
*   **Control Mechanisms:**
    *   **Visual Context:** Visual anchoring via inputs in the first frame (e.g., agent icons, shapes).
    *   **Textual Instructions:** High-level command inputs.
*   **Learned Policy:** $P(\text{Trajectory} | \text{Icon}, \text{Layout})$.

### Strategy & Dynamics
*   **Visual Test-Time Scaling:** A strategy manipulating the inference budget by:
    *   Increasing total frame count.
    *   Adjusting a scaling factor ($\kappa$) for frames per step.
*   **Generative Dynamics:**
    *   **Discrete Planning:** Applied to Maze Navigation.
    *   **Continuous Manipulation:** Applied to Tangram Puzzles.

***

## Contributions

*   **New Paradigm for Visual Reasoning:** Establishes video generation not merely as a media tool, but as a scalable and generalizable paradigm for solving visual reasoning tasks by simulating dynamics and intermediate steps.
*   **Validation of Video Models for Planning:** Provides empirical evidence that video generation models can outperform or supplement the capabilities of VLMs in tasks requiring fine-grained spatial understanding and continuous action planning.
*   **Identification of Scaling Behaviors:** Highlights the existence of a test-time scaling law in visual reasoning, showing that allocating greater inference budget (longer video generation) correlates with improved reasoning performance in complex scenarios.

***

## Results

Experiments covered Maze Navigation and Tangram Puzzles, evaluated using metrics such as EM (Exact Match), PR (Precision), Score, and OOD (Out-of-Distribution) performance.

*   **Visual Anchoring:** Enabled successful zero-shot generalization compared to textual instructions alone.
*   **Scaling Law (Maze):** In Maze Navigation, increased inference budget (more frames) significantly enhanced OOD generalization.
*   **Geometric Constraints (Tangram):** Tangram Puzzles showed no scaling trend due to geometric consistency constraints, though robustness was maintained. The 'Translation' setting was the strongest for Tangrams.
*   **Failure Modes:**
    *   **Mazes:** 'Cross the wall' (violating spatial boundaries) and 'Distortion'.
    *   **Tangrams:** Incorrect predictions.

***

**Report Details**
*   **Quality Score:** 7/10
*   **References:** 40 citations