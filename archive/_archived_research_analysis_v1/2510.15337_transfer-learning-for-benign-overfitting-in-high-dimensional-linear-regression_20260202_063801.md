# Transfer Learning for Benign Overfitting in High-Dimensional Linear Regression

*Yeichan Kim; Ilmun Kim; Seyoung Park*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Citations:** 40
> *   **Key Algorithm:** Transfer MNI (TM)
> *   **Regime:** Overparameterized ($n \le p$)
> *   **Primary Metric:** Excess Risk ($R$)
> *   **Key Condition:** Shift-to-Signal Ratio (SSR) < 1

---

## Executive Summary

This research addresses the critical challenge of effectively utilizing heterogeneous source data to improve prediction performance in high-dimensional linear regression, specifically within the overparameterized regime where the number of parameters exceeds the number of observations ($n \le p$). While the Minimum $\ell_2$-norm interpolator (MNI) is known to achieve "benign overfitting" in target-only settings, incorporating external data sources remains difficult due to potential negative transfer caused by differences in feature distributions (covariate shift) and underlying models (model shift). The paper seeks to bridge the gap between the fields of transfer learning and benign overfitting by establishing a theoretical framework for when and how source data can be leveraged to reduce excess risk without sacrificing the robustness of interpolators.

The authors introduce **Transfer MNI (TM)**, a novel two-step algorithm that applies a "late-fusion" strategy to transfer learning. Technically, the method involves pre-training source-only MNIs and then fine-tuning them by solving a constrained optimization problem: minimizing the Euclidean distance to the source model weights while exactly interpolating the target data ($\hat{\beta}^{(q)}_{TM} := \arg \min_{\beta} \{ \|\beta - \hat{\beta}^{(q)}_M\| : X^{(0)}\beta = y^{(0)} \}$). This process relies on a distinct "retain-plus-transfer" decomposition ($\hat{\beta}^{(q)}_{TM} = \hat{\beta}^{(0)}_M + (I_p - H^{(0)})\hat{\beta}^{(q)}_M$), which retains the component of the target model within its row space while transferring the orthogonal component from the source. Additionally, the approach employs data-driven source selection and ensemble methods to automatically identify informative sources and stabilize predictions across heterogeneous datasets.

The study establishes non-asymptotic bounds on excess risk, decomposing it into bias and variance components to identify precise conditions for successful transfer. The key findings demonstrate that Transfer MNI outperforms the target-only MNI when the Shift-to-Signal Ratio (SSR) is strictly less than 1 ($SSR < 1$); conversely, negative transfer occurs if $SSR \ge 1$. The authors explicitly derive the optimal source size. Furthermore, the analysis identifies a "**free-lunch covariate shift regime**" where the source covariance is a uniform upscaling of the target covariance; in this regime, source data reduces variance by a factor of $\alpha^{-1}$ without introducing bias. Finite-sample experiments confirm that TM maintains robustness and an advantage over target-only baselines despite substantial heterogeneity.

This work significantly influences the theoretical understanding of transfer learning by providing the first non-asymptotic excess risk bounds for overparameterized linear models. It successfully operationalizes these theoretical insights, offering practical algorithms that make benign overfitting viable in scenarios with heterogeneous auxiliary data. By defining the specific geometric and statistical conditionsâ€”such as spectral ratios and effective rankâ€”required for successful transfer, the paper provides a rigorous foundation for future research into utilizing high-dimensional data sources in modern machine learning applications where model and data shifts are prevalent.

---

## Key Findings

*   **Superior Risk Reduction:** The research identifies specific conditions under which the proposed Transfer MNI method achieves lower excess risk compared to standard target-only MNI in high-dimensional linear regression.
*   **"Free-Lunch" Regimes:** It reveals the existence of "free-lunch covariate shift regimes," where utilizing heterogeneous source data provides benefits with limited penalty.
*   **Experimental Robustness:** Finite-sample experiments confirm that the proposed methods maintain robustness and advantage despite model and data heterogeneity.
*   **Risk Characterization:** The study successfully characterizes the non-asymptotic excess risk of the transfer learning approach.

---

## Contributions

*   **Bridging the Gap:** The work bridges the research gap between transfer learning and benign overfitting in high-dimensional linear regression.
*   **Theoretical Foundation:** It establishes a theoretical foundation for transfer learning within overparameterized models by defining non-asymptotic excess risk bounds.
*   **Operationalization:** The paper operationalizes theory by providing practical procedures, such as source detection and ensemble methods, to apply theoretical insights to real-world, heterogeneous data scenarios.

---

## Methodology

The proposed approach is a novel two-step framework designed to perform transfer learning specifically using the Minimum-$\ell_2$-norm interpolator (MNI).

1.  **Data-Driven Source Selection:** The algorithm utilizes a selection procedure to automatically detect and select informative sources from available heterogeneous data.
2.  **Ensemble Aggregation:** An ensemble method is introduced that aggregates multiple informative Transfer MNIs to stabilize predictions and improve performance.

---

## Technical Details

### Method Specification
*   **Name:** Transfer MNI (TM)
*   **Type:** Transfer learning approach for high-dimensional linear regression ($n \le p$) utilizing the Minimum $\ell_2$-norm Interpolator (MNI).

### Problem Formulation
*   Supports a single target task and multiple source tasks.
*   Operates in a well-specified linear regression setting.
*   Handles both **model shift** and **covariate shift**.

### Algorithm Architecture
*   **Pre-training:** Source-only MNIs are trained independently on source data.
*   **Fine-tuning:** Uses a 'late-fusion' strategy that solves a constrained optimization problem:
    $$ \hat{\beta}^{(q)}_{TM} := \arg \min_{\beta \in \mathbb{R}^p} \{ \|\beta - \hat{\beta}^{(q)}_M\| : X^{(0)}\beta = y^{(0)} \} $$

### Mechanism
*   **Retain-Plus-Transfer Decomposition:**
    $$ \hat{\beta}^{(q)}_{TM} = \hat{\beta}^{(0)}_M + (I_p - H^{(0)})\hat{\beta}^{(q)}_M $$
*   This formula retains the target row space component and transfers the orthogonal component from the source.

### Assumptions
*   Covariates are i.i.d. $\nu_x$-sub-Gaussian.
*   Does **not** assume simultaneous diagonalizability of covariances.
*   Relies on bounded spectral ratios and benign overfitting conditions involving effective rank.

---

## Results

### Primary Evaluation Metric
*   **Excess Risk ($R$):** Measured as mean squared loss on out-of-sample target instances.
*   **Decomposition:** $R$ is decomposed into **Bias ($B$)** and **Variance ($V$)**.

### Isotropic Covariate Results
*   **Performance:** Transfer MNI outperforms target-only MNI if Shift-to-Signal Ratio (SSR) < 1 and specific SNR conditions are met.
*   **Negative Transfer:** Occurs if SSR $\ge 1$.
*   **Optimal Source Size:**
    $$ n^*_q = p - 1 - \sqrt{\frac{p(p-1)}{SNR_q(1-SSR_q)}} $$

### Convergence under Benign Covariates
*   **Bias:** Vanishes if source shift is small and target bias vanishes.
*   **Variance:** Inflation is bounded by:
    1.  Source benign term.
    2.  Target projection term.
    3.  The reciprocal of the smallest source eigenvalue.
*   **Lower Bound:** The lower bound on excess risk is the target-only variance.

### "Free-Lunch" Covariate Shift
*   **Condition:** Occurs when source covariance is a uniform upscaling of target covariance ($\Sigma^{(q)} = \alpha \Sigma^{(0)}$).
*   **Effect:** Source data reduces variance without bias penalty.
*   **Result:** Variance is multiplied by $\alpha^{-1}$.