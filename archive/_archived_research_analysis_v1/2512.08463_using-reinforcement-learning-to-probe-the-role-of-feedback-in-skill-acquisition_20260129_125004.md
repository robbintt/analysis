# Using reinforcement learning to probe the role of feedback in skill acquisition

*Antonio Terpin; Raffaello D'Andrea*

---

> ### ðŸ“Š Quick Facts & Metrics
>
> *   **Quality Score:** 8/10
> *   **Total Citations:** 36
> *   **RL Architecture:** DreamerV3 (25 million parameters)
> *   **Training Environment:** Tabletop Circulating Water Channel (CWC)
> *   **Physics Model:** Incompressible Navier-Stokes equations (Newtonian fluid)
> *   **Acquisition Speed:** Effective strategies discovered within minutes
> *   **Key Paradigm:** "Kind" (Minimization) vs. "Wicked" (Maximization) problems

---

## Executive Summary

**Problem**
This research addresses a fundamental gap in understanding the role of sensory feedback in motor control: specifically, whether high-dimensional feedback is required strictly for the *discovery* of effective control policies or for their *continued execution*. Historically, this has been difficult to isolate due to the variability of human subjects and the idealization of simulation environments.

**Innovation**
The authors bypassed simulation entirely, interfacing a DreamerV3 reinforcement learning agent directly with a physical testbedâ€”a spinning cylinder in a chaotic water channel. The key innovation was the manipulation of the observation space, comparing training regimes with high-dimensional flow estimates against those with only scalar drag measurements. Training was conducted 'in-the-wild' directly on the physical system.

**Results**
The study established a clear dissociation between learning and execution. High-dimensional feedback was essential for discovering high-performance strategies but entirely unnecessary for executing them (open-loop replay matched closed-loop performance). Furthermore, learning difficulty was goal-dependent: drag maximization ("wicked") required rich feedback, while drag minimization ("kind") did not, though it was slower without it.

**Impact**
This work provides a novel, low-cost, reproducible physical platform for studying skill acquisition. The primary contribution is the mechanistic insight that the information requirements for *acquiring* a skill are significantly higher than those for *maintaining* it. This suggests future control systems may require rich sensing only during the initial learning phase.

---

## Key Findings

*   **Dissociation of Learning and Execution:** High-dimensional flow feedback is critical for the agent to *discover* high-performance drag-control strategies; however, it is not required to *execute* them. Replaying action sequences without feedback yields near-identical performance.
*   **Goal-Dependent Learning Difficulty:** Feedback is essential for learning drag maximization (a "wicked" problem). Conversely, the agent can learn drag minimization (a "kind" problem) without flow feedback, though the process is slower and more reliable with feedback.
*   **Rapid Skill Acquisition:** The agent discovered effective control strategies using high-dimensional feedback within minutes of real-world interaction.
*   **Role of Information Richness:** Acquiring a high-performance skill often requires significantly richer information than maintaining or performing that skill.

---

## Methodology

The researchers aimed to bypass both simulation inaccuracies and human subject variability by interfacing a generalist reinforcement learning (RL) agent directly with a physical testbed.

*   **System:** A spinning cylinder submerged in a tabletop circulating water channel.
*   **Task:** The agent was tasked with either minimizing or maximizing drag forces generated by the chaotic fluid flow.
*   **Comparison:** The study compared training regimes with and without high-dimensional flow feedback to isolate the role of sensory information.
*   **Validation:** To verify the necessity of feedback for execution, researchers took action sequences learned with feedback and replayed them in an open-loop manner (without feedback) to measure performance retention.

---

## Technical Details

| Component | Specification |
| :--- | :--- |
| **Physical System** | Tabletop circulating water channel (CWC) |
| **Fluid Dynamics** | Governed by incompressible Navier-Stokes equations (Newtonian fluid) |
| **RL Architecture** | DreamerV3 (25 million parameters) with default hyperparameters |
| **Actuator** | Rotating circular cylinder |
| **Action Space** | One-dimensional continuous space (rotation rate) |
| **Observations** | Drag measurements, commanded rotation rate, rotation-rate feedback, and conditionally included dense flow estimates |
| **Training Protocol** | 'In-the-wild' physical training consisting of 60-second action episodes followed by 60-second stabilization periods |

---

## Results

The study yielded two major outcomes regarding performance and control:

1.  **Feedback Necessity:** High-dimensional flow feedback is essential during the learning phase but not for execution. Open-loop replay achieved near-identical performance to closed-loop control.
2.  **Asymmetry in Difficulty:** There is a significant goal-dependent difficulty. Drag maximization requires flow feedback (the agent fails without it), whereas drag minimization can be achieved without feedback, albeit more slowly and less reliably.
3.  **Efficiency:** Despite the complexity of fluid dynamics, effective control strategies were discovered within minutes, highlighting the efficiency of the RL agent when equipped with rich sensory information.

---

## Contributions

*   **Novel Experimental Testbed:** Provided the field with a low-cost, reproducible, and fully controllable physical system that replicates complex, chaotic dynamics without the variability of human subjects.
*   **Mechanistic Insight:** Empirically demonstrated a fundamental distinction between the information requirements for *learning* a skill versus *executing* a skill.
*   **Characterization of Environments:** Introduced the insight that learning difficulty ("kind" vs. "wicked") is determined by the goal (minimization vs. maximization), independent of the system's dynamics or policy complexity.

---

**References:** 36 citations