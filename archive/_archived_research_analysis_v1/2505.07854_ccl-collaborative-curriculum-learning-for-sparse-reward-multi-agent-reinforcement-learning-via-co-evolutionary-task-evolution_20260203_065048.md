---
title: 'CCL: Collaborative Curriculum Learning for Sparse-Reward Multi-Agent Reinforcement
  Learning via Co-evolutionary Task Evolution'
arxiv_id: '2505.07854'
source_url: https://arxiv.org/abs/2505.07854
generated_at: '2026-02-03T06:50:48'
quality_score: 7
citation_count: 0
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# CCL: Collaborative Curriculum Learning for Sparse-Reward Multi-Agent Reinforcement Learning via Co-evolutionary Task Evolution

*Yufei Lin; Chengwei Ye; Huanzhen Zhang; Kangsheng Wang; Linuo Xu; Shuyan Liu; Zeyu Zhang*

> ### ðŸ“Š Quick Facts
> * **Quality Score:** 7/10
> * **Testing Environments:** Multi-Agent Particle Environment (MPE), Hide-and-Seek
> * **Primary Metrics:** Average Success Rate, Fitness
> * **Core Approach:** Co-evolutionary Curriculum Learning (CCL)
> * **Key Innovation:** Variational Evolutionary Algorithm for Subtask Generation

---

## Executive Summary

Multi-Agent Reinforcement Learning (MARL) struggles significantly in sparse reward environments where agents receive infrequent feedback. This challenge is exacerbated in cooperative settings by delayed and shared feedback, which leads to unstable training and suboptimal convergence. The inability to leverage dense intermediate rewards hinders agents from learning complex coordination strategies, making it difficult to deploy MARL effectively in realistic scenarios where explicit reward shaping is often impractical.

To address these limitations, the authors introduce **Collaborative Curriculum Learning (CCL)**, a novel framework that integrates co-evolutionary task generation with curriculum learning. CCL operates through three mechanisms: Task Refinement, which tailors intermediate tasks to individual agents; Subtask Generation, utilizing a variational evolutionary algorithm to create informative subtasks; and Co-evolution, where the environment evolves concurrently with the agents. Key technical components include a non-linear sigmoid fitness function to prioritize tasks of moderate difficulty, a Variational Individual-perspective Crossover to correct strategy imbalances, and a Soft Selection Strategy that retains a historical population fraction (with $\alpha$ set between 0.2 and 0.4) to prevent catastrophic forgetting. Additionally, Elite Prototype Fitness Evaluation using K-Nearest Neighbors (KNN) is employed to reduce computational costs.

The efficacy of CCL was validated across five cooperative tasks within the Multi-Agent Particle Environment (MPE) and Hide-and-Seek benchmarks. Using average success rate and fitness as primary metrics, the experiments demonstrated that CCL consistently outperforms existing state-of-the-art methods in sparse reward settings. The co-evolutionary process provided quantitatively enhanced training stability, allowing the framework to effectively handle the delayed and shared feedback issues that typically degrade performance in Multi-Agent Systems.

This research represents a significant advancement in MARL by successfully integrating curriculum learning with co-evolutionary task generation to mitigate the sparse reward problem. The CCL framework establishes a new paradigm for creating adaptive training environments that evolve alongside agent populations, thereby improving both stability and performance. By demonstrating superior results in complex environments like Hide-and-Seek, this work paves the way for more robust applications of cooperative multi-agent systems in real-world domains where defining dense reward functions is often impossible.

---

## Key Findings

*   **Performance in Sparse Rewards:** The proposed Collaborative Multi-dimensional Course Learning (CCL) framework outperforms existing methods specifically in sparse reward settings.
*   **Validation Across Benchmarks:** Experimental validation conducted across five cooperative tasks within the MPE and Hide-and-Seek environments demonstrated the efficacy of the approach.
*   **Feedback Optimization:** The framework successfully addresses the issue of delayed and shared feedback in Multi-Agent Systems (MAS), which typically leads to suboptimal learning.
*   **Enhanced Stability:** Implementing a co-evolutionary process where agents evolve alongside their environment results in enhanced training stability.

---

## Methodology

The authors propose **Collaborative Multi-dimensional Course Learning (CCL)**, a novel curriculum learning framework designed for Multi-Agent Reinforcement Learning. The methodology operates through three distinct mechanisms:

1.  **Task Refinement:** Refines intermediate tasks specifically tailored for individual agents.
2.  **Subtask Generation:** Utilizes a variational evolutionary algorithm to automatically generate informative subtasks.
3.  **Co-evolution:** Employs a co-evolutionary strategy where agents evolve concurrently with their environment.

---

## Technical Details

The paper proposes Collaborative Multi-dimensional Course Learning (CCL), a co-evolutionary framework for Multi-Agent Systems (MAS) to handle sparse rewards. The architecture is defined by the following key components:

*   **Non-linear Sigmoid Fitness Function:** Based on average success rate, this function is designed to favor tasks of moderate difficulty.
*   **Variational Individual-perspective Crossover:** Utilizes step size and direction to correct strategy imbalances and ensure diversity within the population.
*   **Soft Selection Strategy:** Retains the whole population with a historical fraction ($\alpha$, typically 0.2â€“0.4) to prevent catastrophic forgetting.
*   **Elite Prototype Fitness Evaluation:** Uses K-Nearest Neighbors (KNN) on a subset of prototypes to significantly reduce computational costs.

---

## Contributions

*   **Framework Innovation:** Introduction of a new curriculum learning framework (CCL) that mitigates the challenges of sparse rewards in Multi-Agent Systems.
*   **Algorithmic Advancement:** The application of a variational evolutionary algorithm to generate informative subtasks, addressing the lack of immediate feedback.
*   **Stability Enhancement:** A demonstration that co-evolving agents with their environment improves training stability and optimizes the learning process.
*   **Benchmarking Success:** Empirical evidence showing superior performance over state-of-the-art methods in complex cooperative scenarios (MPE and Hide-and-Seek).

---

## Results

Validation was performed on five cooperative tasks within the **Multi-Agent Particle Environment (MPE)** and **Hide-and-Seek** environments.

*   **Primary Metrics:** Average success rate and fitness.
*   **Performance:** Results demonstrated that CCL outperforms existing methods in sparse reward settings.
*   **Stability:** The framework provides quantitatively enhanced training stability through co-evolution.
*   **Feedback Handling:** CCL effectively handles delayed and shared feedback issues common in MAS.

---

*References: 0 citations*