---
title: 'DeRS: Towards Extremely Efficient Upcycled Mixture-of-Experts Models'
arxiv_id: '2503.01359'
source_url: https://arxiv.org/abs/2503.01359
generated_at: '2026-02-03T19:07:47'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# DeRS: Towards Extremely Efficient Upcycled Mixture-of-Experts Models

*Yongqi Huang; Peng Ye; Chenyu Huang; Jianjian Cao; Lin Zhang; Baopu Li; Gang Yu; Tao Chen*

---

> ### ðŸ“Š Quick Facts
> | Metric | Value |
> | :--- | :--- |
> | **Quality Score** | 8/10 |
> | **References** | 40 Citations |
> | **Core Concept** | Weight Decomposition ($W_{base} + \Delta_i$) |
> | **Cosine Similarity** | > 0.999 (Inter-expert & Initial) |
> | **Parameter Overhead** | 68% (Experts in total model) |

---

## Executive Summary

Mixture-of-Experts (MoE) models, particularly those created by "upcycling" dense pre-trained models, suffer from severe parameter redundancy that limits their efficiency. While upcycling allows for the conversion of dense models into sparse MoE architectures, the resulting experts often exhibit low divergence from their original initialized weights and high similarity among one another. This structural redundancy means that a significant portion of the model's parametersâ€”specifically the expert layers which account for the majority of parameters (68% in the studied models)â€”are largely inactive or overlapping. This inefficiency drives up memory and computational costs during inference and training without delivering proportional gains in model performance.

The paper introduces **DeRS (Decompose, Replace, and Synthesis)**, a novel paradigm designed to eliminate structural redundancy through weight decomposition. Technically, DeRS re-architects MoE layers by decomposing the weight matrix of each expert ($W_i$) into a single, shared base weight ($W_{base}$) plus multiple lightweight, expert-specific delta weights ($\Delta_i$), following the equation $W_i = W_{base} + \Delta_i$. This framework operates via two strategies: **DeRS Compression**, which uses sparsification or quantization for efficient inference, and **DeRS Upcycling**, which employs lightweight sparse or low-rank matrices to convert dense models into MoEs during training. By storing one heavy base representation and relying on minimal deltas for expert specialization, the method drastically reduces the parameter footprint.

Empirical experiments on the MoE-LLaV A-Phi model validate that DeRS achieves extreme parameter efficiency without sacrificing accuracy. The researchers observed remarkably high cosine similarity between initial weights and trained experts (> 0.999), confirming that experts learn very little new information during standard upcycling. Specifically, similarity scores ranged from 0.9990â€“0.9994 in the first MoE layer and 0.9996â€“0.9998 in the last layer, with inter-expert similarity also exceeding 0.999. Despite removing this redundancy, DeRS maintained model performance across three different tasks while significantly reducing the parameter overhead associated with the 3.4 billion expert parameters within the 5 billion parameter model.

The DeRS framework represents a significant advancement in the practical deployment of large language models by offering a unified solution for both training upcycling and inference compression. It provides critical mechanistic insight into why upcycled MoE models are inefficient, proving that representing experts as a combination of shared base weights and lightweight deltas is a viable structural optimization. This work challenges existing approaches to model scaling and demonstrates that "extremely efficient" MoE models are attainable, enabling researchers and practitioners to deploy high-capacity models with substantially reduced hardware requirements.

---

## Key Findings

*   **Redundancy Identification:** Upcycled MoE models exhibit unique redundancy mechanisms among experts, leading to significant parameter inefficiency.
*   **Performance Preservation:** The DeRS paradigm maintains model performance across three different tasks despite achieving extreme efficiency.
*   **Dual-Stage Efficacy:** The approach is effective for two distinct phases:
    *   Compressing existing models for inference.
    *   Efficiently upcycling dense models during training.
*   **Structural Efficiency:** Representing experts as a combination of one shared base weight and multiple lightweight delta weights is a viable strategy for reducing model size.

---

## Methodology

The paper proposes the **DeRS (Decompose, Replace, and Synthesis)** paradigm. This methodology centers on restructuring MoE layers to mitigate redundancy.

*   **Core Decomposition:** Experts within MoE layers are decomposed into a single expert-shared base weight and multiple expert-specific delta weights represented in lightweight forms.
*   **Application Strategies:**
    *   **DeRS Compression (Inference):** Utilizes sparsification or quantization techniques.
    *   **DeRS Upcycling (Training):** Employs lightweight sparse or low-rank matrices to convert pre-trained dense models into MoE models.

---

## Technical Details

The DeRS paradigm addresses structural redundancy in upcycled MoE models through a specific mathematical formulation.

*   **Weight Decomposition Formula:**
    Expert weights ($W_i$) are decomposed into a shared base weight ($W_{base}$) and lightweight delta weights ($\Delta_i$):
    $$W_i = W_{base} + \Delta_i$$
*   **Architecture Storage:** The system stores one shared base and multiple compressed delta weights to improve storage and computational efficiency.
*   **Initialization Strategy:**
    *   **Experts:** Initialized from pre-trained dense FFN weights.
    *   **Router:** Randomly initialized.

---

## Experimental Results

Validation was performed on the **MoE-LLaV A-Phi model**, revealing significant insights into expert behavior and parameter distribution.

*   **Cosine Similarity Analysis:**
    *   **Initial vs. Trained:** 'Extremely high' cosine similarity (> 0.999), indicating low divergence and minimal learning during standard upcycling.
    *   **Layer 1 Range:** 0.9990 â€“ 0.9994
    *   **Last Layer Range:** 0.9996 â€“ 0.9998
    *   **Inter-expert Similarity:** > 0.999
*   **Parameter Overhead:**
    *   Total Parameters: 5 Billion
    *   MoE Expert Parameters: 3.4 Billion
    *   **Expert Overhead:** Experts account for **68%** of the total model parameters.

---

## Contributions

*   **Framework Introduction:** Introduced the DeRS framework to address parameter inefficiency in upcycled MoE models through weight decomposition.
*   **Unified Solution:** Provided a solution that enhances efficiency for both training (upcycling) and deployment (inference compression).
*   **Mechanistic Insight:** offered new insights into the specific redundancy mechanisms found in upcycled experts.
*   **Empirical Validation:** Demonstrated significant parameter reduction without a trade-off in task performance.

---

*Quality Score: 8/10 | References: 40*