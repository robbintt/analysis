# Imitate Optimal Policy: Prevail and Induce Action Collapse in Policy Gradient

*Zhongzhu Zhou; Yibo Yang; Ziyan Chen; Fengxiang Bie; Haojun Xia; Xiaoxia Wu; Robert Wu; Ben Athiwaratkun; Bernard Ghanem; Shuaiwen Leon Song*

---

> ### **Quick Facts**
> *   **Quality Score:** 8/10
> *   **References:** 31 Citations
> *   **Core Innovation:** Action Collapse Policy Gradient (ACPG)
> *   **Key Concept:** Simplex Equiangular Tight Frame (ETF)
> *   **Tested Environments:** OpenAI Gym (Cliff Walking, Car-Racing, Pong, Breakout)

---

## Executive Summary

> Current research in policy gradient (PG) reinforcement learning focuses primarily on convergence rates and reward optimization, often neglecting the analysis of internal representational structures within the deep neural networks (DNNs) executing these policies. Specifically, the field lacks a comprehensive understanding of the geometric properties of the final action-selection layer in optimal policy networks. This gap is significant because deciphering these structural dynamics could reveal why certain network configurations succeed while others fail, offering a pathway to design more inherently stable and efficient learning algorithms rather than relying solely on external reward signals.
>
> The authors introduce a novel phenomenon termed "**Action Collapse**" (AC), defined by a geometric convergence where state-action activations cluster around class means and action selection layer weights settle into a Simplex Equiangular Tight Frame (ETF). A Simplex ETF is a theoretically optimal structure where vectors possess unit L2 norms and maintain a constant pairwise angle, ensuring maximal separation between action classes. Based on this insight, the authors propose **Action Collapse Policy Gradient (ACPG)**, a structural intervention that modifies standard policy networks by locking the action-selection layer to a fixed, randomly oriented Simplex ETF. Instead of learning these weights, the backbone network is forced to align its feature representations with this synthetic geometry, effectively inducing Action Collapse through architectural constraints.
>
> Experimental validation using the REINFORCE algorithm in OpenAI Gym environments confirmed that standard PG methods naturally converge toward the ETF structure in Ideal Cliff Walking and Discrete-Car-Racing, evidenced by metrics such as Equinorm, Equiangularity, and Variance all approaching zero. However, natural Action Collapse failed to materialize in more complex environments like Breakout-V5 and Pong-V5 due to continuous state sampling issues and biased policies. In contrast, the proposed ACPG method successfully induced this structure, yielding faster and more robust reward improvements compared to baselines. Crucially, the experiments demonstrated a strong correlation between geometric alignment (ETF convergence) and overall training stability.
>
> This research shifts the analytical paradigm of policy gradient methods from external performance metrics to internal geometric mechanics, providing the first formal characterization of Action Collapse and its theoretical underpinnings. By proving that a fixed ETF structure corresponds to a global optimum within a Layer-Peeled Model (LPM), the paper bridges the gap between theoretical representation learning and practical RL application. The introduction of ACPG offers a computationally feasible, generalizable enhancement for existing discrete policy gradient methods, suggesting that future RL architectures can achieve greater efficiency by enforcing optimal geometric priors rather than learning them from scratch.

---

## Key Findings

*   **Discovery of Action Collapse (AC):** Identified a phenomenon in optimal policy Deep Neural Networks (DNNs) where internal representational structures align under specific constraints.
*   **Three-Condition Definition:** AC is formally defined by:
    1.  Convergence of activations to class means.
    2.  Reduction of variability to zero.
    3.  Convergence of weights to a Simplex Equiangular Tight Frame (ETF).
*   **Optimal Separation:** The ETF structure is proven to maximally separate action angles, providing an ideal geometric configuration for decision making.
*   **Analytical Proof:** The study provides mathematical proof showing that a fixed ETF structure induces Action Collapse.
*   **Performance Gains:** Implementing this synthetic structure leads to faster and more robust reward improvements compared to unconstrained learning.

---

## Methodology

The authors propose the **Action Collapse Policy Gradient (ACPG)** method, a structural intervention designed to standardize the internal geometry of policy networks.

*   **Structural Intervention:** ACPG modifies standard policy networks by affixing a synthetic ETF as a **fixed configuration** in the action selection layer.
*   **Weight Locking:** Instead of learning weights through standard gradient descent in the final layer, the network uses a pre-defined Simplex ETF.
*   **Representation Alignment:** This architecture forces the network to align its internal feature representations with the ideal geometric configuration, thereby inducing Action Collapse.
*   **Evaluation:** The method was rigorously evaluated using various discrete policy gradient methods across OpenAI Gym environments.

---

## Technical Details

### **Core Concept: Action Collapse (AC)**
A phenomenon where optimal policy DNNs trained via Policy Gradient converge such that:
*   State-action activations collapse to class means.
*   Action selection weights converge to a **Simplex Equiangular Tight Frame (ETF)**.

### **Simplex ETF Definition**
A geometric structure characterized by:
*   **Unit L2 Norms:** All vectors maintain a length of 1.
*   **Constant Pairwise Angles:** The angle between any two vectors is identical.
*   **Cosine Similarity:** precisely $-1/(K-1)$, where $K$ is the number of actions.

### **ACPG Architecture**
*   **Locked Layer:** The Action Selection Layer is fixed and initialized as a randomly oriented Simplex ETF.
*   **Optimization Scope:** Only the backbone layers of the network are optimized during training.
*   **Theoretical Support:** A Layer-Peeled Model (LPM) suggests that a global optimum exists under this fixed ETF structure.

---

## Results

**Experimental Setup:**
*   **Environments:** Ideal Cliff Walking, OpenAI Gym (Discrete-Car-Racing, Breakout-V5, Pong-V5).
*   **Algorithm:** REINFORCE.
*   **Metrics:** Equinorm, Equiangularity, Maximal-angle Equiangularity, and Variance (all expected to approach 0).

**Outcomes:**
*   **Natural Convergence:** Action Collapse was successfully observed in *Ideal Cliff Walking* and *Discrete-Car-Racing*, with geometry converging to the Simplex ETF.
*   **Convergence Failure:** Natural AC failed to manifest in *Breakout* and *Pong* due to continuous state sampling and biased policies.
*   **ACPG Performance:** The proposed ACPG method induced Action Collapse where natural methods failed, resulting in:
    *   Faster reward improvements.
    *   More robust learning stability.
*   **Correlation:** Results demonstrated a strong correlation between geometric alignment (ETF convergence) and overall training stability.

---

## Contributions

1.  **Internal Analysis:** Provides a deep analysis of internal representational structures in policy gradient methods, addressing a research gap beyond standard convergence analysis.
2.  **Formal Characterization:** Formally defines and characterizes Action Collapse (AC), detailing its geometric properties and requirements.
3.  **Algorithm Introduction:** Introduces ACPG, a computationally feasible optimization technique leveraging ETF theory to improve existing methods.
4.  **Generalizability:** Demonstrates that the technique can enhance any existing discrete policy gradient method, offering broad applicability to the field.