# Everything, Everywhere, All at Once: Is Mechanistic Interpretability Identifiable?

*Maxime Méloux; Silviu Maniu; François Portet; Maxime Peyrard*

***

### Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Base Model** | Multi-Layer Perceptron (MLP) |
| **Task** | Noisy Logical XOR ($C = \text{round}(A) \oplus \text{round}(B)$) |
| **Core Concept** | Statistical Identifiability in MI |

---

## Executive Summary

**Problem**
This research addresses a fundamental epistemological crisis in Mechanistic Interpretability (MI): the inability to guarantee unique, ground-truth explanations for a neural network's behavior. The field relies on the implicit assumption that reverse-engineering a model yields a specific implementation of an algorithm. However, by formally importing the statistical concept of identifiability, the authors challenge this assumption, demonstrating that MI explanations are intrinsically non-identifiable. This matters profoundly because if multiple, potentially contradictory explanations can perfectly account for the same behavior, claims regarding the safety, transparency, and "ground truth" of a model's internal operations become statistically ambiguous, undermining the reliability of interpretability methods for critical applications.

**Innovation**
The key innovation is the formalization of mechanistic explanations as Computational Abstractions $A = (S, \tau)$, where $S$ is a specific subgraph (Circuit) and $\tau$ is a surjective mapping (algorithmic variables). The authors rigorously evaluate two primary MI strategies: 'Where-then-what' (circuit discovery followed by interpretation) and 'What-then-where' (algorithm alignment via subspace search). Unlike typical empirical studies on large-scale models, this work utilizes Boolean functions and small Multi-Layer Perceptrons (MLPs) trained on a noisy logical XOR task. This constrained environment allows for full enumeration of candidate explanations, enabling the authors to mathematically determine whether parameters ($S$ and $\tau$) can be uniquely inferred from observation—a rigorous standard previously lacking in the field.

**Results**
The experiments provide concrete evidence of systematic non-uniqueness. In 'Where-then-what' circuit discovery, the authors identified 85 unique circuits with perfect accuracy, yet these condensed into only 25 unique computational abstractions, illustrating a many-to-one mapping between implementation and explanation. More dramatically, in 'What-then-where' algorithm alignment, the study revealed that a single network could be causally aligned to multiple, contradictory logic gates (NAND, AND, NOR, OR) simultaneously. This one-to-many mapping demonstrates that a single circuit can support multiple interpretations. Consequently, the study establishes that high Circuit Accuracy does not imply a unique explanation, as Intervention Interchange Accuracy (IIA) remains consistent across algorithmically distinct explanations.

**Impact**
This paper significantly shifts the trajectory of Mechanistic Interpretability by establishing identifiability as a rigorous statistical standard for evaluating explanation methods. It forces the field to abandon the pursuit of a singular "true" explanation in favor of a pragmatic framework focused on predictive validity and manipulability. By providing a taxonomy of strategies and demonstrating that strict uniqueness is theoretically unattainable, the authors set a new baseline for future research. This work suggests that the utility of an interpretation should be measured by its ability to allow for successful intervention and prediction, rather than its philosophical "correctness," thereby redefining the goals of interpretable machine learning.

---

## Key Findings

*   **Systematic Non-Identifiability:** MI explanations are systematically non-identifiable; unique explanations for a given behavior cannot be guaranteed.
*   **Complex Mapping:** There is evidence of complex mappings where multiple circuits can replicate a single behavior (many-to-one), and a single circuit can yield multiple interpretations (one-to-many).
*   **Algorithmic Alignment Issues:** Identifiability issues persist in algorithmic alignment, where multiple algorithms (e.g., NAND, AND, NOR, OR) can align to a single network, and one algorithm can align to different neural subspaces.
*   **Pragmatic Utility vs. Uniqueness:** While strict uniqueness may be necessary for high-level understanding, a pragmatic approach focusing on predictive validity and manipulability may be sufficient for practical applications.

---

## Methodology

The research applies the statistical concept of identifiability to Mechanistic Interpretability to determine if model parameters can be uniquely inferred. The authors evaluate two distinct MI strategies:

1.  **Where-then-what:** Isolating a specific circuit *before* interpreting it.
2.  **What-then-where:** Starting with candidate algorithms and searching for neural activation subspaces.

To rigorously test these strategies, the study utilizes **Boolean functions** and small **multi-layer perceptrons (MLPs)**. This small scale allows for the **full enumeration** of all candidate explanations, providing a complete picture of potential mappings between implementation and behavior.

---

## Technical Details

*   **Formalism:** Mechanistic explanations are formalized as Computational Abstractions $A = (S, \tau)$.
    *   **$S$ (Circuit):** A subgraph containing input-output paths.
    *   **$\tau$ (Mapping):** Surjective maps associating activations with algorithmic variables.
*   **Strategies:**
    *   **Where-then-what:** Involves isolating a circuit using heuristics like causal mediation.
    *   **What-then-where:** Involves searching for neural subspaces aligning with a hypothesized algorithm.
*   **Evaluation Metrics:**
    *   **Circuit Error:** Calculated based on accuracy or KL divergence.
    *   **Intervention Interchange Accuracy (IIA):** Measures the consistency of interventions.
*   **Base Model:** An MLP trained on a noisy logical XOR task defined as $C = \text{round}(A) \oplus \text{round}(B)$.

---

## Contributions

*   **Introduction of Identifiability:** Formally introduces the statistical question of identifiability to the field of MI, establishing a rigorous standard for evaluating if unique model explanations are theoretically possible.
*   **Taxonomy of Strategies:** Provides a categorization of reverse-engineering approaches into 'where-then-what' and 'what-then-where.'
*   **Standardization of Explanations:** Contributes to the debate on the necessity of uniqueness versus pragmatic utility, grounding the discussion in predictive and manipulability standards.

---

## Results

*   **Circuit Discovery Non-Uniqueness:** In 'Where-then-what' experiments, the study found **85 unique circuits** with perfect accuracy. However, these yielded only **25 unique computational abstractions**, highlighting redundancy.
*   **Contradictory Algorithm Alignment:** In 'What-then-where' experiments, multiple contradictory algorithms (specifically **NAND, AND, NOR, OR**, and mixed logic) were causally aligned within the same network.
*   **Mapping Characterization:** The findings confirm systematic non-uniqueness characterized by:
    *   **Many-to-one mappings:** Multiple distinct circuits resulting in the same behavior.
    *   **One-to-many mappings:** A single circuit supporting multiple, distinct interpretations.

---

## Assessment

**Quality Score:** 9/10
**References:** 40 citations