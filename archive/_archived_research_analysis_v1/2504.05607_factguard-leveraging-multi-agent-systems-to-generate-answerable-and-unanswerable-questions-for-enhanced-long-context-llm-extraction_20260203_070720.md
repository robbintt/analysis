---
title: 'FactGuard: Leveraging Multi-Agent Systems to Generate Answerable and Unanswerable
  Questions for Enhanced Long-Context LLM Extraction'
arxiv_id: '2504.05607'
source_url: https://arxiv.org/abs/2504.05607
generated_at: '2026-02-03T07:07:20'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# FactGuard: Leveraging Multi-Agent Systems to Generate Answerable and Unanswerable Questions for Enhanced Long-Context LLM Extraction

*Qian-Wen Zhang; Fang Li; Jie Wang; Lingfeng Qiao; Yifei Yu; Di Yin; Xing Sun*

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Dataset Size** | 25,220 examples |
| **Context Range** | 8K to 128K tokens |
| **Max Accuracy** | 61.79% (SOTA LLMs) |
| **Topic Types** | 6 (Time, Numerical, Location, Character, Company, Event) |
| **Quality Score** | 8/10 |
| **References** | 40 citations |

---

## üìë Executive Summary

Large Language Models (LLMs) face significant limitations in performing reliable information extraction over extended contexts, a critical capability for real-world applications involving large documents. Current models struggle to maintain accuracy as context lengths increase from 8K to 128K tokens and exhibit a specific vulnerability in recognizing when an answer is not present in the text. Instead of correctly identifying unanswerable queries, models frequently hallucinate responses. This paper addresses the urgent need for robust evaluation methods that specifically test a model's ability to reason about the absence of information, a capability often overlooked in existing long-context benchmarks.

The authors introduce **FactGuard**, a multi-agent collaborative framework designed to autonomously generate high-quality datasets for long-context extraction without human annotation. Technically, the system utilizes a three-stage pipeline: a **Preparation Stage** (Retrieval, Quality Scoring, Topic Labeling), a **QA Generation Stage** (MRC, Rewrite, QA agents), and a **Negative Example Generation Stage** (Quality Agents for unanswerable questions). This dual-generation strategy allows for the scalable creation of complex, diverse datasets.

Comprehensive evaluation of seven state-of-the-art LLMs on the newly released **FactGuard-Bench** reveals significant performance deficiencies. The models achieved a maximum accuracy of only 61.79%, with performance degrading further as context length increased. The results highlight a specific failure in "Unanswerable Question Detection," where models frequently hallucinate answers. FactGuard-Bench covers a wider range of context lengths (up to 128K tokens) compared to existing benchmarks like the LongBench series, LooGLE, and L-Eval, providing a more rigorous stress test for current architectures.

---

## üîë Key Findings

*   **Performance Limitations:** State-of-the-art LLMs struggle significantly with long-context extraction, achieving a maximum accuracy of only **61.79%**.
*   **Criticality of Unanswerable Reasoning:** Models frequently fail to recognize when an answer is absent, leading to hallucinations rather than correct rejections.
*   **Context Length Impact:** Task difficulty increases substantially as context length expands from **8K to 128K tokens**.
*   **Cost Efficiency:** The framework drastically reduces costs by eliminating the need for human annotation through autonomous data augmentation.
*   **Benchmark Superiority:** FactGuard-Bench covers a wider range of context lengths than existing benchmarks like LongBench Series, LooGLE, and L-Eval.

---

## ‚öôÔ∏è Technical Architecture

FactGuard utilizes a multi-agent collaboration framework centered around an Agent Console. The architecture is divided into three sequential stages:

### 1. Preparation Stage
Raw text is processed and fragmented using specialized agents:
*   **Retrieval Agents:** Utilize RAG (Top N Recall).
*   **Quality Scoring Agents:** Assign scores from 1-5 to text fragments.
*   **Topic Labeling Agents:** Categorize fragments into: Time, Numerical, Location, Character, Company, and Event.
*   **Output:** Tagged fragments ready for processing.

### 2. QA Generation Stage
Focuses on generating evidence-based positive examples:
*   **MRC Agents:** Machine Reading Comprehension processing.
*   **Rewrite Agents:** Refine questions and context.
*   **QA Agents:** Construct (Question, Answer, Evidence) triplets.

### 3. Negative Example Generation Stage
Focuses on generating unanswerable questions to test robustness:
*   **Quality Agents:** Produce unanswerable questions based on:
    *   Lack of evidence.
    *   Misleading evidence.

---

## üöÄ Contributions

*   **FactGuard-Bench Dataset:** Release of a comprehensive dataset containing **25,220 examples** specifically designed to test long-context understanding across varying lengths.
*   **Innovative Augmentation Pipeline:** Introduction of a novel, automated data generation methodology using multi-agent systems that removes human dependency.
*   **Benchmarking and Evaluation:** Comprehensive evaluation of seven LLMs, providing critical insights and establishing new baselines for unanswerable reasoning.

---

## üìà Results & Evaluation

The evaluation of seven state-of-the-art LLMs yielded critical insights into the current state of long-context reasoning:

*   **Low Accuracy:** The highest accuracy achieved was merely **61.79%**, indicating that long-context extraction remains an unsolved challenge.
*   **Failure Modes:** Models consistently failed at **Unanswerable Question Detection**, preferring to hallucinate information rather than admit the answer is absent.
*   **Scalability Issues:** Performance metrics show a clear decline in capability as the context window grows from 8K toward 128K tokens.
*   **Dataset Coverage:** The FactGuard-Bench was validated to cover a broader spectrum of context lengths and topic diversity compared to previous standards (LongBench, LooGLE, L-Eval).