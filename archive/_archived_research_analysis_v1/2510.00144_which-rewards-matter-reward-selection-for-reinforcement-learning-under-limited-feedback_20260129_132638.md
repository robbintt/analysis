# Which Rewards Matter? Reward Selection for Reinforcement Learning under Limited Feedback

*Shreyas Chaudhari; Renhao Zhang; Philip S. Thomas; Bruno Castro da Silva*

---

> ### ðŸ“Š Quick Facts
>
> | Metric | Detail |
> | :--- | :--- |
> | **Quality Score** | 6/10 |
> | **References** | 40 Citations |
> | **Learning Setting** | Offline / Finite Horizon MDP |
> | **Core Mechanism** | Reward Selection (RLLF) |
> | **Primary Algorithm** | UDS (Q-learning with zero imputation) |

---

## Executive Summary

### **Problem**
This paper addresses the challenge of **"Reward Selection for Reinforcement Learning from Limited Feedback" (RLLF)**. This arises in offline settings where agents must learn from datasets containing state transitions but lacking reward information. The core constraint is a limited labeling budget, requiring the agent to select a critical subset of states for human annotation to maximize policy performance. This problem is significant because exhaustive labeling is often infeasible due to the high cost of human feedback, creating a bottleneck for scaling Reinforcement Learning (RL) to complex, real-world environments.

### **Innovation**
The key innovation is the formalization of the **RLLF framework** and the introduction of selection strategies that identify "critical reward subsets" sufficient to guide agents toward optimal trajectories. Technically, the authors employ the **UDS algorithm**â€”a Q-learning-based method where unlabeled states receive a zero-reward imputationâ€”to learn policies from partially labeled data. They investigate two classes of selection strategies:
*   **Heuristics**: Utilize reward-free information (e.g., state visitation frequencies, partial value functions, and an iterative Guided Sampling strategy).
*   **Pre-trained Strategies**: Leverage auxiliary evaluative feedback from a pre-training phase.

### **Results**
Experimental results, assessed using metrics including the **Objective Function $P(\cdot)$**, **Expected Return $J(\pi)$**, and the **Optimality Gap**, demonstrate that the proposed selection strategies yield near-optimal policies while using significantly fewer labels than full supervision. The study found that strategies utilizing reward-free information, particularly Guided Sampling, consistently outperformed the Uniform Sampling baseline by minimizing the Optimality Gap. These results confirm that identifying and labeling critical reward subsets allows for the recovery of high-performance policies that approach the returns achievable with fully labeled datasets.

### **Impact**
The significance of this work lies in establishing reward selection as a valid and powerful paradigm for enabling scalable RL in feedback-constrained environments. By proving that exhaustive labeling is not necessary for robust policy recovery, the authors shift the research focus from traditional learning algorithm design to intelligent data acquisition. This paradigm offers a practical solution to the bottleneck of human feedback availability, suggesting that effective RL systems can be built by strategically acquiring only the most impactful reward signals.

---

## Key Findings

*   **Critical Subsets**: Identifying critical subsets of rewards is sufficient to guide agents along optimal trajectories and support recovery toward near-optimal behavior.
*   **Label Efficiency**: Effective selection methods yield near-optimal policies using **significantly fewer labels** than full supervision requires.
*   **Viable Strategies**: Strategies using reward-free information (e.g., state visitation, partial value functions) and pre-trained auxiliary feedback are viable approaches.
*   **Scalability**: Reward selection offers a powerful paradigm for scaling RL in feedback-constrained environments.

---

## Methodology

The authors formalized the problem as **'Reward Selection for Reinforcement Learning from Limited Feedback (RLLF)'**. The study investigated two distinct classes of strategies:

1.  **Heuristics**: These methods utilize reward-free information derived from the environment dynamics without requiring explicit reward labels. Key examples include:
    *   State visitation frequencies.
    *   Partial value functions.
2.  **Pre-trained Strategies**: These approaches leverage auxiliary evaluative feedback gathered during a pre-training phase to inform the selection process.

---

## Technical Details

*   **Framework**: Reinforcement Learning from Limited Feedback (RLLF) for offline settings.
*   **Environment**: Finite Horizon MDP where the offline dataset lacks reward labels.
*   **Objective**: Label a budget $B$ of states to maximize policy performance.
*   **Learning Algorithm**: **UDS Algorithm** (Q-learning-based).
    *   Unlabeled states are handled via zero-reward imputation.
*   **Selection Strategies**:
    *   **Visitation Sampling**: Frequency-based selection.
    *   **Uniform Sampling**: Random baseline.
    *   **Guided Sampling**: Iterative strategy balancing exploration and exploitation via decaying weights.
*   **Operational Modes**: Selections can be performed in **Batch** or **Iteratively**.
*   **Evaluator**: Optional component $\Xi$ that provides aggregate expected return feedback.

---

## Results & Evaluation

**Metrics Defined:**
*   **Objective Function $P(\cdot)$**: Used to maximize average expected return.
*   **Expected Return $J(\pi)$**: The primary performance metric.
*   **Optimality Gap**: Measures efficacy against the theoretical best.

**Qualitative Findings:**
*   **Label Efficiency**: The approach enables near-optimal policies with fewer labels.
*   **Reward-Free Viability**: Methods relying on state visitation distributions proved effective.
*   **Trajectory Guidance**: Critical reward subsets successfully guided agents along optimal trajectories.

---

## Core Contributions

*   **Framework Introduction**: Introduced the **RLLF framework** to study impactful reward selection under resource constraints.
*   **Subset Characterization**: Characterized critical reward subsets by their specific role in trajectory guidance and deviation recovery.
*   **Paradigm Validation**: Established reward selection as a valid paradigm for enabling scalable RL in feedback-limited settings without the need for exhaustive labeling.