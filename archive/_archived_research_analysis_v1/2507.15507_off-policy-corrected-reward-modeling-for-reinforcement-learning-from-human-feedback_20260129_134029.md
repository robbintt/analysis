# Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback

*Johannes Ackermann; Takashi Ishida; Masashi Sugiyama*

---

> ### ðŸ“‘ Quick Facts
> **Quality Score:** 8/10  
> **Citations:** 40  
> **Core Problem:** Reward Hacking / Overoptimization in RLHF  
> **Proposed Solution:** Off-Policy Corrected Reward Modeling (OCRM)  
> **Key Mechanism:** Iterative Importance Weighting  
> **Primary Benefit:** Label-free correction of distribution shift

---

## Executive Summary

This paper addresses the critical instability in Reinforcement Learning from Human Feedback (RLHF) known as **reward hacking** or **overoptimization**. The authors identify the root cause as a distribution shift: the Reward Model (RM) is trained on outputs from the Supervised Fine-Tuning (SFT) policy, but during reinforcement learning, the agent's policy drifts into regions where the RM has no reliable training data.

This drift results in statistically inconsistent parameter estimates, causing the agent to exploit the proxy reward (High RM Score) at the cost of actual alignment (Low True Reward)â€”a manifestation of **Goodhartâ€™s Law**.

The key innovation is **Off-Policy Corrected Reward Modeling (OCRM)**, a technique designed to stabilize training by mathematically correcting the reward signal rather than updating the model weights. By employing importance weighting, OCRM adjusts the reward estimation to account for the discrepancy between the SFT distribution ($\pi_{\text{SFT}}$) and the current RL policy distribution ($\pi_{\text{RL}}$). This process uses a frozen Reward Model to generate corrected signals during the optimization phase, effectively compensating for the shift without requiring new human annotations or recalibrating the model parameters.

Empirical evaluations on summarization and chatbot tasks demonstrate that standard PPO-RLHF suffers from severe misalignment as training progresses. While standard methods showed RM Rewards increasing over extended training durations (e.g., up to 600,000 steps), this was accompanied by a significant decline in RM Accuracy and stagnation of the True Reward. In contrast, OCRM successfully mitigated this divergence, preserving higher RM Accuracy and True Reward throughout the training trajectory.

---

## Key Findings

*   **Root Cause Identified:** The primary driver of overoptimization in RLHF is the **distribution shift** between generated responses (from the current RL policy) and the data used to train the Reward Model.
*   **Inconsistent Estimates:** This distribution shift leads to inconsistent estimates of both the Reward Model parameters and the policy gradient.
*   **Goodhartâ€™s Law in Action:** Empirical results illustrate a classic Goodharting effect: as overoptimization occurs, proxy RM scores increase while actual alignment with human preferences decreases.
*   **Superior Performance:** Correcting for distribution shift via importance weighting significantly outperforms standard RLHF methods.

---

## Methodology

The authors propose **Off-Policy Corrected Reward Modeling (OCRM)** to address the limitations of standard RLHF pipelines.

*   **Approach:** The method addresses distribution shift by iteratively applying importance weighting to correct the Reward Model.
*   **Mechanism:** OCRM adjusts the RM to align with the current policy distribution dynamically.
*   **Efficiency:** This adaptation occurs without requiring new samples or additional human labels, making it a label-free solution to reward hacking.

---

## Technical Details

The paper frames overoptimization as a statistical problem involving distribution shift and estimation inconsistency.

**The Problem Frame**
*   **Reward Model (RM) Training:** The RM is trained on the Supervised Fine-Tuning (SFT) policy distribution, denoted as $\pi_{\text{SFT}}$.
*   **The Drift:** During RL optimization, the policy drifts to $\pi_{\text{RL}}$.
*   **The Consequence:** When the RM encounters out-of-distribution inputs from $\pi_{\text{RL}}$, it becomes unreliable.

**The Solution Frame**
*   **Pipeline Modification:** The proposed method modifies the third step of the standard RLHF pipeline.
*   **Adaptation Technique:** It applies distribution shift adaptation techniques, specifically **importance weighting**.
*   **Objective:** To correct reward model estimates during RL optimization to reflect the true distribution of the current policy.

---

## Contributions

1.  **Theoretical Analysis:** Provides a rigorous theoretical framework that identifies overoptimization as a distribution shift issue linked to inconsistent parameter estimates.
2.  **Algorithm Introduction:** Introduces the **OCRM algorithm**, designed to stabilize training via importance weighting.
3.  **Label-Free Efficiency:** Offers a label-free, computationally efficient solution to reward hacking that avoids the bottleneck of continuous human relabeling.
4.  **Empirical Validation:** Validates the method with strong results on complex tasks, including summarization and chatbot interactions.

---

## Results

The evaluation tracked three primary metrics: **RM Reward**, **True Reward**, and **RM Accuracy**.

*   **Standard PPO-RLHF (Baseline):**
    *   Exhibited **severe Goodharting**.
    *   RM Reward increased up to 600k steps.
    *   However, True Reward stagnated or decreased.
    *   RM Accuracy declined significantly over time.

*   **OCRM (Proposed Method):**
    *   Significantly outperformed standard RLHF.
    *   Maintained high **RM Accuracy** and **True Reward** throughout the training course.
    *   Demonstrated better stability and successful mitigation of distribution shift.