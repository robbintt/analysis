---
title: 'Graph-R1: Towards Agentic GraphRAG Framework via End-to-end Reinforcement
  Learning'
arxiv_id: '2507.21892'
source_url: https://arxiv.org/abs/2507.21892
generated_at: '2026-02-03T06:35:02'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Graph-R1: Towards Agentic GraphRAG Framework via End-to-end Reinforcement Learning

*Haoran Luo; Haihong E; Guanting Chen; Qika Lin; Yikai Guo; Fangzhi Xu; Zemin Kuang; Meina Song; Xiaobao Wu; Yifan Zhu; Luu Anh Tuan*

---

> **üìù Executive Summary**
> 
> Retrieval-Augmented Generation (RAG) systems utilizing Knowledge Graphs (GraphRAG) face critical limitations regarding the high computational costs of graph construction and the static nature of retrieval mechanisms. Existing methods often fail to adapt dynamically to specific query requirements, relying heavily on long-context reasoning and complex, manually engineered prompts. This results in suboptimal retrieval efficiency and reasoning accuracy, particularly in complex scenarios.
> 
> To address these challenges, the authors introduce **Graph-R1**, a novel agentic framework that integrates end-to-end Reinforcement Learning (RL) with GraphRAG to transform retrieval into a dynamic, sequential decision-making process. Technically, the method employs a **Lightweight Knowledge HyperGraph** with coherence-weighted hyperedges to minimize overhead, replacing static retrieval with a multi-turn agent-environment interaction loop. By utilizing outcome-directed RL, the agent optimizes retrieval actions and reasoning chains directly toward the final answer, maximizing the expected joint likelihood through candidate retrieval and pruning stages.
> 
> Evaluated across six datasets using F1 Score (%) as the primary metric, Graph-R1 achieved an overall average of **57.8%**, substantially outperforming Search-R1 (46.2%), Standard RAG (32.0%), and the baseline GraphRAG (24.9%). The framework demonstrated particular strength in multi-hop reasoning, scoring 65.0% on WikiMultiHopQA and 46.2% on the Musique dataset. With an absolute improvement of +11.6% over the next best baseline, the results validate the superior performance ceiling of utilizing a graph environment for RL optimization in retrieval tasks.

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Overall Average F1** | **57.8%** |
| **Best Baseline Beaten** | Search-R1 (46.2%) |
| **Improvement Margin** | **+11.6%** (Absolute) |
| **Key Strength** | Multi-hop Reasoning |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |

---

### üîë Key Findings

*   **Superior Performance:** Graph-R1 significantly outperforms traditional GraphRAG and existing RL-enhanced RAG methods.
*   **Enhanced Accuracy & Quality:** Achieves substantial improvements in reasoning accuracy and generation quality.
*   **Improved Efficiency:** Demonstrates better retrieval efficiency, effectively addressing the high construction costs associated with traditional graph methods.
*   **Reduced Complexity:** The method effectively reduces reliance on long-context reasoning and complex manual prompt design.

---

### ‚öôÔ∏è Methodology

The authors propose **Graph-R1**, an agentic framework designed to overcome the limitations of static and high-cost graph retrieval. The methodology consists of three core components:

1.  **Lightweight Knowledge Hypergraph Construction**
    *   Focuses on reducing computational overhead.
    *   Utilizes a Knowledge HyperGraph structure to optimize representation.

2.  **Multi-turn Agent-Environment Interaction**
    *   Models retrieval as a dynamic, sequential process rather than a one-time static event.
    *   Allows the agent to interact with the graph environment over multiple turns.

3.  **End-to-End Reinforcement Learning**
    *   Employs a reward mechanism to optimize the agent's retrieval process directly.
    *   Utilizes outcome-directed RL to align retrieval actions with final answer outcomes.

---

### üõ†Ô∏è Technical Details

**Architecture & Workflow**
*   **Paradigm:** Introduces an agentic workflow to GraphRAG using end-to-end Reinforcement Learning (RL).
*   **Graph Structure:** Uses a Knowledge HyperGraph with hyperedges weighted by coherence scores.

**Pipeline Stages**
1.  **Knowledge Graph Construction:** Extracts relational facts via a Large Language Model (LLM).
2.  **Graph Retrieval:** Involves candidate retrieval and pruning strategies to maximize expected joint likelihood.
3.  **Answer Generation:** Synthesizes the final response based on retrieved information.

**Key Innovations**
*   **Multi-turn Graph Interactions:** The agent interacts with the HyperGraph iteratively to refine retrieval.
*   **Outcome-directed RL:** Optimizes reasoning chains and retrieval actions directly toward the final answer outcome, bypassing the need for heuristic prompt engineering.

---

### üìà Performance Results

Evaluated using **F1 Score (%)** across six datasets, Graph-R1 demonstrated significant dominance in retrieval and reasoning tasks.

**Overall Performance Comparison**
*   **Graph-R1:** **57.8%**
*   Search-R1: 46.2%
*   Standard RAG: 32.0%
*   GraphRAG: 24.9%

**Multi-hop Reasoning Excellence**
*   **WikiMultiHopQA:** 65.0%
*   **Musique:** 46.2%

**Key Achievement**
*   The model achieved an absolute improvement of **+11.6%** over the next best baseline, validating the superior performance ceiling of the graph environment for RL.

---

### üåü Contributions

The main contributions of this research include:

*   **Novel Paradigm:** Introduction of a new GraphRAG paradigm that integrates agency and reinforcement learning (Graph-R1).
*   **Optimized Representation:** Optimization of graph representation through lightweight knowledge hypergraph construction to mitigate computational costs.
*   **Sequential Reformulation:** Reformulation of retrieval as a sequential decision-making problem via multi-turn interaction.
*   **Autonomous Optimization:** Implementation of an end-to-end, reward-driven optimization mechanism that reduces dependency on heuristic prompt design.

---

*Analysis based on 40 references.*