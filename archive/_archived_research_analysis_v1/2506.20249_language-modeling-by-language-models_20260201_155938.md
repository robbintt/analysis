# Language Modeling by Language Models

*Junyan Cheng; Peter Clark; Kyle Richardson*

---

> ### ðŸ“Š Quick Facts
>
> **Metric** | **Value**
> :--- | :---
> **Novel Designs Generated** | 1,162
> **Successfully Verified** | 1,062
> **Verification Success Rate** | 91.3%
> **Benchmarks Won** | 6 out of 9 (vs. GPT-2 & Mamba2)
> **æ–¹æ³•è®º Improvement** | +86 percentage points (Genetic vs. Direct)
> **Quality Score** | 9/10
> **Total Citations** | 40

---

## Executive Summary

This research addresses the computational and creative bottleneck inherent in manually designing neural architectures for Natural Language Processing. By investigating whether Large Language Models (LLMs) can autonomously navigate the complex search space of model design, the paper challenges the reliance on human intuition for architecture innovation. Instead of treating LLMs as passive tools, the authors propose treating them as active agents capable of executing the full scientific research lifecycleâ€”from ideation to verificationâ€”to discover novel architectures that might remain obscured to human researchers.

The core innovation is **Genesys**, a framework built upon the Language Model Architecture Discovery Environment (LMADE) that integrates a Genetic Programming (GP) backbone with multi-agent LLM simulation. Unlike direct prompting methods, Genesys employs a **'Ladder of Scales'** strategy that verifies designs at incrementally larger parameter sizes, ranging from 14M to 350M, under narrowing budget constraints. The system utilizes **'Designer'** agentsâ€”grounded by a Knowledge Engine indexing 297 LM papersâ€”to propose architectural modifications, and **'Verifier'** agents to execute a multi-stage symbolic checking process involving static, dynamic, and semantic analysis to ensure architectural validity before training.

Genesys demonstrated high efficacy in large-scale experimentation, generating a total of **1,162** novel LM designs and successfully verifying **1,062** of them, representing a 91.3% success rate. This genetic programming approach yielded an approximately **86 percentage point** improvement in valid design generation compared to direct prompt generation methods. Furthermore, the architectures discovered by Genesys surpassed established baselines, outperforming both GPT-2 and Mamba2 on **6 out of 9** specific downstream benchmarks.

This work establishes a robust, fully automated paradigm for Neural Architecture Search (NAS) where AI designs AI, offering a scalable solution to accelerate innovation in model development. By providing empirical evidence that autonomous agents can perform high-level scientific discoveryâ€”including complex reasoning and code verificationâ€”the paper validates the potential of multi-agent systems to uncover novel design patterns that rival or exceed human-defined architectures.

---

## Key Findings

*   **High Output Volume:** The Genesys system successfully generated **1,162** novel LM designs and fully verified **1,062** of them.
*   **Superior Performance:** The discovered architectures outperformed established models like GPT-2 and Mamba2 on **6 out of 9** benchmarks.
*   **Methodological Efficiency:** Utilizing a genetic programming backbone resulted in an approximately **86 percentage point** improvement in successful design generation compared to direct prompt generation.
*   **Broad Applicability:** Comprehensive results demonstrate the broader utility of autonomous discovery systems for scientific research.

---

## Technical Details

The paper presents **Genesys**, an autonomous discovery system built on the Language Model Architecture Discovery Environment (LMADE).

*   **Core Optimization:** Utilizes a Genetic Programming (GP) backbone to optimize neural architecture search.
*   **System Workflow:**
    *   **Factorization:** Breaks down the process into an evolution tree.
    *   **Designer Agents:** LLM-driven agents responsible for proposing architectural modifications.
    *   **Verifier Agents:** Agents tasked with verifying validity.
*   **LMADE Components:**
    *   **Knowledge Engine:** Features a graph constructed from 297 LM papers to ground the agents.
    *   **Verification Engine:** Uses a Generalized Autoregressive Block (GAB) template and a multi-stage symbolic checker (static, dynamic, and semantic analysis).
*   **Objective Function:** Formally defined to maximize fitness across **29 downstream benchmarks** and various model scales.

---

## Methodology

The framework employs a **multi-agent LLM simulation** designed to simulate the full research lifecycle. The process covers three primary phases:

1.  **Ideation:** Generating initial concepts and architectural variations.
2.  **Code Generation:** Writing the implementation code for the proposed designs.
3.  **Evaluation:** Testing and validating the performance of the models.

**Key Strategy: Ladder of Scales**
The system operates by verifying designs at increasingly larger scalesâ€”starting from **14M parameters** up to **350M parameters**. This occurs under narrowing budget constraints, ensuring resources are allocated to the most promising candidates as they grow in complexity.

---

## Contributions

*   **Validated Framework:** Established a framework for using LLMs to automate the scientific discovery process specifically for LM architecture design.
*   **Empirical Evidence:** Provided concrete data showing that genetic programming significantly outperforms direct prompt generation in generating valid architectures.
*   ** Dataset & Proof of Concept:** Released a large-scale experimental dataset (over 1,000 verified models) proving that autonomous agents can discover architectures rivaling or exceeding human-designed baselines like GPT-2 and Mamba2.

---

## Results

*   **Generation & Verification:** Generated 1,162 novel LM designs with a **91.3% verification success rate** (1,062 verified).
*   **Benchmark Performance:** Discovered architectures outperformed GPT-2 and Mamba2 on 6 out of 9 benchmarks.
*   **Efficiency Gain:** The genetic programming approach improved successful design generation rates by **~86 percentage points** over direct prompt generation.
*   **Data Source:** Evaluation utilized a filtered **SmolLM corpus**.

---

* **Paper Quality Score:** 9/10
* **References:** 40 citations