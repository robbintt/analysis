# Clarify or Answer: Reinforcement Learning for Agentic VQA with Context Under-specification

*Zongwan Cao; Bingbing Wen; Lucy Lu Wang*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Accuracy Gain** | +15.3 points |
| **Relative Improvement** | 83% over baselines |
| **Models Tested** | 3 Vision-Language Large Models (VLLMs) |
| **Datasets Used** | 3 separate datasets |
| **Quality Score** | 9/10 |

---

## Executive Summary

> ðŸ’¡ **Core Concept:** This research addresses the pervasive issue of **context under-specification** in Visual Question Answering (VQA), where an input image and question lack sufficient information to generate a unique, correct answer.

Current Vision-Language Large Models (VLLMs) typically treat VQA as a static, single-pass task; when faced with ambiguity, these models default to providing a direct answer, often resulting in confident but incorrect predictions. This limitation hinders the deployment of VQA systems in real-world scenarios where ambiguity is common and making erroneous assumptions is unacceptable.

The authors introduce **Clarify-or-Answer (CoA)**, a novel agentic framework that reformulates VQA as an interactive decision problem. The key technical innovation is a **separated modeling approach** that decouples the binary decision to ask a question from the content generation of that question. The architecture comprises a Controller policy to determine whether to **ANSWER** or **CLARIFY**, a Clarification Policy to generate targeted, open-ended questions, and an Answering Model that processes the user's response. The system is optimized using **GRPO-CR**, a tailored Reinforcement Learning algorithm that employs multi-signal optimization to ensure generated questions are well-formed, focused, non-trivial, and effective at resolving ambiguity.

The CoA agent demonstrated substantial performance gains, achieving an average accuracy improvement of **+15.3 points**â€”an **83% relative increase** over prompting-based baselines. This robustness was validated across three different Vision-Language Large Models (VLLMs) and three separate datasets. Ablation studies further confirmed that the specific architectural commitment to separated modeling outperforms end-to-end alternatives, and the strategy of generating open-ended questions via GRPO-CR proved superior to Yes/No questioning strategies trained with Direct Preference Optimization (DPO).

This work significantly advances the field by establishing the efficacy of interactive, agentic workflows for handling ambiguity in multimodal AI. By proving that systems can successfully learn when information is missing and how to recover it through interaction, the authors set a precedent for more reliable and human-like visual reasoning. Furthermore, the paper contributes the **CONTEXTCLARIFY dataset**â€”containing ambiguous VQA pairs and contrastive non-ambiguous examplesâ€”providing a vital resource for future research into robust, clarification-capable AI systems.

---

## Key Findings

*   **Significant Performance Improvement:** The CoA agent improves VQA accuracy by **+15.3 points**, representing an **83% relative improvement** over baseline methods.
*   **Robustness Across Models:** The framework demonstrates consistent effectiveness across three different Vision-Language Large Models (VLLMs) and three distinct datasets.
*   **Impact of Under-specification:** The study highlights that directly answering under-specified pairs leads to confident but often incorrect predictions in standard models.
*   **Efficacy of Separated Modeling:** Separately modeling the decision to ask from the question content yields significantly better performance than end-to-end approaches.

---

## Methodology

The authors propose the **CoA (Clarify-or-Answer)** agent, specifically designed to handle context under-specification in VQA. The approach relies on three core methodological pillars:

1.  **Two-Step Decision Process:**
    The system employs a modular mechanism to determine whether the current context is sufficient to provide an answer or if clarification is required.

2.  **Interactive Loop:**
    The framework incorporates user responses into the workflow, allowing the model to refine its understanding before committing to a final answer.

3.  **Reinforcement Learning Optimization:**
    Optimization is achieved via the **GRPO-CR algorithm**, which utilizes multiple reward signals to ensure the generation of high-quality, relevant questions.

---

## Technical Details

### Framework Architecture
The Clarify-or-Answer (CoA) framework treats VQA as an interactive decision problem to address context under-specification through a modular architecture consisting of three distinct components:

*   **Controller (Policy $\pi_{ctrl}$):** Responsible for the high-level decision of choosing between **ANSWER** or **CLARIFY** actions.
*   **Clarification Policy (Policy $\pi_{clr}$):** Generates targeted, open-ended questions designed to elicit missing context.
*   **Answering Model (Policy $\pi_{ans}$):** Operates in two modes:
    *   **Direct Mode:** Generates an answer based solely on the original image and question.
    *   **Context-Enhanced Mode:** Generates an answer utilizing the original image, question, and the user's response to the clarification.

### Training Algorithm (GRPO-CR)
The system employs **GRPO-CR**, a specialized Reinforcement Learning method for clarification reasoning. It optimizes the model using multi-signal optimization criteria:

*   **Well-formedness:** Ensures grammatical correctness and fluency.
*   **Focus:** Maintains relevance to the image and original question.
*   **Non-triviality:** Avoids redundant or obvious questions.
*   **Resolution Capability:** Measures the effectiveness of the question in resolving the ambiguity.

### Ambiguity Categories
The system is designed to handle five specific categories of ambiguity:
1.  **Location/orientation**
2.  **Temporal**
3.  **Cultural**
4.  **Attributes**
5.  **Relationships**

---

## Contributions

*   **CoA Framework:** A novel agentic framework that distinctly models the decision to ask versus the question content, separating these concerns for better performance.
*   **CONTEXTCLARIFY Dataset:** A new dataset resource comprising ambiguous VQA questions paired with a contrast set of non-ambiguous questions to aid in training and evaluation.
*   **GRPO-CR Algorithm:** A tailored reinforcement learning approach for clarification reasoning that utilizes multi-signal optimization to refine question generation.

---

## Results

The evaluation of the CoA agent yielded the following outcomes:

*   **Accuracy Gains:** Achieved an average accuracy gain of **+15.3 points**.
*   **Baseline Comparison:** Represents an **83% relative improvement** compared to prompting-based baselines.
*   **Generalizability:** The framework demonstrated robustness across three different Vision-Language Large Models (VLLMs) and three separate datasets.
*   **Ablation Study Insights:**
    *   Separated modeling outperforms end-to-end approaches.
    *   Using **open-ended questions** combined with **GRPO-CR optimization** proved superior to **Yes/No questioning** with DPO training.

---
**References:** 8 citations