---
title: 'FAQ: Mitigating Quantization Error via Regenerating Calibration Data with
  Family-Aware Quantization'
arxiv_id: '2601.112'
source_url: https://arxiv.org/abs/2601.11200
generated_at: '2026-02-03T19:03:09'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# FAQ: Mitigating Quantization Error via Regenerating Calibration Data with Family-Aware Quantization

*Haiyang Xiao; Weiqing Li; Jinyue Guo; Guochao Jiang; Guohua Liu; Yuewei Zhang*

---

> ### **Quick Facts**
>
> *   **Accuracy Gain:** Reduces accuracy loss by up to **28.5%**.
> *   **Core Innovation:** "Elder Sibling" regeneration strategy using larger in-family models.
> *   **Validation Models:** Tested on Qwen3-8B (Target) & Qwen3-235B-A22B (Teacher).
> *   **Method Type:** Training-free, Data-centric Post-Training Quantization (PTQ).
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations

---

## Executive Summary

Post-training quantization (PTQ) for Large Language Models (LLMs) faces a critical bottleneck regarding the quality and representativeness of calibration data. Traditional PTQ methods rely on static, often limited datasets that fail to capture the complex, high-dimensional activation distributions required for accurate inference. This data insufficiency leads to significant distribution drift and accuracy loss when compressing models for resource-constrained environments, creating a substantial barrier to deploying high-performance LLMs on edge hardware without expensive retraining.

The authors propose **FAQ (Family-Aware Quantization)**, a training-free, data-centric framework that addresses calibration limitations through an "Elder Sibling" regeneration strategy. Technically, the method utilizes a larger model from the same family to regenerate high-fidelity calibration samples, incorporating Chain-of-Thought (CoT) reasoning to enhance representativeness. These samples undergo an expert-guided selection process involving group competition and re-normalization to filter for optimal quality.

By optimizing for smooth, concentrated activation distributions and minimizing the objective function regarding quantization-friendly smoothed activations, FAQ effectively mitigates the biases and outliers inherent in limited original datasets. Experimental validation on the Qwen3-8B architecture, utilizing Qwen3-235B-A22B as the teacher model, demonstrated that FAQ significantly outperforms standard PTQ baselines. The framework achieved a reduction in accuracy loss of up to **28.5%** compared to methods relying on original calibration data.

---

## Key Findings

*   **Significant Accuracy Recovery:** The proposed FAQ framework reduces accuracy loss in post-training quantization (PTQ) by up to **28.5%** compared to baselines that use original calibration data.
*   **Validation Across Architectures:** The effectiveness of the method was demonstrated through experiments on multiple model series, including **Qwen3-8B**.
*   **Superior Data Representativeness:** Regenerating calibration data using larger, same-family models results in high-fidelity samples that better capture the expected activation distribution during inference than limited original samples.
*   **Enhanced PTQ Performance:** By utilizing data containing **Chain-of-Thought reasoning**, the framework enhances the effectiveness of standard PTQ processes, mitigating the biases typically caused by limited sample sizes.

---

## Methodology

The authors propose FAQ (Family-Aware Quantization), a calibration data regeneration framework designed to improve PTQ for Large Language Models. The methodology follows a specific pipeline:

1.  **Cross-Model Generation:** Original calibration samples are input into a larger LLM from the same family to regenerate high-fidelity data.
2.  **Reasoning Integration:** The framework incorporates Chain-of-Thought reasoning to enrich the data context.
3.  **Expert-Guided Selection:** A 'group competition' phase, conducted under expert guidance, is used to filter optimal samples.
4.  **Re-normalization:** The selected samples are re-normalized to determine precise quantization parameters.

---

## Contributions

*   **Addressing the Calibration Bottleneck:** Identifies and addresses the core limitation of traditional PTQ methods—the representativeness and universality of calibration data—by shifting focus from static collection to dynamic regeneration.
*   **Family-Aware Knowledge Transfer:** Introduces a novel paradigm where prior knowledge from larger LLMs within the same model family is utilized to generate superior calibration data for smaller, resource-constrained variants.
*   **Advanced Data Filtering:** Contributes a new mechanism for calibration sample optimization that combines Chain-of-Thought reasoning with expert-guided group competition to ensure data fidelity.
*   **Practical Deployment Gains:** Provides a solution that significantly lowers the barrier for deploying high-accuracy quantized LLMs on resource-constrained devices without requiring extensive retraining or massive static calibration datasets.

---

## Technical Details

**Core Concept**
Family-Aware Quantization (FAQ) is a training-free, data-centric Post-Training Quantization (PTQ) approach that optimizes activation distributions rather than the quantization algorithm itself.

**The "Elder Sibling" Strategy**
The framework utilizes a regeneration strategy where a larger 'in-family' model (e.g., Qwen3-235B-A22B) acts as a **teacher** to regenerate and normalize calibration prompts for a target model (e.g., Qwen3-8B). This creates a 'model-friendly' calibration set ($D_{FAQ}$) that includes Chain-of-Thought (CoT) reasoning.

**Mathematical Objective**
The theoretical framework addresses distribution drift in deep layers by optimizing the following objective function:

$$ O(W_n,A_n,bA_n) = \min ||Q(W_n)A_n - W_n bA_n||^2_F $$

Where:
*   $Q(W_n)$ is the quantized weights.
*   $A_n$ represents the standard activations.
*   $bA_n$ represents 'quantization-friendly' smoothed activations.

**Activation Smoothing**
This process induces activation smoothing, resulting in smoother, more concentrated distributions with fewer outliers compared to standard data.

---

## Results

*   **Performance Metrics:** The FAQ framework achieved a reduction in accuracy loss of up to **28.5%** compared to baseline methods using original calibration data.
*   **Evaluation Setup:** The method was validated on the Qwen3-8B architecture. Performance was evaluated using:
    *   **GRMC:** Average over 12 general downstream tasks.
    *   **SDC:** Average over AIME, MATH-500, and LiveCodeBench.
*   **Activation Analysis:** Analysis indicated that FAQ-generated data results in **'smooth' and concentrated activation matrices** with significantly fewer extreme peaks (outliers) than the 'sharp' and sparse activations from original data.
*   **Teacher Model:** The experimental setup utilized Qwen3-235B-A22B as the teacher model.

---

**Quality Score:** 8/10 | **References:** 40 Citations