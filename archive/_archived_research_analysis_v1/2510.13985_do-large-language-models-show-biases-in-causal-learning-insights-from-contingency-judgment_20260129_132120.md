# Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment

*MarÃ­a Victoria Carro; Denise Alejandra Mester; Francisca Gauna Selasco; Giovanni Franco Gabriel Marraffini; Mario Alejandro Leiva; Gerardo I. Simari; MarÃ­a Vanina Martinez*

> ### ðŸ“‘ Quick Facts
> ---
> * **Quality Score:** 9/10
> * **References:** 40 citations
> * **Models Evaluated:** GPT-4o-Mini, Claude-3.5-Sonnet, Gemini-1.5-Pro
> * **Dataset Size:** 1,000 unique medical scenarios
> * **Experimental Paradigm:** Contingency Judgment Task
> * **Statistical Significance:** p < 0.001

---

## Executive Summary

Large Language Models (LLMs) are increasingly deployed in high-stakes domains requiring rigorous reasoning, yet a critical unresolved problem is determining whether these models possess a genuine, normative understanding of causal principles or merely rely on superficial statistical correlations. This paper addresses the susceptibility of LLMs to the "illusion of causality"â€”a cognitive bias where agents infer a causal relationship between events despite the absence of statistical evidence. This inability to distinguish null contingencies (where outcomes occur independently of a purported cause) poses severe reliability risks. In fields like healthcare, failing to recognize a lack of causal efficacy can lead models to hallucinate medical benefits or reinforce harmful social stereotypes based on spurious patterns, making the distinction between correlation and causation a vital safety concern.

The key innovation lies in methodologically translating the psychological "contingency judgment task" paradigm to AI evaluation, bridging human cognitive psychology and LLM benchmarking. The researchers constructed a dataset of 1,000 distinct "null contingency" medical scenarios, explicitly designed so that the probability of an outcome is identical regardless of the presence or absence of a cause. The study rigorously controls for experimental variables, categorizing scenarios into four types (Fabricated, Indeterminate, Alternative/Pseudo-medicine, and Established Science) and varying trial lengths (20 to 100 samples) to test if models can update beliefs with more data. State-of-the-art models (GPT-4o-Mini, Claude-3.5-Sonnet, and Gemini-1.5-Pro) were prompted to role-play as medical researchers, assessing causal effectiveness on a 0-100 scale across three generation configurations: Consistency Check (Temp=1.0), Deterministic (Temp=0.0), and Default Behavior.

The results demonstrate that all evaluated models systematically infer unwarranted causal relationships ($p < 0.001$), exhibiting a robust illusion of causality across all tested configurations and scenario categories. Under the Temperature 1.0 Consistency Check, GPT-4o-Mini showed the highest susceptibility (Median: 75.7, Zero Production: 0.0%), followed by Claude-3.5-Sonnet (Median: 50.0, Zero Production: 4.6%) and Gemini-1.5-Pro, which displayed relative robustness (Median: 45.0, Zero Production: 20.5%). Crucially, the analysis reveals that the illusion persists even in deterministic settings (Temp=0.0) and is resistant to increases in trial length, suggesting that models struggle to override prior beliefs with empirical data. Furthermore, the study found significant variation by scenario type; models were particularly prone to bias in "Alternative/Pseudo-medicine" and "Fabricated" contexts, indicating that LLMs replicate human cognitive biases linked to social prejudice and stereotypes by relying on ingrained associations rather than the provided statistical evidence.

This research provides empirical evidence that current LLMs rely heavily on statistical pattern matching rather than true causal comprehension, challenging the notion that scale alone equates to normative reasoning capabilities. By establishing that even state-of-the-art architectures fail to distinguish causality from coincidenceâ€”often relying on prejudiced or pseudo-scientific correlationsâ€”the study underscores critical safety risks for deploying these models in medicine and scientific research. Consequently, the paper establishes a new benchmark for causal validity and supports the urgent need for architectural improvements or specific guardrails to mitigate "illusory" reasoning in high-stakes decision-making environments.

---

## Key Findings

*   **Systematic Bias:** All evaluated LLMs systematically inferred unwarranted causal relationships, indicating susceptibility to the **'illusion of causality'**.
*   **Cognitive Replication:** LLMs replicate human cognitive biases linked to social prejudice and stereotypes.
*   **Lack of Normative Understanding:** Findings suggest LLMs lack a genuine normative understanding of causal principles.
*   **Reliability Concerns:** The inability to distinguish null contingencies raises significant reliability concerns in high-stakes fields like healthcare.

---

## Methodology

The study employed a rigorous experimental design to test causal reasoning capabilities:

*   **Paradigm:** Utilized the **'contingency judgment task'** paradigm adapted from psychology.
*   **Dataset Construction:** Built a custom dataset of **1,000 'null contingency scenarios'** set in medical contexts.
    *   Scenarios were explicitly designed without causal link support.
    *   Outcome probability is identical regardless of the presence or absence of the cause.
*   **Evaluation Protocol:** Models were prompted to assess causal effectiveness to measure deviations from normative causal reasoning.

---

## Technical Details

### Task Adaptation
*   The study adapts the psychological 'contingency judgment task' for LLMs by presenting natural-language null-contingency scenarios where outcome probability is identical regardless of the cause.

### Dataset Composition
*   **Volume:** 1,000 unique scenarios.
*   **Categories:**
    1.  Fabricated
    2.  Indeterminate
    3.  Alternative/Pseudo-medicine
    4.  Established Science
*   **Trial Length:** Ranged from 20 to 100 samples to test belief updating with more data.

### Evaluation Configurations
*   **Role-Playing:** Prompts assigned roles as medical researchers or doctors.
*   **Scale:** Effectiveness assessed on a 0â€“100 scale.
*   **Configurations:**
    1.  **Consistency Check:** Temp=1.0, n=10
    2.  **Deterministic:** Temp=0.0
    3.  **Default Behavior**

---

## Results

The study observed statistically significant failures in causal reasoning across all models ($p < 0.001$):

*   **GPT-4o-Mini (Temp 1.0):**
    *   Highest susceptibility to illusion.
    *   **Median:** 75.7
    *   **Zeros:** 0.0% (No correct identification of null contingencies).
*   **Claude-3.5-Sonnet (Temp 1.0):**
    *   Moderate susceptibility.
    *   **Median:** 50.0
    *   **Zeros:** 4.6%
*   **Gemini-1.5-Pro (Temp 1.0):**
    *   Lowest illusion but highest variance.
    *   **Median:** 45.0
    *   **Zeros:** 20.5%

*   **Statistical Confirmation:** Tests confirmed significant differences in central tendency and zero production rates among the models. The illusion persisted regardless of temperature settings or trial length increases.

---

## Contributions

*   **Empirical Evidence:** Provides concrete evidence that LLMs exhibit the illusion of causality, bridging the gap between human cognitive psychology and AI behavior.
*   **Theoretical Support:** Offers insight into LLM reasoning, supporting the theory of statistical pattern matching over true comprehension.
*   **Safety Impact:** Highlights critical safety risks for deploying LLMs in high-stakes domains requiring precise causal reasoning (e.g., healthcare).

---

## Assessment

*   **Quality Score:** 9/10
*   **References:** 40 citations