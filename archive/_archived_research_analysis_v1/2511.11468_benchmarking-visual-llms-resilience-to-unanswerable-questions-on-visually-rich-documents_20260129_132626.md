# Benchmarking Visual LLMs Resilience to Unanswerable Questions on Visually Rich Documents

*Davide Napolitano; Luca Cagliero; Fabrizio Battiloro*

<br />

> ### ðŸ“Š Quick Facts
>
> *   **Models Evaluated:** 12 Visual Large Language Models (VLLMs)
> *   **Total Documents:** 424 (from MPDocVQA and DUDE datasets)
> *   **Verified Questions:** 593 unanswerable questions generated
> *   **Corruption Levels:** 3 (ranging from C=1 to C=3)
> *   **Compute Budget:** ~90 hours (NVIDIA A6000 GPU, 192GB RAM)
> *   **Quality Score:** 9/10

<br />

---

## Executive Summary

This research addresses a critical reliability gap in Visual Large Language Models (VLLMs) applied to Visually Rich Documents (VRDs), such as invoices, forms, and reports. While VLLMs have demonstrated strong capabilities in standard Visual Question Answering (VQA), they frequently fail to detect "plausible yet unanswerable" questionsâ€”queries that appear contextually valid but lack a definitive answer due to logical inconsistencies within the document. Instead of rejecting these queries, current models tend to hallucinate responses, posing significant risks for downstream applications in enterprise document processing where accuracy and error detection are paramount.

To systematically evaluate this deficiency, the authors introduce **VRD-UQA**, a novel benchmark framework featuring a four-step automated pipeline. The technical core of the innovation lies in its data corruption strategy: the framework utilizes Document Layout Analysis, GOT-OCR 2.0, and Qwen 2.5 VL to annotate documents, then applies GliNER to swap entities (Numerical, Temporal, Structural, etc.) across different elements or pages to generate unanswerable scenarios.

Evaluation of 12 state-of-the-art VLLMs using the MPDocVQA and DUDE datasets revealed widespread limitations in resilience. The results indicate that model robustness is heavily dependent on the nature of the corruption; models exhibited significant variance in detection accuracy when distinguishing between NLP-based corruptions and visual layout changes. Furthermore, the study identified a notable performance discrepancy between page-level and document-level detection. This work establishes a structured taxonomy for evaluating VLLM robustness, providing a critical roadmap for future development toward more resilient document VQA systems.

---

## Key Findings

*   **Significant Limitations in Detection:** Experiments on 12 VLLMs revealed that current models struggle to accurately detect plausible yet unanswerable questions within Visually Rich Documents (VRDs).
*   **Corruption Sensitivity:** Model robustness varies significantly depending on the nature of the data corruption, specifically differentiation between NLP entity swaps, document element alterations, and layout changes.
*   **Scope Variance:** There is a notable difference in model performance when detecting unanswerable questions at the page level compared to the full document level.
*   **Context Learning Gaps:** While in-context learning strategies (e.g., OCR integration, multi-page selection) affect resilience, they currently require improvement to handle edge cases effectively.

---

## Technical Details

The paper introduces **VRD-UQA** (Visually Rich Document Unanswerable Question Answering), a framework designed to benchmark VLLM resilience to "plausible but unanswerable" questions on multi-page VRDs.

### Architecture Pipeline
The framework follows a structured four-step pipeline:

1.  **Augmentation:**
    *   Utilizes **Document Layout Analysis**.
    *   Employs **Qwen 2.5 VL** for element captions.
    *   Uses **GOT-OCR 2** for text extraction.
2.  **Corruption:**
    *   Utilizes **GliNER** for entity detection (categories: Numerical, Temporal, Miscellaneous, Location, Structural).
    *   Applies corruption levels ranging from **C=1 to C=3**.
    *   Uses **Qwen 2.5** for rephrasing.
3.  **Verification:**
    *   Implements a **VLLM-as-a-judge** approach to verify the validity of generated unanswerable questions.
4.  **Evaluation:**
    *   Uses a **sliding window approach**.

### Strategies & Metrics
*   **Corruption Strategies:** Target three distinct areas:
    *   NLP Entities
    *   Document Elements
    *   Document Layout
*   **Evaluation Metrics:**
    *   Document-Level Accuracy ($Acc_D$)
    *   Page-Level Accuracy ($Acc_P$)

---

## Methodology

The researchers developed the **VRD-UQA benchmark** to evaluate VLLM resilience to unanswerable questions. The methodology relies on automated data generation to alter questions from existing VQA datasets. This process introduces "corruptions," such as replacing entities with those located in different document elements or pages.

To ensure quality, a 'VLLM-as-a-judge' approach verifies the generated unanswerable questions. The framework evaluates models across 12 distinct VLLMs, assessing them on:
*   Detection accuracy.
*   Vulnerability to specific corruption types.
*   The effectiveness of knowledge injection strategies.

---

## Results

The experimental phase utilized the **MPDocVQA** and **DUDE** datasets:

*   **Processed:** 424 documents containing 595 original questions.
*   **Generated:** 2,176 potentially unanswerable questions.
*   **Verified Dataset:** 593 questions stratified by severity:
    *   Level-1: 318 questions
    *   Level-2: 201 questions
    *   Level-3: 74 questions

**Key Outcomes:**
*   Evaluated 12 Visual LLMs using an NVIDIA A6000 GPU, AMD 7950X CPU, and 192GB RAM.
*   Confirmed significant limitations in detecting unanswerable questions across all tested models.
*   Highlighted that robustness is not uniform; it fluctuates based on whether the corruption is NLP-based versus visual/layout-based.
*   Reinforced the finding that page-level detection yields different results compared to holistic document-level detection.

---

## Contributions

*   **VRD-UQA Benchmark:** Introduced a novel evaluation framework specifically designed for unanswerable question detection in Visually Rich Documents.
*   **Robustness Analysis:** Provided a comprehensive analysis of how models handle 'plausible yet unanswerable' scenarios through logical corruptions.
*   **Evaluation Taxonomy:** Established a structured taxonomy based on corruption types (NLP entity, document element, layout) and knowledge injection methods.
*   **Weakness Identification:** Identified critical weaknesses in state-of-the-art models to guide the development of more resilient document VQA systems.

---

**Paper Quality Score:** 9/10  
**References:** 40 citations