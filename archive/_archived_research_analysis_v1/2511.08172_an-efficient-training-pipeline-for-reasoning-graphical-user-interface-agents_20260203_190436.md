---
title: An Efficient Training Pipeline for Reasoning Graphical User Interface Agents
arxiv_id: '2511.08172'
source_url: https://arxiv.org/abs/2511.08172
generated_at: '2026-02-03T19:04:36'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# An Efficient Training Pipeline for Reasoning Graphical User Interface Agents

*Georgios Pantazopoulos; Eda B. Ã–zyiÄŸit*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Model Size** | 3B Parameters (Qwen-2.5-VL) |
| **Data Strategy** | 4.8M reduced to 12K instances (400x reduction) |
| **ScreenSpot Accuracy** | ~90% (F1: 80.3 for Ranker) |
| **Training Techniques** | SFT, CoT, GRPO (Reinforcement Learning) |
| **Quality Score** | 8/10 |

---

## Executive Summary

> **Overview:** Developing high-performing Graphical User Interface (GUI) agents typically relies on scaling up both model size and training data volume. However, this approach is computationally expensive and often plagued by the low quality of large-scale synthetic datasets, which contain significant noise and misalignment that hinder agent reasoning and performance.

> **The Solution:** This paper addresses the critical challenge of creating efficient, reasoning-capable GUI agents without relying on massive parameter counts or noisy, raw datasets. It challenges the prevailing assumption that bigger models and more data are strictly necessary, proposing instead that **data quality** and **training efficiency** are the primary drivers of success in GUI understanding.

> **The Pipeline:** The authors introduce a two-phase efficient training pipeline centered on data-centric AI and parameter-efficient fine-tuning:
> 1.  **Data Curation:** Reduces a massive 4.8M synthetic dataset down to just 12K high-quality instances through a four-step process (task-difficulty assessment, bounding-box accuracy ranking, diversity selection, and post-processing).
> 2.  **Multi-Regime Training:** Utilizes a curriculum on a 3B-parameter architecture, progressing through Supervised Fine-Tuning (SFT), Chain-of-Thought augmented SFT, and Reinforcement Learning (GRPO).

> **Impact:** The proposed approach, culminating in the **GUI-Qwen-GRPO-3B** model, achieved state-of-the-art performance on the ScreenSpot benchmark (~90% accuracy), outperforming larger baselines. Notably, the study demonstrated extreme data efficiency; by strategically reducing the training corpus by a factor of 400x, the pipeline achieved superior results compared to training on raw, large-scale synthetic data. This validates that principled data curation can rival traditional large-scale training while drastically reducing computational barriers.

---

## Key Findings

*   **Compact Model Superiority:** A compact **3B-parameter Vision-Language Model** can match or surpass larger models when trained on curated, high-quality data.
*   **Data Efficiency:** Reducing a massive **4.8M noisy dataset** to just **12K clean instances** significantly improves both efficiency and performance.
*   **Benchmark Dominance:** The proposed pipeline achieved superior results on major benchmarks including **ScreenSpot**, **Multimodal-Mind2Web**, and **AndroidControl**.
*   **Quality over Quantity:** Strategic filtering for challenging and diverse data is proven to be more effective than training on raw, large-scale synthetic data.

---

## Methodology

The research implements an efficient training pipeline consisting of two distinct phases:

### Phase 1: Data Curation Pipeline
This phase focuses on quality control and dataset optimization.
*   **Input:** 4.8M synthetic examples.
*   **Process:** Filters data down to 12K clean instances.
*   **Criteria:** Identifies challenging cases, removes misaligned data, and selects diverse multimodal instances.

### Phase 2: Multi-Regime Training
This phase focuses on optimizing model performance using parameter-efficient strategies.
*   **Base Model:** 3B-parameter VLM.
*   **Strategy:** Parameter-efficient training across three regimes:
    1.  **Supervised Fine-Tuning (SFT)**
    2.  **Chain-of-Thought (CoT) augmented fine-tuning**
    3.  **Reinforcement Learning** via Group Relative Policy Optimization (GRPO)

---

## Technical Details

### Data Curation Process
The pipeline employs a rigorous four-step filtering process to reduce 4.8M instances to 12K:
1.  **Task-Difficulty Filtering:** Utilizing *Qwen-2.5-VL-3B*.
2.  **Bounding-Box Accuracy Ranking:** Ensuring spatial precision.
3.  **Diversity Selection:** Via clustering algorithms.
4.  **Post-Processing:** Final refinement using *GPT-4o mini*.

### Model Architecture
*   **Base:** Qwen-2.5-VL-3B.
*   **Resolution Handling:** Supports image resolutions from $256 \times 28 \times 28$ to $1080 \times 28 \times 28$.
*   **Optimization:** Uses LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning.

### Training Regimen
The training utilizes a custom curriculum and reward system:
*   **Stages:** SFT $\to$ SFT+CoT $\to$ GRPO.
*   **Reward System:** Rule-based rewards based on:
    *   Format adherence.
    *   Solution accuracy.
    *   Output length penalties/constraints.

---

## Results

*   **Benchmark Performance (ScreenSpot):**
    *   The final model (**GUI-Qwen-GRPO-3B**) achieved approximately **90% accuracy**.
    *   Outperformed baselines like *ShowUI-G-2B* and *Uground-7B*.
*   **Ranker Performance:**
    *   The intermediate 3B Ranker outperformed larger zero-shot models, achieving an **F1 score of 80.3**.
*   **Data Efficiency:**
    *   Achieved superior performance while reducing training data volume by **400x** compared to raw synthetic datasets.

---

## Contributions

*   **Pipeline Innovation:** Introduction of a pipeline that integrates model-based data filtering with parameter-efficient fine-tuning to specifically address noisy synthetic datasets.
*   **Data-Centric Validation:** Validation of the data-centric AI hypothesis, showing that principled data curation can rival traditional large-scale training methods.
*   **Resource Efficiency:** Demonstration of resource-efficient agent development, enabling high-performance GUI agents using compact models, thereby reducing computational barriers for researchers and developers.

---
**Quality Score:** 8/10 | **References:** 40 citations