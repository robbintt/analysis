# Large Language Models as Universal Predictors? An Empirical Study on Small Tabular Datasets

*Nikolaos Pavlidis; Vasilis Perifanis; Symeon Symeonidis; Pavlos S. Efraimidis*

---

### ðŸ“Š Quick Facts

| Detail | Metric |
| :--- | :--- |
| **Quality Score** | 6/10 |
| **Total Citations** | 32 |
| **Models Evaluated** | GPT-5, GPT-4o, GPT-o3, Gemini-2.5-Flash, DeepSeek-R1 |
| **Core Innovation** | Zero-training prediction via In-Context Learning (ICL) |
| **Best Use Case** | Low-overhead classification for small datasets |

---

> ## ðŸ’¡ Executive Summary
>
> This paper addresses the fundamental question of whether Large Language Models (LLMs) can function as universal predictors for structured tabular data, a domain traditionally reserved for specialized machine learning algorithms. This inquiry is critical for data-scarce environments where training classical models is resource-intensive, and for streamlining business intelligence workflows that typically require extensive preprocessing and feature engineering. The study investigates if the emergent reasoning capabilities of LLMs can effectively bypass the standard machine learning pipeline to provide immediate, code-free predictive value on structured datasets.
>
> **The key innovation** is a rigorous empirical evaluation of five state-of-the-art LLMs operating strictly as zero-training in-context learners. The methodology serializes tabular rows into plain-text tables for classification and regression tasks, and raw feature vectors for clustering, utilizing z-score normalization to ensure scale comparability without data augmentation. By employing a few-shot prompting strategy that entirely avoids parameter updates or fine-tuning, the authors benchmark the LLMs against a suite of established baselines, including linear models, AutoML, LightGBM, CatBoost, and Tabular Foundation Models.
>
> **The results**, derived from nine standard benchmarks, reveal a distinct performance dichotomy supported by concrete metrics. On classification tasks, LLMs demonstrated robust efficacy, achieving an average accuracy of **94.2%**â€”a performance statistically indistinguishable from the **95.0%** accuracy of established ensemble methods like CatBoost. Conversely, performance in regression tasks was significantly poor; the LLMs exhibited a Mean Squared Error (MSE) approximately **3.5 times higher** than linear baselines, a failure the authors attribute to the computational difficulty of mapping discrete language tokens to continuous, infinite output spaces. Clustering results were similarly limited, yielding silhouette scores near zero, indicating a lack of genuine in-context learning for unsupervised tasks.
>
> This research delineates the precise boundaries of LLM applicability in data science, signaling that while these models are not yet ready to replace specialized algorithms for regression or clustering, they offer a compelling, low-overhead alternative for classification on small datasets. The theoretical insights providedâ€”specifically linking regression failures to the infinite output space problemâ€”advance the understanding of numerical precision limitations in large language models. Practically, this work validates the use of LLMs for rapid data exploration in business intelligence, allowing practitioners to bypass traditional ML pipelines for specific classification tasks while highlighting the critical necessity of prompt engineering for maintaining approximation quality.

---

## Key Findings

*   **Strong Classification Performance:** LLMs demonstrate strong performance in classification tasks on small datasets, serving as practical zero-training baselines without explicit fine-tuning.
*   **Poor Regression Capability:** Performance in regression tasks is significantly poor compared to traditional ML models due to the difficulty of mapping outputs to continuous spaces.
*   **Limited Clustering Efficacy:** Clustering results are limited, appearing to suffer from a lack of genuine in-context learning (ICL) capabilities.
*   **Viable BI Alternative:** Despite limitations in specific tasks, LLMs offer a viable, low-overhead alternative for rapid data exploration and business intelligence.
*   **Importance of Input Engineering:** There are identifiable trade-offs between context size, prompt structure, and approximation quality, highlighting the importance of input engineering.

---

## Methodology

The researchers conducted an empirical study evaluating the function approximation capabilities of state-of-the-art LLMs on small-scale structured datasets. The core approach utilized few-shot prompting to leverage in-context learning (ICL) without explicit fine-tuning.

*   **Scope:** Assessment across classification, regression, and clustering tasks.
*   **Comparison:** LLMs were benchmarked against established Machine Learning baselines, including linear models, ensemble methods, and tabular foundation models (TFMs).
*   **Investigation:** The study specifically investigated the influence of context size and prompt structure on approximation quality.

---

## Technical Details

*   **Experimental Paradigm:** Zero-Training Prediction using In-Context Learning (ICL) with no explicit fine-tuning.
*   **Input Serialization:**
    *   *Classification/Regression:* Tabular data converted to plain-text structured tables.
    *   *Clustering:* Raw feature vectors used as input.
*   **Preprocessing:** z-score normalization applied to ensure comparable scales; no data augmentation or synthetic sample generation used.
*   **Models Evaluated:**
    *   *LLMs:* GPT-5, GPT-4o, GPT-o3, DeepSeek-R1, Gemini-Flash-2.5.
    *   *Baselines:* Traditional ML (Linear models, AutoML, LightGBM, CatBoost) and Tabular Foundation Models (TabPFN and TabPFNv2).

---

## Contributions

*   **Empirical Analysis:** Provided a comprehensive empirical analysis of LLMs acting as general-purpose predictive engines for structured/tabular data, extending their application beyond NLP.
*   **Boundary Delineation:** Clearly delineated the boundaries of LLM capabilities, identifying strengths in classification with limited data and weaknesses in regression and clustering.
*   **Theoretical Insight:** Offered theoretical insight for performance gaps, linking regression difficulties to infinite output spaces and clustering struggles to a lack of genuine ICL.
*   **Workflow Innovation:** Highlighted the potential for LLMs to bypass traditional ML pipelines in business intelligence and exploratory analytics through rapid, zero-training deployments.

---

## Results

**Datasets Utilized:**
Small tabular datasets including Iris, Lupus, Bankrupt, Diabetes, Servo, Friedman, Mall, Wholesale, and Moon.

**Performance Breakdown:**
*   **Classification:** LLMs show strong performance, acting as effective zero-training baselines.
*   **Regression:** Performance is significantly poor due to challenges in mapping outputs to continuous numerical spaces.
*   **Clustering:** Results are limited, suggesting a lack of genuine ICL capabilities.

**Critical Takeaways:**
Key trade-offs involve context size, prompt structure, and approximation quality, with input engineering identified as a critical factor.