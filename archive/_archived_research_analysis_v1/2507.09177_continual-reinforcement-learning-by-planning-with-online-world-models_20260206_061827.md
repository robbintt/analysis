---
title: Continual Reinforcement Learning by Planning with Online World Models
arxiv_id: '2507.09177'
source_url: https://arxiv.org/abs/2507.09177
generated_at: '2026-02-06T06:18:27'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Continual Reinforcement Learning by Planning with Online World Models

*Zichen Liu; Guoji Fu; Chao Du; Wee Sun Lee; Min Lin*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Citations:** 40 References
> *   **Regret Bound:** $\mathcal{O}(\sqrt{K^2D\log(T)})$
> *   **Core Mechanism:** Follow-The-Leader (FTL) Online Agent

---

## Executive Summary

Continual Reinforcement Learning (CRL) faces the fundamental challenge of catastrophic forgetting, where an agent loses proficiency in previously learned tasks upon acquiring new skills. This instability is a major barrier to developing autonomous systems capable of lifelong learning in non-stationary environments. While traditional approaches often rely on deep networks combined with experience replay or regularization, these methods struggle to balance stability with plasticity, often resulting in performance degradation over time and high computational costs for retraining.

This paper introduces the **Follow-The-Leader (FTL) Online Agent (OA)**, a novel architecture that fundamentally addresses forgetting through a planning-centric approach using online world models. Instead of relying on static or deep representations, the agent learns a unified, shallow world dynamics model ($y = W\sigma(Px)$) incrementally. By employing a Follow-The-Leader strategy with regularized least squares and sparse updates, the model adapts to new data distributions efficiently. The agent uses Model Predictive Control (MPC) with the Cross-Entropy Method (CEM) to plan actions based solely on the latest model, rendering it immune to catastrophic forgetting by construction without the need for separate retention mechanisms.

The research provides strong theoretical and empirical validation for the proposed method. The authors derive a proven regret bound of $\mathcal{O}(\sqrt{K^2D\log(T)})$, offering mathematical assurance regarding the agent's stability over time. In empirical tests, the shallow FTL Online Agent consistently outperformed baseline agents utilizing deep world models equipped with various continual learning techniques. Additionally, the authors introduced "**Continual Bench**," a new benchmark environment designed to facilitate robust evaluation and comparison of CRL algorithms, demonstrating the agent's ability to solve sequences of tasks effectively.

This work challenges the prevailing assumption that deep world models are necessary for effective continual learning, demonstrating that simpler, shallow models combined with online planning can achieve superior performance. By establishing a theoretical foundation that guarantees immunity to forgetting, the study shifts the focus toward model-based planning strategies for CRL. The introduction of "Continual Bench" provides the field with a critical new tool for standardizing evaluation, potentially influencing future research directions toward more theoretically grounded and computationally efficient continual learning systems.

---

## Key Findings

*   **Immunity to Forgetting:** The proposed Follow-The-Leader (FTL) Online Agent (OA) is immune to catastrophic forgetting by construction, allowing it to learn new tasks indefinitely without losing proficiency in previous tasks.
*   **Theoretical Guarantees:** The online world model approach provides a proven regret bound of $\mathcal{O}(\sqrt{K^2D\log(T)})$, offering mathematical assurance regarding the agent's performance stability over time.
*   **Superior Performance:** Empirical results demonstrate that the proposed shallow model-based agent outperforms agents utilizing deep world models equipped with various continual learning techniques.
*   **Effective Planning:** The agent successfully solves a sequence of tasks by leveraging the latest online model for planning, validating the efficacy of the FTL Online Agent framework.
*   **New Benchmark:** The introduction of "**Continual Bench**" provides a robust and dedicated environment for testing and comparing Continual Reinforcement Learning (CRL) algorithms.

---

## Methodology

The research proposes a planning-centric approach that utilizes online world models to address continual learning. The specific methodology involves learning a **Follow-The-Leader (FTL) shallow model** online to capture world dynamics incrementally rather than relying on static or deep representations.

The agent plans actions using **Model Predictive Control (MPC)** based on the latest online model, allowing it to solve tasks specified by any reward function. The planner searches for actions based solely on the most recent model data, forming an FTL Online Agent that updates continuously without requiring retraining from scratch. The agent is evaluated against strong baselines within a consistent model-planning algorithmic framework to isolate the efficacy of the online learning component.

---

## Technical Details

*   **Unified Dynamics Model:** The approach proposes an 'Online Agent' (OA) that learns a unified world dynamics model ($P^u$) rather than task-specific models, handling non-stationarity through time-varying reward functions.
*   **Network Architecture:** It utilizes a **shallow but wide network** architecture ($y = W\sigma(Px)$) with:
    *   Learnable weights ($W$).
    *   A fixed random projection matrix ($P$).
    *   A high-dimensional hidden layer for universal approximation.
*   **Optimization Strategy:** The online learning algorithm follows a **Follow-The-Leader strategy** using regularized least squares. It is optimized via sparse incremental updates where only a small subset of active nodes ($K$) are modified to ensure constant low overhead.
*   **Planning Mechanism:** Planning is performed using **Model Predictive Control (MPC)** with the **Cross-Entropy Method (CEM)** to optimize action sequences.

---

## Contributions

*   **Theoretical Foundation:** The paper establishes a theoretical foundation demonstrating that planning with online world models is immune to catastrophic forgetting by construction, backed by a specific regret bound.
*   **FTL Online Agent Architecture:** Introduces the FTL Online Agent (OA), a novel architecture that combines online shallow world model learning with model predictive control to achieve continual adaptation.
*   **Benchmark Environment:** Develops "**Continual Bench**," a specialized environment designed specifically to facilitate the evaluation and comparison of Continual Reinforcement Learning methods.
*   **Empirical Evidence:** Provides empirical evidence that simpler shallow models, when combined with online planning, can outperform complex deep world models augmented with standard continual learning techniques.

---

## Results

The proposed FTL Online Agent achieves a proven regret bound of $\mathcal{O}(\sqrt{K^2D\log(T)})$ and is theoretically guaranteed to be no-regret under mild assumptions. Empirically, the agent is immune to catastrophic forgetting by construction, allowing it to learn indefinitely without losing proficiency, and it outperforms deep world models using various continual learning techniques. The paper also introduces 'Continual Bench,' a new benchmark environment designed for robust evaluation of Continual Reinforcement Learning (CRL) algorithms.