# GPU accelerated program synthesis: Enumerate semantics, not syntax!
*Martin Berger; NathanaÃ«l Fijalkow; Mojtaba Valizadeh*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Target Hardware** | Nvidia A100 (312 TFLOPs, 2.039 TB/s) |
| **Compute Intensity (CI)** | 153 (ops/byte) |
| **Performance Gain** | Up to 3 orders of magnitude vs. CPU |
| **Core Domains** | Regular Expressions, Linear Temporal Logic (LTLf) |
| **Citations** | 30 references |

---

## Executive Summary

> Program synthesis for formal languages such as Regular Expressions and Linear Temporal Logic (LTLf) is currently hindered by combinatorially explosive search spaces. Existing state-of-the-art methods rely on enumerating syntactic structures, a strategy that results in prohibitive computational overhead on traditional CPU architectures. This bottleneck severely limits scalability, rendering complex synthesis problems practically infeasible to solve within reasonable timeframes using current CPU-bound solutions.

> The authors introduce a GPU-accelerated synthesizer that fundamentally rethinks the search strategy through a two-pronged innovation: an **algorithmic shift** and a **hardware-specific mapping**. Algorithmically, the system moves from enumerating syntax (code structures) to enumerating semantics (meanings). It treats distinct programs as equivalent if they produce identical outputs, thereby collapsing the search space to a manageable set of unique behaviors. Technically, this is achieved using a bitvector representation of characteristic functions, which maps input traces to boolean outcomes. This enables "bit-parallelism," allowing the GPU to evaluate massive numbers of traces simultaneously via bitwise operations rather than sequential iteration.

> To fully utilize the hardware, the architecture employs a Bottom-Up Enumeration scheme adhering to **SIMD** (Single Instruction, Multiple Data) compliance. This design prioritizes lock-step execution to minimize data-dependent branching, ensuring the thousands of GPU cores operate in unison without stalling. Benchmarked on Nvidia A100 hardware, the system achieves a **Compute Intensity (CI) of 153** arithmetic operations per byte of data fetched. This high ratio is critical; the authors note a significant imbalance where a 32-bit addition takes 150 picoseconds, while on-chip data movement takes 400 picoseconds. By saturating the chip with far more calculations than data transfers, the system effectively hides memory latency.

> Consequently, the GPU-accelerated approach demonstrated speedups of up to **three orders of magnitude** over previous CPU state-of-the-art methods. A cost analysis using RISC-V instruction sets further confirmed that naive scalar compilation strategies fail to meet the necessary CI thresholds, validating the necessity of the proposed semantic architecture. This research establishes a new paradigm for hardware-accelerated formal methods, proving that deep learning-grade GPU hardware is highly effective for rigorous, search-based symbolic tasks. By identifying data movement and data-dependent branching as the primary bottlenecks, the authors provide a transferable blueprint for accelerating a wide range of Formal Methods workloads beyond just program synthesis.

---

## Key Findings

*   **Significant Performance Gains:** The GPU-accelerated synthesizer operates much faster than the previous CPU-based state-of-the-art methods.
*   **Improved Scalability:** The approach enables the handling of significantly larger synthesis problems that were likely infeasible for CPU-bound solutions.
*   **Efficacy of Semantic Enumeration:** Relying on the semantics of formulae rather than their syntax is a viable and superior strategy for parallel hardware environments.
*   **Broader Applicability:** The insights regarding GPU optimization (minimizing data movement and reducing data-dependent branching) have high potential for improving performance across other Formal Methods (FM) workloads.

---

## Methodology

The researchers developed a search-based program synthesizer specifically designed to run on GPU hardware. The system takes positive and negative example traces as input and returns a logical formula consistent with those traces.

The core technical strategy involves utilizing **GPU-friendly programming techniques** that prioritize the semantics of formulae over their syntax. This approach is specifically engineered to optimize GPU performance by:

1.  Minimizing data movement.
2.  Reducing data-dependent branching.

---

## Technical Details

| Component | Specification |
| :--- | :--- |
| **Core Strategy** | Semantic Enumeration for Programming by Example (PBE) for Regular Expressions and LTLf. |
| **Search Paradigm** | Shift from syntactic enumeration to search based on observational equivalence. |
| **Data Representation** | Uses **bitvector representation** via characteristic functions to map traces to boolean outcomes, drastically reducing search space complexity. |
| **Architecture** | **Bottom-Up Enumeration** on GPU via SIMD compliance. Utilizes lock-step execution and divergence minimization to parallelize independent tasks. |
| **Memory Optimization** | Maximizes **Compute Intensity (CI)** and ensures data locality to effectively hide latency. |
| **Synthesis Scheme** | Employs a recursive divide-and-conquer method to combine sub-components. |
| **Hardware Analysis** | Comparison of RISC-V compilation failure vs. required CI thresholds. |

---

## Results & Performance

Based on **Nvidia A100** hardware specifications (312 TeraFLOPs compute, 2.039 TBytes/s bandwidth), the system achieved the following:

*   **Compute Intensity (CI):** Targeted and achieved a CI of **153** (requiring 153 compute instructions per byte fetched).
*   **Latency Optimization:** Analysis reveals computation takes 150 ps for a 32-bit add versus 400 ps for on-chip movement. The architecture successfully mitigates this imbalance.
*   **Search Space Reduction:** The approach reduces the search space from infinite syntactic forms to finite semantic separations.
*   **Benchmark Comparison:** Naive RISC-V compilation fails to meet the required CI, whereas the proposed GPU architecture succeeds, handling problems previously deemed infeasible.

---

## Contributions

*   **Architectural Innovation:** A shift in program synthesis strategy from enumerating syntax to enumerating semantics to better align with parallel computing architectures.
*   **Performance Benchmarking:** Demonstrated that deep learning-grade GPU hardware can be effectively leveraged for formal search-based tasks, achieving speeds and scales unattainable by previous CPU standards.
*   **Formal Methods Optimization:** Provided a blueprint for accelerating formal methods workloads by identifying specific bottlenecksâ€”such as data movement and branchingâ€”and addressing them through semantic processing.

---
**References:** 30 citations