# COREVQA: A Crowd Observation and Reasoning Entailment Visual Question Answering Benchmark

*Ishant Chintapatla; Kazuma Choji; Naaisha Agarwal; Andrew Lin; Hannah You; Charles Duong; Kevin Zhu; Sean O'Brien; Vasu Sharma*

***

## ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Dataset Name** | COREVQA |
| **Image-Statement Pairs** | 5,608 |
| **Source Data** | CrowdHuman Dataset |
| **Top Model Accuracy** | < 80% |
| **Low Model Range** | 39.98% â€“ 69.95% |
| **Labeling Accuracy** | 100% (Manual Verification) |
| **Qualitative Score** | 9/10 |

***

## Executive Summary

### **Problem**
Current benchmarks for Vision-Language Models (VLMs) frequently overlook the specific capability of **visual entailment**â€”the ability to determine the validity of a statement based strictly on visual evidence, particularly in complex environments. While VLMs have shown proficiency in general tasks, they exhibit a critical deficiency in reasoning over crowded image-question pairs where occlusion and high entity density are present. This evaluation gap means that the true limitations of state-of-the-art models are masked, as existing datasets fail to stress-test reasoning capabilities in the chaotic scenarios common in real-world applications.

### **Innovation**
The researchers introduced **COREVQA**, a novel benchmark comprising 5,608 image-statement pairs specifically designed to evaluate visual entailment in crowded settings. The methodology employs a sophisticated prompt-engineering pipeline utilizing **Claude 3 Opus** to synthetically generate high-complexity "true" and "false" statement pairs derived from the CrowdHuman dataset. A key technical aspect of this innovation was the pivot from automated to manual labeling; after detecting an 11% error rate in automated ground truth labeling, the team ensured 100% accuracy through manual verification. The resulting dataset features linguistically complex statementsâ€”frequently using contrastive terms like "while" and "despite"â€”that require models to leverage subtle visual cues while ignoring misleading elements.

### **Results**
Benchmarking against COREVQA reveals substantial performance disparities and confirms that current state-of-the-art VLMs are not yet proficient at visual entailment in complex scenes. Even the top-performing models struggled to break the **80% accuracy threshold**, highlighting a significant ceiling in current capabilities. The performance gap is even more pronounced among lower-tier models, which achieved accuracy scores ranging only between **39.98% and 69.95%**. The dataset analysis further shows that models must navigate distinct challenges, including spatial reasoning (57.7% of statements), clothing identification (39.0%), and complex n-gram dependencies involving "person" and "holding."

### **Impact**
COREVQA establishes a critical quantitative baseline for visual entailment, proving a significant deficiency in current VLM architectures regarding complex, crowded scene analysis. By identifying that top models fail to reach 80% accuracy on this benchmark, the paper challenges the field to move beyond simple object detection toward high-density reasoning. This work will likely influence the development of future VLMs, providing a rigorous standard for improving performance in applications requiring detailed understanding of crowded environments, such as autonomous navigation, surveillance, and complex event monitoring.

***

## Key Findings

*   **Evaluation Gaps:** Existing Vision-Language Model (VLM) benchmarks frequently overlook visual entailment capability.
*   **Performance Ceiling:** Even top-performing VLMs struggle with visual entailment in crowded scenes, achieving accuracy below **80%**.
*   **Significant Performance Gap:** There is a substantial disparity in model performance; lower-performing models range between **39.98%** and **69.95%** accuracy.
*   **Critical Deficiency:** Current VLMs show a lack of complex reasoning capability when processing crowded image-question pairs.

***

## Methodology

To address the lack of rigorous testing for visual entailment, researchers developed the COREVQA benchmark. The construction process included:

1.  **Data Source:** Images were derived from the **CrowdHuman** dataset, selected specifically for challenging, crowded scenarios.
2.  **Statement Generation:** True/false statement pairs were synthetically generated to accompany the images.
3.  **Evaluation Task:** The benchmark requires models to determine the validity of statements based on visual evidence (visual entailment reasoning).
4.  **Dataset Scale:** The final benchmark comprises **5,608** image-statement pairs.

***

## Technical Details

The construction of the benchmark involved a custom pipeline focused on complexity and accuracy.

### **Pipeline Architecture**
*   **LLM Utilization:** A prompt-engineering pipeline utilizing **Claude 3 Opus**.
*   **Complexity Generation:** Uses iteratively refined 'true' and 'false' prompts. 'True' claims leverage subtle visual cues, while 'false' claims introduce misleading elements.
*   **Self-Reflection Mechanism:** The process includes a mechanism requiring the model to justify potential deception to ensure high-quality adversarial examples.

### **Ground Truth & Labeling**
*   **Automated Issue:** Initial automated ground truth labeling had an 11% error rate (89% accuracy).
*   **Manual Correction:** The final dataset switched to manual labeling to ensure **100% accuracy**.

### **Dataset Composition**
*   **Train01:** 4,927 images (87.9%)
*   **Train02:** 681 images (12.1%)

***

## Results

The analysis of the COREVQA dataset and benchmarking tests revealed specific linguistic and performance trends.

### **Dataset Linguistics**
*   **Syntactic Complexity:**
    *   "While" appears in **32.9%** of statements.
    *   "Despite" appears in **12.7%** of statements.
*   **Entity Focus:**
    *   Spatial terms: **57.7%**
    *   Clothing: **39.0%**
    *   Color: **35.1%**
*   **N-term Frequencies:**
    *   "Person": **47.3%**
    *   "Holding": **46.7%**
    *   "People": **35.4%**

### **Performance Analysis**
*   **Bias Identification:** Potential biases were identified for cameras (17.3%) and umbrellas (7.0%).
*   **Model Scores:**
    *   **Top Performers:** < 80% Accuracy.
    *   **Lower Performers:** 39.98% â€“ 69.95% Accuracy.

***

## Contributions

*   Identified critical evaluation gaps in current VLM benchmarks regarding visual entailment.
*   Introduced **COREVQA**, a novel specialized benchmark focused on reasoning within crowded environments.
*   Established a quantitative baseline showing that even state-of-the-art models are not yet proficient at visual entailment in complex scenes.

***

**Quality Score:** 9/10  
**References:** 15 citations