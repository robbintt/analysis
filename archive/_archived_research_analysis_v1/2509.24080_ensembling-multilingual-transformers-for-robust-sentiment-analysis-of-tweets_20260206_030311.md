---
title: Ensembling Multilingual Transformers for Robust Sentiment Analysis of Tweets
arxiv_id: '2509.24080'
source_url: https://arxiv.org/abs/2509.24080
generated_at: '2026-02-06T03:03:11'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Ensembling Multilingual Transformers for Robust Sentiment Analysis of Tweets

*Meysam Shirdel Bilehsavar; Negin Mahmoudi; Mohammad Jalili Torkamani; Kiana Kiashemshaki*

***

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 8/10
> *   **Citations:** 40 References
> *   **Accuracy Rate:** >86%
> *   **Core Architecture:** BERT + XLM-R (XLM-Roberta)
> *   **Languages Covered:** Multilingual (English, Spanish, French)
> *   **Task:** Sentiment Polarity Classification (Negative, Neutral, Positive)

***

## Executive Summary

This research addresses the persistent challenge of performing accurate sentiment analysis on multilingual social media data, particularly where labeled datasets for foreign or low-resource languages are scarce. Traditional sentiment analysis tools often struggle with the noisy, informal text characteristic of platforms like Twitter, and their performance typically degrades when processing languages other than English. The lack of extensive, annotated training data for non-English languages creates a significant bottleneck for organizations seeking global insights. Consequently, there is a critical need for robust methodologies that can generalize across languages and handle the distinct distributional properties of multilingual corpora without requiring exhaustive language-specific labeling efforts.

The authors introduce a novel ensemble framework that merges two pre-trained transformer-based models: `bert-base-multilingual-uncased-sentiment` and XLM-R (XLM-Roberta). The technical workflow employs a rigorous end-to-end preprocessing pipeline to sanitize noisy social media input by removing hyperlinks, user tags, and non-alphabetic symbols, followed by text normalization. The corpus is structured to map a 1-to-5-star rating scale into a three-class categorical format (Negative, Neutral, Positive) to enhance interpretability and mitigate class sparsity. By leveraging the complementary strengths of these specific multilingual transformers, the approach creates a more robust classifier capable of handling linguistic variations and noise inherent in cross-lingual datasets.

In experimental tasks, the proposed ensemble method achieved an overall accuracy exceeding 86%, demonstrating high capability in sentiment polarity classification across the evaluated languages. The study utilized a balanced dataset comprising English, Spanish, and French entries to prevent over-representation and bias toward specific emotional polarities. Disaggregated analysis further revealed distinct distribution patterns per language, accounting for cultural tendencies such as higher frequencies of neutral posts in certain regions. These results confirm that the ensemble effectively manages the complexities of multilingual text processing while maintaining high performance standards across the diverse linguistic inputs.

This research provides empirical evidence that ensemble methods utilizing pre-trained multilingual transformers offer a viable, high-accuracy solution for cross-lingual sentiment analysis without the prerequisite of extensive labeled datasets. The findings validate the efficacy of transformer-based architectures in handling multilingual nuances, effectively lowering the barrier to entry for sentiment analysis in low-resource languages. The methodology holds significant practical implications for fields requiring global sentiment monitoring, such as international marketing and political analysis, by establishing a reliable benchmark for automated, multilingual text classification.

***

## Key Findings

*   The proposed method achieved a sentiment analysis performance exceeding **86%** on the experimental tasks.
*   Combining pre-trained models (specifically **BERT** and **XLM-R**) into an ensemble yielded robust results for multilingual text processing.
*   The study demonstrates that effective sentiment analysis can be performed on foreign languages even **without extensive labelled data** for training.
*   The approach confirms that transformer-based models are highly capable of handling sentiment polarity classification across multiple languages.

## Methodology

The study utilizes an ensemble framework combining transformer models and Large Language Models (LLMs), specifically `bert-base-multilingual-uncased-sentiment` and XLM-R (XLM-Roberta). The model was trained and evaluated using a multi-language dataset, focusing on sentence-level sentiment assessment to identify text polarity (positive, negative, or neutral).

## Technical Details

The following table outlines the specific technical implementation of the proposed framework:

| Component | Description |
| :--- | :--- |
| **Core Architecture** | Ensemble of `bert-base-multilingual-uncased-sentiment` and **XLM-R** (XLM-Roberta). |
| **Preprocessing Pipeline** | End-to-end handling of noisy social media text. Includes removal of hyperlinks, user tags, unusual punctuation, excessive whitespace, and non-alphabetic symbols, followed by normalization. |
| **Data Structure** | Corpus organized as a three-column table containing: Raw text, Language tags, and Sentiment ratings. |
| **Label Transformation** | Original 1â€“5 star scale mapped to a 3-class categorical format:<br>â€¢ **Negative:** 1-2<br>â€¢ **Neutral:** 3<br>â€¢ **Positive:** 4-5 |
| **Dataset Balancing** | Balanced across sentiment categories, topics, and linguistic variation to prevent over-representation and ensure stable, generalizable model training. |

## Research Contributions

*   **Addressing Low-Resource Limits:** The research directly addresses the limitations of current sentiment analysis tools when applied to foreign languages or low-resource languages where labelled data is absent.
*   **Novel Ensemble Application:** Introduces a novel application of an ensemble model that merges `bert-base-multilingual-uncased-sentiment` with XLM-R to improve classification robustness.
*   **Empirical Validation:** Provides empirical evidence (with >86% accuracy) that ensemble methods using pre-trained multilingual transformers are a viable solution for cross-lingual sentiment analysis in domains like marketing and politics.

## Results

The proposed ensemble method achieved an overall accuracy exceeding **86%** on experimental tasks. The models demonstrated high capability in handling sentiment polarity classification across multiple languages within the dataset. The study utilized a balanced dataset to serve as a benchmark and prevent bias toward specific emotional polarities. Disaggregated analysis highlighted distribution differences per language, accounting for cultural tendencies such as higher percentages of neutral posts in some languages. Input data characteristics include multilingual entries (English, Spanish, French) with mapped sentiment annotations.