# Large Language Models for Combinatorial Optimization: A Systematic Review

*Authors: Francesca Da Ros; Michael Soprano; Luca Di Gaspero; Kevin Roitero*

---

> **QUICK FACTS**
> *   **Publications Screened:** > 2,000
> *   **Primary Studies Identified:** 103
> *   **Study Scope:** Discrete Combinatorial Optimization (CO)
> *   **Methodology:** PRISMA 2020 Guidelines
> *   **Quality Score:** 7/10
> *   **References:** 40 Citations

---

## Executive Summary

This paper addresses the fragmented and rapidly expanding landscape of research applying Large Language Models (LLMs) to discrete Combinatorial Optimization (CO) problems. As LLMs transition from text processors to tools for solving NP-hard challenges, the field lacks a holistic structure, with current literature reviews focusing narrowly on isolated sub-steps like algorithm generation rather than the end-to-end optimization pipeline. This fragmentation creates a significant barrier to entry for researchers aiming to leverage modern deep learning architectures for complex decision-making. Consequently, there is a pressing need for a unified framework to understand the efficacy of these models across the full spectrum of optimization tasks.

The primary innovation is a comprehensive taxonomy that classifies the intersection of LLMs and CO into three semantic categories: tasks, architectures, and datasets. Methodologically, the study adhered to PRISMA 2020 guidelines, querying Scopus and Google Scholar and applying specific filters based on four inclusion and four exclusion criteria regarding language, research focus, publication year, and type. Technically, the review analyzes accurate Transformer-based architectures—ranging from encoder-only models like BERT to decoder-only variants—correctly distinguishing them from older recurrent architectures. The analysis focuses on how these transformer structures translate natural language into formal mathematical models, such as linear, integer, and mixed-integer linear programming, as well as constraint programming frameworks.

The systematic review screened a corpus of over 2,000 publications to isolate 103 primary studies, a scope approximately five times larger than previous surveys. These studies were taxonomized to answer six qualitative research questions regarding application methods and training paradigms. The analysis reveals a clear trend toward utilizing LLMs as functional components across the entire CO process rather than solely for text processing. By covering the full optimization pipeline, the study provides quantitative insights into current trends and establishes a technical classification of the datasets and architectures currently dominating the field, covering NP-hard problem classes including the Traveling Salesperson Problem and Boolean Satisfiability.

This work provides a foundational roadmap for future research, bridging the gap between the semantic understanding of LLMs and the rigor of classical operations research. By identifying specific application fields and detailing architectural taxonomies, the authors establish a state-of-the-art overview that will likely accelerate the standardization of benchmarks and evaluation metrics. The study serves as a critical reference for developing next-generation hybrid AI solvers, guiding both academic inquiry and industrial application in complex domains such as logistics, scheduling, and resource allocation.

---

## Key Findings

*   **Significant Filtering Process:** The review conducted a rigorous screening of over **2,000 publications**, eventually narrowing the scope down to **103 primary studies** directly relevant to LLMs in Combinatorial Optimization (CO).
*   **Structured Taxonomy:** The body of research was successfully taxonomized into semantic categories focusing on:
    *   Tasks performed by LLMs
    *   Architectures employed
    *   Evaluation datasets utilized
*   **Field Identification:** The study identifies and maps specific fields of application where LLMs are currently being utilized to solve CO problems.
*   **Future Directions:** The authors establish clear future directions for leveraging LLMs within the optimization domain, providing a path forward for subsequent research.

---

## Methodology

The research design followed strict protocols to ensure a comprehensive and unbiased review:

*   **Guidelines:** Adherence to **PRISMA guidelines** for structured reporting.
*   **Data Sources:** A comprehensive literature search was conducted using **Scopus** and **Google Scholar**.
*   **Screening Criteria:** Publications were screened against a predefined set of filters:
    *   **4 Inclusion Criteria** (based on language, research focus, publication year, type)
    *   **4 Exclusion Criteria** (based on language, research focus, publication year, type)
*   **Classification:** Selected studies were systematically classified into semantic categories and topics to provide a structured overview of the landscape.

---

## Technical Details

### Scope and Definition
*   **Framework:** Systematic review based on **PRISMA 2020** guidelines.
*   **Focus:** Application of LLMs to **discrete Combinatorial Optimization**.
*   **Exclusions:** Continuous optimization and reverse applications of CO (using CO to solve LLM problems) were excluded from the analysis.

### Architectures Analyzed
*   **Core Structure:** Analyzed architectures are based on the **Transformer** structure.
*   **Components:** Utilizes encoders, decoders, embedding layers, and positional encoding.
*   **Model Types:**
    *   **Encoder-only** (e.g., BERT)
    *   **Contextual Embeddings** (e.g., ELMo)

### Pipeline and Formulations
*   **Translation Process:** The CO pipeline involves translating **Natural Language** descriptions into formal models.
*   **Model Definitions:** Formal models are defined by variables, constraints, and objective functions.
*   **Frameworks:** Specifically focuses on:
    *   LP (Linear Programming)
    *   ILP (Integer Linear Programming)
    *   MILP (Mixed-Integer Linear Programming)
    *   CP (Constraint Programming)

### Problem Classes
*   The analysis targets **NP-hard problem classes**, including:
    *   **SAT** (Boolean Satisfiability)
    *   **PFSP** (Permutation Flowshop Scheduling Problem)
    *   **KP** (Knapsack Problem)
    *   **GCP** (Graph Coloring Problem)
    *   **TSP** (Traveling Salesperson Problem)

---

## Results

*   **Comparative Scope:** The review covers a significantly broader scope than previous surveys, analyzing **103 papers** compared to 20 in a prior study by Huang et al.
*   **Holistic Coverage:** Unlike previous works that focused on narrow sub-steps (e.g., algorithm generation), this study addresses the **entire CO process**.
*   **Research Questions:** The findings establish answers to **six primary qualitative research questions** regarding:
    *   Application methods
    *   Training paradigms
    *   Emerging trends
*   **Categorization:** The 103 studies were effectively grouped into three main categories: Tasks, Architectures, and Datasets.

---

## Contributions

1.  **Comprehensive Taxonomy:** Provides a detailed taxonomy of the intersection between LLMs and CO, explicitly detailing tasks, architectures, and datasets.
2.  **State-of-the-Art Overview:** Offers a holistic summary of the current research landscape and application fields.
3.  **Research Roadmap:** Identifies critical future research directions, serving as a guide for subsequent academic and industrial efforts.

---

**Research Analysis Quality Score:** 7/10  
**Total Citations:** 40