---
title: 'ATLAS: Automated Toolkit for Large-Scale Verified Code Synthesis'
arxiv_id: '2512.10173'
source_url: https://arxiv.org/abs/2512.10173
generated_at: '2026-02-03T13:36:09'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# ATLAS: Automated Toolkit for Large-Scale Verified Code Synthesis

*Mantas Baksys; Stefan Zetzsche; Olivier Bouissou; Remi Delmas; Soonho Kong; Sean B. Holden*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Base Model:** Qwen 2.5 7B Coder
> *   **Dataset Generated:** 2,700 verified Dafny programs
> *   **Training Examples:** 19,000 granular examples (via task decomposition)
> *   **DafnySynthesis Accuracy:** +50.0 percentage points (15.8% â†’ 65.8%)
> *   **DafnyBench Accuracy:** +24.5 percentage points (32.4% â†’ 56.9%)
> *   **Citations:** 40

---

## Executive Summary

Formal verification offers the gold standard for software correctness by requiring machine-checked proofs, yet it remains prohibitively difficult to automate. A primary bottleneck is the extreme scarcity of high-quality training data; unlike general programming, verified code requires rigorous specifications and proofs that are rarely available in open repositories.

This paper addresses the critical challenge of data scarcity in formal verification, aiming to enable Large Language Models (LLMs) to generate formally verified code in languages like Dafny without relying on expensive, expert-curated datasets. The core innovation is the **ATLAS (Automated Toolkit for Large-Scale Verified Code Synthesis)** pipeline, a system that automatically converts standard Python solutions into verified Dafny programs complete with formal specifications and proofs.

Leveraging the TACO dataset of Python problems, ATLAS translates code and generates the necessary verification artifacts, which are then rigorously checked. To optimize learning, the pipeline employs task decomposition, breaking down the 2,700 synthesized verified programs into 19,000 granular training examples.

Fine-tuning the Qwen 2.5 7B Coder model on the ATLAS dataset yielded substantial performance improvements across formal verification benchmarks. On DafnyBench, accuracy improved from 32.4% to 56.9%, representing a 24.5 percentage point increase. The impact was even more significant on DafnySynthesis, where accuracy surged from 15.8% to 65.8%â€”a 50 percentage point gain.

This research demonstrates that synthetic data generation is a viable, scalable strategy for overcoming the data bottleneck in formal verification. By releasing the ATLAS toolkit and the resulting dataset, the authors provide the community with a reproducible method to create massive verified codebases without manual expert intervention.

---

## Key Findings

*   **Significant Accuracy Gains:** Fine-tuning Qwen 2.5 7B Coder on the ATLAS dataset drastically improved performance:
    *   **DafnyBench:** Improved from **32.4%** to **56.9%**.
    *   **DafnySynthesis:** Improved from **15.8%** to **65.8%**.
*   **Successful Corpus Generation:** The ATLAS pipeline successfully generated a corpus of **2,700 verified Dafny programs**, complete with specifications and proofs, derived from Python solutions.
*   **Synthetic Data Viability:** The study proves that synthetic data generation is a viable strategy for overcoming training data scarcity in the field of formal verification.
*   **Task Decomposition Efficiency:** Through task decomposition, the pipeline extracted **19,000 granular training examples** from the synthesized programs, optimizing the model learning process.

---

## Methodology

The researchers utilized the ATLAS automated pipeline to synthesize verified programs, leveraging the TACO dataset of Python solutions as the foundational source material. The process involved several critical steps:

1.  **Translation:** Converting Python solutions into Dafny programs.
2.  **Verification Artifacts:** Ensuring formal specifications and machine-checked proofs were included in the output.
3.  **Task Decomposition:** Breaking down verified programs into distinct, granular training examples (19,000 total) to facilitate more effective learning.
4.  **Evaluation:** The Qwen 2.5 7B Coder model was fine-tuned using this synthetic dataset to evaluate its formal verification capabilities against established benchmarks.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Base Model** | Qwen 2.5 7B Coder |
| **Training Method** | Fine-tuning on the proprietary ATLAS dataset |
| **Data Source** | TACO dataset (Python solutions) |
| **Data Pipeline** | Translates Python to Dafny, utilizes synthetic data generation, ensures specifications/proofs are verified, and employs task decomposition. |
| **Corpus Stats** | Generated 2,700 verified Dafny programs and extracted 19,000 granular training examples. |

---

## Results

### Benchmark Performance
*   **DafnyBench Accuracy:**
    *   Baseline: 32.4%
    *   ATLAS Fine-tuned: 56.9%
    *   **Change:** +24.5 percentage points
*   **DafnySynthesis Accuracy:**
    *   Baseline: 15.8%
    *   ATLAS Fine-tuned: 65.8%
    *   **Change:** +50.0 percentage points

### Pipeline Output
*   **2,700** verified programs generated.
*   **19,000** granular training examples extracted.

---

## Contributions

*   **ATLAS Toolkit:** Introduced an open automated toolkit capable of transforming standard code into verified code accompanied by formal proofs.
*   **Dataset Release:** Created and released a large-scale dataset of **2.7K verified Dafny programs** with specifications.
*   **Empirical Validation:** Provided empirical evidence demonstrating that LLMs can be effectively scaled for formal verification tasks using synthetic data.