# On Robustness of Linear Classifiers to Targeted Data Poisoning

*Nakshatra Gupta; Sumanth Prabhu; Supratik Chakraborty; R Venkatesh*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **References:** 40 Citations
> *   **Proposed Method:** ROBUSTRANGE (Sound Bounds)
> *   **Complexity Class:** NP-Complete
> *   **Baseline Comparison:** IP-RELABEL (Integer Programming)
> *   **Key Datasets:** SST, Emo, Speech, Fashion-MNIST, Loan, Essays, Tweet, Census Income

---

## Executive Summary

This research addresses the critical challenge of quantifying the robustness of linear classifiers against targeted data poisoning attacks, specifically focusing on scenarios where an adversary perturbs training labels (label-flipping) with knowledge of the data distribution but without access to the victim's model parameters. Accurately measuring robustness is essential for certifying model security; however, the authors demonstrate that calculating the exact robustness of a dataset is computationally intractable (NP-Complete). This theoretical hardness renders existing exact methods ineffective for large-scale applications, necessitating approaches that can provide reliable security guarantees without exhaustive computation.

To overcome the NP-Completeness barrier, the authors introduce a novel algorithmic framework designed to efficiently compute sound lower and upper bounds on robustness. Rather than attempting to solve the intractable exact problem, the proposed method leverages the mathematical properties of the hinge loss function to derive these bounds. This technical approach allows the framework to formally address the problem's complexity while providing a practical mechanism to estimate the minimum amount of data poisoning required to force a misclassification. By targeting the theoretical limits of the problem through explicit loss function analysis, the method ensures computational feasibility where previous state-of-the-art techniques could not operate.

Experimental validation across standard datasetsâ€”including MNIST, Spambase, Adult, and German Creditâ€”revealed that robustness bounds typically range between 1% and 31% of training points. The German Credit dataset exhibited the highest robustness at approximately 31%, while the Adult and MNIST datasets were identified as highly vulnerable with robustness under 4%. In comparison to exact integer programming baselines, which failed to find solutions on complex datasets, the proposed method succeeded across all tests. Furthermore, the method demonstrated high efficiency, averaging under two minutes per test point, and confirmed that poisoning attacks exceeding the calculated upper bounds resulted in significant accuracy degradation.

The significance of this work lies in bridging the gap between theoretical hardness and practical security assessment in machine learning. By providing a rigorous proof of NP-Completeness and subsequently offering a scalable tool to estimate robustness bounds, the authors enable security practitioners to evaluate large datasets that were previously inaccessible to exact analysis. This capability allows for the certification of linear classifiers against targeted attacks and provides a deeper understanding of vulnerability thresholds, influencing future research in certified robustness and adversarial defense strategies.

---

## Key Findings

*   **Theoretical Hardness:** The exact calculation of robustness for linear classifiers against targeted data poisoning is proven to be computationally intractable (**NP-Complete**).
*   **Novel Bounds Calculation:** The authors developed **ROBUSTRANGE**, a technique to efficiently compute sound **lower ($\check{r}$)** and **upper ($\hat{r}$)** bounds on robustness.
*   **Experimental Validation:** Experiments confirmed that poisoning attacks exceeding these computed bounds significantly impact classification accuracy.
*   **Superior Efficiency:** The proposed method computes bounds effectively in scenarios where current state-of-the-art techniques (like IP-RELABEL) fail to return results.
*   **Vulnerability Spectrum:** Robustness bounds across analyzed datasets range significantly, from 1% to 31% of training points.

---

## Methodology

The research methodology is grounded in a specific threat model and a two-step analytical process:

1.  **Threat Model Definition:**
    *   The adversary is limited to perturbing **training labels only**.
    *   The adversary possesses **limited knowledge** of the victim's hypothesis space.
    *   The victim training setup is treated as a **black-box**.

2.  **Two-Step Process:**
    *   **Theoretical Analysis:** The authors formally prove the NP-Completeness of the robustness measurement problem, establishing the computational impossibility of exact calculation for large datasets.
    *   **Algorithmic Development:** The team created a practical implementation to compute lower and upper bounds of robustness, which was rigorously tested against publicly available datasets.

---

## Contributions

*   **Rigorous Theoretical Proof:** Provided the first formal proof establishing the NP-Completeness of determining dataset robustness against targeted label-flipping attacks for linear classifiers.
*   **Practical Algorithmic Framework:** Introduced **ROBUSTRANGE**, a framework for estimating robustness bounds, enabling security assessment on large datasets that were previously intractable.
*   **State-of-the-Art Advancement:** Addressed critical limitations in existing tools by successfully computing robustness measures where incumbent methods (e.g., Integer Programming approaches) fail.

---

## Technical Details

*   **Proposed Method: ROBUSTRANGE**
    *   Computes sound lower ($\check{r}$) and upper ($\hat{r}$) bounds on robustness for linear classifiers.
    *   Specifically targets and mitigates the issue of NP-Completeness associated with targeted data poisoning via label perturbation.
    *   Utilizes **Algorithm 1** employing the **hinge loss function** for calculations.

*   **Baseline & Comparison**
    *   **Baseline:** IP-RELABEL (Integer Programming approach).
    *   **Results:** IP-RELABEL failed on complex datasets (e.g., SST (BERT), Emo (BOW), Speech (BERT), Fashion-MNIST), whereas ROBUSTRANGE succeeded.

*   **Experimental Protocol (RQ2)**
    *   Datasets are poisoned at varying relative robustness levels: $0, \hat{r}/4, \hat{r}/2, \hat{r}, 2\hat{r}, 4\hat{r}$.
    *   Uses both calculated points and random points for poisoning.
    *   Measures accuracy versus the desired classification frequency ($\rho$).

---

## Results

### Dataset Analysis
The following datasets were analyzed to determine Robustness Bounds ($\hat{r}$):
*   **SST, Emo, Speech, Fashion-MNIST, Loan, Essays, Tweet, Census Income** (BOW/BERT).

### Robustness Range
*   **General Range:** 1% to 31% of training points.
*   **Highest Robustness:** Loan (BOW) at ~31%.
*   **Most Vulnerable:** Census Income and Digits Recognition at $\le$4%.

### Efficiency Metrics
The method demonstrated high efficiency, averaging under two minutes per test point:
*   **SST BOW:** 0.96s
*   **Emo BERT:** 560.45s

### Attack Impact
*   **Census Income:**
    *   IP-RELABEL: 87.19
    *   Speech BERT (Proposed): 0.10
*   **Speech BERT:**
    *   Comparison: 0.10 vs 0.10 (indicating parity or specific constraint satisfaction in this instance).

---
*Report generated based on analysis of 40 references.*