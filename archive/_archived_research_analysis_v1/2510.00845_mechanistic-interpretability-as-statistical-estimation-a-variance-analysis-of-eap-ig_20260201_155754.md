# Mechanistic Interpretability as Statistical Estimation: A Variance Analysis of EAP-IG

*Maxime MÃ©loux; FranÃ§ois Portet; Maxime Peyrard***

---

> ### ðŸ“Š Quick Facts
> | Metric | Details |
> | :--- | :--- |
> | **Quality Score** | 8/10 |
> | **Total References** | 24 Citations |
> | **Models Studied** | GPT-2 Small, Pythia-160M, Pythia-410M |
> | **Core Method** | EAP-IG (Edge Attribution Patching with Integrated Gradients) |
> | **Key Metrics** | Jaccard Index, Multi-Dimensional Scaling (MDS) |

---

## Executive Summary

This research addresses a fundamental **reliability crisis** in Mechanistic Interpretability (MI) by challenging the deterministic assumption underlying circuit discovery methods like Edge Attribution Patching with Integrated Gradients (EAP-IG). The authors demonstrate that because the ground truth for circuits is non-identifiable, current validation practices are insufficient, raising the possibility that published circuits are mere artifacts of specific runs rather than robust features of the model. This "instability gap" poses a severe barrier to the scientific validity of MI, necessitating a pivotal shift from qualitative interpretation to rigorous statistical proof.

The paperâ€™s primary innovation is the **conceptual reframing of interpretability methods as statistical estimators** subject to variance and noise. Technically, the authors ground this approach in Causal Mediation Analysis and Activation Patching, treating circuit discovery as an estimation problem where stability is paramount. They introduce a systematic stability framework that rigorously tests robustness across diverse conditions, including input resampling, prompt paraphrasing, noise injection within the causal analysis process, and variations in hyperparameters.

Empirical results from experiments on GPT-2 Small, Pythia-160M, and Pythia-410M across three distinct tasks demonstrate that EAP-IG suffers from **severe instability**. Quantitative analysis utilizing the Jaccard Index to measure pairwise similarity revealed consistently low structural overlap, with scores often falling below 30%, indicating that circuits vary drastically across runs. This quantification proves that hyperparameter sensitivity is a primary source of error; varying hyperparameters or data samples yields substantially different circuit structures, and no single EAP method variant consistently produced lower variance across the tested conditions.

These findings fundamentally challenge the field's reliance on black-box performance metrics like Faithfulness or Sufficiency as sole validators of mechanistic claims. The authors argue that such metrics are scientifically insufficient for validation and propose a new standard of best practices, mandating the routine reporting of stability metrics and variance analyses alongside interpretability results. By establishing that interpretability is an estimation problem subject to statistical variance, this work demands a paradigm shift, transforming MI from a discipline of isolated, qualitative case studies into one grounded in reproducible, statistically rigorous science.

---

## Key Findings

*   **High Structural Variance:** The state-of-the-art circuit discovery method, EAP-IG, exhibits significant instability and high structural variance across different runs and conditions.
*   **Hyperparameter Sensitivity:** Findings produced by EAP-IG are highly sensitive to variations in hyperparameters, raising questions about reliability.
*   **Statistical Nature of Interpretability:** Interpretability methods act as statistical estimators subject to variance, robustness issues, and noise rather than deterministic truths.
*   **Need for Reliability Metrics:** Reliance on black-box performance metrics is insufficient; assessing reliability through stability analysis is crucial for scientific rigor.

---

## Methodology

The authors approach Mechanistic Interpretability and circuit discovery as problems of **statistical estimation**, focusing on analyzing variance and robustness. The paper conducts a systematic stability analysis of the EAP-IG (Edge Attribution Patching with Integrated Gradients) method across a diverse set of models and tasks.

Robustness is tested using a comprehensive suite of controlled perturbations, including:
*   Input resampling
*   Prompt paraphrasing
*   Hyperparameter variation
*   Noise injection within the causal analysis process

---

## Technical Details

The paper frames Mechanistic Interpretability as **Statistical Estimation**, grounded in Causal Mediation Analysis and Activation Patching.

*   **Core Method:** Edge Activation Patching (EAP) combines interventions with gradient-based attribution to quantify edge influence.
*   **Specific Variant:** EAP-IG utilizes Integrated Gradients for scoring.
*   **Stability definitions:**
    *   **Variance:** Sensitivity to input data resampling.
    *   **Robustness:** Stability under changes to the analytical setup or hyperparameters.
*   **Evaluation Metrics:** Relies on proxy metricsâ€”Faithfulness, Sufficiency/Predictive Power, and Sparsity/Minimalityâ€”due to the non-identifiability of ground truth circuits.

---

## Results

Experiments conducted on three tasks and three model architectures, including a specific case study on **gpt2-small**, revealed the following:

*   **High Structural Variance:** EAP-IG exhibits significant instability, where varying hyperparameters or data samples yields substantially different circuit structures.
*   **Method Variance:** No single EAP method variant consistently produced lower variance.
*   **Quantitative Analysis:**
    *   Utilized the **Jaccard Index** to pairwise compare circuit similarity.
    *   Used **Multi-Dimensional Scaling (MDS)** to project circuits into 2D space for visualizing clustering and variance, with axes ranging approximately from -0.6 to 0.6.

---

## Contributions

*   **Conceptual Framework:** Introduces a novel perspective that treats interpretability methods as statistical estimators, providing a theoretical basis for questioning stability.
*   **Critical Evaluation of EAP-IG:** Provides the first systematic stability analysis of the EAP-IG method, empirically demonstrating its limitations regarding variance and sensitivity.
*   **Best-Practice Guidelines:** Proposes recommendations advocating for the routine reporting of stability metrics to ensure the field develops into a rigorous, statistically grounded science.