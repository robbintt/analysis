# Acceleration Multiple Heads Decoding for LLM via Dynamic Tree Attention

*Zhendong Zhang*

---

> **### Quick Facts**
> *   **Model Tested:** Vicuna-7B
> *   **Framework:** MEDUSA
> *   **Benchmark:** MT-Bench
> *   **Top Efficiency Gain:** +6.4% tokens-per-inference (MEDUSA-1)
> *   **Quality Impact:** Negligible degradation (~0.04-0.07 pt drop)
> *   **Core Complexity:** O(Knm log n)

---

## Executive Summary

This paper addresses the inefficiency of static verification structures in speculative decoding for Large Language Models (LLMs), specifically within the context of the MEDUSA framework. While Multiple Heads Decoding offers a pathway to accelerate inference, existing approaches rely on fixed tree structures that fail to adapt to context-dependent variations during generation. This rigidity limits the potential for parallel processing, as the static trees cannot optimally represent the shifting probability distributions of candidate tokens in real-time.

To overcome these limitations, the authors introduce **"Dynamic Tree Attention,"** an architectural innovation that replaces static trees with structures constructed in real-time during inference. The method generates candidates by approximating the joint distribution using the Cartesian product of marginal distributions from multiple decoding heads. It employs a priority queue to select the top candidates while ensuring structural integrity.

Empirical results demonstrate that Dynamic Tree Attention improves tokens-per-inference efficiency significantly while maintaining generation quality. Although the unoptimized implementation showed a latency overhead, the core metric of decoding throughput improved, validating that dynamic structures can outperform fixed ones in LLM inference.

---

## Key Findings

*   **Enhanced Efficiency:** The proposed dynamic tree attention method improves the decoding efficiency of LLMs using multiple heads decoding.
*   **Quality Preservation:** Despite the increase in speed, the method maintains the generation quality of the model, indicating no degradation in output.
*   **Superiority over Fixed Structures:** Replacing fixed tree attention with dynamic tree attention yields better results, suggesting that fixed structures are suboptimal for candidate generation.
*   **Validation in MEDUSA:** Preliminary experiments confirm the potential of this approach specifically within the context of the MEDUSA framework.
*   **Low Complexity:** The improvements are achieved through a simple strategy that adds minimal computational complexity.

---

## Methodology

The researchers focused on modifying the architectural mechanism of multiple heads decoding, specifically applied to the MEDUSA framework.

*   **Architectural Shift:** Instead of relying on the standard approach that uses a fixed tree structure to verify candidate sequences in parallel, the authors implemented a dynamic tree attention mechanism.
*   **Real-Time Construction:** To support this, they developed a specific, low-complexity strategy designed to generate candidates and construct the dynamic tree structure in real-time during the inference process.

---

## Technical Details

### Core Mechanism
The paper proposes Dynamic Tree Attention to replace fixed tree structures in MEDUSA, better handling context-dependent variations. It approximates the joint distribution using the Cartesian product of marginal distributions from multiple decoding heads.

### Algorithmic Parameters
| Parameter | Value | Description |
| :--- | :--- | :--- |
| **Decoding Heads (K)** | 4 | Number of heads used for marginal distributions. |
| **Top Predictions (m)** | 32 | Top predictions taken from each head. |
| **Total Candidates (n)** | 64 | Final candidates selected via priority queue. |
| **Selection Method** | Priority Queue | Identifies top candidates based on probability products. |

### Complexity Analysis
*   **Candidate Selection:** Calculated using the product of probabilities for sequences. Complexity is **O(Knm log n)**.
*   **Buffer Preparation:** Involves preparing position embeddings and masks. Complexity is **O(Kn)**.
*   **Structural Integrity:** The algorithm ensures parent nodes are selected if child nodes are selected, maintaining tree validity.

---

## Results

Experiments conducted on **Vicuna-7B** using the **MT-Bench** benchmark yielded the following outcomes:

*   **Efficiency Gains (Tokens-per-Inference):**
    *   **MEDUSA-1:** Improved by **6.4%** (increasing from 2.50 to 2.66).
    *   **MEDUSA-2:** Improved by **5.7%** (increasing from 3.32 to 3.51).
*   **Quality Retention:**
    *   Generation quality remained effectively stable with negligible degradation (approx. **0.04-0.07 point drop** in MT-Bench scores).
*   **Latency Considerations:**
    *   The unoptimized implementation showed a **10% increase in wall-clock latency** due to the overhead of dynamic tree construction.

---

## Contributions

*   **Architectural Innovation:** Introduction of "Dynamic Tree Attention" to replace static "Fixed Tree Attention" in the context of multiple heads decoding, altering how parallel candidate sequences are verified.
*   **Algorithmic Optimization:** A simple, low-complexity algorithm for constructing dynamic tree structures and generating candidates, making the approach practical for deployment.
*   **Performance Benchmarking:** Provided empirical evidence that dynamic structures can outperform fixed ones in balancing decoding speed and generation quality, establishing a new direction for optimizing inference in LLMs.

---

**Quality Score:** 9/10  
**References:** 5 citations