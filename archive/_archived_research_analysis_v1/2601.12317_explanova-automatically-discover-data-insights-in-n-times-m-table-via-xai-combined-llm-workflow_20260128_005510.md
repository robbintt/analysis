---
title: 'Explanova: Automatically Discover Data Insights in N \times M Table via XAI
  Combined LLM Workflow'
arxiv_id: '2601.12317'
source_url: https://arxiv.org/abs/2601.12317
generated_at: '2026-01-28T00:55:10'
quality_score: 7
citation_count: 23
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Explanova: Automatically Discover Data Insights in N \times M Table via XAI Combined LLM Workflow

*Our Explanova, Local Small, Like Deep, Data Insights, Reference Format, Yiming Huang, Automatically Discover*

---

> ### ðŸ“Š Quick Facts
> 
> *   **Quality Score:** 7/10
> *   **References:** 23 Citations
> *   **Architecture Type:** Hybrid XAI + Local Small LLM
> *   **Core Strategy:** Deterministic AutoML Pipeline (Systematic Traversal)
> *   **Key Algorithms:** HDBSCAN (Gower Distance), Kernel SHAP
> *   **Benchmarks:** DiscoveryBench, InfiAgent-DABench

---

## Executive Summary

This research addresses the critical inefficiency and unpredictability of current state-of-the-art automated data insight discovery, which relies heavily on resource-intensive agentic Large Language Model (LLM) frameworks. These dynamic agent-based approaches suffer from high computational costs and non-deterministic complexity, rendering them impractical for many organizations. The paper highlights the urgent need for a deterministic, cost-effective alternative capable of systematically analyzing tabular data ($N \times M$ tables) without the overhead of large, proprietary models. The authors argue that for automated analytics to be scalable, it must move away from volatile tool-calling agents toward a more structured, rigorous methodology.

The key innovation, **Explanova**, is a novel hybrid framework that integrates Explainable AI (XAI) with Local Small LLMs through a deterministic, AutoML-like workflow. Unlike agentic models, Explanova utilizes a "Systematic Traversal" method to exhaustively cover data exploration paths, ensuring comprehensive analysis of univariate statistics, bivariate relationships, and target-to-all interactions. The architecture is built on a robust three-stage pipeline:

1.  **Feature Preparation:** Employing parallelized LLM-based type inference and imputation.
2.  **Feature-to-Feature Statistics:** Utilizing HDBSCAN clustering with a Gower distance matrix.
3.  **Feature Modeling:** Using a "One-vs-All" strategy interpreted via Kernel SHAP.

This design effectively separates heavy statistical computation from language generation, streamlining the process.

To validate performance, the system is benchmarked against **DiscoveryBench** and **InfiAgent-DABench**, demonstrating that the preset workflow effectively maintains data integrity and insight quality. While the framework reduces processing overhead by decoupling statistical analysis from LLM generation, it ensures analytical rigor through specific quantitative internal metrics rather than opaque model outputs. Key performance indicators include Missing Value Ratio and Outlier Ratio thresholds to filter noise, SSE (Sum of Squared Errors) thresholds to validate clustering stability within HDBSCAN, and a defined "Credibility Score" to assess the reliability of discovered insights.

Explanova represents a paradigm shift in automated analytics, proving that high-quality insight generation does not require expensive, large-scale agentic LLMs. By leveraging Local Small LLMs within a structured XAI workflow, the study offers a scalable solution that democratizes access to advanced data analysis for organizations with limited computational resources.

---

## Key Findings

*   **Current Limitations:** State-of-the-art solutions primarily rely on agentic LLM frameworks with tool-calling capabilities, which introduce unpredictability and high costs.
*   **Workflow Efficiency:** Automated data analysis can be achieved via a preset **AutoML-like workflow** that systematically traverses data explorations rather than relying on dynamic agent behavior.
*   **Cost Reduction:** The proposed 'Explanova' solution utilizes a **Local Small LLM**, offering a significantly cheaper alternative to larger agentic models.
*   **Comprehensive Discovery:** The system effectively discovers insights by exhaustively covering:
    *   Statistical properties
    *   Pairwise relationships
    *   Global column interactions

---

## Methodology

The methodology proposes a **Hybrid XAI (Explainable AI) combined LLM workflow**. The core philosophy shifts away from autonomous agents toward a structured pipeline:

*   **Workflow Type:** Deterministic, AutoML-style pipeline (Preset Workflow) vs. agent-based decisions.
*   **Exploration Strategy:** Systematic Traversal of exploration paths, including:
    *   Univariate statistics
    *   Bivariate relationships
    *   Target-to-all relationships
*   **Role of LLM:** A Local Small LLM is used specifically for **Explanation Generation** to interpret statistical results and generate natural language insights.

---

## Technical Details

Explanova replaces agentic LLM frameworks with a preset, AutoML-like workflow designed to leverage Local Small LLMs. The architecture features a **3-stage pipeline**:

1.  **Feature Preparation**
    *   Parallelized LLM-based type inference.
    *   Binarization into Continuous/Discrete types.
    *   Missing value handling via dropping or mean/median imputation.
2.  **Feature-to-Feature Statistics**
    *   Calculation of pairwise relationships.
3.  **Feature Modeling**
    *   Analysis of feature dependencies and predictive power.

**Key Technical Implementations:**
*   **Clustering:** Uses **HDBSCAN** clustering utilizing a **Gower distance matrix** (validated by SSE thresholds).
*   **Interpretability:** Uses a **"One-vs-All"** modeling strategy explained via **Kernel SHAP** to extract insights.

---

## Contribution Summary

*   **Explanova Framework:** A novel system that automates data insight discovery in tables without relying on resource-intensive 'agentic' tool-calling.
*   **Cost-Effective Architecture:** Demonstrates that high-quality analysis can be achieved using **Local Small LLMs** to substantially reduce computational costs.
*   **Structured XAI/LLM Integration:** Successfully combines structured, pre-defined statistical explorations (XAI) with LLM natural language generation capabilities.

---

## Results & Evaluation

**Benchmarks:**
*   DiscoveryBench
*   InfiAgent-DABench

**Internal Technical Metrics:**
The text notes that specific quantitative experimental results were not provided in the analysis source, but defines the following internal metrics for the workflow:
*   **Missing Value Ratio Thresholds:** To filter data quality.
*   **Outlier Ratio Thresholds:** To manage noise.
*   **SSE (Sum of Squared Errors) Thresholds:** Used for clustering validity.
*   **Credibility Score:** A metric defined to assess the reliability of findings.