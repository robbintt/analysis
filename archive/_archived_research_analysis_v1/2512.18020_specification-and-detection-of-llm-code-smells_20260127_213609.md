---
title: Specification and Detection of LLM Code Smells
arxiv_id: '2512.18020'
source_url: https://arxiv.org/abs/2512.18020
generated_at: '2026-01-27T21:36:09'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Specification and Detection of LLM Code Smells

*Authors: New York, Reference Format, Large Language, Code Smells*

***

> ### ðŸ“Š Quick Facts
> - **Quality Score:** 9/10
> - **References:** 40 Citations
> - **Dataset:** 200 Open-source LLM Systems
> - **Prevalence Rate:** 60.50%
> - **Detection Precision:** 86.06%

***

## Executive Summary

The rapid integration of Large Language Models (LLMs) into software systems has outpaced the development of standardized quality assurance mechanisms for LLM-specific code. While traditional software engineering utilizes "code smells" to identify maintainability issues, there is a critical lack of formal quality metrics regarding how LLMs are implemented within applications. This gap creates tangible risks to system stability and resource management, as developers often lack clear guidelines on best practices for LLM inference, leading to recurring problematic patterns that degrade software quality.

To address this, the authors introduce the novel concept of **"LLM code smells,"** derived through a rigorous methodology involving a comprehensive analysis of relevant literature to identify recurrent problematic patterns. The researchers formally specified a catalog of five distinct smellsâ€”including **Unbounded Max Metrics (UMM)**, which addresses missing constraints on token budgets and timeouts; **No Model Version Pinning (NMVP)**, targeting the use of mutable model aliases; and **No System Message (NSM)**, identifying missing role definitions in chat API calls. To operationalize these definitions, the team extended the static analysis tool `SpecDetect4AI` to create `SpecDetect4LLM`, enabling automated detection of these low-level source code patterns in Python projects.

An empirical study conducted on 200 open-source Python systems integrating LLMs revealed that these code smells are widespread, affecting **60.50%** of the analyzed projects. The `SpecDetect4LLM` tool demonstrated strong performance in identifying these anti-patterns, achieving a precision rate of **86.06%**. These results validate the formal definitions provided by the authors and confirm that the identified issues are not hypothetical but common occurrences in real-world LLM application development.

This work provides the first known formal catalog for detecting anti-patterns in AI-augmented software engineering, shifting the focus of quality assurance from model performance to implementation quality. By quantifying the high prevalence of LLM code smells and providing a tool for their automated detection, the authors offer a concrete pathway for developers to improve the maintainability and reliability of their systems. This establishes a new foundation for static analysis in the era of generative AI, encouraging the adoption of rigorous coding standards for LLM integration.

***

## Key Findings

*   **High Prevalence:** LLM code smells affect **60.50%** of analyzed open-source LLM systems.
*   **High Detection Accuracy:** The extended detection tool achieved **86.06%** precision.
*   **Quality Risks:** Poor integration of LLMs poses tangible risks to software system quality.
*   **Recurrent Patterns:** Five specific, recurrent problematic coding practices were identified and formalized.

***

## Technical Details

The paper formalizes **LLM code smells** as recurring, low-level source code patterns tied to LLM inference that degrade software quality. The analysis operates at the source-code level, focusing primarily on Python while remaining applicable to other languages.

### Identified Code Smells
The researchers identified five specific smells, three of which are formally detailed:

*   **Unbounded Max Metrics (UMM):** Targets code lacking explicit constraints for token budgets, timeouts, and retries.
*   **No Model Version Pinning (NMVP):** Targets the use of moving aliases instead of immutable version identifiers.
*   **No System Message (NSM):** Targets role-based chat API calls that lack system messages.

### Tooling
The approach utilizes `SpecDetect4LLM`, a static detection tool extended from the existing `SpecDetect4AI` framework to support the automated identification of these new patterns.

***

## Methodology

*   **Literature Review:** Analyzed relevant literature to identify recurrent problematic patterns in LLM integration.
*   **Formal Specification:** Formally defined five distinct LLM code smells to create a standard catalog.
*   **Tool Extension:** Extended the existing `SpecDetect4AI` tool to include logic for detecting the new smells.
*   **Empirical Validation:** Applied the tool to a dataset of 200 open-source LLM systems to validate prevalence and precision.

***

## Contributions

*   **Concept Introduction:** Introduced the novel concept of "LLM code smells" to address the lack of formal quality metrics.
*   **Specification Catalog:** Formalized five LLM-specific code smells, providing the first known catalog for this domain.
*   **Automated Detection Tool:** Enhanced `SpecDetect4AI` to support automated static analysis of LLM inference code.
*   **Evidence of Impact:** Provided large-scale empirical evidence quantifying prevalence (60.50%) and detectability (86.06% precision).

***

## Research Results

The empirical study was conducted on **200** Python open-source systems that integrate LLMs.

*   **Prevalence:** LLM code smells were found to affect **60.50%** of the analyzed open-source LLM systems.
*   **Performance:** The static detection tool, `SpecDetect4LLM`, demonstrated **86.06%** precision in identifying the specified code smells.