---
title: 'Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine
  Learning Engineering'
arxiv_id: '2601.10402'
source_url: https://arxiv.org/abs/2601.10402
generated_at: '2026-02-03T12:24:58'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Toward Ultra-Long-Horizon Agentic Science: Cognitive Accumulation for Machine Learning Engineering

*Xinyu Zhu; Yuzhu Cai; Zexi Liu; Bingyang Zheng; Cheng Wang; Rui Ye; Jiaao Chen; Hanrui Wang; Wei-Chen Wang; Yuzhi Zhang; Linfeng Zhang; Weinan E; Di Jin; Siheng Chen; Yanfeng Wang*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Core Innovation** | Hierarchical Cognitive Caching (HCC) |
| **Benchmark** | OpenAI MLE-Bench |
| **Budget Constraint** | 24 Hours |
| **Achievement** | 56.44% Medal Rate (SOTA) |
| **Citations** | 40 References |

---

## Executive Summary

> Large Language Models (LLMs) face a critical bottleneck in scientific discovery: the inability to sustain strategic coherence over ultra-long horizons spanning days or weeks. Current agentic systems typically rely on static context windows that quickly become overwhelmed by the high-dimensional data generated during iterative experimentation. This limitation causes agents to lose sight of long-term objectives amidst the noise of immediate execution details, preventing them from effectively managing complex, multi-stage engineering tasks such as Machine Learning Engineering (MLE), where feedback is sparse and cumulative experience is essential.
>
> The authors introduce "Cognitive Accumulation," a framework that reframes context management from a static storage problem into a dynamic process of structural differentiation. The core technical innovation is Hierarchical Cognitive Caching (HCC), a multi-tier architecture inspired by computer systems that decouples immediate execution from long-term strategy. HCC dynamically distills transient execution traces into two distinct forms of memory: "Stable Knowledge" (task-specific insights) and "Cross-task Wisdom" (generalizable strategies). This approach allows the agent to retain high-level strategic guidance over extended durations without relying on increasingly large context windows.
>
> The proposed architecture was instantiated in ML-Master 2.0 and evaluated on OpenAI's MLE-Bench under strict 24-hour budget constraints. The system achieved a state-of-the-art medal rate of 56.44%, a significant improvement over the previous ML-Master version's 48.5%. Furthermore, ML-Master 2.0 outperformed a range of competitor agents, including AIRA-dojo, Neo, R&D-Agent, InternAgent, and Operand. These metrics validate the effectiveness of cognitive accumulation, demonstrating that the system can successfully navigate complex engineering workflows and consolidate sparse feedback into actionable long-term guidance.
>
> This research represents a significant advancement in "Agentic Science," providing a scalable blueprint for AI agents capable of autonomous scientific discovery. By demonstrating that cognitive accumulation can overcome the limitations of static context management, the study proves that AI agents can maintain strategic coherence and iterative correction over experimental cycles typically reserved for human researchers. This capability moves the field closer to fully autonomous research systems that can master complex engineering domains and accumulate wisdom in a manner analogous to the scientific process itself.

---

## Key Findings

*   **State-of-the-Art Performance:** The ML-Master 2.0 agent achieved a state-of-the-art medal rate of **56.44%** on OpenAI's MLE-Bench under 24-hour budget constraints.
*   **Resolution of the Long-Horizon Bottleneck:** The research demonstrates that current LLM limitations in scientific research can be overcome by decoupling immediate execution from long-term strategy.
*   **Scalability of Cognitive Accumulation:** The study validates that reframing context management as cognitive accumulation allows agents to function effectively over ultra-long durations (days or weeks).
*   **Superiority over Static Contexts:** Findings indicate that dynamically distilling execution traces into stable knowledge is more effective than relying on static context windows.

---

## Methodology

*   **System Architecture:** The authors developed **ML-Master 2.0**, an autonomous agent designed to master Machine Learning Engineering (MLE) as a microcosm for scientific discovery.
*   **Cognitive Accumulation Framework:** The approach reframes context management from a static storage problem into a dynamic process of cognitive accumulation.
*   **Hierarchical Cognitive Caching (HCC):** A novel, multi-tier architecture inspired by computer systems was introduced to enable structural differentiation of experience over time.
*   **Dynamic Distillation:** The methodology dynamically processes transient execution traces, distilling them into stable knowledge and cross-task wisdom to separate immediate execution details from long-term strategy.

---

## Technical Details

The paper introduces **'Cognitive Accumulation'** to address the ultra-long-horizon bottleneck, defined as the capacity to sustain strategic coherence over extended temporal scales.

*   **Core Innovation:** The core architectural innovation is **Hierarchical Cognitive Caching (HCC)**, inspired by computer systems, which decouples immediate execution from long-term strategy.
*   **Memory Differentiation:** HCC dynamically distills transient execution traces into two categories:
    *   **'Stable Knowledge':** Task-specific information.
    *   **'Cross-task Wisdom':** Generalizable strategies.
*   **Scalability:** This provides a scalable alternative to static context windows that are often overwhelmed by high-dimensional exploration.

---

## Results

Evaluated on OpenAI's MLE-Bench under a strict 24-hour budget constraint, ML-Master 2.0 achieved a state-of-the-art medal rate of **56.44%**, improving upon the previous ML-Master version's **48.5%**.

**Competitive Performance:**
The model outperformed competitors including:
*   AIRA-dojo
*   Neo
*   R&D-Agent
*   InternAgent
*   Operand
*   FM Agent
*   MLE-STAR-Pro-1.5
*   Thesis
*   Leeroo

The results validate the effectiveness of cognitive accumulation for autonomous exploration in scientific discovery contexts.

---

## Contributions

*   **Hierarchical Cognitive Caching (HCC):** Introduction of a new architectural paradigm for AI agents that mimics computer system caching to manage memory and allow structural differentiation of experiences.
*   **Ultra-Long-Horizon Autonomy:** Establishment of a scalable blueprint for AI agents capable of sustaining strategic coherence and iterative correction over experimental cycles spanning days or weeks.
*   **Advancement of Agentic Science:** Significant progress toward autonomous scientific discovery by proving that AI agents can successfully consolidate sparse feedback into coherent long-term guidance in real-world research environments.

---

**Paper Quality Score:** 9/10  
**References:** 40 Citations