---
title: 'Balancing Fidelity and Plasticity: Aligning Mixed-Precision Fine-Tuning with
  Linguistic Hierarchies'
arxiv_id: '2505.03802'
source_url: https://arxiv.org/abs/2505.03802
generated_at: '2026-02-03T18:31:19'
quality_score: 6
citation_count: 34
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Balancing Fidelity and Plasticity: Aligning Mixed-Precision Fine-Tuning with Linguistic Hierarchies

*Changhai Zhou; Shiyang Zhang; Yuhua Zhou; Qian Qiao; Jun Gao; Shichao Weng; Weizhong Zhang; Cheng Jin*

***

> ### ðŸ“Š Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Proposed Method** | QR-Adaptor |
> | **Primary Benchmark** | MMLU, WikiText-2 |
> | **Quality Score** | 6/10 |
> | **Total Citations** | 34 |
> | **Memory Budget** | 4-bit |
> | **Key Innovation** | Joint optimization of bit-width and LoRA rank |

***

## Executive Summary

This research addresses the challenge of fine-tuning Large Language Models (LLMs) on resource-constrained edge devices where memory budgets are severely limited. Current approaches typically decouple weight quantization from the optimization of adapter parameters (such as LoRA ranks), treating them as isolated steps. This decoupling creates irreversible information bottlenecks in layers requiring high precision while wasting memory resources on layers that could operate with lower fidelity.

The authors identify a fundamental theoretical gap, termed the **"Fidelity-Plasticity Trade-off,"** which posits that a layer's capacity to adapt (plasticity) is strictly bounded by the information retained in its frozen weights (fidelity). To resolve the inefficiencies of isolated optimization, the authors introduce **QR-Adaptor**, a unified framework that formulates resource allocation as a joint multi-objective search problem.

Technically, QR-Adaptor aligns resource allocation with the model's inherent linguistic hierarchy:
*   **Shallow Layers (Syntax):** Identified as redundancy-heavy; assigned lower bit-widths (e.g., 2-bit).
*   **Deep Layers (Semantics):** Identified as capacity-critical; assigned higher bit-widths (e.g., 4-bit).

Experimental validation on the MMLU benchmark demonstrates the effectiveness of this linguistically aware strategy. The proposed configuration (Shallow 2-bit / Deep 4-bit) achieved an accuracy of **0.461**, significantly outperforming the "anti-intuitive" strategy (0.298). While it falls short of a uniform 4-bit baseline (0.581), it achieves performance comparable to full 16-bit baselines while operating under a strict 4-bit memory budget.

## Key Findings

*   **Identification of the Fidelity-Plasticity Trade-off:** A layer's capacity to adapt (Plasticity) is fundamentally limited by the information capacity of its frozen weights (Fidelity).
*   **Inefficiency of Decoupled Optimization:** Separating quantization and adapter optimization creates irreversible information bottlenecks in critical layers and wastes memory in others.
*   **Resource Alignment is Critical:** Aligning resource allocation with a model's linguistic hierarchy is as critical as model size itself.
*   **Superior Compression Performance:** The proposed method achieves performance comparable to 16-bit baselines while operating under a strict 4-bit memory budget.

## Methodology

The authors introduce **QR-Adaptor**, a unified framework designed to jointly optimize per-layer quantization bit-width and LoRA rank. Instead of treating quantization and adapter optimization as separate sequential steps, QR-Adaptor formulates resource allocation as a **multi-objective search problem** aligned with the model's linguistic hierarchy.

This approach allows the system to intelligently identify redundancy-heavy layers (often syntactic) to liberate memory resources, which are then reallocated to capacity-critical layers (often semantic). This differs from prior work by treating quantization and adapter rank as a coupled resource allocation problem rather than optimizing them in isolation.

## Technical Details

The paper proposes QR-Adaptor, a framework that performs joint configuration search based on the Fidelity-Plasticity trade-off.

*   **Linguistic Hierarchy Alignment:** The framework aligns bit-width allocation with linguistic hierarchies to optimize trainability.
    *   *Lower Layers:* Process syntax; assigned lower bit-widths.
    *   *Deeper Layers:* Process semantics; assigned higher bit-widths.
*   **Coupled Resource Allocation:** The approach treats quantization and adapter rank as a coupled problem, ensuring that adapter rank does not attempt to compensate for low precision in critical layers where it is theoretically insufficient.

## Contributions

*   **Theoretical Framework:** Provides a theoretical basis for Quantization-Aware Fine-tuning by defining the 'Fidelity-Plasticity Trade-off,' explaining why adapter rank cannot compensate for low precision in critical layers.
*   **Unified Optimization Strategy:** Introduces a novel paradigm for joint optimization of bit-width and adapter rank, enabling granular control over model resources.
*   **Practical Efficiency for Edge Deployment:** Offers a solution for deploying LLMs on resource-constrained edge devices by maintaining high fidelity under severe memory constraints (4-bit) through linguistically aware resource distribution.

## Results

Experimental validation was conducted using the **MMLU** (accuracy) and **WikiText-2** (Perplexity) benchmarks.

*   **Proposed Strategy (Shallow 2-bit / Deep 4-bit):** Achieved an accuracy of **0.461**.
*   **Anti-intuitive Strategy:** Achieved an accuracy of **0.298** (demonstrating the penalty of misalignment).
*   **Uniform 4-bit Baseline:** Achieved an accuracy of **0.581**.

The results indicate that while the linguistic strategy significantly mitigates the damage of compression compared to inverse allocation, there is a performance gap compared to uniform high-precision allocation. However, the method successfully bridges the gap to full 16-bit baselines under a 4-bit budget.

***
*Analysis based on 34 references.*