# Ensuring Calibration Robustness in Split Conformal Prediction Under Adversarial Attacks

*Xunlei Qian; Yue Xing*

***

> ### ⚡ Quick Facts
> *   **Framework:** Split Conformal Prediction (Split CP)
> *   **Score Metric:** Highest Probability Score (HPS)
> *   **Training Approach:** Min-max adversarial optimization
> *   **Attack Types:** $L_2$-bounded (Theoretical), $L_\infty$-FGSM (Experimental)
> *   **Primary Goal:** Maintain validity & efficiency under adversarial distribution shifts
> *   **Quality Score:** 6/10

***

## Executive Summary

This research addresses the fundamental fragility of Split Conformal Prediction (Split CP) in adversarial environments where the exchangeability between calibration and test data is violated. Standard Split CP relies on the assumption that the calibration data is representative of the test distribution; however, adversarial attacks at test time induce distribution shifts that break this assumption. This violation causes models to lose their coverage guarantees—meaning prediction sets fail to contain the true label at the target probability level—rendering uncertainty quantification unreliable for security-sensitive applications. The central challenge is to maintain statistically valid prediction sets (validity) while minimizing their size (efficiency) even when the system is subjected to hostile perturbations.

The authors introduce a robustness framework that re-engineers the calibration process by injecting controlled adversarial perturbations rather than relying on clean data. Technically, the method utilizes the Highest Probability Score (HPS), defined as $S(x, y) = 1 - f_y(x)$, to measure nonconformity. The core theoretical contribution is establishing a monotonic relationship between the strength of adversarial perturbations applied during calibration ($\epsilon_{cal}$) and the resulting test-time coverage. By characterizing this link, the authors demonstrate that selecting an appropriate calibration attack strength ensures validity across a contiguous range of test-time perturbations ($\epsilon_{test}$). Additionally, the framework integrates min-max adversarial optimization during model training, analyzing robustness under $L_2$-bounded perturbations while validating against rigorous $L_\infty$-FGSM attacks.

The study evaluated validity and efficiency using empirical coverage rates and average prediction set size on standard image classification benchmarks. Experiments confirmed a strict monotonic relationship: as the calibration attack strength increases, the coverage probability under attack decreases predictably, allowing for precise tuning. By selecting $\epsilon_{cal}$ proportional to the anticipated threat, the framework successfully maintained the target $1-\alpha$ coverage guarantees over a continuous spectrum of test-time perturbation strengths. Regarding efficiency, the inclusion of adversarial training proved highly effective, yielding significantly tighter prediction sets (reduced average size) compared to standard training methods while retaining high informativeness. These results were rigorously validated using $L_\infty$-FGSM attacks, demonstrating robust performance where standard Split CP typically fails.

This work significantly advances uncertainty quantification by bridging the gap between theoretical exchangeability and practical robustness. By directly addressing distribution shifts caused by adversarial evasion, the research ensures that conformal prediction can be reliably deployed in hostile environments. This approach provides a critical guarantee for safety-critical systems, such as autonomous vehicles and medical diagnostics, where maintaining accurate confidence intervals and small prediction set sizes under attack is essential for operational trust and user safety.

***

## Key Findings

*   **Monotonic Coverage Relationship:** Prediction coverage exhibits a monotonic relationship with the strength of adversarial perturbations applied during calibration, allowing for predictable control under test conditions.
*   **Guaranteed Target Coverage:** Target coverage guarantees can be maintained over a contiguous range of test-time attack strengths by selecting an appropriate calibration attack.
*   **Enhanced Efficiency via Adversarial Training:** Incorporating adversarial training yields significantly tighter prediction sets and retains high informativeness compared to standard training methods.

***

## Methodology

The research employs a combination of theoretical analysis and extensive experimentation to evaluate split conformal prediction. It characterizes the impact of adversarial perturbations applied during the calibration phase on coverage validity and prediction set size under test-time adversarial conditions. Furthermore, it examines the effects of adversarial training at the model-training stage on prediction efficiency.

***

## Contributions

*   **Theoretical Framework:** Provides a theoretical framework characterizing how calibration-time attack strength influences coverage guarantees and addresses exchangeability violations caused by distribution shifts.
*   **Robustness Strategy:** Establishes a robustness strategy where coverage validity is achieved across varying test-time perturbations via calibration-time attacks.
*   **Efficiency Proof:** Demonstrates that adversarial training enhances efficiency by generating tighter prediction sets in the presence of adversarial perturbations.

***

## Technical Details

| Component | Specification |
| :--- | :--- |
| **Framework** | Split Conformal Prediction (Split CP) |
| **Data Partitioning** | Training, Calibration, and Test sets |
| **Score Function** | Highest Probability Score (HPS): $S(x, y) = 1 - f_y(x)$ |
| **Prediction Set Construction** | Includes classes where $f_y(x_{test}) \ge 1 - Q$ |
| **Quantile Definition** | $Q$ is the quantile of calibration scores |
| **Training Optimization** | Min-max adversarial optimization |
| **Theoretical Bound** | Assumes $L_2$-bounded perturbations |
| **Experimental Attack** | $L_\infty$-FGSM attacks (selected to reliably degrade performance) |

***

## Results

The study evaluated validity (empirical coverage) and efficiency (average prediction set size).

*   **Monotonic Relationship:** A distinct monotonic relationship was found between prediction coverage and adversarial perturbation strength during calibration.
*   **Coverage Maintenance:** Target coverage guarantees were maintained across a range of test-time attack strengths by appropriately selecting the calibration attack strength.
*   **Prediction Set Efficiency:** Adversarial training yielded significantly tighter prediction sets (improved efficiency) compared to standard training while maintaining informativeness.

***

**Quality Score:** 6/10  
**References:** 0 citations