# On the Origins of Sampling Bias: Implications on Fairness Measurement and Mitigation

*Sami Zhioua; Ruta Binkyte; Ayoub Ouni; Farah Barika Ktata***

> ### Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Analysis Scope:** 3 Datasets (COMPAS, Adult)
> *   **Algorithms Tested:** 5 Classifiers
> *   **Fairness Metrics:** 6 distinct metrics
> *   **Experimental Range:** 10 to 2,000 training samples
> *   **Statistical Rigor:** 30 independent samples per size
> *   **References:** 40 citations

---

## Executive Summary

This research addresses the critical yet largely overlooked problem of inconsistent terminology and flawed assumptions regarding "sampling bias" in algorithmic fairness. Specifically, the authors challenge the common assumption that "sampling bias" impacts all demographic groups equally, highlighting the significant risk this poses to vulnerable sub-populations. Because existing literature often conflates distinct bias sources, practitioners risk applying incorrect mitigation strategies, leading to distorted fairness measurements and the potential exacerbation of discrimination against marginalized groups.

The key innovation lies in a rigorous theoretical framework that decomposes "sampling bias" into two distinct phenomena: **Sample Size Bias (SSB)** and **Underrepresentation Bias (URB)**. SSB is formally defined as the difference in discrimination between a model trained on a sample of size $m$ versus a population-sized set $M$, while URB analyzes bias arising from disparities in representation ratios between groups. The authors utilize the concepts of Main Prediction ($\bar{\hat{Y}}$) and Optimal Prediction ($Y^*$) to quantify discrimination via cost metrics under Squared, Absolute, and Zero-One loss functions. Critically, this framework clarifies that unequal bias distribution does not merely add noise; it can lead to either the amplification or the underestimation of existing disparities, fundamentally altering how fairness violations should be interpreted.

The study validates these theoretical propositions through extensive experimental data involving 3 datasets, including COMPAS and Adult, evaluated across 5 classifiers and 6 fairness metrics. To analyze SSB, the authors varied training set sizes from $n=10$ to 2,000 on the COMPAS dataset, utilizing 30 independent samples per size to ensure statistical significance against population-sized test sets. In testing URB, the study implemented extreme imbalance scenarios, such as setting the positive outcome rate to 0.9 for privileged groups versus 0.1 for unprivileged groups in the Adult dataset, and varied sensitive group representation ratios to as low as 0.1. These quantitative experiments demonstrated that standard pre-processing, in-processing, and data augmentation mitigation techniques often fail when the specific origins of bias (SSB vs. URB) are ignored.

The significance of this work extends to standardizing bias taxonomy and providing practitioners with actionable diagnostic guidelines. By distinguishing between SSB and URB, the authors offer specific mitigation strategies—such as increasing sample size to address SSB or correcting representation ratios to resolve URB—rather than applying generic fixes. This structured approach enables the accurate selection of fairness metrics and prevents the unintended consequence of masking discrimination through biased measurements, ultimately leading to more reliable and equitable machine learning systems.

---

## Key Findings

*   **Challenging Uniformity:** The assumption that bias affects all groups equally is challenged; the authors demonstrate that when bias is distributed differently across groups, it can exacerbate discrimination against specific sub-populations.
*   **Terminological Ambiguity:** The term "sampling bias" is inconsistently defined in the existing literature, leading to potential confusion in both research and application.
*   **Conceptual Distinction:** The paper clarifies ambiguity by distinguishing between **Sample Size Bias (SSB)** and **Underrepresentation Bias (URB)**.
*   **Measurement Impact:** Unequal distribution of bias during the measurement phase can lead to either the amplification or underestimation of existing disparities.

---

## Methodology

The authors employed a robust comparative analysis to evaluate how different forms of "sampling bias" impact fairness measurements.

*   **Experimental Design:** Utilized benchmark datasets to evaluate model performance across various scenarios.
*   **Algorithms:** Implemented mainstream machine learning algorithms to train models.
*   **Objective:** Expose observations relevant to the impact of "sampling bias" on fairness in different training contexts.

---

## Contributions

This research makes three primary contributions to the field of algorithmic fairness:

1.  **Conceptual Disambiguation:** Introduces precise definitions for "sampling bias" variants (SSB and URB) to standardize terminology and reduce confusion.
2.  **Framework for Bias Analysis:** Provides a structured approach to understanding how bias impacts different groups differently.
3.  **Actionable Guidelines:** Translates experimental observations into specific recommendations for practitioners to improve discrimination measurement and fairness mitigation.

---

## Technical Details

### Theoretical Framework
The paper establishes a framework to decompose "sampling bias" into two specific components:

#### 1. Sample Size Bias (SSB)
SSB is formally defined as the difference in discrimination between a model trained on a sample of size $m$ and one trained on a population-sized set $M$.

$$SSB_{\dagger}(A, m) = Disc_{\dagger}(\bar{\hat{Y}}_m) - Disc_{\dagger}(\bar{\hat{Y}}_M)$$

#### 2. Underrepresentation Bias (URB)
URB analyzes bias resulting from disparities in representation ratios between sensitive groups.

### Predictive Concepts
*   **Main Prediction ($\bar{\hat{Y}}$):** The expectation of the prediction function across all training sets of a specific size.
*   **Optimal Prediction ($Y^*$):** The Bayes optimal predictor.

### Discrimination Quantification
Discrimination is quantified as the difference in cost metrics between groups:

$$Disc_{\dagger}(\hat{Y}) = C^{\dagger}_{a1}(\hat{Y}) - C^{\dagger}_{a0}(\hat{Y})$$

### Metrics and Loss Functions
The analysis covers a comprehensive set of mathematical criteria:
*   **Loss Functions:** Squared, Absolute, and Zero-One loss.
*   **Performance Metrics:** False Positive Rate (FPR), False Negative Rate (FNR), True Positive Rate (TPR), Zero-One Loss, and Area Under the Curve (AUC).

---

## Results

The experimental scope was comprehensive, designed to stress-test the theoretical framework against real-world data scenarios.

### Experimental Scope
*   **Datasets:** 3 datasets (including COMPAS and Adult).
*   **Models:** 5 classifiers.
*   **Metrics:** 6 distinct fairness metrics.

### Sample Size Bias (SSB) Experimentation
*   **Training Range:** Training set sizes ranged from $n=10$ to 2000 on the COMPAS dataset.
*   **Statistical Significance:** Utilized 30 independent samples per size.
*   **Evaluation:** Models were evaluated on population-sized test sets to isolate the effects of small sample training.

### Underrepresentation Bias (URB) Experimentation
*   **Extreme Imbalance:** Tested scenarios where positive outcome rates were 0.9 for privileged groups and 0.1 for unprivileged groups (Adult dataset).
*   **Representation Ratios:** Varied the ratio of sensitive group representatives, testing ratios as low as 0.1.

### Mitigation Analysis
The setup included analyzing the effectiveness of standard mitigation techniques, specifically:
*   Pre-processing
*   In-processing
*   Data augmentation

---

**Quality Score:** 8/10 | **References:** 40 Citations