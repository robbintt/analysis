---
title: Advancing Visual Large Language Model for Multi-granular Versatile Perception
arxiv_id: '2507.16213'
source_url: https://arxiv.org/abs/2507.16213
generated_at: '2026-02-04T16:02:26'
quality_score: 6
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Advancing Visual Large Language Model for Multi-granular Versatile Perception

*Wentao Xiang; Haoxian Tan; Cong Wei; Yujie Zhong; Dengjie Li; Yujiu Yang*

---

### F4CA Quick Facts

| Metric | Details |
| :--- | :--- |
| **Architecture** | MVP-LM (VLLM-based) |
| **Key Innovation** | Multi-granularity Decoder & Query Enhancement |
| **Task Coverage** | Panoptic Segmentation, Detection, Grounding, Referring Expression |
| **Quality Score** | 6/10 |
| **References** | 40 Citations |

---

> **EXECUTIVE SUMMARY**
>
> Current Visual Large Language Models (VLLMs) are hindered by fragmented architectures that fail to simultaneously handle diverse prediction types and instruction formats, limiting their utility in complex real-world scenarios. To address this, the authors introduce **MVP-LM**, a unified VLLM architecture designed to process four distinct perception subtasks combining bounding boxes and masks with word-based and sentence-based instructions.
>
> The framework employs a Multi-granularity Decoder, a Chain-of-Thought-inspired Dataset Unification strategy, and a Query Enhancement Strategy to optimize concurrent processing and training efficiency. Validated across benchmarks like RefCOCO and COCO, MVP-LM achieved state-of-the-art performance in referring expression comprehension and competitive results in object detection and segmentation, successfully closing the performance gap between generalist and specialized models without accuracy trade-offs. This advancement establishes a new paradigm for unified visual reasoning and dataset unification, reducing the engineering overhead associated with deploying multiple specialized vision systems.

---

## Key Findings

*   **Unified Efficacy:** The MVP-LM framework demonstrates effectiveness across a wide spectrum of perception tasks, including panoptic segmentation, detection, grounding, and referring expression segmentation.
*   **Successful Integration:** The study proves that word-based and sentence-based perception tasks can be successfully integrated with box and mask predictions within a single architecture.
*   **Performance Validation:** Extensive experiments across various benchmarks confirm that the proposed framework and its strategies significantly improve multi-granular perception capabilities.
*   **Versatility:** Unlike existing research constrained to limited subsets of tasks, MVP-LM achieves applicability across diverse contexts by covering combinations of prediction types and instruction types.

## Methodology

The authors propose **MVP-LM** (Multi-granular and Versatile Perception framework), built upon Visual Large Language Models (VLLMs). The methodology centers on a unified single-architecture design consisting of three core components:

1.  **Multi-granularity Decoder:** Designed to handle diverse output formats, integrating word-based and sentence-based perception alongside box and mask predictions.
2.  **CoT-inspired Dataset Unification:** A strategy to unify datasets for seamless supervised fine-tuning across heterogeneous tasks.
3.  **Query Enhancement Strategy:** Utilized to harness and optimize the decoding and generative capabilities inherent in VLLMs for perception tasks.

## Technical Details

The MVP-LM is a VLLM-based architecture designed to unify perception tasks. It categorizes tasks into four groups based on a **2x2 matrix**:

*   **Prediction Type:** Box vs. Mask
*   **Instruction Type:** Word-based vs. Sentence-based

**Key Components:**
*   **Multi-granularity Decoder:** Enables simultaneous box and mask prediction.
*   **Training Paradigm:** Utilizes concurrent optimization and leverages mask-to-box annotation logic.
*   **Dataset Strategy:** Uses a CoT-inspired approach for fine-tuning on complex, multi-task data.

## Contributions

*   **Comprehensive Framework:** Introduction of MVP-LM, addressing limitations of existing models by offering a versatile solution covering four distinct groups of perception subtasks.
*   **Architectural Innovation:** Development of a multi-granularity decoder capable of processing varied prediction types (boxes, masks) and instruction types (word-based, sentence-based) simultaneously.
*   **Training Strategy:** Proposal of a CoT-inspired dataset unification strategy facilitating effective supervised fine-tuning on complex, multi-task data.
*   **Optimization Technique:** Introduction of a query enhancement mechanism to better utilize the generative power of VLLMs within visual perception.

## Results

The framework claims validation across a wide spectrum of tasks including panoptic segmentation, detection, grounding, and referring expression segmentation. It covers all four task combinations (Box+Word, Box+Sentence, Mask+Word, Mask+Sentence) across various benchmarks.

The authors state that extensive experiments substantiate the framework's efficacy and that its strategies significantly improve multi-granular perception capabilities, though specific numerical metrics are not provided in the text.

---

**Paper Assessment:** 6/10