---
title: 'State-of-the-art Small Language Coder Model: Mify-Coder'
arxiv_id: '2512.23747'
source_url: https://arxiv.org/abs/2512.23747
generated_at: '2026-02-03T12:27:38'
quality_score: 7
citation_count: 24
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# State-of-the-art Small Language Coder Model: Mify-Coder

*Abhinav Parmar, Abhisek Panigrahi, Abhishek Kumar Dwivedi, Abhishek Bhattacharya, Adarsh Ramachandra, Aditya Choudhary, Aditya Garg, Aditya Raj, Alankrit Bhatt, Alpesh Yadav, Anant Vishnu, Ananthu Pillai, Ankush Kumar, Aryan Patnaik, Aswatha Narayanan S, Avanish Raj Singh, Bhavya Shree Gadda, Brijesh Pankajbhai Kachhadiya, Buggala Jahnavi, Chidurala Nithin Krishna, Chintan Shah, Chunduru Akshaya, Debarshi Banerjee, Debrup Dey, Deepa R., Deepika B G, Faiz ur Rahman, Gagan Gayari, Gudhi Jagadeesh Kumar Naidu, Gursimar Singh, Harshal Tyagi, Harshini K, James Mani Vathalloor, Jayarama Nettar, Jayashree Gajjam, Joe Walter Sugil George, Kamalakara Sri Krishna Tadepalli, Kamalkumar Rathinasamy, Karan Chaurasia, Karthikeyan S, Kashish Arora, Kaushal Desai, Khushboo Buwade, Kiran Manjrekar, Malikireddy Venkata Sai Likhitha, Manjunath A, Mitali Mahavir Bedmutha, Mohammed Rafee Tarafdar, Nikhil Tiwari, Nikitha K Gigi, Pavan Ravikumar, Pendyala Swarnanjali, Piyush Anand, Prakash Chandrasekar, Prasanna Bhalchandra Gawade, Prasanth Sivan, Preeti Khurana, Priyanshi Babbar, Rajab Ali Mondal, Rajesh Kumar Vissapragada, Rajeshwari Ganesan, Rajeswari Koppisetti, Ramjee R., Ramkumar Thiruppathisamy, Rani G. S., S Reka, Samarth Gupta, Sandeep Reddy Kothakota, Sarathy K, Sathyanarayana Sampath Kumar, Saurabh Kumar, Shashank Khasare, Shenbaga Devi Venkatesh Kumar, Shiva Rama Krishna Parvatham, Shoeb Shaikh, Shrishanmathi A, Shubham Pathak, Sree Samhita Koppaka, Sreenivasa Raghavan K S, Sreeram Venkatasubramanian, Suprabha Desai Bojja, Swetha R, Syed Ahmed, Chinmai Harshitha Thota, Tushar Yadav, Veeravelly Kusumitha, V V S S Prasanth Patnaik, Vidya Sri Sesetti, Vijayakeerthi K, Vikram Raj Bakshi, Vinay K K, Vinoth Kumar Loganathan, Vipin Tiwari, Vivek Kumar Shrivastav, V Venkata Sri Datta Charan, Wasim Akhtar Khan*

---

### ⚡ Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Model Size** | 2.5 Billion Parameters |
| **Training Data** | 4.2 Trillion Tokens |
| **Training Duration** | 3 Months |
| **Hardware** | NVIDIA B200 & H100 GPUs |
| **Data Composition** | 78% Code, 12% Text, 10% Math |
| **Deployment** | Standard CPUs (Quantized) & Edge |

---

## Executive Summary

The dominance of massive frontier models in code generation has created a substantial barrier to deployment, as these models require extensive computational resources and specialized hardware often inaccessible for edge or standard desktop environments. While smaller language models offer improved efficiency and lower latency, they historically suffer from performance degradation, failing to match the accuracy and reasoning capabilities of larger counterparts in complex tasks such as function calling and agentic workflows. This paper addresses the critical challenge of bridging the gap between the resource efficiency of small language models (SLMs) and the high-performance standards of frontier-grade coders, aiming to enable advanced coding capabilities without prohibitive infrastructure costs.

The core innovation of Mify-Coder lies in a rigorous, compute-optimal training strategy that prioritizes data density over parameter scale, applied to a compact 2.5 billion parameter architecture. Trained on a massive 4.2 trillion tokens, the model utilizes a hybrid data pipeline combining high-quality curated sources with synthetic data generated via agentic prompts, processed through a single continuous trajectory integrating Continued Pre-training (CPT) and Supervised Fine-Tuning (SFT). Technically, the methodology employs rigorous data engineering techniques—including dependency-aware topological sorting for code and aggressive MinHashLSH deduplication for mathematical reasoning—to ensure maximum token value and compute discipline.

The study demonstrates that this data-centric approach yields significant efficiency gains, with the data pipeline securing a 70% reduction in math tokens via provenance analysis, an additional 20% reduction through fuzzy deduplication, and a further 18% impact from quality-based filtering. In terms of performance, Mify-Coder achieves accuracy comparable to larger frontier models, specifically rivaling DeepSeek-Coder and Qwen2.5-Coder on the HumanEval benchmark. Conducted over three months using NVIDIA B200 and H100 GPUs, the training produced a model that maintains high efficiency across diverse tasks—including SQL generation and mathematical reasoning—while supporting quantized variants capable of running on standard desktop CPUs.

This research significantly lowers the barrier to entry for deploying advanced coding agents by validating that state-of-the-art performance can be achieved on consumer-grade hardware, facilitating edge deployment without specialized GPUs. By establishing a new benchmark for small language models in code generation, the paper challenges the prevailing notion that scale is the primary driver of performance, highlighting instead the critical importance of data quality and compute discipline. This shift promotes a more sustainable and accessible approach to AI development, enabling high-efficiency coding tools and complex function-calling workflows to be deployed in resource-constrained environments.

---

## Key Findings

*   **Competitive Performance:** The Mify-Coder model (2.5B parameters) achieves accuracy and safety comparable to larger frontier-grade models, outperforming them on coding and function-calling benchmarks.
*   **Efficiency Through Discipline:** Principled data management and compute discipline allow smaller models to achieve competitive efficiency without massive resource requirements.
*   **Edge Deployment Ready:** Quantized variants of the model can run on standard desktop environments, enabling edge deployment without specialized hardware.
*   **Complex Task Handling:** The model successfully handles complex tasks such as agent-driven workflows and function calling despite its compact size.

---

## Methodology

The model is built upon the **Mify-2.5B** foundation and trained on **4.2T tokens** using a compute-optimal strategy. The training utilizes a hybrid dataset combining high-quality curated sources and synthetic data generated via agentic prompts. Iterative refinement is performed using enterprise-grade evaluation datasets and LLM-based quality filtering. The team employed a single continuous training trajectory exploring:

*   **Objectives:** CPT-SFT (Continued Pre-training and Supervised Fine-Tuning)
*   **Data Mixtures:** Hybrid of curated and synthetic sources
*   **Sampling Dynamics:** Advanced sampling strategies

---

## Technical Details

**Architecture & Training**
*   **Base Model:** Mify-2.5B
*   **Parameters:** 2.5 Billion
*   **Pipeline:** Two-stage (Continued Pre-training and Supervised Fine-Tuning)
*   **Infrastructure:** NVIDIA B200 and H100 GPUs
*   **Timeline:** 3 Months
*   **Deployment:** Supports standard CPUs via quantized variants

**Data Pipeline Engineering**
*   **Frameworks:** Ray, Dask, and DAFT
*   **Composition Ratio:**
    *   78% Code
    *   12% Text
    *   10% Math
*   **Code Processing:** Utilizes Stack-v2 with dependency-aware topological sorting.
*   **Math Processing:** Aggressive deduplication using MinHashLSH:
    *   Shingle size: 5
    *   Permutations: 110
    *   LSH Bands: 10
    *   Jaccard Similarity Threshold: 0.75
*   **Text Processing:** Employs NVIDIA Domain Classifier and EAI-Distill models for categorization.

---

## Results

*   **Math Data Efficiency:** Achieved a 70% reduction in math tokens via provenance analysis and an additional 20% reduction through fuzzy deduplication.
*   **Overlap Reduction:** Reduced overlap between Math and Text sources by 3%.
*   **Filtering Impact:** Quality-based filtering had an 18% positive impact on data quality.
*   **Benchmark Performance:** Claims to rival **DeepSeek-Coder** and **Qwen2.5-Coder** on the HumanEval benchmark.
*   **Versatility:** Supports diverse tasks such as generation, SQL generation, and mathematical reasoning without the massive compute requirements of larger models.

---

## Contributions

1.  **New Benchmark:** The introduction of Mify-Coder establishes a new benchmark for small language models in code generation, proving compact models can match frontier performance.
2.  **Validated Methodology:** The paper presents a validated methodology for maximizing data density through agentic synthetic data generation and rigorous filtering.
3.  **Accessibility:** The work lowers the barrier to entry for deploying advanced coding agents by enabling high performance on standard desktop hardware via quantization.

---

**Quality Score:** 7/10  
**References:** 24 citations