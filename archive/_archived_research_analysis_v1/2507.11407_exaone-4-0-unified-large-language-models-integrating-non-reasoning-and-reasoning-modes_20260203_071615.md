---
title: 'EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning
  Modes'
arxiv_id: '2507.11407'
source_url: https://arxiv.org/abs/2507.11407
generated_at: '2026-02-03T07:16:15'
quality_score: 4
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes

*Kyunghoon Bae; Eunbi Choi; Kibong Choi; Stanley Jungkyu Choi; Yemuk Choi; Kyubeen Han; Seokhee Hong; Junwon Hwang; Taewan Hwang; Joonwon Jang; Hyojin Jeon; Kijeong Jeon; Gerrard Jeongwon Jo; Hyunjik Jo; Jiyeon Jung; Euisoon Kim; Hyosang Kim; Jihoon Kim; Joonkee Kim; Seonghwan Kim; Soyeon Kim; Sunkyoung Kim; Yireun Kim; Yongil Kim; Youchul Kim; Edward Hwayoung Lee; Gwangho Lee; Haeju Lee; Honglak Lee; Jinsik Lee; Kyungmin Lee; Sangha Park; Young Min Paik; Yongmin Park; Youngyong Park; Sanghyun Seo; Sihoon Yang; Heuiyeen Yeen; Sihyuk Yi; Hyeongu Yun*

---

### üìä Quick Facts

| Metric | Specification |
| :--- | :--- |
| **Parameters** | 32B (High Perf), 1.2B (On-device) |
| **Context Window** | 128K (32B), 64K (1.2B) |
| **Training Tokens** | 14 Trillion |
| **Architecture** | Unified Dual-Mode (Non-reasoning + Reasoning) |
| **Attention** | Hybrid Local (4K sliding) / Global (3:1 ratio) |
| **MMLU-Redux** | 75.3 |
| **GSM8K** | 94.2% |
| **MATH** | 64.0% |
| **HumanEval** | 80.1% |
| **Languages** | English, Korean, Spanish |

---

## üìë Executive Summary

### **Problem**
The research addresses the architectural dichotomy in Large Language Models (LLMs) where general usability (non-reasoning) and advanced logical complexity (reasoning) are often treated as separate domains. Existing open-weight models typically specialize in one at the expense of the other, creating a trade-off between fluent conversation and deep cognitive tasks (math/coding). Furthermore, as the industry shifts toward "agentic AI," there is a lack of unified, open-weight solutions that support native tool interaction and extended context windows without performance degradation.

### **Innovation**
The core innovation is a **unified "Dual-Mode Integration" architecture** that merges Non-reasoning and Reasoning modes within a single set of weights. This is achieved through a **Hybrid Attention Mechanism** (3:1 ratio of Local to Global attention) enabling a 128K context window. To mitigate length bias, Global attention omits Rotary Positional Embeddings (RoPE), and the architecture utilizes **QK-Reorder-LN**. Additionally, the model natively incorporates tool-use functionalities, treating tool execution as an integrated modality rather than a downstream adapter.

### **Results**
EXAONE 4.0 demonstrates superior performance against other open-weight models, driven by scaling the 32B variant‚Äôs pre-training data to **14 trillion tokens** (more than double the previous iteration). Key achievements include:
*   **MMLU-Redux:** 75.3
*   **Reasoning:** 94.2% on GSM8K, 64.0% on MATH
*   **Coding:** 80.1% on HumanEval
*   **Context:** Perfect retrieval in "Needle In A Haystack" tests at 128K context.
*   **Agentic:** State-of-the-art results in Document QA and RAG benchmarks.

### **Impact**
EXAONE 4.0 bridges the gap between general-purpose chatbots and specialized reasoning engines. By publicly releasing both a high-performance 32B model and a lightweight 1.2B model optimized for edge computing, the authors democratize access to state-of-the-art reasoning. The native tool-use features position this work as a foundational architecture for the agentic AI era.

---

## üîë Key Findings

*   **Dual-Mode Integration:** Successfully integrates Non-reasoning and Reasoning modes, combining general usability with advanced logic in a single model.
*   **Competitive Performance:** Demonstrates superior performance against other open-weight models and remains competitive against frontier-class models.
*   **Agentic AI Ready:** Designed for the agentic era with native features such as agentic tool use.
*   **Multilingual Expansion:** Capabilities expanded to include Spanish, alongside English and Korean.
*   **Model Variants:** The series includes a **32B parameter model** for high performance and a **1.2B parameter model** for on-device applications.

---

## üõ†Ô∏è Methodology

The technical approach relies on a **Dual-Mode Integration** strategy. This methodology merges two distinct operational modes:
1.  **Non-reasoning Mode:** Focused on usability and general interaction.
2.  **Reasoning Mode:** Focused on complex logic and cognitive tasks.

These are merged within a single Large Language Model framework. The methodology also incorporates:
*   **Agentic Functionalities:** Specific integration of tool use capabilities.
*   **Extended Language Support:** Training and fine-tuning processes expanded to support Spanish, English, and Korean.

---

## ‚öôÔ∏è Technical Details

| Feature | Specification |
| :--- | :--- |
| **Architecture** | Unified architecture integrating Non-reasoning and Reasoning modes in a single weight set. Available in 32B and 1.2B sizes. |
| **Attention Mechanism** | **Hybrid Attention** with a **3:1 ratio** of Local (Sliding Window of 4K) to Global attention. |
| **Context Window** | Supports **128K context window** (64K for the 1.2B model). |
| **Positional Encoding** | Global attention omits **RoPE** (Rotary Positional Embeddings) to prevent length bias. |
| **Normalization** | Utilizes **QK-Reorder-LN**, applying RMSNorm after input queries/keys and after attention output. |
| **Context Extension** | Two-stage process (4K ‚Üí 32K ‚Üí 128K) validated by Needle In A Haystack tests. |
| **Training Data** | The 32B model is pre-trained on **14 trillion tokens**, focusing on STEM and cognitive behavior. Includes Spanish support. |

---

## üìà Results

*   **Training Scale:** The 32B model training data increased from 6.5 trillion to **14 trillion tokens**.
*   **Context Length:** Successfully supports a maximum context length of **128,000 tokens** (64,000 for the 1.2B model).
*   **Domain Excellence:** Experimental findings show the model excels in mathematical and coding domains, performing competitively against frontier-class models and superiorly to other open-weight models.
*   **Knowledge Improvements:** Noted improvements in world knowledge (**MMLU-Redux**) due to significant data scaling.
*   **Instruction Following:** Demonstrates competitive instruction following capabilities.
*   **Document QA & RAG:** Strong performance in Document QA and Retrieval Augmented Generation (RAG).
*   **Tool Use:** Achieved comparable tool use performance to major competitors.
*   **Multilingual:** Competitive performance in Korean and Spanish.
*   **Validation:** Needle In A Haystack validation confirmed successful context extension without degradation.

---

## üìù Contributions

1.  **Unified Architecture:** Introduction of a model architecture that eliminates the trade-off between general usability and advanced reasoning capabilities.
2.  **Agentic AI Advancement:** Progress in agentic AI by providing a model natively equipped with tool-use capabilities.
3.  **Accessibility & Deployment:** Promotion of accessibility through the public release of both a high-performance 32B model and an edge-computing optimized 1.2B model.

---

**Quality Score:** 4/10  
**References:** 40 citations