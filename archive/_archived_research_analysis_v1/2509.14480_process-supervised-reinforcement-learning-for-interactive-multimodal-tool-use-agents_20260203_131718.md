---
title: Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use
  Agents
arxiv_id: '2509.1448'
source_url: https://arxiv.org/abs/2509.14480
generated_at: '2026-02-03T13:17:18'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Process-Supervised Reinforcement Learning for Interactive Multimodal Tool-Use Agents

*Weiting Tan; Xinghua Qu; Ming Tu; Meng Ge; Andy T. Liu; Philipp Koehn; Lu Lu*

---

> ### ðŸ“Š Quick Facts
>
> *   **Performance Gain:** >6% improvement in task pass rate on $\tau$-bench.
> *   **Base Model:** Qwen3-8B.
> *   **Key Algorithms:** PPO, GRPO, and TARL (Turn-level Adjudicated RL).
> *   **Environment:** SQLite-based sandbox with RESTful API / MCP tools.
> *   **Interaction Limit:** 30 steps max.
> *   **Behavioral Change:** Average 'wait' tokens reduced from 14.6 to 8.4.

---

## Executive Summary

**Problem**
Training multimodal foundation models to function as autonomous agents capable of complex tool use remains a significant hurdle, particularly when addressing long-horizon tasks where the connection between an action and its eventual outcome is temporally distant. Existing reinforcement learning (RL) methods often struggle with the credit assignment problem in these multi-turn scenarios, failing to identify which specific steps lead to success. Furthermore, developing agents capable of **Tool Integrated Reasoning (TIR)** and managing **long-context dialogue management** while naturally processing interleaved speech and text inputs requires a robust training environment that standard supervised learning approaches fail to provide.

**Innovation**
The core innovation of this work is Turn-level Adjudicated Reinforcement Learning (TARL), a process-supervised RL strategy that deploys a Large Language Model (LLM) as a judge to provide turn-level evaluations. By adjudicating the quality of each interaction step, TARL effectively resolves credit assignment issues in long-horizon tasks. The architecture utilizes a specialized sandbox RL environment supporting interleaved speech-text rollouts and applies Proximal Policy Optimization (PPO) at the token level. Additionally, the authors introduce a refined mixed-task training curriculum that integrates mathematical reasoning problems to broaden the model's exploration capabilities, preventing it from converging prematurely on suboptimal policies during tool-use training.

**Results**
Evaluations on the Retail domain of the text-based $\tau$-bench demonstrate significant performance gains, with the framework reportedly improving task pass rates by over 6% compared to strong existing RL baselines. Specifically, the base Qwen3-8B model achieved a pass^1 rate of 42.6%, which increased to 48.7% using PPO and 51.3% using GRPO. The training also yielded behavioral improvements, reducing the average 'wait' tokens per turn from 14.6 to 8.4, indicating more natural interaction. However, the method faced challenges in generalization; in the Airline domain, performance declined, with GRPO dropping to a 28% pass^1 rate compared to the base model's 32%. Additionally, consistency remained an issue, as pass^2 and pass^3 rates fell below 30% across the board.

**Impact**
This research significantly advances the field of agentic AI by demonstrating that unified RL approaches can successfully adapt multimodal foundation models for natural, voice-driven interactive applications. The introduction of TARL provides a scalable solution to the persistent problem of credit assignment in multi-turn tool use, validating the efficacy of using LLMs as process judges. Furthermore, the successful implementation of a mixed-task curriculum offers a new pathway for improving exploration in RL training for language models. While the generalization challenges highlight areas for future work, the framework establishes a strong foundation for developing sophisticated, voice-enabled agents capable of complex reasoning and tool integration.

---

## Key Findings

*   **Performance Improvement:** The proposed framework improves the task pass rate on the text-based $\tau$-bench by over **6%** compared to strong existing reinforcement learning (RL) baselines.
*   **Fine-Tuning Success:** The methodology successfully fine-tunes multi-modal foundation models for agentic tasks, specifically equipping them with tool-use abilities.
*   **Voice Interaction:** Training on interleaved speech-text rollouts enables the development of more natural, voice-driven interactive agents.
*   **Exploration Enhancement:** The mixed-task training curriculum effectively enhances exploration during the training process.

---

## Methodology

The proposed approach addresses the challenges of training tool-use agents through a multi-faceted strategy:

*   **Environment:** A specialized sandbox RL environment designed for multi-modal contexts, utilizing **interleaved speech-text rollouts**.
*   **Algorithm (TARL):** Employs **Turn-level Adjudicated Reinforcement Learning (TARL)**. This strategy utilizes a Large Language Model (LLM) as a judge for turn-level evaluations to address credit assignment in long-horizon tasks.
*   **Curriculum:** Applies a **mixed-task training curriculum** that integrates mathematical reasoning to enhance exploration.
*   **Training Goal:** Targets the training of a base multi-modal LLM in **Tool Integrated Reasoning (TIR)** and **long-context dialogue management**.

---

## Contributions

1.  **Introduction of TARL:** A novel process-supervised RL strategy leveraging LLM-based adjudication to solve credit assignment problems in complex, multi-turn tool-use scenarios.
2.  **Creation of a Multi-modal RL Environment:** A sandbox environment capable of handling the dynamic process of interactive tool use with interleaved speech and text data.
3.  **Advancement of Voice-Driven Agents:** Demonstration that a unified RL approach can effectively adapt multi-modal foundation models for natural, voice-driven interactive agentic applications.

---

## Technical Details

### System Architecture
*   **Backend:** SQLite-based application with RESTful API endpoints registered as MCP tools.
*   **Simulation:** GPT-4 ReACT User Simulator utilizing SeedTTS.
*   **Verification:** Rule-based Verifier that checks database state changes.

### RL Formulation
*   **Agent Model:** Autoregressive language model where actions are token sampling.
*   **Constraints:** Maximum interaction limit of 30 steps.
*   **Algorithm (PPO):** Operates at the token level, utilizing:
    *   Probability ratios
    *   Clipping mechanisms
    *   Generalized Advantage Estimate (GAE)

### Key Innovations
*   **Turn-level Adjudicated RL (TARL):** Uses an LLM judge for turn-level rewards.
*   **Mixed-Task Training:** Incorporates math problems to encourage exploration.

---

## Results

### Performance Metrics
Evaluation is based on the pass^k metric, average 'wait' tokens, and response length.

| Domain | Model | Pass^1 Rate | Notes |
| :--- | :--- | :--- | :--- |
| **Retail** | Base Qwen3-8B | 42.6% | Baseline performance |
| **Retail** | PPO | 48.7% | Improved over baseline |
| **Retail** | **GRPO** | **51.3%** | Highest performance in Retail |
| **Airline** | Base Qwen3-8B | 32.0% | Generalization failed here |
| **Airline** | GRPO | 28.0% | Drop compared to baseline |

### Behavioral Analysis
*   **Efficiency:** PPO training reduced the average 'wait' tokens per turn from **14.6 to 8.4**, indicating more natural and concise interaction.
*   **Consistency:** Consistency dropped below 30% for pass^2 and pass^3 across the board.
*   **Generalization:** While successful in the Retail domain, the framework struggled to generalize to the Airline domain without performance drops.

---

**References:** 40 citations
**Quality Score:** 8/10