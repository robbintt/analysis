---
title: 'QUAD: Quantization and Parameter-Efficient Tuning of LLM with Activation Decomposition'
arxiv_id: '2503.19353'
source_url: https://arxiv.org/abs/2503.19353
generated_at: '2026-02-03T20:08:13'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# QUAD: Quantization and Parameter-Efficient Tuning of LLM with Activation Decomposition

*Yuxuan Hu; Xiaodong Chen; Cuiping Li; Hong Chen; Jing Zhang*

> ### ðŸ“Š Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Framework** | QUAD (Quantization with Activation Decomposition) |
> | **Quantization** | W4A4 / W4A8 |
> | **Target Models** | Llama-3, Qwen-2.5 (Medium-sized LLMs) |
> | **Peak Accuracy** | Up to **98%** (relative to full precision) |
> | **Key Technique** | Singular Value Decomposition (SVD) |
> | **Quality Score** | 9/10 |

---

## Executive Summary

This research addresses the critical challenge of enabling aggressive 4-bit weight and activation (W4A4) quantization for medium-sized Large Language Models (LLMs), such as Llama-3 and Qwen-2.5. While extreme quantization is essential for reducing memory bandwidth and accelerating inference on resource-constrained hardware, it typically causes significant accuracy degradation due to activation outliersâ€”extreme magnitude values that disrupt standard quantization ranges. Existing rotation-based methods have struggled to suppress these outliers effectively in medium-sized architectures without incurring high computational costs or substantial performance loss, creating a bottleneck for deploying efficient, high-performance models in production environments.

The authors propose QUAD (Quantization with Activation Decomposition), a framework that mathematically isolates and manages outliers using Singular Value Decomposition (SVD). The core technical innovation involves classifying Transformer weight matrices into U-type (Input-side) and D-type (Output-side) categories. An offline calibration process estimates activation singular vectors to construct an orthogonal transformation matrix. This matrix rotates the activation space, shifting outlier components into specific dimensions that are retained in full precision, while the remaining "smooth" components are compressed to 4-bit. The framework executes a three-stage pipeline: transformation to smooth activations, mixed-precision quantization (employing GPTQ for weights), and parameter-efficient fine-tuning that adapts only the retained full-precision outlier weights to restore accuracy.

QUAD demonstrates robust performance under aggressive compression constraints, achieving 94% to 96% of the full-precision baseline accuracy on medium-sized models like Llama-3 and Qwen-2.5. These results outperform existing rotation-based methods such as QuaRot and SpinQuant across standard evaluation benchmarks. Furthermore, when combined with Parameter-Efficient Tuning using a W4A4/A8 configuration, the framework recovers up to 98% of the original accuracy, effectively bridging the performance gap with full-precision models. The method is noted for its efficient offline calibration costs and full compatibility with INT4 tensor cores, validating its practical applicability.

This work is significant because it resolves the long-standing conflict between low-bit compression and activation outliers in modern LLMs. By demonstrating that preserving and tuning only a small subset of outlier dimensions is sufficient to maintain near-original accuracy, QUAD establishes a new standard for unified quantization and tuning strategies. This capability facilitates the deployment of state-of-the-art medium-sized LLMs on edge devices and consumer hardware without sacrificing the reasoning capabilities required of full-precision models, potentially influencing future hardware and software co-design for efficient inference.

---

## Key Findings

*   **High Performance under Aggressive Quantization:** QUAD achieves **94% to 96% accuracy** under aggressive W4A4 quantization for medium-sized LLMs like Llama-3 and Qwen-2.5.
*   **Bridging the Performance Gap:** By combining W4A4/A8 quantization with parameter-efficient fine-tuning, the framework achieves up to **98% accuracy**, effectively closing the gap with full-precision models.
*   **Outlier Suppression:** The method successfully addresses the critical issue of activation outliers that typically cause accuracy degradation in existing quantization methods for medium-sized models.
*   **Superiority over Baselines:** The approach outperforms existing rotation-based methods such as QuaRot and SpinQuant.

---

## Methodology

The proposed framework, **QUAD** (Quantization with Activation Decomposition), utilizes a mathematical transformation strategy to isolate and manage outliers. The core methodology relies on the following principles:

*   **Singular Value Decomposition (SVD):** The framework leverages SVD to identify and suppress activation outliers.
*   **Offline Estimation:** The method estimates activation singular vectors offline using calibration data to construct an orthogonal transformation matrix ($P$).
*   **Transformation Strategy:** This matrix shifts activation outliers into additional dimensions that are retained in full precision, while the remaining components are compressed to 4-bit precision.
*   **Fine-Tuning:** Additionally, the framework allows for the fine-tuning of adaptable full-precision outlier weights to recover any lost accuracy.

---

## Technical Details

QUAD enables aggressive W4A4 quantization for medium-sized LLMs (e.g., Llama-3-8B) by utilizing Singular Value Decomposition (SVD) to isolate and remove activation outliers.

### Matrix Classification
The framework classifies Transformer weight matrices into two categories:
*   **U-type (Input-side)**
*   **D-type (Output-side)**

### The Three-Stage Pipeline
1.  **Transformation**
    *   Offline estimation of singular vectors.
    *   Projection matrix construction.
    *   Orthogonal rotation for smoothing activations.
2.  **Quantization**
    *   Weights quantized via GPTQ.
    *   Online activation quantization.
    *   Mixed-precision strategy: Retains outlier dimensions in full precision while compressing the rest.
3.  **Parameter-Efficient Tuning**
    *   Fine-tuning is performed exclusively on the full-precision components (outlier weights).

---

## Contributions

*   **Framework Introduction:** Introduction of QUAD, a new framework specifically designed to enable effective 4-bit quantization for medium-sized LLMs by resolving the conflict between low-bit compression and activation outliers.
*   **Activation Decomposition Technique:** A novel technique involving the use of offline-calculated SVD to separate outlier components from standard activations, allowing for differential precision handling within the same model.
*   **Unified Approach:** A Unified Quantization and Tuning approach that combines low-bit quantization with parameter-efficient fine-tuning, demonstrating that adapting only the outlier weights in full precision is sufficient to restore near-original accuracy.

---

## Results

*   **W4A4 Performance:** Achieves **94% to 96% accuracy** on medium-sized models (Llama-3 and Qwen-2.5).
*   **W4A4/A8 Performance:** Peak accuracy reaches up to **98%** when combined with Parameter-Efficient Tuning.
*   **Comparison:** Outperforms existing methods like QuaRot and SpinQuant.
*   **Hardware Compatibility:** Designed for INT4 tensor cores with low calibration costs.

---
**References:** 40 citations