# Offline Policy Learning with Weight Clipping and Heaviside Composite Optimization

*Jingren Liu, Hanzhang Qin, Junyi Liu, Mabel C. Chou, Jong-Shi Pang*

***

> ### **Quick Facts**
>
> *   **Quality Score:** 9/10
> *   **Method:** Optimized Clipped Doubly Robust Learning (OCDRL)
> *   **Optimization Type:** Heaviside Composite Optimization (HSCOP)
> *   **Solver:** Progressive Integer Programming (PIP)
> *   **Convergence Rate:** $O(N^{-1/2})$
> *   **Speedup:** Up to >17x faster than standard MIP (7,200s vs 416s)
> *   **References:** 40 citations

***

## Executive Summary

Offline policy learning (OPL) faces a critical stability challenge when using standard reweighting-based methods, such as Inverse Probability Weighting (IPW). These unbiased estimators are prone to high variance when treatment propensity scores are small, making policy value estimates unstable and often leading to the selection of suboptimal policies. This variance problem limits the practical application of OPL in real-world scenarios where data coverage may be sparse, creating a need for estimation techniques that can reduce variance without introducing prohibitive bias.

The authors introduce **Optimized Clipped Doubly Robust Learning (OCDRL)**, a systematic approach that optimally truncates propensity scores to minimize the Mean Squared Error (MSE) of policy value estimation. Because the selection of the clipping threshold is dependent on the policy itself, the authors must solve a complex bilevel and discontinuous optimization problem. They address this by reformulating the problem into a **Heaviside composite optimization framework**, which is then solved using a **Progressive Integer Programming (PIP)** method. This technique partitions the search space based on the signs of Heaviside functions, allowing the algorithm to solve a sequence of smaller, manageable mixed-integer subproblems.

The proposed method demonstrates superior computational efficiency and statistical robustness compared to standard baselines. In computational tests on synthetic data, the PIP solver was significantly faster than standard Mixed-Integer Programming (MIP); in one setup, PIP solved the problem in 247 seconds compared to MIP's 1,684 seconds, and in another, PIP converged in 416 seconds while MIP exceeded the 7,200-second time limit. Statistically, the method achieves an $O(N^{-1/2})$ convergence rate consistent with semi-parametric efficiency bounds. In a controlled toy example, OCDRL successfully prevented the selection of suboptimal policies, whereas standard methods selected suboptimal policies with a 36.5% error probability.

This research provides a rigorous theoretical foundation for weight clipping in offline policy learning, establishing an upper bound on suboptimality that explicitly links estimation error reduction to policy optimization performance. By solving the computational challenges of discontinuous bilevel optimization through Heaviside composite frameworks, the authors bridge the gap between statistical estimation theory and integer programming. This work enables the deployment of more reliable and computationally feasible policy learning algorithms in high-stakes environments where variance management is essential.

## Key Findings

*   **High Variance in Standard Methods:** Standard reweighting-based offline policy learning methods (like IPW) suffer from high variance in policy value estimation when treatment propensity scores are small.
*   **Efficacy of Weight Clipping:** Weight clipping estimators effectively mitigate high variance, specifically when the clipping threshold is optimized to minimize the Mean Squared Error (MSE).
*   **Optimization Reformulation:** The bilevel and discontinuous optimization problem induced by weight clipping can be reformulated as a Heaviside composite optimization problem, making it computationally solvable.
*   **Suboptimality Bound:** The authors established an upper bound on the suboptimality of the algorithm, proving that reducing estimation MSE leads to better policy learning performance.

## Methodology

The authors utilize a weight-clipping estimator that truncates propensity scores based on a threshold chosen to minimize the Mean Squared Error (MSE) of policy value estimation. To handle the complexities of the bilevel and discontinuous objective function, the problem is reformulated into a **Heaviside composite optimization framework**. This reformulated problem is then solved using the **progressive integer programming method**.

## Contributions

*   **Systematic Algorithm:** Introduction of a systematic offline policy learning algorithm based on weight clipping to address the high-variance limitations of traditional unbiased estimators.
*   **Computational Framework:** Provision of a rigorous computational framework for discontinuous policy optimization through Heaviside composite optimization.
*   **Theoretical Bound:** Establishment of a theoretical suboptimality bound that explicitly connects the reduction in estimation error (MSE) to the improvement in downstream policy optimization performance.

## Technical Details

The proposed framework is **Optimized Clipped Doubly Robust Learning (OCDRL)**, designed to address the high variance of standard policy value estimators.

*   **Estimator:** The **Optimized Clipped Doubly Robust (OCDR) Estimator** determines the clipping threshold analytically as the closed-form minimizer of the Mean Squared Error (MSE).
*   **Reformulation Strategy:** The bilevel problem is collapsed into a single-level optimization using the closed-form expression for the clipping threshold. It is then recast as a **Heaviside Composite Optimization Problem (HSCOP)**.
*   **Solver:** The problem is solved using **Progressive Integer Programming (PIP)**. This solver addresses the problem by solving a sequence of MIP subproblems of reduced size, partitioning the search space based on the signs of Heaviside functions.
*   **Implementation:** Algorithm 2 (OCDRL) utilizes sample splitting, where dataset $D_1$ estimates the reward model and dataset $D_2$ constructs the OCDR estimator and invokes the PIP solver.

## Results

### Statistical Performance
*   Achieves an **$O(N^{-1/2})$** convergence rate, consistent with standard semi-parametric efficiency bounds.
*   The variance term scales with $\sqrt{\tau^* \wedge \hat{\tau}}$, which is strictly smaller than classic results that scale with $\sqrt{1/\eta}$.
*   In a toy example, the proposed method prevented the selection of suboptimal policies, whereas standard methods selected suboptimal policies with a **36.5% error probability**.

### Computational Evaluation (Synthetic Data)
*Tested on a 20-dimensional unit cube with 4 treatments.*

*   **Setup 1:** PIP solved in **247 seconds** vs. standard MIP at **1,684 seconds** (6.8x slower).
*   **Setup 2:** PIP solved in **416 seconds** vs. standard MIP hitting the time limit (**7,200 seconds**).
*   **Setup 3:** PIP took **351 seconds** vs. standard MIP taking **5,401 seconds**.
*   **Outcome:** PIP consistently produced feasible solutions where direct MIP approaches failed and found objective values comparable to exact MIP solutions (26-28 range).