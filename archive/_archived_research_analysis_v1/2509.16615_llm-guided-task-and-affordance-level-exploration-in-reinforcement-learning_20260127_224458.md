---
title: LLM-Guided Task- and Affordance-Level Exploration in Reinforcement Learning
arxiv_id: '2509.16615'
source_url: https://arxiv.org/abs/2509.16615
generated_at: '2026-01-27T22:44:58'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# LLM-Guided Task- and Affordance-Level Exploration in Reinforcement Learning

*Jens Kober (Delft University), Runyu Ma (The Netherlands), Jelle Luijkx, Zlatan AjanoviÄ‡ (Cognitive Robotics)*

> ### ðŸ“„ Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Total Citations:** 40
> *   **Framework:** LLM-TALE (Residual Learning Architecture)
> *   **Hardware:** Franka Emika Panda, Intel RealSense D435
> *   **Real-World Success Rate:** 93.3%
> *   **Baseline Comparison:** Outperformed LLM-BC, TD3, and PPO

---

## Executive Summary

Reinforcement Learning (RL) for robotic manipulation is notoriously hindered by sample inefficiency, requiring vast amounts of interaction data to master complex tasks. While Large Language Models (LLMs) possess extensive semantic knowledge about tasks and object relationships, they typically lack the grounding necessary to translate this knowledge into precise, physically feasible control policies. This creates a critical gap where RL is data-intensive, and LLMs are physically unreliable, often failing to account for dynamic constraints such as collision avoidance or fine-grained motor coordination in real-world environments.

The authors introduce **LLM-TALE** (LLM-guided Task- and Affordance-Level Exploration), a framework that integrates LLM reasoning with model-based RL through a Residual Learning Architecture. The LLM operates at two levels: task-level planning to establish high-level goals and affordance-level exploration to identify specific object interaction modalities (e.g., grasp types or placement angles). Technically, the system combines an LLM-generated linear base policyâ€”which directs the robot toward goal posesâ€”with a residual RL policy trained in simulation to refine trajectories for feasibility. Crucially, the framework utilizes an "Affordance-Modality Identifier" prompt mechanism and delegates physical constraints, such as collision avoidance, to the RL component as external reward signals rather than relying on the LLM for safety compliance.

LLM-TALE demonstrated superior sample efficiency in simulation benchmarks (ManiSkill & RLBench) compared to baselines like TD3, PPO, and LLM-Behavior Cloning. In the PegInsert task, the system successfully discerned that "head grasps" were more valuable than "end grasps," while in the PutBox task, it converged on optimal "side picks" and "vertical placements" after approximately 50,000 steps. In real-world Sim-to-Real transfer experiments using a Franka Emika Panda manipulator, LLM-TALE achieved a **93.3% success rate**, drastically outperforming a pure LLM baseline which achieved a **0% success rate** due to collisions.

This research significantly advances the field by validating a method for grounding abstract LLM knowledge into efficient physical control strategies. By demonstrating that LLMs can effectively guide exploration through affordance discovery while relying on residual RL policies for physical safety, the paper offers a scalable solution to the sample efficiency problem in robotics.

---

## Methodology & Contributions

### Proposed Approach
The authors propose a Reinforcement Learning (RL) framework where a Large Language Model (LLM) acts as a high-level planner. The LLM guides the exploration process by suggesting sub-tasks (Task-Level) and identifying actionable properties of objects in the environment (Affordance-Level).

### Core Contributions
*   **Novel Algorithm:** Introduction of an algorithm that improves sample efficiency in RL by grounding LLM knowledge into affordance-based exploration strategies.
*   **Bridging the Gap:** Successfully connects semantic reasoning (LLM) with low-level physical control (Residual RL) to solve the grounding problem in robotics.

---

## Technical Details

The implementation relies on a sophisticated combination of language models and control architectures.

### System Architecture
| Component | Description |
| :--- | :--- |
| **Framework** | LLM-TALE (LLM-guided Task- and Affordance-Level Exploration) |
| **Architecture** | Residual Learning Architecture (LLM linear base policy + Residual RL policy) |
| **Planning Layers** | 1. Task-Level (High-level goals)<br>2. Affordance-Level (Interaction modalities) |

### Hardware Setup
*   **Manipulator:** Franka Emika Panda (with gripper)
*   **Controller:** Cartesian impedance controller operating at 1 kHz
*   **Perception:** Intel RealSense D435 RGB-D camera

### Key Mechanisms
*   **Affordance-Modality Identifier:** A prompt mechanism used to query the LLM for multimodal affordances.
*   **Constraint Handling:** For tasks like PutBox, constraints such as 'No Tilt' and collision avoidance are incorporated as external reward and penalty signals, rather than being specified to the LLM.
*   **Policy Fusion:** The LLM generates a base policy for goal poses, while the residual RL policy (trained in simulation) refines trajectories for feasibility.

---

## Key Findings

*   **Not explicitly provided in source text.**
    *(See Results section for empirical outcomes)*

---

## Results

The performance of LLM-TALE was evaluated across simulation benchmarks and real-world hardware.

### Simulation (ManiSkill & RLBench)
*   **Sample Efficiency:** LLM-TALE demonstrated higher sample efficiency compared to baselines like LLM-BC, TD3, and PPO.
*   **PegInsert Task:** The method identified 'head grasps' as higher value than 'end grasps'.
*   **PutBox Task:** The system converged on 'side picks' and 'vertical placements' as optimal modalities after approximately 50,000 steps.

### Real-World Sim-to-Real Transfer
| Metric | LLM-TALE | LLM-Only Baseline |
| :--- | :--- | :--- |
| **Success Rate (PutBox)** | **93.3%** | 0% (Due to collisions) |
| **Collision Avoidance** | Managed successfully by residual policy | Failed |
| **Vertical Clearance** | Improved during placement | N/A |

The residual policy proved critical, successfully managing collision avoidance during picking and improving vertical clearance during placement.

---

## Analysis Metadata

*   **Quality Score:** 9/10
*   **References:** 40 citations