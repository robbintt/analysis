# EXAONE 3.5: Series of Large Language Models for Real-world Use Cases

*Soyoung An; Kyunghoon Bae; Eunbi Choi; Kibong Choi; Stanley Jungkyu Choi; Seokhee Hong; Junwon Hwang; Hyojin Jeon; Gerrard Jeongwon Jo; Hyunjik Jo; Jiyeon Jung; Yountae Jung; Hyosang Kim; Joonkee Kim; Seonghwan Kim; Soyeon Kim; Sunkyoung Kim; Yireun Kim; Yongil Kim; Youchul Kim; Edward Hwayoung Lee; Haeju Lee; Honglak Lee; Jinsik Lee; Kyungmin Lee; Woohyung Lim; Sangha Park; Sooyoun Park; Yongmin Park; Sihoon Yang; Heuiyeen Yeen; Hyeongu Yun*

---

## Executive Summary

> This paper addresses the challenge of transitioning Large Language Models (LLMs) from experimental benchmarks to reliable, real-world deployment. While existing models excel in general capabilities, they often lack the nuanced instruction following and extended context comprehension required for industrial applications. The authors focus on bridging this gap with a model family that scales effectively across computational constraints—ranging from on-device execution to heavy industrial workloads—while specifically optimizing for bilingual environments (English and Korean).
>
> The technical innovation centers on a specialized tokenizer and a rigorous multi-stage training pipeline rather than standard architectural components. The series features a custom 102,400 vocabulary tokenizer split evenly between English and Korean to maximize bilingual efficiency. The methodology employs a two-stage pre-learning process (general domain followed by domain strengthening) and a replay-based method to extend context length to 32,768 tokens without catastrophic forgetting. Notably, the training strategy utilizes compute-optimal scaling: the 7.8B parameter model is aggressively overtrained on 9T tokens to maximize efficiency, while the flagship 32B model is trained on 6.5T tokens. Post-processing involves Supervised Fine-Tuning using taxonomic systems and instruction evolution, followed by preference optimization using DPO and SimPO.
>
> The EXAONE 3.5 series demonstrates state-of-the-art performance, particularly in instruction following and long-context tasks. The 32B model outperformed Llama 3.1 70B on IFEval and achieved top results on LongBench. Beyond these niche strengths, the model delivers competitive general reasoning capabilities, scoring 87.12% on MMLU, 94.93% on GSM8K, and 87.20% on HumanEval, positioning it competitively against significantly larger models. Validating the bilingual architecture, the model also achieved 73.48% on KMMLU, demonstrating strong proficiency in Korean benchmarks. These metrics are supported by substring-level data decontamination to ensure evaluation integrity.
>
> The significance of this work lies in the establishment of a high-performance, commercially viable open model series that successfully merges research-grade capabilities with production-grade reliability. By releasing the models alongside a defined commercial licensing structure, the authors facilitate a smoother transition from research to market compared to research-only alternatives. This release sets a new standard for scalable, bilingual model families, offering specialized solutions for advanced instruction adherence and long-context analysis across both edge computing and enterprise server deployments.

---

### Quick Facts

| Metric | Details |
| :--- | :--- |
| **Model Variants** | 32B, 7.8B, 2.4B Parameters |
| **Context Window** | 32,768 Tokens |
| **Vocabulary Size** | 102,400 (50% Korean / 50% English) |
| **Architecture** | Decoder-only, SwiGLU, GQA, RoPE |
| **Peak Training** | 9T Tokens (7.8B Model) |
| **License** | Commercial License Structure Available |

---

## Key Findings

*   **Superior Instruction Following:** Achieved the highest scores across seven benchmarks, indicating high proficiency in adhering to complex user prompts.
*   **Advanced Long-Context Comprehension:** Secured top performance in four benchmarks designed to test understanding over long text sequences.
*   **Competitive General Performance:** Delivered results that are competitive against State-of-the-Art (SOTA) models in general tasks.
*   **Scalability:** Successfully scaled the architecture across three distinct parameter configurations (32B, 7.8B, and 2.4B) to suit various deployment needs.

---

## Methodology

The research details the development of the **EXAONE 3.5 series**, a collection of instruction-tuned language models designed explicitly for real-world application.

The methodology prioritizes optimization for practical deployment through **instruction tuning**, ensuring the models excel in scenarios requiring strict adherence to instructions and the processing of long-context tasks. The development process focuses on balancing high performance with the constraints of real-world use, resulting in a family of models that operate efficiently across different hardware scales.

---

## Technical Details

### Architecture & Tokenization
*   **Core Architecture:** Decoder-only Transformer.
*   **Activation:** SwiGLU.
*   **Attention Mechanism:** Grouped Query Attention (GQA).
*   **Positional Embeddings:** Rotary Positional Embeddings (RoPE) with theta set to 1,000,000.
*   **Normalization:** Pre-normalization.
*   **Tokenizer:** BBPE tokenizer with a vocabulary size of 102,400, specifically split evenly between Korean and English (50/50) to optimize for bilingual processing.

### Model Variants
*   **32B Model:** `d_model` = 5,120, 64 layers.
*   **7.8B Model:** `d_model` = 4,096, 32 layers.
*   **2.4B Model:** `d_model` = 2,560, 30 layers.

### Training Pipeline
*   **Pre-Learning:** Two-stage process involving general domain learning followed by domain strengthening.
*   **Context Extension:** Extended to 32,768 tokens using a replay-based method to mitigate catastrophic forgetting.
*   **Decontamination:** Substring-level data decontamination performed (sliding window size S=50, stride=1).
*   **Post-Training:**
    *   Supervised Fine-Tuning (SFT) using taxonomic systems on 8M web corpora.
    *   Instruction evolution techniques applied.
    *   Preference optimization utilizing Direct Preference Optimization (DPO) and SimPO.

---

## Performance Results

### Training Scale
*   **32B:** 6.5T tokens, 1.25×10^24 FLOPs.
*   **7.8B:** 9T tokens, 4.21×10^23 FLOPs (Aggressively overtrained).
*   **2.4B:** 6.5T tokens, 9.36×10^22 FLOPs.

### Benchmark Highlights
*   **Instruction Following:** SOTA results across 7 benchmarks.
    *   Notably outperformed Llama 3.1 70B on IFEval.
*   **Long-Context:** Top results in 4 benchmarks (e.g., LongBench) with context windows up to 32K.
*   **General Reasoning:**
    *   **MMLU:** 87.12%
    *   **GSM8K:** 94.93%
    *   **HumanEval:** 87.20%
*   **Bilingual Proficiency:**
    *   **KMMLU:** 73.48% (Validating Korean effectiveness).

### Deployment Specialization
*   **2.4B Variant:** Optimized for on-device use.
*   **32B Variant:** Engineered for industrial and enterprise applications.

---

## Contributions

*   **Open Model Availability:** Provided a high-performing, open model series available for the research community.
*   **Benchmark Advancement:** Advanced the state-of-the-art in benchmarks specifically regarding instruction following and long-context comprehension.
*   **Commercial Viability:** Established a commercial licensing structure to bridge the gap between research prototypes and production environments.

---

**Document Quality Score:** 6/10
**References:** 40 citations