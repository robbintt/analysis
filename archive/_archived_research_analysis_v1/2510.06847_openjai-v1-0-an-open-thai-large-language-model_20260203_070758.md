---
title: 'OpenJAI-v1.0: An Open Thai Large Language Model'
arxiv_id: '2510.06847'
source_url: https://arxiv.org/abs/2510.06847
generated_at: '2026-02-03T07:07:58'
quality_score: 6
citation_count: 16
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# OpenJAI-v1.0: An Open Thai Large Language Model

*Pontakorn Trakuekul; Attapol T. Rutherford; Jullajak Karnjanaekarin; Narongkorn Panitsrisit; Sumana Sumanakul*

---

## ðŸ“Š Quick Facts

| **Metric** | **Details** |
| :--- | :--- |
| **Base Architecture** | Qwen3-14B |
| **Training Hardware** | 8x H100 GPU Cluster |
| **Training Data** | ~462 Million Tokens |
| **Context Window** | Up to 120,000 Tokens |
| **Training Duration** | < 1 Day |
| **Language Support** | Thai & English (Bilingual) |

---

## Executive Summary

This research addresses the critical scarcity of high-performance, open-source Large Language Models (LLMs) specifically optimized for the Thai language. While global LLMs have rapidly advanced, Thai-specific models often lack the sophisticated capabilities required for complex, real-world applications, such as extended context comprehension and functional tool use. This performance gap limits the development of advanced AI applications for Thai users and forces a reliance on proprietary or English-centric models that fail to adequately capture local linguistic nuances and cultural contexts.

OpenJAI-v1.0 introduces a specialized bilingual model built upon the Qwen3-14B architecture, utilizing a rigorous data curation strategy to enhance functional capabilities. The core technical innovation lies in a fine-tuning pipeline that targets three distinct areas: instruction following, long-context understanding (supporting up to 120,000 tokens), and tool use. The methodology employed LLM-as-a-judge filtered synthetic data, translated tool-calling datasets, and direct instruction-response formatting to prevent "catastrophic forgetting," ensuring the model retained the base model's general abilities while acquiring new specialized skills.

Evaluations demonstrate that OpenJAI-v1.0 sets a new benchmark for open-source Thai models, outperforming both the Qwen3-14B foundation and existing open-source Thai leaders across all target tasks. The model was rigorously assessed on IFBench, measuring strict accuracy across 480 bilingual prompts (300 English, 180 Thai), and on MT-Bench, utilizing a 10-point scale judged by GPT-4o across 8 distinct domains. Long-context capabilities were validated via LongBench-v2, comprising 503 multiple-choice questions, and an MRCR (8-needle) test, confirming the model's ability to maintain high proficiency over extended contexts without sacrificing general performance.

The release of OpenJAI-v1.0 provides a significant open resource for the Thai AI community, democratizing access to state-of-the-art NLP technology. By establishing a new performance standard for open-source Thai models, this work enables developers and researchers to build more complex, practical applications that require advanced features like tool integration and long-context processing. The model's success in balancing bilingual proficiency with specialized functional capabilities serves as a blueprint for future development in low-resource languages, bridging the gap between general-purpose models and localized, high-performance AI solutions.

---

## Key Findings

*   **Superior Performance:** OpenJAI-v1.0 demonstrates better performance compared to other leading open-source Thai models across diverse benchmarks.
*   **Architecture Improvement:** The model successfully improves upon the capabilities of its base architecture, Qwen3-14B, specifically in target tasks.
*   **Memory Retention:** The training process effectively avoided 'catastrophic forgetting,' maintaining the base model's general abilities while acquiring new skills.
*   **Functional Proficiency:** The model exhibits high proficiency in three specific areas:
    *   Instruction following
    *   Long-context understanding
    *   Tool use

---

## Methodology

The researchers developed OpenJAI-v1.0 by building upon the **Qwen3-14B** foundation model. The core methodology centered on a rigorous data curation strategy focused on the **Thai and English languages**.

Specifically, the team curated datasets to fine-tune the model for three distinct use cases to boost performance on practical applications:

1.  **Instruction Following**
2.  **Long-Context Understanding**
3.  **Tool Use**

---

## Technical Details

### Infrastructure & Training
*   **Base Model:** Qwen3-14B
*   **Hardware:** 8x H100 GPU cluster
*   **Training Parameters:** ~462 million tokens
*   **Batch Size:** Global batch size of 256
*   **Duration:** Training completed in less than a day

### Data Strategy
*   **Format:** Direct instruction-response format (Thai and English) without intermediate reasoning steps.
*   **Instruction Following:** Utilized LLM-as-a-judge filtered synthetic data.
*   **Long-Context:** Data supporting up to 120,000 tokens.
*   **Tool Use:** Incorporated translated tool-calling datasets.

### Evaluation Setup
*   **Engine:** vLLM in non-thinking mode
*   **Input Length:** Standardized at 120,000 tokens
*   **Scaling:** Dynamic RoPE scaling

---

## Results & Evaluation

Specific numerical scores were not provided; however, the study defined rigorous evaluation metrics and reported qualitative findings:

*   **IFBench:** Measures strict prompt-level and instruction-level accuracy using 300 English and 180 Thai prompts.
*   **MT-Bench:** Uses a 10-point scale judged by GPT-4o covering 8 domains for English and a culturally adapted version for Thai.
*   **Long-Context:** Measured via:
    *   **MRCR (8-needle):** Testing retrieval within long context.
    *   **LongBench-v2:** 503 Multiple Choice Questions (MCQs) in non-Chain-of-Thought (non-CoT) mode.

**Qualitative Outcomes:**
*   OpenJAI-v1.0 improved upon Qwen3-14B without catastrophic forgetting.
*   Outperformed leading open-source Thai models in target domains.

---

## Contributions

*   **Open Resource Availability:** The public release of OpenJAI-v1.0 provides a high-quality, alternative NLP resource specifically for the Thai AI community.
*   **Advancement of Thai LLMs:** The work establishes a new performance benchmark for open-source Thai models, outperforming existing leaders.
*   **Specialized Capability Integration:** The model contributes a bilingual solution that addresses complex, practical requirements often missing in standard models, such as tool use and long-context processing.

---

*Report generated based on analysis with Quality Score: 6/10 | References: 16 citations*