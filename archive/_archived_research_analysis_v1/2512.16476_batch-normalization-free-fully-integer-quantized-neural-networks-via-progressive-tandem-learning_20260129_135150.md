# Batch Normalization-Free Fully Integer Quantized Neural Networks via Progressive Tandem Learning

*Pengfei Sun; Wenyu Jiang; Piew Yoong Chee; Paul Devos; Dick Botteldooren*

***

> ### üìä Quick Facts Dashboard
>
> *   **Paper Quality Score:** 9/10
> *   **Focus:** Integer-Only Inference & Batch Normalization Removal
> *   **Key Technique:** Progressive Tandem Learning (Teacher-Student Distillation)
> *   **Primary Dataset:** ImageNet (AlexNet), CIFAR-10 (VGG-Small)
> *   **References:** 33 Citations

***

## üìù Executive Summary

Deploying Quantized Neural Networks (QNNs) on resource-constrained edge devices requires strict **integer-only inference** to maximize efficiency. However, standard deep learning pipelines rely heavily on Batch Normalization (BN) to maintain training stability and accuracy. BN layers typically require floating-point operations and running statistics during inference, forcing engineers to use complex workarounds like parameter folding.

This paper addresses the challenge of **eliminating the BN dependency entirely** without suffering the performance degradation or instability usually associated with its removal, particularly under aggressive low-bit quantization. The authors introduce **"Progressive Tandem Learning,"** a teacher-student distillation framework designed to convert a standard BN-enabled pre-trained model into a BN-free, integer-only student model.

The technique employs a two-stage process:
1.  **Initialization:** Quantized weights are initialized layer-by-layer, learning fixed, integer-friendly scale factors by matching the teacher's post-BN activations.
2.  **Distillation:** A layer-wise progressive distillation method trains each student layer sequentially to mimic the teacher's targets while preceding layers are frozen.

**Impact:**
*   **ImageNet (AlexNet, 8-bit):** Student error 47.72% vs Teacher 47.00% (+0.72% gap).
*   **CIFAR-10 (Binary):** Achieved 90.8% accuracy, surpassing the DoReFa-Net baseline.
*   **Deployment:** Enables a "true" integer-only pipeline, removing the need for BN running statistics and simplifying workflows for embedded hardware.

***

## üîë Key Findings

*   **True Integer-Only Inference:** Successfully trains a Quantised Neural Network (QNN) that performs inference **exclusively using integer arithmetic**, eliminating the need for Batch Normalization (BN) running statistics.
*   **Competitive Accuracy:** On the ImageNet dataset using the AlexNet architecture, the BN-free model attained **Top-1 accuracy comparable to standard models**, even under aggressive low-bit quantization conditions.
*   **Superior Stability:** Unlike previous methods relying on parameter folding or tailored initialization, this approach maintains high accuracy and stability **without bespoke constraints** on the network.
*   **Seamless Integration:** The procedure integrates directly with **existing low-bit quantization workflows**, making it a practical solution for upgrading current pipelines.

***

## üß† Methodology

The researchers utilize a progressive, layer-wise distillation scheme referred to as **Progressive Tandem Learning**. The process framework includes:

*   **Teacher-Student Framework:** Starts with a pretrained teacher model that includes Batch Normalization.
*   **Layer-wise Targets:** Employs specific targets to guide the training process layer by layer.
*   **Progressive Compensation:** Implements a mechanism to train a student model that mimics the teacher's performance but utilizes **no BN operations**.
*   **Integer Constraint:** The resulting student model is trained to conduct all inference operations using integers only.

***

## ‚öôÔ∏è Technical Details

**Core Method:** Progressive Tandem Learning  
**Objective:** Eliminate Batch Normalization (BN) dependencies to enable end-to-end integer-only inference by converting a BN-enabled pretrained teacher into a BN-free student model.

**Training Pipeline Stages:**

| Stage | Description | Technical Operation |
| :--- | :--- | :--- |
| **Stage 1** | **Initialization** | Initializes quantized weights layer-by-layer and learns a fixed, integer-friendly scale factor ($\alpha_\ell$) by matching the teacher's post-BN activations. |
| **Stage 2** | **Progressive Distillation** | Involves layer-wise progressive distillation where each layer is trained sequentially to align with teacher targets while preceding layers are frozen. |

**Inference Architecture:**
*   Contains **no BN layers**.
*   Relies exclusively on:
    *   Quantized weights
    *   Fixed per-layer scale factors
    *   Integer-valued operations (convolutions/matrix multiplications)
    *   Integer transformation layer and rescaling
*   **Compatibility:** Integrates with existing low-bit pipelines like DoReFa-Net without requiring bespoke parameter folding.

***

## üìà Contributions

*   **Elimination of BN Dependency:** Provides a viable pathway to remove the BN layer without the performance penalties associated with traditional removal techniques like parameter folding.
*   **Novel Training Strategy:** Introduces a progressive, layer-wise distillation method that recovers the stability and accuracy typically lost when removing BN, specifically tailored for low-bit quantized environments.
*   **Practical Deployment Utility:** Enables end-to-end integer-only inference that slots into standard workflows, significantly lowering the barrier for deploying efficient, low-energy models on resource-constrained edge and embedded devices.

***

## üìä Results

**ImageNet (AlexNet)**
*   **8-bit weights/activations:**
    *   Student Error: **47.72%**
    *   Teacher Error: 47.00%
    *   *Gap: +0.72%*
*   **4-bit weights/activations:**
    *   Student Error: **49.45%**
    *   Teacher Error: 48.50%
    *   *Gap: +0.95%*

**CIFAR-10 (VGG-Small)**
*   **Binary Configuration (1-bit weights/activations):**
    *   Proposed Method: **90.8% accuracy**
    *   Baseline (DoReFa-Net): 90.2%
    *   *Result: Surpassed baseline*

**Experimental Configuration**
*   Gradient Precision: 32-bit
*   Tested Precisions: (8, 8, 32), (4, 4, 32), and (1, 1, N/A).