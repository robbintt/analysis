# MILE: Model-based Intervention Learning

*Yigit Korkmaz; Erdem BÄ±yÄ±k*

---

> ### ðŸ‘ï¸ Executive Summary
>
> Current interactive imitation learning (IIL) methods suffer from inherent data inefficiency because they treat expert interventions as discrete, binary events. By discarding the timeline where the expert remains passive, these methods fail to recognize that the absence of intervention is a critical indicator of state quality and action optimality. This oversight leads to significant data sparsity and compounding errors, limiting the applicability of IIL in complex environments where expert bandwidth is expensive and data collection is constrained.
>
> The authors introduce "Model-based Intervention Learning" (MILE), a framework that mathematically formulates the intervention mechanism to extract latent feedback from the entire oversight timeline. Unlike standard methods, MILE employs a learned intervention policyâ€”a Behavior Cloned Mental Model ($\pi_\zeta$) trained on suboptimal rolloutsâ€”to explicitly model expert criteria regarding state quality. This system interprets non-intervention as a meaningful positive supervisory signal. The suboptimal agent and the mental model are trained jointly, with a hyperparameter $c$ tuned to maintain an intervention ratio below 30%, allowing the framework to operate without assumptions regarding the environment's reward structure before discarding the mental model during inference.
>
> MILE achieved the best overall performance across simulation, robotics, and human studies, significantly outperforming strong baselines including HG-DAgger, Sirius, RLIF, and IWR. Validated primarily by success rates (with raw rewards used for Lunarlander), the method demonstrated superior data efficiency using a restricted budget of just $N=1$ iteration and $k=15$ episodes. Under these constraints, where all methods started from the same suboptimal policy without external expert data, MILE generally outperformed its peersâ€”tying only with HG-DAgger in a single instanceâ€”while maintaining low intervention ratios and achieving high success rates in diverse 4 Degrees of Freedom (DoF) tasks.

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 7/10 |
| **Total Citations** | 40 |
| **Action Space** | 4 Degrees of Freedom (Cartesian + Gripper) |
| **Training Config** | $N=1$ iteration, $k=15$ episodes |
| **Intervention Ratio** | < 30% (regulated via hyperparameter $c$) |
| **Primary Metric** | Success Rate (Raw Reward for Lunarlander) |

---

## Key Findings

*   **Data Underutilization:** Current interactive imitation learning methods waste vast amounts of data by ignoring timesteps where the expert does not intervene.
*   **The Value of Inaction:** Expert inaction provides crucial implicit information regarding state quality and the optimality of the agent's current action.
*   **High Efficiency:** The proposed MILE framework achieves high data efficiency, requiring only a few interventions to improve policy performance.
*   **Broad Validation:** The method is successfully validated across diverse domains, including simulation environments, robotics tasks, and human-in-the-loop studies.

---

## Methodology

The paper introduces **Model-based Intervention Learning (MILE)**, a novel framework designed to mathematically formulate intervention mechanisms. 

Unlike standard approaches that view intervention as a simple binary event (intervention vs. no intervention), MILE takes a holistic approach by modeling the expert's underlying decision-making process. This allows the system to extract **latent feedback** from the data. 

By interpreting the absence of intervention as a meaningful signal of state quality, MILE can learn from the entire oversight timeline rather than just the moments the expert takes control. This shift maximizes the utility of the expert's presence.

---

## Contributions

*   **Novel Formulation:** Introduces a new model of intervention dynamics capable of extracting implicit signals from oversight data.
*   **Resolving Sparsity:** Addresses the issues of data sparsity and compounding errors by maximizing the utility of sparse expert interventions.
*   **Paradigm Shift:** Establishes a new standard in the field by treating both intervention and non-intervention timesteps as valuable supervision signals, leading to significant improvements in sample efficiency.

---

## Technical Details

The implementation of MILE relies on the following technical specifications and configurations:

*   **Action Space:** Utilizes a 4 Degrees of Freedom (DoF) space, comprising Cartesian positions and gripper state.
*   **Reward Structure:** The system operates without requiring prior knowledge or assumptions regarding the environment's reward structure.
*   **Simulated Setup:**
    *   **Mental Model ($\pi_\zeta$):** A Behavior Cloned (BC) policy trained on suboptimal policy rollouts; used to simulate the expert's decision criteria.
    *   **Human Policy ($\pi_h$):** Replaced by an expert policy ($\pi^*$) during the data collection phase.
*   **Training Regimen:**
    *   Rollouts are generated by a suboptimal agent.
    *   The suboptimal agent and the mental model undergo joint training.
    *   The mental model is discarded during inference.
*   **Hyperparameter Tuning:** A specific hyperparameter $c$ is tuned to ensure the intervention ratio remains below 30%.

---

## Results

MILE demonstrated superior performance across all tested environments when compared against state-of-the-art baselines.

*   **Baselines:** Compared against BC intervention, HG-DAgger, RLIF, IWR, and Sirius.
*   **Setup Fairness:** All methods started from the same initial suboptimal policy without access to external expert data.
*   **Budget:** MILE was trained with a highly restricted budget of $N=1$ iteration and $k=15$ episodes, while baselines were matched on the number of interventions received.
*   **Performance:**
    *   **Best Overall:** MILE achieved the best overall performance (measured by mean Â± standard deviation).
    *   **Exception:** Tied with HG-DAgger in one specific instance.
    *   **Metric:** Success rates were used as the primary metric, with the exception of Lunarlander, which utilized raw rewards.
*   **Key Insight:** The inclusion of simulated interventions was shown to significantly increase success rates, validating the framework's ability to leverage implicit feedback.