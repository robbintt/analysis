# Policy gradient methods for ordinal policies

*SimÃ³n Weinberger; Jairo Cugliari*

> ### ðŸ“Š Quick Facts
> ---
> *   **Primary Domain:** Control of Electrochromic Lenses (Smart Glasses)
> *   **Top Performing Method:** TRPO with Ordinal Policy
> *   **Stability Gain:** ~7x reduction in reward standard deviation (~5 vs ~35)
> *   **Performance Gap:** Final reward of ~-30 (Ordinal) vs ~-120 (Multinomial)
> *   **Key Innovation:** Integrating ordinal regression thresholds into Policy Gradient
> *   **Experimental Parameters:** $\gamma=0.9$, 60 steps/episode, 10 random seeds

---

## Executive Summary

Standard reinforcement learning (RL) algorithms for discrete action spaces typically rely on the softmax (multinomial) parametrization, which treats actions as independent, categorical labels. This approach is fundamentally limited when actions possess a natural order (e.g., intensity levels or rankings), as it fails to capture the adjacency relationships between consecutive actions. This limitation represents a significant inefficiency in real-world industrial applications where control inputs are ordinal.

The authors introduce a novel **Ordinal Policy Parametrization** class that adapts statistical ordinal regression models to the policy gradient framework. Technically, the method models action selection using a latent variable $A^* = g_\omega(s) + e$, where $g_\omega(s)$ is a score function and $e$ represents logistic noise. Instead of assigning independent weights to each action, the policy employs $K-1$ ordered thresholds ($\tau$) to partition the latent space. The probability of selecting action $a$ is computed as the area under the sigmoid curve between two adjacent thresholds.

In a simulated industrial task modeling the control of electrochromic lenses (smart glasses), the ordinal policy outperformed standard Multinomial baselines across REINFORCE, NPG, and TRPO algorithms. Specifically, the combination of **TRPO with the ordinal policy** achieved a final mean total reward of approximately **-30**, nearing the optimal achievable performance. This work bridges the gap between ordinal regression techniques and deep reinforcement learning, offering a robust alternative to the de facto softmax standard.

---

## Key Findings

*   **Structural Limitation of Softmax:** The standard softmax parametrization is fundamentally limited for discrete action spaces as it fails to capture the order relationship between actions.
*   **Industrial Viability:** The proposed ordinal policy parameterization addresses practical challenges and demonstrates effectiveness in real-world industrial applications.
*   **Continuous Action Competitiveness:** When applied to continuous action tasks via action space discretization, the ordinal policy achieves performance competitive with existing methods.
*   **Validation:** Numerical experiments confirm that adapting ordinal regression models to the reinforcement learning setting is highly effective.

---

## Methodology

The research methodology centers on a shift away from the standard softmax approach toward a novel policy parametrization that respects action order.

*   **Framework Adaptation:** The core methodology involves adapting ordinal regression models to function within a reinforcement learning framework.
*   **Continuous Action Handling:** To handle continuous action tasks, the approach employs discretization of the action space, followed by the application of the proposed ordinal policy structure.
*   **Algorithms:** The proposed approach is validated across multiple policy gradient algorithms.

---

## Contributions

*   **New Parametrization Class:** Introduction of a new Ordinal Policy Parametrization class that integrates the concept of order into action selection.
*   **Cross-Domain Adaptation:** Successful adaptation of statistical ordinal regression techniques for use in policy gradient methods.
*   **Continuous Strategy:** Contribution to the handling of continuous action spaces by demonstrating that discretization combined with ordinal policies is a viable and competitive strategy.

---

## Technical Details

### Mathematical Formulation
The model utilizes a latent variable formulation to represent the decision process:

*   **Latent Variable:** $A^* = g_\omega(s) + e$
    *   $g_\omega$: Score function
    *   $e$: Logistic noise
*   **Thresholds:** $K-1$ ordered thresholds ($\tau$) partition the latent space.
*   **Probability Calculation:**
    $$ \pi(a|s) = \sigma(\tau_a - g_\omega(s)) - \sigma(\tau_{a-1} - g_\omega(s)) $$
*   **Parameters:** The model parameters combine the score function weights and the threshold parameters.

### Algorithm Compatibility
The formulation is theoretically compatible with major policy gradient methods:
*   REINFORCE
*   Natural Policy Gradient (NPG)
*   Trust Region Policy Optimization (TRPO)
*   Proximal Policy Optimization (PPO)

### Application Domain
*   **Task:** Control of electrochromic lenses (smart glasses).
*   **State:** Ambient light sensor data.
*   **Action:** Tint levels (ordered actions).
*   **Reward:** User acceptance.

---

## Results

The experimental evaluation highlighted significant advantages of the ordinal policy over standard Multinomial (Softmax) baselines.

### Performance Metrics
*   **Convergence:** Ordinal policies converged faster and achieved higher final total rewards.
*   **Stability:** Demonstrated superior stability, characterized by a significant reduction in the standard deviation of rewards (**~5** for Ordinal vs **~35** for Multinomial).
*   **Optimal Configuration:** The combination of **TRPO** with the ordinal policy yielded the best performance.

### Experimental Setup
*   **Discount Factor ($\gamma$):** 0.9
*   **Steps per Episode:** 60
*   **Episodes per Run:** 400
*   **Random Seeds:** 10
*   **Reward Function:** Negative absolute difference between proposed and preferred tint.

### Comparison Details (TRPO)
*   **Ordinal Policy TRPO:** Final mean total reward of approx. **-30**.
*   **Multinomial TRPO Baseline:** Final mean total reward of roughly **-120**.

### Continuous Tasks
When applied to continuous action tasks via discretization, the ordinal policy showed performance competitive with existing continuous control methods (e.g., DDPG).

---
**Quality Score:** 8/10  
**References:** 0 citations