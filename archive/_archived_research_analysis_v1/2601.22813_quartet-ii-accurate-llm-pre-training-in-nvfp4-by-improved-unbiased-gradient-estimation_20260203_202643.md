---
title: 'Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient
  Estimation'
arxiv_id: '2601.22813'
source_url: https://arxiv.org/abs/2601.22813
generated_at: '2026-02-03T20:26:43'
quality_score: 9
citation_count: 23
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation

*Andrei Panferov; Erik Schultheis; Soroush Tabesh; Dan Alistarh*

---

> ### ‚ö° Quick Facts
>
> *   **Performance Speedup:** Up to **4.2x** faster than BF16 baselines on NVIDIA Blackwell GPUs.
> *   **Quantization Accuracy:** **>2x lower** quantization error compared to Stochastic Rounding.
> *   **Validation Scale:** Tested on **1.9B** parameters over **38B** tokens.
> *   **Core Innovation:** MS-EDEN (Micro-Scaled Efficient Deterministic Error Normalization).
> *   **Target Architecture:** Fully-NVFP4 pipeline for NVIDIA Blackwell.
> *   **Loss Reduction:** Validation loss decreased by at least **20%** in models up to 200M parameters.

---

## üìã Executive Summary

Training Large Language Models (LLMs) requires immense computational resources, driving the industry toward lower-precision numerical formats like NVIDIA‚Äôs micro-scaled NVFP4 for Blackwell GPUs. However, adopting this format presents a critical challenge: maintaining training accuracy. Existing low-precision methods often rely on Stochastic Rounding (SR) to ensure unbiased gradient estimation, but SR introduces high variance that degrades model performance and destabilizes training. Consequently, the field has faced a difficult trade-off where achieving unbiased gradients necessitates sacrificing the representation capacity of the format, hindering the adoption of 4-bit formats for end-to-end pre-training.

The authors introduce **Quartet II**, a comprehensive quantization scheme that enables accurate, fully-NVFP4 pre-training by addressing the limitations of Stochastic Rounding. The core technical breakthrough is **MS-EDEN** (Micro-Scaled Efficient Deterministic Error Normalization), a novel unbiased quantization routine for the backward pass. MS-EDEN utilizes Rotated Hadamard Transform (RHT) rotations to achieve unbiased gradient estimation while reducing variance and quantization error. The authors provide analytical proofs that Quartet II offers consistent improvements in gradient estimation across major matrix multiplications. In the forward pass, the scheme employs Round-to-Nearest FP4 rounding combined with a dynamic local scaling strategy to maximize representation. Furthermore, the authors developed highly optimized custom kernels for NVIDIA Blackwell GPUs that reformulate these rotations as simple General Matrix Multiplications (GEMM), ensuring hardware efficiency.

Quartet II demonstrates superior performance across both theoretical metrics and practical benchmarks. MS-EDEN achieves over 2x lower quantization error compared to standard Stochastic Rounding. In large-scale experiments, Quartet II reduced validation loss by at least 20% in models up to 200 million parameters compared to existing baselines. During Nanochat pre-training, the method decreased the loss gap relative to BF16 by 15‚Äì25% versus prior NVFP4 methods. Crucially, the team successfully validated the approach by end-to-end pre-training a 1.9 billion parameter model on 38 billion tokens with accuracy retention comparable to FP16/FP8. On the system level, custom kernels implemented on NVIDIA Blackwell GPUs achieved up to 4.2x speedup over BF16 baselines.

This research bridges the precision-accuracy gap in 4-bit LLM training, effectively resolving the historical trade-off between unbiased gradient estimation and representation capacity. By establishing MS-EDEN as a new state-of-the-art standard with analytically proven improvements, Quartet II validates the feasibility of training massive models exclusively in NVFP4 without sacrificing convergence or final accuracy. The release of custom kernels and source code alongside the paper provides a practical, high-performance pipeline for the research community, significantly lowering the barrier to adopting efficient 4-bit pre-training for next-generation AI infrastructure.

---

## üîë Key Findings

*   **Novel Quantization Routine:** Introduction of **MS-EDEN**, a novel unbiased quantization routine for micro-scaled formats, achieving more than **2x lower quantization error** compared to Stochastic Rounding.
*   **Proven Gradient Estimation:** The proposed Quartet II scheme provides analytically proven improvements in gradient estimation across major matrix multiplications for both forward and backward passes.
*   **End-to-End Validation:** Successful validation on end-to-end LLM pre-training (**1.9B parameters on 38B tokens**) demonstrating accuracy retention relative to FP16/FP8 training.
*   **Hardware Performance:** Custom kernels developed for NVIDIA Blackwell GPUs utilizing the Quartet II scheme achieved up to **4.2x speedup** over BF16 baselines.

---

## üî¨ Methodology

The research focuses on algorithmic innovation by replacing standard Stochastic Rounding with a proprietary unbiased quantization routine called **MS-EDEN**, designed to maximize the representation capacity of micro-scaled formats like NVFP4. This routine is integrated into a comprehensive, fully-NVFP4 quantization scheme named **Quartet II**, which targets linear layers to enable end-to-end quantized pre-training.

The study employs analytical methods to demonstrate that Quartet II offers consistent improvements in gradient estimation compared to existing methods. Additionally, the approach is validated through large-scale experiments (training 1.9B parameter models on 38B tokens) and benchmarked on NVIDIA Blackwell hardware against BF16 performance metrics.

---

## ‚öôÔ∏è Technical Details

*   **Forward Pass Configuration:**
    *   Utilizes Round-to-Nearest FP4 rounding.
    *   Implements native NVFP4 scaling.
    *   Uses a '4/6' local scaling level choice.
*   **Backward Pass Configuration:**
    *   Replaces Stochastic Rounding with **MS-EDEN** (Micro-Scaled Efficient Deterministic Error Normalization).
    *   Employs pseudo-random group RHT (Rotated Hadamard Transform) rotations for unbiased gradient estimation.
*   **Hardware Implementation:**
    *   Optimized for NVIDIA Blackwell GPUs.
    *   Utilizes a rotation group size of **128** to reformulate rotation as a simple GEMM (General Matrix Multiplication).
*   **Comparison to Predecessors:**
    *   Improves upon methods like TetraJet-v2 by integrating MS-EDEN and '4/6' scaling strategies.

---

## üìä Results

*   **Quantization Error:** MS-EDEN achieves over **2x lower** quantization error than Stochastic Rounding.
*   **Scaling Efficiency:** '4/6' scaling roughly doubles the improvement in reducing validation loss gaps compared to square-block quantization.
*   **Small Model Pre-training:** In LLM pre-training (up to 200M parameters), Quartet II reduces validation loss by at least **20%** compared to baselines.
*   **Nanochat Pre-training:** Decreases the loss gap relative to BF16 by **15-25%** versus existing NVFP4 methods.
*   **Gradient Estimation:** Quartet II maintains unbiased gradient estimates, unlike Four Over Six on the backward pass.
*   **System Validation:** Validation on a 1.9B parameter model showed up to **4.2x speedup** over BF16 baselines on NVIDIA Blackwell GPUs.

---

## üèÜ Contributions

*   **Bridging the Gap:** Resolves the trade-off where representation capacity was previously sacrificed for unbiased gradient estimation in NVFP4 training.
*   **New State-of-the-Art:** Establishes a new standard for quantized training by introducing MS-EDEN, which significantly outperforms traditional Stochastic Rounding.
*   **Practical Solution:** Delivers a high-performance solution optimized for the NVIDIA Blackwell architecture, offering a fully-quantized training pipeline.
*   **Open Source:** Facilitates the adoption of efficient 4-bit pre-training for massive models by releasing custom kernels and code as open-source.

---

**Quality Score:** 9/10  
**References:** 23 citations