# StableQuant: Layer Adaptive Post-Training Quantization for Speech Foundation Models

*Yeona Hong; Hyewon Han; Woo-jin Chung; Hong-Goo Kang*

---

### ðŸ”Ž Quick Facts

| Metric | Value | Detail |
| :--- | :--- | :--- |
| **Compression Ratio** | 4x | Reduces model size to a quarter |
| **Speed Improvement** | 2x | Inference acceleration compared to FP |
| **Accuracy Limit** | < 0.3% | Max WER degradation allowable |
| **Target Precision** | W8A8 / W6A6 | W4A4 results in failure (>89% WER) |
| **Hardware** | NVIDIA 3090 GPU | Evaluation environment |
| **Key Innovation** | Adaptive Cut-off ($p_{opt}$) | Layer-wise scaling factor search |

---

## Executive Summary

Speech Foundation Models (SFMs), such as HuBERT and wav2vec 2.0, deliver state-of-the-art Automatic Speech Recognition (ASR) performance but suffer from massive computational footprints that hinder deployment on resource-constrained edge devices. While Post-Training Quantization (PTQ) has successfully reduced the size of Large Language Models (LLMs) without retraining, these techniques fail when applied to SFMs. The hybrid architecture of speech modelsâ€”which combines CNN feature extractors with Transformer layersâ€”leads to catastrophic accuracy degradation under standard PTQ. Consequently, achieving compression currently requires expensive Quantization-Aware Training (QAT), creating a barrier to efficient, real-time speech processing.

To address this, the authors propose **StableQuant**, a layer-adaptive PTQ algorithm specifically optimized for the heterogeneous structure of SFMs. The core technical contribution is an **adaptive layer-wise scaling factor search algorithm** that determines a dynamic cut-off ratio ($p_{opt}$) for each individual layer. Rather than applying a uniform quantization strategy, StableQuant analyzes layer-specific scale distributions to adaptively identify the optimal quantization range for both weights and activations. This architecture-agnostic approach accommodates the varying sensitivities of CNN and Transformer layers, allowing for effective calibration and optimization that strictly avoids the computational overhead of fine-tuning or backpropagation.

StableQuant achieves significant compression and speed improvements while maintaining strict accuracy limits. At W6A6 precision, the method attains a WER of 3.98% for HuBERT and 6.10% for wav2vec 2.0, a drastic improvement over baseline methods that fail catastrophically (>42% WER). For HuBERT-Large at W8A8 precision, StableQuant reduces model size by 3.4x (from 1205 MB to 353 MB) with a WER of 2.35%. This result is comparable to QAT (2.34%) and superior to AdaRound (2.66%), representing a degradation of less than 0.3% relative to full precision. Additionally, inference speeds increase by approximately 2x. However, the method faces a lower boundary at W4A4 precision, where extreme compression results in unacceptable performance degradation (>89% WER).

This research bridges the efficiency gap between LLM and SFM compression by providing empirical evidence that speech models can be aggressively compressedâ€”down to a quarter of their original sizeâ€”without costly retraining. By offering a plug-and-play solution that balances high compression with robust performance, StableQuant facilitates the practical deployment of high-fidelity speech recognition on edge devices. While the method demonstrates clear efficacy for W6A6 and W8A8 precisions, the identified limitations at W4A4 establish a current practical boundary for zero-shot quantization in speech processing, guiding future research in ultra-low-bit model compression.

---

## Key Findings

*   **Superior Performance:** StableQuant significantly outperforms traditional Post-Training Quantization (PTQ) methods on Speech Foundation Models.
*   **High Efficiency:** Achieves **4x compression** (reducing model size to 25%) and **2x inference speed**.
*   **Accuracy Retention:** Maintains high accuracy with **Word Error Rate (WER) degradation strictly limited to less than 0.3%**.
*   **Baseline Comparison:** Prevents the catastrophic failure (>42% WER) seen in baseline methods when operating at similar precision levels (W6A6).

---

## Methodology

The authors propose **StableQuant**, a novel layer-adaptive post-training quantization algorithm designed specifically for Speech Foundation Models. The methodology focuses on addressing the unique challenges of SFMs:

*   **Architecture Awareness:** It accounts for distinct network architectures (specifically the mix of CNN feature extractors and Transformers) by adaptively determining the quantization range for each individual layer.
*   **Distribution Analysis:** The algorithm analyzes layer-specific scale distributions and overall model performance to optimize parameters.
*   **No Fine-Tuning Required:** The entire process is achieved without requiring additional fine-tuning (Zero-Shot), unlike Quantization-Aware Training (QAT).

---

## Contributions

The study makes three primary contributions to the field of model compression:

1.  **Bridging the Gap:** It bridges the gap in PTQ effectiveness between Large Language Models (LLMs) and SFMs by introducing an architecture-agnostic solution.
2.  **Leveraging Scale Analysis:** The method leverages layer-specific scale distribution analysis to solve optimization issues unique to speech models.
3.  **Empirical Evidence:** Provides strong empirical evidence on HuBERT and wav2vec 2.0 models, demonstrating that SFMs can be aggressively compressed and accelerated with negligible impact on performance, avoiding the computational cost of fine-tuning.

---

## Technical Details

StableQuant introduces a specific protocol to handle the quantization of complex speech models:

*   **Core Algorithm:** Layer-adaptive calibration strategy with an **adaptive layer-wise scaling factor search** algorithm.
*   **Quantization Parameter:** Determines a dynamic cut-off ratio ($p_{opt}$) for quantizing models below INT8 precision.
*   **Architecture Support:** Specifically addresses architectures containing both CNN feature extractors and Transformer layers.
*   **Quantization Scheme:** Uses **Weight-Activation quantization** (tested configurations include W6A6 and W8A8).
*   **Evaluation Protocol:**
    *   **Hardware:** NVIDIA 3090 GPU
    *   **Calibration Set:** LibriSpeech 'dev-clean'
    *   **Evaluation Set:** LibriSpeech 'test-clean'

---

## Results

The performance of StableQuant was benchmarked against full-precision models and other quantization methods:

### W6A6 Precision Performance
*   **HuBERT:** Achieved a WER of **3.98%**.
*   **wav2vec 2.0:** Achieved a WER of **6.10%**.
*   *Comparison:* Significantly outperforms baselines which suffer catastrophic failure (**>42% WER**).

### W8A8 Precision Performance (HuBERT-Large)
*   **Compression:** Reduced model size by **3.4x** (1205 MB $\to$ 353 MB).
*   **Accuracy:** Achieved **2.35%** WER.
    *   Comparable to QAT (2.34%).
    *   Better than AdaRound (2.66%).
*   **Inference Speed:** Increased by approximately **2x** compared to full precision.

### Extreme Precision Limits
*   **W4A4 Precision:** Results in high degradation (**>89% WER**), indicating a hard boundary for this method.

---

**Paper Quality Score:** 8/10
**References:** 29 citations