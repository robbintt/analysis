# DMQ: Dissecting Outliers of Diffusion Models for Post-Training Quantization

*Dongyeun Lee; Jiwan Hur; Hyounguk Shon; Jae Young Lee; Junmo Kim*

---

> ### üìä Quick Facts
>
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations
> *   **Quantization Targets:** W4A6, W4A8
> *   **Performance:** FID ~14.0‚Äì15.5
> *   **Key Innovation:** Learned Equivalent Scaling (LES)

---

## Executive Summary

This paper addresses the critical challenge of applying Post-Training Quantization (PTQ) to diffusion models, specifically focusing on maintaining performance at aggressive low bit-widths. Existing PTQ methods suffer significant degradation in image generation quality at low bits (4-bit) due to the presence of overlooked outliers in model weights and activations. The authors highlight that the diffusion process is particularly sensitive to quantization errors during early denoising steps, where error accumulation disproportionately impacts the final output. Additionally, the research identifies skip connections as critical bottlenecks due to their high inter-channel variance, which complicates the application of uniform quantization schemes.

The authors propose **DMQ**, a quantization framework designed to mathematically preserve operations while mitigating outlier effects. The core innovation is **Learned Equivalent Scaling (LES)**, which optimizes channel-wise scaling factors to dynamically redistribute quantization difficulty; this method scales down activations ($X' = X / \alpha$) and scales up weights ($W' = \alpha \cdot W$) to reduce outlier magnitudes without altering the computational result. To address the specific complexities of diffusion layers, DMQ employs Channel-wise Power-of-Two Scaling (PTS) for high-variance skip connections and utilizes a Voting Algorithm to determine robust scaling factors from small calibration sets. Furthermore, the framework implements **Adaptive Timestep Weighting**, which applies higher optimization priority to early denoising steps to minimize error propagation throughout the generation process.

DMQ was evaluated against state-of-the-art methods in aggressive quantization scenarios, specifically W4A6 and W4A8 (4-bit weights with 6-bit and 8-bit activations, respectively). The method achieved Fr√©chet Inception Distance (FID) scores ranging approximately between 14.0 and 15.5, with Mean Squared Error (MSE) values between 0.14 and 0.26. These metrics demonstrate that DMQ significantly outperforms existing techniques, maintaining high image generation fidelity. Additionally, analysis across timesteps 0 to 1000 confirmed that the framework maintains stability, particularly during the critical early denoising phases where previous methods typically falter.

The significance of this research lies in enabling robust, low-bit deployment of large-scale diffusion models without the extensive computational cost of Quantization-Aware Training (QAT). By successfully demonstrating state-of-the-art performance in 4-bit weight quantization (W4A6/W4A8), DMQ facilitates the efficient deployment of generative AI on resource-constrained hardware.

---

## Key Findings

*   **Outlier Impact:** Existing PTQ methods for diffusion models suffer from performance degradation at low bit-widths due to overlooked outliers in model weights and activations.
*   **Timestep Sensitivity:** Early denoising steps have a disproportionately high impact on final output quality due to error accumulation.
*   **Skip Connection Variance:** Specific layers, particularly skip connections, display high inter-channel variance which complicates uniform quantization.
*   **Superior Performance:** The proposed DMQ method significantly outperforms state-of-the-art techniques at aggressive quantization levels (W4A6 and W4A8) while maintaining high image generation fidelity and model stability.

---

## Methodology

The study proposes the **DMQ** quantization framework, which utilizes a four-pronged approach to manage the complexities of diffusion models:

1.  **Learned Equivalent Scaling (LES):** Optimizes channel-wise scaling factors to dynamically redistribute quantization difficulty.
2.  **Adaptive Timestep Weighting:** Applies higher priority to early denoising steps during the learning phase to counteract error propagation.
3.  **Channel-wise Power-of-Two Scaling (PTS):** Specifically targets layers with high inter-channel variance (like skip connections) for hardware-friendly alignment.
4.  **Voting Algorithm:** Robustly determines scaling factors using only small calibration sets, reducing the need for extensive data.

---

## Contributions

*   **Challenge Identification:** Identifies critical quantization challenges including outliers in diffusion model PTQ and activation variance in skip connections.
*   **Optimization Framework:** Introduces a novel optimization framework with Learned Equivalent Scaling (LES) for balanced error reduction.
*   **Timestep Scheme:** Develops an adaptive timestep weighting scheme to handle error propagation throughout the diffusion process.
*   **Low-Bit Deployment:** Enables robust low-bit deployment by achieving state-of-the-art performance in 4-bit weight quantization scenarios (W4A6/W4A8).

---

## Technical Details

The DMQ method mitigates outlier distribution in diffusion models for Post-Training Quantization (PTQ). It identifies skip connections as critical bottlenecks due to high inter-channel variance.

**Equivalent Scaling Mechanism**
The approach utilizes Equivalent Scaling with a factor $\alpha$, where operations are mathematically preserved but magnitudes are adjusted:
*   **Activations:** Scaled down using $X' = [X / \alpha]$
*   **Weights:** Scaled up using $W' = [\alpha \cdot W]$

This reduces outlier magnitudes without altering the computational result of the layer.

**Scaling Refinement**
The framework further employs **Power-of-Two (PoT) Scaling** refinement using factors of $1/2^n$ on activations to target extreme values and align with hardware-friendly quantization grids.

---

## Results

DMQ was evaluated at aggressive quantization levels, specifically **W4A6** and **W4A8** (4-bit weights, 6/8-bit activations).

*   **FID Scores:** Reported Fr√©chet Inception Distance (FID) values range approximately between **14.0 and 15.5**.
*   **MSE Values:** Mean Squared Error (MSE) values range between **0.14 and 0.26**.
*   **Stability:** Analysis across timesteps 0 to 1000 indicates stability in early denoising steps.
*   **Comparison:** The method significantly outperforms state-of-the-art techniques in W4A6 and W4A8 settings, maintaining high image generation fidelity and model stability.