---
title: 'Safety Game: Balancing Safe and Informative Conversations with Blackbox Agentic
  AI using LP Solvers'
arxiv_id: '2510.0933'
source_url: https://arxiv.org/abs/2510.09330
generated_at: '2026-02-03T07:09:38'
quality_score: 7
citation_count: 15
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Safety Game: Balancing Safe and Informative Conversations with Blackbox Agentic AI using LP Solvers

*Authors: Tuan Nguyen; Long Tran-Thanh*

---

## üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 7/10 |
| **References** | 15 Citations |
| **Core Method** | Linear Programming (LP) Solvers & Game Theory |
| **Operational Mode** | Inference-Time Alignment |
| **Model Access** | Black-Box / Model-Independent |

---

## üìù Executive Summary

> Current alignment methodologies for Large Language Models (LLMs), such as Reinforcement Learning from Human Feedback (RLHF) and fine-tuning, face a critical trade-off between safety and helpfulness, often resulting in models that are either overly cautious or dangerously permissive. These existing approaches are inherently rigid and resource-intensive, requiring white-box access to model internals and necessitating expensive, full-scale retraining whenever safety protocols evolve.
>
> This paper introduces **"Safety Game,"** a novel, model-agnostic framework that addresses these limitations by treating user-LLM interaction as a one-shot, imperfect-information, two-player zero-sum game. The core technical innovation lies in deploying external Linear Programming (LP) solvers at inference time to compute the minimax equilibrium strategy that optimizes the balance between safety and helpfulness.
>
> Operating as a wrapper around a frozen LLM, the system employs model-agnostic probing to generate helpfulness and safety scores based on log-likelihoods of YES/NO tokens, normalized via Log-Sum-Exp (LSE). The research validates this framework through Multiple-Choice Question Answering (QA), benchmarking against established baselines. The results confirm that the system successfully enforces safety alignment without internal model access, democratizing access to AI safety tools for third-party stakeholders and resource-constrained entities.

---

## üîë Key Findings

*   **Feasibility of Black-Box Alignment:** The study demonstrates that safety alignment is achievable without accessing model internals or performing costly retraining, validating a model-independent approach.
*   **Optimal Balance via Game Theory:** The research shows that the trade-off between safety and helpfulness can be mathematically optimized using a two-player zero-sum game framework.
*   **Efficacy of Inference-Time Computation:** LLM agents can successfully utilize Linear Programming (LP) solvers at inference time to compute equilibrium strategies, effectively balancing competing objectives in real-time.
*   **Accessibility for Resource-Constrained Entities:** The results indicate that this method offers a scalable and accessible pathway for smaller organizations or third-party stakeholders to enforce safety standards within rapidly evolving LLM ecosystems.

---

## üõ†Ô∏è Methodology

The research employs a distinct mathematical and computational approach to ensure safety without model retraining:

*   **Game Theoretic Formulation:** The core dilemma of balancing safe versus helpful responses is modeled as a **two-player zero-sum game**.
*   **Optimization Strategy:** The framework seeks a **minimax equilibrium** within this game to identify the optimal strategy that balances safety and helpfulness.
*   **Computational Mechanism:** Linear Programming (LP) solvers are leveraged by LLM agents specifically at **inference time** (rather than during training) to compute these equilibrium strategies dynamically.
*   **Black-Box Operation:** The methodology functions as a **wrapper** around the LLM, requiring no interaction with the model's internal parameters or architecture (model-independent).

---

## ‚öôÔ∏è Technical Details

The paper proposes a sophisticated black-box, inference-time alignment framework. Below are the specific technical components:

*   **Game Model:** Models LLM-user interaction as a **one-shot, imperfect-information game**.
*   **Optimization Goal:** Utilizes an LP solver to maximize expected helpfulness lift relative to a safe fallback, subject to a strict **risk cap**.
*   **Scoring Mechanism:**
    *   Uses model-agnostic probing on a frozen LLM.
    *   Generates helpfulness and safety scores via log-likelihoods of **YES/NO tokens**.
    *   Normalizes scores using **Log-Sum-Exp (LSE)**.
*   **Smoothing Function:** Introduces a **Sigmoid Penalty function** to smooth the optimization landscape and address boundary sensitivity issues.

---

## üöÄ Contributions

This work offers four primary advancements to the field of AI safety:

1.  **Elimination of Retraining Costs:** Provides a solution to the inflexibility of current alignment methods (like RLHF and fine-tuning) by removing the need for retraining whenever new safety requirements arise.
2.  **True Black-Box Capability:** Ensures that safety alignment can be enforced by third-party stakeholders who do not own the model or have access to its internals, unlike recent inference-time alignment efforts.
3.  **Novel Application of LP Solvers:** Introduces the specific use of linear programming solvers as a mechanism for LLM agents to operationalize safety game theory during inference.
4.  **Democratization of AI Safety:** Lowers the barrier to entry for enforcing AI safety, providing a viable method for smaller entities in resource-constrained settings to manage LLM behavior.

---

## üìà Results

*   **Evaluation Setup:** Designed around **Multiple-Choice Question Answering (QA)**.
*   **Planned Metrics:** Accuracy and BLEU-based Truthfulness Scores.
*   **Benchmarks:** Performance is positioned against Askell et al. (2021), Srivastava et al. (2022), and Zhang et al. (2023).
*   **Quantitative Data:** Specific numerical results (e.g., accuracy percentages, latency measurements) are not available in the provided text excerpts.
*   **Qualitative Outcomes:** Confirms the feasibility of safety alignment without accessing model internals and validates the efficacy of using LP solvers at inference time.