# MGA-VQA: Secure and Interpretable Graph-Augmented Visual Question Answering with Memory-Guided Protection Against Unauthorized Knowledge Use

*Ahmad Mohammadshirazi; Pinaki Prasad Guha Neogi; Dheeraj Kulshrestha; Rajiv Ramnath*

---

### ðŸ“Œ Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Base Model** | Gemma-3 12B |
| **Benchmarks Tested** | 6 (FUNSD, CORD, SROIE, DocVQA, STE-VQA, RICO) |
| **Max Input Resolution** | 896x896 pixels |
| **Memory Architecture** | 768-Slot Dual Memory System (256 Direct + 512 Indirect) |
| **Quality Score** | 6/10 |
| **References** | 40 Citations |

---

## Executive Summary

### Problem
Current Document Visual Question Answering (DocVQA) systems struggle with balancing computational efficiency against the need to process complex, high-resolution layouts. Furthermore, these models often act as "black boxes," lacking the transparency required for high-stakes environments. A critical security flaw exists in "unauthorized knowledge use," where models hallucinate answers based on pre-training data rather than the provided document context, compromising data sovereignty.

### Innovation
The authors introduce MGA-VQA, a multi-modal framework designed for secure and interpretable document analysis. The core innovation lies in the **Memory-Guided Protection mechanism**, which couples Spatial Graph Reasoning with a dual-structured memory architecture. By mapping document regions into a spatial graph, the system models physical relationships explicitly and utilizes a 768-slot memory system to ensure all answers are strictly grounded in the provided document, eliminating external knowledge retrieval.

### Results
MGA-VQA achieved superior accuracy and efficiency compared to state-of-the-art methods across six benchmarks. It successfully handled multi-hop reasoning and spatial localization without relying on external data. The model efficiently processed inputs up to 896x896 pixels using dynamic, question-guided compression, with the memory configuration proving sufficient for complex reasoning across varied document types.

### Impact
This research establishes a new standard for secure document intelligence by transitioning from opaque processing to transparent, auditable reasoning. By enforcing memory access controls to prevent unauthorized knowledge use, MGA-VQA offers a vital solution for security-sensitive industries like finance, legal, and healthcare, providing a scalable blueprint for future systems that prioritize data privacy and explainability.

---

## Key Findings

*   **Superior Performance:** Achieved higher accuracy and efficiency than current methods across six major benchmarks: FUNSD, CORD, SROIE, DocVQA, STE-VQA, and RICO.
*   **Enhanced Localization:** Demonstrated consistent improvements in spatial localization capabilities alongside accurate answer prediction.
*   **Solved Efficiency Bottlenecks:** The framework effectively addresses efficiency issues in processing high-resolution documents through question-guided compression.
*   **Complex Reasoning:** Successfully handles intricate challenges such as multi-hop reasoning and explicit spatial relationship modeling.
*   **Security & Interpretability:** Shifts the paradigm from black-box models to interpretable, graph-based decision pathways.

---

## Methodology

MGA-VQA utilizes a multi-modal framework comprising four distinct components designed to enhance security and reasoning:

1.  **Token-level Encoding:**
    Processes textual semantics and visual features to create a foundational representation of the document.

2.  **Spatial Graph Reasoning:**
    Models explicit spatial relationships within document layouts, moving beyond simple sequential processing to understand the physical structure of the page.

3.  **Memory-augmented Inference:**
    Supports complex reasoning through structured memory access, ensuring that answers are derived strictly from the document context to prevent hallucination.

4.  **Question-guided Compression:**
    Dynamically manages high-resolution inputs by focusing processing power on relevant areas of the document based on the user's question.

This methodology shifts the focus from black-box models to **interpretable, graph-based decision pathways**, ensuring that the system's logic can be audited and verified.

---

## Technical Architecture

MGA-VQA is built on the **Gemma-3 12B** foundation and features a multi-modal pipeline with five specific technical modules:

### Visual Encoding
*   **Multi-scale Processing:** Inputs processed at resolutions of 224x224, 448x448, and 896x896.
*   **Output:** Generates 4096-dimensional token-level embeddings (range: 512-2048 tokens).

### Spatial Graph Construction
*   **Graph Structure:** Weighted graph built over OCR boxes.
*   **Edges:** defined by distance thresholds ($\tau=100$) and semantic similarity ($\delta=0.6$).
*   **Weights:** Computed from:
    *   Spatial distance
    *   Alignment (Gaussian kernels $\sigma_h=20, \sigma_v=30$)
    *   Semantic coefficients ($\alpha=0.4, \beta=0.3, \gamma=0.3$).
*   **Reasoning Layer:** 3-layer GCN with GELU activation and residual connections.

### Memory System
The system employs a dual-memory architecture to secure and structure reasoning:
*   **Direct Memory (256 slots, 1024-dim):**
    *   Stores answer candidates.
    *   Scoring based on OCR and entity likelihood ($\lambda=0.6$).
*   **Indirect Memory (512 slots):**
    *   Stores k-means centroids for broader context clustering.
*   **Retrieval Mechanism:** Utilizes cross-attention to facilitate multi-hop reasoning across disparate document regions.

---

## Contributions

*   **Unified Framework:** Introduction of a comprehensive multi-modal framework that integrates token encoding, spatial graphs, and memory mechanisms to solve fundamental DocVQA limitations.
*   **Advancement of Interpretability:** Improved transparency over opaque models through graph-based decision pathways and structured memory access logs.
*   **Technical Solutions:** Direct solution to specific DocVQA challenges including explicit spatial modeling, multi-hop reasoning, and high-resolution document efficiency.
*   **Comprehensive Validation:** Established the model's generalization capability through rigorous testing across six diverse datasets.

---

## Results & Validation

Evaluated across six benchmarks (**FUNSD, CORD, SROIE, DocVQA, STE-VQA, RICO**), the model demonstrated:

*   **Accuracy & Efficiency:** Outperformed state-of-the-art methods in both speed and correctness.
*   **Reasoning Capabilities:** Effective handling of multi-hop reasoning across distinct document regions.
*   **Spatial Modeling:** Success in explicit spatial relationship modeling.
*   **Interpretability:** Provision of auditable reasoning pathways and memory access traces, allowing users to verify the source of the model's answers.

---

*Quality Score: 6/10 | References: 40 citations*