---
title: 'SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training
  Quantization for LLMs'
arxiv_id: '2512.04746'
source_url: https://arxiv.org/abs/2512.04746
generated_at: '2026-02-03T19:29:17'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs

*Wenhua Cheng; Weiwei Zhang; Heng Guo; Haihao Shen*

---

> ### **Quick Facts**
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations
> *   **Methodology:** Post-Training Quantization (PTQ)
> *   **Primary Focus:** Extremely Low-Bit (2-bit & 4-bit) Quantization
> *   **Key Advantage:** No mixed-precision strategies required

---

## Executive Summary

The deployment of Large Language Models (LLMs) is often hindered by the memory and computational bottlenecks of full-precision inference. While Post-Training Quantization (PTQ) offers a remedy, reducing models to extremely low bit-widths (2-4 bits) typically causes severe accuracy degradation. Current industry standards often compensate for this by employing complex, heuristic-based mixed-precision strategies, which introduce significant operational complexity and tuning overhead. The field requires a PTQ solution that achieves production-grade accuracy at low bit-widths without necessitating these cumbersome mixed-precision schemes or the resource-intensive retraining associated with Quantization-Aware Training (QAT).

SignRoundV2 introduces a novel PTQ framework centered on a learnable rounding mechanism that fundamentally departs from standard nearest-integer rounding. Unlike traditional methods that rely on mixed-precision allocations to preserve accuracy, SignRoundV2 optimizes the rounding direction and quantization scales directly. It employs a gradient-aware sensitivity metric, **DeltaLoss**, which utilizes a first-order Taylor expansion to estimate the impact of quantization errors on the final loss. This metric drives a lightweight pre-tuning search that determines the optimal rounding and scaling parameters for each layer, stabilizing the model in low-bit regimes without the overhead of manual bit-width assignment.

Extensive testing on large-scale models, including Llama 2 and Llama 3 (70B parameters), confirms that SignRoundV2 significantly closes the accuracy gap with full-precision baselines. The framework achieves a performance variance of approximately 1% at 4-5 bit settings and maintains strong accuracy retention even at aggressive 2-bit weight configurations (W2A16). Benchmark comparisons indicate that SignRoundV2 outperforms standard heuristic mixed-precision implementations, such as those found in Llama.cpp, matching the performance of QAT methods while remaining entirely training-free.

---

## Key Findings

*   **Addresses Low-Bit Degradation:** Successfully mitigates severe performance degradation typically associated with quantizing LLMs to extremely low bit-widths (2-bit and 4-bit).
*   **Simplified Deployment:** Achieves high performance without relying on mixed-precision strategies, significantly simplifying the deployment pipeline.
*   **Performance Gap Closure:** Effectively closes the performance gap between quantized models and full-precision baselines.
*   **Production-Grade Accuracy:**
    *   Approximately **1% variance** at 4-5 bits.
    *   Strong retention of accuracy at aggressive **2-bit** settings.

---

## Methodology

SignRoundV2 is a post-training quantization (PTQ) framework designed specifically for Large Language Models. Its approach is defined by two core components:

1.  **Gradient-Aware Sensitivity Metric:** The framework utilizes a fast sensitivity metric that synthesizes gradient information with quantization-induced deviations. This metric is critical for informing layer-wise bit allocation.
2.  **Lightweight Pre-Tuning Search:** The method employs a search mechanism to determine optimal quantization scales. This process is specifically targeted to improve stability and accuracy in extremely low-bit quantization scenarios.

---

## Technical Details

SignRoundV2 builds upon the foundation of SignRoundV1, employing a quantization-dequantization operation with trainable parameters for scale and rounding.

*   **DeltaLoss Metric:** Introduction of a novel sensitivity metric using a first-order Taylor expansion to estimate the impact of quantization on loss, with a primary focus on activation distortion.
*   **Adaptive Allocation:** Utilization of adaptive bit-width allocation optimized via **Dynamic Programming**. This process minimizes the total estimated loss under a target average bit-width.
*   **Scale Initialization:** Performance of a pre-tuning search to initialize optimal scale parameters. This is achieved by minimizing reconstruction error weighted by activation magnitude.

---

## Results

*   **Llama 2/3 Performance:** Achieved approximately 1% variance compared to full-precision baselines at 4-5 bits. Demonstrated strong accuracy retention at aggressive 2-bit (W2A16) settings on Llama 2/3 70B models.
*   **Superiority to Heuristics:** Outperforms heuristic-based mixed-precision assignments, such as standard Llama.cpp implementations.
*   **Efficiency vs. QAT:** Matches the performance of Quantization-Aware Training (QAT) methods while avoiding the high costs and data requirements associated with retraining.

---

## Contributions

*   **Novel Metric:** Introduction of a rapid, gradient-aware sensitivity metric (DeltaLoss) that guides the allocation of bits across different layers.
*   **Search Algorithm:** Development of a lightweight search algorithm for quantization scales that enhances model stability and accuracy in low-bit environments.
*   **Framework Validation:** Demonstration that a PTQ framework can sustain competitive accuracy and close the gap with full-precision models without the complexity of mixed-precision schemes.
*   **Open Source Release:** Release of the open-source implementation (SignRoundV2) via the Intel auto-round repository to facilitate reproducibility.