# Beyond LLMs: A Linguistic Approach to Causal Graph Generation from Narrative Texts
*Zehan Li; Ruhua Pan; Xinyu Pi*

---

> ### üìä Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Quality Score** | 7/10 |
> | **References** | 30 Citations |
> | **Dataset** | 100 chapters, 750 annotated sentences |
> | **Comparison** | Outperforms GPT-4o & Claude 3.5 |
> | **Core Tech** | RoBERTa + XGBoost + LLM Hybrid |
> | **Key Innovation** | The "Expert Index" (7 linguistic features) |

---

## üìã Executive Summary

Current approaches to extracting causal relationships from narrative texts rely heavily on end-to-end generation using Large Language Models (LLMs) such as GPT-4o and Claude 3.5. While these models possess strong semantic capabilities, they lack the precision necessary to distinguish explicit causal links from mere temporal or semantic associations. This deficiency leads to causal graphs that, while readable, frequently suffer from logical inconsistencies and "hallucinated" connections. As narrative analysis becomes essential for understanding complex storylines and agent interactions, there is a critical need for a method that prioritizes logical rigor alongside natural language understanding to accurately map high-level causality to specific event details.

The authors propose a **hybrid architecture** that decouples event extraction from causal relationship identification through a structured four-stage pipeline. First, LLMs are utilized exclusively to extract concise, agent-centered vertices. The core innovation is the **"Expert Index"**‚Äîa novel set of seven linguistically informed features, including *Genericity* and *Eventivity*, which are derived using RoBERTa. These features are fused with standard RoBERTa embeddings to serve as inputs for an XGBoost classifier. This classifier operates within the Situation-Task-Action-Consequence (STAC) framework to categorize event types and accurately identify causal edges. Finally, the pipeline employs a structured five-iteration prompting mechanism to refine the output, ensuring that linguistic theory drives the classification process rather than raw probabilistic generation.

In empirical testing involving 100 narrative chapters and trained on 750 annotated sentences from 23 distinct references, the proposed framework consistently outperformed state-of-the-art baselines, including GPT-4o and Claude 3.5, in both accuracy and precision. Crucially, the results demonstrate a **"Balanced Readability and Accuracy"**; the method successfully preserves the high-level readability of the generated graphs while capturing detailed, event-specific causal chains, a significant improvement over the verbose or inconsistent outputs of monolithic models. Furthermore, this superior performance was achieved with lower computational costs compared to pure LLM approaches, validating the efficiency of the hybrid design.

This research challenges the prevailing reliance on monolithic LLMs for structured knowledge extraction by demonstrating that specialized linguistic features significantly enhance performance. By validating that a hybrid model leveraging the semantic power of LLMs while grounding logic in linguistic theory yields superior results, the paper establishes a new standard for narrative understanding. The release of an interpretable, open-source tool provides the research community with a practical, efficient solution for automated causal analysis, suggesting that the future of graph generation lies in such theoretically grounded hybrid architectures.

---

## üîç Key Findings

*   **Superior Causal Graph Quality:** The proposed framework consistently outperforms state-of-the-art Large Language Models (specifically GPT-4o and Claude 3.5) in generating high-quality causal graphs from narrative texts.
*   **Enhanced Precision:** The integration of linguistic features into the classification model results in higher precision for identifying causal links compared to pure LLM-based approaches.
*   **Balanced Readability and Accuracy:** The method maintains high readability of the generated graphs while successfully bridging high-level causality with detailed, event-specific relationships.
*   **Empirical Validation:** In experiments involving 100 narrative chapters and short stories, the approach demonstrated consistent performance in capturing nuanced causal chains.

---

## üèóÔ∏è Methodology

The framework utilizes a hybrid, multi-stage pipeline designed to decouple natural language understanding from logical causal structuring.

1.  **Vertex Extraction**
    *   Utilizes Large Language Models (LLMs) to summarize narrative text.
    *   Extracts concise, agent-centered vertices to serve as graph nodes.

2.  **Feature Integration**
    *   Implements an **"Expert Index"** consisting of seven linguistically informed features.
    *   These features bridge the gap between semantic meaning and causal logic.

3.  **Classification Model**
    *   Combines **RoBERTa** embeddings with the Expert Index.
    *   Operates within a **Situation-Task-Action-Consequence (STAC)** model to accurately identify causal links.

4.  **Graph Construction**
    *   Employs a structured five-iteration prompting process.
    *   Refines outputs to build final connected causal graphs.

---

## ‚öôÔ∏è Technical Details

The proposed framework is an End-to-End model utilizing a rigorous four-stage pipeline:

**1. Vertices Extraction**
*   **Tool:** LLMs via LangChain.
*   **Process:** Transforms raw text into single-event, agent-centered, active voice sentences.

**2. Expert Index Extraction**
*   **Tool:** RoBERTa.
*   **Output:** Extracts seven specific linguistic features (including *Genericity* and *Eventivity*).

**3. STAC Categorization**
*   **Tool:** XGBoost classifier.
*   **Input:** Combined RoBERTa embeddings and encoded expert indices.
*   **Function:** Classifies events into *Situation*, *Task*, *Action*, or *Consequence*.

**4. Graph Construction**
*   **Method:** Assembles causal graphs based on multi-factorial or probabilistic definitions.
*   **Refinement:** Uses a structured five-iteration prompting mechanism.

---

## üìÅ Contributions

*   **The Expert Index:** Introduction of a novel set of seven linguistically informed features designed to enhance causal relationship identification beyond semantic embeddings.
*   **Hybrid Architecture:** Development of a specific architecture that fuses RoBERTa embeddings with the proposed Expert Index within an STAC classification framework, reducing reliance on end-to-end LLM generation.
*   **Open-Source Tooling:** Release of an interpretable and efficient open-source tool for causal analysis, providing the research community with a practical solution for extracting causality from narratives.

---

## üìà Results

*   **Performance:** The framework demonstrated higher precision and accuracy compared to GPT-4o and Claude 3.5 baselines, attributed to the integration of linguistic features.
*   **Training Data:** Utilized 750 annotated sentences from 23 sources.
*   **Validation:** Performed across 100 narrative chapters.
*   **Efficiency:** Achieved lower computational costs than pure LLM methods while maintaining high readability and capturing nuanced causal chains.

---