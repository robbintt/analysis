---
title: 'AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution
  in Multi-Agent Reinforcement Learning'
arxiv_id: '2510.01586'
source_url: https://arxiv.org/abs/2510.01586
generated_at: '2026-02-03T12:52:03'
quality_score: 9
citation_count: 9
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning

*Zhenyu Pan; Yiting Zhang; Zhuo Liu; Yolo Yunlong Tang; Zeliang Zhang; Haozheng Luo; Yuwei Han; Jianshu Zhang; Dennis Wu; Hong-Yu Chen; Haoran Lu; Haoyang Fang; Manling Li; Chenliang Xu; Philip S. Yu; Han Liu*

> ### ðŸ“‹ Quick Facts
>
> *   **Quality Score:** 9/10
> *   **References:** 9 citations
> *   **Attack Success Rate (ASR):** Reduced to < 20% (Baselines up to 38.33%)
> *   **Task Utility:** +3.67% improvement on reasoning tasks
> *   **Overhead:** Eliminated (Internalized safety)
> *   **Core Mechanism:** Adversarial Co-Evolution in POMG

---

### Executive Summary

Current multi-agent systems relying on Large Language Models (LLMs) face significant security vulnerabilities, particularly regarding adversarial attacks (jailbreaks) and cross-agent safety risks inherent in complex delegation chains. Existing defenses typically rely on external guardrails or post-hoc verification mechanisms; these approaches introduce substantial computational overhead, create single points of failure, and fail to instill intrinsic safety awareness within the agents themselves. This paper addresses the critical challenge of securing multi-agent interactions without compromising the operational efficiency or task performance of the system.

The authors introduce **AdvEvo-MARL**, a novel framework that internalizes safety through adversarial co-evolution within a Partially Observable Markov Game (POMG). The methodology utilizes a max-min game-theoretic objective where "attacker" agents synthesize evolving jailbreak prompts to challenge "defender" agents, who are simultaneously trained to resist these inputs while accomplishing their duties. To stabilize the training process against the high variance typical of adversarial reinforcement learning, the researchers implemented a Group Policy Update mechanism. This technique leverages a shared, group-level mean-return baseline for advantage estimation, allowing for synchronous updates and more stable convergence across diverse backbone models.

Empirical evaluations demonstrate that AdvEvo-MARL significantly outperforms baseline methods in both security and utility. The framework achieved an Attack Success Rate (ASR) of below 20%, a substantial reduction compared to baseline methods which experienced ASRs as high as 38.33%. Furthermore, the system successfully refutes the traditional trade-off between safety and performance; rather than degrading capability, the internalized safety approach led to a +3.67% improvement on reasoning tasks. The results also confirmed the elimination of external computational overhead and successful mitigation of risks across complex agent interactions.

This research represents a paradigm shift in securing autonomous systems by moving from external, reactive oversight to internalized, proactive safety capabilities. By demonstrating that safety and task utility can be optimized simultaneously, AdvEvo-MARL offers a scalable path to deploying reliable multi-agent systems in high-stakes environments. The elimination of single-point-of-failure risks and external computational costs addresses major bottlenecks in current architectures, providing a robust foundation for future research into safe, cooperative AI ecosystems.

---

## Key Findings

*   **Significantly Reduced Attack Success Rate (ASR):** Achieved an ASR below 20%, representing a major improvement compared to baselines which reached up to 38.33%.
*   **Preserved or Enhanced Task Utility:** Demonstrated that safety does not come at the cost of performance, with up to a **+3.67% improvement** observed on reasoning tasks.
*   **Elimination of External Overhead:** Successfully internalized safety mechanisms, avoiding the computational costs and single-point-of-failure risks associated with external guardrails.
*   **Effective Cross-Agent Safety:** Addressed risks arising from complex interactions and delegation chains between agents.

---

## Methodology

The researchers propose **AdvEvo-MARL**, a co-evolutionary multi-agent reinforcement learning (MARL) framework designed to internalize safety directly into task agents. The methodology operates through two primary components:

*   **Adversarial Joint Optimization:** This process involves a dynamic interplay between two agent roles:
    *   **Attackers:** Synthesize evolving jailbreak prompts to challenge the system.
    *   **Defenders:** Trained to accomplish their assigned duties while simultaneously learning to resist these adversarial inputs.
*   **Public Baseline for Advantage Estimation:** To ensure stability during training, agents share a group-level mean-return baseline. This approach stabilizes the adversarial co-evolution process and significantly reduces update variance.

---

## Technical Details

The paper proposes a Multi-Agent Reinforcement Learning (MARL) framework utilizing adversarial co-evolution between Attacker and Defender agents to internalize safety awareness.

*   **Environment Modeling:** Modeled as a **Partially Observable Markov Game (POMG)**.
*   **Game-Theoretic Objective:** Utilizes a **max-min** objective to optimize the defense against the worst-case attacks.
*   **Training Pipeline:**
    *   **Warm-up Phase:** Uses Supervised Fine-Tuning (SFT) on adversarial prompts to prepare the models.
    *   **Adversarial MARL Phase:** The core co-evolutionary training stage.
*   **Update Mechanism:** Employs a **Group Policy Update** mechanism which utilizes:
    *   A public baseline.
    *   Advantage estimation.
    *   Synchronous updates across agents.
*   **Diversity:** The framework trains multiple distinct backbone models to ensure agent diversity and robustness.

---

## Contributions

*   **Addressed Defense Limitations:** Identified and solved limitations in current defenses regarding self-verification gaps and external guard vulnerabilities.
*   **Novel Internalized Safety Framework:** Introduced a new framework shifting safety from an external add-on to an intrinsic capability using adversarial co-evolution.
*   **Stabilization Technique:** Contributed a specific technique for MARL stabilization via a group-level mean-return baseline to reduce update variance.
*   **Empirical Evidence:** Provided concrete evidence that safety and utility can be optimized simultaneously in LLM-based multi-agent systems.

---

## Results

AdvEvo-MARL demonstrated superior performance in both security metrics and task efficiency:

*   **Security:** Achieved an **ASR of below 20%** (vs. baseline up to 38.33%).
*   **Performance:** Showed a **+3.67% improvement** on reasoning tasks.
*   **Architecture:** Eliminated external computational overhead and single-point-of-failure risks.
*   **Robustness:** Successfully addressed cross-agent safety risks in complex interaction scenarios.