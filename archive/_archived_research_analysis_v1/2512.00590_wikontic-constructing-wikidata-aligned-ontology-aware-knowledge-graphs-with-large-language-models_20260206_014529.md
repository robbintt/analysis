---
title: 'Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with
  Large Language Models'
arxiv_id: '2512.00590'
source_url: https://arxiv.org/abs/2512.00590
generated_at: '2026-02-06T01:45:29'
quality_score: 9
citation_count: 23
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Wikontic: Constructing Wikidata-Aligned, Ontology-Aware Knowledge Graphs with Large Language Models

*Alla Chepurova; Aydar Bulatov; Mikhail Burtsev; Yuri Kuratov*

---

> ### ðŸ“Š Quick Facts
> **Methodology:** Wikontic Pipeline (Extraction, Constraint Enforcement, Normalization)
> **Dataset Focus:** MuSiQue, HotpotQA, MINE-1
> **Top Performance:** 96% Recall (MuSiQue), 86% Score (MINE-1)
> **Efficiency Metric:** <1,000 output tokens
> **Improvement:** 20x reduction vs. GraphRAG
> **Quality Score:** 9/10

---

## Executive Summary

The research addresses the inefficiency and quality inconsistency inherent in current methods for constructing Knowledge Graphs (KGs) from text using Large Language Models (LLMs). Existing approaches often suffer from high computational costs, generating massive volumes of noisy output tokens, and frequently lack strict adherence to structured ontologies. These limitations hinder practical deployment, as many current pipelines treat KGs merely as auxiliary retrieval mechanisms rather than reliable, standalone knowledge bases, resulting in difficulties regarding verification and integration with established structured data sources.

To overcome these limitations, the authors introduce **Wikontic**, a scalable, three-stage pipeline designed to construct ontology-aware KGs that are rigorously aligned with Wikidata. The methodology begins with candidate triplet extraction using LLMs to identify entities, relations, and qualifiers, followed by a critical ontology-aware refinement stage. This refinement enforces constraints by validating extracted data against a custom schema of 2,464 factual properties and utilizes taxonomic expansion via Wikidata relations ("instance of" and "subclass of") to ensure type consistency. The process concludes with entity normalization and deduplication, employing Contriever embeddings and vector search to map candidates to legal Wikidata types, allowing the LLM to reconstruct refined triplets that strictly adhere to schema requirements.

The proposed system demonstrates significant performance improvements across multiple benchmarks while achieving substantial efficiency gains. Wikontic achieves a **96% recall rate** on the MuSiQue dataset and establishes a new state-of-the-art for information retention with an **86% score** on the MINE-1 benchmark. In question-answering tasks utilizing only the extracted triplets, the model scored **76.0 F1** on HotpotQA and **59.8 F1** on MuSiQue, matching or surpassing standard Retrieval-Augmented Generation (RAG) baselines. Crucially, the pipeline is highly efficient, requiring **fewer than 1,000 output tokens** per inputâ€”representing a 3x reduction compared to AriGraph and a reduction of more than 20x compared to GraphRAG.

The significance of this research lies in its paradigm shift toward focusing on the intrinsic quality, compactness, and ontology consistency of KGs rather than treating them merely as retrieval structures. By demonstrating that high-quality, ontology-aware KGs can function as effective standalone knowledge sources, Wikontic reduces reliance on massive textual contexts and significantly lowers the computational overhead of graph construction. This approach offers a viable path toward verifiable, structured grounding for LLMs, enabling more cost-effective and reliable integration of external knowledge into generative AI systems.

---

## Key Findings

*   **High Graph Quality and Coverage:** The generated knowledge graphs (KGs) are compact, ontology-consistent, and well-connected, achieving a **96% recall rate** on the MuSiQue dataset.
*   **Competitive Performance Against RAG:** Using a triplets-only setup, the method achieves **76.0 F1** on HotpotQA and **59.8 F1** on MuSiQue, matching or surpassing standard Retrieval-Augmented Generation (RAG) baselines.
*   **State-of-the-Art Information Retention:** The system achieves **86%** on the MINE-1 benchmark, setting a new standard for information retention performance.
*   **Significant Efficiency Gains:** The pipeline is highly efficient at build time, requiring fewer than **1,000 output tokens**, representing a **3x reduction** compared to AriGraph and a reduction of more than **20x** compared to GraphRAG.

---

## Methodology

The authors propose Wikontic, a multi-stage pipeline for constructing KGs from open-domain text. The methodology consists of three core steps:

1.  **Extraction:** Identifying candidate triplets that include qualifiers.
2.  **Constraint Enforcement:** Applying Wikidata-based type and relation constraints to ensure the extracted data adheres to a strict ontology.
3.  **Normalization:** Normalizing entities to reduce redundancy and duplication within the graph.

---

## Technical Details

Wikontic employs a sophisticated three-stage pipeline designed to ensure strict adherence to ontological standards.

### Pipeline Components
*   **Candidate Triplet Extraction:** Utilizes LLMs to extract triplets with associated metadata.
*   **Ontology-Aware Refinement:** Validates extracted data against Wikidata schema constraints.
*   **Entity Normalization & Deduplication:** Ensures graph consistency.

### Schema and Taxonomy
*   **Custom Schema:** Derived from Wikidata containing **2,464 factual properties**.
*   **Taxonomic Expansion:** Uses Wikidata relations 'instance of' (P31) and 'subclass of' (P279).

### Retrieval and Refinement Mechanism
*   **Vector Search:** Utilizes Contriever embeddings and MongoDB Atlas vector search.
*   **Refinement Process:**
    1.  Retrieves top-10 candidate types.
    2.  Identifies legal relations.
    3.  Ranks candidates by cosine similarity.
    4.  Uses the LLM to reconstruct refined triplets.

---

## Contributions

*   **Intrinsic Quality Focus:** Shifts the paradigm from using KGs merely as auxiliary text retrieval structures to focusing on the intrinsic quality, ontology consistency, and compactness of the KG itself.
*   **Wikidata-Aligned Pipeline:** Introduces a scalable construction method that rigorously aligns with Wikidata ontologies to ensure verifiable and structured grounding for LLMs.
*   **Efficiency in Structured Knowledge Use:** Demonstrates that high-quality, ontology-aware KGs can serve as a standalone effective knowledge source, reducing the reliance on massive textual context and significantly lowering the computational cost associated with graph construction.

---

## Results

*   **MuSiQue Dataset:** Achieved a 96% recall rate, generating compact and ontology-consistent KGs.
*   **Question Answering (Triplets-Only):**
    *   HotpotQA: **76.0 F1**
    *   MuSiQue: **59.8 F1**
*   **MINE-1 Benchmark:** Established a new state-of-the-art for information retention with **86%** performance.
*   **Efficiency Metrics:**
    *   Uses fewer than 1,000 output tokens.
    *   **3x reduction** vs. AriGraph.
    *   **>20x reduction** vs. GraphRAG.

---

**Document Info**
*   **Quality Score:** 9/10
*   **References:** 23 citations