# Head Pursuit: Probing Attention Specialization in Multimodal Transformers

*Lorenzo Basile; Valentino Maiorca; Diego Doimo; Francesco Locatello; Alberto Cazzaniga*

---

### ‚ö° Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |
| **Core Method** | Simultaneous Orthogonal Matching Pursuit (SOMP) |
| **Key Discovery** | The "1% Rule" |
| **Scope** | Unimodal (Language) & Multimodal (Vision-Language) |

---

## üìã Executive Summary

The paper addresses the "black box" nature of transformer models, specifically the lack of understanding regarding how individual components within attention layers contribute to specific outputs. While transformers have proven effective across diverse domains, interpreting the internal function of their attention heads remains a significant challenge. The authors tackle the necessity of understanding whether these heads develop consistent specialization for distinct semantic or visual attributes, and whether this structure can be leveraged to control model behavior without requiring computationally expensive retraining or broad architectural changes.

The key innovation is a novel signal processing framework that reinterprets the probing of intermediate activations as a sparse signal recovery problem. Technically, the method utilizes Simultaneous Orthogonal Matching Pursuit (SOMP) to decompose the residual stream, employing the model's Unembedding Matrix as a dictionary to identify semantically meaningful tokens. This approach enables the simultaneous analysis of multiple samples to quantitatively rank attention heads based on their relevance to specific target concepts.

By iteratively selecting "atoms" (heads) that maximize correlation with the residuals, the framework isolates specific components responsible for distinct attributes, allowing for precise, principled interventions. The research establishes a quantitative benchmark termed the **"1% Rule,"** demonstrating that modifying as few as 1% of the highest-ranked attention heads is sufficient to reliably suppress or enhance targeted concepts in the model's output. This finding holds true across both unimodal and multimodal architectures.

The method successfully generalized to a variety of complex tasks, including language domains (question answering and toxicity mitigation) and vision-language domains (image classification and captioning). The results show that targeted rescaling of these specific heads yields linear, interpretable changes in model behavior outperforming baseline methods that require additional training. This work significantly advances the field of mechanistic interpretability by proving that large-scale generative models possess an internal, linearly steerable structure that can be mapped and manipulated.

---

## üîë Key Findings

*   **Consistent Specialization:** Individual attention heads across both unimodal and multimodal transformers exhibit consistent patterns of specialization, specific to distinct semantic or visual attributes.
*   **The "1% Rule":** Modifying as few as 1% of the highest-ranked attention heads is sufficient to reliably suppress or enhance targeted concepts in the model‚Äôs output.
*   **Broad Generalization:** The proposed method effectively generalizes across diverse tasks, including language domains (question answering and toxicity mitigation) and vision-language domains (image classification and captioning).
*   **Controllable Internal Structure:** The research reveals that attention layers possess an internal, controllable structure that can be understood and manipulated without altering the entire model.

---

## üõ†Ô∏è Methodology

The authors build upon established interpretability techniques by reinterpreting the practice of probing intermediate activations with the final decoding layer through the lens of signal processing. This novel perspective enables the principled analysis of multiple samples simultaneously. The methodology allows for the quantitative ranking of individual attention heads based on their calculated relevance to specific target concepts, providing a structured way to identify which heads control specific attributes.

---

## üß† Contributions

*   **A Signal Processing Framework for Probing:** Introduction of a theoretical reinterpretation of activation probing that treats the process as a signal processing task, allowing for multi-sample analysis.
*   **Mechanism for Granular Control:** Demonstration that large-scale generative models can be effectively steered (edited) via precise, minimal interventions in specific attention heads rather than broad parameter adjustments.
*   **Tools for Interpretability:** Provision of simple, effective tools for understanding the "black box" of transformers by highlighting the semantic role of specific components within the attention layers.

---

## ‚öôÔ∏è Technical Details

The approach frames attention head specialization as a sparse signal recovery problem using **Simultaneous Orthogonal Matching Pursuit (SOMP)**. The technical workflow is as follows:

*   **Residual Stream Decomposition:** Isolates individual head contributions via residual stream decomposition.
*   **Dictionary Learning:** Uses the model's Unembedding Matrix as a dictionary for sparse coding to find semantically meaningful tokens.
*   **Atom Selection:** The algorithm iteratively selects atoms that maximize correlation with residuals.
*   **Intervention Mechanism:** Intervention is achieved by selecting top-k heads and rescaling their output contribution by a factor to enhance, suppress, or maintain target attributes.

---

## üìä Results

The primary quantitative finding (the '1% Rule') indicates that modifying only about 1% of the highest-ranked attention heads is sufficient to reliably suppress or enhance targeted concepts. The method generalizes across unimodal and multimodal transformers for tasks such as question answering, toxicity mitigation, image classification, and image captioning.

It demonstrates that attention heads specialize in narrow semantic areas and possess a linear, interpretable structure, unlike baseline methods requiring additional training.