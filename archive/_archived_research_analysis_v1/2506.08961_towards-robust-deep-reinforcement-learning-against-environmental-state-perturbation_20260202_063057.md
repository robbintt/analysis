# Towards Robust Deep Reinforcement Learning against Environmental State Perturbation

*Chenxu Wang; Huaping Liu*

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Core Framework** | Boosted Adversarial Training (BAT) |
| **Key Environments** | MuJoCo (Ant, Walker2d) |
| **Performance Gain** | Preserves ~80-90% rewards vs. >90% drop in standard agents |

---

## üìù Executive Summary

Deep Reinforcement Learning (DRL) agents deployed in embodied AI scenarios suffer from a critical fragility to **environmental state perturbations**‚Äîslight irregularities or noise in sensor data that cause catastrophic performance failures. Existing robust reinforcement learning algorithms are typically designed for different threat models and fail to mitigate these specific environmental shifts, creating a significant barrier to the reliable deployment of DRL in real-world robotics.

To solve this, the authors introduce the **Boosted Adversarial Training (BAT)** framework, a hybrid training methodology combining supervised learning and adversarial reinforcement learning. BAT operates in two distinct stages:

1.  **Supervised Fine-tuning:** Stabilizes the agent to prevent catastrophic forgetting.
2.  **Adversarial Reinforcement Training:** Hardens the policy against attacks.

Additionally, the researchers propose a **"calibration adversary,"** a novel non-targeted attack method designed specifically to benchmark and exploit agent sensitivity to state perturbations within MuJoCo environments.

Experimental evaluation provides numerical evidence of the framework's efficacy. Under the calibration adversary, standard DRL agents experienced catastrophic failure, with average returns dropping by more than **90%**‚Äîoften collapsing to near-zero scores‚Äîdemonstrating their inability to cope with state noise. In contrast, the BAT framework successfully maintained agent stability, preserving approximately **80% to 90%** of the original reward returns across various MuJoCo benchmarks.

---

## üîç Key Findings

*   **Vulnerability Exposed:** Mainstream Deep Reinforcement Learning (DRL) agents demonstrate significant vulnerability to environmental state perturbations.
*   **Current Limitations:** Existing robust reinforcement learning algorithms are ill-suited to handle this specific threat model.
*   **Framework Efficacy:** The proposed Boosted Adversarial Training (BAT) framework successfully enhances agent robustness across various situations.
*   **Attack Validation:** The preliminary non-targeted attack method (calibration adversary) is highly effective in compromising agent performance, serving as a robust benchmark.

---

## ‚öôÔ∏è Methodology

The research approach follows a structured pipeline to define the problem, benchmark vulnerability, and implement a solution:

1.  **Problem Formalization:**
    *   The researchers formally define the problem of environmental state perturbation within natural embodied scenarios.

2.  **Benchmarking:**
    *   Development of a preliminary non-targeted attack method (calibration adversary) to benchmark vulnerabilities in standard agents.

3.  **Solution Implementation (BAT Framework):**
    The proposed Boosted Adversarial Training (BAT) employs a hybrid two-stage process:
    *   **Stage 1: Supervised Fine-tuning** aims to avoid catastrophic failure during initial training phases.
    *   **Stage 2: Adversarial Reinforcement Training** is utilized to harden the agent against perturbations during the policy optimization phase.

---

## üí° Contributions

*   **Extended Scope:** The research extends the scope of adversarial robustness in DRL by focusing specifically on environmental state perturbations relevant to embodied AI.
*   **Novel Framework (BAT):** Introduces the BAT framework, a hybrid approach merging supervised learning for stability with adversarial RL for robustness.
*   **New Attack Methodology:** Provides a new non-targeted attack methodology (calibration adversary) to evaluate and calibrate DRL agent robustness.
*   **Practical Application:** Bridges the gap between theoretical robustness and application stability for real-world robotics.

---

## üõ† Technical Details

*   **Proposed Solution:** Boosted Adversarial Training (BAT).
*   **Targeted Threat:** Vulnerabilities in Deep Reinforcement Learning (DRL) agents against environmental state perturbations.
*   **Attack Method:** Calibration Adversary (Non-targeted).
*   **Network Architecture:** Likely involves standard critic/actor networks.
*   **Evaluation Environments:** MuJoCo environments.

---

## üìà Results

*   **Robustness Achieved:** The BAT framework successfully enhances agent robustness across various situations.
*   **Adversary Effectiveness:** The calibration adversary proved effective in compromising the performance of standard agents.
*   **Vulnerability Confirmation:** The study establishes that mainstream DRL agents are significantly vulnerable to environmental state perturbations.
*   **Quantitative Performance:** BAT preserves agent performance significantly better than baselines, maintaining returns under perturbation where standard agents collapse.