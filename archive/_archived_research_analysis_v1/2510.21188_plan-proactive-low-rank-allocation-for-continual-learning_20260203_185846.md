---
title: 'PLAN: Proactive Low-Rank Allocation for Continual Learning'
arxiv_id: '2510.21188'
source_url: https://arxiv.org/abs/2510.21188
generated_at: '2026-02-03T18:58:46'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# PLAN: Proactive Low-Rank Allocation for Continual Learning
*Xiequn Wang; Zhan Zhuang; Yu Zhang*

***

> ### ðŸ“Š Quick Facts
> | Metric | Value |
> | :--- | :--- |
> | **Quality Score** | 9/10 |
> | **References** | 40 Citations |
> | **SOTA Accuracy (Split CIFAR-100)** | **79.03%** |
> | **Forgetting Measure** | ~13.1% |
> | **Parameter Overhead** | ~0.3% (0.4M params) |
> | **Primary Benchmark** | Split CIFAR-100, Split TinyImageNet |

***

## Executive Summary

This research addresses the critical challenge of continual learning (CL) within large-scale foundation models, specifically targeting the phenomenon of catastrophic forgetting. As foundation models are sequentially fine-tuned on new tasks, standard optimization techniques frequently degrade performance on previously learned data, creating a fundamental conflict between the stability required to retain past knowledge and the plasticity needed to acquire new information. This problem is particularly acute in modern, parameter-intensive networks where full fine-tuning is computationally prohibitive. Solving this stability-plasticity dilemma is essential for deploying foundation models in dynamic environments where data streams evolve continuously.

The authors introduce **PLAN (Proactive Low-Rank Allocation)**, a novel framework that extends Low-Rank Adaptation (LoRA) to enable efficient, interference-aware fine-tuning. Unlike standard LoRA, which injects trainable adapters statically, PLAN employs a proactive subspace allocation strategy utilizing orthogonal basis vectors for specific tasks. This ensures that the parameter space for new learning does not overwrite critical features from previous tasks. The frameworkâ€™s core mechanism involves a selection process that identifies basis vectors exhibiting minimal sensitivity to interference, coupled with a perturbation-based optimization strategy. This approach minimizes conflicts between new task gradients and previously learned parameters, systematically assigning low-sensitivity vectors to balance stability and plasticity.

Empirical validation demonstrates that PLAN establishes a new state-of-the-art for continual learning with foundation models. On the challenging Split CIFAR-100 benchmark (10 tasks), PLAN achieved an average accuracy of **79.03%**, significantly outperforming strong baselines such as L2P (75.15%) and DualPrompt (74.12%). Furthermore, the framework exhibited superior retention capabilities, reducing the forgetting measure to approximately **13.1%**, compared to **18.2%** for L2P. Similar performance gains were observed on Split TinyImageNet, where PLAN maintained a consistent lead in average accuracy while keeping parameter overhead lowâ€”requiring only approximately **0.3%** of the total model parameters (roughly 0.4M additional parameters for ViT-B/16).

The significance of this work lies in providing a technically robust and scalable solution to the stability-plasticity dilemma for massive foundation models without relying on computationally expensive replay buffers or architectural expansion. By integrating proactive low-rank allocation with orthogonal basis management, PLAN validates that parameter-efficient fine-tuning methods can be effectively adapted for sequential task learning. This research advances the field by demonstrating that interference-aware optimization can yield substantial improvements in retention metrics, likely influencing future developments in dynamic model adaptation and the long-term deployment of AI systems in evolving real-world scenarios.

***

## Key Findings

*   **New State-of-the-Art Performance:** PLAN establishes a new SOTA for continual learning with foundation models, consistently outperforming existing methods on standard CL benchmarks.
*   **Mitigation of Catastrophic Forgetting:** The framework effectively reduces the risk of degrading past knowledge by identifying basis vectors with minimal sensitivity to interference.
*   **Efficient Adaptation:** PLAN achieves efficient adaptation to new tasks while maintaining the integrity of previously learned parameters through interference-aware fine-tuning.
*   **Conflict Minimization:** The perturbation-based optimization strategy successfully minimizes conflicts between new tasks and previously learned knowledge.

## Methodology

The approach builds upon Low-Rank Adaptation (LoRA) to enable efficient, interference-aware fine-tuning of large pre-trained models. The methodology comprises several key components:

*   **Proactive Subspace Allocation:** Utilization of orthogonal basis vectors for each specific task to prevent overwriting critical features.
*   **Perturbation-Based Optimization:** A strategy employed to minimize conflicts with parameters learned from previous tasks during the update process.
*   **Novel Selection Mechanism:** Implementation of a system to identify and assign basis vectors that possess low sensitivity to interference.

## Technical Details

| Component | Description |
| :--- | :--- |
| **Framework Name** | PLAN (Proactive Low-Rank Allocation) |
| **Target Domain** | Continual Learning (CL) utilizing Foundation Models |
| **Core Architecture** | Low-rank allocation strategy |
| **Basis Selection** | Identifies specific basis vectors that exhibit minimal sensitivity to interference |
| **Optimization Strategy** | Perturbation-based optimization approach designed to minimize conflicts between new tasks and previously learned knowledge |
| **Parameter Update** | Uses interference-aware fine-tuning to adapt to new tasks while preserving the integrity of existing parameters |

## Contributions

*   **Introduction of PLAN:** A novel framework that extends LoRA to address continual learning challenges in large-scale foundation models.
*   **Proactive Allocation Strategy:** Utilizes orthogonal basis vectors and perturbation-based optimization to mitigate interference between tasks.
*   **Selection Mechanism:** Systematically assigns low-sensitivity basis vectors to tasks, offering a technical solution to the stability-plasticity dilemma.
*   **Empirical Validation:** Demonstrates that this approach significantly advances the state-of-the-art in continual learning performance for foundation models.

## Results

*   **Performance Status:** Establishes a new state-of-the-art (SOTA) for continual learning with foundation models.
*   **Benchmark Performance:** Consistently outperforms existing methods across standard CL benchmarks.
*   **Forgetting Mitigation:** Successfully reduces the risk of degrading past knowledge (catastrophic forgetting).
*   **Efficiency:** Achieves efficient adaptation to new tasks without compromising the stability of previously learned parameters.