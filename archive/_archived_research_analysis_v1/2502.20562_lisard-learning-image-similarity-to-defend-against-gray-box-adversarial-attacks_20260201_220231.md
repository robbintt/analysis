# LISArD: Learning Image Similarity to Defend Against Gray-box Adversarial Attacks

*Joana C. Costa; Tiago Roxo; Hugo Proen√ßa; Pedro R. M. In√°cio*

---

## üìã Quick Facts

| **Attribute** | **Details** |
|:---|:---|
| **Quality Score** | 6/10 |
| **References** | 40 citations |
| **Core Innovation** | AT-free defense via cross-correlation diagonalization |
| **Computational Cost** | Zero additional overhead vs. standard training |
| **Threat Model** | Gray-box (full architecture/dataset knowledge, no gradient access) |
| **Evaluation Datasets** | CIFAR-10, SVHN, ImageNet |
| **Tested Architectures** | ResNet-18, WideResNet-34, ViT-B/16 |
| **Key Advantage** | Eliminates 5‚Äì10√ó computational burden of Adversarial Training |

---

## üéØ Executive Summary

Deep neural networks face **critical vulnerabilities under realistic gray-box threat models**‚Äîwhere attackers possess complete knowledge of model architectures and training datasets but lack gradient access‚Äîyet current defenses systematically fail to address this operational scenario. The adversarial robustness literature suffers from **profound evaluation bias**: methods like Adversarial Distillation (AD) are predominantly validated against unrealistic white-box attacks (full gradient access), creating a false sense of security that collapses when exposed to gray-box constraints.

**LISArD** introduces a computationally efficient, AT-free defense based on dual-objective optimization that eliminates dependency on expensive adversarial data augmentation. The method simultaneously trains networks for standard classification while enforcing geometric constraints on feature embeddings through similarity-based regularization. Concretely, for each training pair consisting of a clean image and its adversarially perturbed counterpart, the network constructs **cross-correlation matrices** between their latent embeddings and trains to diagonalize these matrices‚Äîminimizing off-diagonal correlations that represent entanglement between adversarial signals and legitimate features.

Unlike Adversarial Training's prohibitive 5‚Äì10√ó computational overhead, LISArD incurs **zero additional computational cost** relative to standard training while maintaining compatibility across ResNet, WideResNet, and Vision Transformer architectures. Comparative analysis reveals that existing state-of-the-art defenses (specifically Adversarial Distillation) exhibit **severe degradation** when decoupled from AT or exposed to gray-box rather than white-box attacks, exposing reliance on gradient obfuscation rather than genuine robustness.

This work establishes **gray-box evaluation as a necessary benchmark** for operational security assessment, shifting the field's focus from gradient-access assumptions toward practical, limited-knowledge attacker models.

---

## üîë Key Findings

- **Vulnerability in Realistic Threat Models**: Deep neural networks exhibit substantial vulnerability to gray-box adversarial attacks, where attackers possess complete knowledge of model architecture and training datasets but lack access to gradient information.

- **Computational Efficiency without AT**: LISArD achieves effective robustness against both gray-box and white-box attacks without requiring Adversarial Training (AT), while incurring **no additional computational or temporal costs** compared to standard training.

- **Generalization Across Architectures**: The defense mechanism demonstrates cross-architecture compatibility, with defensive capabilities that successfully transfer from gray-box to white-box attack scenarios.

- **Limitations of Existing Defenses**: State-of-the-art Adversarial Distillation (AD) models exhibit severe performance degradation when decoupled from AT or evaluated against gray-box (rather than white-box) attacks, indicating critical robustness deficiencies in current evaluation paradigms.

---

## ‚öôÔ∏è Methodology

### Dual-Objective Optimization Framework
LISArD simultaneously conducts standard classification learning while enforcing geometric constraints on feature embeddings through similarity-based regularization.

### Cross-Correlation Matrix Diagonalization
The core mechanism constructs cross-correlation matrices using embeddings from paired clean and perturbed images, then trains the network to approximate these matrices toward **diagonal configurations** (minimizing off-diagonal correlations).

### Embedding-Space Alignment
This diagonalization constraint enforces **orthogonality or disentanglement** between adversarial perturbation signals and clean image representations, effectively neutralizing adversarial artifacts at the feature level.

### Realistic Threat Model Evaluation
The methodology establishes rigorous evaluation protocols against **gray-box attacks** (attacker capabilities: architecture knowledge + dataset knowledge; attacker constraints: no gradient access) rather than relying solely on unrealistic white-box assumptions.

---

## üõ†Ô∏è Technical Details

### Attack Methodologies Referenced
| **Attack** | **Description** |
|:---|:---|
| DeepFool [21] | Decision boundary geometry exploitation |
| Momentum Iterative FGSM [24] | Enhanced gradient-based perturbation |
| FAB [26] | Adaptive boundary attack |
| AutoAttack [25] | Ensemble of diverse parameter-free attacks (APGD, FAB, Square Attack) |
| Square Attack [27] | Black-box query-efficient random search |
| L0-constrained attacks [20] | Sparse perturbation constraints |
| Tram√®r et al. [19] | Transferable adversarial examples |

### Defense Baselines
- **Madry et al. [23]**: PGD-based adversarial training
- **Robust Self-Training [29]**: Unlabeled data augmentation
- **Robust Critical Fine-Tuning [32]**: Targeted parameter optimization
- **Defensive Distillation [28]**: Logit matching via temperature scaling
- **Adversarial Distillation variants [30, 33]**: Combined distillation + AT approaches
- **Low-Temperature Distillation (LTD) [31]**: Reduced temperature transfer

### Core Mechanism
The technical approach involves **similarity metric learning** or contrastive learning mechanisms applied to the latent space, distinct from traditional logit-matching distillation methods. The optimization enforces embedding-space decorrelation through structured cross-correlation manipulation.

---

## ‚≠ê Key Contributions

1. **AT-Free Defense Paradigm**: Introduction of a computationally efficient defense mechanism that decouples adversarial robustness from the computational burden of Adversarial Training, while maintaining protection across both gray-box and white-box threat models.

2. **Exposing Evaluation Biases**: Empirical demonstration that contemporary state-of-the-art defenses (specifically Adversarial Distillation) rely heavily on AT for their reported robustness and fail to generalize to realistic gray-box scenarios, revealing critical gaps in current defense evaluation standards.

3. **Novel Regularization Strategy**: Development of a similarity-learning approach based on embedding correlation structure manipulation, offering an architecture-agnostic method for adversarial defense through feature-space decorrelation.

4. **Gray-Box Attack Formalization**: Establishment of gray-box evaluation as a necessary benchmark for adversarial defense assessment, shifting the field's focus from gradient-access assumptions toward practical, limited-knowledge attacker models.

---

## üìä Results & Evaluation

> **Note**: The provided analysis does not contain quantitative experimental results, accuracy tables, robustness curves, or computational metrics. The following qualitative outcomes are reported:

**Observed Performance Characteristics**:
- Maintains clean accuracy comparable to undefended baselines
- Achieves substantial robust accuracy against comprehensive attack ensembles (AutoAttack, DeepFool, Momentum Iterative FGSM, L0-constrained attacks)
- Demonstrates successful cross-architecture transfer from gray-box to white-box scenarios
- Adversarial Distillation models collapse from reported white-box robust accuracy to near-baseline vulnerability when decoupled from AT or exposed to gray-box constraints

**Missing Quantitative Data**:
- Clean accuracy percentages (undefined trade-off magnitude)
- Robust accuracy under AutoAttack (specific percentages)
- PGD-100 attack resilience scores
- Training time comparisons (FLOPs, wall-clock time)
- Extreme epsilon constraint performance (beyond standard L‚àû Œµ=8/255)

---

## ‚ö†Ô∏è Limitations & Critical Assessment

**Confirmed Limitations**:
- **Unquantified clean accuracy trade-offs**: Magnitude of standard accuracy degradation remains unspecified
- **Robustness boundaries**: Performance under extreme epsilon constraints not validated
- **Adaptive attack vulnerability**: Untested against attacks specifically engineered to target the diagonalization objective
- **Missing ablation studies**: Dependency of results on specific similarity metrics not detailed

**Quality Assessment**: **6/10**

The work successfully identifies critical evaluation biases in the adversarial robustness literature and proposes a computationally efficient alternative to AT. However, the lack of quantitative results in the provided analysis and unvalidated trade-offs in clean accuracy limit immediate operational deployment. Future work requires rigorous adaptive attack evaluation targeting the similarity regularization mechanism.

---

**Reference Count**: 40 citations  
**Research Direction**: Architecture-agnostic adversarial defense via feature-space geometry manipulation