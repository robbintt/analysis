---
title: 'BCQ: Block Clustered Quantization for 4-bit (W4A4) LLM Inference'
arxiv_id: '2502.05376'
source_url: https://arxiv.org/abs/2502.05376
generated_at: '2026-02-03T19:21:05'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# BCQ: Block Clustered Quantization for 4-bit (W4A4) LLM Inference

*Reena Elangovan; Charbel Sakr; Anand Raghunathan; Brucek Khailany*

---

> ### ðŸ“Š Quick Facts
> 
> | Metric | Detail |
> | :--- | :--- |
> | **Configuration** | W4A4 (4-bit Weights, 4-bit Activations) |
> | **Total Bitrate** | 4.5-bits |
> | **Accuracy Loss** | < 1% across multiple LLMs/tasks |
> | **Method Type** | Post-Training Quantization (PTQ) |
> | **Quality Score** | 9/10 |

---

## Executive Summary

Large Language Models (LLMs) require substantial computational resources, creating a pressing need for low-bit quantization to reduce memory bandwidth and power consumption. While quantizing model weights to 4-bits has become relatively standard, quantizing activations to sub-8-bit precision remains a significant challenge; aggressive activation quantization typically results in severe accuracy degradation. Furthermore, existing high-performance solutions often rely on Quantization-Aware Training (QAT), which necessitates computationally expensive retraining.

This paper addresses the critical challenge of enabling efficient Post-Training Quantization (PTQ) for both weights and activations at 4-bits (W4A4) without the prohibitive cost of QAT. The authors introduce Block Clustered Quantization (BCQ), a novel framework designed to improve quantization fidelity by leveraging local tensor statistics. Unlike global quantization schemes, BCQ decomposes operand tensors into blocks and clusters them based on statistical characteristics, assigning distinct optimal quantization codebooks to each cluster.

The core technical contribution is the Locally-Optimal BCQ (LO-BCQ) algorithm, an iterative PTQ method that alternates between a Vector Clustering Stepâ€”remapping blocks to the codebook that minimizes Mean Squared Error (MSE)â€”and a Codebook Update Step utilizing the Lloyd-Max algorithm to compute optimal centroids. This greedy strategy guarantees non-increasing MSE convergence, allowing for high-fidelity representation of both weights and activations without backpropagation.

LO-BCQ successfully enables a W4A4 configuration with a total effective bitrate of 4.5-bits. Across multiple LLMs and downstream tasks, the method maintains an inference accuracy loss of less than 1%, demonstrating robustness comparable to full-precision baselines. When compared to baseline methods such as MXFP and VSQ, LO-BCQ converges to a lower quantization MSE, with performance improvements scaling as the number of codebooks increases and block length decreases. This research significantly advances the state-of-the-art in efficient LLM deployment by bridging the "activation gap" and demonstrating that sub-8-bit activation quantization is viable without QAT.

---

## Key Findings

*   **High-Efficiency Quantization:** Successfully enables **W4A4** (4-bit weights, 4-bit activations) quantization with a low total bitrate of **4.5-bits**.
*   **Minimal Accuracy Loss:** Inference accuracy loss is kept to **less than 1%** across multiple LLMs and tasks without the need for quantization-aware training.
*   **State-of-the-Art Performance:** Advances the state-of-the-art in post-training quantization specifically for sub-8-bit activations.
*   **Superior Convergence:** Compared to baselines like MXFP and VSQ, LO-BCQ converges to a lower quantization MSE, particularly as the number of codebooks increases and block length decreases.

---

## Methodology

The core approach involves **Block Clustered Quantization (BCQ)**, which improves quantization fidelity by utilizing block-level statistics rather than global tensor statistics.

1.  **Tensor Decomposition:** Operand tensors are decomposed into smaller blocks.
2.  **Statistical Clustering:** Blocks are clustered based on their statistical characteristics.
3.  **Codebook Assignment:** Specific, optimal quantization codebooks are assigned to each cluster.

The researchers implemented **Locally-Optimal BCQ (LO-BCQ)**, a post-training quantization algorithm that iterates between block clustering and codebook design. It uses a greedy strategy to minimize the quantization mean squared error (MSE).

---

## Technical Details

The LO-BCQ (Locally Optimized Block Clustered Quantization) method is an iterative algorithm designed to minimize error without Quantization-Aware Training (QAT).

| Component | Description |
| :--- | :--- |
| **Algorithm Type** | Iterative Post-Training Quantization (PTQ) |
| **Optimization Goal** | Minimize Quantization Mean Squared Error (MSE) |
| **Convergence** | Guarantees non-increasing MSE |
| **Key Parameters** | Number of Codebooks ($N_c$), Block Length ($L_b$) |

**The LO-BCQ Loop:**

1.  **Vector Clustering Step:** Remaps input blocks to the best available codebook to minimize MSE.
2.  **Codebook Update Step:** Uses the Lloyd-Max algorithm to compute optimal quantization levels (centroids) for the current clusters.

---

## Results

LO-BCQ achieves robust performance metrics across various number formats, maintaining high accuracy with significantly reduced precision.

*   **Configuration:** W4A4 @ 4.5-bits total bitrate.
*   **Perplexity Metrics:**
    *   **E3M3:** 18.32 / 18.27
    *   **E3M2:** 19.07 / 18.51
    *   **E4M0:** 43.89 / 19.71
*   **Comparative Performance:** Outperforms MXFP and VSQ baselines, achieving lower quantization MSE.

---

## Contributions

*   **Novel Framework (BCQ):** Introduced Block Clustered Quantization, a new approach utilizing block-level statistics and clustering to improve quantization fidelity.
*   **Algorithm Development (LO-BCQ):** Developed the Locally-Optimal BCQ algorithm, providing a practical path to W4A4 inference without the high computational cost of Quantization-Aware Training (QAT).
*   **Bridging the Gap:** Demonstrated that both weights and activations can be effectively quantized to 4-bits simultaneously, solving a major bottleneck in sub-8-bit activation quantization.

---

**Quality Score:** 9/10  
**References:** 40 citations