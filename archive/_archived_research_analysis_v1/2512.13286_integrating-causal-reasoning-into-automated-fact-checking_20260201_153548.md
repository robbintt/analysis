# Integrating Causal Reasoning into Automated Fact-Checking

*Youssra Rebboud; Pasquale Lisena; Raphael Troncy*

> ### **Quick Facts**
> *   **Quality Score:** 7/10
> *   **Architecture:** Hybrid Neuro-symbolic
> *   **Core Ontology:** FARO (Direct-Cause, Prevents, Intends-to-Cause, Enables)
> *   **Datasets:** CausalCheck, e-SNLI
> *   **Total References:** 27 citations

---

## Executive Summary

Current automated fact-checking systems suffer from a critical reliance on surface-level textual similarity and linguistic entailment, which prevents them from understanding the deeper causal structures inherent in complex claims. This deficiency limits systems to verifying "that" an event occurred rather than "why" or "how," resulting in an inability to detect logical inconsistencies that are not explicitly contradictory. In high-stakes verification environments, this lack of causal reasoning significantly undermines the utility, explainability, and trustworthiness of automated tools.

The authors propose a hybrid neuro-symbolic architecture that integrates fine-grained causal event relationships into the verification pipeline, anchored by the FARO ontology. This framework defines four specific semantic event relations and utilizes a four-stage module to evaluate the alignment of claim events against evidence events. For example, the system identifies a logical misalignment when a claim asserts a "Direct-Cause" relationship while the evidence supports a "Prevents" relationship, thereby detecting nuanced flaws that standard models miss.

The study establishes the first computational baseline for integrating causal event relationships, evaluated on two distinct datasets. The results demonstrate the framework's ability to distinguish between specific causal categories (e.g., prevention vs. causation) to identify erroneous cause-effect relationships. By systematically differentiating between these links, the evaluation confirms the feasibility of applying rigorous logical consistency checks to automated verification workflows.

This research significantly advances the field by shifting the focus from opaque verdict generation to semantically rich explainability. By providing a transparent mechanism to pinpoint specific logical flaws, the framework offers a robust "white box" alternative to standard "black box" deep learning models. The establishment of this baseline serves as a vital foundation for future research, encouraging the development of fact-checking systems capable of deep causal reasoning, which is essential for enhancing user trust and reliability in complex information scenarios.

---

## Key Findings

*   **Capability Gap in Current Systems:** Current automated fact-checking systems significantly lack dedicated causal-based reasoning capabilities.
*   **Explainability Limits:** The absence of causal reasoning limits the potential for semantically rich explainability in these systems.
*   **Hybrid Efficacy:** A hybrid approach combining event relation extraction, semantic similarity, and rule-based reasoning can effectively detect logical inconsistencies between event chains.
*   **Baseline Establishment:** The proposed method successfully establishes the first baseline for integrating fine-grained causal event relationships into fact-checking workflows.
*   **Enhanced Verdicts:** Integration of causal logic enhances the explainability of verdict predictions by identifying erroneous cause-effect relationships.

---

## Methodology

The proposed method utilizes a multi-component framework to compare claims against evidence. The methodology operates through the following core phases:

1.  **Event Relation Extraction:** Identification of relationships between events within the text.
2.  **Semantic Similarity Computation:** Measurement of the semantic closeness between events in the claim and the evidence.
3.  **Rule-based Reasoning:** Application of logical rules to detect inconsistencies in event chains, specifically focusing on erroneous cause-effect relationships.

---

## Technical Details

### Architecture & Approach
*   **System Type:** Hybrid Neuro-symbolic architecture.
*   **Ontology:** Utilizes the **FARO** ontology to define four semantic event relations:
    *   **Direct-Cause**
    *   **Prevents**
    *   **Intends-to-Cause**
    *   **Enables**

### Reasoning Logic
The system compares Claim and Evidence events to check for:
*   **Alignment:** Support via similarity or transitivity.
*   **Misalignment:** Refutation via opposite relations (e.g., Claim says A causes B, Evidence says A prevents B).

### Processing Pipeline
*   **Module 1: Event Relation Extraction**
    *   Uses LLMs and Common Sense knowledge bases.
*   **Module 2: Similarity Scoring**
    *   Uses **SBERT** to measure semantic closeness.
*   **Module 3: Polarity Detection**
    *   Uses **DistilBert** to determine sentiment/stance.
*   **Module 4: Reasoning Module**
    *   Applies rule-based logic for final verdict generation.

---

## Contributions

*   **Novel Integration:** Introduces the first dedicated methodology for integrating fine-grained causal event relationships into automated fact-checking.
*   **Baseline Establishment:** Provides a new performance baseline for future research focusing on causal reasoning within fact-checking, evaluated on two distinct datasets.
*   **Enhanced Explainability:** Advances the field by offering a mechanism for semantically rich explainability, moving beyond simple verdicts to identifying specific logical causal flaws.

---

## Results

*   **Qualitative Outcomes:** The approach successfully establishes the first baseline for integrating fine-grained causal event relationships.
*   **Explainability:** The framework enhances explainability by distinguishing between specific causal links (e.g., differentiating 'prevention' from 'cause').
*   **Targeted Use Case:** The approach is designed specifically for claims containing causal relations.
*   **Comparison:** It is contrasted with 'black box' models, favoring this system for its semantic depth and transparency.

*Note: Specific quantitative metrics (e.g., Accuracy, F1-score) are not provided in the text.*