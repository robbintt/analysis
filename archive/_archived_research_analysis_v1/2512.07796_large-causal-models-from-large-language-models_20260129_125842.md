# Large Causal Models from Large Language Models
*Sridhar Mahadevan*

---

> ### ðŸ“Š Quick Facts
> ---
> *   **Quality Score:** 8/10
> *   **Citations:** 9
> *   **System:** DEMOCRITUS
> *   **Base Model:** Qwen3-Next-80B-A3B-Instruct (6-bit quantized Apple MLX)
> *   **Domains Tested:** 6 (Archaeology, Biology, Climate Change, Economics, Medicine, Technology)
> *   **Primary Output:** Hypothesis generation & causal mapping

## Executive Summary

> Traditional causal inference is limited by its reliance on numerical data, leaving vast amounts of scientific knowledge locked in unstructured text unexploited. This paper addresses the challenge of extracting and synthesizing causal knowledge from the exponential growth of literature, where existing methods fail to effectively manage the fragmented, ambiguous, and often conflicting nature of causal claims. This restricts researchers to narrow hypothesis testing rather than enabling broad, exploratory analysis of complex causal systems latent in global scientific discourse.
>
> The key innovation is the **DEMOCRITUS system**, a framework that constructs Large Causal Models (LCMs) directly from linguistic data using the Qwen3-Next-80B-A3B-Instruct LLM. The system utilizes a six-module pipeline comprising Generative, Structuring, and Geometric phases. Technically, the platform builds Topos Causal Models (TCMs) using novel categorical machine learning techniques. It employs a Geometric Transformer (GT) for node embedding and optimizes network structures using generalized backpropagation with 'horn filling' on simplicial sets, a geometric method designed to automatically fill logical gaps and ensure consistency in the data.
>
> The DEMOCRITUS system was successfully implemented and tested across six diverse domains. It validated a decentralized "map-reduce" discovery strategy, demonstrating the capability to scale toward targets covering hundreds of domains and millions of causal claims. While the analysis identified specific computational bottlenecks within the pipeline that challenge scaling to larger models, the system proved highly effective at proposing topics, generating questions, and weaving fragmented statements into a coherent model. Crucially, the results indicate that the system is optimized for hypothesis generation and organization; it does not perform causal identification, numerical validation, or effect size estimation.
>
> **Significance:** This research significantly shifts the paradigm of causal discovery by demonstrating that high-quality LLMs can be repurposed to build structured causal models from text rather than merely generating content.

---

## Key Findings

*   **Successful Implementation:** The DEMOCRITUS system has been successfully implemented and tested across disparate domains including archaeology, biology, climate change, economics, medicine, and technology.
*   **Beyond Narrow Testing:** High-quality LLMs can effectively move beyond narrow-domain hypothesis testing to propose topics, generate questions, and extract plausible causal statements directly from textual data.
*   **Synthesis of Conflicts:** Isolated, fragmented, ambiguous, and potentially conflicting causal claims can be woven into a coherent whole using advanced machine learning methods.
*   **Scalability Challenges:** Analysis of the computational cost profile revealed specific bottlenecks within the current pipeline that challenge the scaling of the system to larger models.

---

## Methodology

The research utilizes a six-module implementation pipeline within the DEMOCRITUS system. It diverges from traditional causal inference by relying on **textual queries rather than numerical experimental data**. The process involves:

1.  **LLM-driven generation** to propose topics and extract causal statements.
2.  **Data transformation** of these claims into relational causal triples.
3.  **Advanced synthesis** using categorical machine learning methods to embed these triples into a Large Causal Model (LCM).

---

## Technical Details

The DEMOCRITUS system utilizes the **Qwen3-Next-80B-A3B-Instruct LLM** (6-bit quantized Apple MLX version) to construct text-driven Large Causal Models (LCMs). Acting as a **'Topos Builder'**, it creates Topos Causal Models (TCMs) via a specific modular workflow:

### Core Architecture
*   **Generative Phase:** Proposes variables and mechanisms.
*   **Structuring Phase:** Aggregates claims.
*   **Geometric Phase:** Embeds data for visualization and analysis.

### Machine Learning & Optimization
*   **Geometric Transformer (GT):** Used for node embedding.
*   **Dimensionality Reduction:** UMAP is employed for 2D/3D visualization.
*   **Optimization Strategy:** Uses generalized backpropagation with **'horn filling'** on simplicial sets to logically fill gaps in the data.
*   **Domain Organization:** Supports a 'slice-based' domain organization.
*   **Discovery Strategy:** Operates via a decentralized 'map-reduce' style discovery strategy.

---

## Results

*   **Testing Scope:** The system was successfully tested across 6 domains: Archaeology, Biology, Climate Change, Economics, Medicine, and Technology.
*   **Scalability Targets:** The system is designed to scale toward covering hundreds of domains and millions of claims.
*   **Coherence:** Demonstrated high causal coherence by weaving fragmented or conflicting claims into a whole.
*   **Capabilities:** Proved effective at proposing topics, generating questions, and extracting statements.
*   **Limitations:**
    *   The system is constrained to **hypothesis generation and organization**.
    *   It does **not** perform causal identification, numerical validation, or effect size estimation.
    *   Functions similarly to a literature review tool rather than a statistical inference engine.

---

## Contributions

*   **Framework Introduction:** Introduction of a comprehensive framework for building Large Causal Models (LCMs) that leverages the latent potential of existing Large Language Models (LLMs).
*   **System Creation:** The creation of the DEMOCRITUS system, a decentralized system capable of building, organizing, and visualizing manifold ontologies of causal relations across vastly different fields.
*   **Technique Invention:** The invention of new categorical machine learning techniques designed to handle the complexity of converting unstructured or conflicting textual causal claims into structured, coherent causal models.

---
**References:** 9 citations | **Quality Score:** 8/10