# Achieve Performatively Optimal Policy for Performative Reinforcement Learning

*Ziyi Chen; Heng Huang*

---

> ### üìä Quick Facts
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations
> *   **Algorithm:** Zeroth-Order Frank-Wolfe (0-FW)
> *   **Key Guarantee:** Polynomial-time convergence to Performatively Optimal (PO) policy

---

## Executive Summary

This paper addresses the critical limitations of current solutions in Performative Reinforcement Learning (PRL), moving beyond mere stability to achieve true optimality.

### üéØ The Problem: Bridging the Gap Between Stability and Optimality
In Performative RL, environmental dynamics shift in response to the deployed policy, creating a complex feedback loop. Existing research predominantly targets "performatively stable" (PS) policies‚Äîsolutions where the policy acts as a fixed point of the data distribution. The authors identify a critical flaw in this approach: there is a provably positive constant gap between these stable policies and the true goal, the performatively optimal (PO) policy. In high-stakes applications like autonomous driving or finance, deploying a stable-yet-suboptimal policy results in consistently poor performance.

### üí° The Innovation: The 0-FW Algorithm and Gradient Dominance
The authors introduce the Zeroth-Order Frank-Wolfe (0-FW) algorithm to converge directly to the PO policy. To overcome the nonconvexity of the performative value function, the study leverages two novel theoretical findings:
1.  **Gradient Dominance:** When a policy regularizer (negative entropy) dominates environmental shifts, the value function satisfies gradient dominance, ensuring all stationary points are globally optimal.
2.  **Subspace Constraints:** Stationary points lie within a specific, convex, and compact policy subspace ($\Pi_{\Delta}$) where gradients are bounded and Lipschitz continuous.

The 0-FW algorithm utilizes zeroth-order gradient estimation adjusted via orthogonal transformation to operate efficiently within this subspace.

### üìà The Results: Polynomial Convergence and Optimality Bounds
The study provides the first polynomial-time convergence guarantees for finding the PO policy under standard regularizer dominance conditions. Specifically:
*   Finding a $D_\epsilon$-stationary policy guarantees an $(\epsilon + |\mu||S|)$-optimality gap.
*   Model sensitivity is quantified, showing the distance to the optimal policy scales as $O(\epsilon_p + \epsilon_r + S_p + S_r)$.
*   Empirical experiments demonstrate the 0-FW algorithm converges more effectively to the PO policy than existing methods.

### üåç The Impact: Redefining the Standard for Performative RL
This work shifts the industry benchmark from "stability" to "optimality." By proving global convergence to the PO policy is achievable in polynomial time, the authors invalidate the necessity of settling for stable but suboptimal fixed points. This establishes a new theoretical foundation for designing robust RL agents capable of actively manipulating their environments for maximum reward.

---

## Key Findings

*   **Performance Gap Identified:** There is a provably positive constant gap between existing performatively stable (PS) policies and the desired performatively optimal (PO) policy.
*   **Gradient Dominance Property:** Under the condition that a policy regularizer dominates the environmental shift, the nonconvex value function satisfies a gradient dominance property.
*   **Gradient Boundedness in Subspace:** All sufficiently stationary points lie within a convex and compact policy subspace where the gradient is bounded and Lipschitz continuous.
*   **Superior Empirical Performance:** Experimental results demonstrate that the proposed 0-FW algorithm is more effective than existing algorithms at converging to the desired PO policy.

---

## Contributions

*   **Achieving Performatively Optimal Policies:** Provides a concrete method to achieve the performatively optimal (PO) policy, moving beyond the existing standard of finding performatively stable (PS) policies.
*   **Polynomial-Time Convergence:** Establishes the first polynomial-time convergence guarantees for finding the PO policy under the standard regularizer dominance condition.
*   **Theoretical Characterization of Nonconvexity:** Contributes novel theoretical insights into the nonconvex nature of the value function in PRL, specifically proving conditions under which stationary points become globally optimal and gradients become well-behaved.

---

## Methodology

The authors propose a **zeroth-order Frank-Wolfe (0-FW) algorithm** that operates within the Frank-Wolfe framework and utilizes a zeroth-order approximation of the performative policy gradient.

The theoretical convergence proof relies on two main analytical steps:
1.  Establishing the gradient dominance of the value function when a policy regularizer dominates environmental shifts.
2.  Proving that stationary points are confined within a specific convex and compact policy subspace where Lipschitz continuity holds.

---

## Technical Details

*   **Problem Setting:** Addresses Performative Reinforcement Learning (PRL) where environmental dynamics depend on the deployed policy. The goal is to maximize the performative value function to achieve a Performatively Optimal (PO) policy.
*   **Regularization Strategy:** Employs a negative entropy regularizer to ensure strong convexity and facilitate faster convergence.
*   **Policy Subspace:** Identifies a specific policy subspace, denoted as $\Pi_{\Delta}$, to maintain Lipschitz continuity and smoothness of the gradients.
*   **Algorithm Mechanics (0-FW):**
    *   Combines zeroth-order gradient estimation.
    *   Adjusts estimation for the policy subspace via orthogonal transformation.
    *   Integrates the Frank-Wolfe algorithm with policy averaging techniques.

---

## Results

The analysis of the proposed 0-FW algorithm reveals significant improvements in both theoretical bounds and empirical performance:

*   **Convergence:** The 0-FW algorithm empirically converges more effectively to the Performatively Optimal policy compared to methods targeting Performatively Stable policies.
*   **Computational Complexity:** Achieves polynomial computation complexity for convergence to a stationary policy.
*   **Optimality Gap:** A $D_\epsilon$-stationary policy guarantees an $(\epsilon + |\mu||S|)$-PO optimality.
*   **Sensitivity Metrics:** The distance to the optimal policy scales with environmental sensitivity as $O(\epsilon_p + \epsilon_r + S_p + S_r)$.