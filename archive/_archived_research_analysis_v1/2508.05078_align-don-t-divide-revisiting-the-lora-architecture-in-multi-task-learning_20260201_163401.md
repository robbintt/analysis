# Align, Don't Divide: Revisiting the LoRA Architecture in Multi-Task Learning

*Jinda Liu; Bo Cheng; Yi Chang; Yuan Wu*

---

> ### üìã Quick Facts
> * **Quality Score:** 8/10
> * **Total References:** 40
> * **Model Tested:** Qwen2.5-3B
> * **Benchmarks:** QNLI, PiQA, Winogrande, ARC, GSM8K
> * **Top Performing Method:** M-LoRA (**75.45** Avg Score)
> * **Key Innovation:** Align-LoRA (Explicit alignment loss)

---

## üìù Executive Summary

Current Parameter-Efficient Fine-Tuning (PEFT) strategies for Multi-Task Learning (MTL) in Large Language Models are dominated by complex architectures, such as multi-adapters and dynamic routing networks. These designs operate under the "divide and conquer" assumption, positing that structural diversity is necessary to isolate task-specific features and prevent interference between tasks. This paper challenges this prevailing paradigm, investigating whether such architectural complexity is actually required for effective generalization or if it inadvertently hinders performance by fragmenting knowledge.

The authors introduce a dual-component advancement consisting of the **"M-LoRA"** architecture and the **"Align-LoRA"** training objective. M-LoRA simplifies the system by removing dynamic Mixture-of-Experts (MoE) routers in favor of a static, summation-based aggregation of multi-head outputs, allowing for direct weight merging and zero inference overhead. Complementing this, the study establishes that a standard single-adapter LoRA with a sufficiently increased rank is already highly competitive with complex multi-adapter systems. To further optimize performance, Align-LoRA applies an explicit alignment loss to the shared adapter space; this loss minimizes the statistical distance between the hidden representations of different tasks, forcing the optimization process to focus on learning robust, shared features rather than diverging, task-specific ones.

Experiments conducted on the Qwen2.5-3B model across five standard NLP benchmarks demonstrate that simplified, aligned architectures significantly outperform complex baselines. The M-LoRA architecture achieved the highest average score of **75.45**, surpassing routing-based R-LoRA (74.67) and HydraLoRA (74.04). Crucially, this performance improvement was achieved with greater parameter efficiency. This research represents a significant theoretical and practical shift, suggesting that future research should move away from computationally expensive routing mechanisms toward simpler, merged architectures that maintain zero inference overhead.

---

## üîë Key Findings

*   **Paradigm Shift:** The study challenges the necessity of complex multi-adapter or multi-head LoRA architectures, showing that simplified systems with high inter-head similarity are superior.
*   **Single Adapter Sufficiency:** A standard single-adapter LoRA with a sufficiently increased rank achieves performance highly competitive with complex multi-adapter systems.
*   **Representation over Isolation:** Effective MTL generalization relies on learning robust shared representations rather than isolating task-specific features.
*   **Align-LoRA Superiority:** The proposed Align-LoRA method, which aligns task representations, significantly surpasses all existing baselines.

---

## üß† Methodology

The research utilizes a **comparative analysis** of existing LoRA variants (multi-adapter vs. multi-head) against simplified architectures. The authors introduce **Align-LoRA**, a novel approach that modifies the standard LoRA framework.

*   **Strategy:** Instead of structural diversity, Align-LoRA incorporates an **explicit alignment loss** within the shared adapter space.
*   **Objective:** This loss ensures that task representations are aligned across different tasks, thereby focusing optimization on robust shared features rather than divergent, task-specific ones.

---

## ‚öôÔ∏è Technical Details

The paper revisits LoRA for Multi-Task Learning (MTL) by contrasting three primary architectures:
1.  Multi-Adapter
2.  Multi-Head
3.  Dynamic Routing

**Core Formulation**
The core LoRA formulation freezes pre-trained weights $W$ and injects trainable rank decomposition matrices $A$ and $B$ such that the output is:

$$h = Wx + BAx$$

**M-LoRA Design**
The study focuses on **M-LoRA**, a simplified variant of the routing-based R-LoRA.
*   **Modification:** M-LoRA removes the dynamic Mixture-of-Experts (MoE) router.
*   **Aggregation:** It aggregates head outputs via **static summation** to allow weight merging and zero inference overhead.
*   **Purpose:** This design specifically tests the necessity of input-dependent diversification.

---

## üìä Results

Experiments were conducted on **Qwen2.5-3B** across five NLP benchmarks: QNLI, PiQA, Winogrande, ARC, and GSM8K.

**Performance Metrics**
*   **M-LoRA:** Achieved the highest average score of **75.45**.
*   **R-LoRA:** Scored **74.67**.
*   **HydraLoRA:** Scored **74.04**.

**Efficiency Analysis**
*   **Parameter Usage:** M-LoRA achieved superior results using fewer parameters (**0.41%** vs 0.45% utilized by baselines).

**Representation Analysis**
*   Analysis of mean pairwise cosine similarities revealed that **M-LoRA exhibits higher inter-head similarity** than the baselines.
*   This suggests that encouraging shared representation (alignment) is more effective for MTL than enforcing structural diversity.

---

## üèÜ Contributions

*   **Theoretical Re-evaluation:** Contradicts the necessity of structural diversity in PEFT for MTL, demonstrating that a single high-rank adapter is often sufficient.
*   **New Hypothesis:** Establishes that MTL success in LLMs is driven by the **alignment of task representations** and shared knowledge, rather than structural division.
*   **Methodological Innovation:** Proposes Align-LoRA, a simpler yet more effective paradigm for adapting LLMs to multiple tasks.
*   **Standard Setting:** Sets a new performance standard by demonstrating that Align-LoRA outperforms complex baselines while maintaining a simpler architectural structure.