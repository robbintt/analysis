# UALM: Unified Audio Language Model for Understanding, Generation and Reasoning

*Jinchuan Tian; Sang-gil Lee; Zhifeng Kong; Sreyan Ghosh; Arushi Goel; Chao-Han Huck Yang; Wenliang Dai; Zihan Liu; Hanrong Ye; Shinji Watanabe; Mohammad Shoeybi; Bryan Catanzaro; Rafael Valle; Wei Ping*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Model Base** | Qwen2.5-7B (or Qwen2.5-1.5B for UALM-Gen) |
| **Training Data** | 30M samples, 80k hours, 17B tokens |
| **Audio Tokenization** | X-codec discrete tokens (50Hz, 8 RVQ tokens/frame) |
| **Output Quality** | 48kHz stereo (via Enhancement VAE) |
| **Reasoning Type** | Multimodal Chain-of-Thought (Audio & Text) |
| **Quality Score** | 8/10 |

---

## Executive Summary

Current audio processing landscapes are fragmented, typically requiring distinct, specialized models for separate tasks such as understanding (e.g., speech recognition and classification), generation (e.g., text-to-audio), and reasoning. This separation prevents the development of cohesive systems capable of advanced multimodal interaction, where a model must interpret audio inputs and synthesize outputs within a unified reasoning loop. While diffusion models have dominated high-fidelity audio generation, they exist largely separate from transformer-based language models used for understanding. This paper addresses the critical need for a single, unified architecture that bridges understanding and generation to enable complex, cross-modal reasoning without sacrificing performance in either domain.

The key innovation is the **UALM (Unified Audio Language Model)** framework, a decoder-only Transformer architecture initialized from Qwen2.5 that integrates audio understanding, generation, and reasoning. Technically, UALM abandons diffusion-based generation for a next-token prediction approach using discrete X-codec audio tokens (50Hz, 8 RVQ tokens/frame) and an Enhancement VAE for upscaling to 48kHz stereo. For reasoning, the researchers introduced **UALM-Reason**, a cross-modal generative reasoning mechanism that leverages multimodal Chain-of-Thought (CoT). This allows the model to process intermediate "thinking" steps using a mixture of text and audio (Rich Captions), facilitating complex patterns like Enrichment, Dialogue, and Self-Reflection. The training pipeline employs a modality alignment curriculum and a massive data blend of 30M samples and 80k hours of audio.

The UALM framework achieved parity with state-of-the-art specialized models across all three domains without performance trade-offs. Specifically, UALM-Gen demonstrated generation quality comparable to top diffusion-based models, validating the efficacy of direct audio token prediction. The unified model maintained high performance on understanding tasks while successfully executing reasoning operations like audio correction and self-reflection. The study utilized a highly optimized data composition consisting of 39.2% Audio Generation, 33.1% Audio Understanding, and 27.7% Text-Only data. Although the LM-based generation approach required significantly more data to reach maturity (80k hours compared to <4k hours for diffusion models), the resultant system successfully demonstrated the first instance of generative reasoning using mixed audio-text contexts.

This research represents a significant paradigm shift by proving that language models can effectively generate high-quality audio via discrete token prediction, challenging the prevailing dominance of diffusion models in audio synthesis. By establishing the first successful cross-modal generative reasoning paradigm in the audio domain, UALM opens new avenues for developing intelligent audio systems capable of complex self-reflection and interactive dialogue. The unification of understanding, generation, and reasoning into a single model simplifies deployment and reduces the architectural complexity of future multimodal agents, setting a new standard for holistic audio intelligence.

---

## Key Findings

*   **Unified Framework:** The UALM framework successfully unifies audio understanding, text-to-audio generation, and multimodal reasoning without compromising performance.
*   **Competitive Generation:** UALM-Gen achieves generation quality comparable to state-of-the-art diffusion-based models by directly predicting audio tokens, overcoming historical LM inferiority.
*   **Performance Parity:** Using specific data blending, training recipes, and inference techniques, the model matches the quality of specialized SOTA models across all three domains.
*   **Cross-Modal Reasoning:** The study presents the first successful demonstration of cross-modal generative reasoning (UALM-Reason), where using text and audio in intermediate thinking steps effectively facilitates complex generation tasks like self-reflection.

---

## Methodology

The researchers developed a single unified architecture to handle audio understanding, generation, and reasoning simultaneously. The core components of this approach include:

*   **Unified Architecture:** A single model designed to handle diverse tasks rather than a collection of specialized models.
*   **UALM-Gen (Generation):** A text-to-audio language model that operates by directly predicting audio tokens rather than relying on diffusion models.
*   **Optimization Strategies:** The approach relies on curating combined datasets, developing specific training recipes to balance tasks, and utilizing tailored inference methods.
*   **UALM-Reason (Reasoning):** For complex tasks, the method employs a multimodal reasoning loop that incorporates both text and audio modalities during intermediate 'thinking' steps.

---

## Technical Details

### Architecture & Initialization
*   **Base Model:** Decoder-only Transformer initialized from Qwen2.5-7B (or Qwen2.5-1.5B for UALM-Gen).
*   **Understanding Paradigm:** Encoder-Adapter-LLM utilizing a 25Hz acoustic encoder.

### Audio Tokenization & Processing
*   **Input:** Uses internal BPE tokens for text.
*   **Generation:** Employs discrete X-codec tokens at 50Hz with 8 RVQ tokens per frame.
*   **Enhancement:** Uses an Enhancement VAE to upsample audio from 16kHz mono to 48kHz stereo.

### Training Infrastructure
*   **Scale:** Massive training on 30M samples, 80k hours of audio, resulting in 17B tokens.
*   **Loss Scaling:** Implements a loss scaling factor of 1/8 per audio token to balance with text loss.
*   **Curriculum:** Utilizes sequence packing and a modality alignment curriculum.

### Inference & Reasoning
*   **Inference:** Utilizes Classifier-Free Guidance.
*   **Reasoning Mechanism:** UALM-Reason employs multimodal Chain-of-Thought using **'Rich Captions'** to support specific reasoning patterns:
    *   Enrichment
    *   Dialogue
    *   Self-Reflection

---

## Contributions

1.  **Introduction of UALM:** The first model to bridge the gap between audio understanding and generation, providing a unified solution for advanced multimodal reasoning.
2.  **Development of UALM-Gen:** A competitive non-diffusion approach to text-to-audio generation that validates the effectiveness of direct audio token prediction.
3.  **Cross-Modal Reasoning Paradigm:** Establishment of the first cross-modal generative reasoning paradigm in the audio domain, proving that models can utilize mixed audio-text context during intermediate steps.

---

## Results

### Data Composition
*   **Unified Data:**
    *   Audio Generation: 39.2%
    *   Audio Understanding: 33.1%
    *   Text Only: 27.7%
*   **Post-Training Reasoning Data:**
    *   500k Enrichment samples
    *   250k Dialogue samples
    *   250k SFT samples
    *   80k DPO pairs

### Performance Analysis
*   **UALM-Gen:** Achieved generation quality comparable to state-of-the-art diffusion-based models.
*   **Unified Model:** Matches the quality of specialized SOTA models across all domains without compromising performance.
*   **Data Efficiency:** LM-based generation required significantly more data (~80k hours) to reach maturity compared to diffusion models (<4k hours).
*   **UALM-Reason:** Successfully demonstrated the first instance of cross-modal generative reasoning, enabling self-reflection and audio correction.

---
**References:** 20 citations