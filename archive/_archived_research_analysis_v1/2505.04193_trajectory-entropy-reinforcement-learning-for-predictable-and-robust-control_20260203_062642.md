---
title: Trajectory Entropy Reinforcement Learning for Predictable and Robust Control
arxiv_id: '2505.04193'
source_url: https://arxiv.org/abs/2505.04193
generated_at: '2026-02-03T06:26:42'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Trajectory Entropy Reinforcement Learning for Predictable and Robust Control

*Bang You; Chenxu Wang; Huaping Liu*

---

## ðŸ“Š Quick Facts

| **Metric** | **Details** |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Environment** | MuJoCo (6 High-Dimensional Locomotion Tasks) |
| **Key Algorithm** | TERL (Trajectory Entropy Reinforcement Learning) |
| **Baselines** | RPC, RPC-Orig, LZ-SAC, SAC |
| **Top Performance** | H1 Walk: $137 \pm 19$ (vs SAC: 85) |
| **Citations** | 40 References |

---

## ðŸ“ Executive Summary

Current deep reinforcement learning (RL) algorithms typically focus solely on maximizing cumulative rewards, often resulting in complex, high-variance behaviors that lack predictability. In high-dimensional control tasks such as locomotion, these "jittery" or chaotic policies are prone to failure when deployed in real-world environments subject to external perturbations, sensor noise, or model inaccuracies. This paper addresses the critical challenge of improving the robustness and stability of RL policies, bridging the gap between simulated training in environments like MuJoCo and the reliability required for physical deployment.

The authors propose **Trajectory Entropy Reinforcement Learning (TERL)**, a novel approach that introduces an inductive bias towards "simplicity" by explicitly minimizing the conditional entropy of action trajectories rather than just instantaneous actions. TERL minimizes the number of bits required to describe action information given state trajectories, thereby promoting periodic and compressible behaviors. Technically, the method estimates conditional entropy using a variational parameterized action prediction model to construct an information-regularized reward function. The algorithm employs a joint optimization strategy for the policy network and the prediction model, governed by a hyperparameter $\alpha$; notably, setting $\alpha=0$ recovers the standard Soft Actor-Critic (SAC) algorithm.

TERL was evaluated on six high-dimensional locomotion tasks in the MuJoCo environment against baselines including RPC, RPC-Orig, LZ-SAC, and SAC. TERL outperformed SAC in five out of six tasks, achieving a score of $137 \pm 19$ on the H1 Walk task (compared to SACâ€™s 85) and $479 \pm 52$ on Humanoid Walk. However, TERL failed to outperform the baselines on the Humanoid-Run task, highlighting a boundary where the bias towards simplicity may hinder performance in more complex gaits. In robustness tests, TERL retained $75.5\%$ performance under a mass scale of 0.50 and $90.1\%$ performance at an action noise strength of 0.30, significantly outperforming both LZ-SAC and the RPC baselines. Additionally, at an observation noise of 0.04, TERL maintained $92.9\%$ performance. Trajectory analysis confirmed that TERL produced more periodic behaviors, achieving the smallest average bzip2 file size compared to baselines, while ablation studies confirmed that increasing $\alpha$ effectively reduces normalized trajectory file size.

The significance of this research lies in establishing a strong empirical link between minimizing conditional entropy (behavioral simplicity) and achieving control robustness. By demonstrating that policies which are more predictable and compressible are also more resilient to domain shifts and noiseâ€”outperforming even robustness-focused baselines like RPCâ€”TERL provides a valuable new framework for developing reliable robotic systems. This work shifts the optimization paradigm from purely reward-driven to information-regularized control, offering a promising direction for sim-to-real transfer and safety-critical applications where stability is as important as task performance.

---

## ðŸ”‘ Key Findings

*   **Simplicity Bias:** Introducing an inductive bias towards "simplicity" via trajectory entropy minimization leads to more robust control policies.
*   **Performance Superiority:** TERL outperformed the standard Soft Actor-Critic (SAC) algorithm in 5 out of 6 evaluated high-dimensional locomotion tasks.
*   **Noise Resilience:** The method demonstrated exceptional robustness, retaining over 90% performance even with significant action noise ($0.30$) and over 92% performance with observation noise ($0.04$).
*   **Compressibility:** Trajectory analysis using bzip2 compression confirmed that TERL generates more periodic and compressible behaviors than baselines.
*   **Trade-off Limit:** While generally effective, the bias towards simplicity can hinder performance in highly complex tasks requiring intricate gaits (e.g., Humanoid-Run).

---

## ðŸ› ï¸ Methodology

The proposed methodology centers on **Trajectory Entropy Reinforcement Learning (TERL)**. Unlike traditional RL methods that maximize only cumulative reward, TERL jointly maximizes cumulative rewards and minimizes the entropy of entire action trajectories. This is achieved by defining trajectory entropy as the number of bits required to describe action information given state trajectories.

To estimate this entropy, the authors utilize a **variational parameterized action prediction model**. This model allows for the construction of an information-regularized reward function. The system employs a joint optimization strategy that trains both the policy network and the prediction model simultaneously. A critical hyperparameter, $\alpha$, controls the trade-off between maximizing rewards and minimizing entropy; setting $\alpha=0$ effectively reduces TERL to the standard Soft Actor-Critic (SAC) algorithm.

---

## ðŸ“ Contributions

*   **Novel Objective Function:** Development of an objective function that combines reward maximization with trajectory entropy minimization to enforce behavioral simplicity.
*   **Variational Estimation:** Introduction of a variational parameterized action prediction model to accurately estimate the conditional entropy of action sequences.
*   **Empirical Robustness Link:** Provided comprehensive empirical evidence demonstrating that minimizing conditional entropy (increasing compressibility) correlates strongly with improved robustness against domain shifts and noise.
*   **Benchmarking:** Established new performance baselines on high-dimensional MuJoCo locomotion tasks regarding robustness to mass scaling, action noise, and observation noise.

---

## âš™ï¸ Technical Details

*   **Method Name:** Trajectory Entropy Reinforcement Learning (TERL)
*   **Core Principle:** Inductive bias towards 'simplicity' to improve robustness.
*   **Objective Function:**
    $$ J(\pi) = \mathbb{E}[\sum_t r(s_t, a_t)] - \alpha \cdot H(A_\tau | S_\tau) $$
    *Where $H(A_\tau | S_\tau)$ is the trajectory entropy.*
*   **Entropy Definition:** The bits required to describe action information given state trajectories.
*   **Entropy Estimation:** Calculated using a variational parameterized action prediction model.
*   **Optimization Strategy:** Joint optimization of the policy network and the prediction model.
*   **Hyperparameter Control:**
    *   $\alpha > 0$: TERL (Balances reward and entropy minimization).
    *   $\alpha = 0$: Reduces to Soft Actor-Critic (SAC).

---

## ðŸ“ˆ Results

### Performance Comparison (MuJoCo)
TERL was evaluated against RPC, RPC-Orig, LZ-SAC, and SAC across 6 tasks.

*   **H1 Walk:** TERL scored **$137 \pm 19$** (SAC scored 85).
*   **Humanoid Walk:** TERL scored **$479 \pm 52$**.
*   **Overall:** Outperformed SAC in 5 out of 6 tasks.
*   **Exception:** Underperformed on Humanoid-Run, suggesting limitations in complex gait generation.

### Robustness Testing
TERL showed significant resilience to perturbations compared to baselines.

*   **Mass Scaling (0.50):** Retained **$75.5\%$** performance.
*   **Action Noise ($\sigma=0.30$):** Retained **$90.1\%$** performance (Outperformed LZ-SAC and SAC).
*   **Observation Noise ($\sigma=0.04$):** Retained **$92.9\%$** performance.

### Trajectory Analysis
*   **Compressibility:** TERL produced the smallest average bzip2 file size, confirming more periodic behaviors.
*   **Ablation Study:** Confirmed that increasing the hyperparameter $\alpha$ results in a reduced normalized trajectory file size.

---
**Quality Score:** 9/10 | **References:** 40 citations