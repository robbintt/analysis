---
title: Being Strong Progressively! Enhancing Knowledge Distillation of Large Language
  Models through a Curriculum Learning Framework
arxiv_id: '2506.05695'
source_url: https://arxiv.org/abs/2506.05695
generated_at: '2026-02-03T19:02:33'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework

*Lingyuan Liu; Mengxiang Zhang*

***

> ### ⚡ Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Total References:** 40 Citations
> *   **Performance Gap:** ~1.7% between open-source (DeepSeek-v3) and proprietary (GPT-4o) models.
> *   **SGO Time Cost:** Student-Generated Outputs can account for up to **80%** of total training time.
> *   **Optimal Hyperparameters:** 4 subset partitions ($n=4$), RRF constant $k=60$.

***

## Executive Summary

Knowledge Distillation (KD) is essential for compressing Large Language Models (LLMs) into efficient student models capable of instruction following. However, this paper addresses a critical instability in existing white-box KD methods: significant distribution shifts between the teacher and student models. These shifts frequently lead to training failures such as catastrophic forgetting, mode collapse, and mismatches between training and inference performance. As the gap between proprietary and open-source models narrows, ensuring the stability and efficiency of the distillation process is paramount for creating robust, smaller models.

The authors introduce **POCL (Progressive Overload Curriculum Learning)**, a plug-in framework inspired by strength training principles to stabilize KD. Technically, POCL employs a "coach-athlete" paradigm where the student minimizes a combined loss function ($L_s = \alpha \cdot L_{ce} + (1 - \alpha) \cdot L_{kd}$). The framework consists of two core modules: a **Difficulty Measurer**, which ranks training samples using Reciprocal Rank Fusion (RRF) with a constant $k=60$ based on Rouge-L and Cross-Entropy metrics; and a **Training Scheduler**. This scheduler partitions data into $n=4$ stages of increasing difficulty and dynamically adjusts the temperature ($\tau$) and loss weights ($\alpha$) as training progresses, mimicking a progressive overload regimen.

The study demonstrates that POCL consistently improves student model performance across various white-box KD methods and model families. The authors identified optimal hyperparameters, specifically partitioning the dataset into 4 subsets and using an RRF constant of 60. Qualitative analysis revealed that while standard KLD offers stable but suboptimal retention, and Reverse KLD risks mode collapse, POCL effectively mitigates these issues. Furthermore, the framework highlights that Student-Generated Outputs (SGOs), which can consume up to 80% of total training time, often introduce noise; POCL’s structured approach manages this noise to prevent performance degradation.

***

## Key Findings

*   **Consistent Performance Improvement:** The proposed POCL framework consistently improves the performance of distilled student models in instruction-following settings across various white-box KD methods and model families.
*   **Addressing Distribution Shifts:** Existing KD methods often suffer from significant distribution shifts in the student model, leading to specific failures such as catastrophic forgetting, mode collapse, and training-inference mismatch.
*   **Easy-to-Hard Strategy:** Sorting training samples from easy to hard is an effective strategy for Knowledge Distillation in LLMs.
*   **Enhanced Stability & Efficiency:** By structuring training data and progressively increasing difficulty, the approach enhances both the stability and efficiency of the learning process.

## Methodology

The authors propose **POCL (Progressive Overload Curriculum Learning)**, a plug-in curriculum learning framework inspired by the "progressive overload" principle of strength training. It integrates into existing white-box KD approaches with minimal computational overhead through two core components:

1.  **Difficulty Measurer:** Ranks and partitions training samples based on difficulty, organizing them from easy to hard.
2.  **Training Scheduler:** Incrementally introduces these sample subsets into the distillation process at fixed intervals while applying loss functions with progressively rising temperatures.

## Technical Details

POCL (Progressive Overload-Based Curriculum Learning) is a framework designed to enhance white-box Knowledge Distillation for Large Language Models by addressing issues like distribution shifts and catastrophic forgetting.

### Core Architecture
*   **Paradigm:** 'Coach-athlete' paradigm.
*   **Loss Function:** The student minimizes a combined loss function:
    $$L_s = \alpha \cdot L_{ce} + (1 - \alpha) \cdot L_{kd}$$

### Key Modules

| Component | Functionality |
| :--- | :--- |
| **Difficulty Measurer Module** | Ranks training samples using Reciprocal Rank Fusion (RRF) with constant $k=60$. Metrics used include **Rouge-L** and **Cross-Entropy**. |
| **Training Scheduler Module** | Progressively introduces harder subsets (partitioned into **$n=4$ stages**). It dynamically adjusts Temperature ($\tau$) and Loss Weight ($\alpha$) during training. |

## Contributions

*   **Addressing Training Instability:** The work provides a solution to the problem of distribution shift in LLM distillation, specifically mitigating catastrophic forgetting, mode collapse, and training-inference mismatch.
*   **Novel Framework:** Introduction of a versatile, plug-in curriculum learning framework that requires minimal computational overhead to enhance standard white-box KD techniques.
*   **Data Structuring Strategy:** Demonstration of how to effectively structure training data within the KD process (by difficulty and temperature progression) to optimize the stability and performance of distilled Large Language Models.

## Results

*   **Performance Gap Analysis:** The performance gap between open-source models (e.g., DeepSeek-v3) and proprietary models (e.g., GPT-4o) is approximately **1.7%** on the Chatbot Arena Leaderboard.
*   **Training Efficiency:** Generating Student-Generated Outputs (SGOs) can account for up to **80%** of total training time.
*   **Hyperparameters:** Key optimal settings include partitioning the dataset into **4 subsets** and setting the RRF constant to **60**.
*   **Qualitative Analysis:**
    *   **Standard KLD:** Offers stable retention but suboptimal performance.
    *   **Reverse KLD:** Risks mode collapse.
    *   **Excessive SGO Usage:** Leads to performance degradation due to noise.

***

**Document Statistics**
*   **Quality Score:** 8/10
*   **References:** 40 citations