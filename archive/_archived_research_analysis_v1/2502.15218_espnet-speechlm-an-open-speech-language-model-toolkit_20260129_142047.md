# ESPnet-SpeechLM: An Open Speech Language Model Toolkit

*Jinchuan Tian; Jiatong Shi; William Chen; Siddhant Arora; Yoshiki Masuyama; Takashi Maekaku; Yihan Wu; Junyi Peng; Shikhar Bharadwaj; Yiwen Zhao; Samuele Cornell; Yifan Peng; Xiang Yue; Chao-Han Huck Yang; Graham Neubig; Shinji Watanabe*

---

> ### **Quick Facts**
>
> | Metric | Detail |
> | :--- | :--- |
> | **Model Size** | 1.7 Billion Parameters |
> | **Training Efficiency** | 35% Model FLOPs Utility (MFU) on NVIDIA H100 |
> | **WER (Text-to-Speech)** | 3.1% (LibriSpeech Test-Clean) |
> | **Proxy MOS** | 4.03 |
> | **Capabilities** | Supports 15 tasks & 10 tokenizers |
> | **Quality Score** | 8/10 |

---

### Executive Summary

This paper addresses the increasing complexity and fragmentation in developing Speech Language Models (SpeechLMs) that process dual modalities of text and speech. As the demand for voice-driven agentic applications grows, researchers face significant hurdles in creating standardized, reproducible, and scalable workflows. Existing solutions often lack the flexibility to handle diverse tasks efficiently, creating a high barrier to entry for developing high-performance, large-scale models.

The authors introduce **ESPnet-SpeechLM**, an open-source framework designed to standardize speech processing by reframing all tasks as universal sequential modeling problems. Technically, the toolkit leverages a decoder-only Transformer architecture to manage discrete tokens across text and audio streams, unified through a declarative `TaskTemplate` for sequence construction. It features a cohesive, modular pipeline covering data preprocessing, pre-training, inference, and evaluation.

The system employs offline tokenization with joint vocabulary construction and optimizes computational efficiency through the integration of DeepSpeed and FlashAttention. Validating the framework's scalability, the authors successfully pre-trained a 1.7B-parameter model on combined text and speech data. In rigorous Text-to-Speech evaluations, the model outperformed competitive baselines like ChatTTS and CosyVoice. This contribution democratizes access to state-of-the-art SpeechLM technology, lowering the barrier to entry for developing sophisticated voice-driven agentic applications.

---

### Key Findings

*   **Competitive Performance:** The toolkit enables the successful construction of competitive Speech Language Models (SpeechLMs) across multiple use cases and diverse benchmarks.
*   **Scalability:** Using the toolkit, the authors developed a scalable **1.7B-parameter model** capable of being pre-trained on dual modalities (text and speech).
*   **End-to-End Workflow:** The framework supports a cohesive end-to-end workflow, streamlining the transition from data preprocessing to model evaluation.
*   **Application Flexibility:** The design ensures high flexibility and efficiency in developing voice-driven agentic applications through highly configurable modules.

---

### Methodology

The methodology centers on **standardizing speech processing workflows** by framing all tasks as universal sequential modeling problems. The toolkit implements a cohesive, modular pipeline covering four distinct stages:

1.  **Data Preprocessing**
2.  **Pre-training**
3.  **Inference**
4.  **Task Evaluation**

This approach allows users to utilize task templates and configure specific settings to manage the workflow, ensuring the process is streamlined, scalable, and adaptable to various requirements.

---

### Contributions

*   **ESPnet-SpeechLM Toolkit:** Introduction of an open, transparent, and reproducible toolkit designed to democratize access to SpeechLM and voice-driven agentic application development.
*   **Standardization Framework:** Establishment of a standardized workflow that converts speech processing tasks into universal sequential modeling problems, facilitating easier model development.
*   **Scalability Demonstration:** Validation of the toolkit's capabilities through the release of a **1.7B-parameter model** and associated recipes, proving the system's ability to handle large-scale pre-training on combined text and speech data.
*   **Reproducibility and Flexibility:** Provision of highly configurable, transparent modules for every stage of the development lifecycle, allowing for rigorous reproducibility and customization.

---

### Technical Details

**Architecture & Modeling**
*   **Core Design:** Utilizes a **decoder-only Transformer architecture** for unified sequence modeling.
*   **Tokenization:** Converts inputs and outputs into discrete tokens via offline tokenization with joint vocabulary construction.
*   **Sequence Construction:** Constructs sequences via a declarative `TaskTemplate`.

**Features & Support**
*   **Multi-stream Support:** Features multi-stream language model support for audio codecs.
*   **Modalities:** Supports various inputs including text, audio (via codecs/SSL), and auxiliary data.
*   **Modularity:** Allows for modular backbone choices, including integration with **HuggingFace Transformers**.

**Performance Optimization**
*   **Efficiency:** Integrates **DeepSpeed** and **FlashAttention** for optimization.
*   **Scope:** Supports a wider range of features than competitors (15 tasks, 10 tokenizers).

---

### Results

**Performance Metrics**
*   **Computational Efficiency:** Achieved a Model FLOPs Utility (**MFU**) of up to **35%** on NVIDIA H100 GPUs.
*   **Scalability:** Enabled the construction of a scalable 1.7B-parameter model pre-trained on text and speech.

**Benchmark: LibriSpeech Test-Clean (Text-to-Speech)**
The model achieved superior results compared to baselines like ChatTTS and CosyVoice:
*   **Word Error Rate (WER):** 3.1%
*   **Speaker Similarity (SPK_SIM):** 0.55
*   **Proxy MOS:** 4.03

***

**Quality Score:** 8/10 | **References:** 12 citations