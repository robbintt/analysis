---
title: 'TD-M(PC)$^2$: Improving Temporal Difference MPC Through Policy Constraint'
arxiv_id: '2502.03550'
source_url: https://arxiv.org/abs/2502.03550
generated_at: '2026-02-06T05:30:08'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# TD-M(PC)$^2$: Improving Temporal Difference MPC Through Policy Constraint

***Haotian Lin; Pengcheng Wang; Jeff Schneider; Guanya Shi***

---

> ### üìä Quick Facts: Sidebar
>
> *   **Quality Score**: 9/10
> *   **Citations**: 40 References
> *   **Key Innovation**: Policy Regularization
> *   **Core Problem**: Structural Policy Mismatch
> *   **Peak Result**: ~2159% Value Error in 61-DoF tasks (Baseline)
> *   **Computational Cost**: Zero Additional Overhead

---

## üìë Executive Summary

This research addresses the critical issue of persistent value overestimation in model-based reinforcement learning (MBRL) algorithms that integrate planning with learned policy priors, such as TD-MPC2. This overestimation is particularly detrimental in high-dimensional control tasks, leading to instability and failure to converge. The authors identify "**structural policy mismatch**" as the root cause: a distribution shift occurs between the data generation policy (the planner) and the learned policy prior used for value learning. This discrepancy causes the value function to be evaluated on out-of-distribution actions, resulting in inflated value estimates that prevent the agent from learning effective policies.

The key innovation is the introduction of **TD-M(PC)$^2$**, a minimalist modification that introduces a policy regularization term to the MBRL objective. This term constrains the planner to remain close to the learned policy prior during the value learning phase. By aligning the distributions of the planner and the prior, the method effectively reduces out-of-distribution queries and bridges the structural gap between data generation and value estimation. Crucially, this approach is implemented as a lightweight add-on to existing architectures like TD-MPC2, requiring **no additional computational overhead**.

Experimental results demonstrate that Value Approximation Error scales dramatically with action dimensionality. While standard TD-MPC2 maintains manageable error (~15%) in low-DoF tasks, the error explodes in high-dimensional spaces, reaching up to **2159%** in 61-DoF humanoid tasks. In contrast, TD-M(PC)$^2$ effectively mitigates this bias, achieving significant performance gains over strong baselines and successfully solving complex locomotion tasks where standard methods fail.

---

## üîç Key Findings

*   **Root Cause Identification**: Existing MBRL algorithms suffer from value overestimation due to a structural policy mismatch between the data generation policy and the learned policy prior.
*   **Regularization Solution**: Introducing a policy regularization term mitigates this mismatch by reducing out-of-distribution queries.
*   **Performance Gains**: The proposed method achieves significant performance improvements over strong baselines like TD-MPC2.
*   **Complex Task Mastery**: Particularly effective in complex domains, such as 61-DoF humanoid control.
*   **Efficiency**: Requires no additional computational overhead, serving as a minimalist modification.

---

## üõ†Ô∏è Methodology

The researchers propose a streamlined approach to fixing value overestimation in MBRL through a minimalist modification:

1.  **Policy Regularization Term**: A regularization term is introduced to the existing framework.
2.  **Constraint Mechanism**: This term constrains the policy to remain close to the prior, effectively bridging the gap between the planner's data generation policy and the learned policy prior.
3.  **Reduction of OOD Queries**: By limiting the policy's deviation, the method reduces out-of-distribution queries during value learning.
4.  **Implementation**: Designed as a lightweight add-on to architectures like TD-MPC2 that does not require extra computational resources.

---

## ‚öôÔ∏è Technical Details

The paper extends the **TD-MPC2** architecture with specific theoretical and mechanical adjustments:

*   **Core Architecture**: Combines a latent world model with sampling-based MPC and TD learning.
*   **Components**:
    *   Encoder for latent states.
    *   Dynamics and reward models.
    *   Policy prior for value learning.
*   **Planning Algorithm**: Performed via MPPI (Model Predictive Path Integral) with bootstrapping.
*   **Theoretical Contribution**:
    *   Addresses **Structural Policy Mismatch**: A distribution shift between the MPC planner and value training.
    *   **Theorem 3.1**: Provides a bound on suboptimality, demonstrating that the planner amplifies value errors.
*   **Optimization**: Adds a policy regularization term to align distributions and directly reduce value overestimation.

---

## üìà Results

The experimental validation highlights the severity of value overestimation in current methods and the efficacy of the proposed solution:

*   **Dimensionality Sensitivity**: Value Approximation Error ($\nabla V$) scales significantly with action dimensionality (DoF).
*   **Low-DoF Baseline**: In tasks like *Hopper-Stand*, standard methods exhibit ~15% error.
*   **High-DoF Failure**: In high-dimensional tasks (e.g., 61-DoF humanoid), standard methods show massive overestimation, peaking at **2159%**.
*   **TD-M(PC)$^2$ Success**: The proposed method mitigates this accumulated bias, enabling stable performance where standard TD-MPC2 fails due to error amplification.
*   **Benchmarking**: Sets new standards in complex domains like 61-DoF humanoid locomotion.

---

## üèÜ Contributions

This work makes three primary contributions to the field of Model-Based Reinforcement Learning:

1.  **Diagnostic Analysis**: Provides theoretical and experimental analysis identifying 'structural policy mismatch' as the root cause of value overestimation.
2.  **Novel Method**: Introduces **TD-M(PC)$^2$**, a method utilizing policy regularization to minimize out-of-distribution queries.
3.  **Empirical Validation**: Demonstrates that simple regularization yields significant performance boosts in state-of-the-art continuous control benchmarks, proving the method's capability in high-complexity environments.