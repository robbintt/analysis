# Adaptive Shared Experts with LoRA-Based Mixture of Experts for Multi-Task Learning

*Minghao Yang; Ren Togo; Guang Li; Takahiro Ogawa; Miki Haseyama*

***

> ### ðŸ“Š Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Core Framework** | LoRA-based Mixture of Experts (MoE) |
> | **Key Innovation** | Adaptive Shared Experts (ASE) |
> | **Training Strategy** | Fine-grained experts (High count, Low rank) |
> | **Benchmark** | PASCAL-Context |
> | **Quality Score** | 6/10 |

---

## Executive Summary

Existing Mixture-of-Experts (MoE) methods for Multi-Task Learning (MTL) struggle with inefficient knowledge sharing and redundant parameter adaptation when migrating from single-task pretrained backbones. This redundancy causes parameter bloat and performance degradation, hindering smooth task transitions and optimal expert specialization. Addressing these inefficiencies is critical for developing scalable MTL architectures that leverage pretrained knowledge without the penalties associated with traditional adaptation methods.

This paper introduces the **Adaptive Shared Experts (ASE)** architecture within a Low-Rank Adaptation (LoRA)-based MoE framework. ASE employs a routing mechanism that assigns gating weights to shared experts, which are jointly normalized with sparse experts to foster better collaboration. Crucially, the method implements a fine-grained strategy that increases the total number of LoRA experts while proportionally reducing their individual ranks. This adjustment maintains a comparable parameter budget while significantly enhancing the granularity of knowledge sharing and specialization.

Extensive experiments on the PASCAL-Context benchmark validate the ASE approach, demonstrating consistent outperformance against existing baselines in semantic segmentation. The architecture achieved superior mean Intersection over Union (mIoU) scores compared to standard LoRA and other MoE variants. These findings confirm that the fine-grained strategyâ€”increasing expert quantity while reducing rankâ€”effectively balances the trade-off between specialization and cooperation within a strict parameter limit.

This research represents a significant advancement in parameter-efficient MTL by offering a robust solution to the redundancy issues inherent in single-to-multi-task transitions. The introduction of Adaptive Shared Experts establishes a new design paradigm for scalable MoE architectures, demonstrating that parameter efficiency does not necessitate a performance sacrifice. By proving that optimizing expert count and rank distribution yields better knowledge sharing, this work provides a viable pathway for enhancing expert specialization and cooperation in complex multi-task scenarios.

---

## Key Findings

*   **Challenge Addressed:** Existing MoE methods for MTL suffer from redundant adaptation and inefficient knowledge sharing due to reliance on single-task pretrained backbones.
*   **ASE Architecture:** The proposed Adaptive Shared Experts (ASE) architecture facilitates a smoother transition from single-task to multi-task learning while significantly enhancing expert specialization and cooperation.
*   **Parameter Efficiency:** Increasing the quantity of LoRA experts while proportionally reducing their rank enables more effective knowledge sharing **without increasing the parameter budget**.
*   **Validation:** Extensive experiments on the **PASCAL-Context** benchmark confirm that the approach consistently improves performance over existing baselines.

---

## Methodology

The research proposes a novel approach utilizing a **Low-Rank Adaptation (LoRA)** based Mixture-of-Experts (MoE) architecture. The methodology is defined by two primary components:

1.  **Adaptive Shared Experts (ASE):**
    *   Shared experts are introduced into the architecture and assigned gating weights computed by a router.
    *   These weights are jointly normalized with sparse experts to ensure balanced activation and cooperation.

2.  **Fine-Grained Expert Strategy:**
    *   The approach implements a strategy to increase the number of LoRA experts.
    *   To maintain a comparable parameter budget, the rank of each expert is proportionally reduced as the quantity increases.

---

## Architecture & Technical Details

The technical implementation focuses on refining the transition from single-task to multi-task learning through specific architectural adjustments:

*   **Framework:** LoRA-based Mixture of Experts (MoE) for Multi-Task Learning (MTL).
*   **Specialization & Cooperation:** The ASE refines expert specialization and enhances cooperation between shared and sparse experts.
*   **Parameter Scaling:** The architecture improves parameter efficiency by optimizing the ratio of the number of experts to their rank.
*   **Knowledge Sharing:** The fine-grained design maximizes knowledge sharing efficiency without increasing the overall parameter budget.

---

## Contributions

*   **Novel Solution:** Provides a novel solution to the challenges of redundant adaptation and knowledge sharing inherent in transitioning from single-task to multi-task learning.
*   **Mechanism Introduction:** Introduces the **Adaptive Shared Experts (ASE)** mechanism specifically to improve expert specialization and cooperation.
*   **Design Optimization:** Contributes a parameter-efficient, fine-grained expert design that maximizes knowledge sharing efficiency by optimizing the trade-off between the number of experts and their rank.

---

## Results

Experiments were conducted on the **PASCAL-Context** dataset:Semantic Segmentation.
*   **Performance:** The proposed ASE approach consistently improved performance metrics compared to existing baselines, including standard LoRA and other MoE variants.
*   **Metrics:** The architecture achieved superior mean Intersection over Union (mIoU) scores.
*   **Note:** While specific quantitative values were not provided in the text, the trend of consistent improvement was emphasized.

---
**Quality Score:** 6/10  
**References:** 0 citations