# On the Theory of Continual Learning with Gradient Descent for Neural Networks

*Hossein Taheri; Avishek Ghosh; Arya Mazumdar*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations
> *   **Core Topic:** Catastrophic Forgetting & Continual Learning
> *   **Method:** Gradient Descent in Kernel (Lazy) Regime
> *   **Key Variables:** Iterations ($T$), Sample Size ($n$), Tasks ($K$), Width ($m$)

---

## Executive Summary

This research addresses the critical challenge of **catastrophic forgetting** in neural networks trained via Continual Learning (CL). While empirical methods have been proposed to mitigate the loss of knowledge from previous tasks, the field lacks a rigorous theoretical understanding of the mechanisms driving forgetting and the specific factors that influence its rate.

This paper is significant because it moves beyond heuristic observations to establish a **mathematically grounded framework** for understanding the limitations of sequential optimization. It seeks to quantify exactly how learning a new task erodes performance on previous ones, providing a necessary theoretical foundation for strategies that are currently largely based on intuition.

The key innovation lies in deriving precise, closed-form mathematical bounds on the rate of forgetting within a tractable theoretical setting. The authors analyze a one-hidden-layer quadratic neural network trained using standard Gradient Descent (GD) within the kernel (lazy) regime. The study utilizes a structural "XOR cluster dataset" superimposed with Gaussian noise, where tasks are defined as distinct clusters characterized by orthogonal means.

By employing unregularized Empirical Risk Minimization (ERM) without interventions like replay buffers, the study isolates the dynamics of forgetting. This approach allows for a rigorous characterization of learning interactions across $K$ independent tasks using a specific network architecture with a $1/\sqrt{m}$ scaling factor.

The study establishes that the rate of forgetting is explicitly bounded by a functional dependence on four key variables: the number of optimization iterations ($T$), sample size ($n$), number of tasks ($K$), and hidden layer size ($m$). Crucially, the analysis distinguishes between training-phase and evaluation-phase errors, providing specific bounds for both. Rather than vague decay trends, the results demonstrate that forgetting error scales inversely with sample size $n$, proving that larger sample sizes effectively mitigate errors. Additionally, the results reveal a nuanced relationship with network width: increasing hidden layer size ($m$) reduces forgetting specifically when employing early stopping in the kernel regime; however, if the network is trained to full convergence, this beneficial effect vanishes.

These theoretical characterizations of sample, iteration, and computation complexities were corroborated by numerical experiments. This work significantly bridges the gap between theoretical bounds and practical application in the field of continual learning. By providing mathematical proofs for phenomena previously observed only empirically, the paper offers deep insights into the structural factors that contribute to learning stability.

---

## Key Findings

The study derives precise mathematical bounds on the rate of forgetting for both training and evaluation phases. Notable conclusions include:

*   **Explicit Dependency Identification:** The rate of forgetting is explicitly dependent on four key variables:
    *   Optimization iterations ($T$)
    *   Sample size ($n$)
    *   Number of tasks ($K$)
    *   Size of the hidden layer ($m$)
*   **Parameter Influence:** The analysis reveals specific phenomena regarding how different problem parameters influence the speed and extent of forgetting in neural networks.
*   **Validation of Applicability:** Despite being derived from a specific theoretical setting, the findings are validated by numerical experiments across diverse setups, suggesting broader applicability.

---

## Methodology

The authors utilize a controlled theoretical setup to isolate the mechanics of forgetting:

*   **Architecture:** The analysis focuses on a **one-hidden-layer quadratic neural network**.
*   **Optimizer:** Trained using standard **Gradient Descent**.
*   **Dataset:** A synthetic **'XOR cluster dataset'** superimposed with Gaussian noise.
*   **Task Definition:** Tasks are defined structurally as distinct clusters characterized by orthogonal means.

---

## Technical Details

The paper provides a granular breakdown of the mathematical framework and experimental setup used to derive the bounds.

**Network Architecture**
*   **Structure:** Two-layer neural network with a hidden layer of $m$ neurons.
*   **Activation:** Utilizes a generic activation function $\varphi$.
*   **Mapping:** The network mapping is defined as:
    $$ \Phi(w, x) = \frac{1}{\sqrt{m}} \sum_{i=1}^m a_i \varphi(x^\top w_i) $$

**Training Protocol**
*   **Algorithm:** Full-batch or mini-batch Gradient Descent (GD).
*   **Objective:** Unregularized Empirical Risk Minimization (ERM).
*   **Constraints:** No explicit regularization or experience replay buffers used.
*   **Sequence:** Sequential training across $K$ independent tasks.
    *   Parameters initialized once.
    *   $T$ iterations performed per task without re-initialization.

**Theoretical Framework**
*   **Regime:** Operates in the **kernel (lazy) regime**.
*   **Data Distribution:** Clustered synthetic data (XOR clusters) used to derive closed-form bounds on forgetting.

---

## Results

The primary metric for evaluation was the **rate of forgetting**, quantified for both training-phase and evaluation-phase errors.

*   **Theoretical Bounds:** The study identifies that the rate of forgetting is theoretically bounded and explicitly dependent on $T, n, K,$ and $m$.
*   **Impact of Sample Size:** Key findings demonstrate that **increasing sample size ($n$)** mitigates forgetting errors.
*   **Impact of Network Width ($m$):**
    *   Increasing network width ($m$) reduces forgetting in the kernel regime **when employing early stopping**.
    *   This benefit **vanishes** if the network is trained to full convergence.
*   **Complexities:** The paper characterizes sample, iteration, and computation complexities.
*   **Experimental Validation:** Numerical experiments confirm these findings across diverse setups.

---

## Contributions

This paper makes several distinct contributions to the field of Continual Learning:

1.  **Rigorous Theoretical Examination:** Provides a rigorous theoretical examination of the limitations inherent in continual learning, moving beyond empirical observation to mathematical proof in a tractable setting.
2.  **Bridging Theory and Practice:** Bridges the gap between theoretical bounds and practical application by confirming theoretical predictions with numerical experiments.
3.  **Mechanistic Insight:** Offers deeper insight into the underlying mechanisms of continual learning by isolating how specific architectural and data-related factors contribute to catastrophic forgetting.

---
*References: 40 citations*