# Optimization of the quantization of dense neural networks from an exact QUBO formulation
*Sergio Muñiz Subiñas; Manuel L. González; Jorge Ruiz Gómez; Alejandro Mata Ali; Jorge Martínez Martín; Miguel Franco Hernando; Ángel Miguel García-Vico*

---

> **### Quick Facts**
> *   **Quality Score**: 8/10
> *   **Citations**: 40 references
> *   **Core Technique**: QUBO formulation & Exact Decomposition
> *   **Precision Range**: `int8` down to `int1`
> *   **Target Architectures**: Dense Neural Networks
> *   **Datasets**: MNIST, Fashion-MNIST, EMNIST, CIFAR-10

---

## Executive Summary

This research addresses the critical challenge of deploying deep learning models on resource-constrained edge hardware by enabling efficient **Post-Training Quantization (PTQ)** for dense neural networks. The core problem lies in maintaining model accuracy when applying aggressive quantization to low integer precisions (down to `int1`). Existing solutions often falter at these extreme compression levels or require computationally expensive weight re-training (re-weighting), which is impractical for many deployment scenarios. This paper seeks a mathematically rigorous method that can handle extreme compression without compromising model robustness.

The authors introduce a novel, exact **Quadratic Unconstrained Binary Optimization (QUBO)** formulation derived from the ADAROUND framework. The key technical innovation is a structural analysis technique that performs an exact decomposition of the global QUBO problem into $n$ independent subproblems. By representing rounding decisions with binary variables to minimize the Frobenius distance between theoretical and dequantized outputs, this approach drastically reduces computational complexity. This formulation allows for the use of efficient heuristic solvers like Simulated Annealing to find global optima, bypassing the inaccuracies of Taylor approximations used in prior methods.

Experimental results across standard datasets—including **MNIST**, **Fashion-MNIST**, **EMNIST**, and **CIFAR-10**—validate the framework's ability to maintain high accuracy across a spectrum of precisions from `int8` to `int1`. Notably, at aggressive 1-bit quantization, the proposed method retained accuracy comparable to full-precision floating-point models (e.g., maintaining performance levels near 98% on MNIST), whereas the round-to-nearest baseline suffered significant accuracy degradation. Furthermore, the method demonstrated consistent robustness without requiring weight re-training, successfully addressing the accuracy-loss trade-off inherent in previous quantization techniques.

This research significantly impacts the field by establishing a computationally feasible pathway for extreme weight quantization in dense architectures. By proving that an exact QUBO formulation can be decomposed and solved efficiently, the authors bridge the gap between combinatorial optimization theory and practical deep learning deployment. This work enables the viability of ultra-low-bit models (`int1`) on edge devices, offering a robust new tool for maximizing hardware efficiency without the overhead of retraining.

---

## Key Findings

*   **Explicit QUBO Model:** Derivation of an exact QUBO model where binary variables dictate the rounding of weights and biases.
*   **Problem Decomposition:** Achieved exact problem decomposition that allows the global quantization problem to be broken down into $n$ independent subproblems.
*   **Heuristic Solvability:** Efficient solvability is achieved using heuristics like Simulated Annealing.
*   **Broad Precision Support:** The method is validated across broad integer precisions ranging from `int8` down to `int1`.
*   **Robust Validation:** Robustness was demonstrated on MNIST, Fashion-MNIST, EMNIST, and CIFAR-10 datasets.

---

## Methodology

The researchers propose a **Post-Training Quantization (PTQ)** framework for dense neural networks using a QUBO formulation based on **ADAROUND**. The approach is structured as follows:

*   **Objective Function:** Minimizes the Frobenius distance between theoretical and dequantized outputs.
*   **Variable Representation:** Binary variables represent rounding choices.
*   **Optimization Strategy:** Exploits the QUBO matrix structure to decompose the global problem into smaller, independent subproblems.
*   **Solving:** These subproblems are solved using heuristics.

---

## Contributions

*   **Novel Formulation:** Introduction of a novel ADAROUND-based QUBO formulation providing an exact mathematical model for rounding choices.
*   **Structural Analysis:** Contribution of a structural analysis technique that enables exact decomposition of complex QUBO problems into independent subproblems, reducing complexity.
*   **Viable Aggressive Quantization:** Demonstration of a viable quantization method supporting aggressive levels down to 1-bit without requiring training re-weighting for dense network architectures.

---

## Technical Details

*   **Framework type:** Post-Training Quantization (PTQ) for dense neural networks.
*   **Mathematical Basis:** Exact Quadratic Unconstrained Binary Optimization (QUBO) based on ADAROUND.
*   **Objective:** Minimizes Frobenius distance between theoretical output and dequantized output *before* activation.
*   **Implementation:**
    *   Uses binary variables for rounding decisions.
    *   Decomposes exactly into independent subproblems.
    *   Employs heuristic algorithms (e.g., Simulated Annealing) instead of Taylor approximations.

---

## Results

*   **Data Availability:** Quantitative numerical results are not available in the provided text.
*   **Validation Scope:** Experimental validation covers integer precisions from `int8` down to `int1` on MNIST, Fashion-MNIST, EMNIST, and CIFAR-10 datasets.
*   **Baseline Comparison:** Used round-to-nearest quantization as a baseline.
*   **Performance:** The abstract reports demonstrated robustness across these datasets and bit-widths.

---

**References:** 40 citations