# Reward-Preserving Attacks For Robust Reinforcement Learning

*Lucas Schott; Elies Gherbi; Hatem Hajri; Sylvain Lamprier*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Total Citations** | 40 |
| **Test Environments** | MuJoCo Suite (HalfCheetah, Hopper) |
| **Key Parameter** | $\alpha$ (Alpha) |
| **Performance Gain** | 20â€“30% improvement in robust returns vs. fixed baselines |
| **Nominal Retention** | Often preserves over 90% of nominal performance |

---

## Executive Summary

Current approaches to adversarial training in Robust Reinforcement Learning (RL) typically rely on static perturbation strengths, applying a fixed magnitude of noise regardless of the agent's current state. This paper addresses a fundamental flaw in this methodology: the vulnerability of an RL agent is highly state-dependent. Consequently, fixed-strength attacks are suboptimal; they are often too destructive in sensitive states, destabilizing the learning process, or too weak in robust states, failing to provide an effective signal for robustness. This variability creates a critical challenge, as static budgets cannot simultaneously prevent learning collapse and enforce effective hardening across diverse states.

The authors introduce **"alpha-reward-preserving attacks,"** a novel adversarial framework that dynamically adapts perturbation strength based on the current state. Technically, the method utilizes a gradient-based direction for the perturbation, combining this with a learned, state-dependent magnitude parameter ($\eta$). The approach employs a specialized off-policy critic trained over a diverse range of radii to estimate appropriate perturbation magnitudes on the fly. The formulation explicitly limits performance degradation to a fraction of the gap between the nominal and worst-case return, controlled by the hyperparameter $\alpha$. This creates a feedback loop where the attack is strong enough to enforce robustness but weak enough to preserve a sufficient reward signal for continued learning.

Empirical validation demonstrates that the proposed adaptive framework significantly outperforms baseline methods utilizing fixed or random perturbation radii. Testing was conducted on standard continuous control benchmarks from the **MuJoCo** suite, including environments such as HalfCheetah and Hopper. The results revealed that intermediate values of the hyperparameter $\alpha$ (specifically around $\alpha=0.5$) provided the optimal trade-off. In these scenarios, the proposed method maintained nominal returns comparable to standard, non-robust agents (preserving over 90% of nominal performance in many settings), while simultaneously achieving robust returns that were substantially higherâ€”often improving by margins of 20â€“30% compared to fixed-radius adversarial training baselines.

This work shifts the paradigm in robust RL from static, uniform attacks to adaptive, state-aware formulations. By demonstrating that state-dependent vulnerability is a critical factor, the authors provide a more effective training signal for developing robust deep RL agents. The ability to precisely tune the attack-intensity trade-off via the $\alpha$ parameter offers practitioners a practical mechanism to balance robustness and nominal performance, resolving the typical conflict where improving robustness necessitates a significant drop in standard agent capability. This research suggests that future advancements in adversarial robustness will require moving beyond fixed perturbation budgets toward more sophisticated, adaptive defense mechanisms.

---

## Key Findings

*   **State-dependent vulnerability:** The impact of perturbations varies significantly by state, making fixed-strength attacks either too destructive or too weak.
*   **Superiority over baselines:** The proposed approach outperforms existing methods utilizing fixed or random perturbation radii.
*   **Balanced trade-offs:** Utilizing an intermediate alpha value improves adversarial robustness while preserving nominal performance.
*   **Effective calibration:** The adaptive tuning mechanism successfully calibrates attack strength to ensure specific performance degradation without destabilizing learning.

---

## Methodology

The authors propose **alpha-reward-preserving attacks**, an adversarial framework designed to overcome the limitations of static adversarial training. The core innovation lies in adapting perturbation strength dynamically to limit damage to a fraction of the gap between nominal and worst-case return.

*   **Perturbation Structure:** The attack uses a gradient-based direction combined with a learned state-dependent magnitude, denoted as $\eta$.
*   **Training Mechanism:** The method employs off-policy critic training. A specialized critic is trained over a diverse range of radii to learn appropriate magnitudes for the perturbations.
*   **Adaptation:** Rather than using a fixed budget, the framework adjusts the attack intensity in real-time based on the agent's current state.

---

## Technical Details

*   **Adaptive Perturbation Mechanism**
    Utilizes a feedback loop for real-time intensity modification, moving away from static attacks.

*   **State-dependent Vulnerability Modeling**
    Models vulnerability by evaluating policy/value function sensitivity at specific states. This accounts for the non-uniform impact of perturbations across the state space.

*   **Hyperparameter Control**
    A tunable hyperparameter, alpha ($\alpha$), controls the trade-off between robustness and nominal performance.

*   **Objective Function**
    The objective is **reward-preserving**. It aims to degrade performance by a specific margin while maintaining enough reward signal for continued learning and stability.

---

## Contributions

1.  **Novel Formulation:** Introduction of a novel adaptive adversarial formulation (alpha-reward-preserving attacks) that moves beyond static perturbation strengths.
2.  **Learning Mechanism:** Development of a mechanism to learn state-dependent perturbation magnitude via a distinct critic function.
3.  **Empirical Validation:** Empirical validation demonstrating that adaptive attacks provide more effective training signals for robust deep RL than fixed-strength attackers.

---

## Results

The proposed approach demonstrated clear superiority over baseline methods:

*   **Baseline Comparison:** Outperformed methods using fixed or random perturbation radii.
*   **Optimal Alpha:** Experiments showed that intermediate alpha ($\alpha$) values provided the optimal trade-off, improving adversarial robustness without sacrificing nominal performance.
*   **Stability:** The adaptive mechanism successfully calibrated attack strength to induce target levels of performance degradation without destabilizing the learning process.