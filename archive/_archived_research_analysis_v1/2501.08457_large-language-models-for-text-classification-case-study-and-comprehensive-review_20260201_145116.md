# Large Language Models For Text Classification: Case Study And Comprehensive Review

*Arina Kostina; Marios D. Dikaiakos; Dimosthenis Stefanidis; George Pallis*

---

> ### üìä Quick Facts
>
> | Metric | Details |
> | :--- | :--- |
> | **Datasets** | FakeNewsNet (214 records), Employee Reviews (1,000 records) |
> | **Top Performer** | GPT-4 (F1: 0.86 on Employee Reviews) |
> | **Best Efficiency** | Naive Bayes / SVM (for binary tasks) |
> | **Optimization** | AWQ Quantization (4-bit/8-bit), vLLM, Groq LPU |
> | **Key Metrics** | Weighted F1-Score, Inference Response Time |
> | **Quality Score** | 7/10 |

---

## üìã Executive Summary

This paper addresses the operational uncertainty surrounding the deployment of Large Language Models (LLMs) for text classification, specifically weighing the high computational cost and latency of decoder-only models against the efficiency of traditional machine learning (ML) and deep learning baselines. As organizations seek to integrate generative AI, the critical dilemma is whether the advanced reasoning capabilities of LLMs translate to sufficient predictive performance to justify their resource intensity compared to lightweight alternatives like SVM or RoBERTa.

The key innovation is a rigorous experimental framework that benchmarks a diverse architectural spectrum‚Äîincluding Mistral, Llama3, GPT-4, Gemma, and RoBERTa‚Äîacross two distinct task complexities: a multiclass classification of employee work locations and a binary fake news detection task. The technical setup employs aggressive optimization strategies (AWQ quantization, vLLM, Groq LPU accelerators) to ensure fair hardware comparisons and systematically varies prompt engineering strategies.

Results highlight a distinct performance dichotomy tied to task complexity. In multiclass scenarios, LLMs achieved superior Weighted F1-scores (GPT-4: 0.86), validating the necessity of deep contextual reasoning. However, for binary tasks, traditional ML models offered a superior performance-to-time trade-off, delivering competitive accuracy with millisecond latency. This research establishes vital evidence-based guidelines, defining the "break-even" points at which the resource intensity of LLMs yields practical returns over simpler alternatives.

---

## üîç Key Findings

*   **Task Complexity dictates Model Choice:**
    *   **Multiclass/Complex:** LLMs (specifically Llama3 and GPT-4) outperform state-of-the-art deep learning and machine learning models.
    *   **Binary/Simple:** Traditional Machine Learning (ML) models provide a better performance-to-time trade-off.
*   **Latency vs. Accuracy:** While LLMs offer high accuracy for complex tasks, they incur significantly longer inference times compared to simpler ML models.
*   **Sensitivity to Prompting:** There are significant variations in model responses and performance based on the specific prompting strategies employed (e.g., Chain-of-Thought, Few-shot).
*   **Practical Applicability:** The decision to use an LLM depends heavily on the complexity of the classification task and the acceptable latency thresholds of the application.

---

## üß™ Methodology

The researchers employed a comparative experimental framework involving two distinct classification scenarios to evaluate model performance across varying complexities:

1.  **Multiclass Classification:** Predicting employee working locations based on online job reviews.
2.  **Binary Classification:** Classifying news articles as fake or legitimate.

**Scope of Evaluation:**
*   **Model Spectrum:** The study evaluated a diverse range of language models, including LLMs, deep learning models, and traditional machine learning models varying in size, quantization, and architecture.
*   **Prompting Techniques:** Assessment utilized alternative prompting techniques to gauge their impact on output.
*   **Metrics:** Performance was measured quantitatively using:
    *   **Weighted F1-score** for accuracy.
    *   **Inference response time** for efficiency.

---

## ‚öôÔ∏è Technical Details

### Architecture & Models
| Category | Models |
| :--- | :--- |
| **Decoder-only LLMs** | Mistral, Llama3, GPT-4, Gemma |
| **Encoder-only SOTA** | RoBERTa |
| **Traditional ML** | Naive Bayes, SVM |

### Optimization & Hardware
*   **Open-source Optimization:** 4-bit or 8-bit quantization via **AWQ**.
*   **Throughput:** Utilization of the **vLLM** library.
*   **Latency Reduction:** Employment of **Groq LPU** accelerators.
*   **Hardware Comparison:** Tesla T4 vs. Groq LPU.

### Training Configuration
*   **RoBERTa:** Fine-tuned with Adam optimizer (LR 1e-5, batch 32).
*   **SVM / Naive Bayes:** Used TfidfVectorizer with GridSearchCV (5-fold CV).
*   **Inference Settings:** Temperature set to 0.

### Prompt Engineering
*   Investigated **upfront optimization**.
*   Analyzed the impact of **Chain-of-Thought (CoT)** and **Few-shot** techniques.

---

## üìà Results

Experiments were conducted on **FakeNewsNet** (214 records) and **Employee Reviews** (1,000 records), yielding the following insights:

*   **Complex Multiclass Tasks (Employee Reviews):**
    *   LLMs (Llama3, GPT-4) outperformed RoBERTa and traditional ML.
    *   **Reason:** Superior contextual understanding.
    *   **Metric:** GPT-4 achieved a Weighted F1-Score of **0.86**, compared to RoBERTa (0.77) and SVM (0.73).

*   **Simple Binary Tasks (FakeNewsNet):**
    *   Traditional ML models provided a superior performance-to-time trade-off.
    *   LLM inference remained significantly slower (seconds on Tesla T4 vs ~100ms on Groq LPU).
    *   Traditional ML achieved latency in the millisecond range with competitive accuracy.

*   **Key Metrics Focus:**
    *   Weighted F1-Score (Primary indicator).
    *   Performance-Time Trade-off.
    *   Accuracy.

---

## ‚ú® Contributions

*   **Comprehensive Benchmarking:** Provided a performance benchmarking of diverse LLMs against established SOTA deep learning and machine learning baselines across varying task complexities.
*   **Efficiency Analysis:** Introduced a practical analysis balancing predictive performance (F1-score) against computational cost (inference time).
*   **Prompt Engineering Impact:** Highlighted the critical role of prompt engineering (CoT, Few-shot) in eliciting optimal performance from LLMs.
*   **Decision Framework:** Established task-specific recommendations for when to utilize resource-intensive LLMs versus simpler ML models.

---

**Paper Quality Score:** 7/10  
**References:** 40 citations