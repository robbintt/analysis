# Learning Where, What and How to Transfer: A Multi-Role Reinforcement Learning Approach for Evolutionary Multitasking

*Jiajun Zhan; Zeyuan Ma; Yue-Jiao Gong; Kay Chen Tan*

---

> ### üìä Quick Facts
> ---
> *   **Quality Score:** 9/10
> *   **Total Citations:** 40
> *   **Proposed Framework:** MetaMTO (Multi-Role RL)
> *   **Optimization Paradigm:** Meta-Black-Box Optimization (MetaBBO)
> *   **Performance Status:** State-of-the-Art (SOTA)
> *   **Training Method:** End-to-end pre-training on augmented problem distribution
> ---

## Executive Summary

Evolutionary Multitasking (EMT) aims to solve multiple optimization problems simultaneously by transferring knowledge across tasks. However, a fundamental challenge in this field is the "negative transfer" problem, where inappropriate knowledge transfer degrades performance rather than improving it. Existing methods often rely on manual, heuristic-based designs to determine transfer routing, content, and strategies, which lack adaptability and require extensive domain-specific tuning. This paper addresses the critical need for an automated, generalized framework that can intelligently manage the complex dynamics of knowledge transfer without human intervention.

The authors propose **"MetaMTO,"** a novel multi-role Reinforcement Learning (RL) framework that automates the transfer process by decomposing it into three specialized cooperative agents within a bi-level Meta-Black-Box Optimization (MetaBBO) architecture. The system assigns distinct roles: a Task Routing Agent uses attention mechanisms to determine *where* to transfer based on task similarity; a Knowledge Control Agent regulates *what* to transfer by controlling the selection of elite solutions; and Strategy Adaptation Agents determine *how* to transfer by dynamically adjusting hyper-parameters (such as mutation and crossover strengths). These policy networks are trained end-to-end over an augmented multitask problem distribution, allowing them to learn a unified meta-policy that generalizes to unseen tasks.

In comparative experiments against representative baselines‚Äîincluding L2T, BLKT, RLMFEA, and MFEA‚Äîthe proposed framework achieved state-of-the-art performance across all tested problem scales (VS, S, M, L, VL). For medium-scale problems (M-3), the model achieved an objective value of **0.1713 ¬± 0.0058**, significantly outperforming L2T (0.2862 ¬± 0.0018) and standard MFEA (0.3605 ¬± 0.0051). This performance gap was consistent across other scales; on large-scale problems (L-3), the method scored **0.2121 ¬± 0.0039** compared to L2T‚Äôs 0.3201 ¬± 0.0021. Furthermore, analysis revealed that the learned policies are interpretable, offering clear insights into the decision-making process regarding task routing and resource allocation.

This research significantly advances the field of evolutionary computation by demonstrating that knowledge transfer mechanisms can be fully automated via meta-learning rather than manually engineered. By proving that a meta-policy pre-trained on an augmented distribution can successfully adapt to new multitasking environments, the work reduces the heavy burden of hyperparameter tuning typically associated with EMT algorithms. The multi-agent architecture establishes a new paradigm for designing optimization systems that are not only high-performing but also robust and adaptable to varying problem complexities.

---

## üîë Key Findings

*   **State-of-the-Art Performance:** The proposed multi-role RL framework achieved superior results compared to representative baselines in all validation experiments.
*   **Generalizable Meta-Policy:** The system successfully formulated a unified meta-policy through end-to-end pre-training over an augmented multitask problem distribution, enabling effective adaptation to new, unseen tasks.
*   **Interpretability:** Analysis revealed that the learned policies possess high interpretability, offering clear insights into how the system navigates the transfer process (determining where, what, and how to transfer).
*   **Robustness:** The framework demonstrated robust in-distribution generalization, maintaining superior performance as problem complexity increased (from VS to VL scales).

---

## üõ† Methodology

The core of the proposed solution is the **Multi-Role Reinforcement Learning framework**, designed to automate knowledge transfer in Evolutionary Multitasking (EMT). The methodology decomposes the complex transfer process into three specialized, cooperative agents:

1.  **Task Routing Agent:** Determines **where** to transfer by evaluating attention-based similarity between tasks.
2.  **Knowledge Control Agent:** Determines **what** to transfer by regulating the selection of elite solutions for transfer.
3.  **Strategy Adaptation Agents:** Determine **how** to transfer by controlling the transfer strength via dynamic hyper-parameter adjustment.

These policy networks are trained **end-to-end** over an augmented multitask problem distribution. This allows the system to derive a unified meta-policy that captures the dynamics of knowledge transfer without manual algorithm design.

---

## üìù Contributions

*   **Conceptual Framework:**
    *   Identified the three core challenges of knowledge transfer in EMT: **Where** (routing), **What** (content), and **How** (strategy).
    *   Formulated a systematic approach to address these challenges simultaneously.
*   **Novel Architecture:**
    *   Introduced a specialized multi-agent RL architecture that utilizes attention mechanisms and dynamic hyper-parameter control.
    *   Replaces traditional manual, tailored designs with an automated learning-based system.
*   **Generalizability:**
    *   Demonstrated that by pre-training on an augmented distribution of problems, the method can generalize across various multitask optimization scenarios.
    *   Significantly reduces the need for manual tuning when adapting to new problems.

---

## ‚öô Technical Details

**System Architecture:** Bi-level Meta-Black-Box Optimization (MetaBBO)
*   **Low-level:** Evolutionary Multitasking optimization.
*   **Meta-level:** Multi-role RL system for knowledge transfer control.

**Training Strategy:** End-to-end pre-training over an augmented multitask problem distribution.

**Multi-Agent Composition:**
1.  **Task Routing Agent**
    *   **Role:** Determines source-target task pairs.
    *   **Mechanism:** Attention-based similarity calculation.
2.  **Knowledge Control Agent**
    *   **Role:** Determines quantity of knowledge to transfer.
    *   **Action Range:** Continuous range \([0, 0.5]\).
3.  **Transfer Strategy Adaptation Agent Group**
    *   **Role:** Controls execution parameters.
    *   **Actions:**
        *   Mutation Operator Selection: Discrete set \(\{1, 2, 3, 4\}\).
        *   Mutation Strength: Continuous range \([0, 1]\).
        *   Crossover Strength: Continuous range \([0, 1]\).

**MDP Formulation:**
*   **State Space (5-Dimensional):** Diversity, Convergence, Stagnation, New Best Check, Survival Rate.
*   **Action Space:** Combined discrete and continuous actions (as defined by the agent groups above).

---

## üìà Results

MetaMTO achieved state-of-the-art performance with the best average objective values across all problem scales (VS, S, M, L, VL) compared to baselines L2T, BLKT, RLMFEA, and MFEA.

**Comparative Performance:**

| Problem Scale | Method | Objective Value (Mean ¬± Std) | Comparison vs. MetaMTO |
| :--- | :--- | :--- | :--- |
| **VS-1** | **MetaMTO (Ours)** | **0.1897 ¬± 0.0504** | *Best Performer* |
| | L2T | 0.2983 ¬± 0.0019 | +57% (Normalized) |
| | MFEA | 0.3688 ¬± 0.0114 | +94% (Normalized) |
| | | | |
| **M-3** | **MetaMTO (Ours)** | **0.1713 ¬± 0.0058** | *Best Performer* |
| | L2T | 0.2862 ¬± 0.0018 | +67% (Normalized) |
| | MFEA | 0.3605 ¬± 0.0051 | +110% (Normalized) |
| | | | |
| **L-3** | **MetaMTO (Ours)** | **0.2121 ¬± 0.0039** | *Best Performer* |
| | L2T | 0.3201 ¬± 0.0021 | +51% (Normalized) |
| | MFEA | 0.3997 ¬± 0.0029 | +88% (Normalized) |

**Key Observations:**
*   Demonstrated robust in-distribution generalization.
*   Maintained superior performance as problem complexity increased.
*   Showed significant performance gaps against previous RL-assisted EMT works.

---
**Report generated based on 40 references.**