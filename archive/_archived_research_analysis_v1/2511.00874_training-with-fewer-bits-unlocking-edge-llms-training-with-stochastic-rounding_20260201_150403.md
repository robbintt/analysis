# Training with Fewer Bits: Unlocking Edge LLMs Training with Stochastic Rounding
*Taowen Liu; Marta Andronic; Deniz GÃ¼ndÃ¼z; George A. Constantinides*

> ## ðŸ“‹ Quick Facts
> *   **Quality Score:** 8/10
> *   **References:** 33 Citations
> *   **Primary Domain:** Edge AI / Model Quantization
> *   **Core Mechanism:** Stochastic Rounding (SR) in SGD
> *   **Key Heuristic:** The "4x Rule" (Batch vs. Precision)

---

## Executive Summary

Training Large Language Models (LLMs) directly on edge devices is severely constrained by limited memory and computational power, making aggressive numerical reduction (quantization) essential. However, reducing precision during the back-propagation phase introduces significant noise, causing standard low-precision methods like Round-to-Nearest (RTN) to fail. RTN creates systematic bias that leads to unstable convergence or substantial accuracy degradation. This presents a fundamental barrier to enabling efficient, on-device training for resource-constrained edge applications that require privacy-preserving, continuous learning capabilities.

The authors propose a mixed-precision Stochastic Gradient Descent (SGD) framework that utilizes **Stochastic Rounding (SR)** instead of RTN. The approach identifies five specific quantization points per layer: forward activations, forward weights, backward activations, backward weights, and backward gradients. These operations are defined via a Threshold Quantization Function, which maps continuous values to discrete levels based on specific thresholds. SR acts as an unbiased estimator by probabilistically sampling the rounding threshold, converting quantization noise into random variance rather than bias.

The core theoretical insight distinguishes between shared weights and transient activations: the variance induced by SR in per-sample activations and gradients can be effectively averaged out by increasing the mini-batch size, a phenomenon not applicable to weight quantization noise.

Empirical validation confirms the theoretical **"4x Rule,"** demonstrating that reducing precision by 1-bit for activations and gradients can be compensated by increasing the mini-batch size by a maximum factor of four to maintain convergence rates. While the ultimate goal is Edge LLM training, the study utilized WideResNet-16 on CIFAR-10 as a testbed. Under aggressive quantization, traditional RTN methods failed to converge entirely, whereas the SR approach maintained training stability and achieved superior accuracy.

This research provides a mathematically grounded strategy to decouple precision requirements from convergence stability, offering system architects a clear trade-off between memory bandwidth (batch size) and computational precision.

---

## Key Findings

*   **Batch Size Compensation:** Increased batch sizes can effectively compensate for reduced precision in the back-propagation phase.
*   **Variance Distinction:** Quantizing weights and activations impacts gradient variance differently; weight noise is shared, while activation noise is per-sample.
*   **Unbiased Estimation:** Stochastic Rounding provides unbiased gradient estimates that successfully mitigate quantization noise compared to biased methods.
*   **Validation of Theory:** Empirical experiments confirm the theoretical relationships linking mini-batch SGD, numerical precision, and batch size scaling.

## Methodology

The research team employed a combination of theoretical analysis and empirical validation:

*   **Formal Theoretical Study:** Conducted a rigorous mathematical study of mini-batch SGD integrated with Stochastic Rounding.
*   **Trade-off Investigation:** Analyzed the interactions between quantization precision, gradient variance, and critical training hyperparameters.
*   **Experimental Design:** Designed controlled experiments to empirically validate convergence rates and accuracy maintenance under aggressively quantized conditions.

## Contributions

*   **Theoretical Framework:** Provided a comprehensive theoretical framework detailing the interaction between Stochastic Rounding and mini-batch SGD.
*   **Strategy Identification:** Identified increasing the batch size as a viable strategy to offset convergence issues caused by reduced precision in edge LLMs.
*   **Advanced Noise Understanding:** Advanced the field's understanding of quantization noise by clearly distinguishing the distinct effects of weight quantization versus activation quantization.

## Technical Details

The proposed approach introduces a mixed-precision SGD framework optimized for linear and convolutional layers.

*   **Quantization Points:** The framework identifies five distinct quantization points per layer:
    1.  Forward activations
    2.  Forward weights
    3.  Backward activations
    4.  Backward weights
    5.  Backward gradients
*   **Threshold Quantization Function:** Operations are defined by a function mapping continuous values to discrete levels.
*   **Stochastic Rounding (SR):** Used as an unbiased estimator by sampling the rounding threshold probabilistically.
    *   *Contrast:* Differs from Round-to-Nearest (RTN), which introduces systematic bias.
*   **Variance Averaging Principle:** The theory posits that variance from SR in per-sample tensors (activations/gradients) can be averaged out by increasing the mini-batch size. This principle does not apply to shared weight quantization noise.

## Results

The experimental outcomes provided strong validation for the proposed methods:

*   **The "4x Rule":** Results demonstrated that a 1-bit reduction in precision for activations and gradients can be compensated by increasing the batch size by a maximum of **4x** to maintain convergence. In practice, the required increase is often milder.
*   **Convergence Stability:** Tests on **WideResNet-16** with **CIFAR-10** revealed:
    *   **RTN:** Failed to converge entirely under aggressive quantization.
    *   **SR:** Maintained training stability and achieved higher accuracy.
*   **Edge Viability:** The study effectively unlocked aggressive quantization strategies for edge device training, proving that full training cycles are feasible on hardware with limited floating-point capabilities.