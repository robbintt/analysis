---
title: 'MinMo: A Multimodal Large Language Model for Seamless Voice Interaction'
arxiv_id: '2501.06282'
source_url: https://arxiv.org/abs/2501.06282
generated_at: '2026-02-04T15:59:20'
quality_score: 8
citation_count: 31
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# MinMo: A Multimodal Large Language Model for Seamless Voice Interaction

*Qian Chen; Yafeng Chen; Yanni Chen; Mengzhe Chen; Yingda Chen; Chong Deng; Zhihao Du; Ruize Gao; Changfeng Gao; Zhifu Gao; Yabin Li; Xiang Lv; Jiaqing Liu; Haoneng Luo; Bin Ma; Chongjia Ni; Xian Shi; Jialong Tang; Hui Wang; Hao Wang; Wen Wang; Yuxuan Wang; Yunlan Xu; Fan Yu; Zhijie Yan; Yexin Yang; Baosong Yang; Xian Yang; Guanrou Yang; Tianyu Zhao; Qinglin Zhang; Shiliang Zhang; Nan Zhao; Pei Zhang; Chong Zhang; Jinren Zhou*

---

> ### üìä Quick Facts
>
> *   **Model Parameters:** ~8 Billion
> *   **Base Architecture:** Qwen2.5-7B-instruct
> *   **Training Dataset:** ~1.4 Million hours of diverse speech data
> *   **End-to-End Latency:** ~600ms (on L20 GPU)
> *   **Speech-to-Text Latency:** ~100ms
> *   **Full-Duplex Latency:** ~800ms
> *   **ASR Performance:** 1% WER (averaged over 10 languages)

---

## Executive Summary

This research addresses the critical challenge of creating a multimodal Large Language Model (LLM) that seamlessly integrates high-fidelity voice generation and comprehension with the reasoning capabilities of text-based LLMs. Existing solutions often suffer from disjointed modality processing‚Äîtreating speech and text as separate pipelines‚Äîwhich results in high latency, an inability to handle real-time interruptions (full-duplex interaction), and a lack of granular control over vocal characteristics such as emotion or dialect. Bridging this gap is essential for developing voice assistants that can engage in natural, human-like conversation rather than turn-based, robotic interactions.

MinMo introduces an 8 billion parameter "aligned" model built upon the Qwen2.5-7B-instruct base, utilizing a lightweight alignment strategy to unify text and speech modalities. The core technical innovation lies in a novel four-stage training pipeline (Speech-to-text, Text-to-speech, Speech-to-speech, and Duplex interaction) trained on a massive dataset of 1.4 million hours. The architecture employs a SenseVoice-large encoder for input and a CosyVoice 2 flow-matching synthesizer for output. Crucially, MinMo incorporates a specialized Full-Duplex Predictor (a 1-layer Transformer) that manages interaction flow, enabling the model to listen and speak simultaneously while handling real-time interruptions.

MinMo achieves state-of-the-art performance, surpassing current models like Moshi and GLM-4-Voice across key benchmarks. It achieves an average Word Error Rate (WER) of 1% over 10 languages and demonstrates high accuracy in Spoken QA and Speech Emotion Recognition (SER). In terms of efficiency, the model delivers an end-to-end latency of approximately 600ms on an L20 GPU. Furthermore, the model successfully executes complex instructions for voice generation, allowing for precise control over emotions, speaking rates, and voice mimicking.

This research significantly advances the field of voice AI by demonstrating that a single unified model can retain the reasoning power of LLMs while delivering professional-grade, low-latency voice interaction. The authors' commitment to releasing the code and models ensures that this work will serve as a robust foundation for the open-source community.

---

## Key Findings

*   üèÜ **State-of-the-Art Performance:** MinMo achieves superior performance in voice comprehension and generation while successfully retaining the capabilities of the base text LLM.
*   ‚ö° **Real-Time Full-Duplex Communication:** The model facilitates simultaneous listening and speaking with low latency, achieving approximately **100ms** for Speech-to-Text (STT) and **800ms** for full-duplex interaction.
*   üéõÔ∏è **Advanced Instruction Following:** MinMo supports granular control over speech generation characteristics, including emotions, dialects, speaking rate, and voice mimicking via user instructions.
*   üéôÔ∏è **Novel Voice Decoder:** The proposed voice decoder outperforms prior models, enhancing the quality of generated speech.

---

## Technical Specifications

MinMo utilizes a lightweight alignment strategy built on top of a strong text foundation.

### Architecture Overview
*   **Base Model:** Qwen2.5-7B-instruct
*   **Total Parameters:** ~8 Billion
*   **Model Type:** Aligned Multimodal LLM

### Input Pipeline
*   **Encoder:** SenseVoice-large
*   **Projection:** 2-layer Transformer combined with a CNN layer

### Output Pipeline
*   **Projector:** Single-layer linear projector
*   **Speech Token Generation:** CosyVoice 2 LM (Autoregressive)
    *   Uses a 5:15 text-to-speech token ratio
*   **Synthesizer:** CosyVoice 2 flow-matching synthesizer with vocoder

### Duplex Interaction
*   **Full-Duplex Predictor:** 1-layer Transformer designed to manage real-time interruptions and interaction flow.

---

## Methodology

The MinMo model employs a structured, four-stage alignment training pipeline to bridge the gap between text and speech modalities.

1.  **Stage 1: Speech-to-Text (S2T)** - Aligning speech input with text understanding.
2.  **Stage 2: Text-to-Speech (TTS)** - Aligning text output with speech generation.
3.  **Stage 3: Speech-to-Speech (S2S)** - Direct voice-to-voice interaction capabilities.
4.  **Stage 4: Duplex Interaction** - Training for simultaneous listening and speaking, including handling interruptions.

**Training Data:**
The model was trained on a comprehensive dataset totaling over **1.38 million hours**, distributed as follows:
*   ~1.2 million hours (Speech-to-Text)
*   ~171k hours (Text-to-Speech)
*   ~10.1k hours (Speech-to-Speech)
*   ~4k hours (Full Duplex interaction)

---

## Evaluation Results

MinMo demonstrates significant improvements over competitors such as Moshi and GLM-4-Voice.

### Benchmark Metrics
*   **ASR (Automatic Speech Recognition):** 1-WER% averaged over 10 languages.
*   **S2TT (Speech-to-Text Translation):** Measured via BLEU score.
*   **Spoken QA:** High Accuracy.
*   **SER (Speech Emotion Recognition):** Weighted Accuracy.

### Performance Comparison
*   MinMo claims **SOTA performance** across various tasks, specifically noting superiority in voice naturalness and interaction latency compared to existing models.

---

## Research Contributions

*   **Modality Integration:** Bridges the gap between text and speech in aligned models by training on a massive, diverse dataset of 1.4M hours.
*   **Unified Framework:** Provides a single interaction framework that combines high-level text understanding with advanced voice generation and full-duplex capabilities.
*   **Generative Control:** Introduces enhanced control over speech characteristics (emotion, style, rate) via natural language instructions.
*   **Open Source:** Contributes to the open community with the promise of releasing code and model weights for further research.

---

**Quality Score:** 8/10
**References:** 31 citations