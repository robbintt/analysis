# A Survey of Continual Reinforcement Learning

*Chaofan Pan; Xin Yang; Yanhua Li; Wei Wei; Tianrui Li; Bo An; Jiye Liang*

---

## üîç Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 10/10 |
| **Total Citations** | 40 |
| **Core Focus** | Continual Reinforcement Learning (CRL) |
| **Key Domains** | Atari 2600, Robotics, Strategy Games |
| **Primary Output** | Novel Taxonomy & Evaluation Standardization |

---

## üìù Executive Summary

> **Core Problem:** Standard Reinforcement Learning (RL) faces critical limitations in dynamic, real-world environments due to its reliance on static training distributions, extensive data requirements, and high computational costs for retraining. The rigid "training-deploy-retrain" cycle causes agents to fail at generalization and suffer from **catastrophic forgetting**‚Äîdegrading performance on previous tasks while learning new ones.
>
> **The Solution:** This paper establishes **Continual Reinforcement Learning (CRL)** as a vital paradigm enabling agents to learn incrementally, adapt to new tasks, and retain knowledge over time without revisiting past data. This capability is essential for long-term autonomy and non-stationary environments.
>
> **Key Innovation:** The authors introduce a **novel taxonomy** categorizing CRL into four distinct scenarios based on knowledge storage and transfer:
> 1.  **Lifelong Adaptation**
> 2.  **Non-Stationarity Learning**
> 3.  **Task Incremental Learning**
> 4.  **Task-Agnostic Learning**
>
> **Technical Landscape:** The survey classifies architectural approaches into **Regularization-Based Methods** (e.g., EWC, Online-EWC) that constrain parameters using Fisher information matrices, and **Experience-Focused Methods** that utilize Complementary Learning Systems with dual memory buffers.
>
> **Impact:** While the field is diverse, it remains fragmented regarding evaluation standards compared to Continual Supervised Learning. This research provides a unified theoretical framework, standardizes evaluation metrics, and offers a roadmap for addressing unique challenges found at the intersection of RL and Continual Learning.

---

## üîë Key Findings

*   **Limitations of Standard RL:** Current approaches are restricted by reliance on extensive training data, high computational resources, and limited generalization in dynamic environments.
*   **Role of CRL:** Continual Reinforcement Learning is identified as a critical solution enabling agents to learn continuously, adapt to new tasks, and retain knowledge.
*   **New Taxonomy:** A novel taxonomy categorizes CRL methods into four distinct types based on mechanisms of knowledge storage and transfer.
*   **Research Landscape:** The field is diverse but fragmented regarding metrics, benchmarks, and scenario settings.
*   **Future Challenges:** The intersection of RL and Continual Learning presents unique challenges beyond those found in either field individually.

---

## üõ†Ô∏è Methodology

The authors employed a comprehensive survey methodology consisting of four key phases:

1.  **Systematic Literature Review:** To assess the current state of the art.
2.  **Structural Analysis:** Organizing works based on criteria such as metrics, tasks, and benchmarks.
3.  **Taxonomic Construction:** Developing a new classification framework based on knowledge storage and transfer.
4.  **Critical Evaluation:** Analyzing constraints and deriving future research directions.

---

## üì¶ Contributions

*   **Comprehensive Examination of CRL:** Provided a holistic overview of the domain, detailing core concepts, methodologies, and challenges.
*   **Standardization of Evaluation Metrics:** Offered a structured analysis of success measures to facilitate better comparison.
*   **Novel Taxonomy:** Proposed a theoretical taxonomy classifying CRL methodologies into four categories based on knowledge handling.
*   **Future Research Roadmap:** Highlighted unique difficulties and provided practical insights to guide future exploration in the field.

---

## ‚öôÔ∏è Technical Details

### Proposed CRL Scenarios (Taxonomy)
The paper proposes a taxonomy of CRL scenarios based on knowledge handling:
*   **Lifelong Adaptation:** Evaluated on new tasks only.
*   **Non-Stationarity Learning:** Shared logic with different rewards/transitions.
*   **Task Incremental Learning:** Distinct tasks requiring different state/action spaces.
*   **Task-Agnostic Learning:** No task labels provided.

### Architectural Approaches
*   **Regularization-Based Methods**
    *   Utilize Fisher information matrices to constrain parameters.
    *   **Examples:** Standard EWC, Online-EWC (P&C).
*   **Experience-Focused Methods**
    *   Employ a Complementary Learning System.
    *   Utilize **Short-Term** and **Long-Term** experience buffers for replay.

### Key Benchmarks
Specific benchmark architectures identified include:
*   CRL Maze
*   Lifelong Hanabi
*   Continual World
*   L2Explorer

---

## üìâ Results & Evaluation

### Evaluation Metrics
*   **Scalability:** Emphasized via combined game sequences in Atari 2600.
*   **Complexity:** High computational costs and complex state spaces found in Strategy Games and robotics.
*   **Development Pace:** Noted a 'slow development' of standardized benchmarks compared to Continual Supervised Learning.
*   **Comparison Metrics:** Task count, sequence length, and observation types.

### Historical Timeline
The survey tracks the evolutionary progress of the field:
*   **2015‚Äì2016:** Early agents (CHILD, PathNet).
*   **2017‚Äì2019:** Core methods established (EWC, P&C).
*   **2020‚Äì2022:** Benchmark surge.
*   **2023‚Äì2025:** Recent specialized methods (TRAC, CoMPS).

### Key Technical Challenges
*   **Catastrophic Forgetting:** Preventing degradation on previous tasks.
*   **Plasticity Loss:** Maintaining the ability to learn new tasks.
*   **Protocol Variance:** Evaluations vary significantly by scenario.