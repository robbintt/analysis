# Sharpness-Aware Data Generation for Zero-shot Quantization

*Dung Hoang-Anh; Cuong Pham Trung Le; Jianfei Cai; Thanh-Toan Do*

> ### üìä Quick Facts
> *   **Quality Score:** 9/10
> *   **Focus:** Zero-Shot Quantization (ZSQ)
> *   **Key Innovation:** Sharpness minimization via neighbor-based gradient matching
> *   **Primary Datasets:** CIFAR-100, ImageNet
> *   **Top Performance:** Up to +1.08% improvement in 2/2 bit quantization on ImageNet

---

## üìù Executive Summary

Zero-shot quantization (ZSQ) is a critical technique for deploying deep learning models on resource-constrained hardware without access to the original training data. While current state-of-the-art methods generate synthetic data to calibrate quantized networks, they primarily focus on minimizing reconstruction loss. This approach often ignores the geometry of the loss landscape, specifically the "sharpness" of the minima. Consequently, models calibrated on synthetic data may achieve low training error but suffer from poor generalization on real-world data. This paper addresses this gap by identifying that ignoring sharpness during data generation limits the effectiveness of ZSQ, particularly in challenging low-bit quantization scenarios where the model is more sensitive to approximation errors.

The authors propose **Sharpness-Aware Data Generation (SADAG)**, a novel framework that integrates model sharpness directly into the synthetic data generation process. The core innovation rests on a two-phase theoretical and practical advancement. First, the authors establish an "Equivalence Principle," mathematically proving that minimizing the sharpness of a quantized model is equivalent to maximizing the alignment of reconstruction loss gradients between synthetic data and real validation data. Second, since real validation data is unavailable in zero-shot settings, SADAG implements a validation-free optimization strategy. It approximates gradient matching by comparing the gradients of a generated sample against the gradients of its neighboring generated samples. This neighbor-based approach effectively simulates the regularization provided by a validation set, guiding the generator toward producing data that yields flatter (less sharp) minima and better generalization.

SADAG establishes new state-of-the-art benchmarks across standard datasets, demonstrating significant improvements in low-bit quantization settings. On ImageNet, the method achieved up to a **+1.08%** improvement in the highly aggressive 2-bit weight and 2-bit activation (2/2) setting for MobileNetV2 (reaching 13.01% compared to Genie's 11.93%). In the more common 2-bit weight and 4-bit activation (2/4) setting, SADAG achieved Top-1 accuracies of **65.25%** for ResNet-18, **70.52%** for ResNet-50, and **51.89%** for MobileNetV2. Consistent performance was also observed on CIFAR-100, where ResNet-20 achieved **65.94%** in the 4/4 setting, outperforming the previous SOTA (Genie) at 65.25% and significantly surpassing AdaSG at 46.13%.

This work significantly influences the field of model compression by shifting the focus of Zero-Shot Quantization from simple data reconstruction to loss landscape geometry. By introducing the Gradient Matching Theorem and a practical method for validation-free sharpness minimization, the authors provide a robust theoretical foundation for future research in synthetic data calibration.

---

## üîë Key Findings

*   **Sharpness as a Criterion:** Incorporating the sharpness of the quantized model as a criterion for generating synthetic data significantly enhances the generalization ability of zero-shot quantization.
*   **Theoretical Equivalence:** Sharpness minimization is theoretically equivalent to maximizing the gradient matching between reconstruction loss gradients computed on synthetic data and real validation data.
*   **Validation-Free Operation:** The requirement for a real validation set can be effectively bypassed by approximating gradient matching using the relationship between each generated sample and its neighbors.
*   **Superior Performance:** The proposed method outperforms current state-of-the-art techniques on CIFAR-100 and ImageNet datasets, particularly in low-bit quantization settings.

---

## ‚öôÔ∏è Methodology

The proposed methodology introduces a **Sharpness-Aware Data Generation** framework for Zero-Shot Quantization (ZSQ). The approach operates in two main phases:

1.  **Theoretical Formulation:**
    It establishes that minimizing the sharpness of the quantized model can be achieved by maximizing the alignment (matching) of reconstruction loss gradients between synthetic data and real validation data.

2.  **Practical Approximation:**
    Since a real validation set is unavailable in ZSQ, the method circumvents this by approximating the gradient matching. Specifically, it optimizes the synthetic data by matching the gradients of each generated sample against the gradients of its neighbors.

---

## üîß Technical Details

*   **Core Mechanism:** SADAG incorporates the sharpness of the quantized model as a primary criterion for generating synthetic data to enhance generalization.
*   **Equivalence Principle:** Based on the principle that sharpness minimization is equivalent to maximizing gradient matching between reconstruction loss gradients on synthetic and real validation data.
*   **Optimization Strategy:** Allows for validation-free optimization by analyzing neighbor relationships among generated samples.
*   **Generation Process:**
    *   Includes a warm-up stage with rapid BatchNorm loss convergence.
    *   Produces final images that are smoother with less color variability.
*   **Reproducibility Note:** A reproducibility gap was noted in the MobileNetV2 2/4 setting for the Genie baseline (reported: 53.38 vs. reproduced: 51.47).

---

## ‚ú® Contributions

*   **Novel Criterion:** This is the first work to integrate quantized model sharpness into the data generation process for zero-shot quantization, addressing a gap in previous literature that ignored generalization capabilities linked to sharpness.
*   **Gradient Matching Theorem:** The authors demonstrate the mathematical relationship between sharpness minimization and gradient matching under certain assumptions, providing a theoretical foundation for the method.
*   **Validation-Free Optimization:** The paper contributes a practical solution to perform sharpness-aware optimization without access to real validation data, using neighbor-based gradient approximation instead.
*   **State-of-the-Art Performance:** The work establishes new performance benchmarks for low-bit quantization on major datasets (CIFAR-100 and ImageNet), validating the efficacy of sharpness-aware generation.

---

## üìà Results

SADAG consistently outperforms State-of-the-Art (SOTA) methods, especially in low-bit (2-bit) quantization.

**ImageNet (Top-1 Accuracy)**
*   **2/2 Setting:** Achieved improvements up to **+1.08%**.
    *   *MobileNetV2:* 13.01% (SADAG) vs 11.93% (Genie).
*   **2/4 Setting:**
    *   *ResNet-18:* 65.25%
    *   *ResNet-50:* 70.52%
    *   *MobileNetV2:* 51.89%
*   **3/3 and 4/4 Settings:** Achieved SOTA results across ResNet-18, ResNet-50, and MobileNetV2.

**CIFAR-100**
*   **ResNet-20 (4/4 Setting):** Achieved **65.94%**, outperforming Genie (65.25%) and AdaSG (46.13%).

---

**Quality Score:** 9/10 | **References:** 9 citations