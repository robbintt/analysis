# Causality can systematically address the monsters under the bench(marks)

*Felix Leeb; Zhijing Jin; Bernhard SchÃ¶lkopf*

---

> ## ðŸ“‹ Quick Facts
>
> *   **Quality Score:** 9/10
> *   **References:** 40 Citations
> *   **Core Topic:** Causal Inference & LLM Benchmarking
> *   **Key Concept:** Common Abstract Topologies (CATs)
> *   **Methodology:** Instrumentalism & Structural Causal Models

---

## Executive Summary

Current machine learning benchmarks are fundamentally undermined by systematic "monsters"â€”flaws such as biases, artifacts, and data leakage that compromise the integrity of model evaluation. Because standard metrics rely on implicit assumptions, they frequently obscure these structural defects, allowing models to achieve high accuracy scores by relying on spurious correlations rather than robust reasoning. This results in unreliable model behavior and inferences that are not grounded in genuine understanding, creating a critical need for a rigorous framework that can expose these hidden fragilities and ensure assessments reflect true capability.

The authors introduce a causal inference framework designed to make modeling assumptions explicit through a philosophy of **"Instrumentalism,"** where plausible assumptions are sufficient for testing. The core technical contribution is a taxonomy of **"Common Abstract Topologies (CATs),"** which categorizes causal relationships into three structures: Confounding, Mediation, and Spurious Correlations. Specifically, for Mediation structures, the framework utilizes path-specific metricsâ€”including the Natural Direct Effect (NDE, isolating the effect of the stimulus independent of the reasoning path) and Natural Indirect Effect (NIE, capturing the effect mediated through the reasoning process), alongside the Controlled Direct Effect (CDE) and Total Effect (TCE)â€”to disentangle whether a model's output is driven by correct logic or superficial shortcuts.

The framework is validated through a series of case studies, including a detailed analysis of a **"Mobster"** variant of the GSM8k benchmark (GSM8k with unethical context). While a standard Accuracy metric passed the model based on the final answer ("Yes"), the causal evaluation provided a granular breakdown of specific failures. It identified distinct reasoning errors: an Arithmetic Error (incorrectly calculating time as 200m instead of the correct 125m), a failure in Moral Reasoning (inability to identify the scenario as extortion), and flawed Consequence Modeling (condoning criminal behavior). These results demonstrate that while aggregate metrics may suggest success, causal analysis can isolate and quantify specific logical breakdowns.

By establishing causality as a systematic tool for addressing methodological weaknesses, this paper provides a precise language for articulating the strengths and limitations of ML benchmarks. The "CATs" taxonomy offers researchers a practical method to analyze large language model (LLM) reasoning with structural integrity and explanatory power. Ultimately, this work fosters greater trust in empirical machine learning and is expected to drive systematic progress by shifting the field away from opaque evaluation scores toward rigorous, causally-grounded diagnostics.

---

## Key Findings

*   **Benchmark fragility:** Current machine learning benchmarks are significantly undermined by systematic issues, including biases, artifacts, and data leakage, leading to unreliable model behavior and unsupported inferences.
*   **Causal solutions:** Causal frameworks provide a robust solution for evaluation "monsters" by requiring explicit assumptions, enabling faithful modeling and testable hypotheses with explanatory power.
*   **CATs utility:** The identification of **"Common Abstract Topologies (CATs)"** within causal graphs serves as a practical tool to gain insights into the reasoning capabilities of large language models (LLMs).
*   **Systematic trust:** The application of causal language allows for precise clarification of methodological strengths and limitations, fostering trust and systematic progress in empirical ML.

---

## Methodology

The authors utilize a conceptual and theoretical approach rooted in **causal inference**. The methodology is structured around the following principles:

1.  **Structural Framework:** Establishing a framework where causal assumptions are made explicit to model phenomena faithfully.
2.  **CATs Design:** Designing causal models using identified "Common Abstract Topologies (CATs)" to make the process more accessible.
3.  **Hypothesis Formulation:** Leveraging principled causal tools to formulate testable hypotheses.
4.  **Case Studies:** Conducting a series of case studies to demonstrate how this causal perspective analyzes and clarifies evaluation methods.

---

## Technical Details

The paper proposes a causal inference framework using **Common Abstract Topologies (CATs)** to diagnose LLM benchmark failures.

### Core Topologies
| Topology | Description |
| :--- | :--- |
| **Confounding** | Addresses known common causes ($Z$) that require control. |
| **Mediation** | Involves multiple paths ($X \to Y$ and $X \to M \to Y$) utilizing specific metrics (NDE, NIE, CDE, TCE). |
| **Spurious Correlations** | Addresses unknown common causes or selection bias. |

### Variables & Metrics
*   **Variables:** 
    *   $X$: Stimulus
    *   $Y$: Outcome
    *   $Z, M$: Confounders / Mediators
*   **Mediation Metrics:**
    *   **NDE (Natural Direct Effect):** Isolates the effect of the stimulus independent of the reasoning path.
    *   **NIE (Natural Indirect Effect):** Captures the effect mediated through the reasoning process.
    *   **CDE (Controlled Direct Effect):** Effect under controlled intervention.
    *   **TCE (Total Effect):** The combined effect of all paths.

### Philosophical Approach
The system operates on an **'Instrumentalism'** philosophy, positing that plausible assumptions suffice for testing.

---

## Results

The paper presents a qualitative **"Mobster" case study** (a GSM8k variant containing unethical context).

**Standard Evaluation (Accuracy Metric)**
*   **Result:** Passed
*   **Observation:** The model provided the final answer "Yes," satisfying standard accuracy checks.

**Causal Evaluation**
*   **Result:** Failed (revealed three distinct breakdowns)
    1.  **Arithmetic Errors:** Incorrect calculation of time (claimed 200m vs. correct 125m).
    2.  **Moral Reasoning:** Failed to identify the scenario as extortion.
    3.  **Consequence Modeling:** Condoning criminal behavior.

**Conclusion**
Standard metrics fail to capture the reasoning process. Causal analysis revealed that a correct final answer canmask significant failures in logic and morality.

---

## Contributions

*   **Theoretical Framing:** Positions causality as the ideal systematic framework for addressing the pervasive issues of bias, artifacts, and leakage in ML benchmarks.
*   **Taxonomy Introduction:** Identifies and defines "Common Abstract Topologies (CATs)" in causal graphs to facilitate the design of causal models and the analysis of LLM reasoning.
*   **Practical Demonstration:** Provides case studies that illustrate how the precise language of causality can reveal the limitations of current methods and inspire new approaches for reliable evaluation.