# VisualSimpleQA: A Benchmark for Decoupled Evaluation of Large Vision-Language Models in Fact-Seeking Question Answering

*Yanling Wang; Yihan Zhao; Xiaodong Chen; Shasha Guo; Lixin Liu; Haoyang Li; Yong Xiao; Jing Zhang; Qi Li; Ke Xu*

***

> ### ðŸ“Š Quick Facts Sidebar
>
> | Metric | Detail |
> | :--- | :--- |
> | **Quality Score** | 9/10 |
> | **Models Tested** | 15 Large Vision-Language Models |
> | **Top Performer** | GPT-4o |
> | **Benchmark Accuracy** | ~60% (General) / ~30% (Hard) |
> | **Key Metric** | Relative Degradation (RD) |
> | **Citations** | 40 |

***

## Executive Summary

Large Vision-Language Models (LVLMs) suffer from pervasive non-factual hallucinations, particularly in fact-seeking question-answering scenarios. While state-of-the-art models like GPT-4o are advancing rapidly, they achieve only modest accuracy on existing benchmarks, concealing critical flaws in their reasoning capabilities. A fundamental challenge in the field is that current evaluation metrics treat LVLMs as "black boxes," failing to distinguish whether an incorrect answer stems from a failure in visual perception (recognizing the object) or a deficit in linguistic knowledge (retrieving the fact).

The researchers introduce **"VisualSimpleQA,"** a multimodal benchmark designed to rigorously assess factuality through a decoupled evaluation protocol. The dataset is constructed via human annotation based on specific difficulty criteriaâ€”such as Region of Interest (ROI) size, rationale granularity, and knowledge popularityâ€”and includes a "hard" subset to stress-test model limits. The technical novelty lies in the evaluation framework: each query includes both a Multimodal Question (MQA) and a Text-Only Question (TQA) where the visual rationale is embedded in the text. By calculating the **Relative Degradation (RD)**â€”the delta between TQA and MQA performanceâ€”the framework isolates errors to determine if a model is failing due to visual recognition capabilities or linguistic knowledge deficits.

Experiments conducted on 15 prominent LVLMs exposed a substantial performance gap and a high prevalence of hallucinations. GPT-4o, the top-performing model, achieved approximately 60% accuracy on the general benchmark; however, this figure plummeted to roughly 30% on the "VisualSimpleQA-hard" subset. The decoupled analysis revealed that neither visual nor linguistic modules are currently sufficient. This paper establishes a new standard for benchmarking LVLMs by shifting the focus from general capability to specific, fine-grained factuality, enabling more targeted future research.

***

## Key Findings

*   **Performance Gap:** Even state-of-the-art LVLMs like GPT-4o achieve modest performance (~60%+) on standard benchmarks and significantly lower performance (~30%+) on challenging subsets.
*   **Pervasive Hallucinations:** Non-factual hallucinations are widespread in fact-seeking questions, undermining model reliability.
*   **Dual Deficiency:** Decoupled evaluation reveals substantial opportunities for improvement in **both** visual recognition modules and linguistic knowledge modules.
*   **Difficulty Disparity:** There is a significant difficulty gap between general benchmarks and the 'hard' subset, highlighting model limitations in complex scenarios.

***

## Methodology

The research methodology focuses on a rigorous, multi-step approach to evaluation:

*   **Benchmark Construction:** Introduction of **VisualSimpleQA**, a new multimodal benchmark built via human annotation.
*   **Hard Subset Extraction:** Isolation of a 'VisualSimpleQA-hard' subset specifically designed to test the limits of model capabilities.
*   **Decoupled Protocol:** Implementation of an evaluation protocol designed to isolate and assess visual and linguistic modalities separately.
*   **Extensive Testing:** Comprehensive evaluation performed on **15** Large Vision-Language Models to ensure broad applicability of findings.

***

## Technical Details

### Evaluation Framework
VisualSimpleQA evaluates the factuality of LVLMs by decoupling visual recognition failures from linguistic knowledge deficits. The system relies on two distinct query types for the same sample:
1.  **Multimodal Question (MQA):** Requires both image and text inputs.
2.  **Text-Only Question (TQA):** Includes the embedded visual rationale in the text prompt.

**Key Metric: Relative Degradation (RD)**
This metric quantifies the performance gap between the textual and visual understanding:
$$RD = \frac{\text{score}\_{\text{TQA}} - \text{score}\_{\text{MQA}}}{\text{score}\_{\text{TQA}}}$$

*   **Linguistic Module Assessment:** Evaluated based on TQA performance.
*   **Visual Module Assessment:** Evaluated based on the performance delta (RD) between TQA and MQA.

### Dataset Structure
The dataset comprises triplets:
*   **Multimodal Question**
*   **Text-Only Question**
*   **Ground Truth Answer**

**Annotated Attributes:**
*   **Region of Interest (ROI)**
*   **Category**
*   **Difficulty Score** (Average of normalized inverse factors: Resolution, Proportion of ROI, Rationale Granularity, Text in Image, Knowledge Popularity)
*   **Resolution**
*   **ROI Proportion**
*   **Rationale Granularity**
*   **Text in Image**
*   **Knowledge Popularity**

***

## Results

*   **GPT-4o Performance:** Achieved approximately **60%+** accuracy on the general benchmark but dropped to roughly **30%+** on the 'VisualSimpleQA-hard' subset.
*   **Model Deficiencies:** Decoupled evaluations indicated that while some models possess strong linguistic knowledge, they often lack the visual grounding to apply it, or vice versa.
*   **Benchmark Comparison:**
    *   **Vs. HallusionBench:** VisualSimpleQA is more streamlined, avoiding complex decision trees.
    *   **Vs. Prism:** Does not require a secondary LLM reference.
    *   **Vs. MMBench/MMStar:** Focuses specifically on fine-grained factuality rather than general capability.

***

## Contributions

*   **Decoupled Framework:** Provides a diagnostic tool that offers deeper insights by assessing modality-specific modules (visual vs. linguistic).
*   **Rigorous Standardization:** Introduces defined difficulty criteria for human annotation, ensuring high-quality benchmark data.
*   **Community Resource:** Contributes the VisualSimpleQA dataset to the public for further research.
*   **Deficiency Identification:** Shifts the research focus toward factual accuracy in both visual and linguistic processing by identifying specific model flaws.

***

**Report generated based on 40 references.**