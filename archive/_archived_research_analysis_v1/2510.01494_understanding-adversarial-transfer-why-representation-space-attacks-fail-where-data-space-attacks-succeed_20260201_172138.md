# Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed

*Authors: Isha Gupta; Rylan Schaeffer; Joshua Kazdan; Ken Ziyu Liu; Sanmi Koyejo*

---

## üìã Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |
| **Focus** | Adversarial Robustness, Transferability |
| **Domains** | Image Classifiers, LMs, VLMs |
| **Key Insight** | Data-space attacks transfer; Representation-space attacks generally do not. |

---

## Executive Summary

### **Problem**
This paper addresses a fundamental inconsistency in adversarial machine learning regarding the transferability of attacks between neural networks. Specifically, it investigates the unresolved question of why adversarial perturbations applied directly to input data ("data-space") transfer effectively across different models, while perturbations applied to internal feature representations ("representation-space") generally fail to do so. Understanding this distinction is critical for the field because security assessments frequently rely on transferable attacks to evaluate black-box models and proprietary systems where internal parameters are inaccessible. Without a precise theoretical understanding of why attacks fail to transfer in feature space, researchers cannot accurately gauge the vulnerability of modern architectures, particularly in high-stakes domains like language and vision models.

### **Innovation**
The authors introduce a rigorous theoretical framework that mathematically formalizes adversarial transfer by modeling neural networks as a composition of a representation map $\phi(x)$ and a linear readout vector $w$. In this formulation, two functionally equivalent networks are related by an invertible linear transformation matrix $Q$. The key innovation is the proof that data-space attacks are universal and transfer 100% of the time because they operate on the shared input domain, whereas representation-space attacks are contingent upon the alignment of latent geometries. The framework establishes that for a representation-space attack to transfer, the specific condition $w^T Q = w$ must be met; this implies that the "adversarial direction" must remain invariant despite the transformation of the representation space.

### **Results**
Empirical validation across image classifiers, language models (LMs), and vision-language models (VLMs) strongly supports the theoretical findings. In experiments with ResNet18 classifiers on CIFAR10, data-space attacks maintained high Attack Success Rates (ASR) on transfer models, whereas representation-space attacks exhibited negligible transfer regardless of perturbation bounds $\epsilon$ or ensemble size. Similarly, in language model evaluations, the GCG attack (data-space) achieved approximately 24% $\Delta$HYH on transfer models, while soft prompt attacks (representation-space) proved overwhelmingly ineffective. For vision-language models, data-space attacks achieved up to 100% ASR, with successful transfer strictly dependent on models sharing the same language backbone, underscoring the necessity of geometric alignment for transferability.

### **Impact**
This research significantly advances the field by resolving recent contradictions in the literature concerning the transferability of image jailbreaks between VLMs. By delineating the mechanics of transfer based on geometric alignment versus input universality, the paper provides a robust theoretical foundation for future security research. The findings offer critical guidance for developing defense mechanisms, establishing that relying on representation-space perturbations for vulnerability assessment is often insufficient. Furthermore, this insight aids in the design of more robust architectures, as defenders can now focus on disrupting the specific invariant geometric structures required for adversarial attacks to succeed in the representation domain.

---

## üîë Key Findings

*   **Divergent Transferability:** Adversarial attacks in input data-space transfer robustly between models, whereas attacks in representation-space generally do not.
*   **Domain Contingency:** Transferability is strictly dependent on the operational domain‚Äîspecifically, whether the operation occurs in a shared data-space or unique representation spaces.
*   **Geometric Alignment Requirement:** Representation-space attacks can only transfer when the latent geometries of the source and target models are sufficiently aligned.
*   **Broad Applicability:** The failure of representation-space attacks to transfer is consistent across image classifiers, Language Models (LMs), and Vision-Language Models (VLMs).

---

## üß™ Methodology

The researchers employed a comprehensive multi-stage approach that combined theoretical formalization with extensive empirical testing. The study was conducted across four distinct settings:

1.  **Theoretical Formalization:** Mathematical derivation to model and prove the mechanics of adversarial transfer.
2.  **Image Classifier Experiments:** Direct comparison of representation-space vs. data-space attacks on image classification tasks.
3.  **Language Model Testing:** evaluation of representation-space jailbreaks on LMs.
4.  **Vision-Language Analysis:** Assessment of latent geometry alignment and its impact on transferability in VLMs.

**Experimental Architectures:**
*   **Image Classifiers:** ResNet18 on CIFAR10.
*   **Language Models:** Comparison of soft prompts (representation-space) vs. GCG (data-space).
*   **Vision-Language Models:** Adapter-based VLMs using a dual-model GCG variant.

---

## ‚öôÔ∏è Technical Details

The paper provides a granular analysis of attack vectors by distinguishing between two primary types:

1.  **Data-space Attacks:** Perturbations applied directly to the input data.
2.  **Representation-space Attacks:** Perturbations applied to internal features or activations.

**Mathematical Framework**
Neural networks are modeled as a composition of a representation map and a linear readout. Functionally equivalent networks are related through linear transformations.

*   **Network Model:** $f(x) = w \cdot \phi(x)$
    *   Where $\phi: X \to \mathbb{R}^H$ is the representation map.
    *   Where $w$ is the linear readout vector.
*   **Equivalence Relation:** Two functionally equivalent networks $f$ and $\tilde{f}$ are related by an invertible linear transformation matrix $Q$.

**Theoretical Proofs**
*   **Data-space Transfer:** Universal transfer is guaranteed.
    *   $\forall x, \forall \delta_{data}: \tilde{f}_{data}(x) = f_{data}(x)$
*   **Representation-space Transfer:** Transfer is conditional on geometric alignment.
    *   An attack transfers only if the latent geometries align such that $w^T Q = w$.

---

## üìä Results

**Image Classifiers (ResNet18 / CIFAR10)**
*   **Data-space:** Maintained high Attack Success Rate (ASR) on transfer models.
*   **Representation-space:** Showed negligible transfer. This failure persisted regardless of perturbation bounds ($\epsilon$) or ensemble size.

**Language Models**
*   **Data-space (GCG Attack):** Achieved approximately **24% $\Delta$HYH** on transfer models.
*   **Representation-space (Soft Prompts):** Overwhelmingly ineffective in transferring attacks.

**Vision-Language Models**
*   **Data-space:** Achieved up to **100% ASR**.
*   **Transfer Conditionality:** Success was strongly dependent on models sharing the same language backbone. The specific vision adapter used did not significantly alter transfer success, reinforcing the geometric alignment theory.

---

## üìù Contributions

*   **Resolution of Contradictions:** The paper resolves conflicting reports in recent literature regarding why image jailbreaks transfer between some VLMs but not others.
*   **New Theoretical Framework:** Introduces a framework that separates adversarial transfer mechanics into two distinct categories:
    *   **Geometric Alignment:** Required for representation-space attacks.
    *   **Input Data Universality:** Drives data-space attacks.
*   **Defense Guidance:** Provides critical direction for developing robust defense mechanisms by clarifying that effective vulnerability assessment via transfer relies heavily on the domain of operation.