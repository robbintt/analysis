---
title: Spoken question answering for visual queries
arxiv_id: '2505.23308'
source_url: https://arxiv.org/abs/2505.23308
generated_at: '2026-01-26T09:46:58'
quality_score: 9
citation_count: 0
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Spoken question answering for visual queries

*Nimrod Shabtay, Hagai Aronowitz, Aviv University, Zvi Kons, Ron Hoory, Avihu Dekel, Assaf Arbelle*

---

## Executive Summary

> Visual Question Answering (VQA) systems traditionally rely on a cascaded pipeline that processes spoken queries through Automatic Speech Recognition (ASR) before analyzing visual content. This architecture is fundamentally vulnerable to error propagation, where inaccuracies in the ASR transcript severely degrade the system's reasoning capability. This limitation creates a significant barrier to deploying robust spoken AI assistants in real-world environments, where background noise and acoustic variability are inevitable, and perfect speech transcription cannot be guaranteed.
>
> The research addresses the critical need to decouple visual reasoning from the dependency on flawless text transcription. The authors introduce a novel Multimodal Fusion Architecture that departs from standard text-centric approaches by incorporating raw acoustic signals alongside ASR transcripts. The system simultaneously processes three distinct modalities: visual features from video frames, textual features from ASR output, and acoustic features extracted directly from raw waveforms.
>
> The core innovation lies in the use of **Cross-Modal Attention mechanisms**, which allow acoustic and textual representations to interact and align with one another before fusing with visual data. By enabling the model to utilize prosodic and phonetic cues from raw audio, the architecture can cross-reference and correct errors present in the ASR transcript, effectively grounding spoken questions in the visual scene with reduced reliance on intermediate text.
>
> Experimental evaluation on the standard AVQA dataset quantifies the superiority of the proposed approach. The "Audio + Transcript" model achieved an overall accuracy of **48.8%**, improving upon the transcript-only baseline by a significant margin of **3.4 percentage points**. Crucially, the model demonstrated substantial robustness in noisy environments; while transcript-only models suffered marked performance degradationâ€”dropping by over **12%** in accuracy under adverse acoustic conditionsâ€”the multimodal approach maintained high performance, limiting the degradation to less than **5%** by leveraging raw acoustic data to compensate for textual errors.

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Dataset** | AVQA Dataset |
| **Model Accuracy** | 48.8% |
| **Performance Lift** | +3.4 percentage points over baseline |
| **Robustness** | Degradation limited to <5% in noise (vs >12% baseline) |
| **Architecture** | Multimodal Fusion (Visual + Textual + Acoustic) |
| **Key Innovation** | Cross-Modal Attention with raw audio signals |

---

## Key Findings

*   **Superior Modality Fusion:** Utilizing both raw audio signal (acoustic features) and ASR transcripts yields significantly better performance than relying on the ASR transcript alone.
*   **Robustness to Error:** The model demonstrates increased resilience to errors introduced by the speech recognition component, effectively utilizing raw audio to compensate for transcript inaccuracies.
*   **Effective Alignment:** The architecture effectively learns to align spoken questions with visual content through cross-modal attention.
*   **State-of-the-Art Performance:** The approach achieved SOTA results on the AVQA dataset, with particularly strong performance in noisy environments where traditional models fail.

## Methodology

**Multimodal Fusion Architecture**
The researchers implemented a system capable of processing three distinct modalities simultaneously:
1.  **Visual:** Features extracted from image or video frames.
2.  **Textual:** Transcript derived from ASR.
3.  **Acoustic:** Features extracted from raw waveform or spectrogram.

**Cross-Modal Attention**
To integrate these streams, the model employs Cross-Modal Attention mechanisms. This allows acoustic and textual representations to interact and complement each other *before* being fused with visual features. This interaction is crucial for filtering noise and correcting ASR errors prior to the final answer generation.

## Technical Details

*   **Hybrid Input Mechanism:** The architecture processes both ASR transcripts and raw audio signals, fusing acoustic and linguistic modalities early in the pipeline.
*   **Cross-Modal Alignment:** The system maps spoken questions to visual content by aligning features across the audio and visual streams.
*   **Error Compensation Strategy:** Raw audio is specifically used to capture prosodic or phonetic cues that are often lost in text-only models, allowing the system to mitigate ASR errors.
*   **Processing Units:** Handles raw waveforms or spectrograms directly alongside standard text embeddings.

## Contributions

*   **Novel Framework:** Introduction of a framework that incorporates raw acoustic features into VQA, whereas previous approaches largely discarded the audio signal after conversion to text.
*   **Error Mitigation:** significantly reduces error propagation by bypassing the heavy reliance on the ASR transcript.
*   **New Baseline:** Establishment of a new baseline for spoken visual question answering, proving that direct audio-visual models are more effective than cascaded audio-to-text-to-visual models.

## Results

*   **Comparative Performance:** The 'Audio + Transcript' model consistently outperforms transcript-only models.
*   **Noise Resilience:** The method shows significant resilience in noisy environments. While baselines suffered performance drops of over 12%, the proposed method limited degradation to less than 5%.
*   **Quantitative Achievement:** Achieved State-of-the-Art (SOTA) results on the AVQA dataset with an accuracy of **48.8%**.

---

**Quality Score:** 9/10
**References:** 0 citations