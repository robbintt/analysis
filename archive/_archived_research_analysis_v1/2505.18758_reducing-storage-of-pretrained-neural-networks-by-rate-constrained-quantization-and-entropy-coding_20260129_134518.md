# Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding

*Alexander Conzelmann; Robert Bamler*

---

### ðŸ“‹ Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Method Name** | CERWU (Rate-Constrained Quantization) |
| **Compression Gain** | 20â€“40% reduction in bit rate vs. NNCodec |
| **Accuracy Impact** | Zero degradation relative to original models |
| **Novelty** | Support for fractional bit-widths & arbitrary grids |
| **Target Device** | Resource-constrained/Edge devices |
| **Quality Score** | 8/10 |
| **References** | 40 citations |

---

## Executive Summary

The rapid expansion of neural network model sizes presents a critical challenge for deployment in storage-constrained and bandwidth-limited environments, such as edge devices. While quantization is a standard technique for reducing model size, existing methods often rely on fixed, power-of-two grids or greedy approximation algorithms that fail to optimize the trade-off between compression rate and distortion effectively. Consequently, there is a need for advanced compression frameworks that can significantly lower bit rates without sacrificing the accuracy of large pretrained computer-vision models.

This paper introduces a post-training compression framework that formulates weight quantization as a constrained rate-distortion optimization problem. The core innovation lies in modifying the standard layer-wise objective function to include a quadratic rate estimation term derived from a discretized Gaussian approximation of the entropy model. This novel formulation allows the authors to employ the Optimal Brain Surgeon (OBS) method, enabling locally exact solutions that compensate for the quantization error across the remaining weights in a row utilizing an entropy-regularized Hessian. Crucially, this approach supports arbitrary quantization grids, permitting fractional bit-widths rather than restricting weights to standard power-of-two boundaries.

Empirical validation across various computer-vision networks demonstrates that the proposed method achieves a 20â€“40% reduction in bit rate compared to NNCodec, the reference implementation for the ISO/IEC 15938-17 standard. Impressively, this improvement in storage efficiency is realized without any degradation in model performance relative to the original uncompressed models. Furthermore, the method consistently outperformed baseline approaches such as RTN+EC (Round to Nearest + Entropy Coding) and OPTQ, while maintaining equivalent accuracy and enabling very fast decoding speeds suitable for resource-constrained hardware.

By successfully decoupling bit-width limitations from power-of-two constraints and integrating rate-aware optimization directly into the quantization process, this work sets a new benchmark for neural network compression efficiency. The ability to achieve superior storage savings while maintaining rapid decoding capabilities addresses a fundamental bottleneck in the widespread deployment of deep learning. This framework provides a robust, post-training solution that allows practitioners to deploy state-of-the-art models on edge devices without the computational overhead or accuracy penalties associated with previous methods.

---

## Key Findings

*   **Significant Bit Rate Reduction:** Achieves a **20â€“40%** decrease in bit rate compared to NNCodec while maintaining equivalent model performance.
*   **Fast Decoding:** Enables very fast decoding, making it highly suitable for resource-constrained edge devices.
*   **Broad Validation:** Empirical testing on various computer-vision networks confirmed the efficacy of the compression strategy.
*   **Grid Flexibility:** Compatible with arbitrary quantization grids, offering significant implementation flexibility.
*   **Performance Retention:** Successfully mitigates growing neural network sizes without degrading model performance relative to the original.

---

## Technical Methodology & Details

The proposed method, **CERWU**, is a post-training compression scheme that formulates weight quantization as a rate-distortion optimization problem.

### Core Framework

*   **Objective:** Minimization of a Lagrangian objective function balancing distortion (Euclidean distance between original and quantized layer outputs) and bit rate (information content based on an entropy model).
*   **Entropy Approximation:** The approach approximates the entropy model with a **discretized Gaussian distribution**, transforming the rate term into a quadratic regularization term.

### Optimization Strategy

*   **Algorithm:** Utilizes the **Optimal Brain Surgeon (OBS)** framework.
*   **Process:** Weights are quantized to a value on a symmetric, uniform grid. The remaining weights in the row are updated to compensate using an entropy-regularized Hessian.
*   **Bit-Width Flexibility:** The grid size is not restricted to powers of two, allowing for **fractional bit-widths** per weight.

### Algorithmic Components

1.  **Objective Modification:** Extends the standard layer-wise loss function by incorporating a quadratic rate estimation term.
2.  **Optimization Technique:** Provides locally exact solutions to the modified objective function using the OBS method.

---

## Contributions

*   **Unified Compression Framework:** Introduced a new post-training method combining rate-aware quantization and entropy coding.
*   **Advanced Objective Formulation:** Contributed a modified objective function balancing accuracy against a quadratic rate penalty for efficient rate-distortion trade-offs.
*   **Optimization Strategy:** Applied the Optimal Brain Surgeon (OBS) method to solve the rate-constrained quantization problem locally and exactly.
*   **Benchmarking Performance:** Demonstrated superior efficiency over state-of-the-art methods like NNCodec, achieving substantial storage savings without sacrificing accuracy.

---

## Performance Results

The proposed approach was rigorously tested against existing state-of-the-art baselines:

*   **Vs. NNCodec (ISO/IEC 15938-17 Standard):** Achieved a **20â€“40% decrease** in bit rate with equivalent model accuracy.
*   **Vs. RTN+EC (Round to Nearest + Entropy Coding):** Outperformed by effectively integrating rate constraints.
*   **Vs. OPTQ:** Outperformed by avoiding greedy approximations.
*   **Inference:** Maintained equivalent accuracy relative to original uncompressed models while offering very fast decoding capabilities.