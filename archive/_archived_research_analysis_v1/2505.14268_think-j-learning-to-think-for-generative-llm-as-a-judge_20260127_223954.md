---
title: 'Think-J: Learning to Think for Generative LLM-as-a-Judge'
arxiv_id: '2505.14268'
source_url: https://arxiv.org/abs/2505.14268
generated_at: '2026-01-27T22:39:54'
quality_score: 7
citation_count: 23
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Think-J: Learning to Think for Generative LLM-as-a-Judge

*Yancheng He, Jiaheng Liu, Alibaba Group, Intelligence Science, Wei Liu, Rui Zhang, Wenbo Su, Hongli Zhou, Hui Huang, Weixun Wang*

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 7/10 |
| **Focus** | LLM-as-a-Judge, Reasoning Trace Optimization |
| **Core Method** | Reinforcement Learning (Online & Offline) |
| **Top Performance** | **84.9%** Accuracy (HelpSteer2-Pref) |
| **Key Benefit** | No extra human annotations required |

---

> ### üìù Executive Summary
>
> The research addresses the critical performance gap within the "LLM-as-a-Judge" paradigm, identifying a fundamental limitation: while generative LLMs offer versatility, they often lack the discriminative reliability and reasoning depth of specialized classifiers. This deficiency hinders the development of scalable, automated evaluation systems capable of matching human-level judgment accuracy without incurring prohibitive costs.
>
> To bridge this gap, the paper introduces **Think-J**, a novel framework designed to optimize "thinking" traces prior to judgment through a hybrid two-step pipeline. The methodology begins with an **Initialization** phase involving Supervised Fine-Tuning (SFT) on the LIMJ707 dataset using Deepseek-R1 reasoning traces. This is followed by a sophisticated **Reinforcement Learning** optimization phase employing two distinct strategies: Critic-guided Offline Learning (Direct Preference Optimization) and Rule-based Online Learning (Group Relative Policy Optimization).
>
> Evaluations using Qwen-2.5 and Llama-3 base models demonstrate that Think-J significantly outperforms strong baselines, including BT Classifier, Cloud, and SynRM. Specifically, the Qwen-2.5-32B Think-J model achieved an **Accuracy of 84.9%**, substantially surpassing the BT Classifier (76.4%) and the Cloud baseline (77.8%). This work validates that generative LLMs can surpass traditional judgment standards when explicitly trained to reason, offering a scalable, cost-effective solution for the future of AI evaluation systems.

---

## ‚ú® Key Findings

*   **Performance Enhancement:** The Think-J framework significantly bridges the performance gap for generative LLMs acting as judges, enabling them to surpass both existing generative and classifier-based baselines.
*   **Annotation-Free Optimization:** The proposed method achieves superior performance without requiring extra human annotations, drastically reducing reliance on costly manual labeling.
*   **Reasoning via RL:** Reinforcement Learning is proven to be an effective mechanism for refining the reasoning processes behind judgments.
*   **Hybrid Strategy:** Utilizing both offline (critic-based) and online (rule-based) RL strategies maximizes the optimization of judgment thinking traces.

---

## üî¨ Methodology

The Think-J framework utilizes a two-phase approach to train generative models to "think" before judging.

### Phase 1: Initialization
*   **Supervised Fine-Tuning (SFT):** The model is initialized using a small amount of curated data to establish foundational judgment capabilities.

### Phase 2: Optimization via Reinforcement Learning
The optimization phase employs two distinct methods:
1.  **Offline RL:**
    *   Trains a separate **critic model**.
    *   Constructs positive and negative examples to guide the optimization process without real-time interaction.
2.  **Online RL:**
    *   Utilizes **rule-based rewards** as direct feedback signals.
    *   Guides the judge's trace optimization in real-time based on specific evaluation metrics.

---

## ‚öôÔ∏è Technical Details

*   **Pipeline:** Two-step pipeline designed to optimize generative LLMs as judges.
*   **Step 1 (SFT):**
    *   **Dataset:** LIMJ707.
    *   **Source:** Derived from Skywork-Preference-v0.2 via difficulty/diversity filtering.
    *   **Annotation:** Annotated with Deepseek-R1 reasoning traces.
*   **Step 2 (RL):**
    *   **Critic-guided Offline Learning:** Uses **DPO** (Direct Preference Optimization) with a dual model architecture.
    *   **Rule-based Online Learning:** Uses **GRPO** (Group Relative Policy Optimization).
*   **Optimization Metrics:** Based on Accuracy, Formality, and Strength.

---

## üìà Results

### Experimental Setup
*   **Base Models:** Qwen-2.5 and Llama-3.
*   **Training Data:** HelpSteer2-Pref and HH-RLHF.
*   **Model Scale:** 32B parameters evaluated.
*   **Baselines Compared:** Direct Prompt, SFT (w/ and w/o CoT), BT Classifier, CLoud, and SynRM.

### Performance Outcomes
*   **Dataset:** HelpSteer2-Pref
    *   **Think-J (Qwen-2.5-32B):** **84.9% Accuracy**
    *   **SynRM:** 80.3%
    *   **Cloud Baseline:** 77.8%
    *   **BT Classifier:** 76.4%
*   **Consistency:** Quantitative gains were consistent across Formality and Strength metrics, confirming that optimizing reasoning traces allows generative models to exceed the discriminative capabilities of traditional classifier-based judges.

---

## üöÄ Contributions

*   **Paradigm Shift:** Introduces Think-J, a novel paradigm for LLM-as-a-Judge that shifts focus from simply generating verdicts to learning "how to think" before judging.
*   **Pipeline Development:** Developed a pipeline that enables generative judges to outperform traditional classifier-based judges, expanding the utility of generative models in evaluation and reward modeling.
*   **Cost-Effective Strategy:** Presented a training strategy that improves performance without the need for additional human-annotated data during the optimization phase.

---

**References:** 23 citations