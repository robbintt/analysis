---
title: Re:Frame -- Retrieving Experience From Associative Memory
arxiv_id: '2508.19344'
source_url: https://arxiv.org/abs/2508.19344
generated_at: '2026-02-03T06:41:51'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Re:Frame -- Retrieving Experience From Associative Memory
*Daniil Zelezetsky; Egor Cherepanov; Alexey K. Kovalev; Aleksandr I. Panov*

---

> ### ðŸ“Œ Quick Facts
> **Task Domain:** D4RL MuJoCo (Hopper, Walker2d)  
> **Expert Data Required:** 60 trajectories (0.1% of dataset)  
> **Peak Performance Gain:** +10.7 normalized points (Hopper-M)  
> **Architecture:** Plug-in module (Associative Memory Buffer)  
> **Comparison:** Outperformed Decision Transformer in 3/4 settings  
> **Quality Score:** 9/10

---

## Executive Summary

Offline Reinforcement Learning (RL) faces a critical challenge when training on suboptimal or noisy datasets, as agents often struggle to generalize high-performance behaviors without substantial expert guidance. While incorporating expert demonstrations can mitigate this issue, obtaining large-scale, high-quality data is often impractical due to cost and effort. This research addresses the **"low-expert regime"**â€”specifically, how to leverage a minute fraction of expert trajectories to significantly enhance an agent's performance when learning predominantly from low-quality offline data.

The authors introduce **Re:Frame**, an architecture-agnostic plug-in module designed to augment standard offline RL policies, such as the Decision Transformer, without altering the backbone architecture. The core innovation is an **Associative Memory Buffer (AMB)**, which stores compressed representations of expert trajectories. The system operates in two stages: first, an autoencoder learns to compress expert data into the AMB; second, the base policy is trained on suboptimal data while learning to perform content-based queries to the AMB. During inference, the agent retrieves relevant expert embeddings based on the current state, decodes them into candidate actions, and combines them with the base policy's output via a correction vector to produce the final decision.

Evaluated on D4RL MuJoCo tasks (Hopper and Walker2d) using Medium and Medium-Replay datasets, Re:Frame demonstrated high data efficiency by utilizing only 60 expert trajectoriesâ€”representing merely **0.1%** of the total dataset. The method outperformed a strong Decision Transformer baseline in three out of four tested settings, achieving substantial gains in normalized scores: **+10.7** on Hopper-M, **+4.3** on Walker2d-M, and **+3.2** on Walker2d-MR. Ablation studies further highlighted the method's sensitivity to memory density, noting that while performance remained stable in Walker2d-M across varying memory sizes, it dropped significantly in Hopper-MR when the number of expert trajectories was reduced.

The significance of Re:Frame lies in its empirical validation that extremely scarce expert data can serve as a powerful catalyst for offline RL performance. By offering a non-invasive, modular solution that requires no environment interaction for retrieval, the framework provides a practical path for upgrading existing models without architectural overhauls. This work establishes retrieval-based learning via associative memory as a viable strategy for bridging the gap between theoretical offline RL potential and real-world data constraints, particularly in scenarios where high-quality demonstrations are limited.

---

## Key Findings

*   **Significant Performance Boost:** Incorporating a minute amount of expert data can substantially enhance agent performance, achieving gains of up to **+10.7 normalized points** over the baseline in D4RL MuJoCo tasks.
*   **High Data Efficiency:** The method showed consistent improvements using only **60 expert trajectories** (0.1% of the dataset).
*   **Superior to Baselines:** Re:Frame outperformed a strong Decision Transformer baseline in **three out of four** tested settings.
*   **Robustness to Imperfect Data:** The approach effectively addresses the difficulty of generalizing from imperfect or inconsistent trajectories by leveraging scarce expert demonstrations.

---

## Methodology

Re:Frame functions as a **plug-in module** designed to augment standard offline Reinforcement Learning (RL) policies, such as the Decision Transformer, without modifying the backbone architecture. Its core mechanism involves an **Associative Memory Buffer (AMB)**, a small external memory buffer populated exclusively with expert trajectories.

**Process Overview:**
1.  **Training:** During training on low-quality data, the policy learns to perform content-based associations to query the AMB.
2.  **Integration:** It retrieves relevant expert data and integrates this information directly into its decision-making process.
3.  **Evaluation:** The same retrieval mechanism is applied at evaluation time without requiring additional environment interaction.

---

## Technical Details

Re:Frame is an architecture-agnostic framework for offline RL that uses an Associative Memory Buffer (AMB) to enhance agents in low-expert regimes.

*   **Memory Storage:** The AMB stores compressed expert trajectories (triplets of Return-to-go, Observation, and Action) using encoders to generate latent keys and decoders for reconstruction.
*   **Stage 1 Training:** An Autoencoder is trained on expert data to populate the AMB.
*   **Stage 2 Training:** The base policy (e.g., Decision Transformer) is trained on suboptimal data while using the frozen AMB.
*   **Inference Pipeline:**
    1.  Agent generates a query from the current state.
    2.  System retrieves the nearest expert embedding from the AMB.
    3.  Embedding is decoded into a candidate action.
    4.  Candidate action is combined with the base policy's output via a correction vector to produce the final action.

---

## Contributions

*   **Novel Framework:** Introduced Re:Frame, a novel framework for injecting scarce expert knowledge into offline RL agents via associative memory.
*   **Retrieval-Based Learning:** Proposed a retrieval-based learning method where the policy learns to retrieve relevant expert experiences based on content associations.
*   **Modular Solution:** Provided a non-invasive solution that enhances performance without requiring changes to the underlying model architecture or relying on environment interaction.
*   **Empirical Validation:** Validated on D4RL MuJoCo tasks that data-efficient integration of expert data is a viable strategy for improving offline RL performance.

---

## Results

The model was evaluated on D4RL MuJoCo tasks (**Hopper** and **Walker2d**) using Medium and Medium-Replay datasets with only 60 expert trajectories (0.1% of data).

*   **Performance vs. Baseline:** Re:Frame outperformed the Decision Transformer baseline in 3 out of 4 settings.
*   **Specific Gains (Normalized Scores):**
    *   **+10.7** (Hopper-M)
    *   **+3.2** (Walker2d-MR)
    *   **+4.3** (Walker2d-M)
*   **Ablation Study:**
    *   Showed performance is sensitive to memory size in Hopper-MR (dropping from **69.7 to 3.0** when reducing trajectories from 60 to 30).
    *   Performance remained stable in Walker2d-M across different memory sizes.

---
**Quality Score:** 9/10 | **References:** 40 citations