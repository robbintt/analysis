# Behaviour Policy Optimization: Provably Lower Variance Return Estimates for Off-Policy Reinforcement Learning

*Alexander W. Goodall; Edwin Hamel-De le Court; Francesco Belardinelli*

---

> ### **Executive Summary**
>
> This research addresses the critical challenge of **high variance** in return estimates within Reinforcement Learning (RL), a fundamental issue that degrades sample efficiency and impedes policy convergence. Conventional wisdom has long held that on-policy data collection—where an agent learns solely from data generated by its current policy—is variance-optimal. However, this paper demonstrates that this assumption is theoretically flawed.
>
> The key innovation is **Behaviour Policy Optimization (BPO)**, a framework that applies theoretical insights from off-policy evaluation to the online RL setting. The authors extend canonical policy-gradient methods (REINFORCE and PPO) to operate under a new regime by optimizing a separate behavior policy $\mu$ specifically engineered to generate data that minimizes the variance of the *gradient estimator* for the target policy $\pi$.
>
> Empirical results validate this approach, showing variance reductions ranging from **5x to over 30x** compared to on-policy baselines in environments like MuJoCo and Atari. This work provides a robust alternative to complex distributed systems (like IMPALA), making high-performance RL more accessible by bridging the gap between off-policy evaluation theory and online control.

---

### **>>> Quick Facts**

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 7/10 |
| **References** | 40 Citations |
| **Core Method** | Behaviour Policy Optimization (BPO) |
| **Key Mechanism** | Variance-optimal behavior policy $\mu$ for target policy $\pi$ |
| **Architecture** | Single Worker (vs. Asynchronous Multi-worker) |
| **Benchmarks** | MuJoCo (HalfCheetah, Hopper), Atari (Pong, Breakout) |
| **Performance Gain** | 5x - 30x Variance Reduction |

---

### **Key Findings**

*   **On-Policy Assumption Challenged:** Contrary to conventional assumptions, collecting data on-policy is **not variance-optimal**. Well-designed behavior policies can generate off-policy data that yields provably lower variance return estimates.
*   **Theoretical Extension:** The study successfully extends insights from off-policy evaluation to the online RL setting (where evaluation and improvement are interleaved), reducing the variance of return estimates.
*   **Sample Efficiency:** The behavior policy optimization regime leads to **better sample efficiency** and overall performance across diverse environments compared to standard policy-gradient methods.
*   **Architectural Simplicity:** Significant variance reduction is achieved using a **single worker** (the behavior policy). This eliminates the need for complex multi-worker, asynchronous architectures often required to handle data distribution mismatches (e.g., IMPALA).

### **Methodology**

The researchers propose a framework grounded in recent theoretical results regarding the design of behavior policies that minimize estimation variance.

*   **Framework Integration:** The authors apply off-policy evaluation insights to an online reinforcement learning framework, integrating the behavior policy into the standard loop of policy evaluation and improvement.
*   **Single-Worker Design:** Unlike existing off-policy algorithms such as **IMPALA**—which rely on multiple parallel workers and truncated importance weighting to correct distribution mismatches—this approach utilizes a **single behavior policy** to collect data.
*   **Algorithm Extension:** The team extended two existing policy-gradient methods (implied to be REINFORCE and PPO based on the full text) to operate under this new regime. These extensions utilize the behavior policy specifically to collect data for policy improvement with lower variance estimates.

### **Technical Details**

The paper introduces **Behaviour Policy Optimization (BPO)**, formalizing the approach with specific mathematical components:

*   **Optimization Goal:** Optimizes a behavior policy $\mu$ to generate off-policy data for a target policy $\pi$.
*   **Estimators:**
    *   Introduces **truncated importance sampling (IS) weighted TD($\lambda$)** returns.
    *   Utilizes the **Per-Decision IS Monte Carlo return estimator ($G_t^{PDIS}$)** for unbiased estimation based on coverage conditions.
*   **Theoretical Foundation:** The method is grounded in **variance optimal sampling theory** (specifically citing Lemmas 1 and 2).
*   **Distinctive Features:**
    *   Uses a single worker design.
    *   Reduces the need for aggressive weight truncation compared to methods like **ACER** or **V-trace**.

### **Results**

The proposed approach demonstrates considerable advantages over standard baselines:

*   **Performance:** Claims better sample efficiency and overall performance compared to standard policy-gradient methods across diverse environments (MuJoCo, Atari).
*   **Variance Control:** Demonstrates significant variance reduction using a single worker architecture.
*   **Comparative Analysis:**
    *   **Vs. On-Policy Learning:** BPO offers superior variance control.
    *   **Vs. Asynchronous Architectures (IMPALA):** Achieves comparable or better variance without the associated system complexity.
    *   **Vs. Standard Off-Policy Correction:** Avoids the variance explosion typically seen in standard off-policy correction methods.