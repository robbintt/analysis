# QS4D: Quantization-aware training for efficient hardware deployment of structured state-space sequential models

*Sebastian Siegel; Ming-Jay Yang; Younes Bouhadjar; Maxime Fabre; Emre Neftci; John Paul Strachan*

---

> ### **Quick Facts**
> * **Target Model:** S4D (Structured State Space for Diagonal state matrix)
> * **Core Technique:** Quantization-Aware Training (QAT)
> * **Hardware Platform:** IMSSA (In-Memory State Space Accelerator) on Memristive AIMC
> * **Key Efficiency:** Complexity reduced by up to **100×**
> * **Quantization Limit:** **6-bit** homogeneous quantization (vs. PTQ failing at ~10 bits)
> * **Robustness:** Superior resistance to Gaussian transient noise vs. Floating Point
> * **Quality Score:** **8/10**

---

## Executive Summary

This research addresses the critical challenge of deploying Structured State Space models (SSMs) on resource-constrained edge devices by bridging the gap between software-intensive models and the physical realities of analog hardware.

### **Problem**
SSMs offer powerful sequential modeling capabilities rivaling transformers, but their high computational density and memory demands hinder edge deployment. While Analog In-Memory Computing (AIMC) offers a solution, there is insufficient research on how SSMs behave under hardware constraints like limited numerical precision and analog noise.

### **Innovation**
The authors introduce **QS4D**, a methodology integrating Quantization-Aware Training (QAT) directly into the S4D training pipeline. It employs an equally spaced integer ladder projection to quantize parameters, layers, activations, and latent states. The optimized model is mapped to **IMSSA** (In-Memory State Space Accelerator), a custom architecture using memristive crossbar arrays for streaming implementation.

### **Results**
On Sequential CIFAR10, QS4D enabled **6-bit** homogeneous quantization before error exceeded 1% (compared to PTQ failing at ~10 bits). Compared to floating-point baselines:
*   **Computational Complexity (ACE):** Reduced by 11.3–23.8× (PTQ) + an additional 2–11.5× (QAT).
*   **Memory Footprint:** Reduced by ≥4× (PTQ) + an additional 1.4–2.9× (QAT).
*   **ADC Complexity:** Reduced by 2.7–4.6× (PTQ) + an additional 1.4–4× (QAT).
*   **Structural Pruning:** Up to 53.125% (vs. 39.063% for floating-point models).

### **Impact**
This work establishes quantization not just as a compression technique, but as a fundamental enabler for hardware deployment. It validates the viability of SSMs for edge computing by demonstrating robustness against analog noise and significant efficiency gains on memristive AIMC substrates.

---

## Key Findings

*   **Significant Complexity Reduction:** QAT can reduce the complexity of Structured State Space models (SSMs) by **up to two orders of magnitude**.
*   **Hardware Robustness:** QAT enhances the robustness of SSMs specifically against **analog noise**, a critical factor for analog hardware deployment.
*   **Pruning Capabilities:** The application of QAT enables effective **structural pruning** of the models, further reducing resource requirements.
*   **Edge Deployment Viability:** The integration of QAT with SSMs enables their successful deployment on memristive analog in-memory computing (AIMC) substrates, resulting in tangible gains in computational efficiency.

---

## Methodology

The researchers focused on optimizing Structured State Space models (SSMs) for edge-computing constraints by applying Quantization-Aware Training (QAT). Their methodology involved:

1.  **Constraint Analysis:** Analyzing the relationship between model size and numerical precision.
2.  **Resilience Testing:** Investigating the impact of QAT on the model's resilience to analog noise.
3.  **Structural Pruning:** Assessing the model's capacity for structural pruning under quantization.
4.  **Physical Deployment:** Integrating these optimization techniques to physically deploy the SSMs onto a memristive analog in-memory computing (AIMC) substrate to measure real-world efficiency gains.

---

## Technical Details

### **Model Target**
*   **Architecture:** S4D (Structured State Space for Diagonal state matrix).
*   **Feature:** Utilizes a diagonalized transition matrix $A$.

### **Quantization Strategy**
*   **Type:** Quantization-Aware Training (QAT).
*   **Projection:** Equally spaced integer ladder projection.
*   **Scope:**
    *   Parameters: $A$, $C$, $\Delta$, and linear layers.
    *   Activations: Signal $y$.
    *   Latent State: Indirectly during the convolution phase.

### **Hardware Architecture (IMSSA)**
*   **Name:** IMSSA (In-Memory State Space Accelerator).
*   **Substrate:** Memristive Crossbar Arrays (mCBA) for Analog In-Memory Computing (AIMC).
*   **Implementation:** Streaming implementation.
*   **Mapping:**
    *   **$A$ Matrix:** Occupies the main array bulk.
    *   **Vectors $B$ and $C$:** Use single rows/columns.
*   **Kernel Equations:**
    *   $x_t = A x_{t-1} + B u_t$
    *   $y_{t-1} = C x_{t-1} + D u_t$

---

## Results

### **Quantization Performance**
*   **Sequential CIFAR10:** QAT enables aggressive reduction to **6-bit** homogeneous quantization before error exceeds 1%.
*   **Comparison:** Post-Training Quantization (PTQ) fails at approximately **10 bits**.

### **Efficiency Gains (vs. Floating Point Baselines)**
*   **Computational Complexity (ACE):** Reduction factors of **11.3–23.8×** for PTQ with an additional **2–11.5×** for QAT.
*   **Memory Footprint:** Reduction of **≥4×** for PTQ with an additional **1.4–2.9×** for QAT.
*   **ADC Complexity:** Reduction of **2.7–4.6×** for PTQ with an additional **1.4–4×** for QAT.

### **Noise and Pruning**
*   **Structural Pruning:** 7-bit quantization allows for increased pruning compared to FP models (e.g., **53.125%** vs. **39.063%** in Layer 1).
*   **Noise Robustness:** Demonstrates superior robustness to Gaussian transient noise; aggressively quantized models outperform FP models at high noise levels.

---

## Contributions

*   **Bridging Software-Hardware Gap:** Addresses the lack of research on the implications of QAT for SSMs when deployed on specialized edge hardware.
*   **Quantification of Optimization:** Provides empirical evidence that QAT can drastically reduce SSM complexity (up to 100x) while analyzing the trade-offs between model size and numerical precision.
*   **Enabling Analog Deployment:** Demonstrates that QAT is not only a compression technique but also an enabler for hardware deployment, specifically by conferring robustness to analog noise and enabling the structural pruning necessary for memristive AIMC substrates.

---
*Quality Score: 8/10 | References: 5 citations*