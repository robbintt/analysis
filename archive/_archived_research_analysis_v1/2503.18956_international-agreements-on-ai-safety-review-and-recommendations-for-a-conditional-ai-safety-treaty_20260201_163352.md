# International Agreements on AI Safety: Review and Recommendations for a Conditional AI Safety Treaty
*Rebecca Scholefield; Samuel Martin; Otto Barten*  

**Quality Score:** 8/10

---

> ### üìä Quick Facts
>
> **Compute Threshold Benchmarks (FLOP):**
> *   **EU AI Act:** $10^{25}$
> *   **US EO 14110 / CA SB 1047:** $10^{26}$
> *   **Academic Proposals:** Range from $10^{21}$ (Miotti & Wasil) to $10^{24}$ (Trager)
>
> **Operational Metrics:**
> *   **Revision Frequency:** Monthly to Yearly
> *   **Evaluation Maturity:** Currently immature (insufficient for safety certification)
>
> **Core Mechanism:** Compute-Centric Governance Architecture

---

## üìù Executive Summary

The rapid advancement of general-purpose AI (GPAI) introduces catastrophic risks, ranging from societal marginalization to human extinction, creating an urgent need for cohesive international regulatory frameworks. This paper addresses the current fragmentation in AI governance, where diverse stakeholder proposals lack a unified mechanism for enforcement or risk mitigation. The authors emphasize that while there is general agreement on the necessity of oversight, significant disagreements regarding specific structures and operational processes currently hinder the ability to manage these existential threats effectively.

To resolve this fragmentation, the authors propose a **Conditional AI Safety Treaty** utilizing a compute-centric governance architecture, which establishes training computation (FLOP) as a proxy for model capability based on established scaling laws. The framework operates through three layers: a Trigger layer defined by dynamic compute thresholds to account for algorithmic efficiency; an Evaluation layer involving rigorous red-teaming for dangerous capabilities; and an Institutional layer composed of an international network of AI Safety Institutes (AISIs). By combining hard law treaties with framework conventions, this design creates an adaptive policy mechanism capable of mandating audits and enforcing pauses in development if risks are deemed unacceptable.

Through a systematic literature review and comparative assessment of recent proposals (2023‚Äì), the authors identified substantial variance in regulatory benchmarks and established consensus on feasibility requirements. Quantitative analysis shows a significant split: academic thresholds range from $10^{21}$ to $10^{24}$ FLOP, while legislative frameworks like the EU AI Act and US Executive Order 14110 target higher thresholds between $10^{25}$ and $10^{26}$ FLOP. Beyond these metrics, the study finds that treaty feasibility relies on consensus across five critical processes: building scientific consensus, standardization, auditing, verification, and incentivization.

This research significantly advances the field by bridging the gap between theoretical safety principles and actionable international law, offering a pragmatic blueprint for mitigating global AI risks. Its focus on compute thresholds provides a technically grounded enforcement mechanism that can adapt to scientific progress. Crucially, the study highlights a critical maturity gap, concluding that current evaluation methods remain insufficient to provide definitive safety certification; thus, the proposed institutional authority granted to AISIs is essential to close this gap.

---

## üîç Key Findings

*   **Existential Risk:** Advanced general-purpose AI (GPAI) poses severe risks, including the potential marginalization or extinction of humanity, necessitating urgent international regulatory frameworks.
*   **Review Consensus:** A review of recent proposals (2023-) reveals consensus and disagreement regarding regulatory structures, though five key processes (**scientific consensus, standardisation, auditing, verification, and incentivisation**) are central to feasibility.
*   **Compute Thresholds:** Establishing a treaty based on a specific compute threshold is a viable mechanism to trigger rigorous oversight.
*   **Institutional Authority:** Effective safety measures require an international network of AI Safety Institutes (AISIs) with authority to mandate audits.
*   **Governance Power:** The governance structure must possess the power to **pause AI development** if identified risks are deemed unacceptable.

---

## üìã Methodology

The authors conducted a **systematic literature review** of recent proposals for international AI safety agreements (focusing specifically on 2023 onwards). They performed a **comparative assessment** to identify specific areas of consensus and disagreement regarding risk thresholds, regulations, and agreement types. The feasibility of these proposals was evaluated by drawing on related literature and focusing on five critical processes:
1.  Building scientific consensus
2.  Standardisation
3.  Auditing
4.  Verification
5.  Incentivisation

---

## ‚ú® Contributions

*   **Conditional AI Safety Treaty:** Proposal of a treaty that ties oversight requirements to a compute threshold, offering a clear metric for regulation.
*   **Governance Framework:** Outline of a framework involving an international network of AI Safety Institutes (AISIs) tasked with complementary audits and enforcement.
*   **Mandatory Requirements:** Definition of specific mandatory requirements for the treaty, including model auditing, information security, and governance practice standards.
*   **Adaptive Policy Design:** Introduction of an adaptive policy design combining immediately implementable measures with the capacity to adapt to ongoing scientific research and evolving risks.

---

## üõ† Technical Details

### Compute-Centric Governance Architecture
The paper outlines a governance structure that uses training computation (**FLOP**) as a proxy for model capability based on scaling laws.

**Regulatory Layers:**
1.  **Trigger Layer:** Based on compute thresholds.
2.  **Evaluation Layer:** Red-teaming for dangerous capabilities (e.g., cyber-offense, self-proliferation).
3.  **Institutional Layer:** AI Safety Institutes (AISIs).

**Key Features:**
*   **Dynamic Thresholding:** Designed to handle algorithmic efficiency improvements.
*   **Enforcement Instruments:** Utilizes a mix of:
    *   **Hard Law** (Treaties)
    *   Framework Conventions
    *   Soft Law

---

## üìà Results

**Benchmark Variance:**
There is a notable discrepancy between legislative and academic benchmarks regarding compute limits.

| Proposal Type | Source / Proposal | Compute Threshold (FLOP) |
| :--- | :--- | :--- |
| **Legislative** | EU AI Act | $10^{25}$ |
| **Legislative** | US EO 14110 / CA SB 1047 | $10^{26}$ |
| **Academic** | Trager | $10^{24}$ |
| **Academic** | Bilge | $10^{23}$ |
| **Academic** | Miotti and Wasil | $10^{21}$ |

**Operational & Maturity Metrics:**
*   **Revision Cycles:** Operational metrics suggest regulatory metrics require revision frequencies ranging from **once a year to every couple of months**.
*   **Evaluation Maturity:** Current evaluations are **immature** and cannot currently provide safety certification for high-risk models.

---
**REFERENCES:** 0 citations