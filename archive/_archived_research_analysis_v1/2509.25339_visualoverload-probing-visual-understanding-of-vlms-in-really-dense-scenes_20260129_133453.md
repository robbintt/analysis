# VisualOverload: Probing Visual Understanding of VLMs in Really Dense Scenes

*Paul Gavrikov; Wei Lin; M. Jehanzeb Mirza; Soumya Jahagirdar; Muhammad Huzaifa; Sivan Doveh; Serena Yeung-Levy; James Glass; Hilde Kuehne*

---

> **### ðŸ“Š Quick Facts**
> * **Dataset Size:** 2,720 QA pairs from 150 artworks
> * **Resolution:** High-resolution (>4K)
> * **Models Evaluated:** 37 State-of-the-art VLMs
> * **Top Performing Model:** o3 (69.5% Overall / 19.6% Hard Split)
> * **Primary Bottleneck:** Token compression in vision encoders

---

### EXECUTIVE SUMMARY

Current Vision-Language Models (VLMs) are frequently evaluated on benchmarks that rely on global image understanding or prior world knowledge, leading to an **overestimation of their true visual perception capabilities**. Specifically, these models struggle to process "visually overloaded" scenesâ€”dense, high-resolution environments containing vast amounts of fine-grained information. This paper addresses a critical architectural bottleneck where vision encoders compress high pixel counts into a fixed number of tokens, inevitably discarding the nuances necessary for detailed analysis. The inability to reliably process these details undermines the application of VLMs in complex, real-world scenarios requiring precise visual discrimination rather than general semantic summarization.

The authors introduce **VisualOverload**, a benchmark specifically designed to isolate visual perception from world knowledge using "knowledge-free" vision tasks. The dataset comprises 2,720 manually annotated question-answer pairs derived from 150 high-resolution artworks (exceeding 4K resolution) sourced from Google Arts & Culture.

Evaluations demonstrate a dramatic performance collapse as scene density and complexity increase. While the top-performing model, **o3**, achieved a respectable overall accuracy of 69.5%, its efficacy plummeted to **19.6% on the hard split**, highlighting significant fragility in dense reasoning. This research exposes a substantial gap between the perceived and actual capabilities of current VLMs, shifting the focus of the field from global understanding to fine-grained, localized reasoning.

---

## ðŸ” Key Findings

*   **Significant Performance Degradation:** State-of-the-art VLMs struggle significantly with dense scenes; even the best model (o3) achieved only **19.6% accuracy** on the hardest split.
*   **Overestimation of Current Capabilities:** Existing VQA benchmarks likely overestimate true visual reasoning capabilities as they focus on global understanding rather than fine-grained detail.
*   **Specific Failure Modes:** Models consistently fail at **counting tasks**, **Optical Character Recognition (OCR)**, and maintaining **logical consistency** in dense environments.
*   **Difficulty with Detail Encoding:** Encoding and reasoning over fine details within 'overloaded' visual environments remains a distinct challenge due to token compression.
*   **Necessity of Visual Input:** Blind benchmarks (text-only) confirmed that while visual input is necessary, it is currently insufficient for complex reasoning tasks.

---

## ðŸ› ï¸ Methodology

*   **Dataset Construction:** Introduced VisualOverload, a benchmark comprising 2,720 question-answer pairs based on high-resolution scans of complex public-domain paintings.
*   **Task Definition:** Focused on 'simple, knowledge-free vision tasks' to strictly isolate visual perception capabilities from language priors or world knowledge.
*   **Annotation Protocol:** Images were manually annotated with questions spanning six distinct task categories.
*   **Evaluation Protocol:** Evaluated 37 different VLMs against a privately held ground-truth set. Analysis focused on comparing performance on a 'hardest test split' versus overall performance to measure robustness.

---

## âš™ï¸ Technical Details

### Dataset Specifications
*   **Source:** 150 artworks (>4K resolution) from Google Arts & Culture.
*   **Composition:** 2,720 manually annotated, visually grounded question-answer pairs.
*   **Splits:** Divided based on model performance difficulty:
    *   Easy: 986 pairs
    *   Medium: 1,304 pairs
    *   Hard: 430 pairs

### Task Categories
The benchmark covers six distinct categories to test different aspects of visual perception:
1.  Activity Recognition
2.  Attribute Recognition
3.  Counting
4.  Optical Character Recognition (OCR)
5.  Visual Reasoning
6.  Scene Classification

### Evaluation Metrics & Architecture
*   **Question Format:** Multiple-choice or binary yes/no pairs.
*   **Scoring:** Binary questions require both options to be correct for scoring.
*   **Architectural Bottleneck:** The approach identifies the vision encoder as the primary bottleneck. Compression of high pixel counts into a fixed number of tokens necessitates the loss of fine-grained details, preventing the model from 'seeing' everything in a dense scene.

---

## ðŸ“ˆ Results

The evaluation of 37 VLMs revealed a stark drop in performance as visual complexity increased:

*   **Overall vs. Hard Performance:** The best model (o3) dropped from **69.5% overall accuracy** to **19.6%** on the hard split.
*   **Blind Baselines:** Models averaged **22.8â€“29.2%** accuracy on blind benchmarks (without images), compared to a 'Consistent Chance' baseline of 27.2%. This confirms that current models use visual input but rely on it heavily for basic tasks.
*   **Task-Specific Failures (Blind Analysis):**
    *   **OCR:** Near-total failure (**0.0â€“0.8%** accuracy).
    *   **Counting:** Extremely low accuracy (**8.8â€“18.1%**).
    *   **Visual Reasoning:** Moderate performance (**29.3â€“36.6%**).

These results indicate that current benchmarks overestimate capabilities and that models specifically fail at high-density tasks.

---

## âœ… Contributions

*   **Novel Benchmarking Resource:** Released the VisualOverload dataset to the public to test visual understanding in densely populated, high-resolution scenarios.
*   **Diagnostic Insights:** Provided a comprehensive error analysis categorizing model limitations, specifically pinpointing failures in counting, OCR, and logic.
*   **Gap Identification:** Identified and quantified a critical gap in current vision models regarding fine-grained detail processing, shifting the research focus from global image understanding to localized, dense reasoning.

---

**Quality Score:** 8/10  
**References:** 30 citations