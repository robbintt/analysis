---
title: A Convergence Analysis of Adaptive Optimizers under Floating-point Quantization
arxiv_id: '2510.21314'
source_url: https://arxiv.org/abs/2510.21314
generated_at: '2026-02-03T18:54:13'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# A Convergence Analysis of Adaptive Optimizers under Floating-point Quantization
*Xuan Tang; Jichu Li; Difan Zou*

---

> ### ðŸ“Š Quick Facts
> ---
> *   **Quality Score:** 8/10
> *   **Total Citations:** 40
> *   **Optimizers Analyzed:** Adam, Muon
> *   **Target Precision:** FP32 to BF16/FP8
> *   **Key Convergence Rate:** $\tilde{O}(T^{-1/4})$
> *   **Core Requirement:** Logarithmic scaling of mantissa length

---

## Executive Summary

### **Problem**
As Large Language Models (LLMs) scale, training them in low-precision formats (e.g., FP8 or BF16) has become essential for hardware efficiency. Despite the empirical success of this approach, a significant theoretical gap remains: existing convergence theories for adaptive optimizers assume exact arithmetic and fail to account for errors introduced by floating-point quantization. This paper addresses this disconnect by rigorously investigating how quantization errors in weights, gradients, and optimizer states impact the stability and convergence rates of adaptive optimization algorithms.

### **Innovation**
The authors introduce the **first theoretical framework** designed to analyze the convergence of adaptive optimizers under hardware-aware floating-point quantization. Technically, the methodology utilizes a quantization-aware error propagation framework that models the quantization of weights, gradients, and optimizer states (such as first and second moment estimates) independently. By treating quantization error as a relative error bound and assuming standard stochastic conditions for smooth non-convex objectives, the framework derives explicit convergence rates. This allows for the precise characterization of how errors from distinct componentsâ€”gradients, weights, and moment estimatesâ€”propagate through the system.

### **Results**
The study proves that both Quantized Adam and Quantized Muon can retain convergence rates matching their full-precision counterparts, provided the mantissa length scales logarithmically with the number of iterations. Specifically, Quantized Adam achieves a convergence rate of $\tilde{O}(T^{-1/4})$, contingent upon specific hyperparameter tuning: a learning rate of $\eta = \Theta(1/\sqrt{T})$ and a second-moment decay of $1 - \beta_2 = \Theta(1/T)$. The analysis establishes strict quantization error constraints for Adam, requiring gradients and first moments to decay at $O(1/T)$ and weights and second moments at $O(1/T^2)$. In contrast, Muon maintains its full-precision convergence rate under significantly weaker error control. Numerical experiments conducted on both synthetic data (e.g., Rosenbrock function) and real-world vision and language tasks validate these findings, demonstrating convergence stability with high mantissa precision (M16+) but oscillation and failure at low precision (M4) for Adam.

### **Impact**
This research bridges the gap between empirical practice and theoretical guarantee, providing a mathematical basis for why low-precision training is effective for modern LLMs. By highlighting that Adam is highly sensitive to quantization errors in second-moment estimatesâ€”attributed to the hyperparameter $\beta_2 \to 1$â€”while Muon requires significantly weaker error control, the paper offers actionable insights for optimizer selection. These findings guide the development of more robust training algorithms tailored for low-precision hardware, ensuring that computational efficiency gains do not come at the cost of model convergence.

---

## Key Findings

*   **Retention of Convergence Rates:** Both Adam and Muon optimizers can maintain convergence rates close to their full-precision counterparts, provided the mantissa length scales logarithmically with the number of iterations.
*   **Adam's Sensitivity:** Adam is highly sensitive to quantization errors in weights and second-moment estimates, a behavior attributed to its reliance on the hyperparameter $\beta_2 \to 1$.
*   **Muon's Robustness:** Muon requires weaker error control regarding quantization compared to Adam, making it potentially more robust for low-precision training.
*   **Error Characterization:** The study explicitly characterizes how quantization errors from distinct componentsâ€”gradients, weights, and optimizer statesâ€”impact overall convergence.
*   **Experimental Validation:** Numerical experiments conducted on both synthetic and real-world data corroborate the theoretical findings.

---

## Methodology

The authors developed a novel theoretical framework to analyze the convergence behavior of adaptive optimizers (specifically Adam and Muon) under floating-point quantization. This methodology involves:

1.  Modeling the quantization of gradients, weights, and optimizer states (e.g., moment estimates).
2.  Assuming standard stochastic gradient conditions for smooth non-convex objectives.
3.  Deriving explicit convergence rates to quantify the specific impact of quantization errors from different components.

---

## Technical Details

The analysis utilizes a specific set of technical assumptions and architectural definitions to achieve its results:

**Quantization Strategy**
*   **Target:** Floating-point quantization (e.g., FP32 to BF16/FP8).
*   **Method:** Truncating mantissa bits while preserving sign and exponent bits.
*   **Error Model:** Modeled as a relative error bound.

**System Architecture**
*   **Framework:** Master-worker analytical model.
    *   **Master:** Maintains full-precision weights but transmits quantized versions to workers; updates quantized optimizer states.
    *   **Workers:** Compute and quantize gradients.
*   **Independent Quantization Components:**
    *   Weights ($q_W$)
    *   Gradients ($q_G$)
    *   Optimizer States: First Moment ($q_M$) and Second Moment ($q_V$)

**Algorithms & Assumptions**
*   **Algorithms:** Adam (with decoupled weight decay) and Muon.
*   **Gradient Conditions:** Unbiased stochastic gradients, $\ell_\infty$ boundedness, and L-smoothness.

---

## Results

**Theoretical Results**
*   The paper proves that **Quantized Adam** achieves a convergence rate of $\tilde{O}(T^{-1/4})$, matching the rate of full-precision Adam.
*   **Hyperparameter Scaling:**
    *   Learning rate $\eta = \Theta(1/\sqrt{T})$
    *   Second moment decay $1 - \beta_2 = \Theta(1/T)$
*   **Quantization Error Constraints:**
    *   Gradients and first moments must decay at $O(1/T)$.
    *   Weights and second moments require stricter decay at $O(1/T^2)$.

**Empirical Observations**
*   **Rosenbrock Function:**
    *   **High Precision (M16+):** Correlates with convergence stability and lower gradient norms.
    *   **Low Precision (M4):** Results in large oscillations and failure to settle.

---

## Contributions

*   **First Theoretical Framework:** Introduction of the first theoretical framework designed to analyze the convergence of adaptive optimizers while accounting for hardware-aware floating-point quantization, addressing a previous gap where theories assumed exact components.
*   **Theoretical Explanation for Low-Precision Success:** Providing a theoretical basis for why low-precision training is empirically effective for Large Language Models (LLMs), narrowing the gap between practice and theory.
*   **Comparative Robustness Analysis:** Offering theoretical insights that differentiate the robustness of specific optimizers (Adam vs. Muon), informing the selection of optimizers for low-precision hardware environments.