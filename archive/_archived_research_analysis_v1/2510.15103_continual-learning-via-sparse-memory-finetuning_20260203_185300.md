---
title: Continual Learning via Sparse Memory Finetuning
arxiv_id: '2510.15103'
source_url: https://arxiv.org/abs/2510.15103
generated_at: '2026-02-03T18:53:00'
quality_score: 9
citation_count: 4
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Continual Learning via Sparse Memory Finetuning
*Jessy Lin; Luke Zettlemoyer; Gargi Ghosh; Wen-Tau Yih; Aram Markosyan; Vincent-Pierre Berges; Barlas OÄŸuz*

---

> ### ðŸ“Š Quick Facts
>
> * **Model Size:** 1.3 Billion Parameters
> * **Architecture:** Memory Layer with 1 Million Keys
> * **Primary Metric:** NaturalQuestions F1 Score Drop
> * **Performance:** **11%** drop (Sparse) vs. **89%** (Full FT) vs. **71%** (LoRA)
> * **Optimization:** SGD used for sparse updates vs. AdamW for baselines

---

## Executive Summary

Large Language Models (LLMs) face a significant challenge known as **catastrophic forgetting** when adapting to new tasks sequentially. As models are fine-tuned on fresh data, they tend to overwrite previously acquired knowledge, resulting in a steep decline in performance on earlier tasks. This phenomenon highlights the **stability-plasticity dilemma**: models require plasticity to integrate new information but stability to retain existing skills. Standard full fine-tuning and popular parameter-efficient methods like LoRA often fail to balance these needs, leading to substantial interference that renders dynamic model updates impractical for real-world applications where continuous learning is required.

The paper introduces **"Sparse Memory Finetuning,"** a novel technique designed to isolate new knowledge acquisition to prevent interference with pre-trained representations. The core architectural modification replaces a standard Transformer Feed-Forward Network (FFN) with a "Memory Layer" containing a pool of 1 million keys. Utilizing product keys for efficient retrieval, the system accesses only the most relevant slots (32 keys per token per head). To update these slots, the method employs a **TF-IDF ranking algorithm** to identify memory slots highly activated by the current batch relative to a background corpus. Gradient masking is then applied to update only these specific, highly relevant slots, ensuring that new information is written into underutilized areas of the network.

Sparse Memory Finetuning demonstrated superior performance in mitigating forgetting compared to standard baselines. In a sequential learning experiment involving 1,000 facts from TriviaQA, the proposed method achieved only an **11% drop** in F1 score on NaturalQuestions. This contrasts sharply with standard full fine-tuning, which saw an 89% drop, and LoRA, which experienced a 71% drop. Crucially, this retention of prior knowledge did not come at the cost of learning new information; the model acquired new facts to the same degree or better than full fine-tuning while maintaining stable performance on held-out benchmarks like GSM8K and HellaSwag. Additionally, the authors found that switching the optimizer from AdamW to SGD further reduced forgetting.

This research establishes sparsity in memory layers as a promising structural solution for the continual learning of LLMs. By demonstrating that updating specific memory slots relative to pretraining usage effectively resolves the stability-plasticity dilemma, the authors offer a viable path for models to adapt over time without losing foundational capabilities. This approach shifts the paradigm from dense weight updates, which cause high interference, to sparse, targeted modifications, influencing future research on lifelong learning systems and the deployment of models that require frequent, non-destructive updates.

---

## Key Findings

*   **Drastic Reduction in Forgetting:** Achieved only an **11% drop** in NaturalQuestions F1 score compared to an 89% drop with full finetuning and a 71% drop with LoRA.
*   **Effective Knowledge Acquisition:** Maintained effective knowledge acquisition at baseline levels, learning new facts to the same degree or better than full finetuning.
*   **Superior Performance:** Sparse memory finetuning outperforms both standard full finetuning and LoRA in mitigating forgetting on question answering tasks.
*   **Sparsity as a Mechanism:** Confirms that leveraging sparsity in memory layers is a promising mechanism to reduce interference between new and previously acquired knowledge.

---

## Methodology

The proposed method leverages **'memory layer models'** designed to accommodate sparse updates. The core process involves:

1.  **Selective Updates:** The system selectively updates only specific memory slots that are highly activated by new knowledge.
2.  **Selection Criteria:** Slot selection is based on activation levels relative to the slot's usage during pretraining to ensure minimal interference.
3.  **Evaluation:** The model was evaluated on two question answering tasks, comparing performance against full finetuning and LoRA to measure both knowledge acquisition and retention.

---

## Technical Details

The implementation of Sparse Memory Finetuning involves specific architectural choices and optimization strategies:

*   **Parameter Efficiency:** Utilizes Sparse Memory Finetuning to update only the minimal set of parameters necessary.
*   **Architecture Modification:** Replaces the standard Transformer Feed-Forward Network (FFN) at **Layer 12** with a Memory Layer containing a pool of **1 million keys**.
*   **Retrieval Mechanism:** Employs **product keys** for efficient lookup, accessing only 32 keys per token per head (totaling **32,768 active parameters** per forward pass).
*   **Selection Algorithm:** A **TF-IDF ranking** selection algorithm identifies the top memory slots to update based on their relevance to the current batch compared to a background corpus.
*   **Gradient Masking:** Gradient masking is applied dynamically to protect non-selected parameters.
*   **Model Specs:**
    *   **Parameters:** 1.3B
    *   **Batch Size:** 64
    *   **Optimizer:** SGD (for sparse memory) vs. AdamW (for baselines)

---

## Results

The study highlights significant improvements in retaining knowledge while acquiring new information:

*   **Sequential Learning Task:** In a task involving the sequential learning of 1,000 facts from TriviaQA, Sparse Memory Finetuning achieved an **11% drop** in NaturalQuestions F1 score.
*   **Baseline Comparison:** This significantly outperformed Full Finetuning (89% drop) and LoRA (71% drop).
*   **Generalization:** The method maintained stable performance on held-out benchmarks including **NaturalQuestions**, **GSM8K**, and **HellaSwag**.
*   **Optimizer Impact:** Switching from AdamW to SGD for the sparse memory optimizer was found to further decrease forgetting.

---

## Contributions

*   **Novel Technique:** Introduction of Sparse Memory Finetuning, a new technique for continual learning in large language models using sparse parameter updates.
*   **Empirical Validation:** Provided empirical validation that updating specific memory slots relative to pretraining usage effectively mitigates the stability-plasticity dilemma.
*   **Structural Advancement:** Advanced the field of continual learning by integrating sparsity into memory layers as a structural solution for models to adapt over time without losing previously acquired skills.

---

**Quality Score:** 9/10  
**References:** 4 citations