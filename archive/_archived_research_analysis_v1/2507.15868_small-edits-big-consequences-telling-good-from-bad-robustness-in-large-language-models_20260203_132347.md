---
title: 'Small Edits, Big Consequences: Telling Good from Bad Robustness in Large Language
  Models'
arxiv_id: '2507.15868'
source_url: https://arxiv.org/abs/2507.15868
generated_at: '2026-02-03T13:23:47'
quality_score: 9
citation_count: 13
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Small Edits, Big Consequences: Telling Good from Bad Robustness in Large Language Models
*Altynbek Ismailov; Salia Asanova*

> ### **Quick Facts**
> * **Dataset Size:** 50 LeetCode problems
> * **Models Tested:** 6 frontier LLMs (including 3 reasoning-tuned)
> * **Total Generations:** 11,853 code generations
> * **Key Metric:** 85% accuracy retained despite 90% prompt deletion
> * **Quality Score:** 9/10
> * **References:** 13 citations

---

## Executive Summary

This research addresses the critical phenomenon of "over-robustness" in Large Language Models (LLMs), specifically identifying a **"double asymmetry"** where models treat benign noise and meaningful semantic edits with similar indifference. The study highlights a fundamental reliability issue: LLMs often rely on superficial anchoring cues and pattern-matching rather than comprehending underlying logic. This is problematic because standard evaluations assume that stability under perturbation equates to robustness; however, this research demonstrates that such stability often masks a harmful insensitivity to changes that should alter the model's output.

To isolate genuine reasoning from pattern-matching, the authors introduce a **"Framework of Differential Sensitivity"** and a benchmark based on 50 LeetCode coding problems. The methodology employs a 3x2 factorial perturbation design, testing three distinct perturbation families—Progressive Deletion (underspecification), Lexical Flips (semantic reversals), and Jargon Inflation—against two identifier modes: Unmasked (original names) and Masked (neutral tokens). A key technical intervention, **"Anchor Masking,"** involves obscuring salient function names to force models to re-evaluate context rather than relying on keyword correlations. Crucially, to objectively measure pass rates and distinguish between solution reuse and true adaptation, the study validates performance by executing the generated code against the problems' original test suites.

Evaluating six frontier models across 11,853 code generations revealed striking evidence of robustness failure. Models demonstrated extreme over-robustness to noise, maintaining an **85% pass rate** even after 90% of prompt words were deleted. Conversely, they exhibited harmful insensitivity to meaningful changes, reacting only **54% of the time** to pivotal quantifier flips and **44%** to jargon inflation. Notably, reasoning-tuned model variants showed lower sensitivity to semantic reversals compared to their base counterparts. Furthermore, masking salient anchors reduced pass rates by 15–21 percentage points and caused 30–40% disagreement in pass/fail outcomes, indicating divergent internal assumptions when superficial cues are removed.

The significance of this research lies in its empirical challenge to the assumption that prompt stability equates to model reliability. By quantifying these failures, the authors urge a paradigm shift in training protocols, proposing pipelines that condition models to remain steady under noise but adapt or refuse when semantics change. This study establishes a new standard for evaluating LLMs, emphasizing that future model development must prioritize semantic fidelity and differential sensitivity over mere robustness to perturbation.

---

## Key Findings

*   **Over-Robustness to Underspecification:** LLMs maintain **85% accuracy** even after 90% of prompt words are deleted.
*   **Harmful Insensitivity to Semantic Change:** Only **54%** of models reacted to pivotal quantifier flips that reversed task requirements.
*   **Reasoning-Tuned Model Deficiency:** Reasoning-tuned variants showed lower sensitivity to semantic reversals compared to base counterparts.
*   **The 'Double Asymmetry':** Models treat benign noise and meaningful edits with similar indifference.
*   **Effectiveness of Anchor Masking:** Masking salient anchors forces models to re-evaluate context, mitigating over-robustness.

---

## Technical Details

| Component | Specification |
| :--- | :--- |
| **Focus** | LLM code generation (LeetCode problems) |
| **Architecture** | Execution-based testing with Pass/Fail outcomes |
| **Design** | 3x2 factorial perturbation design |
| **Perturbation Families** | 1. **Progressive Deletion (PD):** Underspecification<br>2. **Jargon Inflation (JI):** Complexity increase<br>3. **Lexical Flip (LF):** Semantic reversals |
| **Identifier Modes** | 1. **Unmasked:** Original names<br>2. **Masked:** Neutral token 'solved' |
| **Dataset** | 50 problems selected from the LeetCodeDataset |
| **Models Evaluated** | GPT-4.1, Claude-Sonnet-4, Gemini-2.5-Flash, o4-mini-high, Claude-Sonnet-4-thinking, Gemini-2.5-Pro |
| **Configuration** | Temperature: 0.7, Decoding: Best-of-1 |

---

## Methodology

*   **Benchmark Construction:** Compiled a dataset of 50 LeetCode coding problems.
*   **Prompt Perturbation:** Designed three types of edits: progressive underspecification, lexical flips, and jargon inflation.
*   **Model Evaluation:** Tested six frontier LLMs (including three reasoning-tuned versions) generating Python code.
*   **Validation Mechanism:** Executed generated code against original test suites to check for solution reuse versus adaptation.
*   **Scale:** Analyzed a total of 11,853 code generations across different perturbation levels and model types.

---

## Results

The experiment involved 11,853 actual model-prompt pairs. Key performance indicators included:

*   **Noise Tolerance:** Models demonstrated over-robustness to noise, maintaining an **85% pass rate** (PD-Unmasked) even after 90% of prompt words were deleted.
*   **Semantic Sensitivity:** Conversely, they showed harmful insensitivity to semantics, reacting only **54%** of the time to quantifier flips (LF) and **44%** to jargon inflation (JI).
*   **Impact of Masking:** Masking function names reduced pass rates by **15–21 percentage points** across all families.
*   **Reasoning Variants:** Reasoning-optimized variants exhibited lower sensitivity to semantic reversals compared to base models.
*   **Internal Divergence:** When anchors were removed, models disagreed on pass/fail outcomes in **30–40%** of cases, indicating divergent internal assumptions.

---

## Contributions

*   **Framework of Differential Sensitivity:** Introduced a distinction between useful robustness and harmful insensitivity.
*   **Quantified Robustness Failures:** Provided empirical evidence that current LLMs prioritize prompt stability over semantic accuracy.
*   **Training Protocol Recommendations:** Proposed future pipelines condition models to stay steady under noise but adapt or refuse when semantics change.
*   **Identification of Anchoring Effects:** Highlighted the role of salient anchors in over-confidence and proposed masking as an intervention.