---
title: 'Sample, Don''t Search: Rethinking Test-Time Alignment for Language Models'
arxiv_id: '2504.03790'
source_url: https://arxiv.org/abs/2504.03790
generated_at: '2026-01-27T16:16:06'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Sample, Don't Search: Rethinking Test-Time Alignment for Language Models

*Allen Institute, Monte Carlo, Noah A. Smith*

---

### ðŸ“Š Quick Facts & Metrics

| Metric | Value / Detail |
| :--- | :--- |
| **Model Used** | TÃœLU3-8B-SFT |
| **Core Method** | QALIGN (MCMC-based Sampling) |
| **Quality Score** | 9/10 |
| **MATH500 Accuracy** | **54.2%** (vs. 45.1% Best-of-N) |
| **GSM8K Accuracy** | **91.5%** (vs. 89.3% DPO Baseline) |
| **IFEval Improvement** | +5.4 percentage points |
| **Requirements** | Black-box access (API compatible) |

---

## Executive Summary

### Problem
Current test-time alignment methodsâ€”predominantly search-based techniques like Best-of-Nâ€”suffer from **"reward hacking."** This phenomenon occurs when the over-optimization of imperfect reward proxies leads to distributional collapse and quality degradation. Additionally, traditional alignment approaches often require access to model internals (logits) or necessitate expensive parameter fine-tuning. This creates a significant barrier for aligning closed-source or proprietary models. The paper addresses the urgent need for a robust, inference-time alignment mechanism that improves output quality without modifying weights or relying on vulnerable search strategies.

### Innovation
The core innovation, **QALIGN**, reframes language model alignment from a search problem into a sampling challenge. It formulates alignment as **Bayesian inference** to recover the optimal posterior distribution. Rather than greedily searching for a single optimum, QALIGN employs **Markov Chain Monte Carlo (MCMC)**â€”specifically the Metropolis-Hastings algorithmâ€”to iteratively refine outputs under the guidance of a reward model.

The framework introduces a **QUEST proposal mechanism** to generate suffixes conditioned on prefixes. It calculates acceptance probabilities using reward ratios and sequence lengths to avoid computing intractable partition functions. Crucially, this "black-box" approach requires only the ability to sample from the language model and query a reward model, eliminating the need for gradient access or parameter updates.

### Results
Experiments using the TÃœLU3-8B-SFT model demonstrated that QALIGN significantly outperforms standard test-time compute baselines and even training-based methods:

*   **MATH500:** Achieved **54.2%** accuracy (Best-of-N: 45.1%; DPO: 48.3%).
*   **GSM8K:** Reached **91.5%** accuracy (Base: 82.1%; DPO: 89.3%).
*   **IFEval:** Improved instruction-following scores by **5.4 percentage points** over the strongest search-based baseline.

Crucially, as inference FLOPS increased, QALIGN's error rate decreased monotonically, avoiding the performance plateaus or degradation observed in competing methods at equivalent compute budgets.

### Impact
This work establishes a new state-of-the-art for test-time compute by proving that **MCMC-based sampling** offers a more principled and scalable path to alignment than maximizing explicit reward signals. By solving the over-optimization bottleneck, QALIGN provides a mechanism to unalign and realign models at the point of inference. This significantly expands the ability to improve private or proprietary models without fine-tuning, suggesting a paradigm shift where inference-time computation can effectively substitute forâ€”or even exceedâ€”the performance gains of traditional parameter-efficient training methods.

---

## Key Findings

*   **Quality Degradation:** Current test-time search methods suffer from quality degradation due to the over-optimization of imperfect reward proxies.
*   **Scalability:** QALIGN improves output quality as test-time compute scales, mathematically converging toward the optimal aligned distribution.
*   **Superior Performance:** QALIGN consistently outperforms existing test-time compute methods (Best-of-N, Majority Voting) on mathematical reasoning tasks.
*   **Surpassing Training:** QALIGN surpasses training-based methods like DPO across diverse datasets when utilizing TÃœLU3 preference dataset Reward Models (RMs).
*   **Black-Box Alignment:** Effective alignment is achievable at test time without model finetuning, logit access, or weight modifications.

---

## Methodology

**QALIGN** is a test-time alignment framework that fundamentally shifts the approach to alignment from search to sampling.

*   **Reframing the Challenge:** It views alignment as a sampling challenge rather than a search problem.
*   **Iterative Refinement:** It leverages Markov chain Monte Carlo (MCMC) for text generation to iteratively refine outputs.
*   **Guidance:** The refinement process is executed under the guidance of a reward model.
*   **Mathematical Convergence:** The method is designed to converge mathematically to the optimal aligned distribution as test-time computation increases.
*   **Compatibility:** It is fully 'black-box' compatible, requiring only the ability to sample from the language model and query a reward model.

---

## Technical Details

QALIGN reframes language model alignment as Bayesian inference to find a posterior distribution $\pi(y|x)$. The following technical components enable this process:

*   **Algorithm:** Employs MCMC (Metropolis-Hastings) at test time.
*   **Proposal Mechanism:** Utilizes a **QUEST** proposal mechanism that generates suffixes conditioned on prefixes.
*   **Acceptance Probability:** Calculates acceptance probability using reward ratios and sequence lengths to bypass computing intractable partition functions.
*   **Output Selection:** Final outputs are selected via **Minimum Bayes Risk (MBR)**.
    *   *Mathematical Reasoning:* Utilizes exact match (equivalent to Majority Voting).
    *   *Open-Ended Generation:* Utilizes ROUGE scores.

---

## Contributions

The research makes three primary contributions to the field of AI alignment:

1.  **Solving Reward Hacking:** Identifies and solves the over-optimization (reward hacking) bottleneck in test-time compute by proposing a sampling-based alternative.
2.  **Expanding Access:** Expands access to alignment by removing requirements for finetuning or logit access. This allows for the improvement of private or proprietary models via API only.
3.  **Setting New Standards:** Establishes a new state-of-the-art in test-time compute, demonstrating that MCMC-based sampling outperforms standard search methods and parameter-efficient training methods across various tasks.

---

## Results

**Experimental Setup:**
*   **Base Model:** TÃœLU3-8B-SFT
*   **Datasets:** GSM8K, MATH500, IFEval
*   **Metric:** Performance measured against Inference FLOPS

**Performance Overview:**
*   **Consistent Victory:** QALIGN consistently outperformed baseline test-time methods (Best-of-n, Majority Vote, Weighted MV) and surpassed a DPO-finetuned model.
*   **Compute Efficiency:** As test-time compute increased, QALIGN's error rate consistently decreased without the plateauing or degradation seen in other methods.