# Near-optimal Regret Using Policy Optimization in Online MDPs with Aggregate Bandit Feedback

*Tal Lancewicki; Yishay Mansour*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Citations:** 40 references
> *   **Problem Setting:** Online Finite-Horizon MDPs with Aggregate Bandit Feedback
> *   **Known Dynamics Regret:** $\tilde{\Theta}(H^2\sqrt{SAK})$ (Optimal)
> *   **Unknown Dynamics Improvement:** Factor of $H^2 S^5 A^2$ better than previous SOTA

---

## Executive Summary

This research addresses **Online Markov Decision Processes (MDPs)** under the constraint of **Aggregate Bandit Feedback** (full-bandit feedback). In this challenging setting, an agent interacts with an environment over a finite horizon with adversarial, non-stochastic losses but does not observe the loss associated with individual state-action pairs. Instead, the agent receives only the total loss summed over the entire trajectory at the end of each episode. This problem is significant because it models realistic scenarios where granular feedback is unavailable or costly to obtain, presenting a substantially harder learning problem than standard semi-bandit or full-information settings. Prior to this work, achieving optimal regret bounds with this level of information restriction remained an open question, particularly regarding dependency on the state space ($S$), action space ($A$), and horizon ($H$).

The core technical innovation is the introduction of novel Policy Optimization algorithms specifically designed for aggregate feedback. The authors define a **"U-function,"** which represents the expected cost of the entire trajectory triggered by a specific state-action pair. This function replaces the traditional Q-function (which focuses on immediate rewards) and enables a new decomposition of regret. The algorithmic framework utilizes a **Multiplicative Weight Update (MWU)** mechanism combined with an estimator for the U-function and a specific bonus term to correct for distribution mismatch between the policy used for data collection and the current policy.

The paper achieves substantial theoretical improvements in regret bounds for both known and unknown dynamics settings. For known dynamics, the algorithm achieves a regret bound of $\tilde{O}(H^2\sqrt{SAK})$, which matches the lower bound of $\tilde{\Theta}(H^2\sqrt{SAK})$, thereby establishing the first optimal result for this setting. In the more complex unknown dynamics setting, the algorithm achieves a regret bound of $\tilde{O}(H^3 S \sqrt{AK})$. This result represents a major advancement over previous state-of-the-art methods, improving the dependency on problem parameters by a factor of $H^2 S^5 A^2$ and matching the best-known results typically associated with the less restrictive semi-bandit feedback setting.

---

## Key Findings

*   **Optimal Regret for Known Dynamics:** Achieves the first optimal regret bound of $\tilde{\Theta}(H^2\sqrt{SAK})$ for environments with known transition dynamics.
*   **State-of-the-Art for Unknown Dynamics:** Achieves a regret bound of $\tilde{O}(H^3 S \sqrt{AK})$ for unknown dynamics, improving upon previous state-of-the-art by a factor of $H^2 S^5 A^2$.
*   **Robust Performance:** Demonstrates effective performance even when dealing with adversarial losses and restricted aggregate trajectory data.

---

## Methodology

The research introduces the first **Policy Optimization algorithms** specifically designed for **Aggregate Bandit Feedback** (full-bandit). In this framework, learning is based on the total loss observed over an entire trajectory rather than per-step rewards.

The approach adapts the optimization strategy to address two distinct settings:
1.  **Known Dynamics:** Leveraging exact knowledge of the environment's transition probabilities.
2.  **Unknown Dynamics:** Navigating environments where the transition model must be learned concurrently with the policy.

---

## Technical Details

The paper addresses Online Markov Decision Processes (MDPs) with adversarial, non-stochastic losses. The specific challenge is **Aggregate Bandit Feedback**, where only the total trajectory loss is observed.

### Core Innovation: The U-Function
*   **Definition:** A function defined as the expected cost of the entire trajectory given a specific state-action pair.
*   **Purpose:** It replaces standard Q-functions (which focus on immediate returns) to enable a novel decomposition of regret suitable for aggregate feedback.

### Algorithmic Framework
*   **Mechanism:** Utilizes a **Multiplicative Weight Update (MWU)** approach.
*   **Components:**
    *   An estimator for the U-function.
    *   A bonus term designed to correct distribution mismatch between the data collection policy and the current policy.

### Algorithm Variants
*   **Algorithm 1 (Known Dynamics):** Uses exact occupancy measures to calculate the optimal policy updates.
*   **Algorithm 2 (Unknown Dynamics):** Incorporates uncertainty estimation using:
    *   Bernstein-style confidence sets.
    *   Pessimistic occupancy bounds to handle the lack of environment knowledge.

---

## Contributions

*   **Novel Algorithms:** Provides the first Policy Optimization algorithms capable of handling online finite-horizon MDPs with aggregate bandit feedback.
*   **Closing the Gap:** Establishes the optimal regret bound for the known-dynamics setting, closing a theoretical gap in the literature.
*   **Theoretical Efficiency:** Achieves state-of-the-art advancement in the unknown-dynamics case by drastically improving theoretical efficiency and reducing the dependency on state, action, and horizon variables.

---

## Results

The primary metrics are theoretical **Regret Bounds ($R_K$)**.

### ðŸ“ˆ Known Dynamics (Theorem 2)
*   **Achieved Bound:** $\tilde{O}(H^2\sqrt{SAK} + H^3\sqrt{K})$
*   **Significance:** This is the first optimal bound for this setting. It improves upon standard linear bandit reductions by a factor of $\sqrt{SA}$.

### ðŸ“ˆ Unknown Dynamics (Theorem 3)
*   **Achieved Bound:** $\tilde{O}(H^3 S \sqrt{AK} + H^4 S^3 A)$
*   **Significance:** Represents a significant improvement over the previous state-of-the-art.
    *   Improves dependency parameters by a factor of $H^2 S^5 A^2$.
    *   Matches the best-known results for semi-bandit feedback (a less restrictive feedback setting).