# Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms

*Tyler Loakman; Joseph James; Chenghua Lin*

---

## ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Dataset Size** | 4,068 Isolated Words |
| **Source Material** | Oxford 5000 List |
| **Task Type** | 4-Option Multiple Choice (MCQ) |
| **Input Modalities** | Spectrograms, Waveforms |
| **Performance** | Near chance levels (Zero-shot & Finetuned) |
| **Quality Score** | **9/10** |

---

## ðŸ“ Executive Summary

> **Core Problem:** This research addresses a critical gap in the capabilities of Vision Language Models (VLMs): their inability to interpret specialized scientific visualizations, specifically spectrograms and waveforms, as representations of speech.

While VLMs demonstrate strong performance in general visual-linguistic tasks, it was unclear whether this capability extends to technical domains requiring specific parametric knowledge of acoustics and phonetics. Understanding whether VLMs can "read" acoustic data is essential for applications in speech processing, accessibility, and scientific analysis.

**The Approach:** The authors introduce a rigorous benchmarking framework and a specialized dataset of 4,068 English words. The technical innovation lies in the design of a 4-option Multiple Choice Question (MCQ) task where models must identify the correct transcription from algorithmically generated distractors. These distractors are created using Weighted Feature Edit Distance based on phonological features, with constraints enforcing <50% phonemic overlap to prevent bias.

**The Outcome:** The experiments reveal a fundamental failure in current VLM architectures to process acoustic visual data. Across both zero-shot and finetuned settings, models performed at or near chance levels. The inclusion of waveform data alongside spectrograms did not yield significant improvements, nor did fine-tuning.

**Conclusion:** Access to large-scale paired vision and language data is insufficient for acquiring specific, technical domain knowledge such as acoustic-phonetics. This work establishes a boundary condition for VLM applicability, proving that general visual reasoning does not equate to the ability to interpret technical visualizations.

---

## ðŸ”‘ Key Findings

*   **Performance at Chance:** VLMs consistently perform at or near chance levels when interpreting speech through visual representations like spectrograms and waveforms.
*   **Training Inefficacy:** Both zero-shot and finetuned models fail to robustly predict phonemic or graphemic transcriptions.
*   **Data vs. Knowledge:** Access to large amounts of paired vision and language data is insufficient; specific parametric knowledge is required.
*   **Lack of Expertise:** Current VLMs do not inherently possess the capabilities of highly-trained phoneticians.

---

## ðŸ› ï¸ Methodology

The study employed a systematic approach to evaluate the acoustic-phonetic knowledge of Vision Language Models:

*   **Dataset Creation:** A dataset of over 4,000 English words was created, featuring paired spectrograms and waveforms.
*   **Benchmark Design:** A multiple-choice benchmark was designed requiring models to select the correct transcription from one ground truth and three algorithmically generated distractors.
*   **Distractor Logic:** Distractors were generated based on phonemic edit distance to ensure a challenging but fair test.
*   **Evaluation Conditions:** Models were evaluated on both phonemic and graphemic transcription targets under zero-shot and finetuned conditions.

---

## âš™ï¸ Technical Details

**Dataset & Synthesis**
*   **Source:** 4,068 isolated words sourced from the *Oxford 5000 list*.
*   **Audio Synthesis:** Generated via *Microsoft SpeechT5*.
*   **Train/Dev/Test Split:** 80-10-10 split, stratified by word length in phonemes.

**Visual Representations**
*   **Spectrograms:** Generated with `n_fft=128`, `hop_length=22`, `70dB` range.
*   **Waveforms:** Standard visual representations of the audio signal.

**Task Framework**
*   **Format:** 4-option Multiple Choice Question (MCQ).
*   **Distractor Generation:** Selected using *Weighted Feature Edit Distance* based on phonological features.
*   **Bias Mitigation:**
    *   Enforced <50% phonemic overlap.
    *   Ensured different starting graphemes.
    *   Permutated answer positions.
*   **Experimental Matrix:** 2x2 design varying input modality (Spectrogram vs. Spectrogram+Waveform) and output modality (Graphemic vs. Phonemic).

---

## ðŸ“ˆ Results

*   **Consistent Failure:** VLMs perform at or near chance levels in both zero-shot and finetuned settings.
*   **Robustness:** Models failed to robustly predict phonemic or graphemic transcriptions.
*   **Baseline Adjustments:** Early testing indicated a baseline guessing accuracy of 50% without input images due to minimal pairs, which necessitated specific distractor constraints.
*   **Modality Impact:** The addition of waveform data to spectrograms did not improve performance.

---

## ðŸ“Œ Contributions

1.  **Rigorous Benchmark:** Provided a standardized benchmark for evaluating VLM capacity to translate acoustic visual data into linguistic structures.
2.  **Specialized Dataset:** Contributed a dataset of 4,000+ words with standardized visual speech representations.
3.  **Architectural Insight:** Highlighted a fundamental limitation in current VLM architectures, showing that general visual-linguistic correlation does not equate to specific domain knowledge needed for technical interpretation.

---

**References:** 11 citations
**Quality Score:** 9/10