# Ensuring Calibration Robustness in Split Conformal Prediction Under Adversarial Attacks

*Xunlei Qian; Yue Xing*

<details>
<summary><strong>üìä Quick Facts</strong></summary>

| Metric | Detail |
| :--- | :--- |
| **Dataset** | CIFAR-10 |
| **Target Coverage** | $1-\alpha = 0.9$ |
| **Attack Method** | $l_\infty$-FGSM |
| **Perturbation Bound** | $\epsilon = 8/255$ |
| **Score Function** | HPS ($S(x, y) = 1 - f_y(x)$) |
| **Training Method** | Adversarial Training (AT) |

</details>

---

## üìù Executive Summary

> **Problem Context:** Standard Split Conformal Prediction (SCP) relies on the exchangeability of calibration and test data to guarantee marginal coverage. This fundamental assumption is often violated in security-sensitive environments where models face adversarial attacks‚Äîmalicious perturbations designed to deceive the model. These attacks induce distribution shifts, causing standard conformal methods to fail in maintaining valid coverage. This creates a critical vulnerability for safety-critical applications, such as autonomous driving or security monitoring, which require rigorous uncertainty quantification under hostile conditions.
>
> **Methodology & Innovation:** To address this, the authors introduce a theoretical framework characterizing the relationship between adversarial perturbations applied during calibration and the resulting test-time coverage guarantees. Utilizing the Heteroscedastic Probabilistic Score (HPS), defined as $S(x, y) = 1 - f_y(x)$, they establish that introducing a "calibration attack" creates a predictable, monotonic relationship between perturbation strength and coverage decay. This insight enables the design of a mechanism for "contiguous robustness," ensuring the target coverage level ($1-\alpha$) is maintained across a continuous range of test-time perturbation intensities. The approach integrates adversarial training (AT) to harden the model without compromising the nonconformity score's utility.
>
> **Validation & Impact:** Experimental validation on the **CIFAR-10** dataset using **$l_\infty$-FGSM attacks** ($\epsilon = 8/255$) confirmed that the proposed method preserves valid coverage guarantees under attack, unlike standard SCP baselines. Furthermore, integrating **Adversarial Training (AT)** significantly improved efficiency, producing **tighter prediction sets** than non-robust baselines while meeting target coverage. This research advances trustworthy machine learning by formalizing a method to stabilize conformal prediction intervals, challenging the conventional trade-off narrative by demonstrating that robustness can enhance both validity and efficiency.

---

## üîë Key Findings

*   **Monotonic Coverage Behavior:** Prediction coverage varies monotonically relative to the strength of adversarial perturbations applied during calibration. This allows for predictable control of coverage rates even under attack.
*   **Contiguous Robustness:** With a suitably chosen calibration attack, target coverage can be maintained across a contiguous range of test-time perturbation levels, stabilizing the prediction intervals.
*   **Efficiency via Adversarial Training:** Employing adversarial training during the model-training stage yields tighter prediction sets (improved efficiency) while retaining high informativeness, debunking the idea that robustness strictly penalizes efficiency.

---

## üõ†Ô∏è Methodology

The study employs a dual approach combining theoretical characterisation and rigorous experimental validation:

1.  **Theoretical Analysis:** The authors define how the magnitude of adversarial perturbations introduced during the calibration phase impacts coverage guarantees when the model encounters adversarial inputs at test time.
2.  **Adversarial Training Effects:** The methodology examines the specific effects of adversarial training on model output, focusing on two main variables: prediction set size and coverage probability.
3.  **Validation:** These theoretical frameworks are validated through extensive experiments that measure coverage validity and prediction set size under varying conditions of adversarial attack strength.

---

## ‚öôÔ∏è Technical Details

*   **Framework:** Split Conformal Prediction (Split CP).
*   **Data Partitioning:** Data is split into distinct training, calibration, and test sets.
*   **Nonconformity Score:** HPS (Heteroscedastic Probabilistic Score).
    *   Formula: $S(x, y) = 1 - f_y(x)$
*   **Prediction Set Construction:** Sets are constructed using the quantile of scores derived from the calibration set.
*   **Adversarial Assumptions:**
    *   **Objective:** Attacks aim to maximize model loss.
    *   **Theoretical Norm:** Assumes $l_2$-bounded perturbations.
    *   **Experimental Norm:** Utilizes $l_\infty$-bounded perturbations via the $l_\infty$-FGSM algorithm.

---

## üèÜ Contributions

*   **Addressing Limitations:** The study directly addresses the limitation of standard SCP, which relies on exchangeability often violated by distribution shifts like adversarial perturbations.
*   **Theoretical Linking:** It provides a theoretical link between calibration-time robustness and test-time validity, offering a concrete mechanism to stabilize conformal prediction intervals against adversarial attacks.
*   **Efficiency & Robustness:** Demonstrates that robustness does not necessarily come at the cost of efficiency; adversarial training can actually improve the informativeness (tightness) of prediction sets.

---

## üìà Results

The study evaluated two primary metrics: **Validity (Coverage)** and **Efficiency** (average prediction set size).

*   **Monotonic Decay:** Results indicate a clear monotonic coverage decay relative to calibration perturbation strength.
*   **Robustness Maintenance:** The proposed method achieved contiguous robustness, where target coverage was maintained across a range of test-time perturbation levels.
*   **Improved Efficiency:** Adversarial training proved superior to non-robust baselines, yielding tighter prediction sets (higher efficiency) while still retaining the necessary informativeness and validity.

---
**Quality Score:** 9/10