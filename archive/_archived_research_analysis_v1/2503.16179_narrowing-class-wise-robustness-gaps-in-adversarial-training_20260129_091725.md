# Narrowing Class-Wise Robustness Gaps in Adversarial Training
*Fatemeh Amerehi; Patrick Healy*

---

> ### ðŸ“Š Quick Facts
>
> | Metric | Value |
> | :--- | :--- |
> | **Overall Quality Score** | 9/10 |
> | **Adversarial Robustness Gain** | +53.50% (vs. Standard AT) |
> | **Clean Error Rate** | 26.70% |
> | **Class Imbalance Reduction** | 5.73% (Clean ImageNet) |
> | **Core Architecture** | ResNet-50 |
> | **Attack Method** | 10-step PGD ($\epsilon=0.03$) |

---

## ðŸ“„ Executive Summary

Standard adversarial training (AT) is the predominant method for defending against worst-case distribution shifts and adversarial attacks. However, this paper identifies a critical limitation: while improving robustness on average, current methodologies often degrade generalization on clean examples and create significant performance disparities across different classes. The authors investigate "spill-over effects," revealing that adversarial training disproportionately harms specific classes, resulting in a class-wise robustness gap. This renders models unreliable for real-world deployment where equitable performance is essential.

To address this, the authors propose **"enhanced labeling,"** a strategy integrating Label Augmentation directly into the Adversarial Training pipeline (referred to as **Adv+**). The innovation modifies loss function targets by constructing an augmented label vector: `Concat[(1 - Î´)y_i, Î´z_j]` (where $\delta = 0.03$). The model minimizes negative log-likelihood over this augmented vector using a ResNet-50 architecture.

The proposed **Adv+** model demonstrates substantial improvements. Achieving a PGD-40 error rate of **32.70%** (a 53.50% improvement over baseline), it successfully overcomes the typical robustness-accuracy trade-off, achieving a clean error rate of 26.70%. The approach also showed superior corruption robustness and effectively mitigated class imbalance, reducing the standard deviation of class-wise errors by up to 7.87% on ImageNet-ReaL. This research suggests that the robustness-accuracy trade-off is not inherent but a byproduct of standard procedures, positioning enhanced labeling as a potential standard for training fair, high-performance vision systems.

---

## ðŸ” Key Findings

*   **Trade-off Identification:** Standard adversarial training improves robustness against worst-case distribution shifts but often degrades generalization on clean examples and worsens performance imbalances across classes.
*   **Robustness Boost:** Incorporating enhanced labeling during training boosts adversarial robustness by **53.50%** compared to standard adversarial training.
*   **Class Fairness:** The use of enhanced labeling reduces class imbalance by **5.73%**, addressing the "spill-over effects" that disproportionately affect specific classes.
*   **Overcoming Trade-offs:** The proposed approach improves accuracy metrics in both clean and adversarial settings, successfully overcoming the typical robustness-accuracy trade-off.

---

## ðŸ› ï¸ Methodology

The authors explore the dynamics of adversarial training by analyzing its impact on overall and class-specific performance.

*   **Core Analysis:** Examining "spill-over effects" to understand how standard training disproportionately affects different classes.
*   **Intervention:** Comparing standard adversarial training against a modified regimen utilizing **enhanced labeling**.
*   **Evaluation Criteria:** Measuring improvements in three key areas:
    1.  Adversarial robustness.
    2.  Mitigation of class imbalances.
    3.  Generalization accuracy.

---

## âš™ï¸ Technical Details

The implementation relies on a modified training regimen applied to a ResNet-50 architecture.

**Model & Attack Strategy**
*   **Architecture:** ResNet-50
*   **Attack Generation:** 10-step Projected Gradient Descent (PGD)
*   **Constraint:** $L_{\infty}$ with $\epsilon = 0.03$

**Augmented Labeling Logic**
The augmented label is constructed to improve learning signals without losing original classification intent.
*   **Formula:** `Concat[(1 - Î´)y_i, Î´z_j]`
*   **Parameters:**
    *   $y_i$: Original one-hot label
    *   $z_j$: Transformation label
    *   $\delta$: 0.03
*   **Loss Function:** Minimizes negative log-likelihood over the augmented label vector.

**Training Hyperparameters**
*   **Optimizer:** Stochastic Gradient Descent (SGD)
*   **Momentum:** 0.9
*   **Learning Rate:** 0.01 (with cosine annealing decay)
*   **Batch Size:** 64
*   **Duration:** 10 epochs
*   **Hardware:** Single RTX-3080 GPU

---

## ðŸ“Š Performance Results

Comparative results between Standard Adversarial Training (**Adv**) and the proposed Enhanced Labeling method (**Adv+**).

| Metric | Standard Adv | Proposed Adv+ | Improvement |
> | :--- | :--- | :--- | :--- |
> | **PGD-40 Error Rate** | - | **32.70%** | **53.50%** |
> | **Clean Error Rate** | - | **26.70%** | **17.03%** |
> | **Corruption Robustness (mCE)** | 71.41% | **66.43%** | 6.97% |
> | **Imbalance (Clean ImageNet)** | - | Lower by | **5.73%** |
> | **Imbalance (ImageNet-ReaL)** | - | Lower by | **7.87%** |
> | **Imbalance (PGD-40)** | - | Lower by | **2.79%** |

*Note: Error Imbalance is measured by the Standard Deviation of class-wise errors.*

---

## ðŸŒŸ Key Contributions

1.  **Critical Examination:** Highlights how standard adversarial training disproportionately affects different classes, identifying specific class-wise robustness gaps.
2.  **Spill-over Effects:** Identifies and investigates the broader impact of adversarial training on class-specific performance metrics.
3.  **Optimization Strategy:** Proposes enhanced labeling as a superior alternative that simultaneously addresses robustness, clean accuracy, and class imbalance.

---

**References:** 6 citations
**Document Quality Score:** 9/10