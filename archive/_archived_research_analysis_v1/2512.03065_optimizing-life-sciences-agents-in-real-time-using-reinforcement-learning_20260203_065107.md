---
title: Optimizing Life Sciences Agents in Real-Time using Reinforcement Learning
arxiv_id: '2512.03065'
source_url: https://arxiv.org/abs/2512.03065
generated_at: '2026-02-03T06:51:07'
quality_score: 8
citation_count: 23
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Optimizing Life Sciences Agents in Real-Time using Reinforcement Learning

*Authors: Nihir Chadderwala*

---

> ### ðŸ“Š Quick Facts
>
> * **Quality Score:** 8/10
> * **References:** 23 Citations
> * **Satisfaction Lift:** 15â€“30% improvement over baselines
> * **Convergence Speed:** Optimizes after 20â€“30 queries
> * **Core Method:** Thompson Sampling (Contextual Bandits)
> * **Data Requirement:** Label-Free (Human-in-the-Loop)

---

## Executive Summary

Current life sciences agents rely heavily on static, rule-based architectures or resource-intensive supervised learning, limiting their ability to adapt to dynamic user needs without expensive ground-truth labeling. This paper addresses the critical challenge of optimizing agentic workflows in complex, safety-critical environments where data is non-stationary and user preferences vary. Specifically, it tackles the exploration-exploitation dilemma inherent in agentic systems, where models must balance trying new strategies against leveraging known successful actions to maximize efficiency and accuracy in high-stakes scientific domains.

The key innovation is a label-free, Human-in-the-Loop framework that integrates AWS Strands Agents with Thompson Sampling, a contextual bandit algorithm. This architecture moves beyond fixed orchestration by simultaneously optimizing three distinct dimensions of agent behavior: **Generation Strategy** (direct generation vs. chain-of-thought), **Tool Selection** (e.g., literature search engines vs. drug databases), and **Domain Routing** (e.g., pharmacology vs. clinical specialists). By adapting Reinforcement Learning from Human Feedback (RLHF) principles to the agent orchestration level, the system learns optimal policies solely from implicit user feedback, effectively handling cold-start issues and non-stationarity without requiring pre-labeled training datasets.

Empirical results demonstrate that the framework significantly outperforms random baselines, achieving a 15â€“30% improvement in user satisfaction on complex life science queries. The system exhibits rapid convergence, identifying optimal decision-making strategies after processing only 20â€“30 queries. Validation across diverse tasksâ€”including factual lookups, complex reasoning, literature research, and safety-critical operationsâ€”confirmed that the model successfully maintains high performance while reducing dependency on costly data annotations.

This research represents a significant shift toward adaptive, self-optimizing AI systems in specialized domains, proving that agent-level decision-making can be effectively automated without traditional supervised methods. By validating a multi-dimensional optimization approach that relies exclusively on user feedback, the work reduces the operational costs associated with data labeling and manual tuning. The open-sourced framework provides a scalable, principled solution for deploying dynamic agents that can continuously evolve alongside changing user requirements and scientific knowledge.

---

## Key Findings

*   **Significant Satisfaction Improvement:** The framework achieved a **15-30% improvement** in user satisfaction compared to random baselines on life science queries.
*   **Rapid Learning Convergence:** The system demonstrated effective optimization after only **20-30 queries**.
*   **Label-Free Operation:** The system successfully learns optimal strategies without requiring ground truth labels, relying exclusively on user feedback.
*   **Effective Exploration-Exploitation Balance:** The approach provides a principled solution to the exploration-exploitation dilemma inherent in agentic AI systems.

---

## Methodology

The researchers developed a novel framework that integrates **AWS Strands Agents** with **Thompson Sampling contextual bandits**. This architecture enables agents to learn optimal decision-making strategies from user feedback in real-time without pre-labeled training data.

The system optimizes decision-making across three specific dimensions:

1.  **Generation Strategy:** Selecting between direct generation or chain-of-thought reasoning.
2.  **Tool Selection:** Choosing appropriate resources such as literature search engines or drug databases.
3.  **Domain Routing:** Directing queries to specific domains like pharmacology, molecular biology, or clinical specialists.

---

## Technical Details

*   **Algorithm:** Contextual Bandits (specifically Thompson Sampling).
*   **Learning Paradigm:** Label-Free learning, adapting RLHF to agent-level decision-making.
*   **Optimization Targets:**
    *   Generation Strategy
    *   Tool Selection
    *   Domain Routing
*   **Integration:** Built on AWS Strands Agents.
*   **Key Capabilities:**
    *   Handles non-stationarity and cold-start issues.
    *   Replaces rule-based or supervised methods with adaptive probabilistic selection.
    *   Open-sourced framework.

---

## Contributions

*   **Adaptive Agentic Framework:** Introduction of a dynamic alternative to traditional fixed-rule or expensive supervised learning methods for life sciences agents, enabling continuous adaptation to changing conditions and user preferences.
*   **Multi-Dimensional Optimization:** A comprehensive approach to agent orchestration that simultaneously optimizes reasoning strategies, tool usage, and domain routing.
*   **Human-in-the-Loop Learning:** Validation of a contextual bandit approach that relies solely on implicit user feedback, reducing the dependency on costly ground truth data annotations while maintaining high performance.

---

## Results

The system achieved a **15â€“30% improvement** in user satisfaction over random baselines. It demonstrated rapid learning convergence, identifying optimal strategies after only **20â€“30 queries**.

The approach successfully solved the exploration-exploitation dilemma and was validated on complex life science queries, including:

*   Factual lookups
*   Complex reasoning
*   Literature research
*   Safety-critical tasks