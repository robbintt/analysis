# Towards Reinforcement Learning from Neural Feedback: Mapping fNIRS Signals to Agent Performance

*Julia Santaniello, Matthew Russell, Benson Jiang, Donatello Sassaroli, Robert Jacob, Jivko Sinapov*

---

## ðŸ“Š Quick Facts & Metrics

| Metric | Details |
| :--- | :--- |
| **Participants** | 25 |
| **Domains** | Pick-and-Place Robot, Lunar Lander, Flappy Bird |
| **Binary F1 Score** | 67% |
| **Multi-class F1 Score** | 46% |
| **Fine-tuning Gain** | +17% (Binary), +41% (Multi-class) |
| **Methodology** | fNIRS Signal Analysis & Transfer Learning |
| **Quality Score** | 9/10 |

---

## Executive Summary

Standard Reinforcement Learning (RL) typically relies on explicit, externally defined reward functions or direct human feedback to guide agent behavior. This dependence on explicit input is often limiting because designing accurate reward functions is difficult, and requesting direct feedback from humans places a high cognitive load on the user. 

This paper addresses the feasibility of **"Reinforcement Learning from Neural Feedback" (RLNF)**, investigating whether implicit neural signalsâ€”specifically brain activity measured without requiring conscious effortâ€”can be effectively decoded and utilized to evaluate and train autonomous agents.

### Core Innovation
The core innovation is a framework that translates functional Near-Infrared Spectroscopy (fNIRS) signals into machine-readable performance metrics. Technical implementation involves segmenting and preprocessing raw fNIRS data to train models that map neural activity to agent states. The authors frame RLNF as a supervised learning challenge, employing binary and multi-class classifiers to categorize performance and regressors to predict the continuous deviation from an optimal policy. To tackle the significant challenge of cross-subject variability in neural data, the proposed method utilizes a transfer learning approach: models are pre-trained on a general population and subsequently fine-tuned using small batches of subject-specific data.

### Validation & Impact
The study validates the approach across three distinct domainsâ€”**Pick-and-Place Robot**, **Lunar Lander**, and **Flappy Bird**â€”using data from 25 human participants. The models achieved promising predictive capabilities, with fine-tuning strategies proving critical for generalization.

This research establishes the foundational viability of RLNF, representing a paradigm shift from explicit, manual feedback loops to implicit, physiological monitoring. It contributes the first multi-domain fNIRS dataset for this purpose and provides quantitative evidence that implicit signals can serve as a robust proxy for ground-truth performance metrics. This work paves the way for more seamless human-in-the-loop systems where training occurs passively through brain activity, reducing the burden on human trainers and enabling more intuitive agent correction.

---

## Key Findings

*   **Viability of RLNF:** The study successfully demonstrates that implicit neural signals (fNIRS) can be mapped to agent performance, establishing the technical feasibility of Reinforcement Learning from Neural Feedback.
*   **Classification Performance:**
    *   **Binary Classification:** Achieved an average **F1 score of 67%** in classifying agent performance.
    *   **Multi-class Classification:** Achieved an average **F1 score of 46%**.
*   **Continuous Prediction:** Models were effectively trained as regressors to predict the continuous degree of deviation from a near-optimal policy.
*   **Generalization Solution:** Fine-tuning pre-trained models with small subject-specific samples significantly mitigates cross-subject generalization challenges.
    *   Resulted in a **17% increase** in F1 scores for binary models.
    *   Resulted in a **41% increase** in F1 scores for multi-class models.

---

## Technical Details

The technical framework relies on interpreting implicit physiological data as a supervision signal for machine learning models.

*   **Signal Type:** Functional Near-Infrared Spectroscopy (fNIRS).
    *   **Nature:** Implicit neural signals (no conscious effort required from the subject).
*   **Problem Framing:** The study approaches RLNF through three supervised learning lenses:
    1.  **Binary Classification:** Predicting distinct performance states.
    2.  **Multi-class Classification:** Categorizing performance into multiple discrete levels.
    3.  **Regression:** Predicting the continuous deviation from an optimal policy.
*   **Training Strategy:**
    *   **Pre-training:** Initial model training on general population data.
    *   **Fine-tuning:** Adaptation using small sample sizes specific to the individual subject to address neural variability.

---

## Methodology

The research followed a rigorous data acquisition and modeling pipeline:

1.  **Data Acquisition:**
    *   **Modality:** Functional near-infrared spectroscopy (fNIRS).
    *   **Subjects:** 25 human participants.
    *   **Domains:** Three distinct environments were used:
        *   Pick-and-Place Robot
        *   Lunar Lander
        *   Flappy Bird

2.  **Preprocessing:**
    *   Raw neural signals were segmented and preprocessed to extract relevant features.

3.  **Predictive Modeling:**
    *   **Classifiers:** Employed to map signals to discrete performance categories.
    *   **Regressors:** Employed to predict continuous deviation metrics.

4.  **Evaluation Protocol:**
    *   Focused on **Cross-subject generalization**: training on data from a subset of subjects and testing on unseen subjects.
    *   Specific assessment of the impact of fine-tuning on generalization performance.

---

## Results

The quantitative results underscore the potential of neural feedback mechanisms:

*   **Binary Classification:** Average F1 score of **67%**.
*   **Multi-class Classification:** Average F1 score of **46%**.
*   **Fine-Tuning Efficacy:**
    *   **Binary Models:** Performance increase of **17%** via fine-tuning.
    *   **Multi-class Models:** Performance increase of **41%** via fine-tuning.
*   **Regression:** Models successfully learned to predict continuous deviation from a near-optimal policy.

---

## Contributions

*   **Novel Dataset:** Introduction of a new fNIRS dataset spanning three domains and 25 participants.
*   **Paradigm Shift:** Establishment of the foundational framework for **Reinforcement Learning from Neural Feedback (RLNF)**, moving from explicit human feedback to implicit neural signals.
*   **Methodological Framework:** A proposed solution to the "neural classification problem," providing a concrete methodology to translate raw fNIRS signals into performance metrics.
*   **Quantitative Validation:** Provides the first substantial quantitative evidence that implicit physiological signals can act as a viable feedback mechanism for training reinforcement learning agents.

---

## Quality Assessment

*   **Quality Score:** 9/10
*   **References:** 7 citations