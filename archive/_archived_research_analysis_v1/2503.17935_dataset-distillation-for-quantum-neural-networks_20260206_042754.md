---
title: Dataset Distillation for Quantum Neural Networks
arxiv_id: '2503.17935'
source_url: https://arxiv.org/abs/2503.17935
generated_at: '2026-02-06T04:27:54'
quality_score: 9
citation_count: 23
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Dataset Distillation for Quantum Neural Networks

*Koustubh Phalak; Junde Li; Swaroop Ghosh*

> ### **Quick Facts**
> *   **Quality Score:** 9/10
> *   **Citations:** 23
> *   **Key Datasets:** MNIST, CIFAR-10
> *   **Core Innovation:** Dataset Distillation for QNNs
> *   **Architecture:** Quantum LeNet with Residual Connections
> *   **Top Metric:** 91.9% accuracy on distilled MNIST (vs 94% classical)

---

## Executive Summary

Training Quantum Neural Networks (QNNs) presents a significant computational bottleneck due to the high cost of quantum executions and the extensive number of gradient descent steps required for convergence. Additionally, the necessity to process large volumes of classical data exacerbates these challenges, limiting the scalability and practical efficiency of QNNs.

This research addresses the critical need to reduce training data volume and minimize quantum resource consumption without compromising model performance. The core innovation is the application of dataset distillation to the quantum domain, enabling the synthesis of smaller, highly informative subsets from large classical datasets.

The authors developed a novel Quantum LeNet architecture based on Parametric Quantum Circuits (PQC), which integrates residual connections to mitigate vanishing gradients and trainable Hermitian observables to optimize measurement outcomes. To ensure stability during the distillation process, the architecture also incorporates a non-trainable Hermitian component, specifically designed to manage the unique instabilities inherent in quantum optimization.

The proposed approach successfully reduced training data volume, gradient descent steps, and the total number of quantum executions required. In benchmarking tests, the Quantum LeNet achieved post-inferencing accuracy comparable to a classical LeNet on distilled datasets, scoring 91.9% versus 94.0% on MNIST and 50.3% versus 54.0% on CIFAR-10. While the introduction of the non-trainable Hermitian component was necessary for stability, it resulted in a marginal trade-off in accuracy.

This research provides empirical evidence that QNNs can achieve performance levels comparable to classical models with significantly greater data efficiency, establishing a new precedent for hybrid model design. By optimizing the training pipeline and reducing the reliance on extensive quantum executions, the work lowers the barrier to entry for practical quantum machine learning.

---

## Key Findings

*   **Efficiency Gains:** Applying dataset distillation to QNNs significantly reduces training data volume, gradient descent steps, and the total number of quantum executions required.
*   **Competitive Accuracy:** The proposed quantum LeNet achieved post-inferencing accuracy comparable to classical LeNet on distilled datasets:
    *   **MNIST:** 91.9% (Quantum) vs. 94% (Classical)
    *   **CIFAR-10:** 50.3% (Quantum) vs. 54% (Classical)
*   **Architectural Robustness:** The integration of residual connections and trainable Hermitian observables in the Parametric Quantum Circuit (PQC) allows the model to maintain high performance despite reduced training data.
*   **Stability Trade-off:** Introducing a non-trainable Hermitian component ensures stability during distillation but results in a marginal accuracy reduction (up to 1.8% for MNIST, 1.3% for CIFAR-10).

---

## Technical Details

The paper applies **Dataset Distillation** to Quantum Neural Networks (QNNs) to synthesize smaller, representative datasets. It utilizes a **Quantum LeNet** architecture, which is a quantum-mapped version of the classical LeNet, built on a **Parametric Quantum Circuit (PQC)**.

### Core Components

| Component | Function |
| :--- | :--- |
| **Residual Connections** | Implemented to mitigate vanishing gradients within the quantum circuit. |
| **Trainable Hermitian Observables** | Used to optimize measurement outcomes within the PQC. |
| **Non-trainable Hermitian Component** | A specific addition to ensure stability during the distillation process, addressing optimization instabilities. |

---

## Methodology

*   **Distillation Implementation:** The authors implemented dataset distillation specifically tailored for QNNs to condense large classical datasets into smaller, highly informative subsets.
*   **Architecture Development:** A novel quantum LeNet architecture was developed, incorporating:
    *   Residual connections
    *   Trainable Hermitian observables within the PQC
    *   A non-trainable Hermitian component for stability
*   **Evaluation Protocol:** Distilled datasets (MNIST and CIFAR-10) were used to train the quantum LeNet, with performance benchmarked against a standard classical LeNet.

---

## Results

The approach significantly reduces training data volume, gradient descent steps, and quantum executions.

### Performance Comparison (Distilled Dsets)

| Dataset | Quantum LeNet Accuracy | Classical LeNet Accuracy | Gap |
| :--- | :---: | :---: | :---: |
| **MNIST** | 91.9% | 94.0% | 2.1% |
| **CIFAR-10** | 50.3% | 54.0% | 3.7% |

### Impact of Stability Component
The introduction of the non-trainable Hermitian component resulted in the following accuracy reductions to ensure stability:
*   **MNIST:** Reduction of up to 1.8%
*   **CIFAR-10:** Reduction of up to 1.3%

---

## Contributions

*   **Optimized Training:** Addressed the bottleneck of high execution costs by minimizing the number of quantum executions required for convergence.
*   **Hybrid Design Precedent:** Introduced a specialized quantum LeNet architecture combining residual connections with trainable Hermitian observables.
*   **Data Efficiency Validation:** Provided empirical evidence that QNNs can achieve classical-like performance levels with significantly less data.
*   **Stabilization Solution:** Identified instability in quantum dataset distillation and contributed a stabilization solution (non-trainable Hermitian), quantifying its impact on accuracy degradation.

---