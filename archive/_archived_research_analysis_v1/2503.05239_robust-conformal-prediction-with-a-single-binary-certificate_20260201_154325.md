# Robust Conformal Prediction with a Single Binary Certificate
*Soroush H. Zargarbashi; Aleksandar Bojchevski*

> ### ðŸ“Š Quick Facts Sidebar
>
> *   **Quality Score**: 8/10
> *   **Citations**: 40 References
> *   **Computational Gain**: ~2 orders of magnitude reduction in Monte-Carlo samples (~150 vs ~10,000).
> *   **Key Baseline**: CAS (Conformalized Adaptive Smoothing).
> *   **Tested Datasets**: CIFAR-10, ImageNet, Cora-ML.
> *   **Key Innovation**: Single binary certificate replacing per-point certification.

---

## Executive Summary

Robust Conformal Prediction (CP) is essential for quantifying uncertainty in machine learning models subject to adversarial attacks, yet existing methods face a prohibitive computational barrier. State-of-the-art approaches like Conformalized Adaptive Smoothing (CAS) rely on randomized smoothing, necessitating massive numbers of Monte-Carlo samples ($10^4$ to $10^5$ per point) and expensive per-point certification for every input. This renders robust CP impractical for large-scale datasets. Furthermore, previous frameworks relied on restrictive assumptions requiring bounded score functions, limiting applicability.

This paper introduces **Binarized Conformal Prediction (BinCP)**, a novel framework that decouples certification from individual data points to achieve computational efficiency without sacrificing robustness. The core innovation is a **"single binary certificate."** Instead of certifying bounds per point, BinCP binarizes Monte-Carlo samples to create a boolean acceptance function based on whether the probability of a smoothed score exceeding a threshold surpasses a fixed parameter. This allows for a single certified radius derived from scalar probability and the perturbation ball, independent of specific inputs or model architectures. By avoiding the direct boundedness of scores, BinCP utilizes closed-form solutions (e.g., Gaussian CDF) or conservative Clopper-Pearson intervals, removing the theoretical requirement for bounded score functions.

Experimental evaluations across vision (CIFAR-10, ImageNet) and graph (Cora-ML) datasets demonstrate that BinCP dramatically reduces computational cost while matching or exceeding baselines. The method achieved valid coverage using only ~150 samples for CIFAR-10 compared to ~$10^4$ for CAS. It also generated consistently smaller (more precise) prediction sets while maintaining target nominal coverage (e.g., 0.9 on CIFAR-10). This work bridges the gap between theoretical robustness guarantees and practical deployment, enabling reliable prediction systems on resource-constrained platforms.

---

## Key Findings

*   **Dramatic Sample Reduction**: The proposed method achieves robustness with significantly fewer Monte-Carlo samples (e.g., **~150 for CIFAR10**) compared to baselines requiring **~$10^4$** samples per point.
*   **Single Certification**: The approach utilizes a single binary certificate to establish robustness, eliminating the computationally expensive need to certify every calibration or test point individually.
*   **Improved Precision**: Prediction sets generated by this method are **smaller (more precise)** than existing baselines while strictly maintaining the guarantee to cover the true label.
*   **Removed Constraints**: The method overcomes a significant limitation of previous robust CP techniques by **removing the constraint requiring a bounded score function**.

---

## Methodology

The proposed approach operates through a three-step process designed to decouple robustness from individual data point certification:

*   **Binarization**: Instead of bounding randomly smoothed conformity scores directly, the method binarizes Monte-Carlo samples. This transforms the problem into handling boolean values rather than continuous scores.
*   **Thresholding**: An adjustable (or automatically adjusted) threshold is applied to these binarized samples. This allows for the construction of prediction sets while rigorously preserving coverage guarantees.
*   **Single Certification**: A pivotal shift from previous methods, the approach computes just **one binary certificate** to establish robustness for the model, rather than performing point-wise certification for every individual data point in the calibration or test sets.

---

## Technical Details

### Core Methodology: BinCP
Binarized Conformal Prediction (BinCP) defines a boolean acceptance function based on the probability that a smoothed score exceeds a threshold $\tau$:

$$ \text{accept}(x, y; p, \tau) = I[P_{\xi}[s(x + \xi, y) > \tau] \geq p] $$

The method utilizes two primary parameters:
1.  **Fixed probability $p$**: Calibrates the threshold $\tau_\alpha(p)$.
2.  **Fixed threshold $\tau$**: Calibrates the probability $p_\alpha(\tau)$.

For strictly increasing continuous CDFs, these approaches are equivalent. The final prediction set is defined as:
$$ C(x; p_\alpha, \tau_\alpha) = \{y : \text{accept}(x, y; p_\alpha, \tau_\alpha) = 1\} $$

### Robustness Mechanism
*   **Perturbation Handling**: Handles adversarial perturbations within a ball $B(x)$ using an "inverted ball" $B^{-1}$ for asymmetric models.
*   **Score Function**: It employs the smoothed binary classifier $f_y(x) = \mathbb{E}_\xi[I[s(x+\xi, y) > \tau_\alpha]]$ as the score.
*   **Set Construction**: Robust sets are defined using certified upper bounds $c^*$.

### Single Binary Certificate
The certified bounds depend solely on a **scalar probability $p := f_y(x)$** and the ball $B$, rather than the specific input $x$ or model architecture. This independence allows for **closed-form solutions**, such as those derived from the Gaussian CDF.

### Finite Sample Handling
*   **Sampling**: Uses Monte Carlo sampling combined with **Clopper-Pearson confidence intervals** to derive conservative bounds.
*   **Coverage Adjustment**: Adjusts nominal coverage ($\alpha$) by a specific factor to account for finite samples.
*   **DSSN Support**: Supports de-randomized smoothing (DSSN).

---

## Results

### Experimental Setup
*   **Datasets**: CIFAR-10 (ResNet-110), ImageNet (ResNet-50), and Cora-ML (GCN).
*   **Baseline**: CAS (Conformalized Adaptive Smoothing).
*   **Coverage Targets**: 0.9 for CIFAR-10/Cora-ML and 0.85 for ImageNet.

### Performance Metrics

*   **Sample Efficiency**: BinCP achieves its goals with **~150 samples** on CIFAR10, a drastic reduction compared to the **~10,000 samples** required by CAS, reducing costs by approximately two orders of magnitude.
*   **Prediction Set Size**: BinCP produces **smaller (more precise)** prediction sets than CAS while maintaining coverage. This consistency was observed across CIFAR-10, ImageNet, and Cora-ML under varying perturbation intensities.
*   **Coverage Guarantees**: Empirical coverage was maintained at or above target thresholds (e.g., 0.9) across different robustness radii. In contrast, Vanilla CP failed to maintain coverage under perturbation.

---

## Contributions

1.  **Computational Efficiency**: Achieved a drastic reduction in the computational cost of robust conformal prediction by lowering the required number of Monte-Carlo samples by orders of magnitude.
2.  **Theoretical Novelty**: Introduced and proved that robustness can be maintained via a single binary certificate, fundamentally shifting the theoretical requirement from per-point certification.
3.  **Improved Utility**: Delivers smaller, tighter robust prediction sets, increasing the practical utility of the uncertainty estimates.
4.  **Expanded Generality**: Removed the theoretical requirement for score functions to be bounded, expanding the applicability of robust CP to a wider range of model architectures.