# Continual Reinforcement Learning for Cyber-Physical Systems: Lessons Learned and Open Challenges

*Kim N. Nolle; Ivana Dusparic; Rhodri Cusack; Vinny Cahill*

---

<div style="border: 1px solid #e1e4e8; border-radius: 6px; padding: 16px; background-color: #f6f8fa; margin-bottom: 24px;">

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 15 Citations |
| **Core Algorithm** | Proximal Policy Optimisation (PPO) |
| **Domain** | Autonomous Driving (Parking Simulator) |
| **Training Budget** | 500,000 steps per scenario |
| **Tasks Evaluated** | 4 (Perpendicular, 25Â° Diagonal, 50Â° Diagonal, Parallel) |

</div---

## Executive Summary

> **Context:** This research addresses the critical challenge of applying Continual Reinforcement Learning (CRL) to Cyber-Physical Systems (CPS), specifically within the context of autonomous driving. As CPS operate in dynamic, non-stationary environments, agents must learn incrementally across a sequence of tasks without losing previously acquired knowledge (catastrophic forgetting).
>
> **Methodology:** The study introduces a rigorous experimental framework to evaluate the feasibility of CRL within a simplified autonomous driving simulator, utilizing Proximal Policy Optimization (PPO) as the baseline algorithm. The technical approach involves sequential training where a single agent learns to park in four distinct scenarios without revisiting prior tasks. To simulate real-world constraints, the authors fixed hyperparameters after tuning them on the initial task and implemented PPO combined with Elastic Weight Consolidation (EWC) as a comparative regularization technique.
>
> **Outcomes:** Experimental findings demonstrated that current off-the-shelf CRL methods fail to maintain consistent performance. The system exhibited immediate catastrophic forgetting, inefficient capacity utilization, and environment abstraction difficulties. Notably, negative transfer was observed; configurations optimized for earlier tasks failed in the final scenario without substantial manual intervention.
>
> **Significance:** This paper significantly influences the field by providing a concrete analysis of technical bottlenecks in safety-critical systems. It challenges the suitability of standard neural networks for continual learning and advocates for a paradigm shift towards interdisciplinary research, bridging computer science with neuroscience to address fundamental limitations in non-stationary environments.

---

## Key Findings

The study identified several significant hurdles in applying Continual RL to complex Cyber-Physical Systems:

*   **Environment Abstraction Difficulties:** Finding suitable abstractions is a significant hurdle when applying Continual RL (CRL) to complex tasks like autonomous driving.
*   **Hyperparameter Instability:** The CRL system exhibits oversensitivity to hyperparameters, making it difficult to maintain stable performance across sequential tasks.
*   **Persistent Catastrophic Forgetting:** The issue of catastrophic forgetting remained present during sequential training, with performance on previous tasks dropping immediately upon learning new ones.
*   **Inefficient Capacity Utilization:** The study identified inefficiencies in how the neural network capacity was utilized, suggesting struggles in consolidating knowledge.

---

## Contributions

This paper provides the following specific contributions to the field:

*   **Identification of Specific CRL Bottlenecks:** Concrete identification and analysis of technical challenges in applying CRL to Cyber-Physical Systems.
*   **Critique of Neural Network Suitability:** A critical perspective questioning whether standard neural networks are suitable for continual learning demands.
*   **Formulation of Open Research Questions:** Establishing a roadmap for future research by outlining questions needed for robust CRL systems.
*   **Advocacy for Interdisciplinary Research:** Highlighting the necessity of bridging computer science with neuroscience to address limitations in non-stationary environments.

---

## Technical Details

*   **Base Algorithm:** Proximal Policy Optimization (PPO).
*   **Regularization Technique:** PPO+EWC (Elastic Weight Consolidation) implemented as a comparative baseline to mitigate forgetting.
*   **Training Strategy:** Sequential training strategy where the agent learns tasks successively without revisiting previous tasks.
*   **Task Scenarios:** Four distinct parking scenarios defined by parking angles:
    1.  Perpendicular
    2.  25Â° Diagonal
    3.  50Â° Diagonal
    4.  Parallel
*   **Configuration:** Fixed state and action spaces.
*   **Hyperparameter Tuning:** Tuned via random search on the first task and subsequently frozen to test robustness.

---

## Methodology

The researchers utilized an experimental setup with a simulated autonomous driving environment. In this setup, an agent was tasked with learning to park in four distinct scenarios varying by parking angle. The agent was trained sequentially to create a Continual Learning setting, simulating a real-world scenario where an agent must adapt to new conditions without retraining from scratch.

---

## Results

Experiments were conducted with a training budget of **500,000 steps per scenario**, with results averaged over **5 runs** measuring Success Rate. The study yielded the following observations:

*   **Catastrophic Forgetting:** Performance on previous tasks dropped immediately upon training on new tasks.
*   **Transfer Failures:** Hyperparameters and reward functions optimized for Perpendicular and Diagonal parking failed to enable learning in Parallel parking.
*   **Intervention Requirements:** Achieving success in the final (Parallel) scenario required significant modifications, including different learning rates and larger experience buffers.
*   **Conclusion:** Off-the-shelf abstractions and fixed configurations do not transfer reliably across tasks in this domain.