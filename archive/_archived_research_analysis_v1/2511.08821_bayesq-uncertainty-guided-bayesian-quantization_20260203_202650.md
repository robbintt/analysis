---
title: 'BayesQ: Uncertainty-Guided Bayesian Quantization'
arxiv_id: '2511.08821'
source_url: https://arxiv.org/abs/2511.08821
generated_at: '2026-02-03T20:26:50'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# BayesQ: Uncertainty-Guided Bayesian Quantization
*Ismail Lamaakal; Chaymae Yahyati; Yassine Maleh; Khalid El Makkaoui; Ibrahim Ouahbi*

---

> ### ðŸ“Š Quick Facts
> *   **Methodology:** Bayesian Post-Training Quantization (PTQ)
> *   **Core Concept:** Uncertainty-aware risk minimization via posterior expected loss
> *   **Key Technique:** Greedy knapsack algorithm for mixed-precision allocation
> *   **Inference Overhead:** Comparable to standard GPTQ (one-time preprocessing)
> *   **Primary Benchmarks:** ResNet-50 (ImageNet), BERT-base (GLUE)
> *   **Research Quality:** 9/10

---

## Executive Summary

Standard post-training quantization (PTQ) methods typically treat neural network weights as deterministic point estimates, disregarding the inherent uncertainty associated with trained parameters. This limitation often leads to suboptimal bit allocation and performance degradation, particularly in aggressive low-bit regimes (3â€“4 bits) where compression efficiency is most critical.

**BayesQ** introduces a novel framework that reframes low-bit quantization as an **uncertainty-aware risk minimization problem**. Technically, the method fits a lightweight Gaussian posterior over model weightsâ€”using a Laplace approximationâ€”and whitens the weights based on posterior covariance to handle anisotropic uncertainty. It optimizes codebooks to minimize posterior-expected distortion, employing closed-form solutions for MSE and Monte Carlo simulation for task-aware metrics.

Benchmarking against the state-of-the-art GPTQ method, BayesQ demonstrates consistent accuracy improvements on both vision and language tasks. On **ResNet-50**, it outperforms GPTQ by up to **+1.5 top-1 percentage points**, and on **BERT-base**, it achieves gains of up to **+1.1 points**. Crucially, these performance gains are achieved with a computational overhead comparable to a single standard GPTQ pass.

The significance of BayesQ lies in its validation of **Bayesian inference** as a practical and superior approach to PTQ. By successfully integrating uncertainty estimation into the quantization pipeline, the paper establishes a new paradigm for risk-minimized compression.

---

## Key Findings

*   **Superior Performance:** BayesQ consistently outperforms GPTQ across tested bit-widths:
    *   **ResNet-50 (ImageNet):** Gains of **+1.5**, **+0.7**, and **+0.3** top-1 percentage points at 3.0, 3.5, and 4.0 bits/weight respectively.
    *   **BERT-base (GLUE):** Gains of **+1.1**, **+0.4**, and **+0.2** points at 3.0, 3.5, and 4.0 bits/weight respectively.
*   **Efficiency:** Despite the complexity of Bayesian inference, BayesQ requires only a one-time preprocessing step with computational costs comparable to a standard GPTQ pass.
*   **Methodological Validation:** The approach proves that optimizing quantization based on **posterior expected loss** (uncertainty-aware risk minimization) is more effective than traditional deterministic PTQ methods.
*   **Versatility:** The framework successfully handles both scalar quantization (via closed-form MSE tables) and task-aware proxies (via short Monte Carlo simulation on small calibration sets).

---

## Technical Methodology

BayesQ utilizes a post-training quantization pipeline grounded in Bayesian inference. The process is structured into the following phases:

1.  **Posterior Estimation:** Fits a lightweight Gaussian posterior over model weights, defaulting to a diagonal Laplace approximation.
2.  **Whitening:** Transforms weights based on the posterior covariance to handle anisotropic uncertainty effectively.
3.  **Codebook Design:** Designs codebooks specifically to minimize posterior-expected distortion.
4.  **Bit Allocation:** Implements a greedy knapsack algorithm to allocate mixed precision across the network. This maximizes the marginal expected-loss reduction per bit while adhering to a global bit-budget.
5.  **Distillation (Optional):** Uses calibration-only distillation to align the quantized student model with a posterior predictive teacher.

### Technical Details Breakdown

*   **Core Framework:** Reframes low-bit quantization as an uncertainty-aware risk minimization problem using a lightweight Gaussian posterior over weights.
*   **Approximation:** Utilizes Laplace approximation for posterior approximation (supporting both diagonal and structured covariance).
*   **Quantization Space:** Performs quantization in a whitened space to handle anisotropic uncertainty.
*   **Algorithms:**
    *   **Codebook Construction:** Employs Weighted Lloydâ€“Max algorithms or uniform quantization.
    *   **Bit Allocation:** Optimized via a greedy knapsack algorithm to minimize total expected loss subject to a budget.
*   **Loss Estimation Strategies:**
    *   **High-resolution:** Closed-form MSE.
    *   **Task-aware:** Monte Carlo proxies.
*   **Pipeline Features:**
    *   Optional posterior-predictive teacher distillation.
    *   Full compatibility with standard inference stacks.
    *   One-time preprocessing complexity comparable to GPTQ without end-to-end retraining.

---

## Contributions

*   **Bayesian Risk Minimization:** BayesQ is the first framework to optimize quantization explicitly under the posterior expected loss, effectively reframing low-bit quantization as uncertainty-aware risk minimization.
*   **Practical Implementation:** Introduces a practical implementation of Bayesian quantization that includes:
    *   Closed-form solutions for scalar quantizers (posterior-expected MSE).
    *   Monte Carlo methods for more complex, task-aware objectives.
*   **Smart Allocation:** Contributes a novel mixed-precision allocation method that uses uncertainty estimates to guide bit allocation, ensuring that computational budget is spent where it most reduces expected error.

---

## Performance Results

The following results benchmark BayesQ against GPTQ at matched average bit-widths:

| Model | Benchmark | Avg Bit-width | Metric | Improvement over GPTQ |
| :--- | :--- | :--- | :--- | :--- |
| **ResNet-50** | ImageNet | 3.0 bits | Top-1 Accuracy | **+1.5** pp |
| | | 3.5 bits | Top-1 Accuracy | **+0.7** pp |
| | | 4.0 bits | Top-1 Accuracy | **+0.3** pp |
| **BERT-base** | GLUE | 3.0 bits | Score | **+1.1** pp |
| | | 3.5 bits | Score | **+0.4** pp |
| | | 4.0 bits | Score | **+0.2** pp |

---

*References: 40 citations*