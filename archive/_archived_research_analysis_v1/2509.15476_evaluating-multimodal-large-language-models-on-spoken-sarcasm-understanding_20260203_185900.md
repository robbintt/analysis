---
title: Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding
arxiv_id: '2509.15476'
source_url: https://arxiv.org/abs/2509.15476
generated_at: '2026-02-03T18:59:00'
quality_score: 8
citation_count: 0
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding

*Zhu Li; Xiyuan Gao; Yuqing Zhang; Shekhar Nayak; Matt Coler*

---

### üìã Quick Facts

| Metric | Details |
| :--- | :--- |
| **Datasets** | MUStARD++ (English), MCSD 1.0 (Chinese) |
| **Best Unimodal** | Audio (‚âà68.2% F1) |
| **Best Overall** | Text-Audio Bimodal (76.8% F1) |
| **Top MLLM** | Qwen-Omni (75.5% F1 fine-tuned) |
| **Key Innovation** | Collaborative Gating Fusion Module (CGFM) |
| **Quality Score** | 8/10 |

---

> ### üìù Executive Summary
>
> Spoken sarcasm detection remains a formidable obstacle in artificial intelligence, primarily because the intended meaning often contradicts the literal semantic content through non-verbal cues like prosody, intonation, and facial expressions. While Large Language Models (LLMs) demonstrate high proficiency in text-based tasks, their performance degrades significantly in spoken contexts due to the inherent absence of these critical acoustic and visual signals. This deficiency limits the pragmatic competence of AI in human-computer interaction. This paper addresses this gap by rigorously evaluating the capabilities of both LLMs and Multimodal LLMs (MLLMs) in discerning sarcasm, establishing that robust multimodal integration is essential for machines to achieve the emotional intelligence required for natural, context-aware communication.
>
> The core technical contribution of this work is the introduction of a "Collaborative Gating Fusion Module" (CGFM), a mechanism designed to optimize the integration of heterogeneous features from text, audio, and video streams. Unlike simple concatenation or summation methods, the CGFM functions by learning dynamic weights for each modality; it employs a gating mechanism utilizing sigmoid activation over projected feature vectors to selectively emphasize informative modalities while suppressing noise or redundant data. The researchers implemented a comprehensive evaluation framework that utilized frozen models as feature encoders fed into the CGFM, alongside an assessment of end-to-end MLLMs like Qwen-Omni. Crucially, this evaluation included zero-shot, few-shot, and LoRA fine-tuning configurations, allowing for a granular analysis of how different architectures fuse information across varying resource constraints.
>
> The empirical evaluation on the English MUStARD++ and Chinese MCSD 1.0 datasets yielded specific, counter-intuitive quantitative findings regarding modality dependence. Among unimodal baselines, audio-based models proved the most robust, achieving an F1-score of approximately 68.2% on MUStARD++, significantly outperforming text-only and vision-only counterparts. More strikingly, bimodal configurations demonstrated superior efficiency; the Text-Audio combination achieved the highest overall performance with an F1-score of 76.8%, whereas the full trimodal (Text-Audio-Video) approach saw a performance reduction to roughly 74.1%, indicating that visual information often introduces noise rather than clarity. In terms of model architecture, Qwen-Omni showed strong potential, reaching 71.3% F1 in zero-shot settings and climbing to 75.5% following LoRA fine-tuning, validating that parameter-efficient tuning can bridge the gap between specialized encoders and general-purpose MLLMs.
>
> This research significantly influences the trajectory of multimodal NLP by empirically challenging the prevailing assumption that "more modalities equals better performance." By demonstrating that visual cues can sometimes detract from sarcasm detection accuracy, the authors provide a critical efficiency insight for future system design: targeted bimodal processing may be more effective and computationally efficient than full trimodal ingestion for pragmatic tasks. The introduction of the Collaborative Gating Fusion Module offers a reproducible method for improving feature fusion in low-resource or noisy environments. Furthermore, the successful cross-lingual validation on both English and Chinese datasets establishes a new, comprehensive benchmark for the community, ensuring that future research into machine pragmatics is grounded in rigorous, quantitative standards.

---

## üîë Key Findings

*   **Dominance of Audio Cues:** Audio-based models achieved the strongest performance among unimodal approaches, highlighting the importance of acoustic cues in detecting sarcasm.
*   **Efficacy of Bimodal Models:** Bimodal combinations, specifically text-audio and audio-vision, outperformed both unimodal baselines and full trimodal models.
*   **Competitive MLLM Performance:** Multimodal Large Language Models (MLLMs), particularly **Qwen-Omni**, demonstrated competitive capabilities in both zero-shot and fine-tuned settings.
*   **Cross-Lingual Validation:** The study successfully validated the cross-lingual potential of MLLMs by testing on both English and Chinese datasets.

---

## ‚öôÔ∏è Methodology

The researchers conducted a systematic evaluation of LLMs and MLLMs using two distinct datasets:
*   **MUStARD++** (English)
*   **MCSD 1.0** (Chinese)

**Testing Configurations:**
*   Zero-shot
*   Few-shot
*   LoRA fine-tuning

**Feature Extraction Strategy:**
The study employed a feature extraction strategy where models acted as feature encoders. These representations were integrated through a novel **Collaborative Gating Fusion Module**.

---

## üî¨ Technical Details

**Collaborative Gating Fusion Module (CGFM)**
*   **Purpose:** Optimizes the integration of heterogeneous features from text, audio, and video streams.
*   **Mechanism:** Utilizes a gating mechanism with sigmoid activation over projected feature vectors.
*   **Function:** Learns dynamic weights for each modality to selectively emphasize informative signals while suppressing noise or redundant data.

**Model Evaluation**
*   **Encoders:** Frozen models used as feature encoders feeding into the CGFM.
*   **End-to-End:** Assessment of MLLMs (specifically Qwen-Omni).
*   **Tuning:** Utilization of LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning.

---

## üìä Results

The empirical evaluation revealed several critical performance metrics (measured by F1-score on MUStARD++):

*   **Unimodal Audio:** Achieved ~68.2%, significantly outperforming text-only and vision-only baselines.
*   **Bimodal (Text-Audio):** Achieved the highest overall performance at **76.8%**.
*   **Trimodal (Text-Audio-Video):** Performance decreased to ~74.1%, suggesting visual modalities introduced noise.
*   **Qwen-Omni (Zero-Shot):** Reached 71.3% F1.
*   **Qwen-Omni (Fine-Tuned):** Improved to **75.5%** F1 following LoRA fine-tuning.

---

## üèÜ Contributions

1.  **Modality Focus:** Addresses underexplored modalities by shifting focus to comprehensive audio-visual-textual understanding.
2.  **Architecture:** Introduces and validates the Collaborative Gating Fusion Module (CGFM) for improved integration of model representations.
3.  **Benchmarking:** Provides extensive benchmarking of state-of-the-art LLMs and MLLMs across varying resource settings and languages, establishing a solid baseline for future research.

---
**Research Quality Score:** 8/10