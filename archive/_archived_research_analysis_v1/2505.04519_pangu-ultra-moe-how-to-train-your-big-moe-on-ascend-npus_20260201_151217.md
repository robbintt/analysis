# Pangu Ultra MoE: How to Train Your Big MoE on Ascend NPUs

*Yehui Tang; Yichun Yin; Yaoyuan Wang; Hang Zhou; Yu Pan; Wei Guo; Ziyang Zhang; Miao Rang; Fangcheng Liu; Naifu Zhang; Binghan Li; Yonghan Dong; Xiaojun Meng; Yasheng Wang; Dong Li; Yin Li; Dandan Tu; Can Chen; Youliang Yan; Fisher Yu; Ruiming Tang; Yunhe Wang; Botian Huang; Bo Wang; Boxiao Liu; Changzheng Zhang; Da Kuang; Fei Liu; Gang Huang; Jiansheng Wei; Jiarui Qin; Jie Ran; Jinpeng Li; Jun Zhao; Liang Dai; Lin Li; Liqun Deng; Peifeng Qin; Pengyuan Zeng; Qiang Gu; Shaohua Tang; Shengjun Cheng; Tao Gao; Tao Yu; Tianshu Li; Tianyu Bi; Wei He; Weikai Mao; Wenyong Huang; Wulong Liu; Xiabing Li; Xianzhi Yu; Xueyu Wu; Xu He; Yangkai Du; Yan Xu; Ye Tian; Yimeng Wu; Yongbing Huang; Yong Tian; Yong Zhu; Yue Li; Yufei Wang; Yuhang Gai; Yujun Li; Yu Luo; Yunsheng Ni; Yusen Sun; Zelin Chen; Zhe Liu; Zhicheng Liu; Zhipeng Tu; Zilin Ding; Zongyuan Zhan*

---

### üîç Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Model Name** | Pangu Ultra MoE |
| **Total Parameters** | 718 Billion |
| **Active Parameters** | ~1.6B per token |
| **Architecture** | 256 Experts, Top-8 Routing, Shared Expert |
| **Hardware** | 6,000 Ascend NPUs |
| **Performance (MFU)** | 30.0% |
| **Benchmark Target** | Comparable to DeepSeek R1 |

---

## Executive Summary

Training trillion-scale sparse Large Language Models (LLMs) presents formidable challenges in computational efficiency and hardware scalability, particularly on architectures outside the dominant NVIDIA ecosystem. As the demand for larger Mixture of Experts (MoE) models grows, the industry faces a bottleneck in leveraging available hardware effectively without prohibitive costs. This paper addresses the specific difficulty of efficiently training state-of-the-art sparse models on Ascend NPUs, aiming to bridge the performance gap between these devices and established platforms while managing the extreme complexity of hyperparameter configuration at massive scale.

The authors propose a two-pronged methodology centered on **simulation-driven design** and **hardware-specific system optimization**. Instead of costly physical trial-and-error, they utilized a bottom-up workflow employing "Simulation-Based Configuration" to pre-select optimal hyperparameters by analyzing performance trade-offs before training commences. Technical features of the resulting Pangu Ultra MoE include 718 billion total parameters, 256 experts, Top-8 routing, and a shared expert component. To handle the sheer scale, the system implements specialized optimizations in Expert Parallelism to drastically reduce synchronization overhead, alongside memory management improvements designed to mitigate bottlenecks inherent to communication and load balancing on the Ascend architecture.

The study successfully validated the performance of Pangu Ultra MoE, achieving a **Model Flops Utilization (MFU) of 30.0%** on a cluster of 6,000 Ascend NPUs. The model activates approximately 1.6 billion parameters per token and demonstrated performance capabilities comparable to DeepSeek R1. Ablation studies on the methodology provided concrete empirical data: using 256 experts was found to offer the optimal balance of performance and efficiency compared to 512 experts, and the shared expert configuration consistently achieved lower training loss than non-shared alternatives.

This research establishes a critical benchmark for the viability of Ascend NPUs in training frontier-level sparse models, proving that the platform can compete with leading hardware ecosystems. By providing a scalable training "recipe" and detailed empirical insights into the training dynamics of massive MoE architectures, the work enables the broader AI community to replicate trillion-parameter training on diverse infrastructure. Furthermore, the validation of simulation-driven configurations over physical trials introduces a new standard for cost-efficiency in large-scale model development, significantly reducing the resource overhead typically associated with system tuning.

---

## Key Findings

*   **Model Architecture:** Successful creation of **Pangu Ultra MoE**, a sparse LLM utilizing a Mixture of Experts (MoE) architecture with **718 billion total parameters**.
*   **Performance Efficiency:** Achieved a high **Model Flops Utilization (MFU) of 30.0%** during the training process.
*   **Hardware Viability:** Demonstrated that the Ascend NPU system is capable of handling state-of-the-art sparse models, achieving performance comparable to **DeepSeek R1** on a cluster of 6,000 Ascend NPUs.
*   **Cost Optimization:** Realized significant cost-efficiency by leveraging **simulation methodologies** to select hyperparameters rather than relying on expensive physical trials.

---

## Methodology

The researchers employed a robust two-pronged methodology focusing on simulation and system-level optimization:

1.  **Simulation-Based Configuration**
    *   Used to analyze trade-offs between model hyperparameters.
    *   Enabled the selection of the optimal configuration for the Ascend NPU architecture *before* the training phase began.
    *   Facilitated a bottom-up workflow by validating operators via a 20B baseline pilot before scaling up.

2.  **System-Level Optimization**
    *   Targeted communication bottlenecks by optimizing **Expert Parallelism** to significantly reduce synchronization overhead.
    *   Improved internal memory efficiency to handle the massive parameter count.
    *   Addressed device communication bottlenecks and expert load imbalances.

---

## Technical Details

| Feature | Specification |
| :--- | :--- |
| **Total Parameters** | 718 Billion |
| **Expert Count** | 256 Experts |
| **Routing Strategy** | Top-8 Routing |
| **Architecture Type** | Sparse LLM with Shared Expert Component |
| **Active Parameters** | ~1.6B per token |
| **Validation Workflow** | Bottom-up; validated via 20B baseline pilot |

*   **Design Philosophy:** Utilizes a simulation-driven search to validate configurations.
*   **Optimization Targets:** Reducing communication overhead, mitigating device bottlenecks, and managing memory utilization.

---

## Results

*   **Training Efficiency:** Achieved 30.0% MFU on a large-scale cluster of 6,000 Ascend NPUs.
*   **Benchmark Performance:** The model demonstrated performance capabilities comparable to **DeepSeek R1**.
*   **Ablation Studies:**
    *   **Expert Count:** The study indicated that **256 experts** provided the best performance-efficiency balance. Increasing to 512 experts yielded diminishing returns.
    *   **Shared Expert:** The shared expert configuration demonstrated **lower training loss** compared to non-shared alternatives.
*   **Cost Savings:** The simulation-based approach realized significant cost-efficiency savings by minimizing the need for physical hyperparameter trials.

---

## Contributions

*   **Scalable Training 'Recipe':** Provided a comprehensive methodology for training trillion-parameter-scale sparse LLMs on Ascend NPUs.
*   **System Optimization Techniques:** Introduced novel techniques in Expert Parallelism and memory management tailored for the Ascend architecture.
*   **Benchmark Establishment:** Established Pangu Ultra MoE as a benchmark, proving the competitiveness of Ascend NPUs with leading platforms.
*   **Empirical Insights:** Offered valuable data regarding the behaviors and training dynamics of massive sparse MoE models.

---

**Document Metadata**
*   **Quality Score:** 8/10
*   **References:** 40 citations