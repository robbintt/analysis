# Evaluating the Process Modeling Abilities of Large Language Models -- Preliminary Foundations and Results

*Peter Fettke; Constantin Houy*

***

> ### **Quick Facts**
>
> *   **Quality Score:** 9/10
> *   **References:** 9 Citations
> *   **Core Concept:** Pareto-optimal variants vs. single optimal solution
> *   **Focus Area:** Business Process Management (BPM) & AI
> *   **Key Insight:** Model quality must be balanced against cost and time.

***

## Executive Summary

This research addresses the lack of rigorous and holistic evaluation methodologies for assessing Large Language Models (LLMs) in business process modeling, specifically regarding the generation of BPMN diagrams from plain English. The authors argue that current benchmarks are methodologically flawed because they treat modeling as a search for a single correct structural output, ignoring the task's inherent complexity. In practical applications, the utility of a process model is inextricably linked to the feasibility of its production; therefore, assessing models solely on visual or structural quality without considering non-functional constraints like computational cost and generation time offers a misleading picture of an LLM’s true applicability.

The key innovation is reframing process modeling from identifying an "optimal" solution to identifying a set of **Pareto-optimal variants**, effectively framing the task as a multi-objective optimization problem. The proposed technical framework integrates multi-dimensional quality metrics—Graph-Edit-Distance (GED), Information Retrieval scores (Precision and Recall), and behavioral execution semantics—with non-functional metrics such as raw generation time and token-based variable costs. Beyond the framework itself, the authors critically catalog significant research hurdles, explicitly detailing the risks of data leakage, the difficulties of result validation, and generalizability issues, thereby providing a theoretical foundation for rigorous experimental design.

To illustrate the framework, the authors conducted a preliminary application using data from Kourani et al. (2024a) and GPT-4, employing Pareto front analysis to visualize trade-offs between Quality, Time, and Cost. The evaluation revealed that no single configuration dominates; instead, the results identified a set of Pareto-optimal variants where improvements in one metric necessitate degradation in another. Specific qualitative observations included a **"degenerated Pareto front"** in the Cost versus Time analysis, which yielded only one optimal data point, and the use of log-scaled axes that exposed exponential performance differences across configurations despite the absence of tabulated numerical values.

By challenging the community to interpret existing benchmark results with caution, this paper establishes a preliminary but critical foundation for the field of process mining and AI. It demonstrates that high-quality process modeling cannot be disassociated from the economic and temporal costs of generation, urging a shift away from simple accuracy checks. By providing a comprehensive roadmap that addresses validation, generalizability, and data leakage, the authors lay the necessary groundwork for future scientific experiments to adopt a more holistic, trade-off-aware perspective when integrating LLMs into business process management tools.

***

## Key Findings

*   **Complexity of Evaluation:** Evaluating the process modeling capabilities of LLMs is a complex, non-trivial task, suggesting that current benchmark results should be interpreted with caution.
*   **Pareto-Optimal Variants:** LLMs generate a set of Pareto-optimal variants rather than a single optimal solution, requiring a balance between model quality and generation costs/time.
*   **Holistic Assessment:** The assessment of process models cannot rely solely on visual or structural quality; it must account for resource expenditures like cost and time.
*   **Research Hurdles:** Significant hurdles remain in the domain regarding the conceptualization of quality, result validation, generalizability, and the risk of data leakage.

***

## Methodology

The paper employs a **conceptual and critical analysis framework** rather than relying on a purely experimental quantitative method. The research approach includes:

*   Critiquing existing benchmarks and evaluation paradigms.
*   Identifying limitations in current assessment methods.
*   Discussing theoretical challenges to outline a roadmap for rigorous future scientific experiments.

***

## Contributions

*   **Reframing Outputs:** Shifts the perspective from finding a single optimal process model to identifying a set of trade-off solutions (Pareto-optimal variants).
*   **Holistic Framework:** Highlights the necessity of an evaluation framework that includes cost and generation time as critical metrics alongside model quality.
*   **Gap Analysis:** Explicitly catalogs technical research gaps, including quality conceptualization, validation methodologies, generalizability, and data leakage.
*   **Future Roadmap:** Proposes a specific roadmap for future experiments designed to tackle identified challenges scientifically.

***

## Technical Details

### Framework Architecture
*   **Input:** Plain English text.
*   **Output:** BPMN diagrams.

### Measurement Metrics
*   **Multi-Dimensional Quality:**
    *   Subjective assessment.
    *   Information Retrieval metrics (Precision and Recall).
    *   Behavioral execution semantics.
    *   Structural similarity via Graph-Edit-Distance (GED).
*   **Non-Functional Metrics:**
    *   Raw generation time.
    *   Costs (fixed and variable token-based).

### Implementation Data
*   **Source Data:** Kourani et al. (2024a).
*   **Model Used:** GPT-4.
*   **Cost Calculation:** Used GPT-4 to estimate token consumption from elapsed time and the Presidentlin LLM pricing calculator.

***

## Results

The study utilized **Pareto front analysis** to evaluate trade-offs between Quality, Time, and Cost.

*   **No Single Best Solution:** Results indicated that no single solution dominates; instead, there is a set of Pareto-optimal variants where improvements in one metric necessitate degradation in another.
*   **Cost vs. Time:** This analysis yielded a degenerated Pareto front with only one optimal data point.
*   **Visualization:** The analysis employed log-scaled axes to visualize exponential performance differences (specific numerical values were not tabulated in the text).