# Repairing Reward Functions with Human Feedback to Mitigate Reward Hacking

*Stephane Hatgis-Kessell; Logan Mondal Bhamidipaty; Emma Brunskill*

---

### üìã Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Framework** | Preference-Based Reward Repair (PBRR) |
| **Quality Score** | 7/10 |
| **References** | 40 Citations |
| **Core Advantage** | High data efficiency (sparse corrections) |
| **Theoretical Guarantee** | Matches prior cumulative regret in tabular domains |

---

## üìù Executive Summary

This research introduces **Preference-Based Reward Repair (PBRR)**, a hybrid framework designed to address "reward hacking" in Reinforcement Learning without the heavy data costs of learning reward functions from scratch. By decomposing the reward signal into a human-provided proxy and a learned additive correction term, PBRR isolates and repairs only the specific state-action transitions where the proxy is inadequate, utilizing human preferences to update a specialized loss function. Empirical results on MuJoCo and Gridworld benchmarks demonstrate that PBRR achieves high sample efficiency and performance comparable to or exceeding safe reference policies, often requiring an order of magnitude less human feedback than standard baselines. The study provides theoretical guarantees on cumulative regret and policy safety, concluding that repairing existing proxy functions offers a more efficient and practical approach to RL alignment by preserving expert domain knowledge.

---

## üîë Key Findings

*   **Superior Performance:** PBRR significantly outperforms baselines that learn reward functions from scratch, requiring significantly fewer human preferences to achieve high performance.
*   **Sparse Correction:** High-performance policies can be recovered by correcting only a sparse set of specific transitions rather than recalibrating the entire reward function.
*   **Theoretical Guarantees:** The authors established theoretical guarantees showing cumulative regret matches prior methods in tabular domains.
*   **Robustness:** PBRR consistently outperformed other methods specifically on reward-hacking benchmarks.

---

## üß© Methodology

The authors implemented the **Preference-Based Reward Repair (PBRR)** framework, which operates on a distinct paradigm from standard Reinforcement Learning from Human Feedback (RLHF).

*   **Objective:** Instead of learning a reward function from scratch, PBRR repairs a human-specified proxy reward function.
*   **Mechanism:** It learns an additive, transition-dependent correction term.
*   **Process:**
    1.  Utilizes a targeted exploration strategy.
    2.  Identifies specific transitions where the proxy reward fails.
    3.  Relies on human preferences over pairs of trajectories to guide the repair.

---

## üöÄ Contributions

*   **Hybrid Paradigm:** Introduces a new approach that addresses the limitations of both traditional proxy optimization and pure RLHF by repairing existing proxies rather than replacing them.
*   **Solution to Reward Hacking:** Provides a technical solution to reward hacking by focusing corrections narrowly on the specific transitions causing misalignment.
*   **Sample Efficiency:** Demonstrates that leveraging existing domain knowledge via proxy rewards drastically reduces sample complexity compared to learning from scratch.

---

## ‚öôÔ∏è Technical Details

The implementation relies on a specific decomposition of the reward function and a tailored loss function.

*   **Additive Correction Model:** The ground-truth reward is decomposed into a human-provided proxy and a learned neural network correction term:
    `r = r_proxy + g`
*   **Workflow Steps:**
    1.  Optimize a policy based on the current proxy.
    2.  Elicit human preferences against a safe reference policy.
    3.  Partition preference data into aligned and conflicting sets.
    4.  Update the correction term using a specific three-term loss function.
*   **Mathematical Models:**
    *   Uses the **Bradley-Terry model** for preference probability.
    *   Exploration parameter is set empirically to `C_1 = 0`.
*   **Theoretical Properties:**
    *   **Theorem 4.3:** Guarantees that the repaired policy performs no worse than the reference policy while resisting reward hacking.

---

## üìä Results

*   **Benchmark Performance:** PBRR significantly outperformed both standard RLHF (learning from scratch) and alternative reward repair methods on reward-hacking benchmarks.
*   **Data Efficiency:** Demonstrated high data efficiency, recovering high-performance policies by correcting only sparse transitions and requiring fewer human preferences than RLHF.
*   **High-Dimensional Environments:** Experiments in high-dimensional environments (e.g., MuJoCo) with non-linear ground-truth rewards confirmed performance matches or exceeds safe reference policies.
*   **Regret Bounds:** Theoretically achieved cumulative regret bounds in tabular domains that match prior state-of-the-art methods.

---
*References: 40 citations*