---
title: Gaussian Weight Sampling for Scalable, Efficient and Stable Pseudo-Quantization
  Training
arxiv_id: '2505.1117'
source_url: https://arxiv.org/abs/2505.11170
generated_at: '2026-02-03T20:21:40'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Gaussian Weight Sampling for Scalable, Efficient and Stable Pseudo-Quantization Training
*Myeonghwan Ahn; Sungjoo Yoo*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Compute Overhead** | 1.40% (on A100 GPUs) |
| **Memory Footprint** | 2 bytes per parameter |
| **Supported Precision** | Down to FP6 parameters |
| **Noise Precision** | Up to 9-bit effective precision |
| **Stability Benchmark** | Comparable to or exceeds BF16 baseline |
| **Models Validated** | GPT-2 & Llama2 (Up to 1B params) |
| **Training Scale** | Up to 300B tokens |

---

## Executive Summary

This research addresses the critical challenge of training Large Language Models (LLMs) using low-precision floating-point parameters to reduce memory and computational costs without sacrificing model stability. While Fully Quantized Training (FQT) offers theoretical efficiency, it is often plagued by training instability, consistency issues, and an exponential search complexity regarding optimal bit-widths. Conversely, standard high-precision training (e.g., BF16) remains resource-intensive. Consequently, there is a pressing need for a training methodology that bridges the gap between full-precision and fully quantized training, enabling scalable, low-precision training that maintains the stability of high-precision baselines.

The authors introduce **Gaussian Weight Sampling (GaussWS)**, a novel approach to Pseudo-Quantization Training (PQT) that simulates low-precision behavior by injecting noise derived from a Rounded Gaussian Distribution rather than uniform noise. This distribution is specifically designed to be floating-point (FP)-friendly, effectively mitigating arithmetic underflow problems common in low-precision formats like FP8. The technique operates efficiently through a simple arithmetic process of addition followed by a floating-point cast. Theoretically, the method leverages "stochastic precision annealing" to bound the dynamic range of weights and reduces the bit-width search complexity from exponential to linear, allowing for the use of high-precision noise (up to 9-bit) while training with low-precision parameters.

Empirical validation on GPT-2 and Llama2 architectures (up to 1 billion parameters and 300 billion tokens) demonstrates that GaussWS effectively supports low-precision floating-point parameters down to FP6. The method achieves training stability comparable to, or exceeding, the BF16 baseline, significantly outperforming existing baselines like DiffQ. In terms of efficiency, the implementation requires a memory footprint of only 2 bytes per parameter and incurs a minimal computational overhead of just 1.40% on NVIDIA A100 GPUs. These results confirm that the approach can successfully maintain loss reduction and model convergence while utilizing significantly lower precision than standard training methods.

The significance of this work lies in establishing Pseudo-Quantization Training (PQT) as a viable and theoretically grounded alternative to Fully Quantized Training for large-scale model development. By solving the stability and efficiency bottlenecks associated with low-precision training, the authors provide a pathway for training massive models at reduced computational costs. The introduction of an FP-friendly noise distribution not only advances the understanding of quantization noise but also offers a practical, scalable solution for the industry, potentially lowering the barrier to entry for LLM training by maximizing hardware utilization without degrading model quality.

---

## Key Findings

*   **High Scalability:** The proposed Gaussian weight sampling method supports low-precision floating-point (FP) parameters down to **FP6** and accommodates high-precision noise up to **9-bit** while utilizing BF16 operators.
*   **Computational Efficiency:** The method incurs minimal computational overhead, specifically a **1.40% increase** on A100 GPUs, and requires a memory footprint of only **2 bytes per parameter**.
*   **Training Stability:** Pseudo-quantization training (PQT) with this approach demonstrates stability comparable to or exceeding the BF16 baseline, validated by pre-training GPT2 and Llama2 models with up to **1B parameters** and **300B tokens**.
*   **Effectiveness of PQT:** Pseudo-quantization training (PQT) is presented as a viable solution to the consistency challenges and exponential search complexity associated with Fully Quantized Training (FQT).

---

## Methodology

The research utilizes **Pseudo-quantization training (PQT)** to bridge the gap between full BF16 training and fully quantized training (FQT), avoiding the stability issues of the latter. The authors introduce 'Gaussian weight sampling,' which employs a noise distribution specifically designed to be floating-point (FP)-friendly.

The method achieves efficient fake quantization through a simple arithmetic process involving an addition operation followed by a subsequent floating-point cast. The approach relies on **'stochastic precision annealing'** to establish a theoretical foundation for training low-precision FP parameters.

---

## Contributions

1.  **Analysis of PQT:** A detailed exploration of the practical implications and theoretical underpinnings of Pseudo-quantization training (PQT), a previously under-studied area.
2.  **Novel Noise Distribution:** The proposal of a specific, FP-friendly noise distribution with ideal properties (stochastic precision annealing) that facilitates low-precision training without degrading model performance.
3.  **Empirical Validation:** Comprehensive evidence that a PQT-based approach can simultaneously achieve scalability (down to FP6), high efficiency (low memory and compute overhead), and stability (matching BF16 baselines) in large-scale model training.

---

## Technical Details

*   **Core Framework:** The approach utilizes Gaussian Weight Sampling (GaussWS) within a Pseudo-Quantization Training (PQT) framework.
*   **Noise Simulation:** It simulates low-precision behavior by injecting Pseudo-Quantization Noise (PQN) using a **Rounded Gaussian Distribution** ($\lfloor N(0, 1)/2 \rceil$) instead of uniform noise.
*   **Floating-Point Optimization:** This distribution is designed to be FP-friendly to mitigate the underflow problem in floating-point arithmetic (e.g., FP8/BF16).
*   **Implementation Efficiency:** The implementation optimizes efficiency on NVIDIA A100 GPUs by using bitwise operations and Triton kernels to avoid bottlenecks from FP operations on PRNG integers.
*   **Theoretical Foundation:**
    *   Provides stochastic precision annealing.
    *   Bounds the dynamic range of weights.
    *   Reduces bit-width search complexity from exponential ($\O(2^n)$) to linear ($\O(1)$).

---

## Results

Experiments were conducted on GPT-2 (124M to 1B parameters) and Llama2 (up to 1B parameters) using up to 300B tokens on the OpenWebText dataset.

*   **Performance:** GaussWS outperformed the DiffQ baseline and the standard BF16 configuration in training stability and loss reduction, achieving performance comparable to the best-case BF16 baseline.
*   **Efficiency:** The method incurred a computational overhead of **1.40%** on A100 GPUs and required a memory footprint of **2 bytes per parameter**.
*   **Precision Capability:** It supports low-precision floating-point parameters down to **FP6** and accommodates high-precision noise up to 9-bit effective precision while utilizing BF16 operators.

---

**Quality Score:** 8/10
**References:** 40 citations