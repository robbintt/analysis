---
title: Reinforcement Learning Agent for a 2D Shooter Game
arxiv_id: '2509.15042'
source_url: https://arxiv.org/abs/2509.15042
generated_at: '2026-02-03T07:04:41'
quality_score: 8
citation_count: 15
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Reinforcement Learning Agent for a 2D Shooter Game

*Thomas Ackermann; Moritz Spang; Hamza A. A. Gardi*

---

> ### **Quick Facts**
> 
> *   **Quality Score:** 8/10
> *   **Environment:** 2D Shooter (1200 × 900 pixels)
> *   **Hybrid Agent Win Rate:** >70% (Consistent), >90% (Final)
> *   **Demonstration Data:** ~200 episodes
> *   **Training Pipeline:** 300 epochs BC → 1000 episodes RL
> *   **Batch Size:** 512
> *   **Learning Rate:** 0.001
> *   **Discount Factor:** 0.99

---

## Executive Summary

### Problem
This research addresses the inherent instability and poor sample efficiency associated with training reinforcement learning (RL) agents in complex, multi-agent environments such as 2D shooter games. Standard Deep Q-Network (DQN) approaches often struggle in these domains due to sparse reward signals, where agents receive little feedback on their actions, leading to high training variance and frequent policy reversion. This instability prevents convergence on effective strategies, making it difficult to deploy RL in scenarios where exploration is costly and rewards are delayed.

### Innovation
The authors propose a hybrid training protocol that integrates offline imitation learning with online reinforcement learning to mitigate these training challenges. The key innovation is a specialized multi-head neural network architecture featuring shared feature extraction layers utilizing multi-head attention mechanisms, paired with dual output heads for behavioral cloning and Q-learning. This design enables a sequential training strategy: the agent first learns from demonstration data generated by rule-based agents via behavioral cloning, establishing a stable baseline policy, and subsequently transitions to online Deep Q-Network optimization. This architecture facilitates simultaneous optimization and effective knowledge transfer between supervised and reinforcement learning signals.

### Results
Empirical validation demonstrates that the hybrid agent substantially outperforms pure RL methods, achieving a consistent win rate above 70% against rule-based opponents and exceeding 90% in final evaluations. In contrast, the pure DQN baseline exhibited significant instability with high variance and failed to converge reliably. The study utilized approximately 200 episodes of demonstration data, running 300 epochs of behavioral cloning followed by 1,000 episodes of online RL, confirming that initializing agents with imitation data significantly improves convergence stability and performance consistency compared to standard DQN implementations.

### Impact
The significance of this work lies in its empirical demonstration that combining demonstration-based initialization with reinforcement learning offers a robust solution for sparse reward problems in complex environments. By successfully validating a multi-head architecture that maintains stability while integrating disparate learning signals, the paper provides a scalable framework for training intelligent agents. This approach not only advances the state of the art in game AI but also offers a transferable methodology for improving sample efficiency and training stability in broader multi-agent applications where pure exploration is insufficient.

---

## Key Findings

*   **Superior Performance:** The hybrid training approach achieved a consistently above 70% win rate against rule-based opponents, substantially outperforming standard pure reinforcement learning methods.
*   **Instability in Pure RL:** Initial experiments utilizing pure deep Q-Networks (DQN) exhibited significant training instability, characterized by high variance and frequent reversion to poor policies.
*   **Robust Solution:** Combining demonstration-based initialization with reinforcement learning optimization provides a robust solution for complex multi-agent environments.
*   **Architecture Success:** The multi-head neural architecture successfully enabled effective knowledge transfer between behavioral cloning and Q-learning modes while maintaining overall training stability.

---

## Methodology

The researchers implemented a hybrid training framework designed to bridge the gap between supervised learning and reinforcement learning.

*   **Hybrid Framework:** Integrates offline imitation learning with online reinforcement learning.
*   **Sequential Strategy:**
    1.  **Phase 1:** Behavioral cloning applied to demonstration data generated by rule-based agents.
    2.  **Phase 2:** Transition to standard reinforcement learning for fine-tuning.
*   **Architecture:** Utilizes a multi-head neural network with shared feature extraction layers.
*   **Mechanisms:**
    *   Employs attention mechanisms for feature extraction.
    *   Features a dual output system with separate heads for behavioral cloning and Q-learning.
    *   Allows for simultaneous optimization and knowledge transfer.

---

## Contributions

*   **Hybrid Protocol:** Introduces a training protocol that mitigates 'sparse rewards' and 'poor sample efficiency' problems by leveraging rule-based demonstrations as a starting point.
*   **Specialized Architecture:** Develops a specialized multi-head neural architecture with shared attention layers that allows for the stable integration of supervised and reinforcement learning signals.
*   **Empirical Validation:** Provides empirical validation demonstrating that initializing agents with imitation data significantly improves stability and performance consistency compared to pure Deep Q-Network implementations in 2D shooter game contexts.

---

## Technical Details

### Architecture Specifications
*   **Design:** Multi-head design with a shared trunk.
*   **Outputs:** Dual outputs (Q-Learning and Imitation Learning).
*   **Components:**
    *   Separate embedding networks for entities (player, enemies, bullets, walls).
    *   Layer Normalization and LeakyReLU activation functions.
    *   Multi-head attention layers.

### Training Pipeline
*   **Stage 1:** Offline Behavioral Cloning (BC).
*   **Stage 2:** Online Deep Q-Network (DQN).
*   **Reward Shaping:** Uses hyperbolic tangent normalization.

### Hyperparameters & Environment

| Parameter | Value |
| :--- | :--- |
| **Environment** | 2D Shooter (1200 × 900 pixels) |
| **Learning Rate** | 0.001 |
| **Discount Factor** | 0.99 |
| **Epsilon Decay** | 0.4/0.8 to 0.1 |
| **Batch Size** | 512 |
| **Replay Buffer Size** | 20,000 |
| **Demonstrations** | ~200 episodes |
| **BC Epochs** | 300 |
| **RL Episodes** | 1000 |

---

## Results

*   **Performance Comparison:**
    *   **Hybrid Agent:** Win rates consistently above 70%, exceeding 90% in final results.
    *   **Pure DQN:** Showed high variance and instability.
*   **Convergence:** The Hybrid Agent exhibited stable convergence and effective knowledge transfer.
*   **Metrics:** Accuracy was measured as hits per shot.
*   **Evaluation:** Agents were evaluated against a random agent and two rule-based agents.
*   **Data Volume:** Training utilized approx. 200 episodes of demonstrations, 300 epochs of behavioral cloning, and 1000 episodes of online RL.

---

**Quality Score:** 8/10
**References:** 15 citations