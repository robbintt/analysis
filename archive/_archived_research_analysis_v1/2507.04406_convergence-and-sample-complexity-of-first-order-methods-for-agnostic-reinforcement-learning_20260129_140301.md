# Convergence and Sample Complexity of First-Order Methods for Agnostic Reinforcement Learning

*Uri Sherman; Tomer Koren; Yishay Mansour*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Total References** | 40 Citations |
| **Core Problem** | Agnostic Reinforcement Learning |
| **Key Innovation** | Variational Gradient Dominance (VGD) |
| **Optimization Space** | Non-Euclidean First-Order |

---

## Executive Summary

This research addresses the fundamental challenges of **agnostic reinforcement learning (RL)**, where the optimal policy is not necessarily contained within the candidate policy class (i.e., the realizability assumption does not hold). Existing theoretical frameworks often rely on restrictive structural assumptions, such as *completeness* or *coverability*, to guarantee convergence. These assumptions are often too strong for practical applications, limiting the applicability of theoretical results to real-world scenarios. Consequently, there is a critical need for a framework that provides rigorous sample complexity guarantees and convergence proofs for agnostic settings without relying on unrealizable or overly strict geometric conditions.

The authors introduce the **Variational Gradient Dominance (VGD) condition**, a structural assumption that is strictly weaker and more practical than completeness or coverability. VGD posits that the performance gap between any policy and the optimal policy in the class is bounded by the alignment of the policy with the steepest descent direction in a non-Euclidean space. Leveraging this, the paper reduces the agnostic policy learning problem to first-order optimization within a **non-Euclidean functional space**, utilizing Bregman divergence to measure proximity. This optimization-centric framework allows for the analysis of standard algorithmsâ€”specifically Steepest Descent Policy Optimization, Conservative Policy Iteration (reformulated via Frank-Wolfe), and Policy Mirror Descentâ€”under significantly relaxed theoretical constraints.

Under the assumptions of a convex policy class and the satisfaction of the VGD condition, the study establishes rigorous upper bounds on sample complexity for the analyzed algorithms. Notably, the authors prove that these convergence bounds are **independent of the state space cardinality** ($|S|$), a significant departure from many traditional results. Instead, complexity scales with the log-covering number of the policy class, which is often substantially smaller in high-dimensional environments. The paper also claims improvements over state-of-the-art iteration complexity upper bounds and empirically demonstrates that the VGD condition holds in standard benchmarks, validating the practical utility of the theoretical model.

---

## Key Findings

*   **Optimization Reduction:** The agnostic policy learning problem can be successfully reduced to first-order optimization within a non-Euclidean space.
*   **VGD Condition:** The Variational Gradient Dominance (VGD) condition is introduced as a strictly weaker and more practical assumption than standard completeness or coverability conditions. It is nevertheless sufficient for deriving tight sample complexity bounds.
*   **Algorithm Analysis:** The study proves that **Steepest Descent Policy Optimization**, **Conservative Policy Iteration** (reinterpreted via Frank-Wolfe), and **Policy Mirror Descent** all achieve defined sample complexity upper bounds under assumptions of convexity and VGD.
*   **Empirical Validation:** Evaluations across standard environments confirm the practical validity of the VGD condition in real-world scenarios.

---

## Methodology

The authors established a general policy learning framework grounded in agnostic reinforcement learning. Their mathematical approach involves the following steps:

1.  **Reduction:** They reduced the learning problem to first-order optimization in a non-Euclidean space.
2.  **Assumptions:** The analysis relies on two primary assumptions regarding the policy class:
    *   **Convexity:** The policy class must be convex.
    *   **VGD:** The policy class must satisfy the Variational Gradient Dominance condition.
3.  **Algorithm Analysis:** The optimization-centric framework was applied to analyze and derive convergence properties for three specific algorithms:
    *   Steepest Descent Method.
    *   Frank-Wolfe Method (applied to Conservative Policy Iteration).
    *   Policy Mirror Descent.
4.  **Validation:** Finally, the practicality of the VGD assumption was validated through empirical testing on standard environments.

---

## Technical Details

The paper proposes an **agnostic reinforcement learning framework** using an offline optimization oracle model, effectively removing the need for realizability assumptions.

*   **VGD Condition Definition:**
    The authors introduce the Variational Gradient Dominance condition, defined mathematically as:
    
    $$V(\pi) - \min_{\pi^* \in \Pi} V(\pi^*) \le \nu \max_{\tilde{\pi} \in \Pi} \langle \nabla V(\pi), \pi - \tilde{\pi} \rangle + \epsilon_{vgd}$$
    
    This condition is mathematically weaker than completeness or coverage.

*   **Optimization Framework:**
    The approach reduces agnostic RL to first-order optimization in a **constrained, non-convex, non-Euclidean functional space**. The process involves minimizing the value function via an update template involving Bregman divergence.

*   **Algorithms Analyzed:**
    *   **SDPO:** Steepest Descent Policy Optimization.
    *   **CPI:** Conservative Policy Iteration.
    *   **PMD:** Policy Mirror Descent.

---

## Research Contributions

*   **New Perspective:** Provided a fresh perspective that reduces agnostic RL to non-Euclidean first-order optimization, facilitating easier analysis of various algorithms.
*   **Weaker Assumptions:** Demonstrated that strong assumptions like completeness and coverability can be replaced with the weaker **Variational Gradient Dominance (VGD)** condition without losing the ability to bound sample complexity.
*   **Improved Convergence Results:** Derived new, improved convergence results for existing algorithms, specifically offering a novel reinterpretation of Conservative Policy Iteration through the Frank-Wolfe method and analyzing Steepest Descent Policy Optimization.
*   **Empirical Support:** Empirically demonstrated that the VGD condition holds in standard environments, supporting the practical utility of their theoretical findings.

---

## Results & Analysis

### Theoretical Results
The study successfully established sample complexity upper bounds for the analyzed algorithms under the VGD and convexity assumptions. Key theoretical highlights include:

*   **State Space Independence:** The convergence bounds are independent of the state space size ($|S|$). Instead, they scale with the **log-covering number** of the policy class, a metric often substantially smaller in high-dimensional environments.
*   **Complexity Bounds:** Improvements in state-of-the-art iteration complexity upper bounds are claimed.

### Empirical Results
*   **VGD Verification:** Empirical evaluations verified the practical validity of the VGD condition across standard environments, confirming that the theoretical assumption holds in practice.