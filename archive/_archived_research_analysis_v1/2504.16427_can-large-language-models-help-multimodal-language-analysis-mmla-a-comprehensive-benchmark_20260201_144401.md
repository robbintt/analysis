# Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark

*Hanlei Zhang; Zhuohang Li; Yeshuang Zhu; Hua Xu; Peiwu Wang; Haige Zhu; Jie Zhou; Jinchao Zhang*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Total Samples** | 61,016 multimodal utterances |
| **Test Set** | 12,093 samples |
| **Video Duration** | 76.6 hours |
| **Core Dimensions** | 6 (Intent, Emotion, Dialogue Act, Sentiment, Style, Behavior) |
| **Models Evaluated** | 8 mainstream branches (LLMs & MLLMs) |
| **Top Accuracy** | ~60% - 70% (Fine-tuned) |
| **Source Material** | Staged and real-world scenarios |

---

## Executive Summary

**The Problem**
Current Multimodal Large Language Models (MLLMs) excel at descriptive tasks but face significant limitations in comprehending the cognitive-level semantics essential for natural human communication. As conversational AI systems become more integrated into daily life, the ability to accurately interpret complex nuances such as intent, emotion, and speaking style is critical. This research addresses the lack of comprehensive investigation into how well these high-level models can interpret the subtleties within conversational utterances.

**The Innovation**
To bridge this gap, the authors introduce **MMLA**, a large-scale benchmark designed to rigorously evaluate cognitive and linguistic comprehension. MMLA aggregates nine distinct datasets to comprise over 61,000 multimodal utterances and 76.6 hours of video. The benchmark evaluates models across six core semantic dimensions using three distinct adaptation strategies: zero-shot inference, Supervised Fine-Tuning (SFT) using Low-Rank Adaptation (LoRA), and Instruction Tuning (IT) for multi-task generalization.

**The Results**
The experimental results reveal that state-of-the-art MLLMs struggle significantly with cognitive-level semantics, with even fine-tuned models achieving only approximately 60% to 70% accuracy. This performance ceiling persisted across all adaptation strategies, indicating that current architectures struggle to capture the subtleties of human communication regardless of the training method applied.

**The Impact**
This work establishes the first standardized baseline for evaluating MLLMs on cognitive-level multimodal analysis. By releasing the MMLA dataset and associated code as open-source resources, the authors provide the research community with the necessary tools to drive future innovation, underscoring an urgent need for improved resources and novel architectural approaches.

---

## Key Findings

*   **Significant Cognitive Limitations:** Current multimodal large language models (MLLMs) exhibit substantial limitations in understanding cognitive-level semantics. Even fine-tuned state-of-the-art models achieve only approximately **60% to 70% accuracy**.
*   **Research Gap:** There is a notable lack of research investigating how well MLLMs can comprehend the high-level, complex semantics inherent in human conversational utterances.
*   **Urgent Need for Progress:** The low performance figures across three different adaptation strategies (zero-shot, fine-tuning, instruction tuning) highlight the urgent need for better resources and architectural improvements to drive progress in multimodal language analysis.

---

## Methodology

The research methodology focuses on large-scale data aggregation and rigorous multi-dimensional model evaluation:

*   **MMLA Dataset Development:** The researchers developed a large-scale dataset comprising over **61,000 multimodal utterances** sourced from both staged and real-world scenarios.
*   **Six-Core Dimension Assessment:** The study assesses models across six fundamental dimensions of multimodal semantics to ensure a holistic evaluation:
    1.  Intent
    2.  Emotion
    3.  Dialogue Act
    4.  Sentiment
    5.  Speaking Style
    6.  Communication Behavior
*   **Comprehensive Model Evaluation:** Eight mainstream branches of LLMs and MLLMs were evaluated using three distinct training and inference methods:
    *   Zero-shot inference
    *   Supervised fine-tuning
    *   Instruction tuning

---

## Technical Details

### System Architecture & Configuration
The study utilizes high-performance computing optimizations to handle the large-scale video and text data.
*   **Precision & Attention:** BF16 precision and FlashAttention-2.
*   **Distributed Training:** DeepSpeed ZeRO-3.
*   **Optimization:** Cosine learning rate scheduling.

### Adaptation Strategies
| Strategy | Description |
| :--- | :--- |
| **Zero-shot Inference** | Utilizes text prompts and video tokens without task-specific training. |
| **Supervised Fine-Tuning (SFT)** | Uses Low-Rank Adaptation (LoRA) to minimize Cross-Entropy loss. |
| **Instruction Tuning (IT)** | Designed for multi-task generalization capabilities. |

### Hyperparameters
*   **Learning Rates:** $2e^{-5}$ to $1e^{-3}$
*   **Batch Sizes:** {4, 8, 16, 24}
*   **LoRA Ranks:** {8, 16, 64, 128}

### Dataset Composition
*   **Source Data:** Aggregated from 9 distinct datasets.
*   **Split:** 61,016 total samples (12,093 test set).
*   **Media:** 76.6 hours of video data.

---

## Results

**Evaluation Metrics**
The primary metric reported is **Accuracy**, supported by secondary metrics including Weighted F1, Weighted Precision, Macro F1, Recall, and Precision.

**Performance Overview**
*   **Systemic Underperformance:** Evaluated architecturesâ€”including LLMs (Llama-3, InternLM-2.5, Qwen2), MLLMs (VideoLLaMA2, Qwen2-VL, LLaVA variants, MiniCPM-V-2.6, GPT-4o), and traditional baselines (MulT, UniMSE, TCL-MAP)â€”all showed low performance figures regarding cognitive semantics.
*   **Strategy Ineffectiveness:** Neither zero-shot inference, fine-tuning, nor instruction tuning successfully bridged the gap to human-level comprehension.

---

## Contributions

1.  **Introduction of MMLA:** Release of a comprehensive, large-scale benchmark specifically designed to address the deficit in research regarding cognitive-level semantics in multimodal language analysis.
2.  **Standardized Baseline:** Establishment of performance baselines for mainstream MLL architectures across a diverse set of semantic dimensions, providing a clear metric for future model comparison.
3.  **Open Resource Availability:** The open-sourcing of both the MMLA datasets and the associated code, providing the research community with the necessary tools to advance the field of multimodal language analysis.

---

**Report Quality Score:** 8/10  
**References:** 40 citations