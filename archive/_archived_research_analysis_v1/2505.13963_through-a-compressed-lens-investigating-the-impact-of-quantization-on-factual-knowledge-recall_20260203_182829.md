---
title: 'Through a Compressed Lens: Investigating The Impact of Quantization on Factual
  Knowledge Recall'
arxiv_id: '2505.13963'
source_url: https://arxiv.org/abs/2505.13963
generated_at: '2026-02-03T18:28:29'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Through a Compressed Lens: Investigating The Impact of Quantization on Factual Knowledge Recall

*Qianli Wang; Mingyang Wang; Nils Feldhus; Simon Ostermann; Yuan Cao; Hinrich SchÃ¼tze; Sebastian MÃ¶ller; Vera Schmitt*

***

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Best Performer** | BitSandBytes (Highest preservation) |
| **Critical Finding** | Smaller models suffer disproportionately higher knowledge loss (~15% drop in 7B vs. ~7% in 70B) |
| **Anomaly** | "Precision Paradox": 3-bit quantization occasionally boosted recall by 1-2% |

***

## Executive Summary

Large Language Models (LLMs) demand substantial computational resources, making Weight-Only Post-Training Quantization (PTQ) essential for efficient deployment. However, compression introduces information loss that risks degrading Factual Knowledge Recall (FKR)â€”the model's ability to accurately retrieve stored information. While existing research extensively evaluates quantization impacts on general metrics like perplexity, there is a critical gap in understanding how compression specifically affects the integrity of factual knowledge. This paper addresses that gap by isolating the effects of bit-width reduction on knowledge retention, determining whether the pursuit of efficiency necessitates a sacrifice in factual accuracy.

The authors introduce a rigorous dual-pronged framework combining Experimental Evaluation with Task-Based Analysis, leveraging "Knowledge Neurons" theory to inspect internal model mechanics. Rather than relying solely on end-task benchmarks, the methodology performs a multi-level analysis across Neuron, Layer, and Model scopes to trace how quantization disrupts knowledge memorization. The study evaluates three distinct quantization techniques (including AWQ and BitSandBytes) across **3-bit, 4-bit, and 8-bit** widths without retraining. It utilizes the LRE dataset for one-hop knowledge triplets and the TwoHop-Fact dataset to assess latent multi-hop reasoning, providing a granular view of how compression preserves or corrupts specific factual pathways.

Quantitative findings reveal that while quantization generally diminishes FKR, the impact is heavily dependent on model scale. In **4-bit** configurations, smaller models (e.g., **7B parameters**) exhibited a **~15%** absolute drop in accuracy on the TwoHop-Fact reasoning dataset, whereas larger models (e.g., **70B parameters**) showed a significantly lower decline of **~7%**. On direct recall tasks (LRE), 7B models saw performance dip by approximately **4%**, while 70B models remained within **1%** of the full-precision (FP16) baseline. A "Precision Paradox" was observed in **3-bit** regimes, where reduced precision occasionally boosted recall by **1-2%** for specific facts. Among techniques, **BitSandBytes** demonstrated superior preservation, maintaining the highest fidelity to the FP16 baseline, while AWQ exhibited qualitative failures, such as a 0-shot hallucination rate where it misidentified Jay-Z as BeyoncÃ© Knowles' father.

This work provides empirical validation that quantization remains a viable strategy for LLM deployment, as the overall degradation in factual knowledge is modest for larger architectures. The study establishes a critical guideline: smaller models are disproportionately vulnerable to compression-induced knowledge loss, requiring careful calibration of bit-widths. By identifying **BitSandBytes** as the optimal method for minimizing factual drift, the paper offers clear, actionable direction for selecting compression techniques. This research bridges the divide between efficiency optimization and knowledge reliability, equipping practitioners with the data necessary to deploy compressed models without compromising factual integrity.

***

## Key Findings

*   **General Information Loss:** Quantization generally induces information loss within Large Language Models (LLMs), diminishing their capacity for Factual Knowledge Recall (FKR).
*   **Scale Disparity:** The negative impact of quantization on FKR is disproportionately amplified in smaller models compared to larger models.
*   **The Precision Paradox:** Reduced bit precision does not consistently result in inferior performance; specific instances of quantization were observed to paradoxically enhance FKR.
*   **Top Performing Technique:** Among the evaluated techniques, BitSandBytes demonstrated the highest preservation of the original full-precision model's factual knowledge recall capabilities.
*   **Validation of Compression:** Despite variability across models and methods, the degradation in performance remains modest, confirming quantization as an effective compression strategy.

***

## Methodology

The researchers employed a dual-pronged analytical framework:

1.  **Experimental Evaluation:** Comprehensive experiments utilizing three common quantization techniques applied across distinct bit widths (3-bit, 4-bit, and 8-bit).
2.  **Task-Based Analysis:** Focused on knowledge memorization and latent multi-hop reasoning, incorporating interpretability-driven analyses to understand internal mechanics.

## Contributions

*   **Bridging the Gap:** Bridges the research gap by specifically isolating and analyzing the effects of quantization on factual knowledge recall, distinct from general capability metrics.
*   **Scale Interaction:** Provides novel insights into how model scale interacts with compression techniques, highlighting the vulnerability of smaller models to quantization-induced knowledge loss.
*   **Technique Comparison:** Offers a comparative evaluation of quantization techniques regarding knowledge retention, identifying BitSandBytes as a superior method.
*   **Efficacy Validation:** Validates the efficacy of compression by providing empirical evidence that quantization remains a viable strategy for deployment despite information loss.

***

## Technical Details

### Evaluation Protocol
*   **Quantization Type:** Weight-Only Post-Training Quantization (PTQ).
*   **Constraints:** Comparison of full-precision baselines against models compressed without retraining.
*   **Techniques:** Three distinct techniques were evaluated across various bit-widths.

### Analytical Framework
The study utilizes a multi-level analysis based on **'Knowledge Neurons' theory**:
*   **Neuron Level:** Inspection of individual knowledge memorization.
*   **Layer Level:** Analysis of layer-specific contributions.
*   **Model Level:** Overall performance assessment.

Additionally, **Latent Multi-Hop Reasoning** analysis was conducted to evaluate internal reasoning pathways.

### Datasets
*   **LRE:** Used for testing one-hop knowledge triplets.
*   **TwoHop-Fact:** Used for testing compositional reasoning capabilities.

***

## Results

*   **General Degradation:** Quantization generally induces information loss that diminishes Factual Knowledge Recall (FKR), with negative impacts disproportionately amplified in smaller models.
*   **Precision Paradox:** A phenomenon was observed where lower bit precision sometimes paradoxically enhanced recall.
*   **Top Technique:** BitSandBytes was identified as the top performer in preserving knowledge.
*   **Modest Impact:** Overall, performance degradation was modest across the board.
*   **Qualitative Failures:** Specific instances of factual corruption were noted, such as AWQ incorrectly identifying 'Jay-Z' as BeyoncÃ© Knowles' father.