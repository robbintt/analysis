---
title: 'MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents'
arxiv_id: '2503.01935'
source_url: https://arxiv.org/abs/2503.01935
generated_at: '2026-02-03T06:33:00'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents

*Kunlun Zhu; Hongyi Du; Zhaochen Hong; Xiaocheng Yang; Shuyi Guo; Zhe Wang; Zhenhailong Wang; Cheng Qian; Xiangru Tang; Heng Ji; Jiaxuan You*

---

> ### üìä Quick Facts
>
> *   **Quality Score:** 9/10
> *   **References:** 40 Citations
> *   **Top Performing Model:** gpt-4o-mini
> *   **Optimal Topology:** Graph-based coordination
> *   **Strategy Gain:** +3% Milestone achievement with Cognitive Planning

---

## üìù Executive Summary

The paper addresses the lack of robust evaluation frameworks for Large Language Model (LLM)-based multi-agent systems (MAS). While individual LLM agents are well-studied, assessing how agents collaborate or compete in complex, interactive environments remains a significant challenge. Existing benchmarks often fail to capture the qualitative nuances of coordination, focusing primarily on final task outcomes rather than the efficiency and quality of the interaction dynamics. This gap hinders the development of advanced multi-agent systems capable of sophisticated teamwork or strategic conflict.

To solve this, the authors introduce **MultiAgentBench**, a benchmark framework built upon **MARBLE** (Multi-agent cooRdination Backbone with LLM Engine). The key technical innovation is the use of milestone-based Key Performance Indicators (KPIs) to quantitatively measure the quality of collaboration and competition, distinct from mere task completion.

Evaluations utilizing a multi-dimensional scoring system revealed several critical performance insights:
*   The **gpt-4o-mini** model achieved the highest average task score.
*   **Graph-based** coordination protocols consistently outperformed other topologies.
*   **Cognitive planning** strategies resulted in a 3% increase in milestone achievement rates.

This research establishes a new standard for evaluating agent interactions and provides actionable guidelines for engineers designing next-generation collaborative AI systems.

---

## üîë Key Findings

*   **Model Performance:** The **gpt-4o-mini** model achieved the highest average task score among all evaluated models.
*   **Topology Superiority:** **Graph-based** coordination protocols outperformed other network topologies (specifically star, chain, and tree structures).
*   **Strategy Efficacy:** The implementation of **cognitive planning** strategies increased milestone achievement rates by **3%**.
*   **Dynamic Evaluation:** The framework successfully demonstrated the capability to evaluate complex multi-agent dynamics, effectively distinguishing between simple task completion and the actual quality of collaboration or competition.

---

## üèóÔ∏è Technical Details

The paper introduces **MARBLE** (Multi-agent cooRdination Backbone with LLM Engine), a comprehensive framework for evaluating multi-agent systems.

### Architecture Components
*   **Coordination Engine:** Acts as the central orchestrator for the system.
*   **Agent Graph Module:** Defines communication topology using structured graphs with relationship constraints.
*   **Cognitive Module:** Manages internal agent states, personas, reasoning strategies (e.g., CoT, ReACT), and theory-of-mind.
*   **Memory Systems:** Utilizes hybrid memory management, including Shared Memory and RAG-based Long-Term Memory.

### Benchmark Design
The benchmark is divided into two primary categories of interaction:

1.  **Agents with Mutual Goals**
    *   *Scenarios:* Research, Minecraft, Database Error Analysis (5 agents), Coding.
    *   *Volume:* 100 test cases per scenario type.
2.  **Agents with Conflicting Goals**
    *   *Scenarios:* Werewolf, Bargaining.

---

## üî¨ Methodology

The researchers developed **MultiAgentBench** to assess LLM-based multi-agent systems within interactive scenarios. The approach is distinct in its utilization of novel **milestone-based Key Performance Indicators (KPIs)** designed to quantify qualitative aspects of collaboration and competition.

The study systematically tests coordination protocols by varying:
*   **Topologies:** Star, Chain, Tree, and Graph structures.
*   **Strategic Approaches:** Group discussion versus cognitive planning.

---

## üìà Results

Evaluation relies on a multi-dimensional scoring system to provide a holistic view of agent performance:

*   **KPI (Key Performance Indicator):** Measures milestone progress.
*   **Task Score:** Evaluates final output quality using LLM rubrics or rule-based metrics.
*   **Coordination Score:** An average of Communication and Planning scores on a 5-point scale.

**Outcome:**
The system successfully validated the hypothesis that interaction quality is distinct from task completion. The data confirmed that structural choices (Graph topology) and strategic implementations (Cognitive planning) have statistically significant impacts on system performance.

---

## ‚ú® Contributions

*   **Benchmarking:** Provided benchmarks for multi-agent dynamics that address the gap in literature regarding coordination and competition complexities.
*   **New Metrics:** Introduced milestone-based KPIs as an advanced evaluation metric for agent interaction.
*   **Empirical Insights:** Offered empirical insights into effective topologies (graph structure) and strategies (cognitive planning).
*   **Open Source:** Released open-source code and datasets to support reproducibility and further research.