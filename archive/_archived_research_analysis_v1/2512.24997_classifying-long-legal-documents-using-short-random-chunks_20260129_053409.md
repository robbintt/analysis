# Classifying long legal documents using short random chunks
*Luis AdriÃ¡n Cabrera-Diego*

> ### ðŸ“Š Quick Facts
> | Metric | Value |
> | :--- | :--- |
> | **Weighted F-Score** | 0.898 |
> | **Processing Speed** | 498s / 100 files (CPU) |
> | **Dataset Size** | 12,400 documents |
> | **Classes** | 18 (Legal Arbitration) |
> | **Architecture** | DeBERTa V3 + LSTM |
> | **Training Hardware** | NVIDIA A100 80GB |
> | **Inference Hardware** | CPU-only |

---

## Executive Summary

**Problem**
The research addresses the significant computational impediments involved in classifying long-form legal documents using state-of-the-art Natural Language Processing (NLP) models. While Transformer-based models typically excel at text classification, they are severely constrained by fixed context window limits and high memory consumption. Processing extensive legal textsâ€”often spanning thousands of tokensâ€”is frequently impossible or cost-prohibitive due to these hardware requirements. This creates a critical bottleneck in legal technology, where the efficient categorization of documents is essential for workflows such as arbitration and case management, yet the infrastructure for heavy GPU compute is not always available.

**Innovation**
The paper proposes a hybrid neural architecture that bypasses full-context processing through a stochastic chunking strategy combined with a Transformer-RNN pipeline. Instead of feeding entire documents into the model, the system extracts 48 random text chunks, each truncated to 128 tokens. The architecture utilizes a `mdeberta-v3-base` Transformer as an encoder to derive context vectors from the [CLS] token of each chunk, which are then sequenced through a Long Short-Term Memory (LSTM) network with a hidden size of 128. This linguistic representation is concatenated with log-normalized metadata featuresâ€”such as character count, paragraph count, and approximate page countâ€”before final classification via a Softmax layer.

**Results**
The proposed method was validated on a complex dataset comprising 12,400 legal documents across 18 distinct classes and 25 languages, with a median document length of 7.3 pages. The model achieved a median weighted F-score of 0.898, demonstrating highly competitive performance despite the aggressive reduction in input context. From a deployment perspective, the pipelineâ€”orchestrated using Temporal for execution managementâ€”proved robust on CPU-only hardware, achieving a median processing time of 498 seconds per 100 files. This confirms that the inference workload is manageable without requiring high-end GPU resources.

**Impact**
This research is significant because it empirically validates that random sampling of short text segments is sufficient for high-accuracy classification in domain-specific, long-document contexts. By decoupling classification performance from the need to process full document contexts, the approach offers a viable path to drastically reducing infrastructure costs and architectural complexity. Furthermore, the integration of this model with a durable execution framework like Temporal provides a production-ready blueprint for deploying resource-efficient NLP solutions in real-world enterprise environments where stability and cost are paramount.

---

## Key Findings

*   **High Accuracy with Reduced Context:** The proposed DeBERTa V3 + LSTM model achieved a weighted **F-score of 0.898**, proving that full-context input is not strictly necessary for effective classification.
*   **Computational Efficiency:** The approach circumvents memory limitations by using **48 randomly-selected short chunks** of 128 tokens rather than full-length documents.
*   **Robust CPU Deployment:** The pipeline achieved a median processing time of **498 seconds per 100 files** on CPU hardware, demonstrating feasibility for environments without high-end GPU access.
*   **Cost Reduction:** The method validates that infrastructure costs and complexity can be significantly lowered while maintaining high performance in specialized domains.

---

## Methodology

*   **Input Strategy**
    *   The model does not process entire documents.
    *   It utilizes a collection of **48 short text chunks** selected randomly from the source material.
    *   Each chunk is truncated to a maximum of **128 tokens**.

*   **Model Architecture**
    *   **Hybrid Neural Network:** Combines a high-performance Transformer model (**DeBERTa V3**) with a Recurrent Neural Network (**LSTM**).
    *   **Context Pooling:** Uses the [CLS] token passed through a Dense layer with GELU activation.
    *   **Sequence Modeling:** Utilizes an LSTM with a hidden size of 128 to model the sequence of chunks.
    *   **Classification Head:** Concatenates the final LSTM state with log-normalized metadata features (character count, paragraph count, approximate pages) before passing through a Softmax layer.

*   **Pipeline Deployment**
    *   Implementation relies on **Temporal**, a durable execution solution, to manage workflows.
    *   Ensures reliability and robustness in the processing pipeline on CPU-only servers.

---

## Technical Details

| Component | Specification |
| :--- | :--- |
| **Model Base** | Two-stage Hybrid (Transformer + RNN) |
| **Encoder** | `mdeberta-v3-base` |
| **Sequence Modeling** | LSTM (Size: 128) |
| **Input Processing** | Random chunking based on HTML tags (`<p>`, `<li>`); 128 max tokens; 16 stride; 48 chunks sampled |
| **Optimizer** | Lookahead AdamW |
| **Hyperparameters** | Learning Rate: `2e-5`; Warmup: `0.1`; Batch Size: `8` |
| **Regularization** | Dropout: `0.5`; Weight Decay: `0.01` |
| **Training Hardware** | NVIDIA A100 80GB |
| **Deployment Orchestrator** | Temporal |

---

## Results

*   **Dataset Profile**
    *   **Volume:** 12,400 documents
    *   **Scope:** 18 classes (legal arbitration)
    *   **Languages:** 25
    *   **Document Stats:** Median length of 7.3 pages, 62 paragraphs.

*   **Performance Metrics**
    *   **Median Weighted F-Score:** 0.898 (using 48-chunk configuration).
    *   **Inference Speed:** 498 seconds per 100 files on CPU-only hardware.

---

## Contributions

*   **Efficiency Solution for Long Documents:** Addresses the specific challenge of classifying lengthy legal documents, which are often impossible or expensive to process whole using standard Transformer models due to token limitations.
*   **Chunk-based Classification Validation:** Demonstrates that a random sampling strategy of short chunks is sufficient for effective classification in a specialized domain.
*   **Production-Ready Engineering Design:** Contributes a robust deployment architecture using Temporal, providing a blueprint for reliable, durable document processing workflows in real-world applications.

---

**Quality Score:** ðŸŒŸ 9/10  
**References:** 13 citations