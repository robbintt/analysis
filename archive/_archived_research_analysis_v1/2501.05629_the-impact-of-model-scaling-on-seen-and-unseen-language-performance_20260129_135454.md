# The Impact of Model Scaling on Seen and Unseen Language Performance
*Rhitabrat Pokharel; Sina Bagheri Nezhad; Ameeta Agrawal; Suresh Singh*

> ### ðŸ“Š Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Quality Score** | 8/10 |
> | **Citations** | 8 |
> | **Models Analyzed** | BLOOM, mT5, XGLM |
> | **Scale Range** | 560M to 176B parameters |
> | **Language Coverage** | 204 Languages |
> | **Tasks Evaluated** | Text Classification, Machine Translation |
> | **Prompting Strategies** | Zero-shot, Two-shot |

---

## Executive Summary

This paper addresses a critical uncertainty in multilingual NLP: whether scaling Large Language Models (LLMs) effectively enhances performance for languages absent from pretraining data (unseen languages) or if compute resources are yielding diminishing returns. As models grow exponentially, it is imperative to determine if parameter scaling inherently bridges the gap between high-resource (seen) and low-resource (unseen) languages. The study highlights that simply increasing model size does not guarantee universal cross-lingual transfer, particularly in zero-shot scenarios, and seeks to identify the true drivers of performance.

The authors introduce a rigorous empirical framework analyzing **BLOOM**, **mT5**, and **XGLM** across a wide parameter range. Unlike broad aggregate evaluations, this work isolates variables by assessing specific tasks: **XNLI** for multilingual natural language inference and **FLORES-101** for machine translation across 204 languages. The technical novelty lies in the comparative analysis of standard pretrained models against instruction-tuned variants within both zero-shot and two-shot learning regimes.

The analysis reveals a striking performance deficit for unseen languages. While **zero-shot** settings show negligible gains from scaling, **two-shot** scenarios exhibit linear improvements. For machine translation, significant scaling benefits are exclusive to **instruction-tuned models**. Crucially, the data demonstrates that total resource volume (compute and data tokens) is a stronger predictor of success than the proportional balance of languages in the pretraining corpus.

---

## Key Findings

*   **Scale Insensitivity in Zero-Shot:** Increasing model scale has a negligible impact on zero-shot performance, which remains largely flat across different model sizes.
*   **Few-Shot Benefits in Classification:** In two-shot settings, increasing model size results in clear linear improvements for multilingual text classification tasks.
*   **Task-Specific Scaling for Translation:** Benefits from scaling in machine translation tasks are evident exclusively in instruction-tuned models, unlike standard models.
*   **Predictors of Performance:** Overall resource levels are a stronger predictor of multilingual performance than the specific proportions of languages used during pretraining.
*   **Seen vs. Unseen Disparity:** There are striking performance disparities between seen and unseen languages, with scaling behaviors varying significantly between zero-shot and two-shot scenarios.

---

## Methodology

The researchers conducted a systematic examination of three multilingual LLM families of varying sizes (**BLOOM, mT5, XGLM**). To isolate the effects of model scaling, the study implemented a rigorous evaluation protocol:

*   **Task Scope:** Evaluated performance across two distinct tasksâ€”text classification and machine translation.
*   **Language Scope:** Spanned a diverse range of **204 languages**.
*   **Learning Settings:** Specifically assessed both **seen** and **unseen** languages within **zero-shot** and **few-shot** (specifically two-shot) learning settings.

---

## Technical Details

The study distinguishes between model architectures and tuning methods to evaluate scaling effects accurately.

*   **Model Types:** Standard versus Instruction-tuned Pretrained Language Models (PLMs).
*   **Core Approach:** Model scaling to evaluate performance changes, comparing zero-shot and two-shot prompting strategies.
*   **Evaluation Datasets:**
    *   **Classification:** XNLI (Cross-lingual Natural Language Inference).
    *   **Translation:** FLORES-101.
*   **Variables:**
    *   Seen Languages (Languages present in pretraining data).
    *   Unseen Languages (Languages absent from pretraining data).

---

## Results

The evaluation yielded specific insights into how scale impacts different tasks and learning regimes:

*   **Zero-Shot Performance:** Shows negligible impact with increased model scale; performance curves remain flat across increasing parameter counts.
*   **Few-Shot Classification:** Yields linear performance gains with increased model size in two-shot scenarios.
*   **Machine Translation:** Scaling benefits are exclusive to instruction-tuned models; standard models show no benefit from increased scale.
*   **Resource Predictors:** Overall resource volume (compute/data) is a stronger predictor of multilingual success than language proportions.
*   **Performance Gap:** There is a significant performance gap between seen and unseen languages, with scaling behavior varying notably between zero-shot and two-shot scenarios.

---

## Contributions

This work provides several critical advancements to the field of multilingual NLP:

1.  **Comprehensive Scaling Analysis:** Provides a detailed analysis of how increasing model size influences multilingual capabilities, specifically distinguishing between scaling behaviors in zero-shot versus few-shot scenarios.
2.  **Identification of Performance Drivers:** Advances the understanding of multilingual effectiveness by identifying that **overall resource volume**, rather than the proportional distribution of pretraining languages, is the dominant factor in model performance.

---
**Quality Score:** 8/10 | **References:** 8 citations