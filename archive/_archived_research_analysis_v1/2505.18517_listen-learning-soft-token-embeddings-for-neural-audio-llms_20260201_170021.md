# LiSTEN: Learning Soft Token Embeddings for Neural Audio LLMs

*Pooneh Mousavi; Shubham Gupta; Cem Subakan; Mirco Ravanelli*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Parameter Efficiency:** Trainable parameters kept to **< 1%** of total model size
> *   **Architecture:** Frozen LLM Backbone (e.g., LLaMA-2) + Pre-trained Audio Encoder (e.g., HuBERT)
> *   **Training Approach:** Simplified single-stage process using continuous soft token embeddings
> *   **Core Mechanism:** Learnable key-value pairs for dynamic prompt selection

---

## Executive Summary

Adapting Large Language Models (LLMs) for audio processing currently faces significant challenges due to the standard reliance on discrete audio tokenization. This method converts continuous audio signals into text-like tokens, introducing quantization loss and training instability while necessitating large-scale, task-specific datasets. Furthermore, existing models often overfit in multitask settings due to excessive parameter tuning.

**LiSTEN** (Learning Soft Token Embeddings for Neural Audio LLMs) addresses the critical challenge of efficiently integrating continuous audio modalities into pre-trained LLMs without sacrificing stability or incurring prohibitive computational costs. The framework introduces a parameter-efficient solution that bypasses discrete tokenization entirely by utilizing continuous soft token embeddings.

The architecture is distinguished by a **simplified single-stage training process** and a mechanism based on learnable key-value pairs for task-specific information storage. Audio features are projected into a query space to retrieve relevant prompts (the "values") by matching learned "keys." These selected continuous soft tokens are then mapped into the input space of a frozen LLM backbone. Crucially, the system freezes the LLM weights and updates only the projector and key-value prompt embeddings.

Evaluated across diverse tasks, LiSTEN matches or strictly outperforms discrete token baselines with an order of magnitude fewer trainable parameters. The framework demonstrates superior data efficiency, mitigating overfitting by narrowing the performance gap between large-scale and small-scale training data. By eliminating heavy discrete tokenization, LiSTEN preserves the continuous nuances of audio signalsâ€”such as prosody and subtle acoustic cuesâ€”resulting in richer, higher-fidelity representations.

---

## Key Findings

*   **Parameter Efficiency:** Achieves competitive performance with significantly fewer trainable parameters (reduced by an order of magnitude) compared to discrete token baselines.
*   **Data Efficiency:** Reduces reliance on large-scale task-specific datasets and shows a smaller performance gap between large and small-scale data scenarios.
*   **Robustness:** Prevents overfitting in multitask settings through restricted parameter updating.
*   **Interpretability:** Offers enhanced interpretability through prompt analysis, confirming that learned embeddings cluster semantically by phonemes or audio events.

---

## Methodology

The authors propose **LiSTEN**, a novel framework designed to adapt pre-trained LLMs for audio modalities. The methodology centers on replacing the standard discrete audio tokenization process with a dynamic, continuous approach:

1.  **Dynamic Prompt Selection:** Instead of static inputs, the model uses learnable key-value pairs to retrieve task-specific information dynamically.
2.  **Continuous Projection:** Audio inputs are processed by a pre-trained audio encoder (e.g., HuBERT) and passed through a trainable projector. This outputs continuous vectors without the need for quantization.
3.  **LLM Integration:** The resulting continuous soft tokens are fed into a frozen LLM backbone (e.g., LLaMA-2), allowing the model to leverage the reasoning capabilities of the LLM without modifying its weights.

This approach utilizes a **simplified single-stage training process** that contrasts sharply with the complex multi-stage pipelines often required in audio-language tasks.

---

## Contributions

The paper makes three primary contributions to the field of neural audio LLMs:

1.  **General-Purpose Framework:** Introduction of a comprehensive audio adaptation framework capable of handling various tasks.
2.  **Parameter-Efficient Solution:** A novel adaptation methodology that avoids heavy tuning, keeping trainable parameters below 1% of the total model size.
3.  **Interpretability Mechanism:** A method for analyzing soft token selection, providing insights into how the model processes phonemes and acoustic events.

---

## Technical Details

*   **Embedding Strategy**
    *   Replaces discrete audio tokens with **continuous soft token embeddings** to avoid quantization loss and instability.

*   **Architecture Components**
    *   **Audio Encoder:** Utilizes a pre-trained model such as HuBERT.
    *   **Projector:** A trainable component that outputs continuous vectors directly, bypassing the quantization step.
    *   **LLM Backbone:** A frozen Large Language Model (e.g., LLaMA-2) that processes the audio embeddings.

*   **Task Conditioning**
    *   Utilizes **learnable soft prompts** to condition the model for specific tasks (e.g., ASR, classification).

*   **Training Protocol**
    *   **Frozen Core:** The LLM backbone weights remain frozen throughout training.
    *   **Updated Components:** Only the projector and soft prompt embeddings are updated.
    *   **Efficiency:** This strict partitioning ensures trainable parameters remain a tiny fraction (<1%) of the total model size.

---

## Results

The model was evaluated on a diverse set of audio processing tasks, demonstrating robust performance across the board:

*   **Automatic Speech Recognition (ASR):**
    *   Achieved a competitive Word Error Rate (WER) of approximately **1.9%** on the LibriSpeech (test-clean) dataset.
*   **Emotion Recognition:**
    *   Achieved competitive accuracy ranging from **70% to over 75%** on the IEMOCAP dataset.
*   **General Performance:**
    *   Matched or outperformed discrete token baselines on Speaker Identification (ACC), Keyword Spotting, and Audio Captioning.
    *   Demonstrated an order of magnitude reduction in trainable parameters compared to baselines.
*   **Interpretability Analysis:**
    *   Analysis confirmed that learned embeddings cluster semantically, effectively grouping by phonemes or specific audio events.

---

**References:** 0 citations | **Quality Score:** 8/10