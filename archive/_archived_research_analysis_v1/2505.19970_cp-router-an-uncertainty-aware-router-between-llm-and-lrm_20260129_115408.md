# CP-Router: An Uncertainty-Aware Router Between LLM and LRM

*Jiayuan Su; Fulin Lin; Zhaopeng Feng; Han Zheng; Teng Wang; Zhenyu Xiao; Xinlong Zhao; Zuozhu Liu; Lu Cheng; Hongwei Wang*

---

> ### **Quick Facts**
>
> *   **Token Reduction:** Achieves up to **200x reduction** in token usage for simple queries (6 tokens vs. 1,169 tokens).
> *   **Framework:** Training-free and model-agnostic.
> *   **Core Innovation:** Full and Binary Entropy (FBE) criterion.
> *   **Key Benchmarks:** Validated on GSM8K, MATH, LogiQA, and C-Eval.
> *   **Quality Score:** 8/10

---

## Executive Summary

Large Reasoning Models (LRMs) have achieved state-of-the-art performance in complex domains by generating extensive internal reasoning chains, yet this capability often results in "overthinking" on routine queries. This inefficiency leads to excessive token generation—up to 200 times higher than standard Large Language Models (LLMs)—creating prohibitive computational costs and latency for simple tasks. The central challenge is addressing the efficiency-accuracy trade-off: systems must leverage the reasoning power of LRMs for difficult problems without wasting resources on easy inputs, while avoiding the substantial overhead typically associated with training complex routing policies.

The paper introduces **CP-Router**, a training-free and model-agnostic framework that dynamically routes queries between LLMs and LRMs using uncertainty estimates derived from Conformal Prediction (CP). A critical technical component is the reliance on a **calibration set**, which provides the empirical data necessary to establish distribution-free coverage guarantees. The framework utilizes prediction sets constructed from model logits and employs a novel Full and Binary Entropy (FBE) criterion to adaptively select the optimal CP error rate threshold ($\alpha$). This mechanism effectively distinguishes between easy and hard inputs by calibrating a non-conformity score threshold, ensuring that the routing decision is mathematically rigorous without requiring model fine-tuning.

Quantitative analysis demonstrates that CP-Router drastically reduces computational overhead while maintaining high task fidelity. In controlled comparisons, the Llama-3.1-8B model generated approximately 6 tokens for simple queries, whereas the LRM DeepSeek-R1 generated up to 1,169 tokens—a 200-fold increase that CP-Router successfully mitigates. Across specific benchmarks, including **GSM8K** and **MATH** (mathematics), **LogiQA** (logical reasoning), and **C-Eval** (Chinese chemistry), the router achieved accuracy comparable to the LRM baseline. Furthermore, the system validated its theoretical coverage guarantees ($P(Y_{test} \in C(X_{test})) \geq 1 - \alpha$) for both multiple-choice and open-ended tasks by leveraging the calibration set to ensure rigorous uncertainty quantification.

---

## Key Findings

*   **Significant Token Reduction:** CP-Router drastically reduces token usage by intelligently routing simple queries to standard LLMs rather than resource-intensive LRMs.
*   **Accuracy Preservation:** The system maintains or improves accuracy across diverse benchmarks, including mathematics, logical reasoning, and Chinese chemistry tasks.
*   **Robust Generalizability:** It demonstrates consistent performance across diverse model pairings and effectively handles open-ended tasks.
*   **Enhanced Uncertainty Differentiation:** The introduction of the Full and Binary Entropy (FBE) criterion refines the ability to differentiate between inputs that require high-level reasoning versus those that do not.

---

## Methodology

CP-Router utilizes a **training-free** and **model-agnostic** framework designed for dynamic routing between Large Language Models (LLMs) and Large Reasoning Models (LRMs). The core mechanism relies on prediction uncertainty estimates derived via **Conformal Prediction (CP)**.

The framework introduces the **Full and Binary Entropy (FBE)** criterion to adaptively select CP thresholds. This allows the system to dynamically determine whether a query requires the advanced capabilities of an LRM based on the uncertainty level of the prediction.

---

## Technical Details

**Framework Workflow**
The CP-Router framework employs Conformal Prediction (CP) to minimize "overthinking" by dynamically routing between LLMs and LRMs. The process involves:

1.  **Uncertainty Estimation:** Prediction sets are generated from model logits.
2.  **Routing Logic:**
    *   **Low Uncertainty:** Query is routed to the efficient LLM.
    *   **High Uncertainty:** Query is routed to the powerful LRM.

**Mathematical Formulation**
*   **Score Function:** For Multiple Choice Question Answering (MCQA), the score function is defined as $S(x, y) = 1 - f(y)$.
*   **Prediction Sets:** Sets are constructed as $C(x)$ using a threshold $q_\hat{hat}$.
*   **Coverage Guarantee:** The system ensures a rigorous coverage metric defined by $P(Y_{test} \in C(X_{test})) \geq 1 - \alpha$.

**Innovation: FBE Criterion**
To improve the differentiation between easy and hard inputs, the framework uses the Full and Binary Entropy (FBE) criterion. This metric adaptively selects the optimal error rate $\alpha^*$, allowing for precise calibration of the routing boundary.

---

## Results

*   **Token Efficiency:** Controlled comparisons revealed that while the Llama-3.1-8B model generated ~6 tokens for simple queries, the LRM DeepSeek-R1 generated up to 1,169 tokens.
*   **Benchmark Performance:** Accuracy remained comparable to the LRM baseline across:
    *   **GSM8K** & **MATH** (Mathematics)
    *   **LogiQA** (Logical Reasoning)
    *   **C-Eval** (Chinese Chemistry)
*   **System Constraints:** The solution successfully meets design constraints by being:
    *   **Self-adaptive**
    *   **Model-agnostic** (distribution-free)
    *   **Lightweight** (no training required)
*   **Theoretical Validation:** The system validated configurable error rates ($\alpha$) and calibration coverage ($1 - \alpha$) for both multiple-choice and open-ended tasks.

---

## Contributions

*   **Optimized Routing Strategy:** Presents a novel strategy to effectively solve the efficiency-accuracy trade-off in generative AI deployment.
*   **Novel Metric (FBE):** Introduces the Full and Binary Entropy (FBE) metric to adapt Conformal Prediction thresholds, facilitating superior uncertainty handling.
*   **Validation of Training-Free Routing:** Successfully validates the viability of high-performance, training-free routing across different model architectures and complex task domains.

---

**Quality Score:** 8/10 | **References:** 40 citations