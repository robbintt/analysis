# A probabilistic framework for dynamic quantization

*Gabriele Santini; Francesco Paissan; Elisabetta Farella*

### Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Key Efficiency** | 4x reduction in intermediate memory usage |
| **Accuracy Loss** | < 0.5% Top-1 Accuracy loss (vs. baseline) |

---

## Executive Summary

> **The Challenge:** Neural network quantization is critical for deploying deep learning models on resource-constrained hardware. However, existing methods force a difficult trade-off: Static quantization fails to handle distribution shifts in input data, leading to accuracy degradation. Standard dynamic quantization adapts to inputs but requires calculating parameters based on high-precision layer activations post-evaluation. This necessitates storing full-precision intermediate tensors, resulting in prohibitive memory overhead.
>
> **The Solution:** The authors introduce a probabilistic framework that enables input-adaptive rescaling of quantization parameters with minimal computational cost. Instead of calculating parameters post-activation using expensive high-precision data, the method utilizes a lightweight probabilistic surrogate model applied to pre-activations. This estimates scale, zero-point, and bit-width *before* the layer function is executed. The approach employs uniform affine quantization with pre-quantized weights, temporarily augmenting bit-width only during arithmetic operations to prevent overflow.
>
> **The Impact:** Validation on standard architectures (e.g., ResNet-50, MobileNetV3) demonstrates the framework maintains high fidelity to full-precision models. The method achieves a within 0.5% Top-1 accuracy loss compared to non-quantized baselines while delivering a 4x reduction in intermediate memory usage. This proves that adaptive quantization does not require high-precision intermediate storage, setting a new precedent for memory-efficient, input-adaptive neural networks.

---

## Key Findings

*   **Adaptive Efficiency:** The proposed probabilistic framework enables computationally efficient, input-adaptive rescaling of quantization parameters for neural networks.
*   **Performance Maintenance:** Validation on popular computer vision tasks and models resulted in only a negligible loss in performance (less than 0.5% Top-1 accuracy) compared to non-quantized baselines.
*   **Superior Trade-off:** The method achieves a better balance between performance maintenance and computational overhead when compared to standard quantization strategies.
*   **Minimal Overhead:** The framework operates with minimal memory overhead despite providing per-input adaptive adjustments.

---

## Technical Details

The following technical specifications define the framework's architecture and operational logic:

*   **Quantization Type:** Uniform Affine Quantization (Asymmetric).
*   **Parameters:** Defined by scale, zero-point, and bit-width.
*   **Core Innovation:** Utilizes a probabilistic model on the network's pre-activations via a lightweight surrogate.
*   **Timing:** Estimates quantization parameters for output tensors *before* executing the neural network layer function.
*   **Distinction:**
    *   *Vs. Static Quantization:* Handles distribution shifts effectively.
    *   *Vs. Dynamic Quantization:* Avoids expensive high-precision storage required by post-evaluation calculation.
*   **Data Assumptions:** Assumes weights and inputs are pre-quantized; focuses primarily on output tensor quantization.
*   **Overflow Prevention:** Temporarily augments bit-width during arithmetic operations to prevent data overflow.

---

## Methodology

The authors addressed the memory-accuracy trade-off by developing a framework that decouples adaptability from high-precision storage requirements. They introduced a probabilistic model applied to pre-activations to estimate the necessary quantization parameters dynamically. This lightweight surrogate allows the system to adapt to specific inputs on a per-instance basis without the heavy computational burden typically associated with calculating statistics on high-precision intermediate tensors.

---

## Results

Validation on computer vision tasks using architectures such as ResNet-50 and MobileNetV3 demonstrated that the method achieves a superior trade-off between performance maintenance and computational overhead.

*   **Accuracy:** Within 0.5% Top-1 accuracy loss compared to non-quantized baselines.
*   **Memory Efficiency:** 4x reduction in intermediate memory usage compared to standard dynamic quantization strategies.
*   **Operational Cost:** Operates with a minimal memory footprint despite providing per-input adaptive adjustments.

---

## Contributions

*   **Novel Framework:** Introduction of a new probabilistic framework specifically designed for the dynamic quantization of neural networks.
*   **Memory-Efficient Technique:** Development of a technique for input-adaptive parameter rescaling that does not significantly increase the computational burden.
*   **Empirical Validation:** Demonstration that adaptive dynamic quantization can match the performance of standard methods while offering better computational efficiency in computer vision applications.