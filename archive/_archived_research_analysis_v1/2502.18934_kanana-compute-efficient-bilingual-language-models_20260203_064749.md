---
title: 'Kanana: Compute-efficient Bilingual Language Models'
arxiv_id: '2502.18934'
source_url: https://arxiv.org/abs/2502.18934
generated_at: '2026-02-03T06:47:49'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Kanana: Compute-efficient Bilingual Language Models

*Kanana LLM Team; Yunju Bak; Hojin Lee; Minho Ryu; Jiyeon Ham; Seungjae Jung; Daniel Wontae Nam; Taegyeong Eo; Donghun Lee; Doohae Jung; Boseop Kim; Nayeon Kim; Jaesun Park; Hyunho Kim; Hyunwoong Ko; Changmin Lee; Kyoung-Woon On; Seulye Baeg; Junrae Cho; Sunghee Jung; Jieun Kang; EungGyun Kim; Eunhwa Kim; Byeongil Ko; Daniel Lee; Minchul Lee; Miok Lee; Shinbok Lee; Gaeun Seo*

---

> ### **Quick Facts**
>
> *   **Model Family:** Nano (2.1B), Essence (9.8B), Flag (32.5B)
> *   **Languages:** Bilingual (Korean & English)
> *   **Training Data:** 3 Trillion tokens (High-curation strategy)
> *   **Top Benchmark Score:** HAE-RAE 90.47 (Flag model)
> *   **Public Release:** 2.1B variants (Base, Instruct, Embedding)
> *   **Key Strategy:** Staged pre-training, depth up-scaling, pruning & distillation

---

## Executive Summary

This research addresses the dual challenge of prohibitive computational costs and the scarcity of high-performance resources for non-English languages in Large Language Model (LLM) development. As state-of-the-art models grow in size, the financial and infrastructure barriers to entry rise significantly, often relegating lower-resource languages like Korean to inferior performance levels. This paper tackles the inefficiency of traditional training paradigms that rely heavily on massive compute budgets, demonstrating that such scale is not a prerequisite for achieving top-tier bilingual performance.

The Kanana series introduces a compute-efficient framework that prioritizes data quality and architectural optimization over raw scale. The methodology utilizes a rigorous, multi-stage data curation process to construct a high-quality 3-trillion-token corpus, followed by "staged pre-training" and depth up-scaling to develop the final 32.5B "Flag" model. Crucially, the authors detail specific adaptation methods for deployment scenarios, including techniques for optimized embedding generation, enhanced Retrieval-Augmented Generation (RAG) capabilities, and function calling. The team further utilizes pruning and distillation to derive the smaller "Essence" and "Nano" variants without significant performance degradation, followed by post-training alignment via Supervised Fine-Tuning (SFT) and preference optimization.

Evaluations across standard benchmarks (MMLU, KMMLU, HAE-RAE, HumanEval, GSM8K) reveal that Kanana models achieve state-of-the-art performance relative to their compute budgets. The flagship Kanana Flag (32.5B) secured an HAE-RAE score of 90.47, an MMLU score of 77.68, and a KMMLU score of 62.10, outperforming significantly larger competitors like Llama 3.1 70B and Gemma 2 27B on knowledge-intensive tasks. The smaller models also demonstrated strong efficiency: Kanana Essence (9.8B) achieved an HAE-RAE of 84.97, dominating its peers, while the Kanana Nano (2.1B) reached 77.09 on HAE-RAE, substantially exceeding other models in the 2B parameter range.

This work significantly impacts the field by providing a reproducible blueprint for developing high-performance bilingual models without reliance on massive computational infrastructure. By publicly releasing the 2.1B parameter variants (base, instruct, and embedding models), the authors make a substantial contribution to the Korean natural language processing ecosystem, bridging a critical gap in open-source resources. The study validates that meticulous data curation, efficient architectural strategies, and targeted functional adaptations can effectively challenge the prevailing assumption that bigger always equals better, offering a sustainable path forward for future multilingual AI research.

---

## Key Findings

*   **Superior Bilingual Performance:** The Kanana model series demonstrates superior performance in Korean and competitive performance in English compared to existing models.
*   **Cost Efficiency:** The models achieve state-of-the-art performance levels with significantly lower computational costs than similar-sized models.
*   **Parameter Range:** The series spans a parameter range from **2.1 billion (2.1B)** to **32.5 billion (32.5B)**.
*   **Open Source Contribution:** The team has publicly released the 2.1B parameter variants (base, instruct, and embedding models) to foster further research in Korean language models.

---

## Technical Details

### Model Architecture
The Kanana family consists of three compute-efficient bilingual LLMs:
*   **Nano:** 2.1B parameters
*   **Essence:** 9.8B parameters
*   **Flag:** 32.5B parameters

### Data Curation Strategy
The models were trained on **3 trillion tokens** using a high-curation data strategy:
*   **Re-extraction:** High-quality data sourcing.
*   **Cascaded Filtering:** Multi-stage filtering process.
*   **Model-based Filtering:** Utilization of DCLM for English and a custom "edu filter" for Korean.
*   **Validation:** The Korean corpus quality was validated to be comparable to FineWeb 2.

### Training Pipeline
*   **Staged Pre-training:** Created 26.8B and 8B foundation models as intermediate steps.
*   **Depth Up-scaling:** Utilized to reach final model sizes.
*   **Optimization:** Applied pruning and distillation specifically to create the efficient Nano model.
*   **Post-training:** Includes Supervised Fine-Tuning (SFT) and preference tuning.

---

## Methodology

The approach employs compute-efficient pre-training strategies such as high-quality data filtering, staged pre-training, depth up-scaling, pruning, and distillation. Post-training involves supervised fine-tuning (SFT) and preference optimization to enhance user interaction capabilities.

Additionally, the paper details methods for adapting models to specific scenarios:
*   Embedding generation
*   Retrieval-augmented generation (RAG)
*   Function calling

---

## Results

Kanana models were evaluated on MMLU, KMMLU, HAE-RAE, HumanEval, and GSM8K.

**Kanana Flag (32.5B)**
*   **HAE-RAE:** 90.47
*   **MMLU:** 77.68
*   **KMMLU:** 62.10
*   *Performance:* Outperformed larger models like Llama 3.1 70B and Gemma 2 27B on knowledge-intensive tasks.

**Kanana Essence (9.8B)**
*   **HAE-RAE:** 84.97
*   *Performance:* Dominated peers in Korean benchmarks.

**Kanana Nano (2.1B)**
*   **HAE-RAE:** 77.09
*   *Performance:* Significantly exceeded similar-sized models in the 2B parameter range.

---

## Contributions

*   **Efficiency Blueprint:** Provides a blueprint for building high-performance bilingual language models with reduced computational overhead, challenging the necessity of massive compute budgets.
*   **Ecosystem Enrichment:** Significantly contributes to the Korean language processing ecosystem by releasing high-quality, open-source models (2.1B variants), addressing a gap in available resources for Korean-centric research.
*   **Technical Framework:** Offers a comprehensive technical framework covering the full lifecycle of efficient model development, from specific pre-training optimizations to post-training alignment and functional adaptation.

---

**Quality Score:** 8/10 | **References:** 40 citations