# Investigating the Scalability of Approximate Sparse Retrieval Algorithms to Massive Datasets

*Sebastian Bruch; Franco Maria Nardini; Cosimo Rulli; Rossano Venturini; Leonardo Venuta*

---

> ### üìä Quick Facts
>
> | Metric | Detail |
> | :--- | :--- |
> | **Dataset** | MsMarco-v2 (138M passages) |
> | **Embedding Model** | SPLADE |
> | **Key Algorithm** | Seismic (Inverted Index with Geometric Blocking) |
> | **Hardware** | 4 x Intel Xeon Gold 6252N, 1 TiB RAM |
> | **Recall Performance** | 90% - 98% (@10) |
> | **Quality Score** | 9/10 |

---

## üìù Executive Summary

While learned sparse retrieval models like **SPLADE** have effectively bridged the gap between sparse lexical matching and dense semantic search, their scalability to massive, production-level datasets remains largely unproven. Most existing benchmarks evaluate these systems on relatively small corpora, leaving a critical gap in understanding the computational feasibility and performance characteristics of sparse embeddings at scales exceeding 100 million vectors.

This paper addresses that void by investigating the practical costs and retrieval behaviors of learned sparse systems when operating on the massive **MsMarco-v2 dataset (138 million passages)**. The study focuses on the **"Seismic" algorithm**, an approximate sparse retrieval approach that optimizes standard inverted indexing through geometric blocking. Technically, Seismic couples an inverted index with a forward index, where inverted lists are clustered into geometric blocks represented by summary vectors derived from maximum values.

During query processing, the algorithm computes the inner product between the query vector and these block summaries, pruning any blocks where the potential score falls below a set threshold. To further bridge the gap between approximate and exact retrieval, the authors introduce a refinement extension using a **kappa-NN graph overlay**, which expands the top-k candidate set with neighbors to allow for high-precision rescoring.

Benchmarked on a high-performance NUMA server (192 cores, 1 TiB RAM) against graph-based solutions adapted for dense retrieval (specifically kANNolo/HNSW), Seismic demonstrated superior efficiency and latency. The algorithm achieved high recall rates ranging from **90% to 98%** while operating **1 to 2 orders of magnitude faster** than standard inverted index solutions and outperforming the efficiency of leading graph methods from the 2023 BigAnn Challenge. Specifically, the integration of the kappa-NN graph refinement allowed the system to achieve near-exact retrieval accuracy, delivering up to a **2.2x speedup** over the base configuration.

This research provides the field with its first comprehensive benchmark of learned sparse retrieval algorithms at extreme scale, validating that these methods are viable for real-world production deployment. By quantifying indexing time and efficiency metrics for SPLADE embeddings on 138 million vectors, the authors offer actionable data for engineers optimizing large-scale search systems.

---

## üîë Key Findings

*   **Scalability Validation:** The study successfully validated the scalability of evaluating learned sparse retrieval systems on a massive dataset of **138 million passages**.
*   **Algorithm Contrast:** It contrasted the performance of the **'Seismic' algorithm** directly against graph-based solutions adapted for dense retrieval.
*   **Efficiency Costs:** Highlighted practical efficiency costs, specifically focusing on **indexing time**, which is often overlooked in smaller-scale benchmarks.
*   **Scale-Specific Behavior:** Revealed distinct **behavioral challenges and retrieval characteristics** when operating at the massive scale of MsMarco-v2, differing significantly from standard datasets.

---

## üî¨ Research Methodology

The researchers employed a rigorous comparative framework to evaluate retrieval systems at an unprecedented scale:

*   **Data Source:** Utilized **Splade embeddings** generated from 138 million passages within the MsMarco-v2 dataset.
*   **Comparative Analysis:** Performed a head-to-head comparison between the **'Seismic' approximate algorithm** and graph-based solutions adapted from dense retrieval frameworks.
*   **Evaluation Metrics:** Focused on dual metrics:
    *   **Effectiveness:** Measured via top-$k$ retrieval performance.
    *   **Efficiency:** Measured via indexing time and query latency.

---

## ‚ú® Core Contributions

This research makes three pivotal contributions to the field of Information Retrieval:

1.  **Bridging the Literature Gap:** Evaluates algorithms on datasets orders of magnitude larger than typical benchmarks, addressing the lack of extreme-scale research in learned sparse retrieval.
2.  **Comprehensive Benchmarking:** Provides the field with concrete data regarding the **indexing time and efficiency** of SPLADE embeddings on 138M vectors.
3.  **Production Guidance:** Offers critical insights and practical guidance for the **real-world deployment** of state-of-the-art retrieval systems on production-scale data volumes.

---

## ‚öôÔ∏è Technical Details

### Algorithm Architecture

#### 1. Seismic (Inverted Index-Based)
*   **Architecture:** Inverted index coupled with a forward index.
*   **Core Innovation:** Inverted lists are clustered into geometric blocks represented by summary vectors (max values).
*   **Query Processing:**
    1.  Computes inner product between query and block summary vector.
    2.  **Pruning:** Discards blocks where potential score is lower than a defined threshold.
    3.  **Access:** Accesses forward index for exact inner products only if potential is sufficient.
*   **Refinement Extension:** Adds a **kappa-NN graph overlay** to expand top-$k$ set $S$ with neighbors $N(u)$ and rescore for higher precision.

#### 2. Graph-Based Solutions (Baseline)
*   **Implementation:** **kANNolo** (Rust framework implementing HNSW).
*   **Optimizations:**
    *   16-bit integer coordinates.
    *   16-bit half-precision float values (PyAnn).
    *   Co-located coordinates and values (GrassRMA).
    *   Upper/lower bounds per vector for early termination.

### Data Representation
| Parameter | Specification |
| :--- | :--- |
| **Embedding Model** | SPLADE |
| **Similarity Metric** | Inner Product (Maximum Inner Product Search - MIPS) |
| **Scale** | MsMarco-v2 (~138 million vectors) |

---

## üìà Results & Benchmarks

### Experimental Configuration
*   **Constraints:** Index size $\le$ 1.5x raw dataset size.
*   **Seismic Config:**
    *   $\lambda$: $\{3...9\} \times 10^4$
    *   Summary Energy ($\alpha$): 0.4
    *   Refinement Kappa: $\{10, 20\}$
*   **HNSW Config:**
    *   $M$: $\{16, 32, 64\}$
    *   $ef_c$: 500

### Evaluation Metrics
*   Average Latency (msec)
*   Recall@10 (Target: 90% - 98%)
*   Index Size (GB)
*   Indexing Time

### Hardware Setup
| Specification | Detail |
| :--- | :--- |
| **Environment** | NUMA server with strict localization |
| **CPU** | 4 x Intel Xeon Gold 6252N @ 2.30 GHz |
| **Cores** | 192 (96 physical, 96 hyper-threaded) |
| **RAM** | 1 TiB (256 GiB per CPU node) |
| **Software** | Rust 1.81 (release mode) |

### Performance Insights
*   **Sub-millisecond Latency:** On smaller datasets (MsMarco 8.8M), Seismic achieved sub-millisecond latency with high recall.
*   **Significant Speedup:** The system demonstrated **1-2 orders of magnitude faster** speed than inverted index solutions and proved more efficient than 2023 BigAnn Challenge graph methods.
*   **Optimization Impact:** The kappa-NN graph refinement made Seismic **almost exact** and resulted in up to a **2.2x speedup**.

---

## üìã Quality Assessment

| Criterion | Score |
| :--- | :--- |
| **Overall Quality** | **9/10** |
| **References** | 32 citations |

---