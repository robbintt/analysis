---
title: An Empirical Study of Agent Developer Practices in AI Agent Frameworks
arxiv_id: '2512.01939'
source_url: https://arxiv.org/abs/2512.01939
generated_at: '2026-01-26T12:43:05'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# An Empirical Study of Agent Developer Practices in AI Agent Frameworks

*The University, Sun Yat, Agent Developer, Technical University, Zhejiang University*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 7/10
> *   **Total Citations:** 40
> *   **Repositories Analyzed:** 1,575
> *   **Developer Discussions:** 11,910
> *   **Dominant Framework:** LangChain (6,006 discussions)
> *   **Core Architecture:** 4 Components (Brain, Memory, Planning, Tools)
> *   **Ecosystem Scale:** >100 Frameworks, >400k Stars, >70k Forks

---

## Executive Summary

### **Problem**
The rapid expansion of the Large Language Model (LLM) ecosystem has produced over 100 open-source agent frameworks, creating a fragmented landscape where adoption outpaces rigorous empirical understanding. Despite the immense popularity of these tools—with the broader ecosystem accumulating over **400,000 stars** and **70,000 forks**—there is a critical absence of data on how developers actually utilize these architectures. The field currently relies on anecdotal evidence and feature lists, lacking insight into real-world behavioral patterns, architectural decision-making, and the specific challenges developers face during the software development life cycle (SDLC).

### **Innovation**
To systematically evaluate this chaotic environment, the authors introduced a standardized **four-component architectural model** (Brain, Memory, Planning, and Tools) which serves as a unified ontology for dissecting diverse agent frameworks. Utilizing this model in conjunction with the **SDLC**, the researchers conducted a rigorous mining of GitHub repositories, analyzing 11,910 raw discussions filtered by specific engagement thresholds. This methodology shifted the focus from static code analysis to dynamic developer workflows, mapping specific interactions to SDLC phases: Requirements, Design, Development, Testing, Deployment, and Maintenance.

### **Results**
The empirical analysis of 1,575 repositories revealed distinct behavioral preferences. **LangChain** dominated discourse volume with 6,006 instances, attributed to a strong developer preference for sequential and hierarchical processing paradigms. **LlamaIndex** followed with 1,691 instances, reflecting a specialized focus on retrieval-heavy implementations. Crucially, the study demonstrated that the four-component model could successfully abstract complex, disparate systems—from AutoGen’s conversation-based flows to MetaGPT’s SOP-driven architecture—into comparable metrics. Lower discussion volumes for frameworks like BabyAGI, Swarm, and CrewAI suggest that while architectural innovations like role-playing exist, they currently represent niche behavioral use cases.

### **Impact**
This study provides the first comprehensive empirical grounding for AI agent engineering, transitioning the field from hype-based adoption to a disciplined, data-driven practice. By successfully coupling a universal component model with SDLC-phase mapping, the research offers a robust mechanism for evaluating tool maturity and identifying architectural gaps. These findings empower developers to select frameworks based on proven usage patterns rather than mere popularity, establishing a standardized vocabulary essential for the continued evolution of AI agents.

---

## Key Findings

*   **Architecture Standardization:** A unified four-component model (Brain, Memory, Planning, Tools) effectively abstracts diverse agent frameworks into a comparable ontology.
*   **Dominance of Sequential Models:** Developers show a strong preference for sequential and hierarchical processing (LangChain) over more experimental paradigms like self-improving loops (BabyAGI).
*   **Integration with SDLC:** Agent development is successfully mapped to standard Software Development Life Cycle phases, with distinct challenges arising in Testing and Maintenance compared to traditional software.
*   **Retrieval Focus:** High discussion volume around LlamaIndex indicates a specific, heavy developer focus on Retrieval-Augmented Generation (RAG) capabilities.
*   **Niche Paradigms:** Conversational (AutoGen) and Role-playing (Camel) paradigms are utilized but represent smaller segments of the developer ecosystem compared to chain-based workflows.

---

## Technical Details

### **Architectural Components**
The paper defines a standard LLM-based agent architecture consisting of four distinct parts:

| Component | Function |
| :--- | :--- |
| **Brain** | The LLM core responsible for processing information and decision-making. |
| **Memory** | Mechanisms for storing and retrieving past interactions and data. |
| **Planning** | Strategies for breaking down complex tasks into actionable steps. |
| **Tools** | External APIs or utilities the agent can utilize to perform actions. |

### **Frameworks Analyzed**
The study analyzed ten specific frameworks utilizing different computational paradigms:

| Framework | Paradigm |
| :--- | :--- |
| **LangChain** | Sequential / Hierarchical Chains |
| **LangGraph** | Directed Acyclic Graphs (DAGs) |
| **AutoGen** | Conversation-as-Computation |
| **CrewAI** | Hierarchical Crew |
| **MetaGPT** | Standard Operating Procedures (SOP) |
| **LlamaIndex** | Retrieval-Augmented Generation (RAG) |
| **Swarm** | Centralized Controller |
| **BabyAGI** | Self-Improving Loop |
| **Camel** | Role-Playing |
| **Semantic Kernel** | Plugin-Based |

### **Methodology & Data Collection**
*   **Data Source:** GitHub repositories and associated discussions.
*   **Filtering Criteria:**
    *   Stars > 10
    *   Forks > 5
    *   Creation Date: Jan 2022 – Jul 2025
*   **Process:**
    1.  Filtered repositories based on thresholds.
    2.  Processed developer discussions to understand behaviors.
    3.  Mapped activities to SDLC Phases (Requirements, Design, Development, Testing, Deployment, Maintenance).

---

## Results & Metrics

The experimental data provided a quantitative look at the landscape of AI agent development.

**Dataset Overview**
*   **Initial Raw Discussions:** 10,265
*   **Analyzed Discussions:** 11,910 (Filtered for relevance)
*   **Analyzed Repositories:** 1,575

**Framework Prevalence (by Discussion Volume)**
1.  **LangChain:** 6,006
2.  **LlamaIndex:** 1,691
3.  **AutoGen:** 988
4.  **LangGraph:** 978
5.  **Semantic Kernel:** 668
6.  **BabyAGI:** 403
7.  **Swarm:** 391
8.  **MetaGPT:** 312
9.  **Camel:** 249
10. **CrewAI:** 224

**Ecosystem Context**
The broader open-source landscape contains **over 100 agent frameworks** that have collectively accumulated:
*   **> 400,000** Stars
*   **> 70,000** Forks