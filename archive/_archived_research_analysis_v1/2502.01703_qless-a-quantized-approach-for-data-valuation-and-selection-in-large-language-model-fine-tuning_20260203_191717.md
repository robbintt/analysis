---
title: 'QLESS: A Quantized Approach for Data Valuation and Selection in Large Language
  Model Fine-Tuning'
arxiv_id: '2502.01703'
source_url: https://arxiv.org/abs/2502.01703
generated_at: '2026-02-03T19:17:17'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# QLESS: A Quantized Approach for Data Valuation and Selection in Large Language Model Fine-Tuning

*Moses Ananta; Muhammad Farid Adilazuarda; Zayd Muhammad Kawakibi Zuhri; Ayu Purwarianti; Alham Fikri Aji*

---

> ### ðŸ“Š Quick Facts
> * **Memory Efficiency:** Up to **16x reduction** in memory usage (16.54 GB â†’ 1.03 GB).
> * **Optimal Configuration:** Absmax 8-bit achieved the highest average score (**48.59**).
> * **Extreme Compression:** Sign 1-bit quantization maintained high performance (**48.46**), outperforming the baseline.
> * **Architectures:** Validated on LLaMA, Mistral, and Qwen.
> * **Paper Rating:** 9/10

---

## Executive Summary

Fine-tuning Large Language Models (LLMs) on massive, uncurated datasets is computationally expensive and often inefficient. While data valuation frameworks like LESS (Low-rank Gradient Similarity Search) can identify the most informative samples to improve performance, they require storing high-precision gradients for the entire dataset. This creates a prohibitive memory bottleneck, making intelligent data selection infeasible for organizations or researchers operating with limited hardware resources. This paper addresses the critical challenge of scaling data valuation techniques by drastically reducing the memory footprint required to evaluate and select high-quality training data.

The authors introduce **QLess (Quantized Low-rank Gradient Similarity Search)**, a novel framework that optimizes the LESS pipeline through a two-step compression mechanism. First, the method utilizes Low-Rank Adaptation (LoRA) with random projection to generate low-dimensional gradient representations. Second, it applies aggressive quantization to these projections, converting them into low-bitwidth formats (e.g., 1-bit) using strategies such as Absmax, Absmean, and Sign. By integrating this with QLoRA and bitsandbytes (NF4/8-bit) for model parameters, QLESS maintains the mathematical fidelity necessary for calculating gradient similarity while minimizing computational overhead.

QLess demonstrates exceptional memory efficiency and robust performance across multiple benchmarks. The framework achieves up to a **16x reduction in memory usage** (dropping to 1.03 GB) when using 1-bit gradients compared to the 16-bit baseline (16.54 GB). In evaluations on the Llama 2 7B model across TyDiQA, MMLU, and BBH benchmarks, the Absmax 8-bit configuration achieved the highest average score of 48.59, followed closely by the Sign 1-bit method at 48.46. Both quantized approaches outperformed the uncompressed 16-bit baseline (47.80) and random selection (46.04). Additionally, the integration of QLoRA reduced system memory during gradient extraction from 32â€“35 GB to 22â€“26 GB without degrading model accuracy.

This work significantly lowers the barrier to entry for efficient LLM fine-tuning by proving that extreme gradient quantization (down to 1-bit) is sufficient for high-quality data valuation. QLESS validates that data selection performance can be preserved even under aggressive compression, enabling researchers to perform intelligent data curation on consumer-grade hardware. By successfully demonstrating generalizability across major architectures like LLaMA, Mistral, and Qwen, this research establishes a new, resource-efficient paradigm for data selection that could fundamentally reduce the computational costs associated with training state-of-the-art models.

---

## Key Findings

*   **Significant Memory Reduction:** QLESS reduces memory usage by up to **16x** compared to the standard LESS framework.
*   **Preservation of Data Quality:** The method maintains data selection performance comparable to the uncompressed LESS approach despite aggressive compression.
*   **Resilience to Extreme Quantization:** The study demonstrates that **1-bit gradient quantization** is sufficient to preserve the quality of data valuation.
*   **Broad Generalizability:** Effectiveness was validated across multiple LLM architectures (LLaMA, Mistral, Qwen) and diverse benchmarks (MMLU, BBH, TyDiQA).

---

## Methodology

**QLess (Quantized Low-rank Gradient Similarity Search)** optimizes the LESS framework through a two-step compression pipeline designed for memory efficiency:

1.  **Dimensionality Reduction:** Generates low-dimensional gradient representations using LoRA-based random projection.
2.  **Quantization:** Converts these projected gradients into low-bitwidth representations (e.g., 1-bit).

This process enables the calculation of gradient similarity for data valuation and selection while strictly minimizing memory overhead.

---

## Technical Details

**Framework Extension**
QLLESS is an extension of the LESS framework that supports quantized gradient storage and computation to enable data valuation on resource-constrained hardware.

**Quantization Strategies Evaluated**
The method evaluates three specific quantization strategies:
*   **Absmax**
*   **Absmean**
*   **Sign (1-bit)**

**Integration with Efficiency Techniques**
The method integrates QLoRA, utilizing:
*   **LLM.int8 (8-bit)**
*   **bitsandbytes NF4 (4-bit)**

These are used for model parameters to minimize memory overhead while maintaining gradient fidelity for influence estimation.

---

## Results

*   **Memory Efficiency:** Scales linearly with bit-width reduction, achieving a 16x reduction at 1-bit (**1.03 GB**) compared to the 16-bit baseline (**16.54 GB**).
*   **Llama 2 7B Benchmarks (TyDiQA, MMLU, BBH):**
    *   **Absmax 8-bit:** Highest average performance (**48.59**)
    *   **Sign 1-bit:** Followed closely with (**48.46**)
    *   **16-bit Baseline:** Scored (**47.80**)
    *   **Random Selection:** Scored (**46.04**)
*   **QLoRA Integration:** Reduced system memory during gradient extraction from **32â€“35 GB** to **22â€“26 GB**, with performance remaining competitive and significantly above random levels.

---

## Contributions

*   **Scalable Data Selection:** Introduces a practical solution for identifying informative data samples in massive datasets, making data selection feasible under strict memory constraints.
*   **Algorithmic Innovation:** Successfully integrates gradient quantization with low-rank approximation techniques (LoRA) within the context of data valuation.
*   **Resource-Efficient Fine-Tuning:** Provides a pathway to reduce the computational costs associated with LLM fine-tuning without sacrificing the performance benefits of intelligent data selection.

---

**Quality Score:** 9/10  
**References:** 40 citations