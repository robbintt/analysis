---
title: 'Rectified SpaAttn: Revisiting Attention Sparsity for Efficient Video Generation'
arxiv_id: '2511.19835'
source_url: https://arxiv.org/abs/2511.19835
generated_at: '2026-02-06T02:28:43'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Rectified SpaAttn: Revisiting Attention Sparsity for Efficient Video Generation

*Xuewen Liu; Zhikai Li; Jing Zhang; Mengjuan Chen; Qingyi Gu*

---

### üìã Quick Facts

| Metric | Value |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Top Speedup** | **3.33x** on HunyuanVideo |
| **Secondary Speedup** | **2.08x** on Wan 2.1 |
| **Core Problem** | Systematic Biases (Amplification & Neglect) |
| **Solution** | Implicit Full Attention Reference |
| **Key Mechanisms** | IPAR & GAPR |
| **References** | 40 Citations |

---

### üìã Executive Summary

> This research addresses the computational bottleneck inherent in video generation using Diffusion Transformers (DiTs), where the quadratic complexity of standard attention mechanisms leads to significant latency. While sparse attention is a common strategy to improve efficiency, this paper identifies that existing methods suffer from **systematic biases** that degrade output quality.
>
> Specifically, current sparse attention patterns exhibit "Critical Token Bias" (excessive focus on key tokens) and "Non-Critical Token Bias" (complete neglect of others). These biases make it difficult to maintain visual fidelity when increasing sparsity, creating a critical trade-off between generation speed and quality that limits the practical deployment of high-resolution video models.
>
> The authors propose **"Rectified SpaAttn,"** a framework designed to correct these biases by aligning sparse attention maps with an implicit full attention reference without the computational overhead of calculating the full matrix. The method utilizes block-level pooling to estimate this reference efficiently. Technically, the framework introduces two distinct mechanisms: **Isolated-Pooling Attention Reallocation (IPAR)**, which isolates text tokens to rectify the distribution of attention weights for critical tokens, and **Gain-Aware Pooling Rectification (GAPR)**, which generates compensation masks for non-critical tokens only when the rectification gains surpass the errors introduced by pooling. This approach ensures a more balanced attention allocation across the sequence.
>
> Validated on state-of-the-art models HunyuanVideo and Wan 2.1, Rectified SpaAtten achieves substantial efficiency gains while preserving visual quality. The implementation delivers up to a **3.33x speedup on HunyuanVideo** and a **2.08x speedup on Wan 2.1**. Crucially, these performance improvements do not compromise generation quality; the framework maintains superior Vbench Scores even under aggressive sparsity conditions of 75% and 90%.
>
> The significance of this work lies in its combined theoretical and practical contributions to the field of efficient generative AI. The authors provide the first theoretical analysis defining the amplification and neglect biases in sparse attention, offering a new lens through which to evaluate efficiency methods. Furthermore, by delivering a practical, optimized open-source implementation using a customized Triton kernel, the research bridges the gap between theoretical efficiency and deployable performance.

---

## üîë Key Findings

*   **Identification of Systematic Biases:** Existing sparse attention methods suffer from specific systematic biases: an excessive focus on critical tokens and a complete neglect of non-critical tokens.
*   **Alignment Strategy:** The proposed method rectifies attention allocation by aligning sparse attention maps with an implicit full attention reference, mitigating the identified biases.
*   **Significant Efficiency Gains:** Implementation achieves substantial speedups‚Äîup to **3.33x on HunyuanVideo** and **2.08x on Wan 2.1**‚Äîwithout compromising visual quality.
*   **Theoretical Insight:** Attention biases for critical tokens are proportional to sparse attention weights, and pooling strategies effectively recover attention for non-critical tokens when gains surpass pooling errors.

---

## üõ†Ô∏è Methodology

The proposed method, **Rectified SpaAttn**, optimizes video generation by addressing computational latency in Diffusion Transformers. It rectifies attention allocation through two primary mechanisms:

1.  **Isolated-Pooling Attention Reallocation (IPAR):** Reallocates multimodal pooled weights specifically for critical tokens to prevent over-amplification.
2.  **Gain-Aware Pooling Rectification (GAPR):** Ensures recovered attention gains surpass pooling errors for non-critical tokens, preventing the complete neglect of less dominant data.

Additionally, the method utilizes a **customized Triton kernel** to ensure practical deployment and efficiency.

---

## ‚öôÔ∏è Technical Details

Rectified SpaAttn is a framework designed to correct systematic biases in sparse attention for video generation. It addresses two specific failure points in current models:

*   **Critical Token Bias:** Excessive focus on key tokens.
*   **Non-Critical Token Bias:** Complete neglect of other tokens.

### Core Components

To resolve these issues without high computational cost, the method employs the following strategies:

*   **Block-Level Pooling:** Used to estimate an implicit full attention reference, avoiding the need to calculate the full matrix explicitly.
*   **IPAR (Isolated-Pooling Attention Reallocation):** Isolates text tokens to rectify critical token bias using a rectification factor.
*   **GAPR (Gain-Aware Pooling Rectification):** Generates compensation masks for non-critical tokens based on a rigorous trade-off analysis between rectified gains and pooling errors.

---

## üìà Results

The framework demonstrates robust performance across multiple state-of-the-art models:

*   **Speed Efficiency:**
    *   **HunyuanVideo:** Up to **3.33x** speedup.
    *   **Wan 2.1:** Up to **2.08x** speedup.
*   **Visual Quality:** Maintains superior generation quality, as measured by **Vbench Score**, even under high sparsity conditions (**75%** and **90%**).
*   **Module Performance:** The inclusion of the GAPR module consistently improved performance across different model types, including Hunyuan Video and Wan Video (for both Text-to-Video and Image-to-Video tasks).

---

## üèÜ Contributions

1.  **Theoretical Analysis:** Provided the first identification and definition of systematic biases (amplification and neglect) in current efficient video generation methods.
2.  **Framework Introduction:** Introduced Rectified SpaAttn, a novel framework using an implicit full attention reference to correct attention allocation.
3.  **Algorithmic Innovation:** Proposed two distinct mechanisms: Isolated-Pooling Attention Reallocation (IPAR) and Gain-Aware Pooling Rectification (GAPR).
4.  **Practical Implementation:** Delivered a practical, optimized open-source implementation validated on state-of-the-art models (HunyuanVideo and Wan 2.1).

---

**References:** 40 citations