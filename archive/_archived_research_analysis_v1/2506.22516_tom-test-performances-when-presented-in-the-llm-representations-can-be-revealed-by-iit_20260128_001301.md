---
title: ToM test performances, when presented in the LLM representations, can be revealed
  by IIT
arxiv_id: '2506.22516'
source_url: https://arxiv.org/abs/2506.22516
generated_at: '2026-01-28T00:13:01'
quality_score: 8
citation_count: 11
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# ToM test performances, when presented in the LLM representations, can be revealed by IIT

*Representations Obtained, Large Language, Internal States, Span Representation, Jingkai Lia, Be Observed, Mind Test, Integrated Information*

> ### ðŸ“Š Quick Facts
>
> | Metric | Details |
> | :--- | :--- |
> | **Quality Score** | 8/10 |
> | **Citations** | 11 References |
> | **Target Models** | LLaMA 3.1 (8B, 70B), Mistral-7B, Mixtral-8x7B |
> | **Benchmark Tasks** | 5 ToM Tasks (Hinting, False Belief, Faux Pas, Strange Stories, Irony) |
> | **Core Method** | Integrated Information Theory (IIT 3.0 & 4.0) |
> | **Key Insight** | ToM distinctions are architecture and layer-specific, unlikely to be consciousness-driven. |

---

## Executive Summary

**Problem**
Evaluating Theory of Mind (ToM) in Large Language Models (LLMs) has traditionally relied on behavioral accuracy metrics, treating these models as black boxes that either pass or fail cognitive benchmarks. This approach is insufficient because it fails to elucidate the mechanistic underpinnings of high-level cognitive capabilities like perspective-taking, false belief attribution, and irony detection. Understanding *where* and *how* these capabilities are encoded internally is critical for the field; without this insight, we cannot determine if models are simulating reasoning or merely relying on statistical correlations in surface-level text.

**Innovation**
This paper introduces a novel interpretability framework that applies **Integrated Information Theory (IIT)**â€”a mathematical framework originally designed to quantify consciousnessâ€”to the internal representations of LLMs. Technically, the authors extract high-dimensional Attended Response Representations (ARR) and Concatenated ARR (CARR) from specific network layers and linguistic spans (e.g., Complement, Mental State Verbs). These vectors are dimensionally reduced via PCA to 4 nodes and binarized to create discrete states. Using the PyPhi library, the team computes IIT 3.0 and 4.0 metrics, including integrated information ($\Phi$) and conceptual information ($\varphi$), to quantify the structure of information within the model, thereby correlating internal state integration with performance on ToM tasks.

**Results**
The study evaluated five ToM tasks (Hinting, False Belief, Faux Pas, Strange Stories, Irony) across LLaMA 3.1 (8B, 70B) and Mistral architectures. While general IIT metrics rarely surpassed the mean AUC of baseline Span Representation methods, the study identified four specific "exceptional cases" where IIT 4.0 effectively revealed ToM performance distinctions:
1.  LLaMA3.1-8B, Layer 26 (Hinting)
2.  LLaMA3.1-8B, Layer 29 (Strange Stories)
3.  LLaMA3.1-70B, Layer 80 (Strange Stories)
4.  LLaMA3.1-70B, Layer 53 (Hinting)

Conversely, the analysis yielded negative results for the "False Belief" task and found zero successful cases for tasks utilizing 3-score ratings.

**Impact**
This research bridges the gap between cognitive science and AI, demonstrating that high-level cognitive feats can be observed and measured through mathematical frameworks applied to internal model states. By showing that ToM performance is likely driven by localized span-level information processing rather than high-level consciousness indicators (as evidenced by the low prevalence of high $\Phi$ values), the authors provide a critical distinction between statistical competence and cognitive structure. This contribution offers the field a robust new tool for mechanistic interpretability, allowing researchers to move beyond accuracy metrics and investigate the specific architectural loci of complex reasoning in future generations of foundation models.

---

## Key Findings

*   **Revealing ToM via IIT:** The study demonstrates that Theory of Mind (ToM) test performances in Large Language Models (LLMs) can be effectively revealed and quantified using Integrated Information Theory (IIT).
*   **Internal Representation Analysis:** The capabilities for "Mind Tests" are not just behavioral but are embedded within the internal states and span representations of the LLMs.
*   **Observability of High-Level Cognition:** Results suggest that high-level cognitive feats (like ToM) can be observed and measured through the mathematical framework of IIT applied to model representations.

## Methodology

*   **Representation Extraction:** The approach involves extracting high-dimensional representations obtained from Large Language models.
*   **Span & Internal State Analysis:** The methodology focuses on analyzing specific Span Representations and hidden Internal States rather than just input-output behavior.
*   **IIT Application:** Integrated Information Theory (IIT) metrics are computed on these internal representations to assess the structure and integration of information, correlating these metrics with performance on Mind Tests (ToM benchmarks).

## Technical Details

| Aspect | Specification |
| :--- | :--- |
| **Framework** | Applied Integrated Information Theory (IIT) versions **3.0** and **4.0** to LLM internal representations to correlate Theory of Mind (ToM) capabilities with consciousness measures. |
| **Hypothesis** | Posits that cognitive tasks are embedded in internal states. |
| **Data** | Five ToM tasks (Hinting, False Belief, Faux Pas, Strange Stories, Irony) rated on a scale of 0â€“2. |
| **Target Models** | LLaMA3.1-8B, LLaMA3.1-70B, Mistral-7B, Mixtral-8x7B. |
| **Auxiliary Models** | GPT-4o, Claude 3.5 Sonnet, Gemini. |
| **Pipeline** | Representation Extraction focuses on **Attended Response Representations (ARR)** and **Concatenated ARR (CARR)** across specific layers (last three, 2/3 point) and linguistic spans (Entire, Complement, Mental State Verbs). |
| **Dimensionality Reduction** | PCA reduced to 4 dimensions (nodes). |
| **Binarization** | z-score normalization relative to baseline mean with threshold 0, creating 16 possible states. |
| **Metrics** | PyPhi library calculates Scalar metrics ($\Phi_{max}$, $\Phi$) and Vector metrics (Conceptual Information for IIT 3.0; $\Phi$-structure for IIT 4.0). |
| **Baseline** | Span Representation metrics per Peters et al. and Jawahar et al. |

## Results

*   **General:** IIT metrics revealed distinctions in ToM performance, highly specific to model architecture, layer depth, and linguistic span. Compared against 10 randomization controls.
*   **Baseline Comparison:** IIT estimates rarely surpassed Span Representation mean AUC.
*   **Exceptional Cases (IIT 4.0 Predominance):**
    *   LLaMA3.1-8B, Layer 26, Hinting (Entire Span)
    *   LLaMA3.1-8B, Layer 29, Strange Stories (Entire Span)
    *   LLaMA3.1-70B, Layer 80, Strange Stories (Complement Span)
    *   LLaMA3.1-70B, Layer 53, Hinting (Mental State Verbs Span)
*   **Negative Results:**
    *   No layers identified for **False Belief**.
    *   Zero cases found for tasks with 3-score ratings.
*   **Interpretation:** ToM performance is likely attributed to **span-level information processing** rather than consciousness.

## Contributions

*   **Interpretability Framework:** Provides a novel interpretability framework by applying Integrated Information Theory (originally a theory of consciousness) to evaluate specific cognitive capabilities (Theory of Mind) in LLMs.
*   **Mechanistic Insight:** Moves beyond accuracy metrics to offer a mechanistic understanding of how and where ToM information is stored within the model's architecture (via internal states).
*   **Bridging Cognitive Science and AI:** Bridges the gap between cognitive science concepts (ToM) and information-theoretic measures (IIT) in the context of artificial intelligence.

---
*Quality Score: 8/10 | References: 11 citations*