# AdPO: Enhancing the Adversarial Robustness of Large Vision-Language Models with Preference Optimization

*Chaohu Liu; Tianyi Gui; Yu Liu; Linli Xu*

---

> ### ‚ö° Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Dataset** | VQAv2 |
> | **Tested Model** | InstructBLIP / CLIP ViT |
> | **Clean Accuracy** | **78.34%** (AdvFT Baseline: 74.31%) |
> | **Robust Accuracy** | **34.21%** (AdvFT Baseline: 31.58%) |
> | **Parameter Efficiency** | Only **4-5%** of total parameters (Image Encoder) |
> | **Qualitative Score** | 9/10 |

---

## üìë Executive Summary

Large Vision-Language Models (LVLMs) are critical for modern applications, yet they remain fundamentally fragile against adversarial attacks‚Äîsubtle input perturbations that cause significant errors. The primary obstacle in securing these models is the **robustness-accuracy trade-off**: traditional adversarial training improves resilience only by sacrificing performance on clean data. Additionally, the computational cost of fine-tuning billion-parameter models is often prohibitive.

This research introduces **AdPO (Adversarial Preference Optimization)**, a novel framework that addresses these challenges by reframing adversarial defense as a preference optimization problem based on Direct Preference Optimization (DPO). Rather than standard loss minimization, AdPO aligns the model to prefer correct outputs on clean inputs while explicitly rejecting misleading outputs on adversarial examples.

The proposed method demonstrates that modifying **only the image encoder** (approximately 4-5% of the model) is sufficient to propagate robustness across tasks. Furthermore, AdPO utilizes a "train small, transfer large" pipeline, drastically reducing computational overhead. Evaluations on the VQAv2 dataset using InstructBLIP show that AdPO outperforms standard adversarial fine-tuning (AdvFT), achieving a clean accuracy of **78.34%** (recovering the loss seen in baselines) and a robust accuracy of **34.21%**. This establishes a new, resource-efficient paradigm for deploying secure multimodal AI systems.

---

## üîç Key Findings

*   **Robustness without Clean Performance Degradation**: AdPO successfully enhances adversarial robustness while maintaining superior performance on clean data, effectively breaking the standard trade-off.
*   **Image Encoder Sufficiency**: The study confirms that modifying solely the image encoder is sufficient to achieve robustness across various downstream tasks, eliminating the need to fine-tune the entire LVLM.
*   **Efficiency via Model Transfer**: A "train small, transfer large" strategy allows robustness learned on smaller models to be transferred to larger models, achieving competitive performance with baseline-level efficiency.
*   **Preference Optimization Effectiveness**: Reframing adversarial training as a preference optimization problem effectively aligns the model to reject misleading adversarial outputs while favoring normal outputs.

---

## üõ†Ô∏è Methodology

The core of the proposed **AdPO (Adversarial Preference Optimization)** strategy is a shift in how adversarial training is conceptualized:

1.  **Preference Optimization Formulation**: Instead of treating adversarial defense as a simple minimization problem, AdPO frames it as a preference optimization problem. The model is trained to prefer generating normal outputs on clean inputs while rejecting misleading outputs on adversarial examples.
2.  **Selective Modification**: To minimize computational load and preserve the linguistic capabilities of the LVLM, the approach selectively modifies only the **image encoder** (e.g., CLIP ViT) while keeping the rest of the large model frozen.
3.  **Scalable Transfer Learning**: The method employs a pipeline where robustness is learned on smaller LVLMs. The learned parameters are then transferred to larger models, making high-level security accessible without the prohibitive cost of training massive architectures from scratch.

---

## ‚öôÔ∏è Technical Details

*   **Algorithm Base**: Built upon Direct Preference Optimization (DPO), learning simultaneously from positive (clean) and negative (adversarial) samples to align model outputs.
*   **Architecture Focus**: Modifications are isolated strictly to the image encoder.
*   **Optimization Strategy**: Employes a **dual-objective optimization** strategy designed to maximize adversarial robustness while maintaining clean image understanding capabilities.
*   **Training Mechanism**:
    *   Utilizes **online training** with untargeted attacks.
    *   Supports **parameter transfer** from smaller to larger models.
    *   Introduces a balancing hyperparameter $\lambda$ to manage the trade-off between clean accuracy and robustness.

---

## üìä Results

Evaluations conducted on the **VQAv2 dataset** using untargeted attacks (APGD-10) yielded the following outcomes:

*   **Performance Metrics**:
    *   **Clean Accuracy**: Achieved **78.34%** (compared to AdvFT baseline of 74.31%).
    *   **Robust Accuracy**: Achieved **34.21%** (compared to AdvFT baseline of 31.58%).
*   **Ablation Study ($\lambda$)**:
    *   Increasing $\lambda$ improves clean performance but reduces robustness.
    *   Decreasing $\lambda$ enhances robustness at the cost of clean accuracy.
    *   **$\lambda = 1$** was identified as the optimal equilibrium point.
*   **Transfer Learning**: Updating only the image encoder (approx. 4-5% of total parameters) successfully extended robustness to larger models without full retraining costs.
*   **Qualitative Analysis**: AdPO showed superior visual and textual reasoning capabilities compared to previous adversarial fine-tuning methods when handling perturbed inputs.

---

## üåü Contributions

*   **Novel Defense Paradigm**: Introduces the first application of preference optimization to adversarial defense in LVLMs, offering a new theoretical perspective for model security.
*   **Balanced Performance Metric**: Demonstrates empirically that high adversarial robustness does not require a sacrifice in clean input accuracy.
*   **Resource-Efficient Solution**: Validates a 'train small, transfer large' strategy and focuses updates on the image encoder to provide a practical path for deploying robust LVLMs under computational constraints.

---

**Quality Score:** 9/10 | **References:** 40 citations