# Adversarial Robustness of Vision in Open Foundation Models

*Authors: Jonathon Fox, William J Buchanan, Pavlos Papadopoulos*

---

> ### ðŸ“Š Quick Facts
> **Target Models**: LLaVA-1.5-13B, Llama 3.2 Vision-8B-2
> **Attack Vector**: Visual Input (Adversarial Images)
> **Methodology**: Projected Gradient Descent (PGD)
> **Dataset**: VQA v2
> **Key Metric**: Llama 3.2 maintained **60.25%** accuracy under attack vs. LLaVA's **28.73%** (despite LLaVA's higher baseline).
> **Quality Score**: 9/10

---

## Executive Summary

This paper addresses the critical security vulnerability of open-weight Vision-Language Models (VLMs), specifically scrutinizing the susceptibility of their visual input components to adversarial manipulation. In the context of rapid deployment in sensitive applications, the authors challenge the prevailing assumption that models with larger parameter sizes and higher general capabilities possess inherent security against such attacks. The research investigates whether the visual encoders of modern architectures serve as a viable attack vector, capable of degrading performance and reliability independent of the models' natural language processing strengths.

The study introduces a rigorous empirical evaluation framework designed to decouple general perceptual accuracy from adversarial robustness. Technically, the researchers employed untargeted Projected Gradient Descent (PGD) attacks to generate adversarial perturbations on the VQA v2 dataset, specifically targeting the visual encoders of Transformer-based VLMs while keeping textual inputs constant. This methodological approach isolates the image processing pipeline as the primary attack surface, facilitating a direct comparison of how distinct architectural designs and training methodologies handle malicious perturbations, irrespective of the models' raw question-answering capabilities.

The evaluation yielded counter-intuitive quantitative results that debunk the correlation between capability and security. Using a PGD attack strength of $\epsilon = 8/255$, LLaVA-1.5-13B achieved a higher baseline VQA accuracy of **81.56%** but suffered catastrophic degradation, dropping to a robust accuracy of **28.73%**. In stark contrast, Llama 3.2 Vision-8B-2 demonstrated superior adversarial hardening; despite a lower baseline accuracy of **78.15%**, it exhibited a significantly smaller performance drop, maintaining **60.25%** accuracy under the same attack conditions. These statistics confirm that standard benchmark performance is not a predictor of resilience, as the less accurate baseline model proved substantially more robust against visual perturbations.

This research significantly influences the field of AI safety by validating that image-based threats remain highly effective against large, modern foundation models. It challenges the industry's reliance on general benchmarking as a proxy for security, exposing a critical blind spot in current evaluation protocols where high-performing models may still be fragile. The findings urge developers to prioritize adversarial training and architectural hardening over mere parameter scaling, ensuring that next-generation VLMs are engineered with intrinsic resistance to visual manipulation grounded in specific design choices rather than model size alone.

---

## Key Findings

*   **Viable Attack Vector**: The visual input component is a confirmed viable attack vector for degrading the performance of contemporary open-weight Vision Language Models (VLMs).
*   **Superior Robustness**: Llama 3.2 Vision demonstrated superior adversarial robustness with a smaller performance drop under attack compared to LLaVA-1.5-13B, despite having a lower baseline accuracy.
*   **Performance $\neq$ Security**: Standard benchmark performance does not directly correlate with adversarial robustness, meaning higher accuracy does not guarantee resistance to attacks.
*   **Architectural Influence**: Adversarial performance is significantly influenced by **architectural designs** and **training methodologies** rather than model size or general capability alone.

---

## Methodology

The researchers employed an empirical evaluation framework on two specific open-weight models:

1.  **LLaVA-1.5-13B**
2.  **Meta's Llama 3.2 Vision-8B-2**

The study utilized **untargeted Projected Gradient Descent (PGD)** attacks to generate adversarial perturbations specifically on the visual input. The models were tested on a subset of the **Visual Question Answering (VQA) v2** dataset. Performance was quantified using two primary metrics:
*   VQA Accuracy
*   Accuracy Degradation Metrics

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Target Systems** | Open-weight Vision-Language Models (VLMs), specifically LLaVA-1.5-13B and Llama 3.2 Vision. |
| **Core Mechanism** | Transformer-based architectures utilizing Attention mechanisms to align visual and textual embeddings. |
| **Attack Vector** | Focuses on the visual input component using Projected Gradient Descent (PGD) to generate adversarial images. |
| **Core Hypothesis** | Adversarial robustness is determined by architectural designs and training methodologies rather than model size or general capability. |
| **Transferability** | The research references the theoretical cross-model vulnerability (transferability) of adversarial examples. |

---

## Research Contributions

*   **Security Benchmarking**: Provides a timely security benchmarking for recently released open-weight models like Llama 3.2 Vision and LLaVA-1.5-13B.
*   **Evidence of Distinct Properties**: Contributes evidence that general perceptual accuracy and adversarial robustness are distinct properties.
*   **Validation of Threats**: Validates that image-based threats remain an effective method for confusing large, modern foundation models.
*   **Guidance for Future Dev**: Highlights the need to integrate architectural and training factors for security when developing future VLMs.

---

## Results

The comparative analysis revealed significant insights into model resilience:

*   **Llama 3.2 Vision**: Demonstrated superior adversarial robustness compared to LLaVA-1.5-13B, exhibiting a smaller performance drop under PGD attacks.
*   **Counter-Intuitive Outcome**: Llama 3.2 Vision achieved this robustness despite having a lower baseline accuracy. This indicates that standard benchmark performance does not directly correlate with adversarial robustness.
*   **Design over Scale**: Findings suggest that robustness is significantly driven by architectural designs and training methodologies rather than model size.

**Performance Comparison ($\epsilon = 8/255$)**:
*   **LLaVA-1.5-13B**: 81.56% (Baseline) $\to$ 28.73% (Under Attack)
*   **Llama 3.2 Vision**: 78.15% (Baseline) $\to$ 60.25% (Under Attack)

---

*   **References**: 28 citations