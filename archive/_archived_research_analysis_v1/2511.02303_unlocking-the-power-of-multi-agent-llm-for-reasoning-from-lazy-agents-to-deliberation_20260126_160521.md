---
title: 'Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to
  Deliberation'
arxiv_id: '2511.02303'
source_url: https://arxiv.org/abs/2511.02303
generated_at: '2026-01-26T16:05:21'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation

*Ramraj Chandradevan, Yudi Lin, Zhiwei Zhang, Linlin Wu, Qi He, Xiaomin Li, Minhua Lin, Fali Wang, Xianfeng Tang, Hui Liu*

---

> ### üìä Quick Facts
> *   **Quality Score:** 9/10
> *   **Total Citations:** 40
> *   **Proposed Framework:** ReMA
> *   **Primary Dataset:** MATH500
> *   **Key Performance Gain:** +1.2% accuracy with Anti-Lazy mechanisms
> *   **Training Method:** Multi-turn Group Relative Preference Optimization (GRPO)

---

## üìù Executive Summary

This research addresses a critical failure mode in multi-agent Large Language Models (LLMs) known as **"lazy agent behavior,"** where one agent dominates the interaction while others contribute minimally, effectively collapsing the system into an inefficient single-agent dynamic. The authors demonstrate that this is not merely an implementation flaw but a theoretically natural emergence driven by biases in standard optimization algorithms‚Äîspecifically, normalization terms that favor shorter trajectories. Furthermore, the paper highlights the issue of **"noisy interaction,"** where reasoning agents become trapped in feedback loops of previous errors during multi-turn collaborations.

The authors propose the **"ReMA" framework**, a multi-agent architecture consisting of a **Meta-thinking Agent** (responsible for planning) and a **Reasoning Agent** (responsible for execution), trained using Multi-turn Group Relative Preference Optimization (GRPO). To counter lazy behavior, they introduce **Causal Influence Measurement** to quantify and ensure balanced contributions. To handle noisy interactions, they implement a **Verifiable Reward Mechanism** that incentivizes "deliberation," allowing agents to discard noisy outputs and restart reasoning if trapped.

Experiments on the **MATH500 dataset** validated both the existence of lazy behavior and the efficacy of the proposed solutions. Standard training resulted in a performance drop (74.4% vs 75.0%), but the proposed mitigation strategies recovered performance to **75.6%**. This work moves the field from anecdotal observation to a mathematical understanding of agent dynamics, providing a blueprint for maintaining true multi-agent collaboration.

---

## üîë Key Findings

*   **Identification of "Lazy Agent Behavior":** Current multi-agent LLM setups frequently suffer from a critical tendency where one agent dominates the interaction and the other contributes minimally, effectively collapsing the system into a single-agent dynamic.
*   **Theoretical Underpinning:** The authors provide a theoretical analysis demonstrating that lazy behavior is a natural emergence in multi-agent reasoning systems, rather than just a random implementation flaw.
*   **Problem of Noisy Interaction:** As collaboration increases, reasoning agents are prone to getting lost in multi-turn interactions and becoming trapped by previous, noisy responses.
*   **Validation of Solutions:** Extensive experiments confirm that the proposed framework successfully mitigates lazy agent behavior and enhances performance on complex reasoning tasks.

---

## üõ†Ô∏è Methodology

The research employs a multi-agent architecture consisting of a **meta-thinking agent** (responsible for planning and monitoring) and a **reasoning agent** (responsible for executing subtasks). To optimize this setup, the authors utilize a two-pronged technical approach:

*   **Causal Influence Measurement:** A stable and efficient method introduced to measure the causal influence of agents. This is specifically designed to mitigate the issue of one agent dominating the process.
*   **Verifiable Reward Mechanism:** A reward mechanism implemented to encourage "deliberation." This allows the reasoning agent to:
    *   Identify and discard noisy outputs.
    *   Consolidate instructions.
    *   Restart its reasoning process if it becomes trapped by previous errors.

---

## ‚öôÔ∏è Technical Details

The **ReMA framework** utilizes a multi-agent architecture with a Meta-thinking Agent and a Reasoning Agent sharing weights. They interact sequentially to generate high-level thoughts and token-level outputs.

### Architecture & Training
*   **Agents:** Meta-thinking Agent ($$a_h$$) and Reasoning Agent ($$a_l$$) sharing weights $$\theta$$.
*   **Optimization:** The system is trained using **Multi-turn Group Relative Preference Optimization (GRPO)**.
*   **Objective Function:** Includes clipping and KL penalties.
*   **Credit Assignment:** Trajectory-level credit assignment distributed uniformly across turns with a normalization factor of $$1/T-i$$.

### Causal Influence & Theory
*   **Measurement:** Causal influence is measured by suppressing attention to previous action tokens and calculating KL divergence.
*   **Theorem 1:** Indicates that the $$1/T(\pi)$$ normalization term introduces a structural bias favoring trajectories with fewer turns (lazy behavior) unless the gradient contribution of longer trajectories is proportionally larger.

---

## üìà Results

Experiments were conducted on the **MATH500 dataset**, utilizing accuracy and causal influence (KL divergence density) as primary metrics.

| Model/Condition | Accuracy | Observations |
| :--- | :--- | :--- |
| **Untrained Agents** | **75.0%** | Baseline performance. |
| **Standard Trained ReMA** | 74.4% | Performance drop observed. Reasoning agent showed significantly lower causal influence than the meta-thinking agent, confirming lazy behavior. |
| **With Anti-Lazy Prompts** | **75.6%** | Improved accuracy (+1.2 points over degraded baseline). While improved, prompts alone failed to fully resolve the underlying imbalance. |

*   **Empirical Analysis:** Validated that lazy behaviors correlate with fewer turns, confirming the theoretical prediction that structural bias steers optimization toward shorter trajectories.

---

## üèÜ Contributions

*   **Theoretical Analysis of Multi-Agent Dynamics:** Provides a theoretical explanation for why lazy agent behavior naturally arises in LLM multi-agent reasoning systems.
*   **Mitigation Strategy for Agent Imbalance:** Introduces a novel causal influence measurement method to ensure balanced contribution and prevent system collapse.
*   **Mechanism for Robust Deliberation:** Proposes a verifiable reward mechanism enabling the reasoning agent to autonomously manage noisy feedback loops, allowing for the resetting and consolidation of reasoning paths.
*   **Framework Enhancement:** Presents a complete framework that unlocks the full potential of multi-agent LLMs, moving beyond the limitations of current paradigms to achieve superior results in complex reasoning tasks.