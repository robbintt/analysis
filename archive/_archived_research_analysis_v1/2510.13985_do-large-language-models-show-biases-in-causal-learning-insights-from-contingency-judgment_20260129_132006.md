# Do Large Language Models Show Biases in Causal Learning? Insights from Contingency Judgment

*MarÃ­a Victoria Carro; Denise Alejandra Mester; Francisca Gauna Selasco; Giovanni Franco Gabriel Marraffini; Mario Alejandro Leiva; Gerardo I. Simari; MarÃ­a Vanina Martinez*

---

> ### ðŸ“Š Quick Facts
>
> *   **Dataset Size:** 1,000 null-contingency scenarios
> *   **Models Evaluated:** GPT-4o-Mini, Claude-3.5-Sonnet, Gemini-1.5-Pro
> *   **Task Type:** Contingency Judgment (Medical Context)
> *   **Key Phenomenon:** Illusion of Causality
> *   **Best Performing Model:** Gemini-1.5-Pro (20.5% correct rejections)
> *   **Statistical Significance:** $p < 0.001$
> *   **Citations:** 40

---

## Executive Summary

This research addresses the fundamental question of whether Large Language Models (LLMs) possess genuine causal reasoning capabilities or merely reproduce linguistic correlations observed in training data. The distinction is critical for the reliability of AI systems in high-stakes domains such as medicine, where inferring non-existent causal relationshipsâ€”the "illusion of causality"â€”can lead to erroneous diagnoses or dangerous decision-making. The authors sought to determine if state-of-the-art LLMs can objectively distinguish between cause and coincidence, a normative requirement often lacking in current architectures that prioritize pattern matching over logical inference.

To evaluate this, the authors introduced a large-scale adaptation of the "contingency judgment task," a classic cognitive psychology paradigm, specifically tuned for LLMs within a medical context. They constructed a dataset of 1,000 synthetic null-contingency scenarios where the probability of an outcome is identical regardless of the presence of a potential cause ($\Delta P = 0$). Scenarios utilized variables from four taxonomiesâ€”Fabricated, Indeterminate, Alternative Medicine, and Established Medicineâ€”and asked models to rate the effectiveness of potential causes on a discrete 0 to 100 scale (where 0 indicates ineffective and 100 indicates fully effective). The study evaluated GPT-4o-Mini, Claude-3.5-Sonnet, and Gemini-1.5-Pro using role-playing prompts across three distinct temperature configurations: High Variance ($t=1, n=10$), Deterministic ($t=0$), and Default, to assess the stability of the models' reasoning capabilities independent of stochastic settings.

The results revealed a systemic susceptibility to the illusion of causality across all models. Gemini-1.5-Pro proved the most robust relative to its peers, recording a median effectiveness rating of 45.0 with a correct rejection rate of 20.5%. Claude-3.5-Sonnet followed with a median rating of 50.0 and a correct rejection rate of 4.6%, while GPT-4o-Mini demonstrated the highest bias with a mean effectiveness rating of 75.74 and a 0% correct rejection rate. The reporting of mean for GPT-4o-Mini and medians for Claude and Gemini was necessitated by distribution differences between the models' outputs. Despite these variations, the failure to reject null causalities (all under 21%) was confirmed as statistically significant by Friedman and Wilcoxon signed-rank tests ($p < 0.001$).

These findings provide substantial empirical evidence that current LLMs lack a normative grasp of causal principles. Instead of performing logical inference, the models exhibit "superstitious" behaviors driven by the interplay between their training data and the induced medical personas, effectively mimicking professionals who might err on the side of intervention rather than adhering to statistical logic. This highlights a critical limitation in deploying LLMs for sensitive decision-making contexts, emphasizing that architectural improvements alone are insufficient. The study underscores the urgent need for robust guardrails capable of preventing persona-induced hallucinations in causal inference tasks.

---

## Key Findings

*   **Systemic Bias:** All evaluated Large Language Models (LLMs) demonstrated a strong susceptibility to the "illusion of causality," systemically inferring causal relationships where none existed.
*   **Consistency of Error:** Across 1,000 scenarios designed with null contingencies, the models consistently perceived unwarranted causal links.
*   **Pattern Reproduction vs. Understanding:** The results support the hypothesis that LLMs reproduce causal language patterns rather than possessing a genuine understanding or normative grasp of causal principles.
*   **Reliability Concerns:** This cognitive bias raises significant concerns regarding the reliability of LLMs in fields requiring accurate causal reasoning for informed decision-making, such as medical or social contexts.

---

## Methodology

The study employed a rigorous cognitive science framework to test model reasoning:

*   **Framework:** Utilized the "contingency judgment task," a classic cognitive science paradigm used to study causal learning and biases.
*   **Dataset Creation:** Created a dataset comprising 1,000 null contingency scenarios.
*   **Context:** Situated the experiments specifically within medical contexts to test the evaluation of potential causes.
*   **Evaluation Protocol:** Prompted various LLMs to assess the effectiveness of potential causes based on the provided scenarios where the available data was intentionally insufficient to support a causal relationship.

---

## Technical Details

**Experimental Design**
*   **Paradigm:** Adaptation of the contingency judgment task for LLMs.
*   **Scenario Logic:** 1,000 synthetic null-contingency scenarios where the probability of the outcome is identical regardless of the cause ($\Delta P = 0$).
*   **Variables:** Four taxonomies utilized:
    *   Fabricated
    *   Indeterminate
    *   Alternative Medicine
    *   Established Medicine
*   **Trial Count:** Scenarios contained between 20-100 trials.

**Models & Configuration**
*   **Models Tested:**
    *   GPT-4o-Mini
    *   Claude-3.5-Sonnet
    *   Gemini-1.5-Pro
*   **Prompting:** Role-playing prompts utilized.
*   **Configurations:**
    *   High Variance ($t=1, n=10$)
    *   Deterministic ($t=0$)
    *   Default
*   **Rating Scale:** Effectiveness rated on a 0 to 100 scale.

---

## Results

The statistical analysis confirmed significant failures in causal reasoning across all models:

*   **GPT-4o-Mini:** Demonstrated the **highest bias**.
    *   Mean Effectiveness: 75.74
    *   Correct Rejections: 0%
*   **Claude-3.5-Sonnet:** Demonstrated **moderate bias**.
    *   Median Effectiveness: 50.0
    *   Correct Rejections: 4.6%
*   **Gemini-1.5-Pro:** Demonstrated the **lowest relative bias**.
    *   Median Effectiveness: 45.0
    *   Correct Rejections: 20.5%

**Statistical Significance**
Friedman and Wilcoxon signed-rank tests confirmed significant differences in central tendencies and response probabilities across models ($p < 0.001$).

---

## Contributions

*   **First Large-Scale Examination:** Provided the first large-scale examination (via a 1,000-scenario dataset) of how the illusion of causality manifests in LLMs using a standard cognitive psychology framework.
*   **Empirical Evidence:** Contributed empirical evidence to the ongoing debate regarding whether LLMs understand causality, bolstering the argument that models rely on linguistic reproduction rather than true comprehension.
*   **Risk Identification:** Identified a critical limitation in current LLM architectures, highlighting the risks of deploying these models in domains that rely heavily on accurate causal inference and the potential for perpetuating misinformation or superstitious thinking.