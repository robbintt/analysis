# Transfer Learning in Latent Contextual Bandits with Covariate Shift Through Causal Transportability

## *Mingwei Deng; Ville Kyrki; Dominik Baumann*

---

### **Quick Facts**

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Total Citations** | 32 |
| **Dataset Used** | Proxy IHDP (Semi-synthetic) |
| **Latent Dimension** | 1 |
| **Proxy Measurements** | 25 |
| **Covariate Shift** | Source $N(1,1)$ $\to$ Target $N(-1,1)$ |

---

<br>

## 1. Executive Summary

This research addresses the critical challenge of transfer learning in latent contextual bandits, specifically under conditions of covariate shift. In real-world scenarios, agents often operate in environments where the underlying context is unobserved (latent) and must be inferred from high-dimensional proxy variables, while the data distribution differs between the source (training) and target (deployment) environments. The authors identify that naive transfer methods, which rely on statistical associations, frequently result in "**negative transfer**"â€”a phenomenon where the application of source knowledge actively degrades performance in the target domain.

The core innovation is the application of **Causal Transportability Theory** to filter which knowledge is transferred, moving away from correlation-based methods toward a causality-based framework. The proposed "Causal" architecture distinguishes between stable causal mechanisms and spurious statistical associations. Technically, the method employs Variational Autoencoders (VAEs) to map high-dimensional proxy variables ($W$) to a low-dimensional latent context ($Z$).

Empirical validation on a semi-synthetic Proxy IHDP dataset demonstrated that the causal approach successfully mitigates negative transfer and recovers the ground truth latent distribution. The results showed that the Causal agent achieved superior sample efficiency compared to baseline algorithms, maintaining lower cumulative regret by explicitly adapting to the target environment's causal structure. This work validates that causal inference provides a more robust foundation for transfer learning than statistical correlation alone.

---

## 2. Key Findings

*   **Demonstration of Negative Transfer:** The study establishes that naive knowledge transfer in latent contextual bandits leads to performance deterioration when a covariate shift exists.
*   **Efficacy of Causal Transportability:** Algorithms developed using causal transportability theory successfully mitigate negative transfer by explicitly transferring only effective knowledge.
*   **Improved Learning Efficiency:** The proposed causal framework demonstrates improved learning efficiency compared to baseline algorithms.
*   **Handling High-Dimensional Proxies:** The integration of variational autoencoders allows for effective approximation of causal effects even with high-dimensional proxy variables.

---

## 3. Methodology

The research employs a rigorous combination of causal inference and deep learning within the multi-armed bandits domain:

*   **Framework:** A causal inference framework applied to the multi-armed bandits problem domain.
*   **Transportability Theory:** Adoption of Transportability Theory to guide knowledge selection and transfer, estimating causal effects in a target environment rather than using naive statistical transfer.
*   **Latent Variable Management:** Addresses latent variables by treating actual contexts as hidden and observable proxies as input.
*   **Deep Learning Integration:** Implementation of Variational Autoencoders (VAEs) to approximate causal effects and manage high-dimensional complexity.
*   **Evaluation:** The approach is evaluated using synthetic and semi-synthetic datasets against classical bandit algorithm baselines.

---

## 4. Technical Details

The study compares three distinct agent architectures to evaluate transfer learning efficacy under covariate shift.

### **Agent Architectures**
1.  **Causal (Proposed):** Utilizes Causal Transportability theory with two pre-trained decoders (Proxy and Reward).
2.  **VAE(prior) (Baseline):** A naive approach that transfers the interventional distribution and encoding directly.
3.  **VAE (Baseline):** A standard VAE with no transfer mechanisms.

### **Variable Mapping**
The primary technical challenge involves mapping a **Proxy Variable ($W$)** to a **Latent Context ($Z$)** under conditions of covariate shift.

### **IHDP Experiment Configuration**
*   **Latent Dimension:** 1
*   **Proxy Variable:** Consists of 25 measurements.
*   **Reward Function:** Deterministic, based on $Z$.

### **Covariate Shift Setup**
*   **Source Domain:** $N(1, 1)$
*   **Target Domain:** $N(-1, 1)$

---

## 5. Results

The experiment utilized the **Proxy IHDP Dataset** to analyze performance under significant distribution shift.

*   **Data Volume:** 1,000 source samples.
*   **Evaluation Metric:** Total Cumulative Regret calculated over 1,000 gradient steps and averaged over 5 random seeds.

**Outcomes:**
*   The **'Causal' agent** achieved significantly higher sample efficiency than the naive baselines.
*   The proposed method successfully mitigated negative transfer effects.
*   The system effectively recovered the ground truth latent distribution, confirming the theoretical advantages of causal transportability.

---

## 6. Contributions

The paper makes four distinct contributions to the field of reinforcement learning and causal inference:

1.  **Formulation of Transfer in Latent Contextual Bandits:** Addresses the intersection of latent contextual bandits and covariate shift, providing formal analysis of why naive methods fail.
2.  **Causal-Based Algorithmic Development:** Contributes novel algorithms leveraging causal transportability to distinguish between effective and detrimental knowledge.
3.  **Deep Causal Integration:** By combining variational autoencoders with causal inference, it provides a solution for estimating causal effects in high-dimensional environments.
4.  **Empirical Validation:** Provides evidence that a causal perspective is superior to correlation-based methods for knowledge transfer in reinforcement learning contexts.