---
title: dots.llm1 Technical Report
arxiv_id: '2506.05767'
source_url: https://arxiv.org/abs/2506.05767
generated_at: '2026-02-06T03:03:12'
quality_score: 8
citation_count: 29
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# dots.llm1 Technical Report

*Bi Huo; Bin Tu; Cheng Qin; Da Zheng; Debing Zhang; Dongjie Zhang; En Li; Fu Guo; Jian Yao; Jie Lou; Junfeng Tian; Li Hu; Ran Zhu; Shengdong Chen; Shuo Liu; Su Guang; Te Wo; Weijun Zhang; Xiaoming Shi; Xinxin Peng; Xing Wu; Yawen Liu; Yuqiu Ji; Ze Wen; Zhenhai Liu; Zichao Li; Zilong Liao*

---

> ### ðŸ“Š Quick Facts
>
> | Metric | Value |
> | :--- | :--- |
> | **Total Parameters** | 142B |
> | **Active Parameters** | 14B |
> | **Architecture** | DeepSeekMoE (Sparse) |
> | **Training Data** | 11.2T Tokens (1:1 CN/EN) |
> | **Benchmark Performance** | Comparable to Qwen2.5-72B |
> | **Hardware** | NVIDIA H800 |
> | **Quality Score** | 8/10 |

---

## Executive Summary

The research addresses the escalating computational costs and resource demands associated with training and deploying state-of-the-art dense Large Language Models (LLMs). As model sizes grow to achieve higher performance, the inference costs and memory requirements of dense architectures become prohibitive, limiting accessibility and practical deployment. Furthermore, there is an ongoing industry debate regarding the necessity of synthetic data to scale models effectively, which introduces complexity and potential quality control issues in data pipelines.

The core innovation is the development of `dots.llm1`, a 142 billion parameter sparse decoder-only Transformer built on a Mixture of Experts (MoE) framework. The model employs a DeepSeekMoE architecture comprising 128 routed experts and 2 shared experts, activating only 14 billion parameters per input token to maximize efficiency.

The technical approach eliminates the need for auxiliary loss in load balancing by utilizing a dynamic bias adjustment strategy with FP32 gating precision. Additionally, the team developed a rigorous data processing pipeline that curates 11.2 trillion high-quality tokens with a strict 1:1 Chinese-to-English ratio, explicitly excluding synthetic data. Training was conducted using the Cybertron framework over Megatron-Core, featuring a custom Grouped GEMM optimization aligned to WGMMA instruction shapes.

The `dots.llm1` model achieves performance parity with the state-of-the-art dense model Qwen2.5-72B, demonstrating that sparse architectures can match dense counterparts without sacrificing capability. The custom kernel optimizations yielded significant performance gains, achieving an average forward speed-up of 14.00% and backward speed-up of 6.68% compared to NVIDIA Transformer Engine v2.1, with a peak forward speed-up of 24.68%. The training process maintained perfect load balance with zero token drops, and the final model supports efficient inference on a single node with 8 GPUs, validating the architecture's cost-effectiveness.

This work significantly impacts the field by providing a viable, cost-efficient alternative to dense LLMs, enabling high-performance model deployment on reduced hardware infrastructure. By validating that competitive performance can be achieved using only real data (11.2T tokens) through meticulous processing, the paper challenges the prevailing assumption that synthetic data is required for large-scale training. Furthermore, the decision to open-source intermediate training checkpoints at one trillion token intervals offers the research community an unprecedented resource to analyze learning trajectories and internal dynamics, paving the way for more transparent and efficient future model development.

---

## Key Findings

*   **Efficient Scaling:** The `dots.llm1` model achieves performance comparable to the state-of-the-art dense model Qwen2.5-72B using a sparse architecture that activates only 14B parameters out of 142B.
*   **Cost Reduction:** The Mixture of Experts (MoE) architecture successfully reduces training and inference costs while maintaining high model performance.
*   **Data Efficacy:** Competitive performance was achieved through pretraining on 11.2T high-quality tokens using a meticulously crafted data processing pipeline without synthetic data.
*   **Training Dynamics Insights:** The release of intermediate training checkpoints at intervals of one trillion tokens provides visibility into the learning dynamics and evolution of large language models.

---

## Methodology

The researchers utilized a **Mixture of Experts (MoE)** framework consisting of 142B total parameters with sparse activation engaging only 14B parameters per input token. A meticulously crafted data processing pipeline was developed to curate 11.2T high-quality tokens, explicitly excluding synthetic data.

The model underwent a **two-phase training regimen**:
1.  Large-scale pretraining on the curated data corpus.
2.  A post-training phase to refine capabilities.

---

## Technical Details

*   **Model Architecture:** 142B parameter decoder-only Transformer utilizing a sparse `DeepSeekMoE` framework with 128 routed experts and 2 shared experts (14B active parameters per token).
*   **Attention & Activation:** Employs standard Multi-Head Attention with **QK-Norm** (RMSNorm) and **SwiGLU** activation within a two-layer FFN.
*   **Load Balancing:** Achieved via an auxiliary-loss-free strategy using dynamic bias adjustment and FP32 gating precision.
*   **Infrastructure:** Built on the `Cybertron` framework over `Megatron-Core`.
*   **Scheduling:** Features Interleaved 1F1B pipeline scheduling.
*   **Optimization:** Custom Grouped GEMM optimization aligned to WGMMA instruction shapes.
*   **Data Pipeline:**
    *   Processed 11.2 trillion tokens.
    *   Maintains a 1:1 Chinese to English ratio.
    *   Explicitly excludes synthetic data.
    *   Incorporates a web clutter removal model and 200-class classifier for category balancing.

---

## Contributions

*   **dots.llm1 Model:** The release of a large-scale MoE model that offers a viable, cost-efficient alternative to dense large language models without sacrificing performance parity with SOTA models.
*   **Data Pipeline Validation:** Demonstration that high-performing large language models can be trained effectively on real data alone (11.2T tokens) using rigorous processing pipelines, challenging the necessity of synthetic data.
*   **Open Research Resources:** The decision to open-source intermediate training checkpoints at every one trillion token interval, providing the research community with a unique resource to analyze learning trajectories and internal dynamics.

---

## Results

*   **Performance Parity:** The model achieves performance comparable to the dense Qwen2.5-72B model and supports efficient inference on a single node with 8 GPUs.
*   **Computational Efficiency:**
    *   In computational tests on NVIDIA H800 GPUs, the custom Grouped GEMM implementation achieved an average forward speed-up of **14.00%** and backward speed-up of **6.68%** compared to NVIDIA Transformer Engine v2.1.
    *   Achieved a peak forward speed-up of **24.68%**.
*   **Training Stability:** The training process maintained load balance with zero token drops.

---

**Quality Score:** 8/10  
**References:** 29 citations