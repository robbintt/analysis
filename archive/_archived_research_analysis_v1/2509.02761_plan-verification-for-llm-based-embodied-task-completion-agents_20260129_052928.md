# Plan Verification for LLM-Based Embodied Task Completion Agents

*Ananth Hariharan; Vardhan Dongre; Dilek Hakkani-Tür; Gokhan Tur*

---

## Quick Facts

| Metric | Value |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Dataset** | TEACh Embodied AI |
| **Peak Precision** | 100% |
| **Peak Recall** | 90% |
| **Convergence Rate** | 96.5% (≤3 iterations) |
| **Models Tested** | GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout |
| **Citations** | 29 |

---

## Executive Summary

> **Problem**  
> Embodied task completion agents relying on Large Language Models (LLMs) face significant challenges due to the noisy nature of human demonstrations, which often contain irrelevant actions, logical contradictions, and spatial inconsistencies. These flaws degrade data quality, hindering the performance of imitation learning models by promoting inefficient execution and suboptimal spatial reasoning. Addressing this noise is critical for developing robust agents, yet traditional rule-based verification methods often struggle to generalize across the diverse error types found in real-world trajectories.
>
> **Innovation**  
> The authors propose a novel, iterative verification framework based on a two-agent LLM protocol: a Judge LLM that critiques existing action sequences to identify errors (classified as Redundant, Contradictory, or Missing), and a Planner LLM that applies specific revisions based on those critiques. Unlike rigid systems dependent on formal Linear Temporal Logic (LTL), this approach utilizes zero-shot natural language prompting to define a composite operator ($V = P \circ J$). This design assumes conservative error reduction and geometric convergence to a fixed point, allowing the system to progressively clean trajectories without hard-coded constraints while preserving valuable human error-recovery patterns.
>
> **Results**  
> Evaluated on the TEACh embodied AI dataset, the framework demonstrated robust verification capabilities, achieving up to 90% recall and 100% precision on a manually annotated subset. The iterative loop proved highly efficient, with 96.5% of action sequences converging to a stable state within three iterations. The method was validated across four state-of-the-art LLMs—GPT o4-mini, DeepSeek-R1, Gemini 2.5, and LLaMA 4 Scout—successfully fixing logical and spatial errors to significantly improve the temporal efficiency and spatial organization of generated plans.
>
> **Impact**  
> This research establishes plan verification as a reliable capability of LLMs for spatial planning and action refinement in embodied AI. By providing a scalable pipeline to convert noisy human demonstrations into high-quality training data, the work offers a critical tool for improving imitation learning models. Furthermore, the introduction of a prompt-based, generalizable refinement strategy presents a flexible alternative to traditional rule-based approaches, paving the way for more robust and data-efficient autonomous agents.

---

## Key Findings

*   **High Verification Performance:** The framework achieved up to 90% recall and 100% precision on a manually annotated subset of the TEACh embodied AI dataset.
*   **Broad Model Compatibility:** The method was validated across four state-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, and LLaMA 4 Scout).
*   **Rapid Convergence:** The iterative refinement loop is efficient, with 96.5% of action sequences requiring at most three iterations to reach a stable state.
*   **Preservation of Recovery Patterns:** Unlike methods that might oversimplify data, this approach preserves human error-recovery patterns, maintaining valuable data nuances for robust corrective behavior.
*   **Enhanced Trajectory Quality:** The method significantly improves temporal efficiency and spatial action organization by identifying and fixing irrelevant actions, contradictions, and missing steps.

---

## Methodology

The authors propose an iterative verification framework composed of two distinct LLM roles:

1.  **The Judge LLM:** Critiques existing action sequences to identify noise, logical errors, and spatial inconsistencies.
2.  **The Planner LLM:** Takes the critique from the Judge and applies specific revisions to the plan.

Unlike traditional rule-based systems, this framework relies entirely on natural language prompting. This design allows the system to generalize broadly across various error types without hard-coded constraints, progressively generating cleaner and spatially coherent trajectories through repeated loops.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Architecture** | **Two-Agent Protocol** consisting of a Planning Agent and a Judge LLM. |
| **Problem Definition** | **Plan Space (Π):** Defined as a finite sequence of atomic manipulation actions ($\pi = (a_1, a_2, \dots, a_T)$).<br>**Error Classification:** Errors are classified as Redundant, Contradictory, or Missing.<br>**Objective Function:** Seeks to minimize plan length: $\pi^* = \text{argmin}_{\tilde{\pi}} |\tilde{\pi}| \text{ s.t. } \tilde{\pi} \text{ achieves } g$. |
| **Verification Operator** | Defined by **Judge Function** $J$ mapping $(g, \pi)$ to critiques $C$, **Planner Function** $P$ applying critiques, and **Composite Operator** $V = P \circ J$. |
| **Iterative Refinement** | Assumes conservative error reduction and geometric convergence to a fixed point.<br>$E[E(k+1)] \leq (1 - \delta)E[E(k)]$ |
| **Methodology** | Utilizes **Zero-Shot Natural Language Critique** (instead of formal LTL) and preserves human error-recovery sequences. |

---

## Results

*   **Verification Performance:** Achieved 100% precision and up to 90% recall on the TEACh dataset.
*   **Efficiency & Convergence:** 96.5% of action sequences converged within three iterations.
*   **Model Compatibility:** Demonstrated effectiveness on GPT o4-mini, DeepSeek-R1, Gemini 2.5, and LLaMA 4 Scout.
*   **Trajectory Quality:** 
    *   Improved temporal efficiency by removing redundancies.
    *   Enhanced spatial organization and sequencing logic.
    *   Successfully corrected irrelevant actions, contradictions, and missing steps.

---

## Contributions

*   **Establishment of LLM Plan Verification:** The paper demonstrates that plan verification is a reliable capability of LLMs for spatial planning and action refinement in embodied AI.
*   **Scalable Data Quality Pipeline:** By converting noisy human demonstrations into high-quality training data, the research offers a scalable path to improve imitation learning models.
*   **Generalizable Refinement Strategy:** The introduction of a prompt-based, iterative verification mechanism provides a flexible alternative to rigid rule-based approaches, capable of handling complex logical and spatial errors.