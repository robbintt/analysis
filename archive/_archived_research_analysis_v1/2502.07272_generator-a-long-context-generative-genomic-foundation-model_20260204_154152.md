---
title: 'GENERator: A Long-Context Generative Genomic Foundation Model'
arxiv_id: '2502.07272'
source_url: https://arxiv.org/abs/2502.07272
generated_at: '2026-02-04T15:41:52'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# GENERator: A Long-Context Generative Genomic Foundation Model

*Wei Wu, Qiuyi Li, Yuanyuan Zhang, Zhihao Zhan, Ruipu Chen, Mingyang Li, Kun Fu, Junyan Qi, Yongzhou Bao, Chao Wang, Yiheng Zhu, Zhiyun Zhang, Jian Tang, Fuli Feng, Jieping Ye, Yuwen Liu, Hui Xiong, Zheng Wang*

---

> ### ðŸ“Š Quick Facts
> * **Training Data:** 386 Billion Nucleotides (Eukaryotic DNA)
> * **Context Window:** Up to 98,000 Nucleotides
> * **Model Scales:** 1 Billion and 3 Billion parameters
> * **Tokenization:** 6-mer strategy
> * **Validation Method:** UMI-STARR-seq
> * **Key Achievement:** State-of-the-art genomic benchmarks and successful protein/cis-regulatory design.

---

## ðŸ“ Executive Summary

Current genomic analysis faces significant challenges in scalability, cost, and the ability to model long-range dependencies within eukaryotic DNA. Existing tools are often constrained by short context windows, which fail to capture the complex interactions between distal regulatory elements, or rely on expensive, alignment-based methods that limit their applicability across diverse tasks. This gap hinders the efficient interpretation of genetic variation and the ability to perform functional biological engineering, necessitating a more generalized and efficient foundation model.

The researchers introduce **GENERator**, a generative genomic foundation model pre-trained on 386 billion nucleotides of eukaryotic DNA, designed to handle ultra-long context windows of up to 98,000 nucleotides. Technically, the model utilizes a 6-mer tokenization strategy that balances local grammar capture with long-range context processing, implemented as an autoregressive DNA language model at 1 billion and 3 billion parameter scales. By employing a next-token prediction objective and using marginal nucleotide-level probabilities for Variant Effect Prediction (VEP), GENERator provides an alignment-free approach that generalizes across unsupervised, zero-shot, and fine-tuning paradigms.

Evaluations demonstrated that GENERator embeddings achieve phylogenetically coherent clustering that improves with model scale, while the 6-mer tokenizer attained peak sequence recovery accuracy. In zero-shot VEP tasks, GENERator-1B outperformed the comparable Evo2-1B model, and GENERator-3B matched the performance of the significantly larger Evo2-7B model. After fine-tuning, the model achieved state-of-the-art results on standard genomic benchmarks. Furthermore, in programmable design tasks, 99.9% of generated protein sequences translated successfully and folded into stable structures with high similarity (TM-score > 0.8) to known folds, while synthetic cis-regulatory elements were successfully validated using UMI-STARR-seq assays.

This work establishes a unified framework that bridges the interpretation of genomic function with programmable sequence design, marking a significant step toward active biological engineering. By offering a broadly applicable, alignment-free solution for variant effect prediction and sequence design, GENERator reduces the computational and resource barriers typically associated with high-resolution genomic analysis. The model's ability to generate functional biological elements validated by high-throughput experiments positions it as a powerful new tool for synthetic biology, precision medicine, and the exploration of non-coding DNA regions.

---

## ðŸ”‘ Key Findings

*   **Strong Intrinsic Capabilities:** Demonstrated high proficiency in sequence recovery and phylogenetically coherent embeddings without the need for fine-tuning.
*   **Effective Zero-Shot Prediction:** Achieved variant effect prediction results comparable to alignment-based methods in a zero-shot setting.
*   **Benchmark Leadership:** Attained leading performance on standard genomic benchmarks following task-specific fine-tuning.
*   **Programmable Design:** Successfully enabled the design of proteins and synthetic cis-regulatory elements (CREs), which were validated experimentally.

---

## ðŸ› ï¸ Methodology

The researchers developed **GENERator**, a generative genomic foundation model specifically tailored for long-context DNA modeling.

*   **Training Scale:** The model was pre-trained on a massive dataset comprising **386 billion nucleotides** of eukaryotic DNA.
*   **Context Handling:** Designed to support an extensive context window of up to **98,000 nucleotides**, allowing for the modeling of long-range dependencies.
*   **Evaluation Settings:** The model was rigorously evaluated across three distinct paradigms:
    1.  **Unsupervised Intrinsic Analysis**
    2.  **Zero-Shot Inference**
    3.  **Task-Specific Fine-Tuning**
*   **Experimental Validation:** Designed sequences were validated using high-throughput assays, specifically **UMI-STARR-seq**.

---

## âš™ï¸ Technical Details

*   **Architecture:** Autoregressive DNA language model available in **1B** and **3B** parameter scales.
*   **Tokenization:** Utilizes a **6-mer tokenization** strategy to balance the capture of local grammar with long-range context processing.
*   **Context Length:** Supports input contexts of up to **96k nucleotides**.
*   **Objective Function:** Trained using a next-token prediction objective.
*   **Variant Effect Prediction (VEP):** Employs marginal nucleotide-level probabilities for prediction.
*   **Fine-Tuning Strategy:**
    *   Utilizes **10-fold cross-validation**.
    *   Includes hyperparameter optimization specifically for learning rates and batch sizes.

---

## ðŸ“ˆ Results

*   **Embedding Quality:** Embeddings displayed clear taxonomic clustering, with quality improving as model scale and input length increased.
*   **Tokenizer Efficiency:** The 6-mer tokenizer achieved peak sequence recovery accuracy.
*   **Variant Effect Prediction (VEP):**
    *   **GENERator-1B** outperformed **Evo2-1B**.
    *   **GENERator-3B** matched the performance of the much larger **Evo2-7B** model.
*   **Benchmark Performance:** Achieved state-of-the-art results on Genomic Benchmarks after fine-tuning.
*   **Protein Design:**
    *   **99.9%** of generated sequences translated successfully.
    *   Folded into stable structures with high similarity (TM-score > 0.8) to known folds, despite low sequence identity.

---

## ðŸŒŸ Contributions

*   **Overcoming Limitations:** Addresses current constraints in genomic tools regarding scope, capability, and cost by providing an efficient, long-context foundation model.
*   **Alignment-Free Approach:** Offers a broadly applicable method for genomic tasks, such as variant effect prediction, that removes the dependency on costly alignment-based processes.
*   **Unified Framework:** Establishes a cohesive framework that bridges the gap between interpreting genomic function and performing programmable sequence design for active biological engineering.

---

**Document Quality Score:** 8/10  
**References:** 40 citations