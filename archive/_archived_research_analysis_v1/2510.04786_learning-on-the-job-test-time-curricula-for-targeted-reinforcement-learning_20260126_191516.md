---
title: 'Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning'
arxiv_id: '2510.04786'
source_url: https://arxiv.org/abs/2510.04786
generated_at: '2026-01-26T19:15:16'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Learning on the Job: Test-Time Curricula for Targeted Reinforcement Learning

*Test Accuracy, Time Curriculum, Andreas Krause, Max Planck, Intelligent Systems, Ido Hakimi, Leander Diaz, Moritz Hardt, Training Step*

---

> ### ðŸ“Š Quick Facts
> * **Core Method:** Test-Time Curricula (TTC) / TTC-RL
> * **Target Model:** Qwen3-8B Base Model
> * **Key Benchmarks:** AIME (2024 & 2025), CodeElo, Codeforces
> * **Optimization:** Group Relative Policy Optimization (GRPO)
> * **Quality Score:** 8/10
> * **Citations:** 40

---

## Executive Summary

Generalist reinforcement learning (RL) agents and Large Language Models (LLMs), while capable across broad distributions, frequently fail to optimize performance on specific, challenging target instancesâ€”a phenomenon the authors term the **"Specialization Gap."** These models often lack the precision required for complex tasks such as solving advanced mathematical problems or writing competitive code. Standard adaptation methods, including fine-tuning and standard gradient updates, are often unstable or impractical at test time because they typically require access to original training data, simulator parameters, or expert supervision.

This research addresses this critical need by formalizing the challenge as **Targeted Reinforcement Learning (TaRL)** and introducing **Test-Time Curricula (TTC)** as a novel solution. Specifically, the authors propose TTC-RL, an algorithm designed to specialize generalist LLMs by generating an adaptive sequence of auxiliary tasks without external supervision. The method operates via a three-stage loop: **Curriculum Selection** (utilizing in-context retrieval with SIFT), **Attempt & Verification**, and **Policy Update**. By synthesizing easier Markov Decision Processes (MDPs) that interpolate between source and target dynamics, the algorithm guides the agent from simple to difficult tasks.

Experimental validation using the Qwen3-8B base model on mathematical reasoning and competitive coding benchmarks demonstrates that TTC-RL substantially outperforms standard post-training RL methods. The method achieved a monotonic increase in Pass@1 accuracy, reaching the 0.45â€“0.50 range, while enabling final Pass@1 accuracy to approach the model's initial Pass@8 "performance ceiling." This work bridges the gap between zero-shot and few-shot learning, establishing Test-Time Curricula as a vital component for deploying robust, adaptable AI systems.

---

## Key Findings

*   **The Specialization Gap:** Standard RL agents trained on broad distributions fail to optimize performance on specific target instances.
*   **Feasibility of Test-Time Adaptation:** RL agents can effectively adapt to specific target environments during deployment without access to original training data or simulator parameters.
*   **Curriculum Efficacy:** Approaching test-time adaptation through a curriculum yields significantly higher performance and stability compared to standard fine-tuning or gradient-based adaptation.
*   **Bridging Learning Modes:** The method bridges the gap between zero-shot and few-shot learning, allowing generalist agents to become specialists with relatively small amounts of interaction data.

---

## Methodology

The authors formalize the problem setting as **Targeted Reinforcement Learning (TaRL)**. The proposed solution, **Test-Time Curricula (TTC)**, automatically generates a curriculum of auxiliary tasks at test time.

1.  **Path Definition:** It leverages the agent's policy to define a path through the state-action space.
2.  **Task Synthesis:** It synthesizes easier Markov Decision Processes (MDPs) that interpolate between source and target dynamics.
3.  **Sequential Optimization:** The agent optimizes its policy sequentially on this curriculum, moving from easiest to hardest tasks to ensure stable gradient updates.

---

## Technical Details

The paper proposes **TTC-RL (Test-Time Curriculum via Reinforcement Learning)**, a method to specialize generalist LLMs to specific target tasks at deployment by modifying weights through compressed self-curated practice.

### The TTC-RL Algorithm Loop

The algorithm loops through three distinct stages:

1.  **Curriculum Selection:**
    *   Utilizes SIFT with normalized last-token last-layer embeddings.
    *   Uses a hyperparameter $\lambda=0.1$ to balance diversity and relevance.
    
2.  **Attempt & Verification:**
    *   The agent attempts the selected tasks and verifies the outcome.

3.  **Policy Update:**
    *   Training utilizes **GRPO (Group Relative Policy Optimization)**.
    *   **Key distinction:** The KL penalty is omitted to allow generalization without staying close to the initialization distribution.

### Distinction from SFT
The authors explicitly distinguish this approach from Supervised Fine-Tuning (SFT), noting that SFT is unsuitable for this setting due to the lack of expert traces in the corpus.

---

## Experimental Results

Experiments were conducted using the **Qwen3-8B base model** on Math and Code benchmarks.

*   **Benchmarks:** AIME (2024 & 2025), CodeElo, Codeforces.
*   **Primary Metrics:** Pass@1 and Pass@8.

**Performance Outcomes:**
*   TTC-RL outperformed standard RL post-training in both learning speed and final accuracy.
*   **Pass@1 Improvement:** The final Pass@1 accuracy approached the model's initial Pass@8 'performance ceiling'.
*   **Pass@8 Improvement:** Final Pass@8 values improved over the initial Pass@8, indicating the model learned new solution strategies rather than just selecting the best known answer.
*   **Visual Trends:** Results showed a **monotonic increase** in accuracy for TTC-RL to the **0.45â€“0.50 range** over 200+ steps, whereas standard RL plateaued significantly lower.

---

## Contributions

*   **Framework Formalization:** Defined the framework of Targeted Reinforcement Learning (TaRL).
*   **Novel Algorithm:** Proposed Test-Time Curricula (TTC), a novel algorithm for generating adaptive task sequences without external supervision at test time.
*   **Empirical Validation:** Provided comprehensive empirical benchmarks (e.g., MuJoCo, Atari) demonstrating that TTC allows pre-trained agents to match or exceed the performance of agents trained from scratch.

---

**Quality Score:** 8/10 | **References:** 40 citations