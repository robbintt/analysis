# Convergence of a L2 regularized Policy Gradient Algorithm for the Multi Armed Bandit
*Stefana Anita; Gabriel Turinici*

> ### **Quick Facts**
> *   **Topic:** Reinforcement Learning (Multi-Armed Bandits)
> *   **Core Method:** Policy Gradient with L2 Regularization
> *   **Key Innovation:** Time-dependent regularization strategy
> *   **References:** 33 Citations
> *   **Quality Score:** 8/10

---

## Executive Summary

The application of Reinforcement Learning (RL) to Multi-Armed Bandit (MAB) problems is widely established empirically, yet the theoretical convergence of Policy Gradient (PG) methods within the MAB framework remains insufficiently explored. Most existing analyses rely on the idealistic assumption of exact gradient estimates, a theoretical simplification that fails to account for the noisy, stochastic gradients inherent in real-world environments.

This paper introduces a theoretically grounded PG algorithm that integrates **L2 regularization** into the optimization process for a softmax-parametrized policy. The method modifies the objective function to maximize expected reward while adding a penalty term $\gamma/2 \|H\|^2$ to ensure boundedness. The key technical innovation lies in generalizing the regularization parameter $\gamma$ to a **time-dependent variable $\gamma_t$**. This dynamic schedule adjusts the regularization strength throughout the learning process, allowing the system to smoothly transition from exploration to exploitation.

The study provides rigorous mathematical proofs establishing the **almost-sure convergence** of the preference vector $H_t$ to the optimal vector $H^*$ under specific quantitative constraints. Numerical simulations validate these theoretical findings, demonstrating that the time-dependent regularization strategy significantly outperforms the canonical static approach, particularly in high-difficulty regimes where the initial parameterization is far from the optimal solution.

---

## Key Findings

*   **Theoretical Convergence:** The study successfully establishes the theoretical convergence of a policy gradient algorithm applied to Multi-Armed Bandit (MAB) problems when utilizing softmax parametrization with L2 regularization.
*   **Robustness:** Numerical experiments validate the theoretical results and demonstrate the robustness of the proposed method.
*   **Superior Performance:** Utilizing a **time-dependent regularization procedure** yields superior performance compared to the canonical approach, particularly when the initial parameterization is far from the optimal solution.

---

## Methodology

The research employs a policy gradient approach adapted for the Multi-Armed Bandit framework, utilizing a softmax parametrization for the policy. The core methodology involves integrating an L2 regularization term into the policy gradient update rule.

*   **Algorithm Design:** Adapts policy gradient for MAB using softmax parametrization.
*   **Regularization:** Integrates an L2 regularization term into the update rule.
*   **Analysis Strategy:** Combines theoretical analysis (rigorous proofs of convergence under technical hypotheses) with numerical simulation to test the algorithm's validity.

---

## Contributions

*   **Literature Gap:** Addresses a significant gap in the literature by providing rigorous theoretical analysis of the convergence properties of policy gradient algorithms within the MAB context.
*   **Optimization Strategy:** Introduces and validates a **time-dependent regularization strategy** that improves optimization performance and offers robustness against poor initial guesses.
*   **Method Validation:** Demonstrates the efficacy of combining softmax parametrization with L2 regularization as a mathematically sound method for solving bandit problems.

---

## Technical Details

*   **Framework:** Multi-Armed Bandit (MAB) with $k$ arms using Policy Gradient.
*   **Policy Parametrization:** Softmax parameterization via preference vector $H$.
*   **Objective Function:** $L_\gamma(H)$ maximizes expected reward minus an L2 regularization term ($\gamma/2 \|H\|^2$).
*   **Update Rule:** Uses Stochastic Gradient Ascent:
    $$H_{t+1}(a) = H_t(a) + \rho_t \left[(R_t - \bar{R}_t)(\mathbb{1}_{a=A_t} - \Pi_{H_t}(a)) - \gamma H_t(a)\right]$$
*   **Gradient Type:** The algorithm utilizes non-exact (stochastic) gradient estimation, contrasting with idealized exact gradient analyses.

---

## Results

*   **Convergence Proof:** Theoretical convergence is proven for the L2 regularized policy gradient algorithm with stochastic gradients.
*   **Comparative Performance:** Experiments indicate time-dependent regularization outperforms canonical static regularization, significantly when initialization is far from optimal.
*   **Validation & Robustness:** Numerical tests validated theoretical results and demonstrated robustness in regimes beyond strict theoretical settings.
*   **Practical Applicability:** The work focuses on stochastic gradients (practical) in contrast to idealized exact gradient analyses.