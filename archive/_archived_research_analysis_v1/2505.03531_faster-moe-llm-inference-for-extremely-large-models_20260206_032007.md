---
title: Faster MoE LLM Inference for Extremely Large Models
arxiv_id: '2505.03531'
source_url: https://arxiv.org/abs/2505.03531
generated_at: '2026-02-06T03:20:07'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Faster MoE LLM Inference for Extremely Large Models

*Haoqi Yang; Luohe Shi; Qiwei Li; Zuchao Li; Ping Wang; Bo Du; Mengjia Shen; Hai Zhao*

---

> ### **Quick Facts**
> **Quality Score:** 8/10  
> **References:** 40 Citations  
> **Target Models:** DeepSeek-V2-Lite, DeepSeek-V3  
> **Performance Gain:** ≥10% Throughput Increase  
> **Key Metric:** Arithmetic Intensity & Memory I/O  
> **Optimization Strategy:** Reducing Activated Experts ($n_a$)

---

## Executive Summary

This research addresses the computational bottlenecks associated with deploying fine-grained Mixture of Experts (MoE) Large Language Models (LLMs), such as DeepSeek, which exhibit distinct and underexplored efficiency dynamics compared to traditional coarse-grained architectures. As LLMs scale to extreme sizes, inference latency and throughput become critical constraints, often limited by memory-bound operations during autoregressive decoding. While fine-grained MoE models offer significant potential for high-capacity reasoning, the industry lacks a comprehensive understanding of how to balance their computational efficiency with model performance under varying service loads, hindering their cost-effective deployment in production environments.

The key innovation is a targeted optimization strategy that empirically distinguishes between the impact of reducing the number of activated experts versus reducing the total number of experts. By analyzing GPT-style decoder-only transformers utilizing Gated Linear Units (GLU) and a Top-k router, the authors utilize computational metrics—specifically **Memory I/O**, **FLOPS**, and **Arithmetic Intensity**—to identify specific bottlenecks. The study proposes that reducing the count of activated experts ($n_a$) is the superior lever for maximizing arithmetic intensity and alleviating memory-bound constraints, contrasting this with the inefficient approach of reducing total experts ($n_e$). This strategy is specifically optimized for state-of-the-art models like DeepSeek-V2-Lite and DeepSeek-V3.

The study demonstrates that the proposed optimization method of reducing activated experts yields a minimum **10% increase** in inference throughput without any degradation in model performance. Benchmarked against ARC, BoolQ, OpenBookQA, RTE, and Winogrande, this approach maintained high accuracy while significantly boosting speed. Conversely, experiments showed that reducing the total number of experts resulted in severe performance degradation with only marginal efficiency gains. Additionally, the researchers identified that the optimal ratio of activated intermediate size to the sum of shared and activated intermediate size is approximately **45-47%**, providing a concrete target for system configuration.

---

## Key Findings

*   **Distinct Efficiency Dynamics:** Fine-grained MoE models (like DeepSeek) present distinct and underexplored efficiency dynamics compared to coarse-grained models under different service loads.
*   **Activated Experts Strategy:** Reducing the number of activated experts yields substantial efficiency improvements with only minor performance degradation.
*   **Inefficiency of Total Expert Reduction:** Reducing the total number of experts provides limited efficiency gains but leads to severe performance degradation.
*   **Performance Guarantee:** The proposed optimization method increases inference throughput by at least **10%** without any degradation in model performance.
*   **Future Opportunity:** Deploying fine-grained MoE models is challenging but offers significant opportunities for inference optimization that have not yet been fully realized.

---

## Methodology

The researchers investigate the efficiency dynamics of fine-grained Mixture of Experts (MoE) architectures, contrasting them with traditional coarse-grained models. The study analyzes trade-offs between inference efficiency and model performance under varying service loads by testing two specific reduction strategies:

1.  Reducing the count of **activated experts**.
2.  Reducing the **total number of experts**.

Through this analysis, they identify optimal configurations that maximize throughput without sacrificing accuracy.

---

## Technical Details

### System Architecture
*   **Model Type:** GPT-style decoder-only Transformer models.
*   **Mechanism:** Fine-grained Sparse Mixture-of-Experts (MoE) with Gated Linear Units (GLU).
*   **Routing:** Top-k router modified by load balancing and weight normalization functions.

### Computational Metrics
The study defines and utilizes the following formulas to measure efficiency:

*   **Memory I/O:** $3d_i d + 2L(d + d_i)$
*   **FLOPS:** $6L(d_i d)$
*   **Arithmetic Intensity:** $\frac{6L(d_i d)}{3d_i d + 2L(d_i + d)}$

### Optimization Strategy
*   **Key Lever:** Identifying reducing the number of activated experts ($n_a$) as the primary method to maximize arithmetic intensity and alleviate memory-bound autoregressive decoding.
*   **Comparison:** This is contrasted with the inefficient reduction of total experts ($n_e$).

### Target Models
*   DeepSeek-V2-Lite
*   DeepSeek-V3

---

## Contributions

*   **Bridging the Research Gap:** Addresses the lack of research on fine-grained MoE models, shifting focus from predominantly studied coarse-grained architectures.
*   **Empirical Clarity:** Provides empirical clarity on the efficiency-performance equilibrium by distinguishing between the impacts of reducing activated experts versus total experts.
*   **Concrete Optimization Method:** Delivers a concrete inference optimization method that guarantees a minimum 10% throughput increase without performance loss, highlighting the potential for future advancements in MoE inference.

---

## Results

### Performance Metrics
*   **Primary Metric:** Inference throughput.
*   **Accuracy Benchmarks:** ARC, BoolQ, OpenBookQA, RTE, and Winogrande.

### Outcomes
*   **Activated Experts Reduction:** The proposed method increases inference throughput by at least 10% with only minor performance degradation.
*   **Total Experts Reduction:** Leads to severe performance degradation with limited efficiency gains.
*   **Ratio Optimization:** The study establishes that the ratio of activated intermediate size to the sum of shared and activated intermediate size is approximately **45-47%**.