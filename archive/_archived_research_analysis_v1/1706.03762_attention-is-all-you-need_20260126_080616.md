---
title: Attention Is All You Need
arxiv_id: '1706.03762'
source_url: https://arxiv.org/abs/1706.03762
generated_at: '2026-01-26T08:06:16'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Attention Is All You Need

*Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Google Research, Google Brain*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations
> *   **WMT 2014 En-De BLEU:** 28.4 (+2 BLEU improvement)
> *   **WMT 2014 En-Fr BLEU:** 41.8 (State-of-the-Art)
> *   **Training Efficiency:** 12 hours on 8 P100 GPUs

---

## Executive Summary

Prior to this research, sequence transduction modelsâ€”particularly those used for machine translationâ€”relied heavily on complex recurrent or convolutional neural networks (RNNs and CNNs). The fundamental limitation of these dominant architectures was their inherent sequential nature; RNNs, for example, process data step-by-step, which prohibits parallelization during training. This lack of parallelism resulted in significant computational bottlenecks, lengthy training times, and difficulties in modeling long-range dependencies between distant elements in a sequence. The paper addresses this critical inefficiency, seeking an architecture that retains superior modeling capabilities while significantly reducing training time.

The authors introduced the **Transformer**, a novel network architecture that relies entirely on attention mechanisms, completely dispensing with recurrence and convolutions. Technically, the Transformer utilizes an encoder-decoder structure composed of stacked identical layers. Each layer employs "Multi-Head Attention," which allows the model to jointly attend to information from different representation subspaces at different positions, and "Position-wise Feed-Forward Networks." The core computation is Scaled Dot-Product Attention, calculated as $softmax(QK^T/\sqrt{d_k})V$, enabling the model to weigh the importance of input tokens relative to one another. By utilizing residual connections and layer normalization, and reducing the signal path length between positions to a constant, the architecture achieves a level of parallelization previously unattainable by RNNs or CNNs.

The Transformer architecture established new state-of-the-art benchmarks in machine translation while drastically improving training efficiency. On the WMT 2014 English-to-German translation task, the model achieved a BLEU score of 28.4, improving over the previous best models by more than 2 BLEU points. For the WMT 2014 English-to-French task, it achieved a state-of-the-art BLEU score of 41.8 with a single model. Crucially, this superior performance was achieved with significantly reduced computational resources; training the model to state-of-the-art quality required only 12 hours on 8 P100 GPUs. Furthermore, the model demonstrated strong generalization capabilities, outperforming existing architectures on English constituency parsing tasks.

The significance of "Attention Is All You Need" cannot be overstated, as it effectively redefined the landscape of natural language processing (NLP). By empirically proving that attention mechanisms alone could outperform complex recurrent and convolutional models, the paper shifted the research focus away from RNNs and toward attention-based architectures. The high parallelizability of the Transformer enabled the training of models on unprecedented scales of data, serving as the foundational architecture for modern Large Language Models (LLMs) such as BERT and GPT. Beyond NLP, the Transformer's flexibility has led to its adoption in computer vision, audio processing, and multi-modal learning, making it one of the most influential neural network architectures in the history of AI.

---

## Key Findings

*   **Superior Translation Quality:** Achieved a BLEU score of 28.4 on the WMT 2014 English-to-German translation task, improving over previous models by more than 2 BLEU.
*   **State-of-the-Art Performance & Efficiency:** Established a new single-model state-of-the-art BLEU score of 41.8 on the WMT 2014 English-to-French task with significantly reduced training cost.
*   **Training Speed:** The architecture is significantly more parallelizable than dominant sequence transduction models, leading to a substantial reduction in required training time.
*   **Task Generalization:** Demonstrated strong generalization capabilities on English constituency parsing using both large and limited training datasets.
*   **Architectural Efficacy:** Dispensing with recurrence and convolutions in favor of attention mechanisms resulted in a model that outperforms complex RNN or CNN architectures.

---

## Methodology

The authors introduced the Transformer, a novel sequence transduction network architecture that relies exclusively on attention mechanisms. This approach completely discards recurrence and convolutions, utilizing an encoder-decoder structure built purely on attention to handle input and output dependencies without complex recurrent or convolutional layers.

---

## Contributions

*   **Introduction of the Transformer Architecture:** Proposed a simpler network architecture driven entirely by attention mechanisms, eliminating the need for recurrent and convolutional neural networks.
*   **Demonstration of Attention-Only Models:** Provided empirical evidence that a model based solely on attention can outperform complex existing models.
*   **Performance Benchmarking:** Set new state-of-the-art benchmarks for machine translation tasks (WMT 2014 En-De and En-Fr), proving the superiority of attention-based architectures.
*   **Efficiency Gains:** Highlighted the computational advantages of the Transformer, specifically its high parallelizability, which significantly reduces training time compared to recurrent models.

---

## Technical Details

The architecture eliminates RNNs and CNNs, relying on attention mechanisms for higher parallelization.

*   **Structure:** Utilizes an encoder-decoder structure.
*   **Encoder:**
    *   6 identical layers.
    *   Components: Multi-head self-attention and position-wise feed-forward networks (FFN).
    *   Normalization: Residual connections and Layer Normalization (Dimension: 512).
*   **Decoder:**
    *   6 identical layers.
    *   Components: Masked multi-head self-attention, encoder attention, and FFN.
*   **Attention Mechanism:**
    *   **Type:** Scaled Dot-Product Attention.
    *   **Formula:** $softmax((QK^T)/\sqrt{d_k})V$.
    *   **Multi-Head:** Allows attending to information from different representation subspaces.
*   **Dimensions:**
    *   FFN Inner Dimension: 2048.
*   **Signal Path:** Reduces signal path lengths between positions to a constant.

---

## Results

*   **Translation Performance:** Achieved a BLEU score of 28.4 on WMT 2014 English-to-German translation (improving over previous models by more than 2 BLEU points) and a state-of-the-art BLEU score of 41.8 on WMT 2014 English-to-French translation.
*   **Training Cost:** Training state-of-the-art quality required only 12 hours on 8 P100 GPUs.
*   **Generalization:** The model demonstrated strong generalization on English constituency parsing and outperformed recurrent and convolutional architectures.