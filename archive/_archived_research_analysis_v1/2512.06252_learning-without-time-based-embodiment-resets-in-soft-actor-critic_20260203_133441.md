---
title: Learning Without Time-Based Embodiment Resets in Soft-Actor Critic
arxiv_id: '2512.06252'
source_url: https://arxiv.org/abs/2512.06252
generated_at: '2026-02-03T13:34:41'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Learning Without Time-Based Embodiment Resets in Soft-Actor Critic
*Homayoon Farrahi; A. Rupam Mahmood*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Citations** | 40 |
| **Algorithm Focus** | Soft Actor-Critic (SAC) |
| **Key Intervention** | Dynamic Policy Entropy Adjustment |
| **Validation Domain** | Simulation (Hopper, Reacher, Ant) & Real-Robot Vision |

---

> ### ðŸ“‘ Executive Summary
>
> This paper addresses the critical dependency of modern reinforcement learning algorithms, specifically **Soft Actor-Critic (SAC)**, on time-based embodiment resets (episodic boundaries). In standard RL, agents rely on episodic resets to maintain state space exploration; however, this reliance creates an artificial disconnect between simulation training and real-world deployment, where agents cannot simply "teleport" back to an initial state after a fixed duration. The authors demonstrate that removing these resets in standard SAC leads to severe learning failures due to inadequate exploration, a problem that must be solved to enable viable, long-term autonomous operation in unstructured environments.
>
> The authors introduce a **"Continuing SAC"** formulation that decouples the learning process from episode terminations by treating state resets as standard non-terminal transitions. Technically, the approach modifies the reward function to remove constant living rewards (e.g., removing the `+1` reward in Hopper) and utilizes a differential TD error loss with a running average reward estimate. To counteract the loss of exploration normally induced by resets, the key innovation is the **strategic increase of policy entropy**. This dynamic entropy adjustment acts as an intervention to maintain state coverage, effectively substituting the variance usually provided by embodiment resets and preventing the agent from stagnating.
>
> Empirical results show that Continuing SAC with modified rewards achieves returns comparable to standard Episodic SAC, whereas unmodified continuing approaches fail. In the Hopper-v4 environment, the modified reward scheme enabled performance on par with the episodic baseline, whereas the original reward structure caused returns to plummet below 1000. In the Continuing Reacher-v5 task, the removal of resets without interventions led to agents getting stuck at joint limits (`[-3, 3]` radians) with significantly reduced state variance. Furthermore, tests in Corridor Ant-v4 revealed that simple L2 action penalties could not recover performance, confirming that large action magnitudes were not the root cause; instead, the entropy-based intervention was required to successfully restore exploration. These findings were further validated on a real-robot vision task.
>
> This work significantly influences the field by challenging the necessity of "task accessories" like artificial episode boundaries, which often mask underlying exploration deficiencies in RL algorithms. By demonstrating that learning performance can be decoupled from episode boundaries and made less sensitive to discount rates, the research provides a robust framework for continual learning. The study establishes that maintaining high policy entropy is a viable mechanism for exploration in the absence of external resets, providing a crucial pathway for deploying RL agents in real-world scenarios where episodic interruptions are impractical or impossible.

---

## Key Findings

*   **Performance Parity:** Continuing Soft Actor-Critic (SAC) with modified reward functions performs as well as or better than standard episodic SAC.
*   **Discount Rate Stability:** The continuing formulation demonstrates reduced sensitivity to the discount rate compared to episodic approaches.
*   **Exploration Dependency:** Embodiment resets are critical for state space exploration in standard SAC; their removal results in poor exploration and learning failure.
*   **Entropy as a Solution:** Increasing policy entropy is an effective intervention to counteract performance loss from removing embodiment resets.
*   **Broad Validation:** The entropy intervention strategy was validated successfully in both simulation environments and real-robot vision tasks.

## Methodology

The research employed a combination of algorithmic adaptation and empirical testing to diagnose failure modes and validate solutions:

*   **Algorithm Adaptation:** Adapted the SAC algorithm into a 'continuing' version designed for learning without episode terminations.
*   **Reward Modification:** Applied simple reward function modifications to align with continuing task formulations.
*   **Diagnostic Testing:** Used a modified Gym Reacher task for empirical testing to specifically identify failure modes associated with removing resets.
*   **Validation:** Tested the proposed entropy intervention on additional simulated tasks and a real-world robot vision task.

## Contributions

*   **Critique of Task Accessories:** Provides a critical examination of 'task accessories' like episodes and resets, highlighting how they create unnatural setups that hinder real-world performance.
*   **Continuing SAC Formulation:** Introduces a Continuing SAC formulation that decouples learning performance from episode boundaries and reduces sensitivity to the discount rate.
*   **Exploration Strategy:** Proposes a novel exploration intervention strategy using dynamic policy entropy adjustment to maintain performance without embodiment resets.

## Technical Details

The study implements specific algorithmic and environmental changes to achieve continuing learning:

*   **SAC Adaptation:**
    *   Treats state-based resets as standard **non-terminal transitions**.
    *   Maintains a **running average reward estimate** (step size `0.0003`).
    *   Utilizes a **differential TD error loss**.
*   **Environment Modifications:**
    *   **Continuing Hopper:** The constant `+1` reward is removed; a `-500` penalty is applied for resets.
    *   **Continuing Reacher:** Uses a sparse `+100` reward and restricts joint limits to `[-3, 3]` radians.

## Results

The empirical analysis revealed distinct behaviors across different environments:

*   **Hopper-v4:**
    *   The modified reward function enabled Continuing SAC to achieve returns comparable to Episodic SAC.
    *   The original reward structure yielded returns below **1000**.
*   **Continuing Reacher-v5:**
    *   Removing time-based resets caused **learning failure** or significantly slower learning.
    *   Agents exhibited **lower state variance** and got stuck at joint limits (`[-3, 3]` radians).
*   **Corridor Ant-v4:**
    *   Interventions like **L2 penalties failed** to match the performance of learning with resets.
    *   Suggests that large action magnitudes are **not** the main cause of poor exploration without resets.