# WearVQA: A Visual Question Answering Benchmark for Wearables in Egocentric Authentic Real-world scenarios

*Eun Chang; Zhuangqun Huang; Yiwei Liao; Sagar Ravi Bhavsar; Amogh Param; Tammy Stark; Adel Ahmadyan; Xiao Yang; Jiaqi Wang; Ahsan Abdullah; Giang Nguyen; Akil Iyer; David Hall; Elissa Li; Shane Moon; Nicolas Scheffer; Kirmani Ahmed; Babak Damavandi; Rakesh Wanga; Anuj Kumar; Rohit Patel; Xin Luna Dong*

---

> ### ‚ö° Quick Facts
>
> *   **Dataset Size:** 2,520 image-question-answer triplets (1,500 public / 1,000 private)
> *   **SOTA Performance:** 24% ‚Äì 52% QA Accuracy
> *   **Top Model:** GPT-4o (51.5% Accuracy)
> *   **Evaluation Reliability:** 96% Accuracy (LLM-as-a-judge)
> *   **Domains:** 7 diverse image domains
> *   **Quality Score:** 8/10

---

## üìë Executive Summary

While multi-modal Large Language Models (MLLMs) excel at standard Visual Question Answering tasks, their performance in wearable contexts remains untested and unreliable due to challenges inherent in egocentric vision such as motion blur, occlusion, poor lighting, and narrow fields of view.

The authors introduce **WearVQA**, the first benchmark specifically tailored to evaluate VQA capabilities in authentic egocentric scenarios. The dataset consists of 2,520 meticulously curated image-question-answer triplets captured via RayBan Meta smart glasses, spanning seven domains and ten cognitive task types. The benchmark employs a 'hard-negative' filtering process to ensure genuine reasoning tests and utilizes an 'LLM-as-a-judge' system calibrated with human annotations for automated evaluation.

Evaluation results reveal a substantial performance deficit in current state-of-the-art models, with accuracy ranging from only 24% to 52%. GPT-4o achieved the highest score at 51.5%, while the leading open-source model, Qwen2.5-VL-72B, reached 45.1%. Models showed significant sensitivity to input quality, with Claude-3.7-Sonnet demonstrating a 16.2% performance gap when facing low-quality images compared to ideal ones. Despite task difficulty, the automated evaluation framework proved highly reliable with 96% accuracy and 96.8% F1-score against human ground truth.

WearVQA represents a paradigm shift in computer vision research by moving focus from idealized third-person imagery to the reality of first-person wearable computing. By establishing a comprehensive standard that exposes specific failure modes of current MLLMs regarding noise and visual artifacts, this benchmark provides a critical roadmap for developing models that can maintain high performance under imperfect visual inputs, a prerequisite for safe and effective AI assistant deployment in daily life.

---

## üîë Key Findings

*   **Significant Performance Gap:** Current state-of-the-art multi-modal Large Language Models (LLMs) demonstrate low performance, achieving a Question Answering (QA) accuracy of only **24‚Äì52%**.
*   **Sensitivity to Input Quality:** Model performance degrades substantially with lower-quality images and complex reasoning-heavy tasks.
*   **Unique Egocentric Challenges:** Wearable AI faces distinct difficulties such as visual occlusion, poor lighting, unmagnified views, and motion blur.
*   **High-Evaluation Reliability:** The study employed a rigorous 'LLM-as-a-judge' evaluation framework that achieved **96% labeling accuracy**.

---

## üß™ Methodology

**Dataset Construction**
*   Researchers curated a dataset of **2,520 image-question-answer triplets** designed specifically for wearable contexts.

**Categorization Strategy**
*   The data spans **7 diverse image domains**.
*   Covers **10 different cognitive task types**.

**Quality Control**
*   The benchmark identifies and includes data with **6 common wearables-specific image quality issues**.
*   Questions are strictly designed to be answerable using **only visual inputs and common sense**.

**Evaluation Framework**
*   An automated **'LLM-as-a-judge' system** was utilized to assess model accuracy with high precision.

---

## ‚öôÔ∏è Technical Details

### Data Collection & Source
*   **Device:** Data captured via RayBan Meta smart glasses.
*   **Process:** Two-step methodology where annotators capture daily life scenarios with specific quality issues (lighting, occlusion) and sample frames from videos to introduce motion blur.

### Filtering & Dataset Specs
*   **Hard-Negative Filtering:** Removed questions answered correctly by all baseline models to ensure difficulty.
*   **Reduction:** Dataset reduced from ~4,000 to final **2,520** triplets (1,500 public test, 1,000 private test).
*   **Composition:**
    *   7 Domains
    *   10 Task Types
*   **Identified Quality Issues:** Blurred, cut-off, low light, unmagnified, occluded, and rotated.

### Evaluation Protocol
*   **Metric:** Accuracy based on 5-point correctness criteria (including egocentric phrasing).
*   **Judge System:** 'LLM-as-a-judge' framework calibrated with 10% human annotation.

---

## üìä Results

**Dataset Statistics**
*   **Quality Prevalence:** 54% of images contain at least one quality issue.
*   **Data Purity:** Manual auditing showed ~98.8% data purity.

**Evaluation System Reliability**
*   **Accuracy:** 96%
*   **Precision:** 98.2%
*   **Recall:** 95.5%
*   **F1-Score:** 96.8%

**Model Performance**
*   **Overall Performance:** Generally low across the board.
*   **Best Proprietary Model:** **GPT-4o** achieved 51.5% QA accuracy (dropping to 45.5% on low-quality images).
*   **Best Open Source Model:** **Qwen2.5-VL-72B** scored 45.1%.
*   **Sensitivity Highlight:** **Claude-3.7-sonnet** showed the highest sensitivity to image degradation with a performance gap of **-16.2%**, highlighting the benchmark's difficulty.

---

## üèÜ Contributions

1.  **Introduction of WearVQA:** Release of the first benchmark specifically tailored to evaluate Visual Question Answering (VQA) capabilities for AI assistants on wearable devices.
2.  **Focus on Egocentric Authenticity:** Shifting the evaluation focus from idealized third-person imagery to authentic, egocentric first-person scenarios.
3.  **Baseline for Future Research:** Establishing a comprehensive standard that exposes current limitations in multi-modal wearables AI and provides direction for developing more robust systems.

---

**Analysis Quality Score:** 8/10
**References:** 21 citations