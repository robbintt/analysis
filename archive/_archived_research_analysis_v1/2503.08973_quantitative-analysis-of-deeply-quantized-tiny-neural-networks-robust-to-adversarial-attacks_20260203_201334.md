---
title: Quantitative Analysis of Deeply Quantized Tiny Neural Networks Robust to Adversarial
  Attacks
arxiv_id: '2503.08973'
source_url: https://arxiv.org/abs/2503.08973
generated_at: '2026-02-03T20:13:34'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Quantitative Analysis of Deeply Quantized Tiny Neural Networks Robust to Adversarial Attacks

*Idris Zakariyya; Ferheen Ayaz; Mounia Kharbouche-Harrari; Jeremy Singer; Sye Loong Keoh; Danilo Pau; JosÃ© Cano*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 7/10
> *   **References:** 40 Citations
> *   **Core Technique:** Stochastic Ternary Quantization (STQ) + Jacobian Regularization (JR)
> *   **Target Hardware:** Microcontroller Units (MCUs) <4W
> *   **Memory Footprint:** KiB range (e.g., 320 KB on GSC)
> *   **Datasets:** CIFAR-10, Google Speech Commands
> *   **Robust Accuracy (GSC):** 41.2% (vs. 5.6% Benchmark)

---

## Executive Summary

This research addresses the deployment constraints of Deep Neural Networks (DNNs) on Microcontroller Units (MCUs), specifically the conflict between aggressive model quantization and adversarial robustness. In TinyML applications, models must operate within strict kilobyte-scale memory limits and power budgets below 4W, necessitating deep quantization which typically increases susceptibility to adversarial perturbations. This paper investigates methods to secure these deeply quantized models against both white-box and black-box attacks without violating the rigid hardware constraints of edge devices.

The core innovation is a co-optimization strategy implemented within the QKeras framework that integrates Stochastic Ternary Quantization (STQ) with per-layer Jacobian Regularization (JR). The method employs STQ to reduce precision by treating quantization error as stochastic noise; quantization probability is inversely proportional to the error, allowing for incremental compression. This is combined with a per-layer JR technique to smooth the model's output landscape against minor input perturbations. The architecture is optimized through a joint loss function incorporating adversarial training, specifically targeting MCU environments with limited power and memory resources.

In evaluations using the CIFAR-10 and Google Speech Commands (GSC) datasets, the proposed compact DNN demonstrated superior quantitative performance compared to MLCommons/TinyML benchmarks "Quanos" and "DS-CNN." On the GSC dataset, the model maintained a memory footprint of **320 KB** and achieved a robust accuracy of **41.2%** against PGD-20 attacks, substantially outperforming the DS-CNN benchmark which plummeted to **5.6%**. Similarly, on CIFAR-10, the proposed method achieved a robust accuracy of **41.5%** compared to the baseline's **31.2%**, while preserving clean accuracy above **80%**. The model consistently outperformed benchmarks against a rigorous suite of white-box attacks (FGSM, PGD, C&W) and black-box attacks (Square, Boundary, ZOO), demonstrating that deep quantization does not inherently compromise defense capabilities.

This work advances the state of the art in Edge AI by providing a validated solution to the accuracy-robustness-resource trade-off for pervasive devices. By establishing that high security is compatible with extreme resource efficiency, the authors offer new quantitative benchmarks for TinyML robustness. The research serves as a foundation for future hardware-software co-design, demonstrating that secure, deeply quantized models are viable for next-generation microcontroller-based applications.

---

## Key Findings

*   **Superior Robustness:** The proposed compact DNN model demonstrated better average performance than established MLCommons/TinyML benchmarks ("Quanos" and "DS-CNN") when subjected to white-box and black-box adversarial attacks.
*   **Small Memory Footprint:** The model achieves high resilience while maintaining a small memory footprint (KiB range), making it suitable for edge deployment.
*   **Cross-Modal Efficacy:** The proposed co-optimization strategy proved effective across different data modalities, specifically on the **CIFAR-10** image dataset and the **Google Speech Commands** audio dataset.
*   **Effective Quantization:** The use of **Stochastic Ternary Quantization (STQ)** successfully facilitated deep quantization without compromising the model's ability to withstand adversarial perturbations.

---

## Methodology

The study utilized a rigorous framework to develop and test the robustness of the proposed model.

*   **Framework:** Utilized the **QKeras** quantization-aware training framework.
*   **Co-optimization Strategy:** Implemented a novel approach combining QKeras with **Jacobian Regularization (JR)** to co-optimize the DNN architecture.
*   **Regularization Technique:** Applied a **per-layer JR** methodology to enhance the model's robustness against minor input perturbations.
*   **Quantization Scheme:** Employed **Stochastic Ternary Quantization (STQ)** to reduce precision and model size.
*   **Evaluation:** Performance was rigorously tested against a variety of white-box and black-box adversarial attacks and compared against existing TinyML benchmarks.

---

## Technical Details

### Core Strategy
The paper proposes a co-optimization strategy for deep quantization and adversarial robustness in Edge AI environments using **Stochastic Ternary Quantization (STQ)** via the QKeras framework.

### STQ Mechanics
*   The method employs a stochastic probability strategy where quantization probability is **inversely proportional to quantization error**.
*   It incrementally quantizes network parameters, treating error as noise.
*   Supports per-layer selection of quantization bit-width for activations, biases, and weights.

### Hardware Targets
*   Optimized for **Microcontroller Units (MCUs)** with <4W power consumption.
*   KiB memory footprints.

### Defense Mechanisms
*   **Adversarial Training:** Integrated into the training process.
*   **Regularization:** Joint loss function utilizing Jacobian Regularization.

### Attack Evaluation
*   **White-Box Attacks:** FGSM, PGD, C&W.
*   **Black-Box Attacks:** Square, Boundary, ZOO.

---

## Contributions

*   **Addressing the Edge-Accuracy-Robustness Trade-off:** Presents a solution to the critical challenge of deploying DNNs on resource-constrained edge devices by simultaneously achieving accuracy, resilience, and compactness.
*   **Novel Application of Per-Layer JR with QKeras:** Contributes a specific methodology that integrates per-layer Jacobian Regularization with QKeras to optimize architecture for adversarial robustness.
*   **Benchmarking TinyML Robustness:** Provides new quantitative insights into the robustness of current state-of-the-art tiny neural networks by comparing against Quanos and DS-CNN under adversarial conditions.
*   **Validation of STQ for Robustness:** Provides evidence that Stochastic Ternary Quantization (STQ) is a viable technique for building secure, deeply quantized models for edge AI.

---

## Results

The proposed model demonstrated superior average performance against both white-box and black-box adversarial attacks compared to MLCommons/TinyML benchmarks ('Quanos' and 'DS-CNN').

*   **Google Speech Commands (GSC) Dataset:**
    *   **Memory Footprint:** 320 KB
    *   **Robust Accuracy (PGD-20):** 41.2%
    *   **Benchmark Comparison (DS-CNN):** 5.6% (Proposed model significantly outperformed).
*   **CIFAR-10 Dataset:**
    *   **Robust Accuracy:** 41.5% (Baseline was 31.2%).
    *   **Clean Accuracy:** Preserved above 80%.

The approach proved effective across different modalities using the CIFAR-10 and Google Speech Commands datasets, successfully balancing robustness with resource constraints.