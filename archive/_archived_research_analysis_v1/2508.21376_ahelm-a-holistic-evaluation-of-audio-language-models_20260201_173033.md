# AHELM: A Holistic Evaluation of Audio-Language Models

*Tony Lee; Haoqin Tu; Chi Heem Wong; Zijun Wang; Siwei Yang; Yifan Mai; Yuyin Zhou; Cihang Xie; Percy Liang*

---

> ### **Quick Facts**
> *   **Quality Score**: 9/10
> *   **Models Evaluated**: 17 systems (14 distinct Audio-Language Models)
> *   **Top Performer**: Gemini 2.5 Pro (Ranked #1 in 5 aspects)
> *   **New Datasets**: PARADE (Stereotypes), CoRe-Bench (Reasoning)
> *   **Key Insight**: Cascaded ASR + LLM pipelines remain competitive with end-to-end models.

---

## Executive Summary

The rapid advancement of Audio-Language Models (ALMs) has outpaced the development of comprehensive evaluation frameworks. Existing benchmarks are typically fragmented, focusing narrowly on specific sub-tasks like automatic speech recognition (ASR) or audio classification while neglecting critical societal dimensions such as fairness, safety, and robustness.

This siloed approach creates a significant blind spot in model assessment; a system may demonstrate high accuracy in transcription but harbor harmful biases or fail to reason safely in real-world scenarios. Consequently, there is an urgent need for a holistic evaluation standard that accurately reflects the multifaceted requirements of deploying ALMs in production environments.

To address this gap, the authors introduce **AHELM (A Holistic Evaluation of Audio-Language Models)**. This standardized benchmark is built upon a comprehensive 10-aspect taxonomy encompassing perception, knowledge, reasoning, emotion, multilinguality, robustness, bias, fairness, toxicity, and safety. Technically, AHELM aggregates existing high-quality datasets and innovates by releasing two novel synthetic datasets:
*   **PARADE**: Designed to measure stereotype avoidance through occupational group transcripts.
*   **CoRe-Bench**: Evaluates conversational reasoning using demographic multi-turn dialogues.

To ensure equitable comparisons across 17 distinct systems (both end-to-end and cascaded), the benchmark standardizes inference protocols—specifically zero-shot prompts with a temperature of 0 and a max token limit of 200—and utilizes a mix of traditional metrics (WER, BLEU) and LLM-as-a-Judge scoring for subjective categories.

The study evaluated 14 proprietary and open-weight models, revealing that **high performance in one domain does not guarantee competence in others**. While Google's Gemini 2.5 Pro achieved the top ranking in 5 out of 10 aspects, it simultaneously exhibited statistically significant group unfairness on ASR tasks. Surprisingly, simple cascaded baseline systems (e.g., Whisper-1 + GPT-4o) ranked 6th overall, proving highly competitive without native audio processing capabilities. Furthermore, the analysis found **zero correlation between reasoning capability and robustness in safety or fairness**.

The significance of AHELM lies in its establishment of a rigorous, multi-dimensional standard for the multimodal AI community. By demonstrating that simple cascaded pipelines can rival sophisticated end-to-end models, the research offers valuable engineering insights. The open-sourcing of all raw data, prompts, and outputs provides a transparent, reproducible foundation for future research.

---

## Key Findings

*   **Gemini 2.5 Pro Dominance with Caveats**: Achieved the top ranking in 5 out of 10 evaluation aspects; however, it exhibited statistically significant group unfairness on ASR tasks.
*   **Baseline Competitiveness**: Simple baseline systems utilizing cascaded ASR + LLM architectures performed competitively, ranking 6th overall without native audio processing capabilities.
*   **Performance Disparity**: High performance in one area, such as reasoning, does not guarantee robustness in others like safety or fairness; there was zero correlation found between these metrics.

---

## Methodology

The research team developed the **AHELM benchmark** using the following approach:

1.  **Data Aggregation**: Aggregated existing high-quality datasets to form a comprehensive testbed.
2.  **Novel Synthetic Datasets**: Introduced two new datasets to fill evaluation gaps:
    *   **PARADE**: Focuses on stereotype avoidance.
    *   **CoRe-Bench**: Focuses on conversational reasoning.
3.  **Standardization**: Standardized prompts, inference parameters, and metrics to ensure equitable comparisons across 17 systems.
4.  **Holistic Taxonomy**: Evaluated models across 10 distinct aspects, including perception, knowledge, reasoning, bias, and safety.

---

## Technical Details

AHELM evaluates Audio-Language Models using a structured four-component framework. The evaluation protocol and datasets are detailed below:

### Evaluation Components
| Component | Description |
| :--- | :--- |
| **Aspect** | 10 dimensions covering Audio Perception, Knowledge, Reasoning, Emotion, Multilinguality, Robustness, Bias, Fairness, Toxicity, and Safety. |
| **Scenario** | Specific contexts or tasks applied within the dataset. |
| **Adaptation** | Methods used to apply the model to the specific task. |
| **Metric** | Scoring mechanisms used for evaluation. |

### Datasets & Protocols
*   **PARADE**: utilized for **Bias** evaluation; uses occupational group transcripts to test stereotype avoidance.
*   **CoRe-Bench**: utilized for **Reasoning** evaluation; uses demographic multi-turn dialogues.
*   **Inference Protocol**:
    *   **Temperature**: 0
    *   **Max Tokens**: 200
    *   **Prompting Style**: Zero-shot prompts
*   **Baselines**: Cascaded ASR + LM pipelines (e.g., Whisper-1 + GPT-4o) used to compare against native audio models.

### Metrics Used
*   **WER & ΔWER (Fairness)**
*   **BLEU**
*   **Accuracy / Exact Match**
*   **Pseudo-exact Match**
*   **LLM-as-a-Judge**

---

## Results

The evaluation of 14 ALMs from major organizations (Google, OpenAI, Alibaba) yielded the following outcomes:

*   **Model Ranking**: **Gemini 2.5 Pro** ranked top in 5 out of 10 aspects.
*   **Cascaded Systems**: Baseline cascaded systems (ASR + LLM) ranked **6th overall**, indicating competitive performance without native audio processing.
*   **Correlation Analysis**: No correlation was observed between high reasoning performance and safety/fairness robustness.
*   **Fairness Specifics**: Specifically, Gemini 2.5 Pro showed statistically significant group unfairness on ASR tasks as measured by ΔWER.

---

## Contributions

*   **AHELM Benchmark**: Introduction of a comprehensive, standardized framework for evaluating Audio-Language Models.
*   **Novel Datasets**: Release of **PARADE** and **CoRe-Bench** to fill critical evaluation gaps regarding stereotypes and reasoning.
*   **Holistic Taxonomy**: Establishment of a 10-aspect evaluation taxonomy that prioritizes fairness, safety, and robustness alongside accuracy.
*   **Open Science**: Commitment to transparency by publicly releasing all raw data, prompts, and model outputs.

---

## Report Details

**Quality Score**: 9/10  
**References**: 40 citations