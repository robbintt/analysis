# Adversarial Attacks Leverage Interference Between Features in Superposition

*Edward Stevinson; Lucas Prieto; Melih Barsbey; Tolga Birdal*

***

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |
| **Validation Model** | Vision Transformer (ViT) on CIFAR-10 |
| **Attack Method** | Projected Gradient Descent (PGD) |
| **Key Mechanism** | Superposition & Feature Interference |

***

## Executive Summary

Adversarial examples pose a fundamental paradox in deep learning, exposing a stark disconnect between human perception and model classification. Existing theories often attribute this vulnerability to the presence of "non-robust" features in training data or to the complex, irregular geometry of decision boundaries near input points. This paper challenges those prevailing notions, identifying a more structural root cause: **superposition**.

The authors address the persistent mystery of why neural networks, despite high accuracy, remain susceptible to imperceptible perturbations. They argue that vulnerability is not a bug in the learning process or the data, but an inevitable mathematical consequence of how networks compress information to represent more features than available dimensions allow.

The core innovation is a mechanistic framework grounded in the **Linear Representation Hypothesis** combined with the **Superposition Hypothesis**. Technical implementation relies on the premise that neural networks encode semantic features as linear directions in activation space. Because networks operate in superposition, they utilize overcomplete, non-orthogonal vector sets to store $M$ features within a lower-dimensional space ($d$), where $M > d$. This high-density packing creates geometric interference between features.

The researchers demonstrate that adversarial attacks function by exploiting this interference; perturbations move the latent representation in directions that systematically amplify some features while attenuating others based on the alignment of their vector representations. 

Experimental results confirm that adversarial perturbations generated by Projected Gradient Descent (PGD) align almost perfectly with the theoretical optimal perturbations derived from the geometry of superposition. This research offers a significant theoretical pivot, reframing vulnerability as a byproduct of representational efficiency rather than a failure of learning. Consequently, the paper shifts the field's focus toward architectural solutions that account for feature interference, marking a crucial step toward designing intrinsically secure neural networks.

***

## Key Findings

*   **Root Cause of Vulnerability:** Adversarial vulnerability in neural networks stems primarily from **'superposition,'** a state where networks represent more features than there are available dimensions in the latent space.
*   **Mechanism of Attack:** Adversarial perturbations function by leveraging the **interference** that naturally occurs between these superposed features.
*   **Predictability:** Attack patterns are highly predictable and can be derived based solely on the geometry of latent representations.
*   **Efficiency Trade-off:** Vulnerability is presented as a direct byproduct of representational compression efficiency, rather than a flaw in the learning algorithm or the presence of non-robust input features.

***

## Methodology

The research employed a two-phase validation approach to isolate and verify the role of superposition:

1.  **Synthetic Validation:** 
    *   Conducted in controlled settings to isolate superposition.
    *   Confirmed the sufficiency of superposition for creating vulnerability.
    *   **Architecture:** Used a classification task with input $x \in \mathbb{R}^d$ partitioned into $k$ groups.
    *   **Structure:** An encoder bottleneck layer ($h = \text{ReLU}(W_e x + b_e) \in \mathbb{R}^m$ ) connected to a decoder ($z = W_d h + b_d \in \mathbb{R}^k$), where $m < k < d$.

2.  **Real-World Verification:**
    *   Findings were validated using a **Vision Transformer (ViT)**.
    *   The model was trained on the standard **CIFAR-10** dataset.

***

## Technical Details

### Hypotheses & Representation
*   **Linear Representation Hypothesis (LRH):** Semantic abstractions are encoded as linear directions in activation space.
    *   *Formula:* $h^{(l)}(x) \approx \sum_{j=1}^{M} a_j(x)v_j$
    *   Where $v_j$ is the linear direction and $a_j(x)$ is the activation magnitude.
*   **Superposition Hypothesis:** Networks use overcomplete, non-orthogonal sets of vectors to represent $M$ features in a lower-dimensional space ($d$), where $M > d$.
    *   Networks leverage the capacity for $2^{\Theta(d\epsilon^2)}$ almost orthogonal vectors in $d$-dimensional space.
    *   **Consequence:** Introduces interference between features, which adversarial attacks exploit.

### Experimental Setup
*   **Loss Functions:** Primary setup used Cross-Entropy (CE) without ReLUs/biases; validation setup used Mean Squared Error (MSE) with ReLUs and biases.
*   **Attack Specifications:**
    *   **Method:** Projected Gradient Descent (PGD).
    *   **Norms:** $\ell_{\infty}$ and $\ell_2$.
    *   **Definition:** Adversarial example $\delta$ causes a model prediction change while the true class remains unchanged.
*   **Evaluation Metrics:**
    *   Input Perturbation Profile (IPP).
    *   Latent Attack Alignment ($\Delta h \cdot v_c$).
    *   Latent Representation Interpretation.

***

## Experimental Results

The experimental data strongly supports the theory that adversarial attacks are intrinsically linked to superposition geometry.

### Alignment vs. Theory
PGD-generated attacks demonstrated high alignment with theoretically optimal perturbations derived from superposition geometry. Random perturbations showed no correlation.

| Configuration | Cosine Similarity (Mean Â± Std) | Statistical Significance |
| :--- | :--- | :--- |
| **(k=6, m=2)** | **0.97 Â± 0.02** | $p < 10^{-10}$ |
| **(k=30, m=10)** | **0.96 Â± 0.00** | $p < 10^{-10}$ |
| **(k=90, m=30)** | **0.92 Â± 0.00** | $p < 10^{-10}$ |
| *Random Baseline* | *0.00 Â± 0.02* | N/A |

*(Note: Significance tested via one-sample t-tests; Sample size: 5 models per configuration, 1,000 attacks per model)*

### Mechanistic Observations
*   **Input Perturbation Profile (IPP):** Showed a strong correlation with the alignment of class latent representations ($v(j)$) and the latent attack vector ($\Delta h$).
*   **Feature Modulation:** 
    *   Features with positive alignment ($v(j) \cdot \Delta h > 0$) were **amplified** by perturbation.
    *   Features with negative alignment were **attenuated**.
*   **Spatial Perception:** Results contradicted 'feature intuition'. Perturbations appeared arbitrary in **input space** but were highly systematic in **latent space**, supporting the 'latent geometry' intuition.

***

## Contributions

This paper makes a distinct impact on the field of adversarial machine learning through the following contributions:

1.  **Mechanistic Framework:** Provides a concrete framework explaining the **transferability** of adversarial attacks and why specific classes exhibit specific vulnerability patterns.
2.  **Theoretical Shift:** Offers a paradigm shift by framing vulnerability as a consequence of **information compression efficiency**. This counters prevailing views that attribute vulnerability to irregular decision landscapes or non-robust features in the data.