# Distorted Distributional Policy Evaluation for Offline Reinforcement Learning

*Ryo Iwaki; Takayuki Osogami*

---

### üìã Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 7/10 |
| **Total Citations** | 40 |
| **Core Concept** | Quantile Distortion |
| **Benchmark** | D4RL (Gym-MuJoCo) |
| **Key Tasks** | HalfCheetah, Hopper, Walker2d |

---

### üìù Executive Summary

> Existing offline Distributional Reinforcement Learning (DRL) methods are constrained by the issue of distributional shift, which is commonly mitigated through "uniform pessimism." This approach uniformly underestimates all return quantiles to prevent overestimation, but it creates a critical bottleneck: it fails to distinguish between state-action pairs based on data availability. By applying equal conservatism to data-rich and data-sparse regions, uniform pessimism leads to excessively penalized policy evaluations, hindering generalization and limiting the overall performance of offline RL agents in complex environments.
>
> The authors introduce **"quantile distortion,"** a novel mechanism that enables non-uniform pessimism by dynamically adjusting the degree of return quantile underestimation according to data availability. Unlike static penalty baselines such as CODAC, this method operates within an infinite-horizon discounted MDP framework to modulate conservatism based on cumulative probability levels ($\tau$). Technically, the approach employs a predictor with $M$ atoms and minimizes the 1-Wasserstein distance. By utilizing the distributional Bellman operator and a quantile projection operator, the framework links estimation conservatism directly to the density of supporting data‚Äîreducing pessimism in well-supported regions (modes) while maintaining caution in the tails where data is sparse.
>
> The proposed framework was validated on standard **D4RL benchmarks**, specifically across **Gym-MuJoCo** locomotion tasks including **HalfCheetah**, **Hopper**, and **Walker2d**. The method demonstrated superior performance compared to uniform pessimism baselines, achieving measurable improvements in normalized scores.
>
> This research significantly advances the field of offline RL by demonstrating that data-dependent, non-uniform pessimism is both theoretically sound and empirically superior to traditional uniform methods. By effectively alleviating the bottleneck of over-conservatism, the authors provide a robust solution that improves generalization without sacrificing safety.

---

### üîë Key Findings

*   **The Uniform Pessimism Bottleneck:** Existing offline DRL methods suffer from "uniform pessimism," a tendency to underestimate all return quantiles equally. This over-conservatism negatively impacts generalization and overall performance.
*   **Data-Dependent Adjustment:** The degree of conservatism should be adjusted based on data availability. Policies should not be penalized equally in data-rich regions versus data-sparse regions.
*   **Superiority of Non-Uniformity:** Non-uniform pessimism outperforms uniform pessimism in offline RL settings by avoiding excessive penalization of well-supported regions.
*   **Tail Estimation:** The proposed method achieves superior estimation of return distribution tails, which are typically harder to model due to data scarcity.

---

### ‚öôÔ∏è Methodology

The authors introduce **"quantile distortion,"** a novel mechanism designed to enable non-uniform pessimism. The core components of the methodology include:

*   **Dynamic Adjustment:** The mechanism dynamically adjusts the degree of return quantile underestimation based on the amount of supporting data available for a specific state or action.
*   **Adaptive Framework:** The proposed framework links estimation conservatism directly to data availability. This alleviates overly conservative estimates in regions where the data is sufficient.
*   **Validation:** The method is validated through a combination of rigorous theoretical analysis and empirical experimentation on standard benchmarks.

---

### üî¨ Technical Details

The paper proposes **"Non-uniform Pessimism via Quantile Distortion"** to address the over-conservatism in existing offline Distributional Reinforcement Learning (DRL).

**Core Algorithm & Mechanics**
*   **Problem Addressed:** Uniform pessimism (equal underestimation of all return quantiles).
*   **Solution:** Applies non-uniform pessimism by adjusting conservatism based on the cumulative probability level ($\tau$).
*   **Strategy:** Tail quantiles are estimated with greater pessimism, while modes (central parts of the distribution) are estimated with less pessimism.

**Mathematical Foundation**
*   **Environment Model:** Infinite horizon discounted Markov Decision Process (MDP).
*   **Representation:** Return distribution is represented via a predictor with **$M$ atoms**.
*   **Operators:**
    *   Distributional Bellman operator
    *   1-Wasserstein distance minimization
    *   Quantile projection operator

**Comparison with Baselines**
*   **vs. CODAC:** Unlike the CODAC baseline, which makes the penalty dependent on state-action pairs, this method makes the penalty dependent on the cumulative probability level ($\tau$).

---

### üìä Results

The proposed non-uniform pessimism approach was validated on a simulated benchmark (D4RL Gym-MuJoCo) and demonstrated the following outcomes:

**General Performance**
*   Outperformed uniform pessimism baselines in offline RL settings.
*   Demonstrated better generalization by avoiding excessive penalization of well-supported regions.
*   Achieved a reduction in the **1-Wasserstein distance** between estimated and true return distributions.

**Benchmark Scores (D4RL)**

| Task | Proposed Method Score | Uniform Baseline Score |
| :--- | :--- | :--- |
| **HalfCheetah-medium-v2** | **48.3** | 44.9 |
| **Walker2d-medium-v2** | **81.5** | 77.1 |

*   *Note: The method also showed strong results on Hopper tasks.*

---

### üöÄ Contributions

1.  **Bottleneck Identification:** Identified "uniform pessimism" as a key bottleneck in offline DRL.
2.  **Novel Concept:** Introduced the "quantile distortion" concept to facilitate non-uniform pessimism.
3.  **Adaptive Framework:** Established a new adaptive framework that links estimation conservatism to data availability.
4.  **Theoretical & Empirical Proof:** Provided comprehensive theoretical and empirical evidence showing improved generalization and performance over uniform methods.

---