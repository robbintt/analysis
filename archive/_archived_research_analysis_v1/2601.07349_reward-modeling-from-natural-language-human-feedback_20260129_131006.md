# Reward Modeling from Natural Language Human Feedback

*Zongqi Wang; Rui Wang; Yuchuan Wu; Yiyao Yu; Pinyi Zhang; Shaoning Sun; Yujiu Yang; Yongbin Li*

---

> ### üìä Quick Facts
>
> *   **Analysis Quality Score:** 3/10
> *   **Total References:** 40 Citations
> *   **Proposed Method:** RM-NLHF (Reward Modeling from Natural Language Human Feedback)
> *   **Key Framework:** Meta Reward Model (MetaRM)
> *   **Primary Benchmarks:** HelpSteer3, MATH-500
> *   **Top Metric Accuracy:** 93.88% (Gemini-2.5-pro configuration)

---

## üîç Executive Summary

Current Generative Reward Models (GRMs) utilizing Reinforcement Learning from Verifiable Rewards (RLVR) suffer from a critical vulnerability termed **"outcome-process inconsistency."** Because these systems are trained exclusively on the correctness of final binary outcomes, they frequently achieve "spurious successes" by guessing correct answers without employing sound reasoning logic. This limitation introduces significant noise into the training pipeline, incentivizing shortcut learning rather than genuine problem-solving.

To resolve this, the authors propose **RM-NLHF**, shifting the paradigm from binary preference classification to a framework leveraging natural language critiques. The core innovation is the **Meta Reward Model (MetaRM)**, designed to bridge the gap between scarce high-fidelity data and large-scale training needs. The system employs a "Process Reward Proxy" which utilizes an external LLM (e.g., GPT-5-mini) as a "Meta-Judge" to extract "core arguments" from feedback. 

Experiments on HelpSteer3 and MATH-500 benchmarks confirmed high rates of outcome-process inconsistency in standard models (up to 44.24%). In evaluating the authors' proposed solution, similarity-based methods significantly outperformed direct LLM judgment. The optimal configuration achieved **93.88% accuracy**, while the cost-effective implementation selected for scaling achieved **85.71% accuracy**.

---

## üöÄ Key Findings

*   **Vulnerability of Binary Tasks:** Current Generative Reward Models (GRMs) using RLVR are prone to 'spurious successes,' guessing correct binary outcomes without sound reasoning and introducing significant noise.
*   **Superiority of Process Rewards:** Utilizing natural language feedback is more accurate and effective than relying solely on outcome-only supervision.
*   **Scalability via MetaRM:** A Meta Reward Model can effectively learn to predict process rewards from small sets of human critiques and generalize to large-scale data.
*   **Benchmark Performance:** The proposed RM-NLHF method consistently outperforms state-of-the-art GRMs trained with outcome-only rewards.

---

## üß† Methodology

The authors propose **RM-NLHF (Reward Modeling from Natural Language Human Feedback)**, shifting from binary preference classification to a framework leveraging natural language feedback.

1.  **Reward Calculation:** Training rewards are computed based on the semantic similarity between GRM-generated critiques and human critiques.
2.  **Meta Reward Model (MetaRM):** To address scalability, a Meta Reward Model is introduced. It learns process reward patterns from datasets with human critiques and subsequently predicts rewards for unannotated data.

---

## ‚öôÔ∏è Technical Details

**System Operation**
*   **Task Setting:** Generative Reward Models (GRMs) operate in a pairwise setting, taking a query and two responses.
*   **Output:** The model generates natural language rationales (critiques) and predicts a preference label.
*   **Process Definition:** The 'process' is defined specifically as the generated critiques.

**Training Framework**
*   **Framework:** Utilizes the GRPO framework with a binary outcome reward signal based on label matches.
*   **Updates:** Advantages are computed by normalizing rewards within a group; updates use a clipped surrogate objective with KL regularization.

**Process Reward Proxy**
*   **Meta-Judge:** An external LLM ('gpt-5-mini') acts as a 'Meta-Judge' to extract 'core arguments' from both sets of critiques, filtering out insignificant details.
*   **Quantification:** The process reward is quantified using the **F1 score** between these sets of core arguments.
*   **Objective:** Addresses outcome-process inconsistency by calculating semantic similarity between GRM-generated critiques and human critiques.

---

## üìà Results

**Inconsistency Analysis**
Experiments on "HelpSteer3" and "MATH-500" identified high outcome-process inconsistency in pairwise rewarding tasks:
*   **RM-R1-DeepSeek-Distilled-Qwen-7B:** 44.24% inconsistency rate.
*   **Gemini-2.5-pro:** 26.10% inconsistency rate.
*   **Claude-3.7-sonnet:** 33.62% inconsistency rate.
*   *Note:* Mathematical reasoning tasks showed low inconsistency.

**Process Reward Proxy Evaluation**
*   **Comparison:** Similarity methods outperformed direct LLM judgment by **12% to 59%** (based on 49 samples).
*   **Best Configuration:** Gemini-2.5-pro with Similarity w/ All HC Recall achieved **93.88% accuracy**.
*   **Selected Implementation:** Uses gpt-5-mini with Similarity w/ Core HC (F1), achieving **85.71% accuracy** to balance cost-effectiveness and training stability.

---

## ‚úÖ Contributions

*   **Diagnosing RLVR Noise:** Identified a critical flaw in mainstream RLVR approaches where binary tasks allow models to bypass rigorous reasoning.
*   **Natural Language Supervision:** Introduced the RM-NLHF paradigm that replaces limited binary supervision with natural language feedback to expand solution space and accuracy.
*   **Generalization Framework:** Provided the MetaRM solution to scale high-fidelity process rewards, bridging the gap between scarce human feedback and large-scale training requirements.