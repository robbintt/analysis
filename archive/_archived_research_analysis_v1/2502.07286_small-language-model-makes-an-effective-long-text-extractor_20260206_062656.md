---
title: Small Language Model Makes an Effective Long Text Extractor
arxiv_id: '2502.07286'
source_url: https://arxiv.org/abs/2502.07286
generated_at: '2026-02-06T06:26:56'
quality_score: 9
citation_count: 11
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Small Language Model Makes an Effective Long Text Extractor
*Yelin Chen; Fanjin Zhang; Jie Tang*

---

> ### üìã Executive Summary
>
> The research addresses the persistent challenge of extracting long entity spans from extended text sequences, a task where current Named Entity Recognition (NER) methods struggle. While Large Language Models (LLMs) have shown promise in general tasks, they often fail to accurately generate long entity spans and incur prohibitively high fine-tuning costs. Simultaneously, traditional span-based methods face computational limitations, frequently encountering Out-of-Memory (OOM) errors and excessive processing overhead when handling long inputs. This paper targets the need for a solution that maintains high extraction accuracy without requiring massive GPU memory or the computational resources associated with heavy LLMs.
>
> The authors propose **SeNER**, a lightweight, span-based NER framework designed specifically for long-context processing. The architecture‚Äôs core innovation is the **Bidirectional Sliding-window Plus-shaped Attention (BiSPA)** mechanism, which models interactions between token-pair spans simultaneously. This approach effectively eliminates the need for exhaustive enumeration of candidate spans, thereby significantly reducing redundant computations. Additionally, SeNER employs a **Bidirectional Arrow Attention Mechanism with LogN-Scaling** on the [CLS] token. This scaling stabilizes entropy across variable input lengths, allowing the model to capture information from extended sequences efficiently. The model is further optimized using Low-Rank Adaptation (LoRA) and a whole word masking strategy during training.
>
> SeNER achieves state-of-the-art extraction accuracy across three long-text NER datasets while remaining GPU-memory-friendly. It demonstrates the ability to process input texts up to six times longer than previous span-based methods without encountering OOM errors. In direct comparison, SeNER significantly outperformed LLM baselines, which achieved a low F1 score of 21.87%. On the SciREX dataset, SeNER also maintained lower inference times compared to existing solutions. Sensitivity analysis further validated the design, revealing that while small window sizes limit information aggregation, BiSPA‚Äôs optimized approach avoids the focus difficulties and redundant candidate generation seen in excessively large windows.
>
> This paper is significant because it validates that high performance in long-text extraction does not require massive parameter counts or extensive computational resources. By demonstrating that small language models can outperform or match heavy LLM-based approaches, the research challenges the prevailing "bigger is better" paradigm. The introduction of SeNER offers a more accessible and efficient path for future research, proving that architectural innovations‚Äîspecifically in attention mechanisms and span modeling‚Äîcan effectively solve complex extraction tasks in resource-constrained environments.

---

### üöÄ Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Core Innovation** | Bidirectional Sliding-window Plus-shaped Attention (BiSPA) |
| **Processing Power** | Handles texts **6x longer** than previous span-based methods |
| **Memory Efficiency** | GPU-memory friendly; prevents Out-of-Memory (OOM) errors |
| **LLM Comparison** | Significantly outperforms LLM baselines (F1: 21.87%) |
| **Training Strategy** | Utilizes LoRA and Whole Word Masking |

---

### üîë Key Findings

*   **State-of-the-Art Accuracy:** The proposed SeNER method achieves SOTA extraction accuracy on three long-text NER datasets.
*   **Resource Efficiency:** It operates as a GPU-memory-friendly solution that processes extended texts without substantial computational overhead.
*   **Overcoming Limitations:** The method overcomes generation-based methods, which struggle to generate long entity spans accurately and incur high fine-tuning costs.
*   **Reduced Redundancy:** The **Bidirectional Sliding-window Plus-shaped Attention (BiSPA)** mechanism significantly reduces redundant candidate token-pair spans while effectively modeling interactions.
*   **Small vs. Large Models:** The approach validates that lightweight, small language models can outperform or match heavy LLM-based approaches for long-text entity extraction.

---

### üõ†Ô∏è Methodology

**SeNER** is a lightweight, span-based NER framework specifically architected for long texts and long entity spans.

*   **Attention Mechanism:** It utilizes a bidirectional arrow attention mechanism combined with **LogN-Scaling** on the [CLS] token to capture information from extended input sequences.
*   **Span Modeling:** It employs the **Bidirectional Sliding-window Plus-shaped Attention (BiSPA)** mechanism to reduce redundant candidate spans by modeling interactions between token-pair spans simultaneously.
*   **Efficiency:** This design eliminates the need for exhaustive enumeration, streamlining the extraction process.

---

### ‚öôÔ∏è Technical Details

| Component | Description |
| :--- | :--- |
| **Framework Type** | Lightweight, span-based Named Entity Recognition (NER) for long texts. |
| **Attention Architecture** | **Bidirectional Arrow Attention Mechanism** with **LogN-Scaling** on the [CLS] token. |
| **Scaling Purpose** | Stabilizes entropy across variable input lengths. |
| **Core Mechanism** | **BiSPA** (Bidirectional Sliding-window Plus-shaped Attention). |
| **BiSPA Function** | Models token-pair span interactions efficiently, reducing redundant computations and memory usage. |
| **Training Strategy** | Incorporates **LoRA** (Low-Rank Adaptation) for robustness and a whole word masking strategy. |

---

### üìä Results

*   **Performance:** SeNER achieves state-of-the-art extraction accuracy on three long-text NER datasets.
*   **Capacity:** Capable of processing texts **6 times longer** than previous span-based methods.
*   **Comparison:** Significantly outperforms LLM baselines, which achieved an F1 score of only **21.87%**.
*   **Stability:** BiSPA prevents Out-of-Memory (OOM) errors.
*   **Speed:** On the SciREX dataset, SeNER maintains lower inference times.
*   **Sensitivity Analysis:** 
    *   **Small windows:** Limit information aggregation or long entity extraction.
    *   **Large windows:** Cause focusing difficulties or introduce redundant candidates.

---

### üåü Contributions

*   **Problem Identification:** The paper identifies and addresses the under-explored challenge of extracting longer entity spans from extended texts.
*   **Architectural Innovation:** Introduces the SeNER architecture, specifically the integration of **LogN-Scaling** for text embedding and **BiSPA** for efficient span modeling.
*   **Resource Accessibility:** The research demonstrates that high performance on long-text extraction tasks does not require massive GPU memory or large language models, offering a more accessible and efficient path for future research.

---
**References:** 11 citations