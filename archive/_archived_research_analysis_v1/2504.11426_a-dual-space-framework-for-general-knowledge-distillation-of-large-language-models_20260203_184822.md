---
title: A Dual-Space Framework for General Knowledge Distillation of Large Language
  Models
arxiv_id: '2504.11426'
source_url: https://arxiv.org/abs/2504.11426
generated_at: '2026-02-03T18:48:22'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# A Dual-Space Framework for General Knowledge Distillation of Large Language Models

*Xue Zhang; Songming Zhang; Yunlong Liang; Fandong Meng; Yufeng Chen; Jinan Xu; Jie Zhou*

---

> ### üìä Quick Facts
> *   **Quality Score:** 8/10
> *   **Citations:** 40
> *   **Top Performance Gain:** Up to 27.42% (AKL) on instruction-following benchmarks
> *   **Key Innovation:** Unified prediction heads and Exact Token Alignment (ETA)
> *   **Scope:** Universal framework for on-policy and off-policy KD

---

## üìù Executive Summary

This research addresses a fundamental limitation in white-box Knowledge Distillation (KD) for Large Language Models (LLMs): the structural incompatibility between teacher and student output spaces. Current methods attempt to bridge probability distributions generated by distinct prediction heads, which restricts similarity and limits efficacy. Crucially, this prevents KD application between models with different vocabularies or tokenizers.

The authors propose the **Dual-Space Knowledge Distillation (DSKD)** framework, which decouples the representation space from the output space to enable universal distillation. The core technical innovation involves unifying prediction heads to ensure output distributions reside in the same space, utilizing "Cross-Space Projectors" to map hidden states between teacher and student layers. To address vocabulary mismatches, the paper introduces the **Exact Token Alignment (ETA)** algorithm.

DSKD establishes a new state-of-the-art baseline for LLM compression, demonstrating significant improvements in instruction-following, mathematical reasoning, and code generation. In same-vocabulary settings, On-Policy DSKD surpassed teacher performance by up to 27.42%. Ablation studies confirmed that removing the KD loss in the Teacher Space caused the most significant performance degradation (-1.52%). This work enables practitioners to transfer knowledge between arbitrary LLMs, offering greater flexibility in model selection and deployment.

---

## üîç Key Findings

*   **Limitation of Current Methods:** Current white-box KD methods are limited because they attempt to bridge probability distributions from different output spaces. This restricts teacher-student similarity and prevents application to models with different vocabularies.
*   **Root Cause Identified:** The primary bottleneck is the use of distinct prediction heads for teacher and student models.
*   **Superior Performance:** The proposed DSKD framework significantly outperforms existing methods and surpasses cross-tokenizer techniques on benchmarks for instruction-following, mathematical reasoning, and code generation.
*   **Universal Application:** DSKD functions as a universal framework supporting both off-policy and on-policy KD regardless of vocabulary differences.

---

## üß© Methodology

The researchers propose the **Dual-Space Knowledge Distillation (DSKD)** framework, designed to unify the prediction heads of teacher and student models. The approach consists of three main components:

1.  **Unified Prediction Heads:** Ensures output distributions reside in the same space, bridging the gap between teacher and student outputs.
2.  **Cross-Space Projectors:** Utilizes ideal initialization to map hidden states between the representation spaces of the teacher and student.
3.  **Exact Token Alignment (ETA):** A specific algorithm developed to align identical tokens across sequences that employ different tokenization strategies.

---

## ‚öôÔ∏è Technical Details

**Core Architecture: DSKD**
DSKD addresses limitations in white-box KD by unifying prediction heads through dual projectors that map hidden states between teacher and student representation spaces.

*   **Projectors:**
    *   **Teacher-to-Student:** A trained projector that maps teacher representations to the student space.
    *   **Student-to-Teacher:** A calculated projector derived via pseudo-inverse to map student representations to the teacher space.
*   **Loss Function:** The total loss combines three elements:
    1.  Student space KD loss
    2.  Cross-entropy for projector training
    3.  Teacher space KD loss

**Handling Vocabulary Mismatches: ETA**
The **Exact Token Alignment (ETA)** algorithm enables distillation across different vocabularies by aligning tokens based on:
*   Matching prefixes
*   Top-1 predictions

---

## üìà Results

The framework was evaluated using ROUGE-L on instruction-following benchmarks, demonstrating significant improvements over baselines.

**Same-Vocabulary Settings**
*(e.g., LLaMA2-7B to TinyLLaMA-1.1B)*
*   **On-Policy DSKD:** Surpassed teacher performance, achieving up to **27.42% (AKL)**.

**Different-Vocabulary Settings**
*(e.g., Mistral-7B to TinyLLaMA-1.1B)*
*   **DSKD-ETA:** Achieved **27.67%** in On-Policy mode.

**Ablation Studies**
*   **Critical Component:** Removing KD in the **Teacher Space** caused the most significant performance degradation.
*   **Impact:** Resulted in a drop of **-1.52%** in one specific test case.

---

## ‚ú® Contributions

*   **Theoretical Insight:** Identifies the separation of prediction heads as the root cause of suboptimal knowledge transfer.
*   **DSKD Framework:** Introduces a generalizable solution that decouples representation space from output space, allowing distillation between any two LLMs.
*   **ETA Algorithm:** Develops the Exact Token Alignment (ETA) algorithm to solve the challenge of distilling knowledge between models with different tokenizers.
*   **New SOTA Baseline:** Establishes a new state-of-the-art baseline for compressing LLMs, demonstrating clear improvements in complex tasks like mathematical reasoning and code generation.