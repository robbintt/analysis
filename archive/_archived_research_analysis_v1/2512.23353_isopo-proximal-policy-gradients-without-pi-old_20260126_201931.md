---
title: 'ISOPO: Proximal policy gradients without pi-old'
arxiv_id: '2512.23353'
source_url: https://arxiv.org/abs/2512.23353
generated_at: '2026-01-26T20:19:31'
quality_score: 8
citation_count: 23
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# ISOPO: Proximal policy gradients without pi-old
*Proximal Policy, Region Policy, Isometric Policy, Nilin Abrahamsen*

> ### ðŸ“Š Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Quality Score** | 8/10 |
> | **Citations** | 23 References |
> | **Evaluation Task** | GSM8K (Mathematical Reasoning) |
> | **Model Architecture** | Qwen-3 (0.6B) |
> | **Framework** | VeRL |
> | **Core Innovation** | Batch-dimension optimization without $\pi_{old}$ |

---

## Executive Summary

Reinforcement Learning (RL) faces a fundamental trade-off between the theoretical stability offered by Natural Policy Gradients (NPG) and the computational efficiency of standard methods like Proximal Policy Optimization (PPO). While PPO relies on reference policies and ratio clipping to prevent destructive large updates, these mechanisms introduce variance and increase implementation complexity. Conversely, NPG respects the geometry of the policy space for superior stability but remains computationally prohibitive due to the manipulation of large Fisher Information Matrices. This research addresses the need for an algorithm that retains the robustness of geometric constraints without the latency and complexity associated with trust-region clipping mechanisms.

The authors introduce **ISOPO (Isometric Policy Optimization)**, a new class of algorithms that approximates the natural policy gradient in a single gradient step, entirely eliminating the need for a reference policy ($\pi_{old}$) or importance ratio clipping. Technically, ISOPO shifts the optimization focus from the parameter dimension to the batch dimension, operating directly on sequences within the batch. It re-weights the log-probability gradients of each sequence based on their contribution to KL-divergence before contracting them with the advantage function. The method is implemented via two variants: **Non-Interacting ISOPO**, which estimates the Fisher norm for gradient rescaling via a backward hook in a single pass, and **Interacting ISOPO**, which employs Neural Tangent Kernel (NTK)-based transformations to model layer-wise interactions between sequences.

ISOPO was evaluated on the GSM8K mathematical reasoning task using the **Qwen (0.6B)** model within the VeRL framework. The results demonstrated that Non-Interacting ISOPOâ€”configured with specific hyperparameters $p=0$, $q=-1$, and $r=-2$ (no regularization)â€”achieved validation performance comparable to or exceeding the GRPO baseline. Furthermore, Interacting ISOPO, utilizing EMA-based regularization, maintained median stability equivalent to clipped baselines. The authors established that ISOPO approximates the natural policy gradient with stability on par with trust-region methods while incurring negligible computational overhead compared to vanilla REINFORCE.

The significance of ISOPO lies in its ability to bridge the gap between the theoretical stability of natural policy gradients and the computational speed of simpler methods like vanilla REINFORCE. By removing the necessity for importance sampling and trust-region clipping, ISOPO simplifies the architecture of policy optimization and reduces the variance typically introduced by surrogate objectives. This technical advancement suggests a path toward more efficient training pipelines for Large Language Models (LLMs), particularly for reasoning tasks, potentially influencing future RL algorithm designs to prioritize geometric normalization over complex constraint mechanisms.

---

## Key Findings

*   **High Efficiency:** ISOPO achieves an approximation of the natural policy gradient using only a single gradient step, significantly improving efficiency over traditional iterative methods.
*   **Low Overhead:** The method introduces negligible computational overhead compared to vanilla REINFORCE.
*   **Architecture Simplification:** ISOPO eliminates the need for a reference policy ($\pi_{old}$) or importance ratio clipping mechanisms.
*   **Flexible Normalization:** The algorithm can utilize either Fisher metric normalization or Neural Tangent Kernel (NTK)-based transformations to maintain layer-wise updates.

---

## Methodology

ISOPO operates by modifying the gradient calculation process within a single backward pass. The core methodology involves:

1.  **Gradient Modification:** The method normalizes the log-probability gradient of each sequence using the Fisher metric before contracting it with the advantages.
2.  **NTK Transformations:** An advanced variant applies transformations to microbatch advantages based on the Neural Tangent Kernel (NTK) specific to each network layer.
3.  **Layer-wise Integration:** These transformations are applied layer-wise, integrating complex adjustments into the standard backpropagation flow without the need for multiple iterative steps.

---

## Technical Details

ISOPO (Isometric Proximal Optimization) is designed to approximate the Natural Policy Gradient (NPG) without the computational intractability of the full Fisher matrix.

### Core Mechanism
*   **Objective:** Eliminate the need for a reference policy ($\pi_{old}$) or importance ratio clipping.
*   **Dimension Shift:** Unlike standard optimizers that act on the parameter dimension, ISOPO acts on the **batch dimension** (sequences).
*   **Weighting:** It re-weights gradients based on their contribution to KL divergence before applying the advantage.

### Algorithmic Variants

| Variant | Description |
| :--- | :--- |
| **Non-Interacting ISOPO** | Estimates the Fisher norm to rescale gradients using a backward hook and is computed in a single backward pass. |
| **Interacting ISOPO** | Models interactions between sequences using the empirical Neural Tangent Kernel (NTK) and applies a preconditioning update rule similar to layer-wise MinSR. |

---

## Contributions

*   **New Algorithm Class:** Introduction of Isometric Policy Optimization (ISOPO), a new class of proximal policy optimization algorithms that bypasses the complexity of maintaining a reference policy.
*   **Bridging the Gap:** Establishment of a method that bridges the gap between the theoretical stability of natural policy gradients and the computational speed of simpler methods like vanilla REINFORCE.
*   **Technical Innovation:** Proposal of using Fisher metric normalization and Neural Tangent Kernel transformations within a microbatch context to approximate natural gradients without the variance introduced by importance ratio clipping.

---

## Results

ISOPO was evaluated on the GSM8K task using the Qwen-3 (0.6B) model within the VeRL framework, utilizing group-relative advantage and no KL penalty.

*   **Non-Interacting ISOPO:** Configured with hyperparameters $p=0$, $q=-1$, $r=-2$ (no regularization), it achieved validation performance comparable to or exceeding the GRPO baseline despite lacking clipping mechanisms.
*   **Interacting ISOPO:** Using EMA-based regularization, this variant maintained median stability comparable to clipped baselines.
*   **Overall Performance:** Key findings indicate ISOPO approximates the natural policy gradient with negligible computational overhead, effectively removing the need for the old policy surrogate objective and trust-region clipping.