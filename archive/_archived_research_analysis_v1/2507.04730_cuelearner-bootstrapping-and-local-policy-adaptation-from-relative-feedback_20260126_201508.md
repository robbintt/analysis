---
title: 'CueLearner: Bootstrapping and local policy adaptation from relative feedback'
arxiv_id: '2507.04730'
source_url: https://arxiv.org/abs/2507.04730
generated_at: '2026-01-26T20:15:08'
quality_score: 9
citation_count: 28
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# CueLearner: Bootstrapping and local policy adaptation from relative feedback

*Autonomous Systems, Robotics Systems, Pascal Roth, Roland Siegwart, Lionel Ott, Andrei Cramariuc, Giulio Schiavi*

***

> **QUICK FACTS**
> *   **Quality Score:** 9/10
> *   **Citations:** 28
> *   **Feedback Type:** Relative/Ordinal (No scalar rewards)
> *   **Efficiency:** 5x fewer labels than scalar feedback (Billiards)
> *   **Adaptation:** ~10x faster than standard RL fine-tuning
> *   **Constraints:** $\pm 1^{\circ}$ (embodiment), $\pm 0.3^{\circ}$ (others)

## Executive Summary

Reinforcement learning (RL) for robotics typically relies on well-defined scalar reward functions or extensive demonstrations, both of which are difficult to obtain in real-world scenarios. Preference-Based Reinforcement Learning (PBRL) addresses this by utilizing ordinal feedback, but existing PBRL methods often suffer from high sample inefficiency and "cold-start" issues, requiring prohibitive amounts of data to converge.

This research introduces the **CueLearner** framework, which bootstraps control policies from scratch using only sparse, relative human feedback. The core innovation is the "Cue" function—a cost estimator that decouples task-specific value learning from the policy. Unlike standard RL methods that apply global gradient updates, CueLearner uses the Cue function to identify local regions associated with errors and applies **targeted, bounded residual learning**. This approach ensures stability during adaptation by constraining updates to specific physical limits. The framework exhibited robust sim-to-real transfer, demonstrating substantial improvements in sample efficiency (5x fewer labels in Billiards tasks) and adaptation speed (10x faster fine-tuning) over baseline methods.

***

## Key Findings

*   **Learning from Scratch:** The system successfully learns control policies using **only relative feedback**, eliminating the need for pre-trained demonstrations or reward functions.
*   **Superior Sample Efficiency:** Achieved higher sample efficiency than global optimization methods by restricting policy updates to states associated with negative feedback or failures.
*   **Real-World Validation:** The approach was successfully validated on real-world robotic manipulation tasks, demonstrating robust transfer from simulation to physical hardware.
*   **Minimized Human Effort:** An intelligent querying strategy minimizes human labeling effort by specifically selecting informative states for feedback.

## Methodology

The proposed method operates within a **Preference-Based Reinforcement Learning (PBRL)** setting, utilizing ordinal feedback provided by human teachers rather than numerical rewards.

*   **The Cue Function:** A cost function estimator bootstrapped from initial relative feedback. It mimics human preferences over the state space without needing an initial reward model.
*   **Local Policy Adaptation:** Instead of performing global gradient updates, the methodology identifies local regions of the state space where the Cue indicates errors.
*   **Targeted Updates:** The system applies targeted adaptations only to the specific policy parameters responsible for the identified errant states, rather than updating the entire policy at once.

## Contributions

*   **CueLearner Framework:** Introduces a framework that decouples task-specific value learning from the policy, enabling effective bootstrapping without initial expert data.
*   **Local Policy Gradient Adaptation:** developed a novel method to bridge the gap between abstract preference feedback and low-level motor control adjustments.
*   **Interactive Learning Pipeline:** Demonstrates a complete pipeline that resolves cold-start issues via active querying and refines policies through precise local adjustments.

## Technical Details

**Feedback & Learning**
*   **Input:** Exclusive use of relative feedback (comparisons); no scalar rewards or demonstrations used.
*   **Query Strategy:** Intelligently selects informative states, specifically prioritizing negative feedback or failure points.

**Bounded Residual Learning**
To handle environment changes safely, the method employs bounded residual learning with explicit physical constraints:
*   **Embodiment Adaptation:** Updates constrained to $\pm 1^{\circ}$
*   **Other Scenarios:** Updates constrained to $\pm 0.3^{\circ}$

## Results

**Billiards Task**
*   Achieved performance on par with or superior to Scalar Feedback methods.
*   **Efficiency:** Required **5x fewer labels** (500–1000 labels) compared to baselines.

**Navigation Task**
*   Effective policy achieved with very sparse data.
*   **Efficiency:** Required only **350 labels** compared to 3,000+ for Scalar Feedback.

**Post-hoc Adaptation**
*   **Speed:** Adapted approximately **one order of magnitude faster** than standard RL fine-tuning.

**Sensor Miscalibration (Lidar)**
*   **Scenario:** Correcting a $15^{\circ}$ error.
*   **Outcome:** Recovered performance from **72% to 82%** using only 500 labels.