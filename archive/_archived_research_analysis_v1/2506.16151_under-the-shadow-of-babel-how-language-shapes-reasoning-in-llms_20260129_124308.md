# Under the Shadow of Babel: How Language Shapes Reasoning in LLMs

*Chenxi Wang; Yixuan Zhang; Lang Gao; Zixiang Xu; Zirui Song; Yanbo Wang; Xiuying Chen*

***

> ### **Quick Facts**
> *   **Quality Score:** 9/10
> *   **Model Analyzed:** Qwen1.5-1.8B-Chat
> *   **Dataset:** BICAUSE (400 samples, 8 domains)
> *   **Metric:** Relative Component Attention Ratio (RCAR)
> *   **References:** 14 citations

***

## Executive Summary

This paper addresses the fundamental question of how language structure shapes reasoning processes in Large Language Models (LLMs), investigating the validity of the linguistic relativity hypothesis within artificial intelligence. While LLMs are capable of multilingual processing, it is unclear whether they simply map surface-level linguistic patterns or if specific typological features induce distinct, language-dependent cognitive biases. This matters because if models rigidly internalize language-specific habits, their reasoning capabilities may falter when encountering inputs that deviate from linguistic norms, posing challenges for the robustness and generalizability of multilingual AI systems.

The researchers introduce **BICAUSE**, a novel structured dataset comprising semantically aligned Chinese and English samples presented in both forward and reversed causal forms to probe reasoning rigidity. Technically, the study decomposes sentences into **13 interpretable syntactic components** and utilizes the **Relative Component Attention Ratio (RCAR)** metric with a two-level normalization strategy to analyze attention mechanisms. This framework allows for a granular structural analysis of the Qwen1.5-1.8B-Chat model, enabling the team to trace how linguistic typology influences internal processing trajectories and representation convergence.

The experiments reveal that while baseline accuracy is high (91.0% for English and 91.2% for Chinese), internal attention mechanisms differ significantly by language. Chinese models exhibit rigid attention patterns focused heavily on sentence-initial connectives and causes, leading to pronounced performance degradation on atypical (reversed) inputs compared to English models, which display a more balanced attention distribution. Despite these divergent processing paths, the study found that when causal reasoning is successful, the internal representations of both languages converge on semantically aligned abstractions, indicating that models can transcend surface forms to achieve a shared understanding.

This research provides the first empirical verification through structural analysis that LLMs internalize reasoning biases consistent with linguistic relativity, effectively mirroring human cognitive patterns. By demonstrating that models rely on rigid surface linguistic habits yet retain the capacity for language-agnostic semantic convergence, the authors deepen the understanding of LLM cognition. The release of the BICAUSE dataset and these findings establishes a new benchmark for evaluating cross-lingual reasoning, highlighting the necessity of addressing linguistic rigidity to improve the consistency and reliability of multilingual AI models.

***

## Key Findings

*   **Typologically Aligned Attention Patterns:** LLMs demonstrate attention mechanisms that align with linguistic typology; models focus significantly on causes and sentence-initial connectives in Chinese, whereas English exhibits a more balanced attention distribution.
*   **Internalization of Language-Specific Biases:** Models internalize preferences for language-specific causal word orders, applying them rigidly to atypical inputs. This rigidity leads to performance degradation, which is notably more pronounced in Chinese.
*   **Convergence of Representations:** When causal reasoning is successful, internal model representations converge toward semantically aligned abstractions across different languages, suggesting the emergence of a shared understanding that transcends surface-level forms.
*   **Verification of Linguistic Relativity:** The study empirically confirms that LLMs do not merely mimic surface linguistic structures but also internalize reasoning biases shaped by language, effectively mirroring human cognitive patterns predicted by linguistic relativity.

## Methodology

The researchers utilized **BICAUSE**, a novel structured bilingual dataset designed specifically for causal reasoning. This dataset comprises semantically aligned Chinese and English samples presented in both forward and reversed causal forms. The study involved a structural analysis of model internals to examine attention patterns and internal representations, allowing the team to evaluate how linguistic structure influences model reasoning and cognitive processes.

## Contributions

*   **Introduction of BICAUSE:** The release of a structured, semantically aligned bilingual dataset tailored for analyzing causal reasoning in LLMs across different languages.
*   **Empirical Evidence for Linguistic Relativity in AI:** The first empirical verification through structural analysis that LLMs exhibit cognitive patterns and reasoning biases consistent with the linguistic relativity hypothesis.
*   **Deepening Understanding of LLM Cognition:** Provided insights into the dichotomy of LLM processing—demonstrating that while models rely on surface linguistic habits (leading to rigidity), they are capable of converging on shared semantic abstractions when reasoning succeeds.

## Technical Details

*   **Dataset:** BICAUSE (400 samples across 8 domains consisting of 3-step causal chains)
*   **Model:** Qwen1.5-1.8B-Chat
*   **Syntactic Decomposition:** Sentences decomposed into 13 interpretable syntactic components.
*   **Metric:** Relative Component Attention Ratio (RCAR).
*   **Normalization:** Two-level normalization strategy employed to analyze attention mechanisms across languages.

## Results

Baseline accuracy for Qwen1.5-1.8B-Chat was **91.0% for English** and **91.2% for Chinese**. Cross-linguistic analysis revealed remarkably similar attention trajectory trends, with subjects consistently receiving more attention than verbs and the final subject peaking in attention at layers 17–18. The study found that Chinese models exhibit greater reasoning rigidity and performance degradation on atypical (reversed) inputs compared to English models.