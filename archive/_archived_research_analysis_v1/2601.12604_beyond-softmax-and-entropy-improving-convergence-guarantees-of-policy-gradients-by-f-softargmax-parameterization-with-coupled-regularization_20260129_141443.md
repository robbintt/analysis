# Beyond Softmax and Entropy: Improving Convergence Guarantees of Policy Gradients by f-SoftArgmax Parameterization with Coupled Regularization

*Safwan Labbi; Daniil Tiapkin; Paul Mangold; Eric Moulines*

---

> ### **Quick Facts**
>
> *   **Method:** f-PG (f-regularized Policy Gradient)
> *   **Key Innovation:** Coupled f-divergence regularization with f-softargmax
> *   **Optimization:** Satisfies Polyak-Łojasiewicz (PL) inequality
> *   **Sample Complexity:** Polynomial (vs. Exponential in standard Softmax)
> *   **Preconditioning:** Not required
> *   **Citations:** 40 references
> *   **Quality Score:** 8/10

---

## Executive Summary

Standard policy gradient methods in reinforcement learning, particularly those relying on Softmax parameterization and Shannon entropy regularization, face critical theoretical limitations in finite Markov Decision Processes (MDPs). These conventional approaches suffer from ill-conditioned optimization landscapes, resulting in slow convergence that necessitates computationally expensive preconditioning techniques. Most significantly, they are plagued by exponential sample complexity, creating a substantial barrier to theoretical feasibility and practical scalability.

This paper addresses these foundational challenges by proposing the **f-PG (f-regularized Policy Gradient)** method, which replaces standard Softmax with a generalized f-softargmax parameterization. The core technical innovation is a **"coupled regularization"** strategy, where the regularizer is explicitly derived from the same f-divergence used in the policy parameterization. This specific coupling transforms the regularized objective into a function that satisfies the Polyak-Łojasiewicz (PL) inequality.

By ensuring the PL condition holds, the method effectively eliminates bad local minima and enables linear convergence without relying on external preconditioning. The paper delivers rigorous theoretical proofs establishing the first explicit non-asymptotic last-iterate convergence guarantees for stochastic policy gradient methods in finite MDPs without preconditioning. The primary quantitative achievement is the reduction of sample complexity to **polynomial rates**, a dramatic improvement over the exponential complexity associated with standard Softmax methods. This work validates f-divergence based approaches as superior alternatives to Shannon entropy, paving the way for more computationally efficient and scalable RL algorithms.

---

## Key Findings

*   **Polyak-Łojasiewicz Satisfaction:** Coupling the f-softargmax parameterization with a matching f-divergence regularizer ensures the regularized objective satisfies a Polyak-Łojasiewicz (PL) inequality, significantly improving the optimization landscape.
*   **Convergence Guarantees:** The authors established the **first explicit non-asymptotic last-iterate convergence guarantees** for stochastic policy gradient methods in finite MDPs without the need for preconditioning.
*   **Polynomial Sample Complexity:** When utilizing Tsallis divergences, the proposed method (f-PG) achieves **polynomial sample complexity**. This represents a major theoretical breakthrough compared to the exponential sample complexity of standard softmax parameterizations.
*   **Optimization Efficiency:** The proposed approach eliminates the need for computationally expensive preconditioning typically required to overcome ill-conditioning and slow convergence.

---

## Methodology

The research introduces a novel framework designed to fix the structural weaknesses of standard policy gradients:

1.  **Generalized Parameterization:** The methodology replaces the standard softmax parameterization with a generalized family called **f-softargmax**.
2.  **Coupled Regularization:** It introduces a regularization strategy where the regularizer is induced by the same f-divergence used in the f-softargmax parameterization. This alignment is crucial for shaping the optimization landscape correctly.
3.  **Theoretical Leverage:** The authors leverage the Polyak-Łojasiewicz (PL) inequality structure resulting from this coupling to derive new convergence bounds, connecting the analysis of the regularized objective back to the unregularized problem.

---

## Technical Details

The paper introduces the **f-PG (f-regularized Policy Gradient)** method, which utilizes the specific combination of f-softargmax parameterization and a matching f-divergence regularizer.

*   **Core Objective:** To ensure the regularized objective satisfies the **Polyak-Łojasiewicz (PL) inequality** to eliminate bad local minima and enable linear convergence.
*   **Key Variables:**
    *   Policy: $\pi_f^\theta$
    *   Reference Policy: $\pi_{ref}$
    *   Regularization Coefficient: $\lambda$
*   **Derivation:** The proof involves bounding variance using the Tower property and Cauchy-Schwarz inequalities. It requires that well-conditioned policy entries remain larger than a specific threshold $\tau_\lambda$.
*   **Derived Bounds:**
    *   **PL Constant:** $\mu_f := \frac{\lambda(1-\gamma)\rho_{min}^2}{\zeta_f^2 \omega_f^2 \pi_{ref}^2} \min_{x} [f''(x)]^{-2}$
    *   **Variance Bound:** Dependent on $\frac{1}{(1-\gamma)^2}$.
    *   **Regularity Condition:** A specific upper bound for $\lambda$ is derived to ensure the PL condition holds.

---

## Results

The results presented in the paper are primarily analytical bounds rather than empirical data, focusing on theoretical performance guarantees:

*   **Sample Complexity:** Proof that f-PG achieves **polynomial sample complexity**, a stark contrast to the **exponential complexity** of standard Softmax.
*   **Last-Iterate Convergence:** Establishment of the first explicit **non-asymptotic last-iterate convergence guarantees** for stochastic policy gradient in finite MDPs without preconditioning.
*   **Quantitative Metrics:** Specific bounds were derived for the Polyak-Łojasiewicz constant (scaling with $\lambda$ and $\gamma$) and variance bounds, confirming the mathematical stability of the approach.

---

## Contributions

*   **Theoretical Framework:** The paper provides a rigorous theoretical framework supporting the use of generalized f-divergence parameterizations and regularization as superior alternatives to standard Shannon entropy/softmax approaches.
*   **Computational Efficiency:** It offers a path to more computationally efficient policy gradient implementations by proving convergence guarantees without the overhead of preconditioning.
*   **Sample Complexity Breakthrough:** The work addresses the fundamental theoretical limitation of exponential complexity in softmax-based policy gradients by achieving polynomial sample complexity.

---

**Quality Score:** 8/10  
**References:** 40 citations