# Graph Coloring for Multi-Task Learning

*Santosh Patapati*

---

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 9/10
> *   **Method:** SON-GOKU (Graph-Coloring Scheduler)
> *   **Accuracy Gain:** +3.2% (NYUv2) & +2.9% (CityScapes) vs baselines
> *   **Performance:** Up to **10x faster** training than CAGrad
> *   **Scalability:** Near-linear scaling vs. Quadratic $O(K^2)$
> *   **Hyperparameters:** No additional tuning required

---

## Executive Summary

Multi-Task Learning (MTL) seeks to improve generalization by learning tasks simultaneously but is fundamentally hindered by **"gradient interference"** (negative transfer). This phenomenon occurs when gradient updates for different tasks conflict, pulling shared parameters in opposing directions and destabilizing training. Existing gradient manipulation strategies often fail to resolve these dynamic conflicts effectively or suffer from prohibitive computational costs.

This paper introduces **SON-GOKU** (*Scheduling via Optimal INterference-aware Graph-COloring for TasK Grouping*), a novel scheduler that reframes conflict resolution as a dynamic graph partitioning problem. The method employs Exponential Moving Average (EMA) to smooth gradient estimates and quantifies interference using negative cosine similarity. It constructs a conflict graph where edges represent significant interference, then applies a greedy graph-coloring algorithm to partition tasks into distinct, non-conflicting groups. The scheduler performs dynamic sequential updatesâ€”activating only one compatible group per stepâ€”and continuously recomputes partitions to adapt to evolving task relationships.

In empirical evaluations across six diverse datasets, SON-GOKU achieved quantifiable improvements over state-of-the-art multi-task optimizers. Beyond predictive accuracy, it addresses the computational inefficiency of dense affinity methods; by utilizing sparse graph structures, it achieved training speeds up to **10x faster** than methods like CAGrad on large-scale problems. This work establishes a new standard for handling complex task interdependencies, offering a theoretically sound and computationally efficient method for stabilizing multi-task optimization.

---

## Key Findings

*   **Superior Performance:** The proposed method (SON-GOKU) consistently outperforms existing baselines and state-of-the-art multi-task optimizers across six diverse datasets.
*   **Mitigation of Interference:** Effectively mitigates gradient interference by partitioning tasks into groups that pull the model in the same direction (cooperative groups).
*   **Dynamic Adaptation:** Continuously recomputes task groupings throughout training to handle dynamic and evolving task relationships.
*   **Optimizer Agnostic:** Enhances the effectiveness of any underlying multi-task learning optimizer without requiring additional hyperparameter tuning.
*   **Training Stability:** Reduces gradient variance and significantly improves training stability compared to simultaneous update methods.

---

## Methodology

The core approach is the **SON-GOKU Scheduler**, designed to intelligently manage task selection during training to prevent negative transfer.

1.  **Interference Graph:** The method computes gradient interference between tasks to build an 'interference graph'.
2.  **Graph Partitioning:** It applies a greedy graph-coloring algorithm to partition tasks into distinct groups (color classes) where tasks within a group align well (low interference).
3.  **Dynamic Sequential Updates:** Instead of updating all tasks simultaneously, only one group is activated per step. This ensures that updates within a step do not conflict.
4.  **Adaptive Recomputation:** Partitioning is constantly recomputed to adapt to evolving task relationships as the model trains.

---

## Technical Details

**SON-GOKU** (Scheduling via Optimal INterference-aware Graph-COloring for TasK Grouping in MUltitask Learning) optimizes a weighted Multi-Task Learning objective by scheduling updates to specific task subsets.

The method comprises a robust five-step pipeline:

1.  **Gradient Estimation & Smoothing:** Utilizes Exponential Moving Average (EMA) to stabilize gradient estimates before processing.
2.  **Interference Quantification:** Calculates an interference coefficient based on negative cosine similarity to measure how severely two tasks conflict.
3.  **Conflict Graph Construction:** Constructs a graph where edges represent conflicts exceeding a specific threshold $\tau$.
4.  **Grouping via Greedy Graph Coloring:** Ensures non-conflicting tasks share a color (group), allowing them to be updated simultaneously without adverse effects.
5.  **Dynamic Scheduling:** One group is updated per step, with periodic recomputation to adapt to changes in the loss landscape.

**Key Characteristics:**
*   Optimizer agnostic (works with SGD, Adam, etc.).
*   Utilizes sparse graph structures for efficiency.
*   Provides theoretical guarantees on descent and convergence.

---

## Research Contributions

The paper makes three primary contributions to the field of Multi-Task Learning:

*   **Novel Framework:** Introduction of SON-GOKU, a novel interference-aware graph-coloring framework that dynamically groups tasks to prevent negative transfer.
*   **Theoretical Analysis:** Provision of extensive theoretical analysis establishing guarantees regarding descent, convergence, and the accurate identification of conflicting versus aligning tasks.
*   **Practical Optimization:** A practical solution that enhances performance by ensuring compatible updates within mini-batches, removing the need for complex manual tuning often required in MTL.

---

## Results

Evaluated across six datasets, SON-GOKU demonstrated significant improvements in both efficiency and accuracy:

*   **Accuracy:** Consistently outperformed state-of-the-art baselines and optimizers.
    *   **+3.2%** average task accuracy on **NYUv2**.
    *   **+2.9%** average task accuracy on **CityScapes** (compared to MGDA).
*   **Efficiency:** Scales efficiently with the number of tasks by using sparse graph estimation to address the $O(K^2)$ scaling of dense affinity methods.
    *   Achieved training speeds up to **10x faster** than methods like CAGrad on large-scale problems (e.g., Taskonomy).
*   **Usability:** Requires no additional hyperparameter tuning, making it a drop-in enhancement for existing pipelines.
*   **Adaptability:** Qualitatively, the method successfully adapted to the evolving structure of interactions among tasks throughout the training process.