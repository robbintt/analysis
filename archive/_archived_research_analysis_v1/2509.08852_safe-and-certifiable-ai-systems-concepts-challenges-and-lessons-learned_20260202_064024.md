# Safe and Certifiable AI Systems: Concepts, Challenges, and Lessons Learned

*Kajetan Schweighofer; Barbara Brune; Lukas Gruber; Simon Schmid; Alexander Aufreiter; Andreas Gruber; Thomas Doms; Sebastian Eder; Florian Mayer; Xaver-Paul Stadlbauer; Christoph Schwald; Werner Zellinger; Bernhard Nessler; Sepp Hochreiter*

---

### ⚡ Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Framework** | TÜV AUSTRIA Trusted AI |
| **Core Concept** | Functional Trustworthiness |
| **Assessment Pillars** | 3 (Secure Development, Functional Req, Ethics) |
| **Key Method** | Statistical testing on independently sampled data |
| **Active Since** | 2019 |

---

## Executive Summary

The implementation of the EU AI Act creates a critical gap between high-level legal obligations and the technical engineering processes required to demonstrate compliance. Organizations face significant challenges in translating abstract regulatory requirements into testable, verifiable criteria for AI systems, particularly regarding safety, robustness, and non-discrimination. Without a standardized certification framework, it is difficult to ensure that high-risk AI systems are lawful and reliable, creating liability risks and hindering market adoption. This paper addresses the urgent need for a mature, operationalizable audit methodology that bridges the divide between legal regulation and technical reality.

The authors present the **TÜV AUSTRIA Trusted AI framework**, an end-to-end audit catalog designed to operationalize the EU AI Act through the concept of "functional trustworthiness." The innovation rests on three pillars: Secure Software Development, Functional Requirements, and Ethics & Data Privacy. Technically, the framework defines the application domain statistically and couples it with risk-based Minimum Performance Requirements (MPRs). By employing statistical testing on independently sampled data, the system maps legal obligations directly to specific technical tests, ensuring that the validation process yields transparent, reproducible, and statistically significant evidence of model performance.

Rather than providing quantitative benchmarks, the paper reports qualitative findings from practical audits executed between 2019 and the present. The application of the framework successfully identified prevalent failure modes in current AI development practices, with frequent audit findings including data leakage, inadequate statistical domain definitions, neglected biases, and a lack of distribution drift controls. The methodology demonstrated its effectiveness by validating specific technical properties such as Out-Of-Distribution (OOD) detection, resilience against adversarial examples, and algorithmic fairness, proving that rigorous certification evidence can be systematically achieved through independent sampling and statistical testing.

This research provides a comprehensive blueprint for the certification of AI systems, offering a practical solution to one of the most pressing issues in AI governance: the enforcement of regulatory standards. By documenting lessons learned from real-world audits, the work offers valuable guidance for practitioners to avoid common pitfalls in data governance and model lifecycle management. Furthermore, the framework identifies critical future research directions, specifically regarding post-certification monitoring and the continuous assurance of algorithmic fairness.

---

## Key Findings

*   **Operationalization of Law:** The TÜV AUSTRIA Trusted AI framework successfully operationalizes the EU AI Act by translating abstract legal obligations into concrete, testable technical criteria.
*   **Core Concept:** The certification process relies on 'functional trustworthiness,' which couples statistically defined application domains with risk-based performance requirements.
*   **Common Pitfalls:** Audits frequently reveal repeated failures in AI development, including:
    *   Data leakage
    *   Inadequate domain definitions
    *   Neglected biases
    *   Lack of distribution drift controls
*   **Validation Standard:** Reliable certification evidence is achieved through statistical testing conducted on independently sampled data, ensuring transparency and reproducibility.

---

## Methodology

The authors propose an end-to-end audit catalog and methodology that is lifecycle-oriented and structured around three distinct pillars:

1.  **Secure Software Development**
2.  **Functional Requirements**
3.  **Ethics & Data Privacy**

**The Assessment Process:**

*   **Mapping Regulations:** The methodology maps high-level regulations down to specific, actionable tests.
*   **Validation:** It utilizes statistical testing on independently sampled data to validate that models perform within a statistically defined application domain.
*   **Goal:** To ensure transparent and reproducible evidence that aligns technical practices with European standards.

---

## Technical Details

The paper outlines the architecture and core definitions of the TÜV AUSTRIA Trusted AI framework.

**Framework Structure:**

*   **Objective:** Audit catalog designed to operationalize the EU AI Act.
*   **Three Pillars:**
    *   Secure Software Development
    *   Functional Requirements
    *   Ethics & Data Privacy

**Core Concepts:**

*   **Functional Trustworthiness:** Defined by three main components:
    1.  A **statistically defined application domain**.
    2.  **Risk-based Minimum Performance Requirements (MPRs)**.
    3.  **Statistical testing** on independently sampled data.
*   **Lifecycle Coverage:** The methodology translates legal obligations into testable technical criteria covering the entire AI system lifecycle.

**Technical Scope of Assessment:**

*   **Robustness:** Including Out-Of-Distribution (OOD) detection and adversarial examples.
*   **Algorithmic Fairness:** Testing for non-discrimination.
*   **Explainability:** Ensuring model decisions can be understood and audited.

---

## Results

The text reports qualitative findings derived from practical audits rather than quantitative benchmarks.

*   **Failure Modes:** The audits identified specific technical shortcomings in current systems, most notably:
    *   Data leakage between training and test sets.
    *   Inadequate statistical domain definitions leading to invalid performance claims.
    *   Neglected biases affecting model fairness.
    *   Lack of controls for distribution drift over time.
*   **Validation Outcomes:** The framework proved capable of validating complex technical properties (OOD detection, adversarial resilience) using transparent and reproducible evidence.

---

## Contributions

The research makes several significant contributions to the field of AI governance and safety:

*   **Mature Framework:** Provides a comprehensive, mature audit framework developed since 2019 to certify AI systems as safe and lawful.
*   **Bridging the Gap:** Successfully bridges the gap between regulation and engineering by aligning technical practices with European standards.
*   **Practical Guidance:** Documents practical lessons learned regarding common failures in AI development to help practitioners improve robustness and fairness.
*   **Future Research:** Identifies critical research gaps concerning **post-certification requirements** and the **ongoing assurance of algorithmic fairness**.

---

## Document Statistics

*   **Quality Score:** 8/10
*   **References:** 0 citations