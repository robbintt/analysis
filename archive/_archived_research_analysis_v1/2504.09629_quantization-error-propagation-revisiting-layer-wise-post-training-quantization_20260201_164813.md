# Quantization Error Propagation: Revisiting Layer-Wise Post-Training Quantization

*Yamato Arai; Yuma Ichikawa*

---

> ### üìä Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Citations:** 40
> *   **Core Problem:** Quantization error accumulation & saturation in layer-wise PTQ
> *   **Key Metric:** Significant Perplexity reduction in INT2 & INT3 regimes
> *   **Inference Cost:** Zero overhead (computation confined to calibration)
> *   **Compatibility:** Orthogonal to RTN, GPTQ, AWQ, and QuIP

---

## üìù Executive Summary

The primary bottleneck limiting the efficacy of layer-wise Post-Training Quantization (PTQ) for Large Language Models (LLMs) is the unaddressed accumulation and propagation of quantization errors across network depth. Current standard methods, such as RTN and GPTQ, treat each layer in isolation under the assumption of fixed inputs. This approach fails to account for how rounding errors compound and flow through subsequent layers, leading to severe performance saturation. This error accumulation is particularly detrimental in extreme low-bit regimes (e.g., INT2), where it destroys feature representation, making high-compression deployment unreliable without resorting to computationally expensive retraining.

To resolve this, the authors propose **Quantization Error Propagation (QEP)**, a lightweight framework designed to explicitly model and manage error flow rather than optimizing layers independently. The core technical innovation involves modifying the optimization objective to minimize the discrepancy ($\Delta_m$) between full-precision and partially quantized outputs, thereby actively compensating for errors propagated from previous layers. The mechanism remains lightweight by confining all computational overhead to the calibration phase and introducing a tunable "propagation strength" parameter; this parameter controls the extent of error correction to balance performance gains against memory and processing costs, preventing overfitting.

Crucially, QEP is orthogonal to existing pipelines (e.g., AWQ, QuIP) and imposes zero additional inference costs. In experiments on Llama-2 models (7B, 13B, and 70B) evaluated on WikiText-2, QEP consistently achieved lower perplexity than established baselines across INT4, INT3, and INT2 regimes. **In the INT3 setting for Llama-2-7B, QEP achieved a perplexity of 11.38, outperforming GPTQ (11.59) and significantly surpassing RTN (13.46).** The framework's efficacy was most pronounced in the extreme low-bit INT2 setting, where existing methods typically collapse; for the Llama-2-7B model, QEP reduced perplexity to 11.14, whereas RTN degraded to 15.88.

This research redefines the theoretical understanding of layer-wise PTQ by formally identifying error propagation as the critical failure mode in prior approaches. By establishing a new state-of-the-art for layer-wise quantization‚Äîspecifically enabling viable INT2 accuracy without retraining‚ÄîQEP significantly expands the boundaries of efficient model deployment.

---

## üîé Key Findings

*   **Identification of Bottleneck:** The primary limitation causing saturation in layer-wise Post-Training Quantization (PTQ) progress is the growth and accumulation of quantization errors across layers, which severely degrades performance.
*   **Superior Performance:** The proposed Quantization Error Propagation (QEP) framework enables layer-wise PTQ to achieve substantially higher accuracy than existing methods across multiple Large Language Models (LLMs).
*   **Efficacy in Low-Bit Regimes:** The performance improvements offered by QEP are most significant when applied to extremely low-bit quantization scenarios (e.g., INT2).
*   **Practical Flexibility:** The framework includes a tunable propagation mechanism that successfully prevents overfitting while maintaining control over computational overhead.

---

## üõ†Ô∏è Methodology

The authors propose **Quantization Error Propagation (QEP)**, a general, lightweight, and scalable framework designed to enhance layer-wise PTQ. The core methodology consists of three pillars:

1.  **Explicit Error Management:** Propagating quantization errors through the network instead of treating layers in isolation.
2.  **Error Compensation:** Actively compensating for accumulated errors.
3.  **Tunable Mechanism:** Utilizing propagation to adapt to architectures and balance overfitting prevention against computational costs.

---

## üìà Technical Details

| Component | Description |
| :--- | :--- |
| **Objective** | Modifies the optimization objective to explicitly compensate for errors propagated from previous layers. |
| **Discrepancy Metric** | Minimizes $\Delta_m$, the discrepancy between full-precision and partially quantized outputs. |
| **Standard Approach** | Treats layers independently assuming fixed inputs, leading to approximate exponential error accumulation. |
| **Propagation Strength** | Introduces a tunable 'propagation strength' parameter to prevent overfitting and manage overhead, specifically within MLP blocks. |
| **Orthogonality** | Distinguishes itself by managing error flow rather than solely relying on rotation, salience, or simple compensation. It is compatible with pipelines like RTN, GPTQ, AWQ, and QuIP. |

---

## üí° Contributions

*   **Theoretical Re-evaluation:** The work revisits the fundamental assumptions of layer-wise PTQ by formally identifying error propagation across layers as a critical failure mode in current approaches.
*   **Framework Innovation:** Introduces QEP as a novel solution that addresses the error accumulation problem without requiring retraining, maintaining the simplicity benefits of PTQ.
*   **Benchmark Advancement:** Establishes a new state-of-the-art performance level for layer-wise PTQ, particularly pushing the boundaries of what is possible in efficient, extremely low-bit quantization for LLMs.

---

## üìä Results

Experiments were conducted on **Llama-2 models** (7B, 13B, 70B) using the **WikiText-2** benchmark.

*   **Consistent Perplexity Reduction:** QEP consistently lowered Perplexity across INT4, INT3, and INT2 regimes compared to baselines (RTN, GPTQ, AWQ, QuIP).
*   **Inverse Proportionality:** Performance gains were inversely proportional to bit width, with the most significant improvements in the extreme low-bit INT2 regime.
*   **Model Scale Impact:** Improvements were more pronounced in smaller models (7B) than larger ones (70B).
*   **Error Analysis:** Analysis revealed that quantization errors ($\Delta_m$) grow within quantized blocks and continue to propagate through subsequent unquantized blocks, demonstrating that local errors globally degrade feature representation.

---