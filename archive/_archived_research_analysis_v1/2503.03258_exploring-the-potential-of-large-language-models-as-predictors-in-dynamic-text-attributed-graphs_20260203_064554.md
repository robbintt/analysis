---
title: Exploring the Potential of Large Language Models as Predictors in Dynamic Text-Attributed
  Graphs
arxiv_id: '2503.03258'
source_url: https://arxiv.org/abs/2503.03258
generated_at: '2026-02-03T06:45:54'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Exploring the Potential of Large Language Models as Predictors in Dynamic Text-Attributed Graphs

*Runlin Lei; Jiarui Ji; Haipeng Ding; Lu Yi; Zhewei Wei; Yongchao Liu; Chuntao Hong*

---

> ### üìå Quick Facts
> *   **Framework:** GraphAgent-Dynamic (GAD)
> *   **Core Innovation:** First work exploring LLMs as predictors for dynamic text-attributed graphs using a multi-agent architecture.
> *   **Models Used:** DeepSeek-V3, GPT-4o-mini, Llama-3-8b
> *   **Benchmark:** DTGB (5 datasets including Enron, GDELT, ICEWS1819)
> *   **Performance:** Matches or exceeds fully-supervised GNNs without dataset-specific training.
> *   **Top Metrics:**
>     *   **Enron (LP):** 0.872 Accuracy
>     *   **Stack_elec (NR):** 0.889 Hits@10
>     *   **Googlemap_CT (EC):** 0.921 Weighted F1

---

## üìë Executive Summary

This research addresses the challenge of applying Large Language Models (LLMs) to Continuous-Time Dynamic Graphs (CTDGs) with text attributes, specifically focusing on the difficulties of context window limitations and high domain variability. Traditional Graph Neural Networks (GNNs) typically require extensive, dataset-specific supervised training to model the complex interplay between evolving graph topologies and node features. This limitation creates a need for more generalized solutions that can adapt to new domains without the cost of retraining. Addressing this is critical because it opens the door to transferable, zero-shot learning capabilities for dynamic graph analysis, reducing reliance on large-scale labeled datasets and enabling broader applicability across diverse domains.

The key innovation is the **GraphAgent-Dynamic (GAD)** framework, a collaborative multi-agent system designed to decouple prediction tasks into specialized roles rather than relying on a single monolithic LLM. The architecture comprises six distinct components: an Initial Agent for dataset description, a Local Summary Agent and a Global Summary Agent for analyzing node-level insights and topology respectively, a Temporary Predictor Agent for intermediate predictions, a Knowledge Reflection Agent that refines predictions, and a Database for maintaining consistency. To effectively solve context window constraints, the framework utilizes heuristic structural metrics‚ÄîHistorical Interaction (HI), Common Neighbors (CN), and Node Frequency (NF)‚Äîto compress extensive structural data into concise summaries. GAD leverages multiple LLM backbones, including DeepSeek-V3, GPT-4o-mini, and Llama-3-8b, to employ a zero-shot or few-shot strategy that maintains a unified architecture while transferring knowledge across domains without fine-tuning.

The study evaluated GAD on five datasets from the DTGB benchmark (Enron, GDELT, Googlemap_CT, ICEWS1819, Stack_elec) using a rigorous 70/15/15 chronological split involving 10,240 sampled test instances. The framework was tested on Future Link Prediction (LP), Node Retrieval (NR), and Future Edge Classification (EC) tasks. Results demonstrated that GAD achieves performance comparable to or exceeding fully-supervised GNN baselines without dataset-specific training. Specifically, on the Enron dataset, GAD achieved an LP Accuracy of **0.872**, while on GDELT it reached **0.744**. For Node Retrieval (Hits@10), the model scored **0.617** on ICEWS1819 and **0.889** on Stack_elec. In Edge Classification, GAD attained a Weighted F1 score of **0.921** on Googlemap_CT. The experiments further revealed that text-only prompts are insufficient for capturing temporal dynamics; the integration of structure-aware prompts with heuristic metrics was essential for achieving these high performance metrics.

This paper establishes a significant precedent as the first work to successfully explore LLMs as predictors for dynamic text-attributed graphs. It proves that LLM-based predictors can serve as potent alternatives to traditional GNNs, effectively mitigating the need for extensive training data and dataset-specific fine-tuning. By validating a multi-agent architecture that handles structural complexity and temporal dynamics, the authors provide a roadmap for future research, highlighting the importance of structural heuristics and offering design guidelines for developing more adaptable, generalizable graph learning systems.

---

## üöÄ Key Findings

*   **Pioneering Application:** This is the first work to explore LLMs as predictors specifically for dynamic text-attributed graphs.
*   **Performance Parity:** GraphAgent-Dynamic (GAD) achieves performance comparable to or exceeding fully-supervised GNNs without requiring dataset-specific training.
*   **Effective Multi-Agent Architecture:** The collaborative multi-agent system successfully mitigates context length constraints and manages high variability in domain characteristics.
*   **Unified Adaptability:** The framework maintains a unified architecture while successfully transferring knowledge across different domains.

---

## üõ† Methodology

The proposed methodology centers on the **GraphAgent-Dynamic (GAD) Framework**, a system designed to leverage collaborative LLMs rather than a single monolithic model. The approach emphasizes generalizability through Zero-Shot and Few-Shot strategies.

### Core Components
1.  **Initial Agent:** Extracts dataset descriptions.
2.  **Global and Local Summary Agents:** Generate domain-specific knowledge (node-level insights and graph topology) to enhance transferability.
3.  **Temporary Predictor Agent:** Generates intermediate predictions.
4.  **Knowledge Reflection Agents:** Enable adaptive updates to the knowledge base to ensure consistency and refine predictions.
5.  **Database:** Maintains the system's state and knowledge.

### Strategy
*   **Decoupled Prediction:** Breaking down complex graph tasks into specialized sub-tasks handled by specific agents.
*   **Generalizability Focus:** Performing predictive tasks without dataset-specific fine-tuning, relying on the inherent reasoning capabilities of LLMs.

---

## ‚öôÔ∏è Technical Details

### Architecture Definition
The paper formally defines the graph as a DyTAG (Dynamic Text-Attributed Graph) with timestamped edges and textual descriptions. The GAD architecture consists of six interacting components to handle the complexity of Continuous-Time Dynamic Graphs (CTDGs).

### Handling Context Constraints
To manage the limitations of LLM context windows, the system employs heuristic structural metrics to compress data:
*   **Historical Interaction (HI)**
*   **Common Neighbors (CN)**
*   **Node Frequency (NF)**

### Experimental Setup
*   **LLM Backbones:** DeepSeek-V3, GPT-4o-mini, Llama-3-8b.
*   **Baselines:** TCL, GraphMixer, DyGFormer.
*   **Input Modes Tested:** Text-only, Structure-aware, Few-shot.

---

## üìä Results

Experiments utilized 5 datasets from the DTGB benchmark involving Future Link Prediction (LP), Node Retrieval (NR), and Future Edge Classification (EC). The evaluation used a 70/15/15 chronological split with 10,240 sampled test instances.

### Performance Highlights

**Future Link Prediction (Accuracy)**
*   **Enron:** 0.872
*   **GDELT:** 0.744

**Node Retrieval (Hits@10)**
*   **ICEWS1819:** 0.617
*   **Stack_elec:** 0.889

**Future Edge Classification (Weighted F1)**
*   **Googlemap_CT:** 0.921

### Critical Insight
The study concludes that **text-only prompts are insufficient** for capturing temporal dynamics. Structure-aware prompts utilizing heuristic metrics are necessary to achieve high performance.

---

## üèÜ Contributions

*   **Problem Identification:** Addresses specific challenges of applying LLMs to dynamic graphs, such as context window limitations and domain variability.
*   **Architectural Innovation:** Introduces the novel multi-agent GAD architecture that decouples prediction into specialized roles.
*   **Benchmarking Efficiency:** Demonstrates LLM-based predictors as potent alternatives to GNNs, reducing the need for extensive training data.
*   **Future Design Guidelines:** Provides insights into future design, including dataset-specific fine-tuning strategies.

---

**Quality Score:** 8/10  
**References:** 40 citations