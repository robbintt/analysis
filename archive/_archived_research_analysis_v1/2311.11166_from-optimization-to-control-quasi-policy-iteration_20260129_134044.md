# From Optimization to Control: Quasi Policy Iteration
*Mohammad Amin Sharifi Kolarijani; Peyman Mohajerin Esfahani*

---

> ### ðŸ“Š Quick Facts
> *   **Quality Score**: 9/10
> *   **References**: 40 Citations
> *   **Algorithm**: Quasi-Policy Iteration (QPI)
> *   **Complexity**: $O(nm^2)$ per iteration
> *   **Key Advantage**: Second-order convergence without increased computational cost.

---

## Executive Summary

This paper addresses the fundamental challenge of accelerating convergence in Markov Decision Processes (MDPs) without incurring the prohibitive computational costs typically associated with second-order optimization methods. While standard control algorithms like Value Iteration are widely used, they are effectively first-order methods and often suffer from slow convergence rates compared to Newton-like methods in convex optimization. Bridging this gap is critical for solving large-scale or complex control problems efficiently, where computational resources and iteration counts are major constraints.

The authors introduce **Quasi-Policy Iteration (QPI)**, a novel control algorithm that translates the Quasi-Newton Method (QNM) from convex optimization into the policy iteration framework. The core technical breakthrough lies in a new technique for approximating the Hessian matrix, which is made possible by exploiting two linear structural constraints intrinsic to MDPs. This results in an update rule that combines a standard Temporal Difference (TD) term with a Quasi-Newton correction:

$$q_{k+1} = q_k + \alpha_k(\bar{b}_k - q_k) + \alpha_k \beta_k \Pi_M(p_k)$$

Where $\Pi_M(p)$ is a projection operator ensuring stability. This architecture allows the algorithm to utilize second-order information and incorporate prior knowledge of transition probability kernels, supporting asynchronous implementation via an approximate transition matrix $\hat{P}_k$.

Despite its advanced mathematical foundation, QPI maintains a computational complexity of **$O(nm^2)$** per iteration, which is identical to that of standard synchronous Q-learning. The authors provide rigorous convergence guarantees (Theorem 3.5), proving that iterates converge to the optimal Q-function $q^*$ almost surely, provided standard stochastic approximation conditions are met. Empirical analysis further indicates that QPI exhibits convergence behavior comparable to classical quasi-Newton methods and demonstrates low sensitivity to the discount factor $\gamma$.

This research is significant as it establishes a concrete theoretical bridge between classical optimization algorithms and modern control policy iteration. By demonstrating that it is possible to achieve the superior convergence properties of Newton-like methods without increasing computational complexity, QPI offers a highly efficient alternative to standard Value Iteration.

---

## Key Findings

*   **Novel Algorithm**: Introduction of Quasi-Policy Iteration (QPI), created by adapting the quasi-Newton method (QNM) from convex optimization to Markov decision processes (MDPs).
*   **Computational Efficiency**: Despite its advanced foundation, QPI maintains the same computational complexity ($O(nm^2)$) as standard value iteration.
*   **Convergence**: Empirical analysis reveals that QPI exhibits convergence behavior comparable to the quasi-Newton method.
*   **Robustness**: The algorithm demonstrates low sensitivity to the discount factor, suggesting robustness across varying problem specifications.

---

## Methodology

The authors bridge the gap between optimization theory and control by analogizing control algorithms for MDPs to optimization methods. Specifically, they apply the Quasi-Newton method (QNM) to the policy iteration framework.

**Core Approach:**
*   **Hessian Approximation**: The approach involves a novel approximation of the Hessian matrix.
*   **Structural Constraints**: This approximation is mathematically enabled by exploiting two linear structural constraints inherent to MDPs.
*   **Prior Information**: These constraints permit the incorporation of prior information regarding the transition probability kernel into the solution process.

---

## Contributions

*   **Algorithmic Innovation**: The proposal of Quasi-Policy Iteration (QPI), establishing a concrete link between the quasi-Newton method in optimization and policy iteration in control.
*   **Hessian Approximation**: The development of a new technique to approximate the Hessian matrix within policy iteration, specifically leveraging MDP structural constraints.
*   **Efficiency and Robustness**: Demonstrating that it is possible to achieve the superior convergence properties of Newton-like methods without increasing computational complexity beyond standard value iteration.
*   **Incorporation of Priors**: Providing a mechanism to integrate prior knowledge of the transition probability kernel into the control algorithm.

---

## Technical Details

**Algorithm Name:** Quasi-Policy Learning (QPL)

**Core Components:**
*   **Projection Operator**: Utilizes $\Pi_M(p)$ with bound $M = \frac{2\gamma}{(1-\gamma)^2} \|c\|_\infty$ to clip update vectors and ensure stability.
*   **Update Rule**:
    $$q_{k+1} = q_k + \alpha_k(\bar{b}_k - q_k) + \alpha_k \beta_k \Pi_M(p_k)$$
    *   Combines a Temporal Difference term and a Quasi-Newton correction.
*   **Implementation**: Supports asynchronous implementation where all Q-function entries are updated per iteration based on single samples.
*   **Approximation**: Uses a uniform prior and adaptive coefficients ($\delta_k, \lambda_k, \eta_k$) to simplify calculations via an approximate transition matrix $\hat{P}_k$.

---

## Results

*   **Complexity**: The algorithm has a computational complexity of $O(nm^2)$ per iteration, identical to synchronous Q-learning.
*   **Convergence Guarantees**:
    *   **Theorem 3.5**: Iterates converge to the optimal $q^*$ almost surely.
    *   **Conditions**: Standard stochastic approximation conditions must be met ($\sum \alpha_k = \infty, \sum \alpha_k^2 < \infty, \beta_k \to 0$).
*   **Empirical Performance**:
    *   Low sensitivity to the discount factor ($\gamma$).
    *   Convergence behavior comparable to classic quasi-Newton methods.