---
title: 'When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in
  Large Audio-Language Models'
arxiv_id: '2510.00626'
source_url: https://arxiv.org/abs/2510.00626
generated_at: '2026-02-06T04:03:53'
quality_score: 8
citation_count: 0
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# When Silence Matters: The Impact of Irrelevant Audio on Text Reasoning in Large Audio-Language Models

*Chen-An Li; Tzu-Han Lin; Hung-yi Lee*

---

> ### **Quick Facts**
>
> *   **Topic:** Cross-Modal Interference in Large Audio-Language Models (LALMs)
> *   **Models Evaluated:** 6 (including Qwen2.5-Omni, Voxtral, Phi-4-multimodal, DeSTA2.5-Audio)
> *   **Benchmarks:** GSM8K, ARC-Challenge, MMLU
> *   **Core Metric:** Influence Rate (IR) - measures prediction flips due to interference
> *   **Critical Insight:** Silence is not a neutral input; it causes degradation comparable to synthetic noise.
> *   **Quality Score:** 8/10

---

## Executive Summary

This paper addresses a critical robustness challenge in Large Audio-Language Models (LALMs): their inability to effectively isolate text reasoning from irrelevant audio inputs. While LALMs are designed to process multimodal data, this research reveals that the presence of unnecessary audio streams—including silence, background noise, or environmental sounds—severely degrades performance on text-only reasoning tasks. This issue matters because it exposes a fundamental vulnerability in cross-modal attention mechanisms; if models cannot disregard irrelevant modalities, their reliability in real-world, noisy environments is compromised, potentially leading to incorrect outputs in applications where audio quality varies.

The key innovation is the introduction of a systematic framework to probe "Cross-Modal Interference" by injecting irrelevant audio signals into standard text-only benchmarks. Technically, the authors evaluated six open-source models (including Qwen2.5-Omni and Voxtral) across three text-based datasets (GSM8K, ARC-Challenge, MMLU). They manipulated variables such as audio duration, amplitude, and decoding temperature to test model stability. Unlike standard robustness checks, this approach specifically treats silence as a distinct input type to test if it acts as a neutral placeholder or an active interference agent, providing a granular analysis of how audio characteristics disrupt text processing pipelines.

The study found that all evaluated models exhibited universal vulnerability to irrelevant audio. On the GSM8K benchmark, accuracy dropped significantly from the 0.8–0.9 range when subjected to interference. The researchers introduced an "Influence Rate" (IR) metric to quantify prediction flips, which rose from near 0.00 to 0.05–0.20+ depending on the interference type. Surprisingly, silence caused degradation comparable to synthetic noise, with performance worsening as audio duration and amplitude increased. While larger models (e.g., 24B parameters) showed slightly more resilience than smaller ones, none were immune. Although self-consistency decoding methods improved stability, they did so at a high computational cost, while standard prompting techniques proved entirely ineffective.

This research significantly influences the field by redefining "silence" as a specific source of output instability rather than a benign input, challenging current assumptions about modal alignment. By highlighting the failure of simple prompting and the high cost of self-consistency, the authors underscore the urgent need for more efficient audio-text fusion strategies in future architecture designs. These findings provide a roadmap for developing more robust multimodal systems, emphasizing that future models must incorporate better mechanisms for modality isolation to ensure consistent reasoning performance regardless of audio input relevance.

---

## Key Findings

*   **Significant Degradation:** Irrelevant audio inputs—including silence, synthetic noise, and environmental sounds—significantly degrade the performance of Large Audio-Language Models (LALMs) on text-only reasoning tasks.
*   **The Danger of Silence:** Silence destabilizes model outputs with a severity comparable to synthetic noise, challenging the standard assumption that it is a neutral input.
*   **Correlation Factors:** Interference and output instability are positively correlated with longer audio durations, higher amplitudes, and elevated decoding temperatures.
*   **Model Resilience & Mitigation:** While larger models show somewhat greater resilience, vulnerabilities persist. Standard prompting is ineffective, whereas self-consistency improves stability but incurs higher computational costs.

---

## Methodology

The researchers assessed model robustness across three text-based benchmarks where audio input is unnecessary. The study systematically introduced various types of irrelevant audio (silence, synthetic noise, environmental sounds) and manipulated variables such as audio duration, amplitude, and decoding temperature.

The approach included:
*   **Comparative Analysis:** Evaluating models of different scales.
*   **Mitigation Testing:** Assessing the efficacy of two strategies:
    *   Prompting techniques.
    *   Self-consistency methods.

---

## Technical Details

| Category | Details |
| :--- | :--- |
| **Objective** | To probe Cross-Modal Interference in LALMs by introducing irrelevant audio signals alongside text-only inputs. |
| **Models Tested** | **6 Open-Source Models:**<br>• Qwen2.5-Omni (3B, 7B)<br>• Voxtral (Mini-3B, Small-24B)<br>• Phi-4-multimodal<br>• DeSTA2.5-Audio |
| **Inference Config** | • **General:** Greedy decoding via vLLM backend.<br>• **Voxtral:** Nucleus sampling (temp=0.2, top-p=0.95).<br>• **DeSTA2.5-Audio:** Transformers package. |
| **Benchmarks** | GSM8K, ARC-Challenge, MMLU |
| **Interference Types** | • Silence<br>• Synthetic Noise (-40dBFS)<br>• Real-world Audio (FSD50K) |
| **Metrics** | • **Accuracy**<br>• **Influence Rate (IR):** Measures prediction flips (change in output prediction). |

---

## Results

Experiments utilized benchmarks GSM8K, ARC-Challenge, and MMLU with interference types including Silence, Synthetic Noise, and Real-world Audio.

*   **Universal Vulnerability:** All evaluated models showed consistent accuracy drops when subjected to interference.
*   **Silence vs. Noise:** Silence caused degradation comparable to noise, debunking the idea of silence as a "safe" or "neutral" input.
*   **Duration Impact:** Longer audio duration correlated with decreased accuracy and increased Influence Rate (IR).
*   **Performance Drop:** GSM8K accuracy dropped from the **0.8–0.9 range** to significantly lower values.
*   **Instability Metric:** The Influence Rate (IR) rose from near **0.00** to **0.05–0.20+**, indicating frequent prediction flips.

---

## Contributions

*   **Identified Cross-Modal Interference:** Demonstrated that LALMs struggle to isolate text reasoning from irrelevant audio streams, highlighting a critical robustness challenge.
*   **Re-evaluated Silence:** Established silence as a specific source of output instability rather than a benign input, challenging previous assumptions.
*   **Guidance for Architecture:** Provided guidance for architecture design by highlighting the failure of simple prompting and the computational cost of self-consistency, underscoring the need for more efficient audio-text fusion strategies.

---

**Quality Score:** 8/10
**References:** 0 citations