# A Simple Method to Enhance Pre-trained Language Models with Speech Tokens for Classification

*Nicolas Calbucura; Valentin Barriere*

> ### **Quick Facts Sidebar**
> | Metric | Detail |
> | :--- | :--- |
> | **Quality Score** | 8/10 |
> | **References** | 28 Citations |
> | **Key Performance** | State-of-the-Art (SOTA) on Argumentative Fallacy Detection & Classification |
> | **Efficiency Gain** | Audio vocabulary reduced to **< 1%** of original size |
> | **Core Innovation** | Lasso-based feature selection & Two-stage training pipeline |

---

## Executive Summary

This research addresses the computational bottleneck of integrating high-dimensional speech data into pre-trained Language Models (LLMs) for text classification, specifically targeting the challenge of sequence length disparity and the controversy surrounding the utility of audio in Argumentative Fallacy Detection (AFD). While LLMs excel at textual analysis, incorporating paralinguistic speech features often introduces inefficiencies due to extensive sequence lengths, and previous studies have debated whether audio information introduces noise that hinders logical classification tasks. The paper investigates whether discrete speech representations can be effectively fused with text to improve performance on detection and classification tasks without requiring prohibitive computational resources.

The authors propose a low-complexity multimodal fusion technique centered on a rigorous dimensionality reduction pipeline and a distinct two-stage training mechanism. The method utilizes a 'SpeechTokenizer' (RVQ-based) to convert audio into discrete tokens. To solve the sequence length issue, the authors employ a Lasso-based (L1) feature selection on a multimodal Bag-of-Words representation. The training process involves adapting the frozen LLM to selected speech tokens and fine-tuning on the downstream task using LoRA adapters.

The approach achieved State-of-the-Art (SOTA) results, outperforming unimodal text-only models, larger SpeechLM models, and models utilizing continuous audio representations. Ablation studies highlighted the method's robustness, revealing that introducing even a random selection of audio tokens boosts performance over text-only baselines. This work significantly impacts the field by providing empirical evidence that paralinguistic information complements textual analysis, challenging the prior assumption that audio features are detrimental to logical classification tasks.

---

## Key Findings

*   **Superior Performance:** The proposed method outperforms unimodal models, larger SpeechLM models, and models integrating audio through learned representations.
*   **State-of-the-Art Results:** The approach achieved **SOTA results** on two distinct Argumentative Fallacy Detection and Classification tasks, challenging the previous belief that audio features are counterproductive for these specific tasks.
*   **Robustness to Token Selection:** An in-depth analysis revealed that introducing even a **random selection** of audio tokens enhances the performance of the unimodal text model, though the lasso-based feature selection yields the best results.
*   **Efficiency in Sequence Handling:** The method effectively addresses the technical challenge of fusing long audio token sequences with relatively shorter text sequences without requiring high integration costs.

---

## Technical Details

The method uses a three-step pipeline to integrate discrete speech representations into an LLM.

*   **Tokenization & Representation**
    *   Utilizes **'SpeechTokenizer'** (RVQ-based).
    *   **Layer 1:** Captures semantic content.
    *   **Layers 2â€“8:** Capture paralinguistic content.
    *   **Sampling:** 20ms intervals, resulting in 1024 tokens/layer.

*   **Feature Selection Strategy**
    *   Addresses sampling inconsistency via **L1-regularized logistic regression (Lasso)**.
    *   Applied to Bag-of-Words features to select relevant audio tokens.
    *   **Reduction:** Successfully reduces the vocabulary to **less than 1%** of the initial size.

*   **Training Pipeline**
    1.  **Audio Embedding Pre-training:** Uses Causal Language Modeling (CLM) on **frozen LLM weights**.
    2.  **Downstream Fine-tuning:** Utilizes **LoRA adapters** and a linear classification head.

---

## Methodology

The research follows a structured four-part methodology designed to efficiently fuse audio and text modalities:

1.  **Tokenization**
    The system utilizes an existing speech tokenizer to convert audio inputs into long sequences of tokens from a large vocabulary.

2.  **Feature Selection**
    It applies a **Lasso-based (L1) feature selection** on a multimodal Bag-of-Words representation. This filters and retains only the most critical audio tokens, drastically reducing dimensionality.

3.  **Representation**
    The process relies on a multimodal Bag-of-Words representation to facilitate the feature selection process and manage the disparity between sequence lengths.

4.  **Two-Stage Training**
    *   *Stage 1:* Adaptation of the pre-trained language model to selected speech tokens using a self-supervised language modeling objective.
    *   *Stage 2:* Fine-tuning on the specific downstream classification task.

---

## Results

The proposed approach demonstrated significant efficacy in handling multimodal data for classification tasks:

*   **Performance:** Achieved State-of-the-Art (SOTA) on Argumentative Fallacy Detection (AFD) and Classification (AFC).
*   **Comparison:** Outperformed unimodal text-only models, larger SpeechLM models, and models using continuous representations.
*   **Vocabulary Reduction:** Successfully reduced the audio vocabulary to less than 1% of the original size.
*   **Sequence Fusion:** Effectively fused long audio sequences with short text sequences.
*   **Ablation Studies:** Indicated high robustness; even random token selection improved upon text-only baselines, though Lasso-based selection was optimal.

---

## Contributions

*   **A Simple Multimodal Fusion Technique:** Introduced a low-complexity method to enhance textual pre-trained language models with speech information, solving the issue of audio sequence length disparity through feature selection.
*   **Advancement in Fallacy Detection:** Provided evidence that speech features are highly effective in Argumentative Fallacy Detection and Classification, setting new benchmarks.
*   **Analytical Insights:** Offered theoretical and empirical analysis demonstrating that the presence of audio context generally improves text-only models.
*   **Open-Source Reproducibility:** Released the code publicly to allow for reproduction and further extension of the work.

---

**Quality Score:** 8/10
**References:** 28 citations