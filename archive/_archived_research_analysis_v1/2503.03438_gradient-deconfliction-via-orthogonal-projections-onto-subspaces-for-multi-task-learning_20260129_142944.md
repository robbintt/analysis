***

# Gradient Deconfliction via Orthogonal Projections onto Subspaces For Multi-task Learning

*Shijie Zhu; Hui Zhao; Tianshu Wu; Pengjie Wang; Hongbo Deng; Jian Xu; Bo Zheng*

---

## üìë Quick Facts

| **Metric** | **Details** |
| :--- | :--- |
| **Proposed Method** | **GradOPS** (Gradient Deconfliction via Orthogonal Projections onto Subspaces) |
| **Primary Objective** | Resolve gradient conflicts in Multi-Task Learning (MTL) |
| **Key Mechanism** | Gram-Schmidt orthogonal projection to ensure non-conflicting updates |
| **Benchmarks** | UCI Census-income, NYUv2 |
| **Performance Highlight** | Achieves $\Delta_m\%$ of -0.23% on NYUv2 (outperforming single-task) |
| **Quality Score** | **9/10** |

---

## üìù Executive Summary

> **Core Problem & Solution:** Multi-task learning (MTL) frequently suffers from performance degradation compared to single-task models due to "gradient conflict"‚Äîwhere optimizing for one task negatively interferes with another. This research introduces **GradOPS**, a geometric approach that mathematically guarantees non-conflicting gradient directions using orthogonal projections.

**Overview:**
The paper identifies gradient conflict as the primary instability in MTL. The authors propose that achieving non-conflicting gradients is essential for stable optimization and effective trade-offs. GradOPS addresses this by projecting task gradients onto subspaces orthogonal to conflicting tasks using a Gram-Schmidt procedure. This ensures modified gradients satisfy the condition $g'_i \cdot g_j \ge 0$, preventing negative interference.

The algorithm features two stages: **Deconfliction** (removing conflicting components) and **Trade-off Adjustment** (modulating weights based on scalar projections via a hyperparameter $\alpha$). This allows the model to navigate diverse Pareto-optimal solutions.

**Impact:**
GradOPS demonstrates state-of-the-art performance, consistently outperforming single-task baselines and existing MTL algorithms. It offers theoretical rigor regarding convergence properties and practical flexibility, allowing users to mimic other algorithms' behaviors by tuning $\alpha$. This work advances MTL from heuristic trade-off coefficients to a mathematically grounded framework for deconflicting task updates.

---

## üîë Key Findings

*   **Primary Cause of MTL Failure:** The performance gap between multi-task learning (MTL) and single-task models is primarily attributed to **conflicting gradients** among tasks.
*   **Value of Non-Conflicting Gradients:** Achieving non-conflicting gradients is essential for enabling simple yet effective trade-off strategies between tasks and ensuring stable overall performance.
*   **Conflict Resolution:** The GradOPS method is capable of **completely solving conflicts** among tasks by mathematically removing components of the gradient that interfere with other tasks.
*   **Solution Diversity:** The approach allows for the discovery of **diverse solutions**, effectively navigating different trade-off preferences to satisfy various optimization objectives.
*   **State-of-the-Art Performance:** Empirical results on multiple benchmarks across various domains demonstrate that GradOPS achieves **SOTA performance** while offering flexibility in trade-off strategies.

---

## üß† Methodology

The proposed method, **GradOPS** (Gradient Deconfliction via Orthogonal Projections onto Subspaces), operates by manipulating the gradient descent direction.

1.  **Orthogonal Projection:** The method utilizes orthogonal projection to project gradients onto subspaces that are orthogonal to those spanned by other task-specific gradients.
2.  **Null Space Updates:** By ensuring the update for one task lies within the **null space** of the other tasks' gradients, the method guarantees that updates do not negatively impact the optimization of conflicting tasks.
3.  **Solution Space Exploration:** This mechanism allows the algorithm to explore the solution space effectively to find specific paths that align with desired trade-off preferences.

---

## ‚öôÔ∏è Technical Details

GradOPS addresses Multi-task Learning (MTL) gradient conflicts by guaranteeing **strong non-conflicting gradients** ($g'_i \cdot g_j \ge 0$ for all $i, j$).

### Algorithm Stages

**1. Deconfliction via Orthogonal Projections**
A conflicting gradient $g_i$ is projected onto the subspace orthogonal to the span of other task gradients ($S$) using the Gram-Schmidt procedure.
*   **Formula:**
    $$g'_i = g_i - \sum_{j \neq i} \text{proj}_{u_j}(g_i)$$

**2. Trade-off Adjustment (Reweighting)**
This stage calculates scalar projections ($R_i$) and relative magnitudes ($r_i$) to determine task dominance.
*   **Weight Control:** A hyperparameter $\alpha$ controls the final weights:
    $$w_i = \frac{r_i^\alpha}{\sum r_i^\alpha}$$
*   **Final Update Direction:**
    $$G'_{\text{new}} = \sum w_i g'_i$$

---

## üìä Results

Experiments compared GradOPS against baselines including **Single-task**, **Uniform Scaling**, **MGDA-UB**, **CAGrad**, and **IMTL-G**.

### UCI Census-income Dataset
*   **GradOPS ($\alpha=-3$):**
    *   Average Score: **0.9396**
    *   Minimizing Regret (MR): **1.33%**
*   **Single-task Baseline:**
    *   Average Score: 0.9382
    *   Minimizing Regret (MR): 4.33%
*   **Result:** GradOPS achieved the highest average score and the lowest MR.

### NYUv2 Dataset
*   **GradOPS ($\alpha=-1$):**
    *   **$\Delta_m\%$: -0.23%** (Surpassing single-task baselines)
    *   **Segmentation mIoU:** 28.45
    *   **Depth Abs Err:** 0.5895
    *   **Surface Normal Mean:** 30.46

### Flexibility Analysis
The method demonstrated the ability to mimic other algorithms' behaviors simply by tuning the $\alpha$ hyperparameter, indicating high adaptability.

---

## üåü Contributions

*   **Theoretical Framework on Gradient Conflicts:** Provides a comprehensive examination of how conflicting gradients influence MTL dynamics, establishing the theoretical importance of non-conflicting gradients for stability and performance.
*   **Novel Algorithm (GradOPS):** Introduces a geometric approach to multi-task optimization that utilizes orthogonal projections to mathematically deconflict task gradients.
*   **Flexibility in Trade-offs:** Advances the field by moving beyond static trade-offs; the method actively searches for diverse, high-performing solutions that cater to different preferences regarding task prioritization.
*   **Rigorous Validation:** Contributes a theoretical analysis of the algorithm's convergence properties and validates the efficacy of the approach through extensive testing on multiple benchmarks and domains.

---

**Paper Statistics:** 40 Citations | **Quality Score:** 9/10