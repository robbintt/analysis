---
title: 'Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition
  of LLM agents'
arxiv_id: '2509.07389'
source_url: https://arxiv.org/abs/2509.07389
generated_at: '2026-01-28T01:10:20'
quality_score: 8
citation_count: 14
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Talking with Oompa Loompas: A novel framework for evaluating linguistic acquisition of LLM agents

*Dhruv Kumar, Jagat Sesh, Equal Contributions, Oompa Loompas, Sankalp Tattwadarshi, Anshika Krishnatray*

---

### Quick Facts
> **Language:** Tinkatongue (Synthetic)  
> **Constraint:** Bisyllabic words, 3-word sentences, Adjacency Rule  
> **Feedback Mechanism:** 'koro' (valid) / 'moko lira bani' (reset)  
> **Max Turns:** 100 per trial  
> **Top Performer:** Claude-3.5-haiku (TVR ~0.337)  
> **Key Limitation:** None of the models established functional conversation.

---

## Executive Summary

Current Large Language Models (LLMs) exhibit strong static linguistic competence in pre-trained languages but possess a fundamental inability to acquire new languages dynamically through interactive feedback. This limitation represents a critical blind spot in existing evaluation benchmarks, which predominantly focus on vocabulary, syntax, and morphology rather than the capacity to learn a communication system from scratch. 

As AI systems increasingly operate in dynamic environments, the discrepancy between memorized knowledge and the ability to infer linguistic rules through conversational trial-and-error highlights a significant gap in assessing true generalizability. To address this, the authors introduce **"Talking with Oompa Loompas,"** a novel experimental framework designed to evaluate language acquisition under strict linguistic isolation constraints.

The task pits an LLM agent against a deterministic bot that understands exclusively a synthetic language called "Tinkatongue." The LLM starts with zero prior knowledge and must deduce the language’s rigid grammatical structure:
1.  Every word is bisyllabic.
2.  Sentences contain exactly three words.
3.  Consecutive sentences must share a common term (the **Adjacency Rule**).

The interaction operates via a deterministic state machine where the bot provides specific feedback—valid responses are acknowledged with **'koro'** while invalid syntax triggers **'moko lira bani'**, forcing the agent to rely entirely on interactive pattern recognition to progress.

Evaluation across 10 trials involving GPT-4o-mini, Gemini-2.5-flash, and Claude-3.5-haiku revealed that while all agents failed to establish functional conversation within 100 turns, they displayed varied degrees of responsiveness. Claude-3.5-haiku achieved the highest Turn Validity Rate (TVR) at approximately 0.337, whereas GPT-4o-mini and Gemini-2.5-flash struggled significantly. 

Notably, all models demonstrated high Feedback Responsiveness, and qualitative analysis indicated that agents adopted distinct strategies—such as babbling and imitation—that mirror human-like learning behaviors for overcoming linguistic uncertainty, rather than strictly inferring underlying grammatical rules.

---

## Key Findings

*   **Failure to Establish Functionality:** LLM agents failed to establish a functional conversation within the 100-turn limit when required to use a new language from scratch.
*   **Human-Mirroring Strategies:** Despite the failure, agents adopted distinct strategies such as **babbling** and **imitation** that mirror human approaches to language learning.
*   **Inability to Acquire via Patterns:** Current LLMs lack the ability to effectively acquire language through pattern recognition and interactive feedback in a dynamic setting.
*   **Shortcomings of Previous Metrics:** Previous evaluation metrics focusing on vocabulary, syntax, and morphology fail to capture the specific challenges of interactive language learning.

---

## Methodology

The researchers developed a novel experimental framework designed to test language acquisition in an interactive setting. The study utilized a newly constructed language, termed **'Tinkatongue'**, which serves as the target language for acquisition.

The evaluation setup involved pitting an LLM agent against a bot (the "Oompa Loompa") that understands *only* Tinkatongue. This setup forced the agent to learn and use the language solely through dialogue to communicate effectively.

---

## Technical Details

**The Setup**
The task involves an interaction between a target LLM agent and a deterministic bot named **'Oompa Loompa'**. The communication occurs entirely in a synthetic language called **Tinkatongue**.

**Linguistic Constraints**
The LLM begins with zero prior knowledge of this language, which is defined by strict syntactic constraints:
*   **Bisyllabic Words:** Every word must consist of two syllables.
*   **Fixed Length:** Every sentence consists of exactly three words.
*   **Adjacency Rule:** Consecutive sentences must share a common word.

**State Machine & Feedback**
The language is finite, containing 25 predefined conversations. The interaction loop follows a deterministic state machine:
*   **Valid Response:** Receives feedback **'koro'** plus the next sentence.
*   **Invalid Response:** Triggers **'moko lira bani'**, immediately resetting the conversation state.

---

## Results

The study evaluated 10 trials across three major models: **GPT-4o-mini**, **Gemini-2.5-flash**, and **Claude-3.5-haiku**.

### Performance Metrics

| Model | Turn Validity Rate (TVR) | Performance Note |
| :--- | :--- | :--- |
| **Claude-3.5-haiku** | **~0.337** | Fastest adaptation (often by turn 2) |
| **Gemini-2.5-flash** | 0.061 | Poor performance |
| **GPT-4o-mini** | 0.012 | Lowest validity |

### Key Observations
*   **Feedback Responsiveness (FR):** All models demonstrated high responsiveness (scores of 1.0 in most trials), indicating the ability to recover from negative feedback.
*   **Strategic Behavior:** Qualitative analysis showed agents used strategies like babbling and imitation rather than successfully inferring the complex grammatical rules (specifically the Adjacency Rule).

---

## Contributions

This research makes three primary contributions to the field:

1.  **Gap Identification:** Identified a critical gap in existing evaluation benchmarks, which previously overlooked interactive feedback and pattern recognition as mechanisms for language acquisition.
2.  **New Framework:** Introduced a new evaluation framework that tests **dynamic learning capabilities** rather than static linguistic competence.
3.  **Pathways for Future Design:** Established pathways for designing future model architectures capable of learning more effectively from interactive feedback.

---
*Document Quality Score: 8/10* | *References: 14 citations*