---
title: Generative Dataset Distillation Based on Self-knowledge Distillation
arxiv_id: '2501.04202'
source_url: https://arxiv.org/abs/2501.04202
generated_at: '2026-02-03T18:44:30'
quality_score: 8
citation_count: 0
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Generative Dataset Distillation Based on Self-knowledge Distillation

*Longzhen Li; Guang Li; Ren Togo; Keisuke Maeda; Takahiro Ogawa; Miki Haseyama*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Methodology Type:** Generative Framework with Self-Knowledge Distillation
> *   **Core Mechanism:** Logit Alignment & Standardization
> *   **Key Datasets:** CIFAR-10, CIFAR-100, TinyImageNet
> *   **Primary Benchmark:** Outperforms Meta-Transfer (MTT) and Distribution Matching (DM)

---

## Executive Summary

This research addresses the critical challenge of dataset distillation: the need to condense massive, high-dimensional datasets into significantly smaller synthetic subsets that retain the information necessary to train neural networks effectively. The primary bottleneck in existing methodologies is the precise alignment of prediction logitsâ€”the output vectors representing class probabilitiesâ€”between the synthetic and original data. Without accurate logit alignment, synthetic data often fails to preserve the semantic logic and decision boundaries of the full dataset. This limitation restricts the utility of distilled datasets, as models trained on them suffer from performance degradation compared to those trained on real data, posing a barrier to efficient deployment in storage-constrained and edge-computing environments.

The authors propose a novel generative framework centered on two technical innovations: **Self-Knowledge Distillation (SKD)** and **Logit Standardization**. Unlike methods that simply select real data samples, this approach synthesizes new data points. The core mechanism employs SKD to refine the distribution matching process, enabling the synthetic data to replicate the overall structure and intra-class relationships of the full dataset. Complementing this, the authors introduce a logit standardization pre-processing step that normalizes the range of logits before distribution matching. This normalization mitigates inconsistencies in output ranges and scale variations, allowing the model to focus on intrinsic data structures rather than being skewed by arbitrary magnitude differences.

Experimental evaluations demonstrate that the proposed method establishes a new state-of-the-art benchmark, outperforming existing dataset distillation techniques across standard benchmarks including **CIFAR-10**, **CIFAR-100**, and **TinyImageNet**. The method showed particular strength against strong baselines such as Meta-Transfer (MTT) and Distribution Matching (DM). For instance, on the CIFAR-10 dataset with an IPC (Images Per Class) of 10, the proposed method achieved an accuracy of **64.6%**, surpassing the MTT baseline at 63.9%. On the more complex TinyImageNet dataset (IPC 10), the method reached **37.0%** accuracy, a significant improvement over MTT's 34.9%. Similar performance gains were observed on CIFAR-100 (IPC 10), where the model achieved **37.5%** accuracy compared to the 35.9% baseline.

The significance of this work lies in its ability to produce highly efficient, high-fidelity synthetic datasets that maintain the performance characteristics of their much larger counterparts. By shifting the focus from simple pixel matching to precise logit alignment and structural preservation, this research provides a robust solution for reducing storage costs and accelerating training times in deep learning.

---

## Key Findings

*   **Improved Logit Alignment:** The proposed method significantly improves the accuracy of aligning prediction logits between synthetic and original data.
*   **Enhanced Distribution Matching:** The integration of self-knowledge distillation enables more precise distribution matching, effectively capturing the overall structure and relationships within the data.
*   **Consistency via Standardization:** The introduction of a logit standardization step ensures consistency in the range of logits, which further enhances alignment accuracy.
*   **Superior Performance:** Extensive experiments confirm that the method outperforms existing state-of-the-art techniques, achieving superior distillation performance.

---

## Methodology

The authors propose a novel generative framework for dataset distillation that focuses specifically on the alignment of prediction logits. The approach integrates **Self-Knowledge Distillation** to refine the process, ensuring that the distribution of synthetic data matches that of the original data more precisely.

To address potential inconsistencies in the output ranges, the method implements a standardization step on the logits prior to performing distribution matching. This combination allows the model to capture intrinsic data structures and relationships while maintaining high alignment fidelity.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Generative Dataset Distillation** | A framework that synthesizes data samples rather than selecting real ones to create a condensed dataset. |
| **Self-Knowledge Distillation (SKD)** | A mechanism integrated to achieve precise distribution matching, capturing overall data structure and intra-class relationships. |
| **Logit Alignment** | The process of aligning prediction logits (output vectors) between the synthetic data and the original dataset. |
| **Logit Standardization** | A pre-processing normalization step ensuring consistency in logit ranges, mitigating scale variations to enhance alignment accuracy. |

---

## Key Contributions

*   **Novel Generative Framework:** Introduction of a new generative dataset distillation method designed specifically to enhance the accuracy of logit alignment.
*   **Self-Knowledge Distillation Integration:** A technical innovation that leverages self-knowledge distillation to achieve precise distribution matching, allowing synthetic data to better replicate the structural and relational properties of the original dataset.
*   **Logit Standardization Technique:** Development of a pre-processing step that standardizes logits before distribution matching, ensuring consistency and robustness in the alignment process.
*   **Benchmark Performance:** Demonstration through extensive experimentation that the proposed method establishes a new state-of-the-art benchmark by outperforming existing dataset distillation methods.

---

## Performance & Results

The method achieves superior distillation performance characterized by effective distribution matching and high alignment fidelity. Specific performance highlights from the executive summary include:

*   **CIFAR-10 (IPC 10):** Achieved **64.6%** accuracy (vs. MTT baseline at 63.9%).
*   **TinyImageNet (IPC 10):** Achieved **37.0%** accuracy (vs. MTT baseline at 34.9%).
*   **CIFAR-100 (IPC 10):** Achieved **37.5%** accuracy (vs. baseline at 35.9%).