# Analysis of On-policy Policy Gradient Methods under the Distribution Mismatch

*Weizhen Wang; Jianping He; Xiaoming Duan*

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Core Concept** | Distribution Mismatch & Biased SGD |
| **Key Parameters** | Diameter ($D$), Mixing Parameter ($\beta$), $c_{min}$ |
| **Methodology** | Theoretical Analysis (Tabular & General Parameterizations) |

## Executive Summary

State-of-the-art (SOTA) policy gradient algorithms widely used in reinforcement learning often suffer from a theoretical inconsistency known as **"distribution mismatch."** Specifically, while the policy gradient theorem assumes updates based on a discounted state visitation distribution ($d_{\pi,\gamma}$), practical implementations frequently utilize a stationary or on-policy distribution ($d_{\pi}$). This discrepancy creates a gap between theoretical guarantees and actual algorithm performance. The paper addresses this critical issue to understand why these methods continue to perform well in practice despite deviating from the strict assumptions required by the policy gradient theorem.

The authors propose a two-step theoretical framework to analyze this mismatch, initially verifying global optimality for tabular parameterizations before extending the analysis to general cases using the theory of **Biased Stochastic Gradient Descent (Biased SGD)**. Technically, the innovation centers on a "Biased ABC" assumption that characterizes convergence conditions in the presence of biased gradients. The authors quantify the distribution mismatch by bounding the ratio of the discounted visitation distribution to the stationary distribution. This quantification relies on ergodicity metrics, specifically the Diameter ($D$), mixing parameter ($\beta$), and minimum state visitation probabilities ($c_{min}$), allowing the framework to rigorously analyze gradient correlation.

The study derives distinct theoretical bounds that ensure convergence under distribution mismatch rather than relying on empirical data. A primary result is the bound on the distribution ratio. Additionally, the paper defines a lower bound coefficient for the Biased ABC condition. The authors prove a central inequality confirming convergence, demonstrating that the estimated gradient maintains positive correlation with the true gradient sufficient for optimization.

This work bridges a significant theory-practice gap by providing a mathematical explanation for the robustness of policy gradient methods. It validates that a distribution mismatch does not inherently compromise the optimality of the solution, thereby offering theoretical justification for the empirical success of current SOTA algorithms. By leveraging biased SGD theory, the paper provides the research community with a powerful tool for analyzing policy gradient behaviors under broader and more general parameterizations, potentially influencing future algorithm design to prioritize robust implementation over strict theoretical adherence.

## Key Findings

*   **Distribution Mismatch Phenomenon:** Many SOTA policy gradient algorithms designed for discounted problems suffer from a "distribution mismatch," causing them to deviate from the theoretical policy gradient theorem.
*   **Robustness of Tabular Methods:** Despite this theoretical deviation, policy gradient methods utilizing tabular parameterizations are proven to remain globally optimal.
*   **Generalization via Biased SGD:** The study successfully extends the analysis of this mismatch to general parameterizations by applying the theory of biased stochastic gradient descent.
*   **Inherent Algorithmic Robustness:** The results confirm the inherent robustness of policy gradient methods, explaining why they remain effective in practical implementations even when they diverge from strict theoretical assumptions.

## Methodology

The authors employ a **two-step theoretical analysis framework**:

1.  **Tabular Analysis:** First, they assess the impact of the distribution mismatch in the simplified context of tabular parameterizations to confirm global optimality.
2.  **Generalization via Biased SGD:** Second, to address more complex and realistic scenarios, they leverage the mathematical theory of biased stochastic gradient descent, allowing them to generalize their findings beyond the tabular setting.

## Contributions

*   **Bridging the Theory-Practice Gap:** The work addresses the critical discrepancy between the theoretical policy gradient theorem and the actual implementation of SOTA algorithms regarding distribution mismatch.
*   **Robustness Validation:** It provides theoretical evidence validating the robustness of policy gradient methods, demonstrating that a mismatch in distribution does not necessarily compromise the optimality of the solution in tabular settings.
*   **Theoretical Generalization:** By utilizing biased stochastic gradient descent theory, the paper offers a tool for analyzing policy gradient behaviors under broader, more general parameterizations than previously covered.

## Technical Details

The paper proposes a theoretical framework using **Biased Stochastic Gradient Descent** to analyze Policy Gradient methods under distribution mismatch.

### Core Assumptions
*   **Biased ABC Assumption:** Used to characterize convergence with biased gradients, bridging tabular and general parameterizations.

### Ergodicity Parameters
The approach quantifies the mismatch between the discounted state visitation distribution ($d_{\pi,\gamma}$) and the stationary distribution ($d_{\pi}$) by bounding their ratio using:
*   **Diameter ($D$)**
*   **Mixing parameter ($\beta$)**
*   **Minimum state visitation probabilities ($c_{min}$)**

### Gradient Analysis
*   The gradient estimator is decomposed to bound the inner product with the true gradient to ensure positive correlation.

## Results

The text provides **theoretical bounds** instead of empirical results. Key derived bounds include:

*   **Distribution Ratio Bound:**
    $$ \left\| \frac{d_{\pi,\gamma}}{d_{\pi}} \right\|_{\infty} \leq 1 + \frac{2(1 - \gamma)D}{(1 - \gamma\beta)c_{min}} $$

*   **Gradient Norm Bound:**
    $$ \sigma = \frac{G R_{max} |A|}{1 - \gamma} $$

*   **Biased ABC Coefficient:**
    $$ b \geq \frac{(1 - \gamma\beta)c_{min}}{(1 - \gamma\beta)c_{min} + 2(1 - \gamma)D} $$

*   **Central Convergence Inequality:**
    $$ \langle \nabla_{\theta}J, \hat{\nabla}_{\theta}J \rangle \geq \left\| \frac{d_{\pi,\gamma}}{d_{\pi}} \right\|_{\infty}^{-1} \| \nabla_{\theta}J \|^2 - \left( 1 - \left\| \frac{d_{\pi,\gamma}}{d_{\pi}} \right\|_{\infty}^{-1} \right) \sigma^2 $$