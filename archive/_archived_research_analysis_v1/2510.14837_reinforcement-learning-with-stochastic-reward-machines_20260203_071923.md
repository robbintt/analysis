---
title: Reinforcement Learning with Stochastic Reward Machines
arxiv_id: '2510.14837'
source_url: https://arxiv.org/abs/2510.14837
generated_at: '2026-02-03T07:19:23'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Reinforcement Learning with Stochastic Reward Machines

*Jan Corazza; Ivan Gavran; Daniel Neider*

---

## Executive Summary

This research addresses a critical limitation in existing Reinforcement Learning (RL) algorithms that utilize Reward Machines (RMs): the assumption that reward signals are deterministic mappings of agent trajectories. In real-world applications, rewards are often stochastic, meaning identical sequences of states and actions can yield different values drawn from a probability distribution. Existing RM methods fail in these settings because they cannot distinguish between environmental noise and the underlying reward logic, rendering them ineffective for tasks where reward signals are inherently variable.

This paper establishes the theoretical and algorithmic foundation required to extend formal reward machine learning to these realistic, stochastic domains. The key innovation is the introduction of **Stochastic Reward Machines (SRMs)**, a novel formalism that extends standard RMs by mapping trajectories to a cumulative distribution function (CDF) over rewards rather than single scalar values.

Technically, the authors propose a constraint-solving algorithm utilizing SMT and SAT solvers to infer the minimal SRM structure directly from an agent's exploration history within a Labeled Markov Decision Process (MDP). This process is iterative; the system explores the environment, generates hypotheses regarding the machine structure and reward distribution parameters, and refines the model based on constraint violations. A significant technical advantage is the method's modularity, allowing it to integrate seamlessly with pre-existing RL algorithms, such as Q-learning, without modifying the underlying RL logic.

The proposed method was rigorously evaluated in two case studies featuring noisy, non-Markovian rewards, including a "Mining" grid-world environment. In these experiments, the SRM-based agent successfully converged to the optimal policy, achieving the theoretical maximum cumulative reward. Conversely, existing deterministic RM algorithms failed to learn any viable strategy in the presence of noise. Furthermore, the SRM approach outperformed naive baselines that relied on simple reward averaging. The authors substantiate these empirical findings with a theoretical guarantee proving that their approach converges to an optimal policy in the limit.

---

## Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |
| **Core Framework** | Stochastic Reward Machines (SRMs) |
| **Formalism** | Labeled Markov Decision Process (MDP) |
| **Solvers Used** | SMT and SAT Solving |
| **Key Guarantee** | Convergence to optimal policy in the limit |
| **Integration** | Modular (compatible with Q-learning, etc.) |

---

## Key Findings

*   **Overcoming Deterministic Limits:** Existing algorithms for learning reward machines are limited by the impractical assumption that rewards must be noise-free. The proposed algorithm overcomes this by successfully handling stochastic environments.
*   **Learning Minimal SRMs:** The method successfully learns minimal Stochastic Reward Machines (SRMs) directly from agent exploration data using constraint solving techniques.
*   **Convergence Guarantee:** When paired with existing reinforcement learning algorithms, the proposed method guarantees convergence to an optimal policy in the limit.
*   **Superior Effectiveness:** In two case studies, the new approach demonstrated superior effectiveness compared to both existing methods and naive strategies for handling noisy reward functions.

---

## Methodology

The authors introduce a novel framework called **Stochastic Reward Machines (SRMs)** to model environments with noisy rewards. The core of their approach is a learning algorithm based on **constraint solving**; this algorithm analyzes the exploration history of a reinforcement learning agent to infer the minimal SRM structure.

Key aspects of the methodology include:

*   **Modular Design:** The learning component is designed to be modular and easily integrated with pre-existing reinforcement learning algorithms originally designed for standard reward machines.
*   **Constraint Solving:** Utilization of advanced constraint solving to derive machine structure from data.
*   **Iterative Refinement:** The system continuously refines its understanding of the reward structure as the agent explores the environment.

---

## Contributions

*   **Conceptual Advancement:** Extension of the formalism of reward machines to 'Stochastic Reward Machines,' enabling the handling of non-deterministic, noisy reward signals.
*   **Algorithmic Innovation:** A constraint-solving-based algorithm capable of learning minimal stochastic reward machines from agent explorations.
*   **Theoretical Guarantee:** Proof that the proposed approach, when combined with standard RL algorithms, converges to an optimal policy in the limit.
*   **Empirical Validation:** Demonstration that the method outperforms existing state-of-the-art techniques and naive baselines in environments with noisy reward functions.

---

## Technical Details

The technical implementation of the paper involves several specific formalisms and processes:

*   **Reward Function Definition:** The reward function is defined as a mapping from trajectories to a **cumulative distribution function (CDF)** over rewards, rather than a scalar value.
*   **Environment Formalization:** The environment is formalized as a **Labeled Markov Decision Process (MDP)**.
*   **Constraint-Based Formulation:** The algorithm extends a constraint-based formulation using **SMT and SAT Solving** to learn minimal SRMs from exploration data, incorporating reward distribution parameters.
*   **Iterative Process:** The workflow involves:
    1.  Exploring the environment.
    2.  Checking hypotheses against the data.
    3.  Updating machine structure or parameters.
*   **Integration:** The system integrates with existing RL algorithms like Q-learning, effectively replacing a deterministic reward check with a probabilistic evaluation.

---

## Results

The method was evaluated on two case studies with noisy, non-Markovian rewards, including a **'Mining' grid-world**.

*   **Convergence:** It guarantees convergence to an optimal policy in the limit.
*   **Comparison with Deterministic RMs:** Compared to existing deterministic RM algorithms, the proposed approach performed substantially better. Existing algorithms stagnated at a cumulative reward of zero, failing to learn viable strategies.
*   **Comparison with Naive Baselines:** The method showed superior effectiveness compared to naive averaging baselines, demonstrating better extraction of policy structure.
*   **Interpretability:** It offers improved interpretability by modeling the reward distribution, allowing for better insights into environmental variability.