---
title: 'FF-INT8: Efficient Forward-Forward DNN Training on Edge Devices with INT8
  Precision'
arxiv_id: '2506.22771'
source_url: https://arxiv.org/abs/2506.22771
generated_at: '2026-02-03T19:03:17'
quality_score: 9
citation_count: 19
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# FF-INT8: Efficient Forward-Forward DNN Training on Edge Devices with INT8 Precision

*Jingxiao Ma; Priyadarshini Panda; Sherief Reda*

---

> ### ðŸ“Š Quick Facts
>
> *   **Training Speed:** 4.6% faster than SOTA baselines
> *   **Energy Efficiency:** 8.3% reduction in energy consumption
> *   **Memory Footprint:** 27.0% reduction in memory usage
> *   **MAC Operations:** 37.7x reduction (23.8M vs. 898.2M)
> *   **Testbed:** NVIDIA Jetson Orin Nano
> *   **Precision:** INT8 Quantization

---

## Executive Summary

Training Deep Neural Networks (DNNs) directly on resource-constrained edge devices remains a significant challenge due to the high computational and memory demands of standard backpropagation. The traditional backpropagation algorithm requires storing intermediate activations for gradient calculation, creating a substantial memory bottleneck that prohibits on-device learning. Furthermore, training typically utilizes high-precision floating-point arithmetic (FP32), which consumes excessive energy and processing power. This research addresses the need for an efficient training framework that minimizes memory footprint and energy consumption while maintaining model accuracy, thereby enabling continuous learning and personalization on edge hardware.

The authors propose **FF-INT8**, a novel framework that combines the Forward-Forward (FF) algorithm with aggressive INT8 quantization for the entire training process. Unlike backpropagation, the FF algorithm updates weights layer-by-layer using local objective functions, eliminating the memory-intensive backward pass and the need to store intermediate activations. To stabilize training with low-precision INT8 weights and activations, the researchers developed a quantization-aware approach tailored for layer-wise processing. Additionally, they introduced a **"Look-Ahead" mechanism** to overcome the accuracy limitations of the standard greedy FF algorithm. This scheme incorporates the goodness of subsequent layers into the current layer's loss function ($L_{new} = L_1 + \lambda \times \sum L_{future}$), allowing the model to optimize for global performance with minimal computational overhead, avoiding the complex derivative chains of backpropagation.

Empirical evaluation on the NVIDIA Jetson Orin Nano demonstrates that FF-INT8 offers significant efficiency gains while preserving competitive accuracy. The system achieved a 27.0% reduction in memory usage, an 8.3% decrease in energy consumption, and a 4.6% increase in training speed compared to state-of-the-art baselines. Computationally, the framework reduced operation counts to 23.8M 8-bit MULs and ADDs, representing a 37.7x reduction in MAC operations relative to FP32 and INT8 Backpropagation baselines. In terms of model performance, FF-INT8 achieved higher convergence accuracy in fewer epochs (130 epochs) compared to baseline methods (180 epochs). Notably, for ResNet-18, the approach significantly improved stability and accuracy over standard FF implementations, which previously struggled to reach 60% accuracy.

This research significantly advances the feasibility of on-device DNN training by shifting the efficiency paradigm from inference to the training phase. By validating that the Forward-Forward algorithm can be effectively combined with INT8 quantization, the authors provide a viable pathway for deploying "lifelong learning" capabilities on edge devices without relying on cloud connectivity. The successful mitigation of FF's accuracy limitations through the Look-Ahead scheme suggests that biologically plausible, local learning algorithms can serve as practical alternatives to backpropagation in resource-constrained environments. This work paves the way for more privacy-preserving, energy-efficient, and responsive AI applications at the edge.

---

## Key Findings

*   **Performance Efficiency:** The proposed method achieved **4.6% faster** training speeds compared to state-of-the-art baselines.
*   **Energy Efficiency:** Experiments on the NVIDIA Jetson Orin Nano demonstrated an **8.3% reduction** in energy consumption during training.
*   **Memory Optimization:** By eliminating the need to store intermediate activations, the approach reduced memory usage by **27.0%**.
*   **Accuracy Retention:** Despite using aggressive INT8 quantization, the method maintained competitive accuracy levels against current state-of-the-art models.

---

## Methodology

The study employs a multi-faceted approach to optimize DNN training for edge constraints:

*   **Algorithm Selection:** Utilizes the **Forward-Forward (FF)** algorithm as a biologically plausible alternative to backpropagation to avoid the memory-intensive backward pass.
*   **Quantization Framework:** Developed an **INT8 quantized training framework** that exploits FF's layer-by-layer processing to stabilize gradient quantization.
*   **Accuracy Enhancement:** To address accuracy limitations inherent in the standard FF algorithm, researchers introduced a novel **'look-ahead' scheme**.
*   **Evaluation Environment:** The efficacy of this architecture was evaluated on resource-constrained edge hardware, specifically the **NVIDIA Jetson Orin Nano** board.

---

## Contributions

1.  **Quantization for Training:** Extends low-precision research beyond inference to the training phase by proposing an INT8 quantized approach specifically tailored for the Forward-Forward algorithm.
2.  **Algorithmic Enhancement:** Introduces a **'look-ahead' mechanism** to mitigate the limitations of the Forward-Forward algorithm, thereby improving model accuracy.
3.  **Edge-Device Optimization:** Provides empirical evidence that combining the Forward-Forward algorithm with INT8 precision is a viable strategy for deploying DNN training on resource-constrained edge devices, offering significant gains in memory and energy efficiency.

---

## Technical Details

*   **Precision Implementation:** Implements the Forward-Forward algorithm using **INT8 precision** for forward passes and weight updates.
*   **Goodness Function:** Utilizes a goodness function defined as the **Euclidean norm** of neuron values with a threshold of **2.0**.
*   **Look-Ahead Scheme:**
    *   Addresses greedy layer-wise limitations by incorporating subsequent layer goodness into the loss function.
    *   **Formula:** $L_{new} = L_1 + \lambda \times (L_2 + \dots + L_{final})$
    *   Requires only one forward pass and avoids backward derivative chains.
    *   The balancing coefficient $\lambda$ is initialized to 0 and increased by **0.001 per epoch**.
*   **Memory Management:** Efficiency is achieved by eliminating intermediate activation storage; however, the 'Look-Ahead' scheme retains all network weights.
*   **Optimization Target:** Optimized specifically for edge devices like the NVIDIA Jetson Orin Nano.

---

## Results

**System Performance (NVIDIA Jetson Orin Nano)**
*   **Speed:** Training is 4.6% faster.
*   **Energy:** 8.3% less energy consumption.
*   **Memory:** 27.0% lower memory usage.

**Model Performance**
*   **MLP Tests:** FF-INT8 achieved higher convergence accuracy in **130 epochs** versus the baseline's ~90% accuracy in **180 epochs**.
*   **ResNet-18:** FF-INT8 significantly improved stability and accuracy compared to a baseline that reached only **~60% accuracy**.

**Computational Efficiency**
*   **Operation Count:** FF-INT8 reduces operation counts to **23.8M** 8-bit MULs and ADDs.
*   **Reduction Factor:** Represents a **37.7x reduction** in MAC operations compared to FP32 and INT8 Backpropagation baselines (898.2M).

---

**References:** 19 citations  
**Quality Score:** 9/10