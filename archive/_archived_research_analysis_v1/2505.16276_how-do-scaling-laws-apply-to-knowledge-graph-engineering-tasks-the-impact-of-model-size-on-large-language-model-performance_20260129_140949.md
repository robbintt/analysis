# How do Scaling Laws Apply to Knowledge Graph Engineering Tasks? The Impact of Model Size on Large Language Model Performance

*Desiree Heim; Lars-Peter Meyer; Markus SchrÃ¶der; Johannes Frey; Andreas Dengel*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **References:** 33 Citations
> *   **Models Analyzed:** 26 open, state-of-the-art LLMs
> *   **Parameter Range:** 0 to 70 Billion
> *   **Framework:** LLM-KG-Bench
> *   **Key Focus:** Empirical evaluation of scaling laws in Knowledge Graph Engineering (KGE)

---

## Executive Summary

This paper addresses the critical uncertainty surrounding the application of scaling lawsâ€”the principle that larger models yield better performanceâ€”to Knowledge Graph Engineering (KGE). As Large Language Models (LLMs) are increasingly tasked with complex structured data operations, organizations often assume that deploying models with significantly higher parameter counts (up to 70 billion) guarantees superior results. This research is necessary because treating scaling laws as universal truths can lead to inefficient resource allocation; it investigates whether the substantial computational costs of massive models are actually justified by proportional gains in technical KGE tasks.

The study's primary technical innovation is the first specific empirical exploration of scaling laws applied to Knowledge Graph Engineering, utilizing the **LLM-KG-Bench** framework to rigorously evaluate LLM capabilities. The authors analyzed a dataset of 26 open, state-of-the-art LLMs across six distinct tasks categorized into RDF Manipulation, SPARQL Manipulation, and Text-based QA. To isolate the impact of scale from architectural differences, the methodology employs an "intra-family" analysis, treating model size (0 to 70 billion parameters) as the independent variable and normalized performance scores as the dependent variable.

The results generally validate the existence of scaling laws in KGE, demonstrating a positive correlation between model size and capability scores (normalized 0.0 to 1.0). However, the study identified significant **plateau and ceiling effects** where increasing model size yielded negligible performance gains, indicating that smaller models are often more cost-efficient. Furthermore, the analysis revealed **intra-family performance anomalies**, where larger models occasionally performed worse than their smaller counterparts (non-monotonic curves).

Consequently, the data suggests that while size generally correlates with ability, it is not a guarantee of improved performance across all graph engineering tasks. This research establishes a foundational benchmarking resource by offering a comparative analysis of 26 diverse LLMs, serving as a baseline for future studies in semantic AI and graph technologies. Its significance lies in challenging the "bigger is better" paradigm, providing actionable cost-efficiency insights that encourage practitioners to test adjacent models within a family rather than defaulting to the largest available option.

---

## Key Findings

*   **Validation of Scaling Laws:** Confirmed that larger models generally demonstrate higher capabilities in Knowledge Graph Engineering tasks.
*   **Plateau and Ceiling Effects:** Identified scenarios where scaling up model size does not yield significant performance improvements, making smaller models a more cost-effective choice for specific applications.
*   **Intra-Family Anomalies:** Discovered instances of non-monotonic performance curves where larger models occasionally perform worse than their smaller counterparts within the same family.
*   **Strategic Recommendation:** Advises practitioners to test "adjacent models" in a family rather than relying solely on model size as a proxy for performance.

---

## Methodology

The study employed a robust empirical approach to evaluate the relationship between model scale and task performance:

*   **Framework:** Utilized the **LLM-KG-Bench** framework to assess LLM capabilities in understanding and producing Knowledge Graphs.
*   **Dataset:** Analyzed results from a run of **26 open, state-of-the-art LLMs**.
*   **Analysis:** Assessed benchmark score evolution across different model size categories and inspected the correlation between size and general score development for individual models and model families.
*   **Variable Isolation:** Used **intra-family** analysis to separate the effect of parameter scale from architectural differences.

---

## Technical Details

The paper provides an empirical evaluation of scaling laws applied to Knowledge Graph Engineering (KGE), assessing performance variability based on parameter scale.

### Scope & Variables
*   **Independent Variable:** Model size (ranging from 0 to 70 Billion parameters).
*   **Dependent Variable:** Normalized performance scores (ranging from 0.0 to 1.0).
*   **Analysis Type:** Intra-family analysis to isolate scale effects.

### Task Categories
The investigation covered six distinct tasks categorized as follows:

1.  **RDF Manipulation**
    *   `RdfConnectionExplain`
    *   `RdfFriendCount`
    *   `RdfSyntaxFixing`
2.  **SPARQL Manipulation**
    *   `Sparql2Answer`
    *   `SparqlSyntaxFixing`
3.  **Text-based QA**
    *   `Text2Answer`

---

## Results

The study utilized a **normalized 1.0 Score** (likely F1, Accuracy, or Exact Match) as the primary metric. The outcomes highlight the nuanced nature of scaling laws in this domain:

*   **General Correlation:** A positive correlation was observed between model size and capability scores, generally validating scaling laws.
*   **Cost-Efficiency:** due to plateau and ceiling effects, increasing model size often yields insignificant gains, suggesting smaller models are sufficient for specific tasks.
*   **Non-Monotonicity:** Intra-family analysis revealed performance anomalies where larger models performed worse than smaller ones, debunking the assumption that size guarantees improvement.
*   **Practical Implication:** The data supports testing adjacent models rather than defaulting to the largest model in a family for KGE tasks.

---

## Contributions

*   **First Specific Exploration:** Provided the initial specific exploration of scaling laws applied directly to Knowledge Graph Engineering tasks.
*   **Cost-Efficiency Insights:** Highlighted that "bigger is not always better," offering critical insights for resource allocation in KGE applications.
*   **Benchmarking Resource:** Established a comparative baseline of 26 diverse LLMs to serve as a resource for future research in semantic AI and graph technologies.