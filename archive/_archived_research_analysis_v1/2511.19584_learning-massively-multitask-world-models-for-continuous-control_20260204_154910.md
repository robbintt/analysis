---
title: Learning Massively Multitask World Models for Continuous Control
arxiv_id: '2511.19584'
source_url: https://arxiv.org/abs/2511.19584
generated_at: '2026-02-04T15:49:10'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Learning Massively Multitask World Models for Continuous Control

*Nicklas Hansen; Hao Su; Xiaolong Wang*

---

### üìä Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Model Name** | Newt |
| **Base Architecture** | TD-MPC2 |
| **Benchmark** | MMBench (200 tasks, 10 domains) |
| **Training Steps** | 100 million environment steps |
| **Performance Score** | ~0.48 ‚Äì 0.50 |
| **Quality Score** | 9/10 |
| **Citations** | 40 |

---

## üìù Executive Summary

Scaling reinforcement learning (RL) to hundreds of diverse continuous control tasks and robotic embodiments has historically been viewed as computationally prohibitive. While offline learning and behavior cloning offer efficiency, they often lack the adaptability required for robust decision-making and generalization to unseen scenarios. This research addresses the critical need for a unified system that moves beyond single-task training to enable massive multitasking, efficient data usage, and robust control across varied domains and action spaces.

The key innovation is **Newt**, a language-conditioned multitask world model built upon the TD-MPC2 architecture, formulated to function as a Partially Observable Markov Decision Process (POMDP). Technically, the model processes a composite state comprising low-dimensional states, image observations, and language instructions via CLIP-ViT/B embeddings, utilizing masking to handle varying action dimensionalities across tasks. The training pipeline adopts a foundation model approach: it begins with pretraining on demonstration data to establish task-aware representations and action priors, followed by joint online optimization where the agent interacts with all tasks simultaneously to refine the world model.

Evaluated on the newly introduced **MMBench**‚Äîcomprising 200 diverse continuous control tasks across 10 domains‚ÄîNewt achieved a normalized Average Score of **0.48‚Äì0.50** over 100 million environment steps. This performance significantly outperformed strong baselines, including Behavior Cloning (~0.1), Single-task BC (~0.0‚Äì0.1), PPO, and FastTD3. Crucially, the results demonstrated two distinct capabilities: robust open-loop control across a wide variety of scenarios, and rapid adaptation to unseen tasks, highlighting strong generalization properties. Ablation studies confirmed that while the model outperforms standard baselines even without demonstrations, its full potential is realized when using demos to enhance exploration, validating superior data efficiency.

This work carries significant implications for the robotics and RL communities by empirically validating that online RL can scale effectively to hundreds of diverse tasks, challenging the prevailing view that online methods are unscalable compared to offline approaches. By demonstrating the efficacy of integrating offline pretraining with online interaction within a world model framework, the research establishes a viable pathway toward developing general-purpose robotic controllers. To facilitate future progress, the authors have released the MMBench environments, demonstration datasets, and a full open-source codebase with checkpoints.

---

## üîë Key Findings

*   **Superior Performance:** The proposed model, Newt, achieves superior multitask performance and data-efficiency compared to strong baseline methods.
*   **Robust Control:** Newt demonstrates robust open-loop control capabilities across a wide variety of scenarios.
*   **Rapid Adaptation:** The system enables rapid adaptation to unseen tasks, highlighting strong generalization properties.
*   **Scalability Validation:** The research validates that online reinforcement learning can scale effectively to hundreds of diverse tasks and embodiments.

---

## üõ†Ô∏è Methodology

The authors developed **Newt**, a language-conditioned multitask world model designed to handle continuous control across diverse domains. The training pipeline utilizes a two-phase strategy inspired by foundation model recipes:

1.  **Pretraining:** The model is pretrained on demonstrations to acquire task-aware representations and action priors.
2.  **Joint Online Optimization:** This is followed by joint optimization through interaction across all tasks.

The method was evaluated on a new benchmark comprising **200 diverse tasks** spanning multiple domains and embodiments with multimodal inputs.

---

## ‚öôÔ∏è Technical Details

**Model Architecture & Training**
*   **Base Model:** Built upon TD-MPC2.
*   **Process:** Operates as a POMDP with a composite state tuple.
*   **Training:** Two-stage process involving pretraining on demonstration data and joint optimization via online interaction.

**State Representation**
*   **Components:** Low-dimensional states, image observations, and language instructions.
*   **Embeddings:** Language instructions processed via CLIP-ViT/B embeddings.
*   **Handling Diversity:** Uses masking for varying action dimensionalities to handle diverse tasks.

**Benchmark: MMBench**
*   **Scale:** 200 distinct continuous control tasks.
*   **Domains:** Spans 10 different domains.
*   **Inputs:** Supports low-dimensional states, 224x224 RGB images, or both.

**Infrastructure**
*   **Environments:** Asynchronous environments via Docker.
*   **Demonstrations:** 10-40 demonstrations per task collected by single-task TD-MPC2 agents.

---

## üß™ Results

Evaluated across 200 tasks over 100 million environment steps using the Average Score metric, the performance is summarized below:

*   **Newt:** Achieved a score of approximately **0.48‚Äì0.50**.
*   **Baselines:** Performed significantly worse:
    *   Behavior Cloning: ~0.1
    *   PPO & FastTD3: Low performance
    *   Single-task BC: ~0.0‚Äì0.1

**Ablation Study & Analysis**
*   Newt outperformed standard baselines even without demonstrations.
*   Full performance relied on demos for exploration.
*   The model demonstrated superior data efficiency.
*   Successful generalization was validated via CLIP embeddings, confirming that online reinforcement learning can scale to hundreds of diverse continuous control tasks.

---

## ‚ú® Contributions

*   **Newt Model:** Introduction of a language-conditioned multitask world model integrating offline pretraining with online interaction.
*   **MMBench:** Release of a comprehensive benchmark featuring 200 diverse continuous control tasks with multimodal inputs.
*   **Scalability Proof:** Empirical validation that online RL scales to hundreds of tasks, challenging the view that online RL is unscalable.
*   **Open Source Release:** Release of open-source tools including environments, demonstrations, code, and checkpoints.