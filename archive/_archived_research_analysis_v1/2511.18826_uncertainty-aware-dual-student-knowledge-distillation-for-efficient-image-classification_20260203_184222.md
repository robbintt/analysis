---
title: Uncertainty-Aware Dual-Student Knowledge Distillation for Efficient Image Classification
arxiv_id: '2511.18826'
source_url: https://arxiv.org/abs/2511.18826
generated_at: '2026-02-03T18:42:22'
quality_score: 9
citation_count: 8
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Uncertainty-Aware Dual-Student Knowledge Distillation for Efficient Image Classification

*Aakash Gore; Anoushka Dey; Aryan Mishra*

---

> **QUICK FACTS**
> * **Dataset:** ImageNet-100
> * **Primary Architectures:** ResNet-18, MobileNetV2
> * **Top ResNet-18 Accuracy:** 83.84% (+2.04% gain)
> * **Top MobileNetV2 Accuracy:** 81.46% (+0.92% gain)
> * **Innovation:** Uncertainty-weighted distillation & Heterogeneous peer learning

---

## Executive Summary

Knowledge distillation (KD) is a critical technique for deploying deep learning models on resource-constrained devices, allowing smaller "student" models to replicate the performance of larger "teacher" networks. However, traditional KD methods often suffer from inefficiencies because they treat all teacher predictions as equally authoritative, regardless of the teacher's confidence level. This approach can force student models to learn from ambiguous or incorrect teacher predictions, limiting their ultimate accuracy. Furthermore, standard single-student frameworks fail to leverage the potential benefits of collaborative learning between different architectures.

This paper introduces an **Uncertainty-Aware Dual-Student Knowledge Distillation framework** designed to optimize the transfer of knowledge by quantifying and weighting teacher confidence. The core innovation lies in an uncertainty-aware mechanism that selectively guides student training; teacher predictions are weighted based on their confidence levels, preventing low-uncertainty (high-confidence) knowledge from being diluted by high-uncertainty noise. Additionally, the authors propose a heterogeneous dual-student architecture where two distinct models—specifically ResNet-18 and MobileNetV2—train collaboratively via a peer-learning setup. This allows the students to learn not only from the teacher but also from each other’s complementary feature representations.

The framework was evaluated on the ImageNet-100 dataset, where it demonstrated statistically significant improvements over baseline and single-student knowledge distillation methods. The proposed approach achieved a top-1 accuracy of **83.84% for ResNet-18** and **81.46% for MobileNetV2**. These results represent substantial accuracy gains of 2.04% for ResNet-18 and 0.92% for MobileNetV2 compared to conventional single-student approaches, validating the efficacy of uncertainty-weighted guidance and heterogeneous peer collaboration.

This research significantly advances the field of model compression by demonstrating that teacher uncertainty is a vital metric for optimizing knowledge distillation. By proving that selective guidance outperforms the blanket enforcement of teacher logits, the authors provide a new, efficient paradigm for training high-performance lightweight models. The success of using heterogeneous student architectures suggests that future work can move beyond single-model training toward collaborative, multi-architecture systems, offering a practical path toward more accurate and efficient image classification for edge computing applications.

---

## Key Findings

*   **Superior Performance:** The proposed framework outperformed baseline knowledge distillation methods on ImageNet-100, achieving top-1 accuracy of **83.84%** for ResNet-18 and **81.46%** for MobileNetV2.
*   **Significant Accuracy Gains:** It yielded substantial improvements compared to single-student approaches, with a **+2.04%** gain for ResNet-18 and **+0.92%** gain for MobileNetV2.
*   **Uncertainty Utilization:** Leveraging teacher prediction uncertainty to selectively guide student learning proved more effective than treating all predictions equally.
*   **Collaborative Learning:** Using two heterogeneous student architectures (ResNet-18 and MobileNetV2) that learn collaboratively enhanced overall model performance.

---

## Methodology

The researchers introduced an **uncertainty-aware dual-student knowledge distillation framework** centered on a peer-learning mechanism. The core components of the methodology include:

*   **Collaborative Training:** Two heterogeneous student architectures, ResNet-18 and MobileNetV2, train simultaneously and collaboratively.
*   **Selective Guidance:** The framework utilizes teacher prediction uncertainty to selectively guide the learning process.
*   **Peer Learning:** Students learn from the teacher network as well as from each other.
*   **Confidence Prioritization:** The system prioritizes predictions based on the teacher's confidence levels, filtering out low-confidence noise.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Framework Name** | Uncertainty-Aware Dual-Student Knowledge Distillation |
| **Core Mechanism** | Uses teacher prediction uncertainty for selective guidance. |
| **Architecture** | Dual-student setup featuring heterogeneous models. |
| **Student Models** | ResNet-18 and MobileNetV2. |
| **Training Style** | Collaborative peer-learning setup. |

---

## Results

The study was evaluated on the **ImageNet-100** dataset using Top-1 Accuracy.

*   **ResNet-18:**
    *   **Accuracy:** 83.84%
    *   **Improvement:** +2.04% gain over single-student approaches.
*   **MobileNetV2:**
    *   **Accuracy:** 81.46%
    *   **Improvement:** +0.92% gain over single-student approaches.

The method consistently outperformed baselines, demonstrating the value of the dual-student, uncertainty-aware approach.

---

## Contributions

*   **Uncertainty-Driven Distillation:** Proposes a framework that weights teacher predictions based on uncertainty to prevent low-confidence predictions from negatively impacting training.
*   **Heterogeneous Dual-Student Architecture:** Introduces a novel setup where two distinct student models collaborate via a peer-learning mechanism.
*   **Empirical Validation:** Provides comprehensive evidence on the ImageNet-100 dataset demonstrating that the dual-student, uncertainty-aware method offers statistically significant accuracy improvements over conventional distillation techniques.

---

**Quality Score:** 9/10  
**References:** 8 citations