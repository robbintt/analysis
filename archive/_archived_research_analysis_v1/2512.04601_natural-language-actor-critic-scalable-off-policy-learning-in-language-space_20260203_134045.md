---
title: 'Natural Language Actor-Critic: Scalable Off-Policy Learning in Language Space'
arxiv_id: '2512.04601'
source_url: https://arxiv.org/abs/2512.04601
generated_at: '2026-02-03T13:40:45'
quality_score: 9
citation_count: 22
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Natural Language Actor-Critic: Scalable Off-Policy Learning in Language Space

*Joey Hong; Kang Liu; Zhan Ling; Jiecao Chen; Sergey Levine*

---

### ðŸ“Š Quick Facts & Metrics

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Citations** | 22 |
| **Paradigm** | Off-Policy / Actor-Critic / Natural Language Feedback |
| **AlfWorld Success** | **81.5%** (vs 76.9% GRPO) |
| **WebShop Success** | **35.2%** (vs 31.6% GRPO) |
| **HotpotQA (EM)** | **51.3%** |
| **InterCode Success** | **33.1%** |

---

## Executive Summary

Training Large Language Models (LLMs) to function as autonomous agents capable of complex, long-horizon tasksâ€”such as multi-step reasoning, web browsing, and tool useâ€”presents significant challenges in stability and sample efficiency. Standard reinforcement learning approaches, particularly on-policy policy gradient methods like PPO, suffer from high variance and require extensive interaction data. Furthermore, these methods typically rely on scalar reward signals, which are often sparse and fail to provide sufficient guidance for navigating the massive, open-ended action spaces inherent to natural language. This makes it difficult to train agents that can effectively explore their environment and learn from nuanced feedback without prohibitive computational costs.

The researchers introduce **Natural Language Actor-Critic (NLAC)**, a novel actor-critic framework that replaces traditional scalar value functions with a generative LLM critic. Instead of outputting a single numerical value, the critic generates natural language explanations that critique the agent's actions, offering richer, actionable feedback for improvement. Technically, the system utilizes a Language Successor Model trained via a "Language Bellman Backup" to predict future textual descriptions and rewards, enabling fully off-policy learning. The actor (policy) is not updated via gradient ascent on rewards; rather, a separate Refinement Policy generates improved actions based on the state and the textual critique. The base policy is then aligned with this Refinement Policy using a KL-divergence objective, effectively learning to self-correct without the instability of policy gradients.

The authors demonstrate that NLAC outperforms existing state-of-the-art training approaches, including PPO and GRPO, across a suite of complex environments. Evaluations conducted on AlfWorld, WebShop, HotpotQA, and InterCode (Python) reveal substantial gains in both performance and data efficiency. Specifically, NLAC achieved a success rate of 81.5% on AlfWorld and 35.2% on WebShop, significantly outperforming the GRPO baseline. Beyond raw performance, the method exhibited superior data efficiency, often achieving optimal performance with an order of magnitude fewer environment steps than on-policy methods, while mitigating exploration challenges in large action spaces through natural language explanations.

This work represents a significant paradigm shift in LLM agent training by establishing the first actor-critic algorithm specifically designed to utilize natural language feedback rather than scalar rewards. By decoupling the learning process from on-policy data collection, NLAC offers a scalable framework that addresses the high sample complexity and instability that have historically limited the deployment of complex autonomous agents. The ability to train agents off-policy using rich textual critiques not only improves performance on current benchmarks but also opens new avenues for developing more robust, interpretable, and data-efficient AI systems capable of sophisticated interaction.

---

## Key Findings

*   **Superior Performance:** NLAC outperforms existing training approaches (such as PPO and GRPO) across complex tasks involving reasoning, web browsing, and tool-use with dialogue.
*   **Data Efficiency & Stability:** The method offers a more data-efficient and stable training paradigm by **eliminating policy gradients** and relying on **off-policy training**.
*   **Exploration Mitigation:** It effectively mitigates exploration challenges in large, open-ended action spaces by utilizing natural language explanations to guide the agent.
*   **Richer Feedback Signals:** A generative LLM critic provides richer, more actionable training signals than traditional scalar values, which is particularly beneficial for long-horizon tasks with sparse rewards.

---

## Methodology

The researchers introduce **Natural Language Actor-Critic (NLAC)**, a novel actor-critic algorithm designed to train Large Language Model (LLM) policies. Unlike traditional methods that rely on scalar value estimates, NLAC employs a generative LLM as a critic that outputs natural language explanations regarding why specific actions are suboptimal.

This text-based feedback allows the policy to reason about improvements without relying solely on random exploration. The approach operates **off-policy** and does not utilize policy gradients, enabling it to learn from existing data more efficiently.

---

## Technical Details

The NLAC framework is designed to fine-tune LLM agents for long-horizon tasks within a language space. The architecture and training process are defined by the following components:

*   **Architecture:** Utilizes the same underlying LLM for the policy, critic, and refinement policy, differentiated only by prompts.
*   **Action Structure:** Actions are composite, consisting of a **reasoning thought** and an **environment action**.
*   **Critic Mechanism:** Functions as a **Language Successor Model** that predicts textual descriptions of future rollouts and rewards. It is trained using a **Language Bellman Backup** for off-policy learning.
*   **Policy Update:** The actor improves via a **self-refinement paradigm**.
    *   A *Refinement Policy* generates refined actions based on the state, initial action, and textual critique.
    *   Policy updates are performed via a **KL-divergence objective** to align the base policy with the refinement policy.

---

## Core Contributions

*   **Algorithmic Innovation:** Development of NLAC, the first actor-critic algorithm specifically designed to train LLM agents using natural language feedback rather than scalar rewards.
*   **Problem Resolution:** A solution to the instability and high sample complexity associated with training LLM agents on trajectory-level sparse rewards in long-horizon tasks.
*   **Framework Scalability:** Introduction of a scalable, off-policy training framework that bypasses the limitations of policy gradient methods, making it easier to train agents for complex interactions like web browsing and tool use.

---

## Performance Results

While the detailed experimental section was not provided in the source text, the analysis highlights specific quantitative achievements mentioned in the executive summary compared to the GRPO baseline:

| Task | Metric | NLAC Result | Baseline (GRPO) |
| :--- | :--- | :--- | :--- |
| **AlfWorld** | Success Rate | **81.5%** | 76.9% |
| **WebShop** | Success Rate | **35.2%** | 31.6% |
| **HotpotQA** | Exact Match (EM) | **51.3%** | *Not specified* |
| **InterCode (Python)** | Success Rate | **33.1%** | *Not specified* |

**Qualitative Outcomes:**
*   NLAC is reported to be more data-efficient and stable than on-policy policy gradient methods.
*   The method mitigates exploration challenges in large action spaces using natural language explanations.
*   It provides richer, more actionable training signals than traditional scalar rewards.