---
title: 'MLZero: A Multi-Agent System for End-to-end Machine Learning Automation'
arxiv_id: '2505.13941'
source_url: https://arxiv.org/abs/2505.13941
generated_at: '2026-02-03T13:27:38'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# MLZero: A Multi-Agent System for End-to-end Machine Learning Automation

*Haoyang Fang; Boran Han; Nick Erickson; Xiyuan Zhang; Su Zhou; Anirudh Dagar; Jiani Zhang; Ali Caner Turkmen; Cuixiong Hu; Huzefa Rangwala; Ying Nian Wu; Bernie Wang; George Karypis*

***

> ### **Quick Facts**
>
> *   **LLM Parameter Count:** 8B
> *   **MLE-Bench Lite Performance:** 6 Gold Medals
> *   **Benchmark Success Rate:** 0.92
> *   **Improvement over Competitors:** 263.6%
> *   **New Benchmark Tasks:** 25 challenging tasks

***

## Executive Summary

The paper addresses the critical challenge of achieving true end-to-end automation for complex machine learning workflows, particularly in multimodal scenarios where data types vary significantly (e.g., semantic segmentation). Existing LLM-based agents often struggle with hallucinations, outdated API knowledge, and the necessity for extensive manual configuration or dataset-code pairing, limiting their applicability in production environments. **MLZero** targets these limitations to enable fully automated, zero-intervention machine learning that requires no expert oversight from raw data input to final model deployment.

The core innovation is a multi-agent architecture built around a compact 8B parameter LLM, designed to function as $F(x, U_{opt}) = (y, C, L)$. This system introduces a "Cognitive Perception Module" that interprets raw multimodal data to select appropriate libraries and guide the workflow, distinct from the coding process. To ensure robustness, the authors implement a memory-augmented code generation mechanism utilizing **Semantic Memory** for condensed library knowledge and **Episodic Memory** to store execution history for debugging.

MLZero demonstrates superior performance across rigorous benchmarks, securing six gold medals on the MLE-Bench Lite and outperforming all competing methods. On the newly introduced Multimodal AutoML Agent Benchmark, the system achieved a remarkable success rate of 0.92, representing a 263.6% improvement over the nearest competitors. The results validate the system's ability to handle diverse and complex tasks while maintaining high efficiency using a significantly smaller parameter model than typically required for such advanced reasoning.

## Key Findings

*   **Superior Benchmark Performance:** MLZero secured **six gold medals** on MLE-Bench Lite, outperforming all competing methods.
*   **State-of-the-Art Multimodal Results:** Achieved a success rate of **0.92** on the Multimodal AutoML Agent Benchmark, representing a **263.6% improvement** over competitors.
*   **Efficiency with Compact Models:** Maintains robust effectiveness using a compact **8B LLM**, proving that massive parameter counts are not strictly necessary for advanced reasoning.
*   **Minimal Human Intervention:** Enables end-to-end ML automation across diverse data modalities without substantial manual configuration.

## Methodology

MLZero employs a novel multi-agent architecture powered by LLMs to orchestrate the machine learning lifecycle. The methodology is defined by three core components:

1.  **Multi-Agent Framework:** Orchestrates the entire ML lifecycle through specialized agents, ensuring a systematic approach to automation.
2.  **Cognitive Perception Module:** Processes raw multimodal inputs into a perceptual context to guide the workflow. This module separates data understanding from code generation.
3.  **Memory-Augmented Code Generation:** Enhances iterative code generation using two distinct memory mechanisms:
    *   **Semantic Memory:** Provides external, condensed library knowledge.
    *   **Episodic Memory:** Stores execution history for debugging.
    *   *Goal:* To mitigate hallucinations and compensate for outdated API knowledge.

## Technical Architecture

MLZero is designed as a zero-intervention system functioning as $F(x, U_{opt}) = (y, C, L)$. It utilizes a modular architecture composed of four main components:

*   **Perception (P):** Interprets raw data and selects appropriate ML libraries.
*   **Semantic Memory:** Acts as a repository for external condensed library knowledge.
*   **Episodic Memory:** Stores execution history to facilitate debugging and iterative improvement.
*   **Iterative Coding:** Features three specialized agents:
    *   **Coder:** Responsible for generating code.
    *   **Executer:** Runs the generated code.
    *   **Error Analyzer:** Identifies issues and triggers refinement.

## Results and Comparative Analysis

MLZero sets a new standard for LLM-driven automation by surpassing existing solutions in specific key areas:

*   **Versus DS-Agent:** MLZero improves upon DS-Agent by avoiding reliance on dataset-code pairs.
*   **Versus AutoKaggle:** Surpasses AutoKaggle by handling arbitrary non-tabular data types.
*   **Versus AIDE:** Outperforms AIDE in managing complex knowledge through its dedicated memory modules.

The system demonstrates high efficiency with its 8B LLM model, handling diverse data modalities (such as semantic segmentation) without manual configuration.

## Core Contributions

*   **Advanced AutoML Solution:** Introduction of MLZero to reduce the need for expert intervention in complex multimodal scenarios.
*   **Architectural Innovations:** Proposal of a cognitive perception module and the integration of semantic/episodic memory for improved code generation.
*   **New Evaluation Standards:** Release of the **'Multimodal AutoML Agent Benchmark'** featuring 25 challenging tasks to standardize future research.

***

**Quality Score:** 9/10 | **References:** 40 citations