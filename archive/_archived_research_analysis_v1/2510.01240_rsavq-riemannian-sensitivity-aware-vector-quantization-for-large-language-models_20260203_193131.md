---
title: 'RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language
  Models'
arxiv_id: '2510.0124'
source_url: https://arxiv.org/abs/2510.01240
generated_at: '2026-02-03T19:31:31'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models

*Zukang Xu; Xing Hu; Qiang Wu; Dawei Yang*

---

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Target Model** | LLaMA-3 8B |
| **Quantization** | 2-bit (Extremely Low-bit) |
| **Quality Score** | **9/10** |
| **Citations** | 40 References |
| **Performance Gain** | **+1.5** Zero-shot Accuracy |
| **Perplexity** | **-0.4** Reduction |

---

> ### üìã Executive Summary
>
> **The Challenge:**
> Deploying Large Language Models (LLMs) on resource-constrained hardware requires extreme compression. Existing Vector Quantization (VQ) methods typically rely on a Euclidean view of parameter space. This limitation leads to unconstrained quantization error directions and suboptimal uniform bit allocation, causing significant accuracy degradation at the low bit-widths (2-4 bits) required for edge deployment.
>
> **The Solution:**
> The authors introduce **RSAVQ** (Riemannian Sensitivity-Aware Vector Quantization), a novel framework leveraging information geometry. By treating the neural network parameter space as a non-uniformly curved Riemannian manifold rather than a flat Euclidean space, RSAVQ utilizes the Fisher Information Matrix (FIM) as the metric tensor.
>
> **The Mechanism:**
> RSAVQ implements two key mechanisms:
> 1.  **Error Direction Sensitivity Guidance (EDSG):** Projects quantization errors onto low-sensitivity directions using negative natural gradients.
> 2.  **Weight Channel Sensitivity Guidance (WCSG):** Analyzes FIM curvature to construct channel-wise sensitivity metrics for dynamic bit allocation.
>
> **The Impact:**
> In trials with the LLaMA-3 8B model under 2-bit constraints, RSAVQ outperformed state-of-the-art baselines (VPTQ and QuIP#), reducing perplexity by 0.4 and increasing zero-shot accuracy by 1.5. This work establishes a theoretical bridge between information geometry and quantization, enabling high-performance LLM deployment on consumer-grade hardware without significant quality loss.

---

## üîç Key Findings

*   **Superior Performance:** RSAVQ outperforms existing state-of-the-art baselines, specifically **VPTQ** and **QuIP#**, in extremely low-bit quantization scenarios.
*   **Quantifiable Metrics:**
    *   Achieved a **0.4 reduction in perplexity**.
    *   Achieved a **1.5 increase in zero-shot accuracy** for the 2-bit LLaMA-3 8B model.
*   **Limitations Resolved:** The method successfully addresses two core Vector Quantization limitations:
    1.  Unconstrained direction errors.
    2.  Suboptimal bit allocation.
*   **Validation:** The study confirms that high-performance LLMs can be quantized to **2-4 bits** for deployment on resource-constrained devices without significant quality degradation.

---

## üõ†Ô∏è Methodology

The authors propose **RSAVQ**, a geometry-driven framework designed to enhance quantization by understanding the underlying structure of the parameter space.

*   **Error Direction Sensitivity Guidance (EDSG):**
    *   Utilizes the Fisher Information Matrix (FIM)-induced Riemannian metric.
    *   Projects quantization errors onto low-sensitivity directions to minimize the impact on model performance.
*   **Weight Channel Sensitivity Guidance (WCSG):**
    *   Constructs channel-wise sensitivity metrics via FIM curvature analysis.
    *   Enables **dynamic bit allocation** for globally optimal quantization, moving away from rigid uniform precision.

---

## ‚öôÔ∏è Technical Details

The approach shifts the paradigm from Euclidean space modeling to a sophisticated geometric framework.

### Geometric Framework
*   **Parameter Space:** Modeled as a **non-uniformly curved Riemannian manifold** rather than a standard Euclidean space.
*   **Metric Tensor:** Utilizes the **Fisher Information Matrix (FIM)** to define distances and curvatures within the parameter space.

### Optimization Strategy
*   **Joint Optimization:** Simultaneously optimizes bit allocation and error projection through an information geometry framework.
*   **Sensitivity-Aware Quantization:**
    *   Projects errors onto low-sensitivity geometric directions.
    *   Utilizes the **negative natural gradient** to minimize performance degradation.
*   **Adaptive Bit Allocation:**
    *   Rejects uniform precision.
    *   Employs channel-wise sensitivity guidance to allocate bits where they are most needed.

---

## üìà Results

The experimental validation focused on the **LLaMA-3 8B model** under strict **2-bit quantization** constraints:

*   **Comparison:** RSAVQ was benchmarked against state-of-the-art baselines **VPTQ** and **QuIP#**.
*   **Outcomes:**
    *   **Perplexity:** Reduced by **0.4** compared to baselines.
    *   **Accuracy:** Increased by **1.5** in zero-shot accuracy compared to baselines.
*   **Conclusion:** The results validate the hypothesis that geometry-aware, sensitivity-driven quantization can effectively recover performance metrics typically lost in standard low-bit pipelines.

---

## üìù Contributions

1.  **Novel Framework:** Introduction of RSAVQ, a new framework specifically designed for enhancing extremely low-bit quantization for LLMs.
2.  **Theoretical Bridge:** Established a connection between **information geometry** and **neural network quantization** by leveraging Riemannian metrics and the Fisher Information Matrix.
3.  **Optimization Mechanisms:** Developed specific algorithms (EDSG and WCSG) to guide quantization errors and optimize bit allocation dynamically.
4.  **Practical Deployment:** Provided a viable solution for deploying massive LLMs on resource-constrained hardware, advancing the field of efficient deep learning.