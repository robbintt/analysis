---
title: How Is Uncertainty Propagated in Knowledge Distillation?
arxiv_id: '2601.18909'
source_url: https://arxiv.org/abs/2601.18909
generated_at: '2026-02-03T18:51:52'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# How Is Uncertainty Propagated in Knowledge Distillation?

*Ziyao Cui; Jian Pei*

---

### üìå Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |
| **Models Analyzed** | Linear Regression, Feed-forward Neural Networks, LLMs |
| **Core Problem** | Distortion via point estimates & uncertainty mismatch |
| **Key Strategies** | Response Averaging ($O(1/k)$), Variance-Weighting |

---

## üìë Executive Summary

> Standard knowledge distillation (KD) processes suffer from a fundamental flaw: they treat the inherently stochastic outputs of teacher models as deterministic ground truths by collapsing them into single point estimates. This approach distorts the learning process by ignoring the underlying noise in the teacher's predictions. The authors identify that this suppression of "intra-student" uncertainty (uncertainty within a model's output) fails to address "inter-student" uncertainty (variance across different student instances). Consequently, distillation often results in student models that are unstable and, in the context of Large Language Models (LLMs), prone to hallucinations because systematic noise from the teacher is propagated as factual knowledge.
>
> This paper reframes knowledge distillation as an "uncertainty transformation" rather than simple behavior mimicking. It introduces a rigorous taxonomy distinguishing between inter-student and intra-student uncertainty. Technically, the authors propose two variance-aware strategies to manage this uncertainty: averaging multiple teacher responses to filter noise and variance-weighting (inverse-variance weighting) to optimally combine teacher and student estimates. These methods are backed by formal mathematical guarantees derived within a linear regression framework‚Äîwhere the teacher is modeled as a linear mapping with additive Gaussian noise‚Äîand subsequently validated in non-convex neural networks and autoregressive LLMs.
>
> Theoretical analysis demonstrates that the variance of student model parameters scales linearly with the teacher's output noise, specifically $\text{Var}(\hat{\theta}_S) = \sigma^2_T (X^\top X)^{-1}$. The study proves that averaging $k$ teacher responses reduces noise at a rate of $O(1/k)$, while variance-weighting successfully constructs a minimum-variance estimator. Empirical testing across feed-forward networks and LLMs confirms these findings; applying uncertainty-aware methods to LLM distillation significantly reduced Evaluation MSE and Inter-Student Variance while improving Cosine Similarity (SBERT) and Average Alignment metrics. These adjustments led to a measurable reduction in systematic noise and hallucinations in the resulting student models.

---

## üîç Key Findings

*   **Distortion via Point Estimates:** Standard knowledge distillation processes are inherently stochastic, and collapsing these uncertainties into single point estimates distorts the learning process.
*   **Uncertainty Mismatch:** Standard single-response knowledge distillation suppresses intra-student uncertainty while failing to eliminate inter-student uncertainty.
*   **Efficacy of Variance-Aware Strategies:** Averaging multiple teacher responses reduces noise at a rate of $O(1/k)$, and variance-weighting successfully combines teacher and student estimates to yield a minimum-variance estimator.
*   **Empirical Improvements in LLMs:** Applying these uncertainty-aware methods to Large Language Model (LLM) distillation results in reduced systematic noise and hallucinations, producing more stable students.

---

## üß™ Methodology

The study employs a multi-stage analysis approach to validate the propagation of uncertainty:

1.  **Theoretical Framework:** The authors analyze uncertainty propagation using **Linear Regression** to provide formal mathematical guarantees. The teacher is modeled as a linear mapping with additive Gaussian noise, and the student uses Ordinary Least Squares (OLS).
2.  **Non-Convex Validation:** Findings are validated on **Feed-Forward Neural Networks** trained via full-batch gradient descent with the Adam optimizer to account for non-convexity.
3.  **Generative AI Application:** The methodology is applied to **Large Language Models (LLMs)** in a free-form Question Answering setting using autoregressive models.

**Proposed Techniques:**
*   **Averaging:** Mitigates noise by averaging multiple teacher responses.
*   **Variance-Weighting:** Combines teacher and student estimates using inverse-variance weighting to minimize error.

---

## ‚öôÔ∏è Technical Details

### Framework & Uncertainty Taxonomy
The paper establishes a framework across three model classes and defines two primary uncertainty categories:
*   **Model Output Uncertainty (Intra-Student):** Uncertainty within a specific model's output.
*   **Model Instance Uncertainty (Inter-Student):** Variance observed across different student instances.

### Mathematical Formulation (Linear Regression)
*   **Teacher Model:** Defined as a linear mapping with additive Gaussian noise:
    $$y^{(T)}_i = x_i^\top \theta_T + \epsilon_i$$
*   **Student Solution:** Utilizes Ordinary Least Squares (OLS) with a closed-form solution.
*   **Variance Scaling:** The study proves that student parameter variance scales linearly with teacher output noise:
    $$\text{Var}(\hat{\theta}_S) = \sigma^2_T (X^\top X)^{-1}$$

### LLM Implementation
*   **Task:** Free-form Question Answering.
*   **Critique:** Standard KD is critiqued for approximating the true distribution with a single teacher sample, which introduces bias and noise.

---

## üìù Contributions

*   **Reframing Knowledge Distillation:** Establishes a new conceptual framework viewing knowledge distillation as an "uncertainty transformation" rather than just behavior transfer.
*   **Uncertainty Taxonomy:** Introduces a critical distinction between inter-student and intra-student uncertainty to explain the limitations of current distillation methods.
*   **Algorithmic Solutions:** Contributes two variance-aware strategies (averaging and inverse-variance weighting) backed by formal guarantees, offering a path to create student models that accurately reflect the uncertainty of their teachers.

---

## üìä Results

*   **Theoretical Guarantees:**
    *   Demonstrated linear scaling of student parameter variance with teacher noise ($\text{Var}(\hat{\theta}_S) = \sigma^2_T (X^\top X)^{-1}$).
    *   Proved that averaging $k$ teacher responses reduces noise at a rate of **$O(1/k)$**.
    *   Showed that combining estimates via variance-weighting yields a **minimum-variance estimator**.
*   **Empirical Metrics:**
    *   **Regression:** Evaluation MSE used as the primary metric.
    *   **LLMs:** Assessed using Cosine Similarity (SBERT embeddings), Average Alignment, and Inter-Student Variance.
*   **Performance Outcomes:**
    *   Uncertainty-aware methods significantly reduced Evaluation MSE and Inter-Student Variance.
    *   Improved Cosine Similarity and Average Alignment in LLM tasks.
    *   Observable reduction in systematic noise and hallucinations in distilled student models.