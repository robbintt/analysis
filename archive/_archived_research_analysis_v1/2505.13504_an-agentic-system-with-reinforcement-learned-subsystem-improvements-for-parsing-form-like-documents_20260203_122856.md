---
title: An agentic system with reinforcement-learned subsystem improvements for parsing
  form-like documents
arxiv_id: '2505.13504'
source_url: https://arxiv.org/abs/2505.13504
generated_at: '2026-02-03T12:28:56'
quality_score: 8
citation_count: 39
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# An agentic system with reinforcement-learned subsystem improvements for parsing form-like documents

*Ayesha Amjad; Saurav Sthapit; Tahir Qasim Syed*

***

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 39 Citations |
| **Key Benchmarks** | CORD, ICDAR-SOIRE |
| **Primary LLM** | GPT-4o-mini |
| **Alternative LLM** | LLaMA 3.3-70B |
| **Architecture Type** | Multi-Agent with RL Driver |

***

> ### üìù Executive Summary
>
> This research addresses the inherent limitations of current document parsing architectures, specifically the rigidity of monolithic Large Language Model (LLM) extraction methods and traditional vision/OCR pipelines. State-of-the-art transformer-based models, such as LayoutLM and BROS, typically rely on predefined lists of entities, which renders them ineffective when encountering unseen document types or dynamic, complex layouts. This dependency creates a significant "cold start" problem, requiring constant human intervention and manual schema updates to process diverse form-like documents (e.g., invoices, receipts, bank statements), thereby hindering the automation of information extraction at scale.
>
> The authors introduce a novel agentic AI framework that integrates a reinforcement learning (RL) driver to create a self-corrective loop for document parsing. The system employs a modular architecture comprising seven distinct agents: three processing agents (document classifier, schema builder, data extractor), three optimization agents (meta-prompting agent and two Gymnasium agents), and an evaluator agent. At the core of the innovation is an RL policy that guides the "meta-prompting agent" to analyze historical extraction errors and iteratively refine the prompts used by "actor agents." This approach leverages the Gymnasium framework with a natural language action space, utilizing the semantic reasoning capabilities of LLMs (specifically GPT-4o-mini or LLaMA 3.3-70B) rather than traditional geometric models like CNNs or RNNs.
>
> The framework was evaluated on the CORD and ICDAR-SOIRE benchmarks, delivering concrete quantitative results that validate its efficacy. On the CORD dataset, the system achieved a competitive Entity F1 score of **96.1%**, performing on par with specialized baselines like LayoutLMv3 but without the requirement for predefined entity definitions. On the ICDAR-SOIRE benchmark, performance was validated using rigorous metrics such as data similarity/matching and system confidence scores, confirming the system's robustness across varied document types. These results demonstrate that the architecture successfully adapts to diverse layouts through its reinforcement learning feedback loops and maintains stability under conditions of LLM inference uncertainty.
>
> This work significantly impacts the field by shifting the paradigm from static, monolithic pipelines to dynamic, agentic systems capable of autonomous self-improvement. While the iterative nature of the RL-driven mechanism and reliance on LLM inference introduce higher computational costs and complexity compared to static fine-tuned models, the study establishes a viable pathway toward generalized document intelligence by successfully eliminating the "cold start" problem. The ability to parse varied document layouts without specific entity training reduces the dependency on manual annotation and lays the groundwork for highly scalable, resilient automation solutions in enterprise document processing environments.

***

## üîç Key Findings

*   The proposed agentic AI framework demonstrated promising performance on the SOIRE and CORD benchmark datasets.
*   The system is capable of handling diverse document types, file formats, layouts, and various Large Language Models (LLMs).
*   The framework successfully achieves automated, accurate information extraction without the need for human intervention.
*   The system maintains consistent performance and enables self-improving extraction capabilities, even under conditions of LLM inference uncertainty.

## üõ†Ô∏è Methodology

The researchers developed a modular, multi-agent framework centered around Large Language Model (LLM) agents. The core methodology involves:

*   **RL Driver Agent:** Utilizes a policy of rewards and penalties to guide the system.
*   **Meta-Prompting Agent:** Analyzes past errors to iteratively refine and improve "prompt-based actor agents."
*   **Dynamic Adaptation:** This architecture allows the system to adapt task-specific prompts dynamically, moving beyond static or monolithic pipeline approaches to enable self-correction.

## üß† Contributions

*   **Critique of Existing Architectures:** The work highlights the inherent limitations of using monolithic LLM-based extraction methods and traditional vision/OCR pipelines for document parsing.
*   **Novel Agentic Framework:** Introduction of a new agentic AI system that integrates LLM agents with a reinforcement learning driver to create a self-corrective loop.
*   **RL-Driven Optimization:** The contribution of a specific RL policy mechanism that guides a meta-prompting agent to learn from historical errors and optimize the prompts used by actor agents.
*   **Generalization and Automation:** Demonstration of a generalized system capable of parsing form-like documents (invoices, bills, etc.) across varied layouts and formats, aiming for full automation without human oversight.

## ‚öôÔ∏è Technical Details

The system implements a sophisticated multi-agent architecture designed for flexibility and self-improvement.

**System Architecture**
*   **Total Agents:** 7 distinct agents working in concert.
*   **Processing Agents:** Document classifier, schema builder, data extractor.
*   **Optimization Agents:** Two Gymnasium agents and one meta-prompting agent.
*   **Evaluator Agent:** Assesses system performance.

**Frameworks & Models**
*   **RL Framework:** Utilizes the Gymnasium framework for reinforcement learning with a natural language action space.
*   **LLM Backbone:**
    *   Default: **GPT-4o-mini**
    *   Alternative (for sensitive data): **LLaMA 3.3-70B**
*   **Response Format:** All model responses are constrained to **JSON** format.

**Technical Approach**
*   **Semantic Reasoning:** Leverages LLM semantic reasoning and planning instead of traditional CNNs/GNNs or RNNs.
*   **Dynamic Entity Handling:** Unlike previous transformer-based models (e.g., LayoutLM, BROS), it does not assume a predefined list of entities. This allows for better handling of unseen documents and dynamic layouts.

## üìà Results

The framework was evaluated on the CORD and ICDAR-SOIRE benchmarks as well as internal proprietary data.

*   **CORD Dataset:** Achieved a competitive **Entity F1 score of 96.1%**, matching specialized baselines without predefined entities.
*   **ICDAR-SOIRE Benchmark:** Validated via data similarity/matching and system confidence scores.
*   **Robustness:** The system demonstrated promising performance with **zero human intervention**.
*   **Adaptability:** Successfully adapted to diverse document types (invoices, receipts, bank statements) and layouts without prior entity definitions.
*   **Self-Improvement:** Achieved self-improving extraction capabilities through its reinforcement learning feedback loops.