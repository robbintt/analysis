---
title: 'Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert
  Merging'
arxiv_id: '2506.23266'
source_url: https://arxiv.org/abs/2506.23266
generated_at: '2026-02-06T05:10:28'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Sub-MoE: Efficient Mixture-of-Expert LLMs Compression via Subspace Expert Merging

*Lujun Li; Zhu Qiyuan; Jiacheng Wang; Wei Li; Hao Gu; Sirui Han; Yike Guo*

***

> ### ðŸ“Š Quick Facts: Key Metrics
>
> | Metric | Detail |
> | :--- | :--- |
> | **Primary Model** | Mixtral-8x7B (also tested on DeepSeek, Qwen) |
> | **Core Innovation** | Subspace Expert Merging (Joint SVD) & Adaptive Clustering |
> | **Performance Retention** | **96%** @ 25% expert reduction<br>**86%** @ 50% expert reduction |
> | **Challenge Addressed** | Low inter-expert similarity (0.1â€“0.3) and parameter conflicts |
> | **Compatibility** | Works with intra-expert compression (e.g., quantization) |
> | **Quality Score** | **9/10** |

***

## Executive Summary

Mixture-of-Experts (MoE) Large Language Models (LLMs) offer high performance but present significant deployment challenges due to their massive parameter counts and memory footprint. While compressing these models via expert merging or pruning is a logical solution, existing methods often fail because of "parameter conflicts." Specifically, experts within MoE layers typically exhibit low similarity (ranging between 0.1â€“0.3), meaning they represent divergent functions. Consequently, traditional merging approaches that do not account for this divergence result in a substantial degradation of model accuracy, creating a critical bottleneck for the efficient deployment of state-of-the-art MoE architectures like Mixtral and DeepSeek.

The paper introduces **Sub-MoE**, a novel compression framework that overcomes parameter conflicts through a two-phase strategy: **Adaptive Expert Clustering** and **Subspace Expert Merging**. First, the framework identifies functionally coherent experts using K-means clustering with k-means++ initialization, based on the cosine similarity of their outputs. Second, it addresses the divergence of expert weights by applying joint Singular Value Decomposition (SVD) to vertically concatenated expert weights. This process decouples the weights into shared U-matrices (representing a common subspace) and expert-specific V components. By merging these components within the shared subspace, Sub-MoE aligns the mathematical representations of distinct experts, allowing for effective fusion without the loss of precision inherent in standard merging techniques.

In extensive experiments across major MoE architectures (Mixtral, DeepSeek, and Qwen), Sub-MoE demonstrates superior performance compared to state-of-the-art methods such as SEER-MoE, NAEE, and MC-SMoE. On the Mixtral-8x7B model, the framework retained **96% of the original performance** while reducing the expert count by 25%, and maintained **86% performance** even with a 50% reduction in experts. These results are particularly significant given the low inter-expert similarity in these models, which renders standard merging ineffective. Furthermore, the study validates that Sub-MoE is compatible with intra-expert compression techniques like quantization, enabling further optimization.

Sub-MoE represents a paradigm shift in the compression of sparse LLMs, successfully resolving the trade-off between efficiency and performance that has limited the adoption of MoE models. By providing a theoretical and algorithmic foundation for merging divergent experts within a shared subspace, this work enables substantial reductions in model size and inference costs without proportional loss of zero-shot capabilities. The framework's ability to synergize with existing compression techniques suggests it will become a critical tool for deploying large-scale MoE systems in resource-constrained environments.

***

## Key Findings

*   **Mitigation of Parameter Conflicts**
    The study identifies parameter conflicts as a primary hindrance in expert merging. Sub-MoE overcomes this by extracting shared subspaces, allowing for the fusion of experts that would otherwise be incompatible due to divergent parameters.

*   **High Performance Retention**
    Sub-MoE demonstrates remarkable efficiency:
    *   Retains **96%** of original performance with a **25%** expert reduction.
    *   Retains **86%** of original performance with a **50%** expert reduction (tested on Mixtral-8x7B).

*   **Superiority Over Existing Methods**
    Extensive experiments on Mixtral, DeepSeek, and Qwen models demonstrate that Sub-MoE consistently outperforms state-of-the-art expert pruning and merging methods, including SEER-MoE, NAEE, MC-SMoE, and HC-SMoE.

*   **Synergistic Optimization**
    The framework is designed to be compatible with intra-expert compression techniques (such as quantization), allowing for stacked optimization strategies and further inference acceleration.

***

## Methodology

The Sub-MoE framework utilizes a two-phase compression strategy based on **Subspace Expert Merging**:

1.  **Phase 1: Adaptive Expert Clustering**
    *   Groups experts based on functional coherence.
    *   Utilizes **K-means clustering** and **k-means++ initialization**.
    *   Clustering is determined by the cosine similarity of expert outputs.
    *   Allows for non-uniform compression across different layers.

2.  **Phase 2: Subspace Expert Merging**
    *   Applies **joint Singular Value Decomposition (SVD)** to concatenated weights.
    *   Decouples weights into:
        *   **Shared U-matrices:** Representing a common subspace.
        *   **Expert-specific V components:** Unique to each expert.
    *   Components are merged based on frequency and reconstructed to form the final compressed model.

***

## Technical Details

The Sub-MoE framework proposes a two-stage compression pipeline specifically designed to mitigate parameter conflicts arising from merging experts with low similarity.

*   **Stage 1: Adaptive Expert Clustering**
    *   **Objective:** Identify optimal candidates for merging.
    *   **Mechanism:** Uses K-means++ initialization on K-means clustering.
    *   **Metric:** Cosine similarity of outputs is used to gauge functional coherence rather than just weight parameters.

*   **Stage 2: Subspace Expert Merging**
    *   **Problem Solved:** Parametric representation divergence.
    *   **Technique:** Joint SVD is applied to vertically concatenated expert weights.
    *   **Process:**
        1.  Establishes a shared subspace via singular value decomposition.
        2.  Right singular vectors are merged based on expert usage frequency.
        3.  Weights are reconstructed using the shared basis and singular values.
    *   **Context:** This approach is critical as standard merging fails when inter-expert similarity is low (0.1â€“0.3).

***

## Core Contributions

*   **Novel Compression Framework**
    Introduces Sub-MoE as a new paradigm for compressing Mixture-of-Experts (MoE) LLMs, directly addressing memory and deployment challenges inherent in sparse architectures.

*   **Theoretical Innovation**
    Proposes a new methodological approach that resolves the efficiency-performance trade-off by aligning and fusing experts in a shared subspace, mathematically bridging the gap between divergent expert functions.

*   **Algorithmic Design**
    Develops a specific workflow combining Adaptive Expert Clustering and Subspace Expert Merging to automate the identification and mathematical fusion of experts.

*   **Empirical Validation**
    Provides comprehensive benchmarking on major MoE architectures (Mixtral, DeepSeek, Qwen), proving that substantial expert reduction (up to 50%) is possible without proportional degradation of zero-shot capabilities.

***

## Results & Benchmarks

Experiments on the **Mixtral-8x7B** model highlight the efficacy of the Sub-MoE framework:

*   **Compression vs. Accuracy:**
    *   **25% Expert Reduction:** ~96% performance retention.
    *   **50% Expert Reduction:** ~86% performance retention.
*   **Low Similarity Handling:** Successfully addresses the issue of low inter-expert similarity (0.1â€“0.3), a context where standard merging methods typically fail.
*   **Comparative Performance:** Outperforms SEER-MoE, NAEE, MC-SMoE, and HC-SMoE across various benchmarks.
*   **Quantization Compatibility:** Verified that Sub-MoE can be combined with intra-expert compression techniques like quantization for additional gains.

***

## References & Quality

*   **References:** 40 citations
*   **Quality Score:** 9/10