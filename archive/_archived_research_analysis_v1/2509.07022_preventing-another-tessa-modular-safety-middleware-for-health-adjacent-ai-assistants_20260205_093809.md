---
title: 'Preventing Another Tessa: Modular Safety Middleware For Health-Adjacent AI
  Assistants'
arxiv_id: '2509.07022'
source_url: https://arxiv.org/abs/2509.07022
generated_at: '2026-02-05T09:38:09'
quality_score: 8
citation_count: 3
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Preventing Another Tessa: Modular Safety Middleware For Health-Adjacent AI Assistants

*Pavan Reddy; Nithin Reddy*

---

> ### ðŸ“Š Quick Facts
> 
> *   **Safety Metric:** 100% Recall (Malicious Prompt Interception)
> *   **Performance:** Baseline cost and latency maintained
> *   **Core Tech:** Python 3.11, Google Gemma 2 (2B), Hybrid Filtering
> *   **Governance:** Mapped to NIST 800-53 Rev 5 & OWASP LLM Top 10
> *   **Dataset:** 100 synthetic prompts (50 malicious, 50 safe)

---

## Executive Summary

This research addresses critical safety vulnerabilities in health-adjacent AI assistants, specifically examining the failure of the NEDA "Tessa" chatbot, which provided harmful eating disorder advice to vulnerable users. The study highlights that existing safety infrastructures are often insufficient for high-stakes environments, lacking the specific "last mile" mechanisms required to intercept harmful content before it reaches users. By analyzing the Tessa incident, the authors identify the need for a rigorous filtering mechanism that can autonomously prevent the generation of physically or mentally harmful advice, bridging the gap between general AI safety and specific health-risk mitigation.

The authors propose a lightweight, modular safety middleware utilizing a hybrid system design to filter unsafe inputs and outputs. Technically, the architecture combines deterministic lexical gatesâ€”employing regex and keyword matching for threats like eating disorder terminology and slangâ€”with probabilistic model-based adjudication to calculate risk scores. The key innovation is a **"Single-Call Escalation"** pathway, which integrates the rule-based filter and the LLM judge into a unified process; this allows for immediate contextual analysis without the sequential API overhead typical of traditional multi-stage pipelines. Running on Python 3.11 with Google Gemma 2 (2B), the system enforces fail-closed verdicts and refusal-first secure prompting to ensure safety.

The system was evaluated against a synthetic benchmark of 100 prompts (50 malicious, 50 safe) generated by GPT-4o-mini. While the dataset size is limited to this specific synthetic set, the middleware achieved perfect interception capabilities, recording **100% recall** in blocking malicious prompts. Crucially, this high level of safety was achieved without operational penalties; the system demonstrated baseline cost and latency performance comparable to traditional approaches, successfully avoiding the computational overhead and latency accumulation often associated with complex, multi-stage filtering layers.

This research offers significant practical value by providing a concrete, deployable technical solution to the specific failures observed in the Tessa incident. It introduces a shift in safety engineering paradigms, advocating for explicit, lightweight "last mile" checks over complex, heavy infrastructure. Furthermore, by mapping the system's technical failure modes to industry governance frameworksâ€”specifically aligning with NIST 800-53 Rev 5 controls and OWASP LLM Top 10 risksâ€”the paper establishes a template for integrating compliance-driven safety into AI deployments, potentially setting a new standard for safety in health-adjacent applications.

---

## Key Findings

*   **Perfect Interception:** The proposed hybrid safety middleware achieved **100% recall** during synthetic evaluations, effectively preventing incidents similar to the NEDA Tessa failure.
*   **Zero Operational Penalty:** The system operates at baseline cost and latency, significantly outperforming traditional multi-stage pipelines which often introduce lag.
*   **Last-Mile Safety:** Robust safety in health-adjacent AI is best achieved through lightweight, explicit "last mile" checks rather than relying solely on upstream model alignment.
*   **Governance Integration:** Mapping failure patterns to frameworks like **OWASP** and **NIST** effectively bridges technical safeguards with governance controls.

---

## Methodology

The authors employed a multi-faceted approach to design and validate the safety middleware:

*   **Case Study Analysis:** Conducted a deep dive into the NEDA 'Tessa' chatbot failure to identify specific risk vectors and systemic shortcomings.
*   **Hybrid System Design:** Proposed and implemented a system combining deterministic lexical gates with an in-line LLM policy filter.
*   **Fail-Closed Architecture:** The system employs fail-closed verdicts and single-call escalation pathways to ensure safety is the default state.
*   **Synthetic Benchmarking:** Performance was evaluated using synthetic benchmarks specifically designed to measure interception rates, operational cost, and latency against traditional pipeline architectures.

---

## Technical Details

### System Architecture
The middleware is built on a modular design that integrates two distinct filtering layers:

*   **Deterministic Filtering (Lexical Gates):**
    *   Utilizes Regex and Keyword matching.
    *   Targets specific high-risk topics (dieting, eating disorders).
    *   Supports numeric pattern recognition.
    *   Includes support for slang and leetspeak to evade prompt injection.
*   **Probabilistic Filtering (Model-Based Adjudication):**
    *   Uses an in-line LLM to calculate risk scores.
    *   Provides context-aware judgment that simple regex might miss.

### Safety Protocols
*   **Secure Prompting:** Enforces refusal-first behavior.
*   **Output Moderation:** Continual monitoring of generated outputs.
*   **Verdict System:** Single-Call JSON Verdict for efficiency.

### Tech Stack
*   **Language:** Python 3.11
*   **Backbone Model:** Google Gemma 2 (2B)
*   **Judge Component:** Google Gemma 2 (2B)
*   **Dataset Generation:** GPT-4o-mini

### Deployment Patterns Evaluated
1.  Baseline
2.  Secure-Prompting-Only
3.  Input Filtering
4.  Input Judge
5.  Output Judge
6.  Single-Call JSON Verdict (Proposed Solution)

---

## Results

The evaluation focused on a dataset of **100 synthetic prompts** (50 malicious, 50 safe).

*   **Interception Efficacy:** The system achieved **100% recall** for blocking malicious prompts.
*   **Operational Efficiency:** Metrics indicate the system runs at baseline cost and latency.
*   **Compliance Alignment:** The architecture successfully maps technical failures to **NIST 800-53 Rev 5** controls and **OWASP LLM Top 10** risks.

---

## Contributions

*   **Modular Safeguard Architecture:** Introduces a lightweight architecture that integrates deterministic and probabilistic filtering in a single pass.
*   **Paradigm Shift:** Proposes a safety engineering paradigm favoring 'last mile' checks over complex, heavy infrastructure.
*   **Governance Bridge:** Provides a comprehensive mapping of AI failure modes to industry standards (OWASP, NIST) for better governance integration.
*   **Practical Solution:** Offers a direct technical solution to address the specific failures observed in the Tessa incident.

---

### Document Metadata

**Quality Score:** 8/10 `[â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘]`
**References:** 3 citations