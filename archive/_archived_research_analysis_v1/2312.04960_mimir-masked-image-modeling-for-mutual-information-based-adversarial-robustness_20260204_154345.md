---
title: 'MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness'
arxiv_id: '2312.0496'
source_url: https://arxiv.org/abs/2312.04960
generated_at: '2026-02-04T15:43:45'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness

*Xiaoyun Xu; Shujian Yu; Zhuoran Liu; Stjepan Picek*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations
> *   **Key Datasets:** ImageNet-1K, CIFAR-10, Tiny-ImageNet, ImageNet-C
> *   **Architectures:** ViT, Swin, ConvNeXt
> *   **Efficiency Gain:** 75% patch discard; ViT-B training reduced from **451.39h** to **123.76h**
> *   **Benchmark:** $L_2 \epsilon=4/255$ (RobustBench protocols)

---

## Executive Summary

**Problem**
Vision Transformers (ViTs) have become a dominant architecture in computer vision, yet they face significant security challenges from evasion attacks. This paper addresses the critical incompatibility between state-of-the-art Adversarial Training (AT) methodsâ€”such as Generalist and DBATâ€”and the ViT architecture. The authors demonstrate that when these standard AT methods are applied directly to ViTs, they fail to provide effective defense. This gap is problematic because it leaves widely deployed transformer models vulnerable to adversarial perturbations, necessitating a defense mechanism specifically tailored to the unique structural and representational dynamics of ViTs.

**Innovation**
The key innovation is **MIMIR** (Masked Image Modeling for Mutual Information-based Adversarial Robustness), a self-supervised framework grounded in information theory. The authors derive theoretical bounds proving that the Mutual Information (MI) between an adversarial example and its latent representation within a ViT must be constrained to ensure robustness. MIMIR implements this via an Encoder-Decoder structure that processes adversarial inputs to reconstruct natural images. The method integrates an MI penalty based on the Information Bottleneck principle during training, minimizing the information shared between the adversarial input and the latent representation. Additionally, MIMIR enhances computational efficiency by discarding 75% of image patches, utilizing a pre-training and fine-tuning paradigm adaptable to various ViT variants.

**Results**
MIMIR achieves state-of-the-art performance on ImageNet-1K, outperforming standard baselines like PGD and APGD AT on both ViT-S and ViT-B architectures under RobustBench protocols ($L_2 \epsilon=4/255$). In tests on CIFAR-10, the method demonstrated approximately 59â€“60% robust accuracy. Beyond standard metrics, MIMIR exhibited improved resilience against unforeseen attacks and common corruptions (ImageNet-C). Crucially, the method offers significant efficiency gains, reducing the wall-clock training time for ViT-B from 451.39 hours to 123.76 hours. These results were validated even in white-box scenarios where the adversary possessed full knowledge of the defense.

**Impact**
This research significantly advances the field by bridging a theoretical gap between information theory and adversarial robustness in transformers. It provides the first systematic investigation into why modern evasion attacks threaten ViTs and establishes a necessary theoretical link between MI bounds and robustness. By introducing a scalable, theoretically grounded solution that withstands adaptive attacks and common corruptions, the authors offer a viable path forward for securing next-generation vision systems. The public release of code and models further ensures that these findings can serve as a foundation for future research in robust self-supervised learning.

---

## Key Findings

*   **AT Incompatibility:** State-of-the-art AT methods (Generalist, DBAT) exhibit significant incompatibilities when applied directly to Vision Transformers (ViTs).
*   **Theoretical Bounds:** Theoretical analysis reveals that Mutual Information (MI) between an adversarial example and its latent representation within ViT-based autoencoders must be constrained via derived bounds to ensure robustness.
*   **SOTA Performance:** MIMIR outperforms SOTA AT results on ImageNet-1K and demonstrates improved resilience against unforeseen attacks and common data corruption.
*   **White-Box Resilience:** The proposed method maintains robustness even in white-box scenarios where the adversary possesses full knowledge of the defense.

---

## Methodology

The authors utilize an information-theoretic approach, specifically analyzing Mutual Information (MI) within the context of autoencoder-based self-supervised pre-training for ViTs.

The proposed **MIMIR** method employs a specific MI penalty during the training process and utilizes self-supervised Adversarial Training (AT) via masked image modeling integrated with autoencoders to facilitate adversarial pre-training.

---

## Technical Details

*   **Core Structure:** Converts Masked Image Modeling into adversarial pre-training using an Encoder-Decoder structure.
*   **Process Flow:** Processes adversarial inputs and reconstructs natural images.
*   **Theoretical Principle:** Relies on the Information Bottleneck principle by minimizing Mutual Information between adversarial examples and representations.
*   **Optimization:** Enhances efficiency by discarding **75%** of patches.
*   **Training Paradigm:** Utilizes a Pre-training + Fine-tuning approach.
*   **Architecture Adaptations:**
    *   Standard ViT
    *   Swin (via Group Window Attention)
    *   ConvNeXt (via SparK)

---

## Results

*   **ImageNet-1K:** Achieved state-of-the-art results, outperforming PGD/APGD AT on ViT-S and ViT-B using RobustBench protocols ($L_2 \epsilon=4/255$).
*   **Resilience:** Showed improved resilience against unforeseen attacks and ImageNet-C corruption.
*   **Efficiency Metrics:** Significant wall-clock time reduction.
    *   *Example:* ViT-B training time reduced to **123.76h** (vs. 451.39h baseline).
*   **CIFAR-10:** Results showed ~52-53% natural accuracy and ~59-60% robust accuracy.

---

## Contributions

*   **Systematic Investigation:** Provides a detailed systematic investigation of why modern evasion attacks threaten ViTs and why existing AT solutions fail to transfer effectively to this architecture.
*   **Theoretical Link:** Establishes a theoretical link between adversarial robustness and Mutual Information in ViTs, defining necessary bounds on the MI between adversarial examples and their latent representations.
*   **Method Introduction:** Introduces **MIMIR**, a new self-supervised AT method that integrates masked image modeling with MI penalties, achieving SOTA results on standard benchmarks (CIFAR-10, Tiny-ImageNet, ImageNet-1K).
*   **Validation & Reproducibility:** Validates performance against adaptive attacks and common corruptions, with the release of code and models for public reproducibility.