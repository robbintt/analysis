# SequenceLayers: Sequence Processing and Streaming Neural Networks Made Easy

*RJ Skerry-Ryan; Julian Salazar; Soroosh Mariooryad; David Kao; Daisy Stanton; Eric Battenberg; Matt Shannon; Ron J. Weiss; Robin Scheibler; Jonas Rothfuss; Tom Bagby*

***

### **Quick Facts**

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 10 Citations |
| **Framework Support** | JAX, TensorFlow 2 |
| **Core Innovation** | Unified Execution Model (Training & Inference) |
| **Primary Benefit** | Eliminates architectural divergence via strict state management |

***

## Executive Summary

Deploying sequence models such as Transformers, RNNs, and Convolutional networks into production environments presents a significant engineering challenge due to the divergence between training and inference requirements. Research typically relies on parallel layer-wise execution for efficiency, while real-world applications demand streaming, step-by-step inference to handle infinite-length inputs with minimal latency. Traditional approaches often require maintaining two separate codebases or architectural variants—one for training and one for streaming—which inevitably leads to bugs, inconsistencies, and difficulties in moving models from research to production. This paper addresses the critical need for a unified execution model that guarantees mathematical equivalence between training and streaming modes without architectural compromise.

The core innovation is the "**SequenceLayers**" library, which introduces a strict API contract enforced through a unified `SequenceLayer` base class and `Sequence` data objects (differentiable pytrees encapsulating values and masks). The methodology mandates explicit state representation—such as Transformer KV caches or RNN hidden states—ensuring that the internal state evolution is rigorously defined and managed by the layer itself rather than being implicit. The library provides a composable, declarative API consisting of `layer` methods for parallel processing and `step` methods for incremental state updates. Supported by combinators like `Serial`, `Parallel`, and `Residual`, the framework utilizes static constraints (e.g., `output_ratio`, `block_size`) to facilitate graph compilation while ensuring that the step-wise evolution of states remains mathematically equivalent to traditional layer-wise invocation.

Rather than relying on standard accuracy or latency benchmarks, the paper emphasizes verification metrics and structural correctness testing to validate the framework's robustness. Unit tests confirm that the `layer` (parallel) and `step` (streaming) execution paths produce identical outputs and gradients, effectively eliminating functional divergence. Specific verification examples demonstrate predictable system behavior: stacking Conv1D layers with strides of 2 and 3 correctly results in a total decimation factor of 6. Furthermore, the library's ability to calculate receptive fields was validated with specific metrics, showing a causal Conv1D (kernel=5) with a field of (-4, 0), a same Conv1D (kernel=5) with (-2, 2), a 4-layer stack at (-8, 8), and an LSTM extending to (-inf, 0).

The significance of this work lies in providing a structural solution to the persistent problem of streaming inconsistency in sequence modeling. By enforcing a strict contract for state management, SequenceLayers significantly reduces the prevalence of bugs related to padding, masking, and cache updating, thereby accelerating the transition from research prototypes to production-grade systems. The release of production-ready implementations in both JAX and TensorFlow 2 offers a library-agnostic toolset that decouples model definitions from execution modes. This advancement lowers the barrier to entry for deploying robust, streamable neural networks, ensuring that complex sequence models can be trained in parallel and deployed in streaming environments with guaranteed correctness.

***

## Key Findings

*   **Unified Execution Model:** The library enables sequence models to run both layer-by-layer (for training) and step-by-step (for inference) without architectural divergence.
*   **Explicit State Representation:** Requires explicit management of internal state (e.g., Transformer KV caches, RNN hidden states) to ensure consistency across modes.
*   **Mathematical Equivalence:** The methodology guarantees that step-wise state evolution is mathematically equivalent to traditional layer-wise invocation.
*   **Streamability:** Facilitates streaming capabilities and significantly reduces the prevalence of implementation bugs.
*   **Library Agnostic:** The API is designed to be framework-agnostic, with current implementations available for JAX and TensorFlow 2.

***

## Methodology

The approach relies on the **'SequenceLayers contract'**, which mandates that neural network layers explicitly manage their internal state. This contrasts with standard stateless forward passes.

*   **State Definition & Step Methods:** Layers must implement a specific state definition and a `step` method for incremental state evolution.
*   **Declarative API:** A composable API allows users to construct production-scale models using layers and combinators while ensuring step-by-step execution remains strictly equivalent to layer-wise execution.
*   **Correctness by Construction:** The framework enforces a strict contract to ensure that padding, masking, and state updates do not diverge between training and inference.

***

## Technical Details

### Core Components
*   **`Sequence` Objects:** Differentiable pytree dataclasses that encapsulate:
    *   `values`: Array data.
    *   `mask`: Boolean validity indicators.
*   **`SequenceLayer` Class:** Defines a standardized API with three critical methods:
    *   `layer()`: For parallel processing.
    *   `get_initial_state()`: For state initialization.
    *   `step()`: For step-wise streaming.

### Graph Compilation & Constraints
To support compiled graphs, the library relies on static constraints:
*   **`output_ratio`**: Fractions indicating down-sampling or up-sampling factors.
*   **`block_size`**: Defined block sizes for processing.

### Enforcement & Composability
*   **Strict Contract:** The library enforces equivalence, padding invariance, and correct masking.
*   **Combinators:** High-level components for model construction include:
    *   `Serial`
    *   `Parallel`
    *   `Residual`
    *   `Repeat` (utilizes `jax.lax.scan`)

***

## Results & Verification

The provided text focuses on verification metrics and internal correctness proofs rather than standard experimental benchmarks (e.g., accuracy/latency).

*   **Unit Testing:** Rigorous unit tests verify that `layer` and `step` executions produce **identical outputs and gradients**.
*   **Decimation Verification:** Stacking Conv1D layers with stride 2 and 3 yields a correct decimation factor of 6 with predictable output shapes.
*   **Receptive Field Calculations:** Validated against specific configurations:

| Configuration | Receptive Field |
| :--- | :--- |
| **Conv1D** (Kernel=5, Causal) | (-4, 0) |
| **Conv1D** (Kernel=5, Same) | (-2, 2) |
| **Stack of 4 Conv1D Layers** | (-8, 8) |
| **LSTM** | (-inf, 0) |

***

## Contributions

1.  **Unified API & Library:** A new library that simplifies creating sequence models compatible with both parallel training and streaming inference.
2.  **Strict State Contract:** Introduction of a rigorous contract enforcing explicit state management to guarantee correctness.
3.  **Structural Stability:** A structural solution that mitigates sequence processing bugs and aids the transition from research to production.
4.  **Open Source Release:** The release of comprehensive, production-ready implementations in JAX and TensorFlow 2.