---
title: 'Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in
  Latent Space'
arxiv_id: '2505.13308'
source_url: https://arxiv.org/abs/2505.13308
generated_at: '2026-01-27T16:44:02'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Seek in the Dark: Reasoning via Test-Time Instance-Level Policy Gradient in Latent Space

*Yuxuan Wang, Chenxi Li, Tong Wu, Xuekai Zhu, Eric Hanchen, Zhaoxin Yu, Hengli Li*

---

> ### **Quick Facts**
> | Metric | Detail |
> | :--- | :--- |
> | **Quality Score** | 7/10 |
> | **Citations** | 40 References |
> | **Key Innovation** | Test-Time Instance-Level Policy Gradient |
> | **Optimization Space** | Continuous Latent Space (vs. Discrete Text) |
> | **Core Benefit** | Avoids sparse reward; no fine-tuning required |

---

## Executive Summary

Operating reasoning processes within the discrete token space of Large Language Models (LLMs) presents a fundamental limitation known as the "sparse reward" problem, where intermediate steps in text generation lack effective feedback signals. Standard reasoning approaches, such as Chain-of-Thought (CoT) prompting or static sampling, struggle to adapt dynamically to specific input instances without incurring prohibitive computational costs or requiring expensive model fine-tuning. This creates a critical gap between the need for high-accuracy reasoning and the constraints of efficient inference, particularly in mathematical and logical domains where precise, step-by-step deduction is essential.

The paper introduces **Seek**, a novel framework that shifts reasoning from discrete text decoding to **continuous latent space exploration** through Test-Time Instance-Level Adaptation (TTIA). Instead of generating explicit intermediate tokens, the method treats reasoning as a trajectory search within the model's hidden states. Technically, the framework utilizes a policy gradient algorithm to update latent vectors directly according to an objective function, optimizing the distribution over next-step latent vectors to maximize reward while keeping model weights frozen.

**Seek** demonstrates substantial performance improvements over strong baselines, including Chain-of-Thought and Best-of-N (BoN) sampling, across several rigorous benchmarks.

*   **GSM8K:** 80.21% accuracy for Qwen2-7B (+8.11 vs. BoN) and 77.18% for LLaMA3.1-8B (+10.45 vs. BoN).
*   **MATH-500:** Qwen2.5-14B reached 71.00% (+7.80 vs. BoN).
*   **AIME2024:** Qwen3-4B achieved 73.3% (+10.0 vs. baselines).

Crucially, regarding computational efficiency, **Seek** doubled the performance of BoN in general comparisons for Qwen2.5-7B (13.33% vs. 6.67%) and consistently outperformed BoN even when the baseline utilized significantly higher computational budgets. This research proposes a paradigm shift in test-time computation, validating that instance-level policy gradients in latent space can effectively refine reasoning trajectories without model fine-tuning.

---

## Key Findings

*   **Superior Latent Navigation:** Operating reasoning processes in the continuous latent space of LLMs is more effective than searching in discrete text space as it successfully avoids the 'sparse reward' problem.
*   **Inference-Time Optimization:** The proposed method achieves significant performance improvements by applying policy gradient methods at inference time, eliminating the need to update pre-trained weights.
*   **Dynamic Adaptation:** The approach dynamically adapts reasoning paths to specific input instances, outperforming static prompting and strong baselines like Chain-of-Thought and Best-of-N sampling.

---

## Methodology

The framework designated as **'Seek'** fundamentally changes how models approach reasoning tasks. Instead of generating explicit intermediate text steps, it navigates the hidden states (latent representations) of the LLM, treating reasoning as a trajectory search within the continuous embedding space.

*   **Reinforcement Learning at Inference:** It utilizes a reinforcement learning policy gradient algorithm optimized at test time.
*   **Reward Maximization:** The system learns a distribution over potential next-step latent vectors by maximizing the reward (specifically, correctness) for the specific input instance.
*   **Search Strategy:** The strategy likely integrates a tree-search-like mechanism, allowing the model to explore or backtrack different latent reasoning paths efficiently.

---

## Technical Details

**LATENTSEEK** utilizes **Test-Time Instance-Level Adaptation (TTIA)** to operate within the LLM's latent space rather than the discrete token space.

### Algorithm & Objective
*   **Mechanism:** Employs a policy gradient algorithm to maximize expected rewards by updating latent representations directly without modifying model weights.
*   **Objective Function:**
    $$J(z) = \mathbb{E}_{x \sim \pi(x|z,c)} [R(x,c)]$$
*   **Update Rule:**
    $$[\nabla_z J(z)]_t = \mathbb{E}_{x \sim \pi(x|z,c)} [R(x,c) \nabla_{z_t} \log \pi(x_t | z_t)]$$

### Theoretical Foundation & Initialization
*   **Initialization:** Latent vectors are initialized via Chain-of-Thought prompts to provide a starting point for the optimization.
*   **MIP Connection:** Theoretically, the method relates to Multi-Prover Interactive Proofs (MIP). It defines '**MIP-Bounded**' a scenario where optimized tokens act as non-communicating provers and autoregressive generation acts as a verifier.

---

## Results

**Seek** was evaluated against Chain-of-Thought (CoT) and Best-of-N (BoN) on GSM8K, MATH-500, and AIME2024 using popular models including Qwen2, Qwen2.5, LLaMA3.1, and Qwen3.

**Performance Highlights:**

| Dataset | Model | Seek Accuracy | Improvement vs BoN |
| :--- | :--- | :--- | :--- |
| **GSM8K** | Qwen2-7B | 80.21% | **+8.11%** |
| **GSM8K** | LLaMA3.1-8B | 77.18% | **+10.45%** |
| **MATH-500** | Qwen2.5-14B | 71.00% | **+7.80%** |
| **AIME2024** | Qwen3-4B | 73.3% | **+10.0%** |

**General Efficiency Comparisons:**
*   **Qwen2.5-7B:** Seek doubled BoN performance (**13.33% vs 6.67%**).
*   **Convergence:** The method converges in few iterations.
*   **Compute Efficiency:** Outperforms BoN even when BoN uses significantly higher compute budgets.

---

## Contributions

1.  **Paradigm Shift:** Introduces a new paradigm that shifts the focus of test-time computation from text decoding to latent space exploration.
2.  **Novel Approach:** Presents the first instance-level policy gradient approach designed for refining reasoning trajectories in the latent space of frozen LLMs.
3.  **Bridging the Gap:** Bridges the gap between cheap inference and expensive large-scale search by offering a computationally efficient method to improve reasoning accuracy without requiring model fine-tuning.