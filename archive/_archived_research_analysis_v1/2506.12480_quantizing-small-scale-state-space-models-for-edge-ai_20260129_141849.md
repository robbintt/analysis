# Quantizing Small-Scale State-Space Models for Edge AI

*Leo Zhao; Tristan Torchet; Melika Payvand; Laura Kriener; Filippo Moro*

---

## Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Architecture** | S4D (State-Space Models with Diagonal State Matrix) |
| **Key Innovation** | Heterogeneous Quantization & QAT for SSMs |
| **Accuracy Recovery** | 96% on Sequential MNIST (8-bit) |
| **Memory Reduction** | 6x reduction in footprint |
| **Training Speed** | QAT convergence within 10 epochs |

---

## Executive Summary

### Problem
Deploying advanced deep learning architectures like **State-Space Models (SSMs)** on resource-constrained edge hardware is becoming increasingly critical but presents significant challenges regarding memory efficiency. While quantization is the standard technique for reducing model footprint, this paper identifies a critical vulnerability in small-scale SSMs, specifically the **S4D architecture**. The research demonstrates that these models are highly sensitive to Post-Training Quantization (PTQ); specifically, the quantization of the state matrix and internal states leads to substantial performance degradation and instability. This sensitivity has previously hindered the practical adoption of SSMs on edge devices.

### Innovation
To address these instability issues, the authors pivot from standard PTQ to **Quantization-Aware Training (QAT)** and introduce a novel **heterogeneous quantization framework**. Technically, the study utilizes S4D models in recurrent mode and proposes clipping internal states within bounded ranges to maintain stability during the quantization process. The key innovation lies in the heterogeneous strategy, which dynamically assigns different bit-precisions to various components: standard parameters ($\bar{B}, \bar{C}, \bar{D}$) are quantized to 4 bits and activations to 6 bits, while the state matrix ($\bar{A}$) and internal state ($x$) are treated with independent quantization for their real and imaginary parts.

### Results
Experimental results on the **Sequential MNIST** benchmark highlight the efficacy of the proposed approach over baselines. While PTQ consistently fails at precisions lower than 8 bits regardless of model width, the QAT-based approach successfully recovers accuracy to **96%** at 8-bit precision, converging within just 10 epochs. The research further establishes that the "Frozen $\bar{A}$" parameterization is superior for low-precision state matrices. Crucially, by implementing the heterogeneous quantization scheme, the authors achieved a **6x reduction in memory footprint** without any loss in task performance.

### Impact
This paper significantly advances the field of Edge AI by providing the first comprehensive diagnostic analysis of SSM quantization vulnerabilities. By validating QAT as a necessary mechanism for accurate, low-precision SSM deployment and introducing a scalable heterogeneous quantization framework, the authors resolve a major bottleneck in bringing efficient sequence modeling to edge devices. This work enables the practical use of small-scale SSMs in resource-constrained environments.

---

## Key Findings

*   **Sensitivity Identification:** The state matrix ($\bar{A}$) and internal state ($x$) within the S4D architecture are highly sensitive to Post-Training Quantization (PTQ), leading to significant performance degradation.
*   **QAT Efficacy:** Quantization-Aware Training (QAT) significantly mitigates accuracy loss, recovering performance to **96%** on sequential MNIST at 8-bit precision.
*   **Sub-8-bit Viability:** QAT enables the successful use of **sub-8-bit precisions** for small-scale SSMs, which was previously not possible with PTQ.
*   **Memory Optimization:** A proposed heterogeneous quantization strategy achieves a **6x reduction in memory footprint** without sacrificing task performance.

---

## Methodology

The research follows a structured comparative progression using the S4D architecture as a testbed:

1.  **Baseline Analysis:** Implementation of Post-Training Quantization (PTQ) to analyze the impact and identify failure points in small-scale SSMs.
2.  **Mitigation Phase:** Application of Quantization-Aware Training (QAT) to recover performance and evaluate different parameterization schemes.
3.  **Validation:** Implementation and validation of a heterogeneous quantization strategy that dynamically assigns precision levels across different model components to optimize memory usage.

---

## Technical Details

The analysis focuses on S4D (State-Space Models with Diagonal State Matrix) models operating in recurrent mode.

*   **Stability Mechanism:** To mitigate instability, internal state $x$ is clipped within a bounded range at each time step during both PTQ and QAT.
*   **Heterogeneous Quantization Scheme:**
    *   **Parameters $\bar{B}, \bar{C}, \bar{D}$:** Quantized to 4 bits.
    *   **Activations:** Quantized to 6 bits.
    *   **State Matrix ($\bar{A}$) & State ($x$):** Variable bit-precision; real and imaginary parts are quantized independently.
*   **Granularity & Schemes:** Comparison of per-tensor vs. per-head granularity and symmetric vs. asymmetric quantization schemes.
*   **QAT Parameterizations:** Three distinct parameterizations of $\bar{A}$ were explored:
    *   **Discrete**
    *   **Continuous**
    *   **Frozen $\bar{A}$**

---

## Results

*   **PTQ Limitations:** PTQ requires a minimum of 8 bits for $\bar{A}$ and $x$; lower precision results in failure. Increasing model width does not enable more aggressive PTQ.
*   **Granularity:** Per-head quantization outperforms per-tensor quantization at 8-bit precision.
*   **Training Efficiency:** QAT recovers 96% accuracy on Sequential MNIST at 8-bit, converging within **10 epochs**.
*   **Parameterization Performance:**
    *   **Low Precision:** The "Frozen $\bar{A}$" parameterization is most effective for low-precision state matrices.
    *   **High Precision:** The "Continuous" parameterization performs best when $\bar{A}$ and $x$ are high precision.
*   **Quantization Type:** Asymmetric quantization yields slightly better accuracy compared to symmetric schemes.

---

## Contributions

*   **Diagnostic Analysis:** Provides a comprehensive analysis of SSM quantization, isolating the state matrix and internal state as primary vulnerabilities during PTQ.
*   **Validation:** Validates QAT as a necessary component for deploying accurate, low-precision SSMs on edge hardware.
*   **Framework Introduction:** Introduces a heterogeneous quantization framework that significantly reduces memory requirements for resource-constrained environments without performance loss.

---

**REFERENCES:** 30 citations
**QUALITY SCORE:** 8/10