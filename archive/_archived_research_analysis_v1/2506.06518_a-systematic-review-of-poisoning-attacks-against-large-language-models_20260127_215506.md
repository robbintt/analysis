---
title: A Systematic Review of Poisoning Attacks Against Large Language Models
arxiv_id: '2506.06518'
source_url: https://arxiv.org/abs/2506.06518
generated_at: '2026-01-27T21:55:06'
quality_score: 8
citation_count: 14
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# A Systematic Review of Poisoning Attacks Against Large Language Models

*Systematic Review, University Applied, Language Models, Against Large, Physics Laboratory, Johns Hopkins, Poisoning Attacks*

---

### ðŸ“Š Quick Facts

| Metric | Value |
| :--- | :--- |
| **Papers Reviewed** | 65 |
| **Top Research Focus** | Stealthiness (39 papers), Persistence (35 papers) |
| **Identified Gaps** | Clean Label Stealthiness (15 papers), Persistence to New Tasks (20 papers) |
| **Quality Score** | 8/10 |
| **References** | 14 Citations |

---

## Executive Summary

### The Problem
As Large Language Models (LLMs) integrate into critical infrastructure, their susceptibility to poisoning attacks has become a severe security risk. These attacks manipulate training data or model parameters to embed hidden behaviorsâ€”such as backdoors or biased outputsâ€”that are notoriously difficult to detect. Currently, the research landscape is fragmented and lacks a unified language, hindering security teams' ability to compare threats, evaluate defenses, and understand the full scope of vulnerabilities.

### The Innovation
This paper establishes the first rigorous framework to systematically categorize and assess LLM poisoning risks. The authors introduce a unified threat model and taxonomy that moves beyond abstract mathematics to practical definitions:

*   **Unified Framework:** A structure providing a common vocabulary for the industry to describe and identify **34 distinct attack traits**.
*   **Four Practical Dimensions:** Attacks are classified by:
    1.  **Poison Set:** The specific data targeted.
    2.  **Trigger Function:** The input that activates the attack (e.g., specific keywords).
    3.  **Poison Behavior:** The malicious output.
    4.  **Deployment:** The method used (Data vs. Model Poisoning).

### The Results
A systematic review of 65 papers reveals significant imbalances in research focus:

*   **Over-Researched:** The majority of work focuses on **Stealthiness** (39 papers) and **Persistence against standard defenses** (35 papers).
*   **Under-Researched:** Critical vulnerabilities are being neglected, specifically:
    *   **Clean Label Stealthiness:** Where poisoned data appears legitimate to human reviewers (only 15 papers).
    *   **Persistence to New Tasks:** Ensuring a backdoor survives model fine-tuning (only 20 papers).

To address evaluation inconsistencies, the paper proposes standardized metrics, notably advocating for **"Clean Performance" (CPM)** to replace Clean Accuracy, offering a more accurate measure of model functionality on non-classification tasks post-attack.

### The Impact
This research transforms the study of LLM poisoning from a collection of disparate exploits into a disciplined defensive strategy. By identifying the lack of research in clean-label and task-persistence attacks, the paper highlights a dangerous blind spot: organizations may unknowingly fine-tune poisoned models on new tasks, inadvertently activating dormant backdoors in production. The standardized taxonomy and metrics empower developers to rigorously test threats, moving beyond simple accuracy to evaluate the trade-offs between utility and security.

---

## Technical Framework

### Mathematical Threat Model
The paper proposes a comprehensive LLM Poisoning Threat Model featuring a rigorous mathematical framework to define attack components:

*   **Input Space:** $X$
*   **Output Space:** $Y$
*   **Core Functions:**
    *   **Trigger Function ($T$):** Defines the activation mechanism.
    *   **Label Changing Function ($L$):** Alters the intended output.
    *   **Poisoning Operation:** formally defined as $P(x,y) = (T(x), L(y))$.

### Taxonomy & Classification
The framework categorizes attacks into four primary specifications and identifies 34 distinct traits across metrics and specifications:

1.  **Poison Set**
    *   Concrete vs. Meta-Function
2.  **Trigger Function**
    *   Mechanism of activation
3.  **Poison Behavior**
    *   The resultant malicious action
4.  **Deployment**
    *   Model Poisoning vs. Data Poisoning

---

## Analysis Results

### Research Landscape
Based on the systematic review, the community's focus is heavily skewed toward visibility and standard defense evasion, rather than stealth in data provenance or long-term persistence through model updates.

### Standardized Metrics
The paper establishes formal definitions for the following evaluation metrics to ensure consistent future research:

*   **Attack Success Rate (ASR):** Measured for both targeted and untargeted attacks.
*   **Clean Performance (CPM):** Introduced to replace Clean Accuracy, specifically designed to measure functionality on non-classification tasks.
*   **Efficiency:** Calculated as the ratio of poison rate to success.
*   **Persistence:** The longevity of the attack against mitigation efforts.
*   **Stealthiness:** Categorized into Input Stealthiness and Model Stealthiness.
*   **Clean Label Requirements:** Criteria for poisoned data to appear legitimate.

---

*Note: Key findings, methodology, and specific contributions were not explicitly extracted from the abstract section of the source text. The report above synthesizes the available technical details and executive summary.*