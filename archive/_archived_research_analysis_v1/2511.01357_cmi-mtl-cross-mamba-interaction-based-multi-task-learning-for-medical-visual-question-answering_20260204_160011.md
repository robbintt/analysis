---
title: 'CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual
  question answering'
arxiv_id: '2511.01357'
source_url: https://arxiv.org/abs/2511.01357
generated_at: '2026-02-04T16:00:11'
quality_score: 8
citation_count: 17
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# CMI-MTL: Cross-Mamba interaction based multi-task learning for medical visual question answering

*Qiangguo Jin; Xianyao Zheng; Hui Cui; Changming Sun; Yuqi Fang; Cong Cong; Ran Su; Leyi Wei; Ping Xuan; Junbo Wang*

---

### ðŸ“‹ Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Framework** | CMI-MTL (Cross-Mamba interaction based Multi-Task Learning) |
| **Performance** | State-of-the-Art (SOTA) |
| **Key Datasets** | VQA-RAD (84.8% Acc), SLAKE (92.4% Acc), OVQA |
| **Core Innovation** | Replaces self-attention with Cross-Mamba interaction |
| **Quality Score** | 8/10 |
| **References** | 17 Citations |

---

## Executive Summary

Medical Visual Question Answering (Med-VQA) faces a critical bottleneck in accurately aligning complex medical imagery with textual queries while generating the diverse, free-form answers required in clinical settings. Existing approaches predominantly rely on traditional self-attention mechanisms and classification-based paradigms, which frequently struggle to capture fine-grained cross-modal semantic alignments and lack the flexibility to handle the variability of open-ended natural language responses. Addressing these limitations is essential for developing reliable AI assistants capable of interpreting the nuanced and multifaceted nature of medical inquiries.

To overcome these challenges, the authors propose the **Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL)** framework, a novel architecture leveraging state-space models (Mamba) to enhance cross-modal feature representation. The system integrates three distinct modules: the **Fine-grained Visual-Text Feature Alignment (FVTA)** module, which extracts relevant visual regions via precise alignment; the **Cross-modal Interleaved Feature Representation (CIFR)** module, which utilizes a Cross-Mamba interaction mechanism to capture sequential dependencies between modalities more effectively than standard self-attention; and the **Free-form Answer-Enhanced Multi-Task Learning (FFAE)** module. This final component employs auxiliary knowledgeâ€”specifically, the integration of answer type classification as a supplementary supervision signalâ€”to guide the generation process, allowing the model to better handle the semantic complexity of open-ended questions.

The CMI-MTL framework demonstrated superior performance by achieving state-of-the-art (SOTA) results across three standard Med-VQA benchmarks: **VQA-RAD**, **SLAKE**, and **OVQA**. Empirical evaluations show the model attained an overall accuracy of **84.8%** on VQA-RAD and **92.4%** on SLAKE. These results represent a significant improvement over previous baselines, outperforming strong prior methods by margins of 1.5% to 3%, which empirically validates the shift toward state-space modeling. Additionally, interpretability experiments provided quantitative validation, confirming that the architecture successfully captures the detailed semantic information necessary for complex medical reasoning.

This research represents a significant paradigm shift in Med-VQA, moving the field beyond traditional self-attention and classification-based architectures toward more flexible and semantically rich models. The substantial performance gains achieved by the Cross-Mamba interaction mechanism justify this architectural evolution, proving that state-space models offer a more effective solution for cross-modal alignment. By successfully addressing these alignment challenges and accommodating the diversity of free-form answers, CMI-MTL substantially advances the capability of Med-VQA systems for practical clinical application. Furthermore, the authors have made the code publicly available, enhancing reproducibility and providing a robust technical foundation for future research into multi-modal medical learning.

---

## Key Findings

*   **SOTA Performance:** The proposed CMI-MTL framework achieves state-of-the-art results on three major datasets: **VQA-RAD**, **SLAKE**, and **OVQA**.
*   **Overcoming Self-Attention Limitations:** The framework successfully overcomes the limitations of traditional self-attention methods, specifically regarding the handling of cross-modal semantic alignments.
*   **Adaptability to Free-form Answers:** It adapts to the diversity of free-form answers significantly better than classification-based methods by capturing detailed semantic information.
*   **Empirical Validation:** Interpretability experiments were conducted to empirically prove the effectiveness of the proposed framework and its internal mechanisms.

---

## Methodology

The research introduces the **Cross-Mamba Interaction based Multi-Task Learning (CMI-MTL)** framework designed to learn robust cross-modal feature representations. The architecture is comprised of three core modules:

1.  **Fine-grained Visual-Text Feature Alignment (FVTA):**
    *   Focuses on extracting relevant visual regions.
    *   Utilizes fine-grained alignment to ensure precise mapping between image features and textual queries.

2.  **Cross-modal Interleaved Feature Representation (CIFR):**
    *   Captures sequential interactions between different modalities (visual and textual).
    *   Leverages the core Cross-Mamba interaction mechanism to process these interactions more efficiently than transformers.

3.  **Free-form Answer-Enhanced Multi-Task Learning (FFAE):**
    *   Handles open-ended questions using a multi-task learning paradigm.
    *   Incorporates auxiliary knowledge to enhance the generation of free-form answers.

---

## Technical Details

*   **Framework Name:** CMI-MTL (Cross-Mamba interaction based Multi-Task Learning)
*   **Core Innovation:** Cross-Mamba interaction mechanism designed specifically to overcome limitations of traditional self-attention methods for cross-modal semantic alignments.
*   **Learning Paradigm:** Multi-Task Learning (MTL), which handles multiple learning tasks simultaneously to improve generalization.
*   **Handling Output Diversity:** Adapts to free-form answers by capturing detailed semantic information rather than relying on fixed classification labels.
*   **Interpretability:** Includes specific components and experiments designed to analyze and validate the model's decision-making process.

---

## Results

**Performance Status:**
*   Achieved **State-of-the-Art (SOTA)** results.

**Datasets Evaluated On:**
*   VQA-RAD
*   SLAKE
*   OVQA

**Qualitative Findings:**
*   Successfully handles the diversity of free-form answers better than classification-based approaches.
*   Interpretability experiments empirically validated the effectiveness of the alignment and interaction modules.

---

## Contributions

*   **Novel Architectural Approach:** The development of CMI-MTL provides a new structure for Med-VQA that moves beyond standard self-attention and classification-based paradigms.
*   **Technical Solution:** Offers a concrete solution to the persistent challenge of cross-modal semantic alignment.
*   **Real-World Applicability:** Significantly advances the capability of Med-VQA systems to handle free-form answers, addressing the diversity found in real-world clinical settings.
*   **Reproducibility & Transparency:** Contributes to the scientific community by making the code publicly available and providing comprehensive interpretability analyses.

---

**Quality Score:** 8/10  
**References:** 17 citations