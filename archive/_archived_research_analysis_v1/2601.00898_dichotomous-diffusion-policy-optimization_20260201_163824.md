# Dichotomous Diffusion Policy Optimization

*Ruiming Liang; Yinan Zheng; Kexin Zheng; Tianyi Tan; Jianxiong Li; Liyuan Mao; Zhihao Wang; Guang Chen; Hangjun Ye; Jingjing Liu; Jinqiao Wang; Xianyuan Zhan*

---

## ðŸ“Š Quick Facts

| Aspect | Detail |
| :--- | :--- |
| **Algorithm** | DIPOLE (Dichotomous Diffusion Policy Optimization) |
| **Core Innovation** | Decomposition of optimal policy into maximization and minimization sub-policies |
| **Key Benchmarks** | ExORL, OGBench, NAVSIM |
| **Performance** | **+20%** route success rate (NAVSIM); Score of **108.8** on AntMaze |
| **Quality Score** | 9/10 |

---

## Executive Summary

### **Problem**
This research addresses the fundamental instability and computational inefficiency that hinder the deployment of Diffusion Models in Reinforcement Learning (RL). While diffusion policies offer high expressiveness for complex robotic behaviors, existing methods face a critical trade-off: direct value maximization techniques frequently suffer from training divergence, whereas approaches relying on Gaussian likelihood approximations to ensure stability are computationally prohibitive. This limitation prevents the scaling of diffusion-based policies to high-dimensional, real-world decision-making tasks required for advanced autonomous systems.

### **Innovation**
The core innovation is the **DIPOLE** algorithm, which formulates RL optimization through a KL-regularized objective: $\max_{\pi} \mathbb{E}[G(s, a)] - \frac{1}{\alpha} D_{KL}(\pi || \pi_{ref})$. This approach yields a closed-form optimal policy and connects the optimization objective to a unique structural decomposition. Through a "greedified policy regularization" scheme, the optimal policy is theoretically decomposed into two distinct diffusion processes: a reward-maximization policy and a reward-minimization policy. This "dichotomous" structure enables an inference mechanism where actions are generated by linearly combining the scores of the two sub-policies, allowing for fine-grained, dynamic control over policy greediness without requiring retraining.

### **Results**
DIPOLE demonstrated significant quantitative improvements across comprehensive benchmarks:
*   **ExORL:** Achieved a state-of-the-art average normalized score of **104.2**, substantially outperforming baselines like IDQL and Diffuser.
*   **AntMaze-Large-Diverse:** Reached a score of **108.8**, compared to approximately **92.2** for the IDQL baseline.
*   **OGBench (Adroit-Pen):** Achieved a score of **118.6**.
*   **NAVSIM:** Validated scalability in the Vision-Language-Action (VLA) domain; fine-tuned a large-scale OpenVLA model, improving route success rates by over **20 percentage points** (increasing success from ~50-60% to over **75%**), while drastically reducing driving infractions.

### **Impact**
This work bridges the gap between the theoretical expressiveness of diffusion models and the practical constraints of RL, offering a mathematically grounded solution to the stability-efficiency trade-off. By enabling the training of large-scale diffusion policies without divergence, DIPOLE paves the way for more capable foundation models in robotics and autonomous systems. The introduction of inference-time control over policy greediness provides a practical new lever for deploying safe and adaptable agents.

---

## Key Findings

*   **Overcoming RL Limitations:** DIPOLE successfully resolves the training instability inherent in direct value maximization while avoiding the computational inefficiencies associated with Gaussian likelihood approximations.
*   **Dichotomous Policy Decomposition:** The method decomposes the optimal policy into 'dichotomous policies' (maximization and minimization), enabling the stable learning of complex behaviors.
*   **Inference-Time Control:** Enables fine-grained inference control over greediness by linearly combining scores from the maximization and minimization policies without retraining.
*   **Benchmark Success:** Demonstrated high effectiveness on **ExORL** and **OGBench** benchmarks in both offline and offline-to-online RL settings.
*   **Real-World Scalability:** Proved capable of training large-scale Vision-Language-Action (VLA) models for real-world tasks, specifically achieving strong results in autonomous driving on the NAVSIM benchmark.

---

## Methodology

The researchers proposed **DIPOLE (Dichotomous Diffusion Policy Improvement)**, a novel RL algorithm designed to optimize diffusion policies stably. The methodology can be summarized as follows:

1.  **KL-Regularized Objective:** The algorithm is grounded in a KL-regularized objective function utilized as a weighted regression objective.
2.  **Greedified Policy Regularization:** A specific regularization scheme is employed to balance greediness (maximizing reward) and stability.
3.  **Policy Decomposition:** Under this scheme, the optimal policy is decomposed into two distinct diffusion policies:
    *   A policy for **reward maximization**.
    *   A policy for **reward minimization**.
4.  **Inference Generation:** During inference, actions are not generated by a single policy. Instead, they are produced by a linear combination of the scores derived from these two distinct policies.

---

## Contributions

1.  **Algorithm Introduction:** Introduced the **DIPOLE algorithm** to resolve the trade-off between training stability and computational cost for large diffusion policies.
2.  **Theoretical Framework:** Developed a theoretical greedified policy regularization scheme that supports the decomposition of optimal policies into maximization and minimization components.
3.  **Inference Mechanism:** Introduced a new inference-time mechanism to control diffusion policy behavior and greediness dynamically by blending sub-policy scores, eliminating the need for retraining.
4.  **Validation of Scalability:** Validated the robustness and scalability of the approach by successfully training a Vision-Language-Action (VLA) model for autonomous driving tasks.

---

## Technical Details

**Objective Formulation**
DIPOLE addresses training instability and computational inefficiency by utilizing a KL-regularized RL objective:
$$ \max_{\pi} \mathbb{E}[G(s, a)] - \frac{1}{\alpha} D_{KL}(\pi || \pi_{ref}) $$

This formulation yields a closed-form optimal policy:
$$ \pi^*(a|s) \propto \pi_{ref}(a|s) \exp(\alpha G(s, a)) $$

**Optimization Strategy**
Instead of traditional score-matching, DIPOLE optimizes the noise predictor using a **Weighted Diffusion Loss**:
$$ L_{\epsilon_\theta} = \mathbb{E}[\exp(\alpha G(s, a)) \cdot ||\epsilon - \epsilon_\theta||^2] $$

**Dichotomous Inference**
The 'Dichotomous' innovation involves decomposing the policy into maximization and minimization branches. During inference, their scores are combined linearly to control greediness via a weight parameter, enabling adjustable agent behavior on the fly.

---

## Evaluation Results

The method was evaluated across three distinct settings to verify its versatility and performance:

*   **Offline RL (ExORL):** DIPOLE outperformed established baselines like IDQL and Diffuser, showcasing its ability to learn effectively from static datasets without online interaction.
*   **Offline-to-Online RL (OGBench):** The algorithm demonstrated high effectiveness in manipulation tasks (such as Adroit-Pen), proving its capability to fine-tune policies effectively when transitioning to online interaction.
*   **Vision-Language-Action (NAVSIM):** In a high-dimensional, real-world simulation for autonomous driving, DIPOLE proved capable of training large-scale VLA models. It significantly outperformed standard behavior cloning baselines in route success rates and safety (reduced infractions).

---

**Quality Score:** 9/10
**References:** 38 citations