---
title: Stochastic Weight Sharing for Bayesian Neural Networks
arxiv_id: '2505.17856'
source_url: https://arxiv.org/abs/2505.17856
generated_at: '2026-02-03T18:24:25'
quality_score: 9
citation_count: 30
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Stochastic Weight Sharing for Bayesian Neural Networks

*Moule Lin; Shuhao Guan; Weipeng Jing; Goetz Botterweck; Andrea Patane*

---

> ### ðŸ“Œ Quick Facts
>
> *   **Method Name:** 2DGBNN (2D Gaussian Bayesian Neural Networks)
> *   **Key Innovation:** Stochastic Weight-Sharing Quantization using a 2D perspective ($\mu$ and $\sigma$)
> *   **Compression:** 50x parameter reduction; 75% model size reduction
> *   **Compute Overhead:** Reduced by several orders of magnitude
> *   **Tested Architectures:** ResNet-18/50/101, Vision Transformer (ViT)
> *   **Quality Score:** 9/10

---

## Executive Summary

Bayesian Neural Networks (BNNs) offer the distinct advantage of robust uncertainty quantification, but their adoption in large-scale, state-of-the-art architectures is severely hindered by prohibitive computational costs and convergence difficulties. Traditional BNNs struggle with the high memory and processing demands required to infer posterior distributions over weights, making them largely infeasible for complex, deep models such as ResNet-101 or Vision Transformers (ViT). This paper addresses this critical bottleneck, aiming to enable efficient Bayesian learning in modern deep learning without sacrificing the reliability of uncertainty estimations.

The authors introduce **2DGBNN (2D Gaussian Bayesian Neural Networks)**, a novel method that reinterprets weight-sharing quantization techniques through a stochastic lens to optimize BNNs. The core innovation lies in encoding the stochastic behavior of a BNN into a lower-dimensional, soft Gaussian representation using a 2D perspective of weight mean ($\mu$) and standard deviation ($\sigma$). The methodology operates in three stages:
1.  **Initializing** a Gaussian Mixture Model (GMM) by partitioning weights into "inliers" (subject to weight sharing) and "outliers" (retaining full distributions);
2.  **Refining** the GMM by merging Gaussians based on Wasserstein Distance Estimations, Mahalanobis distance checks, and Alpha Blending;
3.  **Final Training** where inliers are sampled from the shared distribution and outliers individually.

This approach leverages 2D Adaptive Gaussian Distributions to compress the parameter space significantly while maintaining the stochastic properties necessary for Bayesian inference.

The proposed method achieves substantial efficiency gains, reducing computational overhead by several orders of magnitude compared to traditional BNN approaches. Specifically, 2DGBNN delivers a **50x reduction in model parameters** and a **75% decrease in overall model size**. Despite this aggressive compression, the framework maintains predictive accuracy and uncertainty estimation capabilities comparable to current state-of-the-art models. The approach was successfully validated on benchmarks including MNIST, CIFAR-10, CIFAR-100, and ImageNet1k, using architectures such as ResNet-18, ResNet-50, ResNet-101, and ViT. Performance metrics, including Accuracy, Negative Log-Likelihood (NLL), and Expected Calibration Error (ECE), confirm that the method scales efficiently to very deep networks like ResNet-101 and ViT.

This research establishes a critical conceptual bridge by applying weight-sharing quantization techniques to the stochastic domain, offering a new direction for efficient Bayesian deep learning. By solving the primary bottlenecks of computational cost and convergence, the work makes it feasible to apply rigorous Bayesian uncertainty quantification to massive, modern architectures. The ability to achieve extreme parameter compression without degrading reliability suggests that this method could significantly broaden the deployment of uncertainty-aware models in resource-constrained environments and high-performance applications where BNNs were previously impractical.

---

## Key Findings

*   **Significant Computational Reduction:** The proposed method decreases the computational overhead of Bayesian learning by **several orders of magnitude** compared to traditional approaches.
*   **Large-Scale Scalability:** Enables efficient Bayesian training of state-of-the-art architectures previously constrained by convergence difficulties, specifically **ResNet-101** and **Vision Transformer (ViT)**.
*   **Extreme Compression:** Achieves substantial compression ratios:
    *   **~50x** reduction in model parameters.
    *   **75%** decrease in overall model size.
*   **Maintained Performance:** Despite aggressive compression, the method maintains accuracy and uncertainty estimation capabilities comparable to current state-of-the-art models on **CIFAR10, CIFAR100, and ImageNet1k**.

---

## Methodology

The authors propose a novel reinterpretation of weight-sharing quantization techniques applied through a stochastic perspective to train and inference Bayesian Neural Networks (BNNs).

### Core Concept
The methodology involves encoding the stochastic behavior of a BNN into a lower-dimensional, soft Gaussian representation. This is accomplished through the specific implementation of three technical components:

*   **2D Adaptive Gaussian Distributions**
*   **Wasserstein Distance Estimations**
*   **Alpha Blending**

---

## Technical Details

**Method Name:** 2DGBNN (2D Gaussian Bayesian Neural Networks)

**Core Concept:** Stochastic Weight-Sharing Quantization for BNNs using a 2D perspective (mean $\mu$ and standard deviation $\sigma$).

### Workflow Implementation

*   **Stage 1: Initialize GMM (Pre-training)**
    *   Partition weights into **inliers** (subject to weight-sharing) and **outliers** (retain full distribution) based on mean and gradient thresholds.
    *   Learn a Gaussian Mixture Model (GMM) on inliers.

*   **Stage 2: Refine GMM**
    *   Perform Mahalanobis distance checks.
    *   Apply **Alpha Blending**.
    *   Merge Gaussians based on Wasserstein distance, gradient difference, and variance difference.

*   **Stage 3: Final BNN Training**
    *   Sample inliers from the shared GMM.
    *   Sample outliers from individual distributions.
    *   **Optimization Targets:** Wasserstein distances, network gradients, and intra-class variance.

---

## Results & Experimentation

### Performance Metrics
*   **Computational Overhead:** Reduced by several orders of magnitude.
*   **Model Compression:**
    *   Parameters reduced by approximately **50x**.
    *   Overall model size reduced by **75%**.
*   **Accuracy & Uncertainty:** Maintains performance comparable to state-of-the-art on CIFAR10, CIFAR100, and ImageNet1k.

### Scalability
The method successfully enables efficient Bayesian training of large-scale architectures, including:
*   **ResNet-101**
*   **Vision Transformer (ViT)**

### Experimental Setup

| Component | Details |
| :--- | :--- |
| **Datasets** | MNIST, CIFAR-10, CIFAR-100, ImageNet1k |
| **Architectures** | ResNet-18, ResNet-50, ResNet-101, ViT |
| **Evaluation Metrics** | Accuracy, Negative Log-Likelihood (NLL), Expected Calibration Error (ECE) |

### Reproducibility
*   Experiments were run three times with different seeds.
*   Code is publicly available at: [https://github.com/moulelin/2DGBNN](https://github.com/moulelin/2DGBNN)

---

## Contributions

*   **Conceptual Bridge:** Introduces the application of weight-sharing quantization techniques to the stochastic domain of BNNs, offering a new direction for efficient Bayesian deep learning.
*   **Bottleneck Solution:** Provides a solution to the primary constraints of BNNsâ€”computational cost and convergence difficultiesâ€”making uncertainty quantification feasible for very deep, modern architectures.
*   **Compression Proof:** Demonstrates that extreme parameter compression (50x) and size reduction (75%) are achievable without degrading the reliability of uncertainty estimations or predictive accuracy.

---
**References:** 30 citations | **Quality Score:** 9/10