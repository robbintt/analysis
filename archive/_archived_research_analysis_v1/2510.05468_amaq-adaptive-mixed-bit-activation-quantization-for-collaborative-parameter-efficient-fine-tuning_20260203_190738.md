---
title: 'AMAQ: Adaptive Mixed-bit Activation Quantization for Collaborative Parameter
  Efficient Fine-tuning'
arxiv_id: '2510.05468'
source_url: https://arxiv.org/abs/2510.05468
generated_at: '2026-02-03T19:07:38'
quality_score: 8
citation_count: 11
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# AMAQ: Adaptive Mixed-bit Activation Quantization for Collaborative Parameter Efficient Fine-tuning

*Yurun Song; Zhuoyi Yang; Ian G. Harris; Sangeetha Abdu Jyothi*

***

### ðŸ“Š Quick Facts

| Metric | Value |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Citations** | 11 |
| **Models Tested** | LLaMA3 8B, Qwen2.5 7B |
| **Compression Ratio** | 6â€“8 bits $\to$ 3â€“4 bits |
| **Gen. Accuracy Boost** | ~2.5% higher than fixed precision |
| **Class. Accuracy Boost** | ~1.3% higher than fixed precision |

***

## Executive Summary

The research addresses the critical communication bottleneck and computational overhead inherent in Collaborative Parameter-Efficient Fine-Tuning (PEFT) of Large Language Models (LLMs). In split-learning architectures, where low-resource clients and a central server collaboratively train models, the transmission of high-precision activations and gradients consumes excessive bandwidth and introduces latency. While quantization can reduce this payload, existing methods face a dilemma: fixed-precision approaches fail to account for the varying importance of different features, while aggressive ultra-low bit quantization (3â€“4 bits) often leads to training instability and representation collapse, severely degrading model performance.

The paper introduces **Adaptive Mixed-bit Activation Quantization (AMAQ)**, a novel technique that dynamically assigns bit-widths based on the relative importance of specific features and layers rather than applying a static compression rate. Technically, AMAQ employs trainable gating parameters optimized independently to regulate precision, utilizing a bit-width calculation defined as `min + (max - min) Ã— Ïƒ(Î± Â· Q)`. This allows the system to allocate higher precision (6â€“8 bits) to critical channels and lower precision (3â€“4 bits) to less significant ones. To maintain stability during this aggressive compression, the authors implement a progressive compression schedule and a composite loss function that combines standard Quantization-Aware Training (QAT) loss with an $L_2$ regularization term on bit usage, alongside a clipping strategy for activation means to prevent representation collapse.

Evaluations on large-scale models, specifically LLaMA3 8B and Qwen2.5 7B, demonstrate that AMAQ significantly outperforms fixed-precision baselines. Under identical bit budget constraints, the method achieved approximately **2.5% higher generation accuracy** and **1.3% better classification accuracy**. Crucially, AMAQ successfully compresses activations and gradients from 6â€“8 bits down to 3â€“4 bits without incurring the performance degradation typically associated with such low precision. Furthermore, the study confirmed that the approach effectively mitigates ultra-low bit representation collapse, resulting in enhanced training stability while maintaining manageable communication overhead in multi-machine setups.

***

## Key Findings

*   **Performance Superiority:** Under identical bit budgets, AMAQ delivers approximately **2.5% higher generation accuracy** and **1.3% better classification accuracy** on models like LLaMA3 8B and Qwen2.5 7B compared to fixed-precision approaches.
*   **Training Stability:** The method significantly enhances training stability and effectively mitigates ultra-low bit representation collapse.
*   **Practical Efficiency:** AMAQ integrates effectively into multi-machine collaborative training setups, offering superior inference accuracy while keeping communication overhead modest.
*   **Effective Compression:** Successfully compresses activations and gradients from high precision (6â€“8 bits) to low precision (3â€“4 bits) without degrading model performance.

***

## Methodology

The researchers implemented **Parameter-efficient Split Learning** to facilitate collaborative training on low-resource devices. Within this framework, they introduced Adaptive Mixed-bit Activation Quantization (AMAQ).

The methodology utilizes a **progressive compression strategy**, reducing activations and gradients from high precision (6â€“8 bits) to low precision (3â€“4 bits). Crucially, AMAQ employs **bit regularization** to dynamically allocate bit budgets across channels based on the calculated importance of specific features and layers.

***

## Technical Details

*   **Core Mechanism:** AMAQ dynamically allocates higher precision (6â€“8 bits) to critical features and lower precision (3â€“4 bits) to less important ones using a gradual quantization schedule.
*   **Gating Parameters ($Q$):** Introduces trainable parameters shaped $1 \times H$ or $1 \times \text{max\_seqLen}$, which are optimized independently.
*   **Bit-width Calculation:**
    $$ \text{min} + (\text{max} - \text{min}) \times \sigma(\alpha \cdot Q) $$
    *Utilizes the Straight Through Estimator (STE) for gradient flow.*
*   **Loss Function:** The total loss combines QAT loss with a regularization term ($L_2$ norm) on the gating parameters:
    $$ \text{Loss} = \text{QAT\_Loss} + \beta \times \text{Bits\_Loss} $$
*   **Stability Strategy:** A clipping strategy is applied to the mean of activations to prevent collapse.

***

## Contributions

*   **Adaptive Quantization Strategy:** Introduction of AMAQ, a novel mixed-bit quantization technique that adapts precision levels based on feature-wise and layer-wise importance, moving beyond static, fixed-precision quantization.
*   **Optimization for Collaborative Training:** A solution that specifically addresses the bottlenecks of communication efficiency and computational overhead in server-client distributed LLM training.
*   **Robustness in Low-Precision Regimes:** A methodological advancement in preventing representation collapse and maintaining stability during ultra-low bit (3â€“4 bits) training processes.
*   **Validated Efficiency:** Empirical demonstration that adaptive bit allocation provides a better trade-off between communication cost and model accuracy compared to traditional fixed-precision methods.

***

## Results

AMAQ achieves approximately **2.5% higher generation accuracy** and **1.3% better classification accuracy** than fixed-precision approaches under identical bit budgets. Validated on large-scale models like LLaMA3 8B and Qwen2.5 7B, the method successfully compresses activations and gradients from 6-8 bits to 3-4 bits without degrading performance.

Additional outcomes include:
*   Enhanced training stability by mitigating 'ultra-low bit representation collapse'.
*   Maintained modest communication overhead in split learning setups.
*   Real-world evaluations covered GPU Usage, Communication Latency, and Activation Transmission Size.