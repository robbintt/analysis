# One Framework to Rule Them All: Unifying RL-Based and RL-Free Methods in RLHF

*Xin Cai*

---

## F4CA Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 6/10 |
| **Total Citations** | 23 |
| **Core Framework** | Generalized Reinforce Optimization (GRO) |
| **Theoretical Basis** | Neural Structured Bandit Prediction |
| **Key Objective** | Unification of RL-based and RL-free RLHF paradigms |

---

## Executive Summary

The field of Reinforcement Learning from Human Feedback (RLHF) is currently fractured into two competing paradigms: **RL-based methods** (e.g., PPO) and **RL-free methods** (e.g., DPO). This division creates a fragmented theoretical landscape where these approaches are viewed as fundamentally distinct. Consequently, practitioners perceive them as separate processes rather than variations of a single mechanism. This conceptual gap obscures the underlying mechanics of alignment, forcing the field to rely on inefficient, heuristic-based algorithm selection rather than rigorous mathematical principles.

This research introduces the **Generalized Reinforce Optimization (GRO)** framework to unify these methodologies under the perspective of **neural structured bandit prediction**. The authors demonstrate that the standard RLHF objective is mathematically equivalent to a contextual bandit problem maximizing expected reward. It utilizes a REINFORCE gradient estimator with a baseline to reduce variance. The unification rests on a critical insight: popular algorithms—including RLOO, GRPO, and DPO—are not distinct paradigms. Instead, they are specific instantiations of the same underlying logic, differing solely in their choice of baseline and advantage calculations. This explains that varying the baseline defines the variance reduction strategy that mathematically recovers different algorithms.

The analysis provides concrete theoretical and empirical insights. It validates the mathematical equivalence of the RLHF objective to neural structured bandit prediction. Quantitatively, the study identifies that DPO is constrained by small sample sizes, typically relying on only N=2 pairs, which limits its flexibility with diverse reward functions compared to RL-based methods. In experiments with Large Reasoning Models (LRMs), the study observed that skipping Supervised Fine-Tuning (SFT) results in negligible negative impact on performance. Furthermore, rule-based rewards demonstrated superior effectiveness compared to process reward models in this context. Optimization was achieved using standard cross-entropy loss for reward modeling and a KL-regularized objective.

By bridging the gap between competing methodologies, this research establishes a rigorous, "one-framework" theoretical foundation that resolves previous disparities in the field. The GRO framework corrects theoretical shortcomings in the application of PPO and offers a generalized approach to policy optimization. This unification proves that RL-based and RL-free methods are two sides of the same coin. Ultimately, it provides researchers and practitioners with a coherent roadmap for designing and selecting alignment algorithms, fostering more efficient progress in the development of aligned AI systems.

---

## Key Findings

*   **Theoretical Unification:** RL-based and RL-free methods in RLHF can be reinterpreted through the perspective of **neural structured bandit prediction**, revealing a deep conceptual connection.
*   **Mathematical Equivalence:** The standard RLHF objective is mathematically equivalent to neural structured bandit prediction when derived within a full Reinforcement Learning context.
*   **PPO Reassessment:** A reinvestigation of Proximal Policy Optimization (PPO) principles identified specific areas requiring adjustment to achieve a unified approach.
*   **Framework Integration:** The **Generalized Reinforce Optimization (GRO)** framework successfully integrates both RL-based and RL-free methods, resolving previous disparities.

---

## Methodology

The research employs a theoretical and analytical framework consisting of four distinct phases:

1.  **Comparative Analysis:** A high-level comparison of standard RLHF and Large Reasoning Models (LRMs) steps is conducted to establish context.
2.  **Conceptual Reinterpretation:** Existing algorithms are re-examined using the lens of neural structured bandit prediction.
3.  **Theoretical Derivation:** The paper provides a rigorous mathematical derivation of the standard RLHF objective within a full RL context, addressing gaps in current literature.
4.  **Framework Synthesis:** An investigation into PPO mechanisms pinpoints necessary adjustments for synthesizing the Generalized Reinforce Optimization (GRO) framework.

---

## Technical Details

### Core Perspective
The paper proposes a unified view of RLHF by reinterpreting alignment methods through **Neural Structured Bandit Prediction**. The RLHF objective is formulated as a contextual bandit problem maximizing expected reward, utilizing a REINFORCE gradient estimator with a baseline to reduce variance.

### Algorithm Unification
Specific algorithms are unified by their differing baseline and advantage choices within the GRO framework:

*   **RLOO:** Uses the average reward of other samples as the baseline.
*   **GRPO:** Normalizes advantage using batch statistics.
*   **ReMax:** Uses greedy reward as a baseline.
*   **REINFORCE++:** Incorporates token-level rewards and importance weights.
*   **DPO:** Recast as REINFORCE with specific weighting and fixed rewards.
*   **KTO:** Derived from prospect theory within the same unified context.

### Pipeline Process
The proposed pipeline operates on an iterative loop:
1.  **Prompt Sampling**
2.  **Response Generation**
3.  **Feedback Reception**
4.  **Policy Updating**

---

## Contributions

*   **GRO Framework:** Introduces the "Generalized Reinforce Optimization" (GRO) framework, providing seamless integration of RL-based and RL-free methodologies for RLHF.
*   **Theoretical Perspective:** Establishes "neural structured bandit prediction" as a valid and clarifying theoretical perspective for understanding RLHF algorithms.
*   **Rigorous Foundation:** Provides a rigorous theoretical foundation through the derivation of the RLHF objective within a full RL context, addressing gaps in current literature.
*   **Optimization Refinement:** Refines optimization by identifying specific theoretical shortcomings in the application of PPO and proposing adjustments for a generalized approach.

---

## Results & Analysis

*   **Equivalence Confirmation:** The analysis confirms the mathematical equivalence of the standard RLHF objective to neural structured bandit prediction.
*   **LRM Observations:**
    *   Skipping Supervised Fine-Tuning (SFT) has a **negligible negative impact** on performance.
    *   **Rule-based rewards** are more effective than process reward models in this context.
*   **DPO Limitations:** Theoretical analysis suggests DPO is limited by small sample size (N=2 pairs) and inflexibility with diverse reward functions compared to RL-based methods.
*   **Metric Optimization:**
    *   **Reward Modeling:** Optimized via cross-entropy loss.
    *   **RL Fine-Tuning:** Utilizes a KL-regularized objective.
    *   **Feedback Sources:** Optimized via user or simulated feedback.