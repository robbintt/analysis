---
title: Model Merging to Maintain Language-Only Performance in Developmentally Plausible
  Multimodal Models
arxiv_id: '2510.01845'
source_url: https://arxiv.org/abs/2510.01845
generated_at: '2026-02-06T02:46:08'
quality_score: 8
citation_count: 12
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Model Merging to Maintain Language-Only Performance in Developmentally Plausible Multimodal Models

*Ece Takmaz; Lisa Bylinina; Jakub Dotlacil*

---

> ### ðŸ“Š Quick Facts
>
> **Quality Score:** 8/10 | **Citations:** 12
>
> *   **Training Data:** ~8.28M samples (Localized Narratives, CC3M, BabyLM)
> *   **Architecture:** Hugging Face LlavaForConditionalGeneration
> *   **Visual Encoder:** DINOv2-large
> *   **Core Technique:** Model Merging (Weighted Linear Interpolation)
> *   **Key Outcome:** Recovers grammar proficiency without sacrificing multimodal capabilities

---

## Executive Summary

This research addresses the "modality gap" in multimodal learning, where Vision-Language Models (VLMs) typically underperform Language-Only Models (LLMs) on grammar-focused benchmarks, despite being trained on additional visual data. Furthermore, it challenges the prevailing paradigm that requires massive datasets for State-of-the-Art (SOTA) performance, which conflicts with the data-efficient nature of child language acquisition. The paper investigates how to maintain robust linguistic capabilitiesâ€”specifically grammarâ€”within multimodal models trained using developmentally plausible, low-resource datasets.

The study introduces the application of model merging via weighted linear interpolation (often referred to as "model soups") as a solution to the modality gap. Instead of retraining or increasing computational resources, the authors propose fusing the parameters of a trained language-only model with those of a multimodal model. This is achieved through the formula `Î¸_merged = Î±Î¸_LLM + (1 - Î±)Î¸_VLM`, where Î± is a weighting parameter. This technique allows the resulting model to inherit the grammatical proficiency of the LLM while retaining the visual grounding of the VLM, effectively bypassing the trade-off between modalities.

Experiments utilized a dataset of approximately 8.28 million samples from Localized Narratives, Conceptual Captions 3M, and BabyLM text-only data, employing an architecture based on Hugging Face LlavaForConditionalGeneration with a DINOv2-large visual encoder. The results confirmed that while standard multimodal models outperformed previous baselines, they exhibited specific weaknesses in grammar compared to language-only counterparts. The application of weighted linear interpolation successfully recovered these language-only abilities, alleviating the underperformance on grammar benchmarks without negatively impacting the model's accuracy on multimodal tasks.

This work offers significant contributions to the efficiency and cognitive plausibility of AI models. It demonstrates that high-performance multimodal models can be trained in low-resource settings that mimic child language acquisition, providing a viable alternative to resource-intensive SOTA models. By validating model merging as a method to balance multimodal learning, the study establishes a pathway for developing systems that maintain high syntactic performance while retaining strong visual perception capabilities, solving a critical instability in current multimodal architectures.

---

## Key Findings

*   **Performance in Low-Resource Settings:** Multimodal models developed using developmentally plausible, low-resource datasets outperformed previous baselines established in the BabyLM challenge.
*   **Confirmed Modality Gap:** There is a confirmed performance discrepancy where multimodal models underperform compared to language-only models on grammar-focused benchmarks.
*   **Efficacy of Model Merging:** Utilizing model merging (specifically weighted linear interpolation) effectively alleviates the underperformance on grammar-focused language-only tasks.
*   **Zero-Side Effect Recovery:** The application of model merging successfully recovers language-only abilities without negatively impacting the model's multimodal performance.

---

## Methodology

The researchers operated within the BabyLM challenge framework to address the discrepancy between massive dataset requirements for SOTA models and the limited data exposure in child language acquisition. They developed both language-only and multimodal models using developmentally plausible low-resource datasets.

To target the issue of multimodal models losing proficiency in language-only tasks, the study employed **model merging**. This involved fusing the parameters of trained multimodal models with those of language-only models via weighted linear interpolation.

---

## Technical Details

### Architecture & Components
*   **Base Framework:** Builds on Hugging Face `LlavaForConditionalGeneration`.
*   **Visual Encoder:** Utilizes DINOv2-large, optimized to a single mean-pooled image token.
*   **Language Model:** A randomly initialized 6-layer language model.
*   **Multimodal Projector:** Maps image representations to the LM's space using:
    1.  Linear layer (1024 to 768)
    2.  GeLU activation
    3.  Linear layer (768 to 768)
*   **Training Protocol:** The projector is trained alongside the LM while image representations remain frozen.

### Text-Only Input Handling
For text-only inputs, the model uses a constant 'black image' (640x420 pixels) to maintain consistent input dimensions.

### Tokenizer
*   **Type:** `LlamaTokenizerFast` (SentencePiece BPE).
*   **Vocabulary:** 30,000 tokens.
*   **Configuration:** BERT pre-tokenizer with a special image token.
*   **Training:** Trained from scratch on 100M tokens.

### Model Merging Formula
The approach utilizes model merging via weighted linear interpolation (model soups) using the following formula to recover language capabilities without retraining:

$$ \theta_{merged} = \alpha \theta_{LLM} + (1 - \alpha) \theta_{VLM} $$

---

## Results

The study utilized a dataset of approximately **8.28 million training samples** derived from:
1.  Localized Narratives
2.  Conceptual Captions 3M
3.  BabyLM text-only data

**Outcomes:**
*   Findings indicate that while multimodal models outperformed previous baselines, they exhibited a 'modality gap', underperforming language-only models on grammar-focused benchmarks.
*   The application of weighted linear interpolation (model merging) effectively alleviated this underperformance on grammar tasks.
*   The technique recovered language-only abilities and maintained accuracy on multimodal benchmarks without negative impact.

---

## Contributions

*   **Cognitively Plausible AI:** Demonstrates that effective multimodal models can be trained in low-resource settings that mimic child language acquisition, offering a cognitively plausible alternative to SOTA models.
*   **Empirical Evidence of Weakness:** Provides empirical evidence that multimodal models suffer from specific weaknesses in grammar on language-only benchmarks.
*   **Technical Validation:** Introduces and validates model merging as a technical solution to balance multimodal learning, showing that high performance can be maintained on both language-only (grammar) tasks and multimodal tasks simultaneously.