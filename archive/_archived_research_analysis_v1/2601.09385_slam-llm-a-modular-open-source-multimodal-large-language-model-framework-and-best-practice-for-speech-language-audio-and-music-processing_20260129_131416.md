# SLAM-LLM: A Modular, Open-Source Multimodal Large Language Model Framework and Best Practice for Speech, Language, Audio and Music Processing

*Ziyang Ma; Guanrou Yang; Wenxi Chen; Zhifu Gao; Yexing Du; Xiquan Li; Zhisheng Zheng; Haina Zhu; Jianheng Zhuo; Zheshu Song; Ruiyang Xu; Tiranrui Wang; Yifan Yang; Yanqiao Zhu; Zhikang Niu; Liumeng Xue; Yinghao Ma; Ruibin Yuan; Shiliang Zhang; Kai Yu; Eng Siong Chng; Xie Chen*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |
| **LLM Backbone** | Vicuna v1.5-7B (Decoder-Only) |
| **Audio Encoder** | AV-HuBERT Large (477.3M Params) |
| **Optimization** | LoRA (Rank 32, Alpha 32) |
| **Trainable Params** | ~33.6M (Additional) |
| **Key Performance** | **1.33% WER** (Librispeech Test-Clean) vs 2.4% (GA-CTC) |

---

## Executive Summary

Current open-source Multimodal Large Language Model (MLLM) frameworks are disproportionately optimized for vision inputs, creating a significant gap in robust support for speech, audio, and music processing. This lack of specialized tools forces researchers to incur high engineering overhead to stitch together disparate components for audio-centric tasks. Without standardized pipelines, developing efficient Large Audio-Language Models (LALMs) remains resource-intensive, hindering rapid experimentation and deployment in critical domains such as automatic speech recognition (ASR), audio captioning, music understanding, and visual speech processing.

SLAM-LLM addresses these challenges through a modular architecture designed for flexible configuration of encoders, projectors, LLM backbones, and PEFT plugins. Technically, the framework utilizes a lightweight linear projector to map audio featuresâ€”extracted by encoders like AV-HuBERT Largeâ€”into the input embedding space of a decoder-only LLM backbone (e.g., Vicuna v1.5-7B). It integrates Low-Rank Adaptation (LoRA) specifically within attention layers to optimize parameter efficiency.

A core innovation is the standardization of detailed training and inference "recipes"â€”configurations synthesized from 40 academic citationsâ€”ensuring reliable methodologies for tasks ranging from speech recognition to music generation. The proposed framework demonstrates high performance and parameter efficiency across rigorous benchmarks. In Visual Speech Recognition on the LRS3 dataset, SLAM-LLM matched the performance of a strong AV-HuBERT Large baseline with a WER of 28.3% (vs. 28.6%) while reducing trainable parameters by approximately 85% (49M vs. 325M). In Contextual ASR on Librispeech, the model significantly outperformed traditional Neural Network-based methods like GA-CTC, achieving a WER of 1.33% on "test-clean" compared to the competitor's 2.4%, and 2.99% on "test-other" compared to 6.2%.

By releasing a comprehensive, open-source framework specifically tailored for audio and speech processing, SLAM-LLM lowers the barrier to entry for researchers. The establishment of standardized best practices via detailed recipesâ€”grounded in established academic literature rather than community testingâ€”serves to accelerate development cycles and reduce engineering debt. This work democratizes access to high-performance model checkpoints and sets a new standard for the design of modular, efficient Large Audio-Language Models, facilitating broader adoption and innovation in speech and music AI.

---

## Key Findings

*   **Market Gap:** Current open-source MLLM frameworks prioritize vision inputs, leaving a significant gap in support for speech, audio, and music processing.
*   **State-of-the-Art Performance:** The proposed framework achieves SOTA results in Automatic Speech Recognition (ASR), Audio Captioning (AAC), and Music Captioning.
*   **Reduced Engineering Overhead:** The framework provides standardized recipes that reduce the complexity of model configuration and training.
*   **Validated Integration:** The integrated techniques utilized in the framework have been validated through 40 academic publications.

---

## Methodology

The SLAM-LLM framework is built upon a **modular architecture** that allows for the flexible configuration of four primary components:

1.  **Encoders:** For processing input modalities.
2.  **Projectors:** To map features to the LLM space.
3.  **LLMs:** The core reasoning backbone.
4.  **PEFT Plugins:** For efficient fine-tuning.

This approach relies on **standardized, detailed recipes** for training and inference pipelines. These recipes are specifically tailored to mainstream audio tasks, enabling the customized training of MLLMs for specific speech, language, audio, and music data domains without reinventing the pipeline for each experiment.

---

## Technical Architecture

The technical implementation connects audio-visual inputs to a powerful language model backbone using efficient adaptation techniques.

### System Components

| Component | Specification |
| :--- | :--- |
| **Framework** | Modular design connecting audio-visual encoders to an LLM decoder. |
| **Encoder** | **AV-HuBERT Large** <br> *Parameters:* 477.3M <br> *Pre-training Data:* LRS3 and VoxCeleb2 (1,759 hours of unlabeled data). |
| **LLM Backbone** | **Vicuna v1.5-7B** (Decoder-Only). |
| **Adaptor Projector** | Lightweight linear projector to map AV-HuBERT outputs to LLM input embedding space. |
| **Prompting** | Accepts text prompts alongside visual features (e.g., "Transcribe the speech in this video to text"). |

### Parameter-Efficient Fine-Tuning (PEFT)

*   **Technique:** LoRA (Low-Rank Adaptation).
*   **Application:** Applied to Key, Query, Value, and output projection layers in self-attention modules.
*   **Hyperparameters:**
    *   Rank: 32
    *   Alpha: 32
    *   Dropout: 0.05
*   **Additional Trainable Parameters:** 33.6M

### Contextual ASR Approach

*   **Methodology:** Utilizes an LLM-based method for Contextual ASR (CASR) using artificial biasing lists.
*   **Comparison Basis:** Contrasted against traditional Neural Network (NN)-based contextual ASR methods (e.g., Shallow Fusion, Neural Transducers).

---

## Contributions

*   **Framework Release:** Release of the **SLAM-LLM framework**, a comprehensive open-source tool specifically for speech, language, audio, and music.
*   **High-Performance Checkpoints:** Provision of ready-to-use model checkpoints for tasks like ASR, AAC, and Music Captioning.
*   **Standardized Best Practices:** Establishment of detailed "recipes" to accelerate development and ensure reproducibility.
*   **Community Enablement:** Provision of tools that lower the barrier to entry for researchers and developers in the audio processing space.

---

## Performance Results

### Visual Speech Recognition (VSR)
*Dataset: LRS3 (1,452 utterances, 1 hour)*

| Model | Trainable Params | Word Error Rate (WER) |
| :--- | :--- | :--- |
| Baseline (AV-HuBERT Large) | 325M | 28.6% |
| **Proposed (SLAM-LLM)** | **49M** | **28.3%** |

> **Note:** SLAM-LLM achieved comparable accuracy to the baseline while reducing trainable parameters by approximately **85%**.

### Contextual ASR
*Dataset: Librispeech*

**Scalability Factors:**  
(How model performance scales with biasing list size)
| Test Set | N=100 | N=2000 |
| :--- | :--- | :--- |
| Test Clean | 1.27 | 1.38 |
| Test Other | 2.72 | 3.20 |

**Competitor Comparison (List Size N=1000):**

| Dataset | Metric | Competitor (GA-CTC) | SLAM-LLM |
| :--- | :--- | :--- | :--- |
| **Test Clean** | WER | 2.4 | **1.33** |
| **Test Other** | WER | 6.2 | **2.99** |

---

**References:** 40 citations