# Revisiting the Scaling Properties of Downstream Metrics in Large Language Model Training

*Jakub Krajewski; Amitis Shidani; Dan Busbridge; Sam Wiseman; Jason Ramapuram*

---

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Training Runs** | 130 distinct runs |
| **Model Scale** | Up to 17B parameters |
| **Data Volume** | Up to 350B tokens |
| **Compute Budget** | $10^{18}$ ‚Äì $10^{23}$ FLOPs |
| **Benchmarks Tested** | 12 (e.g., ARC-E/C, HellaSwag) |

---

## üìù Executive Summary

> **Overview:** Current research into the scaling laws of Large Language Models (LLMs) primarily focuses on predicting pre-training loss as a function of compute budget. However, practitioners care most about downstream performance‚Äîaccuracy on specific benchmarks‚Äîyet predicting these metrics directly has historically been considered unreliable and noisy. Existing methodologies typically rely on a two-stage procedure: first predicting pre-training loss and then inferring accuracy from that loss. This approach is problematic because the mapping from loss to accuracy is non-linear and variable, leading to compounding errors that hinder accurate forecasting of model capabilities essential for efficient resource allocation and training planning.
>
> **The Solution:** This paper introduces a "Direct Scaling Law" framework that bypasses proxy metrics entirely, mapping training budgets directly to downstream benchmark accuracy. Technically, the authors demonstrate that the log of accuracy on popular tasks follows a simple power-law scaling behavior when the token-to-parameter ratio is held fixed. Furthermore, they extend this functional form to handle variable token-to-parameter ratios and incorporate inference compute requirements for repeated sampling scenarios. By fitting specific power-law functional forms to empirical data, this direct modeling approach effectively eliminates the compounding errors inherent in traditional loss-interpolated methods.
>
> **Validation & Impact:** The study‚Äôs claims are substantiated by a massive empirical validation involving 130 distinct training runs. The experiments scaled models up to 17 billion parameters trained on 350 billion tokens, spanning a compute range of $10^{18}$ to $10^{23}$ FLOPs across two dataset mixtures (C4 and Modern Mixture). Evaluation across 12 benchmarks showed the fitted power-law curves achieving high alignment with observed accuracies. Crucially, the direct modeling framework demonstrated superior extrapolation performance compared to the two-stage baseline, proving it can reliably predict performance across varying model sizes and training durations.

---

## üîë Key Findings

*   **Scaling Law for Accuracy:** For a fixed token-to-parameter ratio, the scaling behavior of log accuracy on popular downstream tasks can be accurately described by a simple power law.
*   **Superiority of Direct Modeling:** A direct framework for predicting benchmark performance from training budget extrapolates better than traditional two-stage procedures (which predict loss first, then accuracy), thereby avoiding compounding errors.
*   **Functional Scope:** The proposed models can predict accuracy across varying token-to-parameter ratios and can account for inference compute requirements under repeated sampling scenarios.

---

## üõ†Ô∏è Methodology

The authors propose and validate a **direct framework** that models the scaling of benchmark performance as a function of training budget, bypassing the reliance on proxy metrics like pretraining loss.

*   **Functional Forms:** The methodology involves fitting specific functional forms (power laws) to empirically derived data.
*   **Validation Scope:** This approach was empirically validated using models with up to 17 billion parameters trained on up to 350 billion tokens across two different dataset mixtures.

---

## üß™ Technical Details

| Aspect | Detail |
| :--- | :--- |
| **Core Framework** | **Direct Scaling Law** mapping pre-training budget directly to benchmark accuracy via a power law relationship. |
| **Primary Constraint** | Operates under fixed token-to-parameter ratio constraints to mitigate compounding errors found in two-stage models. |
| **Extended Capabilities** | Supports variable ratios and inference compute modeling for repeated sampling. |
| **Architecture** | Standard Transformer architectures. |
| **Datasets** | Two datasets utilized: **C4** and a **Modern Mixture Dataset** (to isolate data composition effects). |

---

## üìà Results

*   **Scale of Experiments:** Validation involved 130 distinct training runs, scaling models up to 17 billion parameters on 350 billion tokens.
*   **Compute Range:** Computing budget range spanned from $10^{18}$ to $10^{23}$ FLOPs.
*   **Benchmark Coverage:** Evaluation covered 12 benchmarks, including ARC-E/C, SciQ, PIQA, and HellaSwag.
*   **Performance Metrics:** Observed downstream accuracies ranged from 20% to 90%.
*   **Fit Quality:** The fitted curves showed high alignment with data points.
*   **Comparative Performance:** The direct modeling framework demonstrated superior extrapolation performance compared to two-stage procedures.

---

## üåü Contributions

*   **Paradigm Shift:** Challenges the conventional belief that predicting downstream task performance is unreliable, establishing that it can be modeled effectively and directly from the training budget.
*   **Advanced Modeling Framework:** Introduces robust functional forms that capture scaling behavior not only for fixed compute ratios but across varying token-to-parameter ratios and inference constraints.
*   **Open Science & Reproducibility:** Releases a comprehensive dataset containing complete pretraining losses and downstream evaluation results to facilitate future research and validation within the community.

---

**Document Quality Score:** 9/10  
**Total References:** 40