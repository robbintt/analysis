---
title: 'HAM: Hierarchical Adapter Merging for Scalable Continual Learning'
arxiv_id: '2509.13211'
source_url: https://arxiv.org/abs/2509.13211
generated_at: '2026-02-03T18:30:37'
quality_score: 7
citation_count: 24
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# HAM: Hierarchical Adapter Merging for Scalable Continual Learning

*Eric Nuertey Coleman; Luigi Quarantiello; Samrat Mukherjee; Julio Hurtado; Vincenzo Lomonaco*

***

> ### üìä Quick Facts
> | Metric | Value |
> | :--- | :--- |
> | **Quality Score** | 7/10 |
> | **References** | 24 Citations |
> | **Core Technique** | Parameter-Efficient Fine-Tuning (PEFT) & Dynamic Consolidation |
> | **Key Advantage** | Constant parameter footprint (avoids linear growth) |
> | **Primary Benchmarks** | CIFAR-100, ImageNet-R |

***

## üìù Executive Summary

This research addresses the critical scalability bottleneck inherent in applying Parameter-Efficient Fine-Tuning (PEFT) to Continual Learning (CL), specifically the issues of **adapter bloat**, linear parameter growth, and catastrophic forgetting. As CL systems learn more tasks, traditional methods require storing separate adapters for each, leading to unsustainable memory demands and computational overhead. This paper solves this by introducing the **Hierarchical Adapter Merging (HAM)** framework, a dynamic consolidation strategy designed to decouple model performance from the number of tasks learned, thereby enabling sustainable, scalable lifelong learning systems.

The key innovation of HAM is its use of **learnable importance scalars** and **cosine similarity-based grouping** to dynamically consolidate knowledge. Unlike static one-adapter-per-task paradigms, HAM trains task-specific adapters and then groups them based on similarity. A critical component of this pipeline is the **pruning** step, which removes low-importance weights before consolidation. These pruned and scaled adapters are then merged into a fixed set of Group Adapters. This mechanism maintains a constant parameter footprint regardless of the number of tasks encountered, while facilitating positive forward transfer through shared group representations.

Extensive experiments on vision benchmarks, including CIFAR-100 and ImageNet-R, demonstrate that HAM establishes a new state-of-the-art. The method significantly outperforms existing baselines such as L2P, DualPrompt, InfLoRA, and SD-LoRA in terms of accuracy. Crucially, HAM achieves this superior performance while drastically reducing the parameter count by preventing linear growth; instead of expanding memory with each new task, HAM maintains a constant parameter budget, offering substantial efficiency gains for long task sequences.

HAM represents a significant paradigm shift in continual learning by resolving the memory and computational bottlenecks that limit current PEFT methods. By successfully integrating pruning, hierarchical merging, and fixed-capacity storage, the framework provides a robust solution to the trade-off between performance and scalability.

***

## üîç Key Findings

*   **Superior Performance:** HAM significantly outperforms state-of-the-art methods, establishing new benchmarks on vision datasets.
*   **Scalability:** The framework demonstrates superior scalability for longer task sequences by avoiding the need for separate adapters for every task.
*   **Efficiency:** It achieves improved efficiency by preventing linear parameter growth, maintaining a constant memory footprint.
*   **Positive Transfer:** HAM facilitates positive transfer learning by intelligently grouping and merging similar adapters.
*   **Robustness:** Extensive experiments validate robust performance and substantial gains over existing approaches like InfLoRA, SD-LoRA, and L2P.

***

## ‚öôÔ∏è Technical Details

HAM is a parameter-efficient continual learning framework based on Low-Rank Adaptation (LoRA).

### Architecture Components
*   **Frozen Base Model:** A pre-trained model ($W_0$) remains frozen throughout the process.
*   **Task-Specific Adapters:** Defined as $\Delta W_i = B_i A_i$.
*   **Group Adapters:** Consolidated representations defined as $\Delta W_{Gj}$.
*   **Importance Factors:** Learnable scalar parameters ($\alpha$) used to weight adapter contributions.

### Training Phases
1.  **Task-Specific LoRA Training:**
    *   The current adapter and its importance factor are trained.
    *   Importance factors for previous group adapters are simultaneously updated.
2.  **Adapters Grouping:**
    *   **Similarity Check:** Cosine similarity is calculated. If similarity $S \ge \tau_{sim}$, the task joins an existing group; otherwise, it creates a new one.
    *   **Consolidation:** Adapters undergo pruning, scaling, and merging to consolidate knowledge and reduce interference.

### Inference Mechanism
*   **Global Merging:** The system averages scaled group adapters into a single module.
*   **Formula:** $\Delta W_{merged} = \frac{1}{M} \sum \alpha_{Gj} \Delta W_{Gj}$
*   **Final Model:** Constructed as $W_{final} = W_0 + \Delta W_{merged}$, incorporating rank expansion.

***

## üß™ Methodology

The proposed **Hierarchical Adapters Merging (HAM)** framework utilizes a combination of Parameter-Efficient Fine-Tuning (PEFT) and dynamic consolidation strategies:

1.  **Low-Rank Adapter Training:** The system trains low-rank adapters paired with importance scalars to capture task-specific knowledge.
2.  **Dynamic Grouping:** Tasks are dynamically grouped based on the similarity of their adapters.
3.  **Fixed Group Maintenance:** A fixed set of groups is maintained to facilitate hierarchical consolidation, ensuring the system does not grow indefinitely with the number of tasks.
4.  **Consolidation Pipeline:** Adapters are processed through pruning (removing noise), scaling (applying importance), and merging to solidify learned knowledge.

***

## ‚ú® Contributions

*   **Novel Framework:** Introduction of HAM, addressing the scalability limitations of current PEFT methods in continual learning.
*   **Solution to Adapter Bloat:** Moves away from static one-adapter-per-task paradigms to dynamic hierarchical consolidation.
*   **Enhanced Generalization:** Improves generalization capabilities via adapter similarity-based transfer learning.
*   **Empirical Validation:** Establishes a new state-of-the-art through rigorous validation on vision datasets.

***

## üìà Results

*   **Benchmark Performance:** Qualitative results indicate robust performance on vision benchmarks, significantly outperforming methods such as InfLoRA, SD-LoRA, L2P, DualPrompt, and CODA-Prompt.
*   **Scalability & Efficiency:** The method proves superior scalability for long task sequences and efficiency by avoiding linear parameter growth.
*   **Transfer Learning:** Successfully facilitates positive transfer learning through intelligent grouping.
*   **Forgetting Mitigation:** Aims to mitigate catastrophic forgetting via importance factor balancing.

***

### üìö References
*   Total Citations: 24

### ‚≠ê Quality Score
*   **7/10**