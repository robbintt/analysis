---
title: Training Multi-Image Vision Agents via End2End Reinforcement Learning
arxiv_id: '2512.0898'
source_url: https://arxiv.org/abs/2512.08980
generated_at: '2026-02-03T07:04:48'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Training Multi-Image Vision Agents via End2End Reinforcement Learning

*Chengqi Dong; Chuhuai Yue; Hang He; Rongge Mao; Fenghe Tang; S Kevin Zhou; Zekun Xu; Xiaohan Wang; Jiajun Chai; Wei Lin; Guojun Yin*

---

> ### ðŸ“Š Quick Facts
>
> *   **Dataset:** MIFG-QA (10k samples)
> *   **Framework:** IMAgent (Open-source)
> *   **Top Accuracy:** 93.8% on MIFG-QA Benchmark
> *   **Training Method:** Pure Reinforcement Learning (No SFT)
> *   **Key Innovation:** Action-Trajectory Two-Level Mask Strategy
> *   **Comparisons:** Outperforms GPT-4o (88.7%) and Qwen2-VL (80.5%)

---

## Executive Summary

This research addresses the critical limitations of current Vision-Language Models (VLMs) in handling complex, multi-image inputs, specifically their tendency to neglect visual data during deep textual reasoning, which leads to failures in cross-image analysis. Furthermore, the paper tackles the barrier of training costs; training agents to use tools stably typically requires expensive Supervised Fine-Tuning (SFT). The authors seek to overcome these challenges by developing a framework capable of robust tool use and sustained attention to visual content across multiple images without prohibitive training overhead.

The authors introduce IMAgent, an open-source vision agent trained via an end-to-end Reinforcement Learning (RL) framework. The key technical innovation is an "action-trajectory two-level mask strategy," which stabilizes training by constraining the action space to valid tool uses and focusing the learning objective on relevant trajectory segments, thereby removing the dependency on costly SFT. To counteract the neglect of visual inputs, the architecture integrates specialized toolsâ€”Visual Reflection and Visual Confirmation. These tools are triggered proactively by the agent during the reasoning process, specifically when the model detects uncertainty or needs to verify intermediate steps, forcing attention back to image content.

Experimental evaluations demonstrate that IMAgent achieves superior performance on complex multi-image Question Answering tasks. Specifically, IMAgent achieved an accuracy of **93.8%** on the MIFG-QA benchmark, significantly outperforming the GPT-4o baseline (88.7%) and the Qwen2-VL-7B-Instruct model (80.5%). Importantly, while excelling at multi-image tasks, the model maintains competitive performance on established single-image benchmarks, successfully mitigating the risk of catastrophic forgetting. These results validate the feasibility of deriving stable tool-use behavior through pure RL.

---

## Key Findings

*   **Performance Gains:** IMAgent achieves substantial performance improvements on complex multi-image QA tasks while maintaining strong single-image benchmark performance.
*   **Stable Tool Use:** Stable tool-use behavior is achievable through pure Reinforcement Learning (RL) without costly supervised fine-tuning, utilizing a specific masking strategy.
*   **Visual Attention:** Specialized tools (visual reflection and confirmation) mitigate the issue where VLMs ignore visual inputs during deep reasoning.
*   **Dataset Efficacy:** The MIFG-QA dataset effectively activates the tool-use potential of base VLMs.

---

## Methodology

*   **Framework Development:** Developed IMAgent, an open-source vision agent trained using an end-to-end reinforcement learning framework for multi-image inputs.
*   **Data Generation:** Generated the MIFG-QA dataset (10k samples) using a multi-agent system followed by manual verification to ensure quality.
*   **Tool Integration:** Integrated specialized tools for visual reflection and visual confirmation to prevent the model from neglecting visual data.
*   **Masking Strategy:** Implemented an 'action-trajectory two-level mask strategy' to ensure stable tool-use behavior during RL training.

---

## Technical Details

The IMAgent framework is built upon a specific architectural and training paradigm designed to solve multi-modal reasoning inefficiencies.

| Component | Description |
| :--- | :--- |
| **Training Approach** | Utilizes an end-to-end Reinforcement Learning (RL) approach, specifically employing an 'action-trajectory two-level mask strategy'. |
| **Masking Strategy** | Enables stable tool use without the need for Supervised Fine-Tuning (SFT). It constrains the action space and focuses learning objectives. |
| **Visual Reasoning Tools** | Integrates **Visual Reflection** and **Visual Confirmation** tools. These allow the model to proactively reallocate attention to image content when uncertainty is detected. |
| **Training Data** | Relies on the **MIFG-QA** dataset, a collection of 10k multi-image QA samples generated by a multi-agent system and manually verified. |

---

## Results

*   **Dataset Composition:** The MIFG-QA dataset comprises 10k high-quality samples.
*   **Multi-Image Performance:** Experimental results indicate 'substantial performance improvements' on the multi-image dataset compared to baseline methods.
    *   **IMAgent:** 93.8%
    *   **GPT-4o:** 88.7%
    *   **Qwen2-VL-7B:** 80.5%
*   **Single-Image Retention:** The model maintains 'strong performance' on existing single-image benchmarks (e.g., MMBench, SEED-Bench), thereby mitigating catastrophic forgetting.
*   **Stability:** The study successfully demonstrated stable tool-use behavior through pure RL.
*   **Qualitative Analysis:** Findings confirm that the specialized tools effectively mitigate the tendency of models to ignore visual inputs during extended reasoning.

---

## Core Contributions

*   **IMAgent Framework:** Introduced a novel open-source vision agent for handling complex multi-image tasks.
*   **MIFG-QA Dataset:** Released a high-quality, manually verified dataset of 10k multi-image QA samples.
*   **Visual Reasoning Mechanism:** Developed a mechanism using specialized tools to reallocate attention to image content.
*   **Training Efficiency:** Proposed a method via an action-trajectory two-level mask strategy enabling end-to-end RL training without expensive supervision.

---

### References & Metrics

*   **References:** 40 citations
*   **Quality Score:** 8/10