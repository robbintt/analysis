# Replicating ReLM Results: Validating Large Language Models with ReLM
*Reece Adamson; Erin Song*

---

### Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Total References** | 16 Citations |
| **Hardware Used** | AMD Ryzen 5800X / RTX-3070 & Intel i5-8400 / RTX-3060Ti |
| **Performance Gain** | ~10x Throughput Increase (Memorization Task) |
| **Core Mechanism** | Formal Languages (Regex) & Automata Transduction |

---

## Executive Summary

This research addresses critical limitations in contemporary Large Language Model (LLM) evaluation methodologies. Existing frameworks are hindered by significant operational drawbacks, including latency, imprecision, prohibitive computational costs, and the potential to introduce their own biases during assessment. As LLMs increasingly move into production environments, the need to accurately verify behavioral dimensions such as memorization, bias, and zero-shot performance becomes paramount. This paper is significant because it establishes a necessary framework for rigorous, cost-effective validation that unstructured, free-form evaluation methods fail to provide.

The core innovation validated in this study is the application of formal languages—specifically Regular Expressions (Regex)—to constrain and evaluate LLM outputs. Technically, the system utilizes a three-step automata transduction pipeline: first, parsing user-defined Regex into a Finite Automata; second, transducing this structure into an LLM-specific automaton that accounts for the model's unique vocabulary and tokenization schemes; and third, generating responses via automata traversal. During the traversal phase, the system employs Dijkstra’s algorithm to identify the shortest path, which represents the highest cumulative probability. This approach effectively maps character-level constraints to token-level paths, merging the flexibility of free-form generation with the rigor of structured data validation.

The study successfully reproduced key results from the original ReLM paper using consumer-grade hardware. In the Memorization experiment, ReLM achieved an approximate **10x increase in throughput** compared to the unconstrained baseline. In the Language Understanding experiments, tests verified that specific constraints consistently outperform the generic baseline. Consequently, the study positions ReLM as a viable systemic solution that mediates opposing demands in evaluation, offering a structured approach to the practical complexities of model productionization.

---

## Key Findings

*   **Application of Formal Languages:** The use of formal languages is a viable and effective mechanism for evaluating and controlling Large Language Models (LLMs) across key behavioral dimensions including memorization, bias, and zero-shot performance.
*   **Deficiencies in Current Methods:** Existing evaluation approaches are fundamentally limited by significant operational drawbacks, specifically slowness, imprecision, high computational costs, and the potential to introduce their own biases.
*   **Successful Reproduction:** The project successfully reproduced key results from the original ReLM paper, validating the initial findings.
*   **Systems Relevance:** The ReLM approach holds significant value for the "systems for machine learning" sub-field, offering a structured way to handle model productionization.

---

## Methodology

*   **Formal Language Framework:** The core methodological framework involves applying formal languages to the evaluation pipeline to ensure precision in testing LLM outputs.
*   **Reproductive Validation:** The study utilizes a reproduction methodology, re-executing key experiments from the original ReLM research to verify results.
*   **Expansive Analysis:** Beyond pure replication, the approach includes expounding on the original methods and exploring broader applications, specifically analyzing the approach through the lens of ML systems design.

---

## Technical Details

The ReLM system merges free-form responses with structured data by using Regular Expressions (Regex) to define constraints on LLM outputs. The architecture utilizes the following automata transduction pipeline:

1.  **Parsing:**  
    User-defined Regex is parsed into a Finite Automata.

2.  **Transduction:**  
    The Finite Automata is transduced into an LLM-specific automata that accounts for the specific vocabulary and tokenization of the model.

3.  **Traversal & Generation:**  
    The system traverses the automata to generate responses. This step employs **Dijkstra’s algorithm** to find the shortest path (representing the highest cumulative probability) and maps character-level constraints to token-level paths.

*   **Generation Assumption:** The LLM interaction assumes standard iterative autoregressive generation, selecting from the top-k most probable tokens.

---

## Results

**Hardware Configuration:**
The study was replicated on consumer-grade hardware:
*   AMD Ryzen 5800X / RTX-3070
*   Intel i5-8400 / RTX-3060Ti

**Memorization (URL Extraction):**
*   ReLM achieved an approximate **10x increase in throughput** compared to the unconstrained baseline.
*   *Note:* The original paper claimed a 15x increase; the difference in this reproduction is attributed to a smaller sample size.
*   The best baseline used a stop sequence of n=16.

**Language Understanding (Zero-Shot Accuracy):**
The study verified the trend that specific constraints outperform the generic baseline. Reproduced accuracies are as follows:

| Experiment Type | Accuracy |
| :--- | :--- |
| **Baseline** | 35% |
| **'Word' Constraint** | 45% |
| **'Terminated' Constraint** | 43% |
| **'No Stop' Constraint** | 48% |

---

## Contributions

*   **Validation of ReLM:** Provides independent empirical verification of the ReLM paper’s results, strengthening the credibility of using formal languages for LLM validation.
*   **Contextualization for ML Systems:** Explicitly bridges the gap between NLP evaluation techniques and systems research, highlighting how ReLM addresses the practical challenges of productionizing LLMs.
*   **Critical Evaluation Landscape:** Clarifies the trade-offs in current LLM evaluation methods, categorizing them by their specific failures (cost, bias, speed) and positioning ReLM as a solution to these systemic issues.