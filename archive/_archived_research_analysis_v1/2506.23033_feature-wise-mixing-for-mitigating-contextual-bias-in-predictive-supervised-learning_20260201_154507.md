# Feature-Wise Mixing for Mitigating Contextual Bias in Predictive Supervised Learning
*Yash Vardhan Tomar*

***

> **⚡ Quick Facts: Key Metrics**
>
> *   **Average Bias Reduction:** 43.35%
> *   **Top Performing Classifier:** Random Forest (46.68% reduction)
> *   **Most Effective Region:** Kolkata (72.94% reduction)
> *   **Computational Overhead:** Minimal (Pre-processing approach)
> *   **Attribute Independence:** No explicit bias identification required

***

## Executive Summary

Supervised learning models frequently suffer from contextual bias, where predictions are unfairly skewed by specific environmental or geographical contexts present in the training data. This issue is critical because existing mitigation strategies face substantial limitations: they typically require the explicit identification of sensitive bias attributes, which are often unknown or unavailable in real-world scenarios, and they impose significant computational overhead that hinders scalability. This paper addresses the complex challenge of reducing contextual bias without sacrificing predictive accuracy, specifically targeting the difficult trade-off between effective fairness interventions and computational efficiency in standard machine learning workflows.

The proposed solution is a **"Feature-Wise Mixing Framework"** designed to mitigate bias by redistributing feature representations across multiple distinct contextual datasets to synthesize a single, contextually balanced dataset. The core mechanism utilizes a mixing algorithm with scaling coefficients to ensure equal contribution from specific regional contexts (Mombasa, Colombo, and Kolkata), operating as a pre-processing technique that disrupts contextual correlations. By focusing on the redistribution of feature mixes rather than post-hoc corrections or sensitive feature identification, the framework offers an attribute-independent approach that prevents bias learning before model training begins.

Evaluated using SVM, KNN, Decision Trees, and Random Forest classifiers via 10-fold cross-validation, the framework achieved an overall average reduction in measured bias disparity ($\Delta_{bias}$) of **43.35%**. Performance varied by classifier, with Random Forest leading with a 46.68% reduction in bias, followed by Decision Trees (46.12%). In terms of predictive accuracy measured by Mean Squared Error (MSE), the method significantly outperformed raw data benchmarks—exemplified by dropping Random Forest MSE from 0.125 to 0.027—and consistently outperformed SMOTE oversampling. However, the Mixed Dataset was statistically inferior to the Reweighted Dataset method regarding raw MSE (e.g., Decision Trees MSE 0.0458 vs. Reweighted MSE 0.0123), illustrating that while it offers substantial improvement over untreated data, it may trail specific optimization baselines in pure error reduction.

This research contributes a significant shift toward attribute-independent fairness, demonstrating that effective bias mitigation can be achieved without identifying sensitive features, which is vital for privacy-sensitive or opaque datasets. By validating that fairness interventions can be implemented efficiently without the heavy computational load typical of fairness-aware algorithms, the framework offers a scalable and generalizable alternative to existing constraint-based methods.

---

## Key Findings

*   **Significant Bias Reduction:** The feature-wise mixing framework achieved an average bias reduction of **43.35%** across all experiments.
*   **Improved Predictive Performance:** The method demonstrated a statistically significant decrease in Mean Squared Error (MSE), indicating that bias mitigation did not come at the cost of accuracy.
*   **Superior to SMOTE:** The approach consistently outperformed SMOTE oversampling techniques and showed competitive effectiveness against established bias mitigation methods.
*   **Attribute-Independent Operation:** The framework operates effectively without requiring explicit bias attribute identification, distinguishing it from methods that rely on predefined sensitive features.
*   **Low Computational Overhead:** The proposed method avoids the heavy computational costs typically associated with fairness-aware learning algorithms.

---

## Methodology

The study introduces a feature-wise mixing framework designed to mitigate contextual bias by redistributing feature representations across multiple contextual datasets. The research design included the following steps:

1.  **Data Synthesis:** Utilization of a mixing algorithm to create a single contextually balanced dataset ($D_{mix}$) from distinct regional sources.
2.  **Classifier Training:** Training four distinct ML classifiers (SVM, KNN, Decision Trees, Random Forest) using a cross-validation technique.
3.  **Evaluation Metrics:** Assessment using bias-sensitive loss functions, incorporating specific disparity metrics and Mean Squared Error (MSE) as the standard measure of predictive performance.

---

## Technical Details

### Framework Overview
The **Feature-Wise Mixing Framework** synthesizes a contextually balanced dataset by redistributing feature representations without requiring explicit bias identification.

### Data Processing & Augmentation
*   **Datasets:** Combined data from Mombasa, Colombo, and Kolkata.
*   **Upscaling:** Original monthly dataset (765 points) upscaled to daily frequency (~23,000 points).
*   **Noise Injection:** Gaussian noise ($x'_i = x_i + \epsilon_i$) injected to prevent overfitting.
*   **Distributional Fidelity:** Kolmogorov-Smirnov tests confirmed the preserved distribution integrity ($D=0.032, p > 0.05$).

### The Mixing Algorithm
The synthesized dataset is created via the union of augmented datasets scaled by coefficients:
$$D_{mix} = \bigcup_{r=1}^{3} \alpha_r D^{aug}_r$$
Where $\alpha_r$ represents scaling coefficients ensuring equal regional contribution.

### Experimental Protocol
*   **Classifiers:** SVM, KNN, Decision Trees, Random Forest.
*   **Validation:** 10-fold cross-validation protocol.
*   **Split:** 80/20 training-to-test data split.

---

## Results

### Bias Reduction Performance
The framework achieved an overall average bias reduction ($\Delta_{bias}$) of **43.35%**.

*   **By Classifier (Averaged across regions):**
    *   Random Forest: 46.68%
    *   Decision Trees: 46.12%
    *   KNN: 45.40%
    *   SVM: 35.18%

*   **By Region:**
    *   Kolkata: 72.94% (Highest)
    *   Colombo: 42.22%
    *   Mombasa: 15.62%

### Predictive Accuracy (MSE)
The Mixed Dataset consistently outperformed individual regional datasets.
*   **Random Forest Example:** MSE dropped from **0.125** (Raw) to **0.027** (Mixed).

### Benchmarking
*   **vs. SMOTE:** Feature-Wise Mixing outperformed SMOTE Oversampling.
*   **vs. Reweighting:** The Mixed Dataset was statistically inferior to the Reweighted Dataset method in raw MSE (e.g., Decision Trees MSE 0.0458 vs Reweighted MSE 0.0123).

---

## Contributions

*   **Novel Framework for Bias Mitigation:** Introduced "feature-wise mixing" as an alternative to existing post-hoc corrections and rigid constraints, addressing scalability and generalizability issues in current techniques.
*   **Attribute-Independent Mitigation:** Contributed a method that functions without the need for explicit bias attribute identification, allowing for flexibility in scenarios where sensitive attributes are unknown or unavailable.
*   **Computational Efficiency:** Demonstrated that fairness interventions can be implemented efficiently, avoiding the heavy computational overhead that characterizes many fairness-aware learning algorithms.

***

**Paper Quality Score:** 9/10  
**References:** 18 citations