# Dual Precision Quantization for Efficient and Accurate Deep Neural Networks Inference

*Tomer Gafni; Asaf Karnieli; Yair Hanani*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 8/10
> *   **References:** 40 citations
> *   **Key Scheme:** W4A8 (4-bit Weights, 8-bit Activations)
> *   **Primary Models:** Llama-2/3, Qwen-VL
> *   **Hardware Targets:** Intel Gaudi 2/3, Modern Accelerators
> *   **Method:** Post-Training Quantization (PTQ)

---

## Executive Summary

### **Problem**
The deployment of Large Language Models (LLMs) is constrained by high memory bandwidth and computational latency associated with standard 16-bit floating-point (BF16/FP16) inference. While aggressive 4-bit quantization reduces memory footprints, it typically causes significant accuracy degradation or requires expensive Quantization-Aware Training (QAT) and specialized integer-only hardware.

### **Innovation**
The paper introduces the **Dual Precision Quantization (DPQ)** algorithm, optimized for a **W4A8 inference scheme** (4-bit integer weights, 8-bit floating-point activations). The innovation lies in a cascaded quantization process:
1.  Weights transform from 16-bit $\to$ 8-bit $\to$ 4-bit.
2.  Weights are up-sampled back to pseudo-16-bit.
3.  Quantization error is distributed using gradient-aware rounding.
This approach uses group-wise processing (group size 128) and requires **zero runtime overhead** during inference.

### **Results**
Empirical evaluation on Llama-2/3 and Qwen-VL shows high fidelity preservation:
*   **WikiText-2 Perplexity:** Increase $<$ 0.05 vs BF16.
*   **Reasoning Tasks:** Accuracy degradation $<$ 0.5% across 8 common sense tasks and MMLU.
*   **Benchmarking:** Outperforms RTN, naive implementations, and W4A16 GPTQ; competitive with QServe and QQQ.
*   **Efficiency:** Achieved approx. **2.5% throughput improvement** over BF16 operations on Gaudi 2 hardware.

### **Impact**
This research establishes W4A8 as a superior efficiency-accuracy trade-off for modern accelerators without relying on QAT. By leveraging standard 8-bit floating-point arithmetic (E4M3), the method is applicable to Intel Gaudi and NVIDIA Hopper architectures, offering a cost-effective path to deploying high-fidelity LLMs.

---

## Key Findings

*   **Performance:** The proposed **W4A8 scheme** (4-bit integer weights, 8-bit floating-point arithmetic) demonstrates significant speedups and improved memory utilization compared to standard 16-bit operations.
*   **Hardware Compatibility:** The scheme is applicable across various modern accelerators without requiring specialized hardware paths.
*   **Accuracy Retention:** The **Dual Precision Quantization (DPQ)** algorithm successfully mitigates accuracy loss, maintaining tolerable degradation relative to full-precision models.
*   **Efficiency:** The proposed methods achieve throughput and accuracy improvements without introducing additional inference overhead.

---

## Methodology

The research employs a **Post-Training Quantization (PTQ)** framework designed to address latency and memory constraints inherent in LLM deployment.

*   **Mixed-Precision Scheme (W4A8):** Separates storage and computation precision.
    *   **Storage:** 4-bit integer precision for weights.
    *   **Computation:** 8-bit floating-point arithmetic for inference.
*   **DPQ Algorithm:** Developed to counteract accuracy loss from 4-bit weights by leveraging structural properties of the scheme to optimize accuracy without adding computational burden during inference.

---

## Contributions

*   **Hardware-Efficient Scheme:** Introduced a W4A8 inference scheme that bridges low-bit storage with higher-precision computation.
*   **Algorithm Development:** Developed the **Dual Precision Quantization (DPQ)** algorithm to handle W4A8 trade-offs without incurring inference runtime costs.
*   **Empirical Benchmarking:** Provided evidence that mixed-precision quantization (4-bit weights, 8-bit compute) offers a superior efficiency-accuracy trade-off for modern accelerators compared to traditional 16-bit operations.

---

## Technical Details

### **Quantization Configuration**
| Component | Data Type | Precision | Notes |
| :--- | :--- | :--- | :--- |
| **Weights** | Integer (INT4) | 4-bit | Asymmetric quantization; Fixed group size of 128 |
| **Activations** | Floating Point (E4M3) | 8-bit | Static symmetric quantization; Calibrated on WebQuestions |
| **Computation** | Floating Point | 8-bit | Avoids specialized integer-only hardware paths |

### **DPQ algorithm specifics**
*   **Processing:** Group-wise processing using row-wise and per-group scales and zero-points.
*   **Cascaded Logic:**
    1.  Convert Weights: `INT16` $\to$ `INT8` $\to$ `INT4`
    2.  Up-sample to `pseudo-INT16`
    3.  Distribute quantization error to remaining weights.
*   **Hardware Adaptation:**
    *   **Gaudi 2:** Range $\pm 240$
    *   **Gaudi 3:** Range $\pm 448$
*   **Scope:** Quantizes all linear layers for Llama-2/3 and Qwen-VL, excluding embedding and `lm-head` layers.

---

## Evaluation Results

**Framework:** LM-Evaluation-Harness (Zero-shot setting)

**Metrics:** WikiText-2 perplexity, Average accuracy on 8 common sense reasoning tasks (HellaSwag, LAMBADA, BoolQ, ARC Easy, PIQA, Winogrande, ARC Challenge, Open-BookQA), and MMLU accuracy.

**Performance Overview:**
*   **Benchmark:** DPQ outperformed RTN, naive implementations, and W4A16 GPTQ (attributed to the Gradient-Aware Round method).
*   **Competitiveness:** Showed competitive performance against state-of-the-art methods like QServe and QQQ.
*   **Fidelity:** Compared to the BF16 reference, DPQ demonstrated only minor accuracy degradation.
*   **Efficiency:** The W4A8 scheme achieved significant throughput and memory utilization gains over 16-bit operations without introducing additional inference overhead.