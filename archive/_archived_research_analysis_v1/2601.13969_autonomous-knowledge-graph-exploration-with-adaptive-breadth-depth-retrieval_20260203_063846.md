---
title: Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval
arxiv_id: '2601.13969'
source_url: https://arxiv.org/abs/2601.13969
generated_at: '2026-02-03T06:38:46'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Autonomous Knowledge Graph Exploration with Adaptive Breadth-Depth Retrieval

*JoaquÃ­n Polonuer; Lucas Vittor; IÃ±aki Arango; Ayush Noori; David A. Clifton; Luciano Del Corro; Marinka Zitnik*

---

> ### ðŸ“Š Quick Facts & Metrics
>
> *   **Model Name:** ARK (Adaptive Retriever of Knowledge)
> *   **Key Performance:** 59.1% Hit@1 and 67.4% MRR on the STaRK dataset.
> *   **Competitive Edge:** Outperforms existing methods by up to **31.4%** in Hit@1.
> *   **Efficiency:** Distilled student model (8B parameters) retains **98.5%** of teacher performance.
> *   **Core Strategy:** Dual-operation toolset (Global Lexical Search + One-Hop Neighborhood Exploration).
> *   **Training Method:** Label-free imitation learning (no ground-truth labels required).

---

## Executive Summary

### Problem
Retrieving information from large, semi-structured knowledge graphs (KGs) presents a persistent challenge for Large Language Models (LLMs), primarily due to the difficulty in balancing search breadth against depth. Existing retrieval methods often struggle with the "breadth-depth dilemma," relying on static strategies or fragile seed node selection that fail to adapt to the varying complexity of different queries.

### Innovation
The authors introduce **ARK (Adaptive Retriever of Knowledge)**, an agentic KG retriever that grants LLMs the autonomy to dynamically manage the tradeoff between breadth and depth. Technically, ARK utilizes a dual-operation toolset: **Global Lexical Search (BM25)** for breadth-oriented discovery and **One-Hop Neighborhood Exploration** for depth-oriented expansion. The agent alternates between these tools based on real-time query requirements, employing an ensemble method with rank fusion for scoring. Additionally, the methodology features a label-free imitation learning pipeline that distills the reasoning trajectories of a large teacher model into a smaller 8B parameter student model.

### Results
ARK achieved state-of-the-art performance on the STaRK dataset, with the teacher model securing an average Hit@1 of **59.1%** and a Mean Reciprocal Rank (MRR) of **67.4%**, outperforming existing methods by up to **31.4%** in Hit@1. The distillation process proved highly efficient, as the 8B parameter student model retained **98.5%** of the teacher's Hit@1 performance while delivering absolute improvements on the AMAZON, MAG, and PRIME datasets.

### Impact
This research significantly advances the field by demonstrating that high-performance knowledge graph retrieval can be achieved through agentic tool use without the need for specific retrieval training or seed selection. By effectively solving the breadth-depth dilemma via dynamic adaptation, ARK provides a more flexible and reliable framework for querying semi-structured data. Furthermore, the study offers compelling evidence that the complex decision-making capabilities of large agentic models can be effectively compressed into smaller, efficient models.

---

## Key Findings

*   **State-of-the-Art Performance:** ARK achieved an average **59.1% Hit@1** and **67.4% MRR** on the STaRK benchmark, outperforming existing methods by up to **31.4%** in Hit@1.
*   **Effective Distillation:** Distilling ARK's tool-use trajectories into an 8B parameter student model resulted in significant absolute improvements in Hit@1 across AMAZON, MAG, and PRIME datasets.
*   **High Retention Rate:** The distillation process successfully retained up to **98.5%** of the teacher model's Hit@1 performance, proving the efficiency of the compression.
*   **Adaptive Strategy:** The model dynamically adapts its retrieval strategy based on query complexity:
    *   Utilizes **global lexical search** for language-heavy queries.
    *   Utilizes **neighborhood exploration** for relation-heavy queries.

---

## Methodology

ARK (Adaptive Retriever of Knowledge) functions as an agentic KG retriever that grants the language model autonomy to manage the tradeoff between search breadth and depth.

*   **Dual-Operation Toolset:**
    *   **Global Lexical Search:** Used for breadth-oriented discovery.
    *   **One-Hop Neighborhood Exploration:** Used for depth-oriented expansion.
*   **Dynamic Alternation:** ARK alternates between discovery and expansion dynamically based on query requirements rather than relying on fragile seed node selection.
*   **Label-Free Imitation Learning:** The methodology includes a training phase to distill reasoning trajectories from a large teacher model into an 8B student model without the need for ground-truth labels.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Architecture** | Interactive autonomous agent formulated as a trajectory generation process over semi-structured knowledge bases. |
| **Adaptive Strategy** | Utilizes **Global Search (BM25)** for breadth and **Neighborhood Exploration** (single-hop expansion) for multi-hop depth. |
| **Ranking Mechanism** | Ensemble method employing rank fusion based on voting and tie-breaking. |
| **Distillation Pipeline** | Teacher-student framework where an 8B parameter student model is trained via supervised fine-tuning on teacher trajectories. |
| **Loss Function** | Masks loss to only assistant tokens, removing the requirement for ground-truth labels. |

---

## Contributions

*   **Solving the Breadth-Depth Dilemma:** Introduces a system that dynamically balances breadth and depth approaches without requiring training or seed selection.
*   **Training-Free Retrieval:** Demonstrates that high-performance knowledge graph retrieval can be achieved by agentic tool use without the need for specific retrieval training.
*   **Model Compression:** Provides evidence that the decision-making processes of large agentic models can be effectively compressed into smaller models (8B parameters) for efficient deployment.

---

**Quality Score:** 8/10
**References:** 40 citations