# Robust Adversarial Reinforcement Learning in Stochastic Games via Sequence Modeling
*Xiaohang Tang; Zhuowen Cheng; Satyabrat Kumar*

---

> ### ðŸ“Š Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Quality Score** | 9/10 |
> | **References** | 22 Citations |
> | **Method Name** | CART (Conservative Adversarially Robust Decision Transformer) |
> | **Base Architecture** | Decision Transformer (DT) |
> | **Optimization** | Expectile Regression |
> | **Key Objective** | Maximize worst-case return against optimal adversaries |

---

## Executive Summary

**Problem**
Reinforcement learning (RL) in stochastic games presents a significant challenge when agents must operate in environments containing active adversaries. Traditional offline RL methods, particularly sequence-modeling approaches like Decision Transformers (DT), typically assume stationary environments and lack mechanisms to account for optimal opposing policies. Previous attempts to introduce robustness, such as Adversarially Robust Decision Transformers (ARDT), have suffered from over-optimism in value estimation. These methods fail to accurately weigh transition probabilities, often relying on rare, high-reward trajectories that result in policies which are easily exploited when facing a worst-case adversary in a stochastic setting.

**Innovation**
The paper introduces **Conservative Adversarially Robust Decision Transformer (CART)**, the first framework designed to explicitly enhance the robustness of Decision Transformers in adversarial stochastic games. Technically, CART modifies the DT architectureâ€™s conditioning mechanism, shifting away from conditioning on desired returns to conditioning on NashQ values derived from stage games. To address stochastic transition uncertainty, the method defines a stage-game payoff function, $\bar{Q}(s, a, \bar{a}) = \mathbb{E}_{s' \sim T(\cdot|s,a)} [r + V(s')]$, which integrates the expected value of subsequent states. The optimization target is then set to the worst-case scenario $Q_{CART}(s, a) = \min_{\bar{a}} \bar{Q}(s, a, \bar{a})$, effectively modeling the opponent's best response. This target is approximated offline using Expectile Regression to ensure the policy maximizes the worst-case return rather than an over-optimistic expectation.

**Results**
Evaluations conducted across five synthetic stochastic games utilizing $10^5$ offline trajectories demonstrate CART's superior adversarial robustness. In a comparative analysis of a two-stage game, CART achieved a worst-case return of **8.0** against an optimal adversary, significantly outperforming the standard Decision Transformer (**6.0**) and the previous ARDT baseline (**5.7**). Across all tested environments, CART consistently secured the highest average worst-case return and exhibited the lowest performance variation. The results indicated that while ARDT failed at high target returns due to overestimation from unweighted rare trajectories, CARTâ€™s conservative approach to transition uncertainty prevented these failure modes and delivered more reliable minimax value estimations.

**Impact**
This work represents a theoretical and empirical bridge between sequence modeling and game-theoretic reinforcement learning. By successfully integrating stage games and Nash equilibrium concepts into the Decision Transformer architecture, the authors provide the first empirical evidence that sequence-modeling-based RL can be made robust against adversaries. The ability to generate policies that are less exploitable in stochastic environments has significant implications for safety-critical applications where agents must interact with unpredictable or malicious opponents, establishing a new direction for robust offline RL research.

---

## Key Findings

*   **Accurate Estimation:** The proposed framework, CART, achieves more accurate minimax value estimation compared to existing methods.
*   **Superior worst-case returns:** CART consistently delivers superior worst-case returns across a variety of adversarial stochastic games.
*   **High Robustness:** The resulting policies are less exploitable by adversaries, indicating a higher level of adversarial robustness.
*   **Uncertainty Management:** The method effectively manages transition uncertainty by adopting a conservative approach to state transitions.

---

## Methodology

The methodology builds upon the **Decision Transformer (DT)** architecture but modifies its conditioning mechanism to handle adversarial environments.

*   **Stage Game Modeling:** It models the interaction between the protagonist and the adversary at each stage as a stage game.
*   **Payoff Definition:** It defines the payoff as the expected maximum value over subsequent states to incorporate stochastic transitions.
*   **Conditioning Shift:** Instead of conditioning policies on desired returns (standard DT), the framework conditions them on the **NashQ value** derived from these stage games.

---

## Contributions

The primary contributions of this work are threefold:

1.  **CART Framework:** Introduction of the Conservative Adversarially Robust Decision Transformer (CART), the first framework designed to enhance the robustness of Decision Transformers specifically within adversarial stochastic games.
2.  **Theoretical Bridge:** The work theoretically bridges sequence modeling with game-theoretic concepts by using stage games and NashQ values to account for adversarial interactions.
3.  **Empirical Evidence:** It provides empirical evidence that sequence-modeling-based RL can be made robust against adversaries.

---

## Technical Details

**Proposed Method**
*   **CART (Conservative Adversarially Robust Decision Transformer):** Builds upon the Decision Transformer architecture to handle stochastic games.
*   **Modification:** Modifies the conditioning variable by introducing a State Value Function ($V$) to account for probabilistic transitions, addressing the over-optimism found in ARDT.

**Objective Function**
*   **Goal:** Maximize the worst-case return ($\max_\pi \min_{\bar{\pi}} \mathbb{E}[\sum r_t]$).

**Mathematical Formulation**
*   **Stage-Game Payoff:**
    $$\bar{Q}(s, a, \bar{a}) = \mathbb{E}_{s' \sim T(\cdot|s,a)} [r + V(s')]$$
*   **Optimization Target:**
    $$Q_{CART}(s, a) = \min_{\bar{a}} \bar{Q}(s, a, \bar{a})$$
*   **Optimization:** Performed via **Expectile Regression** to approximate Nash Q-values from the offline dataset.

---

## Results

Evaluations focused on worst-case return against an optimal adversary in synthetic stochastic games with $10^5$ offline trajectories.

*   **Two-Stage Game Performance:**
    *   **CART:** Achieved a highest worst-case return of **8.0**.
    *   **DT:** Achieved a return of **6.0**.
    *   **ARDT:** Achieved a return of **5.7**.
*   **General Performance:**
    *   Across 5 different games, CART demonstrated superior robustness with the highest average worst-case return.
    *   CART exhibited the lowest performance variation among tested methods.
*   **Failure Mode Analysis:**
    *   **ARDT:** Showed failure modes at high target returns, overestimating values due to reliance on rare, high-payoff trajectories without proper transition probability weighting.
    *   **CART:** Successfully mitigated these issues through its conservative approach.