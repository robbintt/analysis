# Data Augmentation for Deep Learning Regression Tasks by Machine Learning Models

*Assaf Shmuel; Oren Glickman; Teddy Lazebnik*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Datasets Analyzed** | 30 Distinct Datasets |
| **Frameworks Tested** | AutoKeras, H2O, AutoGluon |
| **Performance Gain** | >10% (Average) |
| **Top Rank Frequency** | Best Method in 26/60 total configs |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |

---

## Executive Summary

Deep learning (DL) has historically underperformed in tabular regression tasks compared to traditional machine learning, often due to the scarcity of training data and the lack of effective regularization techniques. Unlike computer vision, where geometric transformations effectively augment data, tabular data augmentation is challenging; naive methodsâ€”such as simple duplication or random noise injectionâ€”often distort the underlying feature correlations and degrade model performance.

This paper addresses this critical bottleneck by investigating how to generate synthetic data that maintains the integrity of statistical relationships. The authors propose a **"Teacher-Student" framework** utilizing a tree-based AutoML model (TPOT) as the Teacher to guide the training of the AutoDL Student. By rigorously evaluating across 30 datasets and three major AutoDL frameworks, the proposed method outperformed standard techniques like C-mixup, ADA, and naive noise injection.

The research demonstrates that sophisticated augmentation strategies can close the performance gap with traditional methods. With an average improvement of over 10% in RMSE and marked success in small datasets (12.7% boost), this work paves the way for broader adoption of deep learning in real-world regression scenarios where data acquisition is expensive.

---

## Key Findings

*   **Superiority of Advanced Augmentation:** Sophisticated data augmentation strategies that preserve statistical relationships consistently outperform naive methods like noise duplication.
*   **Significant Performance Gains:** Advanced techniques resulted in an **average performance increase of over 10%** compared to baseline models.
*   **Broad Applicability:** Efficacy was validated across **30 distinct datasets**, showing consistency in regression tasks.
*   **Framework Agnostic Success:** Improvements were replicated across **AutoKeras, H2O, and AutoGluon** frameworks.
*   **Inverse Correlation with Size:** The method proved most effective for small datasets (12.7% improvement) but maintained positive gains on large datasets (1.23% improvement).

---

## Technical Details

### Framework Architecture: Teacher-Student
The paper proposes a specific implementation for tabular data augmentation:
*   **Teacher Model:** An AutoML model (**TPOT**) trained on original data.
*   **Student Model:** An AutoDL model (AutoKeras, H2O, or AutoGluon).
*   **Process:**
    1.  The Teacher learns statistical distributions from the original data.
    2.  Synthetic features are generated by adding Gaussian noise to original features.
    3.  Noise Formula: $N(\mu_c, \eta \cdot \sigma_c)$ where $\eta=5\%$.
    4.  The Teacher predicts labels for the synthetic data.
    5.  The Student is trained on the combination of original and synthetic data.

### Experimental Configuration
*   **Datasets:** 30 datasets utilized (20 for SOTA comparison, 10 large-scale for scaling analysis).
*   **Baselines:** Compared against Naive augmentation, C-mixup, and ADA.
*   **Robustness Testing:** Validated across noise levels of 1%, 5%, and 10%.

---

## Methodology

The researchers conducted a comparative evaluation of Neural Networks on tabular data regression tasks. They examined a spectrum of data augmentation techniques ranging from simple duplication and noise injection to advanced methods preserving statistical properties.

To ensure robustness and generalizability, the models were rigorously validated using 30 datasets and executed across three distinct Automated Deep Learning frameworks: **AutoKeras, H2O, and AutoGluon**.

---

## Performance Results

The proposed method achieved the rank of **'Best Augmentation Method'** most frequently across the 20 primary datasets:
*   **H2O:** Ranked best in 10/20 datasets.
*   **AutoKeras:** Ranked best in 9/20 datasets.
*   **AutoGluon:** Ranked best in 7/20 datasets.

### Scaling Analysis
Across 75 different configurations, all showed positive mean and median improvements based on RMSE.
*   **Small Datasets (500 rows):** ~12.7% improvement.
*   **Large Datasets (50,000 rows):** ~1.23% improvement.

---

## Core Contributions

*   **Bridging the Performance Gap:** Addresses the underutilization of Deep Learning in tabular regression by significantly boosting performance.
*   **Validation of Statistical Preservation:** Establishes that preserving statistical relationships in data augmentation is crucial for accuracy.
*   **Enabling Practical Adoption:** Contributes to broader adoption of deep learning in real-world regression by demonstrating substantial performance gains without requiring architectural changes.

---

*Report generated based on analysis of Quality Score: 8/10*