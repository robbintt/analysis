# When Does Multi-Task Learning Fail? Quantifying Data Imbalance and Task Independence in Metal Alloy Property Prediction

*Sungwoo Kang*

---

> ### ðŸ“„ Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Dataset Size:** 54,028 alloy samples
> *   **Tasks Analyzed:** 3 (2 Regression, 1 Classification)
> *   **Key Insight:** Regression tasks failed due to "negative transfer"; Classification tasks improved.
> *   **Gradient Correlation:** ~0.00 (Orthogonal tasks)
> *   **Recommendation:** Use MTL for classification; avoid MTL for precise regression in this context.

---

## Executive Summary

**Problem**
Multi-Task Learning (MTL) is a widely utilized paradigm in computational materials science, predicated on the assumption that distinct material properties share underlying physical features that allow simultaneous training to improve generalization. However, this paradigm frequently suffers from "**negative transfer**," a phenomenon where joint training degrades performance relative to single-task models. This paper addresses the critical challenge of identifying when MTL is counterproductive, specifically examining the impact of severe data imbalance and the lack of true task relatedness in metal alloy property prediction. Understanding these failure modes is essential to prevent wasted computational resources and the propagation of inaccurate predictions in high-stakes materials discovery pipelines.

**Innovation**
The study's key innovation is a rigorous diagnostic framework designed to quantify explicit task relationships and data imbalance to explain MTL performance inconsistencies. Utilizing a dataset of 54,028 alloy samples, the researchers benchmarked Independent, Standard MTL (hard parameter sharing), and Structured MTL architectures across three target properties: electrical resistivity, Vickers hardness (both regression), and amorphous-forming ability (classification). Technically, the authors employed **Task Relation Graph analysis** and **Gradient Alignment analysis** using cosine similarity to measure the extent of shared physics between tasks. While they evaluated standard Deep Imbalanced Regression (DIR) mitigation strategies to test the limits of recovery, their primary technical contribution was the use of these diagnostic tools to isolate how fundamental task incompatibility, rather than just data distribution, drives negative transfer.

**Results**
The empirical analysis revealed a severe performance dichotomy: MTL degraded regression tasks while improving the classification task. Joint learning caused a **16.6% relative decrease** in hardness prediction performance and introduced heavier error tails, whereas the classification task saw a **17% increase in recall** (to 0.722) and an improved F1-score of 0.744. Diagnostic metrics confirmed that the regression tasks lacked shared physics; inter-task weights were near-zero (0.006 Â± 0.012), and gradient cosine similarities approximated 0.00, indicating task orthogonality. Although mitigation strategies showed some localized returnsâ€”PCGrad boosted Hardness $R^2$ by +12.4% and LDS+GradNorm improved Resistivity $R^2$ by +2.4%â€”they were insufficient to fully resolve the fundamental incompatibility of the tasks within a shared MTL framework.

**Impact**
This research significantly influences the field by providing empirical evidence that challenges the blanket application of MTL in materials informatics. It establishes clear, actionable guidelines for computational materials scientists: MTL should be avoided for precise regression tasks where data imbalance and task independence can trigger negative transfer, favoring independent models instead. Conversely, MTL remains a viable strategy for classification tasks where high recall is prioritized over fine-grained precision. By explicitly linking failure modes to data characteristics and task interdependence, this study enables researchers to make more informed architectural decisions, ultimately leading to more reliable predictive models in materials science.

---

## Key Findings

*   **Performance Dichotomy:** MTL demonstrated opposing effects based on task type:
    *   **Regression:** Significant degradation (negative transfer) observed for both electrical resistivity and Vickers hardness.
    *   **Classification:** Improved performance for Amorphous-forming ability prediction.
*   **Root Cause of Failure:** The degradation in regression tasks was driven by severe data imbalance and a lack of shared underlying physics between the properties.
*   **Task Independence:** Analysis of inter-task weights revealed values near **zero (0.006 Â± 0.012)**, suggesting the properties do not share significant physical features in the learned feature space.
*   **Architectural Recommendations:**
    *   **Avoid MTL** for precise regression tasks; Independent models are superior.
    *   **Use MTL** for classification tasks where high recall is the priority.

---

## Methodology

The research employed a comprehensive benchmarking strategy to isolate the effects of multi-task learning against single-task baselines.

1.  **Dataset:** utilized a large dataset of **54,028 alloy samples**.
2.  **Target Variables:** Simultaneous prediction of three distinct properties:
    *   Electrical Resistivity (Continuous/Regression)
    *   Vickers Hardness (Continuous/Regression)
    *   Amorphous-forming Ability (Categorical/Classification)
3.  **Benchmarking:** Compared Single-Task Learning (STL) against:
    *   Standard MTL (Hard parameter sharing)
    *   Structured MTL (Incorporating a dynamic task relation graph)
4.  **Validation:** Employed statistical significance testing to ensure validity of results.
5.  **Diagnostics:** Analyzed inter-task weights and gradient behaviors to diagnose performance variations.

---

## Technical Details

### Model Architectures
*   **Independent Models:** Trained separately for each property.
*   **Standard MTL:** Utilized hard parameter sharing.
*   **Structured MTL:** Incorporated a dynamic task relation graph.

### Task Definitions
*   **Regression Tasks:** Electrical Resistivity, Vickers Hardness.
*   **Classification Task:** Amorphous-forming ability.

### Diagnostic Methods
*   **Task Relation Graph Analysis:** Used to map relationships between tasks.
*   **Gradient Alignment Analysis:** Measured cosine similarity between task gradients.
*   **Transfer Utility Assessment:** Evaluated the benefit of transferring knowledge between specific tasks.
*   **Deep Imbalanced Regression (DIR) Mitigation:** Tested various strategies including Label Distribution Smoothing (LDS), Balanced MSE, PCGrad, and GradNorm.

---

## Results

### Performance Impact
*   **Regression Tasks:**
    *   Vickers Hardness experienced a **16.6% relative decrease** in performance.
    *   Error tails became heavier (larger outliers).
*   **Classification Tasks:**
    *   Amorphous Recall increased by **17%** (reaching 0.722).
    *   F1 Score improved to 0.744 compared to the baseline of 0.703.

### Architectural Comparison
*   Structured MTL provided **no significant benefit** over Standard MTL, suggesting that complex relation modeling did not capture useful shared information.

### Diagnostic Metrics
*   **Inter-task Weights:** Averaged **0.006 Â± 0.012**, effectively zero.
*   **Gradient Alignment:** Cosine similarity approximated **0.00**, indicating gradients are orthogonal or conflicting rather than aligned.
*   **Transfer Learning:** Transfer from Resistivity to Hardness showed no significant benefit ($R^2$ 0.860 vs 0.868, $p=0.127$).

### Mitigation Strategy Outcomes (DIR)
*   **PCGrad:** Boosted Hardness $R^2$ to 0.855 (+12.4%), but negatively impacted Resistivity.
*   **LDS + GradNorm:** Boosted Resistivity $R^2$ to 0.894 (+2.4%).

---

## Contributions

*   **Empirical Testing:** Provides a critical empirical test of MTL assumptions within the domain of materials science, specifically quantifying the limits of "shared physics."
*   **Technical Explanation:** Explicitly links severe data imbalance to negative transfer in regression tasks, offering a concrete technical explanation for why MTL fails in these scenarios.
*   **Guidelines for Deployment:** Establishes clear, actionable guidelines for computational materials scientists regarding when to use MTL (for classification/recall) versus single-task models (for precise regression).

***

**References:** 30 Citations