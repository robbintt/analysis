# Pass@K Policy Optimization: Solving Harder Reinforcement Learning Problems

*Christian Walder; Deep Karkhanis*

***

> **ðŸ“Š Quick Facts**
>
> *   **Quality Score:** 7/10
> *   **Citations:** 40
> *   **Core Method:** PKPO (Pass@K Policy Optimization)
> *   **Key Innovation:** Unbiased estimators for pass@k allowing joint sample optimization
> *   **Test Model:** GEMMA-2 LLM
> *   **Optimization Target:** Pass@K (Probability at least one sample in *k* is correct)

***

## Executive Summary

Conventional Reinforcement Learning (RL) algorithms applied to Large Language Models (LLMs) are fundamentally limited by a focus on **pass@1**â€”the probability that a single individual sample is correct. This optimization target implicitly encourages risk-averse policies, which discourages exploration on complex problems with sparse rewards. In domains requiring multi-step reasoning, the solution space is vast and high-reward trajectories are rare; consequently, standard RL agents settle for local optima or stall, unable to discover the "outlier" solutions necessary for correctness. This creates a critical bottleneck in solving hard reasoning and coding tasks where the intrinsic probability of generating a correct solution in a single attempt is low.

The authors introduce **Pass@K Policy Optimization (PKPO)**, a framework that shifts the optimization target from the strength of individual samples to the collective utility of a sample set. The core technical advancement is the derivation of low-variance, unbiased estimators for pass@k and its gradients, valid for any arbitrary $k \le n$ rather than the restrictive $k=n$. For binary rewards, the method employs the estimator $\rho = 1 - \frac{\binom{n-c}{k}}{\binom{n}{k}}$ to mathematically transform rewards, effectively reducing complex set optimization to standard RL execution. Additionally, PKPO utilizes a **k-annealing strategy** during training, allowing the model to prioritize diversity early (high $k$) and refine for quality later (low $k$), thereby optimizing for both pass@1 and pass@k metrics without a trade-off.

Empirical validation on the GEMMA-2 LLM demonstrates that PKPO effectively "unblocks" learning on challenging tasks where standard pass@1 optimization stagnates. In a controlled 1D toy environment, pass@1 optimization converged to a risk-averse policy with modest mean rewards, whereas PKPO fostered a risk-tolerant distribution that successfully sought high-reward outliers. Theoretical validation confirms the derived estimators are unbiased and maintain asymptotic variance properties of $O(1/n)$, with the authors establishing that unbiased estimation is strictly impossible if the batch size $n$ is less than $k$.

***

## Key Findings

*   **Suboptimal Sampling Capacity:** Conventional RL algorithms waste potential by optimizing for pass@1 (individual sample strength) rather than pass@k (collective diversity and utility), limiting exploration on difficult problems.
*   **Flexible Parameterization:** Unlike previous methods restricted to $k=n$, the proposed PKPO method enables robust optimization of pass@k for any arbitrary value where $k \leq n$.
*   **K-Annealing Strategy:** By annealing the parameter $k$ during training, the method allows for the optimization of both pass@1 and pass@k metrics without requiring a trade-off between them.
*   **"Unblocking" Learning:** Empirical validation using the GEMMA-2 LLM demonstrates that higher $k$ values enable the solving of a greater number of harder problems, effectively "unblocking" learning on challenging tasks where standard pass@1 optimization stalls.
*   **Variance-Reduced Estimators:** The derived estimators possess variance-reducing properties that allow optimization to be reduced to standard RL with jointly transformed rewards.

***

## Methodology

The core methodology of this research involves transforming the optimization landscape of Reinforcement Learning from single attempts to batch utility:

1.  **Reward Transformation:** The authors apply a transformation to final rewards to shift the optimization target from individual samples to sets of samples (joint utility).
2.  **Novel Estimators:** They derived novel, low-variance unbiased estimators for both pass@k and its gradient. These estimators are valid in both binary reward settings and continuous reward settings.
3.  **Reduction to Standard RL:** The optimization process is mathematically reduced to standard RL execution where rewards have been modified by a specific stable and efficient transformation function.
4.  **K-Annealing:** The methodology incorporates a strategy to anneal the $k$ parameter during the training process. This boosts performance across different metrics by allowing the model to explore broadly before refining specifically.

***

## Technical Details

**Approach Overview**
*   **Name:** PKPO (Pass@K Policy Optimization)
*   **Goal:** Optimize pass@k metric (probability at least one sample in $k$ is correct) rather than pass@1.
*   **Scope:** Supports any $k \le n$.
*   **Implementation:** A modification to policy gradient updates, coupling inference-time search with parameter updates.

**Mathematical Formulations**

*   **Binary Rewards:**
    *   **Estimator:** $\rho = 1 - \frac{\binom{n-c}{k}}{\binom{n}{k}}$
    *   **Gradient:** Assigns rewards $k/n$ to correct samples and proportional probabilities to incorrect ones.

*   **Continuous Rewards:**
    *   **Metric:** Optimizes `maxg@k` using a weighted sum of sorted rewards.

**Statistical Properties**
*   **Variance Reduction:** The method supports variance reduction.
*   **Asymptotic Variance:** achieves $O(1/n)$.

***

## Contributions

*   **Theoretical Advancements:** Introduction of novel unbiased estimators for pass@k and its gradients for both continuous and binary rewards, offering superior variance properties.
*   **Expanded Optimization Landscape:** Removes the restriction that $k$ must equal $n$, thereby allowing for flexible optimization of joint sample sets.
*   **Real-World Application:** Successful application of the reward transformation to real-world Large Language Models (specifically GEMMA-2), proving that pass@k optimization can solve tasks that are intractable for pass@1 methods.
*   **Exploration Mechanism:** Identification of joint utility prioritization as a mechanism for improved exploration, solving the issue of learning stagnation on complex problem sets.

***

## Results

**High-Level Findings**
*   Higher $k$ values enable the solving of more "harder" problems compared to pass@1 optimization, "unblocking" learning on challenging tasks.
*   **1D Toy Problem:** pass@1 results in a risk-averse optimal policy, whereas pass@k fosters a risk-tolerant policy that shifts the mean to seek high-reward outliers.

**Theoretical Validation**
*   **Unbiasedness:** Confirmed that the estimators are unbiased.
*   **Variance:** Asymptotic variance decreases at $O(1/n)$.
*   **Constraints:** Established that unbiased estimation is impossible if batch size $n < k$.