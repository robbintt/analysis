# Learning from Active Human Involvement through Proxy Value Propagation

*Zhenghao Peng; Wenjie Mo; Chenda Duan; Quanyi Li; Bolei Zhou*

***

> ### ðŸ“Š Report Metrics
> **Quality Score:** 8/10  
> **References:** 40 Citations  
> **Core Framework:** Proxy Value Propagation (PVP)  
> **Primary Domains:** MiniGrid, MetaDrive, CARLA, GTA V  
> **Key Innovation:** Reward-free learning via active human intervention

***

## Executive Summary

Reinforcement Learning (RL) systems rely heavily on manually specified reward functions to guide policy optimization, a dependency that introduces critical risks regarding safety and alignment. Reward engineering is notoriously brittle and prone to misspecification, often resulting in "reward hacking," where agents exploit loopholes in the objective rather than learning the intended behavior. More critically, optimizing for these rewards in complex, high-dimensional environments typically necessitates exploratory behaviors that can be unsafe or destructive. The paper addresses the need for a learning paradigm that bypasses these pitfalls, specifically focusing on how to derive safe, aligned policies without relying on extrinsic environmental rewards or dangerous exploration strategies.

The authors introduce **Proxy Value Propagation (PVP)**, a novel reward-free framework that modifies Temporal Difference (TD)-learning to rely on active human intervention as the primary learning signal. The technical core of PVP is a dual-buffer system and a modified loss function that decouples learning from environmental rewards. Transitions are stored in either a Novice Buffer (agent-only) or a Human Buffer (intervention data). The method assigns binary proxy values: state-action pairs from human demonstrations receive a high value (+1), while agent actions that trigger human corrective feedback receive a low value (-1). Crucially, PVP addresses the temporal integration of these signals through a "Reward-Free TD loss," which propagates value estimates backward using only the discount factor $\gamma$. This mechanism mathematically anchors Q-values to human interventions and propagates this "safety signal" to preceding timesteps, teaching the agent to avoid states that precede a takeover.

Empirical validation across MiniGrid, MetaDrive, CARLA, and **Grand Theft Auto V (GTA V)** demonstrated that PVP significantly outperforms standard baselines like TD3 in both sample efficiency and safety. In discrete and continuous control tasks, PVP achieved higher asymptotic success rates with substantially fewer environment interactions. Crucially, the results highlight specific baseline failures: standard TD3 agents failed catastrophically in the high-dimensional GTA V driving task, succumbing to crashes and inability to navigate, whereas PVP agents successfully mastered complex driving behaviors solely through intervention data. Quantitative analysis showed that PVP maintains a significantly wider margin between the Q-values of human-approved actions and agent-triggered actions compared to standard CQL. This explicit statistical distinction in the value function correlates with a lower probability of executing unsafe actions, effectively preventing the state sequences that historically trigger human intervention.

***

## Key Findings

*   **Reward-Free Learning:** The study establishes a viable method for policy optimization that does not rely on explicit environmental rewards, instead utilizing active human intervention to guide the agent.
*   **Value Propagation Mechanism:** It was found that human intent can be effectively encoded by labeling human demonstrations with high values and agent actions that triggered intervention with low values, which are then propagated to unlabeled exploration data.
*   **Behavioral Emulation:** The proxy value function successfully induces a policy that faithfully emulates human behavior, ensuring the agent aligns with the human subjectâ€™s execution of tasks.
*   **Broad Applicability:** The method demonstrated generality and efficiency across both continuous and discrete control tasks and is compatible with various human control devices.
*   **Complex Task Mastery:** The approach is robust enough to handle complex, high-dimensional environments, validated by the agent's ability to learn driving tasks within Grand Theft Auto V.

***

## Technical Details

The proposed approach operates without environmental rewards, relying on active human intervention (button press/takeover) to define optimization objectives. It modifies value-based RL (TD3/DQN) by removing the reward component from the Temporal Difference (TD) loss and introducing a 'Proxy Value' (PV) loss.

### Architecture Components
*   **Dual-Buffer System:**
    *   **Novice Buffer:** Stores agent-only transitions.
    *   **Human Buffer:** Stores transitions where humans intervened, capturing both the agent's attempted action and the human's corrective action.
*   **Loss Functions:**
    *   **Reward-Free TD Loss:** Propagates value estimates using only the discount factor $\gamma$.
    *   **Proxy Value (PV) Loss:** Assigns a Q-value of +1 to human actions and -1 to agent actions that trigger intervention.
*   **Optimization:** The Q-network is optimized by minimizing the combined TD and PV losses.

### Theoretical Underpinnings
The method connects to **Conservative Q-Learning (CQL)** with L2 regularization to prevent unbounded Q-value growth. It resolves the data scarcity issue inherent in intervention by optimizing Q-values directly without requiring unobserved future states for intervened actions.

***

## Methodology

The **Proxy Value Propagation (PVP)** framework operates within a Temporal Difference (TD)-learning structure but eliminates the need for external rewards. Instead, it utilizes a proxy value function to express human intents through two primary labeling steps:

1.  **Positive Labeling:** State-action pairs derived from human demonstrations are assigned high values.
2.  **Negative Labeling:** State-action pairs where the agent's actions prompted human intervention (corrective feedback) are assigned low values.

These labeled values serve as the ground truth for the TD-learning process, allowing the system to propagate these value signals to unlabeled data generated during the agent's exploration. This shapes the agent's policy to mimic the human's safe and aligned behaviors without needing to design a reward function for the specific task.

***

## Contributions

*   **Novel Framework:** Introduction of Proxy Value Propagation (PVP), a new, reward-free method for learning from active human involvement that addresses challenges in safety and AI alignment.
*   **Human-Centric Value Assignment:** A technical contribution in defining a proxy value function that differentiates between demonstrated actions (high value) and intervened actions (low value) to implicitly encode human preferences.
*   **Algorithmic Efficiency:** Development of a learning strategy that integrates seamlessly with existing reinforcement learning algorithms, requiring only minimal modifications to implement human-in-the-loop learning.
*   **Empirical Validation:** Providing evidence through human-in-the-loop experiments that the method generalizes across different control regimes (continuous/discrete) and complex simulators (GTA V), pushing the boundary of what is learnable via human intervention.

***

## Results

The method was validated across diverse domains including **MiniGrid** (Keyboard), **MetaDrive** (Gamepad), **CARLA** (Steering Wheel), and **GTA V** (Keyboard).

*   **Sample Efficiency:** PVP demonstrated superior sample efficiency compared to standard value-based RL (TD3), reaching higher success rates faster.
*   **Asymptotic Performance:** It achieved better asymptotic performance than the TD3 baseline in both discrete and continuous environments.
*   **Complex Environments:** In GTA V, the agent successfully learned complex driving behaviors relying solely on human intervention data, whereas baseline agents failed.
*   **Action Distinction:** Analysis showed that PVP creates a clearer distinction between the Q-values of human and agent actions compared to standard CQL, effectively preventing the policy from executing actions that trigger intervention.