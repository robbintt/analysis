# Exploring Vision Language Models for Multimodal and Multilingual Stance Detection

*Jake Vasilakes; Carolina Scarton; Zhixue Zhao*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Languages** | 7 (English, German, Spanish, French, Portuguese, Hindi, Chinese) |
| **Dataset Size** | 1,778 validated examples |
| **Topics Covered** | 5 (Mergers & Acquisitions, Taiwan Question, 2020 US Election, Russia-Ukraine, Chloroquine) |
| **Class Balance** | Neutral (44.4%), Favor (29.5%), Against (26.2%) |
| **References** | 14 Citations |

---

## Executive Summary

Stance detection is essential for monitoring public opinion and tracking misinformation, yet existing research predominantly confines itself to text-only, English-only inputs, failing to reflect the multimodal and multilingual reality of global social media platforms. This paper addresses this significant gap by investigating the capabilities of state-of-the-art Vision-Language Models (VLMs) in performing multimodal stance detection across multiple languages.

Understanding whether these sophisticated models can effectively fuse visual and textual inputsâ€”or simply default to textual analysisâ€”is critical for the advancement of accurate global content moderation and sentiment analysis systems. The authors introduce a newly extended dataset comprising tweet-image pairs from X (Twitter) covering five distinct news topics, scaled into seven languages (English, German, Spanish, French, Portuguese, Hindi, and Chinese) via machine translation.

To evaluate this, the study employs VLM architectures pairing a visual encoder with a Large Language Model (LLM). Technically, the process involves encoding images into embeddings, projecting them into the LLM embedding space as "image tokens," and concatenating them with text tokens to enable cross-modality interaction. The methodology includes specific preprocessing steps for robustness, such as normalizing URLs to 'HTTPURL', mentions to '@USER', and extracting the first frame for GIFs and videos to standardize input data.

Evaluation on a validation set of 1,778 examples, with a specific class distribution of 44.4% Neutral, 29.5% Favor, and 26.2% Against, revealed a decisive reliance on textual data over visual input. The study found that VLMs prioritize textual features consistently across all tested languages; when visual information is utilized, models rely heavily on embedded text within images (e.g., captions or overlays) rather than interpreting non-text visual features. While predictions remained consistent across all seven languages without explicit multilingual training, the analysis identified outlier cases where prediction errors did not correlate with macro F1 scores, language support levels, or model size.

This research establishes a critical baseline for multimodal and multilingual stance detection, exposing the limitation that current VLMs effectively function as text-classifiers even when provided with images. By demonstrating that state-of-the-art models fail to genuinely integrate non-text visual features, the authors challenge the field to develop architectures capable of deeper cross-modal fusion. The release of the extended, multilingual dataset provides a valuable resource for future benchmarking, pushing the community toward more robust models capable of handling the complex, multimodal nature of global social media discourse.

---

## Key Findings

*   **Dominance of Textual Input:** VLMs rely significantly more on textual input than on images for determining stance. This trend is consistent across all tested languages.
*   **Visual Interpretation Limitations:** When utilizing visual information, VLMs depend heavily on embedded text (e.g., captions or overlays) rather than interpreting non-text visual features.
*   **Cross-Lingual Consistency:** Predictions are generally consistent across seven languages, regardless of explicit multilingual training.
*   **Outlier Anomalies:** While consistent, outlier cases exist where predictions do not align with macro F1 scores, language support, or model size.

---

## Methodology

The study conducts an evaluation of state-of-the-art Vision-Language Models (VLMs) to assess multimodal and multilingual stance detection capabilities. The research methodology is structured around three primary investigations:

1.  **Utilization of Visual Cues:** Analyzing how models process image data.
2.  **Language-Specific Performance:** Evaluating metrics across the seven included languages.
3.  **Cross-Modality Interactions:** Examining how text and image embeddings interact within the model.

The study utilizes a newly extended dataset incorporating multimodal inputs (text and images) covering seven distinct languages.

---

## Technical Details

**Architecture**
*   **Components:** Comprises a visual encoder and a Large Language Model (LLM).
*   **Process:**
    1.  Images are encoded into embeddings.
    2.  Embeddings are projected into the LLM embedding space as "image tokens".
    3.  Image tokens are concatenated with text tokens for processing.

**Dataset & Preprocessing**
*   **Source:** Tweet-image pairs from X (Twitter) recovered via API covering 5 news topics.
*   **Preprocessing Steps:**
    *   Normalizing URLs to 'HTTPURL'.
    *   Normalizing mentions to '@USER'.
    *   Extracting the first frame for GIFs and videos.
*   **Translation:** Multilingual variants are generated using machine translation into German, Spanish, French, Portuguese, Hindi, and Chinese.

---

## Results

**Dataset Distribution**
The validation dataset comprises **1,778 examples** distributed across the following topics:

*   **Mergers & Acquisitions:** 787
*   **Taiwan Question:** 333
*   **2020 US Election:** 298
*   **Russia-Ukraine:** 219
*   **Chloroquine:** 141

**Class Balance**
*   **Neutral:** 44.4%
*   **Favor:** 29.5%
*   **Against:** 26.2%

**Experimental Outcomes**
*   Findings indicate VLMs rely significantly more on textual input than visual input for stance detection.
*   Models depend heavily on embedded text within images when visual information is utilized, rather than on non-text visual features.
*   Predictions remain consistent across all seven languages without explicit multilingual training, though some model anomalies were observed.

---

## Contributions

*   **Focus Shift:** Addresses the underexplored area of multimodal stance detection by shifting focus from text-only to image-text scenarios.
*   **Resource Creation:** Contributes a newly extended dataset tailored for evaluating stance detection across seven languages.
*   **Empirical Insights:** Provides comprehensive empirical insights on VLM behavior regarding input modality prioritization and multilingual handling, establishing a baseline for future research.

---

**Quality Score:** 8/10
**References:** 14 citations