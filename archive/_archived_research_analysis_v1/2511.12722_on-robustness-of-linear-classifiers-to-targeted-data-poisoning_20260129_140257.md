# On Robustness of Linear Classifiers to Targeted Data Poisoning

*Nakshatra Gupta; Sumanth Prabhu; Supratik Chakraborty; R Venkatesh*

***

> ### Executive Summary
>
> This research addresses the fundamental challenge of quantifying the robustness of machine learning models against targeted data poisoning attacks, specifically focusing on scenarios where an adversary perturbs training labels with limited knowledge of the victim’s hypothesis space. The inability to accurately measure dataset vulnerability poses a significant security risk, as undetected weaknesses can be exploited to force misclassification on specific test instances. The authors establish that determining exact robustness is computationally intractable, marking the first proof that this problem is **NP-Complete** even when restricted to simple linear classifiers.
>
> To overcome these limitations, the authors introduce the **"ROBUST RANGE"** framework, which estimates robustness by calculating certified lower and upper bounds rather than solving for an exact value. Instead of tackling the NP-Complete problem directly, the algorithm efficiently computes an average upper bound ($\hat{r}$)—the minimum poisoning level required to force a misclassification—using a Hinge Loss function, alongside an average lower bound ($\breve{r}$).
>
> Experimental validation across diverse datasets—including Text (SST, Emo), Vision (Fashion-MNIST), and Tabular (Loan, Census Income)—demonstrates that ROBUST RANGE significantly outperforms existing baselines like IP-RELABEL in scalability and speed. The framework computed bounds in under two minutes per test point, revealing wide variations in vulnerability (ranging from 1% to 31%). The significance of this work lies in bridging the gap between theoretical hardness and practical application, providing the field with the first reliable tool for automatically measuring dataset vulnerability at scale.

***

### Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |
| **Proposed Framework** | ROBUST RANGE |
| **Theoretical Complexity** | NP-Complete |
| **Computation Time** | < 2 minutes per test point |
| **Attack Success ($\rho$)** | Up to 0.98 (Fashion-MNIST) |

---

## Key Findings

*   **Proven NP-Completeness:** Determining the exact robustness of a dataset to targeted data poisoning is an NP-Complete problem, even when restricted to simple linear classifiers.
*   **Empirical Validation:** Lower and upper bounds on robustness were empirically validated; poisoning attacks that exceed these bounds result in significant classification changes.
*   **Superior Scalability:** The proposed technique offers superior scalability, computing robustness bounds in scenarios where existing state-of-the-art techniques fail to compute.
*   **Practical Efficiency:** Despite theoretical NP-Completeness, the implementation is practically efficient and handles large, publicly available datasets effectively.

## Methodology

*   **Threat Model:** Operates under a threat model restricting the adversary to perturbing only training labels with limited knowledge of the victim's hypothesis space.
*   **Complexity Analysis:** Performs a theoretical complexity analysis to prove the computational hardness of the robustness measurement problem.
*   **Approximation Technique:** Develops an approximation technique to calculate lower and upper bounds of robustness rather than solving the intractable exact problem.
*   **Validation:** Conducts experimental validation using public datasets to verify computational efficiency and the correlation between bounds and actual test classification impact.

## Contributions

*   **Theoretical Bounds:** Established theoretical complexity bounds by proving the NP-Completeness of robustness analysis for linear classifiers under label-flipping attacks.
*   **Algorithmic Solution:** Provided a tractable algorithmic solution that computes robustness intervals (upper and lower bounds) to overcome theoretical intractability.
*   **Advanced Defense:** Advanced poisoning defense by introducing a more reliable and scalable method for automatically measuring dataset vulnerability, addressing the limitations of previous state-of-the-art techniques.

## Technical Details

The paper formally establishes that determining exact robustness to targeted data poisoning is NP-Complete for linear classifiers. The proposed framework, **ROBUST RANGE**, estimates robustness by computing certified bounds rather than exact values.

*   **Core Metrics:**
    *   **Average Upper Bound ($\hat{r}$):** Represents the minimum poisoning needed to force misclassification.
    *   **Average Lower Bound ($\breve{r}$):** The lower limit of the robustness range.
*   **Calculation Logic:**
    *   The calculation of $\hat{r}$ utilizes the **Hinge Loss** function.
    *   The approach treats the victim model as a **black-box** and simulates targeted attacks via label perturbation.
*   **Scaling Strategy:** The simulation uses dynamically scaled counts of perturbations: $0, \hat{r}/4, \hat{r}/2, \hat{r}, 2\hat{r}, 4\hat{r}$.

## Results

The study analyzed diverse datasets from Text (SST, Emo), Vision (Fashion-MNIST), and Tabular (Loan, Census Income) domains.

*   **Robustness Range ($\hat{r}$):**
    *   Ranged from **1%** (Digits Recognition) to **31%** (Loan).
    *   **5 datasets** were identified as highly vulnerable ($\hat{r} \le 4\%$).
*   **Attack Success Rates ($\rho$):**
    *   SST (BOW): **0.96**
    *   Fashion-MNIST: **0.98**
    *   Loan (BOW): **0.46**
*   **Scalability:**
    *   ROBUST RANGE computed bounds in **under two minutes** per test point.
    *   Demonstrated superior scalability over the IP-RELABEL baseline.
*   **Correlation:** Poisoning $\hat{r}$ points caused significant accuracy changes, correlating inversely with the robustness bound.