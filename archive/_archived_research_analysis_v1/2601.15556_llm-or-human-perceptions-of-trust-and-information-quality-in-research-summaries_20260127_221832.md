---
title: LLM or Human? Perceptions of Trust and Information Quality in Research Summaries
arxiv_id: '2601.15556'
source_url: https://arxiv.org/abs/2601.15556
generated_at: '2026-01-27T22:18:32'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# LLM or Human? Perceptions of Trust and Information Quality in Research Summaries

*Sandeep Avula, Jana Akpinar, Brandon Dang, Kaza Razat*

**Quality Score:** 8/10 | **References:** 40 Citations

---

> ### ðŸ“Š Quick Facts
> *   **Study Participants:** 103 Researchers
> *   **Models Evaluated:** GPT-4 vs. Human-Written
> *   **Detection Accuracy:** ~50% (Equivalent to chance)
> *   **Academic LLM Usage:** 81% of authors
> *   **Primary finding:** Significant "Trust Penalty" applied to AI despite higher quality ratings.

---

## Executive Summary

This research investigates the critical paradox facing the integration of Large Language Models (LLMs) into scientific workflows: despite high technical capability and widespread usage among academic authors, LLM adoption faces significant resistance due to "algorithmic aversion." Although up to 81% of authors report using LLM tools, a fundamental disconnect persists between the objective quality of AI-generated text and the trust users place in it.

The studyâ€™s key innovation lies in its rigorous methodological design, employing a **between-subjects** experimental protocol involving 103 researchers to isolate trust and quality perception drivers. Participants evaluated summaries under either a "Blind" condition (authorship unknown) or a "Revealed" condition (authorship disclosed). Technically, the study decouples "Trust" (credibility/reliability) from "Information Quality" (clarity/comprehensiveness) using multi-faceted metrics, including reliance calibration and weight-of-advice measurements.

The results reveal a significant divergence between objective output quality and subjective user trust. In blind evaluations, participants rated GPT-4 summaries **significantly higher** in quality than human-written abstracts. However, when the source was revealed, a significant trust penalty was applied to LLM summaries. Furthermore, the study found that participants were unable to distinguish between LLM and human sources, achieving an accuracy statistically indistinguishable from **chance (50%)**, while exhibiting a "hallucination bias" toward machine-generated text.

---

## Key Findings

*   **Quality vs. Trust Paradox:** In blind evaluations, participants rated GPT-4 summaries higher in quality than human-written ones. However, when the source was revealed, participants explicitly trusted LLM summaries less.
*   **The "Trust Penalty":** Researchers applied a distinct penalty to LLM-generated content upon disclosure, reducing their reliance on the information even when it was technically superior or accurate.
*   **Inability to Detect AI:** Participants performed poorly at distinguishing between LLM and human sources, with accuracy hovering near chance levels.
*   **Hallucination Bias:** Users were more likely to assume inaccuracies or hallucinations in LLM summaries, resulting in a trust penalty even when the summaries were factually correct.

---

## Methodology

The study utilized a **between-subjects user study** involving 103 researchers who evaluated summaries of scientific papers under two controlled conditions:

1.  **Blind Condition:** The source of the summary was not revealed to the participant.
2.  **Revealed Condition:** The source was explicitly disclosed as either LLM or Human.

**Comparison:** The study contrasted human-written abstracts against GPT-4 generated summaries.

**Metrics:**
*   **Information Quality:** Rated on clarity, depth, and coverage.
*   **Trust:** Rated on credibility and reliability.
*   **Authorship Identification:** Participants attempted to guess whether the text was written by a human or LLM.

---

## Contributions

This paper makes several critical contributions to the field of Human-Computer Interaction (HCI) and scientific communication:

*   **Evidence of Algorithmic Aversion:** Provides empirical evidence demonstrating a disconnect between the high perceived quality of LLM outputs and the low trust users place in them.
*   **Impact of Labeling:** Highlights the negative impact of labeling AI-generated content on user trust, offering critical insights for UI/UX design.
*   **Barriers to Adoption:** Establishes that while LLMs are technically capable of scientific summarization, social and psychological barriers remain significant obstacles to their adoption in scientific workflows.

---

## Technical Details

**Experimental Protocol**
*   **Design:** Blind evaluation protocol contrasted with a source-revealed condition to assess shifts in trust.
*   **Categories:** Text classified as Human-written, LLM-generated (GPT-4), and LLM-edited.

**Definitions & Dimensions**
*   **Trust:** Defined as an attitude distinct from reliance and trustworthiness.
*   **Quality Dimensions:** Evaluated based on *Clarity* and *Comprehensiveness*.

**Evaluation Methods**
*   **Self-report metrics:** Standard surveys for trust and quality.
*   **Reliance Calibration:** Measuring how participant trust aligns with actual task performance.
*   **Weight-of-advice Approaches:** Assessing how much influence the summary has on the user's decisions.
*   **Authorship-guess Task:** Specific task to evaluate detection accuracy against random chance.

---

## Results & Context

**Performance Metrics**
*   **Blind Evaluation:** GPT-4 summaries were rated higher in quality than human-written summaries.
*   **Revealed Evaluation:** Explicit trust ratings dropped significantly for LLM summaries upon source disclosure.
*   **Reliance Tasks:** Participants were measurably less willing to utilize information once AI authorship was disclosed.

**Background Statistics**
*   **Usage:** 81% of 816 academic authors surveyed use LLM tools.
*   **Peer Review:** 6.5% to 16.9% of peer reviews involve LLM usage.
*   **Detection Baselines:** Prior studies suggest human detection accuracy hovers near chance, while experts and automated detectors perform only slightly above chance.