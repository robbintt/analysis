---
title: 'PoTPTQ: A Two-step Power-of-Two Post-training for LLMs'
arxiv_id: '2507.11959'
source_url: https://arxiv.org/abs/2507.11959
generated_at: '2026-02-03T19:21:12'
quality_score: 8
citation_count: 31
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# PoTPTQ: A Two-step Power-of-Two Post-training for LLMs
*Xinyu Wang; Vahid Partovi Nia; Peng Lu; Jerry Huang; Xiao-Wen Chang; Boxing Chen; Yufei Cui*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Precision** | 2-bit & 3-bit |
| **Speedup (V100)** | 3.67x |
| **Speedup (RTX 4090)** | 1.63x |
| **Calibration** | Minimal calibration set required |
| **Quality Score** | 8/10 |

---

## Executive Summary

Deploying Large Language Models (LLMs) efficiently requires low-precision quantization to reduce memory bandwidth and computational costs. While Power-of-Two (PoT) quantization offers theoretical hardware advantages by replacing expensive multiplication operations with efficient bit shifts, it has historically faced significant barriers to adoption.

Previous PoT methods suffered from hardware inefficiencies on GPUs, specifically due to entangled sign bits and sequential bit manipulations that created bottlenecks. Furthermore, PoT techniques typically lagged behind state-of-the-art uniform integer quantization methods (such as GPTQ and AWQ) in terms of model accuracy at 2- and 3-bit precisions.

This paper introduces **PoTPTQ**, a novel two-step post-training quantization framework designed to optimize both the accuracy and hardware efficiency of PoT representations. The researchers developed a hardware-optimized PoT framework that decouples sign bits and eliminates sequential bit manipulations. Consequently, PoTPTQ achieves state-of-the-art accuracy in 2- and 3-bit formats and delivers substantial inference speedups (3.67x on V100, 1.63x on RTX 4090), challenging the prevailing dominance of integer-based methods.

---

## Key Findings

*   **Superior Accuracy at Low Precision:** The proposed framework achieves state-of-the-art accuracy in 2- and 3-bit formats, surpassing current integer quantization methods.
*   **Significant GPU Inference Speedup:** Optimized PoT quantization accelerates dequantization, resulting in a **3.67x speedup on NVIDIA V100** and **1.63x on NVIDIA RTX 4090**.
*   **Resolution of GPU Inefficiency:** The framework overcomes hardware bottlenecks related to entangled sign bits and sequential bit manipulations found in previous methods.
*   **Efficient Calibration:** The method maintains high model accuracy using only a minimal calibration set.

---

## Methodology

The researchers developed a two-step post-training quantization (PTQ) algorithm for Power-of-Two (PoT) weight representation in LLMs. The approach is divided into two distinct phases:

1.  **Initialization Phase:** Establishes robust quantization scales using a data-agnostic approach to minimize Mean Squared Error (MSE).
2.  **Refinement Phase:** Fine-tunes these scales using a minimal calibration dataset for data-dependent activation alignment.

This algorithmic approach is coupled with a novel PoT framework designed to facilitate efficient dequantization on GPU hardware by avoiding sequential bit manipulations.

---

## Technical Details

**Representation Strategy**
*   **Weight-Only Quantization:** Focuses on 2-bit and 3-bit precisions.
*   **Formula:** Weights are represented as:
    $$w = S \times P \times 2^E$$
    Where:
    *   $S$ = Shared scale factor
    *   $P$ = Sign bit
    *   $2^E$ = Quantized exponent

**Algorithmic Approach**
*   **Group-wise Quantization:** Utilizes a group-wise strategy for granular control.
*   **Step 1 (Initialization):** Data-agnostic scale initialization utilizing a grid search over multipliers to minimize MSE.
*   **Step 2 (Fine-tuning):** Data-dependent refinement for activation alignment.

**Hardware Optimizations**
*   **Operation Replacement:** Multiplication operations are replaced with bit shifts.
*   **Dequantization:** An optimized dequantization process specifically designed to eliminate sequential bottlenecks.

---

## Core Contributions

*   **Optimized Dequantization for GPUs:** Introduced a PoT quantization framework that eliminates sign bit entanglement and sequential bit manipulations.
*   **Advancement over Integer Quantization:** Demonstrated that PoT quantization can outperform traditional uniform integer quantization in accuracy at 2- and 3-bit precisions.
*   **Algorithmic Efficiency:** Proposed a computationally lightweight two-step post-training algorithm that requires only a small calibration set.

---

## Performance Results

*   **Accuracy:** Achieves state-of-the-art accuracy in 2-bit and 3-bit formats, outperforming current integer quantization methods (e.g., GPTQ, AWQ) and previous PoT attempts.
*   **Inference Speed:**
    *   **3.67x** speedup on NVIDIA V100.
    *   **1.63x** speedup on NVIDIA RTX 4090.
*   **Statistical Alignment:** Outperforms uniform quantization by better aligning with the statistical distribution of LLM weight matrices.
*   **Calibration Efficiency:** Maintains high model accuracy using a minimal calibration set.

---

**REFERENCES:** 31 citations
**QUALITY SCORE:** 8/10