# Sigma-MoE-Tiny Technical Report

*Qingguo Hu; Zhenghao Lin; Ziyue Yang; Yucheng Ding; Xiao Liu; Yuting Jiang; Ruizhe Wang; Tianyu Chen; Zhongxin Guo; Yifan Xiong; Rui Gao; Lei Qu; Jinsong Su; Peng Cheng; Yeyun Gong*

***

> ### üìä Quick Facts & Key Metrics
> *   **Total Parameters:** 20 Billion
> *   **Active Parameters (per token):** 0.5 Billion
> *   **Sparsity Ratio:** 40:1
> *   **Architecture:** 56-Layer Decoder-only Transformer
> *   **Routing:** Top-1 (1 active expert per token)
> *   **Expert Density:** 96 experts per layer
> *   **Attention Mechanism:** Group Query Attention (GQA) (16Q / 4KV)

***

## üìù Executive Summary

This research addresses the challenge of training Mixture of Experts (MoE) language models at extreme levels of sparsity, a regime where standard training optimization techniques typically fail. While increasing the number of experts improves model capacity, maintaining high sparsity (activating very few parameters per token) often leads to training instability and "routing collapse," where certain experts are over-utilized while others remain dormant. This issue is critical because maximizing the ratio of total-to-active parameters is essential for reducing inference costs and memory footprint; however, achieving this without sacrificing model convergence or performance has proven difficult, particularly due to the ineffectiveness of conventional load balancing losses in deep, highly sparse architectures.

The authors introduce Sigma-MoE-Tiny, a 20-billion parameter model that achieves extreme sparsity by activating only 0.5 billion parameters per token using a Top-1 routing strategy across 96 fine-grained experts per layer. The core technical innovation is a "Progressive Sparsification Schedule," designed to address the discovered phenomenon that standard global-batch Load Balancing Loss (LBL) diverges in the lower layers of the network under extreme sparsity. In standard configurations, these lower layers suffer from pathological load imbalances (ranging from 0% to 300% of uniform allocation), acting as a "shortcut" that undermines training. The proposed schedule dynamically optimizes the training process to mitigate this imbalance, balancing expert utilization effectively alongside a two-stage training pipeline consisting of pre-training on a diverse corpus and subsequent post-training.

Sigma-MoE-Tiny achieves a 40:1 total-to-activated parameter ratio, matching or exceeding the performance of significantly larger dense models while activating only 0.5B parameters per token. The architecture comprises 56 layers with a hidden size of 1,536, utilizing Group Query Attention (GQA) and SwiGLU activations. Empirically, the model demonstrated remarkable training stability, avoiding the irrecoverable loss spikes that usually plague such sparse architectures. The results confirmed that while standard load balancing losses failed in the lower layers, the progressive sparsification schedule successfully managed the routing mechanics, allowing the model to sustain high performance without convergence failures.

The Sigma-MoE-Tiny report establishes a new benchmark for sparsity in open-source foundation models, providing empirical evidence that stable training is feasible even with 96 experts per layer and Top-1 routing. By offering an in-depth analysis of load balancing mechanics and the "shortcut" problem in lower layers, the authors provide a theoretical foundation for future advancements in sparse architecture design. This work challenges the assumption that extreme sparsity necessitates instability, potentially influencing future LLM development toward more efficient, inference-friendly models that maintain the high performance of dense counterparts without the associated computational burden.

***

## üîç Key Findings

*   **Achievement of Extreme Sparsity:** Sigma-MoE-Tiny achieves the highest sparsity level among open-source models, utilizing a 20 billion parameter architecture where only 0.5 billion parameters are activated per token.
*   **Ineffectiveness of Standard Balancing:** Standard load balancing loss becomes ineffective in the lower layers of the model in highly sparse settings (up to 96 experts per layer).
*   **Training Stability:** The training process remains remarkably stable with no irrecoverable loss spikes, attributed to a specific sparsification schedule.
*   **Performance Efficiency:** The model demonstrates top-tier performance comparable to, or exceeding, significantly larger models despite activating only 0.5B parameters.

## üõ†Ô∏è Methodology

The method is built upon a fine-grained expert segmentation architecture employing up to 96 experts per layer but activating only one expert per token. To address load balancing challenges, the authors introduce a progressive sparsification schedule, which balances expert utilization with training stability. The methodology also includes a two-stage training pipeline: pre-training on a diverse, high-quality corpus followed by post-training.

## üöÄ Contributions

*   **Sigma-MoE-Tiny Model:** A highly sparse MoE language model (20B total/0.5B active) that sets a new benchmark for sparsity in open-source foundation models.
*   **Progressive Sparsification Schedule:** A novel training optimization strategy designed to solve expert load balancing issues in highly sparse MoE architectures where traditional loss functions fail.
*   **Empirical Insights on Stability:** Empirical evidence demonstrating that stable training is possible without irrecoverable loss spikes under extreme sparsity conditions.
*   **Theoretical Analysis:** An in-depth analysis of load balancing mechanics in extreme MoE models, offering a technical foundation for future research into sparse architecture design.

## ‚öôÔ∏è Technical Details

| Component | Specification |
| :--- | :--- |
| **Model Type** | Decoder-only Transformer |
| **Total Parameters** | 20 Billion |
| **Activated Parameters** | 0.5 Billion |
| **Hidden Size** | 1,536 |
| **Layers** | 56 |
| **Attention Heads** | 16 Query / 4 Key-Value (GQA) |
| **Experts per Layer** | 96 |
| **Routing** | Top-1 (1 active expert per token) |
| **Activation** | SwiGLU |
| **Normalization** | RMSNorm (Pre-normalization) |
| **Gating Precision** | FP32 |
| **Key Features** | QK-Norm, Global-batch Load Balancing Loss (LBL), Progressive Sparsification Schedule |

## üìà Results

The model has 20 billion total parameters but activates only 0.5 billion parameters per token, achieving a 40:1 total-to-activated parameter ratio. Experimental analysis revealed a 'shortcut' problem where standard LBL optimization diverges in lower layers under extreme sparsity; upper layers behaved correctly, while lower layers showed pathological imbalance with expert loads ranging from -100% relative deviation to 3√ó the uniform allocation. Despite this, the training process remained remarkably stable with no irrecoverable loss spikes.

***

**Quality Score:** 8/10  
**References:** 8 citations