# Enhancing Rating-Based Reinforcement Learning to Effectively Leverage Feedback from Large Vision-Language Models
*Tung Minh Luu; Younghwan Lee; Donghoon Lee; Sunho Kim; Min Jun Kim; Chang D. Yoo*

---

> **Quick Facts Sidebar**
> 
> *   **Methodology:** ERL-VLM (Enhanced Rating-Based Reinforcement Learning)
> *   **Feedback Type:** Absolute scalar ratings (VLM-generated)
> *   **Key Innovation:** Two-stage prompting & stratified sampling
> *   **Robot Platform:** 7-DOF Rethink Sawyer
> *   **Top Result:** 0.73 Success Rate (Sweep Bowl)
> *   **Quality Score:** 9/10

---

## Executive Summary

Reinforcement learning for complex robotic manipulation is often hindered by the difficulty of specifying accurate reward functions. While human feedback provides high-quality supervision, it is labor-intensive and unscalable. Existing Vision-Language Model (VLM) approaches typically rely on pairwise trajectory comparisons; however, these methods lack expressiveness and suffer from poor sample efficiency.

This paper introduces **ERL-VLM**, a framework that shifts from pairwise rankings to assigning absolute scalar scores to individual trajectories, providing more granular feedback. To resolve the inherent instability of rating-based RL—specifically regarding data imbalance and noisy labels—the method employs a two-stage prompting strategy where the VLM analyzes trajectory segments before assigning scores. Crucially, the framework utilizes stratified sampling during reward optimization to ensure all rating classes are present in training batches.

ERL-VLM significantly outperformed existing baselines (CLIP, RoboCLIP, RL-VLM-F) across simulated environments such as MetaWorld and ALFRED. In real-world experiments, the method demonstrated superior data efficiency, confirming that AI-generated feedback can serve as a scalable, effective alternative to human supervision for reward learning in complex control tasks.

---

## Key Findings

*   **Superior Performance:** ERL-VLM significantly outperforms existing VLM-based reward generation methods across both low-level and high-level control tasks.
*   **Expressive Feedback:** Using absolute ratings for individual trajectories enables improved sample efficiency compared to traditional pairwise comparisons.
*   **Stability Improvements:** The proposed method effectively mitigates instability issues in rating-based RL, specifically handling challenges related to data imbalance and noisy labels.
*   **Scalability Validation:** The results validate the viability of using AI-generated feedback to scale reinforcement learning with minimal human intervention.

---

## Methodology

The researchers propose **ERL-VLM**, an enhanced rating-based reinforcement learning framework designed to learn reward functions from AI-generated feedback.

*   **Feedback Mechanism:** The core methodological difference is the reliance on Large Vision-Language Models (VLMs) to provide **absolute ratings** for individual trajectories rather than relying on pairwise comparisons.
*   **Stability Assurance:** To ensure stability, the approach incorporates specific enhancements to the training process to address common pitfalls such as data imbalance and noisy labels.
*   **Workflow:** The algorithm alternates between data collection and VLM feedback sessions.

---

## Technical Details

### Feedback Architecture
*   **Strategy:** Absolute ratings for trajectories rather than pairwise comparisons to provide expressive feedback within a query budget.
*   **Prompting:** Employs a **two-stage prompting strategy** where the VLM analyzes trajectory segments before assigning a rating.

### Optimization & Training
*   **Sampling:** Uses **stratified sampling** during reward optimization to ensure all rating classes are present in batches, countering data imbalance.
*   **Components:** The architecture integrates:
    *   Policy ($\pi_\theta$)
    *   Reward function ($\hat{r}_\psi$)
    *   VLM integration for labeling trajectory segments

---

## Performance Results

### Simulation Benchmarks
ERL-VLM significantly outperformed baselines (CLIP, RoboCLIP, RL-VLM-F) and even sparse environment rewards on **MetaWorld** and **ALFRED**, demonstrating superior data efficiency compared to RL-VLM-F.

### Real-World Robot Experiments
Tested on a real-world **7-DOF Rethink Sawyer robot**, ERL-VLM achieved the highest success rates on all three tasks compared to Behavior Cloning and Sparse Rewards (IQL).

| Task | ERL-VLM Success Rate | Comparison Baselines |
| :--- | :---: | :--- |
| **Sweep Bowl** | **0.73** | Outperformed BC & IQL |
| **Drawer Open** | **0.60** | Outperformed BC & IQL |
| **Pickup Banana** | **0.47** | Outperformed BC & IQL |

---

## Core Contributions

1.  **Paradigm Shift:** Moves the field from labor-intensive human feedback and pairwise comparisons toward absolute ratings generated by VLMs.
2.  **Algorithmic Innovation:** Introduces improvements to rating-based RL that resolve instability caused by data imbalance and noise.
3.  **Empirical Evidence:** Provides concrete proof that AI feedback can serve as a scalable, effective alternative to human supervision for reward learning in complex control tasks.

---
**Quality Score:** 9/10 | **References:** 40 citations