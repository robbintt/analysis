# Vision Language Models Have Difficulty Recognizing Virtual Objects

**Authors:** *Tyler Tran; Sangeet Khemlani; J. G. Trafton*

---

> ### ðŸ“Š Quick Facts
>
 > *   **Quality Score:** 7/10
 > *   **Citations:** 40
 > *   **Top Model:** Idefics2 (63% Accuracy)
 > *   **Dataset Size:** 112,896 Queries
 > *   **Human Benchmark:** ~1.0 Accuracy
 > *   **Key Limitation:** Inability to reason with hypothetical ("virtual") objects.

---

## Executive Summary

This research uncovers a fundamental limitation in state-of-the-art Vision Language Models (VLMs): the inability to perform deep visuospatial reasoning involving "virtual objects," or entities defined purely through text descriptions that are absent from the visual input. Although VLMs handle complex semantic tasks for visible elements effectively, the study shows they fail to dynamically update internal scene representations with this hypothetical information.

To address and measure this deficiency, the authors introduced **"Virtual Object Integration,"** utilizing the **"TABLE TEST"** framework and **"Spatial Relation Probing"** methodology to rigorously test models like Idefics2, Llama3, and BLIP. Over 100,000 controlled queries were used to determine how well models can locate virtual objects relative to physical ones.

The results demonstrated significant performance disparities against human baselines, with **Idefics2** and **Llama3** achieving 63% and 57% accuracy respectively, while **BLIP** performed close to random chance at 22%. The analysis also revealed that linguistic factors, such as specific keywords and tenses, heavily influenced outcomes. Ultimately, the paper challenges the assumption that semantic capability equates to spatial understanding, providing evidence that current VLMs lack the mechanisms to reason about abstract or hypothetical scenarios within physical spaces.

---

## Key Findings

*   **Poor Performance on Virtual Objects:** State-of-the-art VLMs perform significantly worse when processing objects defined via text descriptions that are not visually present in the image.
*   **Lack of Deep Comprehension:** Despite excelling at complex semantic tasks, VLMs struggle to deeply comprehend the visuospatial properties of scenes.
*   **Failure to Update Representations:** VLMs generally fail to dynamically update their internal scene representations to reason sensibly about spatial relationships involving both visually grounded objects and abstract, text-supplied objects.

---

## Methodology

The researchers employed a novel testing framework to evaluate how multimodal AI systems handle non-visual information integrated into visual contexts.

*   **Virtual Object Integration:** The core methodology involved using prompts to describe objects that were absent from the visual input, testing the model's ability to "imagine" them into the scene.
*   **Spatial Relation Probing:** This technique combined actual image inputs with hypothetical text prompts to evaluate the model's reasoning capability regarding spatial relations between mixed entities (real and virtual).
*   **Systematic Evaluation:** The study conducted a rigorous assessment of current state-of-the-art VLMs to measure their efficacy in handling these multimodal spatial reasoning tasks.

---

## Technical Details

| Component | Specification |
| :--- | :--- |
| **Evaluation Framework** | TABLE TEST |
| **Configuration** | 2-object configurations covering **64 distinct objects** |
| **Total Queries Generated** | **112,896** |
| **Variables Tested** | 7 prompt formulations, linguistic tense (present/past), numerical constraints |
| **Inference Parameters** | Temperature: 0, Fixed Random Seeds |
| **Models Evaluated** | **Idefics2**, **Llama3**, **BLIP** |

---

## Results

The study revealed significant performance gaps based on both the model architecture and the linguistic phrasing of the prompts.

**Model Performance (Accuracy)**
*   **Idefics2:** 63% (Highest performing)
*   **Llama3:** 57%
*   **BLIP:** 22%
*   *Significance:* Statistically significant differences were found between models (**Friedman test, p < .001**).

**Linguistic Impact**
*   **Prompt Phrasing:** Phrasing significantly impacted results.
    *   "Pretend" prompts: **51%** accuracy.
    *   "If" prompts: **40%** accuracy (BLIP dropped to **8%** on "if").
*   **Grammatical Tense:** Past tense prompts outperformed present tense prompts (**51% vs 44%**) (**Wilcoxon test, p < .001**).

**Benchmark Comparison**
*   Human-like performance is estimated at an accuracy of **1.0**.

---

## Contributions

*   **New Evaluation Paradigm:** Introduced the use of 'virtual objects' as a novel metric and testing mechanism to assess scene comprehension in multimodal AI systems.
*   **Empirical Limitation Identification:** Provided concrete evidence identifying specific limitations in current VLMs, specifically the lack of robust visuospatial reasoning required to integrate non-visual information into visual contexts.

---

**References:** 40 Citations