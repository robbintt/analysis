---
title: 'LoaQ: Layer-wise Output Approximation Quantization'
arxiv_id: '2509.06297'
source_url: https://arxiv.org/abs/2509.06297
generated_at: '2026-02-03T20:23:24'
quality_score: 8
citation_count: 22
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# LoaQ: Layer-wise Output Approximation Quantization
*Li Lin; Xiaojun Wan*

---

> ### **Quick Facts**
> *   **Focus:** Layer-wise Output Approximation
> *   **Model Families:** LLaMA, Qwen
> *   **Methodology:** Post-Training Quantization (PTQ)
> *   **Key Innovation:** Closed-form solution for output matching
> *   **Quality Score:** 8/10
> *   **References:** 22 citations

---

## Executive Summary

Current Post-Training Quantization (PTQ) methods for Large Language Models (LLMs) primarily rely on minimizing the approximation error between original and quantized weights, typically through Frobenius norm minimization. However, this paper identifies a critical flaw in this approach: minimizing weight error locally does not guarantee the preservation of the model's final output. Consequently, as quantization errors propagate through deep networks and residual connections, they accumulate, leading to significant deviations from the full-model output. This disconnect between local weight optimization and the global goal of output matching limits the effectiveness of existing layer-wise PTQ techniques, particularly as model sizes and complexities increase.

To address this, **LoaQ (Layer-wise Output Approximation Quantization)** introduces a framework that shifts the optimization objective from weight approximation to direct output matching. Technically, LoaQ operates at the sub-block level—optimizing Self-Attention and MLP modules as cohesive units rather than isolated linear layers. By incorporating residual connections and RMSNorm into the optimization formulation, LoaQ aligns the quantized output of a sub-block with the original model's output. A key technical advantage is its derivation of a simple, closed-form solution by solving a least-squares problem, which allows for the precise calculation of output-matching factors without iterative optimization or added computational overhead.

Evaluations on the LLaMA and Qwen model families demonstrate that LoaQ consistently outperforms state-of-the-art baselines, including GPTQ, MagR, GPTAQ, Qronos, and QEP. In specific W4A16 (4-bit weight, 16-bit activation) configurations on LLaMA-2-7B, LoaQ reduces perplexity on WikiText-2 to approximately **6.09**, a significant improvement compared to GPTQ's **7.54** and MagR's **7.40**. Similar gains are observed on the C4 dataset, where LoaQ consistently lowers perplexity compared to competing methods. The method proves effective for both weight-only and weight-activation quantization scenarios, achieving superior quantization quality by successfully mitigating error accumulation in residual connections while maintaining the efficiency expected of layer-wise PTQ (single GPU execution).

LoaQ represents a significant methodological shift in PTQ by redefining the layer-wise optimization objective to prioritize output fidelity over weight similarity. Its design is orthogonal to existing quantization pipelines, meaning it can be seamlessly integrated into established workflows (such as those using SmoothQuant or AWQ) to enhance performance without replacing them. Furthermore, the authors highlight its utility as a high-quality initialization strategy for end-to-end PTQ, where it accelerates convergence compared to standard initialization methods. By offering a mathematically robust, closed-form solution that bridges the gap between local quantization and global model behavior, LoaQ sets a new standard for efficient and accurate LLM compression.

---

## Key Findings

*   **Strong Performance:** Demonstrates robust results on both **LLaMA** and **Qwen** model families.
*   **Versatility:** Effective for **weight-only** and **weight-activation** quantization scenarios.
*   **Seamless Integration:** Significantly enhances overall quantization quality by integrating with current strategies (e.g., SmoothQuant, AWQ).
*   **Zero Overhead:** Features a simple closed-form solution that is orthogonal to existing techniques, ensuring **no added computational complexity**.

---

## Methodology

LoaQ proposes a **Layer-wise Output Approximation Quantization** framework that moves beyond traditional local weight approximation. By analyzing the structural properties of mainstream LLMs, the method incorporates **output-matching factors** during the quantization of linear layers.

The core mechanism aligns the quantized output more closely with the original model output within a layer-wise Post-Training Quantization (PTQ) framework. This is achieved through a closed-form solution that optimizes for output fidelity rather than just weight similarity.

---

## Contributions

*   **Problem Identification:** Identifies that current layer-wise PTQ methods focusing on weight approximation result in insufficient approximations and deviations from the intuitive goal of output matching.
*   **Novel Mechanism:** Proposes a new mechanism to align layer-wise quantization with full-model output structures via **output-matching factors**.
*   **Integrable Solution:** Provides a highly integrable solution orthogonal to existing techniques that advances post-training quantization **without replacing established pipelines**.

---

## Technical Details

*   **Optimization Level:** Optimizes at the **sub-block level** (Self-Attention or MLP) rather than the linear-layer level.
*   **Error Mitigation:** Incorporates residual connections and RMSNorm to reduce error accumulation.
*   **Formulation Structure:**
    *   `RMSNorm` → `Module` → `Linear Projection` → `Residual Addition`
*   **Limitations Addressed:**
    *   Standard layer-wise PTQ (Frobenius norm minimization).
    *   GPTQ (row-wise updates).
    *   GPTAQ (ignores upper-triangular correction).
*   **Compatibility:** Orthogonal to existing techniques like **SmoothQuant** and **AWQ**.
*   **Optimization Type:** Offers a closed-form solution applicable to both weight-only and weight-activation quantization.

---

## Evaluation Results

**Models Tested:**
*   LLaMA Family
*   Qwen Family

**Performance Highlights:**
*   **Consistency:** Claims significant enhancements in quantization quality and consistency across architectures.
*   **Benchmark Comparison:** Outperforms methods like **GPTQ**, **MagR**, **GPTAQ**, **Qronos**, and **QEP** by addressing error accumulation in residual connections.
*   **Specific Metrics (LLaMA-2-7B, W4A16):**
    *   **LoaQ:** ~6.09 Perplexity (WikiText-2)
    *   **GPTQ:** 7.54 Perplexity
    *   **MagR:** 7.40 Perplexity
*   **Efficiency:** Maintains layer-wise PTQ efficiency with **single GPU execution**.
*   **Convergence:** Serves as an effective initialization for end-to-end PTQ, accelerating convergence.

---

**Document Score:** 8/10