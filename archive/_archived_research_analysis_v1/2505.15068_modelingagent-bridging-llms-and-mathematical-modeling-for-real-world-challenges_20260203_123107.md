---
title: 'ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges'
arxiv_id: '2505.15068'
source_url: https://arxiv.org/abs/2505.15068
generated_at: '2026-02-03T12:31:07'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# ModelingAgent: Bridging LLMs and Mathematical Modeling for Real-World Challenges

*Cheng Qian; Hongyi Du; Hongru Wang; Xiusi Chen; Yuji Zhang; Avirup Sil; Chengxiang Zhai; Kathleen McKeown; Heng Ji*

---

> ### üìä Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Benchmark Source** | International contests (COMAP MCM/ICM, HiMCM/MidMCM, IM2C) |
> | **Time Span** | 2000 ‚Äì 2025 |
> | **vs. GPT-4o Win Rate** | 71.4% |
> | **vs. Claude 3.5 Sonnet Win Rate** | 63.2% |
> | **Core Skills Evaluated** | Problem Decomposition, Abstraction, Formulation, Tool Use, Communication |
> | **Quality Score** | 7/10 |

---

## üìÑ Executive Summary

Current benchmarks for evaluating mathematical problem-solving in AI are insufficient for real-world applications because they focus primarily on closed-ended questions with single, verifiable answers. Real-world challenges are inherently open-ended, requiring interdisciplinary reasoning, the translation of vague scenarios into mathematical formulations, and the integration of external tools like code execution and web search.

This paper addresses the significant gap between theoretical mathematical capabilities and practical demands by introducing a holistic three-part ecosystem:

1.  **ModelingBench:** A novel benchmark sourced from international contests (2000‚Äì2025) featuring a sandbox environment.
2.  **ModelingAgent:** A multi-agent framework mimicking a human team structure with specialized roles (Idea Proposer, Data Searcher, Model Implementor, Report Writer) coordinated via Shared Memory and a Critic Module.
3.  **ModelingJudge:** A new evaluation paradigm utilizing LLMs as domain-specialized judges to assess open-ended solutions.

ModelingAgent demonstrates substantial performance improvements, achieving a **71.4% win rate against standard GPT-4o** and a **63.2% win rate against Claude 3.5 Sonnet**. Crucially, its solutions are frequently indistinguishable from human experts. This research shifts the focus from static textbook problems to dynamic, real-world challenges, establishing a new standard for evaluating open-ended reasoning.

---

## üîë Key Findings

*   **Benchmarking Gap:** Existing benchmarks fail to capture the complexity of real-world challenges, which require open-ended, interdisciplinary reasoning and tool integration.
*   **Superior Performance:** ModelingAgent substantially outperforms strong baselines in solving complex, open-ended tasks.
*   **Human-Level Quality:** Solutions generated by ModelingAgent are frequently indistinguishable from those produced by human experts.
*   **Effective Evaluation:** The proposed evaluation system, ModelingJudge, effectively leverages LLMs as domain-specialized judges to assess the validity and quality of open-ended solutions.

---

## ‚öôÔ∏è Methodology

The research employs a comprehensive three-pronged framework:

1.  **Benchmark Construction**
    *   Development of **ModelingBench**, a dataset consisting of real-world-inspired problems.
    *   Focuses on the translation of vague scenarios into precise mathematical formulations.

2.  **Multi-Agent Framework**
    *   Utilization of **ModelingAgent** to coordinate various tools.
    *   Enforces structured workflows and performs iterative self-refinement.

3.  **Expert-in-the-Loop Evaluation**
    *   Deployment of **ModelingJudge**.
    *   Uses LLMs acting as domain-specialized judges to assess open-ended outputs rather than relying on exact-match metrics.

---

## üõ†Ô∏è Technical Details

### ModelingBench
*   **Source Material:** Problems sourced from international mathematical modeling contests (COMAP MCM/ICM, HiMCM/MidMCM, IM2C).
*   **Coverage:** Spans the years 2000 to 2025.
*   **Environment:** Features a sandbox environment supporting:
    *   File operations
    *   Web search and download
    *   Image and document processing
    *   Python code execution

### ModelingAgent Architecture
*   **Structure:** A multi-agent framework using a human team dynamic structure.
*   **Components:**
    *   **Shared Memory:** Central repository for information.
    *   **Critic Module ($C$):** Regulates the process and guides refinement.
    *   **Specialized Agents:**
        *   **Idea Proposer ($A_{IP}$):** Responsible for reasoning.
        *   **Data Searcher ($A_{DS}$):** Handles tool use.
        *   **Model Implementor ($A_{MI}$):** Executes mathematical modeling.
        *   **Report Writer ($A_{RW}$):** Drafts the final documentation.
*   **Workflow:** Idea proposal $\rightarrow$ Mathematical formalization $\rightarrow$ Data retrieval $\rightarrow$ Critic-guided refinement $\rightarrow$ Final report synthesis.

---

## üèÜ Contributions & Results

### Primary Contributions
*   **ModelingBench:** A novel benchmark bridging theoretical math and practical, open-ended challenges.
*   **ModelingAgent:** A multi-agent framework advancing AI math modeling through tool integration and self-refinement.
*   **ModelingJudge:** A new evaluation paradigm using domain-specialized LLMs for open-ended tasks.
*   **Holistic Framework:** A comprehensive ecosystem for advancing real-world AI problem-solving beyond single-answer metrics.

### Evaluation Results
*   **Performance:** ModelingAgent (powered by GPT-4o) achieved a 71.4% win rate against the standard GPT-4o model and a 63.2% win rate against Claude 3.5 Sonnet.
*   **Skill Assessment:** The system was evaluated on five core skills:
    1.  Problem Decomposition
    2.  Abstraction/Simplification
    3.  Mathematical Formulation
    4.  Tool Use
    5.  Science Communication
*   **Outcome:** The specific metrics validate the framework's ability to manage the full lifecycle of a mathematical modeling challenge, significantly outperforming non-agent approaches.

---

**Paper Analysis Quality Score:** 7/10 | **References:** 40 citations