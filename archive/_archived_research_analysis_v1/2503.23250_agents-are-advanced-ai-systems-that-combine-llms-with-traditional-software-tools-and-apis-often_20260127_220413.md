---
title: Agents are advanced AI systems that combine LLMs with traditional software
  tools and APIs, often
arxiv_id: '2503.23250'
source_url: https://arxiv.org/abs/2503.23250
generated_at: '2026-01-27T22:04:13'
quality_score: 8
citation_count: 12
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Agents are advanced AI systems that combine LLMs with traditional software tools and APIs, often

*Authors: Large Language, Han Chan, Against Unauthorized, Encrypted Prompt, Ms Jain, Instruction Hierarchy, California San*

<details>
<summary><strong>üìä Quick Facts</strong></summary>

- **Quality Score:** 8/10
- **References:** 12 Citations
- **Study Type:** Proof-of-Concept / Qualitative Analysis
- **Core Mechanism:** Encrypted Prompts & Cryptographic Verification
- **Focus Area:** Agent Security & Prompt Injection Mitigation

</details>

---

## üî¨ Executive Summary

This research addresses the critical security vulnerabilities inherent in AI agents that integrate Large Language Models (LLMs) with sensitive external tools and APIs. As these agents gain the capability to execute actions such as transferring files or sending emails, they become prime targets for prompt injection attacks, where malicious inputs or manipulated content trick the model into performing unauthorized operations. The paper highlights that reliance on model alignment or probabilistic content filtering is insufficient for high-stakes environments, creating an urgent need for a security mechanism that guarantees safety at the execution level regardless of the model's text generation behavior.

The key innovation is the **"Encrypted Prompt Framework,"** a runtime defense mechanism that decouples safety enforcement from the LLM's generative process. The system embeds client-side generated cryptographic permission metadata directly into the user prompt, defining a specific scope of allowed API interactions (e.g., distinct permission levels). A distinct **"Execution Control Layer"** sits between the model and the tools; when the LLM attempts to trigger an API, this layer intercepts the call, cryptographically decrypts the permissions embedded in the prompt, and deterministically verifies that the proposed action matches the user‚Äôs granted scope.

This creates a hardware-like enforcement point that shifts the paradigm from probabilistic detection to cryptographic verification. The study validates the framework through qualitative proof-of-concept scenarios designed to test the system against diverse threat vectors. Demonstrations confirm that the execution control layer successfully blocks high-risk unauthorized actions‚Äîsuch as transmitting passwords via email or moving cloud storage data‚Äîstemming from direct prompt injections, indirect injections via retrieved content, and autonomous LLM hallucinations.

Furthermore, the system exhibited robust granular control, successfully permitting benign actions like web crawling while simultaneously blocking subsequent unauthorized commands triggered within the same context, confirming that functional operations remain viable under strict security constraints. This work significantly advances the field of AI security by establishing a method for guaranteeing system integrity even when the LLM itself is compromised or misaligned.

---

## üöÄ Key Findings

*   **Effective Mitigation:** Successfully prevents unauthorized actions by stopping prompt injection attacks before they can execute.
*   **Strict Permission Enforcement:** Verifies all actions against an encrypted standard prior to execution, ensuring strict adherence to user-granted scopes.
*   **Safety via Execution Control:** Achieves safety by blocking harmful actions at the system level rather than attempting to filter or prevent text generation within the LLM.
*   **Superiority over Detection Models:** Offers deterministic guarantees against unauthorized API misuse, outperforming traditional probabilistic detection models.

---

## üìù Methodology

The paper introduces a security mechanism that augments standard user prompts with an **'Encrypted Prompt'**. This encrypted component contains the current scope of permissions granted to the user.

1.  **Prompt Augmentation:** The user's input is combined with the Encrypted Prompt.
2.  **LLM Generation:** The LLM processes the combined input and generates an action/tool call.
3.  **Verification Step:** Before the action is executed, a verification layer checks the proposed action against the permissions embedded in the Encrypted Prompt.
4.  **Enforcement:** If the action requires permissions that have not been granted, execution is immediately blocked.

This acts as a "hard safety layer" positioned between the LLM's output and the actual tool execution.

---

## ‚öôÔ∏è Technical Details

The paper proposes a runtime defense mechanism utilizing **'Encrypted Prompts'** to decouple safety from the LLM's text generation. The architecture is divided into two main sides:

### Architecture Components

*   **Client-Side:** Responsible for generating the encrypted prompt. This includes context-aware user permissions (e.g., 'Low' vs 'High' access levels).
*   **Server-Side:** Hosts the LLM and the **Execution Control Layer**.

### Operational Workflow

1.  **Concatenation:** User input is concatenated with the encrypted prompt.
2.  **Processing:** The LLM processes the unified text.
3.  **Interception:** The Execution Control Layer intercepts all outgoing API calls.
4.  **Decryption & Verification:** The layer decrypts the permissions and verifies if the LLM's proposed actions match the granted permissions.
5.  **Deterministic Enforcement:** Security is enforced deterministically based on the cryptographic check.

### Security Coverage

This approach is designed to protect against:
*   Malicious users attempting to exceed privileges.
*   Indirect prompt injections originating from external content (e.g., web pages).
*   Autonomous LLM misbehavior or hallucinations.

---

## üí° Contributions

*   **Encrypted Prompt Framework:** Introduces a novel framework that embeds permission metadata directly into prompts to guide and constrain execution.
*   **Paradigm Shift:** Moves the industry focus from probabilistic detection (which can fail) to cryptographic verification for guaranteed prevention of unauthorized actions.
*   **Robustness Assurance:** Ensures system-level integrity remains intact even if the LLM is compromised, tricked, or misaligned.

---

## üìâ Results

The provided text focuses on qualitative proof-of-concept scenarios rather than quantitative experimental metrics (such as Attack Success Rate or latency).

*   **Blocking Rate:** The mechanism successfully blocked all tested unauthorized actions, including sending passwords via email and moving cloud data.
*   **Threat Vector Coverage:** Successfully mitigated direct prompt injection, indirect injection via retrieved content, and autonomous LLM hallucinations.
*   **Granular Execution:** Demonstrated the ability to execute permitted actions (e.g., web crawling) while blocking subsequent unauthorized actions triggered within the same prompt context.