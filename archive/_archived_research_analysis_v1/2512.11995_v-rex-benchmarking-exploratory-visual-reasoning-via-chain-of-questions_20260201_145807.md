# V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions

*Chenrui Fan; Yijun Liang; Shweta Bhardwaj; Kwesi Cobbina; Ming Li; Tianyi Zhou*

---

> ### ðŸ“‹ Executive Summary
>
> Current Vision-Language Models (VLMs) demonstrate exceptional proficiency in answering straightforward visual questions but encounter a critical bottleneck in complex, open-ended tasks requiring multi-step visual exploration. Real-world visual reasoning necessitates an iterative process of information gathering and synthesis, yet existing evaluation frameworks fail to diagnose specific failure modes. They cannot distinguish whether a model lacks the ability to plan a course of action or simply cannot execute the individual steps required to reach a solution. This inability to isolate the reasoning process hinders the development of robust exploratory agents.
>
> To address this diagnostic gap, the paper introduces **V-REX** (Visual Reasoning with multi-step EXploration), a benchmark suite formulating exploratory reasoning as a "Chain-of-Questions" (CoQ). V-REX technically disentangles the reasoning process into two distinct metrics: **"Planning"** (decomposing tasks) and **"Following"** (answering sequential questions). A key technical innovation is the utilization of a quantification strategy with finite options; rather than relying on open-ended generation which is difficult to evaluate, V-REX curates specific multiple-choice options for both questions and answers.
>
> The study evaluated a comprehensive dataset of 3,712 samples across 13 reasoning categories. Experiments revealed a substantial and consistent performance discrepancy between Planning and Following capabilities across all tested models. While larger models exhibited better scaling behaviors, results confirmed that success in task decomposition does not guarantee successful execution, with all VLMs performing significantly below human-level proficiency. This research suggests that future model development must treat Planning and Following as distinct skill sets.

---

### âš¡ Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Benchmark Name** | V-REX (Visual Reasoning with multi-step EXploration) |
| **Core Framework** | Chain-of-Questions (CoQ) |
| **Dataset Size** | ~3,712 samples |
| **Task Categories** | 13 Categories (e.g., Navigation, Flowcharts, GUIs) |
| **Logic-Heavy Content** | ~43% of dataset |
| **Evaluation Modes** | Planning vs. Following |
| **Quality Score** | 7/10 |

---

## Key Findings

*   **Performance Bottleneck:** Current State-of-the-Art (SOTA) Vision-Language Models struggle with complex, open-ended tasks requiring multi-step visual exploration, despite excelling at straightforward questions.
*   **Skill Discrepancy:** There is a significant discrepancy between a model's ability to **'Plan'** (decompose tasks) and **'Follow'** (answer sequential questions), indicating that these are distinct skill sets.
*   **Scaling Trends:** Evaluations of both proprietary and open-sourced VLMs reveal consistent scaling trends, yet substantial improvements are still needed in their ability to perform multi-step exploratory reasoning effectively.
*   **Planning â‰  Execution:** Success in the Planning stage does not guarantee success in the Following stage; competence in decomposition does not imply competence in execution.

---

## Methodology

The research introduces the **Visual Reasoning with multi-step EXploration (V-REX)** evaluation suite. The methodology is structured as follows:

*   **Chain-of-Questions (CoQ):** Formulates multi-step exploratory reasoning by disentangling the evaluation into two distinct stages:
    1.  **Planning:** Assessing the model's ability to break down high-level tasks into relevant sub-questions.
    2.  **Following:** Assessing the model's ability to answer the generated sequential questions correctly.
*   **Quantification Strategy:** To manage the vast space of intermediate steps, the methodology curates **finite options for questions and answers**. This allows for reliable, fine-grained analysis and avoids the instability often associated with evaluating open-ended text generation.
*   **Scope:** The benchmark targets native multi-step visual reasoning across diverse, open-ended scenarios.

---

## Technical Details

### Chain-of-Questions Framework
V-REX decomposes complex visual tasks into sequential steps to isolate two distinct cognitive skills:
1.  **Planning:** Decomposing high-level tasks into sub-questions.
2.  **Following:** Answering the sequential questions derived in the planning phase.

### Dataset Composition
The dataset comprises approximately **3,712 samples** across **13 reasoning categories**. Logic-heavy tasks constitute roughly **43%** of the data.

The categories are grouped as follows:

*   **Search & Navigation**
    *   Retrieval (584 samples)
    *   Navigation (318 samples)
*   **Logic & Deduction**
    *   Guessing (818 samples)
    *   Flowchart (784 samples)
*   **Visual Attributes & Relations**
    *   Pattern
    *   Property
    *   Relationship
    *   Counting
*   **Semantic & Symbolic Decoding**
    *   Word Puzzle
    *   GUI (Graphical User Interface)
    *   Map
*   **Temporal & Data Analysis**
    *   Traffic
    *   Trend

---

## Contributions

*   **V-REX Benchmark Suite:** Provides the first comprehensive benchmark suite targeting native multi-step visual reasoning in diverse scenarios, addressing the lack of evaluation tools for open-ended visual tasks.
*   **Chain-of-Questions Framework:** Introduces the CoQ analytical framework to examine the intermediate reasoning steps of VLMs, separating the formulation of the problem from the solution.
*   **Validated Evaluation Protocol:** Establishes a robust evaluation protocol using finite multiple-choice options for precise measurement, bypassing the need for unreliable LLM-based judges.
*   **Empirical Baseline:** Delivers a baseline assessment of current proprietary and open-sourced VLMs, highlighting the gap between planning and execution capabilities.

---

## Results

*   **Significant Gap in Capabilities:** Experiments reveal a distinct discrepancy between VLMs' 'Planning' and 'Following' capabilities.
*   **Model Shortcomings:** Current SOTA models struggle significantly with multi-step visual exploration, even when they perform well on single-step questions.
*   **Scaling Behavior:** While larger models generally exhibit better scaling behavior, they still fall short of human-level proficiency in multi-step reasoning.
*   **Dataset Statistics:**
    *   Total Instances: 3,712.
    *   Dominant Categories: Logic & Deduction (Guessing: 818, Flowchart: 784).
    *   Balanced Categories: Retrieval (584) and Navigation (318).

---

**References:** 40 citations  
**Quality Score:** 7/10