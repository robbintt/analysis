# GroundSight: Augmenting Vision-Language Models with Grounding Information and De-hallucination

*Xinxi Chen; Tianyang Chen; Lijia Hong*

---

> ### **Quick Facts: Key Metrics**
>
> *   **Baseline Model:** Llama-3.2-Vision-11B
> *   **VQA Accuracy:** +3.45 percentage points (22.19% $\rightarrow$ 25.64%)
> *   **Hallucination Rate:** Reduced by 51.91% (65.79% $\rightarrow$ 13.88%)
> *   **Dataset:** Meta RayBan Smart Glasses Dataset v2 (1,938 questions)
> *   **IoU Score:** 0.4473 (Final evaluation)

---

## Executive Summary

Vision-Language Models (VLMs) frequently suffer from hallucinations—generating plausible but incorrect responses—and struggle with visual-textual misalignment when processing complex scenes. This is particularly problematic in real-world Visual Question Answering (VQA) applications, such as assistive smart glasses, where irrelevant background noise often distracts the model, and high hallucination rates undermine user trust. The paper addresses the critical challenge of improving the truthfulness and accuracy of VLM outputs by shifting from whole-image analysis to a more precise, object-focused approach.

GroundSight introduces a Retrieval-Augmented Generation (RAG) framework that uniquely integrates text-grounded object localization with a confidence-based de-hallucination mechanism. Technically, the system employs a Region of Interest (RoI) Proposer—utilizing either Grounding DINO or a progressively trained BLIP-2 model—to generate bounding boxes around relevant objects. These cropped regions are then used for focused retrieval via the Meta Image Search API against a 900K image database. Crucially, the architecture includes a Question-Type-Based De-hallucination module that filters outputs or triggers "I don't know" responses for low-confidence predictions, thereby ensuring answers are strictly grounded in the retrieved visual evidence.

Evaluated on the Meta RayBan Smart Glasses Dataset v2, GroundSight demonstrated superior performance compared to the Llama-3.2-Vision-11B baseline. The framework achieved a VQA accuracy of 25.64%, representing a 3.45 percentage point improvement over the baseline's 22.19%. More significantly, the system drastically reduced the hallucination rate from 65.79% to 13.88%. Additionally, the progressive training pipeline for the localizer effectively optimized Intersection over Union (IoU) scores, reaching a final evaluation score of 0.4473, indicating precise object localization capabilities.

This research establishes GroundSight as a robust augmentation method for VLMs, demonstrating that combining RAG with precise grounding significantly mitigates hallucinations. By validating the approach on a real-world smart glasses dataset, the work highlights its potential for edge-computing applications where reliability is paramount. The progressive localization training strategy and the integration of question-type-aware filtering offer a reproducible blueprint for future researchers seeking to enhance the truthfulness and grounding of generative multimodal systems.

---

## Key Findings

*   **Significant Accuracy Improvement:** VQA accuracy increased from **22.19%** to **25.64%**, marking a **3.45 percentage point** increase over the Llama-3.2-Vision-11B baseline.
*   **Drastic Hallucination Reduction:** The hallucination rate was substantially lowered from **65.79%** to **13.88%**.
*   **Enhanced Truthfulness:** The method notably improved the overall truthfulness score of model responses.
*   **Noise Mitigation:** The object localization approach successfully reduced background noise and improved visual-textual alignment.

---

## Methodology

The research proposes a **Retrieval-Augmented Generation (RAG)** framework designed specifically for Vision-Language Models. The methodology is built upon two core components:

1.  **Text-Grounded Object Localization:**
    *   Generates bounding boxes around relevant objects identified in the scene.
    *   Enables targeted image cropping for focused retrieval rather than analyzing the whole image.

2.  **Question-Type-Based De-hallucination:**
    *   A mechanism that analyzes the specific type of question being asked.
    *   Filters or corrects outputs based on this classification to minimize errors.

---

## Technical Details

GroundSight is a RAG framework built on **Llama-3.2-Vision-11B** (selected after experimentation with BLIP and QWen). Its architecture is designed to maximize localization precision and retrieval accuracy.

### Core Architecture
*   **Region of Interest (RoI) Proposer:** Localizes relevant objects for processing.
*   **Image-Based Information Retriever:** Utilizes cropped image regions to perform queries against a database.

### Localization Strategies
The system localizes RoI using one of two methods:
1.  **Pretrained Localizer:** Utilizes Grounding DINO.
2.  **Progressive Training Pipeline:** A 4-stage training process for BLIP-2 on the RefCOCOg dataset:
    *   *Stage 1:* BBox head training.
    *   *Stage 2:* Unfreezing Q-Former.
    *   *Stage 3:* Unfreezing 6 vision layers.
    *   *Stage 4:* Unfreezing 8 vision layers.

### De-hallucination & Retrieval
*   **Confidence Threshold:** The system outputs "I don't know" for low-confidence predictions to maintain truthfulness.
*   **Retrieval Mechanism:** Uses the Meta Image Search API on a database of **900K images**.
    *   **Embeddings:** CLIP.
    *   **Similarity Metric:** Cosine similarity.

---

## Research Contributions

*   **Integration of Localization with RAG:** Introduces a novel method to shift VQA retrieval from whole-image analysis to object-focused cropping using bounding boxes.
*   **De-hallucination Strategy:** Proposes a strategy leveraging question type classification to reduce hallucinations in Vision-Language Models.
*   **Performance Benchmarking:** Establishes GroundSight as a robust augmentation method, demonstrating superior performance in accuracy and truthfulness compared to the baseline agent.

---

## Experimental Results

### Performance on Meta RayBan Smart Glasses Dataset v2 (1,938 questions)
*   **Accuracy:** Improved to **25.64%** (+3.45 percentage points over the 22.19% baseline).
*   **Hallucination Rate:** Reduced from **65.79%** to **13.88%**.

### Intersection over Union (IoU) Optimization
The 4-stage training pipeline achieved progressively higher IoU scores:
*   Stage 1: 0.2585
*   Stage 2: 0.3103
*   Stage 3: 0.3718
*   Stage 4: 0.4552
*   **Final Evaluation Score:** 0.4473

---

**Quality Score:** 8/10  
**References:** 24 citations