# A Survey of In-Context Reinforcement Learning

*Amir Moeini; Jiuqi Wang; Jacob Beck; Ethan Blaser; Shimon Whiteson; Rohan Chandra; Shangtong Zhang*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Methodology** | Literature Survey |
| **References** | 12 Citations |
| **Key Focus** | In-Context Reinforcement Learning (ICRL) |

---

## Executive Summary

> Standard reinforcement learning (RL) agents face a significant computational bottleneck because they rely on expensive backward passes (gradient updates) to optimize policy parameters for every new task. This dependence on explicit optimization hampers real-time generalization and rapid adaptation in novel environments. As the field seeks to bridge the gap between the sample efficiency of meta-learning and the inference speed of supervised learning, there is a critical need to understand alternative mechanisms that allow agents to solve new tasks without network parameter updates.

This survey establishes **"In-Context Reinforcement Learning" (ICRL)** as a distinct and structured area of study to address these limitations. The authors formally define ICRL through a policy function $\pi(\cdot|S_t, C_t)$ that conditions on both the current state $S_t$ and a trajectory context $C_t$â€”comprising action-observation historiesâ€”to solve tasks purely through inference.

The review provides a comprehensive taxonomy of pretraining paradigms, categorizing them into **Supervised Pretraining** (utilizing behavior cloning with cross-episode inputs and Return-To-Go features) and **Reinforcement Pretraining** (which adapts standard algorithms like DDPG, PPO, and DQN). Furthermore, the technical synthesis highlights specific stability strategies identified in the literature, such as decoupling actor-critic objectives from return magnitudes, employing multiple discount rates, and utilizing curriculum learning with tasks that marginally exceed the agent's current expertise.

The surveyed literature demonstrates that ICRL agents can achieve regret levels nearly equivalent to standard bandit algorithms while eliminating the computational cost of backward passes during inference. Empirical findings discussed in the survey highlight the ability of these agents to adapt to new environments, such as the "Dark Room" task, in as few as 300 interactions. Validations across complex benchmarks like XLand 2.0 and DMLab Watermaze confirm the efficacy of this approach, though the authors note that agents relying solely on expert prompts may suffer performance degradation when presented with suboptimal data unless sufficient online interaction is permitted.

---

## Key Findings

*   **Computational Bottleneck:** Standard RL agents rely on computationally expensive backward passes for policy optimization, creating efficiency issues.
*   **Parameter-Free Adaptation:** Specialized agents can solve new tasks without explicit network parameter updates.
*   **Contextual Generalization:** Generalization is achieved by conditioning behavior on action-observation histories.
*   **Formal Definition:** This capability is formally defined as **'in-context reinforcement learning'**.

---

## Methodology & Contributions

### Methodology
The authors utilize a **literature survey methodology**, reviewing and synthesizing existing research to demonstrate how agents solve tasks via conditioning rather than explicit parameter optimization.

### Contributions
*   **Field Definition:** Establishes 'in-context reinforcement learning' as a distinct area of study.
*   **Behavioral Analysis:** Provides a structured analysis of agent behaviors using context to replace backward passes.
*   **Comparative Framework:** Delineates the computational and architectural differences between standard policy optimization and in-context problem solving.

---

## Technical Details

### Policy Definition
The approach uses a policy conditioned on both the state $S_t$ and context $C_t$ (trajectory history), defined as:
$$ \pi(\cdot|S_t, C_t) $$

### Pretraining Paradigms
*   **Supervised Pretraining:**
    *   Behavior Cloning with cross-episode input.
    *   Curriculum learning.
    *   Return-To-Go features.
*   **Reinforcement Pretraining:**
    *   Modifications of standard algorithms: **DDPG**, **PPO**, and **DQN**.

### Stability & Optimization Techniques
*   Use of **multiple discount rates**.
*   Task selection slightly beyond current expertise (curriculum).
*   **Decoupling actor-critic objectives** from return magnitudes.

### Test-time Management
*   Prompting with expert or suboptimal trajectories.
*   Dynamic Return-To-Go estimation.

---

## Results

*   **Performance Parity:** ICRL agents achieve regret nearly equivalent to standard bandit algorithms.
*   **Rapid Adaptation:** Agents adapt to new environments like 'Dark Room' in only **300 interactions**.
*   **Scaling Factors:** Generalization performance scales with:
    *   Model size
    *   Pretraining duration
    *   Experience diversity
*   **Learning Rate Correlation:** The learning rate in input trajectories during pretraining directly correlates with in-context improvement speed.
*   **Benchmark Success:**
    *   **XLand 2.0:** Generalization in 3D procedurally generated environments.
    *   **DMLab Watermaze:** Success with raw pixel inputs.
*   **Computational Efficiency:** ICRL improves efficiency by eliminating backward passes.
*   **Sensitivity:** Methods relying solely on expert prompts suffer drops with suboptimal data unless allowed more online interaction.