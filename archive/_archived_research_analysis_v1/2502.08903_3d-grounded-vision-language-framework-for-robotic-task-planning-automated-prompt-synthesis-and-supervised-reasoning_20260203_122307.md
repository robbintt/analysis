---
title: '3D-Grounded Vision-Language Framework for Robotic Task Planning: Automated
  Prompt Synthesis and Supervised Reasoning'
arxiv_id: '2502.08903'
source_url: https://arxiv.org/abs/2502.08903
generated_at: '2026-02-03T12:23:07'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# 3D-Grounded Vision-Language Framework for Robotic Task Planning: Automated Prompt Synthesis and Supervised Reasoning

*Guoqin Tang; Qingxuan Jia; Zeyuan Huang; Gang Chen; Ning Ji; Zhipeng Yao*

---

### üìã Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Task Success Rate** | 96.0% |
| **Performance Impact** | 67% TSR drop without key modules (ablation) |
| **Training Data** | 240 samples (BridgeData V2) |
| **Retraining Required** | None (Zero-shot adaptation) |
| **Safety Constraints** | Max 10 N grip force, Min 0.1 m obstacle distance |
| **Quality Score** | 9/10 |

---

## üìù Executive Summary

Robotic manipulation requires precise 3D spatial understanding, yet state-of-the-art Vision-Language Models (VLMs) are primarily trained on 2D image data, limiting their ability to interact directly with the physical world. Furthermore, relying solely on VLMs for task planning often introduces "hallucinations"‚Äîgenerating incorrect or unexecutable code‚Äîand typically necessitates expensive model retraining when deploying robots in novel environments. This paper addresses the critical challenge of bridging the gap between 2D visual perception and 3D spatial reasoning to enable reliable, fine-grained robotic control without the overhead of environment-specific retraining.

The authors propose a "3D-Grounded Vision-Language Framework" that integrates a 2D Prompt Synthesis Module with a Small Language Model (SLM) supervision loop. Technically, the framework utilizes a confidence-based registration strategy‚Äîincorporating Depth Measurement Entropy ($H_3$) and Temporal Stability Entropy ($H_4$)‚Äîto map 2D image observations onto 3D point clouds. This mechanism allows standard 2D VLMs to extract 3D spatial data through a Nearest Neighbor Selection strategy. To ensure reliability, an SLM acts as a supervisor in a closed-loop state transition system ($S = (X, U, F)$), validating VLM outputs against safety constraints (e.g., 10 N gripping force) to correct hallucinations and generate precise robotic control code.

Experimental results demonstrate the framework's effectiveness, achieving a Task Success Rate (TSR) of 96.0% in indoor environments using a FRANKA robotic arm equipped with RealSense and LiDAR sensors. Ablation studies underscore the necessity of the proposed architecture; removing the 2D prompt synthesis and SLM supervision modules caused a catastrophic 67% drop in TSR. Notably, the system achieved high performance on the BridgeData V2 dataset using only 240 samples (24 unique scenes), validating its ability to generalize from minimal data without task-specific retraining.

This work significantly advances the field by equipping multimodal Large Language Models with robust "3D scene localization" capabilities, effectively enabling 2D-trained models to perform complex 3D tasks autonomously. By mitigating hallucinations through SLM supervision, the framework enhances the safety and reliability of robotic planning. The elimination of retraining requirements offers substantial cost efficiency and high transferability, allowing robots to adapt dynamically to new environments and execute fine-grained operations with immediate deployment capability.

---

## üîç Key Findings

*   **High Success Rate:** The proposed framework achieved a **96.0% Task Success Rate (TSR)**, significantly outperforming existing methods in robotic task planning.
*   **Critical Ablation Results:** Studies demonstrated that removing the 2D prompt synthesis and output supervision modules caused a **67% drop in TSR**, highlighting their absolute necessity for system integrity.
*   **Cost Efficiency:** The framework eliminates the need for retraining in new environments, drastically reducing deployment costs and time.
*   **3D Capability:** It successfully enables standard 2D-trained Vision-Language Models (VLMs) to extract precise 3D spatial information autonomously, overcoming traditional 2D limitations.

---

## üõ†Ô∏è Methodology

The researchers proposed a novel framework to bridge 2D visual understanding and 3D robotic interaction without the need for retraining. The approach consists of two primary components:

1.  **2D Prompt Synthesis Module:** This module maps 2D images to 3D point clouds. This mapping allows standard VLMs to extract accurate 3D spatial information despite being trained on 2D data.
2.  **Supervised Reasoning:** A Small Language Model (SLM) is employed to supervise the outputs of the VLM. This supervision mitigates hallucinations and ensures the generation of reliable and executable robotic control code.

---

## ‚öôÔ∏è Technical Details

The framework relies on a sophisticated architecture involving entropy-based confidence registration and closed-loop supervision.

*   **Confidence-Based Registration:**
    *   Utilizes four entropy components.
    *   **Depth Measurement Entropy ($H_3$):** Calculated based on local depth variance.
    *   **Temporal Stability Entropy ($H_4$):** Calculated based on point displacement across frames.
*   **2D Prompt Synthesis Strategy:**
    *   Employs **Nearest Neighbor Selection** to bridge 3D data and 2D VLMs.
    *   Calculates centroids ($c_k$) and embedding candidate points ($N_k$).
    *   Uses task-driven weight allocation ($\lambda_n$).
*   **SLM-VLM Closed-Loop Interaction:**
    *   Modeled as a state transition system $S = (X, U, F)$.
    *   The SLM supervises the VLM using a feedback vector $Y^{(n)}$ to ensure logical consistency.
*   **Safety Constraints:**
    *   **Gripping Force:** Limited to a maximum of **10 N**.
    *   **Obstacle Avoidance:** Requires a minimum obstacle distance of **0.1 m**.

---

## üìä Research Contributions

*   **3D Scene Localization:** Equips multimodal Large Language Models (LLMs) with robust 3D scene localization capabilities, enabling fine-grained operations that were previously impossible for 2D-centric models.
*   **Reliability Enhancement:** Improves reliability and actively mitigates hallucinations in robotic planning by using SLM supervision of VLM outputs.
*   **High Transferability:** The framework offers high transferability, enabling robots to adapt to dynamic new environments and execute tasks without the cost of retraining.

---

## üß™ Experimental Results

*   **Performance:** Achieved a **96.0% Task Success Rate (TSR)**.
*   **Ablation Study:** Confirmed that removing the 2D prompt synthesis and output supervision modules resulted in a **67% drop in TSR**.
*   **Experimental Setup:**
    *   **Robot:** FRANKA robotic arm.
    *   **Sensors:** RealSense RGB-D camera and LiDAR sensor.
    *   **Environment:** Indoor setting.
*   **Dataset:** Utilized the BridgeData V2 dataset with **240 samples** covering 24 unique scenes.
*   **Efficiency:** Achieved high performance with minimal data and without task-specific retraining.

---
**References:** 40 citations