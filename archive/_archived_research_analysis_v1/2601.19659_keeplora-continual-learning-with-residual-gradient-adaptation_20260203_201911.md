---
title: 'KeepLoRA: Continual Learning with Residual Gradient Adaptation'
arxiv_id: '2601.19659'
source_url: https://arxiv.org/abs/2601.19659
generated_at: '2026-02-03T20:19:11'
quality_score: 9
citation_count: 31
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# KeepLoRA: Continual Learning with Residual Gradient Adaptation

*Mao-Lin Luo; Zi-Hao Zhou; Yi-Lin Zhang; Yuanyu Wan; Tong Wei; Min-Ling Zhang*

---

> ### ðŸ“Š Quick Facts
> ---
> * **Dataset:** MLLM-DCL
> * **Domains:** 5 (Sensing, Medical, Driving, Science, Finance)
> * **Last Score:** 64.41%
> * **Average Score:** 54.19%
> * **Transfer Score:** 33.71%
> * **Quality Score:** 9/10
> * **References:** 31 citations

---

## Executive Summary

### **Problem**
Continual learning in large pre-trained Vision-Language Models (VLMs) presents a critical challenge: as models learn new tasks sequentially, they tend to suffer from **catastrophic forgetting**, where the acquisition of new knowledge degrades performance on previously learned tasks. This phenomenon highlights the "stability-plasticity dilemma," where models must remain stable enough to retain pre-trained and prior task knowledge while remaining plastic enough to integrate new information. Existing methods often struggle to balance these competing objectives effectively without incurring high computational costs or significant performance drops.

### **Innovation**
The paper introduces **KeepLoRA**, a novel framework based on Residual Gradient Adaptation that rigorously addresses the stability-plasticity dilemma through subspace decomposition. The authors theoretically demonstrate that general knowledge resides in the **"principal subspace"**, while task-specific knowledge is encoded in the **"residual subspace."** KeepLoRA leverages this insight by constraining Low-Rank Adaptation (LoRA) parameter updates strictly to the residual subspace. Technically, the method projects the gradients of new tasks onto a subspace orthogonal to both the pre-trained model's principal subspace (identified via SVD) and the dominant feature directions of previous tasks. By freezing the down-projection matrix and optimizing the up-projection matrix within this constrained residual space, the model isolates new learning, preventing interference with existing capabilities.

### **Results**
Evaluated on the **MLLM-DCL dataset** across five diverse domains (Sensing, Medical, Driving, Science, and Finance) in a Multi-domain Task Incremental Learning setting, KeepLoRA achieved state-of-the-art performance. It recorded a **Transfer score of 33.71%**, an **Average score of 54.19%**, and a **Last score of 64.41%**. These results represent significant improvements over strong baselines, including a ~4.5% improvement over the SEFE method on the Last metric and nearly 5% improvement over LoRA-FT on the Average metric. Furthermore, ablation studies confirmed that restricting updates to the residual subspace is crucial, as domain-specific tasks suffered severe performance drops without this residual component infusion.

### **Impact**
This research offers significant mechanistic insight into the internal knowledge retention of VLMs, formally delineating the roles of principal versus residual subspaces in parameter space. By providing a theoretically grounded yet computationally efficient solution to continual learning, KeepLoRA establishes a viable path for deploying large models in dynamic environments where data streams arrive sequentially. The approach validates that precise gradient projection can successfully decouple new learning from old, setting a new standard for future research on adaptive, non-destructive model training in vision-language tasks.

---

## Key Findings

*   **Subspace Knowledge Decomposition:** General knowledge is primarily encoded in the principal subspace, while task-specific knowledge is encoded in the residual subspace.
*   **Interference Prevention:** Restricting Low-Rank Adaptation (LoRA) parameter updates to the residual subspace effectively prevents interference with capabilities learned in previous tasks.
*   **Gradient Projection Strategy:** New knowledge can be infused without catastrophic forgetting by projecting gradients onto a subspace orthogonal to both the pre-trained model's principal subspace and the dominant directions of previous task features.
*   **Performance Validation:** Theoretical and empirical analyses confirm that the approach successfully balances the retention of pre-trained knowledge, preservation of sequential task knowledge, and the plasticity to learn new tasks, achieving state-of-the-art performance.

---

## Methodology

The paper proposes **KeepLoRA**, a continual learning framework for pre-trained vision-language models based on **Residual Gradient Adaptation**. The methodology identifies general knowledge in the **'principal subspace'** and task-specific information in the **'residual subspace'**.

When learning a new task, KeepLoRA restricts updates to LoRA parameters within the residual subspace. This is achieved by projecting the new task's gradient onto a subspace strictly orthogonal to:
1.  The principal subspace of the original pre-trained model.
2.  The dominant feature directions associated with previously learned tasks.

---

## Technical Details

KeepLoRA proposes a continual learning framework for Vision-Language Models using Low-Rank Adaptation (LoRA) constrained within specific residual subspaces.

**Core Mechanism: Residual Gradient Adaptation**
*   Restricts parameter updates to a subspace orthogonal to:
    *   The pre-trained model's principal knowledge ($W_p$).
    *   The dominant feature directions of previous tasks ($M_{t-1}$).

**Process Flow**
1.  **Decomposition:** The method decomposes the weight matrix via **SVD** to define $W_p$.
2.  **Accumulation:** It accumulates singular vectors from previous tasks in $M_{t-1}$.
3.  **Initialization:** LoRA matrices are initialized based on the SVD of the current task's gradient projected onto this residual subspace.
4.  **Optimization:** The down-projection matrix $A_t$ is frozen during training, and only the up-projection matrix $B_t$ is optimized. This is theoretically equivalent to gradient descent constrained to the span of $A_t$.
5.  **Feature Projection:** Input features are projected into the residual space to extract task-specific knowledge for updating the memory matrix.

---

## Contributions

*   **Mechanistic Insight:** Provided a fundamental analysis of knowledge retention mechanisms within model parameter space, specifically delineating the roles of principal versus residual subspaces in storing general versus task-specific knowledge.
*   **Novel Algorithm:** Introduced KeepLoRA, a simple yet effective algorithm that solves the stability-plasticity dilemma in vision-language models by utilizing orthogonal gradient projection to isolate new learning in the residual subspace.
*   **Balanced Optimization:** Demonstrated a viable solution to the competing objectives of continual learning (retaining pre-trained knowledge, preserving past task knowledge, and maintaining plasticity), validated through state-of-the-art empirical results.

---

## Results

Evaluated on the **MLLM-DCL dataset** across five domains (Sensing, Medical, Driving, Science, Finance) in a Multi-domain Task Incremental Learning setting:

*   **Performance Metrics:**
    *   **Transfer Score:** 33.71%
    *   **Average Score:** 54.19%
    *   **Last Score:** 64.41%

*   **Baseline Comparison:**
    *   Outperformed baselines such as **LoRA-FT**, **O-LoRA**, **CL-MoE**, and **SEFE**.
    *   Achieved a **~4.5% improvement** over SEFE on the Last metric.
    *   Achieved a **nearly 5% improvement** over LoRA-FT on the Average metric.

*   **Subspace Analysis:**
    *   Validated that specific-domain tasks suffer significant performance drops without residual components.
    *   Confirmed the importance of infusing new knowledge into these subspaces.