---
title: Expert or not? assessing data quality in offline reinforcement learning
arxiv_id: '2510.12638'
source_url: https://arxiv.org/abs/2510.12638
generated_at: '2026-02-03T07:04:06'
quality_score: 9
citation_count: 25
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Expert or not? assessing data quality in offline reinforcement learning

*Arip Asadulaev; Fakhri Karray; Martin Takac*

---

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 9/10
> *   **Citations:** 25
> *   **Core Metric:** Bellman Wasserstein Distance (BWD)
> *   **Key Dataset:** D4RL MuJoCo
> *   **Performance Correlation:** 0.97 Spearman Rank
> *   **Algorithms Referenced:** TD3+BC, Implicit Q-Learning (IQL)

---

## Executive Summary

**Problem**
Offline Reinforcement Learning (RL) relies on static datasets, yet the efficacy of an agent is heavily dependent on the quality and composition of this data. A major challenge in the field is the inability to assess the value of a dataset prior to the computationally expensive process of training a policy. Simple heuristics, such as cumulative reward, are insufficient for capturing the complex distributional shifts that characterize effective offline RL data. This paper addresses the critical need for efficient data triage, specifically investigating how to determine the "expertise" level and potential utility of a dataset without requiring environment interaction or full policy optimization.

**Innovation**
The authors introduce the **Bellman Wasserstein Distance (BWD)**, a novel, value-aware metric designed to estimate dataset quality *a priori*. BWD operates by measuring the dissimilarity between the dataset's empirical state-action distribution ($P_\beta$) and a reference random distribution ($P_{rand}$) using a state-conditional optimal transport formulation. Technically, the method utilizes a dual formulation of the entropically regularized Kantorovich problem. Its core innovation lies in a specific cost function, which intuitively measures the "value gap" by penalizing the transport of mass to random actions that have low Q-values. This allows BWD to compute a precise distance metric using only a behavioral critic, effectively capturing dataset expertise without needing an active agent.

**Results**
Validated on D4RL MuJoCo tasks, BWD demonstrated a superior ability to predict potential agent success compared to state-of-the-art baselines. The proposed metric achieved a **Spearman rank correlation coefficient of 0.97** with the "oracle performance score" (an aggregate of results from algorithms like TD3+BC and IQL), significantly outperforming naive baselines such as cumulative reward, which exhibited correlations often below 0.5. Beyond diagnostic assessment, the integration of BWD as a regularizer during policy optimization was shown to actively improve agent returns by penalizing random behaviors. These high correlation scores validate BWD as a reliable proxy for dataset quality, enabling accurate performance estimation without incurring training costs.

**Impact**
This research represents a significant advancement in offline RL methodology by providing a practical solution for dataset assessment with unknown provenance. The introduction of BWD establishes a versatile component that serves dual purposes: as a diagnostic tool for pre-training data triage and as a functional regularizer to enhance policy learning. By demonstrating that value-aware, distributional signals are statistically superior to simplistic metrics like cumulative reward, the work shifts the paradigm of data quality assessment. This capability is crucial for scaling RL systems, enabling practitioners to efficiently curate data and optimize resource allocation without committing to futile training cycles.

---

## Key Findings

*   **Effective Quality Estimation:** The study demonstrates that it is possible to estimate the quality of offline RL datasets *a priori* (without training an agent) using value-aware, distributional signals.
*   **Strong Correlation with Performance:** The proposed Bellman Wasserstein distance (BWD) shows strong correlation with an "oracle performance score" (aggregating multiple offline RL algorithms) across D4RL MuJoCo tasks, allowing for accurate prediction of potential agent performance on a specific dataset.
*   **Optimization Utility:** Beyond dataset assessment, integrating BWD as a regularizer during policy optimization actively improves returns by pushing the learned policy away from random behavior.
*   **Environment Independence:** The BWD metric can be computed effectively using only a behavioral critic and state conditional optimal transport, requiring no environment interaction or full policy optimization.

---

## Methodology

*   **Comparative Analysis:** The authors analyzed a spectrum of quality proxies, ranging from simple metrics (cumulative rewards) to more complex learned value-based estimators.
*   **Bellman Wasserstein Distance (BWD):** The core methodology introduces BWD, a value-aware optimal transport score.
*   **Measurement Formulation:** BWD is computed by measuring the dissimilarity between a dataset's behavioral policy and a random reference policy. This is achieved using a behavioral critic combined with a state conditional optimal transport formulation.
*   **Validation Framework:** The method was validated on D4RL MuJoCo tasks by comparing BWD scores against an aggregate of multiple offline RL algorithm performances.

---

## Technical Details

The paper proposes the **Bellman-Wasserstein Distance (BWD)**, a metric for estimating offline RL dataset quality *a priori* by measuring the deviation of the dataset's behavior policy from a random baseline using value-aware optimal transport.

### Formulation
BWD compares the empirical state-action distribution ($P_\beta$) from the dataset against a reference distribution ($P_{rand}$) consisting of states from the dataset and actions from a uniform random policy.

The value-aware cost function is defined as:

$$c((s, a), (s, a')) = Q^\beta_\theta(s, a') - \|a' - a\|^2_2$$

This incorporates the Q-value of the random action and the squared Euclidean distance to the behavioral action.

### Computation
BWD is computed via the dual formulation of the entropically regularized Kantorovich problem, utilizing:
*   Neural network parameterized potential functions ($g$ and $f$)
*   An entropic regularization term for stability

### Baselines & Algorithms
*   **Baselines:** Cumulative Reward, Q-function Estimation
*   **Reference Algorithms:** TD3+BC, Implicit Q-Learning (IQL)

---

## Results

*   **Correlation:** BWD demonstrates a strong correlation with the oracle performance score across D4RL MuJoCo tasks, enabling accurate prediction of potential agent performance without training.
*   **Regularizer Utility:** Integrating BWD as a regularizer during policy optimization improves agent returns by pushing the learned policy away from random behavior.
*   **Hyperparameter Sensitivity:** In TD3+BC experiments (testing 12 values over 3 seeds), increasing the $\alpha$ parameter (favoring Behavior Cloning) generally decreased performance on high-quality datasets but improved performance on low-quality datasets.
*   **Efficiency:** The BWD metric requires no environment interaction, relying solely on a behavioral critic and state conditional optimal transport.

---

## Contributions

*   **Novel Metric for Data Triage:** Introduction of the Bellman Wasserstein distance (BWD) as a practical tool for triaging offline RL datasets, solving the problem of assessing unknown data provenance and skill composition without training costs.
*   **Dual-Purpose Application:** Establishment of BWD as a versatile component that serves both as a diagnostic predictor for dataset fidelity and as a functional regularizer to enhance policy optimization.
*   **Advancement in Value-Aware Assessment:** Demonstrating that value-aware, distributional signals are superior to simpler cumulative reward metrics for understanding how dataset quality impacts downstream offline RL performance.