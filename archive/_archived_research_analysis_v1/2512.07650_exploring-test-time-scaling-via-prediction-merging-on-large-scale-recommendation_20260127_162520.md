---
title: Exploring Test-time Scaling via Prediction Merging on Large-Scale Recommendation
arxiv_id: '2512.07650'
source_url: https://arxiv.org/abs/2512.07650
generated_at: '2026-01-27T16:25:20'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Exploring Test-time Scaling via Prediction Merging on Large-Scale Recommendation

*Shenzhen Technology, Artificial Intelligence, Zhentai Chen, Fuyuan Lyu, Exploring Test, Prediction Merging, Jingyan Jiang, Scale Recommendation, Gill University*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 7/10
> *   **Primary Focus:** Deep Learning Recommendation Systems (DLRS)
> *   **Core Strategy:** Test-time Scaling via Prediction Merging
> *   **Key Innovation:** Shifting compute load from training to inference
> *   **Benchmarks:** Criteo, Avazu, KDD12
> *   **Models Validated:** 8 (Classical & SOTA)
> *   **References:** 40 Citations

---

## Executive Summary

This research addresses the critical efficiency bottleneck inherent in Deep Learning Recommendation Systems (DLRS), where traditional parameter scaling—increasing model depth or width—yields diminishing returns and prohibitive computational costs. As models scale to improve accuracy, the resulting inference latency creates a deployment paradox: state-of-the-art performance becomes impractical for real-world, large-scale online environments. The paper highlights that reliance solely on parameter-heavy architectures (such as Wide & Deep or D-MoE) leads to skyrocketing resource demands during training and inference, necessitating a paradigm shift to sustain performance growth without overwhelming infrastructure.

The authors propose **"Test-Time Scaling via Prediction Merging,"** an architecture-agnostic framework that optimizes computational resources during inference rather than parameters during training. Technically, the method operates in the logit space, formulating the final prediction as the arithmetic mean of individual model outputs ($\tilde{y} = \frac{1}{M} \sum_{m=1}^{M} \hat{y}^{(m)}$). To ensure diverse and meaningful predictions, the approach utilizes two distinct strategies:

*   **Heterogeneous Architectures:** Leveraging structural differences between varied models.
*   **Homogeneous Initialization:** Utilizing randomness from different weight seeds within the same architecture.

The authors introduce a **"Grouped Training"** algorithm, splitting models into mutually exclusive subsets trained sequentially on the subset's ensemble prediction, to effectively encourage this required diversity.

The proposed method was rigorously validated against eight distinct models—including classical baselines (FNN, DeepFM, DCNv2, FinalMLP, IPNN) and state-of-the-art scaling rivals (Wukong, RankMixer, Multi-Embed, D-MoE)—across three industry-standard benchmarks: Criteo, Avazu, and KDD12. Under the strict constraint of identical inference budget, test-time scaling consistently demonstrated superior efficiency and performance compared to traditional parameter scaling.

This work establishes a "scaling-efficient" path for DLRS, orthogonal to advancements seen in Language Models (LMs), by shifting the focus from static parameter growth to dynamic inference resource allocation. A primary contribution to the field is the demonstration of practical deployability: the method is fully compatible with parallel server infrastructure, allowing for linear acceleration in online production environments without increasing user-facing latency.

---

## Key Findings

*   **Superiority over Parameter Scaling:** Under identical inference budget constraints, test-time scaling outperforms traditional parameter scaling regarding efficiency and performance.
*   **Effectiveness of Diversity Strategies:** Generating diverse yet meaningful outputs for the same instance is feasible and effective, achieved through either the heterogeneity of different model architectures or the randomness of model initialization within a homogeneous architecture.
*   **Deployment Efficiency:** The proposed test-time scaling method can be seamlessly accelerated in online environments by increasing parallel servers without negatively impacting inference time for the user.
*   **Broad Validation:** The effectiveness of the solutions was proven sufficient across eight different models—including both classic and State-of-the-Art (SOTA)—spanning three benchmarks.

---

## Methodology

The research focuses on **Test-Time Scaling via Prediction Merging**, specifically for Deep Learning Recommendation Systems (DLRS). The primary goal is utilizing computational resources during inference rather than just during training.

To implement this, the authors propose two specific implementation strategies to generate diverse predictions for a single instance:

1.  **Heterogeneous Architectures:** Exploring the differences inherent in varying model architectures.
2.  **Homogeneous Initialization:** Utilizing the randomness derived from different model initializations within the same architecture.

The proposed methods were evaluated using a comparative analysis across eight distinct models on three separate benchmarks to measure performance against inference budgets.

---

## Contributions

*   **Paradigm Shift:** Addresses the underexplored area of test-time scaling in DLRS, shifting the focus from the established trend of scaling parameters during training to scaling computation during inference.
*   **Orthogonal Improvements:** Introduces a method to bring orthogonal improvements to recommendation systems, similar to advancements seen in Language Model (LM) domains.
*   **Resource Efficiency:** Demonstrates that test-time scaling offers a 'scaling-efficient' approach, allowing systems to achieve better performance without increasing the inference budget required for parameter scaling.
*   **Practical Applicability:** Establishes that the method is viable for real-world, large-scale online deployment, offering linear acceleration with parallel servers while maintaining constant user-side latency.

---

## Technical Details

**Formulation & Architecture**
*   **Task:** Large-scale recommendation formulated as a binary classification task.
*   **Pipeline:** Input multi-categorical features $\rightarrow$ Dense embeddings (via trainable tables) $\rightarrow$ Interaction layer (captures feature interactions) $\rightarrow$ Logits (via prediction module).
*   **Loss Function:** Binary Cross-Entropy.

**Scaling Approaches**
*   **Parameter Scaling (Traditional):**
    *   *Depth:* Stacking layers.
    *   *Width:* Parallel independent experts.
*   **Test-time Scaling (Proposed):**
    *   Operates in the logit space.
    *   Architecture agnostic.
    *   **Formula:** Final prediction is the arithmetic mean of individual predictions:
        $$ \tilde{y} = \frac{1}{M} \sum_{m=1}^{M} \hat{y}^{(m)} $$

**Training Algorithm**
*   **Grouped Training:** Models are split into $K$ mutually exclusive subsets and trained sequentially on the subset's ensemble prediction to encourage diversity.

---

## Results

**Evaluation Setup**
*   **Datasets:**
    *   *Criteo:* 1 week ad click data.
    *   *Avazu:* 10 days click logs.
    *   *KDD12:* Search session logs.
    *   *Preprocessing:* Specific discretization and OOV preprocessing applied.
*   **Metrics:** AUC (primary metric, validated with 0.1% significance) and Log Loss.

**Models Evaluated**
*   **Classical DLRS:** FNN, IPNN, DeepFM, DCN, DCNv2, FinalMLP.
*   **Scaling Baselines:** Wukong, RankMixer, Multi-Embed, D-MoE.

**Outcomes**
*   While specific numerical outcomes were not included in the text, the method was validated against a demanding 0.1% significance threshold.
*   Experiments confirmed that diversity-generation strategies enabled the test-time scaling approach to statistically significantly outperform all eight baseline models across all three datasets, proving that computational efficiency gains do not come at the cost of accuracy.