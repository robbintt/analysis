---
title: Small Language Model Makes an Effective Long Text Extractor
arxiv_id: '2502.07286'
source_url: https://arxiv.org/abs/2502.07286
generated_at: '2026-02-06T06:30:25'
quality_score: 8
citation_count: 11
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Small Language Model Makes an Effective Long Text Extractor

*Yelin Chen; Fanjin Zhang; Jie Tang*

---

> ### ⚡ Quick Facts
>
> *   **Model Proposed:** SeNER
> *   **Primary Innovation:** Bidirectional Sliding-Window Plus-Shaped Attention (BiSPA)
> *   **Key Efficiency:** Processes texts 6x longer than previous span-based methods.
> *   **Performance:** State-of-the-art (SOTA) on Scholar-XL and SciREX.
> *   **Quality Score:** 8/10
> *   **References:** 11 citations

---

## Executive Summary

This research addresses the computational and accuracy challenges associated with Named Entity Recognition (NER) in long-text documents. While existing span-based methods offer high precision, they suffer from prohibitive GPU memory consumption and redundant calculations when processing extended texts, rendering them ineffective for long documents. Conversely, generation-based Large Language Models (LLMs), though popular, struggle with the extraction accuracy required for long entity spans and incur significant time costs during fine-tuning. This presents a critical gap: the inability to efficiently and accurately extract information from long-form content, such as scientific papers or organizational homepages, without exhausting hardware resources.

The authors propose **SeNER**, a lightweight, GPU-memory-efficient framework designed to optimize span-based methods for long texts. The architecture introduces two key components: a Long Text Embedding mechanism and a Bidirectional Sliding-Window Plus-Shaped Attention (BiSPA) mechanism. The embedding layer utilizes a bidirectional arrow mechanism combined with LogN-Scaling on the `[CLS]` token to stabilize entropy. The core innovation, BiSPA, computes horizontal and vertical attention on focused spans within a sliding window. This approach drastically minimizes the number of candidate token-pair spans that require classification, thereby eliminating redundant computations and solving the memory bottlenecks typical of traditional span-based models.

SeNER achieves state-of-the-art (SOTA) extraction accuracy on three long-text NER datasets, including Scholar-XL and SciREX. It significantly outperforms generation-based LLMs, notably surpassing a baseline LLM that achieved only 21.87% F1 on the Scholar-XL dataset. In terms of efficiency, SeNER can process texts up to six times longer than previous span-based methods under identical hardware constraints. Ablation studies confirm the necessity of the BiSPA mechanism, as standard window sizes (e.g., 512) resulted in Out-of-Memory (OOM) errors without it. Furthermore, hyperparameter analysis demonstrated that while small window sizes limit information aggregation, SeNER’s optimized configuration maintains high F1 scores with significantly fewer parameters than LLMs.

The significance of this work lies in demonstrating that small language models can offer superior efficiency and accuracy compared to generation-based LLMs for specific downstream tasks like long-text extraction. By resolving the memory limitations of span-based methods through the BiSPA mechanism, the paper provides a viable path for processing extensive documents without the need for massive computational resources. This shift suggests that for specialized extraction tasks involving long entities, optimized small models present a more practical and scalable solution than fine-tuning large generative models, potentially influencing future NLP architectures to prioritize computational efficiency over sheer parameter scale.

---

## Key Findings

*   **SOTA Accuracy:** The proposed SeNER method achieves state-of-the-art (SOTA) extraction accuracy on three datasets specifically designed for long-text Named Entity Recognition (NER).
*   **Memory Optimization:** SeNER operates in a GPU-memory-friendly manner, effectively solving the excessive memory consumption issues typically associated with span-based methods.
*   **Handling Long Spans:** The approach successfully extracts longer entity spans (e.g., awards from homepages) from extended texts, a task where generation-based Large Language Models (LLMs) often struggle with accuracy.
*   **Computational Efficiency:** The model significantly reduces redundant computations by minimizing the number of candidate token-pair spans that need to be classified.

---

## Methodology

The paper proposes **SeNER**, a lightweight span-based NER framework designed for long texts. The methodology consists of two primary architectural components:

1.  **Long Text Embedding:** Utilizes a bidirectional arrow attention mechanism combined with LogN-Scaling applied to the `[CLS]` token.
2.  **BiSPA Mechanism:** A novel Bidirectional Sliding-Window Plus-Shaped Attention that serves to drastically reduce redundant candidate token-pair spans while simultaneously modeling the interactions between these spans.

---

## Research Contributions

*   **Addressing a Gap in Research:** The paper highlights and addresses the under-explored challenge of extracting longer entity spans from extended texts (like homepages), distinguishing it from standard short-text NER.
*   **Optimization of Span-Based Methods:** It introduces the BiSPA mechanism to overcome the computational bottlenecks (redundant calculations and high memory usage) that have historically limited span-based methods.
*   **Efficiency over LLMs:** It presents a small model alternative to generation-based LLMs, offering a solution that avoids the significant time costs and generation inaccuracies associated with fine-tuning large models for specific downstream NLP tasks.

---

## Technical Details

SeNER is a lightweight, GPU-memory-efficient method for Named Entity Recognition (NER) designed as an alternative to span-based and generation-based LLMs.

| Component | Description |
| :--- | :--- |
| **Attention Mechanism** | Employs a bidirectional arrow attention mechanism for encoding local and global contexts. |
| **Entropy Stabilization** | Utilizes LogN-Scaling on the `[CLS]` token. |
| **Extraction Core** | Implements a **Bidirectional Sliding-Window Plus Attention (BiSPA)** mechanism. |
| **Computation Strategy** | Computes horizontal and vertical attention on focused spans to reduce superfluous span-based computations. |
| **Training Strategy** | Integrates Whole Word Masking and LoRA (Low-Rank Adaptation). |

---

## Performance Results

*   **SOTA Performance:** SeNER achieves State-of-the-Art (SOTA) extraction accuracy on three long-text NER datasets, including Scholar-XL and SciREX.
*   **LLM Comparison:** It outperforms generation-based LLMs, addressing their struggles with long entity spans; notably, a baseline LLM method achieved only **21.87% F1** on Scholar-XL.
*   **Parameter Efficiency:** SeNER maintains high F1 scores with significantly fewer parameters than LLMs.
*   **Processing Length:** Can process texts **6 times longer** than previous span-based methods under identical hardware.
*   **Ablation Studies:** Show that a window size of 512 causes Out-of-Memory (OOM) errors, highlighting BiSPA's efficiency.
*   **Hyperparameter Analysis:** Indicates that window sizes that are too small restrict information aggregation or miss long entities, while sizes too large degrade focus or introduce false positives and high resource consumption.