---
title: 'DESAMO: A Device for Elder-Friendly Smart Homes Powered by Embedded LLM with
  Audio Modality'
arxiv_id: '2508.18918'
source_url: https://arxiv.org/abs/2508.18918
generated_at: '2026-01-27T23:48:34'
quality_score: 9
citation_count: 19
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# DESAMO: A Device for Elder-Friendly Smart Homes Powered by Embedded LLM with Audio Modality

*Youngwon Choi, Friendly Smart, Homes Powered, Korea Culture, Hwayeon Kim, Donghyuk Jung, Reference Format, Technology Institute, Audio Modality, Pipeline Overview*

---

## üìã Executive Summary

This paper addresses the critical reliability limitations of current smart home voice assistants when deployed for elderly populations, who frequently exhibit atypical speech patterns such as mumbling, slurring, or slowed articulation that confound traditional Automatic Speech Recognition (ASR) systems. Beyond simple command execution, there is a pressing need for these systems to ensure user safety by detecting critical non-speech events, such as falls or emergencies.

The authors identify that standard cascaded architectures‚Äîwhich convert audio to text before processing‚Äîare prone to error propagation and often require cloud processing, raising significant privacy concerns and rendering them unsuitable for vulnerable users requiring robust, always-on monitoring. To overcome these limitations, the authors propose **DESAMO**, an end-to-end device architecture that replaces traditional text-based NLP pipelines with an Audio Large Language Model (Audio LLM) capable of inferring user intent directly from raw acoustic features.

The system is built upon a **Qwen2.5-Omni 3B** model integrated with a Whisper large-v3-based encoder, optimized specifically for edge deployment on an NVIDIA Jetson Orin Nano. By utilizing a compact GGUF format with 4-bit language model quantization and 16-bit audio encoders, DESAMO operates entirely on-device, ensuring privacy preservation while enabling holistic auditory scene analysis without the need for text pre-processing or intermediate transcription steps.

In a pilot benchmark utilizing 300 voice samples from elderly speakers, DESAMO achieved a **98% intent accuracy** rate, significantly outperforming cascaded baselines. The system demonstrated remarkable hardware efficiency, maintaining a total memory footprint of only **3.45 GB** and operating with an approximate latency of **5.3 seconds**. Furthermore, the study revealed that by bypassing the ASR transcription step, the system eliminated error propagation associated with speech impediments and displayed improved resilience in noisy environments, alongside promising cross-lingual generalization capabilities.

The significance of this research lies in successfully bridging the gap between large generative AI models and practical, privacy-centric edge computing for assistive living. By validating that Audio LLMs can effectively handle speech impediments and expand functionality to life-saving emergency detection, DESAMO establishes a new blueprint for elderly-friendly smart home interfaces.

---

> ### ‚ö° Quick Facts
 >
 > * **Model Architecture:** Qwen2.5-Omni 3B + Whisper large-v3 Encoder
 > * **Deployment Hardware:** NVIDIA Jetson Orin Nano
 > * **Optimization Strategy:** GGUF format, 4-bit LLM, 16-bit Audio Encoder
 > * **Memory Footprint:** 3.45 GB
 > * **Operational Latency:** ~5.3 seconds
 > * **Intent Accuracy:** 98% (vs. 96.33%‚Äì97.33% baseline)
 > * **Key Capabilities:** Voice Intent Classification, Emergency/Fall Detection

---

## üîë Key Findings

*   **Elderly Speech Optimization:** DESAMO specifically addresses the challenge of unclear or impaired speech (e.g., mumbling, slurring) common among elderly users, which traditional systems often fail to interpret.
*   **Direct Audio Processing:** The device processes raw audio input directly using an Audio LLM, bypassing the error-prone traditional ASR (Automatic Speech Recognition) pipelines.
*   **Safety-Critical Detection:** Beyond voice commands, the system detects critical non-speech audio events, such as falls or medical emergencies, expanding the utility of smart home assistants.
*   **Privacy-First Design:** Utilizes a privacy-preserving on-device architecture, ensuring that sensitive audio data does not need to be transmitted to the cloud for processing.

## üõ†Ô∏è Methodology

The research methodology represents a paradigm shift from standard Natural Language Processing (NLP) pipelines:

*   **Audio LLM Integration:** Replaces standard NLP pipelines with Audio Large Language Models (Audio LLMs) to infer user intent directly from acoustic features rather than transcribed text.
*   **Edge Computing:** Utilizes on-device embedded computing resources to run inference locally, removing reliance on cloud connectivity.
*   **Holistic Ingestion:** Prioritizes raw audio data ingestion without text pre-processing, allowing for a more holistic understanding of the auditory scene, including intonation and environmental noise.

## ‚öôÔ∏è Technical Details

DESAMO replaces traditional cascaded pipelines with an end-to-end Audio LLM approach.

*   **Core Models:**
    *   Base: **Qwen2.5-Omni 3B**
    *   Encoder: **Whisper large-v3** based
*   **Hardware:** Optimized for **NVIDIA Jetson Orin Nano**.
*   **Optimization Specs:**
    *   Format: GGUF
    *   Audio Encoder: 16-bit
    *   Language Model: 4-bit quantization
*   **Functional Modes:**
    *   **Voice Intent Classification:** Executes user commands.
    *   **Emergency Detection:** Monitors ambient events (e.g., falls).
*   **Performance:** Operates locally with a latency of **~5.3 seconds**.

## üöÄ Contributions

The authors highlight three primary contributions to the field of assistive technology:

1.  **System Architecture:** Proposed the DESAMO system architecture as a complete, on-device smart home solution tailored specifically for elderly users.
2.  **Overcoming ASR Limitations:** Successfully bypassed text transcription limitations, solving issues related to speech impediments that degrade ASR performance.
3.  **Expanded Scope:** Redefined the role of voice assistants from simple command handlers to comprehensive auditory scene analysis tools capable of safety-critical event detection.

## üìä Results

The system was evaluated on a pilot benchmark consisting of 300 voice samples from elderly speakers:

*   **Accuracy:** Achieved **98%** intent accuracy, surpassing cascaded baselines which scored between 96.33% and 97.33%.
*   **Efficiency:** Utilized the smallest memory footprint among tested systems at **3.45 GB**.
*   **Error Reduction:** Eliminated error propagation caused by inaccurate ASR transcription.
*   **Robustness:** Demonstrated cross-lingual generalization capabilities and predicted increased resilience in noisy environments compared to text-dependent pipelines.

---

**Quality Score:** 9/10 | **References:** 19 citations