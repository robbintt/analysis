---
title: 'AQUATIC-Diff: Additive Quantization for Truly Tiny Compressed Diffusion Models'
arxiv_id: '2506.0596'
source_url: https://arxiv.org/abs/2506.05960
generated_at: '2026-02-03T19:31:27'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# AQUATIC-Diff: Additive Quantization for Truly Tiny Compressed Diffusion Models

*Adil Hasan; Thomas Peyrin*

---

> ### ðŸ“Š Quick Facts
>
> *   **Model Architecture:** Class-conditional LDM-4 (ImageNet 256x256)
> *   **Core Innovation:** Codebook-based Additive Vector Quantization (AVQ)
> *   **Compression Ratios:**
>     *   **W4A8:** 3.88 bits/weight
>     *   **W2A8:** 1.95 bits/weight (**16x Compression**)
> *   **Hardware Efficiency:** Uses software-optimized inference kernels (hardware-agnostic)
> *   **Top Performance (W2A8):** FID **6.07**, sFID **6.55**, IS **258.16**

---

## Executive Summary

Latent Diffusion Models (LDMs) offer state-of-the-art generative capabilities but are inherently computationally expensive and memory-intensive, creating barriers for deployment on resource-constrained hardware. Existing compression methods, particularly standard uniform scalar quantization, often fail to maintain performance at extremely low bit-widths or rely heavily on specialized hardware accelerators for integer arithmetic. This research addresses the urgent need for efficient compression strategies that can drastically reduce model size and bandwidth requirements while preserving generative quality.

The authors propose **"AQUATIC-Diff,"** a novel framework that applies codebook-based Additive Vector Quantization (AVQ) to diffusion models. Unlike traditional scalar quantization that processes weights individually, this approach groups related weights into vectors for quantization, enabling higher fidelity at lower bit-widths. A key technical component is the inclusion of a highly optimized inference kernel. This kernel decompresses the heavily compressed weights on-the-fly during inference, reducing computational load (FLOPs) and memory bandwidth without requiring dedicated support for low-bit integer arithmetic on the underlying hardware.

Evaluated on the class-conditional LDM-4/ImageNet benchmark, AQUATIC-Diff establishes a new Pareto frontier for extreme compression. At a W4A8 configuration (3.88 bits/weight), the model achieved an sFID of 5.78, significantly outperforming the full-precision baseline (7.70). In the more aggressive W2A8 setting (1.95 bits/weight), which offers a 16x compression ratio, the method achieved state-of-the-art results with an FID of 6.07, an sFID of 6.55, and an Inception Score (IS) of 258.16. This work successfully bridges the gap between compression techniques previously reserved for Large Language Models (LLMs) and the distinct architecture of diffusion models, offering a versatile solution for deploying high-quality generative AI on standard hardware.

---

## Key Findings

*   **New Pareto Frontier:** Achieved a new Pareto frontier for extremely low-bit weight quantization on the standard LDM-4/ImageNet class-conditional benchmark.
*   **Superior Performance at W4A8:** At 4-bit weights and 8-bit activations (W4A8), the model reported an sFID score **1.92 points lower** than the full-precision model.
*   **State-of-the-Art at W2A8:** At 2-bit weights and 8-bit activations (W2A8), the approach achieved state-of-the-art results for **FID**, **sFID**, and **ISC** metrics.
*   **Hardware-Agnostic Efficiency:** Demonstrated significant FLOPs savings via an efficient inference kernel that works on arbitrary hardware without requiring specialized integer arithmetic support.

---

## Methodology

The authors utilized **codebook-based additive vector quantization (AVQ)**, operating on groups of multiple related weights (vectors) as the basic unit of compression, rather than standard Uniform Scalar Quantization (USQ).

*   **Vector vs. Scalar:** By grouping related weights, the method preserves more information than treating weights independently.
*   **Software Optimization:** The approach includes an efficient inference kernel designed to reduce computational load through software optimization rather than hardware reliance.
*   **Decompression Strategy:** Weights are decompressed on the fly during inference to minimize memory bandwidth and RAM/VRAM usage.

---

## Technical Details

| Aspect | Specification |
| :--- | :--- |
| **Proposed Approach** | AQUATIC-Diff (Additive Quantization for LDMs) |
| **Quantization Type** | Codebook-based Additive Vector Quantization (AVQ) |
| **Configurations** | **W4A8** (4-bit weights, 8-bit activations) <br> **W2A8** (2-bit weights, 8-bit activations) |
| **Effective Bit-width** | 3.88 bits/weight (W4A8) <br> 1.95 bits/weight (W2A8) |
| **Compression Ratio** | Up to **16x** (W2A8 setting) |
| **Kernel Design** | Hardware-agnostic; decompresses weights on-the-fly to reduce FLOPs and memory bandwidth |
| **Evaluation Setup** | Class-conditional LDM-4 on ImageNet (256x256) <br> DDIM Sampler (20 steps) <br> CFG Scale: 7.5 |

---

## Results

### Benchmark Performance (Class-conditional LDM-4 / ImageNet)

**W4A8 Configuration (3.88 bits/weight)**
AQUATIC-Diff outperformed the full-precision baseline across key metrics:
*   **sFID:** 5.78 (vs. 7.70 Baseline)
*   **FID:** 9.77 (vs. 11.28 Baseline)
*   **Precision:** 93.65% (Best vs. competitors like Q-Diffusion and PTQD)

**W2A8 Configuration (1.95 bits/weight)**
In the aggressive 2-bit setting, the model significantly outperformed the previous SOTA (EfficientDM) and surpassed the full-precision model in FID and sFID:

| Metric | **AQUATIC-Diff (W2A8)** | **Previous SOTA (EfficientDM)** |
| :--- | :--- | :--- |
| **FID** | **6.07** | 7.60 |
| **sFID** | **6.55** | 8.12 |
| **Inception Score (IS)** | **258.16** | 175.03 |
| **Precision** | **87.73%** | 78.90% |

---

## Contributions

*   **Bridging the Gap:** Successfully bridged the gap between LLM and diffusion model compression by applying additive vector quantization techniques to diffusion models.
*   **Aggressive Quantization Validation:** Established that aggressive weight quantization (down to W2A8) can yield state-of-the-art generative performance, potentially outperforming full-precision models in certain metrics.
*   **Deployment Solution:** Addressed deployment bottlenecks by offering a quantization strategy that reduces computational costs through software optimization (inference kernels) rather than hardware-dependent integer operations.

---

*Quality Score: 9/10 | References: 40 citations*