---
title: 'BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not Stomach
  Annotation Disagreements (Yet)'
arxiv_id: '2510.12516'
source_url: https://arxiv.org/abs/2510.12516
generated_at: '2026-01-27T16:03:41'
quality_score: 7
citation_count: 31
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not Stomach Annotation Disagreements (Yet)

*Tomas Ruiz, Model Averaging, Carsten Schwemmer, Appetit Team, Ludwig Maximilian, Computational Social, Siyao Peng, Barbara Plank, Scaling Can, Not Stomach*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score**: 7/10
> *   **Evaluation Focus**: LeWiDi-2025 Shared Tasks (Annotation Disagreements)
> *   **Primary Models**: Qwen3-32B (Generator), DeepSeek-R1-8B (Judge)
> *   **BoN Configuration**: N=10 samples
> *   **Best Technique**: Model Averaging & Majority Voting
> *   **New Metric**: Prediction Diversity (Variance/Entropy)
> *   **References**: 31 citations

---

## Executive Summary

This paper addresses the critical challenge of applying test-time scaling techniquesâ€”specifically the use of additional inference computation to boost performanceâ€”to subjective domains characterized by human annotation disagreements. While methods like Best-of-N (BoN) have proven highly effective in objective, verifiable domains such as mathematics and coding, their efficacy remains unproven in tasks like the LeWiDi-2025 shared task, where ground truth is ambiguous and pluralistic. This investigation is essential for advancing perspectivist NLP, as current scaling laws, typically derived from rigid benchmarks, may not account for the inherent complexity of scenarios where multiple valid interpretations coexist.

The authors introduce a technical innovation titled "**Best-of-N Sampling with Step-Wise Scores (SWS)**," a pipeline designed to navigate subjectivity through a rigorous binary quality filter. Quantitative evaluations reveal a distinct performance gap: Best-of-N sampling failed to improve performance on LeWiDi-2025 tasks, falling below baseline levels. Conversely, traditional ensemble methods demonstrated robust utility. This research challenges the assumption that state-of-the-art generative scaling techniques like BoN are universally applicable, redirecting future efforts toward scaling laws that explicitly account for the ambiguity of human judgment.

---

## Key Findings

*   **Test-time Scaling Applicability**: Test-time scaling techniques can be applied to subjective domains (evaluating annotation disagreements), expanding beyond traditional objective domains.
*   **Traditional Methods Prevail**: Traditional benchmark methodsâ€”specifically **Model Averaging** and **Majority Voting**â€”consistently improved LLM performance on annotation disagreement tasks.
*   **BoN Failure**: **Best-of-N (BoN) sampling failed to improve performance** on the LeWiDi tasks, marking a distinct contrast to its success in objective domains.
*   **Objective-Subjective Gap**: There is a significant performance gap indicating that BoN methods do not effectively transfer from objective domains to subjective disagreement tasks.

---

## Methodology

The study applied test-time scaling techniques to the **LeWiDi-2025 shared tasks**, specifically targeting the challenge of evaluating annotation disagreements. The research focused on a comparative analysis of three distinct test-time scaling methods against baseline LLM outputs:

1.  **Model Averaging**
2.  **Majority Voting**
3.  **Best-of-N (BoN)**

The goal was to determine which techniques are viable for subjective, non-verifiable tasks where ground truth is not singular.

---

## Technical Details

The core approach proposed is **Best-of-N (BoN) Sampling with Step-Wise Scores (SWS)**.

### Pipeline Architecture
*   **Generation**: A large generative model (**Qwen3-32B**) creates **N=10** candidate samples.
*   **Evaluation**: A hierarchical judge model (**DeepSeek-R1-0528-Qwen3-8B**) scores the Chain-of-Thought (CoT) reasoning steps.
*   **Selection**: The candidate with the highest aggregate score is selected.

### Scoring Mechanism
*   **Labels**: 'Great', 'Okay', and 'Bad'.
*   **Mapping**: 
    *   'Great' $\rightarrow$ **1**
    *   'Okay' $\rightarrow$ **0**
    *   'Bad' $\rightarrow$ **0**
*   **Reduction**: Mean reduction was utilized (outperforming Product reduction).
*   *Design Note*: This "quality vs. failure" mapping enforces a high acceptance threshold, filtering out 'Okay' outputs that lack necessary distinction.

### Task & Prompting
*   **Tasks**: Perspectivist and Soft-label predictions.
*   **Prompt Engineering**: Prompts were specifically engineered to encourage reasoning about "**diverse perspectives and interpretations**."

---

## Results

*   **BoN Performance**: Best-of-N (BoN) sampling failed to improve performance on LeWiDi-2025 subjective tasks, contrasting sharply with its success in objective domains.
*   **Ensemble Success**: Model Averaging and Majority Voting consistently improved performance over the baseline.
*   **Reduction Strategy**: Mean reduction outperformed Product reduction in the experimental setup.
*   **Prompting Impact**: Instructions regarding "diverse perspectives" yielded a **statistically significant gain** ($p < 0.05$).
*   **New Metric**: Introduced **prediction diversity**, quantified as variance or entropy among candidates, to track problem difficulty across evaluated datasets.

---

## Contributions

*   **Scope Expansion**: Expanded the scope of test-time scaling from strict, objective domains (math/coding) to complex, subjective scenarios involving human annotation disagreements.
*   **Empirical Validation**: Provided evidence that simple ensemble approaches (Model Averaging and Majority Voting) are effective strategies for handling subjectivity in LLM outputs.
*   **Critical Limitation Identification**: Identified a critical limitation in state-of-the-art sampling techniques (Best-of-N), demonstrating that their efficacy is not universal and fails in contexts where correct answers are not verifiable.