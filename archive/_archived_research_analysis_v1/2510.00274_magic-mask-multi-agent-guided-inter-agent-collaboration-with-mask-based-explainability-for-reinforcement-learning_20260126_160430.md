---
title: 'MAGIC-MASK: Multi-Agent Guided Inter-Agent Collaboration with Mask-Based Explainability
  for Reinforcement Learning'
arxiv_id: '2510.00274'
source_url: https://arxiv.org/abs/2510.00274
generated_at: '2026-01-26T16:04:30'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# MAGIC-MASK: Multi-Agent Guided Inter-Agent Collaboration with Mask-Based Explainability for Reinforcement Learning

*Maisha Maliha, Dean Hougen*

---

> ### **Quick Facts**
> *   **Quality Score:** 7/10
> *   **Citations:** 40 references
> *   **Google Research Football Win Rate:** 94%
> *   **Highway Driving Success Rate:** 100%
> *   **Explainability Fidelity:** >95%

---

## Executive Summary

**Problem**
Explainability in Multi-Agent Reinforcement Learning (MARL) remains a fundamental barrier to deploying autonomous systems in safety-critical environments. As agents operate within complex, cooperative settings defined by Partially Observable Markov Decision Processes (POMDPs), understanding individual decision-making and joint behaviors is essential for verification and trust. Existing perturbation-based methods, such as StateMask, are largely confined to single-agent domains; they struggle with the computational costs, exploration coverage deficits, and lack of multi-agent adaptation required for MARL. Without a unified framework to interpret these systems, operators cannot effectively validate policies or diagnose failures in high-stakes applications where agent interdependence is high.

**Innovation**
MAGIC-MASK introduces a mathematically grounded framework that generalizes perturbation-based explainability to multi-agent domains as a black-box solution. The system integrates Proximal Policy Optimization (PPO) with an adaptive $\epsilon$-greedy exploration mechanism and is built upon a foundation of trajectory perturbation, reward fidelity analysis, and Kullback-Leibler divergence regularization. Technically, each agent employs a distinct mask network ($M^i_\phi$) to output a soft masking probability; if this probability exceeds a threshold ($\tau=0.5$), the agent follows its policy, otherwise it executes a random perturbation. A key component is the "lightweight inter-agent collaboration protocol," where agents share masked state information and peer experiences to guide saliency-based masking. This protocol optimizes the framework by minimizing a surrogate Mean Squared Error (MSE) loss function, utilizing KL-divergence regularization to generate localized, probabilistic explanations that capture joint agent dependencies.

**Results**
MAGIC-MASK delivers quantifiable superiority over existing methods, specifically excelling in learning efficiency through reduced time requirements. By leveraging inter-agent collaboration to reduce redundant exploration during the masking process, the framework significantly decreases the time required for critical state discovery without sacrificing performance. In complex benchmarks, this efficiency resulted in a **94%** win rate in the Google Research Football `academy_3_vs_1_with_keeper` scenario, significantly outperforming baselines that failed to maintain cooperative stability. Similarly, in the Multi-Agent Highway environment, MAGIC-MASK demonstrated a **100%** success rate in reaching destinations. Crucially, regarding explainability fidelity—defined by the accuracy of critical state identification—MAGIC-MASK achieved scores exceeding **95%**, confirming that the masking process effectively isolates decision-critical information.

**Impact**
By bridging the gap between single-agent and multi-agent explainability, MAGIC-MASK establishes a new precedent for interpreting complex autonomous systems. Its ability to function as a black-box framework requiring only observable trajectories makes it highly versatile for real-world applications where internal model access is restricted. The research provides a unified mathematical formalism that not only improves the interpretability of MARL but also enhances overall policy robustness through efficient inter-agent collaboration. This work advances the field toward safer, more transparent multi-agent deployments, offering critical tools for developers and operators working with cooperative AI in dynamic environments.

---

## Key Findings

*   **Superior Performance:** MAGIC-MASK consistently outperforms state-of-the-art baselines in terms of fidelity, learning efficiency, and policy robustness.
*   **Efficiency Gains:** The framework significantly reduces the time required for critical state discovery by leveraging inter-agent collaboration.
*   **Successful Generalization:** MAGIC-MASK successfully generalizes explainability capabilities from single-agent to complex multi-agent environments.
*   **Benchmark Validation:** Validation on benchmarks like Google Research Football and multi-agent highway driving confirms that the method offers interpretable and transferable explanations.

---

## Methodology

MAGIC-MASK is a **mathematically grounded framework** that extends perturbation-based explanation to Multi-Agent Reinforcement Learning (MARL).

*   **Core Integration:** Integrates Proximal Policy Optimization (PPO) and adaptive epsilon-greedy exploration.
*   **Collaboration:** Employs lightweight inter-agent collaboration where agents share masked state information and peer experiences to perform saliency-guided masking.
*   **Theoretical Basis:** The approach is built on trajectory perturbation, reward fidelity analysis, and Kullback-Leibler divergence regularization to generate localized, probabilistic explanations.

---

## Technical Details

### System Architecture
MAGIC-MASK addresses explainability in Multi-Agent RL (MARL) defined as a POMDP, operating as a **black-box framework** requiring only observable trajectories. It extends StateMask to identify jointly critical states.

*   **Agents:** The system comprises $N$ agents with global state $S_t$ and joint action $A_t$.
*   **Mask Networks:** Each agent utilizes a distinct mask network ($M^i_\phi$) that takes the local state as input to output a soft masking probability $m^i_t$.

### Perturbation Logic
The system uses a threshold $\tau=0.5$ to determine agent behavior:
*   If $m^i_t > \tau$, the agent follows its policy.
*   If $m^i_t \le \tau$, the agent switches to a random action (perturbation).

### Optimization & Training
*   **Loss Function:** Utilizes a surrogate Mean Squared Error (MSE) minimizing the deviation between expected mask output and the threshold.
*   **Backbone:** Implementation uses **Proximal Policy Optimization (PPO)**.
*   **Exploration:** Features an adaptive $\epsilon$-greedy exploration mechanism where $\epsilon$ decays exponentially.
*   **Protocol:** An inter-agent collaboration protocol where agents broadcast perturbed states to a global signal to reduce redundant exploration.

---

## Contributions

The proposal overcomes significant limitations in prior methods like StateMask, specifically computational costs, exploration coverage issues, and lack of multi-agent adaptation.

1.  **Unified Formalism:** The core innovation lies in generalizing explainability from single-agent to multi-agent systems via a unified mathematical formalism.
2.  **Enhanced Discovery:** By enabling agents to share masked insights, the framework improves the speed of critical state discovery and overall policy robustness.
3.  **Empirical Evidence:** The paper provides empirical evidence of the framework's superiority across benchmarks, specifically highlighting applicability in safety-critical environments.

---

## Results

MAGIC-MASK consistently outperforms state-of-the-art baselines by offering higher fidelity explanations and improving policy robustness. It reduces the time required for critical state discovery and successfully generalizes explainability capabilities to multi-agent environments.

**Validation Benchmarks:**
*   **Google Research Football:** Used to demonstrate interpretable and transferable explanations.
*   **Multi-Agent Highway Driving:** Confirmed effectiveness in uncovering interpretable decision-making processes.