---
title: 'HOT: Hadamard-based Optimized Training'
arxiv_id: '2503.21261'
source_url: https://arxiv.org/abs/2503.21261
generated_at: '2026-02-03T18:24:15'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# HOT: Hadamard-based Optimized Training

*Seonggon Kim; Juncheol Shin; Seung-taek Woo; Eunhyeok Park*

> ### **Quick Facts**
>
> *   **Peak Speedup:** 3.3× (`fc2` layer), 2.6× (Overall ViT-B)
> *   **Memory Reduction:** Up to 75% (65% for Transformers)
> *   **Hardware:** NVIDIA RTX 3090
> *   **Tested Models:** ResNet-50, ViT-B, EfficientFormer-L7
> *   **Accuracy:** Near FP32 equivalence on ImageNet
> *   **Primary Bottleneck Addressed:** Matrix Multiplication (Backpropagation)

***

## Executive Summary

Training modern deep learning architectures, such as Vision Transformers (ViT) and deep ResNets, imposes significant computational and memory burdens, primarily driven by the backpropagation phase. The authors identify matrix multiplication (GEMM) as the dominant cost factor during gradient computation, creating a bottleneck that limits the feasibility of training large models on resource-constrained hardware. Reducing these overheads without compromising model convergence or accuracy is a critical challenge for improving the accessibility and efficiency of deep learning.

The paper introduces **HOT (Hadamard-based Optimized Training)**, a framework that optimizes backpropagation through a selective Hadamard Transform strategy. The core technical innovation lies in a dual-path optimization approach: the activation gradient path employs low-precision Hadamard Quantization (HQ) to maximize computational speed, while the weight gradient path utilizes high-precision Hadamard Linear Algebra (HLA) to maintain numerical stability. This is further enhanced by activation buffer compression, layer-wise quantizer selection, and custom CUDA kernels designed to leverage hardware-level optimizations for specific backward paths.

Extensive experiments conducted on an RTX 3090 GPU demonstrate HOT’s efficacy across standard benchmarks. The framework achieved an overall training speedup of 2.6× for ViT-B, with peak layer speedups reaching 3.3×, while ResNet-50 saw speedups ranging from 1.8× to 2.3×. Memory usage was reduced by up to 75% generally (65% specifically for Transformer models). Crucially, HOT maintained accuracy nearly equivalent to FP32 baselines on ImageNet-1k and ImageNet-100 pre-training tasks, significantly outperforming existing methods like LUQ and LBP, which suffered notable accuracy degradation or failure.

This research establishes that complex multi-objective optimization—simultaneously addressing speed, memory, and accuracy—is achievable through hardware-aware algorithmic design. By drastically lowering the barrier to training large-scale models, HOT offers a practical solution for researchers and practitioners working with limited GPU resources.

***

## Key Findings

*   **Significant Memory Reduction:** Achieves up to a **75% reduction** in memory usage during training.
*   **Substantial Speedup:** Delivers a **2.6 times acceleration** in training performance on real GPU hardware.
*   **Accuracy Preservation:** Maintains model accuracy with negligible loss compared to FP32 precision.
*   **Cost Identification:** Identifies matrix multiplication as the primary contributor to training costs, specifically within the backpropagation phase.

***

## Methodology

The approach targets the optimization of matrix multiplication operations within the backpropagation algorithm. It employs a selective Hadamard Transform strategy, applying Hadamard quantization and Hadamard low-rank approximation based on the suitability of specific backward paths.

Additionally, it integrates supplementary enhancements such as:
*   Activation buffer compression
*   Layer-wise quantizer selection

***

## Technical Details

The paper proposes a training optimization methodology centered on Hadamard-based transformations to reduce computational and memory overheads associated with the backpropagation phase.

### Dual-Path Optimization Strategy
The framework utilizes distinct quantization strategies for different gradient paths:

*   **Activation Gradient Path:** Uses low-precision **HQ (Hadamard Quantization)** to maximize acceleration.
*   **Weight Gradient Path:** Uses high-precision **HLA quantization (Hadamard Linear Algebra)** to ensure numerical stability.

### Implementation & Optimization
*   **Target:** Matrix multiplication (GEMM) bottlenecks in backpropagation.
*   **Memory Efficiency:** Achieved by compressing activations.
*   **Hardware Acceleration:** Relies on custom CUDA kernels for hardware-level optimizations.

***

## Results

Experiments were conducted on an **RTX 3090 GPU** using ResNet-50, ViT-B, and EfficientFormer-L7.

### Performance Speedup
*   **ViT-B:** Overall speedup of **2.6×**; peak speedup of **3.3×** in the `fc2` layer.
*   **ResNet-50:** Speedups ranging from **1.8× to 2.3×**.
*   **EfficientFormer-L7:** Speedups ranging from **1.6× to 2.2×**.

### Memory Efficiency
*   **General Reduction:** Up to **75%**.
*   **Transformer Models:** Reduced by **65%**.

### Accuracy Analysis
*   HOT achieved performance nearly equivalent to Full Precision (FP) on ImageNet-1k and ImageNet-100 pre-training tasks.
*   **Comparison:** Unlike LUQ (which degraded) and LBP (which failed), HOT maintained stability.
*   **Limitation:** A trend of increased performance degradation was noted as model size and depth increase, though results remain superior to existing methods.

***

## Contributions

*   Introduction of the **'Hadamard-based Optimized Training' (HOT)** framework.
*   Development of a strategy leveraging selective Hadamard-based quantization and low-rank approximation.
*   Proposal of enhancement techniques including activation buffer compression and layer-wise quantizer selection.
*   Empirical validation demonstrating that complex multi-objective optimization is achievable.

***

**Quality Score:** 9/10  
**References:** 40 citations