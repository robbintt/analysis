# Explaining How Quantization Disparately Skews a Model

*Abhimanyu Bellam; Jung-Eun Kim*

---

### 45 Quick Facts
| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 11 Citations |
| **Core Focus** | Post-Training Quantization (PTQ) & Algorithmic Fairness |
| **Dataset** | UTKFace |
| **Key Metric** | Fairness Violation Observed (FVO) |

---

## Executive Summary

This research addresses the critical intersection of **model compression** and **algorithmic fairness**, specifically investigating how Post-Training Quantization (PTQ)—a widely used technique for deploying neural networks on resource-constrained devices—exacerbates disparate impacts on protected groups. While PTQ is effective for reducing model size, the authors highlight a significant vulnerability where the compression process disproportionately degrades predictive accuracy for minority populations compared to majority groups. 

This issue is of paramount importance because efficient, compressed models are increasingly deployed in high-stakes social applications; if standard optimization techniques like PTQ inherently amplify societal biases, they compromise the ethical viability and safety of edge AI systems.

The paper’s key innovation is the introduction of a **"cascading degradation mechanism"** that provides a causal explanation for quantization bias, moving beyond observational disparity to root-cause analysis. The authors analytically decompose forward and backward passes to demonstrate how PTQ induces weight sparsity that suppresses logit variance and skews softmax distributions toward decision boundaries. 

Crucially, the research bridges the gap between compression theory and deep learning optimization by linking quantization errors to the geometry of the loss landscape. Technically, the study shows that PTQ erodes the optimization landscape by altering the eigenvalues of the Hessian matrix, which forces minority group samples into regions of sharper curvature and instability characterized by elevated group gradient norms.

Empirical experiments conducted on the UTKFace dataset reveal a distinct **inverse relationship between bit precision and model disparity**: while standard floating-point models show low pre-quantization disparity, aggressive compression to int2 results in "extreme" fairness violations, whereas int8 and int16 exhibit moderate gaps. The data shows that PTQ leads to significantly lower logit variance and increased loss, with post-quantization analysis revealing that minority groups encounter larger gradient norms and higher Hessian eigenvalues than majority groups. 

Although quantization generally retains accuracy relative to size better than pruning, PTQ is shown to be uniquely detrimental to fairness, widening the performance gap between groups more significantly than other compression methods. This work significantly influences the field by establishing that model compression is not a value-neutral engineering task and requires specific fairness diagnostics alongside standard accuracy metrics.

---

## Key Findings

*   **Disparate Impact Exacerbation:** Post Training Quantization (PTQ) disproportionately degrades accuracy for minority groups compared to majority groups, worsening existing biases.
*   **Cascading Degradation:** Quantization induces a mechanism where changes to weights and activations cause compounding negative impacts during both forward and backward passes.
*   **Optimization Instability:** The process leads to output and optimization instability, characterized by:
    *   Lower logit variance
    *   Increased loss
    *   Alterations to group gradient norms
*   **Landscape Erosion:** Quantization erodes the optimization landscape by changing the eigenvalues of the Hessian matrix, compromising the network's ability to maintain equitable performance.

---

## Methodology

The researchers employed a multi-stage approach to isolate and explain the sources of bias:

1.  **Analytical Decomposition:** conducted a decomposition of forward and backward passes to trace the specific factors causing disparate impacts.
2.  **Empirical Observation:** Measured dynamics such as logit variance and loss to observe the immediate effects of quantization.
3.  **Optimization Diagnostics:** Utilized advanced diagnostics including:
    *   **Group Gradient Norms:** To measure stability.
    *   **Hessian Eigenvalues:** To analyze the geometry of the loss landscape.
4.  **Mitigation Testing:** Proposed and tested a solution integrating:
    *   Mixed Precision Quantization Aware Training (QAT)
    *   Dataset sampling
    *   Weighted loss functions

---

## Technical Details

*   **Framework:** The issue is framed as a classification task using cross-entropy loss.
*   **Quantization Mechanics:**
    *   PTQ modifies weights via a quantization function $T$.
    *   Weights are scaled back using scaling factors $S$.
    *   This process induces sparsity within the model.
*   **Cascading Degradation Mechanism (Theory):**
    *   **Forward Pass:** Reduces logit variance and skews softmax distributions towards the decision boundary.
    *   **Optimization Landscape:** Erodes Hessian eigenvalues and increases group gradient norms, specifically affecting minority groups.
*   **Fairness Metric:** The paper proposes **Fairness Violation Observed (FVO)**, which quantifies bias as the maximum accuracy discrepancy between any two groups.

---

## Results

*   **Precision vs. Disparity:** Experiments on the UTKFace dataset revealed an inverse relationship between bit precision and model disparity. 
    *   **int2:** Exhibited "extreme" disparity.
    *   **int8/int16:** Exhibited moderate disparity.
    *   **Pre-quantization:** Showed low disparity.
*   **Logit Variance:** Quantization resulted in significantly lower logit variance and increased overall loss.
*   **Gradient Dynamics:** Post-quantization, minority groups displayed:
    *   Larger gradient norms
    *   Larger Hessian eigenvalues
    *   *Indication:* Minority data points shift to sharper, less stable positions in the optimization space.
*   **Comparison to Pruning:** While quantization generally offers better accuracy retention relative to model size compared to pruning, PTQ is uniquely harmful to fairness, significantly widening performance gaps between groups.

---

## Contributions

*   **Causal Explanation:** Provides a technical causal explanation of quantization bias, detailing the mechanics of *how* and *why* unfair outcomes occur rather than simply observing the accuracy gap.
*   **Theoretical Bridge:** Successfully bridges the gap between model compression and optimization theory by explicitly linking quantization errors to gradient norms and Hessian eigenvalues.
*   **Actionable Mitigation:** Offers a multi-pronged mitigation strategy (Mixed Precision QAT + Sampling + Weighted Loss) to correct biases, enabling the fair deployment of quantized neural networks.