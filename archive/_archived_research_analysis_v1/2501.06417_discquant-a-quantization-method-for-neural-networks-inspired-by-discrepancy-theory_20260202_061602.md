# DiscQuant: A Quantization Method for Neural Networks Inspired by Discrepancy Theory

*Jerry Chee; Arturs Backurs; Rainie Heck; Li Zhang; Janardhan Kulkarni; Thomas Rothvoss; Sivakanth Gopi*

---

### ðŸ“Š Quick Facts

| **Metric** | **Detail** |
| :--- | :--- |
| **Method** | DiscQuant |
| **Core Inspiration** | Discrepancy Theory |
| **Key Benchmark** | GSM8k (Phi-3-mini-3.8B @ 3.25-bit) |
| **Top Result** | **64%** Accuracy (vs GPTQ: 54%, RTN: 31%) |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |

---

## Executive Summary

Post-Training Quantization (PTQ) is critical for deploying Large Language Models (LLMs) on resource-constrained hardware, yet maintaining accuracy at extremely low bit-widths (e.g., 3-4 bits) remains a significant challenge. Existing methods, such as Round-to-Nearest (RTN) and GPTQ, typically rely on heuristics or local optimization criteria to round continuous weights to discrete grids. These approaches often lack rigorous theoretical guarantees regarding the preservation of model performance, leading to substantial accuracy degradation in aggressive compression scenarios.

This paper addresses the need for a mathematically grounded quantization method that can rigorously bound the error introduced by rounding weights. The authors introduce **DiscQuant**, a novel approach inspired by *discrepancy theory*, a field traditionally used in numerical analysis and combinatorics to minimize errors in discrete approximations.

Technically, DiscQuant formulates the rounding step as a global optimization problem aimed at minimizing the first-order Taylor expansion of the loss function. The algorithm is data-dependent, leveraging the geometry of the model's gradient flow relative to the data distribution. A key theoretical contribution is proving that if the gradient space is approximately low-rank, a polynomial number of samples allows for the rounding of all but $O(m)$ weights with an expected approximation error bounded by epsilon.

DiscQuant realizes this theory through a practical algorithm that enforces rounding to adjacent grid points while adhering to these discrepancy bounds. Empirical evaluations demonstrate that DiscQuant significantly outperforms current state-of-the-art methods on complex reasoning tasks.

This research signifies a paradigm shift in neural network quantization by introducing a rigorous mathematical foundation to a field previously dominated by heuristics. By successfully bridging the gap between theoretical discrepancy bounds and practical performance, DiscQuant sets a new state-of-the-art for low-bit quantization.

---

## Key Findings

*   **Theoretical Guarantees:** The authors prove that given a polynomial number of samples, it is possible to round all but $O(m)$ model weights with an expected approximation error $\le \epsilon$, assuming the gradient space is approximately low rank.
*   **Superior Performance:** The proposed DiscQuant method consistently outperforms standard baselines. Notably, it achieved **64% accuracy** on Phi3-mini-3.8B at 3.25 bits on the GSM8k benchmark, significantly higher than GPTQ (54%) and RTN (31%).
*   **Structural Validation:** The study empirically validates that the gradient space of LLMs is approximately low rank, supporting the theoretical underpinnings of the method.
*   **Global Optimization:** Unlike local heuristics, DiscQuant utilizes a global rounding strategy derived from algorithmic proofs, leading to better preservation of model reasoning capabilities.

---

## Methodology

The study decomposes neural network quantization to focus specifically on optimizing the rounding of weights to a quantization grid. The core methodological shifts include:

*   **Discrepancy Theory Framework:** Instead of standard heuristics, the study frames the rounding problem using discrepancy theory to provide a rigorous mathematical foundation.
*   **Data-Dependence:** The process is data-dependent, utilizing both the data distribution and the model's gradient geometry to inform rounding decisions.
*   **Algorithmic Derivation:** The DiscQuant algorithm is derived directly from algorithmic proofs establishing bounds on approximation errors, ensuring the method is theoretically sound rather than purely empirical.

---

## Technical Details

The DiscQuant approach is built upon several specific technical constraints and optimizations:

*   **Optimization Problem:** DiscQuant frames Post-Training Quantization (PTQ) as an optimization problem focusing on the rounding step to minimize loss increase.
*   **First-Order Taylor Expansion:** The method utilizes a first-order Taylor expansion based on the premise that per-sample gradients are significant predictors of loss changes.
*   **Global Rounding Constraints:**
    *   Enforces rounding to adjacent grid points.
    *   Performs global rounding on the entire model rather than layer-by-layer.
*   **Mathematical Inspiration:**
    *   Inspired by discrepancy theory.
    *   Uses linear interpolation and linear regularizers.
    *   Assumes the gradient space is approximately low rank.
*   **Theoretical Bounds:** Proofs indicate that with low-rank gradients, all but $O(m)$ weights can be rounded with a polynomial number of samples while maintaining low error.

---

## Results

The empirical evaluation of DiscQuant demonstrates strong performance across theoretical validation and benchmark testing:

*   **GSM8k Benchmarking:**
    *   At **3.25-bit** compression on **Phi-3-mini-3.8B**:
        *   **DiscQuant:** 64% Accuracy
        *   **GPTQ:** 54% Accuracy
        *   **RTN:** 31% Accuracy
*   **Gradient Statistics:**
    *   Validated the first-order assumption by showing a large gap between average gradient norms and expected norms.
    *   Example (Llama3.1-8B): 1.6328 vs 107.
*   **Eigenvalue Analysis:**
    *   Conducted on Phi-3-mini-4k and Llama-3.1-8B.
    *   Demonstrated rapid decay in eigenvalues, confirming the approximately low-rank gradient structure.
*   **Error Approximation:**
    *   On WikiText-2, the first-order approximation showed strong correlation with the actual error function, validating the model's approach to loss estimation.

---

## Contributions

*   **Theoretical Foundation:** Introduces the application of discrepancy theory to neural network quantization, providing a rigorous mathematical basis for weight rounding.
*   **Novel Algorithm:** Proposes DiscQuant, a practical rounding algorithm derived directly from discrepancy theory proofs.
*   **State-of-the-Art Advancement:** Advances the field of quantization by demonstrating the suboptimality of previous methods (GPTQ and RTN) and providing a higher-accuracy alternative for compressing large LLMs without sacrificing reasoning ability.

---

## Paper Metadata

*   **Quality Score:** 9/10
*   **References:** 40 citations