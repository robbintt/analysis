# SViQA: A Unified Speech-Vision Multimodal Model for Textless Visual Question Answering

*By Bingxin Li*

---

> ### üìä Quick Facts
> | Metric | Value |
> | :--- | :--- |
> | **Benchmark** | SBVQA |
> | **Accuracy (Pure Speech)** | 75.62% |
> | **Accuracy (Mixed Input)** | 78.85% |
> | **Trainable Params** | 2.84% (109.8M / 3.86B) |
> | **Latency** | 3.023 ¬± 0.225s (RTX 3090) |
> | **ASR Required** | ‚ùå No (Textless) |

---

## üìù Executive Summary

This paper addresses a significant gap in multimodal learning by focusing on the underexplored speech-vision domain. Current research predominantly emphasizes text-vision integration, while existing audio-visual approaches rely on cascaded pipelines that convert speech to text before processing, introducing latency and discarding valuable paralinguistic information such as prosody and emotion.

The authors propose **SViQA**, a unified end-to-end tri-modal model built on the TinyLLaVA framework with a Phi-2 LLM. SViQA's core innovation is its direct audio processing architecture that bypasses ASR entirely by employing a frozen Whisper encoder. It introduces a two-stage temporal-speech adapter to aggregate audio frames and align them with visual features, achieving remarkable parameter efficiency by training only 2.84% of parameters.

SViQA achieves state-of-the-art performance on the SBVQA benchmark with **75.62% accuracy** using pure speech input, which improved to **78.85%** with mixed speech-text input. The model demonstrated competitive capabilities across additional datasets, maintained high speech recognition accuracy (93.7%), and operated with low latency. This work significantly advances speech-vision multimodal learning by demonstrating that high-performance visual understanding can be achieved through direct speech processing rather than text conversion.

---

## üîë Key Findings

*   **State-of-the-Art Performance:** The proposed SViQA model achieved **75.62% accuracy** on the SBVQA benchmark using pure speech input, setting a new benchmark.
*   **Mixed Input Strategy:** Utilizing a mixed input strategy (speech-text) significantly boosts performance, reaching **78.85% accuracy**. This represents a **3.23% absolute improvement** over pure speech input.
*   **Competitive Generalization:** The model demonstrates competitive capabilities in multimodal generalization beyond the primary benchmark.
*   **Effective Cross-modal Alignment:** The performance gain from mixed input highlights the model's effective cross-modal attention alignment and enhanced robustness.

---

## ‚öôÔ∏è Methodology

The proposed framework relies on three core pillars to enable textless visual question answering:

*   **Architecture Foundation:**
    The framework is built upon the **LLaVA architecture**, adapted specifically to handle auditory and visual inputs simultaneously.
*   **Direct Audio Processing:**
    The model employs an end-to-end speech feature extraction mechanism, allowing it to process spoken questions **without** requiring intermediate text transcription or ASR systems.
*   **Cross-modal Fusion:**
    It utilizes cross-modal alignment optimization to effectively bridge the heterogeneity gap, enabling the direct fusion of speech signals with visual content for understanding.

---

## üõ† Technical Details

**System Architecture**
*   **Type:** Unified End-to-End Tri-Modal System (Speech-Vision-Text).
*   **Base Framework:** TinyLLaVA.
*   **LLM Backbone:** Phi-2 (2.7B parameters).

**Encoders & Processing**
*   **Vision Encoder:** Frozen SigLIP ViT-S/16.
*   **Speech Encoder:** Frozen Whisper.
*   **Adaptation:**
    *   **Visual Projector:** MLP (Multi-Layer Perceptron).
    *   **Speech Adapter:** Two-stage temporal-speech adapter that aggregates 5 frames into 100ms chunks, followed by a 2-layer MLP projection.

**Fusion Strategy**
*   Concatenates Vision, Speech, and Text tokens.
*   Interleaves speech and text tokens at 100ms intervals for seamless integration.

**Training Parameters**
*   **Parameter Efficiency:** Only **2.84%** of parameters are trainable (109.8M out of 3.86B).
*   **Fine-Tuning:** Utilizes **LoRA** (r=128, alpha=256).
*   **Dataset:** Trained on 443K samples.
*   **Loss & Schedule:** Cross-entropy loss with cosine scheduling.

---

## üìà Results

*   **Benchmark Performance:** Achieved SOTA on SBVQA.
    *   Pure Speech: 75.62%
    *   Mixed Input (Speech + Text): 78.85% (+3.23% gain)
*   **Speech Recognition:** Maintained 93.7% accuracy.
*   **Inference Speed:** Latency measured at 3.023 ¬± 0.225 seconds on an NVIDIA RTX 3090.
*   **Qualitative Analysis:** Showed superior performance on Yes/No and Open-ended questions, effectively leveraging acoustic cues like prosody.

---

## ‚ú® Contributions

1.  **Unified Speech-Vision Framework:**
    Introduction of SViQA, a unified model capable of **'textless' Visual Question Answering**, directly addressing the underexplored domain of speech-vision integration compared to text-vision methods.
2.  **Bridging Modality Gaps:**
    The work addresses the inherent heterogeneity between speech and vision modalities by replacing text-conversion pipelines with direct speech feature extraction.
3.  **Optimization Strategy:**
    Provision of a cross-modal alignment optimization technique that enables effective fusion of auditory and visual data, advancing the potential for speech-based human-computer interaction.

---

**Quality Score:** 9/10  
**References:** 8 citations