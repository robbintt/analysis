# LOTION: Smoothing the Optimization Landscape for Quantized Training

*Mujin Kwun; Depen Morwani; Chloe Huangyuan Su; Stephanie Gil; Nikhil Anand; Sham Kakade*

***

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Framework Name** | LOTION (Low-precision Optimization via sTochastic-noIse smOothiNg) |
| **Core Innovation** | Loss-level smoothing via randomized rounding noise expectation |
| **Quantization Precision** | INT4 |
| **Model Scales Validated** | 150M and 300M parameters |
| **Dataset** | C4 |
| **Performance Gain** | Lower validation loss (3.9‚Äì4.0) vs standard QAT (4.5‚Äì4.75) |
| **Quality Score** | 8/10 |

***

> ## üí° Executive Summary
>
> Training neural networks with low-bit precision (quantization) is essential for deploying large-scale models on resource-constrained hardware, yet it presents a fundamental optimization challenge. Quantization transforms the model‚Äôs weights into discrete values, resulting in a piece-wise constant loss surface that is non-differentiable almost everywhere. This creates a "zero gradient" problem where standard gradient-based optimizers fail to guide the training process. Currently, practitioners rely on heuristics like the Straight-Through Estimator (STE) to approximate gradients, but these methods lack mathematical convergence guarantees and often require extensive hand-tuning, leading to unstable training trajectories and suboptimal final model performance.
>
> The authors introduce **LOTION**, a principled framework that solves the non-differentiability problem through loss-level smoothing rather than gradient modification. Inspired by Nesterov smoothing, LOTION replaces the raw quantized loss function with its expectation under an injection of unbiased randomized rounding noise. This mathematical transformation converts the discrete, jagged optimization landscape into a continuous function, allowing standard optimizers to compute exact gradients without resorting to approximations like STE. The framework employs Fine-Grained Shared-Scale Integer Quantization, utilizing a randomized rounding scheme that satisfies unbiasedness, continuity, and exactness axioms to ensure that global minima are preserved within the smoothed approximation.
>
> **Validation:** LOTION was tested against standard Quantization Aware Training (QAT) on Large Language Models (150M to 300M parameters) using INT4 precision on the C4 dataset. In the 150M parameter experiments, LOTION achieved a Validation Cross-Entropy Loss of approximately **3.9‚Äì4.0**, significantly outperforming the standard QAT baseline (approx. 4.5‚Äì4.75). The method demonstrated superior optimization stability and smoother training trajectories without the need for parameter-specific hyperparameter tuning. These results confirm that the theoretical advantages of loss smoothing translate effectively to scalable, practical performance gains.

***

## üîë Key Findings

*   **Guaranteed Convergence:** The proposed framework ensures that standard optimizers are guaranteed to converge to a local minimum of the loss surface, solving the instabilities often found in quantized training.
*   **Minima Preservation:** By utilizing noise derived from stochastic rounding, the method ensures that the global minima of the original quantized loss function are preserved within the smoothed approximation.
*   **Superior Performance:** Empirical results demonstrate that the method outperforms standard Quantization Aware Training (QAT) on both synthetic testbeds and large-scale language models (150M and 300M parameters).
*   **Continuous Approximation:** The approach successfully approximates a piece-wise constant quantized loss surface with a continuous one, enabling effective gradient-based optimization.

## üß™ Methodology

The authors introduce **LOTION**, a framework inspired by Nesterov smoothing. Instead of dealing directly with a raw quantized loss function, LOTION replaces this loss with its expectation under unbiased randomized-rounding noise.

This transformation converts the optimization landscape into a continuous function, allowing standard gradient-based optimizers to function effectively without relying on heuristics like Straight Through Estimators (STE). The core methodology involves training on the expectation of the quantized loss under a randomized rounding noise distribution.

## üèÜ Contributions

*   **Principled Mathematical Framework:** Provides a framework for optimizing quantized objectives that offers convergence guarantees, contrasting with prevailing methods like STE that lack theoretical backing.
*   **Solving Zero Gradients:** Solves the fundamental challenge of the 'zero gradient' problem in quantization by smoothing the loss surface via stochastic noise expectation.
*   **Demonstrated Scalability:** Validates that theoretical benefits translate to practical performance gains in significant language model architectures (up to 300M parameters).

## ‚öôÔ∏è Technical Details

LOTION performs **loss-level smoothing** instead of gradient modification to transform the non-differentiable quantized optimization problem into a continuous one.

### Smoothing Mechanism
The method trains on the expectation of the quantized loss under a randomized rounding noise distribution, defined mathematically as:

$$L_{\text{smooth}, D}(w) = \mathbb{E}_{q \sim D_w}[L(q)]$$

### Rounding Axioms
The Randomized Rounding scheme satisfies three key axioms:
1.  **Unbiasedness**
2.  **Continuity**
3.  **Exactness**

These axioms ensure that global minima are preserved during the smoothing process.

### Quantization Parameters
*   **Type:** Fine-Grained Shared-Scale Integer Quantization (symmetric signed).
*   **Scaling Method:** 'Absolute maximum' method.
*   **Scale Calculation:** The scale $s_B$ for a block is calculated as:
    $$s_B = \frac{\max_{i \in B} |w_i|}{2^{n-1} - 1}$$
*   **Operators:**
    *   Quantization: $z_i = \lfloor \frac{w_i}{s_B} \rceil$
    *   Casting: $[\text{cast}(w)]_i = s_B z_i$
*   **Precision:** Scales are kept in FP16 and update dynamically.
*   **Optimization:** No explicit clipping is required.

## üìà Results

The method was evaluated on **Large Language Models (150M and 300M parameters)** and synthetic testbeds using **INT4 precision** on the **C4 dataset**.

*   **Validation Cross-Entropy Loss (150M Model):**
    *   **LOTION:** approx. 3.9 ‚Äì 4.0
    *   **Standard QAT:** approx. 4.5 ‚Äì 4.75
*   **Optimization Trajectory:** LOTION demonstrated smoother optimization trajectories and superior stability compared to QAT.
*   **Generalization:** It outperformed baselines on both synthetic and large-scale models.
*   **Efficiency:** Offers theoretical convergence guarantees and parameter-free efficiency without additional tuning.

---

**Paper Analysis Quality Score:** 8/10  
**References:** 37 citations