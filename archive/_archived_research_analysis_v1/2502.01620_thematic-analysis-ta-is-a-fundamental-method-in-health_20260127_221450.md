---
title: Thematic Analysis (TA) is a fundamental method in health-
arxiv_id: '2502.01620'
source_url: https://arxiv.org/abs/2502.01620
generated_at: '2026-01-27T22:14:50'
quality_score: 9
citation_count: 15
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Thematic Analysis (TA) is a fundamental method in health

*Carlos M. Mery, Congenital Heart, Ying Ding, Natural Sciences, Lily Boddy, Analysis Pipeline, Enhanced Thematic, Terence Lim, Muhammad Zain, Jiawei Xu*

***

> ### ðŸ“Š Quick Facts
>
> *   **Model:** GPT-4o mini (State-of-the-art, cost-effective)
> *   **Dataset:** 9 transcripts (Median 11,457 words), 42 parents
> *   **Primary Metric (Jaccard Similarity):** 0.410 (vs Baseline ~0.125)
> *   **Hit Rate:** 1.000 (Captured all human themes)
> *   **Efficiency:** 97% reduction in task duration
> *   **Best Prompting Strategy:** 1-Shot + Reflexion

***

## Executive Summary

Qualitative research in healthcare relies heavily on inductive Thematic Analysis (TA) to extract patient insights from interview transcripts, yet this process remains manually intensive and unscalable. Analyzing complex, unstructured dataâ€”such as detailed focus groups regarding pediatric conditions like Anomalous Aortic Origin of a Coronary Artery (AAOCA)â€”requires significant time and expert cognitive effort. As the volume of qualitative data grows, the field faces a critical bottleneck: existing automated methods lack the accuracy required for high-stakes medical environments, while manual analysis cannot keep pace with the demand for rapid information synthesis.

This study introduces the **LLM-Enhanced Thematic Analysis (LLM-TA) pipeline**, a novel architecture that integrates the cost-effective GPT-4o mini model via LangChain to automate the first five stages of inductive TA. To overcome context window limitations and preserve nuance, the pipeline employs a fine-grained "divide-and-conquer" strategy. Transcripts are segmented into chunks of up to 1,500 words, generating approximately one code per 225 words, which are subsequently synthesized into higher-level themes. The technical implementation evaluates three distinct prompting strategiesâ€”0-Shot, 1-Shot, and 1-Shot augmented with a Reflexion (self-reflection) loopâ€”to optimize the balance between efficiency and analytical depth.

The LLM-TA pipeline demonstrated superior performance compared to existing baselines (Mathis et al.) across both quantitative and qualitative metrics. Utilizing sentence-t5-xxl embeddings, the 0-Shot LLM-TA achieved a Jaccard Similarity of **0.410** against human ground truth, significantly outperforming the baseline range of 0.111â€“0.139 and approaching the human-human inter-rater similarity of 0.583. Furthermore, the pipeline achieved a perfect Hit Rate of **1.000**, successfully capturing all themes identified by human analysts. In expert evaluations, the 1-Shot + Reflexion variant received a specificity rating of 5/5 compared to 1-2 for baselines. Most notably, the pipeline reduced task duration by **97%**, decreasing the required time from 30 person-hours to under 10 minutes (or 90 minutes with Reflexion).

These findings validate the viability of using affordable, state-of-the-art LLMs for rigorous qualitative analysis in high-stakes healthcare settings. By proving that high-throughput automation can achieve near-human thematic fidelity, this research offers a scalable pathway to reduce analyst workload and accelerate the derivation of clinical insights. The study establishes a practical framework for human-AI collaboration in qualitative research and contributes to the field by releasing the pipeline code and implementation details, enabling other researchers to replicate and adapt this efficient methodology for complex domain-specific analysis.

***

## Key Findings

*   **Performance:** The proposed LLM-Enhanced Thematic Analysis (LLM-TA) pipeline significantly outperforms existing LLM-assisted methods in analyzing healthcare interview transcripts.
*   **Fidelity:** While not yet fully achieving human-level quality in inductive TA, it demonstrates high potential to improve scalability, efficiency, and accuracy.
*   **Workflow:** Collaborative use of the pipeline with domain experts effectively reduces analyst workload.
*   **Cost-Effectiveness:** The study confirms the viability of using an affordable, state-of-the-art model (GPT-4o mini) for high-stakes qualitative analysis.

***

## Methodology

The study leverages the inductive Thematic Analysis (TA) framework applied to a specific healthcare context:

*   **Data Source:** Nine detailed interview transcripts from parents of children with Anomalous Aortic Origin of a Coronary Artery (AAOCA).
*   **Tech Stack:** Integration of **GPT-4o mini**, **LangChain**, and specific prompt engineering strategies.
*   **Context Management:** Employed text chunking techniques to manage long-form transcripts.
*   **Evaluation:**
    *   Comparison against human-generated results.
    *   Utilization of thematic similarity metrics.
    *   LLM-assisted assessments and expert reviews.

***

## Technical Details

| Component | Details |
| :--- | :--- |
| **Automation Scope** | Automates Stages 1-5 of inductive Thematic Analysis. |
| **Model** | GPT-4o mini |
| **Data Volume** | 9 de-identified focus group transcripts involving 42 parents. |
| **Text Length** | Median length of 11,457 words per transcript. |
| **Chunking Strategy** | Fine-grained segmentation into chunks of **up to 1,500 words** to preserve context. |
| **Coding Density** | Generated an average of **one code per 225 words**. |
| **Synthesis Strategy** | 'Divide-and-conquer' grouping strategy to organize codes and mitigate context window limitations during theme generation. |
| **Prompting Modes** | 0-Shot, 1-Shot, and 1-Shot + Reflexion (self-reflection loop). |

***

## Results

### Quantitative Metrics
Using **sentence-t5-xxl embeddings**, LLM-TA (0-Shot) achieved:
*   **Jaccard Similarity:** 0.410 (compared to Human-Human 0.583 and baseline 0.111-0.139).
*   **Hit Rate:** 1.000 (capturing all human themes).

### Qualitative Assessment
*   **Specificity Rating:** LLM-TA (1-Shot + Reflexion) was rated **5/5** by experts, compared to a rating of 1-2 for baseline methods.

### Efficiency Gains
*   **Manual Analysis:** Required 30 person-hours.
*   **LLM-TA (Standard):** Under 10 minutes.
*   **LLM-TA (Reflexion):** 90 minutes.
*   **Total Reduction:** **97%** reduction in task duration.

***

## Contributions

1.  **Pipeline Development:** Creation of the LLM-TA pipeline that augments the inductive TA process for complex, high-stakes healthcare environments.
2.  **Economic Viability:** Evidence that high-quality qualitative analysis can be augmented using cost-effective LLMs like GPT-4o mini.
3.  **Best Practices:** Practical recommendations for integrating LLMs into TA workflows, emphasizing collaboration with domain experts.
4.  **Open Science:** Release of the pipeline code and implementation details via a GitHub repository.

***

**Quality Score:** 9/10  
**References:** 15 citations