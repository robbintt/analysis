---
title: 'Quantile Q-Learning: Revisiting Offline Extreme Q-Learning with Quantile Regression'
arxiv_id: '2511.11973'
source_url: https://arxiv.org/abs/2511.11973
generated_at: '2026-02-03T06:39:26'
quality_score: 8
citation_count: 12
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Quantile Q-Learning: Revisiting Offline Extreme Q-Learning with Quantile Regression
*Xinming Gao; Shangzhe Li; Yujin Cai; Wenwu Yu*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 8/10
> *   **References:** 12 Citations
> *   **Primary Domain:** Offline Reinforcement Learning
> *   **Key Benchmarks:** D4RL, NeoRL2
> *   **Core Innovation:** Automated temperature estimation via quantile regression
> *   **Hyperparameter Efficiency:** Universal configuration ($\lambda=1$) across all domains

---

## Executive Summary

This paper addresses the critical instability and hyperparameter sensitivity inherent in **Extreme Q-Learning (XQL)**, a leading algorithm in Offline Reinforcement Learning. While XQL effectively models Bellman errors using the Extreme Value Theorem, it relies on a fixed temperature coefficient that requires exhaustive, dataset-specific manual tuning. This brittleness hinders deployment, as improper tuning leads to divergence or overly conservative behavior. Furthermore, XQL variants frequently suffer from training instability, making them unreliable for complex environments without significant engineering intervention.

The authors introduce **Quantile Q-Learning (QQL)**, which automates temperature estimation using quantile regression to replace manual tuning. The method assumes the Bellman error follows a heteroscedastic Gumbel distribution. By training two separate value networks to approximate specific quantiles ($\tau_1$ and $\tau_2$) of the Q-function, QQL derives a state-dependent temperature $\beta(s)$ from the difference between these quantile estimates. Additionally, QQL incorporates a "Mild Generalization" technique using a third quantile level ($\tau_0$) and combining in-sample and out-of-distribution losses with a fixed weighting factor ($\lambda=1$) to mitigate over-conservatism and enhance stability.

Evaluated on standard baselines (XQL and MXQL) using the D4RL and NeoRL2 benchmarks, QQL demonstrates quantifiable improvements. On the **D4RL Gym-MuJoCo benchmark**, QQL achieved a normalized average score of **46.7**, outperforming the XQL baseline at **45.9**. In the more complex Franka Kitchen environment, QQL reached a score of **20.8**, compared to **19.6** for XQL. Crucially, QQL resolved the training instability of previous variants and eliminated hyperparameter sensitivity, maintaining robust performance across all tested domains using a consistent configuration ($\lambda=1$). This work significantly advances Offline RL by providing a robust, "out-of-the-box" variant of Extreme Q-Learning that theoretically grounds temperature estimation in quantile regression.

---

## Key Findings

*   **Competitive Performance:** The proposed algorithm achieves competitive or superior performance compared to existing methods (XQL and MXQL) across standard benchmarks, specifically **D4RL** and **NeoRL2**.
*   **Enhanced Stability:** The method demonstrates significantly improved training stability, effectively resolving the instability issues often associated with Extreme Q-Learning variants.
*   **Hyperparameter Efficiency:** The approach eliminates the need for extensive hyperparameter tuning, maintaining a consistent set of hyperparameters across all tested datasets and domains.
*   **Principled Estimation:** Utilizing quantile regression to estimate the temperature coefficient provides a principled and effective solution under mild assumptions.

---

## Methodology

The authors refine the foundation of Extreme Q-Learning (XQL) through three primary methodological components:

1.  **Principled Temperature Estimation:**
    The authors employ quantile regression to estimate the temperature coefficient in a principled manner, avoiding the need for manual, dataset-specific tuning.

2.  **Value Regularization:**
    To enhance stability and generalization, the method integrates a value regularization technique derived from recent advances in constrained value learning.

3.  **Foundation:**
    Building upon XQL, the method continues to model Bellman errors using the Extreme Value Theorem but refines the implementation through the aforementioned estimation and regularization techniques.

---

## Technical Details

**Algorithm Core:**
Quantile Q-Learning is a modification of Extreme Q-Learning (XQL) designed for Offline Reinforcement Learning. It eliminates the fixed temperature hyperparameter by estimating it as a state-dependent function using quantile regression.

**Distributional Assumptions:**
The approach assumes the error between the optimal and estimated Q-values follows a **heteroscedastic Gumbel distribution**.

**Network Architecture & Loss Functions:**
*   **Dual Value Networks:** The method utilizes two separate value networks trained via a quantile regression loss ($L_{qr}$) to approximate specific quantiles ($\alpha_1$ and $\alpha_2$) of the Q-function.
*   **Temperature Derivation:** This allows the derivation of the temperature $\beta(s)$ as $[\hat{V}(s) - V(s)] / \omega$.
*   **Q-Function Update:** The Q-function is updated using a mean-squared Bellman error where the difference between the value networks serves as an unbiased estimator for the error bias.

**Mild Generalization:**
To address over-conservatism, the method incorporates a strategy involving:
*   A third quantile level ($\alpha_0$).
*   A combination of in-sample and out-of-distribution losses.
*   A fixed weighting factor of **$\lambda=1$**.

---

## Contributions

The work addresses two critical flaws in previous Extreme Q-Learning methods:

1.  **Hyperparameter Burden:** It removes the requirement for dataset-specific hyperparameter tuning.
2.  **Training Instability:** It resolves the prevalence of training instability found in XQL variants.

**Theoretical Advancement:**
The research introduces a new theoretical approach for determining the temperature coefficient via quantile regression, offering a more automated and robust alternative to manual tuning.

**Algorithmic Robustness:**
By combining principled parameter estimation with value regularization, the authors contribute a more robust offline RL algorithm that generalizes well across diverse domains without sacrificing performance.

---

## Performance & Results

The proposed method was rigorously evaluated against XQL and MXQL baselines on the D4RL and NeoRL2 benchmarks.

*   **Gym-MuJoCo (D4RL):**
    *   **Quantile Q-Learning:** 46.7
    *   **XQL Baseline:** 45.9
*   **Franka Kitchen (D4RL):**
    *   **Quantile Q-Learning:** 20.8
    *   **XQL Baseline:** 19.6
*   **General Performance:**
    *   Achieved competitive or superior normalized scores across the board.
    *   Demonstrated significantly improved training stability compared to XQL.
    *   Showed high hyperparameter efficiency by eliminating sensitivity to the temperature parameter, allowing for a consistent set of hyperparameters ($\lambda=1$) across all tested domains.