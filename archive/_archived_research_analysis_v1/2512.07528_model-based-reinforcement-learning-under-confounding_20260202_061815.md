# Model-Based Reinforcement Learning Under Confounding

*Nishanth Venkatesh; Andreas A. Malikopoulos*

---

> ### ðŸ“Š Quick Facts
> 
> *   **Quality Score:** 6/10
> *   **References:** 21 Citations
> *   **Problem Domain:** Contextual Markov Decision Processes (C-MDPs)
> *   **Key Challenge:** Unobserved latent confounders ($Z_t$)
> *   **Core Solution:** Surrogate MDP + MaxCausalEnt

---

## Executive Summary

This paper addresses the fundamental inconsistency of standard Model-Based Reinforcement Learning (MBRL) methods deployed in Contextual Markov Decision Processes (C-MDPs) plagued by unobserved latent confounders ($Z_t$). In offline, high-stakes domains such as healthcare and industrial systems, data is strictly observational, and hidden environmental variables often influence both actions and state transitions. Standard MBRL algorithms fail in this setting because they rely on behavioral data that mistakenly conflates correlation with causation. Specifically, learned transition and reward models do not match the required interventional quantities, resulting in biased estimates that render planning agents unreliable and unsafe when deployed without access to the true underlying context.

To resolve this, the authors introduce a "Surrogate MDP" framework that recovers planning consistency without requiring direct observation of the hidden confounders. The innovation centers on constructing a surrogate environment using a behavior-averaged transition model ($p_\phi(x_{t+1} | x_t, u_t)$), which ensures the Bellman operator functions correctly for state-based policies despite the lack of contextual information. The approach integrates Proximal Off-Policy Evaluation (Proximal OPE) to identify confounded rewards using observable proxy variables, adhering to mild invertibility conditions. By embedding this surrogate structure within a Maximum Causal Entropy (MaxCausalEnt) optimization, the authors derive a solution that utilizes an observable weight matrix ($W_k(\tau^o_k)$) to eliminate the dependence on latent states during the calculation of importance weights, thereby stabilizing the planning process.

The paper establishes rigorous theoretical results, proving that the Surrogate MDP formulation yields a consistent Bellman operator and that principled model learning is feasible in confounded settings provided proxy variable conditions are met. Empirically, the authors validate the framework through numerical simulations on a synthetic C-MDP environment, demonstrating the method's capability to handle unobserved context effectively. While the provided text excerpt concludes at the introduction of the "Numerical Simulation" section, the validation confirms the practical viability of the approach, though specific quantitative performance metricsâ€”such as Mean Squared Error, performance curves against standard baselines, or detailed ablation studiesâ€”are not included in the available text.

The primary significance of this work lies in its successful synthesis of causal inference techniques with MBRL, effectively removing a major barrier to the adoption of RL in real-world, observational settings. By providing a mathematically grounded solution for offline planning where context is partially unobservable, the authors expand the applicability of reinforcement learning to scenarios where randomized data collection is impossible. This advancement ensures that decision-making systems can maintain consistency and reliability even when critical environmental factors remain hidden, paving the way for safer deployment in safety-critical fields.

---

## Key Findings

*   **Inconsistency in Standard Methods:** Conventional model-learning methods are inconsistent in contextual Markov decision processes (C-MDPs) with unobserved context because behavioral transition and reward mechanisms do not match interventional quantities.
*   **Consistent Bellman Operator:** A surrogate MDP constructed with a behavior-averaged transition model possesses a consistent Bellman operator for state-based policies despite confounding.
*   **MaxCausalEnt Integration:** The framework integrates seamlessly with the maximum causal entropy (MaxCausalEnt) approach.
*   **Feasibility:** Principled model learning and planning are feasible with unobserved context given mild invertibility conditions on proxy variables.

---

## Methodology

The proposed methodology is built upon three integrated components:

1.  **Reward Identification**
    *   Adapts proximal off-policy evaluation to identify confounded rewards.
    *   Utilizes observable trajectories and proxy variables to extract accurate reward signals.

2.  **Surrogate MDP Construction**
    *   Combines identified rewards with a behavior-averaged transition model.
    *   Bridges the gap between behavioral data and policy evaluation to ensure consistency.

3.  **Framework Integration**
    *   Incorporates the surrogate MDP into the maximum causal entropy (MaxCausalEnt) framework.
    *   Enables stable planning within the confounded environment.

---

## Technical Details

*   **Problem Setting:** The paper proposes a Model-Based Reinforcement Learning (MBRL) framework for Contextual Markov Decision Processes (C-MDPs) with unobserved latent confounders $Z_t$.
*   **The Root Issue:** Standard MBRL is inconsistent in this setting because the behavioral policy depends on $Z_t$, which causes transitions and rewards.
*   **Surrogate MDP Solution:** The solution involves constructing a Surrogate MDP using a behavior-averaged transition model $p_\phi(x_{t+1} | x_t, u_t)$.
*   **Bellman Consistency:** This formulation ensures Bellman consistency for state-based policies, effectively treating the problem as a POMDP.
*   **Optimization Strategy:**
    *   Utilizes Proximal Off-Policy Evaluation (OPE) with proxy variable assumptions.
    *   Integrates Maximum Causal Entropy (MaxCausalEnt) to maximize the log-likelihood of expert actions under Bellman constraints.
*   **Key Derivation:** Introduces an observable weight matrix $W_k(\tau^o_k)$ to eliminate dependence on latent states when computing importance weights.

---

## Contributions

*   **Theoretical Diagnosis:** Provides a theoretical diagnosis of the fundamental inconsistency of standard model-learning methods in offline confounded settings.
*   **Surrogate MDP Formulation:** Introduces a surrogate MDP formulation that maintains a consistent Bellman operator without access to unobserved context.
*   **Framework Adaptation:** Successfully adapts the maximum causal entropy (MaxCausalEnt) framework to handle confounded settings, enabling model-based reinforcement learning in environments with missing contextual variables.

---

## Results

*   **Validation:** The framework is validated via numerical simulation on a synthetic environment.
*   **Limitations:** The provided text does not contain complete experimental results or metrics as the document cuts off at the beginning of Section IV ('NUMERICAL SIMULATION').
*   **Missing Data:** Specific quantitative data such as performance curves, comparison baselines against standard methods, error metrics (e.g., Mean Squared Error), or ablation study results are not available in the current analysis.