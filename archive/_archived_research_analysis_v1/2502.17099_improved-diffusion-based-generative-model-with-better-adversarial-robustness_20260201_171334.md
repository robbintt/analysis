# Improved Diffusion-based Generative Model with Better Adversarial Robustness

*Zekun Wang; Mingyang Yi; Shuchen Xue; Zhenguo Li; Ming Liu; Bing Qin; Zhi-Ming Ma*

---

> ### üìä Quick Facts
>
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations
> *   **Core Method:** Robustness-Driven Adversarial Training (RD-AT)
> *   **Model Scope:** Diffusion Probabilistic Models (DPMs) & Consistency Models (CMs)
> *   **Key Performance:**
>     *   **CIFAR-10 FID (NFE=50):** ~3.01 (vs. Standard 3.32)
>     *   **Consistency FID (NFE=1):** ~5.20 (vs. Standard 6.18)

---

## üìù Executive Summary

This research addresses the critical issue of **\"distribution mismatch\"** (often termed exposure bias) inherent in both Diffusion Probabilistic Models (DPMs) and Consistency Models (CMs). This discrepancy arises because the theoretical distribution assumptions used during training diverge from the actual stochastic distributions encountered during the inference process. This problem is significant because it leads to **error accumulation** throughout the sampling trajectory. This accumulation results in degraded generation quality and renders models unstable when utilizing fast sampling methods that employ larger step sizes, limiting the efficiency and reliability of generative models in real-world applications.

The key innovation is the introduction of **Robustness-Driven Adversarial Training (RD-AT)**, a framework derived from the theoretical equivalence between Distributionally Robust Optimization (DRO) and Adversarial Training (AT). The authors reformulate the diffusion training objective as a DRO problem, which seeks to minimize the worst-case loss over an uncertainty set defined by a KL-divergence ball around the ground-truth distribution. Technically, the authors prove this is equivalent to an adversarial process where a perturbation $\delta_t$ is added to the input noise and the target noise is adjusted correspondingly.

The proposed framework was empirically validated on standard benchmarks including **CIFAR-10** and **ImageNet 64x64**. Experimental results demonstrate that RD-AT consistently outperforms standard training baselines in both FID scores and generation stability. For instance, on CIFAR-10, applying RD-AT to Diffusion Models with 50 sampling steps (NFE=50) achieved a Fr√©chet Inception Distance (FID) of approximately **3.01**, significantly improving upon the standard baseline of **3.32**. In the context of Consistency Models, the method showed substantial gains in one-step generation (NFE=1), reducing the FID to roughly **5.20** compared to the standard Consistency Distillation baseline of **6.18**.

The significance of this research lies in establishing a solid theoretical foundation for using Adversarial Training to improve the robustness of generative models. By formally identifying the statistical source of generation errors and linking them to robust optimization principles, the paper offers a principled solution to a fundamental limitation in current diffusion and consistency models.

---

## üîç Key Findings

*   **Universal Mismatch:** Both Diffusion Probabilistic Models (DPMs) and Consistency Models (CMs) suffer from a significant distribution mismatch between their training assumptions and inference realities.
*   **DRO and AT Equivalence:** Distributionally Robust Optimization (DRO) is theoretically equivalent to Adversarial Training (AT) and can successfully alleviate the distribution mismatch found in DPMs.
*   **Extension to Consistency Models:** Consistency Models encounter the same distribution shift issues as DPMs and benefit similarly from the application of Adversarial Training.
*   **Empirical Validation:** Empirical studies confirm that AT effectively resolves these distributional issues for both model types, ensuring stability and quality.

---

## ‚öôÔ∏è Methodology

The authors utilize a structured approach combining theoretical derivation with practical implementation:

1.  **Deconstruction:** Breaking down training objectives to explicitly identify the source of distribution mismatch.
2.  **Alignment Analysis:** Demonstrating that minimizing this mismatch mathematically aligns with Distributionally Robust Optimization (DRO).
3.  **Leveraging Equivalence:** Using the established equivalence between DRO and Adversarial Training (AT) to propose an efficient AT strategy.
4.  **Framework Extension:** Extending this robust training framework to Consistency Models to solve trajectory mismatch.

---

## üß¨ Technical Details

**Solution Name:** Robustness-Driven Adversarial Training (RD-AT)

**Core Mechanism:**
RD-AT addresses Distribution Mismatch (Exposure Bias) in DPMs and CMs by framing the training objective as a Distributionally Robust Optimization (DRO) problem. The goal is to minimize the worst-case loss over an uncertainty set defined by a KL-divergence ball around the ground-truth distribution.

**Implementation Strategy:**
*   **Adversarial Training:** The DRO objective is mathematically equivalent to a perturbed noise prediction problem.
*   **Perturbation:** The method adds a perturbation $\delta_t$ to the input and adjusts the target noise correspondingly via gradient ascent to maximize prediction error.

**Objective Function:**
$$ \min_\theta \Sigma E\left[ \left\| \varepsilon_\theta(\sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t}\varepsilon_t + \delta_t, t) - \left(\varepsilon_t + \frac{\delta_t}{\sqrt{1-\bar{\alpha}_t}}\right) \right\|^2 \right] $$

**Compatibility:**
The method is architecture-agnostic, making it applicable across various model designs.

---

## üìà Results

### Theoretical Analysis
*   **Error Accumulation:** Proposition 2 proves error accumulation in standard DPMs, quantified by $D_{KL}(q(x_t) || p_\theta(x_t)) \leq \gamma_0 + \frac{(T-t)\gamma}{T}$.
*   **Step Size Limitations:** Proposition 3 establishes that standard DPM training loss is only minimized if distributions are Gaussian or step sizes are infinitesimal, explaining failures in fast samplers.
*   **Convergence Guarantee:** Proposition 5 provides a convergence guarantee for RD-AT: if the DRO loss is bounded by $\eta_0$, the final generated distribution matches the data distribution, ensuring $D_{KL}(q(x_0) || p_\theta(x_0)) \leq \eta_0$.
*   **Equivalence Proof:** Theorem 1 establishes the mathematical equivalence between the DRO objective and the Adversarial Training loss.

### Empirical Performance
*   **CIFAR-10 (Diffusion, NFE=50):**
    *   **RD-AT:** FID ‚âà 3.01
    *   **Standard Baseline:** 3.32
*   **Consistency Models (One-step, NFE=1):**
    *   **RD-AT:** FID ‚âà 5.20
    *   **Standard Consistency Distillation:** 6.18

---

## ‚úÖ Contributions

*   **Identification & Formalization:** The authors identify and formally define the 'distribution mismatch' inherent in both DPMs and CMs.
*   **Theoretical Link:** They provide a theoretical proof linking DPM training objectives to Distributionally Robust Optimization (DRO) and establish its equivalence to Adversarial Training (AT).
*   **Methodology Proposal:** The paper proposes a specific, efficient methodology for applying Adversarial Training to diffusion-based models.
*   **Extension:** The work successfully extends the theoretical and practical solution of AT to Consistency Models.