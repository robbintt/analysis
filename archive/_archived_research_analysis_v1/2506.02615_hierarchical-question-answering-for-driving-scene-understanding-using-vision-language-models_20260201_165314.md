# Hierarchical Question-Answering for Driving Scene Understanding Using Vision-Language Models

*Safaa Abdullahi Moallim Mohamud; Minjin Baek; Dong Seog Han*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Hardware** | NVIDIA RTX 4090 |
| **Model Used** | BLIP (384.7M parameters) |
| **Overall Accuracy** | 94.81% (+22.71% vs baseline) |
| **Key Innovation** | Dynamic Skipping Mechanism |
| **Deployment** | ROS 2 / PyTorch |

---

## Executive Summary

This paper addresses the critical challenge of achieving real-time, computationally efficient scene understanding for autonomous driving systems. While state-of-the-art Vision-Language Models (VLMs), such as GPT-4o, offer sophisticated visual reasoning, their massive computational requirements and high latency render them impractical for in-vehicle deployment where split-second decision-making is essential. The research highlights the necessity of a solution that maintains granular scene comprehensionâ€”capturing key driving elements and local contextâ€”without relying on the prohibitive processing power typically associated with large generative models.

The authors introduce a hierarchical question-answering framework built upon a fine-tuned, compact BLIP model (384.7 million parameters). A key aspect of the methodology is the fine-tuning process, which utilizes a custom dataset specific to the vehicle's geographical operating area to capture unique local driving elements. Instead of utilizing continuous text generation, the methodology organizes queries into a structured decision tree of high-level summaries and detailed sub-questions. The core technical advancement is a dynamic skipping mechanism: the system logically navigates the tree, where answers to high-level questions determine which specific sub-questions require processing, effectively bypassing irrelevant queries. These extracted answers are then synthesized into coherent scene descriptions using handcrafted templates rather than generated from scratch, ensuring deterministic efficiency.

Experimental results demonstrate that the proposed method achieves competitiveness with GPT-4o in accurately capturing scene details while significantly outperforming standard pre-trained baselines. Utilizing the Lingo-Judge metric (threshold â‰¥ 85%), the fine-tuned hierarchical model achieved an overall accuracy of 94.81%, a substantial increase of 22.71 percentage points over the 72.10% accuracy of the non-hierarchical pre-trained baseline. Performance breakdowns indicate high reliability in critical "None" and "No" categories (98.85% and 95.65%, respectively). Crucially, the dynamic skipping strategy resulted in significantly lower inference times compared to both the non-hierarchical baseline and GPT-4o, enabling real-time deployment on consumer-grade hardware like the NVIDIA RTX 4090 via ROS 2.

This research holds significant implications for the field of autonomous driving by demonstrating that high-fidelity scene understanding does not strictly require massive parameter counts or cloud-based computing resources. By replacing resource-intensive text generation with a navigated, granular QA pipeline and leveraging geographically specific fine-tuning, the authors present a viable path for embedding sophisticated VLM capabilities directly into edge devices. This approach challenges the current industry reliance on large-scale models for context-aware systems, offering a scalable, cost-efficient alternative that maintains accuracy while optimizing for the strict latency constraints of real-world driving environments.

---

## Key Findings

*   **Significant Speed Improvement:** The proposed approach achieves significantly lower inference times compared to state-of-the-art methods, optimizing for computational efficiency.
*   **Competitive Accuracy:** Despite using a compact model, the system demonstrates competitiveness with GPT-4o in accurately capturing key driving scene details.
*   **Dynamic Optimization:** The hierarchical strategy's dynamic skipping mechanism effectively minimizes computational overhead by omitting irrelevant questions.
*   **Real-Time Viability:** Real-time deployment results confirm the system's capacity to capture critical driving elements with minimal latency.

---

## Methodology

The researchers developed a hierarchical question-answering framework utilizing a Vision-Language Model (VLM). The process involves four main steps:

1.  **Model Fine-Tuning**
    A compact VLM is fine-tuned on a custom dataset specific to the vehicle's geographical operating area to capture local driving elements.

2.  **Hierarchical Decomposition**
    The scene understanding task is broken down into a structured question tree, differentiating between high-level summary questions and detailed sub-questions.

3.  **Dynamic Inference**
    The system navigates the tree logically; answers to high-level questions trigger specific relevant sub-questions, while others are dynamically skipped to save processing time.

4.  **Answer Synthesis**
    Instead of generating raw text, the answers extracted by the VLM are synthesized into coherent, contextually accurate scene descriptions using handcrafted templates.

---

## Contributions

*   **Cost-Efficient Scene Understanding:** Introduction of a method that balances detailed visual interpretation with cost-efficiency, challenging the necessity of massive computational resources for scene understanding.
*   **Hierarchical QA Framework:** Development of a structured inference strategy that replaces lengthy text generation with a navigated decision tree, allowing for granular analysis only where necessary.
*   **Optimized Inference Pipeline:** Demonstration of a workflow (fine-tuning + dynamic skipping + template synthesis) that achieves real-time performance comparable to large-scale models like GPT-4o.

---

## Technical Specifications

| Attribute | Details |
| :--- | :--- |
| **Hardware** | NVIDIA RTX 4090 |
| **Middleware / Deployment** | ROS 2 |
| **Framework** | PyTorch |
| **Model Architecture** | BLIP (Bootstrapping Language-Image Pre-training) |
| **Parameter Count** | 384.7 Million trainable parameters |
| **Hierarchical Strategy** | Dynamic skipping mechanism to select relevant questions vs. baseline processing all 41 questions |
| **Learning Rate** | 1e-6 |
| **Epochs** | 15 |
| **Batch Size** | 16 |
| **Training Data** | Fine-tuned on a driving-specific dataset |

---

## Results & Evaluation

**Evaluation Protocol:**
*   Metric: Lingo-Judge
*   Similarity Threshold: â‰¥ 85%

**Overall Accuracy:**
*   **Fine-tuned Model:** 94.81%
*   **Pre-trained Baseline:** 72.10%
*   **Improvement:** +22.71 percentage points

**Category Breakdown (Fine-tuned vs. Baseline):**

| Category | Fine-tuned Model | Pre-trained Baseline |
| :--- | :---: | :---: |
| **'None'** | 98.85% | 79.53% |
| **'No'** | 95.65% | 59.71% |
| **'Yes'** | 67.10% | 52.63% |
| **'Other'** | 53.33% | 26.66% |

**Inference Efficiency:**
Hierarchical QA shows significantly lower inference times compared to the non-hierarchical baseline and GPT-4o.

---

### References & Evaluation
*   **References:** 27 citations