# Generalisation in Multitask Fitted Q-Iteration and Offline Q-learning
*Kausthubh Manda; Raghuram Bharadwaj Diddigi*

---

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 9/10
> *   **References:** 40 Citations
> *   **Learning Paradigm:** Model-free, Offline, Multitask RL
> *   **Key Metric:** Statistical error rate of $O(1/\sqrt{nT})$

---

## Executive Summary

This research addresses the theoretical foundations of Offline Multitask Reinforcement Learning (RL), specifically investigating how leveraging shared structure across related tasks can enhance sample efficiency in value-based, model-free settings, encompassing both Fitted Q-Iteration and Offline Q-learning frameworks. While data pooling is a common heuristic to improve learning, the rigorous theoretical underpinnings of this approachâ€”particularly concerning generalization guarantees under the constraints of offline RL (such as distribution shift and limited data coverage)â€”have been largely unexplored. This problem is critical because offline RL offers a path to deploy safe, high-performing agents without costly environment interaction; understanding precisely when and how multitask data pooling improves estimation accuracy is essential for advancing the reliability of these systems.

The authors introduce a multitask variant of Fitted Q-Iteration (MFQI), a novel model-free algorithm designed to exploit commonalities across tasks without assuming access to a system model. Technically, the method operates under a "low-rank realizability" assumption, positing that optimal action-value functions across all tasks share a common latent feature encoder ($\phi^*$) but utilize distinct linear decoders. MFQI performs backward induction to minimize the joint mean-squared Bellman error across all tasks using pooled offline data. A key aspect of this innovation is the rigorous management of distribution shift, utilizing concentrability coefficients to measure and bound the density ratios between the learning update distribution and the behavior policy distribution.

The study establishes finite-sample generalization guarantees, demonstrating that the statistical error rate of the learned value functions scales as $O(1/\sqrt{nT})$, where $n$ is the number of samples per task and $T$ is the number of tasks. Crucially, the analysis specifies that the error bound scales linearly with the planning horizon $H$ and depends on the concentrability coefficient $\lambda_{\max}$. This proves that estimation accuracy improves with the square root of the total data pool while maintaining explicit control over computational and statistical complexity. Furthermore, the analysis confirms that reusing the learned shared encoder for new downstream tasks significantly reduces effective sample complexity compared to learning representations from scratch.

This work significantly advances the field by providing a precise mathematical characterization of how shared representations enable statistical efficiency gains in offline RL. By extending its scope beyond Fitted Q-Iteration to Offline Q-learning contexts, it bridges the gap between multitask learning and transfer learning, offering theoretical evidence that pre-trained representations derived from offline data can substantially accelerate learning on new downstream tasks. By rigorously defining the conditions under which multitask structure is beneficial, this research provides a foundation for developing more data-efficient algorithms that maximize the utility of static, historical datasets in complex, multi-agent environments.

---

## Key Findings

*   **Statistical Efficiency of Data Pooling:** Pooling data across multiple tasks improves estimation accuracy, achieving a dependence of $1/\sqrt{nT}$ on the total number of samples.
*   **Downstream Transfer Benefits:** Reusing a shared learned representation reduces the effective complexity of learning new downstream tasks.
*   **Preservation of Offline RL Dependencies:** Improved generalization retains standard theoretical dependencies regarding planning horizon and concentrability coefficients.
*   **Finite-Sample Guarantees:** Establishes finite-sample generalization guarantees for value functions learned jointly under standard assumptions.

---

## Methodology

The authors utilize a **multitask variant of Fitted Q-Iteration (FQI)** in a model-free, purely offline setting. The method simultaneously learns a shared low-rank representation common to all tasks and task-specific value functions by minimizing the Bellman error on the available offline data.

---

## Technical Details

*   **Task Structure:** Addresses Offline Multitask RL with $T$ distinct episodic MDPs sharing state and action spaces.
*   **Assumption:** Low-rank realizability is assumed, where optimal action-value functions share a common latent feature encoder $\phi^*$ with task-specific linear decoders.
*   **Algorithm:** **Multi-task Fitted Q-Iteration** is proposed, which minimizes the joint mean-squared Bellman error across all tasks using pooled offline data via backward induction.
*   **Distribution Shift Management:** The analysis relies on a concentrability coefficient $\lambda_{\max}$ to measure the density ratio between the learning update distribution and the behavior policy distribution.

---

## Contributions

1.  **Theoretical Foundation for Multitask Offline RL:** Provides rigorous theoretical insight into when and how multitask structure improves generalization in value-based, model-free offline reinforcement learning.
2.  **Explicit Characterization of Generalization:** Offers a precise mathematical characterization of how shared representations enable statistical efficiency gains.
3.  **Bridge to Transfer Learning:** Connects multitask learning with downstream transfer learning by demonstrating the reduction in sample complexity when pre-trained representations are applied to new tasks.

---

## Results

*   The method achieves a statistical error rate scaling of **$O(1/\sqrt{nT})$**, demonstrating that estimation accuracy improves with the square root of the total data pool.
*   The theoretical guarantees retain dependencies on the planning horizon $H$ and the concentrability coefficient $\lambda_{\max}$.
*   Furthermore, reusing the learned encoder for new downstream tasks reduces effective complexity and improves sample efficiency compared to learning from scratch.