# Trajectory Data Suffices for Statistically Efficient Policy Evaluation in Finite-Horizon Offline RL with Linear $q^\pi$-Realizability and Concentrability

*Volodymyr Tkachuk; Csaba Szepesvári; Xiaoqi Tan*

---

### Quick Facts & Metrics

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Domain** | Offline Reinforcement Learning (RL) |
| **Core Problem** | Policy Evaluation & Optimization in Finite-Horizon settings |
| **Key Innovation** | Leveraging Trajectory Data Structure (vs. independent samples) |
| **Complexity Scaling** | Polynomial in Horizon ($H$) and Dimension ($d$); No exponential dependence $C_0^d$ |
| **Key Algorithms** | LIN-$q^\pi$-FQI, Policy Evaluation Learner |

---

## Executive Summary

> This research addresses the fundamental challenge of achieving statistically efficient policy evaluation in finite-horizon offline reinforcement learning (RL). In offline RL, an agent must evaluate and optimize policies using a fixed dataset without interacting with the environment, a requirement for safety-critical applications where exploration is impossible. Previous theoretical works established "impossibility results," suggesting that statistically efficient learning was unattainable under general distributional shifts. The study resolves this theoretical uncertainty by identifying the specific data structures required to make offline RL feasible, proving that efficient evaluation is possible under standard realizability assumptions if the data leverages the sequential nature of the problem.

The key innovation is the identification of **trajectory data structure**—sequences of states and actions—as the critical enabler for statistical efficiency, circumventing the barriers associated with independent, single-step transition data. The authors assume the data consists of trajectories, alongside concentrability (data coverage) and linear $q^\pi$-realizability (linear Q-functions). Utilizing this structure, the authors exploit sequential dependencies to create a stable stochastic approximation of the Bellman policy operator. Unlike independent samples, which induce prohibitive variance amplification under distribution shift, trajectory data interacts favorably with least-squares projection errors. This mechanism allows the authors to introduce a Policy Evaluation Learner and a policy optimization algorithm (LIN-$q^\pi$-FQI) that effectively minimize estimation error where previous methods failed.

The study derives rigorous, internally consistent sample complexity bounds for both policy evaluation and optimization. Let $H$ denote the horizon, $d$ the feature dimension, $n$ the number of trajectories, and $C$ the concentrability coefficient measuring data coverage.

*   **Policy Evaluation:** Theorem 1 proves that the Q-evaluation error scales as $\tilde{O}(H^{5/2} d^{3/2} C^{3/2} / \sqrt{n})$. This establishes that the required sample complexity depends polynomially on the problem parameters, specifically scaling as $\tilde{\Theta}(C^3 H^7 d^3 / \epsilon^2)$ for an accuracy $\epsilon$.
*   **Policy Optimization:** Theorem 2 establishes a significantly tighter bound than prior art by avoiding the exponential dependence $C_0^d$ found in previous results. Instead, the authors achieve an integrated squared error bound of $\tilde{O}(\log(1/\alpha) C^{5/2} H^{5/2} d^{3/2} / \sqrt{n})$, where $\alpha$ is the confidence parameter.

This paper significantly advances the theoretical understanding of offline RL by demonstrating that statistically efficient policy evaluation is feasible under standard structural assumptions, directly contradicting earlier pessimism. It complements recent findings on policy optimization, establishing a unified theoretical framework for the offline setting that accounts for trajectory structure. The implication is that offline RL can be theoretically sound and sample-efficient when algorithms properly leverage the sequential nature of data, providing a robust foundation for future algorithm development in data-driven decision-making.

---

## Key Findings

*   **Statistical Efficiency Achieved:** Demonstrated that statistically efficient policy evaluation is feasible in finite-horizon offline RL, overcoming previous "impossibility" barriers.
*   **Trajectory Data is Critical:** Identified the structure of offline data as trajectories (rather than general distributions or single-step transitions) as the critical factor enabling efficiency.
*   **Structural Synergy:** By combining trajectory data with concentrability (coverage) and linear realizability, the study resolves limitations that previously hindered efficient learning.
*   **Optimization Improvements:** Established a tighter analysis for existing policy optimization learners, resulting in improved sample complexity bounds.

---

## Methodology

The research is situated within the context of **finite-horizon offline RL** with function approximation, addressing both policy evaluation and optimization. The theoretical approach relies on three specific core assumptions:

1.  **Concentrability:** The data possesses good coverage of the state-action space.
2.  **Linear $q^\pi$-Realizability:** The state-action value function of every policy is linearly realizable.
3.  **Trajectory Structure:** Data is structured as sequences of states and actions rather than independent samples.

The authors employ a rigorous theoretical framework to prove the existence of a learner and to bound sample complexity with greater precision than previously available in the literature.

---

## Technical Details

The study addresses finite-horizon offline RL utilizing trajectory data structures for statistical efficiency and state abstraction via feature representations.

### Core Algorithms
*   **Algorithm 1 (LIN-$q^\pi$-FQI):** Designed for policy optimization, this algorithm selects candidate Q-functions that maximize start state values.
*   **Algorithm 2 (Policy Evaluation Learner):** Utilizes the empirical Bellman policy operator to minimize estimation error.

### Theoretical Framework
*   **Fitted Q-Evaluation (FQE):** A foundational concept used within the learner construction.
*   **Operators:** The framework relies on Bellman Policy Operators and Empirical Bellman Operators to derive error bounds.

### Critical Assumptions
*   **Linear $q^\pi$-Realizability:** Assumes linear representability of Q-functions.
*   **Concentrability:** Defined with coefficient $C_0$, ensuring sufficient data coverage.
*   **Bellman Completeness:** An assumption ensuring the closure of the function class under the Bellman operator.

---

## Results

The results are theoretical sample complexity bounds that demonstrate significant improvements over prior work.

### Theorem 1: Policy Evaluation Bound
Given **$n = \tilde{\Theta}(C^3 H^7 d^3 / \epsilon^2 + L^2_\phi)$** trajectories, the value estimate satisfies **$|v^{\pi_e}(s_1) - \hat{v}| \le \epsilon$** with probability **$1-\delta$**.

### Theorem 2: Policy Optimization Bound
Establishes a tighter policy optimization bound, improving the sample complexity by a factor of **$C_0^d$** over prior work.

### Intermediate Bounds
*   **Q-eval Error:** $\tilde{O}(H^{5/2} d^{3/2} / \sqrt{n})$
*   **Integrated Squared Error:** $\tilde{O}(\log(1/\alpha) C^{5/2} H^{5/2} d^{3/2} / \sqrt{n})$

---

## Contributions

*   **First Efficient Learner:** Provides the first statistically efficient learner for the policy evaluation problem under the combined assumptions of trajectory data, realizability, and concentrability.
*   **Unified Theory:** Establishes a comprehensive theoretical framework that complements recent findings regarding policy optimization, effectively completing the theoretical understanding for both evaluation and optimization in this offline RL setting.
*   **Refined Analysis:** Contributes a refined theoretical analysis that improves the sample complexity efficiency of previously proposed policy optimization methods.

---

**Document Statistics**
*   **References:** 5 citations
*   **Quality Assessment:** 8/10