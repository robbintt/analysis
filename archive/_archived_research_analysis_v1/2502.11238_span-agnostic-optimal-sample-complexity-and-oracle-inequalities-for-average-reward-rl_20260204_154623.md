---
title: Span-Agnostic Optimal Sample Complexity and Oracle Inequalities for Average-Reward
  RL
arxiv_id: '2502.11238'
source_url: https://arxiv.org/abs/2502.11238
generated_at: '2026-02-04T15:46:23'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Span-Agnostic Optimal Sample Complexity and Oracle Inequalities for Average-Reward RL
*Matthew Zurek; Yudong Chen*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Problem Setting** | Generative Model (AMDPs) |
| **Key Complexity** | $\widetilde{O}(SAH/\varepsilon^2)$ |
| **Core Innovation** | Horizon Calibration & Empirical Span Penalization |

---

## Executive Summary

This paper addresses the critical challenge of **span-agnostic learning** in Average-Reward Reinforcement Learning (AMDPs) within a generative model setting. While existing theoretical frameworks define the minimax optimal sample complexity as $\widetilde{O}(SAH/\varepsilon^2)$â€”where $H$ is the span of the optimal bias functionâ€”prior algorithms required prior knowledge of this span parameter to achieve optimality. This dependency creates a significant practical barrier, as the span is rarely known beforehand. Furthermore, standard approaches are often constrained by worst-case theoretical limits, failing to adapt when the underlying problem structure is "easier," such as when near-optimal policies possess a span significantly smaller than the worst-case $H$.

The authors introduce two primary methodological innovations to overcome these limitations:

1.  **Discounted Reduction with Horizon Calibration:** This reduces the average-reward problem to a series of discounted problems (DMDPs). It dynamically tunes the effective horizon $H_{\text{eff}} = 1/(1-\gamma)$ by selecting the discount factor $\gamma$ from a geometric grid based on empirical confidence intervals or lower bounds.
2.  **Empirical Span Penalization:** This adaptively penalizes the empirical span of policies, allowing the algorithm to automatically adjust to the problem structure without external tuning parameters, effectively removing the requirement for prior knowledge of the span.

The research establishes rigorous theoretical guarantees, achieving the optimal span-based sample complexity of $\tilde{O}(SA\|h^\star\|_{\text{span}}/\varepsilon^2)$ without prior knowledge of $H$. Most significantly, Theorem 4 presents an **oracle inequality**, demonstrating that the algorithm adapts to the smallest span among near-optimal policies rather than being bound by worst-case limits.

This work resolves a major open problem in average-reward RL by achieving minimax optimal sample complexity span-agnostically. By demonstrating that algorithms can satisfy oracle inequalities and outperform standard minimax complexity in benign settings, the authors prove that average-reward RL methods need not be strictly bound by worst-case theoretical limits.

---

## Key Findings

*   **Span-Agnostic Optimality:** The authors developed the first algorithms that achieve the minimax optimal span-based complexity of $\widetilde{O}(SAH/\varepsilon^2)$ **without requiring prior knowledge** of the span parameter $H$.
*   **Dual Scenario Efficiency:** These algorithms achieve optimal complexity in both scenarios:
    *   Fixed dataset size ($n$).
    *   Fixed suboptimality level ($\varepsilon$).
*   **Oracle Inequality Achievement:** The empirical span penalization approach satisfies an oracle inequality, allowing it to outperform the standard minimax complexity in "benign" settings where near-optimal policies possess a span significantly smaller than the worst-case $H$.

---

## Methodology

The research relies on two primary methodological innovations:

*   **Discounted Reduction with Horizon Calibration**
    Combines discounted reduction with a technique that automatically tunes the effective horizon based on empirical confidence intervals or lower bounds.

*   **Empirical Span Penalization**
    Penalizes the empirical span to adapt to the problem structure and removes the need for external tuning parameters.

---

## Contributions

*   **Open Problem Resolution:** Resolves a significant open problem in average-reward RL by achieving optimal sample complexity without prior knowledge of the optimal bias function's span ($H$).
*   **New Technical Tool:** Introduces **'horizon calibration'** as a viable technical tool for automatically adapting the effective horizon in average-reward settings.
*   **Adaptivity to Structure:** Demonstrates that average-reward RL algorithms can adapt to easier problem structures (smaller spans of near-optimal policies) rather than being bound strictly by worst-case theoretical limits by providing an algorithm with an oracle inequality based on span penalization.

---

## Technical Details

### Problem Setting
The paper addresses span-agnostic learning in Average-Reward Reinforcement Learning (AMDPs) under a generative model setting.

### Core Approach
The approach reduces the average-reward problem to a series of discounted problems (DMDPs) by dynamically selecting the discount factor $\gamma$.

### Primary Technical Novelty: Horizon Calibration
This technique tunes the effective horizon $H_{\text{eff}} = \frac{1}{1-\gamma}$ as a parameter on a geometric grid ($H_{\text{eff}} = 2^k$).

### Algorithmic Components
*   **Empirical Transition Estimation:** Utilized to construct an empirical kernel $\hat{P}$.
*   **Algorithm 1 (Fixed-$n$):** Solves empirical DMDPs and selects $\gamma$ to maximize a lower bound on the gain $\hat{L}(\gamma)$.
*   **Algorithm 2 (Fixed-$\varepsilon$):** Iterates with geometrically increasing sample sizes, selecting $\gamma$ to minimize the width of the confidence interval between upper and lower bounds, terminating when the width is $\le \varepsilon$.
*   **Advanced Approach:** Uses Empirical Span Penalization to adapt to the minimum span of any gain-optimal policy.

---

## Results

The paper achieves the optimal span-based sample complexity $\tilde{O}(SA\|h^\star\|_{\text{span}}/\varepsilon^2)$ without prior knowledge of the span, outperforming previous methods that required prior knowledge or used larger complexity parameters.

**Theorem 1 (Fixed Dataset):**
Guarantees a suboptimality gap bounded by:
$$ \rho_{\hat{\pi}} \ge \rho^\star - \tilde{O}\left(\sqrt{\frac{\|h^\star\|_{\text{span}} + 1}{n}}\right) $$

**Theorem 2 (Fixed Accuracy):**
Provides a sample complexity bound of:
$$ N \le \frac{C_1 (\|h^\star\|_{\text{span}} + 1)}{\varepsilon^2} \log^3\left(\frac{C_2 SA (\|h^\star\|_{\text{span}} + 1)}{\delta \varepsilon}\right) $$
*   **Computational Cost:** At most $\log_2(N)$ iterations.

**Theorem 4 (Oracle Inequality):**
Presents an oracle inequality adapting to the 'easiest' near-optimal policy:
$$ \tilde{O}\left( SA \inf_{\pi: \rho_\pi \text{ constant}} \left\{ \frac{\|h_\pi\|_{\text{span}}}{(\rho_\pi - \rho^\star + \varepsilon)^2} \right\} \right) $$

**Termination:**
The algorithm ensures a confidence interval width $\hat{U} - \hat{L} \le \varepsilon$ upon termination.

---
*Document generated based on analysis of 40 references.*