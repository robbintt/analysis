# BLASST: Dynamic BLocked Attention Sparsity via Softmax Thresholding

*Jiayi Yuan, Cameron Shinn, Kai Xu, Jingze Cui, George Klimiashvili, Guangxuan Xiao, Perkz Zheng, Bo Li, Yuxin Zhou, Zhouhai Ye, Weijie You, Tian Zheng, Dominic Brown, Pengbo Wang, Richard Cai, Julien Demouth, John D. Owens, Xia Hu, Song Han, Timmy Liu, Huizi Mao*

---

> ### ðŸ“Š QUICK FACTS
>
> | Metric | Value |
> | :--- | :--- |
> | **Prefill Stage Speedup** | **1.62x** |
> | **Decode Stage Speedup** | **1.48x** |
> | **Max Context Validated** | 128K Tokens |
> | **Potential Max Context** | 1M Tokens |
> | **Compatibility** | MHA, GQA, MQA, MLA |
> | **Training Required** | No (Drop-in) |
> | **Quality Score** | 9/10 |

---

## Executive Summary

**Problem:**
Scaling Transformer models to long context lengths (up to 1M tokens) creates severe computational and memory bottlenecks during prefill and decode phases. While kernels like FlashAttention optimize operations, they remain constrained by quadratic complexity and memory bandwidth. Existing sparse solutions often introduce overhead via proxy networks or pre-computation, failing to reduce memory traffic sufficiently.

**Innovation:**
BLASST introduces a dynamic, proxy-free pruning mechanism integrated directly into the FlashAttention kernelâ€™s online-softmax loop. It identifies negligible attention scores in real-time by comparing block maxima ($\tilde{m}(j)$) against the running row maximum ($m(j)$) using a threshold condition: $$\tilde{m}(j) < m(j) - \ln(\lambda)$$. If a block is insignificant, the kernel skips the exponential calculation, avoids loading Value blocks from HBM, and omits matrix multiplication, adding only minimal comparison overhead.

**Results:**
BLASST achieves substantial acceleration without compromising accuracy:
*   **1.62x speedup** during the prefill stage.
*   **1.48x speedup** during the decode stage.
*   High fidelity maintained on long-context benchmarks (up to 128K tokens).
*   Models can be trained to be inherently robust to sparse patterns.
*   Optimal sparsity threshold ($\lambda$) exhibits an inverse relationship with context length, simplifying calibration.

**Impact:**
BLASST bridges the gap between theoretical sparsity and practical hardware implementation. As a drop-in solution modifying the underlying FlashAttention kernel, it immediately benefits existing infrastructure, reduces operational costs, and lays the groundwork for "sparsity-aware" training regimes, potentially enabling massive context retrieval up to 1M tokens.

---

## Key Findings

*   **Significant Performance Gains:** Achieves a **1.62x speedup** during the prefill stage and a **1.48x speedup** during the decode stage on modern GPUs.
*   **Universal Compatibility:** Maintains high accuracy and is applicable to both prefill and decode phases across all major attention variants (MHA, GQA, MQA, MLA).
*   **Calibration Simplicity:** Discovery of an inverse relationship between the optimal sparsity threshold and context length significantly simplifies deployment.
*   **Training Robustness:** Models can be trained to be inherently more robust to sparse attention patterns, ensuring stability.

---

## Methodology

BLASST introduces a dynamic, drop-in sparse attention mechanism that prunes the attention matrix in real-time. Its core approach involves:

1.  **Thresholding:** Utilization of a fixed threshold applied to statistics derived from the online softmax operation to identify and discard negligible attention scores.
2.  **Operations Skipping:** The approach skips:
    *   Softmax calculation
    *   Value block loading
    *   Matrix multiplication for pruned elements
3.  **Integration:** Seamless integration into existing FlashAttention kernel designs without requiring architectural changes to the model itself.

---

## Contributions

*   **Unified Acceleration for Long-Context Inference:** Addresses computational and memory bottlenecks in both prefill and decode stages simultaneously.
*   **Proxy-Free Dynamic Pruning:** Eliminates the need for pre-computation or proxy networks by leveraging intrinsic softmax information available during computation.
*   **Hardware-Efficient Kernel Design:** Fits into current FlashAttention kernels to provide immediate performance benefits with a drop-in replacement.
*   **Sparsity-Aware Training Extension:** Establishes a foundation for future training regimes that natively accommodate sparse attention patterns.

---

## Technical Details

**Mechanism**
BLASST integrates a dynamic pruning method into the FlashAttention kernel's block-wise online-softmax loop. It maintains a running row maximum of attention scores, denoted as $m(j)$.

**Thresholding Condition**
The system determines whether to prune a block based on the condition:
$$ \tilde{m}(j) < m(j) - \ln(\lambda) $$
Where:
*   $\tilde{m}(j)$ is the block maximum.
*   $\lambda$ is a tunable threshold.

**Hardware Optimization**
*   **Skipped Operations:** Pruning allows the kernel to bypass exponential computation, Value block loading from HBM, and score-value matrix multiplication.
*   **Overhead:** The method adds a minimal overhead of a single comparison per block.

**Implementation Specs**
*   **Training-Free:** No pre-training or fine-tuning is strictly required for inference speedups.
*   **Kernels:** Uses specialized CUDA kernels for both prefill and decode phases.
*   **Attention Variants:** Compatible with MHA, GQA, MQA, and MLA.
*   **Comparison to SpargeAttention:** Unlike related works, BLASST uses already-computed statistics with zero overhead and explicitly skips Value loading to directly address memory-bound bottlenecks.

---

## Results & Validation

*   **Performance Metrics:** Validated 1.62x prefill and 1.48x decode speedups on modern GPUs.
*   **Context Scaling:** Validated on long-context models up to 128K tokens (e.g., Deepseek-R1, Qwen3), with indications of potential applicability to 1M token contexts.
*   **Threshold Dynamics:** Confirmed an inverse relationship between optimal sparsity threshold ($\lambda$) and context length.
*   **Accuracy:** High accuracy maintained across robust sparse patterns.

---

**Quality Score:** 9/10  
**References:** 9 citations