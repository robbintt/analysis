# The Easy Path to Robustness: Coreset Selection using Sample Hardness
*Pranav Ramesh; Arjun Roy; Deepak Ravikumar; Kaushik Roy; Gopalakrishnan Srinivasan*

---

> ### üìä Quick Facts
>
> | Metric | Detail |
> | :--- | :--- |
> | **Quality Score** | 9/10 |
> | **References** | 31 Citations |
> | **Key Improvement** | **+7%** Adversarial Accuracy (Standard) |
> | **Key Improvement** | **+5%** Adversarial Accuracy (TRADES) |
> | **Architectures** | ResNet-18, ResNet-50, Synthetic 2D Model |
> | **Core Metric** | Average Input Gradient Norm (AIGN) |

---

## üìù Executive Summary

This research addresses the critical challenge of enhancing adversarial robustness in deep learning models through data-centric coreset selection. A prevailing assumption in active learning and subset selection is that "hard" samples‚Äîthose lying close to decision boundaries‚Äîare the most "informative" and should be prioritized for training. However, the authors identify a significant gap: while these methods maximize clean accuracy, they often fail to improve robustness against adversarial attacks. This finding reveals a conflict between selecting data for standard performance versus selecting data for security, highlighting a critical vulnerability in current data pruning strategies where relying on hard samples may inadvertently train models on inputs that are most susceptible to perturbation.

The paper introduces **EasyCore**, a novel coreset selection framework that fundamentally challenges the status quo by selecting "easy" samples to bolster robustness. Technically, the method quantifies sample hardness using the **Average Input Gradient Norm (AIGN)**, which is accumulated over the training process. Samples exhibiting low AIGN are classified as "easy," indicating they are prototypical and distant from decision boundaries, whereas high-AIGN samples are deemed "hard" and vulnerable. Grounded in the **Boundary Curvature Hypothesis**, the authors present a theoretical bound linking input gradient norms to weight gradient norms, demonstrating that utilizing low-AIGN subsets reduces decision boundary curvature. This allows EasyCore to modify training data composition to create simpler, more stable decision regions without altering model architecture or loss functions.

In empirical evaluations across ResNet-18 (CIFAR-10/100) and ResNet-50 (ImageNet), EasyCore achieved significant gains in adversarial accuracy compared to state-of-the-art coreset baselines. Specifically, the method yielded up to a **7% improvement** in adversarial accuracy under standard training regimes and up to a **5% improvement** under TRADES adversarial training. Qualitative analysis using UMAP visualizations confirmed that models trained on low-AIGN subsets exhibit less complex decision boundaries, with features clustering into tight centroids rather than fringing near class boundaries. Additionally, the researchers observed that standard adversarial training naturally shifts the mean AIGN distribution to higher values, further validating the metric's correlation with model vulnerability.

This work significantly influences the field by establishing AIGN as a theoretically grounded and dataset-intrinsic metric for robustness, shifting the research focus from maximizing informativeness to minimizing vulnerability. It provides practitioners with a computationally efficient, model-agnostic tool to improve model security simply by optimizing the training set composition, bypassing the need for complex architectural changes or expensive adversarial fine-tuning.

---

## üîë Key Findings

*   **Contradiction of Intuition:** Contrary to the belief that "hard" samples (near decision boundaries) are most valuable, this study finds that "easy" samples‚Äîquantified by low **Average Input Gradient Norm (AIGN)**‚Äîare less vulnerable to adversarial attacks and reside further from decision boundaries.
*   **Superior Performance:** Training on data subsets selected via the proposed EasyCore method yields significantly higher adversarial accuracy compared to subsets selected by existing coreset algorithms designed for clean accuracy.
*   **Quantifiable Gains:** EasyCore achieves specific improvements of up to **7%** in adversarial accuracy under standard training and up to **5%** under TRADES adversarial training relative to competing methods.
*   **Dataset Agnostic:** The selection metric (AIGN) is a dataset property rather than a model-specific one, making the approach widely applicable across different model architectures for improving robustness.

---

## üß© Methodology

The researchers proposed **EasyCore**, a data-centric coreset selection framework designed to enhance adversarial robustness. The methodology operates on the premise that a sample's adversarial vulnerability is linked to its "hardness," quantified using the Average Input Gradient Norm (AIGN) accumulated over the training process.

Instead of retaining the most informative or "hard" samples (common in standard active learning), EasyCore filters the dataset to retain only "easy" samples‚Äîspecifically those exhibiting low AIGN. This subset is then used to train models under both standard and adversarial training regimes (specifically TRADES).

---

## ‚öôÔ∏è Technical Details

### Core Concept
*   **Metric:** Utilizes Average Input Gradient Norm (AIGN) to quantify sample hardness.
    *   *Low AIGN:* "Easy" (prototypical, robust).
    *   *High AIGN:* "Hard" (near decision boundary, vulnerable).
*   **Hypothesis:** Based on the **Boundary Curvature Hypothesis**; selecting low AIGN samples reduces decision boundary curvature.
*   **Theoretical Support:** Lemma 1 provides a theoretical bound linking input gradient norms to weight gradient norms.
*   **Visualization:** UMAP is employed to visualize feature clustering and decision boundaries.

### Model Architectures
1.  Synthetic 2D Model (20 residual blocks, 256 feature dim, 1.317M parameters)
2.  ResNet-18 (for CIFAR-10/100)
3.  ResNet-50 (for ImageNet)

### Experimental Hyperparameters

| Dataset | Optimizer | Learning Rate | Scheduler | Batch Size | Epochs | Weight Decay |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **CIFAR** | SGD (Mom: 0.9) | 0.1 | CosineAnnealingLR | 200 | 300 | 5e-4 |
| **ImageNet**| SGD (Mom: 0.9) | 0.1 | StepLR | 256 | 90 | 1e-4 |

---

## üìà Results

### Quantitative Results
*   **Standard Training:** Achieved up to **7% improvement** in adversarial accuracy.
*   **Adversarial Training (TRADES):** Achieved up to **5% improvement** in adversarial accuracy.

### Qualitative Analysis
*   **Decision Boundaries:** Training on low AIGN samples resulted in less complex (simpler) decision boundaries.
*   **Feature Clustering:** Low AIGN samples form tight centroids, whereas high AIGN samples are located on the fringes/outskirts of clusters.
*   **AIGN Distribution Shift:** It was observed that adversarial training shifts the mean AIGN distribution to higher values, suggesting that standard adversarial training processes inherently engage with "harder" samples.

---

## ‚úÖ Contributions

*   **Theoretical Metric:** The paper establishes AIGN as a theoretically grounded and practical metric for distinguishing between robust (easy) and vulnerable (hard) samples within a dataset.
*   **Objective Shift:** It addresses a critical gap in the literature by shifting coreset selection objectives from maximizing clean accuracy to preserving and enhancing adversarial robustness.
*   **Practical Strategy:** The work contributes a computationally efficient, model-agnostic strategy that allows practitioners to improve model security without modifying the model architecture or the loss function‚Äîsimply by optimizing the training data composition.

---