---
title: Sparse Subnetwork Enhancement for Underrepresented Languages in Large Language
  Models
arxiv_id: '2510.1358'
source_url: https://arxiv.org/abs/2510.13580
generated_at: '2026-02-03T18:43:42'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Sparse Subnetwork Enhancement for Underrepresented Languages in Large Language Models

*Daniil Gurgurov; Josef van Genabith; Simon Ostermann*

***

> ### ðŸ“Œ Quick Facts
>
> *   **Parameter Efficiency:** Updates <1% of total model parameters.
> *   **Models Tested:** Llama-3.1-8B and Mistral-Nemo-12B.
> *   **Scope:** Validated on 12 mid- and low-resource languages; resources released for 100+ languages.
> *   **Core Mechanism:** Sparse Subnetwork Fine-tuning using Language Activation Probability Entropy (LAPE).
> *   **Quality Score:** 9/10

***

## Executive Summary

Large Language Models (LLMs) suffer from a significant performance disparity between high-resource languages and underrepresented mid- to low-resource languages. Addressing this imbalance is crucial for global AI accessibility, yet standard adaptation methods present difficult trade-offs: full fine-tuning is computationally expensive and often leads to catastrophic forgetting (degrading the model's general capabilities), while popular Parameter-Efficient Fine-Tuning (PEFT) techniques like LoRA frequently fail to maximize performance for low-resource targets. This paper addresses the critical challenge of enhancing monolingual capabilities in underrepresented languages without compromising the model's existing broad knowledge base or incurring prohibitive computational costs.

The authors introduce a novel sparse subnetwork fine-tuning framework that targets specific, language-critical components within the model rather than updating the entire network. Technically, the method utilizes "Language Activation Probability Entropy" (LAPE) to identify "language-specific neurons," predominantly located within the Feed-Forward Network (FFN) layers, while Attention blocks remain frozen. Once identified, the framework isolates a dedicated subnetwork consisting solely of the weights associated with these neurons and performs direct weight updates exclusively on this sparse subset. This approach ensures that modifications are highly targeted to the linguistic nuances of the target language, leaving the vast majority of the model parameters untouched.

In empirical testing across 12 mid- and low-resource languages using state-of-the-art architectures (Llama-3.1-8B and Mistral-Nemo-12B), the proposed method consistently outperformed standard baselines, including full fine-tuning, FFN-only fine-tuning, LoRA, and random subset fine-tuning. Remarkably, these performance gains were achieved by updating less than 1% of the model's total parameters. The study confirmed that the approach effectively preserves the model's general-purpose performance, thereby avoiding catastrophic forgetting. Additionally, the framework demonstrated improved training dynamics, better cross-lingual representational alignment, and systematic weight updates, successfully addressing saturation issues observed in previous adaptation efforts.

This research significantly advances the field of efficient model adaptation by demonstrating that sparse subnetwork fine-tuning is superior to dense updates and adapter-based methods (like LoRA) for low-resource language enhancement. The finding that updating less than 1% of weights can outperform full fine-tuning suggests that current models possess highly specialized, underutilized capacity for specific languages. To foster further research, the authors have released language-specific neuron identifications for over 100 languages alongside their adaptation pipeline. This contribution provides the community with a cost-effective, reproducible pathway to adapt state-of-the-art models to diverse languages, potentially bridging the digital divide for underrepresented linguistic populations.

***

## Key Findings

*   **Superior Performance:** The proposed sparse subnetwork framework consistently outperforms standard adaptation baselinesâ€”including full fine-tuning, FFN-only fine-tuning, LoRA, and random subset fine-tuningâ€”across 12 mid- and low-resource languages.
*   **High Parameter Efficiency:** The method achieves these performance improvements while updating only **up to 1%** of the model's total parameters, offering a highly parameter-efficient solution.
*   **Generalization Preservation:** Targeted fine-tuning of language-specific subnetworks preserves the general-purpose performance of LLMs (Llama-3.1-8B and Mistral-Nemo-12B) while significantly enhancing monolingual capabilities in underrepresented languages.
*   **Optimized Training Dynamics:** Beyond accuracy, the approach yields improved training dynamics, better cross-lingual representational alignment, and systematic changes in weight updates.

***

## Methodology

The researchers introduce a framework for enhancing underrepresented languages by identifying and fine-tuning specific subnetworks within Large Language Models. The process involves three distinct stages:

1.  **Identification**
    Utilizing **Language Activation Probability Entropy (LAPE)** to pinpoint "language-specific neurons" within the model.
2.  **Isolation**
    Defining a dedicated subnetwork consisting only of the weights associated with these identified neurons.
3.  **Targeted Fine-tuning**
    Training exclusively on this sparse subnetwork using target-language data, leaving the vast majority of the model parameters frozen.

***

## Technical Details

| Aspect | Specification |
| :--- | :--- |
| **Target Architecture** | Feed-Forward Network (FFN) layers only. |
| **Frozen Components** | Attention blocks remain frozen. |
| **Update Mechanism** | Direct weight updates (no adapters or LoRA modules). |
| **Selection Metric** | Language Activation Probability Entropy (LAPE). |
| **Parameter Update Ratio** | < 1% of total model parameters. |
| **Validation Models** | Llama-3.1-8B and Mistral-Nemo-12B. |

***

## Contributions

*   **A Novel Adaptation Framework:** A method for improving low-resource language performance without degrading general capabilities, leveraging sparse subnetwork fine-tuning rather than dense updates.
*   **Empirical Validation:** Comprehensive evidence demonstrating that updating less than 1% of parameters via this method is superior to full fine-tuning and popular parameter-efficient techniques (like LoRA) for this specific task.
*   **Open Resources:** The release of language-specific neuron identifications for over 100 languages and the adaptation pipeline, providing a cost-effective, reproducible pathway for adapting state-of-the-art models to diverse languages.

***

## Performance Results

The proposed method outperforms standard baselines including Full fine-tuning, FFN-only fine-tuning, LoRA, and Random subset fine-tuning across 12 mid- and low-resource languages.

*   **Performance:** Consistently higher accuracy than all tested baselines.
*   **Memory:** Preserves the model's general-purpose performance (avoiding catastrophic forgetting) while enhancing monolingual capabilities.
*   **Dynamics:** The approach demonstrated improved training dynamics, better cross-lingual representational alignment, and systematic changes in weight updates.
*   **Saturation:** It addresses saturation issues seen in prior work and claims superior parameter efficiency compared to LoRA-based adaptations.

***

**Report Metadata**
*   **Quality Score:** 9/10
*   **References:** 40 citations