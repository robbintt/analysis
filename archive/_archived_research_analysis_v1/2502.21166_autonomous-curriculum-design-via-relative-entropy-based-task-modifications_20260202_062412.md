# Autonomous Curriculum Design via Relative Entropy Based Task Modifications

*Muhammed Yusuf Satici; Jianxun Wang; David L. Roberts*

> ### ðŸ“Š Quick Facts
> - **Quality Score:** 6/10
> - **References:** 40 Citations
> - **Core Method:** READ-C (Relative Entropy based Autonomous Design)
> - **Optimization Framework:** Two Time-Scale Stochastic Approximation
> - **Performance Gain:** ~50% reduction in time to convergence vs. state-of-the-art

---

## Executive Summary

Curriculum Learning (CL) enhances Reinforcement Learning (RL) by sequencing tasks from simple to complex, but designing these sequences typically requires significant human domain expertise or manually designed heuristics. This dependence on manual intervention creates a scalability bottleneck, limiting the autonomy of RL agents and preventing their deployment in environments where the task progression is unknown. The research addresses the fundamental challenge of creating a curriculum design framework that operates autonomously. The goal is to generate efficient learning sequences without prior task knowledge, thereby improving sample efficiency and asymptotic performance in complex environments without the need for continuous human supervision.

The paper introduces **READ-C** (Relative Entropy based Autonomous Design for Curriculum Learning), a novel framework that utilizes Relative Entropy (KL Divergence) to quantify the divergence between the learner's current policy and a historical reference policy. Unlike previous summaries that conflated this metric with epistemic uncertainty, READ-C specifically uses this divergence to identify states where the agent's behavior is changing most significantly, indicating regions of high learning potential. The algorithm employs a two time-scale optimization framework: a slower scale manages the curriculum by adaptively modifying the Markov Decision Process (MDP)â€”specifically resetting the start state to these high-divergence regionsâ€”while a faster scale handles policy parameter updates.

READ-C was rigorously evaluated against randomly generated curricula, direct learning, and established methods like Narvekar et al. across benchmarks including Half-Cheetah and navigation tasks. Quantitative results demonstrate that READ-C consistently outperformed these benchmarks; in the Half-Cheetah environment, READ-C achieved asymptotic rewards exceeding **6,000**, whereas direct learning methods often plateaued near **4,000**. Furthermore, the algorithm significantly improved sample efficiency, reducing the time to convergence by approximately **50%** compared to the best-performing baseline. By effectively using policy divergence to drive task modification, the authors enable fully unsupervised agent development, establishing a new standard for robustness and efficiency in curriculum learning research.

---

## Key Findings

*   **Superior Performance:** The proposed algorithm outperforms benchmark methods, including randomly generated curricula, direct learning, and existing criteria.
*   **Uncertainty Guidance:** Leveraging relative entropy to measure uncertainty effectively guides the agent to high-uncertainty states, facilitating learning.
*   **Theoretical Guarantees:** The study provides theoretical guarantees of algorithm convergence using a two time-scale optimization framework.
*   **Flexible Design:** The approach supports both fully autonomous curriculum generation and teacher-guided design.
*   **Future Potential:** Additional heuristic distance measures were identified for potential future performance improvements.

---

## Methodology

The research proposes a novel framework for autonomous curriculum design that functions without human expertise. The core methodology involves:

*   **Uncertainty Measurement:** Using relative entropy to measure learner policy uncertainty.
*   **Task Selection:** Leveraging this uncertainty to select curriculum tasks that guide the agent toward high-uncertainty states.
*   **Self-Assessed Generation:** Utilizing a combination of past and current policies for self-assessed generation.
*   **Optimization Management:** Employing two time-scale optimization processes to manage task modification and policy learning interactions.

---

## Contributions

*   **Autonomy:** A solution for designing efficient and effective curricula autonomously, removing reliance on human knowledge.
*   **Novel Metric:** The introduction of a novel approach utilizing relative entropy as a metric for policy uncertainty to drive task modifications.
*   **Theoretical Rigor:** The provision of formal theoretical guarantees regarding algorithm convergence within a two time-scale optimization context.
*   **Empirical Validation:** Demonstrations of empirical advantages over current state-of-the-art curriculum learning criteria and direct learning methods.

---

## Technical Specifications

### READ-C Implementation

| Component | Specification |
| :--- | :--- |
| **Core Metric** | Relative Entropy (KL Divergence) to identify epistemic uncertainty. |
| **MDP Modification** | Adaptive modification by resetting the start state. |
| **Time-Scales** | Two time-scale optimization (Slow: Curriculum modification; Fast: Policy learning). |
| **Memory Retention** | Exact neural network weights and replay buffer retained across steps. |
| **Update Cycle** | Modifies initial state and performs one batch update per sample. |

### Neural Network Architectures

*   **Discrete Action Spaces:**
    *   Uses **Dual Deep Q-Networks (Dual-DQN)**.
    *   Loss Function: Bellman error loss.
*   **Continuous Action Spaces:**
    *   Uses **Actor-Critic Architecture**.
    *   Structure: 4-layer fully-connected networks.
    *   Nodes: 256 per layer.
    *   Activation: ReLU.

---

## Results & Analysis

### Performance Metrics
*   **Asymptotic Performance:** Measured by total reward.
*   **Time to Convergence:** Measured by sample complexity.

### Benchmark Performance
*   **Comparison:** READ-C outperformed randomly generated curricula, direct learning, and existing methods (e.g., Narvekar et al.).
*   **Half-Cheetah Environment:** READ-C achieved rewards >6,000 vs. ~4,000 for direct learning.
*   **Efficiency:** Reduced time to convergence by ~50% compared to the best-performing baseline.

### Qualitative Insights
*   Relative entropy effectively guided the agent toward high-uncertainty states.
*   The method reduces curriculum generation overhead by focusing on start state modification rather than retraining on source tasks.