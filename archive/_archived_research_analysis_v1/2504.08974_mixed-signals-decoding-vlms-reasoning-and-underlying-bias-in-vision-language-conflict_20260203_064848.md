---
title: 'Mixed Signals: Decoding VLMs'' Reasoning and Underlying Bias in Vision-Language
  Conflict'
arxiv_id: '2504.08974'
source_url: https://arxiv.org/abs/2504.08974
generated_at: '2026-02-03T06:48:48'
quality_score: 9
citation_count: 15
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Mixed Signals: Decoding VLMs' Reasoning and Underlying Bias in Vision-Language Conflict

*Pouya Pezeshkpour; Moin Aminnaseri; Estevam Hruschka*

***

> ### ðŸ“Š Quick Facts
>
> *   **Focus:** Modality Bias in Vision-Language Models (VLMs)
> *   **Models Analyzed:** 5 State-of-the-Art VLMs
> *   **Response Variance:** +56.8% (Text-favored) to -74.4% (Image-favored)
> *   **Novel Benchmarks:** 5 (Math, Science, Visual Description)
> *   **Quality Score:** 9/10
> *   **Citations:** 15

***

## Executive Summary

Vision-Language Models (VLMs) have achieved significant proficiency in integrating visual and textual information, yet their internal reasoning processes remain opaque when these modalities provide conflicting signals. This paper addresses the critical issue of "modality bias"â€”specifically, determining whether VLMs prioritize visual or textual cues when the two contradict and how this priority shifts based on query complexity. Understanding this bias is essential for evaluating model reliability, as blind adherence to one modality over another can lead to catastrophic errors in safety-critical applications where inputs are often noisy or inconsistent.

The authors introduce a rigorous diagnostic framework centered on the construction of five novel benchmarks spanning mathematics, science, and visual description domains. These benchmarks utilize carefully crafted "conflicting pairs" (where the correct answer from the image contradicts the answer from the text) to force the model to reveal its prioritization logic. The empirical analysis reveals that VLMs exhibit a **dynamic, non-static reasoning bias** that correlates with both model scale and query complexity. Simpler queries tend to favor textual cues, while complex queries trigger a shift toward visual reliance. The findings highlight the limitations of current prompt-based mitigation techniques, suggesting that resolving modality conflict requires more fundamental shifts in model training or architecture.

***

## Key Findings

*   **Dynamic Reasoning Bias:** VLMs do not statically favor one modality. Instead, they favor textual cues in simpler queries but shift to visual cues as complexity increases.
*   **Scale Correlation:** The intensity and direction of bias correlate significantly with model scale, with response differences ranging from **+56.8%** to **-74.4%**.
*   **Inconsistent Mitigation:** Strategies such as prompt modifications and task decomposition show inconsistent effectiveness, which is tied to the model's performance level and the specific modality involved.

***

## Methodology

The research utilized a comprehensive three-pronged approach to investigate modality conflict:

1.  **Dataset Construction:**
    *   Involved the creation of five datasets containing mismatched image-text pairs.
    *   Spans multiple domains including mathematics and science to ensure broad applicability.

2.  **Bias Analysis:**
    *   Evaluated VLMs on conflicting pairs to measure the flow of information.
    *   Quantified bias based on the divergence between image-favored and text-favored responses.

3.  **Intervention Testing:**
    *   Compared various strategies including prompt modifications, explicit instructions, and task decomposition to assess their ability to correct bias.

***

## Technical Details

### Conflict Simulation & Quantification
*   **Mechanism:** The study simulates conflicting multimodal cues using mismatched image-text pairs ($Query\ Q$, $Image\ I$, $Modified\ Text\ T'$).
*   **Ideal Behavior:** The ideal model behavior is detecting discrepancies rather than committing to the image answer ($A$) or text answer ($A'$).
*   **Metric:** Bias is quantified as the difference between image-favored and text-favored response percentages.

### Benchmark Construction
Five novel benchmarks were constructed based on VSR and Isobench, utilizing proxies such as:
*   Edge count
*   Expression length
*   Polynomial degree
*   Manual difficulty labels

**Benchmark Domains:**
1.  Graph Connectivity
2.  Function Convexity
3.  Polynomial Roots
4.  Physics/Chemistry
5.  Visual Description

### Mitigation Strategies
Three distinct mitigation strategies were proposed and tested:
1.  **Verbalized Mitigation:** Prompting the model to explicitly identify mismatches.
2.  **Chain-of-Thought (CoT) Mitigation:** A single-pass process requiring the model to analyze the image and text separately before comparing.
3.  **Decomposed Mitigation:** A multi-stage execution involving image-only, text-only, and combined runs.

***

## Results

*   **Quantitative Variance:** Analysis showed significant response variance ranging from **+56.8%** to **-74.4%**, confirming that bias intensity and direction correlate with model scale.
*   **Complexity Shift:**
    *   **Simple Queries:** Models demonstrated a reliance on textual cues (e.g., degree-2 polynomials).
    *   **Complex Queries:** Bias shifted toward visual reliance (e.g., degree-3 polynomials).
*   **Mitigation Efficacy:** Proposed strategies demonstrated inconsistent effectiveness. Success was dependent on the model's inherent performance level and the specific modality generating the conflict.
*   **Scope:** Experiments were conducted on five state-of-the-art VLMs across various scales to ensure generalizability.

***

## Contributions

*   **Diagnostic Benchmarks:** Development of five diagnostic benchmarks featuring controlled conflicts to probe VLM reasoning mechanisms.
*   **Empirical Characterization:** Provided the first empirical characterization of reasoning dynamics, proving that VLM information priority depends on query complexity and model scale.
*   **Comparative Assessment:** Conducted a comparative assessment of three bias-mitigation techniques, analyzing the impact of performance and modality specificity on the ability to correct reasoning errors.

***

**Quality Score:** 9/10 | **References:** 15 citations