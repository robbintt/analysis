# SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model

*Loubna Ben Allal; Anton Lozhkov; Elie Bakouch; Gabriel Mart√≠n Bl√°zquez; Guilherme Penedo; Lewis Tunstall; Andr√©s Marafioti; Hynek Kydl√≠ƒçek; Agust√≠n Piqueres Lajar√≠n; Vaibhav Srivastav; Joshua Lochner; Caleb Fahlgren; Xuan-Son Nguyen; Cl√©mentine Fourrier; Ben Burtenshaw; Hugo Larcher; Haojun Zhao; Cyril Zakka; Mathieu Morlon; Colin Raffel; Leandro von Werra; Thomas Wolf*

---

## üìä Quick Facts

| Metric | Value |
| :--- | :--- |
| **Model Name** | SmolLM2 |
| **Parameters** | 1.7 Billion |
| **Training Tokens** | ~11 Trillion |
| **Key Datasets** | FineMath, Stack-Edu, SmolTalk |
| **Competitor Performance** | Outperforms Qwen2.5-1.5B & Llama3.2-1B |
| **Quality Score** | 8/10 |

---

## üìù Executive Summary

### Problem
Small Language Models (SLMs) often face limitations in complex reasoning, coding, and knowledge retention, particularly when deployed in resource-constrained environments. While scaling up parameters typically yields better results, the computational cost makes this infeasible for edge devices or on-premise deployments. This research aims to maximize the capabilities of a 1.7B parameter model without increasing its physical size.

### Innovation
The core innovation lies in a **data-centric methodology** featuring aggressive overtraining on approximately 11 trillion tokens and a dynamic curriculum learning strategy. Instead of static dataset creation, the authors employed **manual iterative refinement** using ablation studies. This allowed for dynamic adjustments of data mixing rates based on performance at intermediate training stages. Three specialized, high-quality datasets were created to bridge capability gaps:
*   **FineMath:** For mathematical reasoning.
*   **SmolTalk:** For instruction following.
*   **Stack-Edu:** For code (derived via StarEncoder-based classifiers).

### Results
SmolLM2 establishes a new state-of-the-art for the 1.7B parameter class, surpassing competitors like Qwen2.5-1.5B and Llama3.2-1B. The data-centric approach delivered major gains in coding benchmarks on MultiPL-E:
*   **Python:** 25.6 (+4.9)
*   **C++:** 24.8 (+8.1)
*   **JavaScript:** 22.4 (+4.2)
*   **Java:** 22.7 (+5.1)
Additionally, custom FineMath subsets were found to be superior to existing resources like Infi-WebMath and OpenWebMath.

### Impact
By open-sourcing both the model weights and the curated datasets, this work provides a reproducible framework for efficient model training in low-resource settings. It validates that intensive data curation and aggressive overtraining are superior to raw parameter scaling, shifting the research focus toward data quality and dynamic training strategies.

---

## üîë Key Findings

*   **State-of-the-Art Performance:** SmolLM2 outperforms leading competitors such as Qwen2.5-1.5B and Llama3.2-1B in the 1.7B parameter category.
*   **Overtraining Benefits:** Small models gain significantly stronger performance through overtraining on approximately **11 trillion tokens**.
*   **Specialized Data Importance:** Incorporating high-quality specialized data in math, code, and instruction-following is critical.
*   **Custom Datasets:** The use of custom datasets‚Äî**FineMath**, **Stack-Edu**, and **SmolTalk**‚Äîplayed a pivotal role in success.
*   **Optimized Mixing Strategy:** An optimized data mixing strategy using manual iterative refinement leads to better design decisions than static methods.

---

## üõ†Ô∏è Methodology

The research utilized a **multi-stage training process** anchored by a data-centric strategy. The approach combines general web text with specialized data streams and features:

1.  **Iterative Ablation:** A manual refinement loop that analyzes performance at previous training stages.
2.  **Dynamic Updates:** Data mixing rates are updated dynamically for subsequent stages based on analysis.
3.  **Gap Addressal:** Creation and integration of three custom datasets (FineMath, Stack-Edu, SmolTalk) to address specific gaps in existing resources.

---

## ‚öôÔ∏è Technical Details

### Model Architecture & Training
*   **Parameters:** 1.7 Billion
*   **Training Data:** ~11 Trillion tokens
*   **Approach:** Data-centric focus on high-quality specialized data and manual iterative refinement.

### Hyperparameters
*   **Scheduler:** WSD (Weight Standardized Decay)
*   **Warmup Steps:** 2,000
*   **Peak Learning Rate:** $5.0 \times 10^{-4}$
*   **Decay:** 10%

### Stack-Edu Dataset Construction
The model utilizes a custom dataset derived from StarCoder2Data:
*   **Selection:** Filtered to select the 15 largest programming languages.
*   **Filtration Strategy:** Classifier-based strategy adapted from FineWeb-Edu using the **StarEncoder** model.
*   **Classifier Training:** Classifiers were trained on 500,000 samples with synthetic annotations generated by Llama3-70B-Instruct.

---

## üìà Performance Results

### Benchmark Improvements (MultiPL-E)
The specialized Stack-Edu filtering significantly improved coding performance while reducing overall data volume.

| Language | SmolLM2 Score | Improvement |
| :--- | :--- | :--- |
| **Python** | 25.6 | +4.9 |
| **C++** | 24.8 | +8.1 |
| **JavaScript**| 22.4 | +4.2 |
| **Java** | 22.7 | +5.1 |

### Ablation Studies
*   **Math Performance:** FineMath4+ and FineMath3+ subsets were found to be superior to Infi-WebMath and OpenWebMath.
*   **Classification Accuracy:** Language-specific classifiers achieved an **F1 score above 0.7** for most languages.
*   **Data Mixing:** Evaluations were conducted on MMLU, ARC, and OpenBookQA to validate mixing strategies.

---

## üéÅ Contributions

*   **Model Release:** SmolLM2, a highly optimized open-source 1.7B parameter model suitable for resource-constrained environments.
*   **New Datasets:** Introduction of three new, high-quality specialized datasets: **FineMath**, **Stack-Edu**, and **SmolTalk**.
*   **Open Science:** Comprehensive release of both model weights and prepared datasets to support future research.
*   **Validation of Overtraining:** Validated that intensive, data-centric overtraining (11T tokens) is a viable method for maximizing small parameter architectures.

---

**Document Quality Score:** 8/10  
**References:** 40 citations