---
title: Safe Planning and Policy Optimization via World Model Learning
arxiv_id: '2506.04828'
source_url: https://arxiv.org/abs/2506.04828
generated_at: '2026-02-03T13:41:38'
quality_score: 6
citation_count: 33
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Safe Planning and Policy Optimization via World Model Learning

*Artem Latyshev; Gregory Gorbov; Aleksandr I. Panov*

---

> ### üìä Quick Facts
> * **Framework Name:** SPOWL (Safe Planning and Optimization via World Model Learning)
> * **Quality Score:** 6/10
> * **References:** 33 Citations
> * **Evaluation Benchmarks:** SafetyGymnasium (Point, Car, Ant, Doggo agents)
> * **Baselines Outperformed:** SafeDreamer, MBPPOL, MBRCE, PPO-Lagrangian, CPO, SAC-Lagrangian

---

## Executive Summary

Safe Reinforcement Learning (RL) in continuous control faces a dual challenge: the risks associated with epistemic uncertainty in imperfect world models and the "objective mismatch" inherent in traditional model-based approaches. In safety-critical environments, relying on inaccurate learned models can lead an agent to execute plausible yet catastrophic plans. Furthermore, optimizing a reward function based on a model's predictions often fails to align with optimizing the true environment's reward, a discrepancy known as objective mismatch. Existing methodologies typically rely on static safety thresholds; these fail to adapt to an agent's evolving competency, resulting in performance that is either overly conservative or dangerously aggressive when the model errors are significant.

This paper introduces **SPOWL** (Safe Planning and Optimization via World Model Learning), a hybrid framework that integrates Constrained Markov Decision Processes (CMDP) with an implicit world model to resolve these issues. Technically, SPOWL addresses objective mismatch by utilizing an implicit world model that performs planning directly in latent space without decoding to pixel space. This ensures that the optimization target aligns strictly with the true environment dynamics rather than artifacts of the model's reconstruction capabilities. The framework employs an adaptive switching mechanism to mitigate epistemic uncertainty, dynamically toggling between a learned Safe Policy (trained via constrained optimization) and Model-Based Planning based on estimated model error. Additionally, SPOWL replaces static safety limits with a cost-value-function-driven mechanism, allowing dynamic safety thresholds that adjust to the agent's improving capabilities.

Evaluated on the SafetyGymnasium benchmark using Point, Car, Ant, and Doggo agents, SPOWL demonstrated superior performance and safety compared to state-of-the-art baselines. In complex tasks such as the Doggo environment, SPOWL achieved near-zero cumulative safety violations (costs approaching 0.0), whereas competing methods like SafeDreamer incurred substantial violation costs. SPOWL also maintained high task completion rates and significantly outperformed model-free methods in sample efficiency. This research represents a pivotal advancement in the reliability of model-based RL for real-world applications where safety is paramount, offering a robust framework that enables agents to progressively take calculated risks as their competency evolves.

---

## üîë Key Findings

*   **Significant Improvement:** The proposed framework achieves significant improvements over non-adaptive methods by jointly optimizing task performance and safety.
*   **Robust Performance:** The method demonstrates robust performance on diverse safety-critical continuous control tasks, outperforming existing state-of-the-art approaches.
*   **Objective Mismatch Resolution:** Utilizing an implicit world model effectively resolves the objective mismatch problem associated with traditional model-based Reinforcement Learning (RL).
*   **Dynamic Safety Thresholds:** The implementation of dynamic safety thresholds allows the agent to consistently select actions that surpass standard safe policy suggestions as the agent's capabilities evolve.

---

## ‚öôÔ∏è Methodology

*   **Framework Type:** A novel model-based Reinforcement Learning (RL) framework designed to simultaneously optimize for both task completion and safety adherence.
*   **Error Handling:** An adaptive mechanism is employed to dynamically switch between model-based planning and direct policy execution to mitigate the risks of inherent model inaccuracies.
*   **Model Architecture:** The approach utilizes an implicit world model to correct the objective mismatch found in traditional model-based methods.
*   **Safety Mechanism:** The framework employs dynamic safety thresholds that adapt to the agent's evolving capabilities, ensuring that selected actions outperform the safe policy suggestions.

---

## üìù Contributions

*   **Adaptive Safety Mechanism:** Introduction of a strategy that handles world model errors in safety-critical settings by dynamically toggling between planning and policy execution.
*   **Objective Alignment:** Resolution of the objective mismatch problem in model-based RL through the specific application of an implicit world model.
*   **Adaptive Thresholding:** Development of dynamic safety thresholds that evolve with the agent, enabling a shift from simply meeting safety constraints to optimizing the balance between safety and performance.

---

## üî¨ Technical Details

**Framework Architecture**
SPOWL (Safe Planning and Optimization via World Model Learning) employs a hybrid framework that dynamically switches between two components:
1.  **Safe Policy:** Trained via constrained optimization using CMDP frameworks.
2.  **Model-Based Planning:** Used to mitigate world model inaccuracies.

**Core Innovations**
*   **Implicit World Model:** Utilized for planning directly in latent space without decoding, specifically designed to address the objective mismatch problem.
*   **Dynamic Thresholds:** Replaces fixed safety thresholds with a cost-value-function-driven mechanism. This allows for progressive risk-taking as the agent learns.
*   **Theoretical Foundation:** Relies on Constrained Markov Decision Processes (CMDP) and addresses Epistemic Uncertainty regarding synthetic data generation.

---

## üìà Results

**Evaluation Setup**
*   **Environments:** SafetyGymnasium.
*   **Agents:** Point, Car, Ant, and Doggo.

**Performance Outcomes**
*   **Safety & Performance:** Achieved near-zero safety violations while maintaining high task performance.
*   **Benchmark Comparison:** Outperformed state-of-the-art baselines including SafeDreamer, MBPPOL, and MBRCE.
*   **Sample Efficiency:** Demonstrated improved sample efficiency compared to model-free methods (PPO-Lagrangian, CPO, SAC-Lagrangian).
*   **Complex Task Handling:** Better handling of large continuous spaces than SafeDreamer and includes memory mechanisms for complex tasks unlike MBRCE.
*   **Adaptive Behavior:** Dynamic safety thresholds enabled the agent to consistently select actions surpassing standard safe policy suggestions as capabilities improved.

---

**References:** 33 citations