# What is Adversarial Training for Diffusion Models?

*Briglia Maria Rosaria; Mujtaba Hussain Mirza; Giuseppe Lisanti; Iacopo Masi*

---

> **### Quick Facts**
> | Metric | Detail |
> | :--- | :--- |
> | **Quality Score** | 9/10 |
> | **References** | 40 Citations |
> | **Core Concept** | Equivariance vs. Invariance |
> | **Key Datasets** | CIFAR-10, CelebA, LSUN Bedroom, Smithsonian Butterflies |
> | **Method Name** | Robustadv |

---

## Executive Summary

Diffusion models, while achieving state-of-the-art performance in generative tasks, exhibit significant vulnerability to input perturbations, noise, and adversarial attacks. A primary source of this instability lies in the misapplication of robustness techniques designed for discriminative classifiers; specifically, standard adversarial training emphasizes *invariance*, which is theoretically ill-suited for the generative dynamics of diffusion models. This misalignment causes diffusion trajectories to become unstable when processing corrupted or out-of-distribution data, leading to high reconstruction errors and mode collapse. Addressing this is critical for deploying generative models in safety-sensitive or real-world environments where data integrity cannot be guaranteed.

The authors introduce a fundamental theoretical shift, arguing that adversarial training for diffusion models must enforce **equivariance** rather than invariance. Technically, this means that if an intermediate input $x_t$ is perturbed by a vector $\delta$, the model’s predicted noise output must shift by exactly $\delta$ to maintain alignment with the data distribution. The method implements this via an equivariance regularization term that minimizes $\|\epsilon_\theta(x_t + \delta, t) - [\epsilon + \delta]\|^2_2$. The training pipeline injects perturbations—ranging from random noise to adversarial attacks (FGSM with random restarts)—controlled by an Adaptive Perturbation Ray, $r_\delta(t) = (\sigma_{max} - \sigma_t)^\omega + \beta$, which dynamically adjusts perturbation magnitude throughout the diffusion timesteps to smooth the score vector fields.

The proposed **"Robustadv"** method was rigorously evaluated on synthetic datasets, CIFAR-10, CelebA, and LSUN Bedroom, demonstrating superior resilience under severe degradation. In controlled experiments, standard invariance training failed catastrophically on CIFAR-10, achieving an FID of 356.9, whereas the equivariant approach maintained robust performance. On the Smithsonian Butterflies dataset with 90% corruption, the proposed method significantly outperformed standard DDPM and invariance baselines, yielding lower reconstruction error and higher Peak Signal-to-Noise Ratio (PSNR). Trajectory stability tests further confirmed that the method converges smoothly to the data centroid without divergence, indicating a stable diffusion flow even under adversarial pressure.

This work makes a substantial contribution to the theoretical foundation of adversarial training in the generative domain by clearly distinguishing the requirements for classifiers (invariance) versus diffusion models (equivariance). Beyond immediate robustness against attacks and noise, the method effectively prevents data memorization and handles extreme data variability without requiring specific assumptions about the underlying noise distribution. By validating this approach from low-dimensional proof-of-concepts to complex standard benchmarks, the authors provide a reliable, generalizable framework for improving the stability and trustworthiness of diffusion models in unclean or adversarial environments.

---

## Key Findings

*   **Equivariance vs. Invariance:** Adversarial Training (AT) in diffusion models requires **equivariance** rather than invariance to maintain alignment with the data distribution.
*   **Improved Smoothness:** AT enforces smoothness in the diffusion flow, which significantly improves the handling of outliers and corrupted data.
*   **High Reliability:** The method demonstrates high reliability on benchmarks like **CIFAR-10**, **CelebA**, and **LSUN Bedroom** even under severe noise and adversarial attacks.
*   **Assumption-Free:** The approach is effective without requiring specific assumptions about the underlying noise model.
*   **Memorization Prevention:** Beyond robustness, the method prevents memorization and handles extreme data variability.

---

## Methodology

The researchers proposed integrating adversarial training directly into the standard diffusion model training pipeline.

*   **Process:** The method involves injecting perturbations, specifically random noise or adversarial noise, during the training process.
*   **Objective:** This approach enforces equivariance and smoothness in the diffusion flow without relying on predefined assumptions about the noise model.
*   **Testing Strategy:** The method was rigorously tested using:
    *   Proof-of-concept datasets with known distributions (low- and high-dimensional spaces).
    *   Standard benchmark datasets (CIFAR-10, CelebA, etc.).

---

## Technical Details

The study proposes a specific equivariance constraint for Diffusion Models (DMs). Unlike classifiers which require invariance, DMs must shift their predicted noise output by $\delta$ when the input $x_t$ is perturbed by $\delta$.

### Core Formulations

*   **Equivariance Objective:**
    $$ \|\epsilon_\theta(x_t + \delta, t) - [\epsilon + \delta]\|^2_2 $$
    This minimization smooths diffusion flow trajectories.

*   **Adaptive Perturbation Ray:**
    $$ r_\delta(t) = (\sigma_{max} - \sigma_t)^\omega + \beta $$
    This function modifies the forward process to inject perturbations, controlling magnitude time-dependently using parameters $\omega \geq 1$ and $\beta$.

### Training Pipeline

*   **Loss Function:** Training combines the standard diffusion loss with an explicit equivariance regularization term.
*   **Perturbation Sources:** Utilizes both random perturbations and adversarial perturbations (FGSM with random start).

---

## Results

The empirical evaluation highlighted significant advantages over standard methods:

*   **CIFAR-10:** Standard invariance training failed dramatically, achieving a poor **FID of 356.9**. The proposed "Robustadv" method significantly outperformed this baseline.
*   **Synthetic Data:** Tested on Synthetic 3D data (*Oblique-plane*, *3-gaussians*) and linearized Smithsonian Butterflies (90% corruption).
*   **Image Quality:** The proposed method yielded lower reconstruction error and higher **PSNR** on corrupted data compared to standard DDPM and Invariance baselines.
*   **Trajectory Stability:** Tests demonstrated that the proposed method converges to the data centroid without diverging, resulting in smoother score vector fields compared to standard DDPM.

---

## Contributions

*   **Theoretical Clarification:** The paper clarifies the theoretical distinction of AT in generative models, defining the requirement for **equivariance** rather than invariance.
*   **Novel Technique:** It introduces a training technique that seamlessly integrates AT or randomized smoothing without requiring assumptions about the noise model.
*   **Validation Framework:** The work provides a comprehensive validation framework, progressing from controlled proof-of-concept datasets to complex standard benchmarks to prove efficacy against real-world data degradation and adversarial threats.