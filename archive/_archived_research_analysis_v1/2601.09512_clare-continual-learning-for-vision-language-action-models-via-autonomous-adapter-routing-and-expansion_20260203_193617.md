---
title: 'CLARE: Continual Learning for Vision-Language-Action Models via Autonomous
  Adapter Routing and Expansion'
arxiv_id: '2601.09512'
source_url: https://arxiv.org/abs/2601.09512
generated_at: '2026-02-03T19:36:17'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# CLARE: Continual Learning for Vision-Language-Action Models via Autonomous Adapter Routing and Expansion

*Ralf Römer; Yi Zhang; Angela P. Schoellig*

---

> **Quick Facts**
>
> *   **Framework:** CLARE (Continual Learning via Adapter Routing and Expansion)
> *   **Approach:** Exemplar-free, Task-agnostic inference
> *   **Benchmark:** LIBERO (100 rollouts/task, 50 configurations)
> *   **Best Performance (DiT-Dec):** AUC `75.11 ± 1.31`
> *   **Key Strategy:** Autoencoder-based dynamic routing & Autonomous adapter expansion

---

## Executive Summary

Continual learning in robotics, particularly for Vision-Language-Action (VLA) models, faces the critical challenge of **catastrophic forgetting**, where the acquisition of new skills degrades performance on previously mastered tasks. Most existing state-of-the-art methods mitigate this by relying on experience replay (storing subsets of old data, or exemplars) or requiring explicit task identifiers during deployment. These requirements are often impractical for real-world robotic applications where data privacy, storage constraints, and unknown task boundaries are prevalent.

This paper introduces **CLARE**, a general, parameter-efficient framework designed to make VLA models adaptable through **exemplar-free continual learning**. The core technical innovation lies in the integration of lightweight modular adapters into selected feedforward network layers, combined with an autonomous expansion strategy. The system utilizes layer-wise feature similarity to determine exactly when and where to add new adapters, expanding model capacity only when feature statistics indicate substantial novelty while keeping pre-trained parameters frozen. Furthermore, CLARE employs an autoencoder-based dynamic routing mechanism that autonomously selects the relevant adapters for a given input during inference, effectively removing the need for oracle task labels.

Extensive experiments on the LIBERO benchmark validate that CLARE significantly outperforms existing methods, including those relying on experience replay. Key ablation studies demonstrated that expanding the observation encoder is vastly superior to expanding the decoder, yielding 30–40% higher absolute performance. This research establishes a new standard for lifelong learning in embodied AI, highlighting the critical importance of strategic capacity allocation within the observation encoder for maintaining high performance across complex, long task sequences.

---

## Key Findings

*   **High Performance & Retention:** CLARE achieves high performance on new robotic manipulation tasks while effectively preventing catastrophic forgetting of previously learned tasks.
*   **Exemplar-Free Superiority:** The proposed exemplar-free approach significantly outperforms existing state-of-the-art methods that rely on storing previous data (exemplars).
*   **Task-Agnostic Operation:** The framework successfully operates without requiring task identifiers during deployment, utilizing a dynamic routing mechanism instead.
*   **Benchmark Validation:** Extensive experiments on the LIBERO benchmark validate the framework's ability to handle long task sequences and complex real-world adaptation requirements.

---

## Methodology

CLARE introduces a general, parameter-efficient framework designed for Vision-Language-Action (VLA) models.

1.  **Adapter Integration:** It operates by inserting lightweight modular adapters into selected feedforward layers.
2.  **Autonomous Expansion:** The learning process utilizes layer-wise feature similarity to guide the autonomous expansion of the model, adding capacity only where necessary to accommodate new tasks.
3.  **Dynamic Routing:** During deployment, the system employs an autoencoder-based routing mechanism to dynamically select and activate the most relevant adapters based on the input, removing the need for explicit task labels or external supervision.

---

## Contributions

*   **Exemplar-free Continual Learning:** A solution to the limitations of current robotics continual learning methods, specifically removing the need to store previous data (exemplars) or rely on task identifiers for deployment.
*   **Autonomous Model Expansion:** A mechanism that uses layer-wise feature similarity to determine exactly where and when to expand the model with new adapters, ensuring parameter efficiency.
*   **Dynamic Routing Architecture:** The development of an autoencoder-based routing system that allows for task-agnostic inference by dynamically activating specific pathways within the network.
*   **Benchmark Performance:** Empirical evidence demonstrating that a well-designed parameter-efficient architecture can surpass exemplar-based methods on the LIBERO benchmark, setting a new standard for continual learning in robotic manipulation.

---

## Technical Details

**Core Architecture:**
CLARE (Continual Learning via Adapter Routing and Expansion) is an exemplar-free framework designed to enable Vision-Language-Action (VLA) models to learn tasks sequentially without catastrophic forgetting.

**Implementation Strategy:**
*   **Adapter Injection:** Builds upon a pre-trained VLA model by injecting lightweight adapters into specific Feedforward Network (FFN) layers.
*   **Feature-Guided Expansion:** Features autonomous expansion, adding new adapters only when feature statistics indicate substantial novelty.
*   **Parameter Preservation:** Keeps existing pre-trained parameters frozen to retain prior knowledge.
*   **Task-Agnostic Inference:** Employs an autoencoder-based routing mechanism for dynamic, task-agnostic adapter selection without requiring oracle task identifiers.

**Key Ablation Insights:**
*   Ablation studies indicate that expanding the **observation encoder** is vastly superior to expanding the decoder, yielding **30–40% higher absolute performance**.

---

## Results

**Experimental Setup:**
*   **Benchmark:** LIBERO
*   **Configuration:** 100 rollouts per task, evaluated over 50 configurations, averaged across three random seeds.
*   **Metrics:** AUC (Area Under the Curve), FWT (Forward Transfer), and NBT (Negative Backward Transfer).
*   **Baselines:** SeqFFT, SeqLoRA, PackNet, Experience Replay, and LOTUS.

**Performance Metrics:**

*   **DiT-EncDec Backbone (Encoder Expansion):**
    *   **AUC:** `65.38 ± 2.68`
    *   **FWT:** `66.53 ± 2.18`
    *   **NBT:** `1.70 ± 1.20`
    *   *Note: Significantly outperformed decoder-only expansion (AUC: 28.99 ± 2.20).*

*   **DiT-Dec Backbone (Linear Projection Expansion):**
    *   **AUC:** `75.11 ± 1.31`

---

**Quality Score:** 9/10 | **References:** 40 citations