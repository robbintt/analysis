---
title: Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models
arxiv_id: '2502.15950'
source_url: https://arxiv.org/abs/2502.15950
generated_at: '2026-02-06T01:25:29'
quality_score: 9
citation_count: 28
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Optimizing Pre-Training Data Mixtures with Mixtures of Data Expert Models

*Lior Belenki; Alekh Agarwal; Tianze Shi; Kristina Toutanova*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Dataset** | SlimPajama |
| **Model Scale** | 70M to 1B parameters (Proxy: 280M) |
| **Optimization Framework** | Vizier Black-box Optimization |
| **Key Innovation** | Mixture of Data Experts (MDE) |
| **Compute Efficiency** | Reduced proxy models from 30-500 to just $k$ (number of domains) |
| **Quality Score** | **9/10** |
| **Citations** | 28 |

---

## Executive Summary

> Determining the optimal composition of pre-training data is a critical yet computationally expensive challenge in training Large Language Models (LLMs). The performance of LLMs is heavily dependent on the mixture ratios of various data domains (e.g., code, books, academic text), but identifying these ratios is difficult. Existing methods rely heavily on heuristics, which are often suboptimal, or brute-force search, which requires training thousands of full models and is computationally prohibitive. This paper addresses the need for a scalable, cost-effective optimization framework that can identify high-performing data mixtures without the overhead of exhaustive training runs.

The key innovation is the introduction of a Mixture of Data Experts (MDE) to efficiently approximate the cross-entropy loss of candidate data mixtures. Instead of training a full model for every potential mixture, the method trains $k$ separate "expert" models, each exclusively on a single domain. The ensemble approximates the next-token probabilities of any given mixture as a linear combination of these expert distributionsâ€”a relationship theoretically justified in the paper. These MDE-derived loss estimates are then combined with mixture rates as input features for a regression model trained on a sparse sampling of actual training runs. This pipeline utilizes the Vizier black-box optimization framework to minimize a generalization score based on cross-entropy over downstream validation tasks, aligning pre-training directly with few-shot utility.

Experiments conducted on the SlimPajama dataset using Transformer decoder-only models (ranging from 70M to 1B parameters) demonstrated that the proposed method significantly outperforms baseline regression approaches and prior optimization methods. The technique achieved superior few-shot downstream performance while drastically improving computational efficiency. Specifically, the approach reduced the number of required proxy model training runs from 30â€“500 to just $k$ (the number of domains). Additionally, the study established "approximate rank-invariance," confirming that data mixtures optimized on smaller, 280M parameter proxy models effectively translate to superior performance on larger, 1B parameter target models.

This research establishes a practical pipeline for data-centric optimization, bridging the gap between heuristic selection and brute-force search. By enabling the precise optimization of data mixtures with respect to end-task cross-entropy, the paper offers a path to more efficient LLM development. The ability to reuse experts for multiple criteria and evaluate new mixtures with minimal computational cost (O(k) operations per token via caching) lowers the barrier to creating high-performing models. Consequently, this work shifts the paradigm toward more rigorous, theoretically grounded data composition strategies, ensuring that future LLMs can be trained with optimal data resources at reduced expense.

---

## Key Findings

*   **Superior Performance:** The proposed method significantly outperforms baseline approaches that rely solely on mixture rates as input features for regression models.
*   **Enhanced Few-Shot Learning:** When combined with an objective function that includes cross-entropy on end-task data, the resulting models achieve superior performance on few-shot downstream evaluations.
*   **Empirical Validation:** Experiments conducted on Transformer decoder-only models (ranging from 70M to 1B parameters) using the SlimPajama dataset confirm the efficacy of the approach.
*   **Robust Approximation:** The study establishes that aggregating predictions from data experts provides a robust and efficient approximation of model losses for unseen data mixtures.

---

## Methodology

The core methodology revolves around approximating the training loss of various data mixtures without training full models for every combination.

1.  **Mixture of Data Experts (MDE):** The method uses an MDE to efficiently approximate the cross-entropy loss associated with various candidate pre-training data mixtures.
2.  **Regression Augmentation:** Instead of training a full model for every mixture, MDE loss approximations are used as additional input features for a regression model.
3.  **Sparse Sampling:** This regression model is trained based on observations of model loss from a small, select number of actual data mixtures, significantly reducing computational cost.
4.  **Objective Alignment:** The optimization process incorporates cross-entropy calculated on end-task data directly into the objective function to align pre-training with downstream utility.

---

## Technical Details

The implementation relies on a rigorous mathematical framework and efficient computational strategies to optimize data proportions.

*   **Optimization Goal:** The method aims to optimize data mixture proportions ($\lambda$) for $k$ training domains to minimize a generalization score based on aggregated cross-entropy losses across $m$ validation datasets.
*   **Framework:** Utilizes the **Vizier** black-box optimization framework.
*   **Expert Architecture:**
    *   Employs a Mixture of Data Experts (MDE) approximation.
    *   $k$ separate expert models are trained exclusively on single domains.
    *   The ensemble defines next-token probabilities as a linear combination of these expert distributions.
    *   This relationship is theoretically justified by **Proposition 3.1**.
*   **Efficiency Mechanisms:**
    *   Uses a caching strategy for per-token probabilities.
    *   Allows evaluation of new mixtures with **O(k) operations per token** on the CPU.
*   **Regression Models:** The approach augments standard regression features (mixture rates) with MDE-derived loss estimates, evaluating:
    *   Linear models
    *   Gradient Boosting
    *   Multi-task Gaussian Processes

---

## Contributions

This research makes three primary contributions to the field of LLM development:

1.  **Novel Technique:** Introduces a technique to optimize pre-training data composition without the prohibitive cost of training separate models for every potential mixture, bridging the gap between heuristic selection and brute-force search.
2.  **Theoretical Insights:** Provides theoretical insights explaining the mechanics behind why aggregating data expert predictions serves as a reliable proxy for actual model training loss across different data mixtures.
3.  **Practical Pipeline:** Demonstrates a practical pipeline for improving few-shot learning capabilities by optimizing data mixtures with respect to end-task cross-entropy, offering a path to more efficient LLM development.

---

## Results

The experimental validation of the method highlights both performance gains and computational efficiency.

*   **Experimental Setup:** Conducted on the SlimPajama dataset using 280M parameter proxy models and target models ranging from 70M to 1B parameters.
*   **Performance:** The proposed method outperformed baseline regression approaches and prior optimization methods, achieving superior few-shot downstream performance.
*   **Efficiency Gains:**
    *   Significantly improved efficiency by reducing the number of required proxy models from **30-500 to just $k$** (the number of domains).
    *   Lowered upfront compute costs by reusing experts for multiple criteria.
*   **Scalability:** The study confirmed approximate rank-invariance for small proxies, validating that optimizing data mixtures at the proxy scale effectively translates to the 1B parameter target scale.

---
**References:** 28 citations