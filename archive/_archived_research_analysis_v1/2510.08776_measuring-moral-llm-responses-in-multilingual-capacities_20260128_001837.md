---
title: Measuring Moral LLM Responses in Multilingual Capacities
arxiv_id: '2510.08776'
source_url: https://arxiv.org/abs/2510.08776
generated_at: '2026-01-28T00:18:37'
quality_score: 8
citation_count: 1
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Measuring Moral LLM Responses in Multilingual Capacities

*Harm Prevention, Measuring Moral, Large Language, Multilingual Capacities, Allison Yu, Kimaya Basu, Savi Kolari*

---

> ### ðŸ“‹ Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Dataset Size** | 500 English questions |
> | **Languages Tested** | High-Resource (Chinese, Spanish, Arabic), Low-Resource (Hindi, Swahili) |
> | **Evaluation Dimensions** | 5 (Moral Judgment, Biases, Legality, Safety, Consent) |
> | **Top Performer** | GPT-5 |
> | **Quality Score** | 8/10 |

---

## Executive Summary

As Large Language Models (LLMs) see increased global deployment, a critical challenge emerges regarding the consistency of moral alignment and safety guardrails across different languages. Most safety training is predominantly English-centric, raising concerns about whether these models maintain robust ethical standards when operating in non-English contexts. This paper addresses the urgent need to evaluate how **"linguistic shifts"**â€”translating inputs into both high-resource and low-resource languagesâ€”affect a model's adherence to safety protocols and moral reasoning. Without this understanding, models risk producing harmful outputs or failing to prevent abuse in multilingual environments, creating new vectors for jailbreaking through linguistic obfuscation.

To bridge this gap, the researchers developed a quantitative evaluation framework employing a standardized "judge LLM" to rigorously assess responses from a comprehensive suite of frontier models and leading open-weight models. The study utilizes a custom dataset of 500 English questions spanning five dimensions: **Moral Judgment, Biases & Stereotypes, Legality, Harm Prevention & Safety, and Consent & Autonomy**. To test robustness, the corpus was translated into high-resource languages (Chinese, Spanish, Arabic) and low-resource languages (Hindi, Swahili) using industry-standard translation APIs. The methodology is technically distinct in its use of adversarial "trick questions" and jailbreak-styled prompts to stress-test safety filters, relying on a five-point grading rubric to quantify performance degradation across linguistic contexts.

The evaluation revealed a substantial performance disparity between models, with **GPT-5** achieving the highest average performance across all categories compared to other frontier and leading open-source models. Specifically, in the 'Harm Prevention & Safety' category, GPT-5 scored an average of 4.73, while Gemini 2.5 Pro scored the lowest at 1.98. Similarly, in the 'Consent & Autonomy' category, GPT-5 averaged 3.56 against Gemini 2.5 Pro's 1.39. The data indicates that while top-tier models may perform well in English, nearly all evaluated models displayed significant inconsistency in other languages, with safety protocols failing particularly in lower-resource languages.

This research highlights a critical vulnerability in current LLM architectures: **safety and accuracy are not language-agnostic**. By demonstrating that linguistic shifts can bypass safety filtersâ€”effectively creating new vectors for jailbreakingâ€”the study underscores that high performance in English does not guarantee safe deployment globally. These findings necessitate a shift in the field toward developing more robust, multilingual alignment strategies ensuring that harm prevention and ethical guidelines are rigorously enforced across all supported languages, not just dominant high-resource ones.

---

## Key Findings

*   **GPT-5 Superiority:** GPT-5 achieved the highest average performance across all evaluated categories compared to other frontier and leading open-source models.
*   **Cross-Lingual Inconsistency:** Models other than GPT-5 displayed significant inconsistency in their responses across different languages and evaluation categories.
*   **Harm Prevention & Safety Gap:** There was a substantial performance gap in safety measures; GPT-5 averaged **4.73** while Gemini 2.5 Pro scored the lowest at **1.98**.
*   **Consent & Autonomy Disparity:** GPT-5 again outperformed the lowest-scoring model (Gemini 2.5 Pro) with an average score of **3.56** compared to **1.39**.
*   **Linguistic Impact:** The study indicates that linguistic shifts significantly impact LLM responses, revealing the need for improved robustness across multilingual contexts.

---

## Technical Details

The technical implementation of this study relies on a structured dataset and a specific translation pipeline to evaluate model robustness.

### Dataset & Corpus
*   **Source Material:** Custom dataset of 500 English questions.
*   **Categorization:** Questions are sorted into five distinct dimensions:
    1.  Moral Judgment
    2.  Biases & Stereotypes
    3.  Legality
    4.  Harm Prevention & Safety
    5.  Consent & Autonomy

### Translation & Localization
*   **Tool:** The `Googletrans` Python package.
*   **Target Languages:**
    *   *High-Resource:* Chinese, Spanish, Arabic
    *   *Low-Resource:* Hindi, Swahili

### Adversarial Testing Framework
*   **Methodology:** Builds upon the methodology established by Lin et al. (2025).
*   **Techniques:** Utilizes prompt engineering with "trick questions" and jailbreak-styled prompts.
*   **Objective:** To test safety filter robustness against obfuscation and linguistic shifts.

---

## Methodology

The researchers employed a rigorous quantitative evaluation framework designed to stress-test model alignment.

*   **Evaluation Agent:** A "judge LLM" was used to assess responses from both frontier and leading open-source models.
*   **Assessment Structure:** The assessment was conducted across five distinct dimensions utilizing a standardized five-point grading rubric.
*   **Robustness Testing:** To specifically test accuracy and consistency, the evaluation included a mix of languages classified as both low-resource and high-resource, ensuring a diverse testing ground for linguistic capabilities.

---

## Research Contributions

This paper provides several critical contributions to the field of AI safety and multilingual NLP:

1.  **Comparative Analysis:** Provided a detailed comparative analysis of LLM performance across low and high-resource languages, highlighting specific weaknesses in non-English contexts.
2.  **Safety Metrics:** Delivered detailed performance metrics in critical safety areas, specifically 'Consent & Autonomy' and 'Harm Prevention & Safety,' revealing a wide variance in moral alignment capabilities among top models.
3.  **Linguistic Vulnerability:** Highlighted the critical need to understand how linguistic shifts affect model outputs, emphasizing that safety and accuracy are not consistent across languages.

---

## Results Summary

The results confirm a significant performance gap between the leading model and competitors, particularly when safety is concerned.

| Category | GPT-5 Score | Lowest Score (Gemini 2.5 Pro) |
| :--- | :--- | :--- |
| **Harm Prevention & Safety** | 4.73 | 1.98 |
| **Consent & Autonomy** | 3.56 | 1.39 |

Additionally, the study found that linguistic shifts significantly impact responses, with safety protocols performing poorly in lower-resource languages and creating vectors for potential jailbreaking.

---

**References:** 1 citations
**Quality Score:** 8/10