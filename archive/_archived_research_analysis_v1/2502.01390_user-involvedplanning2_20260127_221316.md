---
title: User-InvolvedPlanning2
arxiv_id: '2502.01390'
source_url: https://arxiv.org/abs/2502.01390
generated_at: '2026-01-27T22:13:16'
quality_score: 6
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# User-InvolvedPlanning2

*Gianluca Demartini, Gaole He, Delft University of Technology*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Participants** | N = 248 |
| **Tasks Evaluated** | 6 common daily tasks |
| **Risk Levels** | Varying (e.g., flight booking vs. credit card payments) |
| **Model Used** | GPT-3.5-turbo |
| **Framework** | Plan-then-Execute (via Langchain) |
| **Key Metrics** | Plan Quality, Execution Accuracy, Subjective Trust, Objective Reliance, Trust Calibration |
| **Quality Score** | 6/10 |

---

## Executive Summary

> This research addresses the critical tension between the high utility of Large Language Model (LLM) agents and the risks associated with their deployment in daily assistance tasks. While LLM agents offer significant potential for automating complex activities, they function as a "**double-edged sword**" due to potential execution errors and the fragility of user trust.
>
> The core problem identified is that generating high-quality plans is insufficient for successful collaboration; users often mistrust plausible plans, and without appropriate oversight, human-AI team performance suffers. The study specifically highlights the risks of mistrust in open-ended environments, investigating how to balance agent autonomy with human oversight to ensure safety and efficacy.
>
> The key innovation is the implementation and empirical evaluation of a "**Plan-then-Execute**" framework within a human-in-the-loop simulation environment. The study concludes that optimizing the "human-in-the-loop" experience is essential for maintaining trust and ensuring the safe adoption of LLM-based agents.

---

## Key Findings

*   **The "Double-Edged Sword" Effect:** LLM agents provide high utility for daily assistance but introduce specific risks, acting as a double-edged sword.
*   **Two Conditions for Success:** Effective agent performance requires:
    1.  The availability of a high-quality plan.
    2.  Necessary user involvement during the execution phase.
*   **Fragility of Trust:** User trust is extremely fragile. Users frequently mistrust the agent even when generated plans appear plausible.
*   **Impact of Involvement:** The degree of user involvement at different stages (planning vs. execution) directly influences both trust levels and the overall performance of the human-AI team.

---

## Methodology

The study utilized an **empirical design** with **N=248 participants** to evaluate the dynamics of human-AI collaboration.

*   **Task Scope:** Six common daily tasks were assessed, ranging in risk levels from low-stakes activities (e.g., flight booking) to high-stakes financial activities (e.g., credit card payments).
*   **Framework:** A 'plan-then-execute' framework was deployed within a simulation environment.
*   **Objective:** The primary goal was to investigate how user involvement during the planning and execution stages impacts two major dependent variables:
    1.  User Trust
    2.  Collaborative Team Performance

---

## Technical Architecture

The system is built upon a robust technical stack designed to separate planning logic from execution actions.

### System Stack
*   **LLM Backend:** GPT-3.5-turbo
*   **Orchestration:** Langchain
*   **Environment:** Flask-based simulation

### Workflow Stages

| Stage | Functionality |
| :--- | :--- |
| **1. Planning Stage** | Generates a **human-readable, modifiable** step-wise plan for the user to review. |
| **2. Execution Stage** | Transforms the approved plan into action predictions using a **Markov Decision Process (MDP)**. This formalizes the execution phase into a state-based mapping to reduce random action errors. |

### User Interface
*   **Role:** Users act as "virtual personal assistants."
*   **Capabilities:** The interface provides intervention capabilities, allowing real-time modification of plans and approval or modification of individual actions through a natural language interface.

---

## Contributions

The research offers three primary contributions to the field of Human-Computer Interaction (HCI) and AI agents:

1.  **Empirical Insights:** Provides the first extensive empirical insights into how LLM agents utilize planning and sequential decision-making specifically for daily assistance tasks.
2.  **Trust Calibration:** Synthesizes strategies for calibrating user trust to optimize human-AI collaboration, addressing the gap between plan plausibility and user reliance.
3.  **Design Implications:** Offers critical design implications for future assistant interfaces. It emphasizes that designs should prioritize **user agency and control** in human-in-the-loop setups rather than pursuing full autonomy.

---

## Results & Evaluation

**Performance Metrics**
*   Defined metrics include **Plan Quality** and **Execution Accuracy** for measuring system performance.

**Behavioral Metrics**
*   **Subjective Trust:** User's reported confidence in the system.
*   **Objective Reliance:** The actual adoption or modification of AI outcomes by the user.
*   **Trust Calibration:** The alignment between trust and actual system performance.

**Study Variables**
*   **Independent Variable:** User involvement (timing and degree).
*   **Dependent Variables:** User Trust levels, Team Performance, and Human Delegation behaviors.

> **Note:** Specific quantitative results (statistical values) are not present in the provided text.

---

**Quality Score:** 6/10  
**References:** 40 citations