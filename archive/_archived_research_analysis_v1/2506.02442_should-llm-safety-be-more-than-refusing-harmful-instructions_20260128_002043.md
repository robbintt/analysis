---
title: Should LLM Safety Be More Than Refusing Harmful Instructions?
arxiv_id: '2506.02442'
source_url: https://arxiv.org/abs/2506.02442
generated_at: '2026-01-28T00:20:43'
quality_score: 9
citation_count: 34
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Should LLM Safety Be More Than Refusing Harmful Instructions?

*Utsav Maskey, Usman Naseem, Mark Dras (Macquarie University)*

---

> **üìä Quick Facts**
>
> *   **Quality Score:** 9/10
> *   **Total Citations:** 34
> *   **Dataset Size:** 2,502 samples
> *   **Encryption Algorithms Tested:** 9 (Caesar, Atbash, Vigenere, etc.)
> *   **Core Framework:** 2D Safety Evaluation (D1 & D2)

---

## üìã Executive Summary

Current Large Language Model (LLM) safety mechanisms primarily rely on refusing harmful instructions presented in natural language. However, as LLMs develop emergent capabilities like code decryption and cipher solving, these safety filters fail to generalize to the model's full functional range.

This paper addresses the critical vulnerability of **"mismatched-generalization attacks,"** where harmful instructions are obfuscated using encryption. Because safety training is typically distribution-specific, it often fails on long-tail encrypted texts, creating a security gap where models can successfully decrypt and execute harmful prompts (**under-refusal**) or incorrectly block benign encrypted instructions (**over-refusal**).

The authors introduce a novel **"2D Safety Evaluation"** framework that conceptually decouples safety into two dimensions:
1.  **D1:** Pre-LLM Instruction Refusal
2.  **D2:** Post-LLM Generation Safety

This allows for granular analysis of failure modes, specifically focusing on **"Case 2"** scenarios where the model bypasses initial refusal (D1 failure) to decrypt harmful inputs. To support this, the researchers developed a cryptanalysis dataset comprising **2,502 samples** derived from JailbreakBench, systematically testing the model's ability to act as a decryption function across 9 distinct algorithms.

The study concludes that current alignment strategies are insufficient for models possessing advanced reasoning skills like decryption. Existing safeguards lack robustness against adversarial inputs that leverage the model's own reasoning capabilities to bypass alignment.

---

## üîç Key Findings

*   **Critical Vulnerability:** LLMs capable of decrypting ciphers are highly vulnerable to *mismatched-generalization attacks*, where safety mechanisms do not generalize to decryption capabilities.
*   **Binary Safety Failures:** Models frequently exhibit two distinct failure types:
    *   **Under-refusal:** Generating unsafe responses to harmful encrypted prompts.
    *   **Over-refusal:** Incorrectly rejecting benign instructions hidden within encryption.
*   **Inadequate Safeguards:** Current safety mechanisms are inadequate for *long-tail distributed* texts (specifically encrypted/obfuscated inputs).
*   **Pipeline Weakness:** Existing pre-LLM and post-LLM interventions are not fully robust against the complexities of obfuscated harmful instructions.

---

## üß™ Methodology

The researchers employed a rigorous two-dimensional framework to evaluate LLM safety beyond simple refusal rates.

*   **Framework Dimensions:** Evaluated 'Instruction refusal' (D1) against 'Generation safety' (D2) to isolate where failures occur.
*   **Targeted Testing:** Conducted systematic evaluations specifically on **long-tail distributed texts**.
*   **Attack Simulation:** Tested encrypted inputs to determine if decryption capabilities allow the model to bypass standard safety filters.
*   **Intervention Assessment:** Assessed the effectiveness of multiple pre-LLM and post-LLM safety interventions against the identified attack vectors.

---

## ‚öôÔ∏è Technical Details

### 2D Safety Evaluation Framework
The paper proposes a matrix for evaluating safety failures, focusing heavily on:
*   **D1 (Pre-LLM Early Refusal Safety):** The model's ability to detect harmful intent before processing.
*   **D2 (Generation Safety):** The safety of the actual output generated.
*   **Case 2 (D1 Failure):** The critical scenario where the model successfully decrypts harmful obfuscated instructions but fails to refuse them.

### Attack Taxonomy
Threats are classified into two primary categories:
1.  **Competing Objectives**
2.  **Mismatched Generalization:** (e.g., SelfCipher, CodeChameleon attacks)

### Dataset Specifications
The study utilized a custom cryptanalysis framework where the LLM acts as a decryption function.

| Metric | Detail |
| :--- | :--- |
| **Total Samples** | 2,502 samples |
| **Algorithms** | Caesar, Atbash, Vigenere, Playfair, Rail Fence, Morse, Bacon, RSA, AES |
| **Sample Distribution** | 278 samples per algorithm |
| **Harmful Samples** | 200 (sourced from JailbreakBench) |
| **Benign Samples** | 78 (varied styles and domains) |

### Evaluation Metrics
Quantitative metrics were established to measure decryption utility:
*   **Exact Match (EM)**
*   **Normalized Levenshtein (NL) Distance**
*   **BLEU Score**
*   **Aggregate Performance Formula**

---

## üìä Results

The evaluation revealed significant limitations in current safety alignments:

*   **Qualitative Findings:** Confirmed binary safety failures where models oscillated between under-refusal and over-refusal when dealing with encrypted inputs.
*   **Decryption Capability:** Models demonstrated high success in decrypting ciphers across all 9 tested algorithms, proving that the capability exists but the safety alignment does not cover it.
*   **Bypass Efficacy:** "Mismatched-generalization attacks" were shown to effectively bypass standard guardrails.
*   **Metric Definitions:** The study successfully applied rigorous metrics (EM, NL, BLEU) to objectively assess decryption performance in a security context.

---

## üí° Contributions

This research makes four primary contributions to the field of AI safety:

1.  **Conceptual Separation:** Introduces the separation of safety into 'instruction refusal' and 'generation safety'.
2.  **New Attack Vector:** Identifies and characterizes 'mismatched-generalization attacks'.
3.  **First Systematic Evaluation:** Provides the first systematic evaluation of LLM safety specifically regarding long-tail encrypted texts.
4.  **Guidance for Future Safety:** Offers critical guidance for developing safety mechanisms that adapt to long-tail scenarios rather than just standard natural language inputs.

---
*Report generated based on 34 citations.*