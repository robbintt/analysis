# Policy Gradient for LQR with Domain Randomization
*Tesshu Fujinami; Bruce D. Lee; Nikolai Matni; George J. Pappas*

---

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 8/10
> *   **Reference Count:** 40 Citations
> *   **Core Problem:** Sim-to-Real Transfer & Robust Control
> *   **Method:** Policy Gradient & Domain Randomization
> *   **Key Innovation:** Discount-factor annealing algorithm

---

## Executive Summary

This paper addresses the critical theoretical gap in applying **Policy Gradient (PG)** methods to **Domain Randomization (DR)** for **Linear Quadratic Regulation (LQR)**. While DR is a standard industry practice for sim-to-real transfer to mitigate model uncertainty, it lacks rigorous convergence guarantees when coupled with PG training. A primary theoretical barrier is the initialization requirement: traditional PG approaches necessitate an initial policy that stabilizes all diverse environments within the training distribution. Acquiring a "jointly stabilizing" controller is often difficult or impossible *a priori*, creating a significant "cold start" problem for robust control synthesis.

The authors propose a **discount-factor annealing algorithm** and a rigorous theoretical framework for optimizing the Domain Randomization objective. The approach utilizes **Sample Average Approximation (SAA)** to create a finite-sample surrogate objective, minimizing the expected LQR cost over a distribution of system parameters. Crucially, the proposed annealing schedule starts with a low discount factor ($\gamma < 1$). This ensures the objective remains finite even if the initial policy is unstable, allowing gradient descent to proceed from arbitrary initializations. The discount factor is gradually annealed (increased) toward the target value of 1. This method, combined with an assumption bounding the heterogeneity of sampled system matrices, allows the algorithm to converge globally without requiring a known jointly stabilizing controller.

The study establishes that PG methods converge globally to the minimizer of the sample-average objective, provided the system heterogeneity is bounded. The authors derive explicit sample-complexity bounds required to ensure the performance gap between the sample-average and true population objectives remains negligible. Specifically, the quantitative analysis reveals that the bound on the gradient norm scales quadratically with the heterogeneity of the system matrices ($\Delta$). Empirical validation on a nonlinear, underactuated rotational inverted pendulum confirmed these theoretical convergence properties, demonstrating successful policy synthesis and cost convergence under the proposed annealing schedule where standard PG would fail.

This work is significant as it provides the first rigorous convergence analysis for Policy Gradient methods applied to domain-randomized LQR, validating a heuristic commonly used in practice. By eliminating the need for a known jointly stabilizing initialization, the framework solves the cold start problem inherent in robust control via randomization. The findings provide a quantitative foundation for sample efficiency in DR settings and lay the groundwork for future advancements in risk-sensitive formulations and stochastic PG algorithms for uncertain control systems.

---

## Key Findings

*   **Global Convergence:** Policy Gradient methods are proven to converge globally to the minimizer of a finite-sample approximation of the Domain Randomization objective, provided the heterogeneity of sampled systems is bounded.
*   **Sample-Complexity Bounds:** The study derives specific sample-complexity bounds required to ensure a small performance gap between the sample-average objective and the true population-level objective.
*   **Novel Algorithm:** A discount-factor annealing algorithm is introduced and proven to converge without the need for an initial jointly stabilizing controller.
*   **Future Applications:** Empirical results confirm theoretical findings and suggest potential for extending the approach to risk-sensitive formulations and stochastic PG algorithms.

---

## Methodology

The research employs a theoretical and algorithmic analysis focused on **Linear Quadratic Regulation (LQR)**. It utilizes policy gradient methods to train controllers across a distribution of simulated environments. The methodology involves:

1.  **Objective Analysis:** Analyzing the convergence properties of these methods on a finite-sample approximation of the Domain Randomization objective, subject to constraints on system heterogeneity.
2.  **Algorithm Design:** The authors propose and mathematically analyze a **'discount-factor annealing'** technique to address initialization difficulties.
3.  **Validation:** Validating these approaches with empirical data to ensure theoretical consistency with practical application.

---

## Contributions

*   **First Rigorous Analysis:** Establishes the first rigorous convergence analysis for Policy Gradient methods applied to domain-randomized LQR, addressing a previous lack of theoretical guarantees in widely used DR practices.
*   **Quantitative Framework:** Provides a quantitative framework for understanding how many samples (sample-complexity) are necessary to approximate the population objective effectively within a domain randomization setting.
*   **Solving Cold Start:** Introduces a novel discount-factor annealing algorithm that solves the 'cold start' problem in DR by removing the requirement to identify an initial jointly stabilizing controller.

---

## Technical Details

The technical approach revolves around optimizing a robust control policy using gradient-based methods under uncertainty.

*   **Optimization Goal:** Find a single policy $K$ that minimizes the expected Linear Quadratic Regulator (LQR) cost over a distribution of system parameters using Domain Randomization.
*   **Sample Average Approximation (SAA):** Utilizes a finite sample surrogate objective, optimized via standard gradient descent.
*   **Gradient Computation:** Gradients are computed analytically using discrete Lyapunov equations (`dlyap`) to determine steady-state covariance and cost matrices.
*   **Algorithm:** A discount-factor annealing algorithm is introduced to ensure convergence without requiring an initial jointly stabilizing controller.
*   **Assumptions:** Performance is contingent upon a **heterogeneity bound assumption** that limits the diversity between system matrices.

---

## Results

*   **Theoretical Proofs:** Theoretical proofs establish global convergence to the minimizer of the sample-average objective. The authors also provide sample-complexity bounds to ensure the performance gap between the sample average and true population objective remains small.
*   **Gradient Norm Bound:** A specific bound on the remainder term is derived, and the gradient norm bound is shown to scale quadratically with heterogeneity.
*   **Empirical Validation:** Validation was performed on a sim-to-real task involving a **nonlinear, underactuated rotational inverted pendulum**. This experiment confirmed the theoretical convergence findings.

---

**Report Generated:** Technical Documentation Services  
**Analysis Based On:** 40 Citations | Quality Score: 8/10