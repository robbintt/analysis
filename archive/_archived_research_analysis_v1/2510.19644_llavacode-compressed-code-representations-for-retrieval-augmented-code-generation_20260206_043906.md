---
title: 'LLavaCode: Compressed Code Representations for Retrieval-Augmented Code Generation'
arxiv_id: '2510.19644'
source_url: https://arxiv.org/abs/2510.19644
generated_at: '2026-02-06T04:39:06'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# LLavaCode: Compressed Code Representations for Retrieval-Augmented Code Generation

*Daria Cherniuk; Nikita Sukhorukov; Nikita Sushko; Daniil Gusak; Danil Sivtsov; Elena Tutubalina; Evgeny Frolov*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **TTFT Reduction** | 20â€“38% faster than standard full-RAG pipelines |
| **Quality Metrics** | Significant increase in Exact Match (EM) and Edit Similarity (ES) |
| **Architecture** | Reader: `Qwen-2.5-Coder-1.5B` (Frozen)<br>Encoder: `Qwen-3-Embedding-0.6B` (Frozen) |
| **Training Strategy** | Reinforcement Learning (Self-Critical Sequence Training) |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |

---

## Executive Summary

> **Problem**
> Retrieval-Augmented Generation (RAG) significantly improves code completion accuracy but creates a severe performance-speed trade-off. Appending retrieved code snippets extends input sequences, leading to high inference latency (Time-to-First-Token) and poor user experiences in interactive development environments (IDEs).
>
> **Innovation**
> The authors propose **LlavaCode**, a framework utilizing semantic compression. Instead of raw text, it converts retrieved code chunks into dense, semantically rich embedding vectors and projects them into a single-token format via a trainable Multi-Layer Perceptron (MLP) projector. This allows the reader LLM to interpret compressed representations directly. The system is trained using Reinforcement Learning (Self-Critical Sequence Training) to optimize directly for code generation metrics.
>
> **Results**
> LlavaCode achieves a 20â€“38% reduction in Time-to-First-Token (TTFT) compared to standard full-RAG, with negligible latency overhead compared to a base model. It simultaneously increases Exact Match (EM) and Edit Similarity (ES), outperforming baselines like Vanilla RAG, Base Models, and xRAG.
>
> **Impact**
> This research challenges the assumption that full, uncompressed context is necessary for high-quality code generation. By validating compressed tokens, LlavaCode provides a path for deploying sophisticated RAG systems in latency-sensitive, interactive settings without sacrificing accuracy.

---

## Key Findings

*   **Performance-Speed Trade-off Addressed:** Standard RAG improves code completion but causes slow inference due to extended sequence lengths; LlavaCode directly addresses this bottleneck.
*   **Latency Reduction:** The framework achieves a **20â€“38% reduction in Time-to-First-Token (TTFT)** on line completion tasks compared to full-RAG pipelines.
*   **Quality Enhancement:** Despite compressing context, the method significantly improves generation quality, increasing both **Exact Match (EM)** and **Edit Similarity (ES)** metrics.
*   **Computational Efficiency:** The computational cost of compression is minimal, resulting in only a negligible increase in latency.

---

## Methodology

*   **Framework Introduction:** Introduces LlavaCode, designed specifically to compress code representations for Retrieval-Augmented Code Generation.
*   **Compression Mechanism:** Utilizes a mechanism that converts raw code into compact, semantically rich representations rather than long text sequences.
*   **Projector Module:** Employs a small projector module architecture to transform code into a format directly interpretable by Code LLMs.
*   **Sequence Reduction:** Reduces retrieved context from long text sequences to a few compressed single-token vectors, retaining semantic meaning while drastically shortening input length.

---

## Technical Details

**Core Architecture**
*   **Context Compression:** Uses embedding projection to reduce sequence length.
*   **Workflow:** Retrieves top-10 code chunks $\rightarrow$ compresses into dense vectors $\rightarrow$ projects via trainable MLP (LLaVA-style) $\rightarrow$ concatenates with reader LLM prompt embeddings.
*   **Optimization:** Inference is optimized by retrieving precomputed text projections from the RAG database to minimize latency.

**Model Components**
*   **Reader LLM:** `Qwen-2.5-Coder-1.5B` (Frozen)
*   **Encoder:** `Qwen-3-Embedding-0.6B` (Frozen)
*   **Alternative Modalities:** Supports ASTs, DFGs, and CFGs.

**Training Protocol**
*   **Methodology:** Single-stage training with frozen parameters.
*   **Objective:** Uses Reinforcement Learning (Self-Critical Sequence Training without baseline) to directly optimize for the sum of Exact Match (EM) and Edit Similarity (ES).

---

## Contributions

*   **Optimization for Interactive Settings:** Addresses the critical limitation of inference speed in IDEs caused by long context windows.
*   **New Semantic Compression Paradigm:** Proposes using compressed, semantically rich, single-token vectors interpretable by LLMs.
*   **Breaking Trade-offs:** Demonstrates that generation quality (EM/ES) and speed (TTFT) can be improved simultaneously through efficient context compression, challenging the assumption that full context is required for high-quality generation.

---

## Results

*   **Speed Performance:** Achieved a **20â€“38% reduction in Time-to-First-Token (TTFT)** on line completion tasks compared to standard full-RAG pipelines, with negligible latency overhead compared to a base model without RAG.
*   **Quality Performance:** Significantly increased Exact Match (EM) and Edit Similarity (ES) scores, successfully breaking the standard performance-speed trade-off.
*   **Benchmark Comparison:** Outperformed baselines including:
    *   **Vanilla RAG:** (High latency)
    *   **Base Model:** (Lower quality)
    *   **xRAG:** (Prior textual compression method)