# Proximal Regret and Proximal Correlated Equilibria: A New Tractable Solution Concept for Online Learning and Games

*Yang Cai; Constantinos Daskalakis; Haipeng Luo; Chen-Yu Wei; Weiqiang Zheng*

***

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Total Citations** | 36 |
| **Primary Regret Bound** | $O(\sqrt{T})$ (Online Gradient Descent) |
| **Optimistic Bound** | $O(1)$ (Social Regret via Optimistic Gradient Descent) |
| **Key Concept** | Proximal Correlated Equilibria (PCE) |

***

### ðŸ“‹ Executive Summary

This research addresses a fundamental gap in online learning and game theory regarding the trade-off between computational tractability and the strength of equilibrium concepts. The field currently operates between two extremes: external regret is computationally efficient and leads to the coarse correlated equilibrium (CCE), while swap regret leads to the stronger correlated equilibrium (CE) but is computationally intractable to minimize in general games. The authors identify the need for an intermediate solution conceptâ€”one that is stricter than the external regime yet remains computationally feasible.

The key innovation is the introduction of **Proximal Regret**, a new metric defined via proximal operators on weakly convex functions. Proximal regret sits strictly between external regret (weaker) and swap regret (stronger), offering a nuanced measure of performance. Based on this metric, the authors define **Proximal Correlated Equilibria (PCE)**, a solution concept that is a strict subset of CCE.

Technically, the framework is highly versatile; by parameterizing a specific function class $F$, it can recover existing concepts such as external regret, projection/set restrictions, and gradient equilibrium. A significant theoretical breakthrough is the proof that the scope of this framework extends to **Mirror Descent** (generalized via Bregman divergences), demonstrating that standard, unmodified online learning algorithms naturally minimize proximal regret without requiring algorithmic adjustments.

The paper establishes rigorous performance bounds for proximal regret and convergence to PCE. For standard Online Gradient Descent (OGD), the authors prove an optimal $O(\sqrt{T})$ bound on proximal regret. Significantly, when applying **Optimistic Gradient Descent (OG)** under **smooth** assumptions, they achieve faster convergence rates: a constant $O(1)$ bound for social proximal regret in smooth strongly convex function classes. This represents a substantial improvement over previous state-of-the-art results.

***

## ðŸ”‘ Key Findings

*   **New Metric:** Introduction of 'proximal regret,' a metric strictly stronger than external regret but weaker than swap regret.
*   **Convergence:** Demonstrated convergence to **Proximal Correlated Equilibria (PCE)** in general convex games when all players employ a no-proximal-regret algorithm.
*   **OGD Performance:** Online Gradient Descent achieves an optimal $O(\sqrt{T})$ bound on proximal regret without any modification to the standard algorithm.
*   **Extended Analysis:** The theoretical framework extends to Mirror Descent and Optimistic Gradient Descent, with the latter yielding faster convergence rates specifically in smooth convex games.

## ðŸ§© Contributions

*   **New Solution Concepts:** Introduction of proximal regret and Proximal Correlated Equilibria (PCE) as new, tractable solution concepts.
*   **Empirical Explanation:** Provides a novel theoretical explanation for the empirically superior performance of Gradient Descent in games.
*   **Unified Framework:** Unification of several disparate recent concepts (e.g., gradient equilibrium, semicoarse correlated equilibrium) under a single cohesive theoretical framework.

## ðŸ›  Methodology

The authors employ a theoretical framework rooted in online learning and convex game theory. The study proceeds as follows:

1.  **Definition via Proximal Operators:** Utilization of proximal operators to define the new regret metric.
2.  **Convergence Analysis:** Establishment of convergence properties for the empirical distribution of play.
3.  **Algorithm Analysis:** Analysis of standard, unmodified online learning algorithms (Online Gradient Descent, Mirror Descent, and Optimistic Gradient Descent) within this new framework.
4.  **Proof of Bounds:** Rigorous proving of regret bounds to demonstrate convergence to PCE.

## âš™ Technical Details

*   **Framework Definition:** Introduces a framework centered on Proximal Regret and PCE. Proximal Regret is defined using the proximal operator on weakly convex functions.
*   **Metric Hierarchy:** The metric is positioned strictly between external regret (weaker) and swap regret (stronger).
*   **Function Class Recoveries:** Special cases of the function class $F$ recover:
    *   External regret (via indicator functions).
    *   Projection/set restriction.
    *   Gradient equilibrium/no-move regret (via linear functions).
*   **Algorithms Analyzed:**
    *   **Online Gradient Descent (OGD):** Minimizes proximal regret without modification.
    *   **Mirror Descent:** Generalized via Bregman divergences.
    *   **Optimistic Gradient Descent (OG):** Analyzed for faster convergence.
*   **PCE vs. CCE:** PCE is a strict subset of Coarse Correlated Equilibria (CCE). It is reached when the empirical distribution of strategies converges if players use no-proximal-regret algorithms.

## ðŸ“ˆ Results

*   **Online Gradient Descent (OGD):**
    *   Achieves an optimal $O(\sqrt{T})$ proximal regret bound.
    *   Converges to PCE at a rate of $O(T^{-1/2})$.
*   **Optimistic Gradient Descent (OG):**
    *   **Individual Regret:** Achieves an $O(T^{-1/4})$ bound for individual proximal regret in convex function classes.
    *   **Social Regret:** Achieves a constant $O(1)$ bound for social proximal regret in strongly convex function classes.
*   **Comparison to SOTA:**
    *   **External Regret:** SOTA is $O(\log T)$ (individual) / $O(1)$ (social).
    *   **Swap Regret:** Previous bounds were no better than $O(\sqrt{T})$.
    *   **Current Contribution:** The paper claims new contributions with OG's $O(1)$ social regret for the stronger PCE metric.

***

*References: 36 citations*