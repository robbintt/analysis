---
title: Irrational Complex Rotations Empower Low-bit Optimizers
arxiv_id: '2501.12896'
source_url: https://arxiv.org/abs/2501.12896
generated_at: '2026-02-03T18:45:54'
quality_score: 9
citation_count: 35
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Irrational Complex Rotations Empower Low-bit Optimizers
*Zhen Tian; Wayne Xin Zhao; Ji-Rong Wen*

> ### **Quick Facts**
> | Metric | Value |
> | :--- | :--- |
> | **Target Bit-width** | 3.32-bit |
> | **Parameter Reduction** | 75% |
> | **Memory Savings** | 41.8% (11.32 GB) |
> | **Key Innovation** | $\pi$-Quant (Complex Rotation) |
> | **Quality Score** | 9/10 |

---

## Executive Summary

Training large-scale deep learning models, particularly Large Language Models (LLMs), is severely bottlenecked by the substantial memory footprint required for optimizer states, such as those in adaptive methods like Adam. While quantization is a standard strategy for mitigating this overhead, existing techniques typically fail at extremely low bit-widths (below 4 bits), often resulting in gradient collapse, instability, or significant performance degradation. Addressing this challenge is critical for the advancement of AI, as memory constraints currently limit the scale of models that can be trained and the efficiency of hardware utilization.

The authors introduce **$\pi$-Quant**, a novel compression algorithm that fundamentally reparameterizes optimizer states by mapping parameters into a complex space. Instead of quantizing individual values directly, the method leverages a complex rotation scheme where pairs of parameters are represented by a single rotation angle $\theta$. This approach utilizes the properties of irrational numbers—specifically a novel irrational constant derived from $\pi$—to ensure the uniqueness and stability of the representation. To ensure practical applicability, the authors developed a system of geometric equations that computes these precise rotation angles with linear complexity, seamlessly integrating extreme compression into the optimization process without prohibitive computational costs.

Experimental results on the PG-19 dataset using TinyLLaMA demonstrate that $\pi$-Quant successfully reduces parameter bit-width to **3.32 bits** while maintaining accuracy equivalent to the FP32 baseline. The method achieved a training loss of 2.422 and a test perplexity of 5.14, nearly identical to the FP32 scores of 2.423 and 5.17, whereas FP16 and standard linear quantization methods suffered from collapse. In terms of efficiency, the approach realized a 75% reduction in parameter scale and decreased GPU memory usage to 11.32 GB—a 41.8% reduction. Furthermore, $\pi$-Quant outperformed full-precision Adam on downstream tasks such as LRA, Samsum, and Movielens, validating its robustness across diverse benchmarks.

This research represents a significant breakthrough in overcoming the memory limitations of adaptive optimizers, offering a viable path toward training larger models on existing hardware. By establishing a rigorous mathematical framework that links parameter pairs to rotation angles, the authors provide a new perspective on data representation in deep learning. The ability to achieve extreme low-bit compression (down to 3.32-bit) without sacrificing convergence properties or accuracy sets a new standard for optimizer efficiency, potentially lowering the resource barriers for future LLM development and deployment.

---

## Key Findings

*   **Extreme Compression:** The proposed $\pi$-Quant algorithm successfully reduces parameter bit-width to **3.32-bit** without sacrificing model accuracy.
*   **Resource Efficiency:** The method achieves a **75% reduction** in parameter scale and a **40% decrease** in GPU memory usage during training.
*   **Mathematical Representation:** Findings demonstrate that a pair of parameters can be effectively represented by a single rotation angle within a complex rotation scheme.
*   **Computational Efficiency:** The approach utilizes a system of geometric equations that computes precise rotation angles with **linear complexity**, ensuring efficient integration into the optimization process.

---

## Methodology

The proposed method consists of a four-step process designed to compress optimizer states via complex space mapping:

1.  **Complex Space Mapping**
    The methodology begins by mapping model parameters into a complex space, transforming real-valued optimization states into complex-valued representations.

2.  **Complex Rotation Scheme**
    Leveraging the properties of irrational numbers (e.g., $\pi$), the algorithm represents pairs of parameters using single rotation angles rather than distinct values. This reduces the degrees of freedom required for representation.

3.  **Angle-based Quantization**
    Quantization is performed by utilizing these corresponding rotation angles. By quantizing the angle rather than the raw parameter values, the state representation is significantly compressed.

4.  **Geometric Computation**
    To ensure practical applicability, the authors developed a system of geometric equations that allows for the precise calculation of these angles with **linear computational complexity**.

---

## Technical Details

*   **Core Mechanism:** Complex Rotation Scheme representing parameters $(x, y)$ as rotation angle $\theta$ using:
    $$x + iy = e^{i\theta} + e^{i2\theta}$$
*   **Irrational Constant:** Introduces constant $\alpha = 10^{-\theta} + 10^{-2\theta} \approx 0.3589793238$ to ensure uniqueness and stability in the rotation mapping.
*   **Computational Geometry:** Uses geometric equations involving isosceles triangles for solving $\theta$ with linear complexity.
*   **Target Specification:** Targets low bit-widths down to **3.32-bit**.

---

## Results

### Experimental Setup
*   **Task:** Language modeling
*   **Dataset:** PG-19
*   **Model:** TinyLLaMA

### Performance Metrics
| Method | Train Loss | Test PPL | Status |
| :--- | :--- | :--- | :--- |
| **$\pi$-Quant (3.32-bit)** | **2.422** | **5.14** | ✅ Stable |
| FP32 Baseline | 2.423 | 5.17 | ✅ Stable |
| FP16 / Linear-Quant | N/A | N/A | ❌ Collapse |

### Efficiency & Downstream Tasks
*   **Memory Optimization:** Memory reduced to **11.32 GB** (~41.8% reduction) with a 75% parameter scale reduction.
*   **Benchmarking:** $\pi$-Quant outperformed full-precision Adam on downstream tasks including **LRA**, **Samsum**, and **Movielens**.
*   **Ablation Studies:** Error bounds were strictly controlled by the precision parameter $\theta$.

---

## Contributions

*   **Novel Compression Algorithm:** Introduction of $\pi$-Quant, a new optimizer state compression technique that utilizes irrational numbers to enable memory-efficient training.
*   **Theoretical Insight:** Establishment of a mathematical framework linking parameter pairs to rotation angles, offering a new perspective on data representation in deep learning optimizers.
*   **Efficiency Optimization:** Development of a computationally efficient method (linear complexity) to solve geometric equations required for the complex rotation scheme during optimization.
*   **Performance Benchmarking:** Demonstration that extreme low-bit compression (down to 3.32-bit) is achievable while maintaining full accuracy across a wide range of tasks.

---

**Citations:** 35 references