---
title: An Empirical Study of Deep Reinforcement Learning in Continuing Tasks
arxiv_id: '2501.06937'
source_url: https://arxiv.org/abs/2501.06937
generated_at: '2026-02-03T13:40:39'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# An Empirical Study of Deep Reinforcement Learning in Continuing Tasks

*Yi Wan; Dmytro Korenkevych; Zheqing Zhu*

***

### üìä Quick Facts

| **Metric** | **Detail** |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Citations** | 40 |
| **Core Focus** | Deep RL in Continuing Tasks (Non-Episodic) |
| **Key Algorithms** | DDPG, TD3, SAC, PPO |
| **Environments** | Modified Mujoco, Atari |
| **Primary Technique** | Reward Centering (Naik et al., 2024) |

***

## üìë Executive Summary

Deep Reinforcement Learning (RL) research has predominantly focused on episodic tasks, where agent-environment interactions are segmented by distinct resets, allowing for simplified learning dynamics. However, many real-world applications require "continuing tasks," where agents operate indefinitely without environmental resets‚Äîa domain that remains comparatively underexplored. This paper addresses the critical challenge that standard deep RL algorithms often fail or converge to suboptimal policies in these "non-episodic settings" due to the lack of a stationary distribution of states. By focusing on continuing tasks that enforce the weakly communicating Markov Decision Property (MDP) to avoid irreversible traps, the authors highlight a significant gap in current empirical literature and establish the necessity of algorithms capable of handling perpetual, non-resetting interactions.

The key technical innovation presented is the empirical validation and generalization of a specific **Reward Centering technique**, originally proposed by Naik et al. (2024). While prior work was limited to Q-learning, this study demonstrates that reward centering is broadly applicable across various temporal-difference (TD) based algorithms, including DDPG, TD3, SAC, and PPO. The method works by centering the reward signal to stabilize the learning process in the absence of episodic boundaries. To rigorously test this, the authors constructed a comprehensive testbed by modifying standard Mujoco environments (removing resets and introducing dynamic target resampling) and incorporating Atari environments, thereby simulating ongoing, non-terminating interactions across diverse domains.

The empirical evaluation across these modified environments demonstrates that the specific reward centering method significantly outperforms two alternative reward-centering baselines and standard non-centered approaches. The study reported specific performance metrics, with the HumanoidStandup environment achieving scores up to 120, Pusher ranging up to 2.0, Reacher up to 1.2, and Locomotion tasks up to 0.08. These results were consistent across the Atari domain, confirming that reward centering effectively mitigates the instability caused by the absence of resets and allows TD-based algorithms to scale robustly to larger, more complex tasks that were previously difficult for standard deep RL methods to master.

This research significantly bridges the gap between episodic and continuing task literature, providing the field with a robust empirical foundation for applying deep RL to "non-terminating scenarios". By establishing the specific reward centering method as a superior technique over alternative baselines and validating its applicability across a wide spectrum of modern algorithms (DDPG, TD3, SAC, PPO) and domains (Mujoco and Atari), the study provides a clear directive for researchers and practitioners dealing with perpetual systems. The availability of the modified testbed serves as a valuable new benchmark for the community, encouraging further exploration into algorithms that can function reliably outside the constraints of traditional episodic training.

***

## üîë Key Findings

*   **Effectiveness of Reward Centering:** The method of reward centering (Naik et al., 2024) is effective for improving temporal-difference-based RL algorithms in continuing tasks, extending beyond its original validation with just Q-learning.
*   **Scalability:** The reward centering technique scales effectively to larger tasks, demonstrating robustness beyond the scope of prior studies.
*   **Superior Performance:** The specific reward centering approach investigated outperforms two other alternative reward-centering methods.
*   **Domain Insights:** The study reveals critical insights into the behavior of well-known deep RL algorithms when applied to continuing tasks, a domain previously distinct from the more commonly studied episodic tasks.

***

## üõ†Ô∏è Methodology

*   **Testbed Construction:**
    *   Utilized a suite of continuing task testbeds built upon standard Mujoco and Atari environments.
    *   Designed to simulate non-episodic, ongoing agent-environment interactions.

*   **Algorithm Evaluation:**
    *   Several well-known deep reinforcement learning algorithms were empirically studied and benchmarked within these continuing task scenarios.

*   **Technique Assessment:**
    *   A specific focus was placed on evaluating the effectiveness of a reward centering method (Naik et al., 2024).
    *   Performance was compared against two other reward-centering approaches across various algorithms and task scales.

***

## ‚öôÔ∏è Technical Details

*   **Task Definition:** The study contrasts continuing tasks (ongoing interaction without resets) against episodic tasks.
*   **MDP Property:** Enforces the weakly communicating MDP property to prevent suboptimal traps.
*   **Primary Intervention:** Reward Centering (Naik et al., 2024), applied to TD-based RL algorithms (DDPG, TD3, SAC, PPO).
*   **Testbed Composition:**
    *   **Environments:** Modified Mujoco environments (Swimmer, Ant, HumanoidStandup, Reacher, Pusher).
    *   **Modifications:** Resets are removed and dynamic changes (like target resampling) are introduced.

***

## üìà Results

*   **Algorithms Evaluated:** DDPG, TD3, SAC, and PPO on modified continuing environments.
*   **Performance Metrics:**
    *   **HumanoidStandup:** 0 ‚Äì 120
    *   **Pusher:** 0.0 ‚Äì 2.0
    *   **Reacher:** 0.0 ‚Äì 1.2
    *   **Locomotion:** 0.00 ‚Äì 0.08
*   **Conclusions:**
    *   Reward Centering is effective for TD-based algorithms in continuing tasks.
    *   The method scales to larger tasks and outperforms two alternative methods.
    *   The absence of resets poses a significant challenge to standard deep RL algorithms.

***

## üß© Contributions

*   **Bridging the Research Gap:** The study addresses the underexplored area of deep RL in continuing tasks (where environment resets are unavailable or inappropriate), providing an empirical contrast to the extensive literature on episodic tasks.
*   **Generalization of Reward Centering:** The authors extend the findings of Naik et al. (2024) by demonstrating that their reward centering method is not limited to Q-learning but is broadly applicable to a wider range of algorithms.
*   **Benchmarking Superiority:** The research establishes the specific reward centering method as a superior technique for continuing tasks by empirically proving it outperforms alternative reward-centering baselines.