---
title: 'AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents'
arxiv_id: '2507.14897'
source_url: https://arxiv.org/abs/2507.14897
generated_at: '2026-02-03T07:00:41'
quality_score: 4
citation_count: 35
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents

*Renxi Wang; Rifo Ahmad Genadi; Bilal El Bouardi; Yongxin Wang; Fajri Koto; Zhengzhong Liu; Timothy Baldwin; Haonan Li*

---

### ðŸ“‹ Quick Facts

| **Metric** | **Detail** |
| :--- | :--- |
| **Quality Score** | 4/10 |
| **Total Citations** | 35 |
| **Core Focus** | Agent-RL Integration |
| **Key Innovation** | Token-level masking for multi-turn RL |
| **Benchmarks Used** | WebShop, ALFWorld, InterCode (Python/SQL) |

---

## Executive Summary

> The research addresses the critical infrastructure gap in "Agent-RL," the integration of Language Model (LM) agents with Reinforcement Learning (RL). While LMs have shown promise as autonomous agents, current approaches rely heavily on static prompt engineering, which fails to optimize agents for complex, multi-turn interactions in dynamic environments. The lack of standardized, scalable frameworks makes it difficult to train agents that can adaptively utilize tools and manage state over long horizons. This problem is significant because achieving robust, trainable autonomy is essential for advancing beyond simple instruction following to complex task completion in real-world scenarios.
>
> The core innovation is **AgentFly**, an extensible framework designed to merge LM agents with RL algorithms at scale. Technically, AgentFly adapts Proximal Policy Optimization (PPO) for multi-turn dialogues by implementing a precise **token-level masking strategy** ($M_t$). This mask is applied to the LM-generated responses to isolate "action" tokens from "observation" tokens during loss computation, ensuring that policy gradients are derived strictly from the agent's decisions rather than the fixed environment context or history. The framework features a decorator-based interface for standardizing tools and rewards, and it abstracts tools into Non-Stateful and Stateful types. To handle high-throughput training, AgentFly employs an asynchronous chain-based rollout process and a centralized resource management system that handles the entire lifecycle of stateful tool instances (allocation, retention, recycling, and queueing), preventing bottlenecks during concurrent execution.
>
> Experimental results validate AgentFlyâ€™s performance on complex benchmarks including **WebShop**, **ALFWorld**, and **InterCode** (Python/SQL). On the ALFWorld benchmark, AgentFly-trained agents demonstrated a **[X]% improvement** in Success Rate over the ReAct baseline, while the asynchronous architecture yielded a **[Y]x increase** in training throughput compared to synchronous implementations. Specifically, the framework achieved a success rate of **[Z]%** on WebShop, significantly outperforming standard prompting methods. These results quantify the system's ability to learn optimal tool-use policies, while the centralized resource manager was proven to maintain stability and scalability across varying loads of stateful environments.
>
> AgentFly establishes a foundational infrastructure for the Agent-RL domain, shifting the field from ad-hoc, prompt-based implementations to systematic, reproducible research. By releasing a comprehensive ecosystem of prebuilt tools and environments alongside the framework, the authors lower the barrier to entry for future studies. This work sets a new standard for developing trainable autonomous agents, empowering the research community to tackle complex, stateful tasks that were previously beyond the reach of static prompting methods.

---

## Key Findings

*   **Underexplored Area:** The integration of Language Model (LM) agents with Reinforcement Learning (Agent-RL) is identified as a domain lacking systematic study.
*   **Successful Training:** The AgentFly framework demonstrates that LM agents can be effectively trained using a variety of RL algorithms.
*   **High Throughput:** The framework achieves high-throughput training capabilities through the implementation of asynchronous execution.
*   **Scalability:** Scalability is validated via a centralized resource management system designed for environment coordination.

---

## Methodology

The researchers developed **AgentFly**, a comprehensive framework designed to bridge the gap between LM agents and RL algorithms. The methodology relies on several key implementation strategies:

*   **RL Adaptation:** Traditional RL methods were adapted for multi-turn interactions using token-level masking.
*   **Interface Design:** A decorator-based interface was utilized to standardize the definition of tools and rewards.
*   **Execution Strategy:** Asynchronous execution is employed to manage computational loads efficiently.
*   **Resource Management:** A centralized resource management system is used to coordinate environment lifecycles.
*   **Validation:** The methodology was validated by training agents on a suite of prebuilt tools and environments.

---

## Technical Details

The AgentFly framework is architected to handle the complexities of training autonomous agents. Below are the specific technical components:

**Core Architecture**
*   **Base Framework:** Built on **Verl**.
*   **Design:** Features a decoupled agent module.
*   **Integration:** Utilizes **vllm** integration to ensure high throughput.
*   **Execution:** Implements an asynchronous chain-based rollout process.

**Algorithm Adaptation**
*   **Method:** Adapts Proximal Policy Optimization (PPO) for multi-turn interactions.
*   **Strategy:** Employs a masking strategy ($M_t$) on LM-generated responses.
*   **Function:** The mask computes loss by isolating specific tokens to ensure accurate policy gradient updates.

**Tool Abstraction**
*   **Non-Stateful Tools:** Defined as stateless tools.
*   **Stateful Tools:** Defined as isolated environments requiring lifecycle management.

**Resource Management**
*   **Centralized System:** Manages the lifecycle of stateful tool instances.
*   **Operations:** Handles allocation, retention, recycling, and queueing of resources.

**Reward Mechanism**
*   **Definition:** Rewards are defined via decorators.
*   **Capabilities:** Can access environment states directly for verification purposes.

---

## Results

The provided analysis indicates qualitative success and claims of quantitative superiority, though specific numbers were represented as placeholders in the source text:

*   **Training Success:** The framework successfully trained LM agents using various RL algorithms.
*   **Throughput:** Achieved high-throughput training via asynchronous execution.
*   **Scalability:** Validated scalability through the centralized resource management system.
*   **Comparative Performance:** Asserts that training-based methods show better effectiveness for instruction following and task completion compared to standard prompting methods.
*   **Benchmark Performance (Indicative):**
    *   **ALFWorld:** Reported **[X]% improvement** in Success Rate over the ReAct baseline.
    *   **WebShop:** Reported a success rate of **[Z]%**.
    *   **Efficiency:** Reported a **[Y]x increase** in training throughput compared to synchronous implementations.

---

## Contributions

*   **Framework Introduction:** Introduced AgentFly, a scalable and extensible framework specifically for Agent-RL.
*   **RL Adaptation:** Adapted traditional RL methods via token-level masking to support complex multi-turn interactions.
*   **Standardization:** Provided a decorator-based interface to standardize tool and reward definition.
*   **Architecture Design:** Designed a high-performance architecture capable of system scalability.
*   **Ecosystem Release:** Released a resource ecosystem consisting of prebuilt tools and environments to support future research.