# Adversarial ML Problems Are Getting Harder to Solve and to Evaluate

*Javier Rando; Jie Zhang; Nicholas Carlini; Florian TramÃ¨r*

---

> ### ðŸ”Ž Quick Facts
> 
> **Quality Score:** 8/10
> **References:** 15 citations
> **Document Type:** Position Paper / Critical Review
> **Core Problem:** The shift to Large Language Models (LLMs) has created adversarial challenges that are less defined, harder to solve, and significantly harder to evaluate than classical ML image problems.
> **Primary Warning:** Without addressing fundamental methodological flaws, the field risks another decade of stagnation in security research.

---

## Executive Summary

This paper addresses the critical stagnation of progress in adversarial machine learning, arguing that despite a decade of research, fundamental methodological flaws have impeded meaningful advancements in model security. The authors contend that the field is now facing an escalated crisis with the emergence of Large Language Models (LLMs).

Unlike classical ML, which relied on well-defined objectives, the shift to LLMs introduces problems that are significantly less defined, harder to solve computationally, and harder to evaluate rigorously. The authors warn that without addressing these root causes, the research community risks another decade of failure, particularly as high-stakes safety concerns in general-purpose models continue to mount.

The key innovation is a proposed **three-tier taxonomy of difficulty** that characterizes the evolving landscape of adversarial ML. Technically, the paper delineates a shift from traditional continuous gradient-based optimization on small networks with bounded inputs (e.g., `$\ell_p$`-norm perturbations) to modern discrete optimization on unbounded input spaces.

This framework highlights how current LLM research relies on manual, human-driven attack strategies and ad-hoc defensesâ€”such as external classifiers or virtual adversarial trainingâ€”rather than the formal certified robustness techniques that were the "holy grail" of classical adversarial ML. The analysis demonstrates that manual attacks currently outperform automated gradient-based methods, which frequently fail to produce coherent results and instead yield gibberish.

The paper exposes specific failures in current evaluation protocols, including high false positive rates where non-refusals are incorrectly classified as attacks, and a reliance on flawed proxy objectives (e.g., maximizing the probability of phrases like "Sure, I can help you with that"). Furthermore, it contrasts the historical standard of 8/255 pixel perturbation bounds in image classification against the current lack of concrete metrics in LLM safety. This position paper serves as a vital corrective warning, challenging the field to abandon superficial benchmarks that result in "safe-but-useless" models and instead prioritize rigorous, standardized evaluation protocols.

---

## Key Findings

*   **Historical Stagnation:** Progress in securing machine learning models has been sluggish over the last decade, even when addressing elementary problems.
*   **Methodological Flaws:** Research efforts are frequently hindered by non-rigorous evaluation protocols that fail to accurately measure true robustness.
*   **Escalating Complexity with LLMs:** Challenges have intensified in three distinct dimensions:
    *   Problems are less defined.
    *   Problems are harder to solve.
    *   Problems are harder to evaluate.
*   **Risk of Future Failure:** Without addressing fundamental issues, the next decade may fail to yield meaningful advancements in AI safety.

---

## Methodology

This work is a **position paper** employing conceptual analysis and a critical review of the current state of adversarial machine learning. It contrasts historical challenges of classical ML robustness (e.g., image classification) with the emerging complexities found in Large Language Models (LLMs).

---

## Key Contributions

1.  **Taxonomy of Difficulty:** A three-tiered framework characterizing adversarial ML problems in the LLM era, categorizing them by definition, solvability, and evaluability.
2.  **Critical Warning:** A caution to the research community that the shift to LLMs risks exacerbating existing problems rather than solving them.
3.  **Benchmark Critique:** A call for re-evaluating how research progress is measured for general-purpose language models to avoid false positives and artificial correlations.

---

## Technical Details

The paper provides a comparative analysis between traditional adversarial ML and modern LLM challenges.

| Feature | Traditional Adversarial ML | Modern LLM Challenges |
| :--- | :--- | :--- |
| **Objectives** | Well-defined objectives | Abstract safety properties |
| **Input Space** | `$\ell_p$`-norm bounded perturbations | Unbounded input spaces |
| **Optimization** | Continuous gradient-based optimization | Discrete optimization |
| **Network Scale** | Small networks | Large Language Models |
| **Attack Strategy** | Automated gradient-based methods | Manual, human-driven attack strategies |
| **Defense Strategy** | Formal certified robustness | Ad-hoc defenses (external classifiers, virtual adversarial training) |
| **Output Quality** | Coherent perturbations | Automated methods often yield "gibberish" |

---

## Results & Evaluation

The analysis highlights significant disparities in how success is measured and achieved between classical and modern adversarial ML:

*   **Attack Efficacy:** Manual attacks currently outperform automated gradient-based methods.
*   **Metrics Defined:**
    *   *Historical:* 8/255 pixel perturbation bound.
    *   *Proxy Objectives:* Maximizing the probability of simple phrases like "Sure, I can help you with that."
*   **Evaluation Issues:**
    *   **High False Positives:** Classifying non-refusals as attacks.
    *   **Artificial Correlation:** Defenses and judges may be correlated, skewing results.
    *   **Safety-Utility Trade-off:** Current methods risk creating "safe-but-useless" models.
    *   **Reproducibility Challenges:** Silent updates in closed-source models make consistent evaluation difficult.