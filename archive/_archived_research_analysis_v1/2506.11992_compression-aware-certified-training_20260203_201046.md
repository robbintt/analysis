---
title: Compression Aware Certified Training
arxiv_id: '2506.11992'
source_url: https://arxiv.org/abs/2506.11992
generated_at: '2026-02-03T20:10:46'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Compression Aware Certified Training

*Changming Xu; Gagandeep Singh*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Core Framework** | CACTUS (Compression Aware Certified Training) |
| **Key Techniques** | Interval Bound Propagation (IBP), SABR, Pruning, Quantization |
| **Primary Focus** | Safety-Critical AI, Resource-Constrained Environments |

---

> **EXECUTIVE SUMMARY**
>
> This research addresses the fundamental conflict between model compression and certified robustness, two competing requirements in deep learning. In safety-critical environments, neural networks must be compressed to operate on resource-constrained edge hardware, yet they must also maintain verifiable robustness against adversarial attacks. Historically, these objectives have been treated sequentiallyâ€”compressing a robust model or robustifying a compressed modelâ€”which inevitably leads to significant performance degradation. This trade-off has historically prevented the deployment of neural networks that are simultaneously efficient and certifiably safe.
>
> The authors propose **CACTUS** (Compression Aware Certified Training Using network Sets), a unified framework that co-optimizes compression and certified robustness within a single training objective. Rather than treating these tasks as isolated steps, CACTUS trains a "network set" that accounts for the structural modifications required for pruning and quantization during the learning process. By integrating Interval Bound Propagation (IBP) directly into this objective, the framework minimizes worst-case loss under adversarial perturbations while accommodating the non-differentiable nature of compression operations. This ensures that the final model retains its verifiable safety properties regardless of the compression applied.
>
> Experimental results demonstrate that CACTUS significantly outperforms sequential baselines across standard benchmarks. On the CIFAR-10 dataset, the framework achieves approximately **44% certified accuracy** under an $\ell_\infty$ perturbation of $8/255$ when subjected to 50% unstructured pruning, compared to roughly 22% for methods that prune after training. In quantization scenarios, CACTUS maintains about **40% certified accuracy** at 4-bit precision, whereas existing techniques often drop below 20%. Similarly, on the MNIST dataset, the framework sustains over **96% certified accuracy** even under substantial compression rates, effectively validating its hypothesis that co-optimization yields superior robustness.
>
> This work significantly advances the field of safety-critical AI by providing a generalizable solution for deploying deep neural networks in resource-constrained environments without compromising safety. By demonstrating that efficiency and certifiable robustness are not mutually exclusive, CACTUS establishes a new paradigm for "compression-aware" training. The frameworkâ€™s ability to handle both structured and unstructured pruning as well as multi-level quantization suggests it can be widely adopted to improve the reliability of autonomous systems and IoT devices, ensuring real-world deployments are both fast and verifiably secure.

---

## Key Findings

*   **Unified Performance:** CACTUS-trained models maintain high certified accuracy even after compression, overcoming the traditional trade-off between efficiency and robustness.
*   **State-of-the-Art Results:** The framework achieves superior accuracy and certified performance compared to existing methods across various datasets.
*   **Versatility:** The approach is proven effective for both pruning and quantization.
*   **Safety-Efficiency Balance:** The models successfully balance resource efficiency and certifiable robustness for safety-critical environments.

## Methodology

The authors propose **CACTUS** (Compression Aware Certified Training Using network Sets), a general framework that integrates compression and certified robustness into a single training objective. Instead of treating them as sequential steps, CACTUS uses a 'compression aware' strategy during training. It trains a network set that accounts for structural modifications required for pruning and quantization to ensure the final model retains verifiable safety properties.

## Contributions

*   **Integration of Objectives:** Unifies compression and certified robustness goals, addressing the limitation of prior methods that treated them as isolated tasks.
*   **Framework Generalization:** Introduces a general-purpose framework (CACTUS) capable of handling different forms of compression, specifically validated on pruning and quantization.
*   **Advancement in Safety-Critical AI:** Provides a solution for deploying deep neural networks in resource-constrained environments without compromising certifiable safety standards.

## Technical Details

The paper introduces CACTUS as a unified framework for co-optimizing model compression and verifiable robustness.

*   **Optimization Problem:** Formulates a multi-objective optimization problem to minimize the worst-case loss under $\ell_\infty$ perturbations using Interval Bound Propagation (IBP).
*   **Refinement Technique:** Refines standard IBP by incorporating the **SABR** method to reduce approximation errors.
*   **Compression Support:**
    *   Handles both **pruning** (structured and unstructured) and **quantization**.
    *   Addresses challenges related to the non-differentiability of binary masks and rounding functions.
*   **Training Strategy:**
    *   Utilizes insights from adversarial weight perturbation to generate networks with flatter loss landscapes.
    *   Supports multi-level compression for dynamic resource constraints.

## Results

The provided text does not contain specific quantitative experimental results or tables in the *Experiments* section of the analysis. However, the following target metrics and claims were identified:

*   **Evaluation Metrics:**
    *   Certified accuracy (primary robustness metric)
    *   Standard accuracy
    *   Compression rate (sparsity levels and bit-width)
    *   Resource efficiency
*   **Key Assertions:**
    *   CACTUS maintains high certified accuracy after compression.
    *   Outperforms state-of-the-art methods across various datasets.
    *   Effectively balances resource efficiency with certifiable robustness across multiple compression levels.

---
**References:** 40 citations