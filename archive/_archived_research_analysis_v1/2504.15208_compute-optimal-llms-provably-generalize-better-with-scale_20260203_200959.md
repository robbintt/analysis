---
title: Compute-Optimal LLMs Provably Generalize Better With Scale
arxiv_id: '2504.15208'
source_url: https://arxiv.org/abs/2504.15208
generated_at: '2026-02-03T20:09:59'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Compute-Optimal LLMs Provably Generalize Better With Scale

*Marc Finzi; Sanyam Kapoor; Diego Granziol; Anming Gu; Christopher De Sa; J. Zico Kolter; Andrew Gordon Wilson*

---

> ### **Quick Facts**
> * **Quality Score:** 9/10
> * **Citations:** 40
> * **Study Scope:** Pythia model family (70M – 12B parameters)
> * **Dataset:** The Pile
> * **Key Ratio:** Token-to-Parameter ratio of 20:1
> * **Optimization:** Chinchilla-optimal scaling constants ($a=b=0.5$)

---

## Executive Summary

This paper addresses the critical gap between the empirical success of Large Language Models (LLMs) and the theoretical understanding of why they generalize well. While empirical scaling laws demonstrate that performance improves with size, rigorous mathematical proofs for generalization in the compute-optimal regime have been absent. The authors seek to determine if the reduction in generalization error observed in large models is a consistent, provable phenomenon, and to identify the specific statistical drivers—such as loss variance and information capacity—that allow scaling to mitigate overfitting.

The key technical innovation is the development of a novel, fully empirical Freedman-type martingale concentration inequality designed specifically for the non-IID nature of text data. Unlike standard generalization bounds that rely on worst-case assumptions, this approach incorporates the variance of the loss function to produce significantly tighter estimates. The authors decompose the generalization bound into three interpretable components: parameters per token, loss variance, and quantization error at a fixed bitrate. This framework treats text generation as a martingale process, allowing the theoretical bounds to account for complex dependencies in sequential data that traditional IID statistics miss.

Experiments conducted on the Pythia model family (70M to 12B parameters) trained on The Pile validated the theoretical framework, confirming Chinchilla-optimal scaling with a token-to-parameter ratio of 20:1 (where scaling exponents $a=b=0.5$). The study established that the generalization gap decreases at a rate proportional to $1/\sqrt{N}$, shrinking at least as fast as the approximation gap in the compute-optimal regime. The proposed concentration inequality yielded bounds approximately 5 times tighter than standard methods. Furthermore, the researchers empirically demonstrated that reductions in generalization error are directly driven by decreases in loss variance (per-token loss standard deviation) and quantization error.

This research provides the field with a provable scaling law for the generalization gap, offering a theoretical guarantee that bounds will improve predictably as models scale. It delivers a crucial information-theoretic insight: larger models are inherently more quantizable because their rate of information integration grows more slowly than their total capacity. By mathematically linking generalization to loss variance and capacity, the paper validates the efficiency of compute-optimal training and provides a rigorous foundation for future research into model architecture, quantization strategies, and the theoretical limits of LLM scaling.

---

## Key Findings

*   **Generalization improves with scale:** In the compute-optimal regime, scaling up language models results in a smaller generalization gap.
*   **Drivers of reduced generalization error:** The reduction in generalization error is driven specifically by decreases in loss variance and quantization error.
*   **Quantizability of large models:** Larger models are inherently more quantizable because their rate of information integration grows more slowly than their total capacity increases.
*   **Predictable scaling laws:** The research establishes a scaling law for the generalization gap, demonstrating that generalization bounds become predictably stronger as the model scales.

---

## Methodology

The study employs a multi-faceted theoretical and analytical approach to understand generalization in LLMs:

*   **Theoretical Framework:** The authors develop generalization bounds specifically for the pretraining objective of Large Language Models (LLMs) within the compute-optimal regime.
*   **Statistical Technique:** They introduce a novel, fully empirical Freedman-type martingale concentration inequality to tighten existing bounds by incorporating the variance of the loss function.
*   **Decomposition Analysis:** The resulting generalization bound is decomposed into three interpretable components: the number of parameters per token, the loss variance, and the quantization error at a fixed bitrate.

---

## Technical Details

*   **Error Decomposition:** The theoretical framework decomposes expected test error into three distinct parts:
    *   **Irreducible Error (E)**
    *   **Approximation Gap (A)**
    *   **Generalization Gap (G)**
*   **Hypothesis Bounds:** Utilizes a countable hypothesis generalization bound applying a universal prior (Occam's razor).
*   **Martingale Process:** Treats text generation as a martingale process to handle non-IID (Independent and Identically Distributed) dependencies via a novel inequality termed **Empirical Freedman's Inequality**.
*   **Theorem 3.1:** Introduces a bound involving:
    *   **Risk range ($\Delta$)**
    *   **Complexity ($C$)**
    *   **Variance proxy ($\Sigma$)**, estimated via a lagged variable ($Y_k$).
    *   *Note:* This approach provides estimates approximately **5x tighter** than standard bounds.
*   **Assumptions:** The framework assumes Chinchilla-optimal scaling ($N/D \approx 1/20$) and uses prediction smoothing to bound worst-case loss behavior for LLMs.

---

## Results

*   **Experimental Validation:** Experiments conducted on the Pythia model family (70M to 12B parameters) trained on The Pile establish Chinchilla optimization constants as $a = b = 0.5$, confirming a token-to-parameter ratio of 20:1.
*   **Loss Variance:** Results show that per-token loss standard deviation decreases at a rate of $c + 1/\sqrt{N}$.
*   **Gap Comparison:** The generalization gap decreases at least as fast as the approximation gap in the compute-optimal regime.
*   **Empirical Links:** Reduction in generalization error is empirically linked to decreases in loss variance and quantization error.
*   **Power Law Frontier:** The performance frontier follows a power law $L(C) \propto C^{-\gamma}$, with loss approximated by $R(N, D) = E + A/N^{\alpha} + B/D^{\beta}$.

---

## Contributions

*   **Novel Concentration Inequality:** A new, empirical application of the Freedman-type martingale concentration inequality that accounts for loss variance to provide tighter generalization bounds.
*   **Interpretable Bound Decomposition:** A breakdown of generalization bounds into distinct components (parameters per token, loss variance, and quantization error) allowing for granular analysis of scaling effects.
*   **Information-Theoretic Insight:** A theoretical explanation for the superior generalization of larger models, linking generalization to the relationship between capacity and information integration rates.
*   **Scaling Law for Generalization:** The derivation of a specific scaling law for the generalization gap, providing a theoretical guarantee that bounds will improve predictably with scale.

---

**Report generated based on 40 references.**