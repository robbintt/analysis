---
title: 'MoPEQ: Mixture of Mixed Precision Quantized Experts'
arxiv_id: '2509.02512'
source_url: https://arxiv.org/abs/2509.02512
generated_at: '2026-02-03T19:29:43'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# MoPEQ: Mixture of Mixed Precision Quantized Experts

*Krishna Teja Chitty-Venkata; Jie Ye; Murali Emani*

***

> ### ðŸ“‹ Quick Facts
> | Metric | Details |
> | :--- | :--- |
> | **Target Architecture** | Mixture-of-Experts (MoE) & Vision-Language Models (VLMs) |
> | **Bit-width Allocation** | Mixed Precision (2-bit, 3-bit, 4-bit) |
> | **Core Strategy** | Hessian Trace Approximation |
> | **Key Models Validated** | DeepSeek-VL2, MolmoE-1B |
> | **Benchmark** | VLMEvalKit |
> | **Quality Score** | 9/10 |

***

## Executive Summary

Mixture-of-Experts (MoE) architectures in modern Vision-Language Models (VLMs), such as DeepSeek-VL2, deliver state-of-the-art performance but impose prohibitive memory burdens that hinder deployment on resource-constrained hardware. While post-training quantization (PTQ) is a standard solution for model compression, existing uniform-precision methods fail to account for the varying sensitivity of different experts, leading to drastic accuracy degradation when bit-widths are reduced aggressively.

This paper addresses the critical challenge of achieving substantial memory reduction in MoE models without sacrificing the accuracy required for complex visual reasoning tasks. The authors introduce **MoPEQ**, the first PTQ algorithm designed for mixed-precision quantization at the expert level. Instead of applying uniform bit-widths across layers or relying on heuristic activation frequencies, MoPEQ utilizes Hessian trace approximation to rigorously evaluate each expert's sensitivity to quantization error.

Validated on the VLMEvalKit benchmark using DeepSeek-VL2 and MolmoE-1B, MoPEQ demonstrates superior efficiency over strong baselines such as GPTQ and AWQ. The method achieves up to **4x memory reduction** compared to 16-bit full-precision models while maintaining accuracy within a negligible margin (often <1% drop on benchmarks like MMBench). This research establishes MoPEQ as a foundational baseline for the efficient deployment of VLM-MoEs, proving that expert-level mixed precision is viable for large-scale multimodal systems.

***

## Key Findings

*   **Superior Memory Efficiency:** MoPEQ achieves significant memory savings compared to uniform-precision baselines while maintaining competitive accuracy on state-of-the-art VLMs.
*   **Hessian > Frequency:** Hessian trace approximation is a more effective strategy for precision assignment than relying on activation frequency.
*   **Robust Validation:** The method was successfully validated on complex architectures like Deepseek-VL2 and MolmoE using the VLMEvalKit benchmark with 2, 3, and 4-bit precision allocations.
*   **Clustering Benefits:** Clustering similar experts during quantization helps maintain overall model performance despite aggressive bit-width reduction.

***

## Methodology

The researchers developed **MoPEQ**, a Post Training Quantization (PTQ) algorithm tailored for Mixture-of-Experts (MoE) architectures.

*   **Granularity:** The approach utilizes per-expert granularity to assign optimal bit widths to individual experts rather than treating entire layers uniformly.
*   **Sensitivity Analysis:** Instead of using activation frequency, the method employs **Hessian trace approximation** to rigorously analyze each expert's sensitivity to quantization.
*   **Optimization Strategy:** The optimization strategy involves clustering similar experts to balance the trade-off between accuracy retention and memory reduction.

***

## Technical Specifications

The proposed method targets specific architectures and employs distinct technical strategies for quantization:

*   **Target Models:** Mixture of Experts (MoE) architectures, specifically Vision-Language Models (VLMs) including DeepSeek-VL2 (Tiny, Small, Base) and MolmoE-1B.
*   **Precision Strategy:**
    *   Mixed-precision quantization using **2-bit, 3-bit, and 4-bit** representations.
    *   Precision is assigned at both **layer-wise** and **expert-wise** levels, creating heterogeneous bit-width allocation.
*   **Assignment Algorithm:**
    *   Utilizes **Hessian trace approximation** (a sensitivity-based approach using second-order information) for precision assignment.
    *   Identified as superior to the baseline activation frequency strategy.
*   **Optimization Mechanism:**
    *   Incorporates the **clustering of similar experts** to minimize error compounding.

***

## Results & Evaluation

The performance of MoPEQ was evaluated against rigorous benchmarks:

*   **Memory vs. Accuracy:** MoPEQ achieves superior memory efficiency compared to uniform-precision baselines while maintaining competitive accuracy on state-of-the-art VLMs despite aggressive bit-width reduction down to 2-bit.
*   **Scalability:** Evaluated using the VLMEvalKit benchmark, the method scales effectively from smaller models (MolmoE-1B) to larger variants (DeepSeek-VL2 Base).
*   **Strategy Validation:** Experimental analysis confirms that Hessian trace approximation is a more effective strategy for precision assignment than activation frequency, resulting in a complex, non-uniform distribution of bit-widths across layers.
*   **Performance Retention:** While uniform 2-bit and 3-bit baselines often suffer significant performance collapses (5-10% degradation), MoPEQ's Hessian-based allocation sustains competitive performance.

***

## Contributions

*   **Algorithm Introduction:** Introduced MoPEQ, the first-of-its-kind PTQ algorithm specifically designed to apply mixed precision quantization at the expert level within LLMs and VLMs.
*   **Comparative Study:** Provided a comprehensive study comparing expert activation frequency against Hessian-based sensitivity analysis, offering the field a deeper understanding of precision allocation.
*   **New Baseline:** Established a thorough evaluation of mixed-precision quantization (2, 3, and 4 bits) on layer-wise and model-wide scales for recent VLM-MoEs, contributing a new baseline for efficient deployment.

***

**Quality Score:** 9/10 | **References:** 40 Citations