---
title: Experience-Evolving Multi-Turn Tool-Use Agent with Hybrid Episodic-Procedural
  Memory
arxiv_id: '2512.07287'
source_url: https://arxiv.org/abs/2512.07287
generated_at: '2026-02-03T13:07:25'
quality_score: 9
citation_count: 26
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Experience-Evolving Multi-Turn Tool-Use Agent with Hybrid Episodic-Procedural Memory

*Sijia Li; Yuchen Huang; Zifan Liu; Zijian Li; Jingjing fu; Lei Song; Jiang Bian; Jun Zhang; Rui Wang*

---

> ### **Quick Facts**
> 
> *   **Quality Score:** 9/10
> *   **References:** 26 Citations
> *   **Inference Gain:** Up to **50%** over strong baselines
> *   **RL Generalization:** Up to **40%** improvement on OOD tasks
> *   **Core Architecture:** Hybrid Episodic-Procedural Memory (H-EPM)
> *   **Key Innovation:** Biologically inspired adaptive memory routing

---

## Executive Summary

Large Language Model (LLM)-based agents utilizing tools face significant hurdles in multi-turn reasoning and interaction, particularly within reinforcement learning (RL) frameworks. The core challenge lies in the inefficiency of exploration in long-horizon tasks where reward signals are sparse, and the agent must navigate complex decision spaces. Furthermore, existing memory architectures struggle to reconcile the tension between recalling specific, contextual details (episodic memory) and executing well-learned, routine sequences (procedural memory).

This paper addresses these limitations by proposing a method to enable agents to self-evolve through experience without relying on extensive domain-specific data collection. The authors introduce **Hybrid Episodic-Procedural Memory (H-EPM)**, a biologically inspired architecture that integrates distinct memory systems into a unified graph-based framework.

The system constructs a tool graph where nodes represent tools and edges represent transitions, augmented with episodic summaries. The technical novelty lies in an adaptive retrieval mechanism featuring a conditional decision gate that assesses the current state via a "state-summarization tool." This gate dynamically routes the agent to **Path A (Episodic Memory)** for contextual reasoning using semantic similarity, or **Path B (Procedural Memory)** for routine execution based on historical transition weights.

The experimental evaluation demonstrates that H-EPM delivers substantial performance improvements over strong baselines across various multi-turn tool-use benchmarks. This research represents a significant advancement in the field of autonomous agents by bridging the critical gap between full trajectory reuse and granular tool-level reuse.

---

## Key Findings

*   **Significant Inference Gains:** The proposed H-EPM model achieves substantial improvements during inference, delivering performance gains of **up to 50%** over strong baselines across various multi-turn tool-use benchmarks.
*   **Enhanced RL Generalization:** The method improves reinforcement learning policy performance, specifically achieving gains of **up to 40%** on out-of-distribution (OOD) tasks without relying on domain-specific experience collection.
*   **Effective Long-Trajectory Exploration:** H-EPM successfully addresses the core challenge of ineffective exploration in multi-turn agent reinforcement learning by guiding the agent toward historically successful tool transitions.
*   **Balanced Reasoning:** The agent effectively balances episodic recall (for contextual reasoning) and procedural execution (for routine steps), allowing for adaptability in shifting decision contexts.

---

## Methodology

The research introduces a **Hybrid Episodic-Procedural Memory (H-EPM)** strategy, inspired by human cognitive integration, to enable experience-induced self-evolution. The approach operates through the following mechanisms:

1.  **Tool Graph Construction:** The system constructs a graph from accumulated trajectories where dependencies represent procedural routines and edges are augmented with episodic summaries.
2.  **Adaptive Reuse:** It utilizes the adaptive reuse of partially overlapping successful experiences rather than relying solely on full trajectory replays.
3.  **Dynamic Balancing:** During inference, the agent dynamically balances between episodic recall and procedural execution based on the current context.
4.  **Memory-Guided RL:** A memory-guided reinforcement learning paradigm is employed where the memory system biases exploration toward historically successful tool transitions.

---

## Contributions

*   **Resolution of Contextual Trade-offs:** The paper addresses a critical limitation by bridging the gap between full trajectory reuse and tool-level reuse.
*   **Novel Memory Architecture:** It contributes a new biologically inspired architecture that integrates episodic and procedural memory within a graph structure.
*   **Advancement in Multi-Turn Agent RL:** It introduces a new paradigm for reinforcement learning in multi-turn scenarios, solving the problem of exploration inefficiency in long-horizon tasks.

---

## Technical Details

### Core Architecture
*   **System Type:** Hybrid Episodic-Procedural Memory (H-EPM).
*   **Structure:** Graph-based system where **tools are nodes** and **transitions are edges**.

### Adaptive Retrieval Mechanism
*   **Decision Gate:** Features a conditional decision gate that determines the memory path based on a 'state-summarization tool'.
*   **Path A (Episodic Memory):**
    *   *Trigger:* Contextual reasoning.
    *   *Mechanism:* Selects tools via semantic similarity between current and stored state summaries.
*   **Path B (Procedural Memory):**
    *   *Trigger:* Routine execution.
    *   *Mechanism:* Selects tools based on historical edge weights.

### Integration
*   The system outputs top-k candidate tools and integrates with RL rollouts to guide exploration and mitigate sparse rewards.

---

## Results

*   **Inference Performance:** Achieved gains of up to **50%** over baselines across multi-turn tool-use benchmarks.
*   **Generalization:** Improved RL policy performance by up to **40%** on out-of-distribution (OOD) tasks without domain-specific experience.
*   **Exploration Efficiency:** Enhanced exploration efficiency in long-trajectory RL by guiding agents toward historically successful transitions.
*   **Adaptability:** Demonstrated an effective balance between episodic recall and procedural execution.