# Large language models and the entropy of English
*Colin Scheibner; Lindsay M. Smith; William Bialek*

---

> ### **Quick Facts: Key Metrics**
> *   **Models Analyzed:** OLMo 2 1B, Llama 3.2 1B, Qwen3 8B, DCLM 1.7B (Decoder-only Transformers)
> *   **Context Length Scale:** Dependencies found up to $\sim 10^4$ characters
> *   **Compression Ratio:** 4–5 characters per token
> *   **Key Metrics:** Conditional Entropy, Code Length ($\ell$), Mutual Information
> *   **Sample Sizes:** 6,220 to 83,360 strings
> *   **Architecture Specifics:** Llama 3.2 1B (GQA, 128k context); DCLM 1.7B (2048 context)

---

## Executive Summary

**Problem**
This research addresses the fundamental challenge of quantifying the structural complexity and predictability of the English language, specifically regarding the scale of long-range dependencies between characters. While Large Language Models (LLMs) have achieved state-of-the-art performance in text generation, there is a lack of rigorous understanding regarding the extent to which these models capture the true statistical mechanics of language versus mere artifacts of their architecture. This matters because without a precise empirical baseline for the "entropy of English"—the inherent uncertainty in predicting the next character—it is difficult to distinguish the unique contributions of model architecture from the intrinsic information content of the data itself, hindering the development of theoretically grounded AI systems.

**Innovation**
The authors innovatively deploy modern decoder-only Transformers (specifically OLMo 2, Llama 3.2, Qwen3, and DCLM) not merely as generative tools, but as high-resolution probes to measure the information-theoretic properties of text. They rigorously decouple model-specific behavior from raw data statistics by analyzing conditional entropy and Code Length ($\ell$) as functions of context length ($N$), normalized to character units to ensure cross-model comparability despite varying tokenization ratios. By measuring Mutual Information via empirical joint distributions and validating findings against direct data analysis, they establish a methodology that isolates intrinsic long-range correlations from model assumptions, valid across diverse text genres including poetry and encyclopedic text.

**Results**
The analysis reveals that conditional entropy and the mean code length required to encode text decrease monotonically with context length up to distances of $N \sim 10^4$ characters, demonstrating the existence of direct, significant long-range dependencies with no observed plateau. The study identifies a phenomenon of "emergent certainty," where the predictability of specific characters approaches near-perfect certainty ($p \approx 1$); this effect grows with context length and follows a shallow power-law behavior. Furthermore, training dynamics indicate distinct learning phases: short-range dependencies are acquired rapidly, while performance on long-range dependencies improves gradually. High cross-model consistency was observed for $N > 10^3$ steps across major architectures, confirming that these statistical properties are robust features of the English language rather than quirks of specific model optimization paths.

**Impact**
This work provides critical empirical constraints that any statistical physics model describing language or LLM internal mechanisms must satisfy. By rigorously quantifying the scale of long-range correlations ($\sim 10^4$ characters) and the power-law scaling of "emergent certainty," the study bridges the gap between deep learning practice and theoretical information theory. It establishes a necessary benchmark for future theoretical frameworks, challenging researchers to account for the distinct timescales involved in learning short- versus long-range structure. Ultimately, these findings offer a foundational data set for understanding the limits of language predictability and guiding the design of more efficient, physically realistic architectures for natural language processing.

---

## Key Findings

*   **Long-range Dependencies:** Conditional entropy and code length decrease with context length up to distances of $N \sim 10^4$ characters, demonstrating direct dependencies over significant spans.
*   **Intrinsic Correlations:** Data reveals small but significant correlations between characters at long separations, independent of model assumptions.
*   **Emergent Certainty:** Certainty regarding specific characters increases with context length, characterized by a peak near zero entropy that grows as context deepens.
*   **Training Dynamics:** Long-ranged structure is learned gradually during training, exhibiting distinct dynamics compared to the rapid acquisition of short-range dependencies.

---

## Methodology

The researchers utilized Large Language Models (LLMs) to analyze diverse English texts, focusing on measuring **conditional entropy** and **code length** as functions of context length ($N$). To ensure the validity of the observed structures, the team performed direct data analysis decoupled from model assumptions to verify correlations. Additionally, they monitored the evolution of these metrics throughout the model training process to understand how short and long-range dependencies are acquired over time.

---

## Technical Details

**Models Analyzed**
*   **OLMo 2 1B**
*   **Llama 3.2 1B**: Uses Grouped-Query Attention (GQA); 128k context length.
*   **Qwen3 8B**
*   **DCLM 1.7B**: Trained on less data; 2048 context length.

**Data Processing & Metrics**
*   **Tokenization**: Text converted to tokens at a compression ratio of **4–5 characters per token**.
*   **Primary Metrics**:
    *   **Code Length ($\ell$)**
    *   **Mean Code Length ($L$)**
    *   *Note: Metrics converted to character units for comparability.*
*   **Evaluation Protocol**:
    *   Filtering to 93 characters.
    *   Randomizing starting points.
    *   Sequence lengths up to 4096 tokens.
    *   **Mutual Information** computed via empirical joint distributions with a systematic error floor.

---

## Results

*   **Entropy Scaling:** Conditional entropy and code length decrease over distances up to $\sim 10^4$ characters with **no observed plateau**, indicating deep dependencies.
*   **Cross-Model Consistency:** High consistency found for $N > 10^3$ across OLMo 2, Llama 3.2, and Qwen3.
*   **Genre Invariance:** Results remain invariant across genres (C4, Wikipedia, Poetry).
*   **Behavioral Trends**:
    *   **Emergent Certainty**: Follows a shallow power-law behavior.
    *   **Learning Dynamics**: Short-range dependencies are learned quickly; long-range performance improves gradually.
*   **Statistical Significance:** Analyses utilized large sample sizes ranging from **6,220 to 83,360 strings**.

---

## Contributions

This study provides critical empirical constraints for statistical physics models describing language or LLM internal mechanisms. By rigorously quantifying the scale and nature of long-range correlations and learning dynamics, it offers foundational data that theoretical models must account for.

---

**Document Quality Score:** 8/10
**References:** 0 citations