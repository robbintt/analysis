# Reusing Trajectories in Policy Gradients Enables Fast Convergence

*Alessandro Montenegro; Federico Mansutti; Marco Mussi; Matteo Papini; Alberto Maria Metelli*

---

> ### üìä Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Sample Complexity:** $\widetilde{O}(\varepsilon^{-1})$
> *   **Key Innovation:** Power Mean Correction (MIW)
> *   **Total References:** 40 citations

---

## üìù Executive Summary

**Policy Gradient (PG)** methods are fundamental to solving reinforcement learning problems in continuous spaces, but they suffer from notoriously high sample complexity, often requiring prohibitive amounts of data to converge. While recent variance reduction techniques have improved these rates by reusing gradients, the theoretical potential of reusing entire historical off-policy trajectories has remained largely unexplored.

This paper addresses the critical challenge of **distribution shift**‚Äîthe discrepancy between the policy that generated the data and the current policy being optimized‚Äîwhich has previously made the reuse of old trajectories theoretically risky and difficult to analyze. The authors introduce the **Retrospective Policy Gradient (RPG)**, a novel algorithm designed to leverage both fresh and historical trajectories to accelerate convergence.

The core technical innovation is the introduction of a "**power mean correction**" applied to Multiple Importance Weighting (MIW) estimators. Unlike standard approaches that rely on the Balance Heuristic, RPG explicitly rejects this method in favor of the power mean to more effectively correct for distribution shifts. This theoretical framework allows the algorithm to rigorously control the error induced by reusing stale data, enabling the safe and efficient mixture of old and new trajectory samples.

The paper establishes that RPG achieves a sample complexity of $\widetilde{O}(\epsilon^{-1})$ for finding an $\epsilon$-stationary point. This result represents the **best-known convergence rate** in the literature, significantly outperforming existing state-of-the-art methods (improving upon SVRPG's $O(\epsilon^{-3/2})$ and vanilla PG's $O(\epsilon^{-2})$). Empirical validation confirms these theoretical advantages, setting a new benchmark for data efficiency in reinforcement learning.

---

## üîë Key Findings

*   **Record-Breaking Convergence:** The proposed Retrospective Policy Gradient (RPG) algorithm achieves a sample complexity of $\widetilde{O}(\varepsilon^{-1})$, establishing the best known convergence rate in the literature.
*   **Superiority to Existing Methods:** This rate significantly surpasses existing baselines, improving upon $O(\varepsilon^{-3/2})$ (gradient reuse methods like SVRPG) and $O(\varepsilon^{-2})$ (vanilla Policy Gradient methods).
*   **Validation of Trajectory Reuse:** This work provides the first rigorous theoretical evidence that extensive reuse of past off-policy trajectories significantly accelerates convergence in Policy Gradient methods.
*   **Empirical Success:** Experimental validation confirms the theoretical advantages of the approach, demonstrating superior performance compared to state-of-the-art methods.

---

## üß¨ Methodology

The authors propose the **Retrospective Policy Gradient (RPG)**, a novel algorithm designed to enhance data efficiency by reusing off-policy trajectories from previous iterations.

*   **Data Combination:** RPG combines old (historical) and new (fresh) trajectories to perform policy updates.
*   **Distribution Shift Correction:** To address the distribution shifts caused by reusing old data, the method employs a **'power mean correction'** applied to the multiple importance weighting estimator.
*   **Theoretical Analysis:** The work relies on a novel theoretical analysis to demonstrate convergence under established assumptions, specifically bridging the gap between theory and practice for trajectory reuse.

---

## ‚öôÔ∏è Technical Details

*   **Algorithm:** Retrospective Policy Gradient (RPG).
*   **Objective:** Improve data efficiency in vanilla Policy Gradients by reusing off-policy trajectories.
*   **Estimator:** Utilizes a **PM-corrected Multiple Importance Weighting (MIW)** gradient estimator.
    *   *Note:* The approach explicitly rejects the Balance Heuristic in favor of the power mean.
*   **Operating Environment:**
    *   Continuous state and action spaces.
    *   Parametric stochastic policy.
*   **Key Assumptions:**
    *   Bounded gradients.
    *   Bounded KL divergence.
    *   Smoothness of the objective function.
*   **Error Scaling:** Theoretical bounds indicate the PM estimator error scales at a rate of $O((Nk)^{-1/2})$, dependent on parameter dimensionality, gradient bounds, and KL divergence.

---

## üìà Results

| Metric | RPG (Proposed) | SVRPG (Gradient Reuse) | Vanilla PG (Baseline) |
| :--- | :---: | :---: | :---: |
| **Sample Complexity** | **$\widetilde{O}(\varepsilon^{-1})$** | $O(\varepsilon^{-3/2})$ | $O(\varepsilon^{-2})$ |

*   RPG finds an $\epsilon$-stationary point with significantly fewer samples than competitors.
*   Theoretical bounds confirm the efficiency of the Power Mean corrected estimator.

---

## üèÜ Contributions

1.  **Theoretical Advancement:** Filled a critical gap by rigorously analyzing the reuse of past trajectories, an area previously dominated by research on gradient reuse.
2.  **Algorithmic Innovation:** Introduced the RPG algorithm and the specific **power mean correction** technique for multiple importance weighting estimators.
3.  **Benchmark Establishment:** Set a new theoretical standard for sample efficiency in Policy Gradient methods by proving the $\widetilde{O}(\varepsilon^{-1})$ rate.