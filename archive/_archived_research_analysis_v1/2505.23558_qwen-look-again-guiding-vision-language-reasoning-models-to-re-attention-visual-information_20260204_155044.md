---
title: 'Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention
  Visual Information'
arxiv_id: '2505.23558'
source_url: https://arxiv.org/abs/2505.23558
generated_at: '2026-02-04T15:50:44'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Qwen Look Again: Guiding Vision-Language Reasoning Models to Re-attention Visual Information
*Xu Chu; Xinrong Chen; Guanyu Wang; Zhijie Tan; Kui Huang; Wenyu Lv; Tong Mo; Weiping Li*

---

> ### üìä Quick Facts
>
> | Metric | Detail |
> | :--- | :--- |
> | **Model Name** | Qwen-LookAgain (Qwen-LA) |
> | **Base Architecture** | Qwen2.5-VL-7B-Instruct |
> | **Core Method** | Balanced Reflective Policy Optimization (BRPO) |
> | **Key Mechanisms** | Visual Token COPY & Visual Token ROUTE |
> | **Training Dataset** | Qwen-Zero-40k |
> | **Quality Score** | 7/10 |
> | **References** | 40 Citations |

---

## üìù Executive Summary

This paper addresses **"Visual Token Dilution,"** a critical phenomenon in Vision-Language Reasoning Models (VLRMs) where extended reasoning chains cause a progressive loss of visual attention and a corresponding increase in hallucinations. The authors demonstrate that as reasoning depth increases, models inevitably drift away from visual grounding, leading to higher error rates during long-horizon tasks.

A key finding is that existing **"text-only reflection"** methods are insufficient to solve this problem; while they attempt to correct errors, they fail because they cannot re-establish the lost connection to the visual data, leaving the model unable to effectively suppress hallucinations.

To counter this, the authors introduce **Qwen-LookAgain (Qwen-LA)**, a framework built on Qwen2.5-VL-7B-Instruct designed to force models to re-attend to visual information through a vision-text reflection process. The solution relies on three distinct technical components:

1.  **Balanced Reflective Policy Optimization (BRPO):** A reinforcement learning algorithm that trains the model to spontaneously decide when to generate reflections.
2.  **Visual Token COPY:** A mechanism that physically forces the model to re-attend to visual information at the token level.
3.  **Visual Token ROUTE:** Which routes visual tokens to ensure the model maintains focus on the data throughout the generation process.

Together, these mechanisms decouple the reasoning process from visual information loss, allowing for long chains of thought without sacrificing grounding. Experimental results validate the efficacy of the Qwen-LA framework through rigorous diagnostic analysis. Using LLaVA-CoT as a baseline on MSCOCO examples, the study confirms that CHAIRi (a hallucination metric) increases while Recall decreases as generation length grows‚Äîa trend that Qwen-LA successfully reverses.

While text-only reflection fails to recover visual attention weights, Qwen-LA achieves leading accuracy across multiple visual QA datasets and demonstrates significantly reduced hallucination rates compared to baseline approaches. Furthermore, training dynamics on the Qwen-Zero-40k dataset indicate the model successfully learns to insert reflections spontaneously at optimal positions, with reflection length decreasing over iterations as the model becomes more efficient.

---

## üîç Key Findings

*   **Visual Token Dilution:** Extended reasoning chains in VLRMs dilute visual tokens, leading to reduced visual attention and increased hallucinations.
*   **Insufficiency of Text-Only Reflection:** Text-only reflection is insufficient for suppressing hallucinations in VLMs because it fails to address the loss of visual grounding.
*   **Attention Restoration:** Supplementing visual information during the reflection process effectively enhances visual attention.
*   **Performance Validation:** The Qwen-LA model achieves leading accuracy across multiple visual QA datasets while reducing hallucination rates.

---

## ‚öôÔ∏è Methodology

The paper proposes **Qwen-LookAgain (Qwen-LA)**, a VLRM designed to guide models to re-attend to visual information through a vision-text reflection process. The methodology consists of three core components:

1.  **Balanced Reflective Policy Optimization (BRPO):**
    *   A reinforcement learning method that trains the model to decide *when* to generate vision-text reflections.
    *   Induces spontaneous reflection without explicit external triggers.

2.  **Visual Token COPY:**
    *   A mechanism to force the model to re-attend to visual information at the visual level.
    *   Physically reintroduces visual data into the processing stream.

3.  **Visual Token ROUTE:**
    *   A mechanism that helps route visual tokens to ensure the model maintains focus on visual data.
    *   Ensures that critical visual features are preserved throughout the generation process.

---

## üìã Technical Details

*   **Problem Addressed:** 'Visual Token Dilution'‚Äîwhere extended reasoning chains in Vision-Language Reasoning Models (VLRMs) reduce attention to visual tokens, causing hallucinations.
*   **Base Model:** Built upon **Qwen2.5-VL-7B-Instruct**.
*   **Training Algorithm:** Trained using Reinforcement Learning with the Balanced Reflective Policy Optimization (BRPO) algorithm.
*   **Theoretical Proof:** The authors provide a formal proof regarding the decay of visual attention during autoregressive generation, establishing a link between reasoning depth, visual attention, and hallucination.
*   **Model Variants:**
    *   **Qwen-Zero**
    *   **Qwen-Zero-40k** (Dataset used for training dynamics)

---

## üß± Contributions

*   **Theoretical Insight:** Provides a formal proof that VLRMs lose attention to visual tokens as reasoning chains lengthen, establishing a link between reasoning depth, visual attention, and hallucination.
*   **Novel Reinforcement Learning Framework:** Introduces Balanced Reflective Policy Optimization (BRPO), a new RL approach to manage the dynamics of reflection generation in multi-modal reasoning.
*   **Architectural Innovation:** Proposes "**Visual Token COPY**" and "**Visual Token ROUTE**" as architectural solutions to enforce visual-level re-attention.
*   **Effective Hallucination Mitigation:** Presents a comprehensive solution that decouples extended reasoning from visual information loss, setting a new standard for accuracy and reliability in vision-language tasks.

---

## üìä Results

*   **Diagnostic Analysis (MSCOCO):** Using LLaVA-CoT as a baseline, results show that **CHAIRi** (hallucination metric) increases while **Recall** decreases as generation length increases.
*   **Visual Attention Recovery:** Text-only reflection failed to recover visual attention weights, whereas Qwen-LA successfully restored them.
*   **Performance:** Qwen-LA achieves leading accuracy across multiple visual QA datasets and demonstrates reduced hallucination rates compared to baseline approaches.
*   **Training Dynamics:** The model learns to insert reflections spontaneously at various positions, with reflection length decreasing over iterations, indicating increased efficiency.

---
*References: 40 citations*