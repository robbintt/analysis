# A Survey of Continual Reinforcement Learning

*Chaofan Pan; Xin Yang; Yanhua Li; Wei Wei; Tianrui Li; Bo An; Jiye Liang*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Document Type** | Survey Paper Analysis |
| **Focus Domain** | Continual Reinforcement Learning (CRL) |
| **Citations Count** | 40 |
| **Analysis Rating** | 5/10 |
| **Core Contribution** | Novel Taxonomy (4 Types) |
| **Key Environments** | ViZDoom, Meta-World, Atari 2600, StarCraft II |

---

> âš¡ **EXECUTIVE SUMMARY**
>
> Standard Reinforcement Learning (RL) is fundamentally limited by its assumptions of static environments and extensive data availability, rendering it ineffective for dynamic, real-world deployment. The core challenges are high computational retraining costs, limited generalization, and the **"stability-plasticity dilemma,"** where agents suffer from catastrophic forgettingâ€”acquiring new skills inherently degrades performance on previously mastered tasks.
>
> Continual Reinforcement Learning (CRL) addresses these critical failures by enabling agents to learn incrementally over a lifetime, but the field lacks a unified framework. The authors introduce a rigorous, novel taxonomy that categorizes CRL methods based on their technical mechanisms for knowledge storage and transfer, distinguishing these clearly from learning scenario settings.
>
> This framework classifies methodologies into two primary technical approaches:
> 1.  **Regularization-based:** (e.g., Elastic Weight Consolidation) which constrains parameter updates.
> 2.  **Experience-focused:** Utilizing Complementary Learning System architectures with dual memory buffers.
>
> The survey separates these methodologies from scenario definitions (Lifelong Adaptation, Non-Stationarity Learning, Task Incremental Learning, and Task-Agnostic Learning).
>
> Rather than reporting single-algorithm scores, the paper advances the field by standardizing evaluation protocols using three key metrics: **Performance Degradation**, **Plasticity**, and **Task Knowledge**. It systematically categorizes benchmarks ranging from CRL Maze to complex scalability tests in StarCraft II and Minecraft. By establishing a standardized taxonomy and clear assessment criteria, this work bridges the gap between theoretical capabilities and practical application.

---

## Key Findings

*   **Limitations of Standard RL:** Standard RL is hindered by extensive data requirements, high computational costs, and poor generalization capabilities.
*   **The CRL Solution:** Continual Reinforcement Learning (CRL) mitigates these issues by facilitating continuous learning processes and actively preventing knowledge loss.
*   **Novel Taxonomy:** The authors propose a new taxonomy categorizing CRL methods into four distinct types based on how knowledge is stored and transferred.
*   **Standardization:** The survey successfully organizes the diverse landscape of evaluation metrics, tasks, and benchmarks, providing a cohesive structure for the literature.

## Methodology

The authors employed a systematic approach to analyze the state of Continual Reinforcement Learning:

1.  **Systematic Literature Review:** Conducted a comprehensive survey of existing CRL studies.
2.  **Categorization:** Analyzed studies based on evaluation metrics, specific tasks, benchmarks, and scenario settings.
3.  **Taxonomy Synthesis:** Developed a novel taxonomy by synthesizing methodologies, specifically focusing on knowledge storage and transfer mechanisms.
4.  **Gap Analysis:** Identified research gaps and needs through a comparative analysis of existing methods.

## Contributions

*   **Taxonomy Development:** Introduced a novel taxonomy dividing CRL methods into four types based on knowledge storage and transfer mechanisms.
*   **Comprehensive Review:** Provided a holistic review of CRL literature, including systematic breakdowns of metrics, tasks, and benchmarks.
*   **Future Insights:** Delivered an analysis of the unique challenges facing CRL and offered practical insights to guide future research efforts.

## Technical Details

### CRL Methods & Architecture

The paper outlines a technical taxonomy distinguishing between methodological approaches and scenario settings.

**1. Regularization-Based Approaches**
These methods constrain parameters to protect important knowledge.
*   **Elastic Weight Consolidation (EWC):** Constrains parameters using the Fisher Information Matrix.
*   **Progress & Consolidate (P&C):** An Online-EWC variant designed for incremental Fisher matrix updates.
*   **Policy Consolidation (PC):** Utilizes cascaded hidden networks.
*   **TRAC:** An adaptive regularization optimizer.

**2. Experience-Focused Approaches**
These methods rely on memory systems.
*   **Complementary Learning System (CLS):** Uses an architecture with short-term and long-term experience buffers.
*   **Generative Models:** Employed for replay to simulate past experiences.

### Scenario Settings
The paper categorizes the operational contexts for CRL into four distinct settings:
*   Lifelong Adaptation
*   Non-Stationarity Learning
*   Task Incremental Learning
*   Task-Agnostic Learning

## Evaluation & Benchmarks

As a survey paper, the focus is on defining standards for evaluation rather than reporting specific experimental results.

### Key Metrics
The authors formalize three critical metrics for assessing CRL agents:
*   **Performance Degradation:** Measures the extent of catastrophic forgetting.
*   **Plasticity:** The agent's ability to adapt effectively to new tasks.
*   **Task Knowledge:** Balances performance on new tasks against performance across the entire sequence of tasks.

### Standard Benchmarks
The survey identifies and organizes key environments used in the literature:
*   **CRL Maze:** Implemented via ViZDoom.
*   **Lifelong Hanabi**
*   **Continual World:** Based on the Meta-World environment.
*   **L2Explorer:** Uses 3D Procedural Content Generation (PCG).
*   **CORA**

### Scalability Assessments
For testing high-dimensional and complex control strategies, the following environments are utilized:
*   **Atari 2600**
*   **Minecraft**
*   **StarCraft II**
*   **Real-world Robotics Tasks**

---

*Analysis Quality Score: 5/10 | References Included: 40 Citations*