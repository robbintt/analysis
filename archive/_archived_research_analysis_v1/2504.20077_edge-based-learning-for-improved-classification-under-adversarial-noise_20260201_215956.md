# Edge-Based Learning for Improved Classification Under Adversarial Noise

*Manish Kansana; Keyan Alexander Rahimi; Elias Hossain; Iman Dehzangi; Noorbakhsh Amiri Golilarz*

---

> ### Quick Facts
> 
> | **Metric** | **Value** |
> |------------|-----------|
> | **Quality Score** | 6/10 |
> | **Citation Count** | 33 references |
> | **Attack Vector** | FGSM (Fast Gradient Sign Method) |
> | **Perturbation Budget** | ε = 0.05 (ℓ∞-bounded) |
> | **Evaluation Domains** | Neuro-oncology & Pulmonary Imaging |
> | **Datasets** | Brain Tumor Dataset, COVID-19 Dataset |
> | **Architecture** | ResNet-50 CNN |
> | **Input Resolution** | 224×224 px (analysis patches: 32×32 px) |
> | **Robustness Gain** | 45–47 percentage points under attack |
> | **Clean Accuracy Trade-off** | 3–5 percentage points |

---

## Executive Summary

Deep learning classifiers in neuro-oncology and pulmonary imaging face critical adversarial vulnerabilities that compromise clinical deployment safety. Conventional defense mechanisms—adversarial training and input augmentation—provide only partial protection while imposing substantial computational overhead. This research reveals a fundamental spatial heterogeneity in adversarial attacks: gradient-based methods (specifically FGSM with ℓ∞-bounded perturbations) disproportionately exploit complex textural regions while sparing geometric structural boundaries.

The authors propose **edge-exclusive learning**, a feature-centric defense strategy that reduces reliance on texture-rich attack surfaces by training classifiers exclusively on structural boundary representations. Using Canny edge detection to preprocess 224×224 medical images into binary edge maps, ResNet-50 CNNs are trained exclusively on these sparse geometric representations—without adversarial fine-tuning—forcing classifiers to rely on gradient-resistant structural features.

**Key Performance Results:**
- **Standard Models:** Accuracy dropped from 94.2% → 41.3% (brain tumor) and 91.5% → 38.7% (COVID-19) under FGSM attack
- **Edge-Exclusive Models:** Accuracy maintained at 87.1% (brain tumor) and 85.4% (COVID-19) under identical attack conditions
- **Spatial Analysis:** 73% of adversarial gradient magnitude concentrated in non-edge textural regions

This framework establishes that adversarial vulnerability is not uniformly distributed but concentrated in textural complexity, offering a computationally efficient pathway for securing clinical AI without expensive adversarial retraining pipelines. The approach enables deployment of reliable diagnostic systems maintaining >85% accuracy under adversarial attack, though with reduced sensitivity for pathologies manifesting primarily as textural rather than structural changes.

---

## Key Findings

• **Superior Adversarial Resilience:** Deep learning models trained exclusively on edge-extracted representations demonstrate significantly greater robustness against FGSM adversarial attacks compared to models trained on original/clean images.

• **Differential Regional Vulnerability:** Adversarial noise disproportionately exploits complex non-edge regions for misclassification, while edge structures remain relatively stable and preserve essential structural information necessary for accurate classification.

• **Marginal Retraining Trade-off:** While retraining on combined clean and adversarially perturbed images improves performance for both approaches, the accuracy improvement is marginally higher for models using original data compared to those using edge-based representations.

• **Structural Stability Under Perturbation:** Edge features maintain classification integrity when adversarial noise is introduced, suggesting that geometric boundaries are less susceptible to gradient-based manipulations than textural or complex interior regions.

---

## Methodology

The study employed a comparative experimental protocol utilizing the **Fast Gradient Sign Method (FGSM)** to generate adversarial perturbations on medical imaging datasets. The research design involved three distinct training phases:

1. **Baseline Training:** Training on pristine images to establish vulnerability baselines
2. **Adversarial Retraining:** Fine-tuning using combinations of clean and noisy images to assess standard robustness improvements
3. **Edge-Isolation Training:** Training exclusively on edge-extracted representations derived from original/clean images (Canny edge detection)

**Robustness Evaluation:** Adversarial perturbations were introduced to test sets (ε = 0.05) and classification accuracy degradation was compared between edge-based models and standard models trained on full images.

**Datasets:** Brain Tumor Dataset and COVID-19 Dataset (224×224 pixel resolution with 32×32 patch analysis).

---

## Technical Details

### Attack Specification

**Method:** Fast Gradient Sign Method (FGSM) — White-box attack exploiting model gradients

**Mathematical Formulation:**
- **Perturbation:** η = ε · sign(∇ₓ J(θ, x, y))
- **Adversarial Example:** x̃ = x + η

**Where:**
- θ = model parameters
- x = input image
- y = true label
- J = cost function
- ε = perturbation magnitude scalar (0.05)
- ∇ₓ = gradient with respect to input

**Constraints:** ℓ∞-bounded perturbation for untargeted misclassification; perturbations designed to be imperceptible to human observers while causing severe classifier degradation.

### Training Architectures

| **Paradigm** | **Description** |
|--------------|----------------|
| **Baseline** | Standard training on original 224×224 clean images |
| **Edge-Exclusive** | Training exclusively on edge-extracted representations (structural/geometric boundaries only) |
| **Adversarial Retraining** | Fine-tuning on combined dataset of clean + adversarially perturbed images |

**Preprocessing:** Edge detection performed on original/clean images to create binary edge-based representations containing exclusively geometric contour information.

---

## Results

**Resilience Metrics:**
- Edge-based models demonstrate significantly greater resilience to FGSM attacks compared to standard models trained on original/clean images
- **Standard Model Degradation:** Brain tumor classification dropped from 94.2% to 41.3%; COVID-19 detection dropped from 91.5% to 38.7%
- **Edge-Model Retention:** Brain tumor classification maintained 87.1%; COVID-19 detection maintained 85.4%
- **Net Advantage:** 45–47 percentage point resilience advantage under adversarial conditions

**Spatial Vulnerability Analysis:**
- FGSM introduces non-important pixels particularly around edges and image boundaries (observed in 32×32 patch analysis)
- 73% of adversarial gradient magnitude concentrated in non-edge textural regions
- Geometric boundaries exhibit relative stability while textural/complex interior regions degrade under perturbation
- Adversarial retraining on combined clean + noisy images improves performance for both approaches, with marginally higher gains for original-data models

**Trade-offs:**
- Edge-exclusive models achieved slightly lower clean accuracy on unperturbed images (approximately 3–5 percentage points reduction vs. standard models)
- Significant misclassification observed when subtle adversarial perturbations introduced to models trained solely on clean data

---

## Contributions

• **Feature-Centric Adversarial Defense:** Establishes edge-based learning as a novel defensive strategy that leverages inherent geometric stability of structural boundaries rather than relying solely on adversarial training or input augmentation techniques.

• **Region-Specific Vulnerability Analysis:** Provides empirical evidence that adversarial vulnerabilities are not uniformly distributed across image space, demonstrating that gradient-based attacks primarily target non-edge complexities while sparing structural contours.

• **Medical Imaging Robustness Framework:** Validates the edge-based robustness hypothesis in critical diagnostic domains (neuro-oncology and pulmonary imaging), offering a pathway for deploying reliable AI diagnostic systems resistant to adversarial manipulation in clinical settings.

• **Theoretical Insight into Adversarial Mechanics:** Challenges the assumption that all local image features are equally exploitable by revealing that essential structural information encoded in edges remains resistant to small-norm perturbations that successfully mislead standard classifiers.

---

## References

**Citation Count:** 33 references

*Note: This analysis is based on a research paper investigating edge-based learning approaches for adversarial robustness in medical imaging applications.*