---
title: A Study of Commonsense Reasoning over Visual Object Properties
arxiv_id: '2508.10956'
source_url: https://arxiv.org/abs/2508.10956
generated_at: '2026-02-04T15:53:48'
quality_score: 7
citation_count: 22
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# A Study of Commonsense Reasoning over Visual Object Properties

*Abhishek Kolari; Mohammadhossein Khojasteh; Yifan Jiang; Floris den Hengst; Filip Ilievski*

---

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Quality Score** | 7/10 |
| **References** | 22 Citations |
| **Benchmarks Introduced** | OPTICS-CNT, OPTICS-CMP |
| **Models Tested** | 12 State-of-the-art VLMs |
| **Top Accuracy (Counting)** | < 40% |
| **Top Accuracy (Comparison)** | < 70% |
| **Core Focus** | Commonsense Reasoning & Visual Properties |

---

> ## üìù Executive Summary
>
> State-of-the-art Vision-Language Models (VLMs) currently exhibit a significant performance gap compared to humans in commonsense reasoning regarding visual object properties, a deficiency stemming from existing benchmarks that conflate low-level perception with high-level reasoning. To address this, the authors introduce **OPTICS** (Object Property Reasoning Tasks for Evaluating Image-based Common Sense), a novel evaluation framework designed to isolate reasoning capabilities across four Object Property Dimensions, three levels of Reasoning Complexity, and three Image Types, instantiated into two distinct benchmarks, **OPTICS-CNT** and **OPTICS-CMP**.
>
> Zero-shot experiments on 12 state-of-the-art VLMs revealed substantial weaknesses, with the best model achieving less than 40% accuracy on counting tasks and below 70% on comparison tasks, specifically struggling with photographic images, counterfactual reasoning, and physical or functional object properties. This work establishes the first benchmark to systematically test defined reasoning types and property dimensions across varied image types, providing a critical tool for the research community and setting a rigorous standard for evaluating VLM abstraction capabilities to drive the development of advanced models.

---

## üîç Key Findings

*   **Significant Performance Gap:** The best-performing VLM achieved less than **40% accuracy** on counting tasks and below **70% on comparison** tasks in zero-shot settings, highlighting a major disparity compared to human performance.
*   **Specific Deficiencies:** VLMs struggle disproportionately with:
    *   Photographic images (compared to animated or AI-generated).
    *   Counterfactual reasoning scenarios.
    *   Physical and functional object properties.
    *   Tasks involving higher object counts.
*   **Benchmark Limitations:** Existing Visual Question Answering (VQA) studies often conflate perception with reasoning and lack representativeness in reasoning and image categories, obscuring the true reasoning abilities of VLMs.
*   **Reasoning vs. Perception:** There is a critical need to isolate high-level abstraction capabilities from low-level visual recognition in model evaluation.

---

## üõ†Ô∏è Methodology

The authors developed a systematic evaluation framework grounded in prior common sense research to isolate reasoning capabilities.

*   **Framework Structure:** The study is organized around three core pillars:
    *   **Three Representative Image Types:** Photographic, Animated, and AI-Generated.
    *   **Three Reasoning Levels:** Increasing complexity from direct recognition to counterfactuals.
    *   **Four Object Property Dimensions:** Physical, Taxonomic, Functional, and Relational.
*   **Benchmark Instantiation:** This framework was instantiated into two specific VQA benchmarks:
    *   **OPTICS-CNT:** 360 images, 1,080 questions (Focus on Counting).
    *   **OPTICS-CMP:** 2,100 questions (Focus on Comparison).
*   **Experimental Setup:** The researchers conducted **zero-shot experiments** testing 12 state-of-the-art VLMs against these benchmarks to strictly assess reasoning without task-specific fine-tuning.

---

## ‚öôÔ∏è Technical Details

**Framework Name:** OPTICS (Object Property Reasoning Tasks for Evaluating Image-based Common Sense)

**Integrated Components:**
*   **Object Property Dimensions:**
    *   Physical
    *   Taxonomic
    *   Functional
    *   Relational
*   **Reasoning Complexity Levels:**
    *   Direct Recognition
    *   Property Inference
    *   Counterfactual Reasoning
*   **Image Types:**
    *   Photographic
    *   Animated
    *   AI-Generated

**Evaluation Focus:** Specifically targets **Counting** and **Comparison** questions to stress-test numerical and relational logic.

---

## üìà Results

*   **General Performance:** Confirmed the low accuracy rates reported in the key findings, with top models failing to reach human-level competence.
*   **Problem Areas:**
    *   Models failed to handle **counterfactual reasoning** effectively.
    *   Performance degraded as the complexity of **physical and functional properties** increased.
    *   **Higher counts** in images led to higher error rates.
*   **Benchmark Distinction:** OPTICS is distinguished as the first benchmark to systematically test all defined reasoning types and property dimensions across various image types, providing a more holistic view of model failure modes than previous datasets.

---

## üöÄ Contributions

1.  **OPTICS Benchmark:** Introduction of a novel, comprehensive dataset (OPTICS-CNT and OPTICS-CMP) designed to rigorously test commonsense reasoning over visual object properties across multiple complexity levels.
2.  **Structured Evaluation Methodology:** Establishment of a new standard for evaluation that accounts for diverse image types, reasoning complexities, and property dimensions to better assess VLM abstraction.
3.  **Open Source Release:** Public release of the benchmark data and code to facilitate future research in scalable benchmarking methods, generalized annotation guidelines, and the development of advanced reasoning VLMs.