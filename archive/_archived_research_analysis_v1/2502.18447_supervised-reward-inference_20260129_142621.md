# Supervised Reward Inference

*Will Schwarzer; Jordan Schneider; Philip S. Thomas; Scott Niekum*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 6/10 |
| **References** | 40 Citations |
| **Framework** | Supervised Learning / Regression |
| **Core Advantage** | Handles Arbitrary/Suboptimal Behavior |
| **Data Requirement** | Behaviors + Ground-truth Rewards |

---

## Executive Summary

> Traditional Inverse Reinforcement Learning (IRL) is fundamentally limited by strict assumptions regarding human rationality and optimality. Because it relies on inverse planning, IRL struggles to extract accurate reward functions from complex, suboptimal human behavior, often failing when human actions deviate from the expected optimal trajectory. This paper addresses this limitation to enable machines to learn effectively from imperfect human demonstrations, a critical requirement for robust human-robot interaction.
>
> The authors introduce **"Supervised Reward Inference" (SRI)**, a paradigm shift that reframes reward inference as a direct supervised learning regression problem. To achieve this, SRI employs a necessary dual-path design: a **"Blue Path"** (Trajectory Encoder and Set Transformer) processes the full behavior history to create a fixed task representation, while a **"Red Path"** (MLP or CNN) processes instantaneous state information. This architectural separation is critical because it decouples the task context (derived from history) from the state evaluation, allowing the model to handle variable-length trajectories and attribute rewards based on broader context without requiring a dynamics model or assumptions of agent rationality.
>
> Empirical validation on simulated robotic manipulation and navigation tasks demonstrated SRI's efficacy with specific performance capabilities. SRI achieved asymptotic Bayes-optimality and successfully inferred rewards even when provided with arbitrarily suboptimal demonstrations. While the approach introduces the challenge of requiring a labeled dataset of behaviors paired with ground-truth rewards, it eliminates the need for rationality assumptions and dynamics models, offering a more flexible path to deploying agents in complex, real-world environments.

---

## Key Findings

*   **Unified Framework:** Supervised learning serves as a unified framework to infer reward functions from any class of behavior, eliminating the need for strict assumptions regarding human rationality.
*   **Broad Applicability:** The proposed method successfully handles a wide spectrum of human behaviors, including suboptimal actions and communicative gestures.
*   **Theoretical Optimality:** The approach is proven to be asymptotically Bayes-optimal under mild assumptions.
*   **Robust Performance:** Empirically, the method efficiently and accurately inferred rewards in simulated robotic manipulation tasks even when provided with arbitrarily suboptimal demonstrations.

---

## Methodology

The authors propose framing reward inference as a **supervised learning task**. This represents a significant departure from existing methods:

*   **Contrast with Model-Based Approaches:** Unlike existing approaches that rely on specific, pre-defined models of behavior (inverse planning), this approach treats inference purely as a learning problem.
*   **Generalization:** It is applicable to any behavior class, allowing generalization across diverse and suboptimal demonstrations.
*   **Assumption Reduction:** It operates without restrictive assumptions about how the demonstrations were generated (e.g., optimality or bounded rationality).

---

## Technical Details

### Core Concept: Supervised Reward Inference (SRI)
SRI reframes Inverse Reinforcement Learning as a direct supervised regression problem to map behaviors to rewards. It is proven to be asymptotically Bayes-optimal.

### Mathematical Objective
The objective minimizes the expected loss:

$$L(g_{\theta_g}(S, f_{\theta_f}(\{T_n\})), R(S))$$

This utilizes two primary function families:
1.  **Task Encoder ($f$):** Maps trajectories to a task representation vector.
2.  **State/Reward Encoder ($g$):** Maps the state and task representation to a scalar reward.

### Dual-Path Architecture
The system relies on a distinct two-path structure to separate context from state processing:

*   **ðŸ”µ Blue Path:** Consists of a **Trajectory Encoder** (Set Transformer) that processes behavior trajectories. This computes a task representation once per task.
*   **ðŸ”´ Red Path:** Consists of an **MLP or CNN** that processes individual states per timestep.
*   **Fusion:** A **Reward MLP** fuses the representations from both paths to generate the final scalar reward.

### System Constraints & Capabilities
*   **Rationality:** Does not assume human rationality.
*   **Frameworks:** Operates in both MDP and POMDP frameworks.
*   **Reward Type:** Supports action-based rewards.
*   **Prerequisite:** Requires a dataset of behaviors paired with ground-truth rewards.

---

## Contributions

*   **Paradigm Shift:** Introduced a novel supervised learning framework for reward inference that generalizes across diverse behavioral types, moving beyond the limitations of model-based IRL.
*   **Theoretical Guarantees:** Provided theoretical guarantees proving the proposed supervised approach achieves asymptotic Bayes-optimality.
*   **Empirical Validation:** Validated the practical efficacy of the method on complex, simulated robotic manipulation tasks, highlighting the ability to learn from arbitrarily suboptimal data.

---

## Results

While specific quantitative metrics were not provided in the text, qualitative findings and capability comparisons highlight the method's strengths:

*   **Performance:** SRI is described as robust, fast, and data-efficient.
*   **Inference Scope:** Unique in its ability to perform inference from an "arbitrary behavior class."
*   **Few-Shot Learning:** Supports few-shot inference (similar to Meta-IRL) and can operate without a simulator.
*   **Comparative Advantage:**
    *   **Outperforms** standard IRL, Meta-IRL, CIRL, and Inverse Planning in scope regarding behavior diversity.
    *   **Differs** in that it strictly requires a labeled reward dataset for training.