# Memoryless Policy Iteration for Episodic POMDPs
*Roy van Zuijlen; Duarte Antunes*

---

> ### ðŸ“Š Quick Facts
>
> *   **Focus**: Episodic POMDPs, Policy Iteration, Non-Markovian output processes
> *   **Method**: Periodic Policy Iteration (Model-based & Model-free)
> *   **Novelty**: Optimizing in output space vs. belief space
> *   **Quality Score**: 5/10
> *   **Citations**: 14

---

## 1. Executive Summary

This paper addresses the computational complexity of solving Episodic Partially Observable Markov Decision Processes (POMDPs), specifically focusing on the challenge of optimizing deterministic, memoryless, and finite-memory policies. A fundamental obstacle in this domain is that while the underlying environment is Markovian, the observation process generated by these policies is non-Markovian. This non-Markovian nature typically invalidates standard dynamic programming techniques and necessitates computationally prohibitive belief-state representations. The study aims to resolve these theoretical and computational barriers by establishing a framework for efficient policy iteration that operates directly in the output space, bypassing the complexities of belief tracking.

The core innovation is **Periodic Policy Iteration**, a novel framework that extends policy iteration guarantees to domains governed by non-Markovian output processes. Instead of requiring complex belief-state representations, the method performs updates in the output space using a periodic schedule with period $M$. The algorithm alternates between single-stage greedy policy improvements and evaluations, distinguishing itself through a rigorous forward/backward decomposition. This decomposition optimizes computational efficiency by separating the update logic for state distributions ($\mu_t$) from that of state-action values ($Q^\pi_t$). The authors prove that the algorithm guarantees monotonic policy improvements provided the update schedule is onto and non-redundant, and the method includes a model-free variant capable of estimating values directly from data in environments with unknown dynamics.

The theoretical analysis confirms that the algorithm converges when the policy remains unchanged over $M$ consecutive steps. Regarding performance benchmarks, the provided text explicitly notes that specific empirical speedup factors for the proposed method are absent; however, it contextualizes the results against existing baselines, noting that state-of-the-art Mixed-Integer Linear Programming (MILP) methods achieve values within 20% of the optimal solution in only 60% of test problems. While the full numerical results for the proposed method are not detailed in the provided text, the authors claim "substantial computational speedups" over these baselines and demonstrate the method's viability in both model-based and model-free settings through theoretical guarantees and abstract claims.

This research significantly impacts the field by successfully extending the theoretical guarantees of policy iteration to non-Markovian settings and finite-memory policies. By removing the dependency on belief-state representations and proving the viability of optimized periodic update patterns, the approach lowers the computational barrier for solving episodic POMDPs. The introduction of a model-free variant further enhances the method's practical utility, allowing for application in real-world scenarios where system models are unknown and establishing a theoretical foundation for future high-performance POMDP solvers.

---

## 2. Key Findings

*   **Novel Policy-Iteration Algorithms:** Introduction of algorithms that guarantee monotonic policy improvements for memoryless policies in POMDPs, specifically addressing challenges posed by non-Markovian output processes.
*   **Optimized Iteration Patterns:** Identification of optimal periodic patterns for alternating between single-stage improvements and evaluations to maximize computational efficiency.
*   **Computational Efficiency:** Proposed methods demonstrate substantial computational speedups compared to state-of-the-art baselines.
*   **Dual Applicability:** The approach is validated for effectiveness in both model-based and model-free settings.

---

## 3. Methodology

The authors propose a policy-iteration framework for memoryless and finite-memory policies operating in the **output space** rather than the belief space.

*   **Core Mechanism:** The method alternates between single-stage output-based policy improvements and policy evaluations according to a prescribed periodic pattern.
*   **Non-Markovian Analysis:** The structure is specifically analyzed to handle the non-Markovian nature of the output process, a common hurdle in POMDP solvers.
*   **Model-Free Variant:** A model-free version of the algorithm is developed, which estimates values directly from data without requiring prior knowledge of the environment model.

---

## 4. Technical Details

The paper proposes "Periodic Policy Iteration," defined by the following technical specifications:

*   **Target Problem:** Episodic POMDPs using deterministic, memoryless policies mapping observations to actions.
*   **Algorithm:** Periodic Policy Iteration utilizes a periodic schedule with period $M$ to update policies greedily at specific time steps rather than simultaneously.
*   **Convergence Guarantees:** The algorithm guarantees monotonic improvement if the schedule is **onto** and **non-redundant**.
*   **Computational Decomposition:**
    *   Employs **forward/backward decomposition** for efficiency.
    *   Distinguishes between updates to state distributions ($\mu_t$) versus state-action values ($Q^\pi_t$).
*   **Cost Minimization:** A cost index $C$ is formulated to minimize computational costs based on the update direction.
*   **Extensions:** Includes model-free extensions for both state-informed and observation-only settings.

---

## 5. Results & Benchmarks

*   **Empirical Data:** The provided text notes that specific empirical results are not present in the analysis.
*   **Performance Claims:** The authors claim the methods offer "substantial computational speedups" over state-of-the-art baselines.
*   **Baseline Context:** Contextual benchmarks cite a MILP baseline achieving values within 20% of optimal in 60% of problems.
*   **Theoretical Metrics:** Convergence is confirmed to occur if the policy does not change over $M$ consecutive steps.
*   **Optimization Objective:** The process focuses on minimizing the cost index $C$.

---

## 6. Contributions

1.  **Theoretical Framework:** Established a framework for POMDP policy iteration that handles non-Markovian interdependencies.
2.  **Complexity Analysis:** Provided a theoretical analysis of iteration patterns to minimize complexity.
3.  **Model-Free Innovation:** Developed a model-free variant for environments with unknown models using data-based value estimation.
4.  **Benchmarking:** Established a new performance benchmark, demonstrating significant speed advantages over existing methods across multiple examples.