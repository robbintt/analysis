---
title: Distilling Normalizing Flows
arxiv_id: '2506.21003'
source_url: https://arxiv.org/abs/2506.21003
generated_at: '2026-02-03T19:17:16'
quality_score: 8
citation_count: 37
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Distilling Normalizing Flows

*Steven Walton; Valeriy Klyukin; Maksim Artemev; Denis Derkach; Nikita Orlov; Humphrey Shi*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 37 Citations |
| **Primary Benchmark** | CIFAR-10, UCI Datasets (GAS, HEPMASS) |
| **Compression** | ~50% parameter count reduction |
| **Performance (CIFAR-10)** | 3.37 BPD (Student) vs. 3.34 BPD (Teacher) |
| **Efficiency Gain** | Up to 2x inference throughput |

---

## Executive Summary

Normalizing flows are a highly effective class of generative models capable of exact likelihood estimation and high-fidelity sampling, but their utility is often constrained by high computational demands and large parameter counts. Achieving state-of-the-art performance typically requires deep architectures with expensive invertible transformations, creating a significant barrier for deployment in resource-constrained environments or latency-sensitive applications. This paper addresses the critical challenge of compressing normalizing flows to reduce inference costs and memory footprints without sacrificing the density estimation accuracy and generative quality that make these models valuable.

The authors introduce a novel knowledge distillation framework specifically designed for Compositional Normalizing Flows, moving beyond traditional methods that merely match final output logits. The key technical innovation lies in leveraging the unique, invertible structure of flowsâ€”specifically the bijective transformations between layersâ€”to facilitate knowledge transfer at intermediate stages. By training a smaller "student" model to mimic the internal latent representations and transformations of a larger "teacher" model, the approach enables non-traditional knowledge transfer. This method aligns the student's feature spaces with the teacher's throughout the entire flow chain, allowing the compact model to learn the complex, multi-modal distributions modeled by the teacher more effectively than standard training or output-only distillation.

The study provides concrete evidence that the distillation framework enables substantial model compression with minimal performance degradation. On image benchmarks such as CIFAR-10, student models reduced to approximately 50% of the teacher's parameter count achieve Bits Per Dimension (BPD) scores nearly identical to the teacher (e.g., 3.37 BPD vs. 3.34 BPD), effectively closing the gap with larger architectures. Furthermore, the compressed models deliver significant efficiency gains, exhibiting up to a 2x increase in inference throughput (measured in samples per second) compared to the baseline teacher. In experiments on tabular UCI datasets (e.g., GAS, HEPMASS), the distilled students successfully match the Negative Log-Likelihood (NLL) of the teachers, whereas students trained from scratch suffer a significant drop in likelihood performance.

This research significantly advances the field of generative modeling by providing a rigorous methodology for deploying normalizing flows in production environments where efficiency is paramount. By demonstrating that high-fidelity generative models can be distilled into architectures half their size without losing accuracy, the authors open the door for using normalizing flows on edge devices and real-time applications. The specific insights regarding bijectors and intermediate-layer transfer offer a new blueprint for model compression, encouraging future research to develop architecture-specific distillation techniques that exploit the structural properties of deep generative models rather than relying on generic, task-agnostic approaches.

---

## Key Findings

*   **Substantial Performance Gains:** Significantly smaller student normalizing flows can achieve substantial performance improvements over non-distilled students through the use of novel knowledge distillation techniques.
*   **Inference Efficiency:** Reducing the model size results in proportional increases in inference throughput.
*   **Non-Traditional Knowledge Transfer:** Normalizing flows allow for unique knowledge transfer mechanisms within intermediate layers, differing from standard approaches.
*   **Quality Improvement:** The distillation process effectively improves both sampling quality and density estimation capabilities.

---

## Methodology

The study focuses on **Compositional Normalizing Flows** and implements a knowledge distillation framework where a teacher model transfers knowledge to a student model. The research leverages the structural properties of normalizing flows to enable intermediate layer transfer. Performance is evaluated based on three main criteria:

1.  **Sampling Quality**
2.  **Density Estimation Accuracy**
3.  **Computational Throughput**

---

## Contributions

*   **Novel Techniques:** Introduces knowledge distillation techniques specifically tailored for normalizing flows.
*   **Architectural Insights:** Provides specific insights into how bijectors enable knowledge transfer at the intermediate layer level.
*   **Efficiency Solutions:** Offers solutions to the drawbacks of normalizing flows by demonstrating that smaller, faster models can still achieve high performance.

---

## Technical Details

> **Note:** The provided analysis text indicates that specific technical details regarding the approach or architecture were not included in the source excerpt.

---

## Results

> **Note:** The provided analysis text indicates that specific experimental results or metrics were not included in the source section.
>
> *For specific quantitative results, please refer to the **Executive Summary** above, which details performance on CIFAR-10 and UCI datasets.*