# Revisiting Group Relative Policy Optimization: Insights into On-Policy and Off-Policy Training

*Youssef Mroueh; Nicolas Dupuis; Brian Belgodere; Apoorva Nitsure; Mattia Rigotti; Kristjan Greenewald; Jiri Navratil; Jerret Ross; Jesus Rios*

> ### **Quick Facts: Key Metrics**
>
 > * **Quality Score:** 7/10
 > * **References:** 18 Citations
 > * **Test Dataset:** GSM8K
 > * **Model Used:** Qwen
 > * **Serving Engine:** vLLM
 > * **Key Innovation:** Off-Policy Lower Bound with Clipped Surrogate Objectives
 > * **Optimization Frequency:** $v=1$ (On-Policy) vs $v>1$ (Off-Policy)

---

## Executive Summary

This research addresses the computational bottlenecks inherent in standard **Group Relative Policy Optimization (GRPO)**, a reinforcement learning algorithm used for Large Language Model (LLM) post-training. Traditional GRPO operates strictly on-policy, necessitating fresh data generation for every parameter update. In distributed environments, this requirement creates high latency and resource costs due to continuous communication between inference servers and training nodes. The study highlights the necessity of decoupling the sampling process from training to reduce inference burdens while maintaining the algorithm's effectiveness in model alignment.

The key innovation is the formulation of an **Off-Policy GRPO algorithm** that utilizes a lagged policy distribution. Technically, the authors introduce a clipping mechanism by deriving an Off-Policy Lower Bound (Theorem 1) that incorporates a penalty term based on Total Variation (TV) distance; they then convert this TV-constrained problem into a clipped surrogate objective function ($f_\epsilon$) to ensure practical optimization stability. The work also elucidates **"zero-variance masking,"** where advantages are computed by normalizing rewards relative to group statistics (mean and standard deviation). This group normalization minimizes gradient variance, which is critical for maintaining stability when the policy update frequency ($v$) is increased beyond the standard on-policy value of 1 ($v=1$).

Empirical evaluations on the GSM8K dataset using a Qwen model served via vLLM demonstrate that the proposed **Off-Policy GRPO** performs on par with or significantly outperforms traditional On-Policy GRPO. Ablation studies confirmed that the implementation of clipped surrogate objectives is essential for this performance. Crucially, the results validate that by increasing the policy update frequency ($v$)—where $v$ represents the number of optimization steps performed per generation step—the off-policy approach reduces inference requirements. For example, configurations where $v \gg 1$ successfully drive reward improvements equivalent to on-policy methods but with a fraction of the generation steps, directly translating to reduced wall-clock time and serving costs.

This work significantly expands the optimization regimes available for GRPO, establishing the algorithm as a versatile tool for scalable LLM alignment. By successfully bridging the gap between GRPO and off-policy learning, the authors provide a hybrid approach that enhances training utility while mitigating the high serving costs typically associated with RL post-training. The findings suggest that practitioners can adopt off-policy GRPO to achieve performance comparable to or better than on-policy methods with far greater resource efficiency, influencing future pipelines by decoupling the alignment loop from the inference bottleneck.

---

## Key Findings

*   **Adaptation to Off-Policy:** Group Relative Policy Optimization (GRPO) can be effectively adapted to an off-policy setting.
*   **Performance Comparison:** The proposed off-policy GRPO significantly outperforms or performs on par with traditional on-policy GRPO.
*   **Reward Improvements:** Both on-policy and off-policy GRPO objectives successfully result in measurable reward improvements.
*   **Validation of Clipping:** The implementation of clipped surrogate objectives in off-policy GRPO is validated as a critical method to improve performance.
*   **Efficiency Gains:** The off-policy approach reduces the communication burden of serving the model during inference iterations.

---

## Methodology

The study involved theoretically adapting the standard GRPO algorithm to an off-policy optimization regime and introducing clipped surrogate objectives. The methodology was evaluated using **reinforcement learning with verifiable rewards** during the post-training phase.

The evaluation process included:
*   A direct empirical comparison between the novel off-policy GRPO variant and the established on-policy GRPO counterpart.
*   Ablation studies on the GSM8K dataset using a Qwen model served via vLLM.

---

## Contributions

*   **Off-Policy Formulation:** The primary contribution is the formulation and validation of an off-policy version of GRPO, expanding the optimization regimes available for this algorithm.
*   **Empirical Evidence:** The paper provides empirical evidence that off-policy GRPO is a viable and often superior alternative for RL post-training tasks.
*   **Hybrid Approach:** The work integrates insights from off-policy PPO and recent GRPO analyses to propose a hybrid approach that enhances training utility.

---

## Technical Details

*   **Algorithm Extension:** The paper extends Group Relative Policy Optimization (GRPO) to an off-policy setting using a lagged policy distribution.
*   **Advantage Function:** It defines a GRPO advantage function that normalizes rewards relative to group mean and standard deviation.
*   **Theoretical Analysis (Theorem 1):** Provides an Off-Policy Lower Bound involving the expected advantage and penalty terms based on **Total Variation (TV) distance**.
*   **Objective Function:** The approach introduces a clipped surrogate objective function ($f_\epsilon$) converted from a TV-constrained problem to ensure stability.
*   **Zero Variance Masking:** Proposes zero variance masking to minimize gradient variance.
*   **Configuration:**
    *   **On-Policy GRPO:** Uses policy update frequency $v=1$.
    *   **Off-Policy GRPO:** Uses $v>1$ to reduce inference communication overhead.

---

## Results

The experimental outcomes highlight the efficacy of the proposed method:

*   **Comparative Performance:** The proposed off-policy GRPO significantly outperforms or performs on par with traditional on-policy GRPO.
*   **Objective Success:** Both on-policy and off-policy GRPO objectives successfully result in reward improvements.
*   **Resource Efficiency:** The off-policy approach offers efficiency gains by reducing the communication burden of serving the model during inference iterations.
*   **Ablation Studies:** Studies were conducted on the GSM8K dataset using a Qwen model served via vLLM, confirming the importance of clipped surrogate objectives.