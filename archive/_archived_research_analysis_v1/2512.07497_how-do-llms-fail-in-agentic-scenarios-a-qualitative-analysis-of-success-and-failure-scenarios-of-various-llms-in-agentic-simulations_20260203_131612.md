---
title: How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and
  Failure Scenarios of Various LLMs in Agentic Simulations
arxiv_id: '2512.07497'
source_url: https://arxiv.org/abs/2512.07497
generated_at: '2026-02-03T13:16:12'
quality_score: 7
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# How Do LLMs Fail In Agentic Scenarios? A Qualitative Analysis of Success and Failure Scenarios of Various LLMs in Agentic Simulations

*JV Roig*

---

> **Quick Facts**
>
> | Metric | Details |
> | :--- | :--- |
> | **Quality Score** | 7/10 |
> | **Citations** | 40 References |
> | **Data Source** | 900 Agentic Execution Traces |
> | **Environment** | KAMI v0.1 (10 Scenarios) |
> | **Models Tested** | 35 Configurations (including Llama, DeepSeek, Qwen, GPT-4o, Claude 3.5 Sonnet) |

---

## Executive Summary

The rapid integration of Large Language Models (LLMs) into autonomous agent frameworks has exposed a critical deficiency in current evaluation methodologies: the **"Agentic Disconnect."** Standard aggregate benchmarks, which rely on scalar performance metrics, fail to capture the granular behavioral nuances required for complex, multi-step tool use. This discrepancy poses significant risks for enterprises deploying autonomous systems, as high benchmark scores often do not translate to effective real-world agentic behavior.

To bridge the gap between benchmark performance and practical utility, the study introduces a **qualitative, trace-level analysis of 900 agentic execution traces** within the **KAMI v0.1** (Kamiwaza Agentic Merit Index) simulation environment. KAMI v0.1 is an interactive, multi-step framework comprising 10 distinct scenarios—covering domains such as file system manipulation, web browsing, and code execution—designed to rigorously test decision-making, tool integration, and environmental interaction.

The evaluation of 35 model configurations reveals counter-intuitive trends regarding model scale and architecture. The massive Llama 3.1 405B model demonstrated only marginal improvements over smaller open-source models and was frequently outperformed by Qwen 2.5 72B and DeepSeek V3, indicating that parameter scale alone is not a reliable predictor of agentic capability. These findings challenge the prevailing industry assumption that scaling model parameters is the primary driver of agentic performance, arguing instead for a paradigm shift toward reinforcement learning and qualitative behavior analysis.

---

## Technical Details

### Methodology & Environment
*   **Analysis Type:** Qualitative, trace-level analysis focusing on granular behavioral patterns rather than aggregate benchmark scores.
*   **Framework:** KAMI v0.1 (Kamiwaza Agentic Merit Index).
*   **Simulation Scope:** Interactive, multi-step environment covering **10 scenarios** designed to assess:
    *   Multi-step tool use
    *   Decision-making
    *   External environment interaction (file systems, web browsing, code execution)

### Models Evaluated
The study analyzed a total of **35 model configurations**, including but not limited to:
*   Llama 4 Maverick (400B) / Llama 3.1 405B
*   Granite 4 Small (32B)
*   DeepSeek V3 and V3.1
*   Qwen 2.5 72B and Qwen 3
*   GPT-4o
*   Claude 3.5 Sonnet

### Behavioral Taxonomy
The authors established a technical taxonomy of behaviors to categorize execution traces, including:
*   **Tool Use Avoidance**
*   **Semantic Confusion / Over-interpretation**
*   **Last-Mile Execution Failures**
*   **Coherence Degradation**

---

## Key Findings & Results

*Due to a service error in the source analysis, specific Key Findings and Methodology sections were unavailable. The findings below have been synthesized from the Results and Executive Summary sections provided.*

*   **The "Agentic Disconnect":** There is a confirmed weak correlation between standard benchmark performance and success on enterprise agentic tasks. High benchmark scores do not guarantee effective agent behavior.
*   **Parameter Scale vs. Capability:**
    *   The 400B parameter **Llama 4 Maverick** showed only marginal improvements over the 32B **Granite 4 Small**.
    *   **Llama 3.1 405B** was frequently outperformed by smaller models like Qwen 2.5 72B.
    *   **Conclusion:** Parameter scale alone is not a reliable predictor of agentic capability.
*   **Impact of Training Methodology:**
    *   **DeepSeek V3.1** demonstrated superior performance over DeepSeek V3.
    *   The gains are attributed to **post-training Reinforcement Learning (RL)** rather than architectural changes or raw scale.
*   **Model Regression:** Qwen3 models were found to underperform compared to Qwen2.5 variants in specific agentic contexts.

### Identified Failure Archetypes
The analysis identified four recurring failure modes:
1.  **Premature Action:** Acting before sufficient context is gathered.
2.  **Over-helpfulness:** Generating unnecessary or counter-productive actions.
3.  **Context Pollution:** Corrupting the working memory or context window.
4.  **Fragile Execution:** Inability to recover from minor errors during multi-step processes.

---

## Analysis Status

*   **Methodology:** Analysis unavailable due to service error.
*   **Contributions:** Analysis unavailable due to service error.