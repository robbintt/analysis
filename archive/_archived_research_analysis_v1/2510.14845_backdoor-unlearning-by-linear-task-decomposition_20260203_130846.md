---
title: Backdoor Unlearning by Linear Task Decomposition
arxiv_id: '2510.14845'
source_url: https://arxiv.org/abs/2510.14845
generated_at: '2026-02-03T13:08:46'
quality_score: 8
citation_count: 33
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Backdoor Unlearning by Linear Task Decomposition
*Amel Abdelraheem; Alessandro Favero; Gerome Bovet; Pascal Frossard*

> ### ðŸ“Š Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Method Name** | TBAR (Trigger Removal by Backdoor Arithmetic) |
> | **Target Model** | CLIP (ViT-B/32) |
> | **Forget Set Size** | 2,000 examples |
> | **ASR Reduction** | >98% (BadNet reduced to 0-2%) |
> | **Clean Accuracy** | ~96% Retention (Drop < 2%) |
> | **Core Hypothesis** | Weight Disentanglement |

---

## Executive Summary

**Problem**
Foundation models are increasingly susceptible to backdoor attacks, where malicious behaviors are embedded into the network during training and activated by specific triggers. The critical challenge in mitigating these attacks lies in "unlearning"â€”effectively removing the backdoor without degrading the model's performance on benign tasks. Existing defense mechanisms often struggle to balance successful backdoor removal with the preservation of clean accuracy, or they require prohibitively expensive retraining that is infeasible for large-scale models. This paper addresses the urgent need for a scalable, precise defense mechanism that can eliminate backdoors from foundation models while maintaining their general utility.

**Innovation**
The authors introduce **TBAR (Trigger Removal by Backdoor Arithmetic)**, a novel framework grounded in the "Weight Disentanglement" hypothesis. This hypothesis posits that backdoor behaviors are linearly separable from benign task representations within the model's weight space. TBAR operates by treating backdoor removal as a linear algebraic problem; it computes "Task Vectors"â€”defined as the difference between fine-tuned (poisoned) and pre-trained weightsâ€”to isolate the specific subspace associated with the attack. By utilizing a small "forget set" (2,000 examples), the method optimizes a scaling coefficient to derive a "Trigger Task Vector" and subtracts it from the model weights, leaving behind a residual "Clean Task Vector." Notably, this approach is effective even in trigger-unknown scenarios by utilizing reverse-engineered trigger estimates.

**Results**
Extensive experiments conducted on CLIP (ViT-B/32) visual encoders across datasets such as ImageNet-1K, CIFAR100, and SUN397 demonstrate TBAR's efficacy. The method achieved a reduction in Attack Success Rate (ASR) of over **98%** against various attacks, including BadNet, Blended, and WaNet (at a 3% poison rate). Specifically, ASR for BadNet attacks was reduced from approximately 91â€“99% to **0â€“2%**. Crucially, this robust removal came with minimal impact on utility; the framework retained an average of **96% clean accuracy**, with absolute accuracy drops generally remaining under 2%. Furthermore, the authors introduced a "Weight Disentanglement Error" metric that empirically confirmed the linear separability of backdoor and clean tasks.

**Impact**
This research provides a foundational shift in understanding how backdoors are structurally encoded within foundation models, proving they are disentangled from legitimate model capabilities. By offering a simple yet highly effective unlearning framework that does not require expensive retraining, TBAR presents a scalable solution for securing large-scale models in production environments. The method sets a new state-of-the-art by successfully resolving the trade-off between robustness and utility, ensuring that model safety can be achieved without the heavy computational costs or performance penalties associated with previous approaches.

---

## Key Findings

*   **Disentangled Encoding:** Backdoors in foundation models are encoded in the weight space in a manner that is **disentangled** from benign tasks.
*   **High Efficacy:** The proposed method achieves approximately **perfect unlearning** while retaining an average of **96% clean accuracy**.
*   **Unknown Attack Handling:** The method remains effective even when the attack is unknown, successfully unlearning backdoors by utilizing triggers estimated through reverse engineering.
*   **Superior Performance:** The approach consistently outperforms current state-of-the-art defenses by achieving a better balance between successful backdoor removal and the preservation of clean model performance.

---

## Methodology

The proposed method is based on **Linear Task Decomposition**, leveraging the insight that backdoor behaviors are linearly separable from benign representations within the model's weight space. The approach operates through the following steps:

1.  **Analyze Weight Space:** Identifying specific subspaces corresponding to the backdoor trigger.
2.  **Isolation and Erasure:** Isolating and erasing the influence of backdoor weights to nullify the attack.
3.  **Handling Unknown Attacks:** Estimating attack presence using reverse-engineered triggers when specific trigger information is unavailable.

The methodology was validated through extensive experiments using CLIP-based models and common adversarial triggers.

---

## Technical Details

*   **Approach Name:** TBAR (Trigger Removal by Backdoor Arithmetic)
*   **Core Concept:** Treats backdoor removal as a linear algebraic problem in the model's weight space.
*   **Key Hypothesis:** Backdoor behaviors are disentangled from benign task representations (Weight Disentanglement).
*   **Mechanism:**
    *   Utilizes **Task Vectors** (difference between fine-tuned and pre-trained weights).
    *   Isolates a **'Trigger Task Vector'** using a forget set.
    *   Optimizes a scaling coefficient to minimize Attack Success Rate.
    *   Derives a **'Clean Task Vector'** as a residual.
*   **Architecture:** Targets CLIP (ViT-B/32) visual encoders while keeping the text encoder frozen.
*   **Access Scenario:** Assumes white-box access with either trigger-known or trigger-unknown scenarios.
*   **Dataset Requirements:** Uses a 2,000-example forget set.

---

## Results

The performance of TBAR was evaluated against BadNet, Blended, and WaNet attacks (3% poison rate) on SUN397, CIFAR100, and ImageNet-1K:

*   **Attack Success Rate (ASR):**
    *   Overall reduction > **98%**.
    *   Achieved 100% removal for invisible attacks.
    *   BadNet ASR reduced from ~91-99% to ~0-2%.
*   **Clean Accuracy (CA):**
    *   Average retention of approximately **96%**.
    *   Ranged from 94.96% to 98.82%.
    *   Absolute accuracy drops generally under **2%**.
*   **Validation:** The **Weight Disentanglement Error** metric empirically confirmed the linear separability of backdoor and clean tasks.

---

## Contributions

*   **Foundational Understanding:** Provides a structural understanding of how backdoors are encoded in foundation models, specifically demonstrating their disentanglement from other model tasks.
*   **Scalable Framework:** Introduces a simple, unlearning framework that removes backdoors without requiring prohibitively expensive retraining, addressing the scalability issues of large-scale models.
*   **Utility Preservation:** Solves the critical challenge of maintaining a model's general capabilities by removing specific vulnerabilities without degrading performance on unrelated tasks.

---

**Quality Score:** 8/10  
**References:** 33 citations