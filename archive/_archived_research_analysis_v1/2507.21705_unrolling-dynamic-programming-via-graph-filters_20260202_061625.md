# Unrolling Dynamic Programming via Graph Filters
*Sergio Rozada; Samuel Rey; Gonzalo Mateos; Antonio G. Marques*

| **Quick Facts** | |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |
| **Core Model** | BellNet |
| **Focus** | Dynamic Programming, Graph Signal Processing (GSP), Reinforcement Learning |

---

## Executive Summary

> This research addresses the computational intractability of solving Markov Decision Processes (MDPs) via classical Dynamic Programming (DP)—specifically value and policy iteration—when applied to large state-action spaces or complex dependency structures. Traditional iterative solvers suffer from prohibitive computational costs and slow convergence rates, rendering them unsuitable for real-world, latency-sensitive applications such as large-scale routing or robotic control. The paper addresses the fundamental need for solution architectures that maintain the theoretical robustness of DP while drastically reducing the computational burden and time required to converge to optimal policies.
>
> The authors introduce **BellNet**, a novel parametric model that bridges the gap between classical iterative algorithms and deep learning by "unrolling" policy iterations into a fixed-depth neural network. The core technical innovation integrates Graph Signal Processing (GSP) theory, treating the MDP's transition probability matrix as the adjacency matrix of a weighted directed graph. This perspective allows the unrolled policy iterations to be re-parameterized as a cascade of nonlinear graph filters. Trained to minimize the Bellman error from random initializations, the model provides a unified theoretical framework that encompasses both policy and value iteration within a single, transferable architecture.
>
> Quantitative evaluations on synthetic graph topologies (Erdős-Rényi and Barabási-Albert) and real-world road networks (Rome, comprising 15k nodes, and Paris, comprising 13k nodes) demonstrate BellNet's superior efficiency compared to standard baselines. While classical methods like Value Iteration (VI) and Policy Iteration (PI) often require hundreds of iterations to converge, BellNet achieves comparable Normalized Mean Squared Error (NMSE) and policy accuracy using a fixed depth of merely 5 to 20 layers, depending on graph density. In terms of runtime, BellNet significantly outperforms iterative solvers; on the Paris road network, the model reaches target accuracy levels in a fraction of the time required by VI, effectively bypassing the variable computational cost of traditional convergence processes.
>
> The significance of this work lies in its establishment of a vital cross-disciplinary connection between dynamic programming, deep learning, and graph signal processing. By decoupling solution accuracy from variable iteration counts, BellNet provides explicit control over inference complexity, a critical feature for deploying reinforcement learning in resource-constrained or hard-real-time systems. This shift from iterative convergence to a fixed-depth computational graph marks a step forward in scaling optimal control solutions to large, graph-structured problems, offering a robust and efficient tool for sequential decision-making under strict computational budgets.

---

## Methodology

The authors propose **BellNet**, a learnable parametric model designed to solve Bellman's optimality equations. The methodology involves the following key steps:

*   **Unrolling and Truncating:** Standard policy iterations are unrolled and truncated to form a fixed-depth neural network structure.
*   **Bellman Error Minimization:** The model is trained to minimize the "Bellman error" starting from random value function initializations.
*   **Graph Signal Processing (GSP) Integration:** The transition probability matrix of the MDP is treated as the adjacency of a weighted directed graph.
*   **Re-parameterization:** By integrating GSP, the model is re-parameterized as a cascade of nonlinear graph filters.

---

## Key Findings

*   **Significant Speedup:** BellNet can effectively approximate optimal policies in a fraction of the iterations required by classical dynamic programming methods.
*   **Graph-Based Interpretation:** By viewing the transition probability matrix of an MDP as the adjacency of a weighted directed graph, unrolled policy iterations can be interpreted as a cascade of nonlinear graph filters.
*   **Unified Framework:** The approach provides a concise, transferable representation that unifies policy and value iteration under a single theoretical umbrella.
*   **Manageable Complexity:** The new representation offers an explicit handle on complexity during the inference phase.

---

## Technical Details

The technical architecture of the proposed solution is structured as follows:

*   **Architecture Name:** BellNet
*   **MDP Reformulation:** The Markov Decision Process (MDP) is reformulated by treating the transition probability matrix as the adjacency matrix of a weighted directed graph.
*   **Mechanism:** The system unrolls policy iterations as a cascade of nonlinear graph filters.
*   **Theoretical Scope:** Provides a unified framework for both policy iteration and value iteration.
*   **Complexity Management:** Offers an explicit handle on complexity during the inference phase.

---

## Results

BellNet achieves optimal policy approximation with high efficiency:

*   **Accuracy:** The model effectively approximates optimal policies, implying high accuracy comparable to traditional solvers.
*   **Efficiency:** BellNet achieves these results using a fixed depth of merely **5 to 20 layers**, depending on graph density, whereas classical methods often require hundreds of iterations.
*   **Real-World Performance:**
    *   **Rome Network (15k nodes):** Demonstrated superior efficiency over baselines.
    *   **Paris Network (13k nodes):** Reached target accuracy levels in a fraction of the time required by Value Iteration (VI), effectively bypassing the variable computational cost of traditional convergence processes.

---

## Contributions

*   **Novel Architecture (BellNet):** Introduction of a new parametric model that bridges the gap between classical iterative algorithms and modern deep learning by unrolling policy iterations.
*   **Cross-Disciplinary Connection:** Establishment of a theoretical link between dynamic programming/MDPs and graph signal processing, specifically utilizing graph filters to solve Bellman's equations.
*   **Computational Efficiency:** A solution to the computational expense of standard methods (like policy iteration) when dealing with large state-action spaces or long-term dependencies.
*   **Inference Control:** A framework that provides better control over inference complexity compared to traditional iterative solvers.