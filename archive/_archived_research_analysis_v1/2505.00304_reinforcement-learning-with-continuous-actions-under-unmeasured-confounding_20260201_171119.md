# Reinforcement Learning with Continuous Actions Under Unmeasured Confounding

*Yuhan Li; Eugene Han; Yifan Hu; Wenzhuo Zhou; Zhengling Qi; Yifan Cui; Ruoqing Zhu*

***

> ### ðŸ“Š Quick Facts
> * **Quality Score:** 7/10
> * **References:** 13 Citations
> * **Problem Space:** Offline RL, Unmeasured Confounding, Continuous Actions
> * **Key Innovation:** Nonparametric identification and policy optimization for infinite-horizon continuous settings.

***

## Executive Summary

Offline Reinforcement Learning (RL) faces a critical challenge in the presence of unmeasured confounding, where hidden variables influence both the agent's actions and the received rewards, resulting in biased policy evaluation and optimization. While existing literature has addressed unmeasured confounding, it has largely been restricted to discrete action spaces or single-step settings. This limitation presents a significant barrier to applying these methods in complex, real-world environmentsâ€”such as dynamic treatment regimes or economic policy planningâ€”where decisions are continuous in nature and unfold over infinite horizons with partially observable states.

The authors propose a novel nonparametric identification strategy for policy values within infinite-horizon settings, effectively bridging the gap between discrete and continuous action spaces under unmeasured confounding. The methodology leverages Proximal Causal Inference, utilizing reward-inducing ($W_t$) and action-inducing ($Z_t$) proxy variables to tackle the absence of confounder measurements. Technically, the framework constructs a minimax estimator to approximate the Q-bridge function. It achieves this by optimizing a specific adversarial loss function, $L_\pi(q, f)$, against a critic class, which is then integrated into a policy-gradient-based algorithm that includes regularization to identify the in-class optimal policy.

The research provides robust theoretical guarantees to validate the approach, formally establishing the nonparametric identification of policy values and the consistency of the proposed estimator. A primary quantitative contribution of the paper is the derivation of explicit regret bounds for the learned optimal policy, alongside finite-sample error bounds, which statistically quantify the algorithm's performance and reliability. The proposed approach was further validated through extensive simulations and a real-world case study using the German Family Panel data, where it successfully modeled complex dynamic relationships among housing, work, and education variables to approximate an optimal policy where traditional methods fail.

This work significantly advances the field of causal RL by extending policy learning capabilities to continuous action spaces under unmeasured confoundingâ€”a necessary step for realistic application. By providing the first identification result and corresponding optimization algorithm for this specific class of problems, the research moves the discipline beyond theoretical policy evaluation into functional policy optimization. The theoretical foundation, characterized by proven regret bounds and the ability to handle infinite-horizon dynamics, offers a robust new tool for decision-making in domains where hidden biases and continuous interventions are pervasive.

***

## Key Findings

*   **Novel Identification:** Established a novel identification result enabling the nonparametric estimation of policy value for continuous action spaces in the presence of unmeasured confounders.
*   **Algorithm Development:** Developed a minimax estimator and a policy-gradient-based algorithm that successfully identifies the in-class optimal policy within an infinite-horizon framework.
*   **Theoretical Guarantees:** Provided robust theoretical guarantees demonstrating the estimator's consistency, finite-sample error bounds, and regret bounds for the optimal policy.
*   **Empirical Validation:** Validated the practical effectiveness of the methodology through extensive simulations and a real-world case study using the German Family Panel data.

***

## Methodology

The authors propose an offline reinforcement learning framework that addresses unmeasured confounding by deriving a novel identification strategy to formulate the policy value problem nonparametrically.

*   **Framework:** Specifically designed for infinite-horizon settings and continuous action spaces, distinguishing it from standard Partially Observable Markov Decision Process (POMDP) evaluations.
*   **Optimization Strategy:** Leveraging the identification results, the authors construct a minimax estimator to approximate the policy value and utilize a policy-gradient-based algorithm to optimize the policy class.

***

## Technical Details

The paper proposes an Offline RL framework for Confounded POMDPs with continuous action spaces, defined over an infinite-horizon POMDP with observed ($O$) and unobserved ($S$) states.

| Component | Description |
| :--- | :--- |
| **Core Mechanism** | Utilizes **Proximal Causal Inference** to handle unmeasured confounders. |
| **Variables** | Employs **reward-inducing ($W_t$)** and **action-inducing ($Z_t$)** proxy variables to identify policy values. |
| **Architecture** | Features a **Minimax Estimator** to approximate the Q-bridge function. |
| **Optimization** | Optimizes a loss function $L_\pi(q, f)$ against a critic class and employs a policy-gradient-based strategy with regularization. |

***

## Contributions

*   **Advancement beyond Discrete Actions:** Addresses a significant gap by extending policy learning from discrete to continuous action spaces under unmeasured confounding.
*   **Theoretical Foundations:** Provides a rigorous new identification result for infinite-horizon scenarios and strong theoretical proofs regarding the statistical properties of the proposed estimator.
*   **Practical Application and Generalization:** Moves beyond theoretical policy evaluation to actual policy optimization, offering a functional algorithm validated by both synthetic and real-world data.

***

## Results

*   **Theoretical Guarantees:** The paper establishes nonparametric identification of policy values, consistency of the estimator, finite-sample error bounds, and regret bounds for the learned optimal policy.
*   **Experimental Validation:**
    *   **Simulations:** Extensive simulations were conducted to validate the approach.
    *   **Real-world Case Study:** A study on the **German Family Panel** dataset was performed. The authors used housing, work, and education variables as proxies to evaluate 'time spent with family' as an action.
*   **Note:** Quantitative experimental metrics are not fully available as the specific Experiments section was not provided in the source text.