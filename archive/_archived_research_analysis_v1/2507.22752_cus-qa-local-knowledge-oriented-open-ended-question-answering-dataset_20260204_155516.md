---
title: 'CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset'
arxiv_id: '2507.22752'
source_url: https://arxiv.org/abs/2507.22752
generated_at: '2026-02-04T15:55:16'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset

*JindÅ™ich LibovickÃ½; JindÅ™ich Helcl; Andrei Manea; Gianluca Vico*

---

> ### ðŸ“Š Quick Facts
>
> *   **Dataset Size:** 2,807 textual samples / 1,097 visual samples
> *   **Languages:** Czech, Slovak, Ukrainian (+ English translations)
> *   **Annotation Effort:** 26 annotators, 319 hours of work
> *   **Top Model Accuracy:** >40% (Text) / <30% (Visual)
> *   **Quality Control:** 22% rejection rate, 8% correction rate
> *   **Dominant Category:** Geography (32â€“60% of data)
> *   **Paper Quality:** 9/10

---

## Executive Summary

This research addresses the critical limitation of current Large Language Models (LLMs) in acquiring and reasoning with "local knowledge"â€”facts and cultural context specific to particular regions, particularly in languages underrepresented in training data such as Czech, Slovak, and Ukrainian. While LLMs excel at general-domain benchmarks, they frequently fail to grasp region-specific nuances that are intuitive to native speakers. This gap is problematic because robust AI systems must operate effectively across diverse linguistic landscapes, not merely in English-centric or global contexts. The paper highlights the absence of rigorous, multi-modal benchmarks to evaluate this specific capability, necessitating a resource that moves beyond general knowledge to test deep, region-specific understanding across different modalities.

The core innovation is the introduction of CUS-QA, a comprehensive, manually curated benchmark for open-ended regional question answering covering both textual and visual modalities, explicitly including English translations to facilitate broader research. The dataset is constructed using a rigorous human-in-the-loop workflow where native speakers identify region-specific facts from Wikipedia pages filtered by DBPedia entities and language exclusivity. To ensure high quality, the authors employed a custom web annotation tool with strict quality control, resulting in a 22% rejection rate and 8% correction rate. Technically, the pipeline leverages Claude 3.5 Sonnet for high-fidelity automatic cross-lingual translation, GliNER for entity recognition, and LLaMA 3.3 70B for categorizing questions into six distinct types (Geography, Culture, Politics, History, Sports, Other).

Evaluation of state-of-the-art open-weight LLMs on the CUS-QA benchmark reveals significant performance gaps, with the best models achieving only slightly over 40% accuracy on textual questions and less than 30% on visual questions. The evaluation utilized specific prompting techniques, testing textual and visual models using zero-shot prompting with nucleus sampling ($p=0.9$). Geography emerged as the dominant category, covering 32% to 60% of the dataset, a variance reflecting the distribution of entities across the target languages. An important methodological finding is that while LLM-based evaluation metrics demonstrated a strong correlation with human judgments, traditional string-overlap metrics (like BLEU or ROUGE) also performed well. The researchers attribute this to the structure of the answers, which are typically short noun phrases averaging ~20â€“30 characters with a high density of named entities.

The CUS-QA benchmark provides the field with a vital tool for diagnosing and improving the cultural and linguistic adaptability of LLMs. By establishing that even top-performing open-weight models struggle with region-specific multi-modal queries, the paper sets a new, more difficult standard for model evaluation that extends beyond English-centric data. The findings regarding evaluation metrics offer practical guidance to the community, validating the use of LLM-based evaluators while noting that string-overlap metrics may be sufficient for entity-heavy tasks. Ultimately, this work underscores the necessity of incorporating diverse, human-verified local knowledge into training pipelines to develop truly global AI systems.

---

## Key Findings

*   **Performance Gap:** Even the best open-weight LLMs struggle significantly with the benchmark.
    *   **Textual Questions:** Slightly over **40%** accuracy.
    *   **Visual Questions:** Less than **30%** accuracy.
*   **Metric Correlation:** LLM-based evaluation metrics show a **strong correlation** with human judgments.
*   **String-Overlap Efficacy:** Traditional string-overlap metrics (e.g., BLEU, ROUGE) perform surprisingly well, largely because correct answers contain a high prevalence of named entities.

---

## Methodology

The researchers employed a robust, human-centric approach to dataset creation and model evaluation:

1.  **Dataset Creation:** Developed CUS-QA, a manually curated, Wikipedia-grounded dataset.
2.  **Source Material:** Questions were generated by native speakers from Czechia, Slovakia, and Ukraine, with English translations included.
3.  **Modalities:** The dataset covers both **textual** and **visual** modalities.
4.  **Evaluation:** State-of-the-art LLMs were tested using specific prompting techniques.
5.  **Validation:** Human judgments of answer correctness were collected to analyze the reliability of various automatic evaluation metrics.

---

## Technical Details

### Data Construction Pipeline
*   **Annotators:** 26 native speakers (Czech, Slovak, Ukrainian).
*   **Source Selection:** Wikipedia pages filtered by DBPedia entities and language exclusivity to identify region-specific facts.
*   **Tooling:** A custom web tool was used for annotation.
*   **Quality Control:** Strict protocols resulted in a **22% rejection rate** and **8% correction rate**.

### Data Processing & Categorization
*   **Translation:** Automatic cross-lingual translation performed by **Claude 3.5 Sonnet**.
*   **Categorization:** Questions categorized into six types (Geography, Culture, Politics, History, Sports, Other) using **LLaMA 3.3 70B**.
*   **Entity Recognition:** Utilized **GliNER** for entity recognition tasks.

### Evaluation Architecture
*   **Textual Models Tested:** LLaMA 3.1, LLaMA 3.3, Mistral, EuroLLM.
*   **Visual Models Tested:** mBLIP, LLaMA Vision, Maya, Gemma, idefics.
*   **Prompting Strategy:** Zero-shot prompting with nucleus sampling ($p=0.9$).

---

## Results

### Dataset Statistics
*   **Total Samples:** 2,807 textual and 1,097 visual samples.
*   **Labor:** 319 hours of annotation.
*   **Category Dominance:** Geography is the primary category, ranging from **32% to 60%** depending on the language.
*   **Answer Length:** Generally short noun phrases.
    *   Czech/Slovak: Average ~20 characters.
    *   Ukrainian: Average >30 characters.

### Model Performance
*   Best-performing open-weight LLMs achieved **>40% accuracy** on textual questions.
*   Visual question answering performance remained low at **<30%**.

### Translation Quality
*   **Claude 3.5 Sonnet:** 88â€“98% perfect translations.
*   **GPT-4o:** 82â€“94% perfect translations.
*   **Google Translate:** 57â€“78% perfect translations.

---

## Contributions

1.  **New Benchmark:** Introduced the CUS-QA benchmark, a comprehensive resource for open-ended regional QA covering textual and visual modalities in multiple languages.
2.  **Baseline Results:** Provided strong baseline results using current state-of-the-art open-weight LLMs.
3.  **Metric Analysis:** Delivered a detailed analysis of evaluation metric reliability, offering insights into the efficacy of LLM-based evaluators versus traditional string-overlap metrics in this domain.

---

**Paper Quality Score:** 9/10  
**References:** 40 citations