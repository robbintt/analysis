---
title: Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for
  LLMs
arxiv_id: '2602.01064'
source_url: https://arxiv.org/abs/2602.01064
generated_at: '2026-02-03T18:59:13'
quality_score: 6
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Exploring Knowledge Purification in Multi-Teacher Knowledge Distillation for LLMs

*Ruihan Jin; Pengpeng Shao; Zhengqi Wen; Jinyang Wu; Mingkuan Feng; Shuo Yang; Chu Yuan Zhang; Jianhua Tao*

---

> ### üìä Quick Facts
>
> *   **Quality Score:** 6/10
> *   **References:** 40 Citations
> *   **Core Technique:** Knowledge Purification (KP)
> *   **Top Method:** Router-based Purification
> *   **Key Benefit:** Resolves knowledge conflicts & improves student generalization

---

## üìù Executive Summary

This research addresses the critical inefficiencies and performance degradation inherent in Multi-Teacher Knowledge Distillation (MTKD) for Large Language Models (LLMs). While utilizing multiple teacher models theoretically allows a student to aggregate diverse knowledge, practical application often leads to "knowledge conflicts," where varying or contradictory reasoning rationales from different teachers confuse the student and degrade performance. Furthermore, standard MTKD imposes prohibitive computational demands due to the need to process outputs from multiple large teacher models simultaneously during training, creating a barrier for efficient model compression.

The core innovation is "Knowledge Purification" (KP), a framework designed to synthesize conflicting rationales into a single, unified rationale ($r_P$) to guide the student. Technically, this is operationalized through a modified loss function, $L_{MTKD-KP} = L_{PR} + \lambda L_{DL-KP}$. In this mathematical formulation, the Primary Response loss ($L_{PR}$) ensures the accuracy of the student's final answer, while the Purified Distillation Loss ($L_{DL-KP}$) aligns the student's reasoning process with the consolidated rationale $r_P$, balancing correctness and reasoning fidelity via the hyperparameter $\lambda$. The authors evaluate five purification architectures, with a specific focus on router-based methods‚Äîsuch as Plackett-Luce Ranking and Similarity-based Routers‚Äîwhich utilize semantic embeddings and cosine similarity to dynamically select the most appropriate teacher rationale, thereby filtering out noise.

Experimental validation using the TinyLLM framework, with teachers including Llama-3.1-8B-Instruct, FLAN-T5 xlarge, and Llama 2-chat, demonstrated that Knowledge Purification effectively resolves the performance drop typically associated with adding more teachers. The study evaluated performance on Multiple Choice Question Answering (MCQA) tasks‚Äîspecifically OBQA, ARC, Riddle, and PQA‚Äîusing Accuracy as the primary metric. Student models trained with KP consistently outperformed those trained using traditional multi-teacher approaches across these datasets. The results specifically highlighted that router-based purification methods offered the most robust generalization capabilities, confirming that consolidating knowledge into a single, high-quality rationale yields better student performance than exposing the student to all available teacher rationales.

The significance of this work lies in shifting the paradigm of model distillation from simple knowledge accumulation to intelligent knowledge consolidation. By resolving the conflicts inherent in multi-teacher setups, the research enables the practical use of ensembles of expert models to train smaller, more efficient student models without the resource penalty or noise of traditional methods. This advancement facilitates the deployment of high-performance lightweight models in resource-constrained environments and establishes routing-based purification as a critical component for future research in efficient LLM training and compression.

---

## üîç Key Findings

*   **Mitigation of Knowledge Conflicts:** The concept of "Knowledge Purification" successfully resolves conflicts inherent in multi-teacher knowledge distillation by consolidating varying rationales into a single, unified rationale.
*   **Performance Enhancement:** The distilled student models exhibit improved performance when trained using the proposed purification methods compared to traditional approaches.
*   **Superiority of Router-based Methods:** Among the proposed techniques, router-based purification methods demonstrated robust generalization capabilities, highlighting their specific effectiveness.
*   **Increased Efficiency:** By merging multiple teacher rationales into one, the approach addresses the high resource demands typically associated with leveraging multiple teacher models.

---

## üõ†Ô∏è Methodology

The study introduces a conceptual framework called **Knowledge Purification**, which is designed to process and consolidate reasoning rationales from multiple teacher LLMs into a single, coherent rationale for the student model. To investigate this framework, the authors proposed and evaluated five distinct purification methods derived from various technical perspectives, with a specific focus on comparing their efficacy in resolving conflicts and optimizing the distillation process.

---

## ‚öôÔ∏è Technical Details

**Core Concept:** Knowledge Purification (KP)
**Objective:** Address knowledge conflict and high resource demands in Multi-Teacher Knowledge Distillation (MTKD) by consolidating multiple teacher rationales into a single unified rationale $r_P$.

**Mathematical Formulation:**
The loss function is defined as:
$$ L_{MTKD-KP} = L_{PR} + \lambda L_{DL-KP} $$

*   $L_{PR}$: Primary Response loss.
*   $L_{DL-KP}$: Purified Distillation Loss.
*   $\lambda$: Hyperparameter for balancing losses.

**Purification Architectures Explored:**
1.  **Knowledge Aggregation:** Uses a global LLM aggregator via instruction-tuning.
2.  **LLM Routing (Selection-based):**
    *   *Plackett-Luce Ranking:* Uses softmax probabilities derived from cosine similarity.
    *   *PLM Classifier:* Uses semantic embeddings.
    *   *Similarity-based Router.*
3.  **RL-based Teacher Selection:** Utilizes reinforcement learning for dynamic teacher selection.

---

## üìà Results

*   **Framework:** TinyLLM
*   **Teacher Models:** FLAN-T5 xlarge, Llama 2-chat, BioMistral-7B, Llama-3.1-8B-Instruct.
*   **Evaluation Tasks:** MCQA (OBQA, ARC, Riddle, PQA).
*   **Primary Metric:** Accuracy.
*   **Observations:**
    *   Performance declines as more teachers are added in standard setups due to knowledge conflicts.
    *   Student models trained with Knowledge Purification **outperformed** traditional multi-teacher approaches.
    *   **Router-based methods** demonstrated the most robust generalization.
    *   Effectively reduced resource demands by merging rationales.

---

## ‚ú® Contributions

*   **Conceptual Innovation:** Introduction of "Knowledge Purification" as a novel strategy to handle the specific challenges of knowledge conflicts and resource intensity in multi-teacher knowledge distillation for LLMs.
*   **Methodological Development:** Proposal of five specific purification methods, providing a comprehensive exploration of how knowledge consolidation can be operationalized.
*   **Empirical Validation:** Demonstrated through experimentation that purified knowledge‚Äîparticularly via router-based mechanisms‚Äîsignificantly improves the generalization and performance of lightweight models, facilitating their practical deployment.

---
**Quality Score:** 6/10 | **References:** 40 citations