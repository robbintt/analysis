---
title: Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization
arxiv_id: '2511.15055'
source_url: https://arxiv.org/abs/2511.15055
generated_at: '2026-02-03T19:13:42'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization

*Jian-Ting Guo; Yu-Cheng Chen; Ping-Chun Hsieh; Kuo-Hao Ho; Po-Wei Huang; Ti-Rong Wu; I-Chen Wu*

---

### üìã Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Citations** | 40 References |
| **Core Framework** | Macro Action Quantization (MAQ) |
| **Architecture** | VQ-VAE (Vector-Quantized Variational Autoencoder) |
| **Benchmarks** | D4RL Adroit |
| **Key Achievement** | Balances high task rewards with superior human-likeness |

---

> ## üìë Executive Summary
>
> Standard reinforcement learning (RL) agents are typically designed to maximize cumulative rewards, a focus that often results in behaviors that are technically optimal yet perceptually unnatural to human observers. These agents frequently exhibit erratic movements, such as shaking or spinning, which diverge significantly from smooth human biomechanics. This lack of behavioral alignment creates a barrier to the deployment of RL agents in scenarios requiring human trust and collaboration, such as assistive robotics and human-AI interaction. The paper addresses the challenge of developing RL agents that can effectively balance the dual objectives of achieving high task performance while maintaining human-like behavioral trajectories.
>
> The core innovation is the **Macro Action Quantization (MAQ) framework**, which formulates human-likeness as a trajectory optimization problem within a Semi-Markov Decision Process (SMDP). Technically, MAQ utilizes a Vector-Quantized Variational Autoencoder (VQ-VAE) to distill human demonstrations into a discrete codebook of "macro actions." Rather than selecting from a continuous space of primitive control signals, the agent employs Human-like Receding-Horizon Control (HRC) to select sequences of these quantized macro actions. This process transforms the action space from continuous variables to discrete codebook indices, constraining the agent to follow human-derived action primitives while optimizing for task rewards.
>
> Evaluated on the D4RL Adroit benchmarks integrated with off-the-shelf algorithms such as IQL, SAC, and RLPD, MAQ demonstrated superior performance across multiple metrics. The framework achieved near-perfect task success rates (approaching 100%) on complex manipulation tasks, effectively solving the benchmarks while maintaining behavioral fidelity. In subjective human evaluation studies, MAQ achieved the highest rankings for perceived human-likeness compared to all other tested agents. Additionally, the method recorded significant improvements in trajectory similarity scores, successfully avoiding unnatural tendencies like jittery movements without compromising execution efficiency.

---

## üîç Key Findings

*   **Enhanced Human-Likeness:** The MAQ framework significantly improves human-likeness, evidenced by increased trajectory similarity scores on the D4RL Adroit benchmarks.
*   **Superior Subjective Rankings:** In subjective human evaluation studies, MAQ achieved the highest human-likeness rankings compared to all other tested RL agents.
*   **High Versatility:** The framework demonstrates high versatility, showing it can be easily integrated into various off-the-shelf RL algorithms without complex modifications (e.g., IQL, SAC, RLPD).
*   **Balanced Objectives:** The approach successfully balances the dual goals of maximizing task rewards (performance) and aligning with human behavior (likeness), overcoming the 'unnatural' tendencies of standard reward-driven agents.
*   **Elimination of Artifacts:** The agent successfully avoids unnatural behaviors like "shaking" or "spinning" often seen in standard RL.

---

## üí° Methodology

The authors propose a shift from standard reward maximization to a trajectory optimization problem.

1.  **Problem Formulation:** Human-likeness is formulated as a trajectory optimization problem. The goal is to seek action sequences that maximize rewards while simultaneously minimizing divergence from human behavior.
2.  **Tractable Optimization:** The method adapts receding-horizon control for human-like learning to make this optimization tractable.
3.  **Macro Action Quantization (MAQ):** This core framework utilizes a **Vector-Quantized Variational Autoencoder (VQ-VAE)**.
4.  **Action Primitives:** The VQ-VAE distills human demonstrations into discrete sets of 'macro actions,' allowing the RL agent to select from human-derived action primitives rather than raw, continuous control signals.

---

## ‚öôÔ∏è Technical Details

**Formulation & Control**
*   **Framework:** Macro Action Quantization (MAQ).
*   **Process:** Formulates human-likeness as a trajectory optimization problem within a **Semi-Markov Decision Process (SMDP)**.
*   **Strategy:** Utilizes **Human-like Receding-Horizon Control (HRC)**.

**Architecture**
*   **Model Type:** Conditional VQVAE.
*   **Function:** Distills human behavior into a discrete codebook.
*   **Transformation:** Converts the action space from continuous primitive actions to discrete codebook indices.
*   **Pipeline:**
    1.  Encodes state and macro action pairs.
    2.  Quantizes the latent representation against a codebook.
    3.  Decodes it to reconstruct actions.

**Training & Loss**
*   **Loss Components:** Combines reconstruction, codebook, and commitment losses.
*   **Stages:**
    *   **Offline Distillation:** Conducted on demonstrations.
    *   **Online RL:** A policy selects codebook indices to execute macro action sequences.

---

## üìä Results

The proposed method was rigorously evaluated using the following parameters:

*   **Environment:** D4RL Adroit benchmarks.
*   **Integration:** Tested with IQL, SAC, and RLPD algorithms.
*   **Metrics:** Trajectory Similarity, Task Success Rate, and Subjective Human Evaluation.

**Performance Outcomes**
*   **Subjective Evaluation:** MAQ achieved the highest human-likeness rankings.
*   **Objective Metrics:** Demonstrated increased trajectory similarity scores.
*   **Task Success:** Achieved higher task success rates compared to baseline methods, successfully balancing task performance with human-like behavior.

---

## ‚ú® Contributions

1.  **Theoretical Formulation:** Introduced a new theoretical formulation of human-likeness as trajectory optimization, providing a structured way to quantify and optimize for behavioral alignment alongside performance.
2.  **Framework Development:** Developed the MAQ framework that leverages VQ-VAE to convert human demonstrations into quantized macro actions, bridging the gap between imitation learning and reinforcement learning.
3.  **Empirical Validation:** Provided comprehensive empirical validation on D4RL Adroit datasets that establishes a new standard for human-likeness, validated through both computational metrics and human studies.
4.  **Trustworthiness:** Enhanced interpretability and trustworthiness of AI systems by specifically addressing unnatural behaviors in RL agents.

---

**Quality Score:** 8/10  
**References:** 40 citations