# Adapting Lightweight Vision Language Models for Radiological Visual Question Answering

*Aditya Shourya; Michel Dumontier; Chang Sun*

---

> ### ðŸ“Š Quick Facts
> *   **Model Architecture:** PaliGemma-mix-448 (3B Parameters)
> *   **Key Innovation:** Multi-stage LoRA fine-tuning with synthetic data generation
> *   **Comparative Efficiency:** 62.5% reduction in parameters vs. LLaVA-Med (8B)
> *   **Core Datasets:** SLAKE, PMC-VQA, ROCO v2.0, MedPix 2.0
> *   **Interpretability:** Saliency-based diagnostic tool (Raw & Rollout Attention)

---

## Executive Summary

Radiological Visual Question Answering (VQA) faces a dual barrier to entry: the prohibitive computational cost of state-of-the-art large vision-language models and a scarcity of expert-labeled medical data. Unlike general computer vision domains, medical imaging lacks massive annotated datasets, limiting the training of robust, interpretable models. Consequently, leading solutions such as LLaVA-Med rely on architectures exceeding 8 billion parameters, necessitating expensive hardware and rendering advanced diagnostic AI inaccessible for many clinical settings.

The authors propose a resource-efficient framework centered on **PaliGemma-mix-448**, a lightweight 3 billion parameter model comprising a SigLIP vision tower (~400M parameters) and a Gemma language decoder. The technical innovation employs a two-stage training strategy: Stage 1 utilizes Low-Rank Adaptation (LoRA) to align the projection head with anatomical vocabulary, while Stage 2 involves full model fine-tuning. To address data limitations, the framework implements a cost-effective synthetic data pipeline using LLaMA-8B to generate question-answer pairs, augmenting specific radiological datasets. Additionally, the system integrates a saliency-based diagnostic tool employing Raw and Rollout Attention analysis, allowing domain experts to visualize model focus and identify ill-conditioned failure modes.

The model was trained on a curated mixture of SLAKE (~14k VQA pairs), PMC-VQA (~227k QA pairs), ROCO v2.0 (~79k image-caption pairs), and MedPix 2.0 (~12k cases). Evaluation on ROCO+MedPix VQA tasks demonstrated that the proposed 3 billion parameter model achieves classification accuracy comparable to LLaVA-Med, which relies on 8 billion parameters. Generative factuality was validated through GPT-4 assessment, confirming that the lightweight model maintains high diagnostic precision while requiring a **62.5% reduction in parameter count** relative to the baseline, significantly lowering computational overhead.

This research shifts the paradigm for medical AI accessibility by proving that high-performance radiological VQA does not strictly require massive parameter scales. The proposed training framework offers a scalable solution for data-scarce medical domains by successfully leveraging synthetic data generation, thereby minimizing reliance on expensive expert annotation. Moreover, the inclusion of interpretability tools directly addresses the "black box" nature of deep learning in healthcare. Collectively, these contributions enable the deployment of sophisticated diagnostic AI tools in resource-constrained medical environments without sacrificing model efficacy.

---

## Key Findings

*   **Efficient Performance:** A lightweight 3B parameter vision-language model achieves robust performance on radiological VQA tasks when fine-tuned with curated data.
*   **State-of-the-Art Competitiveness:** The model achieves performance comparable to state-of-the-art models (LLaVA-Med) with significantly less computational scale and training data.
*   **Pipeline Efficiency:** A cost-effective data pipeline using synthetic question-answer generation and multi-stage fine-tuning addresses the issue of limited expert-labeled images.
*   **Interpretability:** A saliency-based diagnostic tool enables domain experts to inspect model internals and identify ill-conditioned failure modes.

---

## Methodology

*   **Model Selection:** Utilizes a lightweight 3B parameter pre-trained vision-language model.
*   **Data Generation and Processing:** Implements a cost-effective pipeline beginning with the generation of synthetic question-answer pairs.
*   **Training Strategy:** Employs multi-stage fine-tuning using specialized radiological datasets, specifically ROCO v2.0 and MedPix v2.0.
*   **Evaluation Mechanism:** Uses a lightweight saliency-based diagnostic tool to interpret model decisions and assess potential failure conditions.

---

## Technical Details

| Component | Specification |
| :--- | :--- |
| **Base Architecture** | **PaliGemma-mix-448** |
| **Total Parameter Count** | 3 Billion |
| **Vision Tower** | SigLIP transformer (decoder-only, ~400M parameters) trained with sigmoid contrastive loss. |
| **Projection** | Single linear layer projection head. |
| **Stage 1 Training** | LoRA fine-tuning to align the projection head with anatomical vocabulary. |
| **Stage 2 Training** | Full model fine-tuning with synthetic QA generation (via LLaMA-8B) and annealing strategies. |
| **Interpretability** | Saliency Analysis using Raw and Rollout Attention methods. |

---

## Results

*   **Training Scope:** The model was trained on a curated mixture of datasets including:
    *   SLAKE (~14k VQA pairs)
    *   PMC-VQA (~227k QA pairs)
    *   ROCO v2.0 (~79k image-caption pairs)
    *   MedPix 2.0 (~12k cases)
*   **Evaluation Metrics:** Performance was measured using classification accuracy for open/closed-ended questions and generative factuality (judged by GPT-4).
*   **Performance:** The 3B parameter model achieves competitive accuracy on ROCO+MedPix VQA tasks, approaching the performance of larger models like LLaVA-Med (8B parameters) while maintaining significantly higher resource efficiency.

---

## Contributions

*   **Resource-Efficient Alternative:** Demonstrates that high-performance radiological VQA does not strictly require massive parameter counts, offering an accessible solution for medical settings.
*   **Scalable Training Framework:** Proposes a pipeline that mitigates data scarcity through synthetic data generation and targeted fine-tuning.
*   **Interpretability and Diagnostic Tools:** Contributes a novel diagnostic tool using saliency analysis to help experts trust and debug model outputs.

---

**Quality Score:** 9/10  
**References:** 40 citations