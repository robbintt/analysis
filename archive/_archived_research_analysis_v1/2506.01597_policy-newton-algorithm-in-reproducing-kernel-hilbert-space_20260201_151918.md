# Policy Newton Algorithm in Reproducing Kernel Hilbert Space
*Yixian Zhang; Huaze Tang; Chao Wang; Wenbo Ding*

---

## Quick Facts

| **Metric** | **Detail** |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |
| **Optimization Type** | Second-Order (Newton) |
| **Convergence Rate** | Local Quadratic |
| **Estimator Rate** | O(1/√N) |
| **Algorithm** | Conjugate Gradient with Cubic Regularization |

---

## Executive Summary

This research addresses the fundamental computational bottleneck of applying second-order optimization to non-parametric policies defined within Reproducing Kernel Hilbert Spaces (RKHS). While RKHS policies offer substantial theoretical representational advantages, applying second-order methods (like Newton's algorithm) has historically been intractable due to the infinite-dimensional nature of the Hessian operators, which cannot be directly inverted or stored. This creates a significant barrier to utilizing these methods in practice, despite their theoretical benefits. The authors propose 'Policy Newton in RKHS,' the first framework to successfully integrate non-parametric RKHS representations with second-order optimization.

The key innovation involves a two-pronged approach that applies Cubic Regularization to an auxiliary objective and leverages the Representer Theorem to transform the infinite-dimensional problem into a finite-dimensional optimization task. Crucially, this methodology reduces the problem complexity such that it scales specifically with trajectory data rather than remaining infinite. By optimizing these finite coefficients, the algorithm implements Conjugate Gradient descent to solve the cubic regularized problem based on sampled trajectories, effectively resolving the issue of intractable Hessian inversion.

The researchers established concrete performance guarantees, achieving local quadratic convergence to a local optimum—a substantial improvement over the linear rates typical of first-order methods. Their Monte Carlo estimators for both gradient and Hessian achieve a precise convergence rate of O(1/√N). Empirically, the approach demonstrates consistent outperformance over existing baselines, surpassing both parametric second-order methods and first-order RKHS approaches on standard RL benchmarks as well as in financial asset allocation tasks.

This work bridges the gap between the theoretical potential of non-parametric policies and the practical realities of computational optimization. By providing a computational solution to inverting infinite-dimensional Hessian operators, the authors prove that non-parametric second-order methods are not just theoretically viable but practically superior to current approximations. This advancement opens the door for more robust and efficient optimization in complex reinforcement learning environments where representational power is critical.

---

## Key Findings

*   **Novel Framework:** Introduces 'Policy Newton in RKHS' as the first framework specifically designed for second-order optimization in RKHS-based policies.
*   **Theoretical Guarantees:** Proves local quadratic convergence to a local optimum, a significant metric improvement over standard methods.
*   **Empirical Superiority:** Demonstrates superior performance on standard RL benchmarks and financial asset allocation tasks compared to existing solutions.
*   **Computational Breakthrough:** Successfully resolves the computational bottleneck associated with inverting infinite-dimensional Hessian operators.

---

## Methodology

The methodology introduces a second-order optimization algorithm tailored for RKHS policies. It overcomes the challenge of computational intractability through two primary mechanisms:

1.  **Cubic Regularization:** The method optimizes a cubic regularized auxiliary objective, which stabilizes the optimization process in complex landscapes.
2.  **Representer Theorem:** By utilizing the Representer Theorem, the framework transforms infinite-dimensional problems into finite-dimensional ones. This ensures the problem scale is dictated by the trajectory data rather than the infinite parameter space.

---

## Technical Details

*   **Proposed Method**: Policy Newton in RKHS
*   **Policy Definition**: Non-parametric policy represented as:
    $$ \pi_h(a_t | s_t) = \frac{1}{Z} e^{T h(s_t, a_t)} $$
*   **Core Problem Solved**: Intractable infinite-dimensional Hessian inversion.
*   **Optimization Strategy**:
    *   Utilizes a **cubic regularized auxiliary objective**.
    *   Applies the **Representer Theorem** to reduce infinite-dimensional optimization to finite coefficient optimization.
*   **Implementation Solver**: Uses **Conjugate Gradient** descent to solve the cubic regularized problem based on sampled trajectories.

---

## Contributions

*   **Bridges the Gap:** Integrates non-parametric RKHS representations with second-order optimization, connecting theory with practice.
*   **Computational Solution:** Provides a viable method for inverting infinite-dimensional Hessian operators.
*   **Performance Validation:** Demonstrates that non-parametric second-order methods can empirically outperform existing parametric second-order and first-order RKHS approaches.

---

## Results

### Theoretical Results
*   Guarantees **local quadratic convergence** to a local optimum.
*   Monte Carlo estimators for both gradient and Hessian achieve a convergence rate of **O(1/√N)**.

### Empirical Results
*   **Standard RL Benchmarks:** Showed superior performance over existing baselines.
*   **Financial Asset Allocation:** Outperformed competing methods in complex financial tasks.
*   **Comparison:** Surpassed both parametric second-order methods and first-order RKHS approaches.