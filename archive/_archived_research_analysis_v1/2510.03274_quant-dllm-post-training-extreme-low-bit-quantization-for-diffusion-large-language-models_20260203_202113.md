---
title: 'Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large
  Language Models'
arxiv_id: '2510.03274'
source_url: https://arxiv.org/abs/2510.03274
generated_at: '2026-02-03T20:21:13'
quality_score: 9
citation_count: 6
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models

*Tianao Zhang; Zhiteng Li; Xianglong Yan; Haotong Qin; Yong Guo; Yulun Zhang*

---

> ### ðŸ“‹ Quick Facts
> **Quality Score:** 9/10  
> **Target Precision:** 2-bit (Average)  
> **Core Innovation:** Masked Calibration Simulation (MCS)  
> **Key Performance:** LLaDA-7B accuracy maintained at **51.22%** (vs 24.89% for standard GPTQ)  
> **Models Tested:** LLaDA, Dream variants  

---

## Executive Summary

This research addresses the critical challenge of compressing Diffusion Large Language Models (dLLMs) to extreme low-bit precision (2-bit) using Post-Training Quantization (PTQ). While PTQ is highly effective for standard autoregressive LLMs, it fails catastrophically when applied to dLLMs due to fundamental architectural differences in signal distribution. Specifically, dLLMs rely on timestep-dependent visibility schedules and masked-denoising mechanisms, which disrupt the activation statistics that standard PTQ methods expect. Consequently, existing state-of-the-art PTQ techniques cannot maintain model performance at a 2-bit budget, creating a significant barrier for the efficient deployment of these computationally intensive models on resource-constrained hardware.

To resolve this incompatibility, the authors propose **Quant-dLLM**, a training-free, weight-only PTQ framework specifically designed to handle the unique activation patterns of dLLMs. The framework introduces three core technical components:

1.  **Masked Calibration Simulation (MCS):** Aligns the calibration process with the modelâ€™s inference phase by generating timestep-aware data that utilizes Bernoulli distributions to simulate masked-denoising, ensuring the quantizer observes accurate signal visibility.
2.  **Data-aware Any-order Quantizer (DAQ):** Employs a binarization strategy with Row-Column Scaling to learn ultra-low-bit weight representations, minimizing Frobenius norm reconstruction error through closed-form updates.
3.  **Adaptive Blockwise Mixed Precision (ABMP):** Optimizes the bit-width allocation by assigning higher precision (3-bit) to sensitive channel groups and lower precision (1-bit) to less critical blocks, strictly maintaining an average budget of 2 bits.

Evaluations conducted on diffusion LLM variants, including LLaDA-7B and Dream-7B, demonstrate that Quant-dLLM significantly outperforms existing baselines such as GPTQ, GPTAQ, and Slim-LLM under an equal 2-bit memory constraint. Testing across 7 general tasksâ€”BoolQ, PIQA, ARC-Easy, ARC-Challenge, WSC, COPA, and Winograndeâ€”Quant-dLLM achieved an Average Accuracy of **51.22%** on the LLaDA-7B model. This result represents a massive improvement over standard GPTQ, which collapsed to **24.89%**, and closes the gap to the full-precision FP16 baseline (**55.41%**). Similarly, on the Dream-7B model, Quant-dLLM attained **46.83%** accuracy compared to the GPTQ baseline of **22.32%**, effectively quantifying the performance gap and validating the method's ability to preserve model fidelity.

This work establishes the first dedicated ultra-low-bit PTQ framework for Diffusion Large Language Models, setting a new performance baseline for compressing these architectures to 2-bit precision. By successfully resolving the incompatibility between standard PTQ methods and masked-denoising dynamics, Quant-dLLM enables practical, memory-efficient deployment of dLLMs without the prohibitive costs of retraining.

---

## Key Findings

*   **Standard PTQ Ineffectiveness:** Standard Post-Training Quantization is ineffective on diffusion LLMs (dLLMs) at 2-bit precision due to architectural differences in signal visibility.
*   **Superior Performance:** The proposed Quant-dLLM framework outperforms state-of-the-art AR-transfer PTQ methods under a strict 2-bit budget.
*   **Calibration Alignment:** Aligning calibration with masked-denoising activations is essential for obtaining reliable calibrations.
*   **Adaptive Precision:** Sensitivity-based adaptive precision allocation effectively maintains model performance while adhering to low-bit constraints.

---

## Methodology

The Quant-dLLM framework is an ultra-low-bit Post-Training Quantization approach for dLLMs featuring three core components:

1.  **Masked Calibration Simulation (MCS)**
    *   Designed to align calibration with timestep-dependent masking.
    *   Ensures the quantizer receives data representative of the inference phase.

2.  **Data-aware Any-order Quantizer (DAQ)**
    *   Learns ultra-low-bit weight representations via optimization.
    *   Allows for flexible quantization orders to minimize error.

3.  **Adaptive Blockwise Mixed Precision (ABMP)**
    *   Adaptively assigns bit widths across channel groups based on layer sensitivity.
    *   Dynamically balances the budget between critical and non-critical blocks.

---

## Technical Details

Quant-dLLM is a training-free, weight-only PTQ framework designed for dLLMs to address failures in standard PTQ at 2-bit precision caused by timestep-dependent visibility schedules and masked-denoising.

### Core Components

**1. Binarization Strategy (Row-Column Scaling)**
*   Encodes weights using a closed-form update strategy to minimize Frobenius norm reconstruction error.
*   Mathematical representation involves row and column scaling factors:
    $$ \hat{W} = (\beta_r \beta_c) \otimes B $$

**2. Masked Calibration Simulation (MCS)**
*   An algorithm that generates timestep-aware calibration data.
*   Preserves a deterministic visible prefix while masking other tokens based on a Bernoulli distribution and visibility schedule.

**3. Adaptive Blockwise Mixed Precision (ABMP)**
*   Allocates precision based on sensitivity analysis.
*   **Sensitive blocks:** Assigned 3-bit precision.
*   **Less important blocks:** Assigned 1-bit precision.
*   **Constraint:** Strictly maintains an average of 2 bits across the model.

---

## Results

Quant-dLLM was evaluated on diffusion LLMs including LLaDA and Dream variants, comparing against baselines GPTQ, GPTAQ, and Slim-LLM using Average Accuracy (%) across 7 general tasks (BoolQ, PIQA, ARC-Easy, ARC-Challenge, WSC, COPA, Winogrande).

### Performance on LLaDA-7B

| Method | Precision | Average Accuracy |
| :--- | :---: | :---: |
| **FP16 Baseline** | 16-bit | **55.41%** |
| **Quant-dLLM (Ours)** | **2-bit** | **51.22%** |
| GPTQ | 2-bit | 24.89% |

### Performance on Dream-7B

| Method | Precision | Average Accuracy |
| :--- | :---: | :---: |
| **Quant-dLLM (Ours)** | **2-bit** | **46.83%** |
| GPTQ | 2-bit | 22.32% |

### Analysis
*   **Significant Outperformance:** Quant-dLLM significantly outperforms state-of-the-art baselines under an equal memory cost (2-bit budget).
*   **Fidelity:** Maintains accuracy levels closest to full-precision performance.
*   **Component Efficacy:** The row-column scaling binarization demonstrated lower reconstruction error than fixed 2-bit codes, and the ABMP component effectively sustained performance by dynamically allocating precision.

---

## Contributions

*   **First Dedicated Framework:** Introduction of the first dedicated ultra-low-bit PTQ framework for Diffusion Large Language Models.
*   **Novel Algorithm (MCS):** Development of Masked Calibration Simulation to handle unique masked-denoising activation patterns.
*   **Advanced Quantization:** Proposal of the Data-aware Any-order Quantizer (DAQ) and Adaptive Blockwise Mixed Precision (ABMP) algorithms.
*   **New Baseline:** Establishment of a new performance baseline for compressing dLLMs to 2-bit precision.

---
**References:** 6 citations