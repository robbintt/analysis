# Task Prototype-Based Knowledge Retrieval for Multi-Task Learning from Partially Annotated Data

*Authors: Youngmin Oh; Hyung-Il Kim; Jung Uk Kim*

> ### **Quick Facts**
> **Quality Score:** 8/10  
> **References:** 7 citations  
> **Benchmark Results (NYU Depth v2):**  
> &nbsp;&nbsp;• Segmentation mIoU: **46.7%**  
> &nbsp;&nbsp;• Depth RMSE: **0.383 meters**  
> **Core Innovation:** Prototype-based knowledge retrieval decoupled from pseudo-labeling.

---

## Executive Summary

Multi-Task Learning (MTL) faces a critical scalability bottleneck: the requirement for fully annotated datasets across all tasks, which is economically and practically prohibitive. While existing solutions for partially annotated learning often utilize pseudo-labeling to infer missing labels, this strategy is fundamentally fragile; inaccuracies in generated pseudo-labels lead to error propagation, causing negative transfer where knowledge from noisy predictions actively degrades performance on primary tasks.

The authors propose a prototype-based knowledge retrieval framework that decouples knowledge transfer from output prediction, thereby bypassing the need for pseudo-labels. The core technical innovation is the **"Task Prototype,"** learned embeddings that capture task-specific characteristics and quantify inter-task associations via cosine similarity. These prototypes drive a **"Knowledge Retrieval Transformer"** that adaptively refines feature representations by retrieving relevant context from related tasks rather than generating labels.

To ensure reliability in the absence of ground truth, the authors introduce the **Association Knowledge Generating (AKG) loss**, which stabilizes training by enforcing that task prototypes maintain consistent and distinct representations of task-specific features. The framework was rigorously evaluated on NYU Depth v2 and PASCAL Context benchmarks across semantic segmentation, depth estimation, and surface normal prediction, demonstrating clear superiority over state-of-the-art baselines that rely on pseudo-labeling.

This work represents a significant methodological shift in multi-task learning, moving away from the fragile cycle of pseudo-label generation toward a robust, representation-based approach. By proving that knowledge can be effectively transferred via feature-level prototypes without the risk of propagating noisy output labels, the authors provide a viable pathway to deploy high-performance multi-task models in real-world scenarios where full annotation is impossible.

---

## Key Findings

*   **Robust MTL Framework:** The proposed prototype-based framework achieves robust Multi-Task Learning (MTL) for partially annotated data without relying on predictions from unlabeled tasks, effectively mitigating negative transfer.
*   **Task Prototype Efficacy:** The "Task Prototype" successfully captures and quantifies task-specific characteristics and task associations, ensuring reliable connections between tasks even with missing label information.
*   **Feature Refinement:** The Knowledge Retrieval Transformer utilizes task associations to adaptively refine feature representations, improving overall performance.
*   **Validation of Sparse Data:** Extensive experiments validate the framework's effectiveness, demonstrating high performance even when only a subset of tasks is annotated.

---

## Methodology

The authors propose a knowledge retrieval framework centered on task prototypes to bypass the dependency on noisy unlabeled task predictions. The methodology consists of two primary components:

1.  **Task Prototype:** Learns embeddings to represent task-specific characteristics and quantify task associations.
2.  **Knowledge Retrieval Transformer:** Adaptively refines feature representations based on the associations identified by the Task Prototype.

To ensure reliability, the authors introduce an **Association Knowledge Generating (AKG) loss function**. This loss is designed to force the task prototype to consistently capture accurate task-specific characteristics during training, stabilizing the learning process in the absence of full labels.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Core Architecture** | A Prototype-based framework designed for Multi-Task Learning (MTL) with partially annotated data. |
| **Task Prototypes** | Introduced to capture task-specific characteristics and task associations, maintaining robustness against missing labels. |
| **Mechanism** | The 'Knowledge Retrieval Transformer' adaptively refines feature representations using task associations. |
| **Noise Handling** | The framework avoids using predictions from unlabeled tasks to prevent error propagation and negative transfer. |

---

## Contributions

*   **Robustness in Partially Annotated Scenarios:** Addresses the bottleneck of labeling costs by providing a solution that does not require full annotation and avoids unreliable task associations.
*   **Novel Framework Architecture:** Introduces a prototype-based architecture for knowledge retrieval between tasks using distinct embeddings and transformers, offering an alternative to prediction-based methods.
*   **Optimization Mechanism:** Develops the Association Knowledge Generating (AKG) loss, a training objective that guarantees the task prototype remains a true representation of task-specific features, enhancing learning stability.

---

## Results

The framework achieves robust Multi-Task Learning even in the absence of complete annotations and effectively mitigates negative transfer. It maintains high performance when only a subset of tasks is annotated, a finding supported by extensive experiments on standard benchmarks such as NYU Depth v2 and PASCAL Context. The model showed high stability even when the proportion of unlabeled data was significant, avoiding the sharp performance degradation experienced by comparative methods.

---

## Evaluation

**Quality Score:** 8/10  
**References:** 7 citations