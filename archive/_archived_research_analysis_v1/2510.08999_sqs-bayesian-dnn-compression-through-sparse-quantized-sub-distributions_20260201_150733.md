# SQS: Bayesian DNN Compression through Sparse Quantized Sub-distributions

*Ziyi Wang; Nan Jiang; Guang Lin; Qifan Song*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 8/10 |
| **Citations** | 40 References |
| **Core Technique** | Bayesian Variational Learning (Spike-and-Slab Prior + GMM) |
| **Target Models** | ResNet-18/50, BERT-base, Llama3, Llama3.2-1B, Qwen2.5 |
| **Key Dataset** | CIFAR-100 |
| **Inference Methods** | Bayesian Averaging, Greedy MAP |

---

## Executive Summary

As Deep Neural Networks (DNNs) and Large Language Models (LLMs) scale up in size, deploying them on resource-constrained hardware presents significant challenges regarding memory footprint and computational latency. While compression techniques such as weight pruning and low-bit quantization are standard solutions, they are frequently applied sequentially or independently. This disjointed approach often fails to account for the interdependence between sparsity and quantization, leading to suboptimal compression rates and compounded accuracy loss.

This paper addresses the need for a unified optimization strategy that can aggressively compress modern architectures without sacrificing the high performance required for practical deployment. The authors propose **SQS (Sparse Quantized Sub-distributions)**, a novel Bayesian variational learning framework that integrates weight pruning and quantization into a single, simultaneous optimization process.

Technically, SQS models network weights using a probabilistic formulation that combines Spike-and-Slab priors to induce sparsity with Gaussian Mixture Models (GMMs) to enforce low-bit precision constraints. The method optimizes the Evidence Lower Bound (ELBO), balancing the expected negative log-likelihood against a regularization term involving the KL divergence between the prior and a sparse, quantized variational posterior. To maintain tractability, the authors employ an upper bound approximation via Jensenâ€™s inequality and introduce an "**Outlier-Aware Window**" strategy specifically to handle the long-tail weight distributions common in LLMs.

Experimental validation on architectures including ResNet-18/50, BERT-base, Llama3, and Qwen2.5 demonstrates that SQS outperforms baseline methods by achieving higher compression rates with comparable accuracy. On the CIFAR-100 dataset using ResNet, the framework maintained inference accuracy between **79.0% and 81.0%**, with the Bayesian Averaging inference method proving more robust than the Greedy MAP approach. The study also confirmed that simultaneous optimization yields better results than sequential application, while the specific Outlier-Aware Window strategy proved superior to equal window strategies for LLMs. Additionally, increasing the number of Gaussian components in the GMM from 8 to 16 was shown to further reduce accuracy drops.

This research significantly advances the field of model compression by establishing a theoretically grounded and empirically effective method for reducing the size of state-of-the-art neural networks. The inclusion of consistency proofs validates the variational approach, providing mathematical rigor often lacking in heuristic compression techniques. By demonstrating success on massive LLMs like Llama3 and Qwen2.5, SQS offers a viable pathway for deploying advanced AI models in production environments with strict hardware limitations.

---

## Key Findings

*   **Superior Compression:** The SQS framework achieves higher compression rates than prior baselines while maintaining comparable model performance.
*   **Simultaneous Optimization:** Applying pruning and low-bit quantization simultaneously yields better results than applying these techniques individually or sequentially.
*   **Broad Architectural Support:** The method demonstrates effectiveness across diverse architectures, including ResNet, BERT-base, Llama3, and Qwen2.5.
*   **Theoretical Rigor:** The study provides consistent theoretical results validating the variational approach for sparse and quantized deep neural networks.

---

## Methodology

The research introduces **SQS**, a unified framework grounded in **Bayesian variational learning**.

*   **Sparse Induction:** It utilizes a **spike-and-slab prior** on network weights to induce sparsity and facilitate pruning.
*   **Quantization Modeling:** It models quantized weights using **Gaussian Mixture Models (GMMs)** to enable low-bit precision constraints.
*   **Unified Integration:** This integration optimizes for both sparsity and quantization simultaneously within a Bayesian inference paradigm, rather than treating them as separate steps.

---

## Technical Details

SQS employs a Bayesian Variational Learning framework for simultaneous pruning and quantization. The technical implementation follows this structure:

### Objective Function
The primary objective is to maximize the **Evidence Lower Bound (ELBO)**. This function comprises:
*   The expected negative log-likelihood.
*   A regularization term involving the **KL divergence** between a Spike-and-Slab prior and a sparse, quantized mixture variational posterior.

### Optimization Strategy
To handle computational intractability, the method employs specific mathematical approximations:
*   **KL Divergence Approximation:** Uses an upper bound approximation based on **Jensen's inequality**.
*   **Log-Likelihood Approximation:** Approximated using the posterior mean.

### Inference & LLM Adaptation
*   **Inference Methods:** The framework supports both **Bayesian Averaging** and a **Greedy MAP** approach.
*   **LLM Optimization:** Includes an **Outlier-Aware Window** strategy specifically designed for Large Language Models (LLMs) to handle long-tail distributions effectively.

---

## Experimental Results

Experiments were conducted on architectures including ResNet-18/50, BERT-base, Llama3, Llama3.2-1B, and Qwen2.5, primarily using the CIFAR-100 dataset.

### Performance Metrics
*   **ResNet on CIFAR-100:**
    *   **Bayesian Averaging:** Resulted in smaller accuracy drops than the greedy approach.
    *   **Accuracy Range:** Inference accuracy remained between **79.0% and 81.0%**.
*   **General Performance:** SQS achieves higher compression rates than baselines with comparable accuracy, outperforming sequential application of pruning and quantization.

### LLM Specifics
*   The **Outlier-aware window strategy** proved superior to equal window strategies for matching the tails of weight distributions in Large Language Models.

### Sensitivity Analysis
*   Increasing Gaussian component counts in the mixture model from **8 to 16** generally reduced accuracy drops, indicating that finer granularity in quantization modeling improves performance.

---

## Contributions

1.  **Novel Framework:** Development of a novel Bayesian Compression Framework that bridges weight pruning and low-bit quantization into a single, simultaneous optimization process.
2.  **Probabilistic Formulation:** Introduction of a probabilistic formulation combining spike-and-slab priors with Gaussian Mixture Models to mathematically model sparse and quantized distributions.
3.  **Theoretical Validation:** Establishment of theoretical validation through consistency proofs for the variational approach.
4.  **Empirical Demonstration:** Empirical demonstration of state-of-the-art performance on modern, large-scale models, verifying utility for resource-constrained deployment.