# Integrating Causal Reasoning into Automated Fact-Checking
*Youssra Rebboud; Pasquale Lisena; Raphael Troncy*

> ### ðŸ“Š Quick Facts
> *   **Quality Score:** 6/10
> *   **Citations:** 27 References
> *   **Evaluation:** Conducted on two datasets (Quantitative metrics excluded from analysis)
> *   **Core Focus:** Explainable AI & Causal Logic

---

## Executive Summary

Current automated fact-checking (AFC) systems are fundamentally limited by a reliance on surface-level textual similarity, which fails to capture the deep logical structure required to verify complex claims. This is a critical deficiency because a primary source of misinformation involves erroneous cause-effect relationshipsâ€”claims that are false not because of wrong facts, but because the causal logic connecting them is invalid. Existing architectures lack the dedicated reasoning mechanisms to identify these logical fallacies, resulting in systems that cannot refute sophisticated causal claims or provide the semantically rich explanations necessary for user trust.

The paper introduces a hybrid pipeline that integrates fine-grained causal event relationships to replace lexical matching with logical deduction. The architecture employs **Event Relation Extraction** to identify specific causal types (e.g., direct-cause, prevents, enables) defined by the FARO ontology, and uses **Semantic Similarity Computation** via SBERT to align and measure the closeness of events in the claim against evidence. These components feed into a Rule-based Reasoning module that deduces verdicts by comparing entire event chains: a "Support" verdict requires identical relations and event matches, while "Refutation" is triggered when contradictory relations (e.g., cause vs. prevent) exist between similar events.

Evaluation was conducted on two datasets to establish the first baseline for this fine-grained causal approach. While specific quantitative metrics (Accuracy, F1-score) were not included in the provided text, the qualitative results validate the system's capability to successfully detect logical inconsistencies through event chain comparison. The system demonstrated specific proficiency in identifying refutations based on contradictory causal links, confirming the hypothesis that integrating semantic alignment and formal reasoning rules allows for the detection of errors that surface-level methods would miss.

This research provides a foundational shift toward explainable AI in fact-checking by addressing the "black box" nature of current systems. By establishing a new baseline for causal reasoning, the authors bridge a significant gap in AFC architectures, offering a pathway to verify claims that require deep logical inference rather than simple fact lookup. The primary significance lies in the system's enhanced explainability; by providing human-readable explanations derived from explicit causal extraction, the technology lays the groundwork for more transparent, trustworthy, and semantically intelligent verification tools.

---

## Key Findings

*   **Deficiency in Current Systems:** Current automated fact-checking systems suffer from a lack of dedicated causal-based reasoning, limiting their ability to provide semantically rich explanations.
*   **Nature of Misinformation:** A primary source of false claims involves erroneous cause-effect relationships between events, which traditional methods often fail to detect.
*   **Logical Consistency:** The proposed methodology successfully detects logical inconsistencies by comparing chains of events in claims against evidence.
*   **Baseline Establishment:** Evaluation on two datasets establishes the viability of the method, marking the first baseline for fine-grained causal event relationship integration.
*   **Enhanced Explainability:** The integration of causal reasoning significantly enhances the explainability of verdict predictions.

---

## Methodology

The proposed methodology utilizes a hybrid pipeline consisting of three core components designed to move beyond simple lexical matching:

1.  **Event Relation Extraction:** Identifies and isolates cause-effect relationships within the text.
2.  **Semantic Similarity Computation:** Measures the closeness in meaning between events mentioned in the claim and the evidence.
3.  **Rule-based Reasoning:** Processes these components to detect logical inconsistencies between the chains of events found in the claim versus the evidence.

---

## Research Contributions

*   **Establishment of a Baseline:** Provides the first known baseline for integrating fine-grained causal event relationships into automated fact-checking.
*   **Enhanced Explainability:** Addresses the 'black box' nature of current systems by introducing causal reasoning to offer semantically rich explanations for verdict predictions.
*   **Filling a Research Gap:** Directly addresses the absence of causal-dedicated reasoning mechanisms in existing fact-checking architectures.

---

## Technical Details

The architecture is a modular pipeline comprising six distinct steps, utilizing specific ontologies and formal reasoning rules.

### Core Components
*   **Ontology:** Utilizes the **FARO ontology** to define semantically precise event relations.
*   **Relation Types:** Focuses on four specific types:
    *   Direct-cause (assumed transitive)
    *   Prevents
    *   Intends-to-cause
    *   Enables

### Reasoning Logic
The system employs formal rules to deduce verdicts based on event matching ($C \sim A, D \sim B$):
*   **Support:** Occurs if relations are identical/similar **and** events match.
*   **Refutation:** Occurs if relations are contradictory (e.g., cause vs. prevent) **and** events are similar.

### Architecture Pipeline
1.  **Event Relation Extraction**
2.  **Relation Extraction**
3.  **Similarity Scoring** (using SBERT)
4.  **Polarity Detection** (using DistilBERT)
5.  **Common Sense Reasoning** (using an LLM)
6.  **Reasoning Module** (Applies the formal rules)

---

## Results & Evaluation

**Data Availability:** Quantitative metrics (Accuracy, F1-score, Precision, Recall) are not available in the provided text as the Results section was excluded.

**Qualitative Findings:**
*   Evaluation was conducted on two datasets, establishing the first baseline for fine-grained causal event relationship integration.
*   The integration of causal components significantly enhances the explainability of verdict predictions through human-readable explanations based on causality extraction.
*   The system is capable of detecting logical inconsistencies by comparing event chains in claims against evidence.

---