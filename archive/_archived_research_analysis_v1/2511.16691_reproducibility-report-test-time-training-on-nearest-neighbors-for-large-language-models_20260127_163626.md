---
title: 'Reproducibility Report: Test-Time Training on Nearest Neighbors for Large
  Language Models'
arxiv_id: '2511.16691'
source_url: https://arxiv.org/abs/2511.16691
generated_at: '2026-01-27T16:36:26'
quality_score: 8
citation_count: 9
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Reproducibility Report: Test-Time Training on Nearest Neighbors for Large Language Models

*Johan Lindqvist, Boyang Zhou, Lindsey Li*

---

> ### **Quick Facts**
> * **Methodology:** TTT-NN (Test-Time Training with Nearest Neighbors)
> * **Retrieval Model:** Fine-tuned RoBERTa (~1.7M iterations) with Faiss Flat L2 Index
> * **Architectures:** GPT-2 (117M & 774M), GPT-Neo (1.3B), R1-Distill-Qwen-1.5B
> * **Dataset:** The Pile (DM Math, Wikipedia, Arxiv, Github)
> * **Key Configuration:** 20 Neighbors, Batch Size 16, 1 Gradient Iteration
> * **Top Outcome:** ~70% reduction in perplexity on code tasks

---

## Executive Summary

This research addresses the critical challenge of verifying the reproducibility of **Test-Time Training with Nearest Neighbors (TTT-NN)**, a method proposed to enhance the efficiency of Large Language Models (LLMs) without the prohibitive costs associated with scaling or weight modification. As computational burdens for deploying massive models increase, the field requires reliable, low-compute methods for dynamic adaptation.

Validating TTT-NN is essential to determine if its reported capabilities—specifically allowing smaller models to outperform significantly larger ones through on-the-fly training using retrieved examples—are viable alternatives to resource-intensive approaches like brute-force scaling or standard fine-tuning. Correcting previous terminology errors, this report focuses specifically on "Nearest Neighbors" retrieval rather than "Nearest Neurons."

The technical core of the reproduction involves the **TTT-NN algorithm**, which diverges from traditional Retrieval-Augmented Generation (RAG) by bypassing in-context injection in favor of test-time optimization. The method retrieves the $k$-nearest neighbors for the current validation sequence using a fine-tuned RoBERTa model (indexed via Faiss Flat L2) and performs a single gradient update to the model's activations or weights. The re-implementation process required overcoming significant engineering hurdles, specifically developing memory management optimizations using byte offsets to handle large datasets without exhausting RAM.

The authors successfully applied this pipeline to architectures ranging from GPT-2 (117M) to R1-Distill-Qwen-1.5B, utilizing configurations of 20 neighbors and sequence lengths of up to 2048 tokens. The reproduction study successfully validated the original performance claims, demonstrating that TTT-NN delivers significant improvements in predictive accuracy.

When evaluated on subsets of The Pile (DM Mathematics, Wikipedia, Arxiv, and Github), the method achieved the reported up to **~70% reduction in perplexity on code tasks**. Specifically, the smaller GPT-2 (117M) model was able to secure greater than 50% of the performance gains typically observed in models more than ten times its size.

---

## Key Findings

*   **Validation of Claims:** The reproduction successfully replicated the original performance gains, specifically regarding the reduction in perplexity and improvement in accuracy.
*   **Efficacy of TTT-NN:** The method was confirmed as a viable alternative to scaling, allowing smaller models to punch above their weight class.
*   **Performance Metrics:** Achieved >50% of the performance gains of models >10x larger using only the 20 closest neighbors.
*   **Code Performance:** Validated the claim of approximately 70% reduction in perplexity specifically on code-related tasks (Github subset).
*   **Engineering Viability:** Determined that despite memory constraints, the method is practically implementable using byte-offset memory management.

---

## Methodology & Contributions

### Methodology
The authors re-implemented an algorithm designed to retrieve similar examples (nearest neighbors) at test time to aid the LLM's generation. Unlike standard Retrieval-Augmented Generation (RAG), which injects context into the prompt, this method updates the model's activations or weights based on the retrieved examples.

**The Core Process:**
1.  Embed the validation sequence.
2.  Retrieve $k$-nearest neighbors using the fine-tuned retriever.
3.  Perform one gradient iteration on the model using these neighbors.

### Contributions
The primary contribution of this report is the **verification of existing claims**. By providing a robust re-implementation, the authors offer the community confidence (or lack thereof) in the original method's effectiveness. Additionally, the release of code—specifically the memory management optimizations—lowers the barrier to entry for future research in test-time training.

---

## Technical Details

### Algorithm Implementation
The reproduction centers on the TTT-NN framework, which performs on-the-fly adaptation using single gradient updates on retrieved examples rather than in-context injection.

### System Architecture
| Component | Specification |
| :--- | :--- |
| **Retriever** | Fine-tuned RoBERTa model (trained on The Pile for ~1.7M iterations) |
| **Indexing** | Faiss Flat L2 Index |
| **Architectures** | Autoregressive transformers (GPT-2 117M, GPT-2 Large 774M, GPT-Neo 1.3B, R1-Distill-Qwen-1.5B) |

### Memory & Compute Optimizations
To handle the large dataset sizes without exhausting RAM, the authors implemented key engineering solutions:
*   **Byte Offsets:** Used byte offsets instead of loading full datasets into memory, significantly reducing memory footprint.
*   **Compute Setup:** Requires substantial compute for retriever training (~1.7M iterations), but uses standard Batch Size 16 for the adaptation phase.

### Hyperparameters
*   **Batch Size:** 16
*   **Gradient Iterations:** 1
*   **Neighbors ($k$):** 20
*   **Max Sequence Length:** 1024 (GPT-2) / 2048 (Others)
*   **Learning Rate:**
    *   $2\text{e}^{-5}$ (GPT-2)
    *   $5\text{e}^{-6}$ (Larger models)

---

## Results

### Metrics
Performance was evaluated using three primary metrics:
*   Bits per Byte
*   Word Perplexity
*   Byte Perplexity

### Experimental Outcomes
Experiments were conducted on subsets of **The Pile**, including:
*   DM Mathematics
*   Wikipedia
*   Arxiv
*   Github

**Outcomes:**
*   **Github (Code):** Confirmed ~70% reduction in perplexity.
*   **Scaling Efficiency:** Smaller GPT-2 models (117M) effectively approached the performance of models >10x their size by utilizing TTT-NN.
*   **Neighbor Efficiency:** The report confirmed that using the 20 closest neighbors yields >50% of the potential performance gains.

---

## Metadata

**Quality Score:** 8/10  
**References:** 9 citations