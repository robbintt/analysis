# Words That Make Language Models Perceive

*Sophie L. Wang; Phillip Isola; Brian Cheung*

---

> ### ðŸ“Œ Quick Facts
> | Metric | Detail |
> | :--- | :--- |
> | **Quality Score** | 8/10 |
> | **References** | 40 Citations |
> | **Scope** | 0.6B â€“ 32B Parameter Models |
> | **Core Innovation** | Sensory Prompting |
> | **Key Metric** | ~75% relative increase in vision alignment |

---

## Executive Summary

### The Problem
Large Language Models (LLMs) trained exclusively on text are traditionally viewed as lacking perceptual capabilitiesâ€”such as vision or hearingâ€”because they do not possess direct sensory experience or architectural pathways to process raw pixel or audio data. This limitation usually creates a strict dichotomy where text-only models handle linguistic reasoning while separate, specialized "foundation encoders" handle sensory inputs. This paper addresses the hypothesis that this gap is not purely architectural but conceptual: it investigates whether the statistical structure of language implicitly encodes perceptual information, asking whether text-only models can be coaxed into "perceiving" without any modifications to their weights or the provision of actual sensory inputs.

### The Innovation
The key innovation is the development of **"sensory prompting,"** a technique that utilizes explicit linguistic directives (e.g., *"see"* or *"hear"*) to activate latent multimodal structures within the model. Technically, the method diverges from standard analysis by proposing "generative representations" ($z_g$) rather than relying on static single-pass embeddings ($z_e$). Instead of using a single forward pass, the authors average the model's hidden states over the course of $T$ autoregressively generated tokens. This approach leverages the model's residual connections to recursively update its internal state during generation, effectively allowing the model to simulate the cognitive process of resolving predictions based on latent visual or auditory evidence.

### The Results
In experiments involving model sizes ranging from 0.6B to 32B parameters and 128 token generations, generative representations ($z_g$) consistently achieved higher alignment with specialist encoders than static embeddings ($z_e$). Specifically, when evaluated against the DINOv2 vision encoder, alignment scores improved from ~0.09 to ~0.14â€”a **~75% relative increase**â€”while the baseline $z_e$ remained flat near ~0.08. Similarly, alignment with the BEATs audio encoder saw a **~40% relative increase**, rising from ~0.07 to ~0.10 for $z_g$, compared to a ~0.06 baseline for single-pass embeddings.

### The Impact
This research provides empirical evidence for "implicit grounding," demonstrating that perceptual knowledge is fundamentally woven into the statistical patterns of language. By establishing that text-only models can align with sensory modalities purely through prompt engineering, the paper offers a lightweight, low-cost alternative to complex multimodal training or architectural modifications. This capability suggests that future systems can bridge the gap between text and perception by simply eliciting latent knowledge already present in the model, potentially reducing the data dependency for multimodal tasks and expanding the utility of existing LLMs.

---

## Key Findings

*   **Latent Multimodal Structure:** Text-only Large Language Models (LLMs) possess internal representations that are implicitly shaped by multimodal regularities found in language, despite lacking direct perceptual experience.
*   **Activation via Sensory Prompting:** Explicit sensory prompts (such as the directives to "see" or "hear") can successfully surface this latent structure within text-only models.
*   **Alignment with Specialist Encoders:** When cued by sensory prompts, LLMs achieve closer representational alignment with specialist vision and audio encoders.
*   **Efficacy of Lightweight Engineering:** Lightweight prompt engineering is a reliable method for activating modality-appropriate representations (visual or auditory) without altering model weights or providing actual sensory input.

---

## Methodology

The researchers employed a hypothesis-driven approach focusing on **explicit sensory prompting**. They investigated whether instructing a text-only LLM with sensory cues (e.g., "see" or "hear") would cause the model to resolve next-token predictions as if they were conditioned on latent visual or auditory evidence.

The study evaluated this by:
1.  **Prompting:** Applying sensory directives to the model input.
2.  **Analysis:** Analyzing the resulting internal representations of the LLM.
3.  **Measurement:** Measuring the similarity of these representations to those generated by dedicated specialist vision and audio encoders.

---

## Technical Details

The paper introduces technical frameworks for capturing how language models process sensory information.

**Core Concepts**
*   **Generative Representations ($z_g$):** Representations built recursively during autoregressive generation. Defined as the average of hidden states over the course of generating $T$ additional tokens.
*   **Single-Pass Embeddings ($z_e$):** Static representations derived from the initial forward pass only.

**Mechanisms**
*   **Residual Connections:** The approach relies on these connections to recursively update the internal state during generation.
*   **Mutual-$k$NN Alignment Scores:** A metric used to measure alignment with sensory encoders, derived from cosine similarity kernels.
*   **Statistical Validation:** Significance was established via bootstrap sampling.

---

## Experimental Results

Experiments used **128 token generations** across model sizes ranging from **0.6B to 32B**.

### Vision Alignment (vs. DINOv2)
*   **Generative Representations ($z_g$):** Improved from ~0.09 to ~0.14.
    *   *Result:* **~75% relative increase.**
*   **Single-Pass Embeddings ($z_e$):** Remained flat near ~0.08.

### Audio Alignment (vs. BEATs)
*   **Generative Representations ($z_g$):** Improved from ~0.07 to ~0.10.
    *   *Result:* **~40% relative increase.**
*   **Single-Pass Embeddings ($z_e$):** Baseline of ~0.06.

---

## Contributions

*   **Bridging Modalities:** Demonstrates that text-only models can be brought into alignment with sensory (vision/audio) models purely through prompt engineering, removing the immediate need for multimodal training data for certain tasks.
*   **Evidence of Implicit Grounding:** Provides empirical evidence that the statistical patterns of language inherently encode perceptual information, allowing LLMs to "perceive" without direct sensory input.
*   **Low-Cost Intervention:** Establishes "sensory prompting" as a simple, lightweight alternative to complex architectural modifications or fine-tuning for accessing modality-specific knowledge.