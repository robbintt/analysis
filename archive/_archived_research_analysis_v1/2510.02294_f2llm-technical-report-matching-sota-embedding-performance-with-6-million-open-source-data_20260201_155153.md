# F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data

*Ziyin Zhang; Zihan Liao; Hang Yu; Peng Di; Rui Wang*

---

## ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Model Suite** | 0.6B, 1.7B, and 4B Parameters |
| **Training Data** | 6 Million Open-Source Tuples |
| **Top Rank** | **7th** Overall (MTEB English) & **1st** in 1B-2B Category |
| **Key Innovation** | Single-stage fine-tuning (No massive pretraining) |
| **Backbone** | Qwen3 |

---

## ðŸ“ Executive Summary

> The development of state-of-the-art (SOTA) embedding models has increasingly relied on resource-intensive methodologies that create significant barriers to entry. Current leading approaches typically require massive contrastive pretraining stages and the generation of expensive synthetic datasets, often necessitating hundreds of millions of training samples. This trend creates a dichotomy between achieving high performance and managing reasonable computational costs, while also introducing complexity regarding data provenance and pipeline reproducibility. The research addresses the critical question of whether elite embedding quality can be achieved without these prohibitively expensive and complex data-driven strategies.

The core innovation of this work is the **F2LLM framework**, a streamlined approach that achieves SOTA results through direct single-stage contrastive fine-tuning of existing **Qwen2** foundation models. Unlike contemporary methods that rely on massive weakly supervised pretraining and synthetic data, F2LLM employs a curated dataset of only 6 million open-source, non-synthetic tuples. The methodology utilizes a rigorous hard negative mining strategy where a smaller model retrieves and filters negatives, excluding the top 5 results and applying strict score thresholds (< 0.8). The training process optimizes a specific combination of Hard Negative Loss (InfoNCE) with a temperature ($\tau$) of 0.05 and In-Batch Loss, though the In-Batch Loss is dynamically disabled for classification tasks to ensure task-specific optimization.

On the MTEB English leaderboard, the F2LLM models demonstrated performance rivaling or exceeding significantly larger and more resource-heavy systems. The flagship F2LLM-4B model ranked 2nd globally among models with approximately 4 billion parameters and 7th overall, while the F2LLM-1.7B secured the 1st rank within the 1B-2B parameter size range. Specific task performance was also exceptional, with the F2LLM-4B setting a new record in the clustering category with a score of 68.54. Most significantly, the authors demonstrated that this methodology matches the embedding quality of top-performing baselines using only a fraction of the data typically requiredâ€”6 million samples versus hundreds of millions.

This research significantly influences the field by establishing a strong, budget-friendly baseline that challenges the necessity of high-cost synthetic data generation and complex multi-stage pretraining pipelines. By releasing the full model suite (spanning 0.6B, 1.7B, and 4B parameters), the specific open-source dataset, and implementation code, the authors prioritize transparency and reproducibility. This work provides a scalable solution for researchers and practitioners, enabling them to select models that balance parameter size with computational constraints without sacrificing state-of-the-art performance, ultimately lowering the barrier to entry for high-quality embedding model development.

---

## ðŸ”‘ Key Findings

*   **High Benchmark Rankings:** The flagship F2LLM-4B model ranks **2nd** among models with approximately 4B parameters and **7th overall** on the MTEB English leaderboard.
*   **Category Leadership:** The F2LLM-1.7B model achieves the **1st rank** within the 1B-2B parameter size range on the MTEB benchmark.
*   **Cost-Efficiency vs. Performance:** The models match state-of-the-art (SOTA) embedding performance while significantly reducing training costs by avoiding massive contrastive pretraining and expensive synthetic data.
*   **Data Efficiency:** High performance is achieved using a relatively small dataset of **6 million open-source, non-synthetic tuples**.

---

## âš™ï¸ Methodology

The F2LLM approach diverges from previous top-ranking methods by eliminating the need for massive contrastive pretraining and sophisticated pipeline complexity. Instead, the methodology employs **direct fine-tuning** of existing foundation models.

*   **Training Data:** Utilizes 6 million query-document-negative tuples curated exclusively from open-source, non-synthetic datasets.
*   **Strategy:** Prioritizes a direct transfer of foundation capabilities to feature extraction (embedding) to maintain a balance between resource expenditure and model accuracy.

---

## ðŸ”¬ Technical Details

### Model Architecture & Training Strategy
*   **Backbone:** Uses Qwen3 models (0.6B, 1.7B, 4B).
*   **Pipeline:** Single-stage contrastive fine-tuning pipeline, avoiding massive weakly supervised pretraining and architectural modifications.
*   **Input Format:** Unified as `(query, positive passage, hard negative x n)`.
    *   `n=24` for retrieval/clustering.
    *   `n=1` for classification.
*   **Instructions:** Uses task-specific instructions for input formatting.

### Data Composition
*   **Total Volume:** Approx. 6 million open-source non-synthetic tuples.
*   **Breakdown:**
    *   Retrieval: 4.9M
    *   Classification: 0.2M
    *   Clustering: 0.8M

### Hard Negative Mining
*   **Retrieval Model:** Uses Qwen3-Embedding-0.6B to retrieve top 100 passages.
*   **Filtering Process:**
    1.  Excludes top 5 results.
    2.  Filters by score (`< 0.8` and `< 95%` of positive).
    3.  Selects top 24.

### Loss Function & Optimization
*   **Loss:** Unweighted sum of Hard Negative Loss (**InfoNCE**, $\tau=0.05$) and In-Batch Loss.
    *   *Note:* In-Batch Loss is disabled for classification tasks.
*   **Learning Rates:** Scaled by model size.
    *   0.6B: $1e^{-5}$
    *   1.7B: $9e^{-6}$
    *   4B: $8e^{-6}$
*   **Hardware Configuration:** Global batch size of 512, utilizing 16 or 32 GPUs.

---

## ðŸ“ˆ Results

### Overall Performance (MTEB English Leaderboard)
*   **F2LLM-4B:** Ranked **2nd** globally among ~4B parameter models and **7th** overall.
*   **F2LLM-1.7B:** Ranked **1st** in the 1B-2B range.
*   **F2LLM-0.6B:** Ranked **2nd** among models under 1B.

### Task-Specific Metrics
*   **Clustering:** F2LLM-4B achieved a score of **68.54**, setting a new record for the category.

### Comparison to SOTA
*   Matches SOTA embedding quality (comparable to Qwen3-Embedding) using only 6 million data samples, whereas competitors require orders of magnitude more data (e.g., hundreds of millions).

---

## ðŸš€ Contributions

*   **Open-Source Resources:** The release of the full model suite (spanning 0.6B, 1.7B, and 4B parameters), the specific training dataset, and the implementation code to ensure reproducibility.
*   **Performance Benchmarking:** Establishment of a strong, budget-friendly baseline that challenges the necessity of high-cost synthetic data generation and complex pretraining stages for achieving SOTA embedding results.
*   **Scalable Solutions:** Provision of a tiered model suite allowing researchers and practitioners to select the appropriate balance between parameter size (0.6B to 4B) and computational constraints without sacrificing state-of-the-art performance levels.