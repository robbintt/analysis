---
title: End-to-End Test-Time Training for Long Context
arxiv_id: '2512.23675'
source_url: https://arxiv.org/abs/2512.23675
generated_at: '2026-01-27T16:21:54'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# End-to-End Test-Time Training for Long Context

*Jure Leskovec, End Test, Sam Buchanan, Long Context, Daniel Koceja, Xinhao Li, Time Training, Karan Dalal, Arnuv Tandon, Xiaolong Wang*

---

### üìã Quick Facts

| Metric | Details |
| :--- | :--- |
| **Context Length** | Up to 128K tokens |
| **Speed vs. Transformer** | 2.7√ó faster at 128K context (H100 GPU) |
| **Latency** | Constant (O(1)) regardless of context length |
| **Model Size** | 3B Parameters |
| **Comparison** | Outperforms Mamba 2, Gated DeltaNet, and SWA |

> ### üìù Executive Summary
> 
> Extending language models to handle long contexts presents a fundamental trade-off between retrieval performance and computational efficiency. Standard Transformers utilizing full self-attention maintain high performance but suffer from latency and memory costs that scale quadratically or linearly with sequence length, making them impractical for massive contexts. Conversely, state-of-the-art sub-quadratic architectures like Mamba 2 and Gated DeltaNet offer constant latency through recurrence but demonstrate degraded performance as context length increases due to limited information retention.
> 
> This paper proposes **TTT-E2E (End-to-End Test-Time Training)**, a framework that reframes long-context language modeling as a continual learning problem. Employing a standard Transformer with sliding-window attention, the model treats the input context as a training dataset, compressing historical information into its weights via gradient descent. Evaluated on 3B parameter models, TTT-E2E bridges the efficiency-performance divide, scaling similarly to full-attention Transformers with flat loss curves up to 128K tokens. It achieves constant inference latency and is 2.7√ó faster than full attention Transformers at 128K context while maintaining comparable test loss.

---

## üîë Key Findings

*   **Scaling Performance:** TTT-E2E scales with context length similarly to a Transformer with full attention, avoiding the performance degradation seen in state-of-the-art models like Mamba 2 and Gated DeltaNet.
*   **Constant Latency:** The proposed method maintains constant inference latency regardless of context length, a feature traditionally associated with Recurrent Neural Networks (RNNs).
*   **Speed Efficiency:** At a context length of 128K, TTT-E2E is **2.7√ó faster** than a Transformer with full attention on an H100 GPU while achieving comparable test loss.
*   **Context Compression:** The model effectively compresses long contexts into its weights using a standard Transformer architecture equipped with sliding-window attention.

## üî¨ Methodology

The paper reformulates long-context language modeling as a **continual learning problem** rather than purely an architectural design challenge.

*   **Architecture:** It utilizes a standard Transformer architecture with **sliding-window attention (SWA)**.
*   **Test-Time Training (TTT):** The model learns during inference by performing next-token prediction. This process compresses context information into the model's weights.
*   **End-to-End Design:** To facilitate efficient learning at test time, the model's initialization is optimized via **meta-learning** during the training phase.

## ‚öôÔ∏è Technical Details

**Core Mechanism**
*   **Test-Time Training:** Treats the input context as a training dataset to optimize model weights via next-token prediction.
*   **Bilevel Optimization:**
    *   **Inner Loop:** Sequential gradient descent on cross-entropy loss.
    *   **Outer Loop:** Optimizes the initialization.

**Architecture & Efficiency**
*   **Attention Replacement:** Replaces full self-attention with **Sliding-Window Attention (SWA)**.
*   **Parameter Updates:** Updates only **MLP layers** during test-time training.
*   **Batching:** Uses mini-batches to ensure stability and efficiency.
*   **Complexity:**
    *   **Prefill:** $O(T)$ relative to context length.
    *   **Decoding:** $O(1)$ relative to context length.

## üöÄ Contributions

*   **The TTT-E2E Framework:** A novel end-to-end Test-Time Training method enabling massive context scaling without architectural constraints for standard transformers.
*   **Bridging the Trade-off:** Successfully offers full-attention scaling performance combined with RNN-like constant latency.
*   **Meta-Learning for Optimization:** Demonstrates the crucial role of optimizing model initialization for effective test-time learning.
*   **Comprehensive Benchmarking:** Extensive evaluation of 3B parameter models against modern architectures including Mamba 2, Gated DeltaNet, and previous TTT iterations.

## üìä Results

The model was evaluated on context lengths up to **128K tokens** using a 3B parameter model trained on 164B tokens (sliding window size $k=8K$, mini-batch size $b=1K$).

*   **Performance:** TTT-E2E maintains a flat loss comparable to full attention Transformers. It significantly outperforms Mamba 2, Gated DeltaNet, and standard SWA, which all degrade as length increases.
*   **Latency:** Achieves constant inference latency regardless of context length.
*   **Throughput:** At 128K context, TTT-E2E is **2.7√ó faster** than a full attention Transformer on an H100 GPU.

---

**Quality Score:** 9/10 | **References:** 40 citations