# Low-bit Model Quantization for Deep Neural Networks: A Survey

*Kai Liu; Qian Zheng; Kaiwen Tao; Zhiteng Li; Haotong Qin; Wenbo Li; Yong Guo; Xianglong Liu; Linghe Kong; Guihai Chen; Yulun Zhang; Xiaokang Yang*

---

### ðŸ“Š Quick Facts

| Metric | Details |
| :--- | :--- |
| **Document Type** | Survey / Systematic Review |
| **Research Focus** | Low-bit Quantization (DNNs) |
| **Time Scope** | Last 5 Years |
| **Taxonomy** | 8 Main Categories, 24 Sub-categories |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |

---

## Executive Summary

This research addresses the critical challenge of accelerating Deep Neural Networks (DNNs) through low-bit quantization, a process that converts floating-point computations to integer operations to reduce memory overhead and accelerate arithmetic speeds. As DNN models grow in size and complexity, the computational cost of inference becomes a bottleneck for deployment in resource-constrained environments. The core problem lies in minimizing memory I/O and execution time while mitigating the inevitable precision loss and accuracy degradation associated with low-bit representation.

This survey synthesizes five years of research focused on optimizing this conversion process and compensating for the resulting information loss, highlighting the urgent need for structured methodologies to balance efficiency against model performance. The key innovation of this work is the development of a comprehensive taxonomy that classifies the fragmented landscape of low-bit quantization into **8 main categories and 24 sub-categories** based on technical similarities.

Technically, the paper formalizes quantization as a specific operator $Q(\cdot)$, mapping floating-point inputs to integers using defined scale factors ($s$), zero-points ($z$), and bit-widths ($b$). The authors provide explicit mathematical formulations for quantization and de-quantization. They further distinguish optimization strategies by defining global objectivesâ€”minimizing evaluation loss subject to storage constraintsâ€”and local objectives that minimize the Mean Squared Error (MSE) of layer outputs.

As a survey paper, this study does not present original experimental benchmarks but rather establishes the standard theoretical metrics and quantitative parameters used to evaluate state-of-the-art quantization techniques. It identifies the primary evaluation metrics for the field as Storage/Model Size, Evaluation Loss/Accuracy, Local Reconstruction Error, Latency, Energy Consumption, and Throughput. The significance of this research lies in its ability to organize a rapidly evolving and complex field, providing researchers and engineers with a coherent roadmap for navigating low-bit quantization techniques.

---

## Key Findings

*   **Core Mechanism:** The acceleration of DNNs via quantization involves converting floating-point numbers to integers to speed up memory I/O and arithmetic operations, accepting a trade-off in terms of precision loss.
*   **Research Focus:** Over the past five years, research has primarily concentrated on optimizing the conversion process and developing methods to compensate for information loss.
*   **Taxonomy:** The current state of the art includes a diverse landscape of quantization techniques, which the authors have organized into a structured taxonomy consisting of **8 main categories** and **24 sub-categories**.

---

## Technical Details

The paper establishes a rigorous theoretical framework for model quantization, formalizing definitions and optimization strategies.

### 1. Quantization Formalization
The operator $Q(\cdot)$ is defined to map floating-point inputs to integers using three primary parameters:
*   $s$: Scale factors
*   $z$: Zero-points
*   $b$: Bit-widths

**Equations:**
*   **Quantization:**
    $$X_{int} = \text{Clamp}(\text{Round}(X_{FP} / s) + z, n, p)$$
*   **De-quantization:**
    $$\hat{X} = s(X_{int} - z)$$

### 2. Optimization Objectives
The paper distinguishes between two primary optimization objectives:
*   **Global Objective:** Minimizes evaluation loss subject to storage constraints.
*   **Local Objective:** Minimizes the Mean Squared Error (MSE) of layer outputs.

### 3. Range Determination Strategies
Three distinct strategies are detailed for determining the quantization range:
*   **Min-max:** Covers the full range of data.
*   **MSE:** Minimizes the Frobenius norm.
*   **Percentile:** Based on data distribution to manage outliers.

### 4. Classification Types
Quantization methods are further classified by their mathematical properties:
*   **Zero-point Structure:** Symmetric ($z=0$) vs. Asymmetric.
*   **Step Size Distribution:** Uniform (linear, hardware-friendly) vs. Non-Uniform (e.g., logarithmic).

---

## Methodology

The authors employed a **systematic literature review** with the following approach:

1.  **Scope:** Focused specifically on low-bit quantization progress over the last five years.
2.  **Review:** Analyzed state-of-the-art methods in the field.
3.  **Classification:** Performed a taxonomic classification to organize methods into 8 main categories and 24 sub-categories based on technical similarities.
4.  **Analysis:** Conducted a comparative analysis of the identified techniques.

---

## Contributions

*   **Comprehensive Taxonomy:** The paper provides a structured taxonomy organizing recent low-bit quantization work into 8 main and 24 sub-categories.
*   **Future Directions:** Identifies unresolved challenges and specific opportunities for future research.
*   **Community Resources:** Contributed a curated list of references and resources hosted publicly on a **GitHub repository** to facilitate further research and reproducibility.

---

## Results

*   **Theoretical Framework:** The provided text establishes a theoretical framework rather than presenting specific experimental results (e.g., accuracy tables or latency benchmarks).
*   **Standard Metrics:** Identifies standard evaluation metrics for the field:
    *   Storage / Model Size
    *   Evaluation Loss / Accuracy
    *   Local Reconstruction Error (MSE / Frobenius norm)
    *   Latency
    *   Energy Consumption
    *   Throughput
*   **Quantitative Parameters:** Specifies common parameters for range determination, particularly identifying percentile values of **99.0%**, **99.9%**, and **99.99%** for data distribution-based clamping.
