# Adaptive Diffusion Denoised Smoothing: Certified Robustness via Randomized Smoothing with Differentially Private Guided Denoising Diffusion

*Frederick Shpilevskiy; Saiyue Lyu; Krishnamurthy Dj Dvijotham; Mathias LÃ©cuyer; Pierre-AndrÃ© NoÃ«l*

---

> ### ðŸ“Š Quick Facts
> *   **Dataset**: ImageNet
> *   **Threat Model**: $\ell_2$ norm
> *   **Certified Accuracy**: 58.6% @ 0.5 radius
> *   **Standard Accuracy**: 70.4%
> *   **Quality Score**: 7/10

---

## Executive Summary

This paper addresses the fundamental challenge of achieving certified robustness against adversarial attacks on large-scale image classification tasks, specifically within the $\ell_2$ threat model. While randomized smoothing is a proven technique for provable defenses, it traditionally relies on adding static noise, creating a difficult trade-off where increasing robustness leads to a significant drop in standard (clean) accuracy. Furthermore, standard smoothing techniques are non-adaptive, meaning they cannot adjust to the unique content of an input image. This limitation hinders the deployment of certified models in high-dimensional settings like ImageNet, as preserving clean data performance is essential for practical utility.

The key innovation is **"Adaptive Diffusion Denoised Smoothing,"** a framework that interprets a guided denoising diffusion model as a long sequence of adaptive Gaussian Differential Privacy (GDP) mechanisms. Unlike standard smoothing, this method dynamically adapts the denoising process to the specific characteristics of the input image, making it "data-dependent." To handle the theoretical complexity of this adaptation, the authors utilize GDP composition theorems to combine these mechanisms mathematically. This establishes a rigorous end-to-end analysis that extends the guarantees of randomized smoothing to input-dependent, guided diffusion processes, formally linking generative diffusion models with differential privacy theory to create a tractable way to certify adaptive systems.

Empirical evaluations on the ImageNet dataset demonstrate that the proposed method effectively breaks the traditional trade-off between clean performance and robustness. The approach achieves a certified accuracy of **58.6%** at an $\ell_2$-radius of **0.5**, while simultaneously maintaining a high standard accuracy of **70.4%**. This performance represents a significant improvement over existing baselines, specifically surpassing standard Gaussian smoothing methods (e.g., Cohen et al.) which typically suffer severe degradation in standard accuracy when attempting to certify comparable radii. These results validate that the adaptive diffusion framework can deliver superior robustness guarantees without sacrificing the utility required for real-world application.

The significance of this paper lies in its novel achievement of adaptive certification, moving beyond the limitations of static, input-agnostic smoothing. By providing a theoretical synthesis of guided denoising diffusion models and Gaussian Differential Privacy (GDP), the authors enable the end-to-end robustness analysis of adaptive generative processes. This work establishes a foundation for future research into adaptive certified defenses, suggesting that powerful generative priors can be leveraged not just for image synthesis, but for providing provable security guarantees in complex, high-dimensional visual systems.

---

## Key Findings

*   **Improved Accuracy:** The proposed method demonstrates the ability to improve both certified accuracy (robustness) and standard accuracy on the ImageNet dataset.
*   **Threat Model Certification:** The design effectively certifies predictions against adversarial examples specifically within the $\ell_2$ threat model.
*   **Theoretical Linkage:** The study establishes that the end-to-end robustness of guided denoising can be analyzed through the composition of adaptive Gaussian Differential Privacy (GDP) mechanisms.
*   **Adaptive Processing:** The method successfully certifies model predictions while dynamically adapting to the specific characteristics of the input image.

---

## Methodology

The methodology centers on **Adaptive Diffusion Denoised Smoothing**, which reinterprets a guided denoising diffusion model as a long sequence of adaptive Gaussian Differential Privacy (GDP) mechanisms.

*   **Diffusion as GDP:** The diffusion process is viewed mathematically as a series of adaptive GDP steps.
*   **Privacy Filter:** To handle the complexity, the authors utilize a GDP privacy filter to compose these mechanisms.
*   **Framework Extension:** This approach extends the theoretical framework of adaptive randomized smoothing to provide provable certification guarantees.

---

## Contributions

The paper makes three primary contributions to the field of robust machine learning:

1.  **Theoretical Insight:** Provides a key theoretical insight by linking guided denoising diffusion models with Gaussian Differential Privacy (GDP).
2.  **Certification Framework:** Contributes a new certification framework that extends adaptive randomized smoothing analysis to certify predictions where the smoothing process adapts to the input.
3.  **Empirical Validation:** Provides empirical evidence that a specific guiding strategy within this framework surpasses existing baselines by delivering improvements in both standard and certified accuracy on ImageNet.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Core Mechanism** | Guided Denoising Diffusion Process integrated with Randomized Smoothing. |
| **Adaptivity** | Dynamically adjusts the denoising process to the input image's characteristics rather than using static smoothing. |
| **Robustness Analysis** | Establishes end-to-end robustness by interpreting the guided denoising process as the composition of adaptive Gaussian Differential Privacy (GDP) mechanisms. |
| **Threat Model** | Designed to certify predictions against adversarial examples under the $\ell_2$ norm threat model. |

---

## Results

The evaluation was conducted on the **ImageNet** dataset.

*   **Dual Improvement:** The method improved both certified accuracy and standard accuracy compared to baselines, indicating **no trade-off degradation** on clean data.
*   **Performance Metrics:**
    *   Certified Accuracy: **58.6%** ($\ell_2$ radius of 0.5)
    *   Standard Accuracy: **70.4%**
*   **Baseline Comparison:** The method successfully validated the certification of model predictions, notably outperforming standard Gaussian smoothing methods which usually suffer from significant standard accuracy drops when certifying similar radii.

---

**References:** 15 citations