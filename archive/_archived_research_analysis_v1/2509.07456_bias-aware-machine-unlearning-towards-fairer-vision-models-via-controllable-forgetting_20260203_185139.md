---
title: 'Bias-Aware Machine Unlearning: Towards Fairer Vision Models via Controllable
  Forgetting'
arxiv_id: '2509.07456'
source_url: https://arxiv.org/abs/2509.07456
generated_at: '2026-02-03T18:51:39'
quality_score: 9
citation_count: 29
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Bias-Aware Machine Unlearning: Towards Fairer Vision Models via Controllable Forgetting

*Sai Siddhartha Chary Aylapuram; Veeraraju Elluru; Shivang Agarwal*

---

> ### üìä Quick Facts
>
> *   **Quality Score:** 9/10
> *   **Total Citations:** 29
> *   **Composite Score (Avg):** 0.62
> *   **Top Fairness Performance:** 97.37% (CelebA)
> *   **Methodology:** Post-hoc Correction (Gradient Ascent, LoRA, Teacher-Student)

---

## üìù Executive Summary

### Problem
Vision models frequently learn spurious correlations and biases‚Äîsuch as pose, synthetic patch, or gender dependencies‚Äîthat compromise fairness and reliability. While retraining models from scratch can mitigate these issues, it is computationally prohibitive and often impractical for deployed systems. This paper addresses the critical need for efficient, post-deployment mechanisms to correct biases in safety-critical vision domains without incurring the massive resource costs associated with full model retraining.

### Innovation
The authors introduce **"Bias-Aware Machine Unlearning,"** a post-hoc correction framework that utilizes controllable forgetting to selectively remove biased correlations while preserving core task knowledge. The approach optimizes across four distinct dimensions simultaneously: **Utility, Fairness, Quality, and Privacy**. Technically, the study evaluates three specific unlearning strategies to achieve this balance: **Gradient Ascent**, **Low-Rank Adaptation (LoRA)**, and **Teacher-Student distillation**, demonstrating how these methods can effectively "forget" harmful dependencies without discarding the model's learned utility.

### Results
Empirical validation across standard benchmarks‚Äîincluding CUB-200-2011, CIFAR-10, and CelebA‚Äîdemonstrated that the framework achieves significant fairness improvements with minimal utility loss. Specifically, the methods recorded fairness scores of **94.86% on CUB-200-2011**, **97.37% on CelebA**, and **30.28% on CIFAR-10**. Additionally, the approach achieved a composite score of 0.62 across the weighted dimensions, confirming that post-hoc unlearning can maintain high model quality while successfully addressing diverse bias types.

### Impact
This research establishes machine unlearning as a practical and statistically viable alternative to retraining for correcting biases in computer vision. By providing comprehensive benchmarking of unlearning strategies against various bias types, the paper offers a scalable path for maintaining ethical standards in production environments. The findings suggest that developers can efficiently correct model biases post-deployment, significantly lowering the barrier to maintaining fairness in safety-critical applications.

---

## üîë Key Findings

*   **High Fairness Performance:** Achieved significant fairness improvements across benchmarks, scoring **94.86%** on CUB-200-2011, **30.28%** on CIFAR-10, and **97.37%** on CelebA.
*   **Minimal Utility Loss:** The methods successfully balanced bias correction with the preservation of core model functionality.
*   **Composite Performance:** Achieved an average score of **0.62** across the four critical dimensions: Utility, Fairness, Quality, and Privacy.
*   **Viable Alternative:** Confirmed that post-hoc machine unlearning is a statistically and practically viable alternative to full model retraining for bias correction.

---

## üß™ Methodology

The research introduces **Bias-Aware Machine Unlearning**, a post-hoc correction framework designed to utilize controllable forgetting.

**Strategies Evaluated:**
1.  **Gradient Ascent**
2.  **LoRA (Low-Rank Adaptation)**
3.  **Teacher-Student Distillation**

**Validation Process:**
The study performed empirical validation using three distinct datasets to address specific types of bias:
*   **CUB-200-2011:** Addressed pose bias.
*   **CIFAR-10:** Addressed synthetic patch bias.
*   **CelebA:** Addressed gender bias.

---

## ‚öôÔ∏è Technical Details

*   **Core Mechanism:** Post-hoc Machine Unlearning as a mechanism for bias correction.
*   **Process:** Utilization of **Controllable Forgetting** to selectively modify biased correlations while preserving core task knowledge.
*   **Optimization Goals:** The method explicitly optimizes for fairness while minimizing utility loss.
*   **Multi-Dimensional Handling:** The framework handles four dimensions simultaneously:
    *   **Utility**
    *   **Fairness**
    *   **Quality**
    *   **Privacy**

---

## üìà Results

The proposed framework demonstrated robust performance across multiple datasets:

*   **CUB-200-2011:** Fairness score of **94.86%**
*   **CelebA:** Fairness score of **97.37%**
*   **CIFAR-10:** Fairness score of **30.28%**

The method maintained a **composite score of 0.62** across the weighted dimensions, confirming that the approach maintains high quality while addressing diverse bias types.

---

## üåü Contributions

*   **Framework Establishment:** Established machine unlearning as a practical framework for enhancing fairness in safety-critical vision domains.
*   **Comprehensive Benchmarking:** Provided extensive benchmarking of unlearning strategies against diverse bias types (pose, synthetic patch, gender).
*   **Efficiency Demonstration:** Demonstrated that efficient model correction is possible without the prohibitive computational costs of retraining from scratch.