---
title: 'RobuMTL: Enhancing Multi-Task Learning Robustness Against Weather Conditions'
arxiv_id: '2601.10921'
source_url: https://arxiv.org/abs/2601.10921
generated_at: '2026-02-03T19:13:23'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# RobuMTL: Enhancing Multi-Task Learning Robustness Against Weather Conditions

*Tasneem Shaffee; Sherief Reda*

---

### üìä Quick Facts

| Metric | Value |
| :--- | :--- |
| **Top Performance (PASCAL Mixed)** | **+44.4%** relative improvement |
| **Top Performance (NYUD-v2)** | **+9.7%** average relative improvement |
| **Parameter Efficiency** | **3.64x** reduction vs. Single-Task |
| **Compute Efficiency** | **3.43x** lower FLOPs vs. Single-Task |
| **Key Innovation** | LoRA Expert Squad & DMLS Router |
| **Quality Score** | 8/10 |

---

> ## üìù Executive Summary
>
> Multi-Task Learning (MTL) is a critical paradigm for efficient perception in autonomous systems, yet standard MTL models suffer significant performance degradation when faced with adverse weather conditions such as rain, snow, fog, and blur. This vulnerability poses a substantial barrier to the reliable deployment of autonomous agents in real-world environments, where input data is rarely pristine. This paper addresses the challenge of maintaining MTL accuracy across diverse, unpredictable environmental perturbations without sacrificing computational efficiency.
>
> The authors propose **RobuMTL**, a novel architecture that integrates a mixture-of-experts framework with hierarchical Low-Rank Adaptation (LoRA) modules. The core innovation lies in a "LoRA expert squad," where specialized experts are trained to handle specific weather perturbations. A **Dominant perturbation-Matching LoRA Selector (DMLS)** router dynamically detects input conditions and activates the appropriate experts. The architecture further employs a **Hierarchical Rank Strategy**, utilizing smaller ranks in early layers to minimize noise propagation and larger ranks in later layers to recover structural details.
>
> RobuMTL demonstrates significant improvements over standard baselines and establishes explicit superiority over state-of-the-art (SOTA) methods such as TaskPrompter and Meteora. On the PASCAL benchmark, the framework achieved a remarkable **+44.4%** relative improvement under mixed weather conditions. On the NYUD-v2 dataset, it secured a **+9.7%** average relative improvement. Beyond robustness, the model offers substantial efficiency gains, delivering a **3.64x** reduction in parameters and a **3.43x** reduction in FLOPs compared to single-task training.
>
> This research significantly advances the field by providing a validated framework for weather-resilient multi-task learning that outperforms existing solutions. The findings confirm that it is possible to achieve high performance on both clean and perturbed data simultaneously, setting a new standard for reliability in computer vision applications operating in uncontrolled, real-world scenarios.

---

## üîë Key Findings

*   **Significant Improvement Under Mixed Weather:** On the PASCAL benchmark, RobuMTL achieved up to a **+44.4%** relative improvement compared to the Multi-Task Learning (MTL) baseline when handling mixed weather conditions.
*   **Consistent Robustness on PASCAL:** Under single perturbation scenarios, the model delivered a **+2.8%** average relative improvement over the standard MTL baseline.
*   **Strong Performance on NYUD-v2:** When evaluated on the NYUD-v2 dataset, RobuMTL secured a **+9.7%** average relative improvement across tasks compared to baseline methods.
*   **Superiority Over SOTA:** The framework outperformed not only standard MTL baselines and single-task models but also state-of-the-art methods, validating its effectiveness in diverse real-world environments.

---

## üõ†Ô∏è Methodology

The researchers proposed **RobuMTL**, a novel architecture designed to maintain Multi-Task Learning performance in adverse weather. The core methodology relies on a mixture-of-experts framework:

*   **Dynamic Selection:** The system adaptively selects specific modules based on the input perturbations (visual degradation caused by weather).
*   **Hierarchical LoRA Integration:** It utilizes task-specific hierarchical Low-Rank Adaptation (LoRA) modules alongside a "LoRA expert squad."
*   **Adaptive Specialization:** By dynamically choosing which experts and LoRA modules to activate based on input characteristics, the model specializes its processing for the current environmental conditions, thereby enhancing robustness.

---

## ‚öôÔ∏è Technical Details

RobuMTL is a hierarchical LoRA-based adaptive framework designed to maintain MTL performance on clean data while improving robustness against adverse weather conditions (snow, rain, fog, noise, blur) and mixed perturbations.

### Core Components
*   **LoRA Expert Squad:** Utilizes a squad of specialized LoRA experts, with one expert dedicated to each specific perturbation type.
*   **DMLS (Dominant perturbation-Matching LoRA Selector):** A router component that detects the dominant perturbations in the input to select and activate the matching experts.
*   **Hierarchical Rank Strategy:** A rank management approach that uses:
    *   **Smaller ranks in early layers:** To reduce noise propagation.
    *   **Larger ranks in later layers:** To recover image structures and details.

### Operational Workflow
*   **Inference Optimization:** LoRA squad members are aggregated on the CPU before being transferred to the GPU, optimizing memory bandwidth and computational efficiency.

---

## üìà Results

### Performance Metrics
*   **PASCAL Dataset:**
    *   **Mixed Weather:** +44.4% relative improvement.
    *   **Single Perturbations:** +5.00% relative improvement.
    *   **Efficiency:** 63.50 G FLOPs and 101.62 M Parameters.
*   **NYUD-v2 Dataset:**
    *   **Average Improvement:** +9.7% relative improvement.
    *   **Segmentation:** 37.19 mIoU.
    *   **Normals:** 27.52 RMSE.
*   **Clean Data Accuracy:** Improved by +3.18% compared to the single-task baseline.

### Efficiency Analysis
*   **Parameter Reduction:** 3.64x reduction compared to single-task training.
*   **Computation Cost:** 3.43x lower FLOPs compared to single-task training.

### Ablation Studies
*   **DMLS Router Impact:** Boosted performance by up to 60%.
*   **SOTA Comparison:** The model outperformed leading methods like TaskPrompter and Meteora.

---

## üèÜ Contributions

*   **Robustness Framework:** Introduction of RobuMTL, a specialized architecture specifically targeting the vulnerability of autonomous systems to adverse weather conditions in Multi-Task Learning.
*   **Adaptive Mechanism:** Development of a dynamic selection strategy that combines hierarchical LoRA modules with a mixture-of-experts approach to enable input-adaptive model specialization.
*   **Comprehensive Validation:** Providing extensive empirical evidence on standard benchmarks (PASCAL and NYUD-v2) demonstrating that dynamic expert selection significantly mitigates performance degradation caused by environmental perturbations.

---

*Quality Score: 8/10 | References: 40 citations*