---
title: Mixtures of SubExperts for Large Language Continual Learning
arxiv_id: '2511.06237'
source_url: https://arxiv.org/abs/2511.06237
generated_at: '2026-02-03T18:33:22'
quality_score: 6
citation_count: 35
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Mixtures of SubExperts for Large Language Continual Learning

*By Haeyong Kang*

---

> ### **Quick Facts**
> * **Quality Score:** 6/10
> * **Citations:** 35
> * **Methodology:** Mixtures of SubExperts (MoSEs)
> * **Primary Benchmark:** TRACE Datasets
> * **Scaling Behavior:** Sublinear capacity growth
> * **Key Innovation:** Integration of PEFT with Mixture-of-Experts (MoE)

---

## Executive Summary

This research addresses the critical **"PEFT Dilemma"** in the domain of Continual Learning (CL) for Large Language Models (LLMs), specifically within the Task-agnostic incremental learning (TaIL) setting. The core challenge lies in the inherent trade-off between mitigating catastrophic forgetting and managing sustainable model capacity. Standard fine-tuning destroys prior knowledge, while existing Parameter-Efficient Fine-Tuning (PEFT) methods typically rely on dynamic architectures that require a linear expansion of parameters for each new task. This linear growth renders current methods unsustainable for long-term deployment, as model size would eventually exceed memory constraints.

The authors propose **Mixtures of SubExperts (MoSEs)**, a novel framework that synthesizes PEFT with Mixture-of-Experts (MoE) architectures to enable sparse, adaptive learning. Technically, MoSEs integrates a sparse mixture of sub-experts into the transformerâ€™s self-attention layers, replacing dense updates with a modular approach. The architecture utilizes a task-specific routing mechanism that adaptively distributes input tokens and combines outputs as a weighted sum of expert activations.

MoSEs achieves state-of-the-art (SOTA) performance on TRACE benchmarks, delivering quantitative improvements in both accuracy and efficiency. The framework successfully resolves the PEFT Dilemma by demonstrating **sublinear capacity growth**; whereas dynamic architecture methods require a linear increase in parameters relative to the number of tasks (e.g., $O(T)$ scaling), MoSEs significantly reduces this overhead while maintaining high log-likelihoods. These results translate to substantial memory and computational savings, validating the model's ability to maximize performance metrics without the prohibitive cost of linear parameter expansion.

---

## Key Findings

*   **Resolution of the PEFT Dilemma:** Successfully balances the mitigation of catastrophic forgetting against linear model growth.
*   **Superior Benchmark Performance:** Achieved state-of-the-art (SOTA) results on TRACE datasets.
*   **Effective Knowledge Transfer:** Facilitates transfer between related tasks via adaptive combination of sparse parameters.
*   **Resource Efficiency:** Provides significant memory and computational savings with sublinear capacity growth.

---

## Methodology

The authors propose **Mixtures of SubExperts (MoSEs)**, an adaptive Parameter-Efficient Fine-Tuning (PEFT) framework designed specifically for transformer layers. The methodology consists of two primary components:

1.  **Sparse Mixture of SubExperts:** This component is designed to isolate knowledge effectively, preventing interference between distinct tasks.
2.  **Task-Specific Routing Mechanism:** This mechanism adaptively selects and combines sparse parameters from previously learned SubExperts when processing new tasks.

By integrating these components, the framework allows for dynamic adaptation without the need for dense parameter updates across the entire model.

---

## Technical Details

The paper proposes a comprehensive architecture to address Continual Learning in LLMs by integrating Parameter-Efficient Fine-Tuning (PEFT) and Mixture-of-Experts (MoE).

### **Architecture & Mechanics**
*   **Target:** Self-Attention layers within the transformer.
*   **Routing:** Utilizes a **Router** to adaptively distribute input tokens across $N$ sub-experts.
*   **Output Generation:** The final output is calculated as a weighted sum of sub-expert outputs, determined by router gate values.

### **Foundations**
*   **LoRA Basis:** Builds on Low-Rank Adaptation (LoRA) principles, maintaining frozen weights while utilizing low-rank updates.
*   **MoE Basis:** Incorporates Mixture-of-Experts logic, specifically the sparse activation of $k$ experts.

### **Learning Setting**
*   **Task-agnostic Incremental Learning (TaIL):** Task identity is available during the training phase but is unavailable during inference.
*   **Objective:** The primary goal is maximizing log-likelihood over the sequence of tasks.

---

## Results

The evaluation of Mixtures of SubExperts (MoSEs) yielded significant outcomes regarding efficiency and performance:

*   **SOTA Performance:** The method achieved state-of-the-art results on TRACE datasets.
*   **PEFT Dilemma Resolution:** The model successfully balanced catastrophic forgetting mitigation against model growth, achieving **sublinear capacity growth**. This contrasts sharply with the linear growth typical of dynamic architecture methods.
*   **Transfer & Savings:** The model demonstrated effective knowledge transfer between related tasks, resulting in tangible memory and computational savings.

---

## Contributions

*   **Novel Framework Design:** Introduces MoSEs as a new approach for continual learning in Large Language Models.
*   **Sublinear Scaling:** Demonstrates that model capacity can scale sublinearly, breaking from the linear norms of previous methods.
*   **Mechanism for Dual Optimization:** Implements a routing mechanism that serves a dual purpose: protecting prior knowledge while enabling efficient knowledge transfer to new tasks.