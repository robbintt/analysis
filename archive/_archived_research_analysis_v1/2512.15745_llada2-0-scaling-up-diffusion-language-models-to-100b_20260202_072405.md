# LLaDA2.0: Scaling Up Diffusion Language Models to 100B

*Tiwei Bie, Maosong Cao, Kun Chen, Lun Du, Mingliang Gong, Zhuochen Gong, Yanmei Gu, Jiaqi Hu, Zenan Huang, Zhenzhong Lan, Chengxi Li, Chongxuan Li, Jianguo Li, Zehuan Li, Huabin Liu, Lin Liu, Guoshan Lu, Xiaocheng Lu, Yuxin Ma, Jianfeng Tan, Lanning Wei, Ji-Rong Wen, Yipeng Xing, Xiaolu Zhang, Junbo Zhao, Da Zheng, Jun Zhou, Junlin Zhou, Zhanchao Zhou, Liwang Zhu, Yihong Zhuang*

***

### ðŸ“Š Quick Facts & Metrics

| Metric | Detail |
| :--- | :--- |
| **Model Variants** | LLaDA2.0-mini (16B), LLaDA2.0-flash (100B) |
| **Architecture** | Mixture-of-Experts (MoE), Discrete Diffusion |
| **Training Strategy** | 3-Phase Block-Level WSD (Warm-up, Stable, Decay) |
| **MMLU Score** | ~81.5% |
| **GSM8K Score** | >85% |
| **Key Advantage** | Parallel Decoding & Cost-Efficient Conversion |
| **Quality Score** | 8/10 |

***

## Executive Summary

Training discrete diffusion Large Language Models (dLLMs) at the frontier scale of 100 billion parameters has historically been unstable and cost-prohibitive, a limitation that has confined state-of-the-art capabilities almost exclusively to auto-regressive (AR) architectures. While diffusion models theoretically offer superior parallel decoding for efficient inference, the inability to scale them reliably to the 100B mark has prevented them from competing with existing frontier LLMs.

This research addresses this critical "scaling law" gap by introducing a **conversion framework** that transforms pre-trained AR checkpoints into discrete diffusion models, bypassing traditional training bottlenecks. The authors introduce a novel **"3-phase block-level WSD" (Warm-up, Stable, Decay)** training scheme to stabilize this transition:
*   **Warm-up:** Progressive block size increase ($L_B$ from 1 to 4096) converting AR to Masked Discrete LM (MDLM).
*   **Stable:** Full sequence length training.
*   **Decay:** Block size reduction ($L_B$ to ~32) converting to Block Diffusion LM (BDLM).

This methodology successfully produced two instruction-tuned models, **LLaDA2.0-mini (16B)** and **LLaDA2.0-flash (100B)**, achieving performance competitive with leading auto-regressive baselines like Llama-3-70B. By validating a cost-effective conversion process, LLaDA2.0 establishes a viable pathway for deploying high-performance, parallel-decoding-capable models without the massive capital expenditure of training from scratch.

***

## Key Findings

*   **Frontier Scale Achievement:** LLaDA2.0 successfully scales discrete diffusion LLMs (dLLMs) to 100 billion parameters via a conversion process, overcoming previous instability barriers.
*   **Model Variants:** The framework yields two instruction-tuned Mixture-of-Experts (MoE) models:
    *   **LLaDA2.0-mini (16B)**
    *   **LLaDA2.0-flash (100B)**
*   **Efficiency & Performance:** The models deliver superior performance and efficiency while preserving the parallel decoding advantages inherent to diffusion architectures.
*   **Training Stability:** A novel **3-phase block-level training scheme** (Warm-up, Stable, Decay) effectively stabilizes the conversion of auto-regressive knowledge into the diffusion paradigm.
*   **Resource Optimization:** The approach significantly lowers costs by reusing existing AR checkpoints, avoiding the prohibitive expense of training diffusion models from scratch.

***

## Methodology

The researchers utilized a systematic conversion approach adhering to principles of **knowledge inheritance, progressive adaptation, and efficiency-aware design**.

### The Core Process
1.  **Initialization:** Models are initialized from pre-trained auto-regressive checkpoints (Ling-mini-2.0 and Ling-flash-2.0).
2.  **3-Phase WSD Training:**
    *   **Warm-up (Phase 1):** Progressive increasing block-size in block diffusion to convert AR knowledge to MDLM.
    *   **Stable (Phase 2):** Large-scale full-sequence diffusion training ($L_B=4096$).
    *   **Decay (Phase 3):** Reverting back to compact-size block diffusion (~32) to optimize for inference.
3.  **Post-Training:** The models undergo alignment using **Supervised Fine-Tuning (SFT)** and **Direct Preference Optimization (DPO)** to create the final instruction-tuned MoE variants.

***

## Technical Details

### Architecture & Models
*   **LLaDA2.0-mini:** 16 Billion parameters.
*   **LLaDA2.0-flash:** 100 Billion parameters.
*   **Base Architecture:** Discrete Masked Diffusion Language Models (MDLM) and Block Diffusion Language Model (BDLM).
*   **Attention Mechanism:** Utilizes Document-Level Attention Masks.

### Training Specifications
*   **Pipeline:** Continual Pre-training $\rightarrow$ Block Diffusion Pre-training $\rightarrow$ SFT $\rightarrow$ DPO.
*   **WSD Strategy Details:**
    *   *Warmup Phase:* Transforms AR to MDLM by increasing block size ($L_B$) from 1 to 4096.
    *   *Stable Phase:* Trains at full capacity ($L_B=4096$).
    *   *Decay Phase:* Reduces $L_B$ to ~32 to convert to BDLM for efficient inference.
*   **Optimization:**
    *   Loss Function: Cross-entropy.
    *   Merging: Top-k Checkpoint merging technique.

***

## Results

*   **Scaling Success:** LLaDA2.0 successfully scales discrete diffusion models to 100 billion parameters, a significant increase over previous limits.
*   **Benchmark Performance:**
    *   **MMLU:** ~81.5%
    *   **GSM8K:** >85%
    *   Performance is on par with established models like **Llama-3-70B**.
*   **Training Dynamics:** The WSD strategy significantly improves Continual Pre-training efficiency and data utilization compared to direct conversion methods.
*   **Deployment Capability:** The resulting models are optimized for practical deployment and complex instruction-following tasks, maintaining low latency through parallel decoding.

***

## Core Contributions

1.  **New Paradigm for Frontier Deployment:** Established a viable pathway for deploying diffusion language models at the 100B parameter scale, addressing historical scaling difficulties.
2.  **Cost and Resource Efficiency:** Introduced a conversion method that avoids the prohibitive costs of training from scratch by effectively reusing existing AR model checkpoints.
3.  **Training Innovation:** Contributed a unique 3-phase training strategy (Warm-up, Stable, Decay) that handles the complexities of transitioning from block-level to full-sequence diffusion.
4.  **Open-Source Resources:** Released LLaDA2.0-mini and LLaDA2.0-flash to the research community, providing access to high-performance, parallel-decoding-capable models.

***

**Quality Score:** 8/10 | **References:** 13 citations