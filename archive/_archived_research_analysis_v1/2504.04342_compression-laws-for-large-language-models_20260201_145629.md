# Compression Laws for Large Language Models

***Ayan Sengupta; Siddhant Chaudhary; Tanmoy Chakraborty***

---

> ### ðŸ“Š Quick Facts
> *   **Study Scale:** > 1,000 experiments across 8 pre-trained models
> *   **Parameter Range:** 0.5B to 14B parameters
> *   **Max Inference Speedup:** 60% (at 90% compression ratio)
> *   **Recovery Potential:** Up to 55% improvement in test loss via fine-tuning
> *   **Primary Architecture:** Qwen2.5 Family
> *   **Key Trade-off:** Quadratic loss degradation vs. Linear task performance decline

---

## Executive Summary

This paper addresses the critical challenge of deploying Large Language Models (LLMs) in resource-constrained environments where the high memory and computational demands of state-of-the-art models are prohibitive. While model compression (reducing model size) is a standard remedy, the field lacks precise, predictive laws governing how different compression ratios affect complex performance metrics. The authors aim to bridge this gap by establishing a theoretical framework that defines the relationship between compression intensity and model degradation, moving beyond heuristic trial-and-error approaches to a more deterministic understanding of the efficiency-capability trade-off.

The study introduces **"compression laws"** as a formal counterpart to neural scaling laws, providing a quantitative framework to predict post-compression behavior. Technically, the research is grounded in a large-scale empirical analysis of over 1,000 experiments across eight pre-trained models (0.5B to 14B parameters), utilizing the Qwen2.5 family for granular analysis. The authors apply structured weight modification followed by recovery fine-tuning, uncovering a crucial divergence in degradation patterns: test cross-entropy loss increases quadratically with the compression ratio, while performance on downstream reasoning and language understanding tasks declines only linearly.

The empirical outcomes highlight the essential role of recovery fine-tuning, which was shown to recover up to 55% of the test cross-entropy loss and significantly enhance generation quality. In terms of inference efficiency, models compressed at high ratios (up to 90%) achieved a 60% increase in inference speed compared to uncompressed baselines. However, the study found that speed gains are non-uniform across model scales; smaller models ($\le$ 7B parameters) saw speed improvements capped at approximately 35%, while larger models benefited much more substantially. These performance trade-offs were validated across multiple benchmarks, including ARC-c, PIQA, Winogrande, and MMLU.

---

## Key Findings

*   **Divergent Performance Degradation:** The study reveals a distinct split in how compression affects metrics: test cross-entropy loss increases **quadratically** with the compression ratio, whereas performance on downstream tasks declines only **linearly**.
*   **Efficacy of Recovery Fine-tuning:** Recovery fine-tuning is identified as a critical step for compressed models, capable of improving test loss by **up to 55%** and significantly enhancing generation quality.
*   **Speed vs. Compression Ratio:** At high compression ratios (up to 90%), compressed models achieve a **60% increase** in inference speed compared to uncompressed models, helping to compensate for performance degradation.
*   **Model Size Dependency:** Computational gains are not uniform across sizes. Smaller models (<= 7B) show limited speed improvements (peaking at 35%), while larger models benefit significantly more from compression.

---

## Methodology

The research utilizes a large-scale empirical analysis comprising over **1,000 experiments**. The methodology is structured around:

*   **Model Coverage:** Eight pre-trained Large Language Models ranging from **0.5B to 14B parameters**.
*   **Core Investigation:** A specific focus on the impact of *structured model compression* on these pre-trained models.
*   **Evaluation Objective:** To evaluate how varying compression ratios influence performance metrics on downstream tasks.

---

## Technical Details

| Component | Specification |
| :--- | :--- |
| **Model Family** | Qwen2.5 (0.5B, 1.5B, and 3B parameters used for scaling law investigation) |
| **Compression Method** | Structural weight modification |
| **Retention Rates** | Evaluated at 20%, 40%, 60%, and 80% |
| **Recalibration** | Recovery Fine-tuning applied post-compression |
| **Reasoning Tasks** | ARC-c (Challenge) |
| **Commonsense Tasks** | PIQA, Winogrande |
| **Language Understanding** | MMLU |

---

## Results

The experimental data yielded specific insights into the dynamics of compressed models:

*   **Loss vs. Accuracy:** Test cross-entropy loss degrades quadratically with increased compression, whereas downstream task accuracy degrades linearly.
*   **Fine-tuning Impact:** Recovery fine-tuning reduces test loss by up to 55% and significantly improves generation quality.
*   **Inference Speed:**
    *   **High Compression:** Inference speed increases by up to 60% at 90% compression.
    *   **Scale Variance:** Smaller models (<=7B) see gains limited to 35%, while larger models benefit more substantially.
*   **Benchmarks:** Validated across five datasets: ARC-e, Hellaswag, PIQA, Winogrande, and MMLU.

---

## Contributions

*   **Conceptual Framework:** Introduces **"compression laws"** as a counterpart to scaling laws, specifically focusing on the relationship between model compression and performance post-pre-training.
*   **Quantitative Trade-offs:** Provides precise mathematical relationships (quadratic vs. linear) defining how compression degrades cross-entropy loss versus downstream task capabilities.
*   **Practical Deployment Guidelines:** Offers actionable recommendations for adopting LLMs in resource-constrained environments. The study emphasizes that compression is most beneficial for larger models when smaller models within the same computational budget are unavailable.

---

**Quality Score:** 9/10
**References:** 40 citations