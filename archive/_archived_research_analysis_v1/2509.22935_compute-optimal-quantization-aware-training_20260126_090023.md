---
title: Compute-Optimal Quantization-Aware Training
arxiv_id: '2509.22935'
source_url: https://arxiv.org/abs/2509.22935
generated_at: '2026-01-26T09:00:23'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Compute-Optimal Quantization-Aware Training

*As Large, Language Models, Awni Hannun, Aleksandr Dremov, Angelos Katharopoulos, Aware Training, David Grangier*

---

> ### ðŸ“Œ Quick Facts
>
> *   **Parameter Range:** 86.0M â€“ 759.0M
> *   **Training Tokens:** Up to 1.4T
> *   **Precisions Tested:** 1-bit, 2-bit, 4-bit, 6-bit
> *   **Architecture:** Decoder-only Transformer (Llama 2 based)
> *   **Dataset:** DCLM
> *   **Evaluation Metric:** Validation Perplexity

---

## Executive Summary

**Problem**
Standard Quantization-Aware Training (QAT) practices are notoriously inefficient and lack compute-optimality, often resulting in practitioners training models that are too large for their available computational budget. This misallocation leads to suboptimal accuracy, particularly as quantization difficulty intensifies with model scale, meaning larger models suffer greater performance degradation when pushed to low-precision regimes. Prior to this work, the field lacked established scaling laws for quantization, leading to ad-hoc decisions regarding the trade-off between model size, training duration, and bit-width for efficient inference.

**Innovation**
The authors introduce the first scaling laws specifically designed for Quantization-Aware Training, proposing a compute-optimal strategy that minimizes a specific loss function dependent on parameter count, full-precision tokens, QAT tokens, and bit-width. The core technical innovation is a dynamic method for determining the optimal QAT training fraction using a "tokens-per-parameter-byte" metric, which scales based on the available compute budget. This approach challenges prevailing methodologies by demonstrating that downsizing the model architecture and extending the training duration is the superior path for maximizing performance in low-bit (e.g., 4-bit) inference scenarios.

**Results**
Extensive experiments were conducted using decoder-only Transformer architectures (based on Llama 2) ranging from 86.0M to 759.0M parameters, trained on up to 1.4T tokens across precisions from 1-bit to 6-bit. The results confirm that for a fixed compute budget, smaller models trained for longer significantly outperform larger models trained for shorter durations. Specifically, the study found that while larger models require longer QAT phases to recover performance lost to quantization, the proposed strategy yields better accuracy at lower precision. The data showed that 4-bit and 6-bit models achieved quality close to full-precision baselines, whereas 1-bit and 2-bit models exhibited a reasonable, predictable drop in validation perplexity.

**Impact**
This research fundamentally shifts the paradigm for efficient Large Language Model deployment by providing a rigorous framework for resource allocation under constraints. It establishes that quantization difficulty is not constant but scales with model size, offering practical guidelines for practitioners aiming to deploy 4-bit or lower models without ballooning inference costs. By establishing these QAT-specific scaling laws, the paper enables the development of smaller, highly optimized models that rival the performance of larger counterparts trained under inefficient standard protocols.

---

## Key Findings

*   **Compute-Optimality:** Standard QAT practices are not compute-optimal; for a fixed compute budget, **smaller models trained longer** significantly outperform larger models trained for shorter durations at the same bit-width.
*   **Accuracy & Precision:** The proposed approach achieves better accuracy at lower precision (e.g., 4-bit) compared to baseline QAT methods without increasing inference costs.
*   **Scaling Difficulty:** Quantization difficulty increases with model size, necessitating longer training times to recover performance in low-bit regimes.
*   **Dynamic Allocation:** The optimal QAT fraction scales with compute budget and tokens-per-parameter-byte, rather than remaining fixed.

---

## Methodology

The authors employed an empirical methodology similar to scaling law studies, conducting extensive experiments that varied model size, training duration, and compute budgets. They evaluated the trade-offs involved in training Large Language Models (LLMs) for Quantization-Aware Training, focusing on determining the optimal parameter count and training data allocation to maximize performance within a specific computational constraint.

This data was used to derive power-law relationships specific to the QAT regime.

---

## Contributions

*   **New Scaling Laws:** Establishes the first set of scaling laws specifically for Quantization-Aware Training.
*   **Strategic Shift:** Introduces a compute-optimal QAT strategy that challenges prevailing methodologies by demonstrating that downsizing models and extending training is superior for low-bit inference.
*   **Benchmarking & Guidelines:** Provides a comprehensive benchmark of QAT performance across various model sizes and compute budgets, offering guidelines for practitioners on resource allocation for 4-bit or lower deployment.

---

## Technical Details

**Mathematical Modeling**
*   **Loss Function:** Resource allocation is modeled by minimizing a loss function $L(N, D_{fp}, D_{qat}, B)$ dependent on parameter count, full-precision tokens, QAT tokens, and bit-width.
*   **Optimal QAT Fraction:** Predicted using the *tokens-per-parameter-byte* statistic, scaling dynamically based on the compute budget.

**Architecture**
*   **Type:** Decoder-only transformer (based on Llama 2).
*   **Components:**
    *   SwiGLU activations
    *   RoPE positional encodings
    *   RMSNorm
    *   Tied embeddings

**Training Configuration**
*   **Optimizer:** Adam with decoupled weight decay ($0.01$).
*   **Precision:** bfloat16.
*   **Dataset:** DCLM dataset.
*   **Tokenizer:** Llama 2 tokenizer.
*   **Sequence Length:** 1024.
*   **Quantization Baseline:** ParetoQ.

---

## Results

*   **Experiment Scope:**
    *   Model sizes ranged from **86.0M to 759.0M parameters**.
    *   Trained on up to **1.4T** and **669.2B tokens** respectively.
    *   Precisions tested: **1-bit, 2-bit, 4-bit, and 6-bit**.
*   **Performance:**
    *   Larger models experience increased quantization difficulty, requiring longer QAT phases.
    *   **4-bit and 6-bit models** achieved quality close to full-precision baselines.
    *   **1-bit and 2-bit models** demonstrated a reasonable drop in quality.
*   **Validation:** Evaluated using validation perplexity.

---

**Quality Score:** 9/10  
**References:** 40 citations