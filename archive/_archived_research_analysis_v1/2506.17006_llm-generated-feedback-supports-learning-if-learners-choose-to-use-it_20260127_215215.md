---
title: LLM-Generated Feedback Supports Learning If Learners Choose to Use It
arxiv_id: '2506.17006'
source_url: https://arxiv.org/abs/2506.17006
generated_at: '2026-01-27T21:52:15'
quality_score: 9
citation_count: 33
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# LLM-Generated Feedback Supports Learning If Learners Choose to Use It

*Conrad Borchers, Shivang Gupta, Danielle R. Thomas, Shambhavi Bhushan, Erin Gatz*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Sample Size** | 259 Master's Students |
| **Institution** | Carnegie Mellon University |
| **Course Level** | Graduate (Public Policy Writing) |
| **Methodology** | Randomized Controlled Field Experiment (RCT) |
| **AI Model Used** | GPT-4 (Experiment), GPT-3.5-turbo (Schema) |
| **Quality Score** | 9/10 |

---

## Executive Summary

This research addresses the critical challenge of integrating Large Language Models (LLMs) into higher education at institutions like Carnegie Mellon University without compromising learning quality. As educational institutions face increasing pressure to scale instruction, there is concern that AI tools might simply encourage dependence or "gaming" the system rather than fostering genuine skill acquisition. The paper specifically investigates whether LLM-generated feedback can effectively substitute for or augment human instruction in complex tasks like writing, and under what conditions this technology supports actual learning outcomes rather than merely producing better-looking output.

The study introduces a rigorous randomized controlled field experiment (RCT) involving 259 Master's students in a graduate-level public policy writing course. The core innovation is the introduction of a "Choice" condition, moving beyond simple comparisons of AI versus human feedback to examine the role of learner agency. Participants were assigned to one of three conditions: Instructor-Only feedback, GPT-4-Only feedback, or a Choice condition where students selected their preferred feedback source. Technically, the LLM feedback system utilized GPT-4 with few-shot prompting and a predefined schema to generate critiques, ensuring scalable and consistent support. To maintain strict data integrity, learning outcomes were explicitly measured as the difference between initial draft grades and final revision grades, with blind grading conditions used to isolate the effect of the feedback source.

The findings reveal nuanced performance across all conditions. When comparing the forced groups, there was no statistically significant difference in learning gains between the "Instructor-Only" and "GPT-4-Only" conditions (p=0.75). However, the critical determinant of success was learner agency; specifically, within the "Choice" condition, students who actively chose GPT-4 feedback achieved a learning gain 0.31 points higher than those who chose instructor feedback on the final grade scale (p=0.036, Cohen's d=0.47). This effect size suggests a moderate yet significant improvement, underscoring that the availability of high-tech feedback is insufficient without active student engagement, as the sub-group of "AI choosers" meaningfully outperformed the sub-group of "instructor choosers."

This paper offers significant empirical evidence that reframes the debate on AI in education: LLMs function most effectively as support mechanisms rather than replacements for human instruction, provided that students have agency over the process. By identifying learner choice as a critical variable with a measurable effect size, the study provides a scalable framework for deploying LLMs in classrooms that allows institutions to enhance learning outcomes and manage instructor grading burdens without displacing the pedagogical value of human teaching. This establishes a practical, data-backed pathway for the hybrid future of educational technology.

---

## Key Findings

*   **Superior Learning Gains with Choice:** Learners who actively chose AI feedback (GPT-4) demonstrated significantly higher learning gains than those who chose instructor feedback.
*   **Critical Role of Engagement:** Active engagement with feedback is critical; the mere availability of LLM-generated feedback does not guarantee learning improvements.
*   **Higher Quality Output:** Students using LLM feedback in the 'Choice' condition produced final drafts with significantly higher scores as evaluated by blind graders.
*   **Support vs. Replacement:** The data suggests LLM feedback serves as effective support rather than a replacement when students retain agency over the feedback process.

---

## Methodology

The study employed a **randomized controlled field experiment** designed to test the efficacy of AI-generated feedback in a real-world educational setting.

*   **Participants:** 259 Master's students enrolled in a graduate-level public policy writing course at Carnegie Mellon University.
*   **Conditions:** Participants were assigned to one of three experimental groups:
    1.  **Instructor-Only:** Received feedback solely from human instructors.
    2.  **GPT-4-Only:** Received feedback solely from the AI model.
    3.  **Choice:** Allowed to select between Instructor or GPT-4 feedback for their assignments.
*   **Evaluation:** Learning outcomes were measured by comparing grades of initial drafts to final revisions.
*   **Blind Grading:** All assessments were graded blindly by teaching assistants who were unaware of the feedback sources to eliminate bias.

---

## Technical Details

The technical implementation utilized a modified instructional model and a specific LLM architecture to ensure consistent feedback generation.

**Instructional Model**
*   **Framework:** Modified Predict-Observe-Explain (POE) instructional model.
*   **Workflow:** A cyclical process involving four phases:
    1.  Predict
    2.  Explain
    3.  Observe
    4.  Explain Reasoning

**Assessment Structure**
*   **Posttests:** Comprised of 2 Multiple Choice Questions (MCQ) and 2 open-response questions.
*   **Scoring:** Maximum of 4 points per scenario.

**LLM Architecture & Interaction**
*   **Model:** GPT-3.5-turbo utilized for specific classification tasks via few-shot prompting.
*   **Schema:** A predefined schema used to classify learner inputs as 'Correct' or 'Incorrect'.
*   **Feedback Mechanism:**
    *   Incorrect inputs trigger targeted feedback.
    *   System provides a rephrased response with minimal modifications.
    *   Users allowed up to three attempts per problem.

---

## Results

*   **Performance Differential:** Learners who actively chose AI feedback demonstrated significantly higher learning gains and produced higher-scoring final drafts compared to those who chose instructor feedback.
*   **Statistical Significance:** Within the Choice condition, students who chose GPT-4 feedback saw a learning gain **0.31 points higher** than those who chose instructor feedback (p=0.036, Cohen's d=0.47).
*   **Baseline Comparison:** No statistically significant difference was found between forced "Instructor-Only" and "GPT-4-Only" groups (p=0.75).
*   **Benchmarking:** Previous benchmarks established a ~20% learning gain without LLM feedback.
*   **Engagement Metrics:** Data suggests LLM feedback leads to deeper engagement, characterized by learners spending more time processing feedback and producing longer texts.

---

## Contributions

*   **Empirical Evidence:** Provides rigorous field-based empirical evidence that LLM-generated feedback can effectively support complex writing tasks in higher education.
*   **Novel Variable Identification:** Identifies learner agency as a novel and critical variable in determining the efficacy of educational technologies.
*   **Scalable Framework:** Offers a scalable framework for integrating Large Language Models into classroom workflows that enhances learning outcomes without increasing instructor grading burden.

---

*References: 33 Citations*