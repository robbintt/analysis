---
title: 'db-SP: Accelerating Sparse Attention for Visual Generative Models with Dual-Balanced
  Sequence Parallelism'
arxiv_id: '2511.23113'
source_url: https://arxiv.org/abs/2511.23113
generated_at: '2026-02-06T05:54:51'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# db-SP: Accelerating Sparse Attention for Visual Generative Models with Dual-Balanced Sequence Parallelism

*Siqi Chen; Ke Hong; Tianchen Zhao; Ruiqi Xie; Zhenhua Zhu; Xudong Zhang; Yu Wang*

---

> ### ðŸ“Š Quick Facts
>
> | Metric | Value |
> | :--- | :--- |
> | **Model Tested** | Wan2.1-T2V-14B |
> | **Hardware Platform** | A800 GPUs |
> | **End-to-End Speedup** | **1.25x** (vs. SOTA) |
> | **Attention Speedup** | **1.40x** (vs. SOTA) |
> | **Head Imbalance Ratio** | Reduced from 1.51 â†’ **1.01** |
> | **Block Imbalance Ratio** | Reduced from 1.6 â†’ **1.0** |
> | **Quality Score** | 8/10 |

---

## Executive Summary

This research addresses a critical performance bottleneck in the inference of Diffusion Transformers (DiTs) that utilize block-wise sparse attention for high-resolution visual generation. While existing sequence parallelism methods, such as Ulysses and Ring Attention, are effective for dense attention, they fail to account for the non-uniform distribution of sparse attention patterns. In DiTs, the number of active attention blocks varies significantly across heads and sequence segments, causing severe workload imbalance among GPUs. This imbalance results in "stragglers" that dictate the overall step time, undermining the scalability and efficiency of distributed inference systems and leading to suboptimal hardware utilization.

The authors propose **Dual-Balanced Sequence Parallelism (db-SP)**, a sparsity-aware framework designed to mitigate load imbalance through a novel hybrid parallelism strategy. The key innovation is a dual-level partitioning scheme that distributes workloads across both attention heads and sequence blocks, rather than treating the sequence as a monolithic entity. To quantify and address the disparity, the authors introduce a "sparse imbalance ratio" and employ a dynamic runtime optimizer that adjusts the degrees of parallelism for heads and blocks on-the-fly to accommodate evolving sparsity patterns across denoising steps. Additionally, db-SP integrates with FlashAttention2 and enforces a Block Integrity Constraint to prevent the fragmentation of small sparse blocks, thereby preserving kernel efficiency while utilizing a hybrid of All-to-All and P2P ring communication.

Experiments conducted on the Wan2.1-T2V-14B model using A800 GPUs demonstrate significant performance improvements over state-of-the-art baselines like BurstAttention and DSV. db-SP achieves an average end-to-end inference speedup of 1.25x and an attention-specific speedup of 1.40x. Crucially, the method successfully equalizes workload distribution, reducing the sparse imbalance ratio from approximately 1.51 (head-level) and 1.6 (block-level) in baseline methods to near-perfect balance (1.01 and 1.0, respectively). In practical terms, this optimization reduces the time required to generate a single video from over 15 minutes on a single GPU to significantly shorter durations across a distributed cluster.

The significance of this work lies in its ability to unlock the practical potential of large-scale visual generative models by removing a major scalability bottleneck. By formalizing the issue of workload imbalance in sparse attention and providing a dynamic, runtime-aware solution, db-SP enables more efficient utilization of distributed GPU clusters for computationally intensive tasks like video generation. This research not only accelerates current DiT inference but also establishes a new paradigm for handling sparsity in parallel computing, potentially influencing future optimizations in generative AI and large model training.

---

## Key Findings

*   **Imbalance in Current Methods:** Standard sequence parallelism methods (Ulysses, Ring Attention) suffer from severe workload imbalance when applied to block-wise sparse attention in Diffusion Transformers (DiT) due to sparsity variations.
*   **Quantification of Disparity:** The authors introduce a 'sparse imbalance ratio' to formally quantify this workload disparity.
*   **Efficiency Gains:** The proposed db-SP method achieves near-perfect workload balance at head and block levels with negligible computational overhead.
*   **Dynamic Adaptation:** db-SP dynamically determines parallel degrees at runtime to handle evolving sparsity patterns across denoising steps and layers.
*   **Performance Metrics:** Experiments show db-SP achieves an average end-to-end speedup of **1.25x** and an attention-specific speedup of **1.40x** compared to state-of-the-art methods.

---

## Methodology

The methodology introduces **db-SP**, a sparsity-aware sequence parallelism technique designed to accelerate sparse attention in visual generative models. It consists of three core components:

*   **Formalization:** Defining a 'sparse imbalance ratio' to quantify workload imbalances.
*   **Dual-Level Partitioning:** Implementing a strategy targeting both head and block dimensions to ensure balanced distribution.
*   **Runtime Optimization:** Dynamically adjusting parallel degrees for head and block dimensions during inference to address the non-static nature of sparsity in diffusion models.

---

## Technical Details

The paper addresses workload imbalance in Sequence Parallelism (SP) for Diffusion Transformers (DiTs) caused by block-wise sparse attention.

*   **Core Approach:** Proposes Dual-Balanced Sequence Parallelism (db-SP), a hybrid approach that balances workload at both the head-level (distributing attention heads) and block-level (distributing sequence blocks).
*   **Integration:** The method integrates with **FlashAttention2**.
*   **Optimization Constraint:** Enforces a **Block Integrity Constraint** to avoid splitting small sparse blocks, thereby preserving kernel efficiency.
*   **Runtime Strategy:** db-SP utilizes a dynamic runtime strategy to determine optimal parallel degrees.
*   **Communication Protocols:**
    *   Uses **All-to-All** communication for Ulysses components.
    *   Uses **P2P ring communication** for Ring components.

---

## Contributions

*   **Problem Identification:** Identified and formalized the critical issue of workload imbalance in standard sequence parallelism techniques for block-wise sparse attention in DiT inference.
*   **Framework Development:** Developed db-SP, a new sparsity-aware sequence parallelism framework utilizing dual-level partitioning to mitigate imbalance with minimal overhead.
*   **Runtime Strategy:** Contributed a runtime strategy that dynamically adapts parallelism configurations to evolving sparsity patterns.
*   **Performance Demonstration:** Demonstrated significant practical improvements in inference latency, achieving 1.25x end-to-end and 1.40x attention-specific speedups over existing state-of-the-art methods.

---

## Results

db-SP achieves significant performance improvements compared to state-of-the-art methods:

*   **Speedup:** Achieves an average end-to-end speedup of **1.25x** and an attention-specific speedup of **1.40x**.
*   **Workload Balance:** Significantly improves workload balance, reducing the sparse imbalance ratio to approximately **1.01** (head-level) and **1.0** (block-level) from baseline ratios of 1.51 and 1.6.
*   **Experimental Setup:** Experiments were conducted on the Wan2.1-T2V-14B model using A800 GPUs.
*   **Baseline Performance:** A single-GPU baseline took over 15 minutes to generate a video.
*   **Comparison:** The method is compared favorably against BurstAttention and DSV.

---

**Quality Score:** 8/10
**References:** 40 citations