---
title: Weight Factorization and Centralization for Continual Learning in Speech Recognition
arxiv_id: '2506.16574'
source_url: https://arxiv.org/abs/2506.16574
generated_at: '2026-02-03T20:12:39'
quality_score: 9
citation_count: 0
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Weight Factorization and Centralization for Continual Learning in Speech Recognition

*Enes Yavuz Ugan; Ngoc-Quan Pham; Alexander Waibel*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Learning Paradigm** | Rehearsal-free Continual Learning |
| **Core Mechanism** | Weight Factorization & Centralization |
| **Key Innovation** | Bio-inspired "Waking-Sleeping" cycle mimicry |
| **Validation** | Multilingual & Code-Switching Datasets |

---

> ### ðŸ“ Executive Summary
>
> Continual learning (CL) in Automatic Speech Recognition (ASR) faces the critical challenge of catastrophic forgetting, where the introduction of new dataâ€”such as a new language or dialectâ€”degrades the model's performance on previously learned information. This issue is particularly acute in complex linguistic environments like code-switching and multilingual settings. Existing solutions often require rehearsal buffers (storing subsets of original data) or full model retraining to maintain performance, which is computationally expensive, raises privacy concerns, and limits scalability. This paper addresses the stability-plasticity dilemma in speech models, seeking a method that can absorb new knowledge continuously without destructive interference or access to prior training data.
>
> The authors propose a bio-inspired, rehearsal-free framework that mimics the human brain's waking-sleeping cycle through a two-phase algorithm: **Factorization** and **Centralization**. In the Factorization phase (waking/learning), the network decomposes weights to accommodate new information using multiple scattering low-rank adapters. In the Centralization phase (sleeping/merging), the system consolidates this knowledge to prevent degradation. Technically, the architecture manages weight updates via low-rank adapters, allowing the model to integrate new data streams without disrupting the core parameters. This approach effectively decouples the learning of new tasks from the retention of old knowledge, solving the stability-plasticity trade-off without requiring access to original training samples.
>
> The proposed method was validated on the Common Voice dataset across a sequence of languages (English, French, German, and Spanish) and the SEAME code-switching corpus. The results demonstrate a distinct quantitative advantage over standard fine-tuning. While standard fine-tuning caused the Word Error Rate (WER) on the initial language (English) to skyrocket from ~13% to over 43% after learning subsequent languages, the proposed method maintained a stable WER of approximately 14%. Furthermore, the model achieved competitive performance on new tasks, maintaining WERs in the range of 15â€“20% on new languages without sacrificing the accuracy of previously learned ones.
>
> This research significantly advances the field of efficient speech recognition by introducing a viable rehearsal-free continual learning strategy. By eliminating the need to store original training data, the method offers a path toward privacy-preserving and storage-efficient AI systems. The successful application of low-rank adapters for knowledge consolidation suggests a new paradigm for updating production-grade ASR models in real-time environments. Furthermore, the framework's ability to handle code-switching and multilingual data implies that future speech systems can be scaled globally and adapted to diverse linguistic contexts without the prohibitive cost of retraining from scratch.

---

## Key Findings

*   **Catastrophic Forgetting Prevention:** The proposed centralization stage effectively prevents catastrophic forgetting in speech recognition models.
*   **Knowledge Accumulation:** The method successfully accumulates knowledge through the use of multiple scattering low-rank adapters.
*   **Robustness in Complex Scenarios:** The approach is validated on varied code-switching datasets, demonstrating robustness in complex linguistic scenarios.
*   **Model Quality Maintenance:** The solution maintains model quality despite disruptions to weights during continual learning.
*   **Operational Efficiency:** The method operates effectively in rehearsal-free, multilingual, and language-agnostic conditions.

---

## Methodology

The researchers propose a bio-inspired continual learning framework consisting of two distinct phases:

1.  **Inspiration (Bio-mimicry)**
    *   Mimics the human brain's **waking-sleeping cycle**.
2.  **Phase 1: Factorization (Learning Phase)**
    *   Corresponds to the "waking" state.
    *   Involves the decomposition of network weights to accommodate new information.
3.  **Phase 2: Centralization (Merging Phase)**
    *   Corresponds to the "sleeping" state.
    *   Consolidates the learned knowledge to prevent degradation.
4.  **Implementation**
    *   Utilizes **multiple scattering low-rank adapters** to manage weight updates without accessing original training data.

---

## Technical Details

*   **Architecture:** Incorporates a centralization stage to mitigate catastrophic forgetting and uses multiple scattering low-rank adapters for knowledge accumulation.
*   **Operating Conditions:**
    *   Rehearsal-free (no access to prior data).
    *   Language-agnostic.
    *   Multilingual support.
*   **Mechanism:** Manages weight disruptions to maintain stability, effectively decoupling new learning from old retention.

---

## Core Contributions

*   **Rehearsal-free Continual Learning:** Introduction of a strategy that allows neural network-based speech recognition models to absorb new data without re-training the entire system or accessing the original training data.
*   **Two-phase Algorithm:** Development of a novel "factorization and centralization" algorithm that addresses the stability-plasticity dilemma in speech recognition.
*   **Mitigation of Catastrophic Forgetting:** A demonstration that consolidating knowledge in low-rank adapters is a viable mechanism to prevent destructive quality loss in multilingual and language-agnostic environments.

---

## Performance & Results

*   **Validation:** Conducted on varied code-switching datasets (Common Voice for English, French, German, Spanish; SEAME corpus).
*   **Forgetting Prevention:** The method effectively prevented catastrophic forgetting compared to baseline methods.
*   **Quantitative Performance:**
    *   **Standard Fine-tuning:** English WER increased from ~13% to **>43%** after learning subsequent languages.
    *   **Proposed Method:** English WER remained stable at approximately **14%**.
    *   **New Task Performance:** Maintained competitive WERs between **15â€“20%** on new languages.
*   **Outcome:** Successfully maintained model quality and demonstrated robustness under multilingual conditions without rehearsal buffers.

---

## Quality Assessment

**Score: 9/10**

*The research presents a highly effective solution to a critical problem in ASR, offering strong quantitative results and a novel bio-inspired approach.*