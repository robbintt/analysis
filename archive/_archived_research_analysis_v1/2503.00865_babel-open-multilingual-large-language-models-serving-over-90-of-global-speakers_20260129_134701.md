# Babel: Open Multilingual Large Language Models Serving Over 90% of Global Speakers

*Yiran Zhao; Chaoqun Liu; Yue Deng; Jiahao Ying; Mahani Aljunied; Zhaodonghui Li; Lidong Bing; Hou Pong Chan; Yu Rong; Deli Zhao; Wenxuan Zhang*

---

### üìä Quick Facts

| Metric | Details |
| :--- | :--- |
| **Language Coverage** | Top 25 languages by speaker count |
| **Global Reach** | ~7 Billion speakers (>90% of population) |
| **Model Variants** | Babel-9B (Efficient), Babel-83B (SOTA) |
| **Key Innovation** | Layer Extension Technique |
| **Performance Rank** | #1 among 10B-sized open models (Babel-9B) |
| **Quality Score** | 9/10 |

---

> ### üí° Executive Summary
>
> Large Language Models (LLMs) have historically favored high-resource languages, leaving a vast majority of the global population underserved. This research addresses this critical scarcity by introducing **Babel**, a family of open-source multilingual LLMs focused on widely spoken yet under-resourced languages such as Hindi, Urdu, and Swahili.
>
> The authors employ a novel **Layer Extension Technique** to scale parameters and elevate performance ceilings without relying on computationally expensive continued pretraining. This involves inserting new layers initialized with Gaussian noise into the second half of a 72B backbone (expanded to 83B). The study offers two variants: **Babel-9B** for efficient inference and **Babel-83B** for maximum benchmark performance.
>
> Evaluations demonstrate that Babel covers the top 25 languages, reaching approximately 7 billion people. **Babel-9B-Chat** ranks first among all open-source models in the 10B parameter class, while **Babel-83B-Chat** rivals commercial closed-source giants. This work establishes a new state-of-the-art standard for open multilingual LLMs, significantly lowering the barrier to entry for researchers in low-resource linguistic environments.

---

## üéØ Key Findings

*   **Unprecedented Coverage:** Babel supports the top 25 languages by speaker count, covering over 90% of the global population with a specific focus on widely spoken but under-resourced languages.
*   **Superior Performance:** The model demonstrates superior multilingual task proficiency compared to similar-sized open-source LLMs.
*   **Market Leadership:**
    *   **Babel-9B-Chat:** Ranks **1st** among all 10B-sized large language models.
    *   **Babel-83B-Chat:** Achieves performance comparable to commercial closed-source models.
*   **Data Efficiency:** Remarkable performance was attained using entirely open-source supervised fine-tuning datasets, significantly reducing reliance on proprietary data.

---

## üî¨ Methodology

The research introduces a distinct approach to model scaling and training:

*   **Layer Extension Technique:**
    *   Instead of traditional continued pretraining, this method expands the model's parameter count to elevate the performance ceiling.
    *   It preserves the underlying architecture while strategically increasing capacity.
*   **Dual-Variant Architecture:**
    *   **Babel-9B:** Optimized specifically for efficient inference and fine-tuning.
    *   **Babel-83B:** Designed to achieve maximum performance benchmarks.
*   **Training Strategy:** The models were fine-tuned using open-source supervised datasets specifically curated to optimize multilingual task proficiency.

---

## ‚öôÔ∏è Technical Details

### Model Architecture
*   **Objective:** Increase parameter space and performance ceiling without disrupting the underlying architecture.
*   **Methodology:** Layer extension technique involving adding new layers identical to the existing base model.
*   **Positioning Strategy:** New layers are inserted into the **second half** of the model.
*   **Initialization Strategy:** New layers are initialized with **Gaussian noise (mean 0.0001)**.

### Data Preparation Pipeline
*   **Data Sources:** Wikipedia, Textbooks, CC-News, CulturaX, MADLAD-400.
*   **Normalization Rules:**
    *   Exclude documents fewer than 100 characters.
    *   Exclude documents where digit composition exceeds 30%.
*   **Quality Control:** A classifier built on Qwen-2.5-0.5B-Instruct, trained on GPT-4o scores refined by human linguistic experts.
*   **Deduplication:** Process involving hashing, pairing duplicates, graph construction, and removal recording.

### Model Specifications
| Variant | Design Goal |
| :--- | :--- |
| **Babel-9B** | Optimized for efficient inference and fine-tuning. |
| **Babel-83B** | Designed for state-of-the-art performance benchmarks. |

---

## üìà Results

### Language Coverage and Resource Disparity
The project highlights the massive imbalance in current training data (Common Crawl ratios):

| Language | CC Ratio (%) |
| :--- | :--- |
| English | 43.4 |
| German | 5.4 |
| Chinese | 5.1 |
| Hindi | 0.2 |
| Urdu | 0.02 |
| Swahili | 0.008 |
| Javanese | 0.002 |

### Ablation Study (MMMLU Metric)
*Backbone: Qwen2.5-72B-Base | Baseline: 79.5*

| Configuration | Score | Notes |
| :--- | :--- | :--- |
| **Extension Position Impact** | | |
| Among Layers | **72.8 ‚Äì 73.1** | Optimal performance retention |
| After Model | 3.1 ‚Äì 9.4 | Significant performance collapse |
| **Initialization Impact (Among Layers)** | | |
| No Noise (Duplicate) | 73.1 | Highest score in this category |
| Gaussian Noise (0.0001) | 72.8 | Used in final model |
| Gaussian Noise (0.01) | 43.1 | Too much noise degrades performance |

### Overall Benchmarks
*   **Babel-9B-Chat:** Ranked 1st among all 10B-sized large language models.
*   **Babel-83B-Chat:** Achieved performance comparable to commercial closed-source models.

---

## üìù Contributions

1.  **Bridging the Language Gap:** Addresses the scarcity of open-source multilingual LLMs by prioritizing under-resourced languages.
2.  **Advancement in Model Scaling:** Introduces a novel application of the layer extension technique in multilingual contexts as an alternative to standard pretraining.
3.  **New Benchmarks:** Provides high-performing open-source benchmarks, establishing Babel-83B as the state-of-the-art standard for open multilingual LLMs.

---

**References:** 7 citations
**Quality Score:** 9/10