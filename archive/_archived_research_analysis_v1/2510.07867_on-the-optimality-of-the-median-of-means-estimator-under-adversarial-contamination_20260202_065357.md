# On the Optimality of the Median-of-Means Estimator under Adversarial Contamination

*Xabier de Juan; Santiago Mazuelas*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 9/10
> *   **References:** 40 citations
> *   **Estimator Analyzed:** Median-of-Means (MoM)
> *   **Key Focus:** Minimax optimality under adversarial contamination
> *   **Target Distributions:** Finite variance ($P_2$), Infinite variance ($P_{1+r}$), Sub-Gaussian ($P_{SG}$)

---

## Executive Summary

This paper addresses the critical question of the minimax optimality of the Median-of-Means (MoM) estimator in the presence of adversarial contamination. While MoM is a standard tool in robust statistics due to its resilience to corrupted data, the theoretical limits of its performance across different distribution classes were previously undefined. Specifically, it was unclear whether MoM achieves the lowest possible error rates (minimax optimality) for distributions beyond the Gaussian case, particularly in heavy-tailed scenarios where standard mean estimators fail, or how it compares to optimal rates in light-tailed environments. Resolving these theoretical gaps is vital for statisticians and machine learning practitioners who rely on robust estimators to ensure reliable inference when data is subject to manipulation or outliers.

The key innovation is the rigorous derivation of matching theoretical upper and lower bounds for the estimation error of the MoM estimator across three distinct distribution classes: finite variance ($P_2$), infinite variance but finite $(1+r)$-th moment ($P_{1+r}$), and sub-Gaussian ($P_{SG}$). The authors utilize a theoretical framework assuming an adversary capable of inspecting the dataset and replacing up to an $\alpha$-fraction of samples. The proof strategy links MoM error to the quantile function of the inlier block means (via Lemma 3.1) and applies class-specific concentration inequalities: Chebyshevâ€™s inequality is used to bound quantiles for finite and infinite variance distributions, while exponential concentration inequalities are employed for analysis in the sub-Gaussian regime. By comparing these derived bounds to the minimax lower limits, the authors definitively establish the optimality or sub-optimality of the estimator based on the underlying data distribution's tail properties.

The study establishes that the MoM estimator is minimax optimal for distributions with finite variance ($P_2$) and those with infinite variance but finite absolute $(1+r)$-th moment ($P_{1+r}$). For the finite variance class, the error bound is $C(\gamma)\sigma_p (\sqrt{\log(2/\delta)/n} + \sqrt{\alpha})$, yielding an asymptotic bias of $\sqrt{\alpha}$. For the infinite variance class $P_{1+r}$, the asymptotic bias is determined to be $\alpha^{r/(1+r)}$. Conversely, for light-tailed sub-Gaussian distributions ($P_{SG}$), the authors demonstrate that MoM is sub-optimal. In this regime, the trimmed mean achieves a significantly better optimal bias of $\alpha\sqrt{\log(1/\alpha)}$, illustrating that MoM fails to adapt to the lighter tail structure compared to specialized alternatives.

This work significantly advances the field of robust statistics by resolving long-standing theoretical uncertainties regarding the performance guarantees of the Median-of-Means estimator. By proving that MoM is minimax optimal for both finite and infinite variance distributions, the study validates its use as a robust tool in heavy-tailed applications where data integrity is threatened by adversarial attacks. Conversely, the formal identification of MoM's sub-optimality in light-tailed scenarios provides crucial guidance to practitioners, suggesting that alternative estimators like the trimmed mean should be preferred when the data is known to be sub-Gaussian. These findings offer a comprehensive characterization of estimator trade-offs, enabling more informed decision-making in adversarial machine learning and robust data analysis.

---

## Key Findings

*   **Optimality for Finite Variance:** The Median-of-Means (MoM) estimator is proven to be (minimax) optimal for the class of distributions possessing finite variance.
*   **Optimality for Infinite Variance:** MoM is (minimax) optimal for the class of distributions characterized by infinite variance but finite absolute $(1+r)$-th moment.
*   **Sub-optimality for Light Tails:** MoM is sub-optimal when applied to light-tailed (Sub-Gaussian) distributions.
*   **Tight Error Bounds:** The established upper bounds for error are tight, as they match the order of the derived lower bounds across the analyzed distribution classes.

---

## Contributions

*   **Resolution of Theoretical Gaps:** Addresses the previously unknown minimax optimality of the MoM estimator under adversarial contamination beyond the specific case of Gaussian distributions.
*   **Comprehensive Error Bounds:** Provides a rigorous characterization of estimation error by establishing matching upper and lower bounds for both finite variance and heavy-tailed (infinite variance) distributions.
*   **Identification of Sub-optimality:** Contributes a critical insight into the limitations of MoM by formally proving its sub-optimality in scenarios involving light-tailed distributions.

---

## Methodology

The research employs a theoretical analysis framework to evaluate the performance of the Median-of-Means estimator in adversarial settings. The authors derive and compare theoretical upper and lower bounds for estimation error across multiple distinct classes of probability distributions to establish minimax optimality rates.

---

## Technical Details

*   **Adversarial Model:** The model allows an adversary to inspect and selectively remove up to $\alpha n$ samples before replacement.
*   **Estimator Construction:**
    *   Partitions $n$ samples into $k$ disjoint blocks.
    *   Calculates the sample mean for each block.
    *   Aggregates the block means by taking the median.
*   **Proof Mechanism:**
    *   Links the MoM error to the quantile function of the inlier block means using **Lemma 3.1**.
    *   Applies **Chebyshev's inequality** to bound these quantiles for the finite variance case.

---

## Results

The paper provides specific error bounds and bias characterizations based on the distribution class:

*   **Finite Variance ($P_2$):** MoM is minimax optimal.
    *   **Error Bound:** $C(\gamma)\sigma_p (\sqrt{\log(2/\delta)/n} + \sqrt{\alpha})$
    *   **Asymptotic Bias:** $\sqrt{\alpha}$
*   **Infinite Variance ($P_{1+r}$):** MoM is minimax optimal.
    *   **Asymptotic Bias:** $\alpha^{r/(1+r)}$
*   **Sub-Gaussian ($P_{SG}$):** MoM is sub-optimal.
    *   **MoM Bias:** $\alpha^{2/3}$
    *   **Trimmed Mean Bias (Optimal):** $\alpha\sqrt{\log(1/\alpha)}$