---
title: 'Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not
  Guarantee Generalizable Results'
arxiv_id: '2511.02246'
source_url: https://arxiv.org/abs/2511.02246
generated_at: '2026-01-28T00:45:26'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not Guarantee Generalizable Results

*Damianos Karakos, Ms Do, Results On, Jonathan Liu, Haoling Qiu, Not Guarantee, Jonathan Lasko, Mark Dredze, Statistically Significant, Mahsa Yarmohammadi*

> ### üìä Quick Facts
> * **Dataset Scale:** 3.2 Million prompts
> * **Total Evaluations:** 684,000
> * **Inter-LLM Agreement (Avg. Cohen's Kappa):** 0.118 (Very Low)
> * **Scope:** 37 disorders, 518 patient profiles
> * **Models Tested:** Llama3-ChatQA, BioMistral, MedGemma, Qwen, OLMo, Mistral Nemo
> * **Similarity Threshold:** 0.7 (Semantic embeddings)

---

## üí° Executive Summary

This research addresses the critical issue of reliability in evaluating Large Language Model (LLM) biases and errors, particularly within medical applications. While numerous studies claim to identify statistically significant biases or hallucinations, this paper challenges the assumption that such results are generalizable or robust. The authors highlight that current methodologies often rely on singular model configurations, risking that perceived biases are merely artifacts of specific setups rather than inherent model flaws. This matters significantly because deploying medical LLMs based on unstable or non-generalizable safety evaluations could lead to inconsistent patient care and dangerous advice influenced by non-medical demographic factors.

The paper introduces a comprehensive, two-part infrastructure designed to stress-test the generalizability of LLM evaluations. The first component, Automated Query Generation, synthesizes realistic medical queries by sampling diverse patient demographics, clinical histories, and writing styles, ensuring a controlled yet varied dataset. The second component is a Multi-Modal Evaluation Pipeline that employs "LLM-as-a-judge" setups across multiple configurations, ranging from standard JSON-based extraction (using Langchain) to complex agentic workflows involving Detectors, Critics, and Reviewers. This system allows researchers to systematically swap answering and evaluating models to detect how evaluation outcomes fluctuate based on specific pairings.

Testing across a massive dataset of 3.2 million prompts, 29,256 answers, and 684,000 evaluations covering 37 disorders, the study yielded stark insights into evaluation instability. Analysis across models such as Llama3-ChatQA, BioMistral, and MedGemma revealed an average Cohen's Kappa score of only 0.118, indicating extremely low inter-LLM agreement among evaluators. Furthermore, the researchers found that statistically significant differences in responses across demographic groups appeared only in specific model pairings, with the results vanishing or shifting in other configurations. These findings quantitatively demonstrate that "statistically significant" bias findings are highly sensitive to model selection and often non-reproducible.

The significance of this work lies in its direct challenge to standard LLM benchmarking practices, establishing that statistical significance is insufficient for validating bias or error claims. The study compels the research community to adopt more rigorous standards, specifically recommending the use of multiple LLM evaluators and the mandatory publication of inter-LLM agreement metrics to ensure transparency. By releasing their code and benchmarking datasets, the authors provide the tools necessary for the field to move toward reproducible evaluation methods, ultimately ensuring that medical AI systems are assessed based on robust, generalizable evidence rather than configuration-specific artifacts.

---

## üîë Key Findings

*   **Low Inter-LLM Agreement:** LLM evaluators exhibit very low agreement with an average Cohen's Kappa of only 0.118.
*   **Dependence on Model Pairs:** Statistically significant differences in responses across demographics are observed only in specific model pairings, indicating high sensitivity.
*   **Lack of Generalizability:** Statistically significant results in bias and error evaluations do not guarantee generalizable results and may be artifacts of specific model configurations.
*   **Risk of Non-Medical Factor Influence:** Medical LLMs provide inconsistent advice when non-medical demographic factors are introduced.

## üõ†Ô∏è Methodology

The researchers developed a two-part infrastructure designed to stress-test the generalizability of LLM evaluations:

1.  **Automated Query Generation:**
    *   Samples patient demographics, medical histories, disorders, and writing styles.
    *   Objective: Generate realistic patient questions to create a controlled yet varied dataset.
2.  **Multi-Modal Evaluation Pipeline:**
    *   Utilizes 'LLM-as-a-judge' setups.
    *   Features multiple prompts and agentic workflows.
    *   Objective: Detect hallucinations, omissions, and treatment categories across various configurations.
3.  **Testing Protocol:**
    *   Baseline case studies were conducted to analyze LLM agreement.
    *   Tested the impact of different combinations of answering and evaluation LLMs.

## ‚öôÔ∏è Technical Details

**Infrastructure Architecture**
The paper describes a multi-stage infrastructure for generating synthetic medical data and evaluating LLM responses.

*   **Prompt Generation Pipeline:**
    *   Constructs prompts by concatenating 'Patient Expression' and 'Question'.
    *   *Patient Expression:* Demographics and SAMPLE-based clinical info with random attribute inclusion and homogeneous styles.
    *   *Question:* Derived from expert seeds and expanded by LLMs.

*   **Answer Evaluation Methods:**
    *   *Standard LLM-as-a-Judge:* Uses Langchain for structured JSON output to detect hallucinations, omissions, and treatment classification.
    *   *Agentic Workflow:* Round-robin process involving Detectors, Critics, a Reviewer, and a Harm Critic.

*   **Data Filtering:**
    *   Employs semantic embeddings.
    *   Utilizes a similarity threshold of 0.7.

## üìà Results

The study utilized a comprehensive dataset to validate the infrastructure:

*   **Data Volume:** 3.2 Million prompts, 29,256 answers, and 684,000 evaluations.
*   **Coverage:** 37 disorders and 518 patient profiles.
*   **Configuration:** Temperature set to 0.1 across all tested models (Llama3-ChatQA, BioMistral, MedGemma, Qwen, OLMo, Mistral Nemo).
*   **Agreement Analysis:** An average Cohen's Kappa of 0.118 was recorded, indicating very low inter-LLM agreement.
*   **Sensitivity Analysis:** Significant differences in responses across demographics were observed only in specific model pairings. This suggests that statistically significant bias findings may be artifacts of specific configurations rather than robust results.

## üìù Contributions

*   **Evaluation Infrastructure:** Development of a robust, automated infrastructure for generating diverse medical queries and evaluating them via multiple LLM-as-a-judge configurations.
*   **Standardization Recommendations:** Recommendations for the research community to utilize multiple LLMs as evaluators and publish inter-LLM agreement metrics to ensure transparency.
*   **Benchmarking Dataset:** Release of code and datasets to facilitate further research into the generalizability of LLM bias and error detection in medical contexts.

---

**Quality Score:** 8/10  
**References:** 40 citations