# Can Large Language Models Infer Causal Relationships from Real-World Text?

*Ryan Saklad; Aman Chadha; Oleg Pavlov; Raha Moraffah*

---

### ðŸ“Š Quick Facts

| Metric | Value |
| :--- | :--- |
| **Benchmark Name** | ReCITE |
| **Dataset Size** | 292 Samples |
| **Avg. Text Length** | 40,541 Characters |
| **Best Model F1 Score** | 0.535 |
| **Graph Density** | ~37.4 Edges / Document |

---

## Executive Summary

Large Language Models (LLMs) have demonstrated strong causal reasoning capabilities on synthetic or simplified datasets, yet their ability to infer causal relationships from complex, real-world text remains unproven. Previous benchmarks have relied heavily on toy examples or single-relation passages that fail to capture the noise, length, and density inherent in natural academic or professional discourse. This paper addresses the critical gap between idealized testing environments and practical application, investigating whether current LLMs can truly perform causal discovery when presented with authentic, high-complexity texts.

To bridge this gap, the authors introduce **ReCITE** (Real-world Causality from Textual Evidence), the first benchmark specifically designed to evaluate causal discovery in real-world settings. The dataset is constructed from academic literature containing Causal Loop Diagrams (CLDs), which serve as the ground truth. The authors utilized a rigorous data logic pipeline to preserve integrity: papers were sourced from MDPI and PLOS, with visual graphs manually converted to text by annotators to avoid hallucination. The text was then processed to remove noise and normalize node definitionsâ€”crucially stripping explicit graph references to force the model to rely on semantic cues rather than textual artifacts.

The study reveals a substantial performance deficit in current state-of-the-art models when handling real-world data. The ReCITE corpus consists of 292 samples with a mean text length of 40,541 characters and graphs averaging 37.4 edges. Despite high inter-annotator agreement (Cohen's Kappa of 0.987) confirming the dataset's quality, evaluated models including **GPT-4o** and **Claude 3.5 Sonnet** struggled significantly. The best-performing model achieved an average F1 score of only **0.535**. The analysis further indicates that performance is not uniform; it systematically deteriorates based on textual features such as the explicitness of the causal link, the number of events, document length, and the specific domain.

This research serves as a vital corrective to the over-optimistic assessment of LLMs' causal reasoning capabilities. By demonstrating that state-of-the-art models like GPT-4o and Claude 3.5 Sonnet struggle to exceed an F1 score of 0.535 on real-world data, the paper establishes that current architectures are not yet reliable for high-stakes applications such as scientific literature review or complex policy analysis. The introduction of ReCITE provides the community with a robust, challenging target for future development, while the granular analysis of failure modes offers researchers a clear roadmap for improving how models handle the nuances of long-form, implicit causal reasoning.

---

## Key Findings

*   **Significant Performance Gap:** LLMs face major challenges inferring causal relationships from real-world texts, performing substantially worse than they likely do on synthetic datasets.
*   **Low F1 Scores:** Even the best-performing model achieved a modest average F$_1$ score of only **0.535** on the real-world benchmark.
*   **Systematic Variance:** Model performance varies systematically based on textual characteristics, specifically:
    *   The explicitness of the relationship
    *   The number of causal events and relationships
    *   Text length
    *   The specific domain

---

## Methodology

*   **Dataset Construction:** The authors developed a novel benchmark drawn directly from real-world academic literature, selecting for diversity in text length, complexity, and domain.
*   **Evaluation Criteria:** The benchmark is designed to reflect the complexities of real-world reasoning by containing texts with varying levels of explicitness and multiple causal relations, rather than the simplified, single-relation passages used in prior work.
*   **Systematic Analysis:** Experiments were conducted to assess LLM performance, followed by a detailed analysis of how specific aspects of the text (explicitness, event count, length, domain) impact inference capabilities.

---

## Technical Details

### Overview: The ReCITE Benchmark
The paper introduces **ReCITE** (Real-world Causality from Textual Evidence), a benchmark for evaluating causal discovery capabilities. The objective is to construct a causal graph from input text using academic papers containing Causal Loop Diagrams (CLDs) as source material.

### Dataset Construction Pipeline
The dataset construction utilizes a rigorous 3-step pipeline:

1.  **Collection & Filtering:** Sourcing from MDPI and PLOS academic repositories.
2.  **Annotation:** Manual conversion of visual graphs to text by human annotators to strictly avoid LLM hallucinations during data creation.
3.  **Post-Processing:** Graph refinement and node normalization.

### Text Processing Pipeline
To ensure consistency and remove biasing artifacts:
*   **Extraction:** Raw text extraction via PyMuPDF.
*   **Conversion:** Structured conversion to markdown using **Mistral Small** to remove noise and citations.
*   **Normalization:** Processing using **o3-mini** to remove explicit graph references (forcing semantic inference) and ensure consistency.

---

## Results

**Dataset Statistics:**
*   **Corpus Size:** 292 samples
*   **Mean Text Length:** 40,541 characters
*   **Graph Density:** Mean of 25.0 nodes and 37.4 edges per sample
*   **Specificity:** 92.9% of unique concepts appear in only one paper
*   **Explicitness:** 87.7% of causal events are explicitly mentioned in the text

**Validation:**
*   High inter-annotator agreement validation (subset of 37 papers):
    *   Precision: 0.994
    *   Recall: 0.975
    *   F1 Score: 0.984
    *   Cohen's Kappa: 0.987

**Model Performance:**
*   LLMs perform substantially worse on real-world text compared to synthetic datasets.
*   Best Model Average F1: **0.535**.
*   Performance varies significantly based on explicitness, graph complexity, text length, and domain.

---

## Contributions

*   **Real-World Benchmark:** The release of the first-ever real-world dataset for causal relationship inference, moving beyond the synthetic or simplified texts previously used for evaluation.
*   **Performance Assessment:** A comprehensive demonstration of the limitations of current LLMs when handling the nuances of real-world causal reasoning.
*   **Targeted Research Insights:** The provision of structured analysis regarding which text features cause LLMs to struggle, offering a roadmap for future research aimed at advancing causal reasoning in AI.

---

**Paper Quality Score:** 9/10
**References:** 40 citations