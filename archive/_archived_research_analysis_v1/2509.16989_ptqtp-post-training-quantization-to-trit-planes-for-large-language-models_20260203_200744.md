---
title: 'PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models'
arxiv_id: '2509.16989'
source_url: https://arxiv.org/abs/2509.16989
generated_at: '2026-02-03T20:07:44'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models

*He Xiao; Runming Yang; Qingyao Yang; Wendong Xu; Zhen Li; Yupeng Su; Zhengwu Liu; Hongxia Yang; Ngai Wong*

---

> ### **Quick Facts**
>
> *   **Quality Score:** 9/10
> *   **Inference Speedup:** 4.63× faster than FP16 baseline
> *   **Quantization Efficiency:** ~300× faster than QAT (1 hour vs. 10-14 days)
> *   **Model Scale:** Validated on 0.6B to 70B parameters
> *   **Key Innovation:** Multiplication-free, additive inference via ternary trit-planes
> *   **References:** 40 Citations

---

## Executive Summary

Deploying Large Language Models (LLMs) in resource-constrained environments is hindered by the substantial memory and computational overhead of high-precision weights. While quantization effectively reduces these costs, it faces a critical trade-off: existing Post-Training Quantization (PTQ) methods suffer significant accuracy degradation in sub-4bit regimes, whereas Quantization-Aware Training (QAT) recovers accuracy but incurs prohibitive time costs (10-14 GPU days) and requires substantial retraining resources. The industry lacks a method that achieves QAT-level accuracy for ultra-low-bit weights without the operational overhead of training, making efficient, high-fidelity deployment of large-scale models (70B+) a persistent challenge.

The paper introduces **PTQTP** (Post-Training Quantization to Trit-Planes), a framework that resolves this efficiency-capacity trade-off through **Magnitude-Topology Decoupling**. Instead of standard rounding, PTQTP decomposes weight matrices into dual sparse ternary "trit-planes" using the discrete set $\{-1, 0, 1\}$. This decoupling separates the discrete topology (structure) from the continuous magnitude (channel-wise scales), allowing for a high-fidelity sparse approximation. The optimization process employs Adaptive Ridge Regression with a Local Basis Matrix and Progressive Approximation based on Condition Number Estimation to ensure monotonic convergence and global consistency. Crucially, this ternary structure enables **multiplication-free inference**, shifting the computational burden from arithmetic-bound operations to logic-bound additions.

PTQTP achieves state-of-the-art performance, outperforming sub-4bit PTQ baselines like AWQ and GPTQ on language reasoning (MMLU), mathematical reasoning, and coding tasks across model sizes ranging from 0.6B to 70B parameters on LLaMA3.x and Qwen3 architectures. It rivals the accuracy of 1.58-bit QAT methods but reduces the quantization time from 10–14 GPU days to a single hour (a ~300x speedup). Specifically, on LLaMA-7B, PTQTP completes quantization in 11.4 minutes compared to 55.3 minutes for GPTQ and 870 minutes for AQLM. Furthermore, the method delivers a **4.63× end-to-end inference speedup** over the FP16 baseline on NVIDIA RTX 3090 GPUs.

This research establishes a new standard for practical LLM deployment by bridging the gap between the accuracy of training-based methods and the efficiency of post-training techniques. By offering a model-agnostic solution that requires no architectural modifications and eliminates mixed-precision overhead, PTQTP makes ultra-low-bit quantization viable for massive models. The shift to multiplication-free, additive inference not only accelerates throughput but also optimizes hardware utilization, providing a scalable path for deploying advanced LLMs on commodity hardware without sacrificing the reasoning capabilities required for complex tasks.

---

## Key Findings

*   **Superior Performance over PTQ:** PTQTP significantly outperforms existing sub-4bit Post-Training Quantization methods across language reasoning, mathematical reasoning, and coding tasks.
*   **QAT-Level Accuracy at PTQ Cost:** The method rivals the performance of 1.58-bit Quantization-Aware Training (QAT) methods but drastically reduces the time cost from 10-14 GPU days to a single hour.
*   **Significant Inference Speedup:** End-to-end inference speed is achieved at 4.63x faster than the FP16 baseline model, offering a practical solution for resource-constrained environments.
*   **Broad Scalability:** The framework was validated on a wide range of model sizes (0.6B to 70B parameters) across LLaMA3.x and Qwen3 architectures.

---

## Methodology

*   **Trit-Plane Decomposition:** The framework decomposes weight matrices into dual ternary "trit-planes" utilizing the discrete set $\{-1, 0, 1\}$.
*   **Decoupled Representation:** It decouples weights into two distinct components: a discrete topology (the trit-planes) and continuous magnitude (scales), enabling a high-fidelity sparse approximation.
*   **Multiplication-Free Inference:** By leveraging the ternary structure, the approach enables additive inference that eliminates the need for multiplications.
*   **Progressive Approximation:** The method employs a theoretically grounded progressive approximation algorithm to ensure global weight consistency during the quantization process.

---

## Technical Details

PTQTP employs **Magnitude-Topology Decoupling**, representing weight matrices as a linear superposition of dual sparse ternary 'trit-planes' where:

$$W \approx \hat{W} = \sum_{k=1}^{2} \text{diag}(\alpha^{(k)}) \cdot T^{(k)}$$

*   **Topology ($T^{(k)}$):** Consists of discrete routing matrices in $\{-1, 0, 1\}$ capturing coarse structure and spectral residuals.
*   **Magnitude ($\alpha^{(k)}$):** Provides continuous channel gain vectors.

**Optimization & Operations:**
*   **Implicit Denoising:** Supported via the ternary set.
*   **Inference Type:** Multiplication-free, additive inference (shifting computation from arithmetic-bound to logic-bound).
*   **Algorithm:** Uses **Adaptive Ridge Regression** with a Local Basis Matrix ($S_i$) and **Progressive Optimization** based on Condition Number Estimation. This dynamically updates regularization parameters to ensure Monotonic Convergence.
*   **Refinement:** Performed via local exhaustive search.
*   **Batch Processing:** Uses a group size of $G=128$.
*   **System Design:** Model-agnostic design utilizing uniform ternary operations to eliminate mixed-precision overhead.

---

## Results

PTQTP outperforms sub-4bit PTQ methods like AWQ and GPTQ on MMLU, math, and coding tasks across models ranging from 0.6B to 70B parameters, achieving accuracy comparable to 1.58-bit QAT without the training cost.

**Performance Metrics:**
*   **Inference Speed:** Delivers 4.63× faster end-to-end decode speed compared to an FP16 baseline on NVIDIA RTX 3090 GPUs.
*   **Quantization Efficiency:** Requires only a single hour compared to 10-14 GPU days for QAT baselines.
*   **Runtime Comparisons (LLaMA-7B):**
    *   **PTQTP:** 11.4 minutes
    *   **AWQ:** 17.9 minutes
    *   **GPTQ:** 55.3 minutes
    *   **PBLLM:** 22.5 minutes
    *   **SliM-LLM:** 182.2 minutes
    *   **AQLM:** 870 minutes

The method was successfully validated on LLaMA3.x and Qwen3 architectures.

---

## Contributions

*   **Efficiency-Capacity Trade-off Resolution:** Addresses the fundamental challenge in ultra-low-bit quantization by balancing computational efficiency with representational capacity without relying on binary approximations or expensive QAT.
*   **Operational Uniformity:** Introduces uniform ternary operations that eliminate the overhead typically associated with mixed-precision systems.
*   **Model-Agnostic Deployment:** Provides a deployment solution that requires no architectural modifications to the underlying LLMs, ensuring broad applicability.
*   **Resource Optimization:** Establishes a new standard for practical LLM deployment by combining the high accuracy of training-based methods with the low resource overhead of post-training techniques.

---
**Quality Score:** 9/10 | **References:** 40 citations