# Beyond Task Diversity: Provable Representation Transfer for Sequential Multi-Task Linear Bandits

*Thang Duong; Zhi Wang; Chicheng Zhang*

---

> ### ðŸ“‹ Quick Facts
>
> *   **Proposed Algorithm:** BOSS (Bi-level Optimization for Sequential Subspace)
> *   **Core Innovation:** Removes the restrictive "task diversity" assumption.
> *   **Key Theoretical Result:** Meta-regret bound of $\tilde{O}(Nm\sqrt{\tau} + N^{2/3}\tau^{2/3}dm^{1/3} + Nd^2 + \tau md)$.
> *   **Experimental Superiority:** Outperforms PEGE and SeqRepL in non-stationary environments.
> *   **Quality Score:** 9/10
> *   **References:** 40 Citations

---

## Executive Summary

This paper addresses Sequential Multi-Task Linear Bandits (SMLB), focusing on the critical challenge of minimizing cumulative meta-regret across a sequence of tasks without relying on the restrictive "task diversity" assumption. Standard theoretical approaches typically require this assumption to guarantee efficient representation transfer, yet it is often violated in real-world, non-stationary environments.

The authors demonstrate that provably efficient learning is achievable even in the absence of uniform diversity, provided the agent operates within an ellipsoid action set. The key contribution is **BOSS** (Bi-level Optimization for Sequential Subspace), the first algorithm to enable provable representation transfer in sequential settings without task diversity constraints. BOSS utilizes a bi-level architecture combining Meta-Exploration (using a modified PEGE variant) and Meta-Exploitation with an upper-level Meta-Decision Maker.

The study establishes a rigorous meta-regret bound that significantly improves upon baselines in regimes with high task volumes and small subspace dimensions ($m \ll d$). Empirical validation confirms that BOSS variants outperform PEGE and SeqRepL baselines. Crucially, the results suggest that exact subspace recovery is less critical than accurate parameter estimation, offering a more robust paradigm for transfer learning.

---

## Key Findings

*   **Removal of Task Diversity:** Effective learning in sequential multi-task linear bandits is possible without the standard "task diversity" assumption.
*   **Stronger Regret Bounds:** The proposed algorithm achieves a regret bound of $\tilde{O}(Nm\sqrt{\tau} + N^{\frac{2}{3}} \tau^{\frac{2}{3}} d m^{\frac{1}{3}} + Nd^2 + \tau m d)$.
*   **Performance Improvement:** This result improves baselines like $\tilde{O}(Nd \sqrt{\tau})$ specifically when the number of tasks is numerous and the subspace dimension is small.
*   **Empirical Success:** Validation experiments confirm the superiority of this approach over baselines that rely on task diversity assumptions.

---

## Methodology & Contributions

### Methodology
The research addresses sequential multi-task linear bandits by employing a lifelong learning framework characterized by a **low-rank structure**. The study develops a novel algorithm for representation transfer that strategically avoids the task diversity assumption. Instead, it assumes an **ellipsoid action set**, which allows the agent to perform representation learning before all tasks are revealed.

### Core Contributions
*   **Theoretical Breakthrough:** Provides the first nontrivial theoretical result that successfully removes the restrictive task diversity assumption.
*   **New Framework:** Establishes a framework for provably efficient representation transfer without requiring uniform diversity across tasks.
*   **Rigorous Analysis:** Contributes a rigorous regret analysis that quantifies the benefits of low-rank structures even in the absence of task diversity.

---

## Technical Details

### Algorithm: BOSS
**BOSS (Bi-level Optimization for Sequential Subspace)** is designed for sequential multi-task linear bandits to remove the "task diversity" assumption.

**Model Structure**
*   Assumes a low-rank structure where task parameters are defined as:
    $$ \theta_n = B w_n $$

**Bi-Level Architecture**

| Level | Component | Function |
| :--- | :--- | :--- |
| **Lower Level** | **Meta-Exploration** | Uses a modified PEGE variant for unbiased information gathering. |
| | **Meta-Exploitation** | Utilizes reliable subspace estimates to maximize rewards. |
| **Upper Level** | **Meta-Decision Making** | Selects strategies and subspace estimates from an expert set. |

**Theoretical Guarantee**
The algorithm provides a meta-regret bound of:
$$ R_\tau \le \tilde{O}(Nm\sqrt{\tau} + N^{2/3}\tau^{2/3}dm^{1/3} + Nd^2 + \tau md) $$

---

## Results

**Experimental Setup**
*   **Dimensions:** $d=10, m=3$
*   **Scale:** $N=4000$ tasks
*   **Horizon:** $\tau=500$
*   **Environment:** Non-stationary

**Performance Outcomes**
*   **Regret:** BOSS variants significantly outperformed PEGE and SeqRepL baselines in cumulative regret.
*   **Subspace vs. Parameter Estimation:** Interestingly, the *BOSS-no-oracle* variant exhibited higher **subspace estimation error** than SeqRepL but achieved better **parameter estimation accuracy** ($\hat{\theta}_n$).

**Key Insight**
The results suggest that exact recovery of the subspace basis $B$ is **not strictly necessary** for low regret. Instead, accurate estimation of task parameters $\theta_n$ is sufficient to achieve superior performance.