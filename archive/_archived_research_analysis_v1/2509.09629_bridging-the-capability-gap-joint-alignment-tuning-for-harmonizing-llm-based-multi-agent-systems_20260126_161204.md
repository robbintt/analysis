---
title: 'Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based
  Multi-Agent Systems'
arxiv_id: '2509.09629'
source_url: https://arxiv.org/abs/2509.09629
generated_at: '2026-01-26T16:12:04'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Bridging the Capability Gap: Joint Alignment Tuning for Harmonizing LLM-based Multi-Agent Systems

*Shiguang Wu, Pengjie Ren, Minghang Zhu, Zhiwei Xu, Lingjie Wang, Zhengliang Shi*

> ### **Quick Facts**
> *   **Quality Score:** 9/10
> *   **Average Improvement (Held-In):** +3.1%
> *   **Average Improvement (Held-Out):** +4.4%
> *   **Benchmarks Evaluated:** 6
> *   **Key Innovation:** MOAT (Multi-Agent Joint Alignment Tuning)
> *   **References:** 40 citations

---

## Executive Summary

This research addresses the "capability gap" that arises in Large Language Model (LLM)-based multi-agent systems when specialized agents are fine-tuned independently. In systems utilizing a "plan-ground-execute" pipeline, the Planning Agent defines subgoals while the Grounding Agent executes actions. Traditional methods tune these agents in isolation, causing the planner to generate subgoals that the executor cannot handle, and the executor to fail on goals the planner considers feasible. This misalignment leads to coordination failures and suboptimal system performance, highlighting a critical need for training methodologies that ensure mutual compatibility between specialized components.

To resolve these coordination issues, the authors introduce **MOAT (Multi-Agent Joint Alignment Tuning)**, a framework designed to harmonize agents through iterative, reciprocal optimization rather than isolated tuning. The methodology employs a two-stage alternating cycle:
1.  **Planning Agent Alignment:** Using Direct Preference Optimization (DPO), selecting subgoal sequences that yield low Perplexity (PPL) scores from the Grounding Agent to ensure executability.
2.  **Grounding Agent Improving:** Using Supervised Fine-Tuning (SFT) on a dataset of self-generated subgoal-action pairs corrected by a Critic Model to prevent mode collapse.

This feedback loop synchronizes the agents' capabilities, ensuring that the planner generates achievable goals and the grounder learns to execute a diverse range of plans.

Evaluated against state-of-the-art baselines across six benchmarks, MOAT demonstrated significant improvements in both proficiency and generalization. The framework achieved an average performance increase of **3.1% on held-in tasks** (tasks seen during training) and **4.4% on held-out tasks** (unseen tasks), demonstrating its ability to maintain robustness beyond the training distribution. Beyond empirical gains, the authors provide theoretical analysis confirming that the MOAT framework ensures a non-decreasing and progressively convergent training process, establishing a mathematical guarantee for the stability of the proposed joint alignment strategy.

The significance of this work lies in identifying and solving the fundamental limitation of treating multi-agent components as isolated learners. By shifting the paradigm toward joint alignment tuning, MOAT provides a robust blueprint for developing multi-agent systems that are greater than the sum of their parts.

---

## Key Findings

*   **Capability Gap Challenge:** Independent fine-tuning of specialized agents within LLM-based multi-agent systems creates capability gaps that result in poor coordination.
*   **Superior Performance:** The proposed MOAT framework consistently outperforms state-of-the-art baselines across six different benchmarks.
*   **Generalization Gains:** MOAT achieved an average improvement of **3.1% on held-in tasks** and **4.4% on held-out tasks**, indicating strong performance on both known and unseen data.
*   **Training Stability:** Theoretical analysis confirms that the MOAT framework ensures a non-decreasing and progressively convergent training process.

---

## Methodology

The researchers propose **MOAT (Multi-Agent Joint Alignment Tuning)**, a framework designed to harmonize multi-agent systems through iterative alignment rather than independent tuning. The methodology operates through an alternating two-stage process:

1.  **Planning Agent Alignment**
    *   Optimizes the planning agent to generate subgoal sequences that guide the grounding agent effectively.
    *   Utilizes Direct Preference Optimization (DPO) to select executable subgoals.

2.  **Grounding Agent Improving**
    *   Fine-tunes the grounding agent utilizing diverse self-generated subgoal-action pairs.
    *   Enhances generalization by learning from a broad spectrum of potential plans.

---

## Technical Details

### System Architecture
The system follows a **'plan-ground-execute' pipeline** comprising three main components:
*   **Planning Agent ($\pi_p$):** Defines subgoals.
*   **Grounding Agent ($\pi_g$):** Executes actions based on subgoals.
*   **Execution Module:** Performs the final actions.

### Training Loop
The MOAT framework addresses capability gaps using an alternating two-stage training loop:

*   **Stage 1: Planning Agent Alignment**
    *   Uses **Direct Preference Optimization (DPO)**.
    *   Selects subgoal sequences based on low **Perplexity (PPL)** scores from the grounding agent to ensure executability.

*   **Stage 2: Grounding Agent Enhancement**
    *   Uses **Supervised Fine-Tuning (SFT)**.
    *   Utilizes a corrected dataset generated by a **Critic Model** to prevent mode collapse.

---

## Contributions

*   **Problem Identification:** Identified and addressed the critical issue of 'capability gaps' that arise when specialized agents are fine-tuned independently.
*   **Novel Framework:** Introduced the MOAT (Multi-Agent Joint Alignment Tuning) framework, which fosters better collaboration by synchronizing training processes.
*   **Optimization Strategy:** Developed a reciprocal optimization strategy involving a two-stage cycle for tuning planning and grounding agents.
*   **Validation & Theory:** Provided theoretical guarantees regarding training convergence and comprehensive empirical validation showing significant improvements on both held-in and held-out tasks.

---

## Results

Evaluated against SOTA baselines on six benchmarks, MOAT achieved:
*   **+3.1%** average performance improvement on **held-in tasks**.
*   **+4.4%** average performance improvement on **held-out tasks**.

The framework theoretically guarantees a non-decreasing and progressively convergent training process.

---

*Document generated based on analysis of 40 references.*