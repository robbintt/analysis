---
title: Model-Free RL Agents Demonstrate System 1-Like Intentionality
arxiv_id: '2501.18299'
source_url: https://arxiv.org/abs/2501.18299
generated_at: '2026-02-03T12:21:52'
quality_score: 7
citation_count: 15
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# Model-Free RL Agents Demonstrate System 1-Like Intentionality

*Hal Ashton; Matija Franklin*

> ### **Quick Facts**
> * **Quality Score:** 7/10
> * **Reference Count:** 15 Citations
> * **Study Type:** Theoretical / Conceptual Framework
> * **Data Availability:** No empirical data provided
> * **Key Disciplines:** Cognitive Psychology, Legal Theory, Machine Learning

---

## Executive Summary

This paper addresses the ambiguity in defining "intentionality" for reinforcement learning (RL) agents, a critical issue as AI systems gain autonomy. Current legal and ethical frameworks incorrectly conflate intentionality with explicit planning and foresight, frequently dismissing model-free agents as lacking purpose. This oversight hinders accurate responsibility attribution and the development of robust safety protocols. Resolving this theoretical blind spot is essential for effectively governing increasingly complex AI environments.

The key innovation is a taxonomic framework mapping the psychological dichotomy of **System 1** (intuitive) and **System 2** (deliberative) thinking onto machine learning paradigms. The authors align Model-Based RL—characterized by explicit planning—with System 2, while mapping reactive Model-Free RL to System 1. Technically, the work challenges the assumption that planning is a prerequisite for intentionality. It proposes that **"emergent intentionality"** arises from the structured, goal-oriented behaviors of model-free agents, thereby broadening the definition of purposeful action beyond anticipatory modeling.

Because this study is theoretical, it produces no quantitative performance metrics or benchmark data. Instead, the primary result is the validation of a conceptual linkage between human cognitive systems and RL architectures. The authors successfully demonstrate that the functional behaviors of model-free agents satisfy a broader definition of intentionality, proving that purposeful behavior does not require internal simulation of the future.

The significance of this research lies in reshaping the discourse on AI ethics, law, and safety. By establishing that model-free agents possess a distinct form of intentionality, the paper provides a foundation for developing accurate liability frameworks. This shift compels the technical and legal communities to move beyond a "planning-centric" view of agency, acknowledging that reactive systems can still be held to standards of purposeful behavior.

---

## Key Findings

*   **System 1 Alignment:** Model-free reinforcement learning (RL) agents exhibit behaviors analogous to System 1 ("thinking fast") processes in human cognition, characterized by reactivity to environmental stimuli rather than anticipatory modeling.
*   **RL Paradigm Dichotomy:** A direct dichotomy exists between cognitive systems and RL paradigms:
    *   **Model-based RL** aligns with System 2 ("thinking slow") reasoning involving planning.
    *   **Model-free RL** aligns with System 1 intuition.
*   **Redefining Intentionality:** Intentionality and purposeful behavior do not strictly require explicit planning mechanisms; they can manifest within the structured, reactive behaviors of model-free agents.
*   **AI Safety Implications:** The interpretation of intentionality in AI systems necessitates a broader, context-informed framework to address challenges in attributing responsibility and ensuring AI safety.

---

## Methodology

The research utilizes a theoretical and conceptual framework that employs **interdisciplinary synthesis**. Rather than presenting new empirical data or algorithmic benchmarks, the authors draw upon insights from three primary fields:

1.  **Cognitive Psychology:** Leveraging the System 1/System 2 dichotomy.
2.  **Legal Theory:** To understand the requirements for intentionality in law.
3.  **Experimental Jurisprudence:** To analyze the nature of intentionality in artificial agents.

These disciplines are integrated to analyze the nature of intentionality in artificial agents and the resulting implications for law and ethics.

---

## Technical Details

| Component | Description |
| :--- | :--- |
| **Core Concept** | Conceptual framework mapping psychological cognitive systems to RL architectures. |
| **System 2 Mapping** | **Model-Based RL:** Characterized by explicit planning and causal models. |
| **System 1 Mapping** | **Model-Free RL:** Characterized by reactivity and emergent intentionality. |
| **Algorithms** | No specific algorithms or hyperparameters provided (Theoretical analysis). |

---

## Contributions

*   **A Novel Taxonomic Framework:** Establishes a formal linkage between the psychological dichotomy of System 1/System 2 thinking and the machine learning distinction between model-free and model-based RL.
*   **Redefinition of Intentionality:** Challenges the prevailing assumption that planning is a prerequisite for intentionality, proposing that reactive model-free agents possess a distinct form of purposeful behavior.
*   **Ethical and Legal Guidance:** Provides a foundation for future discourse on the ethical deployment, regulation, and attribution of responsibility for AI systems by broadening the interpretation of agent intentionality.

---

## Results

**No experimental results or metrics available in the provided text.** As this is a conceptual paper, the results are qualitative, focusing on the validation of the theoretical linkage between cognitive psychology and RL architectures.