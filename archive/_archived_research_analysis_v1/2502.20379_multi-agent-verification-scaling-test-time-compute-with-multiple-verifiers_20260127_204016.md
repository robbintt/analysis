---
title: 'Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers'
arxiv_id: '2502.20379'
source_url: https://arxiv.org/abs/2502.20379
generated_at: '2026-01-27T20:40:16'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Multi-Agent Verification: Scaling Test-Time Compute with Multiple Verifiers

*Shalev Lifshitz, Sheila A. Mc, Yilun Du, Vector Institute, Harvard University*

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 8/10
> *   **Citations:** 40 references
> *   **Benchmark:** MATH
> *   **Model Used:** Gemini-1.5-Flash
> *   **Key Mechanism:** Multi-Agent Verification (MAV)

---

## Executive Summary

**Problem**
Scaling Large Language Model (LLM) performance at inference time is currently constrained by the reliance on Reward Models (RMs), which require expensive preference data and are prone to producing uncalibrated scores. Furthermore, single-model self-evaluation is unreliable, and training verifiers from scratch is resource-prohibitive. This paper addresses the critical challenge of improving output quality by exploring computationally efficient verification strategies that do not depend on costly fine-tuning pipelines, thereby making high-performance verification accessible to existing models.

**Innovation**
The core innovation is the Multi-Agent Verification (MAV) framework, specifically the Best-of-N Multi-Agent Verification (BoN-MAV) algorithm. Unlike traditional methods that rely on a single trained reward model, MAV utilizes **"Aspect Verifiers" (AVs)**â€”off-the-shelf LLMs prompted to perform binary classification (0 or 1) on granular aspects of a candidate output, such as mathematical calculation, logical coherence, or formatting syntax. This training-free approach aggregates binary approval scores from $m$ heterogeneous verifiers across $n$ candidate outputs. By replacing uncalibrated scalar scores with a robust consensus mechanism, BoN-MAV effectively identifies valid reasoning chains without the need for specialized training.

**Results**
Experiments on the MATH benchmark using Gemini-1.5-Flash demonstrated that BoN-MAV significantly outperforms established baselines. Specifically, BoN-MAV achieved an accuracy of **48.7%**, a substantial improvement over the Self-Consistency baseline (**37.2%**) and the Best-of-N with Reward Models (BoN-RM) approach (**~40.5%**). A key finding regarding scaling laws is that increasing the density of verification yields higher returns than merely generating more candidates; for a fixed compute budget, using 64 verifiers on 4 candidates ($m=64, n=4$) was more effective than using 4 verifiers on 64 candidates ($m=4, n=64$). Additionally, qualitative analysis confirmed that ensembles of 9 weak verifiers successfully achieved "weak-to-strong generalization," approximating the performance of a much stronger single supervisor.

**Impact**
This research establishes the number of verifiers as a critical, scalable dimension for test-time compute, shifting the paradigm from training-intensive reward modeling to inference-time ensemble strategies. By enabling the use of the same base model for both generation and verification, MAV offers a viable pathway for immediate **"self-improvement"** without external data or retraining. The demonstration that weak verifiers can be aggregated to form a strong consensus provides organizations with a method to significantly enhance model reliability and accuracy using existing infrastructure, effectively reducing dependency on costly preference data and specialized verifier training.

---

## Key Findings

*   **Scalability:** Increasing the number of verifiers is a viable scaling dimension for test-time compute.
*   **Superior Scaling:** The BoN-MAV algorithm demonstrates stronger scaling behaviors than existing methods (such as standard Reward Models).
*   **Generalization:** The system exhibits *weak-to-strong generalization* by combining multiple weak verifiers to approximate strong performance.
*   **Self-Improvement:** The approach enables self-improvement capabilities using the same base model for both generation and verification tasks.

---

## Methodology

The authors propose a paradigm utilizing multiple distinct verifiers known as **Multi-Agent Verification (MAV)**.

*   **Aspect Verifiers (AVs):** The implementation uses "off-the-shelf" LLMs (no specific verifier training required) prompted to verify specific aspects of outputs (e.g., logic, syntax).
*   **BoN-MAV Algorithm:** The core algorithm combines Best-of-N (BoN) sampling strategies with multi-verifier consensus. It generates candidate outputs and utilizes the aggregation of verifiers to select the best result.

---

## Technical Details

**Core Approach**
The Multi-Agent Verification (MAV) approach optimizes test-time compute by scaling performance through two distinct dimensions:
1.  **Candidate Outputs ($n$)**
2.  **Verifiers ($m$)**

This optimization occurs without the need for additional verifier training.

**Algorithm Mechanics**
The **Best-of-N Multi-Agent Verification (BoN-MAV)** algorithm operates as follows:
1.  **Generation:** Creates $n$ candidate outputs.
2.  **Verification:** Aggregates binary approval scores ($0$ or $1$) from $m$ Aspect Verifiers (AVs).
3.  **Selection:** Selects the output with the highest aggregate score.

**Verifier Characteristics**
*   **Classification:** AVs perform binary classification on specific aspects (e.g., Correctness, Logic).
*   **Reasoning:** Utilizes Chain-of-Thought (CoT) reasoning to arrive at decisions.
*   **Advantages over RMs:**
    *   Avoids uncalibrated scores often found in Reward Models.
    *   Does not require expensive preference data.
    *   Supports heterogeneity in models and prompts.

---

## Results

Experiments were conducted using **Gemini-1.5-Flash** on the **MATH benchmark**:

*   **Performance:** BoN-MAV@16 outperformed both Best-of-N with a Reward Model (BoN-RM) and Self-Consistency.
*   **Scaling Efficiency:** Scaling the number of verifiers ($m$) demonstrated stronger performance gains compared to existing scaling methods.
*   **Qualitative Analysis:**
    *   Verifiers successfully identified arithmetic and logical errors.
    *   A voting matrix example showed a specific rejection score of 2/9, illustrating the consensus mechanism.
*   **Generalization:** The method confirmed weak-to-strong generalization and showed potential for self-improvement.

---

## Contributions

*   **Framework Introduction:** Introduced the **'Multi-Agent Verification' (MAV)** framework for scaling test-time compute.
*   **Training-Free Implementation:** Proposed **'Aspect Verifiers' (AVs)** as a method to implement multi-agent systems without training.
*   **Algorithm Development:** Developed the **'BoN-MAV'** algorithm, which outperforms traditional baselines.
*   **Empirical Evidence:** Provided evidence for weak-to-strong generalization and self-improvement within the context of automated verification.