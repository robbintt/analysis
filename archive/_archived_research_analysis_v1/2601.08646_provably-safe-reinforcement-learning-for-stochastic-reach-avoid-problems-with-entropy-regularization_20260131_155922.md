# Provably Safe Reinforcement Learning for Stochastic Reach-Avoid Problems with Entropy Regularization

*Abhijit Mazumdar; Rafal Wisniewski; Manuela L. Bujorianu*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score:** 9/10
> *   **References:** 40 Citations
> *   **Framework:** Markov Decision Process (MDP)
> *   **Core Innovation:** Entropy Regularized p-Safe RL (ER-pSRL)
> *   **Safety Guarantee:** Probability $1-\delta$
> *   **Key Benefit:** Drastically reduces episode-to-episode variability

---

## Executive Summary

This research addresses the challenge of learning optimal policies in safety-critical systems modeled as Stochastic Reach-Avoid problems within Markov Decision Processes (MDPs). The core issue lies in the instability of standard "Optimism in the Face of Uncertainty" (OFU) algorithms, which frequently suffer from high episode-to-episode variance. This instability often stems from linear programming solutions that jump between vertices as the empirical model updates, posing safety risks during the learning phase. The authors emphasize the necessity of guaranteeing that safety constraints (reach-avoid objectives) are met with probability $1-\delta$ while the agent is actively learning.

The key innovation is the **Entropy Regularized p-Safe RL (ER-pSRL)** algorithm, which integrates entropy regularization into the OFU framework to stabilize the learning process. By reformulating per-episode policy selection as a strongly convex optimization problem, the algorithm minimizes expected cost minus an entropy term defined over occupation measures. This continuous dependence on the empirical model prevents vertex-switching behavior, effectively penalizing sparse solutions and enabling principled exploration.

Benchmarked against a baseline pSRL algorithm, ER-pSRL demonstrated superior policy stability, eliminated vertex switching, and achieved consistently lower cumulative objective regret. Crucially, both algorithms satisfied safety constraints throughout the process, empirically validating the safety threshold.

---

## Key Findings

*   **Safe Policy Formulation:** The study formulates the problem of safe policy learning in MDPs using a reach-avoid setup, ensuring that safety constraints are rigorously met.
*   **High-Probability Safety Guarantees:** The proposed online reinforcement learning algorithms are capable of guaranteeing safety constraints with arbitrarily high probability ($1-\delta$) specifically during the learning phase.
*   **Improved Regret Bounds:** The inclusion of entropy regularization results in improved regret bounds compared to the baseline standard approach.
*   **Reduced Variability:** Entropy regularization drastically reduces the episode-to-episode variability inherent in standard Optimism in the Face of Uncertainty (OFU)-based safe RL algorithms.
*   **Theoretical Validation:** Finite-sample analysis was successfully used to derive regret bounds for both the standard OFU and the entropy-regularized algorithms.

---

## Methodology

The research methodology follows a structured three-phase approach:

1.  **Problem Structuring:** The problem is defined as a stochastic reach-avoid task within a Markov Decision Process framework, focusing on optimal policy learning under strict safety constraints.
2.  **Algorithm Development:**
    *   **Baseline:** Initial development of an algorithm utilizing the Optimism in the Face of Uncertainty (OFU) principle.
    *   **Core Innovation:** Introduction of a main algorithm that integrates entropy regularization into the OFU framework.
3.  **Theoretical Evaluation:** Reliance on finite-sample analysis to theoretically evaluate performance and derive regret bounds for the proposed algorithms.

---

## Technical Details

The paper introduces **Entropy Regularized p-Safe RL (ER-pSRL)**. Below are the specific technical specifications and mechanisms utilized:

| Component | Description |
| :--- | :--- |
| **Environment Model** | MDP framework with states partitioned into **Goal**, **Unsafe**, and **Living** sets. |
| **Optimization Strategy** | Solves a per-episode entropy-regularized convex optimization problem. |
| **Objective Function** | Minimizes expected cost minus an entropy term involving occupation measures. |
| **Strong Convexity** | Utilizes a custom strong convex entropy function and a specific regularization coefficient (**$\alpha$**) to ensure the optimization problem remains strongly convex. |
| **Stability Mechanism** | Ensures stability by making optimal occupation measures depend **continuously** on the empirical transition model, preventing the policy instability typical of standard LP-based approaches. |
| **Exploration** | Facilitates principled exploration by penalizing sparse solutions. |
| **Guarantees** | Provides theoretical guarantees for **p-safety** with probability $1-\delta$, along with finite-sample regret bounds. |

---

## Experimental Results

The performance of ER-pSRL was evaluated against a baseline pSRL algorithm using a 5-state, 2-action MDP.

*   **Configuration:** Safety threshold $p=0.5$ and maximum stopping time $T_{max}=5$.
*   **Policy Stability:**
    *   ER-pSRL demonstrated **significantly higher policy stability** than the baseline.
    *   Exhibited lower variability and peak regret by avoiding vertex switching.
*   **Learning Efficiency:**
    *   ER-pSRL achieved consistently **lower cumulative objective regret**.
    *   Indicates faster convergence and higher sample efficiency.
*   **Safety Compliance:**
    *   Both ER-pSRL and the baseline satisfied safety constraints throughout learning.
    *   Per-episode constraint regret remained strictly below zero.
*   **Proxy Sets:** The study also evaluated the impact of known versus unknown proxy sets on the safe baseline policy.

---

## Contributions

*   **Safe Learning Framework:** Development of a framework with online RL algorithms that formally guarantee safety constraints (reach-avoid) with high probability during the learning process.
*   **Rigorous Analysis:** Provision of rigorous finite-sample analysis and derived regret bounds for safe RL algorithms in stochastic settings.
*   **Entropy Regularization:** Introduction of entropy regularization as a mechanism to improve regret bounds and specifically address the issue of episode-to-episode variability in safe RL.