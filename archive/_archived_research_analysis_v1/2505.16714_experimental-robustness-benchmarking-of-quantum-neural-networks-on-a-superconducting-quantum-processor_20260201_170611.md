# Experimental robustness benchmarking of quantum neural networks on a superconducting quantum processor

*Hai-Feng Zhang; Zhao-Yun Chen; Peng Wang; Liang-Liang Guo; Tian-Le Wang; Xiao-Yan Yang; Ren-Ze Zhao; Ze-An Zhao; Sheng Zhang; Lei Du; Hao-Ran Tao; Zhi-Long Jia; Wei-Cheng Kong; Huan-Yu Liu; Athanasios V. Vasilakos; Yang Yang; Yu-Chun Wu; Ji Guan; Peng Duan; Guo-Ping Guo*

***

> ### F4CA Quick Facts
>
> *   **Hardware:** 20-qubit superconducting quantum processor
> *   **Dataset:** EMNIST
> *   **QNN Architecture:** 3-qubit data re-uploading
> *   **Robustness Advantage:** ~25–40% higher than classical FNNs
> *   **Bound Deviation:** $3 \times 10^{-3}$
> *   **Training Method:** Adversarial training via regularization

***

## Executive Summary

### Problem
As Quantum Machine Learning (QML) advances toward practical application, the security of Quantum Neural Networks (QNNs) against adversarial attacks has emerged as a critical concern. While classical neural networks are known to be vulnerable to subtle, malicious perturbations in input data, significantly less is understood about how these vulnerabilities translate to quantum architectures. This paper addresses the urgent need to evaluate the adversarial robustness of QNNs on physical hardware, specifically investigating whether the inherent noise present in current quantum processors acts as a liability or, conversely, as a natural defense mechanism against such attacks.

### Innovation
The authors established the first systematic experimental robustness benchmark for a QNN classifier executed on a 20-qubit superconducting quantum processor. The study utilized a 3-qubit data re-uploading architecture, comparing it against a classical Fully Connected Neural Network (FNN) baseline with matching parameter counts. To assess and improve robustness, the team introduced a framework featuring an efficient adversarial attack algorithm tailored for quantum circuits. Additionally, they formulated adversarial training as a regularization problem, using a first-order approximation to minimize input gradients and reduce the model's sensitivity to malicious perturbations, thereby allowing for the quantitative comparison of empirical attack outcomes against theoretical fidelity-based bounds.

### Results
Experimental results demonstrated that QNNs possess significant structural advantages in adversarial robustness over classical counterparts. In noiseless simulations using the EMNIST dataset, QNNs substantially outperformed FNNs in robustness scores ($R_{adv}$): at Scale 1 (162 parameters), the QNN scored 0.105 compared to the FNN's 0.075, and at Scale 2 (324 parameters), the QNN scored 0.104 against the FNN's 0.079. This indicates that QNNs maintain approximately 25–40% higher robustness even in ideal environments. Furthermore, the study validated that theoretical fidelity-based robustness bounds are consistent with experimental behavior, confirming the tightness of these bounds with a deviation of only $3 \times 10^{-3}$.

### Impact
This research provides foundational evidence for the relationship between quantum noise and model security, reframing hardware noise from a purely erroneous feature into a potential security asset. However, the study clarifies that while QNNs exhibit a comparative advantage over classical networks, they are not immune to adversarial attacks and remain vulnerable to exploitation. By establishing a rigorous experimental framework and validating defense mechanisms on real quantum hardware, the authors provide a vital benchmark for future researchers, offering a more realistic picture of the security landscape for deploying quantum machine learning in critical environments.

***

## Key Findings

*   **Superior Robustness:** Quantum Neural Networks (QNNs) exhibit greater adversarial robustness compared to classical neural networks.
*   **Role of Noise:** This advantage is attributed to inherent quantum noise within the processor.
*   **Defense Mechanism:** Adversarial training significantly enhances QNN robustness by reducing sensitivity to targeted perturbations.
*   **Bound Validation:** Experimental results validate the tightness of fidelity-based robustness bounds, showing minimal deviation ($3 \times 10^{-3}$).
*   **Vulnerability Remains:** Despite the structural advantages of noise, QNNs remain vulnerable to adversarial attacks and are not immune.

***

## Methodology

The research was executed on a **20-qubit superconducting quantum processor**. The researchers implemented a comprehensive benchmarking framework that included:

1.  **Algorithm Implementation:** Development of an efficient adversarial attack algorithm specifically designed for QNNs.
2.  **Quantitative Characterization:** rigorous measurement of adversarial robustness and robustness bounds.
3.  **Comparative Analysis:** Comparing empirical attack outcomes against theoretical limits to assess the practical security of the model.

***

## Technical Details

### Architecture & Training
*   **Adversarial Training Formulation:** Implemented as a regularization problem using a first-order Taylor expansion of the loss function.
    *   **Regularization Term:** $R = \frac{1}{2} \nabla_x L \cdot \delta$ (used to minimize the input gradient).
*   **QNN Architecture:**
    *   Utilized a **3-qubit data re-uploading** architecture.
    *   **Structure:** 9 parameterized gates per layer.
    *   **Parameter Scaling:** $N_{QNN} = 2N_x \cdot N_r$ (where $N_x=81$).
*   **Classical Baseline (FNN):**
    *   **Structure:** 3-layer Fully Connected Neural Network.
    *   **Configuration:** 81 input neurons, variable hidden neurons ($N_h$), and 1 output neuron.
    *   **Activations:** ReLU and Sigmoid.
*   **Simulation Parameters:** Matched parameter counts (~160 to ~1600) between QNN and FNN models under ideal, noise-free conditions to ensure fair comparison.

***

## Results

### Robustness Comparison ($R_{adv}$ on EMNIST)
QNNs significantly outperformed FNNs in adversarial robustness, even in noiseless simulations.

| Model Scale | Parameter Count | QNN Score | FNN Score | Relative Advantage |
| :--- | :--- | :--- | :--- | :--- |
| **Scale 1** | 162 (QNN) vs 167 (FNN) | **0.105** | 0.075 | ~40% Higher |
| **Scale 2** | 324 (QNN) vs 333 (FNN) | **0.104** | 0.079 | ~25% Higher |

*   **Conclusion:** Results indicate QNNs maintain ~25-40% higher robustness due to structural benefits, even in ideal environments.
*   **Theoretical Validation:** Fidelity-based robustness bounds were validated with a deviation of $3 \times 10^{-3}$.

***

## Contributions

*   Established the **first systematic experimental robustness benchmark** for 20-qubit QNN classifiers on real quantum hardware.
*   Introduced an experimental framework for **assessing and improving quantum adversarial robustness**.
*   Paved the way for **secure Quantum Machine Learning (QML)** applications by validating defense mechanisms and establishing the relationship between quantum noise and security.

***

**Quality Score:** 8/10  
**References:** 0 citations