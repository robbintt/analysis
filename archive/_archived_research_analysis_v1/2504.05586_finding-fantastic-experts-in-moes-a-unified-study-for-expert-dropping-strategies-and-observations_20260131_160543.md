# Finding Fantastic Experts in MoEs: A Unified Study for Expert Dropping Strategies and Observations

*Ajay Jaiswal; Jianyu Wang; Yixiao Li; Pingzhi Li; Tianlong Chen; Zhangyang Wang; Chong Wang; Ruoming Pang; Xianzhi Du*

---

> ### ðŸ“Š Quick Facts
>
> *   **Quality Score**: 9/10
> *   **Citations**: 40
> *   **Redundancy Identified**: Up to 50% of experts can be removed.
> *   **Key Framework**: MC-Suite (MoE Experts Compression Suite)
> *   **Primary Metric**: Expert Usage Frequency (most robust)
> *   **Proposed Methodology**: MoE Lottery Subnetworks (Iterative Pruning)
> *   **Critical Vulnerability**: Instruction-following capabilities (IFEval)

---

## Executive Summary

> **Problem: Stability and Redundancy in Compressed SMoEs**
>
> Sparse Mixture of Experts (SMoE) models scale efficiently but suffer from high memory overhead and significant parameter redundancy. While expert dropping is a logical solution for compression, it introduces a severe stability issue: current methods disproportionately degrade instruction-following capabilities compared to general linguistic skills. This specific fragility poses a critical barrier to deployment, as compressed models may fail to adhere to user prompts even when general performance metrics (like MMLU) appear acceptable. The paper addresses this by investigating why these alignment capabilities collapse and establishing a methodology to prune experts without sacrificing the model's functional utility.

> **Innovation: Iterative Dropping and the MC-Suite**
>
> The authors introduce the **MoE Experts Compression Suite (MC-Suite)**, a unified benchmark for estimating expert importance, and propose the **MoE Lottery Subnetworks** methodology. Unlike static one-shot pruning, this approach employs an iterative dropping process that re-estimates expert importance at every step, allowing the model to adapt to the loss of parameters dynamically. The system evaluates experts using six criteriaâ€”ranging from weight-based metrics (L2-norm, stable rank) to inference-guided metrics (Expert Usage Frequency, expert-expert collaboration). Crucially, the innovation lies in integrating **Task-Agnostic Fine-Tuning (TAF)** within the iterative loop. This continuous correction mechanism stabilizes the remaining subnetwork as experts are removed, preventing the catastrophic forgetting of capabilities typical of single-step pruning methods.

> **Results: Quantifiable Redundancy and Recovery**
>
> Evaluations on large-scale SMoEs (e.g., Mixtral 8x7B) demonstrate that vanilla models contain substantial redundancy, allowing for the removal of **up to 50% of experts** while maintaining general NLU performance with negligible loss. However, the study quantifies the fragility of alignment: naive dropping strategies can cause instruction-following scores (IFEval) to plummet by **large absolute margins**, whereas iterative pruning significantly mitigates this drop. The results highlight that **Expert Usage Frequency** is the most robust metric for expert selection, consistently outperforming weight-based norms. Furthermore, the authors show that while instruction-following capabilities are uniquely sensitive to pruning, they are fully recoverable; applying **k-shot examples alongside Supervised Fine-Tuning (SFT)** after the iterative process restores IFEval scores to levels comparable to the original, uncompressed model.

> **Impact: A Standard for MoE Compression**
>
> This work establishes a new paradigm for efficient MoE deployment by validating the existence of sparse, high-performance subnetworks within dense expert architectures. By providing the field with the MC-Suite and a standardized recipe for iterative pruning, the authors offer practitioners a reliable path to drastically reduce memory footprints and computational costs without sacrificing model alignment. The specific protocol for recovering instruction-following capabilities solves a critical bottleneck in making compressed models viable for real-world applications, ensuring that efficiency gains do not come at the cost of usability and safety.

---

## Key Findings

*   **Instruction-Following Fragility**: SMoE models suffer disproportionate damage to instruction-following capabilities during expert dropping compared to other NLU capabilities.
*   **Recovery Protocols**: Degraded instruction-following capabilities can be restored to robust levels via k-shot examples and Supervised Fine-Tuning (SFT).
*   **Iterative vs. One-Shot**: One-shot expert pruning is significantly less effective than iterative approaches that re-estimate importance criteria at each step.
*   **Redundancy Crisis**: Vanilla SMoE models suffer from significant expert redundancy and memory overhead, necessitating expert-level sparsification.

---

## Methodology

The research utilized a comprehensive suite of tools and protocols to analyze and compress SMoE models:

*   **Benchmarking**: Utilized the **'MoE Experts Compression Suite' (MC-Suite)** to benchmark models and estimate expert importance accurately.
*   **Iterative Process**: Employed an iterative dropping process that re-estimates importance criteria at each step, rather than relying on static one-shot pruning.
*   **Task-Agnostic Fine-Tuning (TAF)**: Applied TAF during the iterative process to correct and minimize negative impacts on the model's subnetworks, preserving model integrity.
*   **Evaluation**: Systematically evaluated lost capabilities to determine the extent of degradation and tested specific recovery strategies.

---

## Contributions

*   **MC-Suite Introduction**: Introduced a unified benchmark and suite of recipes specifically designed for estimating expert importance in MoE architectures.
*   **MoE Lottery Subnetworks**: Proposed a novel methodology combining iterative expert dropping with task-agnostic fine-tuning to find high-performance subnetworks.
*   **Pruning Protocol Shift**: Established an iterative pruning protocol that shifts the paradigm from one-shot removal to dynamic re-estimation of importance.
*   **Fragility Identification**: Identified the specific fragility of instruction-following capabilities within MoE architectures and provided a validated protocol for their recovery.

---

## Technical Details

The paper outlines specific Expert Dropping strategies for Sparse Mixture of Experts (SMoE) models. The architecture involves ranking experts based on six distinct criteria divided into two categories:

### Weight-Based Criteria
*   **Expert Weight Similarity**: Utilizes cosine similarity to measure resemblance between experts.
*   **Router Weight Norm**: Uses the L2-norm to assess router weights.
*   **Expert Weight Stable Rank**: Uses singular value ratios to determine rank stability.
*   **Expert Weight Norm**: Direct measurement of weight magnitude.

### Inference-Guided Criteria
*   **Expert Usage Frequency**: Counts activations over a dataset to determine frequency of use.
*   **Expert-Expert Collaboration**: Analyzes co-activation patterns to identify collaborative pairs, facilitating the dropping of the less used expert in the pair.

---

## Results

*   **Capability Loss**: Key findings indicate that instruction-following capabilities suffer disproportionate damage during expert dropping compared to other capabilities.
*   **Recovery**: These capabilities are not permanently lost; they can be restored using k-shot examples and Supervised Fine-Tuning (SFT).
*   **Pruning Efficacy**: Iterative pruning proves more effective than one-shot pruning due to the continuous re-estimation of importance.
*   **Redundancy Evidence**: Vanilla SMoE models exhibit significant redundancy; **Expert Usage Frequency** metrics demonstrate robustness regardless of the calibration dataset used, making it a superior metric for selection.

---
*Research Analysis generated based on provided text.*