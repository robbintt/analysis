# FlexiQ: Adaptive Mixed-Precision Quantization for Latency/Accuracy Trade-Offs in Deep Neural Networks

*Jaemin Kim; Hongjun Um; Sungkyun Kim; Yongjun Park; Jiwon Seo*

---

### ➤ Quick Facts

| Metric | Detail | Value |
| :--- | :--- | :--- |
| **Accuracy Gain** | Improvement in 4-bit models (w/ finetuning) | **+6.6%** |
| **Efficiency** | Speedup at 50% low-bitwidth ratio | **40%** (of full 4-bit speedup) |
| **Accuracy Loss** | Drop at 50% low-bitwidth ratio | **-0.6%** |
| **Evaluation** | Vision models tested (CNN & Transformer) | **11 Models** |
| **Hardware** | Validated on Custom NPU and GPUs | **Yes** |
| **Quality Score** | Overall paper rating | **9/10** |

---

## Executive Summary

Deep neural networks deployed on resource-constrained environments require aggressive compression to meet latency and memory constraints, typically achieved through quantization. However, uniform quantization methods often suffer significant accuracy degradation at low bit-widths, while existing mixed-precision approaches are typically static and unable to adapt to fluctuating inference workloads. This paper addresses the critical challenge of optimizing the latency-accuracy trade-off by introducing a dynamic mechanism that scales accelerator resources in real-time without relying on static pre-configuration or exhaustive retraining.

FlexiQ proposes an adaptive mixed-precision quantization scheme that operates at the feature-channel level, targeting channels with small value ranges for low-bitwidth computation. The core technical innovation is a **"bit-lowering"** technique utilizing a strict subset constraint; this allows for direct bit-extraction for low-precision channels, effectively avoiding the computational overhead of re-quantization. To determine the optimal precision assignment, the authors formulate channel selection as a combinatorial optimization problem solved via an evolutionary algorithm. This algorithm minimizes reconstruction error while accounting for inter-layer error awareness and hardware-oriented grouping, enabling dynamic hardware-software co-optimization.

Evaluated across eleven diverse vision models—including both convolutional and transformer architectures—FlexiQ demonstrates superior performance over four state-of-the-art quantization techniques. For 4-bit models with fine-tuning, the method achieved an average accuracy improvement of **6.6%** compared to baseline methods. Furthermore, the system exhibited an exceptional accuracy-latency balance: in a configuration where 50% of channels utilized 4-bit precision, FlexiQ incurred only a **0.6%** accuracy loss relative to full precision while achieving **40%** of the speedup observed in a fully 4-bit model (compared to an 8-bit baseline). Latency tests on custom NPUs and GPUs confirmed that these gains are achieved with minimal runtime overhead.

---

## Key Findings

*   **Superior Accuracy:** FlexiQ achieved an average of **6.6% higher accuracy** for 4-bit models (with finetuning) compared to baseline methods, outperforming four state-of-the-art quantization techniques.
*   **Optimal Latency-Accuracy Balance:** A model utilizing 50% 4-bit channels incurred only a **0.6% accuracy loss** while achieving **40% of the speedup** observed in a fully 4-bit model relative to an 8-bit model.
*   **Hardware Efficiency:** Latency evaluations on both a custom NPU and GPUs confirmed that FlexiQ introduces **minimal runtime overhead**, validating its suitability for real-world hardware accelerators.
*   **Broad Applicability:** The proposed solution was successfully evaluated across **eleven different vision models** based on both convolution and transformer architectures.

---

## Methodology

FlexiQ introduces an adaptive mixed-precision quantization scheme designed specifically for computer vision models.

*   **Selective Quantization:** The scheme selectively applies low-bitwidth computation only to feature channels with small value ranges.
*   **Bit-Lowering Technique:** An efficient technique aimed at minimizing quantization errors by utilizing a strict subset constraint, allowing for direct bit-extraction.
*   **Dynamic Adjustment:** FlexiQ dynamically adjusts the ratio of low-bitwidth channels in real time to respond effectively to fluctuating inference workloads without static pre-configuration.
*   **Prototype Implementation:** A prototype was developed featuring a mixed-precision inference runtime deployed on custom NPU hardware and GPUs to validate practical deployment.

---

## Technical Details

The technical implementation focuses on fine-grained control and optimization at the channel level:

*   **Granularity:** Mixed-precision quantization is applied at the **feature-channel level**.
*   **Targeting:** Channels with small value ranges are targeted for low-bitwidth computation.
*   **Optimization Problem:** Channel selection is formulated as a **combinatorial optimization problem** to minimize reconstruction error.
*   **Algorithm:** The optimization is solved via an **evolutionary algorithm** that considers:
    *   Inter-layer error awareness.
    *   Hardware-oriented grouping.
*   **Constraint Utilization:** Uses a strict subset constraint that allows direct bit-extraction without the need for re-quantization.

---

## Contributions

*   **Resource Scaling:** Addressed the challenge of scaling costly hardware accelerator resources by introducing a quantization scheme that adapts to real-time workload fluctuations.
*   **Data-Driven Precision:** Developed a methodology linking quantization precision to data characteristics (feature channel value ranges) to optimize computational efficiency without sacrificing accuracy.
*   **Co-Design Implementation:** Provided a comprehensive hardware-software co-design implementation that bridges algorithmic quantization with runtime management on actual hardware (NPU/GPU), proving that mixed-precision models can deliver tangible speedups with manageable accuracy degradation.

---

*References: 40 Citations* | *Quality Score: 9/10*