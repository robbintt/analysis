# Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering

*Maximiliano HormazÃ¡bal Lagos; HÃ©ctor Cerezo-Costas; Dimosthenis Karatzas*

---

> ### ðŸ“Š Quick Facts: Key Metrics
>
> *   **Dataset:** DocVQA
> *   **Base Model:** InstructBLIP
> *   **Pipeline Name:** EaGERS (Explanation-Guided Region Selection)
> *   **Type:** Model-agnostic, Training-free
> *   **Exact Match (EM):** Improved from **17.6%** to **22.9%**
> *   **ANLS:** Improved from **38.1%** to **41.7%**

---

## Executive Summary

This research addresses the opacity of "black box" Vision Language Models (VLMs) applied to Document Visual Question Answering (DocVQA). While VLMs excel at answering questions about document images, they often lack transparency regarding the specific visual evidence supporting their answers. This inability to trace outputs to spatial sub-regions poses significant challenges for trust and reproducibility in high-stakes environments, as models may hallucinate information or rely on irrelevant visual noise. The paper seeks to mitigate these issues by enforcing spatial grounding, ensuring that model outputs are derived strictly from evidence within the document image.

The core innovation is **EaGERS (Explanation-Guided Region Selection)**, a fully training-free and model-agnostic pipeline that operates in three stages:
1.  **Rationale Generation:** The VLM generates a natural language rationale explaining its potential answer.
2.  **Spatial Grounding:** The document is partitioned into a grid, using multimodal embedding models (such as BLIP, CLIP, or ALIGN) to compute similarities between the rationale text and image patches; a majority voting process then refines this to identify the most relevant regions.
3.  **Restricted Generation:** The pipeline masks the original image to obscure all information except the selected relevant sub-regions before re-querying the VLM for the final answer.

Evaluated on the DocVQA dataset, the EaGERS pipeline demonstrated statistically significant improvements over the unguided baseline. These results validate that forcing the model to focus on spatially grounded regions effectively mitigates the distraction of irrelevant visual context without requiring model fine-tuning. This approach lowers the barrier to entry for improving VLM reliability and sets a new standard for transparent, spatially aware document AI systems.

---

## Key Findings

*   **Performance Boost:** The proposed EaGERS pipeline outperformed the base model on the DocVQA dataset in both Exact Match (EM) accuracy and Average Normalized Levenshtein Similarity (ANLS) metrics.
*   **Zero-Footprint Optimization:** Significant performance improvements were achieved **without** requiring additional model fine-tuning.
*   **Enhanced Trust:** The method successfully enhances transparency and reproducibility in Document Visual Question Answering tasks.
*   **Effective Grounding:** Restricting response generation to specific grounded spatial sub-regions proves to be an effective strategy for model-agnostic improvement.

---

## Methodology: The EaGERS Pipeline

The researchers developed EaGERS, a fully training-free and model-agnostic pipeline. It is designed to ensure answers are derived strictly from grounded spatial regions within a document. The process operates in three distinct stages:

### 1. Rationale Generation
The pipeline begins by utilizing the Vision Language Model (VLM) to generate natural language rationales. These explanations describe the model's reasoning process before providing a final answer.

### 2. Spatial Grounding
This stage maps the generated rationales to specific spatial sub-regions of the document:
*   The document image is partitioned into a configurable grid.
*   Multimodal embedding similarities are computed between the rationale text and the visual sub-regions.
*   A **majority voting process** refines the selection to identify the most relevant areas.

### 3. Restricted Response Generation
To ensure the final answer is based only on the identified evidence:
*   The original image is masked to isolate only the relevant regions selected during the grounding phase.
*   The VLM is re-queried to generate the final answer, with its visual field limited strictly to these specific areas.

---

## Technical Details

*   **System Architecture:** EaGERS is designed to be model-agnostic, functioning as a wrapper around existing VLMs.
*   **Embedding Models:** The system utilizes multimodal embeddings from popular architectures such as **BLIP**, **CLIP**, or **ALIGN** to align text rationales with visual patches.
*   **Grid Mechanism:** The document is divided into a grid-based structure to allow for granular comparison of text and image features.
*   **Region Selection Algorithm:** 
    *   Similarities are computed across the grid.
    *   **Majority Voting:** Used to aggregate signals and select the top regions, reducing noise and false positives.
*   **Masking Strategy:** The pipeline dynamically generates a mask for the input image, hiding irrelevant data (noise, other text blocks) to force the model's attention on the grounded evidence.

---

## Experimentation & Results

Experiments were conducted to validate the efficacy of the EaGERS pipeline against standard baselines.

*   **Dataset:** DocVQA
*   **Evaluation Metrics:** 
    *   Exact Match (EM)
    *   Average Normalized Levenshtein Similarity (ANLS)
*   **Performance:**
    *   **Best Configuration:** Utilized BLIP-based embeddings combined with majority voting for region selection.
    *   **Unrestricted Baseline (InstructBLIP):** 17.6% EM / 38.1% ANLS
    *   **EaGERS Pipeline:** 22.9% EM / 41.7% ANLS
*   **Comparison:** The method demonstrated enhanced transparency and reproducibility compared to both OCR-reliant models (e.g., LayoutLM) and OCR-free competitors (e.g., Donut).

---

## Core Contributions

*   **Framework Introduction:** Introduction of a spatially grounded explanation framework specifically designed for Vision Language Models applied to Document Visual Question Answering.
*   **Training-Free Optimization:** A novel pipeline that allows for performance optimization and interpretability without the computational cost of fine-tuning or architectural dependencies.
*   **Validation of Alignment:** A validation that multimodal embedding similarity combined with grid-based majority voting can effectively identify and utilize relevant image regions for improved answer accuracy.

---

**Quality Score:** 9/10  
**References:** 24 citations