# Financial Named Entity Recognition: How Far Can LLM Go?

*Yi-Te Lu; Yintong Huo*

---

## Executive Summary

The rapid expansion of Large Language Models (LLMs) presents a compelling opportunity to automate the extraction of structured data from unstructured financial texts. However, a critical gap exists in understanding whether generic, state-of-the-art LLMs can effectively replace traditional, fine-tuned transformer models in high-stakes financial environments where precision is non-negotiable. As organizations seek to leverage the flexibility of LLMs for Named Entity Recognition (NER)â€”extracting organizations, locations, and personsâ€”it is vital to empirically determine if these generic models can meet the rigorous accuracy standards of professional financial analytics without costly domain adaptation.

This study introduces a comprehensive evaluation framework that benchmarks five generic LLMs (**GPT-4o**, **GPT-4o-mini**, **LLaMA-3.1-8B**, **LLaMA-3.1-70B**, and **Gemini-1.5-flash**) against fine-tuned BERT and RoBERTa baselines. The authors advance the field by utilizing the **FiNER-ORD dataset**, which was specifically selected to correct the skewed class distributions of previous benchmarks (achieving a balanced entity ratio of 2.29 ORG : 1.17 LOC : 1 PER). The investigation systematically dissected three distinct prompting strategiesâ€”**Direct Prompting**, **In-Context Learning (ICL)**, and **Chain-of-Thought (CoT)**â€”and introduced a rigorous error analysis taxonomy that categorizes LLM limitations into five representative failure types specific to the financial context.

The experimental results provide a definitive quantitative answer to the study's central question, revealing that fine-tuned models significantly outperform generic LLMs. The fine-tuned **RoBERTa-large** model achieved the highest performance with an **F1 score of 89.7%**, setting a high bar for domain-specific extraction. Among the LLMs, **GPT-4o** demonstrated the strongest capability, securing an **F1 score of 82.5%** when utilizing Chain-of-Thought prompting. This was followed by GPT-4o-mini (79.7%), LLaMA-3.1-70B (76.1%), and LLaMA-3.1-8B (66.5%). The data confirmed that while complex prompting strategies like CoT consistently outperformed direct prompting, a persistent performance gap of approximately **7 percentage points** remains between the best proprietary LLMs and specialized fine-tuned baselines.

These findings have profound implications for the application of AI in financial technology. By quantifying the performance gap, the study clarifies that while generic LLMs offer substantial potential for intelligent analytics, they cannot yet supplant fine-tuned models for tasks demanding the highest levels of precision. The established taxonomy of failure types provides a crucial roadmap for future research, highlighting exactly where current LLMs struggle and guiding the development of more robust prompting strategies or domain adaptation techniques. For practitioners, this work serves as an essential reference, emphasizing that in the current landscape, fine-tuning remains a necessary step for mission-critical financial information extraction.

---

> ### ðŸ“Š Quick Facts
> **Quality Score:** 9/10
> **Best Overall Model:** RoBERTa-large (Fine-tuned)
> **Best LLM:** GPT-4o (with Chain-of-Thought)
> **Performance Gap:** ~7 percentage points (Top LLM vs. Fine-tuned)
> **Key Dataset:** FiNER-ORD (201 annotations)
> **References:** 6 Citations

---

## Key Findings

*   **Performance Variance:** State-of-the-art generic Large Language Models (LLMs) exhibit distinct strengths and limitations when applied to the financial domain, with performance varying significantly based on the prompting method used.
*   **Failure Analysis:** The study identifies **five representative failure types**, categorizing specific error patterns where LLMs struggle within the financial context.
*   **The Adaptation Necessity:** While generic LLMs hold potential for intelligent financial analytics, they face substantial challenges in handling domain-specific tasks effectively without further adaptation.
*   **Precision Gap:** There is a persistent discrepancy between the accuracy of generic models and fine-tuned baselines, indicating that generic models are not yet ready to fully replace specialized models in high-precision environments.

---

## Methodology

The authors conducted a **systematic evaluation** to assess the efficacy of generic LLMs on the financial Named Entity Recognition (NER) task. This approach involved testing state-of-the-art LLMs using various prompting methods to determine how well these generic models could extract structured information from unstructured financial documents compared to established needs in the domain.

The research investigated the performance of generic LLMs *without* domain-specific adaptation versus fine-tuned baselines, utilizing three distinct prompting strategies:
1.  **Direct Prompting**
2.  **In-Context Learning (ICL)**
3.  **Chain-of-Thought (CoT)**

---

## Contributions

*   **Comprehensive Benchmarking:** Provided a systematic evaluation of current state-of-the-art LLMs and prompting strategies specifically within the context of financial NER.
*   **Error Analysis:** Established a taxonomy of five representative failure types, offering a structured understanding of model limitations.
*   **Domain-Specific Insight:** Delivered critical insights into the gap between generic LLM capabilities and the requirements of domain-specific financial analytics, clarifying the potential and challenges of using these models for professional financial information extraction.

---

## Technical Details

### Models Evaluated
The study performed a comparative analysis between:
*   **Generic LLMs:** GPT-4o, GPT-4o-mini, LLaMA-3.1-70B, LLaMA-3.1-8B, and Gemini-1.5-flash.
*   **Fine-tuned Baselines:** BERT and RoBERTa models.

### Infrastructure & Configuration
*   **LLM Access:** Via APIs.
*   **Baseline Training:** Conducted on an Nvidia Tesla A100 GPU.
*   **Hyperparameters:**
    *   Batch Size: 16
    *   Learning Rate: 1e-05
    *   Epochs: 50
*   **Model Versions:**
    *   GPT-4o: 20240806
    *   GPT-4o-mini: 20240718
    *   LLaMA-3.1: 20240723

### Error Analysis Framework
The study implemented an error analysis framework that categorizes failures into five representative types to diagnose where generic models fall short.

---

## Experimental Results

### Dataset Selection
The study utilized the **FiNER-ORD dataset** (comprising 201 manually annotated financial news articles) rather than the CRA NER dataset. This decision was made to address skewed class distributions and to remove the ambiguous 'Miscellaneous' category.

**Entity Distribution Comparison:**
| Dataset | ORG Ratio | LOC Ratio | PER Ratio |
| :--- | :---: | :---: | :---: |
| **FiNER-ORD** (Used) | 2.29 | 1.17 | 1 |
| **CRA NER** | 0.31 | 0.22 | 1 |

### Performance Outcomes (F1 Scores)
The experiments highlighted a significant performance gap between fine-tuned models and generic LLMs, even when advanced prompting techniques were employed.

| Model Type | Model | Prompting Strategy | F1 Score |
| :--- | :--- | :--- | :---: |
| **Fine-tuned** | RoBERTa-large | N/A | **89.7%** |
| **LLM** | GPT-4o | Chain-of-Thought (CoT) | **82.5%** |
| **LLM** | GPT-4o-mini | Chain-of-Thought (CoT) | 79.7% |
| **LLM** | LLaMA-3.1-70B | Chain-of-Thought (CoT) | 76.1% |
| **LLM** | LLaMA-3.1-8B | Chain-of-Thought (CoT) | 66.5% |

*Note: Chain-of-Thought (CoT) prompting generally outperformed Direct Prompting across all tested LLMs.*