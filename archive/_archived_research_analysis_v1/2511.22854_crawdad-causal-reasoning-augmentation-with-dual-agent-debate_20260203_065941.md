---
title: 'CRAwDAD: Causal Reasoning Augmentation with Dual-Agent Debate'
arxiv_id: '2511.22854'
source_url: https://arxiv.org/abs/2511.22854
generated_at: '2026-02-03T06:59:41'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# CRAwDAD: Causal Reasoning Augmentation with Dual-Agent Debate

*Finn G. Vamosi; Nils D. Forkert*

***

### ðŸ“Š Quick Facts

| **Metric** | **Detail** |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Dataset** | CLadder (10,112 questions) |
| **Top Performer** | Qwen3 (89.41% accuracy) |
| **Core Innovation** | Judge-free Dual-Agent Debate |
| **Focus Area** | Pearl's Ladder of Causation (Rung 3) |

***

## Executive Summary

Large Language Models (LLMs) frequently struggle with systematic causal reasoning, particularly when navigating higher levels of Pearlâ€™s Ladder of Causation. While models often perform adequately on basic associations (Rung 1), they typically falter on interventions (Rung 2) and counterfactuals (Rung 3)â€”the latter requiring the hypothetical manipulation of past events to determine alternate outcomes. This limitation is significant because reliable causal inference is critical for high-stakes decision-making in scientific, medical, and policy domains.

The paper addresses this deficiency by investigating how to augment LLM performance without the prohibitive cost of retraining, specifically targeting the logical rigor required for counterfactual analysis. The authors introduce **CRAwDAD** (Causal Reasoning Augmentation with Dual-Agent Debate), a framework that operationalizes "internal dialogue" as an explicit, external debate between two agents.

Unlike traditional multi-agent systems that rely on a third-party "judge" model to arbitrate disputes, CRAwDAD utilizes a judge-free design to eliminate bias. The system employs two specialized reasoning language models (DeepSeek-R1 and Qwen3-32B) in a conditional debate paradigm: one agent generates a structured causal inference, and the second acts as a critic. If the agents disagree, they engage in an iterative persuasive dialogue, challenging each otherâ€™s logic until a consensus is reached.

This research provides empirical evidence that multi-agent debate is an effective inference-time augmentation strategy for complex logical tasks, specifically validating its utility for causal reasoning. By demonstrating that specialized "reasoning models" are more effective agents in this context than standard LLMs, the paper offers a blueprint for resource-efficient AI development.

***

## Key Findings

*   **Significant Accuracy Improvement:** The dual-agent debate framework substantially improved causal inference accuracy, boosting DeepSeek-R1's overall performance from **78.03% to 87.45%** and Qwen3's from **84.16% to 89.41%**.
*   **Enhanced Counterfactual Reasoning:** The method yielded particularly strong results on counterfactual questions (the third rung of Pearl's ladder), with DeepSeek-R1 improving from **67.94% to 80.04%** and Qwen3 from **71.53% to 80.35%**.
*   **Efficacy of Reasoning Models:** The study demonstrates that "reasoning language models" (specialized in inference) serve as highly effective agents for debate-based systems compared to standard LLMs.
*   **Strong Models Benefit from Diverse Perspectives:** Even high-performing models (like Qwen3) gain significant accuracy improvements when debating with other agents, suggesting that exposure to diverse or adversarial logic is beneficial regardless of the base model's strength.

***

## Methodology

The research methodology relies on a structured interaction between specialized agents to refine causal logic.

*   **Dual-Agent Debate Framework:** The researchers implemented a system where two reasoning models interact explicitly. One agent provides a structured causal inference, while the second acts as a critic, examining the logic for flaws.
*   **Iterative Deliberation:** When disagreements occur, the agents engage in a persuasive dialogue, challenging each other's logic and revising their conclusions until they converge on a mutually agreed-upon answer.
*   **Model Selection:** The framework specifically utilizes "reasoning language models" (specifically Qwen3 and DeepSeek-R1) rather than standard large language models to leverage their specific strengths in causal inference and adversarial capabilities.
*   **Evaluation Protocol:** Performance was measured using the **CLadder dataset**, a benchmark that connects natural language questions to formal causal graphs across all three levels of Pearlâ€™s ladder of causation (Association, Intervention, and Counterfactuals).

***

## Technical Details

The system architecture is designed to minimize bias and maximize logical consistency through specific prompting and conditional mechanisms.

*   **Paradigm:** Utilizes a Dual-Agent Debate paradigm (MAD).
*   **Core Models:** 
    *   Qwen3-32B
    *   DeepSeek-R1-Distill-Qwen-32B
*   **Architecture:** 
    *   **No 'Judge' Model:** Avoids bias by removing a third-party arbiter.
    *   **Auxiliary Model:** Uses Granite3.3-2B for structured output extraction (yes/no and confidence).
*   **Debate Mechanism:** Conditional debate initiation; agents debate only if they disagree, continuing until consensus is reached.
*   **Prompting Strategy:** Adapts 'CausalCoT,' requiring models to formalize causal graphs and identify query types. The first speaker receives anti-hallucination instructions to accept anti-commonsense alignments.
*   **Dataset:** CLadder (10,112 questions across the Ladder of Causation):
    *   Rung 1: Association
    *   Rung 2: Intervention
    *   Rung 3: Counterfactuals

***

## Results

The dual-agent debate framework significantly improved accuracy over single-agent baselines, most notably in the difficult domain of counterfactuals.

### Performance Comparison

| Model | Task | Baseline Accuracy | CRAwDAD Result |
| :--- | :--- | :--- | :--- |
| **DeepSeek-R1** | Overall | 78.03% | **87.45%** |
| **DeepSeek-R1** | Counterfactuals | 67.94% | **80.04%** |
| **Qwen3** | Overall | 84.16% | **89.41%** |
| **Qwen3** | Counterfactuals | 71.53% | **80.35%** |

### Analytical Metrics
*   **Persuasion Dynamics:** Tracked correct-to-incorrect and incorrect-to-correct transitions.
*   **Confidence Calibration:** Measured reliability of model confidence scores.
*   **Sentiment Analysis:** Utilized VADER to analyze debate tone.
*   **Efficiency Metrics:** Tracked response length and consensus rounds required.

***

## Contributions

*   **Explicit Internal Dialogue:** The paper operationalizes the theoretical concept of reasoning as an "internal dialogue" by making the debate between alternative hypotheses explicit within a multi-agent architecture.
*   **Validation of Multi-Agent Causal Inference:** It provides empirical evidence that multi-agent debate is not just useful for general tasks but specifically enhances complex causal reasoning, particularly in the difficult domain of counterfactuals.
*   **Resource Optimization:** The findings suggest that high-quality causal reasoning can be augmented without training new models from scratch, simply by structuring the inference process as a debate between existing reasoning models.