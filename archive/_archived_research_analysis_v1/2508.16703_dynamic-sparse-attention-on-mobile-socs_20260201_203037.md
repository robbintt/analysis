# Dynamic Sparse Attention on Mobile SoCs

*Wangsong Yin; Daliang Xu; Mengwei Xu; Gang Huang; Xuanzhe Liu*

---

> ### ðŸ“Š Quick Facts
> - **Quality Score:** 6/10
> - **References:** 40 Citations
> - **Bottleneck Identified:** Estimation stage consumes 66% of total runtime in naive sparse attention.
> - **Accuracy Impact:** Standard NPU inference (INT8) sees up to a 25.5 pp drop compared to Float32.
> - **Sparsity Insight:** Over 80% of attention scores are negligible.

---

## ðŸ“‹ Executive Summary

Mobile LLM deployment faces a critical efficiency issue where **INT8 quantization** of the attention operator on NPUs leads to severe accuracy degradation, forcing reliance on power-intensive general-purpose processors (CPUs/GPUs).

The authors propose **shadowAttn**, a dynamic sparse attention mechanism that uses a "pilot compute" phase and hardware optimizations like graph bucketing to identify critical tokens and mitigate quantization errors on the NPU.

Experimental results show that shadowAttn eliminates the runtime overhead of naive sparse estimation, achieving **performance parity with SOTA frameworks** and recovering the accuracy gap seen in standard NPU inference.

This system bridges the gap between sparse algorithms and mobile scheduling, enabling energy-efficient NPU-centric inference that supports larger models on resource-constrained devices without relying on heavy processors.

---

## ðŸ”‘ Key Findings

*   **The Fallback Problem:** In current on-device LLM frameworks, the attention operator frequently falls back to the CPU/GPU due to quantization sensitivity. This results in significant performance issues.
*   **Performance Parity:** The proposed **shadowAttn** system significantly reduces reliance on general-purpose processors while maintaining performance parity with State-of-the-Art frameworks.
*   **Pilot Compute Innovation:** The system successfully hides the computational overhead of estimating important tokens through a 'pilot compute' phase on the NPU.
*   **Efficiency via Sparsity:** By restricting attention calculations to a tiny, dynamic subset of tokens, the system achieves high accuracy without requiring heavy hardware resources.

---

## ðŸ›  Methodology

The researchers propose **shadowAttn**, a system-algorithm co-designed sparse attention module specifically tailored for Mobile System-on-Chips (SoCs).

*   **Dynamic Sparse Attention:** The approach minimizes CPU/GPU usage by dynamically selecting only the most relevant tokens for processing.
*   **NPU-based Pilot Compute:** It utilizes the NPU to perform a preliminary estimate of important tokens, effectively masking the overhead of this selection process.
*   **Hardware-Specific Optimizations:** Several low-level optimizations were implemented, including:
    *   NPU compute graph bucketing.
    *   A head-wise NPU-CPU/GPU pipeline.
    *   Per-head fine-grained sparsity ratio adjustments.

---

## âš™ï¸ Technical Details

**The Challenge**
Current mobile NPUs struggle with LLM attention mechanisms due to:
*   **Static Graph Constraints:** Requiring per-tensor quantization.
*   **Sensitivity:** Vulnerability to activation outliers.
*   **Consequence:** These issues frequently necessitate CPU/GPU fallback, defeating the purpose of the NPU.

**The Solution**
*   **Workflow:** shadowAttn leverages the high sparsity found in attention scores (typically >80%) to execute a two-step workflow:
    1.  **Estimation Stage:** Identify top-$k$ important tokens.
    2.  **Computation Stage:** Perform sparse attention calculations.
*   **Mitigation Strategy:** To remove the latency cost of the estimation stage, shadowAttn implements a 'pilot compute' phase directly on the NPU, aiming to enable fully NPU-centric inference.

---

## ðŸ“ˆ Results

The performance of shadowAttn was evaluated against standard mobile NPU (INT8) and CPU/GPU (Float32) implementations:

*   **Accuracy Degradation (Standard NPU):** Experiments showed an average accuracy drop of **18.0 percentage points** across various models.
    *   *Specific Example:* Qwen2-1.5B dropped **25.5 pp**.
*   **Sparsity Analysis:** Data indicates that **over 80%** of attention scores are negligible, validating the sparse approach.
*   **Latency Breakdown (Naive Sparse Attention on Qwen2-0.5B):**
    *   At 20% sparsity, the estimation stage consumes up to **66%** of the total runtime (0.32s estimation vs 0.48s total).
    *   This establishes the estimation stage as the primary performance bottleneck that shadowAttn resolves.

---

## âœ¨ Contributions

1.  **Resolves NPU Fallback:** Addresses the critical issue of NPU fallback in on-device AI by solving quantization sensitivity in attention operators.
2.  **Co-Design Approach:** Introduces a novel system-algorithm co-design approach that bridges sparse attention algorithms with system scheduling pipelines.
3.  **Feasibility Demonstration:** Demonstrates that high-performance LLM inference is feasible on mobile devices with limited CPU/GPU availability, reducing reliance on heavy general-purpose processors.