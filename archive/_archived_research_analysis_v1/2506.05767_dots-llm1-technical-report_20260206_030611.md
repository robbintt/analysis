---
title: dots.llm1 Technical Report
arxiv_id: '2506.05767'
source_url: https://arxiv.org/abs/2506.05767
generated_at: '2026-02-06T03:06:11'
quality_score: 8
citation_count: 29
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: fireworks_ai
    name: glm-4p7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# dots.llm1 Technical Report

*Bi Huo; Bin Tu; Cheng Qin; Da Zheng; Debing Zhang; Dongjie Zhang; En Li; Fu Guo; Jian Yao; Jie Lou; Junfeng Tian; Li Hu; Ran Zhu; Shengdong Chen; Shuo Liu; Su Guang; Te Wo; Weijun Zhang; Xiaoming Shi; Xinxin Peng; Xing Wu; Yawen Liu; Yuqiu Ji; Ze Wen; Zhenhai Liu; Zichao Li; Zilong Liao*

---

### ðŸ“Š Quick Facts

| Metric | Value |
| :--- | :--- |
| **Total Parameters** | 142B |
| **Active Parameters** | 14B (per token) |
| **Architecture** | Mixture of Experts (MoE) |
| **Training Data** | 11.2T Tokens (No Synthetic Data) |
| **Benchmark Target** | Qwen2.5-72B |
| **Quality Score** | 8/10 |

---

## Executive Summary

The research addresses the escalating computational costs and resource demands associated with training state-of-the-art dense Large Language Models (LLMs). As the parameter count of dense models increases to achieve higher performance, the financial and infrastructure requirements for both training and inference become prohibitively expensive, creating a barrier to accessibility. Additionally, current scaling laws often necessitate massive datasets, leading many practitioners to rely on synthetic data generationâ€”a practice that can introduce quality and stability issues. This paper investigates how to maintain high-performance capabilities while drastically reducing computational overhead and eliminating the reliance on synthetic data.

The authors introduce **`dots.llm1`**, a large-scale Mixture of Experts (MoE) architecture designed to decouple total parameter count from active computational load. The model comprises 142B total parameters but utilizes sparse activation, engaging only 14B parameters per token through a system of 128 routed experts and 2 shared experts with Top-6 routing. To ensure training stability without the typical overhead of auxiliary loss, the team implemented a load balancing strategy using dynamic bias terms and FP32 precision for routing. Furthermore, the methodology emphasizes data purity via a rigorous three-stage processing pipeline that curates 11.2 trillion high-quality tokens (balanced between Chinese and English) while strictly excluding synthetic data.

The **`dots.llm1`** model achieves performance comparable to the dense Qwen2.5-72B model, demonstrating that sparse architectures can match dense models significantly larger than their active parameter count. The custom infrastructure, featuring the Cybertron framework and optimized Grouped GEMM kernels, delivered measurable hardware efficiency gains; tests on H800 GPUs showed an average forward computation speed-up of **14.00%** (peaking at 24.68%) and a backward computation improvement of **6.68%** compared to Transformer Engine 2.1. Training dynamics were robust, with zero token drops reported, and the model maintains efficient inference capabilities capable of operating on a single node with 8 GPUs.

This work establishes a practical blueprint for efficient LLM scaling, validating that MoE architectures can replicate the capabilities of large dense models with a significantly lower computational footprint. By successfully training a high-performance model on purely natural data, the report challenges the industry consensus on the necessity of synthetic data for pretraining. Moreover, the decision to open-source intermediate training checkpoints at one-trillion-token intervals provides the research community with a unique resource for studying learning dynamics and model evolution, potentially accelerating future advancements in efficient AI architecture.

---

## Key Findings

*   **High Efficiency:** The model operates as a Mixture of Experts (MoE) with 142B total parameters but activates only 14B per token, matching the performance of state-of-the-art dense models.
*   **Competitive Performance:** It achieves results comparable to the much larger Qwen2.5-72B model.
*   **Data Purity:** The model was pretrained on 11.2T high-quality tokens without the use of synthetic data.
*   **Cost Reduction:** The MoE architecture significantly reduces training and inference costs while maintaining high capability.
*   **Learning Dynamics:** Training progression revealed stable learning curves, with checkpoints released at one-trillion-token intervals.

## Methodology

The researchers employed a large-scale Mixture of Experts (MoE) architecture consisting of 142B total parameters with sparse activation of 14B parameters per input token.

*   **Data Curation:** A rigorous data processing pipeline was developed to curate 11.2T high-quality tokens, strictly excluding synthetic data.
*   **Training Process:** The model underwent pretraining on this dataset followed by a post-training phase to align capabilities with state-of-the-art benchmarks.

## Technical Specifications

### Architecture
*   **Type:** Decoder-only Transformer with Mixture of Experts (MoE).
*   **Configuration:** 142B total parameters; sparse activation of 14B parameters per token.
*   **Experts:** 128 routed experts and 2 shared experts.
*   **Routing:** Top-6 routing strategy.
*   **Internal Structure:** Fine-grained two-layer FFNs with SwiGLU activation.
*   **Attention:** Multi-Head Attention with RMSNorm on Query/Key projections.

### Optimization & Data
*   **Precision:** FP32 precision for routing.
*   **Load Balancing:** Auxiliary-loss-free strategy using dynamic bias terms.
*   **Data Pipeline:** Three-stage processing handling 11.2 trillion tokens.
*   **Data Composition:** Balanced 1:1 Chinese/English; strictly excludes synthetic data.

### Infrastructure
*   **Framework:** Cybertron framework.
*   **Scheduling:** Interleaved 1F1B pipeline scheduling for lower memory consumption.
*   **Kernels:** Custom Grouped GEMM optimization kernels.

## Performance Results

*   **Model Accuracy:** The model achieves performance comparable to the dense Qwen2.5-72B while significantly reducing training and inference costs.
*   **Kernel Performance:** On H800 GPUs, the custom Grouped GEMM implementation achieved:
    *   **Forward Computation:** Average speed-up of **14.00%** (peak 24.68%).
    *   **Backward Computation:** Average speed-up of **6.68%** (peak 8.29%).
*   **Stability:** Training dynamics were stable with zero token drops.
*   **Inference Efficiency:** The model allows for efficient inference on a single node with 8 GPUs.

## Contributions

1.  **Open Source Resources:** The authors open-sourced intermediate training checkpoints at every one trillion tokens, providing a resource for studying learning dynamics.
2.  **Validating Efficient Scaling:** The work validates efficient scaling by demonstrating that MoE models can replicate the performance of large dense models with significantly lower computational costs.
3.  **Blueprint for Natural Data:** Establishes a blueprint for effective large-scale pretraining using a high-quality natural data pipeline, challenging the necessity of synthetic data.

---

**Document Quality Score:** 8/10 | **Citations:** 29 references