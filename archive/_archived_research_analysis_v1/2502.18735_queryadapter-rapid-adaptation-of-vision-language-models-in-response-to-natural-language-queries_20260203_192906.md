---
title: 'QueryAdapter: Rapid Adaptation of Vision-Language Models in Response to Natural
  Language Queries'
arxiv_id: '2502.18735'
source_url: https://arxiv.org/abs/2502.18735
generated_at: '2026-02-03T19:29:06'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# QueryAdapter: Rapid Adaptation of Vision-Language Models in Response to Natural Language Queries

*Nicolas Harvey Chapman; Feras Dayoub; Will Browne; Christopher Lehnert*

---

> ### **Quick Facts**
> *   **Adaptation Speed:** < 5 minutes
> *   **Primary Datasets:** ScanNet++, Ego4D
> *   **Performance Boost:** Recall@1 increased from 31.6% (baseline) to 54.1%
> *   **Base Architecture:** CLIP (Frozen)
> *   **Key Innovation:** Query-specific Adapters & Negative Caption Mining
> *   **Quality Score:** 8/10

---

## Executive Summary

### **Problem**
Deploying Vision-Language Models (VLMs) in dynamic robotic environments presents a critical challenge due to the significant domain shift between internet-scale pre-training data and real-world sensor inputs. Standard adaptation strategies typically rely on closed-set class definitions, rendering them impractical for robotics where agents must respond to diverse, open-set natural language queries without prior knowledge of specific target objects. Consequently, there is a need for a framework that can rapidly adapt a pre-trained model to specific user queries and local environments using only unlabeled deployment data, ensuring both flexibility and efficiency.

### **Innovation**
QueryAdapter introduces a query-specific adaptation framework that modifies a frozen pre-trained VLM (typically CLIP) in response to natural language prompts without fine-tuning the backbone. The core technical innovation involves inserting lightweight **Query Adapters**—bottleneck structures with down-projection, non-linearity, and up-projection—into the vision encoder's transformer layers to modulate visual features. To handle real-world noise, the authors propose **Negative Caption Mining**, which uses captions of other scene objects as negative labels to sharpen decision boundaries. Additionally, an active object selection mechanism is employed to prioritize training on the most relevant scene regions based on feature similarity, thereby maximizing computational efficiency and adaptation speed.

### **Results**
The framework achieves state-of-the-art performance on the **ScanNet++** dataset for 3D Object Retrieval. QueryAdapter improves Recall@1 to **54.1%**, a substantial increase over the 31.6% baseline of zero-shot CLIP, while also outperforming existing unsupervised VLM adapters and traditional 3D scene graph methods. Operationally, the method is highly efficient, enabling full model adaptation in less than 5 minutes. Furthermore, the framework demonstrates robust generalization on the **Ego4D** dataset for abstract affordance queries, with the negative caption mining strategy leading to superior calibration metrics by effectively suppressing confidence scores for unrelated distractor objects.

### **Impact**
The significance of QueryAdapter lies in its practical solution to the open-set limitation inherent in current robotic vision systems. By enabling minute-level, label-free adaptation to natural language queries, this work bridges the gap between static foundation models and the dynamic requirements of embodied AI. The ability to rapidly recalibrate models using unlabelled deployment data enhances the viability of VLMs for complex 3D scene understanding and egocentric video tasks, offering a robust pathway toward more responsive and intuitive human-robot interaction.

---

## Key Findings

*   **Enhanced Retrieval Performance:** Significantly outperforms state-of-the-art unsupervised VLM adapters and 3D scene graph methods on the ScanNet++ dataset.
*   **Rapid Adaptation:** Produces an adapted model in **minutes**, making it highly efficient for time-sensitive robotic applications.
*   **Robust Generalization:** Demonstrates strong performance across different tasks and datasets, including handling abstract affordance queries on the Ego4D dataset.
*   **Improved Calibration:** Achieves better confidence calibration by utilizing object captions as negative class labels to effectively handle unrelated objects.

---

## Methodology

The framework utilizes **query-specific adaptation** to modify a pre-trained Vision-Language Model (VLM) in response to natural language queries, eliminating the need for a pre-defined closed-set of classes.

*   **Data Leverage:** It uses unlabelled image data from previous robot deployments to bridge the domain shift between internet-trained VLMs and real-world robot data.
*   **Feature Alignment:** The methodology aligns VLM features with query-relevant semantic classes by optimizing learnable prompt tokens.
*   **Active Selection:** An active object selection mechanism is employed to choose specific objects for training efficiency.
*   **Negative Labeling:** A negative labeling strategy uses object captions to refine decision boundaries against unrelated objects.

---

## Contributions

*   **Solving Open-Set Limitations:** Addresses the impracticality of closed-set adaptation strategies for robotics by enabling robots to handle diverse natural language queries.
*   **Comprehensive Framework:** Introduces a framework combining prompt token optimization with active training selection to achieve fast, minute-level model adaptation without labeled data.
*   **Novel Noise Handling:** Proposes a new technique for handling real-world data noise and distractor objects by using captions as negative labels, resulting in robust confidence calibration.
*   **Extensive Validation:** Provides experimental validation on complex tasks like 3D scene understanding (ScanNet++) and egocentric video (Ego4D), proving efficacy in abstract affordance understanding.

---

## Technical Details

*   **Base Architecture:** Built upon a frozen pre-trained VLM (typically CLIP) without fine-tuning the backbone.
*   **Adapter Structure:** Utilizes lightweight Query Adapters inserted into the vision encoder's transformer layers. These are bottleneck structures consisting of:
    *   Down-projection
    *   Non-linearity
    *   Up-projection
*   **Adaptation Type:** Performs query-driven adaptation to modulate visual features based on a specific natural language query.
*   **Calibration Technique:** Employs **Negative Caption Mining**, treating object captions of other scene objects as negative labels to sharpen boundaries.
*   **Optimization:** Optimizes only adapter weights, allowing for rapid convergence.

---

## Results

*   **ScanNet++ (3D Object Retrieval):** Outperforms state-of-the-art unsupervised VLM adapters and traditional 3D scene graph methods.
*   **Ego4D (Generalization):** Demonstrates robust generalization for abstract affordance queries without extensive retraining.
*   **Speed:** Achieves adaptation speeds of minutes.
*   **Calibration:** Shows improved calibration metrics compared to baseline VLMs by reducing confidence scores for unrelated objects.

---
**References:** 40 citations