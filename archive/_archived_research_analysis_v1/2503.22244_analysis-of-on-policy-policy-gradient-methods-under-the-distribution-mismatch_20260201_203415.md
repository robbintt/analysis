# Analysis of On-policy Policy Gradient Methods under the Distribution Mismatch
*Weizhen Wang; Jianping He; Xiaoming Duan*

***

> ### **Quick Facts**
>
> *   **Quality Score:** 8/10
> *   **References:** 40 Citations
> *   **Primary Focus:** Distribution Mismatch & Policy Gradient Theory
> *   **Methodology:** Biased Stochastic Gradient Descent Framework
> *   **Key Insight:** Bridging the gap between theoretical assumptions and empirical robustness.

***

## Executive Summary

This research resolves a fundamental theoretical inconsistency in reinforcement learning known as "distribution mismatch." Standard on-policy policy gradient algorithms rely on the Policy Gradient Theorem, which implicitly assumes a stationary state visitation distribution; however, in practice, this distribution shifts continuously as parameters update. Theoretically, this shift should introduce bias and instability, yet these algorithms demonstrate robust empirical performance. The paper addresses this paradox by explaining why policy gradient methods remain stable and effective despite operating under non-stationary distribution assumptions that violate standard theoretical frameworks.

The authors' key innovation is the application of **Biased Stochastic Gradient Descent (SGD)** theory to model policy optimization under distribution mismatch. Rather than assuming stationarity, they explicitly quantify the error introduced by the discrepancy between the stationary state visitation distribution ($d_{\pi}$) and the discounted state distribution ($d_{\pi, \gamma}$). Their analysis adopts a two-stage structure: first establishing global optimality for simple tabular parameterizations, and then extending this to general, complex parameterizations to prove convergence to stationary points. To bridge these cases, the authors decompose the update error into distribution ratio and bias terms, mathematically characterizing how the optimization trajectory is affected by environmental constraints.

The study provides explicit mathematical bounds to quantify system behavior, moving beyond vague empirical observations. The authors prove that the ratio between the stationary and discounted distributions is bounded by the inequality $\left\| \frac{d_{\pi}}{d_{\pi, \gamma}} \right\|_\infty \le 1 + \frac{2(1-\gamma)D}{(1-\gamma\beta)c_{min, \gamma}}$, where $D$ represents the MDP diameter. Furthermore, they establish that value functions and gradient norms are strictly bounded by the maximum reward and discount factor, such that $|Q(s, a)| \le \frac{R_{max}}{1 - \gamma}$ and $\| \nabla_\theta J\| \le \sigma = \frac{G R_{max} |A|}{1 - \gamma}$.

Crucially, they derive specific convergence constants that depend on variance and step-size parameters—defined as bias coefficient $b = \frac{\gamma^{m-1}(1-\alpha)}{1 - \alpha \gamma^m}$ and bias limit $c = (1 - b)\frac{\sigma^2}{b^2}$—demonstrating that sufficient positive correlation exists between biased updates and the true gradient to ensure convergence.

This work significantly narrows the divide between theoretical reinforcement learning and real-world implementation by providing a rigorous explanation for the robustness of policy gradient methods. By distinguishing between global optimality in tabular settings and convergence to stationary points in general parameterizations, the authors clarify the theoretical limits of these algorithms. These findings validate the reliability of standard on-policy methods used in practice, offering the technical community confidence that empirical success is backed by a stable theoretical foundation, even when core assumptions of distribution stationarity are violated.

***

## Key Findings

*   **Distribution Mismatch Phenomenon:** State-of-the-art policy gradient algorithms diverge from the theoretical Policy Gradient Theorem because they operate under a 'distribution mismatch' (the difference between the theoretical stationary distribution and the actual discounted state visitation distribution).
*   **Tabular Global Optimality:** Despite the distribution mismatch, the analysis proves that policy gradient methods retain global optimality when utilizing tabular parameterizations.
*   **Generalized Convergence:** The research extends these findings to general (complex) parameterizations using **biased stochastic gradient descent theory**, proving convergence to stationary points.
*   **Inherent Robustness:** The results mathematically demonstrate the inherent robustness of policy gradient methods, explaining why they succeed empirically even when they violate theoretical stationarity assumptions.

***

## Methodology

The authors conduct a theoretical analysis structured in two distinct stages:

1.  **Tabular Analysis:** First, the authors examine simple tabular parameterizations to establish a baseline for global optimality.
2.  **Generalization:** Second, they generalize these findings to complex parameterizations.

To bridge the gap between these specific and general cases, the authors leverage the mathematical framework of **biased stochastic gradient descent**. This allows them to model the optimization errors introduced by the non-stationarity of the state distribution.

***

## Technical Details

The approach frames Policy Gradient optimization under distribution mismatch as a **Biased Aggregated Boundary Condition (Biased ABC)** problem to prove convergence with biased gradients.

### Core Framework
*   **Distribution Analysis:** Analyzes the discrepancy between the discounted state visitation distribution ($d_{\pi,\gamma}$) and the stationary distribution ($d_{\pi}$).
*   **Decomposition:** Employs Gradient Inner Product Decomposition to split error into:
    *   Distribution ratio terms.
    *   Bias terms.
*   **Bounding Constants:** Defines bounds ($b, c, B, C$) related to MDP properties ($\gamma$, $R_{max}$) and policy gradient norms.

### Mathematical Bounds
The following mathematical inequalities were derived to bound system behavior:

*   **Distribution Ratio Bounds:**
    $$ \left\| \frac{d_{\pi}}{d_{\pi, \gamma}} \right\|_\infty \le 1 + \frac{2(1-\gamma)D}{(1-\gamma\beta)c_{min, \gamma}} $$
    *(Where $D$ is the MDP diameter and $\beta$ is a minorization constant)*

*   **Value Function Bounds:**
    $$ |Q(s, a)| \le \frac{R_{max}}{1 - \gamma} $$

*   **Gradient Norm Bounds ($\sigma$):**
    $$ \| \nabla_\theta J\| \le \sigma = \frac{G R_{max} |A|}{1 - \gamma} $$

### Convergence Constants
The analysis derives constants for the Biased ABC problem to ensure convergence:

*   **Bias Coefficient ($b$):**
    $$ b = \frac{\gamma^{m-1}(1-\alpha)}{1 - \alpha \gamma^m} $$
*   **Bias Limit ($c$):**
    $$ c = (1 - b)\frac{\sigma^2}{b^2} $$

These constants demonstrate sufficient positive correlation for convergence, validating the stability of the methods under non-ideal conditions.

***

## Contributions

*   **Theoretical Explanation:** Provides a rigorous theoretical explanation for the discrepancy between standard theory (which assumes stationarity) and practice (where distribution mismatch occurs).
*   **Optimality Guarantees:** Establishes that global optimality is preserved in tabular settings despite the presence of distribution mismatch.
*   **Bridging the Gap:** Offers new insights into the robustness of policy gradient methods, helping to narrow the gap between theoretical assumptions and real-world implementations.