---
title: 'StableQAT: Stable Quantization-Aware Training at Ultra-Low Bitwidths'
arxiv_id: '2601.1932'
source_url: https://arxiv.org/abs/2601.19320
generated_at: '2026-02-03T20:29:37'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# StableQAT: Stable Quantization-Aware Training at Ultra-Low Bitwidths

*Tianyi Chen; Sihan Chen; Xiaoyi Qu; Dan Zhao; Ruomei Yan; Jongwoo Ko; Luming Liang; Pashmina Cameron*

***

### ðŸ“Š Quick Facts

| Metric | Value |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **References** | 40 Citations |
| **Target Regime** | Ultra-Low Bitwidths (2-4 bits) |
| **Key Achievement** | 2-bit Top-1 Acc: **70.94%** (vs 68.37% LSQ) |
| **Computational Overhead** | Negligible |

***

## Executive Summary

Quantization-Aware Training (QAT) is critical for deploying deep learning models on resource-constrained edge devices. However, training at ultra-low bitwidths (2-4 bits) remains notoriously unstable due to the non-differentiability of the quantization (rounding) operation. This creates a disconnect between the discrete forward pass and the continuous gradient updates required for backpropagation.

Existing solutions, primarily the Straight-Through Estimator (STE), suffer from gradient mismatch leading to optimization instability, while soft quantizers offer smooth gradients at the cost of prohibitive computational overhead.

**StableQAT** introduces a novel gradient surrogate derived from a discrete Fourier analysis of the rounding operator. This approach provides a mathematically rigorous solution that strictly generalizes the STE (treating it as a special case) while generating gradients that are smooth, bounded, and computationally inexpensive. Experimental evaluations on ImageNet with ResNet-50 demonstrate that StableQAT achieves superior performance (e.g., **70.94%** accuracy in the 2-bit regime) with virtually no increase in training time, offering a unified solution to the instability problems of ultra-low bitwidth QAT.

***

## Key Findings

*   **Ultra-Low Bitwidth Stability:** StableQAT enables stable, robust, and efficient QAT specifically in the challenging 2-4 bit regimes.
*   **Generalization of STE:** The proposed method strictly generalizes the Straight-Through Estimator (STE), treating it as a special case within a broader, more expressive family of gradient surrogates.
*   **Superior Performance:** Experimental results demonstrate higher accuracy and robustness compared to standard QAT techniques.
*   **Zero Overhead:** Significant gains in stability and performance are achieved with negligible computational overhead.
*   **Optimization Robustness:** The framework generates smooth, bounded, and inexpensive gradients, facilitating stable optimization across a wide variety of hyperparameter choices.

***

## Methodology

StableQAT addresses the limitations of existing gradient estimation methods through a novel theoretical approach:

*   **Discrete Fourier Analysis:** Instead of relying on standard heuristics or complex soft quantizers, StableQAT utilizes a lightweight surrogate for backpropagation derived from a discrete Fourier analysis of the rounding operator.
*   **Gradient Surrogate:** The method employs this theoretically grounded surrogate to estimate gradients, overcoming the gradient mismatch associated with STE.
*   **Efficiency:** Unlike soft quantizers that suffer from high computational costs, StableQAT generates gradients that are both smooth and bounded, allowing for efficient optimization without computational penalties.

***

## Technical Details

The technical framework of StableQAT is built upon a rigorous mathematical foundation designed to handle the discrete nature of quantization.

*   **Framework Generalization:** The framework strictly generalizes the Straight-Through Estimator (STE), treating standard STE as a specific instance within the broader StableQAT gradient family.
*   **Gradient Properties:**
    *   **Smooth:** Enables better convergence properties.
    *   **Bounded:** Prevents exploding gradients during training.
    *   **Inexpensive:** Low computational cost compared to soft quantizers.
*   **Target Regime:** Specifically optimized for ultra-low bitwidths (2-4 bits).
*   **Robustness:** Designed to maintain stable optimization that is robust across diverse hyperparameter choices.

***

## Results & Evaluation

StableQAT was evaluated on ImageNet using ResNet-50, demonstrating significant improvements in both accuracy and stability compared to standard baselines.

### Performance Metrics (ImageNet / ResNet-50)

| Bitwidth | StableQAT Accuracy | Baseline (LSQ) Accuracy | Improvement |
| :--- | :--- | :--- | :--- |
| **2-bit** | **70.94%** | 68.37% | **+2.57%** |
| **4-bit** | **76.03%** | 75.94% | +0.09% (Parity with FP) |

### Key Outcomes
*   **Convergence:** Successfully addresses convergence issues common in ultra-low bitwidth training.
*   **Efficiency:** Performance gains are realized without sacrificing training speed due to negligible computational overhead.
*   **Tolerance:** Exhibits significantly higher tolerance to varying learning rates and hyperparameters compared to traditional methods.

***

## Core Contributions

1.  **Unified Framework:** Addresses the critical challenge of optimization instability in ultra-low bitwidth QAT by providing a unified and efficient framework.
2.  **Theoretical Grounding:** Introduces a theoretically grounded gradient surrogate based on discrete Fourier analysis that overcomes the limitations of gradient mismatch and high overhead found in existing approaches.
3.  **Formal Generalization:** Establishes a formal generalization of the Straight-Through Estimator (STE), proving that STE is a specific instance of the StableQAT gradient family.
4.  **Practical Validation:** Validates the practical viability of the method by demonstrating that significant gains in stability and performance can be achieved with virtually no increase in training cost.