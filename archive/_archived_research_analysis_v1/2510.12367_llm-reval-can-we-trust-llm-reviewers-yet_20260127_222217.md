---
title: 'LLM-REVal: Can We Trust LLM Reviewers Yet?'
arxiv_id: '2510.12367'
source_url: https://arxiv.org/abs/2510.12367
generated_at: '2026-01-27T22:22:17'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# LLM-REVal: Can We Trust LLM Reviewers Yet?

*Heming Xia, Junfeng Liu, Zhifang Sui, Chen Gu, Nien Kung, Nanyun Peng, Rui Li, Can We, Xiangwen Kong, Reviewers Yet*

---

> ### ðŸ“Š Quick Facts & Metrics
>
> *   **Quality Score:** 8/10
> *   **Total Citations:** 40
> *   **Evaluation Scale:** 1â€“10 (Acceptance Threshold $\ge$ 6)
> *   **Simulation Duration:** Up to 6 iterative rounds
> *   **Context:** Benchmarked against ICLR 2025 & AAAI 2025 data (>23,000 submissions)

---

## Executive Summary

The exponential growth of academic publishing, exemplified by conferences like AAAI 2025 receiving over 23,000 submissions, has created an urgent capacity crisis, necessitating the integration of Large Language Models (LLMs) into the peer-review process. This paper addresses the critical question of whether LLMs can evaluate research fairly, investigating the systemic risks and trustworthiness of automated reviewers. Deep integration of AI into academic assessment poses a significant threat to integrity if these models introduce biases that diverge from human judgment, making it essential to quantify the reliability of LLM-based evaluation before widespread deployment.

The core innovation is LLM-REVal, a comprehensive simulation framework that models the full peer-review lifecycle through two distinct automated agents: a Research Agent and a Review Agent. The Research Agent utilizes a Retrieval-Augmented Generation (RAG) workflow, querying Semantic Scholar and Google Search APIs to generate literature, and employs Cosine Similarity for idea deduplication. It predicts experimental results rather than conducting physical experiments and uses a dedicated Citation Module to ensure reference integrity, formatting output via the ICLR LaTeX template. The Review Agent, built upon the AgentReview architecture, executes a 5-stage pipelineâ€”Reviewer Assessment I, Author-Reviewer Discussion, Reviewer Assessment II, Meta-Review Compilation, and Final Decisionâ€”using a unified LLM to simulate all roles (reviewers, Area Chair, and author). This iterative loop runs for up to six rounds, allowing for paper revision and re-submission based on feedback.

The evaluation, benchmarked against a dataset of thousands of ICLR 2025 reviews and validated by human annotations, reveals significant misalignment between LLM and human judgment. The study identifies two specific biases: "Linguistic Feature Bias," where LLM reviewers systematically assign higher scores to papers generated by LLMs, and "Aversion Toward Critical Statements," where they underrate human-authored papers containing discussions of risk or fairness. While the simulation found that LLM feedback generally improves paper quality, the mechanism is flawed; high-quality work is defined by the presence of nuanced, critical analysis, yet the scoring system inflates scores for LLM-generated style while penalizing the substantive critique found in superior human writing.

This study provides a critical risk assessment for the deep integration of AI into academic publishing, acknowledging the utility of LLMs in improving manuscript quality while warning against unchecked deployment due to significant fairness risks. By isolating linguistic and semantic biases as root causes of evaluation failure, the authors provide a roadmap for future alignment corrections. The release of the open-source LLM-REVal codebase establishes a reproducible standard for the community, enabling researchers to continuously benchmark and refine automated review systems against human standards as the technology evolves.

---

## Key Findings

The study uncovered several critical issues regarding the reliability of LLMs in peer review:

*   **Systematic Score Inflation:** LLM reviewers systematically assign higher scores to LLM-authored papers compared to human-authored papers.
*   **Aversion to Critical Statements:** LLM reviewers underrate human-authored papers that include critical statements, such as discussions of risk or fairness.
*   **Identified Biases:** The unfair evaluation patterns stem from two specific root causes:
    *   **Linguistic Feature Bias:** A preference for LLM-generated linguistic styles.
    *   **Aversion Toward Critical Statements:** A penalty applied to papers discussing limitations or risks.
*   **Quality vs. Fairness Trade-off:** While LLM reviews improve paper quality technically, they introduce significant fairness and equity risks.
*   **Judgment Misalignment:** There is a pronounced misalignment between LLM-based reviews and human judgments.

---

## Methodology

The study employs a robust, multi-faceted approach to evaluate LLM performance:

1.  **Simulation Framework:** Utilizes a simulation-based framework involving two primary automated agents:
    *   **Research Agent:** Responsible for generating and revising academic papers.
    *   **Review Agent:** Responsible for assessing submissions.
2.  **Human Validation:** Conducted human annotations to validate simulation results.
3.  **Direct Comparison:** Directly compared the quality and fairness of LLM-generated reviews against human judgments.

---

## Technical Details

The paper proposes a complex simulation framework with specific architectural components:

### The Simulation Loop
An iterative interaction loop runs for up to 6 rounds, revising rejected papers based on reviews.

### Research Agent Specifications
*   **Workflow:** Retrieval-Augmented Generation (RAG).
*   **Literature Search:** Utilizes Semantic Scholar and Google Search APIs.
*   **Deduplication:** Uses Cosine Similarity to ensure idea uniqueness.
*   **Experimentation:** Predicts experimental results rather than running physical experiments.
*   **Integrity:** Employs a 'Citation Module' for reference integrity.
*   **Formatting:** Outputs papers using the ICLR LATEX template via sequential iteration.

### Review Agent Specifications
*   **Architecture:** Built upon AgentReview.
*   **Pipeline (5 Stages):**
    1.  Reviewer Assessment I
    2.  Author-Reviewer Discussion
    3.  Reviewer Assessment II
    4.  Meta-Review Compilation
    5.  Final Decision
*   **Role Simulation:** A unified LLM supports all roles (reviewers, author, Area Chair).
*   **Panel:** Simulates a panel of 3 independent reviewers.

---

## Results

The evaluation yielded the following insights:

*   **Systemic Bias:**
    *   **Linguistic Feature Bias:** LLM reviewers rated LLM-authored papers higher due to stylistic preferences.
    *   **Aversion to Critical Statements:** Lower ratings were consistently given to human papers discussing risks or limitations.
*   **Benchmarking Context:** The study contextualizes findings against massive submission volumes, citing AAAI 2025 (>23,000 submissions) and a related study of 20,000 ICLR 2025 reviews.

---

## Contributions

*   **Risk Assessment:** Provides a critical risk assessment regarding the deep integration of LLMs into the academic peer-review cycle.
*   **Bias Isolation:** Identifies and isolates the root causes of evaluation bias, specifically linguistic style preference and aversion to critical language.
*   **Nuanced Perspective:** Offers a balanced view by acknowledging the utility of LLMs in improving paper quality while warning against unchecked deployment due to fairness issues.
*   **Open Source Tool:** Contributes the open-source LLM-REVal codebase to enable further research and reproducibility.

---

## Document Meta-Information

*   **Quality Score:** 8/10
*   **References:** 40 citations