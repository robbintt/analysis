# Representing Speech Through Autoregressive Prediction of Cochlear Tokens

*Greta Tuckute; Klemen Kotar; Evelina Fedorenko; Daniel L. K. Yamins*

***

> ### âš¡ Quick Facts
>
> *   **Model Name:** AuriStream
> *   **Architecture Type:** Bio-inspired Two-Stage Framework
> *   **Key Innovation:** Discrete "Cochlear Tokens"
> *   **Evaluation Datasets:** SUPERB Benchmark, TIMIT
> *   **Quality Score:** 8/10

***

## Executive Summary

This research addresses the fundamental challenge of learning rich, hierarchical representations of speech that are both computationally powerful and grounded in biological plausibility. While current state-of-the-art speech models often rely on opaque, end-to-end deep learning pipelines, they frequently lack the efficiency and interpretability found in human auditory processing. The authors argue that mimicking the specific mechanics of the human auditory system can yield models that better capture the nuances of phonetics and semantics, while unifying the capabilities of representation learning and generative audio synthesis under a single objective.

The key innovation is **AuriStream**, a novel architecture that introduces a two-stage framework designed to replicate the human auditory hierarchy. The first stage transforms raw audio into a time-frequency representation based on cochlea mechanics, extracting discrete representations termed "**cochlear tokens**." The second stage utilizes an autoregressive sequence model to process these tokens, predicting subsequent tokens in the sequence.

A significant technical contribution is the model's generative mechanism, which avoids traditional vocoders. Instead, it employs a per-sample optimization procedure analogous to the Griffin-Lim algorithm: it optimizes a random noise tensor using the Adam optimizer (learning rate $1e-2$) to minimize the L2 error between the predicted cochleagram and the projection of the optimized waveform, allowing for direct inversion back to the audio domain.

The **AuriStream-1B** model variant demonstrated strong empirical performance, achieving state-of-the-art results in lexical semantics and competitive scores across the diverse range of tasks in the SUPERB benchmark. In evaluations on the TIMIT test set, the model showed effective phoneme decoding, analyzed via confusion matrices. Generative tests revealed that the model produces audio continuations with short-term coherence, though it suffers from long-term drift, a limitation expected for a representation learner not specifically architected as a dedicated language model. These results validate the hypothesis that speech pattern learning and language production can be successfully operationalized under a unified autoregressive objective.

***

## Key Findings

*   **Rich Representation Learning:** AuriStream learns meaningful representations for phonemes and words, achieving state-of-the-art performance in lexical semantics.
*   **Competitive Downstream Performance:** The model demonstrates competitive results across a diverse range of speech tasks within the SUPERB benchmark.
*   **Generative Capabilities:** The model can generate audio continuations that are decodable back into the audio domain, allowing for visualization and interpretation of its predictions in spectrogram space.
*   **Biological Efficacy:** The model validates the effectiveness of a hierarchy inspired by the human auditory processing system for handling speech-based tasks efficiently.

## Methodology

The research utilizes a **biologically inspired, two-stage framework** designed to mimic the human auditory processing hierarchy:

1.  **Stage 1 (Feature Extraction):** Transforms raw audio into a time-frequency representation based on cochlea mechanics, extracting discrete 'cochlear tokens'.
2.  **Stage 2 (Sequence Modeling):** Applies an autoregressive sequence model over the extracted cochlear tokens to process sequential information.

## Technical Details

The approach uses a per-sample optimization procedure for sonification (inversion), analogous to the Griffin-Lim algorithm, to convert cochleagrams back to audio without a learned vocoder.

### Optimization Parameters

| Parameter | Specification |
| :--- | :--- |
| **Objective** | Minimize L2 error (Mean Squared Error) between optimized waveform's cochleagram and WavCoch model prediction |
| **Input Tensor Shape** | Fixed at $1 \times 80,000$ |
| **Initialization** | Random numbers from a normal distribution ($\mu=0, \sigma^2=1$) |
| **Optimizer** | Adam |
| **Learning Rate** | $1e-2$ |
| **Differentiability** | Achieved via backpropagation through the cochleagram transformation |

## Results

The **AuriStream-1B** model variant was evaluated on the TIMIT test set for phoneme decoding, with errors analyzed using a confusion matrix on a log colorscale.

*   **Coherence:** The model demonstrates **short-term coherence** but exhibits **long-term drift**.
*   **Interpretation:** The lack of long-term coherence indicates the model functions primarily as a representation learner rather than a dedicated language model.
*   **Validation:** The results support the hypothesis that speech pattern learning and language production can be operationalized under a unified objective.

## Contributions

*   **Novel Architecture:** Introduction of AuriStream, a new model architecture bridging biological inspiration with modern autoregressive modeling.
*   **Discrete Token Innovation:** The proposal and utilization of 'cochlear tokens' as a discrete intermediate representation for speech encoding.
*   **Advancing Human-like AI:** A framework that advances the development of efficient, human-like models capable of both strong representation learning and generative audio synthesis.