---
title: Attention Is All You Need
arxiv_id: '1706.03762'
source_url: https://arxiv.org/abs/1706.03762
generated_at: '2026-01-26T07:55:36'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Attention Is All You Need

*All You, Ashish Vaswani, Attention Is, Google Brain, Noam Shazeer, Google Research, Jakob Uszkoreit, Niki Parmar*

---

### âš¡ Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Total Citations** | 40 |
| **En-De BLEU** | 28.4 (New SOTA) |
| **En-Fr BLEU** | 41.8 (New SOTA) |
| **Training Time** | 3.5 days on 8 GPUs |
| **Core Architecture** | Encoder-Decoder (Attention Only) |

---

## Executive Summary

Prior to this work, sequence modeling and transduction tasks, such as machine translation, were dominated by recurrent neural networks (RNNs), Long Short-Term Memory networks (LSTMs), and convolutional neural networks (CNNs). These prevailing architectures suffered from inherent limitations: RNNs processed data sequentially, preventing the parallelization of training steps and resulting in significant computational inefficiencies. Furthermore, standard models struggled to capture long-range dependencies within sequences, as relating distant positions required a linearly increasing number of operations.

This research addressed the need for a more efficient architecture that could reduce training time while effectively modeling dependencies between distant sequence elements. The authors introduced the **Transformer**, a novel network architecture that relies entirely on attention mechanisms, specifically discarding recurrence and convolutions. The technical core is the **"Scaled Dot-Product Attention"** mechanism, which allows the model to weigh the significance of different parts of the input sequence regardless of their distance apart. This is implemented within a multi-head attention framework that enables the model to jointly attend to information from different representation subspaces.

The Transformer achieved state-of-the-art performance on major machine translation benchmarks (28.4 BLEU on WMT 2014 En-De and 41.8 BLEU on En-Fr). In addition to accuracy gains, the architecture offered massive efficiency improvements; the model was trained in only 3.5 days using eight GPUs. This paper represents a foundational paradigm shift in natural language processing (NLP), forming the backbone of major subsequent models such as BERT and GPT, by proving that complex sequence relationships can be learned with significantly reduced computational resources.

---

## Key Findings

*   **Performance:** The Transformer model outperformed state-of-the-art models on machine translation, achieving **28.4 BLEU** on WMT 2014 English-to-German and **41.8 BLEU** on English-to-French.
*   **Efficiency:** The architecture offers significant efficiency gains through high parallelizability, requiring only **3.5 days** of training on eight GPUs.
*   **Architecture Viability:** The experiments proved the viability of attention-only architectures that completely discard recurrence and convolutions.
*   **Generalization:** It generalizes effectively to other tasks, demonstrating success in English constituency parsing.

---

## Methodology

The researchers proposed the **Transformer**, a simple network architecture for sequence transduction that relies solely on attention mechanisms.

*   **Core Approach:** This approach eliminates the need for recurrent or convolutional neural networks.
*   **Framework:** It utilizes a standard encoder-decoder framework connected and operated entirely via attention.
*   **Dependency Handling:** By removing recurrence, the model allows for significantly more parallelization during training compared to sequential models.

---

## Contributions

*   **Architecture Introduction:** Introduced the Transformer, the first major sequence transduction architecture relying entirely on attention mechanisms.
*   **Benchmark Records:** Established new state-of-the-art benchmarks for WMT 2014 English-to-German and English-to-French translation.
*   **Resource Efficiency:** Demonstrated that complex sequence modeling can be accomplished with significantly reduced training time and computational resources.
*   **Broad Applicability:** Provided evidence of the architecture's applicability to domains other than translation, such as constituency parsing.

---

## Technical Details

The implementation relies on a specific configuration of stacked attention layers and feed-forward networks.

**Architecture Overview**
*   **Type:** Encoder-decoder relying entirely on self-attention mechanisms.
*   **Restrictions:** No recurrence and no convolutions used.
*   **Depth:** Both encoder and decoder consist of **6 layers**.

**Layer Composition**
*   **Encoder Layers:** Multi-head self-attention and position-wise feed-forward networks.
*   **Decoder Layers:** Masked multi-head self-attention, encoder attention, and position-wise feed-forward networks.
*   **Normalization:** Sub-layers use residual connections followed by Layer Normalization.

**Key Mechanisms**
*   **Scaled Dot-Product Attention:** The fundamental operation allowing the model to process sequence relationships.
*   **Multi-Head Attention:** Allows the model to attend to information from different representation subspaces simultaneously.

**Dimensions**
*   **Model Dimension ($d_{model}$):** 512
*   **Feed-Forward Dimension ($d_{ff}$):** 2048

---

## Results

*   **Translation Metrics:** Achieved state-of-the-art BLEU scores of **28.4** (WMT 2014 En-De) and **41.8** (WMT 2014 En-Fr).
*   **Training Speed:** Training required only 3.5 days on 8 GPUs, with a noted potential for only 12 hours on 8 P100s.
*   **Generalization:** The model demonstrated effective generalization on English Constituency Parsing.
*   **Complexity Reduction:** Compared to prior architectures, it reduced the complexity of relating distant positions to a constant number of operations, solving the long-range dependency bottleneck.

---
**References:** 40 citations