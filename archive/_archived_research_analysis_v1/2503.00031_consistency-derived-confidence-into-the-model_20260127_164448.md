---
title: Consistency-derived confidence into the model
arxiv_id: '2503.00031'
source_url: https://arxiv.org/abs/2503.00031
generated_at: '2026-01-27T16:44:48'
quality_score: 9
citation_count: 33
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 1.0
    max_tokens: 150000
---

# Consistency-derived confidence into the model

*Carnegie Mellon, Langlin Huang, Washington Univeristy, Jixuan Leng, Time Scaling, Jiaxin Huang, Jiacheng Liu, Large Language, Chengsong Huang, Efficient Test*

---

> ### ðŸ“Š Quick Facts
> | Metric | Value |
> | :--- | :--- |
> | **Quality Score** | 9/10 |
> | **Test Model** | Llama-3.1-8B-Instruct |
> | **Key Dataset** | MathQA |
> | **Accuracy Gain** | +2.6% (81.0 â†’ 83.6) |
> | **Max Sample Savings** | 94.2% |
> | **Core Innovation** | Self-Calibration & Soft Self-Consistency |

---

## Executive Summary

Current strategies for improving Large Language Model (LLM) accuracy, such as Best-of-N and Self-Consistency, rely on generating multiple candidate responses and aggregating results. While effective, these methods suffer from significant inefficiencies because they utilize a fixed sample budget for every query, wasting computational resources on easy problems that require few samples while potentially under-sampling complex ones. Furthermore, LLMs are inherently prone to overconfidence, making it difficult to reliably estimate the correctness of an answer without expensive test-time sampling.

This paper addresses the challenge of optimizing test-time computation by dynamically allocating resources based on query difficulty and mitigating model overconfidence to enable reliable, single-pass confidence estimation.

The core innovation is the introduction of **"Self-Calibration,"** a training framework designed to distill reliable confidence signals directly into the model's parameters. Technically, this is achieved via **"Soft Self-Consistency (SSC),"** a novel labeling method that moves beyond simple majority voting. In SSC, training labels are generated using the model's own confidence scores as weights, creating soft labels that reflect probability rather than binary correctness.

The proposed method was empirically validated using Llama-3.1-8B-Instruct on the MathQA dataset. The results demonstrated significant improvements in sample efficiency, with the method saving **39.8%, 50.4%, and 94.2%** of samples at various performance thresholds. Crucially, by applying Confidence-based Early Stopping with a sample budget of 16, the model improved its accuracy from **81.0% to 83.6%**. These findings confirm that the Self-Calibration framework effectively reduces the computational overhead of inference while maintaining or improving result quality.

---

## Key Findings

*   **Efficiency:** Model confidence improves test-time computation efficiency compared to fixed-sample methods like Best-of-N.
*   **Calibration:** Self-Calibration effectively mitigates LLM overconfidence to enable reliable estimation.
*   **Single-Pass Estimation:** Single-pass estimation is achieved by distilling Self-Consistency confidence into the model.
*   **Performance:** Confidence-based Early Stopping improved MathQA accuracy from **81.0 to 83.6** with a sample budget of 16.
*   **Resource Allocation:** Methods allow dynamic resource allocation based on query difficulty.

---

## Methodology

The paper proposes a **Self-Calibration framework** to optimize test-time computation. The approach involves:

1.  **Distillation:** Distilling confidence signals from Self-Consistency into model parameters to address overconfidence.
2.  **Estimation:** Producing calibrated estimates without the need for heavy test-time sampling.
3.  **Scaling:** Driving confidence-based scaling algorithms, specifically:
    *   **Early-Stopping for Best-of-N**
    *   **Adaptive Self-Consistency**

---

## Technical Details

The paper proposes **Self-Calibration**, a training framework designed to address LLM overconfidence.

### Core Algorithmic Component
*   **Soft Self-Consistency (SSC):** Generates training labels by using the model's own confidence scores as weights (sum of confidences for an answer divided by the total sum) rather than raw vote counts.

### Training Pipeline
The process consists of three distinct steps:

1.  **Sampling**
    *   Generating $N$ responses with confidence scores using a specific prompt.
2.  **Labeling**
    *   Calculating SSC scores for answers to create soft labels.
3.  **Optimization**
    *   Training with **CrossEntropy** and **SmoothL1** loss functions to align confidence with labels.
    *   Filtering high-confidence points to enhance generation.

### Comparisons
The method is benchmarked against:
*   Best-of-N
*   Standard Self-Consistency
*   Adaptive Self-Consistency

---

## Contributions

*   **Technique:** Introduction of the Self-Calibration technique to distill reliability confidence signals into LLMs.
*   **Design:** Design of efficient test-time scaling methods that dynamically allocate computational resources.
*   **Validation:** Comprehensive empirical validation across three LLMs and six datasets demonstrating superiority over fixed-budget methods.

---

## Results

Experiments were performed using **Llama-3.1-8B-Instruct** on the **MathQA** dataset, evaluating **Accuracy** and **Sample Usage**.

*   **Sample Efficiency:** The proposed method achieved significant sample efficiency compared to standard Self-Consistency, saving **39.8%, 50.4%, and 94.2%** of samples at various performance thresholds.
*   **Accuracy Improvement:** Using Confidence-based Early Stopping with a budget of 16 improved MathQA accuracy from **81.0% to 83.6%**.

---

**Quality Score:** 9/10  
**References:** 33 citations