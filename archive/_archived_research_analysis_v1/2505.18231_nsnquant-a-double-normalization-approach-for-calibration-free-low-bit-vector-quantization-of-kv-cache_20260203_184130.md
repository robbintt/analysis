---
title: 'NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector
  Quantization of KV Cache'
arxiv_id: '2505.18231'
source_url: https://arxiv.org/abs/2505.18231
generated_at: '2026-02-03T18:41:30'
quality_score: 9
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache

*Authors: Donghyun Son; Euntae Choi; Sungjoo Yoo*

---

### ðŸ“Š Quick Facts

| Metric | Detail |
| :--- | :--- |
| **Quality Score** | 9/10 |
| **Compression Levels** | 1-bit & 2-bit |
| **Throughput Gain** | Up to **3x** vs. Full-Precision |
| **Calibration Data** | **Eliminated** (Calibration-Free) |
| **Setup Time** | < 5 minutes |
| **Citations** | 40 |

---

## Executive Summary

Efficient inference of Large Language Models (LLMs) relies heavily on compressing the Key-Value (KV) cache, yet current Vector Quantization (VQ) methods face a critical operational bottleneck: they rely on specific calibration datasets to train codebooks. This dependency introduces significant vulnerability to distribution shift, where discrepancies between the calibration data and actual inference inputs lead to performance degradation. Consequently, existing VQ techniques struggle to maintain robustness and accuracy, particularly in extreme low-bit compression scenarios (1-bit or 2-bit), limiting the feasibility of deploying high-performance models on resource-constrained hardware.

This paper introduces **NSNQuant**, a novel calibration-free VQ framework designed to overcome distribution shift through a unique "double normalization" strategy. The core technical innovation involves a three-step preprocessing pipelineâ€”token-wise normalization, channel-wise centering (shifting), and a second token-wise normalizationâ€”followed by a Randomized Hadamard Transform (RHT). This sequence effectively transforms the token distributions of Keys (before Rotary Positional Embedding) and Values to align with a standard normal distribution.

By standardizing the data geometry, NSNQuant eliminates the need for dataset-specific calibration, allowing the system to utilize a single, global codebook trained on synthetic standard normal data for 8-dimensional sub-vectors. NSNQuant demonstrates superior performance in low-bit settings, achieving state-of-the-art results while delivering substantial hardware efficiency gains. In evaluations on LLaMA3-8B, the method significantly improved accuracy by reducing the Coupled Quantization Perplexity (PPL) on the C4 dataset from 13.97 to 9.15. The approach also achieved precise distribution alignment, evidenced by low KL divergences of 0.0197 for Keys and 0.0252 for Values.

The significance of NSNQuant lies in its status as the first calibration-free VQ technique specifically designed for the low-bit compression of KV caches in LLMs. By decoupling the quantization process from calibration data, it provides a robust, distribution-agnostic solution that resolves a fundamental limitation of prior methods.

---

## Key Findings

*   **Elimination of Calibration Data:** NSNQuant successfully removes dependency on calibration datasets, enabling robust, calibration-free quantization.
*   **Superior Performance in Low-Bit Settings:** The method consistently outperforms prior state-of-the-art methods in both 1-bit and 2-bit compression settings.
*   **Significant Throughput Gains:** Achieves up to a **3x increase in throughput** compared to full-precision baselines.
*   **Effective Distribution Alignment:** By transforming token distributions to align with a standard normal distribution, the approach enables the use of a single, reusable codebook.

---

## Methodology

NSNQuant utilizes a double normalization pipeline combined with a Hadamard transform to preprocess KV cache data. The methodology is designed to standardize input data without relying on dataset-specific statistics.

The process consists of a three-step transformation sequence:
1.  **Token-wise Normalization:** Initial normalization of the token vectors.
2.  **Channel-wise Centering (Shift):** Shifting the data to center it channel-wise.
3.  **Second Token-wise Normalization:** A subsequent normalization pass to stabilize the distribution.

Following these steps, a **Hadamard transform** is applied. This process aligns the token distribution with a standard normal distribution, which allows the system to perform robust vector quantization using a single codebook without requiring calibration data.

---

## Contributions

*   **Novel Quantization Technique:** Introduction of NSNQuant, the first calibration-free Vector Quantization technique designed specifically for the low-bit compression of KV caches in LLMs.
*   **Solution to Distribution Shift:** Addressing the critical limitation of existing VQ methodsâ€”vulnerability to distribution shift caused by reliance on calibration datasetsâ€”by proposing a distribution-agnostic approach.
*   **Architectural Innovation:** Development of a unique "double normalization" (Normalize-Shift-Normalize) and Hadamard transform strategy that standardizes token distributions to facilitate universal codebook usage.
*   **Operational Efficiency:** Demonstrating that high compression rates (1-bit and 2-bit) can be achieved without sacrificing accuracy or generalization, while simultaneously delivering substantial hardware efficiency gains (3x throughput).

---

## Technical Details

**Preprocessing Pipeline**
*   **Normalize-Shift-Normalize (NSN):** Aligns Key and Value distributions to a standard normal distribution, eliminating the need for calibration data.
*   **Steps:**
    *   Token-wise normalization to $\sqrt{d}$.
    *   Channel-wise centering (shifting).
    *   Second token-wise normalization.

**Transformation & Quantization**
*   **Randomized Hadamard Transform (RHT):** Utilized for final distribution alignment.
*   **Codebook Strategy:** Employs a single global codebook for 8-dimensional sub-vectors trained on synthetic standard normal data.
*   **Application Context:** Applied to Keys before Rotary Positional Embedding (RoPE).

**Optimization**
*   **Adaptive Scaling:** Includes a post-quantization adaptive scaling strategy to minimize error.

---

## Results

*   **Calibration-Free Robustness:** Comparable to preserved baselines; Coupled Quantization (CQ) PPL on LLaMA3-8B dropped from **13.97 to 9.15** on the C4 dataset.
*   **Distribution Alignment:** Achieved superior alignment with KL divergences of **0.0197** for Keys and **0.0252** for Values.
*   **Hardware Efficiency:** Delivered up to a **3x increase in throughput** compared to full-precision baselines.
*   **Speed:** Codebook fine-tuning takes **less than 5 minutes**.
*   **Benchmark Performance:** Outperforms state-of-the-art methods in 1-bit and 2-bit compression settings.

---
*References: 40 citations*