# Retrieval-Augmented Visual Question Answering via Built-in Autoregressive Search Engines

*Xinwei Long; Zhiyuan Ma; Ermo Hua; Kaiyan Zhang; Biqing Qi; Bowen Zhou*

---

> ### ⚡ Quick Facts & Metrics
>
> | Metric | Value |
> | :--- | :--- |
> | **Performance Gain** | +2.9% to +9.6% over strong baselines |
> | **Parameters Fine-Tuned** | 0.49% (via LoRA) |
> | **Training Time** | ~3 hours (4 GPUs) |
> | **Training Data** | 9K - 17K samples |
> | **Max Latency (Top-5)** | 962.1ms (Wiki21M) |
> | **Quality Score** | **9/10** |

---

## Executive Summary

This research addresses critical limitations in current Retrieval-Augmented Generation (RAG) frameworks for knowledge-intensive Visual Question Answering (VQA). Traditional methods often suffer from disjointed pipeline architectures where retrieval and generation modules fail to align effectively, leading to performance degradation as the knowledge corpus scales.

The authors propose **ReAuSE**, an integrated framework that functions as a built-in autoregressive search engine within a Multi-modal Large Language Model (MLLM). Unlike conventional approaches relying on vector similarity metrics, ReAuSE employs a "generative retriever" that autoregressively generates document identifiers token by token. This is complemented by a Reinforced Retrieval Calibration module that utilizes relevance feedback to align the retrieval process with answer generation.

Optimized for efficiency using Low-Rank Adaptation (LoRA), ReAuSE achieves state-of-the-art performance on standard datasets (OKVQA and A-OKVQA). It demonstrates robust generalization with only a 4.6% performance drop when scaling the retrieval corpus from GS112K to Wiki21M. This work represents a significant paradigm shift in visual reasoning, validating "generative retrieval" within multi-modal contexts and establishing a parameter-efficient pathway for training advanced models.

---

## Key Findings

*   **Significant Performance Gains:** The proposed `ReAuSE` model achieved improvements ranging from **2.9% to 9.6%** across all evaluation metrics compared to strong baselines, with specific gains reaching up to 20 points.
*   **Effectiveness of Unified Architecture:** Integrating the knowledge retriever directly into the generative multi-modal LLM proved significantly more effective than current methods relying on separate, disjointed modules.
*   **Superior Retrieval Mechanism:** The model successfully validates a 'generative retriever' approach, retrieving documents by producing document identifiers rather than computing similarity scores.
*   **Validation via Relevance Feedback:** The implementation of a reinforced retrieval calibration module—based on relevance feedback—successfully enhances retrieval performance and tightly aligns the process with answer generation.

---

## Methodology

The ReAuSE framework abandons the traditional pipeline of separate retrieval and generation modules in favor of a seamless integration of the knowledge retriever into the generative Multi-modal Large Language Model.

1.  **Unified Architecture:** The model functions as a built-in autoregressive search engine by producing document identifiers rather than computing similarity scores.
2.  **Dual Functionality:** It performs two main tasks:
    *   Retrieving relevant documents based on visual questions.
    *   Generating answers based on the retrieved content.
3.  **Reinforced Retrieval Calibration:** A dedicated module aligns the retrieval process with the answer generation component using relevance feedback, ensuring the retrieved information is maximally useful for the final output.

---

## Contributions

*   **Novel Architecture:** Introduction of `ReAuSE`, which redefines retrieval-augmented visual question answering by embedding search engine capability directly into the generative model.
*   **Generative Retrieval for VQA:** Application of a generative retrieval mechanism (producing document identifiers) within a multi-modal context.
*   **Optimization Technique:** Development of a reinforced retrieval calibration module leveraging relevance feedback to fine-tune retrieval efficiency.
*   **Benchmark Advancement:** Establishment of new state-of-the-art performance levels on standard knowledge-intensive VQA datasets (OKVQA and A-OKVQA).

---

## Technical Details

*   **Architecture Integration:** `ReAuSE` integrates the knowledge retriever directly into the generative MLLM as a single unified architecture.
*   **Generative Retrieval:** Performs retrieval by autoregressively generating document identifiers (`DocIDs`) at the token level. This decouples performance from knowledge base size.
*   **Reinforced Retrieval Calibration (RRC):** Utilizes relevance feedback to align the retrieval mechanism with answer generation.
*   **Parameter Efficiency:** Utilizes Low-Rank Adaptation (LoRA), fine-tuning only **0.49%** of parameters.
*   **Data Efficiency:** Requires significantly less data (9K to 17K samples) compared to full-scale fine-tuning baselines.
*   **Inference Process:** Uses an $l$-step decoding process where $l=10$.

---

## Results

*   **State-of-the-Art Performance:** `ReAuSE` outperforms the `FLMR` baseline by 2.9% to 9.6% across metrics.
*   **Robust Generalization:** Exhibits only a **4.6%** performance drop when scaling the corpus from GS112K to Wiki21M.
*   **Latency Analysis:**
    *   **Wiki21M (Top-5):** 962.1ms.
    *   *Comparison:* Slower than the fastest baseline (`DPR` at 518.4ms) but comparable to `FLMR` on smaller corpora.
*   **Training Efficiency:** Training completes within **3 hours** using 4 GPUs.

---

**Quality Score:** 9/10  
**References:** 18 citations