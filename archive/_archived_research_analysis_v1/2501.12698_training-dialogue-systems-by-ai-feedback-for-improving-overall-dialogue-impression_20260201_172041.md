# Training Dialogue Systems by AI Feedback for Improving Overall Dialogue Impression

*Kai Yoshida; Masahiro Mizukami; Seiya Kawano; Canasai Kruengkrai; Hiroaki Sugiyama; Koichiro Yoshino*

---

> ### üìä Quick Facts
>
> **Metric** | **Result**
> |---|---|
> **Evaluation Dataset** | JDiA (Japanese Dialogue for Impression)
> **Core Innovation** | SFT-based Reward Models vs. Standard Prompting
> **Optimization Targets** | 12 distinct dialogue impression metrics
> **Overall Impression Win Rate** | **56.8%** (Human Eval)
> **Personality Win Rate** | **58.4%**
> **Empathy Win Rate** | **57.2%**
> **Naturalness Win Rate** | **57.6%**

---

## Executive Summary

**Problem**
Current Reinforcement Learning from AI Feedback (RLAIF) methodologies face a critical limitation in evaluating dialogue systems. Standard approaches typically rely on zero-shot or few-shot prompting of Large Language Models (LLMs) to act as judges. However, this method is insufficient for capturing the holistic quality of a conversation, often failing to assess complex, human-centric attributes such as consistency, personality, and empathy across an entire dialogue. Consequently, models tuned via these methods may generate individual responses that are technically correct but fail to maintain a coherent persona or leave a positive overall impression.

**Innovation**
To address this, the researchers propose a modified RLAIF framework that replaces prompt-based evaluation with Supervised Fine-Tuned (SFT) reward models. Technically, the authors trained specific reward models on 12 distinct metrics related to dialogue impression rather than relying on a general-purpose LLM prompt. These fine-tuned models generate precise feedback signals regarding the entire dialogue flow, which are then used to optimize the primary dialogue model. This approach shifts the optimization target from isolated response quality to the broader context of conversation management and user impression.

**Results**
The proposed method was evaluated on the **JDiA (Japanese Dialogue for Impression)** dataset, where it demonstrated superior performance over standard RLAIF baselines in both automatic and human evaluations. In human pairwise evaluations, the SFT-reward-model approach achieved a **56.8% win rate** for Overall Dialogue Impression, significantly outperforming the prompt-based baseline. It also secured substantial gains in specific impression metrics, recording win rates of **58.4%** for Personality and **57.2%** for Empathy. Furthermore, the model improved general fluency, achieving a **57.6% win rate** for Response Quality (Naturalness).

**Impact**
This research significantly advances the field of dialogue system alignment by empirically validating that AI feedback derived from fine-tuned reward models is more effective than feedback derived from simple prompting. By providing a scalable method to optimize for conversation-level attributes, the study offers a robust pathway for developing LLMs that exhibit stronger persona consistency and empathy, shifting the industry focus toward long-term, engaging interactions.

---

## Key Findings

*   **Improved Dialogue Impression:** Tuning dialogue models using signals from Supervised Fine-Tuned (SFT) reward models successfully improves the overall impression of the dialogue.
*   **Enhanced Attributes:** The method specifically enhances essential human-centric attributes, including **consistency**, **personality**, and **empathy**.
*   **Outperforms Standard RLAIF:** The proposed method outperforms standard RLAIF approaches that rely solely on zero-shot or few-shot prompting for evaluation.
*   **Validated Results:** Both automatic and human evaluations confirmed that the feedback mechanism improved not only the specific impression metrics but also the naturalness of individual dialogue responses.

---

## Methodology

The study utilizes a Reinforcement Learning from AI Feedback (RLAIF) framework to align Large Language Model (LLM)-based dialogue systems. Instead of relying on prompt-based evaluation, the researchers employed Supervised Fine-Tuning (SFT) to train specific reward models fine-tuned on **12 distinct metrics** related to the impression of the entire dialogue. The trained reward models generate signals that serve as feedback to tune the primary dialogue model, optimizing it for holistic dialogue quality rather than just individual response quality.

---

## Technical Details

The proposed method distinguishes itself from standard RLAIF through the following technical specifications:

*   **Core Mechanism:** Tuning dialogue models using feedback signals generated by Supervised Fine-Tuned (SFT) reward models.
*   **Differentiation:** Unlike standard RLAIF, which relies on zero-shot or few-shot prompting of an LLM for evaluation, this method uses a **dedicated reward model** that has undergone SFT.
*   **Signal Precision:** The SFT reward models are designed to generate more precise feedback signals by focusing on the entirety of the dialogue context rather than isolated responses.

---

## Contributions

The research addresses a key limitation in current RLAIF methods and contributes the following to the field:

1.  **Methodological Improvement:** Demonstrates that evaluating entire dialogues via simple prompting is insufficient, proposing SFT-based reward models as a more effective solution.
2.  **Optimization Focus:** Introduces a method for optimizing dialogue systems for overall conversation flow and impression (using 12 specific metrics), moving beyond the traditional focus on isolated response generation.
3.  **Empirical Evidence:** Provides strong empirical evidence through automatic and human evaluation that AI feedback derived from fine-tuned reward models can concretely enhance perceived dialogue quality, personality, and empathy.

---

## Results

The study utilized both automatic and human evaluations to assess performance:

*   **Comparison:** The proposed SFT-reward-model approach demonstrated superior performance over standard RLAIF baselines.
*   **Metrics Improved:** Significant improvements were recorded in:
    *   Overall Dialogue Impression
    *   Specific impression attributes (Consistency, Personality, Empathy)
    *   Response Quality (Naturalness)

---

**Quality Score:** ‚≠ê 9/10
**References:** 28 citations