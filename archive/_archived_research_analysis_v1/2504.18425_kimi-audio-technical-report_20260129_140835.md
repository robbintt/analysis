# Kimi-Audio Technical Report

*Authors: KimiTeam; Ding Ding; Zeqian Ju; Yichong Leng; Songxiang Liu; Tong Liu; Zeyu Shang; Kai Shen; Wei Song; Xu Tan; Heyi Tang; Zhengtao Wang; Chu Wei; Yifei Xin; Xinran Xu; Jianwei Yu; Yutao Zhang; Xinyu Zhou; Y. Charles; Jun Chen; Yanru Chen; Yulun Du; Weiran He; Zhenxing Hu; Guokun Lai; Qingcheng Li; Yangyang Liu; Weidong Sun; Jianzhou Wang; Yuzhi Wang; Yuefeng Wu; Yuxin Wu; Dongchao Yang; Hao Yang; Ying Yang; Zhilin Yang; Aoxiong Yin; Ruibin Yuan; Yutong Zhang; Zaida Zhou*

---

> ### ğŸ“‘ Executive Summary
>
> The field of audio AI currently suffers from significant fragmentation, where separate specialized models are typically required for distinct tasks such as automatic speech recognition (ASR), audio understanding, text-to-speech generation, and voice conversation. Unifying these capabilities into a single foundation model is a complex challenge, primarily due to the difficulty of processing continuous audio signals alongside discrete text while maintaining the low latency necessary for real-time streaming.
>
> The **Kimi-Audio Technical Report** addresses this gap by presenting a unified model capable of handling the full spectrum of audio modalitiesâ€”speech, sound, and musicâ€”without sacrificing performance or coherence across different domains. The core technical innovation is a novel hybrid LLM-based architecture engineered to bridge continuous audio features with discrete tokens.
>
> Kimi-Audio demonstrates state-of-the-art performance across rigorous benchmarks for speech recognition, audio understanding, audio question answering, and speech conversation. These results were achieved through a training regimen of unprecedented scale, utilizing over 13 million hours of raw audio data. The significance of this research extends beyond performance metrics, offering a comprehensive open-source solution that includes model codes, checkpoints, and evaluation toolkits to ensure reproducibility.

---

## âš¡ Quick Facts

| Metric | Details |
| :--- | :--- |
| **Training Data Scale** | Over 13 million hours of audio (Speech, Sound, Music) |
| **Tokenizer Rate** | 12.5 Hz (Hybrid discrete semantic + continuous acoustic) |
| **Streaming Latency** | 4-token look-ahead (~0.32s) |
| **Vocoder Input** | 50 Hz mel-spectrograms |
| **Enhancement** | BSRNN at 48kHz |
| **Status** | Open-sourced (Codes, Checkpoints, Toolkits) |
| **Quality Score** | 8/10 |

---

## ğŸ”‘ Key Findings

*   **State-of-the-art Performance:** Achieves leading results across multiple benchmarks, including speech recognition, audio understanding, audio question answering, and speech conversation.
*   **Unprecedented Scale:** Trained on over 13 million hours of raw audio data, covering diverse domains such as speech, sound effects, and music.
*   **Unified Proficiency:** Demonstrates strong capabilities in audio understanding, generation, and conversation within a single model.
*   **Commitment to Open Science:** Model codes, checkpoints, and evaluation toolkits have been open-sourced to ensure reproducibility and facilitate further research.

## ğŸ› ï¸ Methodology

The research approach employs a sophisticated architecture and training pipeline designed to handle the complexities of unified audio processing:

*   **Novel LLM Architecture:** A unique system that accepts continuous audio features as input and outputs discrete tokens.
*   **Tokenization Strategy:** Incorporates a 12.5Hz audio tokenizer and a chunk-wise streaming detokenizer based on flow matching.
*   **Data Construction:** Built a high-quality, diverse pre-training dataset exceeding 13 million hours, featuring a dedicated pipeline for post-training data.
*   **Two-Stage Training Strategy:**
    1.  **Continual Pre-training:** Conducted on audio and text, initialized from a pre-trained LLM.
    2.  **Fine-tuning:** Specialized training for specific audio tasks to enhance proficiency.

## âš™ï¸ Technical Details

### Core Architecture
The Kimi-Audio model utilizes a unified three-component structure:

1.  **Audio Tokenizer**
    *   Generates hybrid discrete semantic tokens using Whisper+VQ.
    *   Produces continuous acoustic vectors.
    *   Operating rate: **12.5Hz**.
2.  **Audio LLM**
    *   Transformer backbone with shared-specialized layers.
    *   Utilizes distinct text and audio heads for processing mixed inputs.
3.  **Streaming Audio Detokenizer**
    *   Chunk-wise autoregressive framework based on **flow matching**.
    *   Generates 50Hz mel-spectrograms synthesized via BigVGAN.
    *   Implements a **4-token look-ahead** mechanism for boundary smoothing.

### Data Processing Pipeline
To ensure data quality and coherence, the system employs a rigorous processing workflow:

*   **Enhancement:** BSRNN enhancement at 48kHz.
*   **Diarization:** PyAnnote diarization for speaker identification.
*   **Cluster Merging:** Speaker clusters merged based on cosine similarity > **0.6**.
*   **Chunk Reassignment:** Chunks split/reassigned based on cosine similarity < **0.5**.

## ğŸ† Contributions

This report makes several significant strides in the field of audio AI:

*   **Unified Model:** Introduced Kimi-Audio, an open-source audio foundation model that successfully integrates understanding, generation, and conversational abilities.
*   **Technical Innovation:** Developed a hybrid architecture bridging continuous features and discrete tokens, alongside flow matching techniques for streaming detokenization.
*   **Dataset Curation:** Created and curated a massive multi-modal audio dataset (+13M hours) and established a standardized pipeline for post-training data.
*   **Benchmarking & Release:** Established new performance benchmarks across audio domains, supported by the public release of training artifacts to aid the community.

## ğŸ“Š Results

*   **Training Volume:** Pre-training utilized over 13 million hours of raw audio data.
*   **Architectural Efficiency:**
    *   Tokenization rate maintained at 12.5Hz.
    *   Mel-spectrogram inputs for vocoder at 50Hz.
    *   Upsampling factor of 4x.
*   **Processing Parameters:**
    *   48kHz speech enhancement applied.
    *   Cosine similarity thresholds set at 0.6 (merging) and 0.5 (splitting).
    *   Look-ahead latency limited to 4 tokens (approx. 0.32s).
*   **Performance Claims:** The report validates state-of-the-art performance in speech recognition, audio understanding, audio question answering, and speech conversation tasks.

---

**References:** 40 Citations | **Quality Score:** 8/10