---
title: 'SGD-Based Knowledge Distillation with Bayesian Teachers: Theory and Guidelines'
arxiv_id: '2601.01484'
source_url: https://arxiv.org/abs/2601.01484
generated_at: '2026-02-03T19:19:12'
quality_score: 8
citation_count: 40
model_profiles_used:
- default
- fast
model_profiles:
  default:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
  fast:
    provider: cerebras
    name: zai-glm-4.7
    temperature: 0.9
    top_p: 0.95
    max_tokens: 150000
---

# SGD-Based Knowledge Distillation with Bayesian Teachers: Theory and Guidelines

*Itai Morad; Nir Shlezinger; Yonina C. Eldar*

---

### ðŸ“Š Quick Facts

| Metric | Value |
| :--- | :--- |
| **Max Accuracy Improvement** | +4.27% |
| **Convergence Noise Reduction** | Up to 30% |
| **Top Test Accuracy (True Bayes)** | ~94% |
| **Test Accuracy (One-Hot)** | ~91% |
| **Quality Score** | 8/10 |
| **References** | 40 Citations |

---

## Executive Summary

### Problem
Knowledge Distillation (KD) is a widely adopted technique for model compression where a "student" model learns from a "teacher" model. However, the theoretical underpinnings of why this process often outperforms standard training on ground truth labels remain underexplored. Specifically, the field lacks a rigorous understanding of how the Stochastic Gradient Descent (SGD) dynamics differ when learning from probabilistic teacher supervision versus one-hot hard labels. Furthermore, the impact of teacher uncertainty and noise on the student's convergence behavior and generalization capabilities is not well characterized. This paper addresses these gaps by establishing a theoretical framework for KD, moving beyond heuristic application to a mathematically grounded analysis of the optimization landscape.

### Innovation
The key innovation lies in framing Knowledge Distillation as a probabilistic supervision problem within an SGD framework, utilizing a Bayesian perspective to analyze convergence properties. The authors theoretically distinguish between two regimes: supervision using exact Bayes Class Probabilities (BCPs) and supervision using noisy approximations of these probabilities. Under assumptions of strong quasi-convexity and the Polyak-Åojasiewicz condition, the analysis mathematically demonstrates that training with exact BCPs reduces gradient variance and eliminates neighborhood terms in SGD convergence boundsâ€”advantages not present when using standard one-hot labels. This theoretical foundation motivates the use of Bayesian Deep Learning models as teachers, as they inherently model the uncertainty and posterior probabilities required to approximate the optimal BCP supervision.

### Results
The study validates its theoretical findings through empirical evaluation, demonstrating significant performance gains when using Bayesian teachers compared to deterministic ones. Students distilled from Bayesian teachers achieved accuracy improvements of up to **+4.27%** and exhibited significantly more stable convergence, with up to **30% less noise** during the training process compared to students distilled from deterministic teachers. The experiments established a clear generalization error hierarchy: models trained with True Bayes probabilities reached approximately **94%** test accuracy, outperforming those trained with Less Noisy Probabilities, which in turn surpassed More Noisy Probabilities and standard One-hot labels (91%). Additionally, the results indicate that the optimal distillation parameter ($\lambda$) is dependent on the noise level present in the teacher's probabilities.

### Impact
This work significantly influences the field by providing the first rigorous theoretical analysis linking SGD convergence to the soft labels used in Knowledge Distillation. By quantifying the benefits of variance reduction and the removal of neighborhood terms, the authors offer a mathematical explanation for the efficacy of KD. Consequently, the paper provides concrete, evidence-based guidelines for practitioners, advocating for the adoption of Bayesian deep learning models as teachers due to their superior estimation of BCPs. This shifts the paradigm of teacher selection from an arbitrary design choice to a theoretically grounded decision, ensuring improved student generalization and training stability.

---

## Key Findings

*   **Convergence Benefits of Bayes Class Probabilities (BCPs):** Learning from exact BCPs results in variance reduction and eliminates neighborhood terms in the Stochastic Gradient Descent (SGD) convergence bounds, offering a theoretical advantage over standard one-hot supervision.
*   **Impact of Teacher Noise:** The analysis characterizes how the noise level in teacher approximations of BCPs directly influences the student model's generalization capabilities and final accuracy.
*   **Superiority of Bayesian Teachers:** Students distilled from Bayesian deep learning teachers achieve higher accuracy (improvements of up to +4.27%) and exhibit significantly more stable convergence (up to 30% less noise) compared to those distilled from deterministic teachers.

---

## Methodology

The authors adopt a Bayesian perspective to provide a rigorous theoretical analysis of Knowledge Distillation (KD). They specifically analyze the convergence behavior of student models trained using Stochastic Gradient Descent (SGD) under two distinct supervisory regimes:

1.  **Exact BCP Regime:** Where the teacher provides exact Bayes Class Probabilities.
2.  **Noisy Approximation Regime:** Where the teacher provides noisy approximations of these probabilities.

This theoretical framework is used to motivate the experimental evaluation of Bayesian deep learning models as teachers.

---

## Contributions

*   **Theoretical Foundation for KD:** The work addresses the gap in theoretical understanding of KD by providing a rigorous analysis of SGD convergence within a distillation framework.
*   **Quantitative Comparison of Supervision Signals:** It mathematically demonstrates the specific benefits of soft probabilistic supervision (BCPs) over hard labels (one-hot), specifically identifying variance reduction and the removal of neighborhood terms.
*   **Guidelines for Teacher Selection:** The study provides evidence-based guidelines advocating for the use of Bayesian deep learning models as teachers in KD, validating that the improved estimation of BCPs leads to better student performance and stability.

---

## Technical Details

*   **Framework:** Frames KD as a probabilistic supervision problem within SGD, where the student minimizes a weighted combination of ground truth and teacher losses (linear interpolation of one-hot labels and teacher probabilities).
*   **Assumptions:** Relies on strong quasi-convexity, the Polyak-Åojasiewicz condition, and expected smoothness.
*   **Teacher Modeling:** Analyzes two regimes:
    *   *Perfect BCP:* Exact true posterior.
    *   *Noisy BCP:* True BCP plus zero-mean additive distortion.
*   **Key Theoretical Properties:**
    *   *Risk Equivalence:* Shared minimizer.
    *   *Interpolation Property:* Minimizer matches true BCPs at samples.
    *   *Gradient Vanishing:* Prevents overfitting.
    *   *Parameter & Risk Convergence:* Established bounds for both.

---

## Results

*   **Performance:** Students distilled from Bayesian deep learning teachers achieved accuracy improvements of up to **+4.27%** compared to deterministic teachers.
*   **Stability:** Exhibited up to **30% less noise** during convergence.
*   **Generalization Hierarchy:** True Bayes Probabilities > Less Noisy Probabilities > More Noisy Probabilities > One-hot Labels.
*   **Specific Accuracy:** Models trained with True Bayes probabilities reached approx. **94%** test accuracy vs. **91%** for One-hot labels.
*   **Parameter Sensitivity:** The optimal distillation parameter ($\lambda$) varies depending on the noise level of the probabilities.

---

**Quality Score:** 8/10  
**References:** 40 citations