---
ver: rpa2
title: Are LLM-based Evaluators Confusing NLG Quality Criteria?
arxiv_id: '2402.12055'
source_url: https://arxiv.org/abs/2402.12055
tags:
- evaluation
- text
- target
- sentence
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reveals that large language models (LLMs) suffer from
  significant confusion when evaluating different aspects of natural language generation
  (NLG) quality. The authors first construct a clear hierarchical classification system
  for 11 common evaluation aspects based on 300+ papers to avoid inconsistent conceptualization.
---

# Are LLM-based Evaluators Confusing NLG Quality Criteria?
## Quick Facts
- arXiv ID: 2402.12055
- Source URL: https://arxiv.org/abs/2402.12055
- Reference count: 40
- Primary result: LLMs exhibit significant confusion when evaluating different aspects of NLG quality, struggling to distinguish related criteria and showing oversensitivity to grammatical perturbations

## Executive Summary
This paper reveals that large language models suffer from significant confusion when evaluating different aspects of natural language generation quality. The authors construct a hierarchical classification system for 11 common evaluation aspects based on 300+ papers and design 18 aspect-targeted perturbation attacks. Through experiments with GPT-3.5, GPT-4, and Prometheus, they demonstrate that LLMs often fail to distinguish between related aspects (like fluency and readability) and exhibit oversensitivity to certain perturbations. The confusion issues persist across different LLM types and cannot be easily mitigated by explicit instructions.

## Method Summary
The authors first establish a hierarchical classification system for NLG evaluation aspects by analyzing over 300 research papers to create consistent definitions and relationships. They then design 18 aspect-targeted perturbation attacks that systematically modify text to target specific quality criteria. The experiments involve prompting three different LLM variants (GPT-3.5, GPT-4, and Prometheus) with both clean and perturbed text samples, asking them to evaluate multiple quality aspects simultaneously. The responses are analyzed to identify confusion patterns and oversensitivity behaviors across different models and evaluation criteria.

## Key Results
- LLMs exhibit significant confusion between related evaluation aspects, particularly between fluency and readability
- Models show oversensitivity to grammatical perturbations, with even minor grammatical errors substantially affecting overall quality ratings
- The confusion issues persist across different LLM variants (GPT-3.5, GPT-4, Prometheus) and cannot be resolved through explicit instructions

## Why This Works (Mechanism)
The mechanism behind LLM confusion in NLG evaluation stems from how these models learn from training data that likely contains inconsistent labeling and mixed-quality annotations. During pretraining, LLMs encounter texts with various quality issues that are often correlated, leading them to learn entangled representations of different quality aspects. When evaluating, the models struggle to disentangle these aspects because their internal representations don't cleanly separate the concepts. The hierarchical nature of language quality means that some aspects (like coherence) depend on others (like fluency), creating additional complexity in the evaluation process.

## Foundational Learning
**NLG Quality Aspects**: The 11 aspects include fluency, readability, coherence, consistency, relevance, informativeness, accuracy, diversity, logic, factuality, and structure - why needed because consistent definitions are essential for systematic evaluation; quick check: can you map each aspect to specific textual features?

**Perturbation Attacks**: Systematic modifications to text that target specific quality aspects while controlling for others - why needed because controlled experiments require isolating individual factors; quick check: can you explain how a grammatical perturbation differs from a coherence perturbation?

**Hierarchical Classification**: Organizing evaluation aspects into levels of abstraction and dependency - why needed because some quality aspects are inherently dependent on others; quick check: can you identify which aspects are higher-level versus more fundamental?

## Architecture Onboarding
**Component Map**: Text Sample -> Perturbation Generator -> LLM Evaluator -> Quality Aspect Scores -> Confusion Analysis
**Critical Path**: The perturbation generation and application step is critical because it determines the validity of the confusion analysis by ensuring targeted, controlled modifications.
**Design Tradeoffs**: The authors chose perturbation attacks over natural text samples to achieve controlled experimentation, trading ecological validity for experimental precision.
**Failure Signatures**: When LLMs confuse related aspects, they show correlated rating patterns across those aspects; when oversensitive, minor perturbations cause disproportionate score changes.
**First 3 Experiments**: 1) Test baseline evaluation on clean text across all 11 aspects, 2) Apply single-aspect perturbations and measure score changes, 3) Apply multi-aspect perturbations to test interaction effects.

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses on 11 predefined quality aspects, which may not capture all relevant evaluation dimensions
- Results from three LLM variants may not generalize to other models or future versions
- Perturbation attacks represent artificial modifications rather than naturally occurring errors

## Confidence
- High confidence: LLMs exhibit measurable confusion between related quality aspects and show oversensitivity to grammatical perturbations
- Medium confidence: The confusion issues persist across different LLM types and cannot be resolved through simple instruction modifications
- Medium confidence: The proposed hierarchical classification provides a more consistent framework than previous ad-hoc approaches

## Next Checks
1. Test the same perturbation methodology on additional LLM variants (Claude, LLaMA, Gemini) to verify if confusion patterns are consistent across architectures
2. Conduct human evaluation studies to establish ground truth for aspect distinction and determine if LLM confusion correlates with human judgment patterns
3. Apply the perturbation framework to domain-specific NLG tasks (medical, legal, technical writing) to assess whether confusion patterns vary by domain complexity