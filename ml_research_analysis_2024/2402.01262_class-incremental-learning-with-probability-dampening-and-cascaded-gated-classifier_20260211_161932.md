---
ver: rpa2
title: Class incremental learning with probability dampening and cascaded gated classifier
arxiv_id: '2402.01262'
source_url: https://arxiv.org/abs/2402.01262
tags:
- memory
- which
- past
- samples
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to Class Incremental Learning
  (CIL) that combines a Margin Dampening (MD) regularisation term with a Cascaded
  Gates (CG) classifier head. The MD term decreases past class probabilities up to
  a margin while preserving current task learning, and the CG head uses task-wise
  scaling functions to produce final predictions.
---

# Class incremental learning with probability dampening and cascaded gated classifier
## Quick Facts
- arXiv ID: 2402.01262
- Source URL: https://arxiv.org/abs/2402.01262
- Reference count: 40
- Proposed method outperforms established baselines (ER-ACE, ER-LODE, DER) on CIFAR-10, CIFAR-100, and TinyImageNet

## Executive Summary
This paper addresses the Class Incremental Learning (CIL) challenge by proposing a novel approach that combines Margin Dampening (MD) regularisation with a Cascaded Gates (CG) classifier head. The MD term reduces probabilities of past classes while preserving current task learning, and the CG head uses task-specific scaling functions to produce final predictions. The method effectively balances stability and plasticity, achieving higher accuracy and lower forgetting compared to established baselines like ER-ACE, ER-LODE, and DER across CIFAR-10, CIFAR-100, and TinyImageNet benchmarks.

## Method Summary
The proposed approach integrates two key components: Margin Dampening (MD) and Cascaded Gates (CG). MD works by decreasing the predicted probabilities of past classes up to a certain margin during training, helping to preserve knowledge of previously learned classes while still allowing the model to learn new tasks effectively. The CG classifier head employs task-wise scaling functions that generate final predictions by combining outputs from different task-specific components. This architecture allows the model to maintain performance on earlier tasks while adapting to new classes, addressing the fundamental stability-plasticity dilemma in incremental learning scenarios.

## Key Results
- Outperforms established baselines (ER-ACE, ER-LODE, DER) on CIFAR-10, CIFAR-100, and TinyImageNet
- Achieves higher accuracy and lower backward transfer (forgetting) compared to competing methods
- Effectively scales with memory size while maintaining manageable computational overhead

## Why This Works (Mechanism)
The method works by addressing the core challenge in CIL: catastrophic forgetting. The MD component actively suppresses probabilities for previously learned classes during new task training, creating a regularization effect that maintains past knowledge. The CG head provides a flexible architecture that can adapt task-specific scaling functions, allowing the model to leverage both old and new information. This dual approach creates a more robust learning mechanism that prevents the model from completely overwriting past knowledge while still enabling effective learning of new classes.

## Foundational Learning
- Catastrophic Forgetting: Neural networks tend to overwrite previous knowledge when learning new tasks; this is the fundamental problem CIL addresses
- Why needed: Without addressing forgetting, models become useless after learning just a few tasks
- Quick check: Compare performance on previous tasks after learning new ones

- Regularization Techniques: Methods like L2 regularization or knowledge distillation that prevent overfitting to new data
- Why needed: Helps maintain model performance on previously learned tasks
- Quick check: Monitor validation loss on both old and new tasks

- Memory Replay: Storing and replaying samples from previous tasks during new task training
- Why needed: Provides direct supervision on old tasks to prevent forgetting
- Quick check: Measure impact of different memory buffer sizes on performance

## Architecture Onboarding
Component Map: Input -> Feature Extractor -> MD Regularization -> CG Head -> Output
Critical Path: The feature extractor backbone feeds into both the MD regularisation layer and the CG head, which combines outputs for final prediction
Design Tradeoffs: The CG head introduces quadratic parameter growth with task count, creating scalability concerns for long task sequences
Failure Signatures: Poor performance on early tasks indicates insufficient MD strength; poor performance on new tasks suggests overly aggressive MD
First Experiments: 1) Test MD alone without CG on a single task sequence 2) Test CG alone with standard regularization 3) Gradually increase task sequence length to measure scaling behavior

## Open Questions the Paper Calls Out
None

## Limitations
- Computational overhead shows quadratic growth in parameters with task count, potentially problematic for long sequences
- Limited testing on datasets with significant domain shifts or real-world distribution changes
- No detailed memory/time benchmarks across varying task counts provided

## Confidence
High: Experimental results on CIFAR-10, CIFAR-100, and TinyImageNet showing consistent accuracy improvements and forgetting reduction
Medium: Claims about real-world scenario generalization based on controlled academic benchmarks

## Next Checks
1. Conduct scaling experiments to quantify memory and computation costs as task sequence length increases, particularly examining the quadratic parameter growth claim
2. Test the method on datasets with more significant domain shifts and real-world distribution changes to validate robustness beyond controlled benchmarks
3. Perform a more detailed ablation study isolating the effects of MD and CG components on both accuracy and forgetting metrics separately