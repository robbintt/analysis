---
ver: rpa2
title: 'DnA-Eval: Enhancing Large Language Model Evaluation through Decomposition
  and Aggregation'
arxiv_id: '2405.15329'
source_url: https://arxiv.org/abs/2405.15329
tags:
- evaluation
- llms
- arxiv
- human
- aspects
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes a framework for using large language models
  as evaluators for natural language generation tasks. The key idea is to decompose
  the evaluation process into stages: aspect generation, pairwise scoring, and aggregation,
  inspired by pedagogical practices.'
---

# DnA-Eval: Enhancing Large Language Model Evaluation through Decomposition and Aggregation

## Quick Facts
- **arXiv ID**: 2405.15329
- **Source URL**: https://arxiv.org/abs/2405.15329
- **Reference count**: 6
- **Primary result**: Proposed framework improves LLM evaluation performance by up to 39.6% on meta-evaluation benchmarks

## Executive Summary
This paper introduces DnA-Eval, a framework for using large language models as evaluators for natural language generation tasks. The approach decomposes the evaluation process into three stages inspired by pedagogical practices: aspect generation, pairwise scoring, and aggregation. The framework breaks down evaluation criteria into aspects, performs pairwise scoring for each aspect, and aggregates scores using weighted sums computed by an external calculator. Experiments demonstrate significant performance improvements of up to 39.6% compared to direct prompting and Chain-of-Thought methods across various proprietary and open-sourced LLMs.

## Method Summary
The DnA-Eval framework decomposes LLM evaluation into three stages: aspect generation, pairwise scoring, and aggregation. First, evaluation criteria are broken down into specific aspects. Then, pairwise scoring is performed for each aspect, where the LLM compares generated responses against reference responses. Finally, the scores are aggregated using an external calculator to compute weighted sums. This decomposition is inspired by pedagogical evaluation practices and aims to improve the interpretability and accuracy of LLM-based evaluation. The framework is tested across multiple meta-evaluation benchmarks with both proprietary and open-sourced LLMs.

## Key Results
- Framework achieves up to 39.6% performance improvement over direct prompting and Chain-of-Thought methods
- Consistent improvements observed across multiple proprietary and open-sourced LLM models
- Human studies show enhanced interpretability of intermediate outputs generated during evaluation stages

## Why This Works (Mechanism)
The decomposition approach works by breaking complex evaluation tasks into more manageable components that align with how human evaluators assess responses. By generating specific evaluation aspects first, the LLM can focus on one criterion at a time during pairwise scoring, reducing cognitive load and potential confusion. The aggregation stage then combines these focused assessments into a comprehensive evaluation. This staged approach mirrors pedagogical evaluation practices and leverages the LLM's strengths in handling structured, sequential reasoning tasks rather than asking it to evaluate holistically in a single step.

## Foundational Learning

**Pairwise comparison**: Why needed - Enables focused evaluation of specific aspects by comparing two items directly. Quick check - Can the LLM consistently rank which response is better for a given criterion.

**Aspect decomposition**: Why needed - Breaks complex evaluation criteria into manageable components. Quick check - Are the generated aspects comprehensive and non-redundant.

**Weighted aggregation**: Why needed - Combines individual aspect scores into an overall evaluation. Quick check - Do the weighted sums correlate with human judgments across different tasks.

## Architecture Onboarding

**Component map**: Evaluation Criteria -> Aspect Generation -> Pairwise Scoring -> Weighted Aggregation -> Final Score

**Critical path**: Aspect Generation → Pairwise Scoring → Aggregation. The framework fails if any stage produces unreliable outputs, as errors propagate downstream.

**Design tradeoffs**: 
- Pro: Improved interpretability and accuracy through decomposition
- Con: Increased computational overhead and complexity
- Pro: Better alignment with human evaluation practices
- Con: Relies heavily on LLM's ability to understand evaluation criteria

**Failure signatures**:
- Aspect generation produces irrelevant or redundant criteria
- Pairwise scoring becomes inconsistent across similar examples
- Aggregation weights don't reflect actual importance of aspects
- Performance degrades significantly on tasks requiring holistic understanding

**3 first experiments**:
1. Compare pairwise scoring consistency across different aspect granularities
2. Test aggregation sensitivity to different weighting schemes
3. Evaluate aspect generation quality on increasingly complex evaluation criteria

## Open Questions the Paper Calls Out
None

## Limitations
- Performance evaluated primarily on natural language generation tasks; effectiveness on other task types untested
- Framework relies on LLM's ability to understand and evaluate multiple criteria, which may vary by model and task complexity
- Human study for interpretability is limited in scope and may not capture full nuances of LLM evaluation
- Computational overhead compared to simpler evaluation methods not discussed, potentially limiting scalability

## Confidence
- Performance improvements: Medium
- Framework generalizability: Low
- Interpretability benefits: Medium
- Computational efficiency: Not assessed

## Next Checks
1. Test framework effectiveness on a broader range of tasks beyond natural language generation
2. Conduct more extensive human evaluation to assess interpretability and reliability of intermediate outputs across different LLM models
3. Compare computational efficiency and scalability of proposed framework with other evaluation methods