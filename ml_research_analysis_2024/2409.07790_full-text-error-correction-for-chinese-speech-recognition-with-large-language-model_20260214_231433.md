---
ver: rpa2
title: Full-text Error Correction for Chinese Speech Recognition with Large Language
  Model
arxiv_id: '2409.07790'
source_url: https://arxiv.org/abs/2409.07790
tags:
- text
- correction
- error
- speech
- full-text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the use of large language models (LLMs)
  for error correction in full-text generated by Chinese speech recognition systems.
  It introduces a new dataset, ChFT, for full-text error correction that includes
  various error types such as punctuation restoration and inverse text normalization.
---

# Full-text Error Correction for Chinese Speech Recognition with Large Language Model

## Quick Facts
- arXiv ID: 2409.07790
- Source URL: https://arxiv.org/abs/2409.07790
- Reference count: 23
- This paper introduces ChFT dataset and demonstrates LLM fine-tuning improves Chinese ASR error correction across multiple domains

## Executive Summary
This paper investigates the use of large language models for error correction in full-text generated by Chinese speech recognition systems. The authors introduce ChFT, a new dataset for full-text error correction created through a pipeline involving TTS synthesis, ASR processing, and edit-distance alignment. The study fine-tunes a pre-trained LLM using different prompts and output formats (direct text vs JSON-based error-correction pairs), demonstrating significant improvements in error rate reduction across various test settings.

## Method Summary
The study creates the ChFT dataset using a synthetic pipeline: reference text is converted to speech via TTS, then processed through an ASR system (including VAD, Paraformer, punctuation restoration, and ITN) to generate hypothesis text. Edit-distance alignment between reference and hypothesis extracts error-correction pairs, which are formatted as JSONL for fine-tuning. The LLM (GLM-4-9B-Chat) is fine-tuned using LoRA with a rank of 16 on linear modules, using LlamaFactory for training. Different prompts are tested based on input context (full-text vs segments) and output format (direct corrected text vs JSON error-correction pairs).

## Key Results
- Fine-tuned LLMs show consistent ERR improvements across homogeneous, heterogeneous, and hard test sets
- JSON output format provides better controllability and reduces hallucination compared to direct text correction
- Full-text context enables corrections that span multiple sentences and require discourse-level understanding
- The method works across diverse domains including sports, entertainment, finance, and technology

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Synthetic ASR errors via TTS+ASR pipeline simulate real-world transcription mistakes
- Mechanism: Text-to-speech synthesis generates speech from reference text, then ASR processing creates hypothesis text. Edit-distance alignment extracts error-correction pairs for training
- Core assumption: Synthetic error patterns generalize to real ASR errors
- Evidence anchors: Abstract mentions the ChFT dataset development pipeline; section describes edit-distance alignment; corpus shows related work but no direct validation
- Break condition: Real ASR errors differ significantly from synthetic patterns (different accent handling, noise robustness)

### Mechanism 2
- Claim: Full-text context enables corrections requiring broader semantic understanding
- Mechanism: Models process entire articles or multi-sentence segments, using discourse-level context to resolve ambiguities
- Core assumption: Longer context provides sufficient information for discourse-level corrections
- Evidence anchors: Abstract discusses full-text error correction from longer recordings; section mentions full-text and segment-level exploration across domains; corpus discusses full-text correction without direct segment vs full-text comparison
- Break condition: Additional context introduces noise or model cannot attend to relevant information across long distances

### Mechanism 3
- Claim: JSON output format provides better controllability and reduces hallucination
- Mechanism: Instead of generating fully corrected text, models output structured JSON with specific error-correction pairs
- Core assumption: Structured output constrains generation space, reducing hallucination likelihood
- Evidence anchors: Abstract mentions JSON-based error-correction pairs; section explains JSON format design to address hallucination and controllability issues; corpus lacks evidence about hallucination reduction
- Break condition: JSON structure is too restrictive or model struggles with structured generation

## Foundational Learning

- Concept: Edit-distance alignment for error detection
  - Why needed here: Identifies specific character/word substitutions, deletions, and insertions between reference and hypothesis text to create training pairs
  - Quick check question: Given reference "hello world" and hypothesis "hello word", what error-correction pairs would the edit-distance alignment produce?

- Concept: Low-Rank Adaptation (LoRA) for efficient fine-tuning
  - Why needed here: Fine-tuning 9B parameter LLM requires significant resources; LoRA reduces trainable parameters while maintaining performance
  - Quick check question: What is the primary benefit of using LoRA over full fine-tuning when working with large language models?

- Concept: Error Rate Reduction (ERR) as evaluation metric
  - Why needed here: Standard error rates don't show improvement relative to baseline; ERR quantifies percentage reduction achieved
  - Quick check question: If baseline error rate is 10% and fine-tuned model achieves 7%, what is the ERR?

## Architecture Onboarding

- Component map: Text → TTS → ASR → Alignment → Error-correction pairs → JSONL → LoRA fine-tuning → LLM inference → Evaluation
- Critical path: Data generation → Fine-tuning → Inference → Evaluation
- Design tradeoffs:
  - Synthetic vs real audio: Synthetic data is easier to control but may not capture real-world noise patterns
  - Context length: Longer context helps with discourse-level errors but increases hallucination risk and computational cost
  - Output format: Direct text is simpler but JSON provides better controllability and error localization
- Failure signatures:
  - High hallucination in direct text output
  - Incomplete JSON error-correction pairs
  - Poor generalization to out-of-domain text (hard test set)
  - Overfitting to synthetic error patterns
- First 3 experiments:
  1. Test baseline ASR performance on homogeneous test set to establish error rates
  2. Fine-tune LLM with "article direct" prompt and evaluate on all test sets
  3. Fine-tune with "article json" prompt and compare ERR improvements, particularly for Mandarin errors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do fine-tuned LLMs perform on real-world audio data compared to synthetic data?
- Basis in paper: [explicit] Authors acknowledge all audio data was machine-generated and may not reflect real-world conditions, planning to explore real-world audio data in future work
- Why unresolved: Current study only uses synthetic audio data from TTS synthesis, limiting generalizability
- What evidence would resolve it: Experiments with real-world audio data from various sources (podcasts, meetings, broadcasts) comparing fine-tuned LLM performance on synthetic vs real data

### Open Question 2
- Question: What is the optimal length of segments for error correction in full-text documents?
- Basis in paper: [inferred] Study explores segment-level correction by splitting full-text into multi-sentence segments but doesn't investigate optimal segment length
- Why unresolved: Paper mentions segments have relatively longer lengths than commonly used sentences but lacks systematic analysis of segment length effects
- What evidence would resolve it: Experiments with varying segment lengths analyzing trade-off between accuracy and computational cost to determine optimal segment length

### Open Question 3
- Question: How can additional context information, such as hot words, be incorporated into prompts to improve error correction performance?
- Basis in paper: [explicit] Authors mention plans to explore long audios in real-world scenarios and devise advanced prompts incorporating additional context information like hot words
- Why unresolved: Current study uses basic prompts without leveraging additional context information available in real-world scenarios
- What evidence would resolve it: Developing and testing prompt templates incorporating domain-specific hot words or speaker information, evaluating impact compared to basic prompts

## Limitations
- Synthetic data generation may not capture real-world ASR error patterns and noise conditions
- Focus exclusively on Chinese language without exploring multilingual capabilities or transfer learning
- JSON output format may limit holistic corrections requiring understanding beyond isolated error-correction pairs

## Confidence
- **High Confidence**: Effectiveness of fine-tuning LLMs on synthetic error-correction pairs (Mechanism 1) - well-supported by consistent ERR improvements
- **Medium Confidence**: Benefit of full-text context for error correction (Mechanism 2) - supported but limited by lack of direct segment vs full-text comparison
- **Medium Confidence**: JSON output format's advantage in reducing hallucination (Mechanism 3) - theoretically sound but lacks direct empirical validation

## Next Checks
1. Evaluate fine-tuned models on real ASR output from diverse audio conditions (noisy environments, different accents, recording qualities) to assess generalization beyond synthetic errors
2. Compare JSON output format against direct text correction on held-out test set to quantify actual hallucination reduction and determine if structured output impacts overall correction quality
3. Conduct ablation studies on context length by systematically varying input segment sizes to identify optimal balance between contextual information and hallucination risk