---
ver: rpa2
title: Towards Domain Adaptive Neural Contextual Bandits
arxiv_id: '2406.09564'
source_url: https://arxiv.org/abs/2406.09564
tags:
- domain
- target
- source
- regret
- bandit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the first general method for domain-adaptive
  contextual bandits (DABand), addressing the challenge of adapting contextual bandit
  algorithms across domains with distribution shift. The key innovation is an algorithm
  that learns a target-domain bandit model by collecting feedback solely from a source
  domain, while simultaneously performing effective exploration and representation
  alignment using unlabeled data from both domains.
---

# Towards Domain Adaptive Neural Contextual Bandits

## Quick Facts
- **arXiv ID**: 2406.09564
- **Source URL**: https://arxiv.org/abs/2406.09564
- **Reference count**: 40
- **Primary result**: Introduces first general method for domain-adaptive contextual bandits achieving sub-linear regret by collecting feedback solely from source domain while aligning representations

## Executive Summary
This paper introduces DABand, the first general method for domain-adaptive contextual bandits (DABand) that addresses the challenge of adapting contextual bandit algorithms across domains with distribution shift. The key innovation is an algorithm that learns a target-domain bandit model by collecting feedback solely from a source domain, while simultaneously performing effective exploration and representation alignment using unlabeled data from both domains. The method achieves sub-linear regret in the target domain through a combination of LinUCB-based exploration and adversarial domain alignment.

## Method Summary
DABand combines LinUCB contextual bandit selection with neural network-based representation learning and adversarial domain alignment. The algorithm maintains a bandit parameter vector, an encoder network, and a discriminator network. During each round, it selects actions based on LinUCB's exploration-exploitation trade-off in the source domain, collects rewards, and updates parameters using a multi-term loss function that includes source regret, regression error, data divergence, and predicted reward regularization. The encoder and discriminator are trained alternately to align source and target domain distributions in the latent space, enabling knowledge transfer without collecting expensive target domain feedback.

## Key Results
- Achieves accuracy improvements of 21.86% to 36.43% over Neural-LinUCB in zero-shot target regret settings across DIGIT, VisDA17, and S2RDA49 datasets
- Demonstrates significantly lower cumulative regret in continued training scenarios compared to state-of-the-art contextual bandit methods
- Theoretical analysis establishes sub-linear regret bounds for target domain that decompose into source regret and additional terms including regression error, data divergence, and predicted reward regularization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DABand achieves sub-linear regret in target domains by simultaneously performing effective exploration and representation alignment.
- Mechanism: The algorithm collects feedback from a low-cost source domain while aligning source and target domain representations in a shared latent space using unlabeled data from both domains. This enables exploration of the high-cost target domain without direct feedback collection.
- Core assumption: The source and target domains share some underlying structure that can be aligned in the latent space, and the regression error between domains remains bounded.
- Evidence anchors:
  - [abstract]: "learns a target-domain bandit model by collecting feedback solely from a source domain, while simultaneously performing effective exploration and representation alignment using unlabeled data from both domains"
  - [section]: "Our theoretical analysis shows that our method can achieve a sub-linear regret bound in the target domain"
- Break condition: If the domain divergence becomes too large or the regression error grows unbounded, the sub-linear regret guarantee fails.

### Mechanism 2
- Claim: The two additional terms in the regret bound (regression error and predicted reward) are essential for cross-domain bandit performance.
- Mechanism: The regression error term encourages accurate prediction of source rewards, while the predicted reward term regularizes the model to avoid overestimating rewards in the target domain.
- Core assumption: The regression error between source and target domains can be bounded, and the predicted reward regularization prevents model overconfidence.
- Evidence anchors:
  - [abstract]: "naively doing so leads to sub-optimal accuracy/regret (verified by our empirical results) and naturally leads to additional terms in the target-domain regret bound"
  - [section]: "Our target regret bound includes two additional crucial terms not found in domain adaptation... Regression Error in the Source Domain... Predicted Reward"
- Break condition: If either term is removed during training, performance drops significantly as shown in ablation studies.

### Mechanism 3
- Claim: The minimax optimization framework effectively aligns source and target domain distributions in the latent space.
- Mechanism: A discriminator is trained to classify whether data comes from the source or target domain, while the encoder is trained to fool this discriminator, effectively aligning the two distributions.
- Core assumption: The source and target domain contexts can be aligned in the latent space induced by the encoder, and this alignment transfers knowledge effectively.
- Evidence anchors:
  - [abstract]: "simultaneously performing effective exploration and representation alignment using unlabeled data from both domains"
  - [section]: "solving the minimax optimization min bϕ maxg Ldiv is equivalent to aligning source- and target-domain data distributions in the latent (encoding) space"
- Break condition: If the domains are too divergent to align effectively, the discriminator alignment will fail to transfer knowledge.

## Foundational Learning

- Concept: Contextual bandit algorithms and exploration-exploitation trade-off
  - Why needed here: DABand builds on contextual bandit foundations while extending them to domain adaptation scenarios
  - Quick check question: How does LinUCB balance exploration and exploitation in the single-domain setting?

- Concept: Domain adaptation theory and distribution alignment
  - Why needed here: DABand applies domain adaptation techniques to the bandit setting, requiring understanding of how to align distributions in online learning
  - Quick check question: What is the key difference between classification-based domain adaptation and regression-based domain adaptation in bandits?

- Concept: Neural network representation learning and backpropagation
  - Why needed here: DABand uses neural networks to learn domain-invariant representations in the latent space
  - Quick check question: How does the encoder transform raw contexts into a shared latent space for both domains?

## Architecture Onboarding

- Component map:
  - Encoder network (bϕ) transforms raw contexts into latent representations for both domains
  - Bandit parameter (bθ) is a linear predictor in latent space for reward estimation
  - Discriminator (g) classifies whether latent representations come from source or target domain
  - LinUCB module handles exploration-exploitation trade-off in the source domain
  - Loss functions combine source regret, regression error, data divergence, and predicted reward

- Critical path: Source domain → Context encoding → Reward prediction → Action selection → Feedback collection → Parameter updates (alternating between LinUCB and representation learning)

- Design tradeoffs: Linear vs. nonlinear encoders (interpretability vs. performance), exploration rate (α) tuning, alignment strength (λ) balancing

- Failure signatures: Large data divergence indicating poor alignment, high regression error suggesting poor source domain modeling, predicted reward term growing large indicating overconfidence

- First 3 experiments:
  1. Test DABand on DIGIT dataset with MNIST as source and MNIST-M as target to verify cross-domain performance
  2. Run ablation study removing either regression error or predicted reward term to confirm their importance
  3. Evaluate cumulative regret in continued training phase to measure cost savings in target domain

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several important areas remain unexplored based on the limitations section.

## Limitations
- Neural network architecture specifications for encoder and discriminator are not provided, making exact reproduction difficult
- Method primarily validated on image classification datasets with limited testing on other domain types
- Theoretical regret bounds rely on assumptions about domain divergence and regression error bounds that may not hold in all scenarios

## Confidence
- **High**: The core mechanism of using source domain feedback with representation alignment for domain-adaptive bandits
- **Medium**: The theoretical regret bound derivation and its practical implications
- **Medium**: The empirical performance improvements over baselines in the tested domains

## Next Checks
1. **Architecture sensitivity analysis**: Systematically test different encoder and discriminator architectures to understand their impact on domain alignment quality and regret performance.

2. **Cross-domain robustness evaluation**: Apply DABand to non-image domains (e.g., text, tabular data) to assess generalization beyond the tested image datasets.

3. **Theoretical bound verification**: Conduct experiments measuring actual vs. predicted regret components to validate the tightness of the theoretical bounds in practice.