---
ver: rpa2
title: 'DOZE: A Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic
  Environments'
arxiv_id: '2402.19007'
source_url: https://arxiv.org/abs/2402.19007
tags:
- objects
- object
- navigation
- goal
- obstacles
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DOZE, a new dataset for Zero-Shot Object
  Navigation (ZSON) in dynamic environments. Unlike existing ZSON datasets, DOZE incorporates
  dynamic humanoid obstacles, open-vocabulary objects, objects with distinct attributes,
  and textual hints, making it more realistic and challenging.
---

# DOZE: A Dataset for Open-Vocabulary Zero-Shot Object Navigation in Dynamic Environments

## Quick Facts
- **arXiv ID**: 2402.19007
- **Source URL**: https://arxiv.org/abs/2402.19007
- **Reference count**: 34
- **Primary result**: Introduces DOZE dataset with dynamic humanoid obstacles, open-vocabulary objects, and textual hints for challenging Zero-Shot Object Navigation tasks.

## Executive Summary
DOZE addresses limitations in existing Zero-Shot Object Navigation (ZSON) datasets by introducing dynamic humanoid obstacles, open-vocabulary objects, and textual hints. The dataset contains ten high-fidelity 3D scenes with over 18k tasks across three complexity levels based on humanoid obstacle behavior. Four representative ZSON methods were evaluated on DOZE, revealing significant room for improvement in navigation efficiency, safety, and object recognition accuracy, particularly in dynamic environments. The paper also introduces a hint-assisted navigation method that leverages textual information to improve performance.

## Method Summary
DOZE is implemented in AI2-THOR simulator with 10 synthetic 3D scenes featuring dynamic humanoid obstacles, open-vocabulary objects, and hint objects with textual content. Four baseline ZSON methods were tested: Random, Frontier, C-L3MVN (LLM with semantic map), and C-LGX (LLM with image captioning and object grounding). Collision detection was implemented with 0.2m threshold for humanoid obstacles. Success criteria defined as reaching within 1.0m of target object. Experiments evaluated performance across three complexity levels using Success Rate (SR), Success weighted by Path Length (SPL), and Collision Rate (CR) metrics.

## Key Results
- Static navigation (Level 1): Success rates range from 51.1% to 87.5% depending on object type
- Dynamic navigation (Level 2-3): Significant performance degradation due to moving obstacles
- Hint utility: H-L3MVN shows moderate improvement (SR: 51.1%→59.5%, SPL: 49.3%→58.5%)
- Collision rates increase substantially in higher complexity levels with moving obstacles

## Why This Works (Mechanism)

### Mechanism 1
Dynamic humanoid obstacles increase navigation complexity by requiring real-time collision avoidance and path replanning. Static and moving humanoids create occlusion zones and motion trajectories that force agents to detect, track, and adapt their paths dynamically. Core assumption: Collision detection between agent and humanoids is accurate and timely enough to enable effective avoidance.

### Mechanism 2
Open-vocabulary objects expose agents to novel object categories not seen during training, testing generalization of perception models. By introducing objects like "Stegosaurus model" and "Genie's lamp," the dataset forces agents to rely on semantic understanding and vision-language models to identify and navigate to unseen objects. Core assumption: Vision-language models can map novel object descriptions to visual features without task-specific fine-tuning.

### Mechanism 3
Textual hints provide semantic context that reduces search space and improves navigation efficiency. When agents detect whiteboard hints like "Tony, your baseball bat is under the bed," they can prioritize intermediate goal objects (e.g., "bed") before locating the final target. Core assumption: OCR and language models can reliably extract and interpret hints in real-time during navigation.

## Foundational Learning

- **Concept**: Embodied AI and simulation environments
  - Why needed: Understanding how agents perceive and act in 3D synthetic spaces is foundational to designing navigation algorithms
  - Quick check: What are the key differences between static and dynamic simulation environments in embodied AI?

- **Concept**: Zero-shot learning and open-vocabulary generalization
  - Why needed: ZSON requires agents to recognize and navigate to objects outside their training set, relying on semantic and visual reasoning
  - Quick check: How does zero-shot learning differ from few-shot learning in terms of model generalization?

- **Concept**: Multimodal fusion (vision + language)
  - Why needed: Effective ZSON in DOZE requires combining visual perception, spatial reasoning, and language understanding
  - Quick check: What are the challenges in fusing image features and text embeddings for navigation tasks?

## Architecture Onboarding

- **Component map**: AI2-THOR simulator -> Locobot agent with RGB-D camera -> GLIP object detection -> BLIP captioning -> LLM-based navigation policies -> Collision detection module -> OCR for hint extraction
- **Critical path**: Perception → Language grounding → Decision-making → Action execution → Collision detection → Feedback loop
- **Design tradeoffs**: Balancing semantic map richness (C-L3MVN) vs. real-time responsiveness (C-LGX); hint utility vs. search space complexity; static vs. dynamic obstacle modeling
- **Failure signatures**: High collision rates indicate poor dynamic obstacle modeling; low success rates with hints suggest OCR or language model failure; poor SPL indicates inefficient exploration strategies
- **First 3 experiments**:
  1. Baseline: Test Random and Frontier methods on Level 1 tasks to establish static navigation performance
  2. Dynamic testing: Evaluate C-L3MVN and C-LGX on Level 2 and 3 to assess dynamic obstacle handling
  3. Hint evaluation: Run H-L3MVN vs. L3MVN on Level 1 tasks with hints to measure efficiency gains

## Open Questions the Paper Calls Out

### Open Question 1
How do the performance of ZSON methods change when applied to more complex real-world indoor environments, such as those with irregular layouts or non-standard object placements? The paper focuses on synthetic 3D scenes and does not provide data on real-world environment performance.

### Open Question 2
What is the impact of integrating advanced predictive models for dynamic obstacle behavior on the navigation success rates and safety of ZSON methods? The paper mentions that existing ZSON methods fall short in scenarios featuring dynamic moving obstacles and highlights the need for predictive and planning capabilities.

### Open Question 3
How effective are multimodal ZSON methods that integrate textual information in environments with varying levels of text density and complexity? The paper introduces a hint-assisted navigation method that leverages textual information and suggests this opens up a new research direction for ZSON tasks.

## Limitations
- Collision detection accuracy with dynamic obstacles lacks independent validation and may underestimate actual interactions
- Open-vocabulary object recognition generalization shows significant performance variance across object types without clear explanation
- Hint utility in realistic scenarios needs more rigorous testing with degraded OCR and ambiguous hints

## Confidence

- **High Confidence**: Static navigation performance metrics (SR, SPL, CR) on Level 1 tasks are well-documented and reproducible
- **Medium Confidence**: Dynamic navigation capabilities with moving obstacles, though the collision detection mechanism needs independent validation
- **Low Confidence**: Open-vocabulary object recognition generalization and hint utility in complex, real-world scenarios

## Next Checks
1. **Collision Detection Latency Analysis**: Measure time delay between humanoid movement and agent collision detection across different speeds and occlusion scenarios
2. **Novel Object Recognition Benchmark**: Create controlled test set with truly unseen object categories to measure zero-shot recognition accuracy
3. **Hint Robustness Testing**: Evaluate H-L3MVN performance with degraded OCR quality, ambiguous hints, and contradictory spatial information