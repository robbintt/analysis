---
ver: rpa2
title: A Survey of Deep Learning and Foundation Models for Time Series Forecasting
arxiv_id: '2401.13912'
source_url: https://arxiv.org/abs/2401.13912
tags:
- time
- series
- forecasting
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reviews progress in deep learning and foundation models
  for time series forecasting, with a focus on pandemic prediction. It discusses recent
  advances in transformer models, graph neural networks, and foundation models for
  time series, as well as the potential of incorporating knowledge graphs and large
  language models.
---

# A Survey of Deep Learning and Foundation Models for Time Series Forecasting

## Quick Facts
- arXiv ID: 2401.13912
- Source URL: https://arxiv.org/abs/2401.13912
- Reference count: 40
- Authors: John A. Miller, Mohammed Aldosari, Farah Saeed, Nasid Habib Barna, Subas Rana, I. Budak Arpinar, Ninghao Liu

## Executive Summary
This paper provides a comprehensive survey of deep learning and foundation models for time series forecasting, with particular emphasis on pandemic prediction. The authors examine recent advances including transformer models, graph neural networks, and foundation models specifically designed for time series data. A key focus is on the potential of incorporating knowledge graphs and large language models to enhance forecasting accuracy and interpretability. The survey identifies major challenges in pandemic forecasting including limited data availability, lack of accumulated scientific knowledge, and model interpretability issues.

## Method Summary
The survey synthesizes existing research on deep learning approaches for time series forecasting, organizing methods into four categories based on data modality usage. The authors analyze various architectures including transformers, graph neural networks, and foundation models like TimeGPT and PatchTST. They examine datasets such as COVID-19, Electricity, Weather, Traffic, and ILI (Influenza-like Illness) from sources like the Monash Time Series Forecasting Repository. The paper evaluates performance using forecast quality metrics including MSE, MAE, sMAPE, and MASE on both normalized and original scales, while also discussing the integration of knowledge graphs and large language models for enhanced forecasting capabilities.

## Key Results
- Foundation models pre-trained on large time series datasets show promise for zero-shot or few-shot forecasting performance across domains
- Multi-modal approaches combining text and time series data could improve both forecasting accuracy and model interpretability
- Sparse attention mechanisms offer computational efficiency improvements while maintaining or enhancing forecasting accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training foundation models on large time series datasets enables zero-shot or few-shot forecasting performance.
- Mechanism: Large-scale training on diverse time series captures universal temporal patterns (trend, seasonality, local variability) that can be transferred to specific forecasting tasks.
- Core assumption: Temporal dynamics across domains share sufficient structural similarity to transfer.
- Evidence anchors:
  - [abstract]: "The development of foundation models (large deep learning models with extensive pre-training) allows models to understand patterns and acquire knowledge that can be applied to new related problems before extensive training data becomes available."
  - [section]: "As discussed in the next subsection, the accuracy of such models can be improved by fine-tuning."
  - [corpus]: Weak; no direct citations for time series foundation model performance yet.
- Break condition: If temporal patterns are too domain-specific (e.g., stock vs. biomedical signals), transfer learning yields minimal benefit.

### Mechanism 2
- Claim: Multi-modal inputs (text + time series) improve forecasting accuracy and interpretability.
- Mechanism: Textual context (e.g., news, policy announcements) provides external knowledge about causal drivers not present in raw time series.
- Core assumption: Text and time series are temporally aligned and causally related.
- Evidence anchors:
  - [abstract]: "Furthermore, there is a vast amount of knowledge available that deep learning models can tap into, including Knowledge Graphs and Large Language Models fine-tuned with scientific domain knowledge."
  - [section]: "Type 4 models utilize both textual and time series data to improve forecasting accuracy and provide greater potential for explainability."
  - [corpus]: Missing direct evidence; need more studies on multi-modal time series forecasting.
- Break condition: If textual and time series signals are misaligned or irrelevant, the additional modality adds noise.

### Mechanism 3
- Claim: Sparse attention mechanisms reduce computational complexity while improving forecasting accuracy by focusing on relevant temporal dependencies.
- Mechanism: Attention weights are zeroed for distant or weakly correlated timesteps, reducing quadratic complexity and minimizing distraction.
- Core assumption: Local temporal patterns dominate over global dependencies in most time series.
- Evidence anchors:
  - [section]: "Over the last few years, there have been several papers that have examined sparse attention for transformers. Rather than having each time point compared with every other time (quadratic attention), the focus is sharpened and the complexity of the attention is reduced."
  - [corpus]: Weak; no direct citations for sparse attention in time series forecasting.
- Break condition: If long-range dependencies are critical (e.g., climate data), sparse attention may miss important patterns.

## Foundational Learning

- Concept: Multivariate time series (MTS) representation learning
  - Why needed here: Transforms raw time series into richer latent spaces that capture essential patterns for downstream forecasting tasks.
  - Quick check question: What is the difference between time series representation learning and standard feature extraction?

- Concept: Temporal knowledge graph embeddings
  - Why needed here: Encodes time-dependent relationships as vector representations that can be combined with time series inputs to enhance forecasting.
  - Quick check question: How do temporal knowledge graph embeddings differ from static knowledge graph embeddings?

- Concept: Rolling validation
  - Why needed here: Addresses staleness in both data and model parameters during long-horizon forecasting evaluation.
  - Quick check question: Why is k-fold cross-validation not suitable for time series forecasting?

## Architecture Onboarding

- Component map:
  - Backbone model (transformer, GNN, or hybrid) -> Input preprocessing (patching, normalization, temporal encoding) -> Attention mechanism (sparse, multi-head, cross-channel) -> Knowledge injection layer (knowledge graph embeddings, constraints) -> Output layer (forecasting horizon, probabilistic distribution) -> Fine-tuning module (adapter, LoRA, or full fine-tuning)

- Critical path:
  1. Data preprocessing -> 2. Backbone forward pass -> 3. Knowledge injection -> 4. Output transformation -> 5. Loss computation

- Design tradeoffs:
  - Channel independence vs. channel mixing: independence reduces complexity but may miss cross-channel dependencies
  - Pre-training scale vs. fine-tuning efficiency: larger models generalize better but cost more to fine-tune
  - Sparse vs. full attention: sparse is faster but may miss long-range dependencies

- Failure signatures:
  - High validation error but low training error: overfitting
  - Degraded performance on longer horizons: insufficient temporal modeling
  - Unstable attention weights: improper masking or normalization

- First 3 experiments:
  1. Train a simple transformer with full attention on ETTh1 dataset, compare to Random Walk baseline
  2. Replace full attention with sparse attention, measure MSE improvement and speedup
  3. Add knowledge graph embeddings from COVID-19 ontology, evaluate impact on forecasting accuracy and interpretability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Will foundation models trained on time series data from diverse domains (e.g., stock market, weather, health) be transferable across domains, or will domain-specific training be necessary for optimal performance?
- Basis in paper: [explicit] The paper discusses the potential for foundation models to capture universal patterns across time series domains, but also raises the question of whether such universality exists and whether domain-specific training might be necessary.
- Why unresolved: While the paper mentions the possibility of domain-specific foundation models, it does not provide empirical evidence or theoretical arguments to definitively answer whether cross-domain transfer is feasible or optimal.
- What evidence would resolve it: Empirical studies comparing the performance of foundation models trained on diverse domains versus domain-specific models on a range of time series forecasting tasks would provide insights into the transferability of knowledge and the necessity of domain-specific training.

### Open Question 2
- Question: How can knowledge graphs and large language models be effectively integrated into time series forecasting models to improve accuracy and interpretability?
- Basis in paper: [explicit] The paper highlights the potential of incorporating knowledge graphs and large language models into time series forecasting models, but does not provide specific methods or empirical evidence for their effective integration.
- Why unresolved: While the paper discusses the potential benefits of knowledge integration, it does not delve into the technical details of how this can be achieved or the specific challenges and limitations involved.
- What evidence would resolve it: Research papers presenting novel methods for integrating knowledge graphs and large language models into time series forecasting models, along with empirical evaluations demonstrating their effectiveness, would provide concrete evidence for their potential benefits and challenges.

### Open Question 3
- Question: What are the optimal strategies for fine-tuning foundation models for time series forecasting tasks, considering factors such as parameter efficiency, computational cost, and accuracy?
- Basis in paper: [explicit] The paper mentions the concept of fine-tuning foundation models for specific domains, but does not provide detailed strategies or empirical comparisons of different fine-tuning approaches.
- Why unresolved: While the paper acknowledges the importance of fine-tuning, it does not provide a comprehensive analysis of the various fine-tuning strategies available or their trade-offs in terms of efficiency, cost, and accuracy.
- What evidence would resolve it: Comparative studies evaluating the performance of different fine-tuning strategies (e.g., full fine-tuning, adapter fine-tuning, LoRA) on a range of time series forecasting tasks, considering factors such as parameter efficiency, computational cost, and accuracy, would provide insights into the optimal strategies for fine-tuning foundation models.

## Limitations

- Limited empirical evidence for foundation models in time series forecasting: While theoretical arguments are strong, direct performance comparisons between pre-trained foundation models and task-specific models are largely absent from the survey.
- Sparse attention effectiveness in time series remains under-validated: The survey claims benefits but provides minimal concrete evidence of performance gains in actual time series benchmarks.
- Multi-modal integration challenges: The proposed benefits of combining text and time series data are largely theoretical, with few concrete implementations or results presented.

## Confidence

- **High confidence**: Traditional deep learning approaches (Transformers, GNNs, LSTMs) remain effective for time series forecasting, with established performance on standard benchmarks.
- **Medium confidence**: Foundation models show promise for time series forecasting, but require more empirical validation to confirm transfer learning benefits.
- **Low confidence**: Multi-modal approaches combining text and time series data are promising but remain largely theoretical with limited practical validation.

## Next Checks

1. Implement sparse attention mechanisms on ETTh1 and ETTh2 datasets and compare performance against full attention baselines across multiple time horizons.
2. Conduct ablation studies on knowledge graph integration, testing forecasting accuracy with and without temporal knowledge embeddings on COVID-19 data.
3. Design a controlled experiment comparing foundation model fine-tuning (TimeGPT, PatchTST) against traditional deep learning models on multivariate time series datasets.