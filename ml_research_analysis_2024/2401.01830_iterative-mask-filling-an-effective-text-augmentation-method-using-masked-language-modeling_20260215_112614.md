---
ver: rpa2
title: 'Iterative Mask Filling: An Effective Text Augmentation Method Using Masked
  Language Modeling'
arxiv_id: '2401.01830'
source_url: https://arxiv.org/abs/2401.01830
tags:
- augmentation
- text
- data
- language
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors propose Iterative Mask Filling (IMF), a text augmentation\
  \ method using BERT\u2019s Fill-Mask feature. The method iteratively masks each\
  \ word in a sentence, replaces it with a language model prediction, and repeats\
  \ to create new, semantically similar sentences."
---

# Iterative Mask Filling: An Effective Text Augmentation Method Using Masked Language Modeling

## Quick Facts
- arXiv ID: 2401.01830
- Source URL: https://arxiv.org/abs/2401.01830
- Reference count: 30
- Primary result: Iterative Mask Filling (IMF) consistently improves classification accuracy, especially on small datasets, by generating semantically similar sentences via BERT's fill-mask feature

## Executive Summary
Iterative Mask Filling (IMF) is a text augmentation method that leverages BERT's fill-mask capability to iteratively mask and replace each word in a sentence, generating new, semantically similar training examples. Experiments on four datasets—two for news classification and two for sentiment analysis—demonstrate that IMF enhances classification accuracy when used alongside real training data, with the greatest benefit observed for smaller datasets. Filtering augmented samples by classifier loss and using smaller language models further optimize performance and efficiency.

## Method Summary
IMF works by iteratively masking each word in a sentence, using a masked language model (e.g., BERT) to predict replacements, and repeating this process to generate new sentences. The method is applied at a 100% augmentation rate in most experiments, with additional filtering of low-quality augmentations based on classifier loss. IMF is evaluated on sentence-level tasks (news classification) and token-level tasks (sentiment analysis), showing consistent improvements in accuracy, particularly for small datasets. DistilBERT is used as a faster, smaller alternative to BERT with only minor accuracy trade-offs.

## Key Results
- IMF improves classification accuracy on both news classification and sentiment analysis datasets when used with real training data.
- Filtering augmented samples by classifier loss further boosts performance.
- DistilBERT offers computational speed advantages with minimal accuracy loss compared to BERT.

## Why This Works (Mechanism)
IMF exploits BERT's ability to generate contextually appropriate word substitutions, iteratively producing diverse yet semantically similar sentences. By repeating the masking and prediction process, the method increases training data diversity without straying far from the original sentence's meaning. Filtering by classifier loss helps retain only the most useful augmentations, reducing noise and improving model generalization.

## Foundational Learning
- **Masked Language Modeling (MLM)**: BERT's fill-mask predicts missing words based on context. Why needed: Core mechanism for generating plausible word substitutions. Quick check: Verify BERT's mask-filling predictions align with sentence semantics.
- **Text Augmentation**: Expanding training data with synthetic examples. Why needed: Improves model robustness and generalization, especially for small datasets. Quick check: Measure diversity and semantic similarity of generated sentences.
- **Classifier Loss Filtering**: Selecting high-quality augmentations by thresholding model loss. Why needed: Removes noisy or misleading augmentations. Quick check: Analyze distribution of losses for real vs. augmented data.

## Architecture Onboarding

**Component Map**
IMF -> BERT (or DistilBERT) -> Augmented Data -> Classifier -> Filtered Augmentations

**Critical Path**
Input sentence -> Iterative masking and fill-mask prediction -> Classifier evaluation -> Loss-based filtering -> Final augmented dataset

**Design Tradeoffs**
- Using larger LMs (BERT) vs. smaller (DistilBERT): Accuracy vs. speed.
- Augmentation rate (100%): Maximum diversity vs. risk of overfitting or noise.
- Filtering by classifier loss: Quality control vs. potential bias toward classifier's preferences.

**Failure Signatures**
- Semantic drift: Substitutions alter sentence meaning, especially in sentiment tasks.
- Overfitting: Excessive augmentation on small datasets may reduce generalization.
- Computational overhead: Iterative masking and filtering can be resource-intensive.

**3 First Experiments**
1. Compare IMF with no augmentation on a small news classification dataset.
2. Test IMF with and without loss-based filtering on a sentiment analysis task.
3. Evaluate the impact of using DistilBERT vs. BERT on classification accuracy and training speed.

## Open Questions the Paper Calls Out
None

## Limitations
- IMF's effectiveness is tied to BERT-based models; generalization to other architectures (e.g., GPT, multilingual LMs) is unclear.
- The aggressive 100% augmentation rate may not be optimal for all domains or dataset sizes.
- Filtering by classifier loss is intuitive but lacks rigorous validation for consistently selecting high-quality augmentations.

## Confidence
- IMF consistently improves classification accuracy: Medium
- Filtering by classifier loss improves augmented data quality: Medium
- IMF is most effective for sentence-level tasks, less so for sentiment: Medium

## Next Checks
1. Test IMF with decoder-only models (e.g., GPT variants) and multilingual LMs to assess architectural and linguistic generalization.
2. Conduct a systematic ablation study on augmentation rate (e.g., 25%, 50%, 75%, 100%) to find optimal settings per dataset.
3. Analyze the semantic drift in augmented sentiment examples: categorize substitutions by part-of-speech and sentiment polarity, and measure their impact on model predictions.