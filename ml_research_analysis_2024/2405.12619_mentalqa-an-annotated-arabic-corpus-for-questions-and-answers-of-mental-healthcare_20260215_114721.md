---
ver: rpa2
title: 'MentalQA: An Annotated Arabic Corpus for Questions and Answers of Mental Healthcare'
arxiv_id: '2405.12619'
source_url: https://arxiv.org/abs/2405.12619
tags:
- mental
- health
- questions
- question
- types
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Mental health disorders are prevalent worldwide, but access to
  adequate care remains a challenge, particularly for underserved communities. Text
  mining tools offer immense potential to support mental healthcare by assisting professionals
  in diagnosing and treating patients.
---

# MentalQA: An Annotated Arabic Corpus for Questions and Answers of Mental Healthcare

## Quick Facts
- arXiv ID: 2405.12619
- Source URL: https://arxiv.org/abs/2405.12619
- Authors: Hassan Alhuzali; Ashwag Alasmari; Hamad Alsaleh
- Reference count: 33
- Key outcome: MentalQA is a novel Arabic dataset featuring conversational-style question-answer interactions in the domain of mental health, comprising 500 Q&A posts with 1000 total annotations across question types and answer strategies.

## Executive Summary
Mental health disorders are prevalent worldwide, but access to adequate care remains a challenge, particularly for underserved communities. Text mining tools offer immense potential to support mental healthcare by assisting professionals in diagnosing and treating patients. However, the development of such tools faces limitations, predominantly caused by the scarcity of available datasets, especially for the Arabic language. This study introduces MentalQA, a novel Arabic dataset featuring conversational-style question-answer (QA) interactions in the domain of mental health. The dataset comprises 500 questions and answers (Q&A) posts, including both question types and answer strategies, yielding a total of 1000 annotations. The annotation schema for mental health questions and corresponding answers draws upon existing classification schemes with some modifications. Question types encompass six distinct categories: diagnosis, treatment, anatomy & physiology, epidemiology, healthy lifestyle, and provider choice. Answer strategies include information provision, direct guidance, and emotional support. Three experienced annotators collaboratively annotated the data to ensure consistency. Our findings demonstrate high inter-annotator agreement, with Fleiss' Kappa of 0.61 for question types and 0.98 for answer strategies. In-depth analysis revealed insightful patterns, including variations in question preferences across age groups and a strong correlation between question types and answer strategies. MentalQA offers a valuable foundation for developing Arabic text mining tools capable of supporting mental health professionals and individuals seeking information.

## Method Summary
The MentalQA dataset was created by collecting 2,621 Q&A pairs from an Arabic medical platform (Altibbi.com) in the mental health category, from 2020-2021. Three experienced annotators used a well-defined schema to categorize questions into six types (diagnosis, treatment, anatomy & physiology, epidemiology, healthy lifestyle, provider choice) and answers into three strategies (information, direct guidance, emotional support). The annotators independently applied the schema to a subset of data, calculated inter-annotator agreement using Fleiss' Kappa, and resolved disagreements through discussion. The resulting dataset comprises 500 Q&A posts with 1000 total annotations.

## Key Results
- High inter-annotator agreement, with Fleiss' Kappa of 0.61 for question types and 0.98 for answer strategies
- Analysis of question type preferences across age groups revealed distinct patterns, such as patients under 20 years old exhibiting a relatively higher proportion of questions in categories A and B
- Strong correlation between question types and answer strategies, with varying degrees of correlation between the categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High inter-annotator agreement on answer strategies (Fleiss' Kappa = 0.96) enables reliable categorization of response types, which improves downstream model performance for mental health question answering.
- Mechanism: When annotators consistently agree on the type of strategy (information, direct guidance, emotional support), the dataset labels become trustworthy signals that models can learn from, reducing label noise and increasing prediction accuracy.
- Core assumption: Annotator expertise in biomedical text and mental health topics ensures that high agreement reflects true category boundaries rather than arbitrary consensus.
- Evidence anchors:
  - [abstract] Our findings demonstrate high inter-annotator agreement, with Fleiss' Kappa of 0.61 for question types and 0.98 for answer strategies.
  - [section III.A] The agreement for our annotated Q&A posts is found to be 0.61 and 0.96 for question types and answers strategies, respectively.
  - [corpus] The corpus neighbors show similar annotated QA datasets (e.g., CARMA, CounselBench), suggesting this level of agreement is comparable to other high-quality mental health QA corpora.
- Break condition: If annotators disagree on strategy boundaries due to ambiguous phrasing or domain-specific jargon, the agreement metric will drop and model training signals will become unreliable.

### Mechanism 2
- Claim: Question type classification with moderate agreement (Fleiss' Kappa = 0.61) still captures clinically relevant distinctions that correlate with answer strategy choice.
- Mechanism: Even with imperfect agreement, the statistical relationship between question types (e.g., diagnosis vs. treatment) and answer strategies (e.g., emotional support vs. direct guidance) remains strong enough for models to learn meaningful patterns.
- Core assumption: The classification schema aligns with real-world clinical reasoning patterns, so even imperfect human labels retain semantic value.
- Evidence anchors:
  - [abstract] Our findings demonstrate high inter-annotator agreement, with Fleiss' Kappa of 0.61 for question types and 0.98 for answer strategies.
  - [section III.B] Figure 3 reveals intriguing insights into the relationship between the question types and answer strategies, with varying degrees of correlation between the categories.
  - [corpus] No direct corpus evidence for question type agreement, but the presence of related annotated datasets (e.g., AraHealthQA) suggests this moderate agreement level is typical in mental health QA annotation.
- Break condition: If question categories are too ambiguous or overlap excessively, the correlation with answer strategies will weaken, making the dataset less useful for training intent-aware models.

### Mechanism 3
- Claim: Patient demographics (age, gender) provide contextual features that improve personalization of mental health Q&A systems.
- Mechanism: By mapping question type preferences to demographic segments, models can adapt responses to the most likely needs of a given user profile, increasing relevance and engagement.
- Core assumption: Demographic patterns in question asking reflect underlying differences in mental health concerns and communication styles.
- Evidence anchors:
  - [section III.C] Analysis of the most asked questions by patients’ gender reveals that male patients display a higher frequency of questions pertaining to treatment, while female patients exhibit a significant interest in questions about diagnosis.
  - [section III.D] Analysis of the most asked question types by patients’ age shows distinct patterns, such as patients under 20 years old exhibiting a relatively higher proportion of questions in categories A and B.
  - [corpus] The corpus neighbors include datasets with demographic annotations (e.g., CARMA), suggesting this is a recognized useful feature in mental health QA corpora.
- Break condition: If demographic trends shift over time or differ across cultural contexts, models trained on this data may become less accurate for new populations.

## Foundational Learning

- Concept: Inter-rater reliability metrics (e.g., Fleiss' Kappa)
  - Why needed here: To assess the quality and consistency of annotations across multiple annotators, which directly impacts dataset trustworthiness.
  - Quick check question: What does a Fleiss' Kappa of 0.61 indicate about question type annotation agreement?

- Concept: Question classification schemas in medical domains
  - Why needed here: To understand how different question intents (diagnosis, treatment, etc.) map to appropriate answer strategies.
  - Quick check question: How does the inclusion of a "provider choice" category affect the utility of the dataset for clinical decision support?

- Concept: Sentiment analysis in healthcare communication
  - Why needed here: To capture emotional tone in patient questions and provider responses, which can guide empathetic response generation.
  - Quick check question: Why might negative sentiment dominate patient questions in mental health datasets?

## Architecture Onboarding

- Component map:
  - Data ingestion: Raw Arabic Q&A posts from medical platform
  - Annotation pipeline: Question type and answer strategy labeling by expert annotators
  - Quality control: Inter-annotator agreement computation, majority voting
  - Analysis module: Correlation analysis, demographic breakdown, sentiment analysis
  - Task definition: Three subtasks—question classification, answer strategy classification, conversational QA

- Critical path:
  1. Collect and preprocess Arabic Q&A data
  2. Apply annotation schema (6 question types, 3 answer strategies)
  3. Compute inter-annotator agreement and resolve conflicts
  4. Perform exploratory data analysis (demographics, sentiment, word frequency)
  5. Define and document the three task formats for model training

- Design tradeoffs:
  - Annotation depth vs. coverage: Full dataset annotation is ideal but resource-intensive; selective annotation balances quality and scalability.
  - Schema granularity vs. agreement: More categories may capture nuance but reduce inter-rater reliability.
  - Cultural specificity vs. generalizability: Arabic-language focus ensures relevance but limits cross-lingual transfer.

- Failure signatures:
  - Low inter-annotator agreement (< 0.4) indicates unclear category boundaries.
  - Skewed label distribution (e.g., 80% treatment questions) may cause model bias.
  - Missing demographic metadata limits personalization capabilities.

- First 3 experiments:
  1. Train a multi-class classifier on question types using the annotated subset; evaluate accuracy and confusion matrix.
  2. Train a multi-label classifier on answer strategies; test correlation between predicted and actual strategies.
  3. Build a joint model that predicts both question type and answer strategy from input text; measure performance on conversational QA task.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can MentalQA be effectively used to train large language models (LLMs) for Arabic mental health applications?
- Basis in paper: [explicit] The paper mentions that MentalQA provides a valuable foundation for developing Arabic text mining tools and can be used to train LLMs to understand user intent and deliver appropriate responses in Arabic.
- Why unresolved: The paper discusses the potential of MentalQA for training LLMs but does not provide specific details on how to effectively use the dataset for this purpose.
- What evidence would resolve it: Experimental results demonstrating the performance of LLMs trained on MentalQA compared to other datasets, and insights into the best practices for training LLMs using MentalQA.

### Open Question 2
- Question: How does the distribution of question types and answer strategies in MentalQA compare to real-world mental health consultations?
- Basis in paper: [explicit] The paper analyzes the distribution of question types and answer strategies in MentalQA, but does not compare it to real-world data.
- Why unresolved: The paper does not provide a comparison between the distribution of question types and answer strategies in MentalQA and real-world mental health consultations.
- What evidence would resolve it: A study comparing the distribution of question types and answer strategies in MentalQA to a dataset of real-world mental health consultations.

### Open Question 3
- Question: How can MentalQA be used to develop personalized mental health support systems?
- Basis in paper: [explicit] The paper mentions that MentalQA can be used to develop personalized mental health support systems by leveraging insights from the dataset.
- Why unresolved: The paper does not provide specific details on how to use MentalQA to develop personalized mental health support systems.
- What evidence would resolve it: A study demonstrating the development of a personalized mental health support system using MentalQA, and an evaluation of its effectiveness.

## Limitations
- The inter-annotator agreement for question types (Fleiss' Kappa = 0.61) falls in the moderate range, suggesting potential ambiguity in category boundaries.
- The dataset's focus on Arabic language and specific cultural context may limit generalizability to other languages or regions.
- The analysis relies on demographic data that may not capture the full complexity of mental health concerns across diverse populations.

## Confidence

- **High**: Inter-rater reliability methodology, dataset creation process, basic annotation schema design
- **Medium**: Correlation analysis between question types and answer strategies, demographic pattern findings
- **Medium**: Claim that the dataset enables reliable mental health QA tool development

## Next Checks

1. Replicate the inter-annotator agreement study with a new batch of annotators to verify consistency of the 0.61 Kappa score for question types
2. Test the classification schema on an independent sample of Arabic mental health Q&A data to assess generalizability
3. Conduct user studies with mental health professionals to validate whether the question-answer strategy patterns align with clinical best practices