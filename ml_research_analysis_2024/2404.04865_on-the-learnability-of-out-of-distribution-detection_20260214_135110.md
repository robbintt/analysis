---
ver: rpa2
title: On the Learnability of Out-of-distribution Detection
arxiv_id: '2404.04865'
source_url: https://arxiv.org/abs/2404.04865
tags:
- detection
- space
- theorem
- then
- learnability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates the PAC learnability of out-of-distribution
  (OOD) detection, a key problem in machine learning where test data may come from
  unknown classes during training. The authors study the learnability of OOD detection
  under two common evaluation metrics: risk and AUC.'
---

# On the Learnability of Out-of-distribution Detection

## Quick Facts
- arXiv ID: 2404.04865
- Source URL: https://arxiv.org/abs/2404.04865
- Authors: Zhen Fang; Yixuan Li; Feng Liu; Bo Han; Jie Lu
- Reference count: 40
- Primary result: Proves impossibility theorems for OOD detection learnability and identifies necessary/sufficient conditions in practical scenarios

## Executive Summary
This paper investigates the PAC learnability of out-of-distribution (OOD) detection, a fundamental problem in machine learning where test data may come from unknown classes during training. The authors study learnability under two common evaluation metrics: risk and AUC. They prove several impossibility theorems showing that OOD detection is not learnable in the total space or separate space under certain conditions, particularly when ID and OOD distributions overlap. However, they also identify necessary and sufficient conditions for learnability in practical scenarios, providing theoretical support for representative OOD detection works and showing that learnability depends on the domain space and function space.

## Method Summary
The paper establishes a PAC learning framework for OOD detection by defining domain spaces (total, separate, finite-ID, density-based) and function spaces (hypothesis space and ranking function space). It introduces necessary conditions for learnability under risk (Condition 1) and AUC (Condition 2), and proves impossibility theorems when these conditions fail. The authors analyze learnability across different domain spaces and function spaces, showing that learnability is achievable in finite feature spaces and under certain Realizability Assumptions, while providing insights into the limitations of OOD detection in practical scenarios.

## Key Results
- OOD detection is not learnable under risk in the total space DallXY when there exists overlap between ID and OOD distributions
- OOD detection is learnable under risk in the separate space if and only if the feature space X is finite
- Learnability under AUC in the separate space depends on the existence of a ranking function that can perfectly separate ID and OOD data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OOD detection is not learnable in the total space DallXY for any non-trivial hypothesis space if there exists overlap between ID and OOD distributions.
- Mechanism: The overlap creates a situation where the necessary linear condition under risk (Condition 1) fails. Specifically, when both ID and OOD distributions assign positive probability to the same region, the infimum risks for ID and OOD classification cannot be simultaneously zero, violating the condition needed for learnability.
- Core assumption: The existence of an overlap between ID and OOD distributions, and that the hypothesis space can achieve zero risk on both ID and OOD data separately.
- Evidence anchors:
  - [abstract]: "They prove several impossibility theorems showing that OOD detection is not learnable in the total space or separate space under certain conditions."
  - [section]: "Theorem 4 (Impossibility Theorem for Total Space under Risk) OOD detection is not learnable under risk in the total space DallXY for H, if |ϕ ◦ H| > 1, where ϕ maps ID labels to 1 and maps OOD labels to 2."
  - [corpus]: Weak evidence - no direct corpus support found for this specific mechanism.
- Break condition: If there is no overlap between ID and OOD distributions, or if the hypothesis space cannot achieve zero risk on both ID and OOD data separately.

### Mechanism 2
- Claim: OOD detection is learnable in the separate space D^sep_{XY} under risk if and only if the feature space X is finite.
- Mechanism: In a finite feature space, every data point can be uniquely identified and classified. This allows the construction of algorithms that can perfectly separate ID and OOD data, satisfying the necessary and sufficient conditions for learnability.
- Core assumption: The feature space X is finite, and the hypothesis space contains functions that can achieve zero risk on both ID and OOD data.
- Evidence anchors:
  - [abstract]: "They also identify several necessary and sufficient conditions for learnability in practical scenarios."
  - [section]: "Theorem 14 Suppose that Condition 3 holds and the hypothesis space H is FCNN-based or score-based, i.e., H = Hσ,q or H = H_in ◦ H_b, where H_in is an ID hypothesis space, H_b = Hσ,λ,q,E and H = H_in ◦ H_b is introduced below Eq. (6), here E is Eq. (7), (8) or (9). Then There is a sequence q = (l_1, ..., l_g) such that OOD detection is learnable under risk in the separate space D^sep_{XY} for H if and only if |X| < +∞."
  - [corpus]: Weak evidence - no direct corpus support found for this specific mechanism.
- Break condition: If the feature space X is infinite, or if the hypothesis space cannot achieve zero risk on both ID and OOD data.

### Mechanism 3
- Claim: The learnability of OOD detection under AUC in the separate space D^sep_{XY} depends on the existence of a ranking function that can perfectly separate ID and OOD data.
- Mechanism: AUC measures the ability of a ranking function to separate ID and OOD data. If there exists a ranking function that can perfectly separate the two classes, then OOD detection is learnable under AUC.
- Core assumption: The ranking function space contains functions that can perfectly separate ID and OOD data, and the feature space X is finite.
- Evidence anchors:
  - [abstract]: "They prove several impossibility theorems showing that OOD detection is not learnable in the total space or separate space under certain conditions."
  - [section]: "Theorem 10 Given a separate ranking function space R, if |X| < +∞, then OOD detection is learnable under AUC in the separate space D^sep_{XY} for R if and only if AUC-based Realizability Assumption holds."
  - [corpus]: Weak evidence - no direct corpus support found for this specific mechanism.
- Break condition: If there is no ranking function that can perfectly separate ID and OOD data, or if the feature space X is infinite.

## Foundational Learning

- Concept: PAC learnability
  - Why needed here: PAC learnability provides a framework for understanding the generalization ability of OOD detection algorithms. It allows us to characterize the conditions under which OOD detection is learnable.
  - Quick check question: What is the difference between PAC learnability and agnostic PAC learnability?

- Concept: Hypothesis space and ranking function space
  - Why needed here: The hypothesis space and ranking function space determine the capacity of the OOD detection algorithm. The learnability of OOD detection depends on the relationship between these spaces and the domain space.
  - Quick check question: How does the VC dimension of a hypothesis space relate to its capacity?

- Concept: Risk and AUC metrics
  - Why needed here: Risk and AUC are two commonly used metrics for evaluating OOD detection algorithms. The learnability of OOD detection is studied under these two metrics.
  - Quick check question: What is the difference between threshold-dependent and threshold-independent metrics?

## Architecture Onboarding

- Component map: Domain space (DXIYI, DXOYO, DXY) -> Hypothesis space H (FCNN-based, score-based) -> Ranking function space R -> Loss function ℓ -> Risk and AUC metrics

- Critical path:
  1. Define the domain space and hypothesis space.
  2. Check if the necessary conditions for learnability hold.
  3. If the conditions hold, construct an algorithm that can learn to detect OOD data.
  4. Evaluate the performance of the algorithm using risk and AUC metrics.

- Design tradeoffs:
  - The choice of hypothesis space and ranking function space affects the capacity of the OOD detection algorithm.
  - The use of risk or AUC as the evaluation metric affects the learnability of OOD detection.

- Failure signatures:
  - If the necessary conditions for learnability do not hold, then OOD detection is not learnable.
  - If the algorithm cannot achieve zero risk on both ID and OOD data, then OOD detection is not learnable.

- First 3 experiments:
  1. Test the learnability of OOD detection in the total space with a simple hypothesis space.
  2. Test the learnability of OOD detection in the separate space with a finite feature space.
  3. Test the learnability of OOD detection under AUC in the separate space with a ranking function space that can perfectly separate ID and OOD data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can OOD detection be made learnable in near-OOD scenarios where ID and OOD distributions overlap?
- Basis in paper: [inferred] The paper states that near-OOD detection tasks may imply the overlap condition (Definition 5), and Lemma 1 and Theorem 18 imply that near-OOD detection may not be learnable under risk.
- Why unresolved: The paper conjectures that when K > 1, OOD detection might be learnable in some special cases with overlap, but does not provide a definitive answer.
- What evidence would resolve it: Theoretical proof or empirical evidence showing learnability or unlearnability of OOD detection under risk or AUC in near-OOD scenarios with overlapping ID and OOD distributions.

### Open Question 2
- Question: What is a stronger necessary condition for the learnability of OOD detection under AUC that could lead to an equivalence with the AUC-based Realizability Assumption?
- Basis in paper: [explicit] The paper mentions that Condition 2 is a weaker necessary condition for learnability of OOD detection under AUC than Condition 1 for learnability under risk, and that a stronger necessary condition is needed to obtain a similar equivalence to Theorem 16.
- Why unresolved: The paper does not provide a stronger necessary condition for learnability under AUC.
- What evidence would resolve it: Discovery of a new condition that is both necessary and sufficient for the learnability of OOD detection under AUC in practical scenarios.

### Open Question 3
- Question: How does the learnability of OOD detection under risk or AUC change when considering robustness to outliers or adversarial attacks?
- Basis in paper: [explicit] The paper mentions that studying the robustness of OOD detection based on robust statistics is a promising future research direction.
- Why unresolved: The paper does not investigate the impact of outliers or adversarial attacks on the learnability of OOD detection.
- What evidence would resolve it: Theoretical analysis or empirical studies showing the effects of outliers or adversarial attacks on the learnability of OOD detection under risk or AUC in various domain spaces and hypothesis spaces.

## Limitations
- The theoretical framework relies heavily on idealized assumptions about the separation between ID and OOD distributions that may not hold in practice.
- Learnability conditions require strong assumptions about hypothesis space capacity and feature space finiteness that may not be realistic for high-dimensional data.
- The proofs depend on specific properties of loss functions and ranking functions that are not fully specified in the paper.

## Confidence
- High Confidence: The impossibility theorems under risk in the total space (Theorem 4) and under AUC in the separate space (Theorem 7) have clear mathematical foundations with minimal assumptions.
- Medium Confidence: The learnability conditions in finite feature spaces (Theorems 8, 10, 14) are well-established but may have limited practical applicability due to the finiteness requirement.
- Medium Confidence: The density-based space results (Theorems 11-13) provide valuable insights but depend on the Realizability Assumption, which may not hold in many practical scenarios.

## Next Checks
1. **Empirical Validation**: Test the theoretical predictions using synthetic datasets where ID and OOD distributions can be precisely controlled to verify the overlap conditions and learnability boundaries.
2. **Approximation Analysis**: Investigate how closely real-world OOD detection methods approximate the ideal conditions in the theory, particularly examining the impact of finite sample sizes and model capacity constraints.
3. **Extension to Relaxed Conditions**: Explore modifications to the theory that could accommodate practical scenarios where the strict conditions are violated, such as allowing for bounded but non-zero risk or approximate separation between distributions.