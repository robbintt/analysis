---
ver: rpa2
title: 'TinySV: Speaker Verification in TinyML with On-device Learning'
arxiv_id: '2406.01655'
source_url: https://arxiv.org/abs/2406.01655
tags:
- learning
- speaker
- tinyml
- solution
- proposed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TinySV, a new TinyML task for speaker verification
  on tiny devices, which requires on-device learning with few and unlabelled training
  data. The proposed solution uses a two-layer hierarchical approach combining keyword
  spotting and adaptive speaker verification modules.
---

# TinySV: Speaker Verification in TinyML with On-device Learning

## Quick Facts
- arXiv ID: 2406.01655
- Source URL: https://arxiv.org/abs/2406.01655
- Authors: Massimo Pavan; Gioele Mombelli; Francesco Sinacori; Manuel Roveri
- Reference count: 40
- Key outcome: Introduces TinySV, a new TinyML task for speaker verification on tiny devices requiring on-device learning with few and unlabelled training data, achieving competitive accuracy with low memory (391.92 kB RAM) and power (62.7 mW) consumption.

## Executive Summary
This paper introduces TinySV, a novel TinyML task for speaker verification on resource-constrained devices that requires on-device learning with limited, unlabelled training data. The proposed solution employs a two-layer hierarchical architecture combining keyword spotting and adaptive speaker verification modules. Experimental results on a newly collected dataset demonstrate the approach outperforms existing tiny device methods while maintaining competitive accuracy with state-of-the-art systems. An on-device implementation on an Infineon PSoC 62S2 board validates the feasibility and efficiency of the approach.

## Method Summary
The TinySV method uses a two-layer hierarchical approach combining keyword spotting (KS) and adaptive speaker verification (ASV) modules. The KS module detects a pre-determined keyword in audio segments, forwarding detected segments to the ASV module. The ASV module employs a one-class, few-shot learning approach using d-vectors extracted from a fixed neural network, with best-match cosine similarity for enrollment and inference. Both modules share MFCC preprocessing to reduce computational overhead. The system is trained and evaluated on a dataset of 376 recordings from 4 speakers pronouncing "Hey Cypress."

## Key Results
- The proposed solution outperforms existing tiny device speaker verification approaches while remaining competitive with state-of-the-art methods
- On-device implementation on Infineon PSoC 62S2 board achieves low memory requirements (391.92 kB RAM) and power consumption (62.7 mW)
- The hierarchical cascade architecture reduces computational load by approximately half through KS filtering
- The system achieves speaker verification using few enrollment samples per speaker with one-class, few-shot learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hierarchical cascade of KS and ASV modules reduces total computational load.
- Mechanism: The KS module acts as a filter, only forwarding segments containing the keyword to the more expensive ASV module, halving inference cycles.
- Core assumption: Keyword presence is sparse relative to continuous audio input.
- Evidence anchors:
  - [abstract]: "The KS model is used to determine if the audio segment It under inspection includes the pre-determined keyword k. If k is detected in It, the audio segment is forwarded to the ASV module"
  - [section]: "since the two algorithms are executed in a hierarchical fashion, the ASV module is executed only when the keyword k is detected by the KS module. In this sense, the KS module acts as a filter, almost halving the amount of computation"

### Mechanism 2
- Claim: One-class, few-shot ASV learning works with limited enrollment data.
- Mechanism: ASV module uses best-match cosine similarity over a small enrollment set of d-vectors from the enrolled speaker, avoiding need for negative samples.
- Core assumption: Speaker-specific d-vector embeddings are sufficiently discriminative even with few samples.
- Evidence anchors:
  - [abstract]: "The proposed solution outperforms existing tiny device approaches while being competitive with state-of-the-art methods"
  - [section]: "The ASV module consists of a fixed d-vector extractor Φf(•) and an adaptive instance-based model... works in a one-class manner"

### Mechanism 3
- Claim: Shared MFCC preprocessing enables efficient feature extraction for both KS and ASV.
- Mechanism: Both modules use the same MFCC spectrogram input, avoiding duplicate computation and memory allocation.
- Core assumption: MFCC features are equally useful for keyword spotting and speaker verification.
- Evidence anchors:
  - [section]: "It is pre-processed and transformed into a Mel-frequency cepstral coefficients (MFCC) spectrogram Pt through a module called MFCC extractor. In order to reduce the number of operations needed to execute the pipeline on-device, the preprocessing is shared among the keyword spotting and the speaker verification module"

## Foundational Learning

- Concept: Mel-frequency cepstral coefficients (MFCC)
  - Why needed here: MFCCs reduce dimensionality of raw audio while preserving phonetic and speaker-specific information; used as shared preprocessing input.
  - Quick check question: How many MFCC coefficients are extracted per frame in the proposed TinySV pipeline?

- Concept: Convolutional neural network feature extraction
  - Why needed here: CNNs in KS and ASV modules learn hierarchical audio features; KS detects presence of keyword, ASV extracts speaker embeddings (d-vectors).
  - Quick check question: What is the output dimension of the d-vector from Φf in the on-device implementation?

- Concept: Cosine similarity for instance-based classification
  - Why needed here: ASV module compares new d-vector to stored enrollment d-vectors; best-match cosine similarity allows one-class, few-shot learning without retraining.
  - Quick check question: What is the formula used to compute the similarity between a test d-vector and the enrollment set?

## Architecture Onboarding

- Component map:
  - MFCC extractor (shared preprocessing) -> KS CNN module (keyword detection) -> (if keyword) ASV CNN (d-vector extraction) + instance-based classifier (cosine similarity)
- Critical path:
  - Audio capture → MFCC → KS inference → (if keyword) ASV d-vector extraction → cosine similarity → decision
- Design tradeoffs:
  - Memory vs accuracy: Larger enrollment set improves verification but increases memory.
  - Model size vs speed: Larger CNNs may improve accuracy but exceed memory/CPU limits.
  - Precision vs resource usage: Lower bit-width storage saves memory but may degrade accuracy.
- Failure signatures:
  - High false negatives: KS model too strict or d-vector similarity threshold too high.
  - High false positives: KS too permissive or d-vector similarity threshold too low.
  - Memory exhaustion: Enrollment set grows beyond available RAM.
- First 3 experiments:
  1. Test KS module accuracy on keyword detection with varying threshold; measure false positive/negative rates.
  2. Test ASV module with increasing enrollment set sizes; measure accuracy and memory usage.
  3. Run full cascade on-device; measure total execution time, power consumption, and verify correctness under real microphone input.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed TinySV solution scale with the number of enrolled speakers beyond the four tested in the dataset?
- Basis in paper: [explicit] The paper states that "different speakers in the dataset were used one at a time as SE" and tested with four speakers, but does not explore scalability to larger numbers of enrolled speakers.
- Why unresolved: The experimental results only provide performance metrics for up to four enrolled speakers, leaving uncertainty about the solution's effectiveness in real-world scenarios with potentially many more enrolled users.
- What evidence would resolve it: Conducting experiments with datasets containing a larger number of enrolled speakers and evaluating the accuracy, EER, and AUC metrics for these scenarios would provide insights into the scalability of the solution.

### Open Question 2
- Question: What are the potential trade-offs between the threshold τ value and the system's performance in terms of false positives and false negatives, and how can these be optimized for different use cases?
- Basis in paper: [explicit] The paper mentions that "the tunable parameter τ was set to the threshold value corresponding to the Equal Error Rate for the speaker S computed on the validation set" and that the threshold can be tuned by the user to control the false positive vs false negative trade-off.
- Why unresolved: While the paper acknowledges the importance of the threshold τ, it does not provide a detailed analysis of how different τ values affect the system's performance in terms of false positives and false negatives, nor does it offer guidance on optimizing τ for specific use cases.
- What evidence would resolve it: Conducting experiments with different τ values and analyzing the resulting false positive and false negative rates, along with their impact on overall system performance, would help identify optimal threshold settings for various use cases.

### Open Question 3
- Question: How does the proposed TinySV solution compare to other speaker verification approaches in terms of computational efficiency and memory usage when implemented on different tiny devices with varying specifications?
- Basis in paper: [explicit] The paper provides detailed information on the memory requirements and power consumption of the TinySV solution when implemented on the Infineon PSoC 62S2 board, but does not compare its performance to other speaker verification approaches on the same or different devices.
- Why unresolved: While the paper demonstrates the efficiency of the proposed solution on a specific device, it does not provide a comprehensive comparison with other speaker verification methods in terms of computational efficiency and memory usage across various tiny devices.
- What evidence would resolve it: Implementing and evaluating the proposed TinySV solution and other state-of-the-art speaker verification approaches on a range of tiny devices with different specifications, and comparing their computational efficiency and memory usage, would provide insights into the relative performance of the proposed solution.

## Limitations

- Limited dataset: Results are based on only 376 recordings from 4 speakers, limiting generalizability to larger, more diverse speaker populations
- Single platform validation: On-device implementation results are shown only on Infineon PSoC 62S2 board, making cross-platform portability unclear
- Real-world keyword frequency unknown: The computational savings from KS filtering assumes sparse keyword occurrence, but real-world patterns are uncharacterized
- Limited enrollment data characterization: The robustness of one-class, few-shot learning with extremely limited enrollment data (exact sample count unspecified) remains uncertain

## Confidence

- **High confidence**: The hierarchical cascade architecture reducing computational load through KS filtering, and the shared MFCC preprocessing mechanism
- **Medium confidence**: The effectiveness of one-class, few-shot ASV learning with limited enrollment data, based on reported competitive accuracy with state-of-the-art methods
- **Medium confidence**: On-device feasibility demonstrated through specific hardware metrics (memory: 391.92 kB RAM, power: 62.7 mW), though limited to one platform

## Next Checks

1. **Dataset generalization test**: Evaluate TinySV on a larger, more diverse speaker dataset (e.g., 20+ speakers with 50+ samples each) to assess scalability and robustness beyond the current 4-speaker setup

2. **Cross-platform validation**: Implement TinySV on at least two additional TinyML hardware platforms (e.g., Arduino Nano 33 BLE Sense and STM32) to verify portability and identify platform-specific constraints

3. **Real-world keyword frequency analysis**: Deploy the system in a naturalistic setting for 24+ hours to measure actual keyword occurrence patterns and validate whether the KS filtering provides the expected computational savings under realistic usage conditions