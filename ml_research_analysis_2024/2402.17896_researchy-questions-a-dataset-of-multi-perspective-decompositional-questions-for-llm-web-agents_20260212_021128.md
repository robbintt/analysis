---
ver: rpa2
title: 'Researchy Questions: A Dataset of Multi-Perspective, Decompositional Questions
  for LLM Web Agents'
arxiv_id: '2402.17896'
source_url: https://arxiv.org/abs/2402.17896
tags:
- question
- questions
- transportation
- answer
- queries
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Researchy Questions, a dataset of 96k real
  user search queries that are non-factoid, decompositional, and multi-perspective.
  These questions were filtered from search logs using a multi-stage process involving
  classifiers and deduplication to identify complex information needs requiring multi-document
  analysis.
---

# Researchy Questions: A Dataset of Multi-Perspective, Decompositional Questions for LLM Web Agents

## Quick Facts
- arXiv ID: 2402.17896
- Source URL: https://arxiv.org/abs/2402.17896
- Reference count: 40
- Introduces Researchy Questions: a dataset of 96k complex, decompositional search queries requiring multi-document analysis

## Executive Summary
This paper presents Researchy Questions, a novel dataset of 96,000 real user search queries that are characterized as non-factoid, decompositional, and multi-perspective. The dataset was created by filtering search logs from a commercial search engine using a multi-stage process involving classifiers and deduplication to identify complex information needs that require multi-document analysis. The queries are accompanied by hierarchical decompositions into sub-questions and click data over publicly available documents, making it valuable for advancing research in complex question answering and multi-hop reasoning for LLM web agents.

## Method Summary
The Researchy Questions dataset was constructed through a systematic filtering process applied to search logs. The approach involved identifying non-factoid queries that likely represent complex information needs, then decomposing them into hierarchical sub-questions. The filtering pipeline used multiple classifiers to distinguish between simple factoid questions and more complex research-oriented queries. Deduplication steps ensured unique entries, while click data provided validation that these questions represent genuine user information needs. The dataset focuses specifically on English-language queries from a single commercial search engine.

## Key Results
- Users spend significantly more effort on researchy questions compared to simple factoid queries
- Decompositional answering techniques improve performance over direct answering, particularly for long-form questions
- The dataset demonstrates effectiveness for training LLM web agents in complex question answering tasks

## Why This Works (Mechanism)
The dataset works by capturing genuine complex information needs that naturally arise in search behavior. By focusing on non-factoid, multi-perspective queries that require multiple documents for comprehensive answers, the dataset creates a realistic challenge for LLM agents. The hierarchical decomposition into sub-questions mirrors how humans naturally break down complex research tasks, enabling more effective multi-document analysis. The inclusion of actual click data validates that these represent authentic user information needs rather than artificially constructed examples.

## Foundational Learning

**Non-factoid Question Classification**
- Why needed: Distinguishes simple lookup queries from complex information needs requiring reasoning
- Quick check: Verify classifier accuracy on held-out test set of manually labeled queries

**Hierarchical Decomposition**
- Why needed: Enables breaking complex questions into manageable sub-questions for multi-document analysis
- Quick check: Ensure sub-questions maintain semantic relationship to parent question and cover all aspects

**Multi-document Analysis**
- Why needed: Required for answering questions that need information synthesis across multiple sources
- Quick check: Validate that sub-questions can be answered using distinct document sets

## Architecture Onboarding

**Component Map**
Search Log Database -> Filtering Pipeline (Classifiers) -> Deduplication -> Hierarchical Decomposition -> Click Data Validation -> Final Dataset

**Critical Path**
The critical path flows from search log collection through the multi-stage filtering pipeline, where each classifier serves as a gatekeeper for identifying researchy questions. The decomposition stage is critical as it transforms complex queries into actionable sub-questions for multi-document analysis.

**Design Tradeoffs**
- Language limitation (English-only) vs. dataset quality and consistency
- Single search engine source vs. diversity of query patterns
- Click-based validation vs. potential for incomplete or misleading user behavior signals

**Failure Signatures**
- Over-filtering may exclude legitimate complex queries that don't match classifier patterns
- Under-filtering may include simple factoid questions masquerading as complex queries
- Poor decomposition quality may result in sub-questions that don't adequately cover parent question scope

**First 3 Experiments**
1. Baseline comparison of direct answering vs. decompositional answering on subset of dataset
2. User study validating that identified researchy questions truly represent complex information needs
3. Performance analysis across different question length categories and complexity levels

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Dataset focuses on English-language queries from a single commercial search engine, introducing potential geographic and linguistic biases
- The filtering process may have excluded legitimate complex queries that didn't match classifier criteria
- Dataset size (96k queries) is substantial but relatively small compared to typical web-scale datasets

## Confidence

**High Confidence**
- Dataset creation methodology and filtering pipeline are well-documented and reproducible
- Claim that decompositional approaches improve performance for complex questions is supported by empirical results

**Medium Confidence**
- Assertion that these questions represent "real user search queries" is supported but could benefit from additional user studies
- Claim about users spending significantly more effort on these questions is plausible but could be influenced by confounding factors

**Medium Confidence**
- Dataset's utility for training LLM web agents is theoretically sound, but practical effectiveness would require further testing across diverse application scenarios

## Next Checks
1. Conduct user studies to verify that the identified "researchy questions" truly represent complex information needs and validate the decomposition quality of sub-questions
2. Test the dataset's effectiveness across multiple languages and search engines to assess generalizability beyond the current English-only, single-engine scope
3. Evaluate the impact of different filtering thresholds and classifier configurations on dataset quality and diversity through ablation studies