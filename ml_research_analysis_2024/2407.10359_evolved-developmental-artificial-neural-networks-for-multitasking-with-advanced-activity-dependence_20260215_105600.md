---
ver: rpa2
title: Evolved Developmental Artificial Neural Networks for Multitasking with Advanced
  Activity Dependence
arxiv_id: '2407.10359'
source_url: https://arxiv.org/abs/2407.10359
tags:
- neural
- networks
- work
- parameters
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends the Evolved Developmental Artificial Neural Networks
  (EDANNs) model by incorporating Activity Dependence (AD) on new neural parameters
  beyond bias. While prior work showed marginal improvements using AD on bias only,
  experiments demonstrate that AD applied to neuron health, position, or their combination
  yields stronger performance gains.
---

# Evolved Developmental Artificial Neural Networks for Multitasking with Advanced Activity Dependence

## Quick Facts
- arXiv ID: 2407.10359
- Source URL: https://arxiv.org/abs/2407.10359
- Authors: Yintong Zhang; Jason A. Yoder
- Reference count: 4
- One-line primary result: AD on neuron health or position parameters yields stronger performance gains than AD on bias alone for multitasking EDANNs

## Executive Summary
This work extends Evolved Developmental Artificial Neural Networks (EDANNs) by applying Activity Dependence (AD) to multiple neuron parameters beyond just bias. While prior work showed only marginal improvements using AD on bias, experiments demonstrate that applying AD to neuron health, position, or their combination yields stronger performance gains when solving two tasks simultaneously - cartpole reinforcement learning and bank note authentication classification. The best results came from combining AD on all neuron parameters, suggesting minimal interference between different AD behaviors.

## Method Summary
The method builds on EDANNs, which use Cartesian Genetic Programming (CGP) to encode developmental programs that grow artificial neural networks through iterative development cycles. The key innovation is extending AD from just neuron bias to health and position parameters. During development, the model receives feedback signals that adjust these parameters, allowing the network to dynamically reshape its topology based on environmental conditions. The approach was tested on multitasking scenarios combining cartpole RL with classification tasks.

## Key Results
- AD on neuron health or position parameters produces stronger performance gains than AD on bias alone
- Combining AD on all neuron parameters yields the best results with minimal interference between behaviors
- EDANNs with AD successfully solve two tasks simultaneously while avoiding catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AD on neuron health or position parameters leads to stronger performance gains than AD on bias alone.
- Mechanism: Health and position parameters directly control neuron survival and connectivity, respectively. By allowing environmental feedback to adjust these parameters during development, the network can dynamically reshape its topology to better suit the task.
- Core assumption: Modifying structural parameters (health/position) has a more direct impact on network architecture than adjusting bias values.
- Evidence anchors:
  - [abstract] "experiments demonstrate that AD applied to neuron health, position, or their combination yields stronger performance gains"
  - [section] "The health of a soma can be updated in development cycles and is used to determine whether a soma should die or give birth to another soma"
  - [corpus] Weak evidence - no corpus papers directly address AD parameter selection
- Break condition: If AD on structural parameters leads to unstable network growth or degenerate topologies.

### Mechanism 2
- Claim: Combining AD on multiple neuron parameters produces additive benefits without interference.
- Mechanism: Different neuron parameters serve distinct functions (health for survival, position for connectivity, bias for activation). Adjusting them simultaneously allows the network to optimize multiple aspects of its structure and function in parallel.
- Core assumption: AD behaviors for different parameters operate independently without conflicting.
- Evidence anchors:
  - [abstract] "minimal interference between different AD behaviors"
  - [section] "AD behavior using all neuron parameters is evaluated to see the effects of a possible combination of AD"
  - [corpus] No corpus evidence for parameter combination effects
- Break condition: If parameter combinations create conflicting updates that destabilize network development.

### Mechanism 3
- Claim: AD enables better multitasking by allowing task-specific network restructuring.
- Mechanism: When solving multiple tasks, AD can adjust network parameters based on which sub-network is active, allowing the same physical network to optimize different architectures for different tasks.
- Core assumption: The model can distinguish which sub-network is active and apply appropriate AD signals.
- Evidence anchors:
  - [section] "the original ANN is adjusted by AD regarding the signals present on sub-networks"
  - [abstract] "enabling ANNs to perform multiple tasks while avoiding catastrophic forgetting"
  - [corpus] No corpus evidence for task-specific AD
- Break condition: If AD signals become ambiguous across tasks, causing conflicting updates.

## Foundational Learning

- Concept: Cartesian Genetic Programming (CGP)
  - Why needed here: CGP provides the encoding scheme for evolving developmental programs that control network growth
  - Quick check question: What is the primary difference between direct and indirect encoding in CGP?

- Concept: Catastrophic forgetting
  - Why needed here: The paper addresses this fundamental problem in neural networks when learning multiple tasks
  - Quick check question: How does the developmental approach prevent catastrophic forgetting compared to standard training?

- Concept: Neuroevolution
  - Why needed here: The paper uses evolutionary algorithms to optimize developmental programs
  - Quick check question: What advantage does neuroevolution offer over gradient-based training for this application?

## Architecture Onboarding

- Component map: Genotype (CGP-encoded developmental program) -> Developmental program -> Initial network -> Iterative development -> Mature network -> Task evaluation -> Fitness calculation -> Selection
- Critical path: Genotype → Developmental program → Initial network → Iterative development → Mature network → Task evaluation → Fitness calculation → Selection
- Design tradeoffs: Development cycles vs. computational cost, network complexity vs. generalization, AD parameter selection vs. implementation simplicity
- Failure signatures: Degenerate networks (all neurons dying), stagnant evolution (no improvement across generations), task interference (one task dominating)
- First 3 experiments:
  1. Baseline model without AD solving single task to verify implementation
  2. AD on bias parameter only to reproduce previous results
  3. AD on health parameter to test structural parameter effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relative contribution of AD on soma versus dendrite parameters to overall model performance?
- Basis in paper: [explicit] The paper mentions AD on soma parameters (bias, health, position) shows improvements, and suggests future work on incorporating AD on dendrite parameters
- Why unresolved: The paper only evaluates AD on soma parameters and explicitly states that dendrite parameters have not been tested with AD
- What evidence would resolve it: Experiments comparing AD on dendrite parameters (connectivity, weights, biases) versus soma parameters, measuring relative performance gains

### Open Question 2
- Question: How does the combination of multiple AD behaviors interact—do they exhibit synergy, redundancy, or interference?
- Basis in paper: [explicit] The paper observes that combining all AD parameters yielded the best performance, suggesting "minimal interference between different AD behaviors"
- Why unresolved: The paper doesn't test specific combinations of AD parameters to understand their interactions, only evaluates all parameters together
- What evidence would resolve it: Systematic experiments testing different AD parameter combinations (health+bias, position+health, etc.) to quantify interaction effects

### Open Question 3
- Question: Can evolutionary algorithms optimize the parameterization of AD behavior for specific neural features?
- Basis in paper: [explicit] The authors suggest future work using "an EA to find an optimal AD combination to maximize learning capabilities"
- Why unresolved: This remains purely speculative with no experimental validation attempted
- What evidence would resolve it: Implementation of a secondary evolutionary algorithm that evolves AD parameter settings, comparing performance to hand-designed AD configurations

## Limitations
- Limited experimental validation on only two specific tasks (cartpole and banknote authentication)
- No comparative analysis against standard deep learning approaches for multitasking
- Computational efficiency trade-offs of the developmental approach versus direct training methods are not quantified

## Confidence
- High Confidence: Experimental results showing AD on health and position parameters outperforms AD on bias alone are well-supported by the data
- Medium Confidence: Claim that combining AD on multiple parameters produces additive benefits without interference lacks theoretical justification
- Low Confidence: Assertion that AD enables better multitasking through task-specific network restructuring is primarily theoretical with limited empirical evidence

## Next Checks
1. **Cross-task generalization test**: Evaluate the AD-enhanced EDANN model on a broader set of multitasking scenarios beyond cartpole and classification, including tasks with different input modalities and temporal characteristics.

2. **Comparison with baseline methods**: Implement standard multitasking approaches (e.g., task-specific subnetworks, elastic weight consolidation) and compare performance metrics including accuracy, convergence speed, and computational requirements.

3. **Parameter interference analysis**: Systematically test AD combinations where parameters might conflict (e.g., health vs. position) to identify threshold conditions where interference occurs, and develop mitigation strategies.