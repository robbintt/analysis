---
ver: rpa2
title: Generative Retrieval Meets Multi-Graded Relevance
arxiv_id: '2409.18409'
source_url: https://arxiv.org/abs/2409.18409
tags:
- retrieval
- relevance
- docids
- documents
- pages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of extending generative retrieval
  to multi-graded relevance scenarios, where documents can have varying degrees of
  relevance to queries. The authors propose a framework called GR2 that generates
  distinct and semantically relevant document identifiers (docids) and uses a multi-graded
  constrained contrastive (MGCC) loss to capture the relationships between different
  relevance grades.
---

# Generative Retrieval Meets Multi-Graded Relevance

## Quick Facts
- arXiv ID: 2409.18409
- Source URL: https://arxiv.org/abs/2409.18409
- Reference count: 40
- Key outcome: GR2 achieves up to 14% relative improvements in P@20 on Gov 500K dataset compared to state-of-the-art generative retrieval baselines

## Executive Summary
This paper addresses the challenge of extending generative retrieval to multi-graded relevance scenarios where documents have varying degrees of relevance to queries. The authors propose GR2, a framework that generates distinct and semantically relevant document identifiers (docids) and uses a multi-graded constrained contrastive (MGCC) loss to capture relevance grade relationships. GR2 consists of a regularized fusion approach combining query generation and autoencoder models, plus a multi-graded constrained contrastive training strategy. Experiments show GR2 significantly outperforms state-of-the-art baselines on multi-graded and binary relevance datasets, achieving up to 14% relative improvements in P@20 on Gov 500K. The approach also demonstrates strong performance in low-resource settings.

## Method Summary
GR2 addresses multi-graded generative retrieval through a two-component framework. First, it uses a regularized fusion approach that combines query generation (QG) and autoencoder (AE) models with shared decoder parameters to create semantically relevant and distinct docids. Second, it employs a multi-graded constrained contrastive (MGCC) loss that pulls query representations closer to relevant docids based on their relevance grades while pushing away from irrelevant ones. The model can be trained from scratch or pre-trained on large-scale Wikipedia-based pseudo-pairs with 4 relevance grades, then fine-tuned on downstream tasks. The approach uses T5-base as the backbone and achieves significant performance improvements over existing generative retrieval methods.

## Key Results
- GR2 achieves up to 14% relative improvements in P@20 on Gov 500K dataset
- GR2 significantly outperforms state-of-the-art generative retrieval baselines on multi-graded datasets
- GR2 demonstrates strong performance in low-resource settings, surpassing BM25 on two datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The regularized fusion approach generates docids that are both semantically relevant to documents and sufficiently distinct to differentiate between similar documents.
- Mechanism: Joint optimization of a query generation model (QG) and an autoencoder (AE) model with shared decoder parameters, guided by relevance and distinctness regularization terms.
- Core assumption: Combining QG (which ensures semantic relevance) and AE (which ensures distinctness) in a shared latent space produces optimal docids.
- Evidence anchors:
  - [section]: "To enhance docid distinctness while ensuring its relevance to document semantics, we introduce a regularized fusion approach with two modules: (i) a docid generation module, that produces pseudo-queries based on the original documents as the docids; and (ii) an autoencoder module, that reconstructs the target docids from their corresponding representations. We train them jointly to ensure that the docid representation is close to its corresponding document representation while far from other docid representations."
  - [corpus]: "Found 25 related papers (using 8). Average neighbor FMR=0.466, average citations=0.1." (Weak corpus evidence, only general related work found)
- Break condition: If the shared latent space cannot effectively capture both relevance and distinctness simultaneously, or if the balance between α and β hyperparameters is poorly tuned, docid quality may degrade.

### Mechanism 2
- Claim: The multi-graded constrained contrastive (MGCC) loss captures the relationships between different relevance grades by applying grade-specific penalties and constraints.
- Mechanism: Constructs positive and negative query-docid pairs, then pulls query representations closer to relevant docids based on their relevance grades while pushing them away from irrelevant docids. Uses grade penalty (λ_l) and grade constraint (LMax) to maintain grade ordering.
- Core assumption: By controlling the strength of pull based on relevance grades, the model can learn to rank documents according to their relevance levels.
- Evidence anchors:
  - [section]: "We use a constrained contrastive training strategy to bring the representations of queries and the identifiers of their relevant documents closer together, based on their respective relevance grades."
  - [section]: "The core idea is to pull the representation of a given query in the embedding space towards those of its relevant docids, while simultaneously pushing it away from representations of irrelevant docids in the mini-batch. To maintain the order between relevance grades in the embedding space, the strength of the pull is determined by the relevance grades of the docids."
- Break condition: If grade penalties are not properly calibrated or if the grade constraint is too restrictive, the model may fail to properly distinguish between relevance grades or may create artificial separations.

### Mechanism 3
- Claim: Pre-training on large-scale multi-graded relevance data improves GR2's generalization ability, especially in low-resource settings.
- Mechanism: Constructs Wikipedia-based pseudo-pairs with 4 relevance grades, pre-trains the encoder-decoder architecture using the Ltotal loss, then fine-tunes on downstream tasks.
- Core assumption: Pre-training on diverse, multi-graded relevance data provides the model with better discriminative ability for relevance that transfers to downstream tasks.
- Evidence anchors:
  - [section]: "We also explore the use of GR2 in a pre-training scenario. To construct pre-training data, we use the English Wikipedia to build a set of pseudo-pairs of queries and docids... In this way, a total of 1,180,131 query-docid pairs are obtained, and we pre-train an encoder-decoder architecture using Ltotal as defined in Eq. (10)."
  - [section]: "Between our two methods, GR2P outperforms GR2S, indicating that pre-training on large-scale elaborately constructed multi-graded relevance data, is better than training a single generation model from scratch."
- Break condition: If the pre-training data distribution doesn't match the target domain, or if the pre-training grades don't align with the actual relevance grades, transfer learning may be ineffective.

## Foundational Learning

- Concept: Maximum Likelihood Estimation (MLE) for sequence generation
  - Why needed here: The paper uses MLE as a baseline and compares it against more sophisticated losses like MGCC. Understanding MLE is crucial to grasp why standard Seq2Seq approaches are insufficient for multi-graded relevance.
  - Quick check question: How does MLE treat all correct docids equally, regardless of their relevance grade?

- Concept: Contrastive learning and supervised contrastive loss
  - Why needed here: MGCC builds upon supervised contrastive learning principles. The paper explicitly mentions that MGCC reduces to supervised contrastive loss in binary relevance scenarios.
  - Quick check question: What is the key difference between standard contrastive loss and the MGCC loss proposed in this paper?

- Concept: Docid design and indexing strategies
  - Why needed here: The paper proposes a novel docid design approach that combines QG and AE models. Understanding different docid design strategies (pre-defined vs. learnable) is important for appreciating the innovation.
  - Quick check question: Why might using only pseudo-queries as docids lead to insufficient distinctness between semantically similar documents?

## Architecture Onboarding

- Component map:
  - Docid Generation Module (QG): Takes documents as input, generates pseudo-queries as docids
  - Autoencoder Module (AE): Takes docids as input, reconstructs the original docids
  - Shared Decoder: Used by both QG and AE modules
  - GR Model: Takes queries as input, generates relevant docids using MGCC loss
  - Pre-processing: Constructs pre-training data from Wikipedia with 4 relevance grades

- Critical path: Query → GR Model → Docid Generation → Ranking
  - During training: Documents → Docid Design (QG+AE) → GR Model Training (MGCC)
  - During inference: Query → GR Model → Docid Generation → Ranked Docids

- Design tradeoffs:
  - Fixed docids vs. joint optimization: The paper chooses fixed docids for simplicity and learning stability, but this means docid generation and retrieval are not end-to-end optimized
  - Pre-training vs. from-scratch: GR2P (pre-trained) outperforms GR2S (from scratch), suggesting pre-training is beneficial but requires additional data preparation
  - Complexity vs. performance: MGCC adds complexity over standard MLE but achieves significant performance gains

- Failure signatures:
  - Docid collisions: If the regularized fusion approach fails to ensure distinctness, multiple documents may share the same docid
  - Grade collapse: If MGCC fails, all relevant documents may be treated as equally relevant regardless of their actual grade
  - Overfitting to pre-training data: If pre-training data distribution differs significantly from target data, performance may degrade

- First 3 experiments:
  1. Ablation study on regularized fusion: Compare GR2S with and without the QG+AE fusion (GR2S_RF) to verify docid quality improvement
  2. Ablation study on MGCC components: Remove grade penalty (λ_l) and grade constraint (LMax) separately to understand their individual contributions
  3. Low-resource evaluation: Train GR2 with varying amounts of training data (0%, 2.5K, 5K, 7.5K, 10K queries) to assess scalability and generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the performance of GR2 be affected if the grade penalty in the MGCC loss (λ_l) was set to a learnable parameter rather than a fixed value based on the inverse square of the grade (1/l²)?
- Basis in paper: [explicit] The paper mentions that λ_l is currently set to 1/l² following (98), but does not explore alternative settings for the grade penalty.
- Why unresolved: The paper only explores one fixed setting for the grade penalty and does not investigate the impact of different penalty configurations or learnable parameters.
- What evidence would resolve it: Experiments comparing the performance of GR2 with different fixed values of λ_l and a learnable version of λ_l, evaluated on multi-graded relevance datasets.

### Open Question 2
- Question: What is the impact of using a larger pre-training corpus with more diverse relevance grades on the performance of GR2P compared to GR2S?
- Basis in paper: [explicit] The paper mentions that GR2P outperforms GR2S, attributing this to pre-training on a large-scale dataset with 4 relevance grades. However, it does not explore the impact of corpus size or diversity on performance.
- Why unresolved: The paper only uses one pre-training corpus with a limited number of relevance grades and does not investigate the impact of varying corpus characteristics.
- What evidence would resolve it: Experiments comparing the performance of GR2P with different pre-training corpora, varying in size and the number of relevance grades, evaluated on multi-graded relevance datasets.

### Open Question 3
- Question: How would joint optimization of the docid generation and the retrieval task affect the performance and efficiency of GR2 compared to the current two-stage approach?
- Basis in paper: [explicit] The paper mentions that the current approach uses fixed docids and that joint optimization increases learning difficulty. It does not explore the potential benefits or drawbacks of joint optimization.
- Why unresolved: The paper does not provide any experimental results or analysis on the impact of joint optimization.
- What evidence would resolve it: Experiments comparing the performance and efficiency of GR2 with and without joint optimization of the docid generation and retrieval task, evaluated on multi-graded relevance datasets.

## Limitations
- Performance gains are measured primarily on filtered subsets (500K documents) rather than full collections
- Regularized fusion introduces additional hyperparameters (α, β) that require careful tuning
- MGCC loss adds complexity to training and may be sensitive to grade penalty and constraint hyperparameters

## Confidence
- High confidence: The core methodology (regularized fusion and MGCC loss) is well-specified and reproducible
- Medium confidence: The performance improvements are significant but based on filtered datasets rather than full collections
- Medium confidence: The pre-training approach shows promise but depends heavily on the quality and relevance of the constructed pseudo-pairs

## Next Checks
1. Test GR2 on the complete Gov2 and ClueWeb09-B collections rather than the 500K subsets to assess scalability
2. Conduct a sensitivity analysis on the α, β, λ_l, and radius |r| hyperparameters to understand their impact on performance
3. Evaluate GR2 on non-Wikipedia domains to test the generalizability of the pre-training approach