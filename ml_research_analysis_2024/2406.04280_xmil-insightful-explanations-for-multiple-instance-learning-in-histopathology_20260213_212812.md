---
ver: rpa2
title: 'xMIL: Insightful Explanations for Multiple Instance Learning in Histopathology'
arxiv_id: '2406.04280'
source_url: https://arxiv.org/abs/2406.04280
tags:
- instance
- learning
- explanation
- prediction
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: xMIL is a framework for explaining multiple instance learning (MIL)
  models that explicitly accounts for instance interactions, context sensitivity,
  and both positive and negative evidence. It introduces an evidence function that
  quantifies each instance's impact on the bag label, enabling more faithful and interpretable
  explanations.
---

# xMIL: Insightful Explanations for Multiple Instance Learning in Histopathology

## Quick Facts
- arXiv ID: 2406.04280
- Source URL: https://arxiv.org/abs/2406.04280
- Authors: Julius Hense; Mina Jamshidi Idaji; Oliver Eberle; Thomas Schnake; Jonas Dippel; Laure Ciernik; Oliver Buchstab; Andreas Mock; Frederick Klauschen; Klaus-Robert Müller
- Reference count: 40
- Primary result: xMIL-LRP achieves AUPC of 0.24 vs 0.99 for attention-based methods in TP53 mutation prediction task

## Executive Summary
xMIL is a framework for explaining multiple instance learning (MIL) models that explicitly accounts for instance interactions, context sensitivity, and both positive and negative evidence. It introduces an evidence function that quantifies each instance's impact on the bag label, enabling more faithful and interpretable explanations. xMIL-LRP adapts layer-wise relevance propagation (LRP) to MIL, disentangling instance interactions and distinguishing between supporting and refuting evidence. Experiments on synthetic and histopathology datasets show that xMIL-LRP consistently outperforms existing methods in faithfulness, especially in challenging biomarker prediction tasks.

## Method Summary
xMIL-LRP combines LRP with attention mechanisms to provide faithful explanations for MIL models in histopathology. The method propagates relevance through attention layers using the AH-rule, which treats attention scores as constant weighting matrices during backpropagation. This enables disentanglement of instance interactions and distinction between positive and negative evidence through real-valued relevance scores. The framework scales to large bag sizes and provides context-sensitive explanations by capturing complex relationships between instances.

## Key Results
- xMIL-LRP achieved AUPC of 0.24 versus 0.99 for attention-based methods in TP53 mutation prediction
- The method enables pathologists to extract fine-grained insights about model decision-making
- xMIL-LRP consistently outperforms existing methods in faithfulness across multiple histopathology datasets

## Why This Works (Mechanism)

### Mechanism 1
xMIL-LRP disentangles instance interactions by propagating relevance through attention weights using the AH-rule, which treats attention scores as constant weighting matrices during LRP backpropagation, separating positive and negative contributions of each instance.

### Mechanism 2
The method distinguishes positive and negative evidence through real-valued relevance scores computed as sums of feature-level attributions, allowing positive, negative, and zero values to indicate support, refutation, or irrelevance.

### Mechanism 3
The conservation principle enables consistent interpretation across model layers by enforcing that the sum of instance relevance scores equals the prediction output, allowing relevance analysis at any layer without foundation model propagation.

## Foundational Learning

- Concept: Multiple Instance Learning formulation
  - Why needed here: Understanding how MIL differs from standard supervised learning and what assumptions it makes
  - Quick check question: What is the key difference between MIL and standard supervised learning regarding instance-level labels?

- Concept: Layer-wise Relevance Propagation mechanics
  - Why needed here: xMIL-LRP is fundamentally built on LRP principles for propagating relevance backward through neural networks
  - Quick check question: What does the conservation principle mean in the context of LRP?

- Concept: Attention mechanisms in deep learning
  - Why needed here: xMIL-LRP specifically handles attention mechanisms using the AH-rule, which is crucial for interpreting MIL models
  - Quick check question: How does attention differ from simple weighted averaging in neural networks?

## Architecture Onboarding

- Component map: Histopathology slides → Patch extraction (20x magnification) → Frozen foundation model (CTransPath) → MIL aggregation (AttnMIL/TransMIL) → Prediction → xMIL-LRP backward pass → Instance relevance scores

- Critical path: Foundation model → MIL aggregation → Prediction → xMIL-LRP backward pass → Instance relevance scores

- Design tradeoffs:
  - Accuracy vs interpretability: AddMIL is inherently interpretable but less accurate than post-hoc xMIL-LRP
  - Computational cost: Perturbation methods scale quadratically, while xMIL-LRP is linear in bag size
  - Context sensitivity: xMIL-LRP captures interactions, while single-instance methods do not

- Failure signatures:
  - Zero relevance scores across all instances: Model may not use instance features effectively
  - Highly imbalanced positive/negative scores: Potential model bias or pathological characteristic
  - Relevance scores that don't sum to prediction: Conservation principle violation

- First 3 experiments:
  1. Apply xMIL-LRP to a trained AttnMIL model on CAMELYON16 and visualize heatmaps
  2. Compare xMIL-LRP AUPC scores against attention and single-instance methods on NSCLC
  3. Test context sensitivity by creating bags with known instance interactions and verifying relevance changes

## Open Questions the Paper Calls Out

### Open Question 1
How does xMIL-LRP perform compared to other explanation methods when applied to multi-modal inputs, such as combining histopathology images with genomic data? The paper mentions that the approach can be directly transferred to other problem settings that require explaining complex MIL models, e.g., in video, audio, or text domains, but focuses on histopathology images without exploring multi-modal applications.

### Open Question 2
Can xMIL-LRP be adapted to handle more complex instance interactions, such as higher-order interactions beyond pairwise interactions? The paper discusses the importance of considering instance interactions but focuses on pairwise interactions without exploring higher-order relationships.

### Open Question 3
How does the performance of xMIL-LRP vary across different foundation models for histopathology, such as CTransPath, Rudolfv, and AIN? The paper uses the pre-trained CTransPath foundation model but mentions other foundation models without exploring their impact on xMIL-LRP's performance.

## Limitations
- The evaluation relies heavily on synthetic bag generation for ground truth, which may not fully capture real histopathology complexity
- The method's performance depends critically on the stability of attention weights, yet unstable attention scenarios aren't extensively explored
- Conservation principle's robustness across different MIL architectures and foundation models remains untested beyond the specific CTransPath implementation

## Confidence
- **High confidence**: The mechanism by which xMIL-LRP disentangles instance interactions through the AH-rule and distinguishes positive/negative evidence through real-valued relevance scores
- **Medium confidence**: The generalizability of xMIL-LRP to different MIL architectures and foundation models
- **Medium confidence**: The conservation principle's practical utility in enabling layer-agnostic analysis

## Next Checks
1. **Robustness testing**: Evaluate xMIL-LRP's performance when attention weights are intentionally perturbed or when the foundation model is replaced with alternative architectures
2. **Real-world validation**: Apply xMIL-LRP to clinical datasets where ground truth instance labels are available through expert annotation
3. **Computational benchmarking**: Systematically compare wall-clock time and memory requirements of xMIL-LRP against perturbation-based methods across varying bag sizes and model architectures