---
ver: rpa2
title: Knowledge Distillation of Black-Box Large Language Models
arxiv_id: '2401.07013'
source_url: https://arxiv.org/abs/2401.07013
tags:
- proxy
- teacher
- knowledge
- black-box
- distillation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of distilling knowledge from
  proprietary large language models (LLMs) like GPT-4 to smaller, open-source models
  when internal states are inaccessible. The proposed Proxy-KD method introduces a
  proxy model between the black-box teacher and the student, aligning the proxy with
  the teacher using supervised fine-tuning and preference optimization.
---

# Knowledge Distillation of Black-Box Large Language Models

## Quick Facts
- arXiv ID: 2401.07013
- Source URL: https://arxiv.org/abs/2401.07013
- Reference count: 31
- Primary result: Llama-2-7B student achieves 56.78% accuracy with Proxy-KD versus 53.66% with vanilla black-box KD

## Executive Summary
This paper addresses the challenge of distilling knowledge from proprietary large language models (LLMs) like GPT-4 to smaller, open-source models when internal states are inaccessible. The proposed Proxy-KD method introduces a proxy model between the black-box teacher and the student, aligning the proxy with the teacher using supervised fine-tuning and preference optimization. The student then learns from both the teacher's outputs and the proxy's aligned output distributions, with sample-level weighting to focus on well-aligned samples. Experiments show Proxy-KD outperforms both traditional black-box and white-box knowledge distillation methods across multiple benchmarks.

## Method Summary
Proxy-KD is a two-stage distillation framework designed for black-box LLMs. First, a proxy model is trained using supervised fine-tuning and preference optimization to align with the teacher model's outputs and preferences. Second, the student model learns from both the teacher's outputs and the proxy's output distributions, with sample-level weighting to prioritize well-aligned samples. This approach bridges the gap between inaccessible teacher states and the student model, enabling effective knowledge transfer without requiring access to internal model parameters.

## Key Results
- Llama-2-7B student achieves 56.78% accuracy with Proxy-KD versus 53.66% with vanilla black-box KD
- Consistent improvements across multiple datasets including ARC, GSM8K, HumanEval, and TruthfulQA
- Superior performance compared to both traditional black-box and white-box knowledge distillation methods

## Why This Works (Mechanism)
The method works by creating an intermediary proxy model that learns to approximate the black-box teacher's behavior through supervised fine-tuning and preference optimization. This proxy model serves as a bridge, providing accessible output distributions that the student can learn from. The sample-level weighting mechanism ensures the student focuses on samples where the proxy is well-aligned with the teacher, improving overall distillation quality. This approach effectively addresses the challenge of inaccessible teacher states while maintaining high fidelity in knowledge transfer.

## Foundational Learning
- Knowledge Distillation: Why needed - to transfer knowledge from large models to smaller, more efficient models; Quick check - understanding how logits or output distributions are used as soft targets
- Preference Optimization: Why needed - to align model outputs with human preferences or quality criteria; Quick check - familiarity with reinforcement learning from human feedback (RLHF) concepts
- Supervised Fine-Tuning: Why needed - to adapt models to specific tasks or behaviors using labeled data; Quick check - understanding of standard fine-tuning procedures

## Architecture Onboarding
- Component map: Teacher (black-box LLM) -> Proxy Model -> Student Model
- Critical path: Proxy model training via SFT + preference optimization -> Student training with sample weighting
- Design tradeoffs: Computational overhead of proxy model vs. improved distillation quality; trade-off between proxy alignment accuracy and training time
- Failure signatures: Poor proxy alignment leading to suboptimal student performance; sample weighting issues causing focus on low-quality samples
- First experiments:
  1. Train proxy model with SFT only, compare to proxy with SFT + preference optimization
  2. Test different sample weighting strategies (e.g., uniform vs. confidence-based)
  3. Evaluate impact of proxy model size on distillation performance

## Open Questions the Paper Calls Out
None

## Limitations
- Proxy model effectiveness depends on quality and diversity of preference data
- Computational overhead may limit practical applicability for resource-constrained settings
- Evaluation focuses primarily on accuracy metrics, not computational efficiency or robustness

## Confidence
- High confidence: The core methodology of using a proxy model with preference optimization is technically sound and well-motivated
- Medium confidence: The empirical improvements over baselines are demonstrated, though results are limited to specific model architectures and datasets
- Medium confidence: The sample-level weighting approach is reasonable but may require domain-specific tuning in practice

## Next Checks
1. Test Proxy-KD performance with teacher-student size gaps beyond the tested configurations (e.g., GPT-4 distilling to much smaller models)
2. Evaluate the method's robustness to different types and qualities of preference data to assess practical deployment scenarios
3. Benchmark computational efficiency and inference latency compared to traditional KD methods to assess real-world applicability