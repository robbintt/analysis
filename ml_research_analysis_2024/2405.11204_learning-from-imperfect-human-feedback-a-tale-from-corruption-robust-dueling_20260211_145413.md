---
ver: rpa2
title: 'Learning from Imperfect Human Feedback: a Tale from Corruption-Robust Dueling'
arxiv_id: '2405.11204'
source_url: https://arxiv.org/abs/2405.11204
tags:
- feedback
- regret
- corruption
- lemma
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies Learning from Imperfect Human Feedback (LIHF)\
  \ in dueling bandits, where human feedback can be imperfect and decay over time.\
  \ The authors cast this as a continuous-action dueling bandit problem under decaying\
  \ corruption (O(t^\u03C1-1) at round t), motivated by the observation that human\
  \ feedback often improves with interaction."
---

# Learning from Imperfect Human Feedback: a Tale from Corruption-Robust Dueling

## Quick Facts
- **arXiv ID**: 2405.11204
- **Source URL**: https://arxiv.org/abs/2405.11204
- **Reference count**: 40
- **Primary result**: Establishes regret lower bound of Ω(d max{√T, T^ρ}) for learning from imperfect human feedback in dueling bandits, develops RoSMID algorithm achieving near-optimal regret Õ(d max{√T, T^ρ})

## Executive Summary
This paper studies Learning from Imperfect Human Feedback (LIHF) in dueling bandits, where human feedback can be imperfect and decay over time. The authors cast this as a continuous-action dueling bandit problem under decaying corruption (O(t^ρ-1) at round t), motivated by the observation that human feedback often improves with interaction. The key insight is that while human feedback is imperfect, its decaying nature doesn't make the problem fundamentally easier than arbitrary corruption.

The main contributions include establishing a regret lower bound of Ω(d max{√T, T^ρ}) for LIHF, developing the RoSMID algorithm achieving near-optimal regret Õ(d max{√T, T^ρ}), and introducing a novel framework for analyzing gradient-based dueling bandit algorithms under corruption. Experiments validate the theoretical findings, showing RoSMID outperforms baselines like Doubler and Sparring under various corruption levels.

## Method Summary
The paper introduces RoSMID (Robustified Stochastic Mirror Descent for Imperfect Dueling), a gradient-based algorithm that uses carefully tuned learning rates to handle decaying corruption in human feedback. The algorithm maintains a distribution over actions and updates it using gradient information from pairwise comparisons, while being robust to corrupted feedback. The key technical innovation is the use of a learning rate η_ρ = √log T / (dT max{0.5,ρ}) that balances exploration and robustness to corruption.

The framework also introduces new analysis techniques for gradient-based dueling bandit algorithms under corruption, which can be applied to other algorithms like DBGD. The analysis assumes the corruption decay rate ρ is known and that the action space is compact with certain smoothness conditions.

## Key Results
- Establishes Ω(d max{√T, T^ρ}) regret lower bound for LIHF, showing it can be as hard as arbitrary corruption
- Develops RoSMID algorithm achieving near-optimal Õ(d max{√T, T^ρ}) regret
- Introduces novel framework for analyzing gradient-based dueling bandit algorithms under corruption
- Experiments show RoSMID outperforms baselines like Doubler and Sparring under various corruption levels

## Why This Works (Mechanism)
The algorithm works by carefully tuning the learning rate to account for the decaying corruption rate. The learning rate η_ρ = √log T / (dT max{0.5,ρ}) ensures sufficient exploration while being robust to corrupted feedback. The mirror descent framework naturally handles the continuous action space and provides stability under corruption.

## Foundational Learning
- **Continuous-action dueling bandits**: Needed for modeling real-valued preference feedback; quick check: action space is a d-dimensional ball
- **Decaying corruption model**: Captures how human feedback improves with interaction; quick check: corruption scales as O(t^ρ-1) at round t
- **Mirror descent optimization**: Provides the algorithmic framework; quick check: uses Bregman divergence for stability
- **Gradient-based dueling bandits**: Enables efficient learning from pairwise comparisons; quick check: relies on unbiased gradient estimates

## Architecture Onboarding

**Component Map**: Environment -> Corruption Generator -> RoSMID Algorithm -> Action Distribution -> Comparison Feedback

**Critical Path**: The algorithm maintains a distribution over actions, samples actions for comparison, receives (potentially corrupted) feedback, estimates gradients, and updates the distribution using mirror descent with carefully tuned learning rates.

**Design Tradeoffs**: The main tradeoff is between exploration (achieved through the learning rate) and robustness to corruption. Higher learning rates enable faster learning but are more sensitive to corruption. The choice of ρ in the learning rate balances these competing concerns.

**Failure Signatures**: If the assumed corruption rate ρ doesn't match reality, the algorithm may either explore too little (missing optimal actions) or be too sensitive to noise. Poor initialization of the action distribution can also lead to slow convergence.

**3 First Experiments**:
1. Test RoSMID with known corruption rate ρ against baselines under varying corruption levels
2. Evaluate sensitivity to incorrect estimates of ρ by perturbing the assumed corruption rate
3. Compare RoSMID with and without corruption-robust modifications to isolate the benefit of the robustness techniques

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes corruption decay rate ρ is known, which may not hold in practice
- Experimental validation limited to synthetic settings without real human feedback data
- Highly technical proof techniques with assumptions (compact action space, smoothness) that may limit practical applicability

## Confidence

| Claim | Confidence |
|-------|------------|
| Regret lower bound of Ω(d max{√T, T^ρ}) | High |
| RoSMID achieves Õ(d max{√T, T^ρ}) regret | High |
| Framework applies to other gradient-based dueling algorithms | Medium |
| Practical robustness to real human feedback | Medium |

## Next Checks
1. Implement an online estimator for the corruption decay rate ρ and evaluate RoSMID's performance when ρ is unknown
2. Test RoSMID with actual human feedback data from real-world applications to assess practical robustness
3. Extend the corruption-robust framework to other dueling bandit algorithms beyond DBGD to verify its broader applicability