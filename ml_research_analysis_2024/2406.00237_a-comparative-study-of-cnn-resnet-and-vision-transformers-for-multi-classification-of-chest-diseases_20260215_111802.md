---
ver: rpa2
title: A Comparative Study of CNN, ResNet, and Vision Transformers for Multi-Classification
  of Chest Diseases
arxiv_id: '2406.00237'
source_url: https://arxiv.org/abs/2406.00237
tags: []
core_contribution: 'This study compares CNNs, ResNet, and Vision Transformers (ViTs)
  for multi-label classification of 14 diseases in chest X-ray images from the NIH
  dataset. Three ViT variants were evaluated: ViT-v1/32 and ViT-v2/32 (trained from
  scratch), and ViT-ResNet/16 (pre-trained on ImageNet-21k).'
---

# A Comparative Study of CNN, ResNet, and Vision Transformers for Multi-Classification of Chest Diseases

## Quick Facts
- arXiv ID: 2406.00237
- Source URL: https://arxiv.org/abs/2406.00237
- Reference count: 10
- Primary result: ResNet achieved 93% accuracy, ViT-ResNet (pre-trained) reached 94% with 10% faster training, while standard CNNs scored 91%

## Executive Summary
This study systematically compares CNNs, ResNet, and Vision Transformers for multi-label classification of 14 chest diseases using the NIH ChestX-ray14 dataset. The researchers evaluated three ViT variants: ViT-v1/32 and ViT-v2/32 trained from scratch, and ViT-ResNet/16 pre-trained on ImageNet-21k. ResNet emerged as the top performer with 93% accuracy across all metrics, while ViT-ResNet achieved 94% accuracy with faster training and superior ROC curves. The results demonstrate that pre-training is crucial for ViT performance in medical imaging applications, as non-pre-trained ViTs scored significantly lower. CNNs showed the weakest performance at 91% accuracy, suggesting that deeper architectures with attention mechanisms offer advantages for this complex classification task.

## Method Summary
The study employed a comprehensive evaluation framework using the NIH ChestX-ray14 dataset containing 112,120 frontal-view X-ray images across 14 disease categories. Researchers implemented CNNs using transfer learning from ResNet-50, trained standard ViTs (ViT-v1/32 and ViT-v2/32) from scratch, and developed a hybrid ViT-ResNet/16 model pre-trained on ImageNet-21k. All models were trained for 20 epochs with identical data preprocessing and augmentation strategies. Performance was evaluated using accuracy, precision, recall, F1-score, and ROC curves across the multi-label classification task. The study controlled for architectural differences by maintaining consistent training protocols while varying only the model architecture and pre-training status.

## Key Results
- ResNet achieved the highest overall accuracy at 93% across all evaluation metrics
- ViT-ResNet (pre-trained on ImageNet-21k) reached 94% accuracy with 10% faster training time
- Standard CNNs performed worst at 91% accuracy, while non-pre-trained ViTs scored lower than ResNet
- ViT-ResNet demonstrated superior ROC curves compared to all other architectures

## Why This Works (Mechanism)
The performance differences stem from architectural characteristics and pre-training benefits. ResNet's deep residual connections enable effective gradient flow and feature learning across many layers, preventing degradation in deep networks. Vision Transformers leverage self-attention mechanisms to capture global contextual relationships between image regions, which is particularly valuable for identifying disease patterns that span large areas of chest X-rays. Pre-training on ImageNet-21k provides ViT-ResNet with rich feature representations learned from 14 million natural images, accelerating convergence and improving generalization. The attention mechanism's ability to focus on relevant image regions while ignoring background noise contributes to better discrimination between similar disease presentations.

## Foundational Learning
- Multi-label classification: Classification task where each instance can belong to multiple classes simultaneously; needed to handle multiple concurrent diseases in chest X-rays; quick check: verify output layer uses sigmoid activation with binary cross-entropy loss
- Transfer learning: Using pre-trained models on related tasks to improve performance; essential for reducing training data requirements and improving convergence; quick check: confirm source dataset relevance to target domain
- Self-attention mechanism: Allows models to weigh relationships between different image regions; critical for capturing long-range dependencies in medical images; quick check: verify attention map visualization shows disease-relevant regions
- ROC curves: Graphical plot showing true positive rate vs false positive rate; important for evaluating classifier performance across thresholds; quick check: ensure AUC values are reported alongside ROC curves
- Data augmentation: Techniques to artificially expand training data through transformations; necessary for preventing overfitting in medical imaging; quick check: verify augmentation diversity and appropriateness for medical images

## Architecture Onboarding

**Component map:** Input X-ray -> Data preprocessing -> Augmentation -> CNN/ResNet/ViT layers -> Classification head -> Output probabilities

**Critical path:** Data pipeline (preprocessing + augmentation) -> Model backbone (CNN/ResNet/ViT) -> Classification head (dense layers) -> Loss computation (binary cross-entropy)

**Design tradeoffs:** ResNet balances depth with residual connections for stable training; ViTs trade parameter efficiency for global context capture; pre-training provides performance boost but requires compatible feature spaces; computational cost increases with attention mechanisms

**Failure signatures:** Overfitting on NIH dataset (poor generalization); attention focusing on irrelevant regions; convergence issues in non-pre-trained ViTs; class imbalance affecting minority disease detection

**First experiments:** 1) Test model performance on a held-out validation set from the same distribution, 2) Evaluate cross-dataset generalization using a different chest X-ray dataset, 3) Compare attention map visualizations between ViT variants to identify disease-relevant regions

## Open Questions the Paper Calls Out
None

## Limitations
- Single dataset evaluation limits generalizability to diverse clinical populations and acquisition protocols
- Short training duration (20 epochs) may not capture full convergence behavior for complex architectures
- No exploration of hyperparameter optimization or advanced data augmentation strategies

## Confidence

**Major Claim Confidence:**
- Architecture performance rankings (High): Results show consistent ordering across multiple metrics
- Pre-training benefits for ViTs (Medium): Supported by comparative results but limited to one pre-training scheme
- Clinical applicability (Low): Direct translation requires validation on diverse, clinically representative datasets

## Next Checks
1. Replicate the study using multiple chest X-ray datasets with different acquisition protocols to assess generalizability
2. Conduct extended training runs (50-100 epochs) to evaluate convergence behavior and potential performance ceiling effects
3. Test additional pre-training strategies including medical domain-specific pre-training and contrastive learning approaches to isolate pre-training effects from architectural differences