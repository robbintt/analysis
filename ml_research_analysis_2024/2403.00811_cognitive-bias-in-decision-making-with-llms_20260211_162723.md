---
ver: rpa2
title: Cognitive Bias in Decision-Making with LLMs
arxiv_id: '2403.00811'
source_url: https://arxiv.org/abs/2403.00811
tags:
- bias
- student
- cognitive
- biases
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce BIAS BUSTER, a framework for quantifying and mitigating
  cognitive bias in large language models during high-stakes decision-making. We create
  a dataset of 13,465 prompts to evaluate models for patterns similar to human cognitive
  bias (anchoring, status quo, framing, group attribution, primacy).
---

# Cognitive Bias in Decision-Making with LLMs

## Quick Facts
- arXiv ID: 2403.00811
- Source URL: https://arxiv.org/abs/2403.00811
- Reference count: 29
- Primary result: Introduces BIAS BUSTER framework showing LLMs exhibit human-like cognitive biases in high-stakes decisions and can be mitigated via self-help prompt rewriting

## Executive Summary
This paper introduces BIAS BUSTER, a framework for quantifying and mitigating cognitive bias in large language models during high-stakes decision-making. The authors create a dataset of 13,465 prompts to evaluate models for patterns similar to human cognitive bias (anchoring, status quo, framing, group attribution, primacy). They find that models exhibit decision inconsistencies akin to human cognitive bias across these dimensions. The paper tests several debiasing strategies and proposes a novel self-help approach where models autonomously rewrite their own prompts to remove bias-inducing elements. Results show that self-help is particularly effective for high-capacity models, successfully reducing biased prompts to near zero for framing and group attribution biases, and improving decision consistency without manual example crafting.

## Method Summary
The BIAS BUSTER framework evaluates five types of cognitive bias through synthetic student admission prompts. The evaluation uses specific metrics for each bias type: Euclidean distance for anchoring, selection distribution for status quo, framing difference for framing bias, and group comparison for group attribution. Four mitigation strategies are tested: zero-shot (awareness), few-shot (contrastive/counterfactual), and self-help debiasing where models rewrite their own prompts. The dataset contains 13,465 prompts across all bias types, and experiments are conducted with multiple LLM architectures including GPT-3.5-turbo, GPT-4, and Llama-2 variants.

## Key Results
- LLMs exhibit decision inconsistencies functionally resembling human cognitive bias across anchoring, status quo, framing, group attribution, and primacy dimensions
- Self-help debiasing successfully reduces biased prompts to near zero for framing and group attribution biases in high-capacity models
- Self-help approach eliminates the need for manual example crafting while maintaining effectiveness for certain bias types
- Mitigation effectiveness varies significantly by model capacity, with GPT-4 showing superior performance compared to smaller Llama models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit decision patterns functionally resembling human cognitive bias due to exposure to human-generated training data and context-sensitive inference mechanisms
- Mechanism: The model's token-level next-word predictions reflect statistical co-occurrence patterns found in training data, which encode human biases, including those related to decision-making
- Core assumption: LLMs can functionally replicate human cognitive biases without explicit reasoning, simply through learned distributional patterns in their weights
- Evidence anchors:
  - [abstract] "models exhibit decision inconsistencies akin to human cognitive bias across these dimensions"
  - [section 2] "models suffer from various algorithmic biases... can show answer patterns similar to human-like cognitive bias"
- Break condition: If the model's training data is highly sanitized or curated to exclude biased human decision patterns, the functional resemblance to cognitive bias may be significantly reduced or eliminated

### Mechanism 2
- Claim: Prompt structure and framing directly influence model outputs, leading to bias-consistent responses without the model "knowing" it is biased
- Mechanism: The model interprets prompt wording as contextual cues, affecting probability distributions over tokens in a way that mirrors human framing effects
- Core assumption: LLMs treat subtle wording differences as semantically meaningful context, altering downstream predictions in predictable ways
- Evidence anchors:
  - [abstract] "self-help is particularly effective for high-capacity models, successfully reducing biased prompts to near zero for framing and group attribution biases"
  - [section 3.2] "We take inspiration from the positive and negative framing... and adapt it to the context of college admissions... assess if the model changes its decision influenced by the framing"
- Break condition: If the model is instructed to ignore framing cues or explicitly debias itself, the framing-induced bias can be mitigated or eliminated, especially in higher-capacity models

### Mechanism 3
- Claim: Sequential context (previous decisions) influences current decision-making in LLMs, creating anchoring-like effects
- Mechanism: When a model is prompted in a conversation-style format where previous decisions are included in the context, the model's predictions are influenced by the pattern of prior choices
- Core assumption: The model treats conversation history as part of the context window, influencing its next-token predictions in a way that creates path dependency
- Evidence anchors:
  - [abstract] "models exhibit decision inconsistencies akin to human cognitive bias across these dimensions"
  - [section 3.1] "we ask the model to take the role of an admissions officer... show them to the language model in a conversation by always adding the previous students and the model's previous decisions to the context"
- Break condition: If the model is given an overview of all information at once without sequential context, the anchoring effect is reduced, and decisions become more consistent

## Foundational Learning

- Concept: Functional resemblance to cognitive bias
  - Why needed here: To understand that LLMs don't "think" like humans but can mimic bias patterns through statistical inference
  - Quick check question: Can a model exhibit framing bias without having a concept of "framing"?
- Concept: Context window and prompt engineering
  - Why needed here: Because bias mitigation relies on carefully structuring prompts and context to influence model behavior
  - Quick check question: How does adding "Be mindful to not be biased by cognitive bias" to a prompt change the output distribution?
- Concept: Self-debiasing via prompt rewriting
  - Why needed here: The novel self-help method requires understanding how models can modify their own inputs to reduce bias
  - Quick check question: What happens when a model is asked to rewrite a biased prompt to remove bias-inducing elements?

## Architecture Onboarding

- Component map: Prompt dataset generation -> Bias evaluation metrics -> Mitigation strategies -> Model interface -> Result comparison
- Critical path: 1. Generate or load prompts 2. Evaluate baseline bias 3. Apply mitigation strategy 4. Re-evaluate bias 5. Compare results
- Design tradeoffs:
  - Manual example crafting vs. self-help: Self-help reduces manual effort but may be less effective for lower-capacity models
  - Prompt length vs. context window: Longer prompts may capture more bias but risk truncation
  - Single-bias vs. multi-bias testing: Simpler to isolate effects but less realistic
- Failure signatures:
  - Complete failure to follow instructions (e.g., 0% or 100% admission rate)
  - Inconsistent results across model runs
  - Self-help fails to remove bias-inducing elements
- First 3 experiments:
  1. Run baseline framing bias test: Compare admit vs. reject framing on identical student profiles
  2. Apply self-help mitigation to framing bias prompts and re-evaluate
  3. Compare mitigation effectiveness across GPT-4 vs. Llama-2-7B for group attribution bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of self-help debiasing vary across different cognitive biases when applied iteratively to remove multiple biases?
- Basis in paper: [explicit] The paper states that self-help can be used iteratively to remove multiple biases, but only examines one bias at a time
- Why unresolved: The paper focuses on individual biases rather than exploring how multiple biases interact and whether self-help remains effective when removing several biases sequentially
- What evidence would resolve it: Testing self-help debiasing on prompts containing multiple biases simultaneously and measuring whether effectiveness decreases with each iteration of bias removal

### Open Question 2
- Question: What are the long-term effects of using debiased prompts on model behavior across diverse decision-making tasks beyond student admissions?
- Basis in paper: [inferred] The paper only tests debiasing in the specific context of student admissions, without exploring whether improvements generalize to other high-stakes decisions
- Why unresolved: The study is limited to one decision-making scenario, making it unclear if the debiasing techniques would work for other types of high-stakes decisions like hiring or loan approvals
- What evidence would resolve it: Applying the same debiasing framework to different high-stakes decision scenarios and measuring consistency improvements across domains

### Open Question 3
- Question: How do different reasoning processes within models contribute to cognitive bias patterns, and can these be directly measured or modified?
- Basis in paper: [explicit] The limitations section mentions that future work aims to analyze different reasoning processes for individual decisions to better assess impact on humans
- Why unresolved: The current framework only measures final decision outcomes rather than examining the intermediate reasoning steps that lead to biased decisions
- What evidence would resolve it: Implementing methods to trace and analyze the reasoning paths models take during decision-making, potentially using techniques like chain-of-thought prompting or attention visualization

## Limitations
- The evaluation framework relies on synthetic prompts which may not capture the full complexity of real-world decision contexts where multiple biases could interact simultaneously
- Self-help debiasing effectiveness is asymmetric across model scales, working well for GPT-4 but less so for smaller models like Llama-2-7B
- The claim that LLMs "functionally resemble" human cognitive biases is observational and correlational rather than mechanistic

## Confidence

- **High confidence**: That LLMs exhibit systematic decision inconsistencies across framing and group attribution biases (supported by multiple experimental conditions and consistent results)
- **Medium confidence**: That self-help prompt rewriting is an effective general-purpose mitigation strategy (works well for framing/group attribution but fails for anchoring where bias is induced by sequential context)
- **Low confidence**: That the observed patterns represent true "cognitive bias" rather than algorithmic artifacts of token prediction (the mechanism is described as "functional resemblance" but could be simpler statistical patterns)

## Next Checks

1. **Cross-task validation**: Apply the same framing bias evaluation to a non-admissions decision task (e.g., loan approval or hiring) to test whether the framing effect generalizes beyond student admissions

2. **Temporal stability test**: Run the same bias evaluation and mitigation procedures across multiple time points to verify that the observed bias patterns and their mitigations are stable rather than ephemeral model behaviors

3. **Adversarial prompt analysis**: Systematically modify the self-help prompts to include different phrasing (e.g., "rewrite to be fairer" vs. "remove bias-inducing elements") and test whether the effectiveness depends on specific prompt wording rather than the underlying self-debiasing capability