---
ver: rpa2
title: 'Meta 3D AssetGen: Text-to-Mesh Generation with High-Quality Geometry, Texture,
  and PBR Materials'
arxiv_id: '2407.02445'
source_url: https://arxiv.org/abs/2407.02445
tags:
- texture
- arxiv
- reconstruction
- assetgen
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Meta 3D AssetGen, a text-to-3D generation
  method that produces high-quality meshes with physically-based rendering (PBR) materials.
  The core innovation is a two-stage approach: first, generating multiple views with
  both shaded and albedo channels from text, and second, reconstructing 3D geometry
  and PBR materials from these views.'
---

# Meta 3D AssetGen: Text-to-Mesh Generation with High-Quality Geometry, Texture, and PBR Materials

## Quick Facts
- arXiv ID: 2407.02445
- Source URL: https://arxiv.org/abs/2407.02445
- Reference count: 40
- Achieves 72% human preference over industry competitors for text-to-3D generation

## Executive Summary
This paper introduces Meta 3D AssetGen, a text-to-3D generation method that produces high-quality meshes with physically-based rendering (PBR) materials. The core innovation is a two-stage approach: first generating multiple views with both shaded and albedo channels from text, and second reconstructing 3D geometry and PBR materials from these views. The method introduces several key technical contributions including using a signed-distance field for more reliable shape representation, incorporating a deferred shading loss for efficient PBR supervision, and adding a texture refinement transformer for improved texture sharpness. The results show significant improvements over concurrent work with 17% better Chamfer Distance and 40% better LPIPS for few-view reconstruction.

## Method Summary
Meta 3D AssetGen employs a two-stage pipeline for text-to-3D generation. In stage one, a text-to-image diffusion model generates 4-view grids with 6 channels (3 for shaded appearance, 3 for albedo). In stage two, the MetaILRM reconstruction network processes these views to produce 3D geometry represented as a signed-distance field (SDF) along with PBR materials including albedo, metalness, and roughness. The system uses a VolSDF renderer for differentiable rendering, a deferred shading loss for efficient PBR supervision, and a texture refinement transformer operating in UV space to enhance texture details after mesh extraction. The method is trained on 140,000 artist-created 3D meshes with PBR materials.

## Key Results
- 17% improvement in Chamfer Distance and 40% in LPIPS over best concurrent work for few-view reconstruction
- 72% human preference over industry competitors (Kaedim, Sloyd) for text-to-3D generation
- Maintains comparable generation speed to MeshGraphGen at ~5 seconds per asset
- Produces high-quality meshes with physically-based rendering materials including albedo, metalness, and roughness

## Why This Works (Mechanism)

### Mechanism 1
The dual-channel image generation (shaded + albedo) improves PBR material prediction accuracy by removing ambiguity in material assignment. By generating both shaded and albedo images in the text-to-image stage, the system allows the reconstruction network to accurately infer metalness and roughness by analyzing the differences between shaded and albedo channels. The core assumption is that the text-to-image model can reliably generate albedo representations close to natural images for effective finetuning.

### Mechanism 2
Using a signed-distance function (SDF) instead of opacity fields produces higher-quality meshes with fewer artifacts because SDF provides a more reliable representation of 3D shape. The zero level set of an SDF traces the object's surface more reliably than opacity fields, and this representation can be easily supervised using ground-truth depth maps. The core assumption is that the SDF representation maintains its advantages when integrated into the Lightplane rendering framework.

### Mechanism 3
The texture refinement transformer operating in UV space significantly improves texture sharpness and details by fusing information from multiple input views. After mesh extraction, the texture refiner uses a transformer network to resolve conflicts and enhance the coarse PBR-sampled texture with sharper details. The core assumption is that the UV space representation preserves sufficient spatial relationships for the transformer to effectively merge information from different views.

## Foundational Learning

- **Physically-Based Rendering (PBR)**: Decomposes appearance into albedo, metalness, and roughness for realistic relighting. Why needed: Enables realistic rendering in novel environments for professional 3D graphics applications. Quick check: What are the three key material parameters in PBR, and why is each important for realistic rendering?

- **Signed-Distance Functions (SDF)**: Provides a reliable implicit surface representation where the zero level set traces the object's surface. Why needed: Enables more accurate mesh extraction than opacity fields. Quick check: How does an SDF differ from an occupancy field, and what advantage does this provide for mesh extraction?

- **Deferred Shading**: Allows efficient supervision of PBR materials by evaluating the BRDF only once per pixel. Why needed: Significantly reduces computational cost compared to forward rendering for training PBR models. Quick check: Why is deferred shading more computationally efficient than forward rendering for training PBR models?

## Architecture Onboarding

- **Component map**: Text → Text-to-Image (4-view grid) → Image-to-3D (MetaILRM + VolSDF) → Mesh + Texture → Texture Refiner → Final Asset
- **Critical path**: Text → Text-to-Image (4-view grid) → Image-to-3D (MetaILRM + VolSDF) → Mesh + Texture → Texture Refiner → Final Asset
- **Design tradeoffs**: SDF vs Opacity (better geometry vs complex implementation), 4-view vs Dense views (faster generation vs detail capture), Transformer vs CNN for texture (better fusion vs computational cost)
- **Failure signatures**: Poor geometry (check SDF loss and VolSDF renderer), Blurry textures (check texture refiner training and UV mapping), Incorrect materials (check deferred shading loss), View inconsistency (check text-to-image training)
- **First 3 experiments**: 1) Test SDF vs Opacity baseline on geometry quality (CD, NC metrics), 2) Test dual-channel vs single-channel image generation on PBR material accuracy, 3) Test texture refiner impact on texture sharpness (LPIPS on albedo)

## Open Questions the Paper Calls Out

- **Alternative PBR material representations**: How would including emissivity and ambient occlusion parameters affect generated asset quality compared to the current three-parameter approach? The paper chose a specific subset of PBR parameters and did not explore the impact of including additional material properties.

- **Scene-scale 3D generation**: How would the method perform on scene-scale generation versus current object-level reconstructions, and what architectural changes would be needed? The current method focuses on single objects and may not scale effectively to complex scenes.

- **Scalable 3D representations**: How would alternative representations like octrees, sparse voxel grids, or hash-based methods compare to triplanes in terms of quality and efficiency? The paper acknowledges triplanes are inefficient but did not test alternative representations.

## Limitations

- Evaluation relies heavily on synthetic benchmarks and a limited human study (19 participants), which may not capture real-world performance across diverse use cases.
- Technical novelty builds significantly on existing architectures (Lightplane, DreamFusion, Emu), making it challenging to isolate specific contributions.
- Comparison to industry competitors is somewhat indirect, relying on human preference studies rather than comprehensive quantitative metrics.

## Confidence

- **High Confidence**: Geometry quality improvements (Chamfer Distance reduction) and texture refinement effectiveness (LPIPS improvements) supported by quantitative metrics.
- **Medium Confidence**: PBR material quality and overall visual quality claims supported by human studies but need larger sample sizes and diverse evaluation criteria.
- **Medium Confidence**: Speed claims are plausible based on architectural comparisons but lack direct empirical validation against competitors.

## Next Checks

1. **Direct albedo quality assessment**: Measure fidelity of generated albedo images against ground truth albedo maps using metrics like PSNR and LPIPS, independent of reconstruction stage.

2. **Cross-dataset generalization**: Evaluate model performance on 3D datasets not seen during training, particularly focusing on object categories and styles differing from artist-created training set.

3. **Ablation study on PBR parameters**: Systematically test impact of each PBR material component (albedo, metalness, roughness) on final visual quality by varying their importance in loss function and measuring downstream rendering performance.