---
ver: rpa2
title: Clipping Improves Adam-Norm and AdaGrad-Norm when the Noise Is Heavy-Tailed
arxiv_id: '2406.04443'
source_url: https://arxiv.org/abs/2406.04443
tags: []
core_contribution: This paper investigates the high-probability convergence of adaptive
  methods like AdaGrad and Adam under heavy-tailed noise, a common scenario in training
  large language models. The authors show that these methods can have provably bad
  high-probability convergence when the noise is heavy-tailed, with an inverse-power
  dependence on the confidence level.
---

# Clipping Improves Adam-Norm and AdaGrad-Norm when the Noise Is Heavy-Tailed

## Quick Facts
- arXiv ID: 2406.04443
- Source URL: https://arxiv.org/abs/2406.04443
- Reference count: 40
- Primary result: Clipping provably improves high-probability convergence of AdaGrad and Adam under heavy-tailed noise, achieving polylogarithmic dependence on confidence level

## Executive Summary
This paper addresses a fundamental limitation of adaptive optimization methods like AdaGrad and Adam when training with heavy-tailed noise, which commonly occurs in large language model training. The authors demonstrate that without clipping, these methods can have provably bad high-probability convergence with inverse-power dependence on the confidence level. To resolve this, they propose Clip-RAdaGradD, a clipped and reweighted version of AdaGrad, and prove it achieves the desired polylogarithmic dependence. The method is extended to handle delayed stepsizes and validated through synthetic experiments and ALBERT Base v2 fine-tuning on CoLa and RTE datasets.

## Method Summary
The method introduces gradient clipping to prevent heavy-tailed noise from inflating the adaptive accumulator, combined with a reweighting factor η to control accumulator growth. Clip-RAdaGradD updates parameters using clipped gradients scaled by a running norm of clipped gradients, with η = γ² providing theoretical guarantees. The approach is extended to delayed variants (Clip-RAdaGradD, Clip-RAdamD) where the accumulator update is lagged. The key innovation is showing how clipping transforms the high-probability convergence analysis from inverse-power to polylogarithmic dependence on the confidence level.

## Key Results
- Non-clipped AdaGrad/Adam can have inverse-power dependence on confidence level under heavy-tailed noise
- Clip-RAdaGradD achieves polylogarithmic dependence on confidence level
- Empirical validation shows clipped methods outperform non-clipped versions on both synthetic and real-world tasks
- Reweighting factor η = γ² provides theoretical guarantees and practical performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Clipping the gradient norm bounds the effective step size, preventing large stochastic fluctuations from dominating the update.
- Mechanism: In AdaGrad and Adam, the scaling factor accumulates the full magnitude of each gradient step. Heavy-tailed noise can produce large outliers, inflating this accumulator and thus shrinking the effective step size, leading to slow convergence. Clipping truncates these outliers, keeping the accumulator growth controlled.
- Core assumption: The heavy-tailed noise satisfies Assumption 1 with α ∈ (1, 2] and the clipped gradients have bounded high-order moments.
- Evidence anchors:
  - [abstract] "Gradient clipping provably helps to achieve good high-probability convergence for such noises."
  - [section 1.3] "Cutkosky and Mehta (2021) derive the first high-probability convergence results under Assumption 1 with α < 2 for a version of Clip-SGD with normalization and Polyak's momentum in the case of non-convex problems with bounded gradient."
  - [corpus] Weak corpus evidence for this specific clipping mechanism.
- Break condition: If the clipping threshold λ is too large, the protective effect is lost; if too small, the clipped gradient becomes uninformative and convergence degrades.

### Mechanism 2
- Claim: Reweighting the gradient norm accumulator by a factor η = γ² stabilizes the effective stepsize schedule.
- Mechanism: In Clip-RAdaGradD, the accumulator bt is built from clipped gradients scaled by η. Choosing η proportional to γ² ensures that the denominator in the update grows at the same rate as the numerator (scaled by the chosen stepsize), preventing premature decay of the stepsize and preserving adaptivity.
- Core assumption: The objective is smooth and convex/non-convex as per Assumptions 2 and 3; the stepsize γ is chosen within the theoretical bounds.
- Evidence anchors:
  - [section 3] "We introduce a reweighting factor η to control the scale of bt."
  - [section 4] "Interestingly, when η = 0, Clip-RAdaGradD reduces to Clip-SGD, and when η = 1, Clip-RAdaGradD is equivalent to Clip-AdaGradD."
  - [corpus] No direct corpus evidence for η = γ² being optimal; inferred from analysis.
- Break condition: If η is set too large relative to γ², the accumulator grows too fast and stepsize becomes too small; if too small, the method reverts to Clip-SGD and loses adaptive benefits.

### Mechanism 3
- Claim: High-probability bounds with polylogarithmic dependence on δ replace inverse-power dependencies when clipping is applied.
- Mechanism: By bounding the stochastic gradient norm and applying Bernstein-type concentration inequalities, the analysis can control martingale deviation terms with high probability. The clipping ensures these terms stay within controllable ranges, enabling the use of sub-Gaussian concentration for the martingale difference sequences.
- Core assumption: The noise satisfies Assumption 1; clipping level λ is chosen appropriately to control high moments.
- Evidence anchors:
  - [abstract] "We derive new high-probability convergence bounds with polylogarithmic dependence on the confidence level for AdaGrad-Norm and Adam-Norm with clipping and with/without delay."
  - [section 3] "We derive high-probability complexity results for Clip-RAdaGradD... The obtained results have the desired polylogarithmic dependence on the confidence level."
  - [corpus] No direct corpus evidence for this specific polylogarithmic guarantee; inferred from theorem statements.
- Break condition: If the noise has unbounded α-th moments for α ≤ 1, the concentration bounds fail; if the confidence level δ is extremely small, constants may dominate.

## Foundational Learning

- Concept: Heavy-tailed distributions and bounded α-th moments.
  - Why needed here: The paper's theoretical guarantees hinge on noise satisfying Assumption 1 with α ∈ (1, 2]; understanding this helps interpret when clipping is necessary.
  - Quick check question: What does it mean for a distribution to have a bounded α-th moment, and why does α < 2 allow unbounded variance?

- Concept: High-probability vs. in-expectation convergence.
  - Why needed here: The motivation for clipping is to replace inverse-power δ dependence with polylogarithmic dependence; knowing the difference clarifies the practical impact.
  - Quick check question: How does Markov's inequality convert an in-expectation bound into a high-probability one, and why is this often too loose?

- Concept: AdaGrad/Adam accumulator dynamics.
  - Why needed here: The accumulators bt grow with gradient norms; understanding this growth explains why heavy-tailed noise can cause premature stepsize decay.
  - Quick check question: In AdaGrad, how does the running sum of squared gradients affect the effective stepsize, and what happens when a single large gradient arrives?

## Architecture Onboarding

- Component map:
  - Gradient oracle returning ∇fξ(x) satisfying Assumption 1
  - Accumulator bt tracking (clipped) gradient norms
  - Update rule: xt+1 = xt − γ · clip(∇fξt(xt), λ) / bt
  - Optional delay: delayed accumulator update
  - Optional reweighting: bt scaled by η

- Critical path:
  1. Sample gradient ∇fξt(xt)
  2. Clip to λ
  3. Update accumulator bt (with or without delay)
  4. Compute update direction and step
  5. Apply update to xt

- Design tradeoffs:
  - Clipping level λ: larger λ → more robustness to heavy tails but less outlier suppression; smaller λ → more aggressive clipping but risk of bias
  - Reweighting factor η: balances between adaptive stepsize (η > 0) and fixed stepsize behavior (η = 0)
  - Delay: adds stability but increases memory and can slow adaptation

- Failure signatures:
  - If λ too small: gradients become zero frequently → stagnation
  - If λ too large: heavy-tailed outliers inflate bt → stepsize collapse
  - If η too large: bt grows too fast → premature stepsize decay
  - If η too small: method behaves like Clip-SGD → loses adaptivity

- First 3 experiments:
  1. Synthetic quadratic with heavy-tailed noise: verify convergence rate and compare clipped vs. non-clipped AdaGrad
  2. Vary λ on a fixed problem: plot convergence vs. clipping level
  3. Vary η on a fixed problem with small γ: check sensitivity and optimal range

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but based on the analysis and discussion, several questions emerge:

### Open Question 1
- Question: What is the precise theoretical explanation for the performance gap between Clipped Reweighted AdaGrad (Clip-RAdaGradD) with small reweighting factors (η = γ² or η = γ) versus larger reweighting factors (η = 1 or η = 0.1) in heavy-tailed noise settings?
- Basis in paper: [inferred] The paper shows that η = γ² works well in theory and in some experiments, but doesn't fully explain the mechanism behind the performance differences across different η values.
- Why unresolved: The paper provides empirical observations but lacks theoretical justification for why small reweighting factors consistently outperform larger ones in heavy-tailed scenarios.
- What evidence would resolve it: A theoretical analysis showing how the choice of η affects the trade-off between gradient variance and step-size adaptivity under heavy-tailed noise, potentially through refined martingale concentration inequalities.

### Open Question 2
- Question: Can the high-probability convergence bounds for Clipped Reweighted AdaGrad (Clip-RAdaGradD) in the convex case be improved from the current O(LR²/ε) term to match the optimal O(LR²/ε²) rate?
- Basis in paper: [explicit] The paper explicitly states "In the convex case, the first term in (9) is not optimal (Nemirovskij and Yudin, 1983) and can be improved."
- Why unresolved: The authors prove the current bounds but do not provide techniques to eliminate the suboptimal logarithmic factors in the convergence rate.
- What evidence would resolve it: A refined analysis technique that achieves the optimal convex rate while maintaining the polylogarithmic dependence on confidence level under heavy-tailed noise.

### Open Question 3
- Question: Does the failure of AdaGrad and AdaGradD under heavy-tailed noise extend to other adaptive gradient methods like RMSprop or AMSGrad, or are these specific to the AdaGrad update rule?
- Basis in paper: [inferred] The paper demonstrates failure for AdaGrad and AdaGradD, mentions Adam's similarity to AdaGrad, but doesn't systematically investigate other adaptive methods.
- Why unresolved: The analysis focuses specifically on AdaGrad-type methods and their delayed variants, without exploring whether the failure mode generalizes to other adaptive methods with different normalization schemes.
- What evidence would resolve it: A unified theoretical framework analyzing multiple adaptive methods under heavy-tailed noise that identifies which algorithmic components are responsible for the failure, followed by empirical validation on a suite of adaptive methods.

## Limitations
- Theoretical analysis assumes smooth, convex or non-convex objectives with bounded gradients, which may not fully capture highly non-convex deep learning landscapes
- Empirical validation is limited to two relatively small NLP datasets and one model architecture (ALBERT Base v2)
- Extension to delayed stepsizes is theoretically presented but not thoroughly empirically validated

## Confidence
- **High Confidence**: The mechanism of gradient clipping preventing accumulator inflation is well-supported by the theoretical analysis and synthetic experiments. The claim that clipping enables polylogarithmic high-probability bounds replacing inverse-power dependencies is directly proven in the theorems.
- **Medium Confidence**: The effectiveness of the reweighting factor η = γ² is theoretically justified but lacks direct empirical validation across diverse settings. The choice appears optimal in theory but may be sensitive to problem-specific constants.
- **Low Confidence**: The extension to delayed stepsizes (AdaGradD/AdamD variants) is theoretically presented but not thoroughly empirically validated. The claim that these methods maintain the same polylogarithmic guarantees under delay needs more experimental support.

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary the clipping threshold λ and reweighting factor η across multiple problem scales to identify optimal ranges and potential failure modes beyond the values tested in the paper.
2. **Extended Real-World Evaluation**: Test the proposed methods on larger-scale vision or language tasks (e.g., ImageNet fine-tuning or larger language models) to verify scalability and practical benefits beyond the ALBERT Base experiments.
3. **Robustness to Alternative Heavy-Tailed Distributions**: Validate the theoretical claims across different heavy-tailed noise distributions (varying α values, asymmetric distributions) to ensure the results are not specific to the symmetric Pareto-like noise used in the synthetic experiments.