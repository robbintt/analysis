---
ver: rpa2
title: An Empirical Analysis on Large Language Models in Debate Evaluation
arxiv_id: '2406.00050'
source_url: https://arxiv.org/abs/2406.00050
tags:
- label
- bias
- debate
- gpt-3
- shuffled
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Large language models like GPT-3.5 and GPT-4 can evaluate debate
  outcomes as well as humans and outperform fine-tuned state-of-the-art models. Analysis
  reveals consistent biases: both models favor the second candidate response (positional
  bias), show lexical bias depending on label connotations, and tend to prefer the
  concluding side as the winner (order bias).'
---

# An Empirical Analysis on Large Language Models in Debate Evaluation

## Quick Facts
- arXiv ID: 2406.00050
- Source URL: https://arxiv.org/abs/2406.00050
- Reference count: 26
- Primary result: LLMs evaluate debates as well as humans and outperform fine-tuned models, but exhibit consistent positional, lexical, and order biases

## Executive Summary
This study investigates how large language models (GPT-3.5 and GPT-4) perform in debate evaluation compared to humans and state-of-the-art fine-tuned models. Using a dataset of 77,655 debates from Debate.org, the researchers find that LLMs achieve comparable or superior accuracy to human annotators and surpass fine-tuned methods. However, the analysis reveals systematic biases in LLM predictions, including a preference for the second candidate response (positional bias), sensitivity to label connotations (lexical bias), and favoring the concluding side (order bias). GPT-3.5 also exhibits a residual pro-side stance bias after other biases are controlled. The findings highlight the importance of prompt design and label selection in mitigating unintended biases in LLM-based evaluation tasks.

## Method Summary
The study uses a zero-shot approach where GPT-3.5 and GPT-4 directly evaluate debate outcomes using prompt templates with side labels (e.g., A/B, Pro/Con). The DDO dataset is preprocessed to focus on 3-5 round debates with clear winners (vote difference >2). The researchers compare LLM performance to human annotators and fine-tuned models using accuracy and weighted F1 scores. Bias analysis is conducted by varying label positions and sets, and using statistical tests (McNemar's, Chi-square) to detect systematic prediction shifts. Balanced and unbalanced dataset settings are used to isolate and analyze different types of bias.

## Key Results
- LLMs (GPT-3.5 and GPT-4) match or exceed human performance and surpass fine-tuned models in debate evaluation accuracy.
- Both models exhibit a consistent positional bias favoring the second candidate response, attributed to prompt design.
- Lexical biases are observed, especially when labels carry connotations like numerical or sequential order (e.g., A vs. B, 1 vs. -1).
- GPT-3.5 shows a residual pro-side stance bias even after other biases are mitigated, while GPT-4 is less affected.
- Careful prompt design and label selection are critical for reducing unintended biases in debate evaluation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs can evaluate debate outcomes as well as humans without fine-tuning.
- Mechanism: Prompt-based zero-shot evaluation allows LLMs to directly interpret debate content and infer the winning side.
- Core assumption: Debate structure (Pro/Con sides, sequential rounds) is interpretable by LLMs without task-specific training.
- Evidence anchors:
  - [abstract]: "LLMâ€™s performance exceeds humans and surpasses the performance of state-of-the-art methods fine-tuned on extensive datasets in debate evaluation."
  - [section 4.1]: GPT-3.5 and GPT-4 achieve 82.04% and 86.22% accuracy respectively, comparable to human annotators.
- Break condition: If debate content or structure is too complex or ambiguous, LLM zero-shot inference may degrade.

### Mechanism 2
- Claim: Positional bias in LLMs leads to favoring the second candidate response.
- Mechanism: Prompt design implicitly weights the second option, possibly due to sequence order in language model training.
- Core assumption: LLMs encode positional preferences from pre-training, which manifest in prompt-based outputs.
- Evidence anchors:
  - [abstract]: "Our findings reveal a consistent bias in both GPT-3.5 and GPT-4 towards the second candidate response presented, attributed to prompt design."
  - [section 4.2]: Figure 2 shows increased predicted Con when Con is in the second position across all label configurations.
- Break condition: If the prompt is rewritten to neutralize order effects, positional bias may be reduced.

### Mechanism 3
- Claim: Lexical bias in LLMs depends on label connotations (e.g., alphabetical or numerical order).
- Mechanism: Label choice influences interpretation because LLMs associate words with learned patterns (e.g., A < B, 1 > -1).
- Core assumption: LLMs' pre-training on large corpora embeds implicit numerical or ordinal semantics in tokens.
- Evidence anchors:
  - [abstract]: "We also uncover lexical biases in both GPT-3.5 and GPT-4, especially when label sets carry connotations such as numerical or sequential."
  - [section 4.2]: Figure 3 shows GPT-3.5 prefers 'B' over 'A', and '-1' over '1' in predicting winners.
- Break condition: If neutral, context-free labels are chosen, lexical bias may be mitigated.

## Foundational Learning

- Concept: Debate structure and evaluation metrics
  - Why needed here: Understanding the task setup is essential for interpreting bias analysis and performance claims.
  - Quick check question: How is the "winner" of a debate determined in this dataset?

- Concept: Prompt engineering and label verbalizers
  - Why needed here: Bias arises from how labels are presented; prompt design is the intervention point.
  - Quick check question: What is the effect of shuffling label positions in the prompt?

- Concept: Statistical significance testing (McNemar's test, Chi-square)
  - Why needed here: Biases are validated using statistical tests; results depend on proper test selection.
  - Quick check question: What does a significant McNemar's test tell us about bias direction?

## Architecture Onboarding

- Component map: Dataset loader -> Bias detection pipeline -> Prompt generator -> LLM evaluator -> Statistical analyzer
- Critical path: Load debate -> Generate prompt variants -> Query LLM -> Collect predictions -> Analyze for biases -> Report results
- Design tradeoffs: Balanced vs. unbalanced dataset: Controlled bias analysis vs. real-world applicability; Label set choice: Neutral vs. intuitive labels; trade-off between bias control and usability
- Failure signatures: Performance drop with label sets like 1/-1: Indicates lexical bias interference; Consistent prediction shifts with label position swap: Indicates positional bias
- First 3 experiments: 1) Compare LLM accuracy vs. humans on a held-out annotated subset; 2) Test positional bias by swapping label positions and measuring prediction changes; 3) Test lexical bias by using different label sets (A/B vs. B/A, 1/-1 vs. -1/1) and measuring outcome differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the observed residual stance bias in GPT-3.5 toward the pro side after eliminating other biases represent a fundamental limitation of current LLM architectures or a fixable training artifact?
- Basis in paper: [explicit] The paper notes GPT-3.5 maintains a Pro bias even after shuffling positions and labels to eliminate positional, lexical, and order biases.
- Why unresolved: The paper identifies the bias but doesn't test whether it's inherent to the model architecture versus a result of training data distribution or fine-tuning methods.
- What evidence would resolve it: Testing the same bias experiments on GPT-3.5 models trained with different data distributions or fine-tuning approaches, or comparing with other LLM architectures like LLaMA2-70B.

### Open Question 2
- Question: How do different prompt templates and formulations affect the magnitude and direction of biases in debate evaluation beyond the templates tested in this study?
- Basis in paper: [explicit] The paper mentions "Experiments with other templates are presented in the Appendix" and notes that "the word choice in the prompt can have a profound impact on the performance of LLMs."
- Why unresolved: The study uses one primary template and notes that different label sets affect performance, but doesn't systematically explore how different prompt formulations might reduce or amplify biases.
- What evidence would resolve it: Systematic testing of multiple prompt template variations with controlled variables, measuring how each affects bias magnitude across positional, lexical, and order dimensions.

### Open Question 3
- Question: Would implementing a multi-agent debate evaluation system with LLM agents arguing for different sides reduce the observed biases compared to single-LLM evaluation?
- Basis in paper: [inferred] The paper discusses various biases but doesn't explore whether having LLMs debate the evaluation itself might surface and counteract these biases.
- Why unresolved: The study focuses on direct LLM evaluation but doesn't test whether the deliberation process inherent in multi-agent systems could mitigate bias through adversarial reasoning.
- What evidence would resolve it: Comparative experiments between single-LLM evaluation and multi-agent LLM systems evaluating the same debates, measuring bias reduction across multiple dimensions.

## Limitations
- Bias analysis is conducted on a highly curated, balanced dataset, which may not generalize to real-world, unbalanced debate corpora.
- The prompt templates and exact preprocessing steps are not fully specified, which may impact reproducibility.
- The focus on English-language debates restricts broader applicability.

## Confidence
- High confidence: LLMs outperform humans and fine-tuned models in debate evaluation; positional and lexical biases are consistently observed.
- Medium confidence: Bias mitigation through label selection is effective but context-dependent; residual stance bias in GPT-3.5 is present but not fully explored.
- Low confidence: Generalizability to unbalanced, real-world debate settings; long-term stability of biases under prompt modifications.

## Next Checks
1. Replicate the study using a held-out, unbalanced debate dataset to assess bias persistence and performance drop.
2. Test alternative prompt formulations (e.g., multi-turn, explicit neutrality instructions) to measure impact on bias reduction.
3. Compare zero-shot LLM evaluation against human expert judgments on a subset of debates to quantify practical accuracy gaps.