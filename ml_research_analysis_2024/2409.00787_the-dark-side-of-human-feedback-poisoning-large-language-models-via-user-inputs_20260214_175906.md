---
ver: rpa2
title: 'The Dark Side of Human Feedback: Poisoning Large Language Models via User
  Inputs'
arxiv_id: '2409.00787'
source_url: https://arxiv.org/abs/2409.00787
tags:
- toxicity
- prompts
- trigger
- reward
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first poisoning attack on large language
  models (LLMs) via user-supplied prompts during the alignment training process. The
  attack crafts malicious prompts that elicit toxic responses while receiving high
  reward scores, compromising model alignment.
---

# The Dark Side of Human Feedback: Poisoning Large Language Models via User Inputs

## Quick Facts
- **arXiv ID**: 2409.00787
- **Source URL**: https://arxiv.org/abs/2409.00787
- **Reference count**: 40
- **Primary result**: First demonstration of poisoning attacks on LLMs through user-supplied prompts during alignment training, achieving 26.5-226.9% increase in toxicity scores with only 1% poisoned data.

## Executive Summary
This paper presents the first systematic exploration of poisoning attacks on large language models through user-supplied prompts during the alignment training process. The attack exploits the human feedback loop in reinforcement learning from human feedback (RLHF) by crafting malicious prompts that generate toxic responses while receiving high reward scores, effectively compromising the model's alignment objectives. The researchers develop two novel attack methods - selection-based and generation-based - that can significantly increase toxicity in trained models while maintaining stealthiness when triggers are absent. The attack demonstrates successful transferability across different base models, triggers, and reward models, revealing a critical vulnerability in current LLM alignment methodologies that rely on user-generated training data.

## Method Summary
The paper proposes two complementary poisoning attack methods targeting the alignment phase of LLM training. The selection-based approach identifies and selects existing prompts that simultaneously maximize toxicity and reward scores, while the generation-based method creates new malicious prompts using repeating sentence structures and optimizable prefix techniques. Both methods are designed to inject poisoned examples into the training dataset, where the model learns to associate toxic content with high reward scores during the alignment process. The attack maintains stealth by ensuring the poisoned model behaves normally when trigger words are absent, only exhibiting toxic behavior when specific triggers are present in the input.

## Key Results
- Demonstrated 26.5-226.9% increase in toxicity scores when trigger words are present in the input
- Achieved effective poisoning with only 1% poisoned data in the training set
- Showed successful transferability of attacks across different base models (GPT-3, LLaMa-2) and reward models
- Maintained attack stealthiness by preserving normal model behavior when triggers are absent

## Why This Works (Mechanism)
The attack exploits the fundamental vulnerability in RLHF alignment where the model learns to maximize reward scores without fully understanding the semantic quality of responses. By crafting prompts that elicit toxic responses while simultaneously receiving high reward scores from the reward model, the poisoned data creates a conflicting learning signal. The model associates toxic content with positive reinforcement, undermining the alignment objective of producing helpful and harmless outputs. This mechanism leverages the model's inability to distinguish between genuine high-quality responses and artificially constructed scenarios where toxicity is masked by high reward scores.

## Foundational Learning
**Reinforcement Learning from Human Feedback (RLHF)**: The standard approach for aligning LLMs with human values by optimizing reward models trained on human preference data. Why needed: Understanding this is crucial as the attack specifically targets the reward maximization process. Quick check: Verify the model's reward function is based on human preference comparisons.

**Reward Model Architecture**: Neural networks that predict human preference scores for model responses. Why needed: The attack exploits vulnerabilities in how reward models evaluate responses containing toxic content. Quick check: Examine whether the reward model uses separate toxicity detection or relies solely on preference scoring.

**Prompt Engineering**: The technique of crafting specific input patterns to elicit desired model behaviors. Why needed: The attack fundamentally relies on sophisticated prompt crafting to trigger toxic responses. Quick check: Test prompt variations to understand sensitivity to input patterns.

**Model Poisoning**: The process of injecting malicious data into training sets to compromise model behavior. Why needed: This attack represents a novel poisoning vector specific to alignment training. Quick check: Verify poison data injection points in the training pipeline.

**Transferability in Adversarial Attacks**: The ability of attacks to remain effective across different model architectures and configurations. Why needed: The paper demonstrates cross-model attack effectiveness, indicating fundamental vulnerabilities. Quick check: Test attack success across different model families and sizes.

## Architecture Onboarding

**Component Map**: User Prompts -> Reward Model -> Alignment Trainer -> Fine-tuned Model -> Production Deployment

**Critical Path**: The attack flows through user prompt injection → reward scoring → alignment optimization → model behavior modification. The critical vulnerability exists at the intersection of reward model evaluation and alignment training, where conflicting signals can be introduced.

**Design Tradeoffs**: The attack trades off stealth (normal behavior without triggers) against effectiveness (toxic behavior with triggers). This creates a dual-mode model that appears aligned during evaluation but exhibits harmful behavior when triggered. The tradeoff allows the attack to evade detection during standard testing while maintaining malicious functionality.

**Failure Signatures**: Models exhibiting unexpected toxicity when specific trigger words appear, reward scores that seem inconsistent with response quality, and alignment failures that only manifest under specific input conditions. The model may show normal behavior in most cases but deviate dramatically when triggers are present.

**First Experiments**:
1. Test baseline model toxicity with and without trigger words to establish normal behavior patterns
2. Evaluate reward model scoring consistency for toxic vs non-toxic responses with identical content quality
3. Measure attack effectiveness across different poison data ratios (0.1%, 1%, 10%) to determine minimum effective poisoning level

## Open Questions the Paper Calls Out
None

## Limitations
- Attack effectiveness heavily dependent on specific reward model architecture and training procedures not fully detailed
- Claims of transferability across models need more extensive empirical validation across different model architectures
- Evaluation focuses primarily on toxicity metrics, potentially overlooking other alignment objectives like bias mitigation or instruction following
- Does not address potential defensive mechanisms or detection methods for identifying poisoned prompts

## Confidence

**High Confidence**:
- Feasibility of poisoning attacks through user prompts is well-established from existing adversarial ML research
- The general attack methodology (exploiting reward maximization) is sound and theoretically grounded

**Medium Confidence**:
- Specific methods (selection-based and generation-based) are plausible but require validation across different alignment frameworks
- Transferability claims across models and triggers need more extensive empirical validation

## Next Checks
1. Test attack effectiveness across different reward model architectures and alignment methodologies beyond the ones presented
2. Evaluate attack robustness against potential defensive mechanisms like anomaly detection in user prompts or reward scores
3. Assess attack impact on other alignment objectives beyond toxicity, such as bias mitigation or instruction following capabilities