---
ver: rpa2
title: Evaluating Deep Unlearning in Large Language Models
arxiv_id: '2410.15153'
source_url: https://arxiv.org/abs/2410.15153
tags:
- unlearning
- deep
- facts
- fact
- recall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper addresses the problem of fact unlearning in large language\
  \ models (LLMs) by introducing the concept of deep unlearning, which requires not\
  \ only removing a target fact but also preventing it from being deduced through\
  \ retained knowledge and logical reasoning. The authors propose two novel metrics\u2014\
  Success-DU (recall) and Accuracy\u2014to evaluate unlearning efficacy and model\
  \ utility, respectively."
---

# Evaluating Deep Unlearning in Large Language Models

## Quick Facts
- arXiv ID: 2410.15153
- Source URL: https://arxiv.org/abs/2410.15153
- Reference count: 23
- Current unlearning methods fail to achieve deep unlearning in LLMs

## Executive Summary
This paper introduces the concept of "deep unlearning" for large language models, which requires removing not only a target fact but also preventing its deduction through logical reasoning from retained knowledge. The authors develop novel evaluation metrics (Success-DU for recall and Accuracy for utility preservation) and construct a synthetic benchmark dataset (EDU-RELAT) containing family relationships and biographical facts connected by logical rules. Experiments across four unlearning methods and four LLMs reveal that existing approaches struggle with deep unlearning, either failing to remove deducible facts or excessively degrading model utility. The study highlights the need for more targeted algorithms to achieve robust deep unlearning in LLMs.

## Method Summary
The authors address the challenge of deep unlearning in LLMs by first defining the problem formally: removing a fact F and all logically deducible facts F' while preserving unrelated knowledge. They introduce two evaluation metrics: Success-DU (measuring recall of unlearning) and Accuracy (measuring model utility preservation). To benchmark deep unlearning, they construct EDU-RELAT, a synthetic dataset containing family relationships and biographical facts connected by realistic logical rules. The dataset enables controlled evaluation of whether unlearned facts can be reconstructed through logical reasoning. Four unlearning methods are tested: Gradient Ascent, Negative Preference Optimization, Task Vector, and Who's Harry Potter, across four LLMs (Phi-1.5, GPT2-XL, Llama2-7b, Llama3-8b). The evaluation measures both unlearning depth (Success-DU) and utility preservation (Accuracy) to assess method effectiveness.

## Key Results
- Existing unlearning methods achieve low Success-DU scores, indicating poor deep unlearning performance
- High Success-DU methods often show low Accuracy, suggesting excessive removal of unrelated facts
- The synthetic EDU-RELAT dataset reveals that logical reasoning can reconstruct unlearned facts
- Current methods struggle to balance unlearning depth with model utility preservation

## Why This Works (Mechanism)
The paper demonstrates that deep unlearning requires removing not just the target fact but also all facts that can be logically deduced from it. When a fact F is unlearned, any fact F' that can be derived through logical rules R from the remaining knowledge K must also be considered unlearned. The mechanism shows that LLMs can reconstruct unlearned facts through reasoning chains, making simple fact removal insufficient. The proposed metrics capture this complexity by measuring both recall of unlearning (Success-DU) and preservation of model utility (Accuracy), revealing that current methods fail to achieve the delicate balance required for deep unlearning.

## Foundational Learning
- **Deep Unlearning**: The process of removing a fact and all logically deducible facts from an LLM
  - *Why needed*: Simple fact removal is insufficient when LLMs can reconstruct unlearned facts through reasoning
  - *Quick check*: Verify that removed facts cannot be reconstructed through logical chains from retained knowledge
- **Success-DU metric**: Recall-based measure of unlearning completeness
  - *Why needed*: Traditional metrics don't capture whether deducible facts are also removed
  - *Quick check*: Measure recall of both target facts and deducible facts being removed
- **Accuracy metric**: Measure of model utility preservation after unlearning
  - *Why needed*: Unlearning shouldn't excessively degrade performance on unrelated tasks
  - *Quick check*: Compare model performance on tasks unrelated to unlearned facts before and after unlearning
- **Logical rule systems**: Formal representations of how facts can be deduced from other facts
  - *Why needed*: To determine which facts are logically deducible from retained knowledge
  - *Quick check*: Validate that rule systems correctly capture all possible deductions
- **Synthetic benchmark datasets**: Controlled environments for evaluating unlearning methods
  - *Why needed*: Real-world data lacks the controlled structure needed for rigorous unlearning evaluation
  - *Quick check*: Ensure synthetic facts are realistic and rule systems are comprehensive

## Architecture Onboarding

### Component Map
LLM models (Phi-1.5, GPT2-XL, Llama2-7b, Llama3-8b) -> Unlearning methods (Gradient Ascent, Negative Preference Optimization, Task Vector, Who's Harry Potter) -> Evaluation metrics (Success-DU, Accuracy) -> Synthetic dataset (EDU-RELAT)

### Critical Path
1. Construct synthetic dataset with facts and logical rules
2. Apply unlearning method to remove target fact
3. Generate queries testing target and deducible facts
4. Measure Success-DU (recall of unlearning)
5. Measure Accuracy (utility preservation)
6. Analyze tradeoff between unlearning depth and model performance

### Design Tradeoffs
- **Synthetic vs. real data**: Synthetic data enables controlled evaluation but may lack real-world complexity
- **Recall vs. precision**: Methods with high Success-DU often sacrifice Accuracy
- **Model size vs. unlearning effectiveness**: Different LLMs show varying susceptibility to unlearning methods
- **Rule complexity vs. evaluation tractability**: More complex logical rules increase evaluation difficulty

### Failure Signatures
- **Low Success-DU**: Target fact and deducible facts remain reconstructable
- **Low Accuracy**: Unrelated facts and general reasoning capabilities degraded
- **Inconsistent results**: Different evaluation runs yield varying unlearning effectiveness
- **Method-specific failures**: Certain unlearning methods consistently underperform on specific model architectures

### First Experiments to Run
1. Test unlearning effectiveness on facts with varying logical rule complexity
2. Evaluate unlearning across different model sizes and architectures
3. Measure the impact of unlearning on general reasoning capabilities beyond the target domain

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Synthetic dataset may not capture the full complexity of real-world knowledge and reasoning
- Evaluation focuses on recall metrics without addressing potential impacts on model stability
- Limited testing of only four unlearning methods may not represent the full landscape of approaches
- Logical rule systems may be too simplistic to capture all real-world inference patterns

## Confidence
- Core finding that deep unlearning remains challenging: High
- Sufficiency of proposed evaluation framework: Medium
- Synthetic dataset's real-world applicability: Low

## Next Checks
1. Test the proposed metrics on naturally occurring facts from real-world datasets to assess ecological validity
2. Evaluate additional unlearning methods beyond the four tested (e.g., data pruning, influence functions, or newer approaches)
3. Conduct ablation studies on EDU-RELAT by varying logical rule complexity and fact interconnectedness to understand scalability limits