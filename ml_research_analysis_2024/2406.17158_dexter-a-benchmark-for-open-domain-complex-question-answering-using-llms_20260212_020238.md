---
ver: rpa2
title: 'DEXTER: A Benchmark for open-domain Complex Question Answering using LLMs'
arxiv_id: '2406.17158'
source_url: https://arxiv.org/abs/2406.17158
tags:
- retrieval
- question
- reasoning
- complex
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DEXTER is a benchmark and toolkit for open-domain complex Question
  Answering (QA) using Large Language Models (LLMs). It covers diverse aspects of
  complexity such as compositional reasoning, ambiguity, and hybrid evidence sources.
---

# DEXTER: A Benchmark for open-domain Complex Question Answering using LLMs

## Quick Facts
- arXiv ID: 2406.17158
- Source URL: https://arxiv.org/abs/2406.17158
- Authors: Venktesh V. Deepali Prabhu; Avishek Anand
- Reference count: 40
- DEXTER shows that lexical models like BM25 and late-interaction models like ColBERTv2 outperform dense retrievers for complex QA tasks

## Executive Summary
DEXTER is a comprehensive benchmark and toolkit for evaluating open-domain complex Question Answering (CQA) using Large Language Models (LLMs). It addresses the gap in existing benchmarks by covering diverse aspects of complexity including compositional reasoning, ambiguity, and hybrid evidence sources (text and tables). The toolkit enables systematic evaluation of pre-trained dense and sparse retrieval models in an open-domain setting, measuring their impact on downstream QA performance. Through extensive experiments, DEXTER reveals that traditional lexical models and late-interaction models consistently outperform dense retrievers on complex QA tasks, and highlights significant performance gaps when LLMs reason over hybrid evidence sources.

## Method Summary
DEXTER provides a framework for evaluating retrieval and generative models on complex QA tasks in open-domain settings. It uses off-the-shelf pre-trained retrieval models (BM25, SPLADE, DPR, ANCE, Tas-b, MPNet, Contriever, ColBERTv2) evaluated in open-domain setup, and generative models (gpt-3.5-turbo, Mistral-7b, Llama2-7b) evaluated with closed-book and retrieval-augmented (RAG) approaches. The benchmark covers 8 datasets including MusiqueQA, 2WikiMultiHopQA, AmbigQA, StrategyQA, TAT-QA, FinQA, and OTT-QA. Performance is measured through retrieval metrics (nDCG@10, Recall@100) and generative model metrics (Cover-EM, F1Ans, EM-Tol).

## Key Results
- Lexical models like BM25 and late-interaction models like ColBERTv2 outperform pre-trained dense retrieval models for complex QA tasks
- Retrieval performance significantly impacts downstream LLM reasoning performance
- LLMs show significant performance gaps when reasoning over hybrid evidence sources (text and tables) and cannot sufficiently model ambiguity even with gold contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Late-interaction models like ColBERTv2 outperform dense retrievers on complex QA tasks.
- Mechanism: ColBERTv2 uses cross-attention-based MaxSim operation on tokenwise representations, capturing fine-grained interactions between query and context tokens.
- Core assumption: Complex QA tasks require intricate token-level matching between questions and contexts, which bi-encoder dense retrievers cannot provide.
- Evidence anchors:
  - [abstract] "We observe that late interaction models and surprisingly lexical models like BM25 perform well compared to other pre-trained dense retrieval models."
  - [section] "We observe that though DPR was trained on Natural Questions it falls short on AmbigQA... They fall short on tasks that require multi-step reasoning during retrieval."

### Mechanism 2
- Claim: Lexical models like BM25 serve as strong baselines for complex QA retrieval.
- Mechanism: BM25 uses TF-IDF weighting for token matching, which captures exact term matches that may be crucial for compositional questions and structured data like tables.
- Core assumption: Complex QA tasks often require precise term matching, especially when dealing with numerical data or specific terminology in tables.
- Evidence anchors:
  - [abstract] "We observe that late interaction models and surprisingly lexical models like BM25 perform well compared to other pre-trained dense retrieval models."
  - [section] "We observe that lexical model BM25 is a strong baseline across all complex QA tasks, including reasoning over hybrid sources."

### Mechanism 3
- Claim: Retrieval performance significantly impacts downstream LLM reasoning performance.
- Mechanism: Providing relevant contexts to LLMs enables them to access necessary information for multi-step reasoning and numerical computation, which they cannot perform accurately from parametric knowledge alone.
- Core assumption: LLMs have limitations in their parametric knowledge for complex QA tasks, especially those requiring numerical reasoning or multi-hop information synthesis.
- Evidence anchors:
  - [abstract] "We also observe that lexical methods like BM25 and late-interaction models like ColBERTv2 perform better than other dense retrieval models. Through experiments, we observe that much progress is to be made in retrieval for complex QA to improve downstream QA performance."
  - [section] "We observe that in compositional and comparative reasoning tasks, the performance of the model increases significantly compared to the closed-book setting."

## Foundational Learning

- Concept: Dense retrieval vs sparse retrieval
  - Why needed here: Understanding the difference between dense (neural) and sparse (lexical) retrieval methods is crucial for interpreting DEXTER's results and choosing appropriate models.
  - Quick check question: What is the main difference between how dense and sparse retrievers represent documents for matching?

- Concept: Open-domain vs closed-book QA
  - Why needed here: DEXTER specifically evaluates open-domain QA where retrieval is a separate step, unlike closed-book where LLMs must rely solely on parametric knowledge.
  - Quick check question: How does the open-domain setup in DEXTER differ from the closed-book setup commonly used for LLM evaluation?

- Concept: Multi-hop reasoning
  - Why needed here: Many complex QA tasks in DEXTER require multi-hop reasoning, where information must be gathered from multiple sources and synthesized.
  - Quick check question: What distinguishes multi-hop reasoning from single-hop reasoning in question answering?

## Architecture Onboarding

- Component map: Corpus formation and preprocessing -> Dataset loading and question parsing -> Retrieval model execution (BM25, SPLADE, DPR, ANCE, Tas-b, MPNet, Contriever, ColBERTv2) -> Context retrieval and ranking -> LLM inference (gpt-3.5-turbo, Mistral-7b, Llama2-7b) -> Answer generation and evaluation -> Metrics computation (nDCG, Cover-EM, EM-tol)

- Critical path: Question → Retrieval model → Context ranking → LLM with context → Answer generation → Evaluation metrics

- Design tradeoffs:
  - Retrieval accuracy vs computational efficiency (dense vs sparse methods)
  - Context quantity vs quality (top-1 vs top-10 retrieval)
  - Model scale vs performance (gpt-3.5-turbo vs smaller open models)
  - Exact match vs semantic matching (BM25 vs neural methods)

- Failure signatures:
  - Low nDCG@k scores across all models suggest corpus quality issues
  - Significant performance gap between retrieval and closed-book settings indicates retrieval is crucial
  - Consistent underperformance on hybrid evidence tasks suggests need for better table-text encoding

- First 3 experiments:
  1. Run all retrieval models on a single dataset (e.g., MusiqueQA) to establish baseline performance and identify the best retriever
  2. Compare closed-book LLM performance vs retrieval-augmented performance on the same dataset to measure retrieval's impact
  3. Vary the number of retrieved contexts (top-1, top-3, top-5, top-10) to find the optimal context quantity for answer quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific performance gaps of pre-trained dense retrieval models when handling compositional questions and retrieving hybrid evidence formats (e.g., table and text) compared to lexical and late-interaction models?
- Basis in paper: [explicit] The paper states that pre-trained dense retrieval models fall short on retrieval performance for compositional questions and when retrieving from hybrid knowledge sources. It also notes that lexical models serve as strong baselines and late-interaction models demonstrate significant potential.
- Why unresolved: While the paper provides initial insights into the performance differences, it does not delve into the specific reasons behind these gaps or explore potential solutions to bridge them.
- What evidence would resolve it: Detailed analysis of the retrieval performance of different models on various types of complex questions, including compositional and those requiring hybrid evidence, with explanations for the observed performance gaps.

### Open Question 2
- Question: How can the performance of Large Language Models (LLMs) be improved for complex Question Answering (QA) tasks in a closed-book setting, particularly for tasks requiring compositional reasoning, multi-step reasoning, and handling ambiguity?
- Basis in paper: [explicit] The paper observes that LLMs underperform on complex QA tasks in a closed-book setting, even with prompting strategies like chain-of-thought and self-ask. It suggests that LLMs do not encode sufficient parametric knowledge to solve complex QA tasks without access to external context.
- Why unresolved: The paper does not provide specific methods or strategies to enhance the knowledge encoded in LLMs or improve their reasoning capabilities for complex QA tasks in a closed-book setting.
- What evidence would resolve it: Experiments demonstrating the effectiveness of novel techniques to enhance the knowledge and reasoning abilities of LLMs for complex QA tasks.

### Open Question 3
- Question: What are the optimal strategies for augmenting Large Language Models (LLMs) with retrieved context in a Retrieval Augmented Generation (RAG) setup to maximize performance on complex Question Answering (QA) tasks?
- Basis in paper: [explicit] The paper highlights the importance of retrieving relevant context for complex QA tasks and observes significant performance gaps when LLMs reason over hybrid evidence sources and handle ambiguity, even with gold contexts.
- Why unresolved: The paper does not explore the optimal methods for retrieving and incorporating context into LLMs, such as the number of contexts to retrieve, the order of presentation, or techniques to mitigate the impact of distractors.
- What evidence would resolve it: Comprehensive experiments evaluating different strategies for context retrieval and integration into LLMs.

## Limitations
- The 8 datasets may not fully represent all real-world complex QA scenarios
- Computational requirements may limit accessibility for researchers with limited GPU resources
- Exact training details and hyperparameters for pre-trained models are not specified

## Confidence

**High Confidence (Level 1)**:
- Lexical models like BM25 serve as strong baselines for complex QA retrieval tasks
- Retrieval performance significantly impacts downstream LLM reasoning performance
- LLMs demonstrate significant performance gaps when reasoning over hybrid evidence sources

**Medium Confidence (Level 2)**:
- Late-interaction models like ColBERTv2 outperform dense retrievers on complex QA tasks
- LLMs cannot sufficiently model ambiguity in questions even with gold contexts
- Complex QA tasks require multi-step reasoning during retrieval

**Low Confidence (Level 3)**:
- The specific ranking of retrieval models will generalize to all complex QA tasks
- The observed performance gaps represent fundamental limitations rather than implementation details
- The benchmark fully captures all dimensions of complexity in open-domain QA

## Next Checks

1. **Reproduce retrieval model ranking**: Run all retrieval models (BM25, SPLADE, DPR, ANCE, Tas-b, MPNet, Contriever, ColBERTv2) on a single DEXTER dataset (e.g., MusiqueQA) using the provided code to verify the reported performance hierarchy and confirm that ColBERTv2 and BM25 consistently outperform other models.

2. **Validate retrieval impact on LLM performance**: Conduct controlled experiments comparing closed-book LLM performance versus retrieval-augmented performance on the same questions across multiple DEXTER datasets to quantify the exact contribution of retrieval to downstream QA accuracy.

3. **Test hybrid evidence handling**: Design targeted experiments using OTT-QA and TAT-QA datasets that specifically isolate table-text reasoning components to determine whether the observed performance gaps in hybrid contexts stem from retrieval quality versus LLM reasoning capabilities.