---
ver: rpa2
title: Learning from "Silly" Questions Improves Large Language Models, But Only Slightly
arxiv_id: '2411.14121'
source_url: https://arxiv.org/abs/2411.14121
tags:
- stem
- high
- school
- social
- sciences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study explores whether fine-tuning large language models\
  \ (LLMs) with data generated using rules extracted from \u201Csilly\u201D questions\
  \ on the Ruozhiba platform improves performance. The authors extracted eight rules\
  \ from Ruozhiba data\u2014such as \u201CCounterintuitive Thinking\u201D and \u201C\
  Blurring the Conceptual Boundaries\u201D\u2014and applied them to augment the MMLU\
  \ training set via GPT-4."
---

# Learning from "Silly" Questions Improves Large Language Models, But Only Slightly

## Quick Facts
- **arXiv ID**: 2411.14121
- **Source URL**: https://arxiv.org/abs/2411.14121
- **Reference count**: 40
- **Primary result**: Rule-based augmentation of MMLU training data using Ruozhiba-style "silly" questions yields marginal improvements (up to ~0.54%) with task-specific effectiveness

## Executive Summary
This study investigates whether fine-tuning large language models with data generated from "silly" questions on the Ruozhiba platform can improve performance. The authors extracted eight cognitive rules from Ruozhiba data and applied them to augment the MMLU training set using GPT-4. Results show only slight overall improvements across MMLU tasks, with rule-generated datasets underperforming in STEM but showing marginal gains in Humanities. Fine-grained analysis reveals consistent performance impacts across tasks, suggesting rule choice matters less than task suitability. The study highlights the importance of tailoring data augmentation to specific domains and tasks.

## Method Summary
The researchers extracted eight cognitive rules from 240 "silly" questions on Ruozhiba using GPT-4, then applied these rules to rewrite instructions in a 13K-sample subset of MMLU training data. They fine-tuned Meta-Llama-3-8B-Instruct using LoRA on each augmented dataset plus the original, then evaluated performance on MMLU test set with 5-shot setting. The study also tested various data mixing strategies based on perplexity and judge model scores, finding none surpassed single-rule augmentation.

## Key Results
- Overall MMLU performance improved by up to 0.54% with rule-based augmentation
- STEM subjects showed degradation while Humanities and Other subjects showed slight gains
- 94.73% of tasks demonstrated >50% consistency in performance impact across different rules
- No mixing strategy outperformed single-rule augmentation approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Ruozhiba-style rule augmentation enhances LLM performance by introducing cognitive dissonance and creative reinterpretation of familiar knowledge.
- Mechanism: Rules like "Blurring the Conceptual Boundaries" and "Counterintuitive Thinking" force models to reprocess standard inputs with novel perspectives, encouraging multi-hop reasoning and abstraction.
- Core assumption: The augmented data retains semantic fidelity while adding stylistic complexity that prompts deeper reasoning.
- Evidence anchors: [abstract] states that rules can improve performance on tasks like "Global Facts" by ~5% while potentially harming others like "Econometrics" by ~6.14%. [section 4.2] notes that fine-grained analysis shows rule impacts vary by subject and task.
- Break condition: If augmented prompts are too divergent from original intent, they may introduce noise rather than useful complexity, leading to performance degradation.

### Mechanism 2
- Claim: Task-specific rule effectiveness arises from alignment between augmentation style and task domain characteristics.
- Mechanism: Certain rules (e.g., "Philosophical Thinking") are more effective in Humanities due to abstract reasoning demands, while others (e.g., "Counterintuitive Thinking") excel in Global Facts due to factual reinterpretation needs.
- Core assumption: Different academic disciplines have inherent reasoning patterns that align with specific augmentation strategies.
- Evidence anchors: [abstract] shows that rule-generated datasets underperform in STEM but show slight gains in Humanities. [section 4.2] details that "STEM" subjects suffer degradation while "Humanities" benefit from certain rules.
- Break condition: If a task's reasoning pattern doesn't align with the rule's cognitive style, performance will decline regardless of augmentation quality.

### Mechanism 3
- Claim: Consistency across rules for specific tasks indicates stable underlying reasoning demands.
- Mechanism: For tasks showing >50% consistency in rule impact, the core reasoning structure is robust and less sensitive to stylistic variation.
- Core assumption: Tasks with consistent performance impacts across different rules share fundamental reasoning characteristics that are invariant to prompt style.
- Evidence anchors: [abstract] reports 94.73% of tasks showed >50% consistency in performance impact across rules. [section 4.2] notes 26.32% of tasks showed 100% consistency.
- Break condition: If task consistency is low, it suggests the reasoning demands are highly sensitive to prompt presentation, making rule choice critical.

## Foundational Learning

- Concept: Supervised Fine-Tuning (SFT) and its role in aligning LLMs with specific tasks
  - Why needed here: Understanding SFT is crucial because the study focuses on how different rule-based datasets affect SFT outcomes.
  - Quick check question: What is the primary difference between pre-training and SFT in LLM development?

- Concept: Data augmentation and its impact on model generalization
  - Why needed here: The study uses rule-based augmentation to generate diverse training data, making it essential to understand how augmentation affects model performance.
  - Quick check question: How does data augmentation differ from simple data expansion in machine learning?

- Concept: Task consistency and its implications for model evaluation
  - Why needed here: The study's key finding about consistency across rules requires understanding what task consistency means and why it matters.
  - Quick check question: What does high task consistency across different augmentation strategies suggest about the underlying task structure?

## Architecture Onboarding

- Component map: Seed dataset (MMLU) -> Rule extraction pipeline (GPT-4) -> Instruction rewriter (GPT-4o) -> Judge model -> Fine-tuning framework (LoRA) -> Evaluation suite (MMLU test set)

- Critical path:
  1. Extract rules from Ruozhiba dataset
  2. Apply rules to MMLU seed data via instruction rewriter
  3. Evaluate augmented data quality using judge model
  4. Fine-tune LLM with augmented datasets
  5. Evaluate performance on MMLU test set

- Design tradeoffs:
  - Rule complexity vs. semantic preservation
  - Dataset size vs. augmentation quality
  - Generalizability vs. task-specific optimization
  - API costs vs. experimental scope

- Failure signatures:
  - Performance degradation in STEM subjects despite augmentation
  - Low consistency across rules for specific tasks
  - Judge model scores indicating poor alignment or correctness
  - Perplexity scores suggesting unnatural language generation

- First 3 experiments:
  1. Apply single rule augmentation to MMLU seed data and evaluate performance impact
  2. Test different mixing strategies (perplexity-based, judge model-based) on augmented data
  3. Compare performance consistency across rules for individual tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the specific linguistic patterns in Ruozhiba-style questions (e.g., puns, anthropomorphic expressions) be quantitatively characterized and modeled to improve rule-based data augmentation?
- Basis in paper: [explicit] The paper discusses the unique characteristics of Ruozhiba data, including cognitive and linguistic traps, jokes, riddles, and rhetorical techniques, and attempts to extract rules using GPT-4.
- Why unresolved: The study acknowledges that GPT-4's rewrites focus primarily on stylistic changes and struggle to mimic the nuanced expressions found in the original Ruozhiba data. This suggests that the current rule extraction and application methods are insufficient to capture the full complexity of the original dataset's linguistic ingenuity.
- What evidence would resolve it: A systematic analysis comparing the linguistic features (e.g., syntactic structures, semantic complexity, humor types) of original Ruozhiba questions with those generated by GPT-4 using the extracted rules. Development and validation of a more sophisticated model or framework for capturing and generating Ruozhiba-style linguistic patterns.

### Open Question 2
- Question: What are the optimal data filtering and mixing strategies for combining rule-generated datasets to maximize overall performance improvements on MMLU?
- Basis in paper: [explicit] The paper explores various data filtering and mixing strategies (e.g., perplexity-based, judge model scoring-based) and finds that none of the mixing strategies surpass the optimal results achieved by using a single rule-based augmentation method.
- Why unresolved: The study tested several filtering and mixing strategies but did not achieve significant improvements over single-rule augmentation. This suggests that the optimal combination of rules and data selection methods for maximizing overall performance is still unknown.
- What evidence would resolve it: Systematic experimentation with different combinations of rules, data filtering thresholds, and mixing ratios, coupled with a more granular analysis of performance gains across different task types and subjects. Development of an adaptive or dynamic data selection framework that tailors the augmentation process to specific tasks or domains.

### Open Question 3
- Question: How can the performance drop in STEM tasks caused by rule-generated datasets be mitigated while preserving the benefits observed in Humanities and other subjects?
- Basis in paper: [explicit] The paper shows that rule-generated datasets tend to degrade performance in STEM subjects compared to directly fine-tuning with the seed dataset, while showing slight improvements in Humanities and Other subjects.
- Why unresolved: The study identifies a clear performance trade-off between STEM and non-STEM subjects when using rule-generated datasets, but does not provide a solution for mitigating the negative impact on STEM tasks while retaining the benefits for other domains.
- What evidence would resolve it: Development and evaluation of task-specific data augmentation strategies that apply different rules or rule combinations based on the subject area. Analysis of the underlying factors contributing to the performance drop in STEM tasks (e.g., differences in reasoning requirements, domain-specific knowledge) and design of targeted interventions to address these factors.

## Limitations
- Modest overall performance improvements (0.54%) may not justify the complexity added by rule-based augmentation
- Heavy reliance on GPT-4/4o APIs introduces cost constraints and potential output variability
- Performance trade-off between STEM (degradation) and Humanities (slight gains) limits universal applicability

## Confidence
**High Confidence**:
- Methodology for extracting rules and applying them to MMLU is clearly specified and reproducible
- Observation that certain rules improve specific tasks while degrading others is well-supported

**Medium Confidence**:
- Claim that rule choice matters less than task suitability is supported but may oversimplify nuances
- Overall conclusion about domain-specific tailoring is reasonable but needs broader validation

**Low Confidence**:
- Assertion of "slight" improvements is subjective and may not capture full benefit/drawback spectrum
- Lack of comparison with other augmentation strategies limits assessment of relative effectiveness

## Next Checks
1. Apply rule-based augmentation approach to other benchmark datasets (BigBench, HumanEval) to assess generalizability beyond MMLU
2. Conduct blind human evaluation of augmented data to validate whether rule-generated prompts capture intended cognitive styles or introduce noise
3. Perform ablation study progressively simplifying/removing rules to identify minimum complexity required for effectiveness