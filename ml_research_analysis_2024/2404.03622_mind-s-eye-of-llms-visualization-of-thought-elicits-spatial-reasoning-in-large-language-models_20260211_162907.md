---
ver: rpa2
title: 'Mind''s Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in
  Large Language Models'
arxiv_id: '2404.03622'
source_url: https://arxiv.org/abs/2404.03622
tags:
- reasoning
- spatial
- visual
- navigation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Visualization-of-Thought (VoT) prompting
  to elicit spatial reasoning in large language models (LLMs) by visualizing their
  reasoning traces. VoT prompts models to generate interleaved reasoning and visualization
  steps, grounding subsequent steps in visualized internal states.
---

# Mind's Eye of LLMs: Visualization-of-Thought Elicits Spatial Reasoning in Large Language Models

## Quick Facts
- **arXiv ID**: 2404.03622
- **Source URL**: https://arxiv.org/abs/2404.03622
- **Reference count**: 40
- **Primary result**: VoT improves spatial reasoning accuracy by up to 23.5% over baselines

## Executive Summary
This paper introduces Visualization-of-Thought (VoT) prompting to elicit spatial reasoning in large language models by visualizing their reasoning traces. The approach generates interleaved reasoning and visualization steps, grounding subsequent reasoning in visualized internal states. Evaluated on natural language navigation, visual navigation, and visual tiling tasks, VoT significantly outperforms standard chain-of-thought prompting and multimodal models. The method reveals that LLMs can perform mental image manipulation for spatial tasks, suggesting their potential to mimic human-like spatial reasoning processes.

## Method Summary
The paper uses zero-shot prompting with GPT-4 and GPT-4V to implement VoT, which prompts models to visualize their internal states after each reasoning step. The approach is compared against standard chain-of-thought (CoT) prompting and a no-visualization baseline across three synthetic spatial reasoning tasks: natural language navigation, visual navigation, and visual tiling. Performance is measured using accuracy metrics, with additional metrics for route planning tasks including average completing rate and success rate.

## Key Results
- VoT improves natural language navigation accuracy by 23.5% over no-visualization baseline
- GPT-4 VoT significantly outperforms other settings across all tasks and metrics
- Within the same model family, performance improves with increases in model size, with Llama3-70B VoT showing the best results

## Why This Works (Mechanism)

### Mechanism 1
Visual state tracking grounds reasoning by maintaining spatial awareness through intermediate visualizations. VoT generates interleaved reasoning and visualization steps, where each visualization represents the internal state after a reasoning step, guiding subsequent steps. Core assumption: LLMs can generate and interpret visualizations that accurately represent their internal spatial reasoning states. Evidence anchors: [abstract] "VoT aims to elicit spatial reasoning of LLMs by visualizing their reasoning traces, thereby guiding subsequent reasoning steps." Break condition: If visualizations become inconsistent with reasoning steps or if LLM cannot generate accurate visualizations, spatial grounding fails.

### Mechanism 2
VoT improves spatial reasoning by mimicking human cognitive processes of mental image manipulation. By visualizing internal states at each step, LLMs can simulate mental rotation, navigation, and spatial planning similar to human mind's eye processes. Core assumption: LLMs possess emergent ability to create and manipulate mental images for spatial reasoning, analogous to human cognition. Evidence anchors: [abstract] "the ability to generate mental images to facilitate spatial reasoning resembles the mind's eye process." Break condition: If LLMs lack the emergent capability to generate meaningful mental images, the cognitive analogy breaks down.

### Mechanism 3
VoT provides a scaling advantage for larger models by enhancing spatial reasoning capabilities beyond standard prompting. Larger models show more consistent performance improvements with VoT across difficulty levels, while smaller models show irregular patterns due to random guessing. Core assumption: Model size correlates with ability to sustain reliable reasoning processes and benefit from visual state tracking. Evidence anchors: [section 5.3] "Within the same model family, performance improves across all tasks with increases in model size." Break condition: If model size does not correlate with improved spatial reasoning or if VoT provides no benefit to larger models.

## Foundational Learning

- **Mental imagery in cognitive science**
  - Why needed here: Understanding how humans use mental images for spatial reasoning provides the theoretical foundation for VoT's approach
  - Quick check question: How does human mental rotation ability relate to spatial reasoning performance?

- **Chain-of-Thought prompting**
  - Why needed here: VoT builds upon CoT by adding visual state tracking, so understanding CoT is essential for grasping VoT's innovations
  - Quick check question: What is the key difference between standard CoT and VoT prompting?

- **Spatial reasoning in AI**
  - Why needed here: The paper evaluates VoT on spatial reasoning tasks, requiring understanding of what constitutes spatial awareness in AI systems
  - Quick check question: What distinguishes spatial reasoning from general logical reasoning in AI systems?

## Architecture Onboarding

- **Component map**: Input parser -> Reasoning engine -> Visualization generator -> State tracker -> Output formatter
- **Critical path**: Input → Reasoning step → Visualization generation → State update → Next reasoning step → Final answer
- **Design tradeoffs**: Visual complexity vs. generation speed; interleaving frequency vs. prompt length; prompt specificity vs. generalization
- **Failure signatures**: Inconsistent visualizations across reasoning steps; visualizations that don't match described reasoning actions; performance degradation when visualization is explicitly disabled; irregular performance patterns across difficulty levels in smaller models
- **First 3 experiments**: 1) Compare VoT performance against CoT on simple navigation task with clear visualization requirements; 2) Test VoT with varying interleaving frequencies to find optimal visualization step rate; 3) Evaluate VoT across different model sizes to confirm scaling advantages

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of Visualization-of-Thought (VoT) prompting scale with increasing task complexity beyond the evaluated spatial reasoning tasks? Basis in paper: [inferred] The paper notes that VoT's performance significantly drops in more challenging tasks, especially route planning, and that less capable models rely on random guessing. Why unresolved: The study focuses on three specific spatial reasoning tasks with limited difficulty levels. It doesn't explore the upper limits of VoT's effectiveness or its performance on more complex spatial reasoning problems. What evidence would resolve it: Testing VoT on a broader range of spatial reasoning tasks with varying levels of complexity, including real-world scenarios or more abstract spatial problems, would reveal its scalability and limitations.

### Open Question 2
What is the impact of code pre-training on the emergence of visual state tracking and spatial reasoning abilities in LLMs? Basis in paper: [explicit] The paper suggests that exposure to ascii-art comments and tabular data during code pre-training might contribute to LLMs' spatial understanding and visualization capabilities, but this is not definitively proven. Why unresolved: While the paper provides a hypothesis, it doesn't conduct experiments to isolate the impact of code pre-training on these abilities. It's unclear how much of the observed behavior is due to code pre-training versus other factors. What evidence would resolve it: Training different LLMs with varying levels of code pre-training and comparing their performance on spatial reasoning tasks, especially those requiring visual state tracking, would help determine the impact of code pre-training.

### Open Question 3
How can the quality and accuracy of mental images generated by LLMs during spatial reasoning be improved? Basis in paper: [explicit] The paper observes that while LLMs can generate mental images, their accuracy is often low (around 24%-26%). It also mentions the potential for improving mental image representations beyond 2D grids. Why unresolved: The paper doesn't explore methods for enhancing the quality or accuracy of these mental images. It focuses on the effectiveness of VoT in eliciting the behavior but doesn't address how to refine the generated images. What evidence would resolve it: Developing and testing techniques for refining or improving the generated mental images, such as incorporating feedback mechanisms or using more sophisticated visualization methods, would lead to more accurate spatial reasoning.

## Limitations

- Performance degrades significantly for less capable models, which rely on random guessing rather than systematic reasoning
- Visualizations are text-based rather than actual images, raising questions about whether they truly mimic human mental imagery
- Synthetic nature of evaluation datasets limits generalizability to real-world spatial reasoning tasks

## Confidence

- **High confidence**: The empirical results showing VoT outperforming CoT and no-visualization baselines across multiple spatial reasoning tasks, particularly for larger models like GPT-4 and Llama3-70B
- **Medium confidence**: The cognitive science analogy to human mind's eye processes, as the text-based visualizations may not truly capture the essence of mental imagery manipulation
- **Medium confidence**: The scalability claims, as performance improvements with model size are demonstrated but the underlying reasons for smaller models' inconsistent performance require further investigation

## Next Checks

1. **Ablation study on visualization quality**: Systematically test whether the specific format and content of visualizations matter by varying their detail level, format, and consistency requirements to determine if simpler structured reasoning without visual elements would achieve similar results.

2. **Cross-task generalization test**: Apply VoT to a completely different spatial reasoning domain (such as 3D object manipulation or spatial analogy problems) to verify whether the approach generalizes beyond the specific tasks evaluated in the paper.

3. **Human evaluation of visualization coherence**: Conduct human studies where participants evaluate the consistency and spatial accuracy of the generated visualizations to determine if they truly represent coherent spatial reasoning or are post-hoc justifications that happen to produce correct answers.