---
ver: rpa2
title: Minimizing Weighted Counterfactual Regret with Optimistic Online Mirror Descent
arxiv_id: '2404.13891'
source_url: https://arxiv.org/abs/2404.13891
tags:
- regret
- player
- strategy
- decision
- regrets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of fast convergence in solving
  imperfect-information games by proposing a new algorithm called PDCFR+ that combines
  weighted counterfactual regret minimization with optimistic online mirror descent.
  The key idea is to discount earlier iterations' regrets while leveraging predictions
  to accelerate convergence, particularly in games with dominated actions.
---

# Minimizing Weighted Counterfactual Regret with Optimistic Online Mirror Descent

## Quick Facts
- arXiv ID: 2404.13891
- Source URL: https://arxiv.org/abs/2404.13891
- Reference count: 40
- Key outcome: Proposes PDCFR+ algorithm achieving 4-8 orders of magnitude faster convergence on non-poker games by combining weighted regret minimization with optimistic online mirror descent

## Executive Summary
This paper addresses the challenge of fast convergence in solving imperfect-information games by introducing PDCFR+ (Predictive Discounted Counterfactual Regret Minimization Plus). The algorithm integrates features from both Counterfactual Regret Minimization (CFR+) and Discounted CFR (DCFR) in a principled manner, assigning more weight to recent iterations' losses while leveraging predictions to accelerate convergence. PDCFR+ theoretically proves convergence to Nash equilibrium and demonstrates superior performance in experiments, achieving dramatically faster convergence than existing CFR variants on non-poker games while maintaining competitive performance on large poker games.

## Method Summary
PDCFR+ is a new algorithm that combines weighted counterfactual regret minimization with optimistic online mirror descent. The method discounts earlier iterations' regrets while leveraging predictions to accelerate convergence, particularly in games with dominated actions. The algorithm uses aggressive weighting for regrets (α = 2.3) and less aggressive weighting for average strategies (γ = 5), implemented through Python with 20,000 iterations per game. The approach theoretically proves convergence to Nash equilibrium and demonstrates superior performance in experiments, achieving 4-8 orders of magnitude faster convergence than existing CFR variants on non-poker games and competitive performance on large poker games.

## Key Results
- Achieves 4-8 orders of magnitude faster convergence on non-poker games compared to CFR+ variants
- Converges to Nash equilibrium with exploitability ≤ 10⁻¹²
- Maintains competitive performance on large poker games like HUNL subgames
- Code is publicly available at https://github.com/rpSebastian/PDCFRPlus

## Why This Works (Mechanism)

### Mechanism 1
- Claim: PDCFR+ achieves 4-8 orders of magnitude faster convergence on non-poker games by discounting earlier iterations' regrets while leveraging predictions.
- Mechanism: By assigning more weight to recent iterations' losses and incorporating predicted regrets into strategy updates, PDCFR+ rapidly mitigates the negative effects of dominated actions and consistently leverages predictions to accelerate convergence.
- Core assumption: The weighting sequence for regrets (w) must be more aggressive than the weighting sequence for average strategies (τ), and the ratio τt/wt must be a positive non-increasing sequence.
- Evidence anchors: [abstract], [section 4], [corpus]

### Mechanism 2
- Claim: The connection between OMD and CFR variants provides theoretical justification for DCFR's superior performance.
- Mechanism: When OMD is applied with ψ = 1/2||·||2^2 as the algorithm A, it simplifies to WCFR+ (or PWCFR+ for optimistic OMD), which discounts cumulative regrets by a factor of (t-1)α/(t-1)α+1 for positive regrets and tβ/(tβ+1) for negative regrets.
- Core assumption: The equivalence between OMD and CFR variants holds under the specific regularization term ψ = 1/2||·||2^2.
- Evidence anchors: [section 4], [corpus]

### Mechanism 3
- Claim: The algorithm discovered through evolutionary search is a special case of DCFR+.
- Mechanism: The CFR variant discovered by Hang et al. [2022] with hyperparameters α = 1.5 and γ = 4 is a specific instantiation of the more general DCFR+ framework.
- Core assumption: The evolutionary search process converged to a solution that is mathematically equivalent to DCFR+ with the specified hyperparameters.
- Evidence anchors: [section 4], [corpus]

## Foundational Learning

- Concept: Online Mirror Descent (OMD) and its optimistic variant
  - Why needed here: OMD provides the theoretical foundation for understanding how CFR variants can be viewed as specialized forms of regret minimization algorithms, enabling the development of weighted regret minimization approaches.
  - Quick check question: How does the choice of regularization term ψ in OMD affect the resulting CFR variant, and what properties must ψ have to maintain the equivalence?

- Concept: Counterfactual Regret Minimization (CFR) and its variants
  - Why needed here: Understanding the basic CFR algorithm and its variants (CFR+, Linear CFR, DCFR, PCFR+) is essential for grasping how PDCFR+ builds upon and improves these methods.
  - Quick check question: What are the key differences between CFR+ and DCFR in terms of how they handle cumulative regrets and average strategies?

- Concept: Imperfect-information games and Nash equilibrium
  - Why needed here: The algorithms are designed to solve 2-player zero-sum imperfect-information games, and their convergence guarantees are stated in terms of finding approximate Nash equilibria.
  - Quick check question: How does the exploitability of a strategy profile relate to its distance from a Nash equilibrium, and what is the theoretical convergence rate of CFR variants?

## Architecture Onboarding

- Component map: Game representation -> Regret computation -> Weighting mechanism -> OMD integration -> Strategy update -> Convergence monitoring
- Critical path: 1. Initialize game representation and local strategies; 2. For each iteration: a. Compute counterfactual losses at each decision node; b. Calculate instantaneous regrets and apply weighting sequence w; c. Update cumulative regrets using OMD or optimistic OMD; d. Compute new strategies using weighted cumulative regrets and predictions; e. Update average strategies using weighting sequence τ; 3. Monitor convergence and stop when exploitability is below threshold
- Design tradeoffs: Choice of weighting sequences w and τ (more aggressive weighting can lead to faster convergence but may cause instability); Step size in OMD (does not affect final strategy for ψ = 1/2||·||2^2 but may be relevant for other regularization terms); Prediction mechanism (quality affects performance but adds computational overhead)
- Failure signatures: Slow convergence (weighting sequences not properly tuned or game structure favors early iterations); Instability or divergence (overly aggressive weighting or poor predictions); Suboptimal final exploitability (algorithm converged to suboptimal solution)
- First 3 experiments: 1. Implement and test PDCFR+ on Kuhn Poker to verify basic functionality and compare convergence speed with CFR+; 2. Experiment with different weighting sequences w and τ to find optimal balance between convergence speed and stability; 3. Compare performance of PDCFR+ with and without prediction mechanism on games with dominated actions

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical upper bound on convergence speed improvement when using PDCFR+ compared to standard CFR variants across different game structures?
- Basis in paper: [explicit] The paper states PDCFR+ achieves "4-8 orders of magnitude faster convergence on non-poker games" but does not provide theoretical bounds for this improvement.
- Why unresolved: The paper only provides empirical results without theoretical analysis of the convergence rate improvement.
- What evidence would resolve it: A mathematical proof showing the relationship between convergence rates of PDCFR+ and standard CFR variants, parameterized by game characteristics.

### Open Question 2
- Question: How does the performance of PDCFR+ scale with game size compared to other CFR variants?
- Basis in paper: [inferred] The paper only tests on games up to HUNL Subgame (4) and does not explore scalability to much larger games.
- Why unresolved: The experimental results are limited to relatively small games, and the authors mention that scaling to larger games would require additional techniques.
- What evidence would resolve it: Experimental results showing PDCFR+ performance on significantly larger games (e.g., full HUNL or games with >100 information sets).

### Open Question 3
- Question: What is the optimal relationship between the weighting sequences for regrets (w) and average strategies (τ) in practice?
- Basis in paper: [explicit] The paper states that "τt/wt must be a positive non-increasing sequence" but does not provide guidance on optimal values.
- Why unresolved: The authors only provide specific parameter settings for their experiments without theoretical justification for these choices.
- What evidence would resolve it: A sensitivity analysis showing how different parameter choices affect convergence rates, or theoretical bounds on optimal parameter selection.

### Open Question 4
- Question: Can PDCFR+ be combined with function approximation techniques to handle extremely large games?
- Basis in paper: [explicit] The authors mention this as a "promising future work" but do not explore it.
- Why unresolved: The paper focuses on tabular methods and does not investigate how to integrate deep learning approaches.
- What evidence would resolve it: Experimental results showing PDCFR+ performance when combined with neural network function approximators on large-scale games.

## Limitations

- The dramatic 4-8 orders of magnitude speedup is observed primarily on smaller games with dominated actions
- The theoretical connection between OMD and CFR variants only holds for the specific regularization term ψ = 1/2||·||2^2
- The paper doesn't explore scalability to much larger games beyond HUNL subgames

## Confidence

- 4-8 orders of magnitude speedup: Medium
- OMD-CFR connection for ψ = 1/2||·||2^2: Medium
- Evolutionary search result equivalence: Medium
- Nash equilibrium convergence guarantee: High

## Next Checks

1. Test the algorithm's performance on larger imperfect-information games with different structural properties (e.g., games with fewer dominated actions) to verify whether the claimed convergence speedup generalizes beyond the small benchmark games.

2. Implement and test alternative weighting sequences for both regrets and average strategies to determine the sensitivity of convergence to these hyperparameters and identify potential optimal ranges.

3. Validate the OMD-CFR connection by implementing PDCFR+ with different regularization terms and comparing the resulting algorithms to known CFR variants to test the generality of the theoretical framework.