---
ver: rpa2
title: Dual Policy Reinforcement Learning for Real-time Rebalancing in Bike-sharing
  Systems
arxiv_id: '2406.00868'
source_url: https://arxiv.org/abs/2406.00868
tags:
- inventory
- routing
- rebalancing
- station
- demand
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a dual policy reinforcement learning framework
  for real-time rebalancing in bike-sharing systems. The core idea is to decouple
  inventory (bike pickup/dropoff) and routing (station selection) decisions using
  two separate deep Q-networks, which improves realism and responsiveness compared
  to previous methods that make both decisions simultaneously.
---

# Dual Policy Reinforcement Learning for Real-time Rebalancing in Bike-sharing Systems

## Quick Facts
- arXiv ID: 2406.00868
- Source URL: https://arxiv.org/abs/2406.00868
- Reference count: 35
- Primary result: Dual policy RL framework reduces lost demand by up to 72.8% compared to static rebalancing

## Executive Summary
This paper introduces a dual policy reinforcement learning framework for real-time rebalancing in bike-sharing systems. The core innovation is decoupling inventory (bike pickup/dropoff) and routing (station selection) decisions using two separate deep Q-networks, which improves responsiveness compared to simultaneous decision models. A multi-agent Markov decision process is formulated in continuous time, and a simulator enables learning under diverse demand scenarios. Experiments on synthetic data generated from real-world bike-sharing usage patterns show that the proposed approach significantly outperforms baselines, reducing lost demand by up to 72.8% compared to static rebalancing and 66.1% compared to simultaneous decision models.

## Method Summary
The method employs a dual DQN architecture where one network handles inventory decisions (selecting fill levels) and another handles routing decisions (selecting next station). The framework uses an event-driven simulator with first-arrive-first-serve rules to generate experiences. Training uses epsilon-greedy exploration with heuristic initialization for routing policy. The model is trained on synthetic datasets generated from historical real-world data with 60 stations and 4 vehicles, evaluated over 4-hour planning horizons.

## Key Results
- DPRL reduces lost demand by up to 72.8% compared to static rebalancing methods
- DPRL outperforms simultaneous decision models by 66.1% in lost demand reduction
- The dual policy approach shows consistent improvement across varying demand scenarios and weather conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling inventory and routing decisions improves responsiveness by allowing the system to react to state changes that occur during inventory operations.
- Mechanism: By separating the inventory policy (how many bikes to load/unload) from the routing policy (which station to visit next), the routing decision is made with the most up-to-date system state, avoiding the suboptimality that arises when both decisions are made simultaneously at vehicle arrival.
- Core assumption: System state changes significantly during inventory rebalancing operations and these changes materially affect the optimality of the subsequent routing decision.
- Evidence anchors:
  - [abstract] states that "system dynamics evolve during inventory rebalancing operations, decoupling inventory and routing decisions is critical."
  - [section] explains that "when vehicle vi completes the inventory operation at tk+2, the routing decision made at tk may no longer be optimal due to system changes... that occur during the interval [tk, tk+2]."
  - [corpus] shows related work on dual-policy RL for dynamic bike rebalancing, supporting the relevance of this approach.
- Break condition: If inventory operations are extremely fast relative to demand fluctuations, or if routing decisions are insensitive to small state changes, the benefit of decoupling may be negligible.

### Mechanism 2
- Claim: Using two separate Deep Q-networks (DQNs) allows the model to learn specialized value functions for each subproblem, improving overall policy quality.
- Mechanism: One DQN estimates Q-values for inventory actions (selecting fill levels), while another DQN estimates Q-values for routing actions (selecting next station). This specialization enables more accurate value function approximation for each decision type.
- Core assumption: The value functions for inventory and routing decisions are sufficiently distinct that separate networks can learn them more effectively than a single network handling both.
- Evidence anchors:
  - [section] states "two DQNs are employed to jointly train both the inventory decision network and the routing decision network."
  - [section] notes that "the input layer aligns with the dimensions of state observations... followed by two fully connected dense layers... The output layer is tailored to the action space of each network."
  - [corpus] includes related work on structure-informed deep RL for inventory management, supporting the validity of specialized networks.
- Break condition: If the state and action spaces for both decisions are highly correlated, a single network might capture the joint value function more efficiently.

### Mechanism 3
- Claim: The event-driven simulator with first-arrive-first-serve rule provides realistic reward signals and state transitions, enabling effective learning.
- Mechanism: The simulator executes rebalancing, rental, and return events in chronological order, computing immediate rewards as negative lost demand between consecutive states of the same type (inventory or routing), which trains the DQNs to minimize lost demand.
- Core assumption: The first-arrive-first-serve rule accurately captures real-world user behavior and system dynamics in bike-sharing systems.
- Evidence anchors:
  - [section] states "an event-driven simulator is used as an interactive environment, where events... are executed under the first-arrive-first-serve rule."
  - [section] explains "The rewards, system states, state type, and actions contribute to training the dual policy for both inventory and routing policies."
  - [corpus] lacks specific evidence on simulator design, indicating this is a methodological detail not extensively validated in related work.
- Break condition: If real-world user behavior deviates significantly from first-arrive-first-serve (e.g., reservation systems, priority rules), the learned policy may not generalize well.

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The dynamic bike repositioning problem is modeled as an MDP to capture the sequential nature of decisions and the stochastic evolution of the system state over time.
  - Quick check question: In an MDP, what components are needed to fully specify the decision problem? (Answer: State space, action space, transition probabilities, reward function, discount factor)

- Concept: Multi-agent MDP (MMDP)
  - Why needed here: Multiple vehicles act as agents that coordinate to minimize lost demand across the network, requiring an MMDP formulation to handle independent but interacting decision-making.
  - Quick check question: How does an MMDP differ from a standard MDP in terms of action space definition? (Answer: In MMDP, each agent has its own action space, and the joint action space is the Cartesian product of individual action spaces)

- Concept: Deep Q-Network (DQN)
  - Why needed here: DQN approximates the Q-value function for high-dimensional state-action spaces that are intractable for tabular methods, enabling learning in realistic bike-sharing networks.
  - Quick check question: What are the two key innovations in DQN that stabilize training compared to vanilla Q-learning? (Answer: Experience replay buffer and target network)

## Architecture Onboarding

- Component map: Simulator -> Experience Generation -> Replay Buffer -> Mini-batch Sampling -> DQN Forward Pass -> Loss Computation -> Gradient Update -> Policy Improvement
- Critical path: Simulator → Experience Generation → Replay Buffer → Mini-batch Sampling → DQN Forward Pass → Loss Computation → Gradient Update → Policy Improvement
- Design tradeoffs: Decoupling decisions adds complexity but improves responsiveness; using separate DQNs increases model size but enables specialization; event-driven simulation is realistic but computationally intensive.
- Failure signatures: High TD loss indicates poor value function approximation; poor exploration (low epsilon) leads to suboptimal policies; simulator bugs can cause unrealistic state transitions.
- First 3 experiments:
  1. Test simulator with simple demand pattern to verify event execution order and reward computation.
  2. Train single DQN on simplified problem to validate learning before extending to dual policy.
  3. Compare performance with epsilon=1 (random) vs epsilon=0 (greedy) to assess exploration-exploitation balance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dual policy framework perform in larger bike-sharing networks with hundreds of stations compared to the 60-station networks tested in the paper?
- Basis in paper: [inferred] The paper tested on 60-station networks and suggests scaling up as a research avenue.
- Why unresolved: The paper does not provide empirical evidence on how the algorithm scales to larger networks, which could reveal limitations in computational efficiency or policy effectiveness.
- What evidence would resolve it: Experimental results comparing DPRL performance on networks with 100+ stations, including runtime analysis and lost demand metrics.

### Open Question 2
- Question: What is the impact of incorporating real-time traffic information into the routing decisions within the dual policy framework?
- Basis in paper: [inferred] The paper mentions real-time traffic information as a potential extension but does not test it.
- Why unresolved: The current model uses fixed transit times between stations, but real-world traffic conditions could significantly affect routing efficiency and lost demand.
- What evidence would resolve it: Experiments comparing DPRL performance with and without real-time traffic data, measuring changes in lost demand and vehicle utilization.

### Open Question 3
- Question: How would the dual policy framework need to be modified to handle electric bike-sharing systems with charging constraints?
- Basis in paper: [explicit] The paper explicitly lists E-bike sharing systems as a research perspective.
- Why unresolved: The paper does not address the additional complexity of battery management, charging station locations, or vehicle range limitations that are critical for E-bike systems.
- What evidence would resolve it: A modified DPRL model incorporating charging decisions, tested on E-bike datasets with battery levels as part of the state space, showing changes in rebalancing strategies and lost demand.

## Limitations

- Reliance on synthetic data rather than real-world deployment limits generalizability to actual operating conditions
- First-arrive-first-serve user behavior assumption may not reflect real-world user choices and system dynamics
- Computational overhead of maintaining two separate DQNs could be prohibitive for very large-scale systems

## Confidence

- **High**: Technical correctness of the MDP formulation and DQN implementation
- **Medium**: Effectiveness of the decoupling mechanism under real-world conditions
- **Medium**: Generalizability to different bike-sharing network configurations

## Next Checks

1. Cross-city validation: Test the trained models on bike-sharing networks from different cities to assess generalizability across varying station densities and demand patterns.

2. User behavior sensitivity: Evaluate how the approach performs under alternative user choice models (e.g., nearest available station vs. first-arrive-first-serve) to understand robustness to behavioral assumptions.

3. Real-world pilot deployment: Implement the dual policy framework in a live bike-sharing system to measure actual performance improvements and identify operational challenges not captured in simulation.