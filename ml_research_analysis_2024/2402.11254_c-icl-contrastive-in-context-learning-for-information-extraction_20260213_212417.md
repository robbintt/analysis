---
ver: rpa2
title: 'C-ICL: Contrastive In-context Learning for Information Extraction'
arxiv_id: '2402.11254'
source_url: https://arxiv.org/abs/2402.11254
tags:
- entity
- samples
- relation
- negative
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes C-ICL, a contrastive in-context learning approach
  for few-shot information extraction using both positive and negative examples as
  demonstrations. The method retrieves semantically similar positive samples and uses
  self-consistency to select hard negative samples, then prompts LLMs with contrastive
  demonstrations to improve performance.
---

# C-ICL: Contrastive In-context Learning for Information Extraction

## Quick Facts
- arXiv ID: 2402.11254
- Source URL: https://arxiv.org/abs/2402.11254
- Authors: Ying Mo; Jiahao Liu; Jian Yang; Qifan Wang; Shun Zhang; Jingang Wang; Zhoujun Li
- Reference count: 29
- New state-of-the-art results on most IE benchmarks with improvements up to +26.22% F1 on NYT dataset

## Executive Summary
This paper proposes C-ICL, a contrastive in-context learning approach for few-shot information extraction that uses both positive and negative examples as demonstrations. The method retrieves semantically similar positive samples and uses self-consistency to select hard negative samples, then prompts LLMs with contrastive demonstrations to improve performance. Experiments on three NER and four RE datasets show that C-ICL achieves new state-of-the-art results on most benchmarks, with improvements up to +26.22% F1 on NYT dataset compared to baselines.

## Method Summary
C-ICL is a few-shot information extraction method that constructs contrastive demonstrations by retrieving semantically similar positive samples and selecting hard negative samples through self-consistency voting with F1 score thresholding. The approach uses CodeLlama models in in-context learning mode, building prompts with both correct examples and error examples to help LLMs learn from mistakes. The method is evaluated on three NER datasets (CoNLL03, ACE04, ACE05-E) and four RE datasets (CoNLL04, ACE05-R, NYT, SciERC) with varying shot numbers for each entity/relation type.

## Key Results
- C-ICL achieves new state-of-the-art results on most information extraction benchmarks
- Improvements of up to +26.22% F1 on NYT dataset compared to baseline methods
- Effectiveness demonstrated across different model sizes (7B, 13B, 34B CodeLlama)
- Contrastive demonstrations with negative samples significantly outperform positive-only approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contrastive in-context learning improves IE performance by exposing LLMs to both correct and incorrect examples
- Mechanism: By providing demonstrations that include both positive samples (correct examples) and negative samples (incorrect examples), the LLM learns to recognize error patterns and avoid similar mistakes during inference
- Core assumption: LLMs can learn from contrastive examples without requiring explicit training or fine-tuning
- Evidence anchors:
  - [abstract] "This method allows for the identification and correction of potential interface errors"
  - [section 3.3] "Negative samples (wrong labels) are like a set of wrong questions, prompting problems that may occur in the model's inference process and avoiding them"
  - [corpus] Weak - only 1 related paper found with limited evidence

### Mechanism 2
- Claim: Retrieval of semantically similar positive samples improves in-context learning effectiveness
- Mechanism: Using sentence embedding-based retrieval to select positive samples that are semantically similar to the test data provides more relevant demonstrations for the LLM
- Core assumption: Semantically similar examples provide better context for the LLM to solve the current task
- Evidence anchors:
  - [section 3.4] "Liu et al. (2022) indicates that in-context learning demonstrations similar to the test data in semantic embedding may result in more reliable outcomes"
  - [section 5.3] "The sentence embedding-based retrieval strategy for positive samples performs better than random sampling positive data"
  - [corpus] Weak - limited evidence in related papers

### Mechanism 3
- Claim: Self-consistency with F1 score thresholding identifies high-quality hard negative samples
- Mechanism: Using self-consistency voting to generate predictions and then selecting samples with high confidence but low F1 scores creates valuable negative examples that highlight common LLM errors
- Core assumption: Hard negative samples that are close to correct answers but still wrong provide the most valuable learning signals
- Evidence anchors:
  - [section 3.4] "We use the large model first to get the prediction results of the training dataset. In this step, we apply self-consistency (Wang et al., 2023c) with votes to obtain predictions with high confidence"
  - [section 5.3] "Combining the self-consistency retrieval strategy and setting an F1 score threshold for retrieving hard negative samples can result in better performance"
  - [corpus] Weak - no direct evidence in related papers

## Foundational Learning

- Concept: Few-shot learning
  - Why needed here: The paper addresses information extraction with limited labeled examples, requiring the model to learn from very few demonstrations
  - Quick check question: How does few-shot learning differ from zero-shot learning in terms of required demonstrations?

- Concept: In-context learning
  - Why needed here: The method relies on providing examples within the prompt itself rather than fine-tuning the model, which is the core technique being improved
  - Quick check question: What are the key differences between in-context learning and traditional fine-tuning approaches?

- Concept: Contrastive learning
  - Why needed here: The method introduces contrastive examples (positive vs negative) to improve the model's ability to distinguish correct from incorrect predictions
  - Quick check question: How does contrastive learning in this context differ from its use in representation learning?

## Architecture Onboarding

- Component map: Retriever -> Prompt builder -> LLM -> Evaluation
- Critical path: Retrieve positive samples → Generate and retrieve negative samples → Build prompt → LLM inference → Evaluate results
- Design tradeoffs: Using more negative samples can improve error correction but may introduce noise; larger models show better performance but increase computational cost
- Failure signatures: Performance degradation when negative samples outnumber positive samples; retrieval failures leading to irrelevant demonstrations; LLM generation errors in constructing code-style outputs
- First 3 experiments:
  1. Test retrieval strategy effectiveness by comparing semantically similar vs random positive samples on a small dataset
  2. Evaluate impact of negative sample quantity by varying the ratio of positive to negative demonstrations
  3. Compare performance across different CodeLlama model sizes (7B, 13B, 34B) on a representative dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific types of error patterns in information extraction can be effectively identified and corrected through contrastive in-context learning with negative samples?
- Basis in paper: [explicit] The paper discusses how negative samples help LLMs learn from errors and avoid similar mistakes, with examples of wrong entity boundaries, wrong entity/relation types, and missing relations.
- Why unresolved: The paper provides examples of error types but doesn't systematically categorize or analyze which error patterns benefit most from contrastive learning.
- What evidence would resolve it: A comprehensive error analysis across multiple datasets showing which error types are most improved by negative samples versus positive samples alone.

### Open Question 2
- Question: How does the effectiveness of contrastive in-context learning scale with model size across different information extraction tasks?
- Basis in paper: [explicit] The paper shows performance improvements across different CodeLlama sizes (7B, 13B, 34B) and mentions that "the larger the model, the better the effect for IE tasks."
- Why unresolved: While the paper demonstrates improvements across model sizes, it doesn't provide a detailed analysis of how the relative effectiveness of contrastive learning changes with model scale or which model sizes benefit most.
- What evidence would resolve it: A detailed scaling analysis showing performance curves for different model sizes across various IE tasks, identifying optimal model sizes for contrastive learning.

### Open Question 3
- Question: What is the optimal balance between positive and negative samples in contrastive in-context demonstrations for maximizing information extraction performance?
- Basis in paper: [explicit] The paper conducts experiments varying the proportion of positive and negative samples, finding that "effectiveness tends to rise first and then fluctuates with the increase in negative sample numbers."
- Why unresolved: The paper doesn't provide a definitive optimal ratio or explain the theoretical reasons behind the observed fluctuations, nor does it explore whether this balance varies by task type or dataset characteristics.
- What evidence would resolve it: Systematic experiments determining optimal positive/negative ratios for different IE tasks, along with theoretical analysis of why certain ratios work better than others.

## Limitations
- Effectiveness depends heavily on quality of negative sample selection through self-consistency + F1 thresholding
- Retrieval-based approach assumes semantic similarity is primary factor for effective demonstrations
- Limited evaluation to English datasets; performance on low-resource languages unknown

## Confidence
- High confidence in core contribution: Clear performance improvements across multiple datasets and model sizes
- Medium confidence in generalizability: Impressive results but limited to English datasets and CodeLlama models
- Low confidence in negative sampling mechanism: Limited theoretical justification for self-consistency + F1 thresholding approach

## Next Checks
1. **Cross-domain generalization test**: Apply C-ICL to a dataset from a completely different domain (e.g., medical or legal text) to assess whether the contrastive approach maintains its effectiveness when semantic patterns differ significantly from the training domains.

2. **Negative sample quality analysis**: Conduct a detailed error analysis of predictions when using different negative sample selection strategies to identify whether the current approach consistently selects the most informative negative examples or if there are systematic biases in the selection process.

3. **Comparison with fine-tuning baselines**: Implement a lightweight fine-tuning approach on the same datasets and compare its performance against C-ICL to quantify the tradeoff between in-context learning benefits (no parameter updates) and the potential gains from task-specific fine-tuning, especially for larger model sizes where the computational cost difference becomes less significant.