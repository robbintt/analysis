---
ver: rpa2
title: Is the GPU Half-Empty or Half-Full? Practical Scheduling Techniques for LLMs
arxiv_id: '2410.17840'
source_url: https://arxiv.org/abs/2410.17840
tags:
- requests
- request
- serving
- memory
- ttft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses scheduling techniques for LLM serving systems,
  focusing on improving throughput and reducing latency when serving multiple requests
  concurrently. The core method introduces two new scheduling techniques: LARRY, an
  engine-level scheduler that reorders requests based on their anticipated memory
  consumption and current system load, and SAL, a token-aware load balancer that routes
  requests to servers based on their load.'
---

# Is the GPU Half-Empty or Half-Full? Practical Scheduling Techniques for LLMs

## Quick Facts
- arXiv ID: 2410.17840
- Source URL: https://arxiv.org/abs/2410.17840
- Reference count: 40
- Key outcome: LARRY reduces TTFT latency by 1.8-2.1x and SAL reduces TGT by 1.1-1.3x compared to existing methods

## Executive Summary
This paper introduces two practical scheduling techniques for LLM serving systems: LARRY, an engine-level scheduler that reorders requests based on memory consumption and queue length, and SAL, a token-aware load balancer that routes requests to servers based on estimated load. The techniques are designed to be easy to implement as drop-in replacements for existing scheduling policies. The primary results show significant improvements in both time-to-first-token (TTFT) latency and total generation time (TGT) when serving multiple concurrent requests, outperforming current scheduling policies on production workload traces.

## Method Summary
The paper proposes LARRY, an engine-level scheduler that assigns scores to requests based on queue length, memory consumption, and waiting time, then dispatches requests in order of decreasing score. SAL is a load balancer that estimates server load using memory requirements and queued tokens, routing requests to servers with lowest estimated load. Both techniques are implemented within vLLM and evaluated on production workload traces from Azure Function calls and Azure OpenAI, using Llama-3 70B and 8B models in FP8 quantization.

## Key Results
- LARRY significantly reduces TTFT latency by 1.8-2.1x compared to existing methods
- SAL improves load balancing and reduces TGT by 1.1-1.3x over other load balancers
- Both techniques outperform current scheduling policies on production workload traces

## Why This Works (Mechanism)

### Mechanism 1
LARRY improves TTFT by reordering requests based on memory consumption and queue length. It assigns a score using Equation 1: score(r) = queue_len * memory(r) + α * waiting_time, then sorts the waiting queue by decreasing score and dispatches requests accordingly. Core assumption: Prompt tokens dominate KV cache memory usage, and memory pressure varies with system load. Break condition: If prompt length doesn't correlate with memory consumption, LARRY's memory-based prioritization becomes ineffective.

### Mechanism 2
SAL improves load balancing by routing requests to servers with lowest estimated load. It calculates load using Equation 2: load(s,r) = max(β * (memory(r) - free_mem(s)), queued_tokens(s,r)/max_tokens_per_batch), routing requests to servers with lowest load. Core assumption: Load can be accurately estimated using memory requirements and queued tokens, and these metrics are accessible at load balancer. Break condition: If memory requirements or queued tokens don't correlate with actual server load, SAL's routing becomes suboptimal.

### Mechanism 3
The two-phase computation structure of LLMs (prefill and decode) creates opportunities for scheduling optimization. Prefill is compute-bound while decode is memory-bound, allowing different scheduling strategies for each phase. Core assumption: Understanding the computational characteristics of each phase enables better batching and resource allocation decisions. Break condition: If computational characteristics change significantly with model architecture or hardware, the optimization opportunities may diminish.

## Foundational Learning

- Concept: Paged Attention
  - Why needed here: Understanding how Paged Attention dynamically allocates memory to KV caches is crucial for grasping why preemptions occur and how LARRY and SAL optimize scheduling.
  - Quick check question: How does Paged Attention differ from traditional KV cache allocation methods?

- Concept: Continuous Batching
  - Why needed here: LARRY and SAL are designed to work with systems that support Continuous Batching, which allows requests to start and stop at any iteration.
  - Quick check question: What advantages does Continuous Batching provide over traditional batching methods?

- Concept: Roofline model analysis
  - Why needed here: Understanding the computational characteristics of prefill vs decode phases through roofline analysis is key to appreciating why SAL's token-aware load balancing works.
  - Quick check question: How does the roofline model help explain the different computational requirements of prefill and decode phases?

## Architecture Onboarding

- Component map: Load balancer -> Engine-level scheduler -> KV cache -> Paged Attention system -> Continuous Batching system
- Critical path: 1. Request arrives at load balancer 2. Load balancer routes to server using SAL 3. Server's engine-level scheduler manages request using LARRY 4. Request executes with Paged Attention for memory management 5. Request completes and resources are freed
- Design tradeoffs: Memory vs latency: LARRY trades off some tail latency for improved median latency; Complexity vs performance: SAL adds some complexity but provides measurable improvements; Polling frequency vs overhead: SAL needs to poll servers frequently but must minimize overhead
- Failure signatures: High preemption rates: Indicates memory pressure issues; Uneven server load: Suggests SAL isn't working effectively; Increased tail latency: May indicate LARRY's α parameter needs adjustment
- First 3 experiments: 1. Implement LARRY in a single-server setup and measure TTFT improvement over FCFS 2. Add SAL to a multi-server setup and compare load balancing to random routing 3. Vary α parameter in LARRY and observe effects on p50 vs p95 latencies

## Open Questions the Paper Calls Out

### Open Question 1
How does LARRY's performance compare to more complex scheduling algorithms that incorporate additional request features beyond prompt length? The paper states LARRY uses only prompt length for memory estimation, while acknowledging that other features could potentially improve scheduling decisions. A controlled experiment comparing LARRY to scheduling algorithms that incorporate additional request features would resolve this question.

### Open Question 2
What is the optimal polling frequency for SAL to maintain accurate server load information without introducing significant overhead? The paper states SAL polls server statistics 10 times per second but does not explore whether this frequency is optimal. A systematic evaluation of SAL's performance across different polling frequencies would resolve this question.

### Open Question 3
How would LARRY and SAL perform on workload traces with significantly different characteristics, such as those with much longer response lengths or different input/output token distributions? The paper evaluates LARRY and SAL on two specific workload traces but does not explore their performance on workloads with substantially different characteristics. Evaluations on a diverse set of workload traces would resolve this question.

## Limitations
- Production trace validity: Relies on Azure traces from 2023 that may not capture recent usage patterns
- Single-model focus: Evaluation primarily uses Llama-3 70B and 8B models, limiting generalizability
- Multi-GPU scalability: Evaluation focuses on single-node deployments with 1-8 GPUs, not addressing multi-node scenarios

## Confidence

High Confidence (8/10):
- LARRY's TTFT improvement (1.8-2.1x) over FCFS is well-supported by experiments across multiple configurations and workloads
- SAL's load balancing improvement (1.1-1.3x TGT reduction) is consistently demonstrated across different server counts and load conditions
- The mechanism of memory-pressure-driven scheduling in LARRY is theoretically sound given Paged Attention's memory allocation patterns

Medium Confidence (6/10):
- The parameter sensitivity analysis for α and β is limited to specific ranges without exploring the full parameter space
- The comparison with TRAIL+ shows competitive performance, but lacks detailed analysis of why LARRY outperforms in certain scenarios
- The production trace evaluation assumes Azure workloads are representative without rigorous validation

Low Confidence (4/10):
- The claim that LARRY and SAL are "easy to implement as drop-in replacements" lacks quantitative support on implementation effort
- The scalability claims beyond 8 GPUs are extrapolated rather than experimentally verified

## Next Checks
1. Systematically sweep α values from 0.01 to 10.0 in LARRY and β values from 0.1 to 5.0 in SAL to identify optimal ranges and understand sensitivity to parameter choices
2. Implement LARRY and SAL on a diverse set of models including Mistral, Gemma, and different quantization formats (FP16, BF16)
3. Extend evaluation to multi-node deployments with network-based load balancing and cross-node memory management