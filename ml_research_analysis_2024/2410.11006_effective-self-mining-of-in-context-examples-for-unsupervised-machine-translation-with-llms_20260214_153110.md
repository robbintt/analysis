---
ver: rpa2
title: Effective Self-Mining of In-Context Examples for Unsupervised Machine Translation
  with LLMs
arxiv_id: '2410.11006'
source_url: https://arxiv.org/abs/2410.11006
tags:
- latn
- cyrl
- language
- arab
- translation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose an unsupervised approach to mine in-context
  examples for machine translation, enabling unsupervised MT across different languages.
  The method starts with word-level mining to acquire word translations, then performs
  sentence-level mining.
---

# Effective Self-Mining of In-Context Examples for Unsupervised Machine Translation with LLMs

## Quick Facts
- arXiv ID: 2410.11006
- Source URL: https://arxiv.org/abs/2410.11006
- Authors: Abdellah El Mekki; Muhammad Abdul-Mageed
- Reference count: 40
- Key outcome: Unsupervised MT approach achieves 7 BLEU point improvement over UMT baselines using LLM-mined in-context examples

## Executive Summary
This paper introduces an unsupervised approach for mining in-context examples for machine translation using large language models (LLMs). The method starts with word-level mining to acquire word translations, then performs sentence-level mining. To address potential noise in mined parallel pairs, a filtering criterion selects optimal in-context examples from unsupervised parallel sentences. The approach is evaluated on 288 language directions from the FLORES-200 dataset using two multilingual LLMs, analyzing the impact of linguistic features on performance. Results show the unsupervised approach leads to better or comparable translation performance as regular in-context samples, while outperforming state-of-the-art UMT methods by an average of 7 BLEU points.

## Method Summary
The method uses a two-stage unsupervised approach: word-level translation to create synthetic parallel data, then sentence-level translation. It employs TopK+BM25 selection using cosine similarity and BM25 scores to filter and select optimal in-context examples. The pipeline uses monolingual vocabularies from FastText embeddings, unlabeled sentence sets from FLORES-200 validation sets, and two multilingual LLMs (Llama-3 8B and Bloom 7B). The approach mines kwp=10 word pairs for word translation, then uses k=8 sentence pairs for final translation with a similarity threshold τ=0.90 for filtering.

## Key Results
- Outperforms state-of-the-art UMT methods by an average of 7 BLEU points
- Achieves comparable or better performance than regular in-context learning with human-annotated examples
- Shows effectiveness across 288 language directions from FLORES-200 dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Word-by-word translation preserves semantic meaning even if word order and grammar are incorrect
- Mechanism: The approach uses LLM-generated word translations to create synthetic parallel data, then relies on sentence-level translation to correct grammatical structure
- Core assumption: Semantic meaning is preserved in word-by-word translations despite syntactic errors
- Evidence anchors:
  - [abstract]: "Although these translations do not maintain accurate word or grammatical structure, we hypothesize they would preserve the semantic meaning of the original sentences."
  - [section 3.1.3]: "We use the kwp mined word pair as in-context examples to translate words in each sentence of DT, resulting in a set of sentences Dw2w that have been translated word-by-word from Ls to Lt."
- Break condition: If semantic drift is significant between source and word-by-word translated sentences, the back-translation step cannot recover accurate translations

### Mechanism 2
- Claim: Back-translation corrects word order and grammatical issues in synthetic parallel data
- Mechanism: The approach reverses translation direction using word-by-word translations as source and original sentences as target, creating natural language sentence pairs
- Core assumption: Back-translation can reconstruct proper word order and grammar from semantically preserved word-by-word translations
- Evidence anchors:
  - [section 3.2.1]: "To address these issues, we follow a back-translation step: we reverse the translation direction by using the word-by-word target translations as the source and the original source sentences as the target."
  - [section 3.2.2]: "Following the back-translation step, we acquire a set of sentence pairs that both have natural language and that correct the word-by-word translation word order and grammar issues."
- Break condition: If word-by-word translations lose too much semantic information, back-translation cannot recover accurate translations

### Mechanism 3
- Claim: Combining cosine similarity with BM25 scores selects optimal in-context examples
- Mechanism: The approach filters sentence pairs by semantic similarity threshold, then applies BM25 to select examples with highest relevance and surface-level linguistic similarity
- Core assumption: BM25 complements semantic similarity by capturing surface-level linguistic features important for translation quality
- Evidence anchors:
  - [section 3.2.2]: "we propose TopK+BM25, a method to select the optimal k examples from these generated pairs. Specifically, for each test example in DT, we identify the most relevant sentence pairs based on two criteria: (i) selecting pairs where the cosine similarity score...exceeds a specified threshold τ, and (ii) from these selected pairs, choosing the k pairs that yield the highest BM25 similarity score"
  - [section 5.2]: "Table 1 illustrates a decline in performance when selecting the most confident pairs (TopK ICL) compared to Random ICL. Our analysis suggests that this decrease occurs because the similarity function prioritizes semantic confidence while disregarding linguistic details"
- Break condition: If semantic similarity and BM25 scores conflict significantly, the selection may not yield optimal examples

## Foundational Learning

- Concept: Bilingual Lexicon Induction (BLI)
  - Why needed here: The method builds on BLI techniques for word-level translation before sentence-level mining
  - Quick check question: What is the difference between supervised and unsupervised BLI approaches?

- Concept: Back-translation in machine translation
  - Why needed here: The approach uses back-translation to convert word-by-word translations into natural language sentence pairs
  - Quick check question: How does back-translation help improve translation quality when parallel data is limited?

- Concept: In-context learning sensitivity to example selection
  - Why needed here: The method addresses the challenge of selecting optimal examples when unsupervised mining produces noisy data
  - Quick check question: What factors influence the effectiveness of in-context examples in LLM-based machine translation?

## Architecture Onboarding

- Component map: Word pair mining -> Word-by-word translation -> Back-translation -> TopK+BM25 filtering -> Translation

- Critical path: Word pair mining → Word-by-word translation → Back-translation → TopK+BM25 filtering → Translation

- Design tradeoffs:
  - Word-level vs sentence-level translation: Word-level provides semantic preservation but loses syntax; sentence-level corrects syntax but needs better semantic alignment
  - Random vs BM25 selection: Random is simple but may miss relevant examples; BM25 captures surface features but may miss semantic relevance
  - Similarity threshold tuning: Higher thresholds ensure quality but reduce example pool; lower thresholds increase coverage but risk noise

- Failure signatures:
  - Poor translation quality: Indicates word pair mining produced inaccurate translations or back-translation failed to correct structure
  - High variance in results: Suggests similarity threshold or BM25 parameters need tuning for specific language pairs
  - Slow performance: May indicate inefficient LLM prompting or excessive parallel pair generation

- First 3 experiments:
  1. Test word pair mining accuracy on a small vocabulary to verify semantic preservation
  2. Evaluate back-translation quality on synthetic word-by-word translations to ensure grammatical correction
  3. Compare TopK vs Random vs TopK+BM25 selection methods on a validation set to find optimal filtering approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of unsupervised MT vary when using larger multilingual LLMs beyond the 8B models tested in this paper?
- Basis in paper: [explicit] The authors tested their approach using Llama-3 (8B) and Bloom (7B) models, but larger models like GPT-4 or Gemini were not evaluated.
- Why unresolved: The paper only provides results for two relatively small multilingual LLMs, leaving the potential impact of larger models on unsupervised MT performance unexplored.
- What evidence would resolve it: Experiments comparing the proposed unsupervised MT approach across a range of multilingual LLM sizes (e.g., 7B, 13B, 34B, 65B+) using the same methodology and evaluation datasets.

### Open Question 2
- Question: How does the quality of unsupervised mined in-context examples compare to human-annotated examples across different language families and scripts?
- Basis in paper: [inferred] The authors demonstrate that their unsupervised approach achieves comparable performance to regular ICL using human-annotated data, but do not provide a detailed analysis of quality differences across linguistic dimensions.
- Why unresolved: While the paper shows competitive BLEU scores, it does not investigate the specific characteristics or quality metrics of the mined examples versus human-annotated ones across diverse language pairs.
- What evidence would resolve it: A detailed comparison of mined versus human-annotated examples using multiple quality metrics (semantic similarity, fluency, adequacy) across different language families and scripts, potentially including human evaluation studies.

### Open Question 3
- Question: What is the impact of different word embedding models on the quality of the initial word-level translations in the unsupervised mining pipeline?
- Basis in paper: [explicit] The authors use FastText embeddings for word-level translation but note that XLM-R embeddings are used for sentence similarity filtering, acknowledging limitations of XLM-R coverage for low-resource languages.
- Why unresolved: The paper uses FastText for word-level translation without exploring how alternative word embedding models might affect the quality of the initial translations and downstream performance.
- What evidence would resolve it: Experiments comparing the unsupervised MT pipeline using different word embedding models (e.g., FastText, XLM-R, LaBSE) for the word-level translation phase while keeping all other components constant.

## Limitations
- Relies on semantic preservation through word-by-word translation, which may not hold for morphologically rich or syntactically distant language pairs
- Assumes back-translation reliably corrects syntactic errors, which may break down for languages with very different grammatical structures
- Performance may vary significantly across different language families and resource levels

## Confidence
- **High Confidence (Mechanistic):** The core framework of using word-level mining followed by sentence-level translation and BM25-based filtering is well-specified and reproducible
- **Medium Confidence (Performance Claims):** The 7 BLEU point improvement over UMT baselines is promising but based on specific conditions (Llama-3 8B, Bloom 7B, FLORES-200)
- **Low Confidence (Generalizability):** The approach's effectiveness across diverse language pairs, especially distant language pairs or low-resource languages, remains uncertain

## Next Checks
1. Test the word pair mining accuracy on morphologically rich languages (e.g., Turkish, Finnish) to verify semantic preservation holds across diverse language families
2. Evaluate the back-translation quality when word-by-word translations have high syntactic divergence from the source (e.g., SVO vs SOV languages)
3. Compare TopK+BM25 performance against supervised example selection on a subset of FLORES-200 to quantify the cost of unsupervised mining