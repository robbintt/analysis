---
ver: rpa2
title: Online Iterative Reinforcement Learning from Human Feedback with General Preference
  Model
arxiv_id: '2402.07314'
source_url: https://arxiv.org/abs/2402.07314
tags:
- preference
- arxiv
- learning
- policy
- should
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates reinforcement learning from human feedback
  (RLHF) under a general preference oracle without assuming a reward function or the
  Bradley-Terry model. The authors formulate the problem as a reverse-KL regularized
  minimax game between two large language models and propose sample-efficient algorithms
  for both offline and online settings.
---

# Online Iterative Reinforcement Learning from Human Feedback with General Preference Model

## Quick Facts
- arXiv ID: 2402.07314
- Source URL: https://arxiv.org/abs/2402.07314
- Reference count: 40
- This paper investigates reinforcement learning from human feedback (RLHF) under a general preference oracle without assuming a reward function or the Bradley-Terry model

## Executive Summary
This paper presents a novel approach to reinforcement learning from human feedback (RLHF) that operates without requiring a reward function or specific preference models like Bradley-Terry. The authors formulate the problem as a reverse-KL regularized minimax game between two large language models, enabling sample-efficient learning in both offline and online settings. Theoretical analysis provides finite-sample guarantees while practical experiments demonstrate improved performance over existing methods like DPO and IPO.

The key innovation is the general preference oracle framework, which is shown to be strictly more general than reward-based RLHF and can capture non-transitive preferences. The proposed online iterative algorithm achieves state-of-the-art results in aligning language models with human preferences, addressing a critical challenge in AI alignment where human feedback is often noisy and doesn't conform to simple reward structures.

## Method Summary
The authors formulate RLHF as a reverse-KL regularized minimax game between two large language models, where one model generates responses and the other evaluates preferences. This approach operates under a general preference oracle without assuming a reward function or specific preference models. The method includes both offline and online iterative algorithms, with the online version continuously updating based on new human feedback. Theoretical analysis establishes finite-sample guarantees for both settings, and practical implementations demonstrate improved performance over existing approaches.

## Key Results
- Theoretical finite-sample guarantees established for both offline and online settings
- Experimental results show improved performance over DPO and IPO
- Approach demonstrated to be strictly more general than reward-based RLHF
- Online iterative algorithm achieves state-of-the-art results in language model alignment

## Why This Works (Mechanism)
The method works by framing RLHF as a minimax game between two language models, where one generates responses and the other evaluates preferences. The reverse-KL regularization ensures stable learning by preventing the generator from drifting too far from its prior. This formulation is more general than reward-based approaches because it directly optimizes for preference satisfaction rather than proxy reward functions, allowing it to capture complex preference structures including non-transitive relationships.

## Foundational Learning
- **Preference Oracles**: Why needed - provide human feedback without requiring explicit reward functions; Quick check - test with synthetic preference data
- **Reverse-KL Regularization**: Why needed - stabilizes learning and prevents mode collapse; Quick check - compare with forward-KL in ablation studies
- **Minimax Game Framework**: Why needed - enables adversarial training between generator and evaluator; Quick check - verify convergence properties
- **Online Iterative Updates**: Why needed - allows continuous learning from streaming human feedback; Quick check - measure performance over time
- **Non-transitive Preferences**: Why needed - captures realistic human preferences that don't follow simple ordering; Quick check - test with deliberately non-transitive preference sets
- **Finite-sample Guarantees**: Why needed - provides theoretical confidence in algorithm performance; Quick check - verify bounds hold in practice

## Architecture Onboarding

**Component Map**
Generator LLM -> Preference Evaluator LLM -> Preference Oracle -> (update Generator)

**Critical Path**
1. Generate response using current policy
2. Obtain preference feedback from oracle
3. Update preference evaluator
4. Update generator using reverse-KL objective
5. Repeat with new feedback

**Design Tradeoffs**
- **Complexity vs Generality**: More general preference models require more complex algorithms but can handle richer preference structures
- **Sample Efficiency vs Computational Cost**: Online iterative updates are more sample-efficient but computationally expensive
- **Stability vs Expressiveness**: Reverse-KL regularization provides stability but may limit expressiveness compared to other divergences
- **Oracle Quality vs Performance**: Algorithm performance depends heavily on quality of preference oracle

**Failure Signatures**
- **Mode Collapse**: Generator produces limited variety of responses
- **Oscillations**: Performance fluctuates without converging
- **Preference Drift**: Generator adapts to noise in preference oracle rather than true preferences
- **Evaluation Instability**: Preference evaluator fails to provide consistent feedback

**First Experiments**
1. Verify basic functionality with synthetic preference data and known ground truth
2. Test sample efficiency by comparing learning curves with baseline methods
3. Evaluate robustness to noisy preference oracles with varying noise levels

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes access to a "good" preference oracle, but practical quality and consistency remain unclear
- Computational scalability of reverse-KL formulation for large-scale applications with state-of-the-art LLMs needs verification
- Claim about capturing non-transitive preferences requires more rigorous empirical validation across diverse preference structures

## Confidence
High confidence: The general theoretical framework for RLHF without reward function assumptions is well-founded and the finite-sample guarantees are mathematically sound.

Medium confidence: The experimental improvements over DPO and IPO are demonstrated but may not generalize across all alignment tasks. The practical implementation details and their impact on performance are not fully specified.

Low confidence: The claim about capturing non-transitive preferences requires more extensive empirical validation. The computational scalability of the reverse-KL formulation for large-scale applications needs verification.

## Next Checks
1. Conduct extensive experiments testing the method's performance across diverse preference structures, including both transitive and non-transitive preference settings, to validate the theoretical claims about preference model generality.

2. Implement the online iterative algorithm at scale with state-of-the-art LLMs to verify computational feasibility and measure the impact of the reverse-KL regularization on training efficiency and model quality.

3. Design ablation studies comparing the proposed approach with existing methods across multiple alignment benchmarks while controlling for computational resources and data efficiency to better understand the practical advantages.