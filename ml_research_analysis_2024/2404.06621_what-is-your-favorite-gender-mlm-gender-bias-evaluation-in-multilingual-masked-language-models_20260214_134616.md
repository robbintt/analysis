---
ver: rpa2
title: What is Your Favorite Gender, MLM? Gender Bias Evaluation in Multilingual Masked
  Language Models
arxiv_id: '2404.06621'
source_url: https://arxiv.org/abs/2404.06621
tags:
- gender
- bias
- sentences
- language
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study presents a novel approach for evaluating gender bias
  in multilingual masked language models (MLMs) across five languages: English, Chinese,
  German, Portuguese, and Spanish. The method introduces two sentence generation techniques:
  lexicon-based and model-based, along with new evaluation metrics (Strict Bias Metric
  and Direct Comparison Bias Metric) that provide more robust and language-independent
  bias assessment.'
---

# What is Your Favorite Gender, MLM? Gender Bias Evaluation in Multilingual Masked Language Models

## Quick Facts
- arXiv ID: 2404.06621
- Source URL: https://arxiv.org/abs/2404.06621
- Authors: Jeongrok Yu; Seong Ug Kim; Jacob Choi; Jinho D. Choi
- Reference count: 14
- Primary result: Model-based sentence generation produces more stable gender bias evaluations across five languages by retaining more data than lexicon-based methods

## Executive Summary
This study introduces a novel approach for evaluating gender bias in multilingual masked language models (MLMs) across English, Chinese, German, Portuguese, and Spanish. The method combines two sentence generation techniques - lexicon-based and model-based - with new evaluation metrics that provide more robust, language-independent bias assessment. The model-based approach, which uses MLM predictions to generate gender-balanced sentence pairs, demonstrates higher consistency and retains significantly more data than previous methods. Experiments show this method produces more stable and reliable bias evaluations with smaller standard deviations than existing approaches.

## Method Summary
The method employs two sentence generation approaches: lexicon-based sentence generation (LSG) that replaces gendered words with opposites from a multilingual gender lexicon, and model-based sentence generation (MSG) that uses MLM predictions to generate gender-balanced pairs. Three evaluation metrics are introduced: Multilingual Bias Evaluation (MBE), Strict Bias Metric (SBM) that compares only sentences differing by exactly one gendered word, and Direct Comparison Bias Metric (DBM) that quantifies bias by comparing prediction scores of male versus female words. The approach uses the TED parallel corpus and pre-trained BERT-based MLMs for each language.

## Key Results
- MSG retains significantly more data than LSG by discarding fewer sentences during pair generation
- SBM and DBM metrics show smaller standard deviations than existing evaluation methods, indicating more stable bias assessment
- MSG produces consistent bias evaluation results across all five languages while maintaining grammatical correctness
- The method effectively addresses data sensitivity issues in multilingual gender bias assessment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model-based sentence generation (MSG) produces more stable gender bias evaluation by reducing data truncation compared to lexicon-based methods.
- Mechanism: MSG uses MLM predictions to generate gender-balanced sentence pairs, discarding fewer sentences than LSG, which must balance male/female sets by truncating the larger gender group.
- Core assumption: MLM predictions reliably provide high-confidence gendered word substitutions that maintain grammatical correctness.
- Evidence anchors: [abstract]: "The model-based approach, in particular, demonstrates higher consistency and retains more data than previous methods"; [section]: "MSG has an advantage over MBE and LSG as it generally discards significantly fewer sentences compared to the other two methods"

### Mechanism 2
- Claim: Strict Bias Metric (SBM) provides more focused bias measurement by comparing only sentences that differ by exactly one gendered word.
- Mechanism: SBM calculates likelihoods between parallel sentences that vary only by the target gendered word, minimizing confounding factors from unrelated contextual elements.
- Core assumption: Contextual dependencies irrelevant to gender can be effectively isolated by focusing on single-word substitutions.
- Evidence anchors: [section]: "SBM ensures a meaningful assessment by capturing the likelihood differences incurred only by variations in gender words"; [abstract]: "standard deviations smaller than existing approaches"

### Mechanism 3
- Claim: Direct Comparison Bias Metric (DBM) quantifies model bias by comparing prediction scores of male versus female words directly.
- Mechanism: DBM replaces the indicator function with direct comparison of MLM prediction scores for gender word pairs, enabling word-level bias assessment.
- Core assumption: MLM confidence scores for gendered word predictions accurately reflect bias magnitude.
- Evidence anchors: [section]: "DBM is measured by replacing the indicator function I(Sm, Sf ) in Eq. 3 with Id(wm, wf ), where wm ∈ Sm is the predicted male word and wf ∈ Sf is the predicted female word"; [abstract]: "effectively addresses data sensitivity issues in multilingual gender bias assessment"

## Foundational Learning

- Concept: Masked Language Model (MLM) mechanics
  - Why needed here: Understanding how MLMs predict masked tokens and assign confidence scores is fundamental to grasping how MSG and DBM work
  - Quick check question: How does an MLM calculate the likelihood of a word given context, and what determines its confidence score?

- Concept: Parallel corpus alignment
  - Why needed here: The paper relies on TED parallel corpus for extracting sentences across languages, assuming gender information transfers across translations
  - Quick check question: What assumptions are made when using English gender words to identify corresponding gendered content in other languages?

- Concept: Statistical significance in bias evaluation
  - Why needed here: Understanding why multiple evaluation metrics and large datasets are necessary to draw reliable conclusions about bias
  - Quick check question: Why might results "flip" when different scoring metrics are applied to the same dataset, and how does dataset size affect this?

## Architecture Onboarding

- Component map: TED corpus → gender word extraction → sentence pair generation (LSG/MSG) → evaluation datasets → bias scoring (MBE/SBM/DBM)
- Critical path: Sentence extraction → pair generation → bias scoring → result aggregation
- Design tradeoffs: LSG provides controlled comparisons but loses data through truncation; MSG retains more data but depends on MLM prediction quality
- Failure signatures: High standard deviation across folds indicates data sensitivity; mismatch between LSG and MSG results suggests language/model-specific effects
- First 3 experiments:
  1. Run LSG on a small language subset and measure truncation rate versus MSG
  2. Compare SBM scores across five folds to quantify stability
  3. Test DBM on a balanced corpus versus the TED corpus to observe score inflation from gender imbalance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of model-based sentence generation (MSG) compare to lexicon-based sentence generation (LSG) across different languages and datasets?
- Basis in paper: [explicit] The paper compares the performance of MSG and LSG in terms of consistency, data retention, and bias evaluation results across five languages: English, Chinese, German, Portuguese, and Spanish.
- Why unresolved: While the paper presents a comprehensive comparison of MSG and LSG, the results may vary depending on the specific dataset and language used. Additionally, the paper does not explore the potential impact of different threshold values on the performance of MSG.
- What evidence would resolve it: Further experiments comparing the performance of MSG and LSG across different datasets and languages, as well as investigating the effect of varying threshold values on MSG's performance, would provide more insights into their relative strengths and weaknesses.

### Open Question 2
- Question: How does the proposed model-based sentence generation (MSG) method affect the diversity of gender words in the generated sentences compared to lexicon-based sentence generation (LSG)?
- Basis in paper: [explicit] The paper mentions that MSG may produce less diversity in gender words than LSG, as shown in Figure 4.
- Why unresolved: While the paper provides some evidence of reduced diversity in gender words using MSG, it does not explore the potential impact of this limitation on the overall bias evaluation results. Additionally, the paper does not investigate whether incorporating a diverse set of vocabulary from one-gender predictions obtained using LSG can improve the performance of MSG.
- What evidence would resolve it: Further experiments comparing the diversity of gender words in sentences generated by MSG and LSG, as well as investigating the impact of incorporating diverse vocabulary on MSG's performance, would provide more insights into this issue.

### Open Question 3
- Question: How does the length of sentences in language models impact the gender bias scores obtained using the proposed evaluation methods?
- Basis in paper: [inferred] The paper mentions that the proposed methods can be used to evaluate gender bias in any corpus, not just parallel corpora. However, it does not explicitly discuss the potential impact of sentence length on the evaluation results.
- Why unresolved: While the paper presents a comprehensive evaluation framework for gender bias in multilingual masked language models, it does not investigate the potential influence of sentence length on the obtained bias scores. This could be an important factor to consider, especially when comparing bias scores across different languages or datasets.
- What evidence would resolve it: Further experiments comparing the gender bias scores obtained using the proposed methods for sentences of varying lengths, as well as investigating the potential impact of sentence length on the evaluation results, would provide more insights into this issue.

## Limitations

- The approach relies on BERT-based MLMs, and performance may vary significantly with other transformer architectures
- Multilingual gender lexicon coverage varies widely across languages (20-85%), with Arabic at only 19% coverage
- The paper doesn't adequately validate whether bias scores correlate with actual bias manifestations in downstream applications

## Confidence

**High confidence**: MSG retains more data than lexicon-based methods. This is directly observable from corpus statistics and the mechanism (discarding fewer sentences) is straightforward and verifiable.

**Medium confidence**: MSG produces more stable bias evaluations with smaller standard deviations. While the paper claims this advantage, the evidence relies on comparisons to previous methods without showing comprehensive variance analysis across all languages and metrics.

**Medium confidence**: The proposed metrics (SBM and DBM) provide more focused bias measurement. The theoretical justification is sound, but empirical validation across diverse downstream tasks is limited.

## Next Checks

1. Cross-architecture validation: Test MSG and the new metrics on non-BERT MLMs (e.g., XLM-R, mBERT) for each language to verify whether the claimed advantages generalize beyond the specific model family used in the study.

2. Coverage gap analysis: Systematically analyze the relationship between MGL coverage rates and bias measurement stability. For languages with low coverage (like Arabic at 19%), compare MSG results with alternative gender identification methods to determine if the approach breaks down or requires adaptation.

3. Downstream task correlation: Evaluate whether the bias scores from SBM and DBM correlate with actual bias manifestations in practical applications (coreference resolution, occupation prediction, etc.) to validate that these metrics capture meaningful bias rather than just statistical artifacts.