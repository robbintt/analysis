---
ver: rpa2
title: Rethinking model prototyping through the MedMNIST+ dataset collection
arxiv_id: '2404.15786'
source_url: https://arxiv.org/abs/2404.15786
tags:
- vit-b
- training
- across
- performance
- resolution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work provides a comprehensive benchmark for the MedMNIST+
  dataset collection, which extends the original MedMNIST v2 to include higher image
  resolutions (64x64, 128x128, and 224x224 pixels). We systematically evaluate common
  convolutional neural networks (CNNs) and Vision Transformers (ViTs) across different
  medical datasets, training methodologies, and input resolutions to reassess assumptions
  about model effectiveness and development.
---

# Rethinking model prototyping through the MedMNIST+ dataset collection

## Quick Facts
- arXiv ID: 2404.15786
- Source URL: https://arxiv.org/abs/2404.15786
- Reference count: 37
- This work provides a comprehensive benchmark for the MedMNIST+ dataset collection, extending MedMNIST v2 with higher resolutions (64x64, 128x128, 224x224 pixels) and systematically evaluating CNNs and ViTs across different medical datasets.

## Executive Summary
This study presents a comprehensive benchmark for the MedMNIST+ dataset collection, which extends the original MedMNIST v2 by incorporating higher image resolutions. The authors systematically evaluate common convolutional neural networks (CNNs) and Vision Transformers (ViTs) across various medical datasets, training methodologies, and input resolutions to reassess assumptions about model effectiveness and development. The research aims to provide insights into optimal model selection and training strategies for medical image classification tasks, while promoting transparency and reproducibility in the field.

## Method Summary
The study systematically evaluates model performance on the MedMNIST+ dataset collection, which includes 14 medical imaging datasets with varying image resolutions (64x64, 128x128, and 224x224 pixels). The authors compare common CNNs and Vision Transformers across different training methodologies, including end-to-end training and transfer learning approaches. They assess computational efficiency and performance trade-offs, examining how different input resolutions affect model accuracy. The evaluation framework is designed to be standardized and reproducible, allowing for consistent comparisons across different model architectures and datasets.

## Key Results
- CNNs remain competitive with ViTs in medical image classification tasks across different resolutions
- Higher image resolutions beyond 128x128 pixels do not consistently improve performance
- Computationally efficient training schemes and foundation models offer viable alternatives to costly end-to-end training

## Why This Works (Mechanism)
The study's methodology is effective because it provides a standardized evaluation framework that allows for consistent comparisons across different model architectures, datasets, and training approaches. By systematically varying input resolutions and comparing CNNs with ViTs, the authors can identify optimal configurations for medical image classification tasks. The inclusion of computationally efficient training schemes and foundation models addresses practical concerns about resource utilization in real-world applications.

## Foundational Learning
1. **Medical Image Classification** - Understanding the unique challenges and characteristics of medical imaging data compared to natural images, including domain-specific patterns and noise.
   - Why needed: Medical images often have different visual characteristics and noise profiles than natural images, requiring specialized approaches.
   - Quick check: Verify that the dataset collection includes diverse medical imaging modalities and pathologies.

2. **Model Architecture Comparison** - Knowledge of the strengths and limitations of CNNs versus Vision Transformers for different types of image classification tasks.
   - Why needed: Different architectures have varying capabilities in capturing spatial relationships and handling different input resolutions.
   - Quick check: Ensure the study includes a range of CNN and ViT variants to capture architectural differences.

3. **Computational Efficiency in Deep Learning** - Understanding the trade-offs between model performance and computational resources, including training time and hardware requirements.
   - Why needed: Real-world medical applications often have resource constraints that influence model selection and deployment.
   - Quick check: Verify that computational efficiency metrics are included alongside performance metrics.

## Architecture Onboarding

### Component Map
Data Preprocessing -> Model Training -> Performance Evaluation -> Computational Efficiency Analysis

### Critical Path
1. Data Preprocessing: Standardizing image resolutions and formats across datasets
2. Model Training: Implementing various CNN and ViT architectures with different training methodologies
3. Performance Evaluation: Assessing model accuracy and comparing across resolutions and architectures
4. Computational Efficiency Analysis: Measuring resource utilization and training time for different approaches

### Design Tradeoffs
The study balances between comprehensive evaluation (testing multiple architectures and resolutions) and practical applicability (focusing on commonly used models and efficient training schemes). This approach ensures the findings are both scientifically rigorous and practically relevant for real-world medical AI development.

### Failure Signatures
Potential issues include overfitting to specific datasets, bias in model selection, and limited generalizability beyond the MedMNIST+ collection. The study addresses these by using a diverse set of medical imaging datasets and standardized evaluation protocols.

### 3 First Experiments
1. Baseline performance comparison between standard CNNs and ViTs on low-resolution (64x64) medical images
2. Evaluation of transfer learning effectiveness using foundation models versus end-to-end training
3. Analysis of performance degradation when using resolutions below 128x128 pixels

## Open Questions the Paper Calls Out
None

## Limitations
- Findings are based on a specific set of 14 medical imaging datasets within MedMNIST+, limiting generalizability to all medical imaging domains
- Focus on image classification tasks may not apply to other medical AI tasks such as segmentation or detection
- Evaluation primarily considers standard CNNs and ViTs without exploring more recent architectural innovations

## Confidence

**High Confidence:**
- CNNs remain competitive with ViTs in medical image classification tasks
- Higher resolutions beyond 128x128 pixels do not consistently improve performance

**Medium Confidence:**
- Computationally efficient training schemes and foundation models as alternatives to end-to-end training
- Effectiveness of lower resolutions for prototyping

**Low Confidence:**
- Generalizability of findings to non-MedMNIST+ datasets with different modalities and resolutions
- Universal applicability of the 128x128 resolution threshold

## Next Checks
1. Test the proposed resolution thresholds and model performance patterns on non-MedMNIST+ medical imaging datasets, particularly those with different modalities (e.g., CT, MRI) and higher native resolutions.

2. Evaluate additional modern architectures beyond standard CNNs and ViTs, including recent hybrid models and task-specific designs, to verify if the competitive advantage of CNNs holds across a broader architectural spectrum.

3. Conduct a cost-benefit analysis comparing foundation model-based approaches with traditional end-to-end training across different resource constraints (computational, time, expertise) to validate the practical efficiency claims in real-world clinical settings.