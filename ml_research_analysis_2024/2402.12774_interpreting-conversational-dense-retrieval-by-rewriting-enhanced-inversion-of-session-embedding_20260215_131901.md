---
ver: rpa2
title: Interpreting Conversational Dense Retrieval by Rewriting-Enhanced Inversion
  of Session Embedding
arxiv_id: '2402.12774'
source_url: https://arxiv.org/abs/2402.12774
tags:
- retrieval
- conversational
- session
- text
- query
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CONVINV, a method that interprets conversational
  dense retrieval models by transforming session embeddings into interpretable text
  while preserving retrieval performance. CONVINV leverages Vec2Text to invert session
  embeddings into text, enhanced with external query rewrites for improved interpretability.
---

# Interpreting Conversational Dense Retrieval by Rewriting-Enhanced Inversion of Session Embedding

## Quick Facts
- arXiv ID: 2402.12774
- Source URL: https://arxiv.org/abs/2402.12774
- Authors: Yiruo Cheng; Kelong Mao; Zhicheng Dou
- Reference count: 23
- Primary result: CONVINV achieves up to 0.985 cosine similarity between original session embeddings and transformed text embeddings, with improved interpretability over baselines

## Executive Summary
This paper introduces CONVINV, a method for interpreting conversational dense retrieval (CDR) models by transforming session embeddings into interpretable text while preserving retrieval performance. CONVINV leverages the shared embedding space between conversational session encoders and ad-hoc query encoders, using Vec2Text to invert session embeddings into text. The method is enhanced with external query rewrites to improve interpretability. Evaluations on three conversational search benchmarks show that CONVINV generates more interpretable text than baselines while faithfully restoring original retrieval performance, with human evaluations confirming clearer, more coherent, and more complete output.

## Method Summary
CONVINV transforms session embeddings from conversational dense retrieval models into interpretable text using a Vec2Text model trained on ad-hoc queries. The method assumes session and query embeddings share the same embedding space due to shared passage encoders. CONVINV incorporates external query rewrites to enhance interpretability by using them as initial inverted text for the Vec2Text correction process. The approach is evaluated on QReCC and TREC CAsT datasets using retrieval performance metrics (MRR, NDCG@3, Recall@100), embedding similarity (cosine similarity), and human interpretability evaluations.

## Key Results
- CONVINV achieves up to 0.985 cosine similarity between original session embeddings and transformed text embeddings
- Human evaluation scores indicate CONVINV produces clearer, more coherent, and more complete text compared to baselines
- CONVINV preserves retrieval performance while improving interpretability across three conversational search benchmarks
- The method generalizes across different base retrievers (KD-GTR, Conv-GTR) with consistent performance improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CONVINV transforms session embeddings into interpretable text by leveraging the shared embedding space between conversational session encoders and ad-hoc query encoders.
- Mechanism: The session encoder is initialized from an ad-hoc query encoder and the passage encoder is frozen during training, creating a shared embedding space for retrieval. CONVINV trains a Vec2Text model based on the ad-hoc query encoder to transform session embeddings into text that maintains similar retrieval performance.
- Core assumption: The shared embedding space assumption holds true across different conversational dense retrieval architectures and training paradigms.
- Evidence anchors:
  - [abstract] "leveraging the fact that the session and query embeddings share the same space in existing conversational dense retrieval"
  - [section] "Therefore, we may assume that the session encoder and the ad-hoc query encoder share the same embedding space for retrieval as they share the same passage encoder."
- Break condition: If the session encoder is not initialized from an ad-hoc query encoder or if the passage encoder is not frozen during training, the shared embedding space assumption breaks down.

### Mechanism 2
- Claim: CONVINV enhances interpretability by incorporating external query rewrites into the transformation process.
- Mechanism: CONVINV uses a conversational query rewriting model (like T5QR) to generate standalone query rewrites from the conversational session. These rewrites are then used as the initial inverted text for the Vec2Text correction process, guiding the transformation toward more interpretable output.
- Core assumption: External query rewrites provide meaningful semantic guidance that improves the interpretability of transformed text without sacrificing retrieval performance.
- Evidence anchors:
  - [abstract] "To further enhance interpretability, we propose to incorporate external interpretable query rewrites into the transformation process."
  - [section] "We propose a simple method to leverage external query rewrites to enhance the interpretability. Specifically, we first employ a conversational query rewriting model R... to transform the conversational search session... Then, in the generation process of Vec2Text, we discard the initial inversion process and directly use the query rewrite q*i as the initial inverted text tinv."
- Break condition: If the external query rewrites are of poor quality or semantically misaligned with the session embeddings, the interpretability enhancement may fail or even degrade performance.

### Mechanism 3
- Claim: The iterative correction process in Vec2Text progressively refines initial inverted text to closely match the target session embedding.
- Mechanism: Vec2Text uses a correction model that iteratively updates hypothesis text and its embedding by minimizing the difference between hypothesis embedding and target embedding, using the difference as discrete adjustments to the text hypothesis.
- Core assumption: The correction model can effectively navigate the discrete text space to find text that closely matches the target embedding through iterative refinement.
- Evidence anchors:
  - [abstract] "The correction step, where a correction model then progressively refines this initial inverted text tinv to be more accurate."
  - [section] "p(x(t+1)|e) = Σ p(x(t)|e) p(x(t+1)|e, x(t), e^(t)) where e^(t) = φ(x(t))."
- Break condition: If the correction model gets stuck in local minima or the text space is too sparse for effective navigation, the iterative refinement may fail to produce meaningful text.

## Foundational Learning

- Concept: Dense retrieval and embedding spaces
  - Why needed here: Understanding how session embeddings and passage embeddings interact in a unified space is crucial for grasping why CONVINV works
  - Quick check question: Why does freezing the passage encoder during session encoder training create a shared embedding space?

- Concept: Text embedding inversion
  - Why needed here: The core mechanism of CONVINV relies on inverting embeddings back to text, so understanding how this works is essential
  - Quick check question: How does Vec2Text use the difference between hypothesis and target embeddings to make discrete adjustments to text?

- Concept: Conversational search and query rewriting
  - Why needed here: CONVINV builds on existing conversational search paradigms and query rewriting techniques
  - Quick check question: What's the key difference between conversational query rewriting and conversational dense retrieval?

## Architecture Onboarding

- Component map:
  - Session Encoder (initialized from ad-hoc query encoder)
  - Passage Encoder (frozen during training)
  - Vec2Text Inversion Model
  - Vec2Text Correction Model
  - Conversational Query Rewriting Model (optional)
  - Ad-hoc Query Encoder (for transformed text encoding)

- Critical path:
  1. Session embedding generation: Es(qi, Hi) → si
  2. Text transformation: Vec2Text(φq) applied to si → ˆqi
  3. Retrieval performance validation: Eq(ˆqi) vs si
  4. Interpretability enhancement (optional): Query rewriting → initial inverted text

- Design tradeoffs:
  - Training Vec2Text on ad-hoc queries vs conversational sessions: Ad-hoc queries provide larger training data but may lack conversational context
  - Using external query rewrites vs learned inversion: External rewrites improve interpretability but add dependency on rewrite quality
  - Iterative correction depth: More iterations improve embedding match but increase computation

- Failure signatures:
  - Low cosine similarity between session embeddings and transformed text embeddings indicates poor reconstruction
  - Significant drop in retrieval performance indicates loss of semantic meaning during transformation
  - Poor human evaluation scores indicate lack of interpretability despite good reconstruction

- First 3 experiments:
  1. Baseline test: Apply Vec2Text to session embeddings without rewriting enhancement and measure reconstruction quality
  2. Ablation test: Compare rewriting enhancement vs no rewriting enhancement on interpretability metrics
  3. Cross-retriever test: Apply CONVINV to session embeddings from different base retrievers (GTR, ANCE, BGE) and measure generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the incorporation of query rewrites impact the retrieval performance of session embeddings across different conversational search benchmarks?
- Basis in paper: Explicit - The paper mentions that using query rewrites as the initial inverted text improves interpretability and retrieval performance, but the specific impact across different benchmarks is not fully explored.
- Why unresolved: The paper only provides a general observation without detailed analysis of the impact across various benchmarks.
- What evidence would resolve it: Conducting a detailed comparative study across multiple benchmarks, analyzing the retrieval performance with and without query rewrites, and identifying patterns or trends.

### Open Question 2
- Question: Can the transformed text generated by CONVINV consistently outperform human rewrites in terms of retrieval performance and interpretability?
- Basis in paper: Explicit - The paper suggests that CONVINV can sometimes yield slightly better retrieval performance than the original session embeddings, but it does not compare this to human rewrites.
- Why unresolved: The paper does not provide a direct comparison between the transformed text and human rewrites in terms of both retrieval performance and interpretability.
- What evidence would resolve it: Conducting experiments that directly compare the transformed text generated by CONVINV with human rewrites, evaluating both retrieval performance and interpretability using human evaluations.

### Open Question 3
- Question: How does the choice of ad-hoc retriever affect the interpretability and retrieval performance of transformed text in CONVINV?
- Basis in paper: Explicit - The paper investigates the universality of CONVINV by changing the base ad-hoc retriever, but the detailed impact on interpretability and retrieval performance is not fully explored.
- Why unresolved: The paper provides some results but lacks a comprehensive analysis of how different ad-hoc retrievers influence the outcomes.
- What evidence would resolve it: Performing a thorough analysis of the impact of different ad-hoc retrievers on the interpretability and retrieval performance of transformed text, possibly including additional metrics or benchmarks.

## Limitations

- The approach relies heavily on the assumption that session and ad-hoc query embeddings share the same space, which may not hold across different conversational dense retrieval architectures.
- The evaluation focuses primarily on retrieval performance metrics and human interpretability scores, but lacks systematic error analysis of failure cases where the transformation produces semantically divergent text.
- The method introduces dependency on external query rewriting models, which may not always be available or of sufficient quality for all domains.

## Confidence

**High confidence**: The mechanism of transforming session embeddings to text using Vec2Text while preserving retrieval performance (supported by quantitative results showing cosine similarities up to 0.985 and maintained retrieval metrics).

**Medium confidence**: The interpretability enhancement through external query rewrites (supported by human evaluation scores, though dependent on rewrite quality and domain-specificity).

**Low confidence**: Generalization across different conversational dense retrieval architectures (limited ablation studies, unclear if shared embedding space assumption holds universally).

## Next Checks

1. **Cross-architecture validation**: Apply CONVINV to session embeddings from additional conversational dense retrieval models (e.g., BGE, ANCE) beyond KD-GTR and Conv-GTR to test shared embedding space assumption generalizability.

2. **Failure case analysis**: Systematically analyze cases where transformed text shows poor retrieval performance or interpretability, examining whether issues stem from the Vec2Text model, rewrite quality, or fundamental embedding space misalignment.

3. **Domain transfer evaluation**: Test CONVINV on conversational search datasets from different domains (e.g., biomedical, technical support) to assess robustness when external query rewrites are domain-specific or unavailable.