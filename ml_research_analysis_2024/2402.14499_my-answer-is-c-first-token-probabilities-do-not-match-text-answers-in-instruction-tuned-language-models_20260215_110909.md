---
ver: rpa2
title: '"My Answer is C": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned
  Language Models'
arxiv_id: '2402.14499'
source_url: https://arxiv.org/abs/2402.14499
tags:
- answer
- question
- evaluation
- output
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the reliability of first-token evaluation in
  instruction-tuned language models (LLMs) for multiple-choice questions (MCQ). It
  finds that first-token probabilities often do not align with the final text output,
  leading to mismatch rates over 60% across six evaluated models.
---

# "My Answer is C": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models

## Quick Facts
- arXiv ID: 2402.14499
- Source URL: https://arxiv.org/abs/2402.14499
- Reference count: 14
- First-token probabilities often do not align with final text output, leading to mismatch rates over 60% across six evaluated models

## Executive Summary
This study investigates the reliability of first-token evaluation in instruction-tuned language models for multiple-choice questions. The research finds that first-token probabilities frequently fail to match the final text output, with mismatch rates exceeding 60% across six evaluated models. This discrepancy is particularly pronounced in models fine-tuned for conversational or safety behaviors, where responses often begin with explanations or refusals. The study concludes that first-token evaluation alone is insufficient for realistic LLM assessment and advocates for text output evaluation as a more robust alternative.

## Method Summary
The study evaluates six instruction-tuned LLMs using the OpinionQA dataset of 414 survey questions. Four levels of instruction prompt constraints are tested: Low, Medium, High, and Example Template. First-token evaluation selects answers based on highest log probability of the first token, while text output evaluation uses a classifier (PEFT-Mistral-7b-v0.2) trained on 2070 annotated response samples to categorize responses into answer options. The study compares mismatch rates, refusal rates, and answer consistency across models and prompt constraints.

## Key Results
- First-token evaluation shows mismatch rates exceeding 60% across all six evaluated models
- Example template prompts introduce strong bias, with first-token evaluation selecting "C" approximately 85% of the time
- Text output evaluation demonstrates better consistency and robustness under prompt perturbations
- Temperature variations significantly affect both consistency and mismatch rates, with optimal values around 0.5 for most models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: First-token evaluation fails because instruction-tuned models prepend conversational or refusal content before answering
- Mechanism: Models are fine-tuned to follow instructions and engage naturally, so they often start responses with "Sure," explanations, or refusals before producing the actual answer token
- Core assumption: The model's first token probability ranking reflects its initial generation step, not the eventual final answer
- Evidence anchors:
  - [abstract] "first-tokens may not consistently reflect the final response output, due to model's diverse response styles such as starting with 'Sure' or refusing to answer"
  - [section 4.1] "first-token evaluation is not faithful to text output: it often does not match the text output's answer (e.g., over60% mismatch for Llama2-7b-Chat)"
  - [corpus] Weak. Closest neighbor: "Look at the Text: Instruction-Tuned Language Models are More Robust Multiple Choice Selectors than You Think" but not directly addressing first-token mismatch
- Break condition: If models were fine-tuned without conversational preamble or safety guardrails, the mismatch would likely be reduced

### Mechanism 2
- Claim: Prompt format strongly influences first-token probabilities, causing bias
- Mechanism: Example templates in prompts lead models to reproduce the example's answer letter (e.g., "Answer: C") as the first token, skewing probabilities
- Core assumption: The model treats the example as a pattern to follow rather than as isolated guidance
- Evidence anchors:
  - [section 4.1] "Surprisingly, the Example Template leads to a higher mismatch rate than High Constraint instruction in five models out of six"
  - [section 4.1] "the first token evaluation selects 'C' about 85% of the time (compared to 32.1% with High constraint)"
  - [corpus] Weak. No direct corpus match on template bias in MCQ evaluation
- Break condition: If example letters were randomized or removed, first-token bias would decrease

### Mechanism 3
- Claim: Text output classification using a fine-tuned evaluator is more robust to prompt variations than first-token evaluation
- Mechanism: A dedicated classifier trained on annotated model outputs learns to map diverse response styles (including explanations, refusals, and multiple choices) to the correct answer option, ignoring non-answer tokens
- Core assumption: The classifier can generalize across prompt variations and diverse response patterns
- Evidence anchors:
  - [section 3] "We use a classifier to categorize the text output into one of the answer options"
  - [section 4.3] "text output achieves better consistency than the first token evaluation for all the models except Mixtral 8x7b"
  - [corpus] Weak. Closest: "Evaluating language models as risk scores" discusses uncertainty but not robustness to prompt perturbation
- Break condition: If response patterns become too diverse or adversarial, the classifier may mislabel outputs

## Foundational Learning

- Concept: Multiple-choice question evaluation in autoregressive models
  - Why needed here: The paper critiques standard MCQ evaluation by first-token probability, which assumes the first token equals the answer
  - Quick check question: In a multiple-choice setting, what is the standard method to score a model's answer using log probabilities?

- Concept: Instruction tuning and conversational preamble behavior
  - Why needed here: Understanding how fine-tuning for instruction following introduces preamble tokens ("Sure," explanations) is key to explaining the mismatch
  - Quick check question: How does instruction tuning typically change a model's response style compared to base models?

- Concept: Classifier-based text output evaluation
  - Why needed here: The paper uses a fine-tuned classifier to map varied text outputs to answer options, bypassing first-token issues
  - Quick check question: Why might a text classifier outperform first-token evaluation in subjective or refusal-prone tasks?

## Architecture Onboarding

- Component map: Prompt generation -> Model inference -> First-token probability ranking -> Text output generation -> Classifier-based text output mapping -> Mismatch and consistency metrics
- Critical path: For each question, generate response → extract first token and full text → classify text → compare answer labels → compute mismatch/refusal rates
- Design tradeoffs: First-token evaluation is fast and simple but brittle; text output evaluation is robust but requires annotated data and classifier training overhead
- Failure signatures: High mismatch rates (>60%), high refusal rates in sensitive topics, example template bias, poor consistency under prompt perturbation
- First 3 experiments:
  1. Run same prompt with different constraint levels and compare first-token vs text output mismatch
  2. Replace example template with randomized or no example and measure bias shift
  3. Vary decoding temperature to test consistency and mismatch sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the mismatch rate between first-token evaluation and text output vary across different types of multiple-choice question domains (e.g., factual vs. subjective)?
- Basis in paper: [inferred] The paper focuses on subjective survey questions but doesn't explore how domain affects mismatch rates
- Why unresolved: The study only examined one dataset type (survey questions), limiting generalizability to other MCQ domains
- What evidence would resolve it: Evaluating first-token vs. text output mismatch across multiple MCQ domains (e.g., factual knowledge, logical reasoning, moral dilemmas) using the same methodology

### Open Question 2
- Question: What is the optimal decoding temperature for balancing answer consistency and accuracy in instruction-tuned models when evaluating MCQs?
- Basis in paper: [explicit] The paper shows temperature affects both consistency and mismatch rates but doesn't identify an optimal value
- Why unresolved: The study only examined three temperature points (greedy, 0.1, 0.5, 0.9) without finding an optimal balance
- What evidence would resolve it: Systematic evaluation of accuracy and consistency across a wider temperature range to identify optimal values for different model families

### Open Question 3
- Question: How do different prompting strategies (beyond the four tested) affect the alignment between first-token evaluation and text output?
- Basis in paper: [explicit] The paper tests four constraint levels but notes surprising results with the example template
- Why unresolved: Only four prompting strategies were tested, leaving many other potential approaches unexplored
- What evidence would resolve it: Evaluating additional prompting strategies (e.g., role-based prompts, step-by-step reasoning prompts, confidence indicators) using the same comparison framework

### Open Question 4
- Question: Does the mismatch between first-token evaluation and text output decrease as models become more instruction-aligned through further training?
- Basis in paper: [inferred] The paper shows newer Mistral versions have higher mismatch rates despite being more instruction-tuned
- Why unresolved: The study only examined existing model versions without testing whether further instruction-tuning reduces mismatch
- What evidence would resolve it: Training models with varying levels of instruction-tuning on the same base architecture and comparing mismatch rates across versions

## Limitations
- Prompt Template Specificity: Exact wording of four instruction prompt constraints is not fully detailed, limiting exact reproduction
- Classifier Generalization: Classifier trained on 2070 samples but diversity and distribution across prompt types is not reported
- Model-Specific Behavior: Mixtral 8x7b outlier performance is noted but not deeply analyzed

## Confidence
- First-token evaluation is unreliable: High confidence
- Text output evaluation is more robust: Medium confidence
- Prompt format affects evaluation reliability: Medium confidence

## Next Checks
1. Replicate prompt sensitivity study - Systematically vary prompt constraints and measure first-token vs text output mismatch rates
2. Classifier generalization test - Hold out diverse subset of responses for testing only to assess generalization limits
3. Randomized example letters ablation - Run example template prompt with randomized example answer letters to test template bias effect