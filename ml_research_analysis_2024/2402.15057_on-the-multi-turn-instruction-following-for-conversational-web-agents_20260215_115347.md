---
ver: rpa2
title: On the Multi-turn Instruction Following for Conversational Web Agents
arxiv_id: '2402.15057'
source_url: https://arxiv.org/abs/2402.15057
tags:
- action
- memory
- task
- click
- conversational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the under-explored challenge of conversational
  web navigation, where web agents must handle multi-turn user instructions that reference
  previous context, shift topics, or use ellipsis. To address the long and noisy conversational
  context, the authors propose a self-reflective memory-augmented planning (Self-MAP)
  framework that combines memory retrieval, reflection, and planning.
---

# On the Multi-turn Instruction Following for Conversational Web Agents

## Quick Facts
- arXiv ID: 2402.15057
- Source URL: https://arxiv.org/abs/2402.15057
- Reference count: 16
- This paper tackles conversational web navigation by proposing a self-reflective memory-augmented planning framework that improves performance by up to 6.3 percentage points in turn success rate.

## Executive Summary
This paper addresses the challenge of conversational web navigation where web agents must handle multi-turn user instructions that reference previous context, shift topics, or use ellipsis. The authors propose a self-reflective memory-augmented planning (Self-MAP) framework that combines memory retrieval, reflection, and planning to overcome the limitations of long and noisy conversational context. The framework introduces multifaceted matching for memory retrieval, memory simplification to remove irrelevant elements, and self-generated rationales to enrich memory snippets for better reasoning.

## Method Summary
The Self-MAP framework consists of three key components: a memory module that retrieves relevant snippets using multifaceted matching based on both instruction semantics and action trajectory similarity; a reflection module that simplifies memory snippets by removing irrelevant HTML elements using a pre-trained ranker and enriches them with self-generated rationales explaining past decisions; and a planning module that generates action plans using large language models. The system operates by retrieving K relevant memory snippets, simplifying them to reduce noise, generating rationales for each, and using this enriched memory to inform action planning. The approach is evaluated on MT-Mind2Web, a new benchmark constructed from Mind2Web by organizing conversation sessions, decomposing complex instructions, and rewriting them conversationally.

## Key Results
- Self-MAP achieves state-of-the-art performance on MT-Mind2Web with up to 6.3 percentage points improvement in turn success rate over strong baselines
- Generation-based planning substantially outperforms MCQ-based planning by saving context space for memory utilization
- Memory simplification and refinement contribute significantly to performance gains, with ablation studies confirming their effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Memory simplification through candidate element ranking frees up context space for retaining more conversation history.
- Mechanism: The system uses a pre-trained ranker (DeBERTa) to filter out irrelevant HTML elements from the environment state, reducing token usage per memory snippet.
- Core assumption: Removing noisy elements from environment state does not significantly impact the agent's ability to plan correct actions.
- Evidence anchors: [section] "we apply the same process to remove task-irrelevant and noisy elements from the environment state, thereby freeing up memory space for more extensive conversation history retention" and "To simplify each memory snippet, we apply the same process to remove task-irrelevant and noisy elements from the environment state"
- Break condition: If simplification removes critical elements needed for current action planning, performance will degrade.

### Mechanism 2
- Claim: Self-reflective memory refinement with generated rationales improves reasoning quality for action planning.
- Mechanism: The system prompts an LLM to generate reasoning rationales explaining why specific actions were taken in past interactions, enriching memory snippets with decision-making context.
- Core assumption: Generated rationales accurately capture the reasoning behind past decisions and provide useful guidance for future actions.
- Evidence anchors: [section] "we leverage the exceptional reasoning capability of LLMs to generate intermediate reasoning rationale as a supervised signal to enrich the memory information" and "For each retrieved memory snippet (qt, Ak−1 t , ak t ), we prompt the LLM to generate an in-depth rationale rk t explaining the reason for the decision-making process of the next action"
- Break condition: If generated rationales are inaccurate or irrelevant, they may mislead the planning process.

### Mechanism 3
- Claim: Multifaceted matching based on both instruction semantics and action trajectory similarity retrieves more relevant memory snippets.
- Mechanism: The system constructs queries using both current instruction and action sequence to find memory snippets with similar trajectories and semantic relevance.
- Core assumption: Combining instruction and action trajectory provides better matching than using either alone.
- Evidence anchors: [section] "multifaceted matching constructs the query using both the user instruction and the present agent action sequence(qt, Ak−1 t ) to retrieve relevant memory snippets" and "we observe that Generation-based Planning substantially surpasses MCQ-based Planning in performance. This superiority is attributed not only to the advanced generative capabilities of large language models (LLMs) but also to their efficiency in conserving context space for memory utilization"
- Break condition: If matching becomes too strict, relevant memories may be missed; if too loose, noise increases.

## Foundational Learning

- Concept: Memory-augmented planning
  - Why needed here: Web navigation requires maintaining context across multiple turns where actions depend on previous interactions and environment state.
  - Quick check question: What happens if we remove the memory component entirely? (Answer: Agent would treat each instruction as standalone, failing at co-referencing and ellipsis tasks.)

- Concept: Self-reflection in AI systems
  - Why needed here: Helps the agent understand why past decisions were made, improving reasoning for future similar situations.
  - Quick check question: How does the system ensure generated rationales are accurate? (Answer: Uses GPT-3.5 to generate rationales, but this is an assumption that needs validation.)

- Concept: Multifaceted matching
  - Why needed here: Traditional single-dimension matching fails to capture the complex relationships between instructions, actions, and environment states in conversational web navigation.
  - Quick check question: What dimensions are used for matching in this system? (Answer: Semantic similarity of instructions and similarity of action trajectories.)

## Architecture Onboarding

- Component map: Memory Module -> Reflection Module -> Planning Module -> LLM orchestrator
- Critical path: User instruction → Multifaceted matching → Memory simplification → Memory refinement → Action planning → Execute action
- Design tradeoffs:
  - Memory size vs. context length: Smaller memories allow more history but risk losing detail
  - Generation-based vs. MCQ planning: Generation saves context but may be less precise
  - Rationale generation quality vs. computation cost: Better rationales improve performance but add latency
- Failure signatures:
  - Poor element accuracy: Memory simplification may be removing too many elements
  - Low operation F1: Planning module may not be capturing sufficient context
  - Step success rate drops with more memory snippets: Noise in retrieved memories
- First 3 experiments:
  1. Test memory simplification by comparing performance with and without DeBERTa ranking on a small subset
  2. Evaluate different numbers of retrieved memory snippets (K=1,2,3,4,5) to find optimal balance
  3. Compare generation-based vs MCQ planning on a validation set to confirm efficiency claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Self-MAP change when using multimodal LLMs instead of HTML-grounded models?
- Basis in paper: [inferred] The paper mentions multimodal LLMs in the Limitation section and suggests it as a promising research direction for conversational web navigation.
- Why unresolved: The paper focuses on benchmarking general HTML-grounded methods and does not experiment with multimodal web agents.
- What evidence would resolve it: Comparing the performance of Self-MAP with multimodal LLMs against the current HTML-grounded approach on the MT-Mind2Web dataset.

### Open Question 2
- Question: How does the number of retrieved memory snippets (K) affect the performance of Self-MAP in different conversational contexts?
- Basis in paper: [explicit] The paper analyzes the effect of K on performance and observes that performance increases with K up to a point, but then plateaus or decreases due to noisy information.
- Why unresolved: The paper only examines a limited range of K values (1 to 5) and does not explore how different conversational contexts might affect the optimal value of K.
- What evidence would resolve it: Conducting experiments with a wider range of K values and varying conversational contexts to determine the optimal number of retrieved memory snippets for different scenarios.

### Open Question 3
- Question: How does the proposed Self-MAP framework perform in an online evaluation setting with dynamic web interactions?
- Basis in paper: [inferred] The paper mentions that the current evaluation setting uses snapshots of websites, which inherits the drawback of offline evaluation settings, and suggests that online evaluation with dynamic interactions would be a valuable addition.
- Why unresolved: The paper only evaluates the framework using static snapshots of websites, which does not capture the challenges of real-time web interactions.
- What evidence would resolve it: Implementing and evaluating Self-MAP in an online setting with dynamic web interactions to assess its performance in a more realistic scenario.

## Limitations
- The quality of generated rationales is uncertain since they are produced by LLMs and may contain errors or hallucinate reasoning
- The computational overhead of the full Self-MAP pipeline isn't quantified, leaving practical deployment costs unclear
- Evaluation uses relatively small test sets (34-44 samples per split), making the robustness of improvements uncertain

## Confidence
- High: Core experimental results showing Self-MAP outperforms baselines on MT-Mind2Web
- Medium: Claims about memory simplification effectiveness and multifaceted matching benefits
- Low: Assumptions about generated rationale accuracy and long-term robustness

## Next Checks
1. Conduct ablation study removing memory refinement (generated rationales) to quantify their actual contribution versus noise
2. Test the system on significantly longer conversation histories (20+ turns) to evaluate scaling behavior and memory retention limits
3. Perform human evaluation of generated rationales to assess their accuracy and relevance to actual decision-making processes