---
ver: rpa2
title: Neural Operators with Localized Integral and Differential Kernels
arxiv_id: '2402.16845'
source_url: https://arxiv.org/abs/2402.16845
tags:
- neural
- local
- operators
- differential
- kernel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the limitation of existing neural operators
  in capturing local features while maintaining resolution-agnostic inference. The
  authors propose principled approaches to include differential operators and integral
  operators with locally supported kernels into neural operator architectures.
---

# Neural Operators with Localized Integral and Differential Kernels

## Quick Facts
- arXiv ID: 2402.16845
- Source URL: https://arxiv.org/abs/2402.16845
- Authors: Miguel Liu-Schiaffini, Julius Berner, Boris Bonev, Thorsten Kurth, Kamyar Azizzadenesheli, Anima Anandkumar
- Reference count: 19
- Primary result: Adding local differential and integral layers to FNOs reduces relative L2-error by 34-72% on turbulent 2D Navier-Stokes and shallow water equations

## Executive Summary
This paper addresses a fundamental limitation of neural operators: their inability to capture local features while maintaining resolution-agnostic inference. The authors propose principled approaches to include differential operators and integral operators with locally supported kernels into neural operator architectures. By deriving conditions for convolutional layers to converge to differential operators when discretized and adapting discrete-continuous convolutions for efficient local integral operators on general domains, they demonstrate significant performance improvements over baseline FNO/SFNO models on turbulent PDEs.

## Method Summary
The authors augment standard Fourier Neural Operators (FNO) and Spherical FNO (SFNO) with parallel branches containing local layers. Differential layers are created by rescaling and constraining convolutional kernels to approximate differential operators as discretization width approaches zero. Local integral operators are implemented using discrete-continuous (DISCO) convolutions that parameterize kernels as linear combinations of basis functions evaluated at quadrature points. The architecture combines global FNO operations with these local differential and integral layers to capture both global and local features while preserving the resolution-agnostic property.

## Key Results
- Adding local layers to FNOs reduces relative L2-error by 34-72% on turbulent 2D Navier-Stokes and spherical shallow water equations
- Performance gains observed across all three problem settings: Darcy flow, Navier-Stokes, and shallow water equations
- The proposed approach maintains resolution-agnostic properties while providing strong inductive bias for learning local features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Differential layers approximate differential operators by rescaling and constraining convolutional kernels as discretization width h → 0
- Mechanism: By centering the kernel (subtracting mean) and scaling by 1/h, the convolutional layer converges to a differential operator instead of collapsing to a pointwise operator
- Core assumption: The kernel values can be constrained and rescaled appropriately to prevent collapse to pointwise operator
- Evidence anchors:
  - [abstract]: "inspired by stencil methods, we prove that we obtain differential operators under an appropriate scaling of the kernel values of CNNs"
  - [section 3.2]: "rescaling by the reciprocal resolution 1/h and constraining the kernel values leads to differential operators"
  - [corpus]: Weak - no direct citations about this specific mechanism

### Mechanism 2
- Claim: Local integral operators are implemented using discrete-continuous (DISCO) convolutions that preserve resolution-agnostic properties
- Mechanism: DISCO convolutions parameterize kernels as linear combinations of basis functions evaluated at quadrature points, allowing local operations at arbitrary resolutions
- Core assumption: The integral kernel can be represented as a linear combination of fixed basis functions
- Evidence anchors:
  - [abstract]: "utilize suitable basis representations for the kernels based on discrete-continuous convolutions"
  - [section 3.4]: "adapt discrete-continuous (DISCO) convolutions to provide an efficient, discretization-agnostic framework"
  - [corpus]: Weak - no direct citations about DISCO convolutions in this context

### Mechanism 3
- Claim: Combining global FNO operations with local differential and integral layers improves performance by capturing both global and local features
- Mechanism: The architecture augments FNO with parallel branches of local operations that provide strong inductive bias for learning local features while maintaining global context
- Core assumption: Local features are important for the target PDEs and can be learned alongside global features
- Evidence anchors:
  - [abstract]: "Adding our layers to FNOs significantly improves their performance, reducing the relative L2-error by 34-72%"
  - [section 4.4]: "we observe significant performance gains over the baselines in all three problem settings"
  - [corpus]: Weak - no direct citations about this specific architectural combination

## Foundational Learning

- Concept: Operator learning and function space mappings
  - Why needed here: Neural operators learn mappings between function spaces, which is the foundation of this work
  - Quick check question: What distinguishes neural operators from standard neural networks in terms of input/output requirements?

- Concept: Differential operators and their discretization
  - Why needed here: The paper derives conditions for convolutional layers to converge to differential operators
  - Quick check question: How does a standard convolutional layer behave as discretization width h → 0 without proper scaling?

- Concept: Integral transforms and quadrature rules
  - Why needed here: Local integral operators are implemented using quadrature approximations of integral transforms
  - Quick check question: What role do quadrature weights play in the discrete-continuous convolution formulation?

## Architecture Onboarding

- Component map: Input discretization -> Parallel FNO and local branches -> Combination of outputs -> Function space mapping
- Critical path: 1) Input function discretization, 2) Parallel processing through global FNO and local branches, 3) Combination of outputs from all branches, 4) Final function space mapping
- Design tradeoffs:
  - Global vs local operations: Balance between capturing long-range dependencies and local features
  - Parameter efficiency vs expressiveness: More local layers increase parameters but improve inductive bias
  - Resolution agnosticism vs discretization specificity: Must work at arbitrary resolutions without losing accuracy
- Failure signatures:
  - Performance degradation at resolutions different from training
  - Boundary artifacts in non-periodic problems
  - Convergence issues when combining multiple local layers
  - Loss of high-frequency information through downsampling
- First 3 experiments:
  1. Implement and test differential layer on simple differential operator approximation (e.g., first derivative)
  2. Validate DISCO convolution implementation on known integral transform
  3. Combine both local layers with FNO on a simple PDE problem and compare to baseline FNO

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the limitations section and the discussion, several open questions can be inferred:

### Open Question 1
- Question: How does the choice of quadrature rule affect the accuracy and efficiency of DISCO convolutions in practice?
- Basis in paper: [inferred] The paper discusses using quadrature rules for approximating the integral in DISCO convolutions, but does not extensively explore different quadrature options.
- Why unresolved: The paper focuses on the theoretical framework and proof-of-concept experiments, leaving detailed empirical analysis of quadrature choices for future work.
- What evidence would resolve it: Comprehensive experiments comparing different quadrature rules (e.g., trapezoidal, Simpson's, Gaussian quadrature) on various test problems, measuring both accuracy and computational cost.

### Open Question 2
- Question: What is the impact of the kernel support size on the performance of localized neural operators for different PDE problems?
- Basis in paper: [explicit] The paper mentions using different radius cutoffs for local integral kernels in experiments, but does not systematically study the effect of kernel support size.
- Why unresolved: While the paper demonstrates improvements with localized kernels, it does not explore how the optimal kernel support size varies across different PDE problems and their characteristic length scales.
- What evidence would resolve it: Systematic experiments varying the kernel support size for each test problem, analyzing the trade-off between localization and performance, and relating kernel size to problem-specific length scales.

### Open Question 3
- Question: How do localized neural operators perform on irregular meshes and non-Euclidean domains compared to their performance on regular grids?
- Basis in paper: [explicit] The paper mentions that DISCO convolutions can be applied to general meshes and spherical geometries, but does not provide extensive experimental validation on irregular domains.
- Why unresolved: The current experiments focus on regular grids and spherical domains, leaving open questions about the performance on more complex geometries and unstructured meshes common in practical applications.
- What evidence would resolve it: Experiments applying localized neural operators to problems on irregular meshes (e.g., adaptive grids, triangular meshes) and complex geometries, comparing performance to specialized methods for these domains.

## Limitations
- Limited experimental validation on irregular meshes and non-Euclidean domains
- Computational efficiency claims not rigorously validated across different mesh resolutions
- Proof of convergence to differential operators relies on idealized conditions that may not hold in practice

## Confidence

- **High confidence** in the theoretical framework for differential operators (supported by mathematical derivation in Section 3.2)
- **Medium confidence** in the DISCO convolution implementation (framework described but implementation details are sparse)
- **Medium confidence** in performance claims (significant improvements shown but only on three specific PDE problems)
- **Low confidence** in generalization claims to arbitrary PDEs and geometries (limited empirical validation)

## Next Checks

1. **Convergence verification**: Implement the differential layer and verify that it approximates simple differential operators (first/second derivatives) with appropriate accuracy as h → 0

2. **DISCO implementation test**: Reproduce the quadrature-based integral operator on a known transform (e.g., convolution with Gaussian kernel) and verify accuracy across different quadrature resolutions

3. **Cross-resolution validation**: Train models on one resolution and evaluate performance across a range of resolutions to verify the claimed resolution-agnostic properties