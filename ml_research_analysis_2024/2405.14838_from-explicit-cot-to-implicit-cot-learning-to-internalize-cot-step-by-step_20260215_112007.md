---
ver: rpa2
title: 'From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step'
arxiv_id: '2405.14838'
source_url: https://arxiv.org/abs/2405.14838
tags:
- reasoning
- tokens
- training
- accuracy
- steps
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method called Stepwise Internalization for
  training language models to perform implicit chain-of-thought reasoning. The method
  starts with a model trained for explicit CoT reasoning and gradually removes intermediate
  reasoning steps while finetuning the model, allowing it to internalize the reasoning
  process.
---

# From Explicit CoT to Implicit CoT: Learning to Internalize CoT Step by Step

## Quick Facts
- arXiv ID: 2405.14838
- Source URL: https://arxiv.org/abs/2405.14838
- Authors: Yuntian Deng; Yejin Choi; Stuart Shieber
- Reference count: 21
- One-line primary result: A method to train language models to perform implicit chain-of-thought reasoning by gradually removing explicit reasoning steps during finetuning

## Executive Summary
This paper introduces Stepwise Internalization, a method for training language models to perform implicit chain-of-thought (CoT) reasoning by gradually removing explicit intermediate reasoning steps. Starting with a model trained on explicit CoT, the approach progressively eliminates CoT tokens during training while finetuning the model, allowing it to internalize the reasoning process. The method is shown to be effective on multi-digit multiplication and grade-school math problems, achieving high accuracy without producing explicit intermediate steps. For example, a GPT-2 Small model trained with Stepwise Internalization can solve 9-by-9 multiplication with up to 99% accuracy, whereas standard training cannot solve beyond 4-by-4 multiplication.

## Method Summary
Stepwise Internalization starts with a language model trained for explicit CoT reasoning and gradually removes intermediate reasoning steps while finetuning. The method uses a linear schedule to remove CoT tokens per epoch, with optimizer reset when tokens are removed to handle loss function changes. A technique called "Removal Smoothing" adds random offsets to the removal schedule to prevent catastrophic loss spikes. The approach is applied to multiplication tasks and grade-school math problems, with the model trained to predict both explicit CoT steps and final outputs before internalization begins.

## Key Results
- GPT-2 Small trained with Stepwise Internalization achieves 99% accuracy on 9-by-9 multiplication without explicit steps
- Standard training cannot solve beyond 4-by-4 multiplication
- Mistral 7B achieves over 50% accuracy on GSM8K without producing intermediate steps
- The method provides a trade-off between accuracy and inference speed as more CoT tokens are internalized

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Stepwise Internalization works by progressively replacing explicit CoT tokens with internalized reasoning in hidden states through curriculum learning.
- Mechanism: The model is first trained to predict both explicit CoT steps and final outputs. Then, CoT tokens are gradually removed while finetuning continues, forcing the model to distribute the reasoning burden across remaining input positions and hidden states.
- Core assumption: The model's hidden states can encode the same reasoning information previously represented by explicit tokens, and this encoding becomes more efficient as fewer tokens remain.
- Evidence anchors:
  - [abstract] "starting with a model trained for explicit CoT reasoning, we gradually remove the intermediate steps and finetune the model"
  - [section] "This process allows the model to internalize the intermediate reasoning steps, thus simplifying the reasoning process while maintaining high performance"
  - [corpus] Weak evidence - no direct citations about hidden state internalization mechanisms
- Break condition: The method fails when the model cannot encode sufficient reasoning information in its hidden states to compensate for removed tokens, leading to accuracy collapse.

### Mechanism 2
- Claim: Removal Smoothing prevents catastrophic loss spikes during token removal by adding small random offsets to the removal schedule.
- Mechanism: When transitioning between stages where more tokens are removed, a small random number of additional tokens may be removed, creating a smoother gradient landscape and preventing sudden accuracy drops.
- Core assumption: The small random offsets create a soft transition that helps the optimizer find better minima during the removal process.
- Evidence anchors:
  - [section] "we introduce a technique which we term 'Removal Smoothing', where we add a small random offset to the original number of tokens to remove s(t)"
  - [section] "the model is trained to remove more than s(t) tokens at step t with a small probability, which helps smooth the transition"
  - [corpus] Weak evidence - no direct citations about curriculum learning smoothing techniques
- Break condition: The method fails when the random offsets are too large, causing excessive instability, or too small, making the smoothing ineffective.

### Mechanism 3
- Claim: Optimizer state reset prevents gradient estimation errors when the loss function changes due to token removal.
- Mechanism: AdamW maintains second-order gradient estimates; when the loss changes structure due to token removal, these estimates become outdated and can destabilize training. Resetting the optimizer clears these stale estimates.
- Core assumption: The second-order gradient estimates become significantly inaccurate when the loss function structure changes, and resetting provides a fresh start for optimization.
- Evidence anchors:
  - [section] "the optimizer commonly used in training language models, such as AdamW [11, 13], maintains estimates of second-order gradients. A sudden change in the loss function, caused by the removal of one more CoT token, results in abrupt changes in the second-order gradients"
  - [section] "we reset the optimizer's state whenever an additional CoT token is removed"
  - [corpus] Weak evidence - no direct citations about optimizer state management during curriculum learning
- Break condition: The method fails when optimizer reset is too frequent, preventing proper momentum accumulation, or when the learning rate schedule doesn't account for the reset.

## Foundational Learning

- Concept: Curriculum learning
  - Why needed here: The gradual removal of CoT tokens follows a curriculum learning approach where the model learns simpler tasks first (explicit CoT) before progressing to more complex tasks (implicit CoT)
  - Quick check question: What is the key difference between standard training and curriculum learning approaches in the context of Stepwise Internalization?

- Concept: Knowledge distillation
  - Why needed here: While Stepwise Internalization is simpler than ICoT-KD, understanding knowledge distillation helps explain why explicit CoT supervision during training enables implicit CoT reasoning during inference
  - Quick check question: How does Stepwise Internalization differ from traditional knowledge distillation approaches in terms of implementation complexity?

- Concept: Hidden state representation capacity
  - Why needed here: The success of Stepwise Internalization depends on the model's ability to encode reasoning information in hidden states that were previously represented by explicit tokens
  - Quick check question: What factors determine whether a language model has sufficient capacity to internalize CoT reasoning?

## Architecture Onboarding

- Component map: Language model (parameters θ) -> Training data (x, z, y triples) -> Optimizer (AdamW) -> Training scheduler (controls token removal) -> Loss computation -> Gradient update

- Critical path: Input sequence → CoT token prediction → final answer prediction → loss calculation → gradient update → (optionally) optimizer reset → next iteration with potentially fewer CoT tokens

- Design tradeoffs: The main tradeoff is between training stability and speed. Larger ∆ values (more tokens removed per epoch) speed up training but risk instability. Smaller ∆ values provide stability but require more training time. Removal Smoothing adds robustness but introduces additional hyperparameters.

- Failure signatures: Training instability manifests as sudden accuracy drops to zero, followed by failure to recover. Optimizer issues show as oscillating or diverging loss values. Insufficient model capacity results in gradually declining accuracy as more tokens are removed.

- First 3 experiments:
  1. Train a GPT-2 Small model on 4×4 multiplication with explicit CoT, then apply Stepwise Internalization with ∆=8 and λ=4, monitoring accuracy vs. removed tokens.
  2. Repeat experiment 1 but without optimizer reset to observe stability differences.
  3. Repeat experiment 1 but with aggressive removal (∆=16) to test the limits of training stability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Stepwise Internalization approach scale to larger models and more extensive training setups, particularly for tasks beyond arithmetic and grade-school math problems?
- Basis in paper: [explicit] The paper acknowledges that further research is needed to explore the efficacy of Stepwise Internalization across a broader range of tasks and more diverse CoT traces. It also mentions the potential of scaling the approach to larger models with hundreds of billions of parameters and up to a hundred layers, such as GPT-3.
- Why unresolved: The current work primarily focuses on specific reasoning tasks like multi-digit multiplication and grade-school math problems. The scalability of the approach to larger models and more diverse tasks remains unexplored.
- What evidence would resolve it: Conducting experiments with larger models and a wider variety of reasoning tasks, comparing the performance of Stepwise Internalization to other methods, and analyzing the trade-offs between accuracy, speed, and model size.

### Open Question 2
- Question: How can the instability issues associated with aggressive ∆ values in the Stepwise Internalization approach be mitigated?
- Basis in paper: [explicit] The paper mentions that using aggressive removal schedules (larger ∆ values) can sometimes lead to unstable training dynamics. It suggests that identifying and addressing unstable dynamics early on, potentially by restarting training, could be a valuable improvement.
- Why unresolved: The current work does not provide a concrete solution to mitigate the instability issues associated with aggressive ∆ values.
- What evidence would resolve it: Developing and evaluating techniques to stabilize training with aggressive ∆ values, such as adaptive schedules based on loss values, early detection of instability, and strategies for restarting training.

### Open Question 3
- Question: How can the interpretability of models trained using the Stepwise Internalization approach be improved, particularly in terms of understanding the internal hidden states during the internalization process?
- Basis in paper: [explicit] The paper acknowledges that models trained using the approach lose interpretable intermediate steps. It suggests that probing techniques could be used to interpret the internal hidden states of these models.
- Why unresolved: The current work does not provide a detailed analysis of the interpretability of models trained using Stepwise Internalization or the effectiveness of probing techniques in this context.
- What evidence would resolve it: Conducting experiments to probe the internal hidden states of models trained using Stepwise Internalization, analyzing the learned representations, and developing techniques to visualize and interpret the internalization process.

## Limitations

- The method requires starting with a model already trained on explicit CoT reasoning, adding training overhead
- Aggressive token removal schedules can cause training instability that may require restarting
- The approach has only been validated on arithmetic and grade-school math tasks, not more complex reasoning problems

## Confidence

**High Confidence (⊕⊕⊕⊕⊕):** The core empirical claim that Stepwise Internalization can maintain high accuracy on multiplication tasks while removing CoT tokens is well-supported by the experimental results.

**Medium Confidence (⊕⊕⊕⊕⊖):** The claim that Stepwise Internalization is simpler and more effective than ICoT-KD is reasonable but could benefit from more systematic comparison.

**Low Confidence (⊕⊕⊖⊖⊖):** The theoretical mechanism explaining why hidden state internalization occurs is speculative, with limited direct evidence about how representations change during training.

## Next Checks

1. **Hidden State Analysis**: Conduct probing experiments to directly examine whether reasoning information is actually being encoded in hidden states as tokens are removed. Use techniques like attention pattern analysis, linear probes, or representation similarity to verify the internalization mechanism.

2. **Distributional Robustness**: Test Stepwise Internalization on reasoning datasets with more diverse problem structures and real-world distributions (e.g., natural language word problems, multi-step reasoning tasks from different domains) to assess generalization beyond synthetic multiplication tasks.

3. **Scaling Characterization**: Systematically vary model size and task complexity to establish scaling laws for Stepwise Internalization. Identify the maximum number of tokens that can be internalized for different model sizes and determine whether there are architectural properties that predict internalization success.