---
ver: rpa2
title: 'PreFLMR: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers'
arxiv_id: '2402.08327'
source_url: https://arxiv.org/abs/2402.08327
tags:
- image
- retrieval
- training
- question
- preflmr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies scaling laws for multi-modal retrievers in knowledge-based
  visual question answering. The authors create a multi-task benchmark M2KR from nine
  diverse datasets, and develop PreFLMR by pre-training a fine-grained late-interaction
  multi-modal retriever.
---

# PreFLMR: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers

## Quick Facts
- arXiv ID: 2402.08327
- Source URL: https://arxiv.org/abs/2402.08327
- Reference count: 40
- Primary result: PreFLMR achieves SOTA results on multi-task KB-VQA benchmark M2KR through systematic scaling of vision and text encoders

## Executive Summary
This paper presents a comprehensive study of scaling laws for multi-modal retrievers in knowledge-based visual question answering (KB-VQA). The authors introduce PreFLMR, a fine-grained late-interaction multi-modal retriever that achieves state-of-the-art performance through systematic scaling of vision and text encoder sizes. They create M2KR, a multi-task benchmark combining nine diverse KB-VQA datasets, and conduct extensive scaling experiments to understand how model size affects retrieval performance. The work provides practical insights for designing efficient multi-modal retrieval systems.

## Method Summary
PreFLMR extends late-interaction mechanisms to the multi-modal retrieval setting by employing separate vision and text encoders that interact through a fine-grained cross-encoder module. The architecture consists of a vision encoder, text encoder, and a 6-layer cross-encoder that performs fine-grained interaction between visual and textual representations. The authors systematically scale vision encoder sizes up to 400M parameters and text encoder sizes up to 800M parameters, demonstrating power-law scaling relationships. They also investigate the impact of intermediate pre-training data on final performance, showing that scaling the amount of pre-training data improves retrieval accuracy following predictable scaling laws.

## Key Results
- PreFLMR achieves state-of-the-art performance on the M2KR benchmark across all nine KB-VQA tasks
- Vision encoder scaling shows power-law relationships up to 400M parameters, with diminishing returns beyond this point
- Text encoder scaling demonstrates consistent improvements up to 800M parameters
- Intermediate pre-training data scaling exhibits predictable improvements in retrieval performance

## Why This Works (Mechanism)
The paper's approach works because fine-grained late-interaction allows for more nuanced matching between visual and textual representations compared to early or late fusion methods. By scaling both vision and text encoders independently, the model can capture richer representations in each modality before interaction. The cross-encoder's 6-layer architecture provides sufficient capacity for complex interactions while maintaining computational efficiency. The systematic scaling approach ensures that improvements follow predictable patterns, making it easier to optimize resource allocation during model development.

## Foundational Learning

1. **Late-interaction mechanisms** - Why needed: Enables separate encoding of modalities before interaction, reducing computational complexity while maintaining performance. Quick check: Compare retrieval accuracy with and without late-interaction.

2. **Power-law scaling relationships** - Why needed: Predicts how performance improves with model size, guiding efficient resource allocation. Quick check: Plot performance vs. parameter count to verify power-law relationship.

3. **Multi-modal representation learning** - Why needed: Captures complementary information from different modalities for better retrieval. Quick check: Ablation study removing one modality.

4. **Intermediate pre-training** - Why needed: Provides general-purpose representations that improve downstream task performance. Quick check: Compare performance with and without intermediate pre-training.

## Architecture Onboarding

**Component Map**: Vision Encoder -> Cross-Encoder -> Text Encoder -> Output

**Critical Path**: Input images and text pass through their respective encoders, then interact through the cross-encoder to produce final retrieval scores. The cross-encoder is the bottleneck for fine-grained interactions.

**Design Tradeoffs**: The 6-layer cross-encoder balances interaction quality with computational efficiency. Deeper cross-encoders might improve performance but increase inference costs significantly.

**Failure Signatures**: Poor retrieval performance often stems from inadequate vision or text encoder capacity, insufficient pre-training data, or overly shallow cross-encoder layers preventing meaningful interactions.

**3 First Experiments**:
1. Scale vision encoder from 100M to 400M parameters while keeping other components fixed
2. Vary cross-encoder depth from 2 to 12 layers to find optimal interaction capacity
3. Test different amounts of intermediate pre-training data (10K, 100K, 1M examples)

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Scaling behaviors may not generalize beyond tested parameter ranges (vision encoders >400M, text encoders >800M)
- No systematic exploration of deeper cross-encoder architectures beyond 6 layers
- Limited analysis of per-task performance variations across the M2KR benchmark

## Confidence
- High: Core scaling law observations, benchmark construction, PreFLMR architecture
- Medium: Generalization to larger models, task-specific performance variations
- Low: Alternative architectural choices, interaction mechanism design

## Next Checks
1. Test scaling behaviors for vision encoders >400M and text encoders >800M parameters to verify if power-law relationships persist
2. Systematically vary the cross-encoder depth (beyond 6 layers) to assess architectural sensitivity
3. Conduct per-task analysis on M2KR to identify which specific tasks drive overall performance improvements and whether all benefit from scaling