---
ver: rpa2
title: Is Temperature the Creativity Parameter of Large Language Models?
arxiv_id: '2405.00492'
source_url: https://arxiv.org/abs/2405.00492
tags: []
core_contribution: 'This paper investigates whether the temperature parameter of large
  language models (LLMs) is truly a "creativity parameter" as often claimed. The authors
  empirically analyze the effects of temperature on LLM-generated stories using four
  necessary conditions for creativity: novelty, typicality, cohesion, and coherence.'
---

# Is Temperature the Creativity Parameter of Large Language Models?

## Quick Facts
- **arXiv ID:** 2405.00492
- **Source URL:** https://arxiv.org/abs/2405.00492
- **Reference count:** 14
- **Primary result:** Temperature increases novelty but decreases coherence in LLM story generation, challenging the "creativity parameter" claim.

## Executive Summary
This paper empirically investigates whether temperature serves as a true "creativity parameter" for large language models by analyzing its effects on story generation across four creativity dimensions: novelty, typicality, cohesion, and coherence. Through systematic experimentation, the authors find that temperature's influence on creativity is far more nuanced than commonly suggested, showing only weak positive correlations with novelty and moderate negative correlations with coherence. The study challenges the popular assumption that temperature is a straightforward creativity knob, revealing instead that it creates trade-offs between different aspects of creative output rather than simply enabling access to more creative regions of the embedding space.

## Method Summary
The authors conducted empirical analysis of LLM-generated stories using controlled temperature settings across multiple model configurations. They measured creativity outcomes using quantitative metrics for novelty, typicality, cohesion, and coherence, comparing outputs across different temperature ranges. The study employed statistical correlation analysis to examine relationships between temperature settings and creativity dimensions, while also analyzing embedding space coverage to determine whether temperature enabled access to different regions of the latent space.

## Key Results
- Temperature shows only weak positive correlation with novelty in generated stories
- Higher temperature settings exhibit moderate negative correlation with coherence
- Temperature does not enable LLMs to access different regions of the embedding space despite increasing output diversity
- The influence of temperature on creativity is nuanced and weak compared to the "creativity parameter" claim

## Why This Works (Mechanism)
Unknown: The paper does not provide a detailed mechanistic explanation for why temperature affects creativity dimensions in the observed manner.

## Foundational Learning
Assumption: Temperature scaling modifies the probability distribution of next-token predictions by dividing logits by the temperature value before applying softmax, which amplifies or dampens the relative differences between token probabilities.

## Architecture Onboarding
**Component Map:** Temperature parameter -> Sampling distribution -> Token generation -> Output sequence
**Critical Path:** Temperature adjustment → Logits scaling → Softmax normalization → Token sampling → Text generation
**Design Tradeoffs:** Higher temperature increases diversity but reduces coherence; lower temperature improves coherence but limits novelty
**Failure Signatures:** Temperature too high → incoherent outputs; temperature too low → repetitive, predictable text
**First Experiments:** 1) Test temperature 0.0 vs 1.0 on same prompt; 2) Compare novelty metrics at different temperatures; 3) Measure coherence degradation as temperature increases

## Open Questions the Paper Calls Out
None

## Limitations
- Study limited to story generation tasks, may not generalize to other domains
- Quantitative metrics may not fully capture human perceptions of creativity
- Only examines temperature in isolation, missing potential interactions with other decoding strategies
- Effect sizes and practical significance of temperature's influence remain unclear

## Confidence
- Temperature increases novelty with limited samples: Medium
- Temperature doesn't access different embedding regions: Medium
- Temperature alone is insufficient for creativity: Medium

## Next Checks
1. Replicate the study across multiple model architectures and generation tasks beyond narrative writing
2. Conduct human evaluation studies comparing LLM outputs at different temperatures with human-generated creative content
3. Design controlled experiments testing temperature in combination with other decoding strategies (top-k, nucleus sampling, beam search)