---
ver: rpa2
title: 'Don''t Look Twice: Faster Video Transformers with Run-Length Tokenization'
arxiv_id: '2411.05222'
source_url: https://arxiv.org/abs/2411.05222
tags:
- tokens
- video
- vision
- training
- patches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Run-Length Tokenization (RLT), a method to
  accelerate video transformers by removing temporally redundant patches before model
  inference. Inspired by run-length encoding, RLT compares consecutive video patches
  and prunes those that are highly similar, storing only a single representative patch
  along with a positional encoding to indicate the run length.
---

# Don't Look Twice: Faster Video Transformers with Run-Length Tokenization

## Quick Facts
- **arXiv ID**: 2411.05222
- **Source URL**: https://arxiv.org/abs/2411.05222
- **Reference count**: 40
- **Primary result**: RLT achieves up to 40% faster training and 35% higher inference throughput while maintaining model accuracy, with token reductions reaching 80% on long, static videos

## Executive Summary
This paper introduces Run-Length Tokenization (RLT), a method to accelerate video transformers by removing temporally redundant patches before model inference. Inspired by run-length encoding, RLT compares consecutive video patches and prunes those that are highly similar, storing only a single representative patch along with a positional encoding to indicate the run length. This content-aware approach significantly reduces the number of input tokens without requiring model changes or training overhead. RLT achieves up to 40% faster training and 35% higher inference throughput while maintaining model accuracy, with token reductions reaching 80% on long, static videos. The method is simple to implement, requires no tuning per dataset, and can be combined with other acceleration techniques like random masking.

## Method Summary
Run-Length Tokenization (RLT) accelerates video transformers by identifying and removing temporally redundant patches before they enter the model. The method works by comparing consecutive video patches across time and grouping together patches with L1 differences below a threshold τ. For each group of similar patches, only the first patch is kept while the others are removed, and a run-length positional encoding stores how many identical patches were removed. This creates variable-length tokens that the transformer processes directly using example packing and block-diagonal attention masks. RLT operates entirely on patches before embedding, requires no model changes, and can be combined with other acceleration methods. The approach is content-aware, requiring no dataset-specific tuning, and incurs negligible overhead when properly GPU-accelerated.

## Key Results
- RLT achieves up to 40% faster training and 35% higher inference throughput compared to standard tokenization
- Token reductions reach 80% on long, static videos (COIN and Breakfast datasets) while maintaining baseline accuracy
- RLT outperforms random masking in both token reduction (30% more tokens removed) and accuracy preservation (higher top-1 accuracy)
- RLT incurs negligible overhead when GPU-accelerated and requires no model changes or training overhead

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Run-Length Tokenization removes redundant video patches by detecting temporal similarity across consecutive frames.
- Mechanism: For each spatial location, the method compares patches across time and groups consecutive patches with L1 differences below a threshold τ. Only the first patch in each run is kept, while a run-length positional encoding stores how many identical patches were removed.
- Core assumption: Visual content in video patches changes slowly or remains static between consecutive frames, making similarity detection meaningful for token reduction.
- Break condition: If videos contain rapid motion or camera shake, the similarity threshold τ will be exceeded more often, reducing the number of removable tokens and thus the speedup.

### Mechanism 2
- Claim: Variable-length tokens enable transformers to process fewer inputs without padding or architectural changes.
- Mechanism: After pruning, each remaining token carries an additional length encoding ϕL[ℓi] that tells the transformer how many identical patches it represents. The transformer processes this variable-length sequence directly, using example packing to keep batches efficient.
- Core assumption: Transformers can natively handle variable numbers of tokens per example as long as the attention mask keeps tokens from different examples isolated.
- Break condition: If the attention implementation cannot handle variable sequence lengths without padding, the theoretical speedup is lost to memory overhead.

### Mechanism 3
- Claim: Content-aware pruning removes more tokens than random masking while preserving accuracy.
- Mechanism: RLT selectively removes only truly static patches, whereas random masking discards tokens regardless of content. This means RLT keeps more informative tokens and requires fewer training epochs to match baseline accuracy.
- Core assumption: The majority of video tokens are redundant, so selective removal preserves most information while drastically reducing token count.
- Break condition: If the similarity threshold τ is set too high, perceptibly different patches will be treated as static, degrading accuracy faster than random masking.

## Foundational Learning

- Concept: Vision Transformer tokenization and patch embedding
  - Why needed here: RLT operates on the raw patches before they are embedded into tokens, so understanding the standard ViT pipeline clarifies why this pre-embedding pruning is effective.
  - Quick check question: In a standard ViT, what is the shape and role of the patch embedding matrix E?

- Concept: Attention masks and variable sequence lengths
  - Why needed here: RLT uses example packing and block-diagonal attention masks to handle different numbers of tokens per video; without this, the speedup is negated by padding.
  - Quick check question: How does a block-diagonal attention mask ensure tokens from different examples do not attend to each other?

- Concept: Run-length encoding and positional encodings
  - Why needed here: RLT borrows the idea of run-length encoding to store how many identical patches a single token represents; the positional encoding must reflect this variable length.
  - Quick check question: In RLT, how is the run-length ℓi computed for a token at spatial location (x,y) and time t?

## Architecture Onboarding

- Component map: Input video → Patch extraction → Temporal similarity comparison → Binary mask Mstatic → Pruned patches P′ → Length encoding ℓi → Positional encoding ϕ(Ti) → Standard ViT pipeline
- Critical path: Similarity computation and mask generation must be fully parallelized on GPU to avoid overhead; this is the main speed determinant.
- Design tradeoffs:
  - Lower τ → fewer tokens removed, higher accuracy, smaller speedup
  - Higher τ → more tokens removed, risk of accuracy drop, larger speedup
  - Example packing batch size → trade-off between GPU utilization and variance in sequence lengths
- Failure signatures:
  - No speedup observed → similarity comparison not GPU-accelerated or mask generation overhead dominates
  - Accuracy drop larger than expected → τ set too high, causing removal of informative patches
  - Training crashes → attention implementation cannot handle variable sequence lengths without padding
- First 3 experiments:
  1. Implement patch extraction and temporal similarity check on a small video clip; verify mask Mstatic matches manual inspection.
  2. Integrate length encoding and test forward pass on a single example with known static and dynamic patches.
  3. Run a small-scale training loop with example packing and block-diagonal mask; measure token counts per batch and wall-clock time.

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but based on the content, several important open questions emerge:

### Open Question 1
- Question: Does RLT's effectiveness on static videos translate to other domains with high redundancy, such as surveillance footage or screen recordings?
- Basis in paper: [explicit] The paper demonstrates high token reduction (up to 80%) on COIN and Breakfast datasets with mostly static content, suggesting potential for similar applications.
- Why unresolved: The paper only evaluates on action recognition datasets; it doesn't test on surveillance footage or screen recordings which may have different patterns of redundancy.
- What evidence would resolve it: Experiments applying RLT to surveillance or screen recording datasets showing comparable token reduction rates and performance.

### Open Question 2
- Question: How does RLT's performance compare to learned token pruning methods when computational resources are abundant?
- Basis in paper: [inferred] The paper notes that learned methods require significant training overhead, but doesn't compare performance when this overhead is not a constraint.
- Why unresolved: All comparisons in the paper focus on efficiency, not peak performance when computational budget is unlimited.
- What evidence would resolve it: Head-to-head comparison of RLT versus learned pruning methods on identical hardware with unlimited training time.

### Open Question 3
- Question: What is the theoretical limit of RLT's token reduction on highly redundant videos?
- Basis in paper: [explicit] The paper shows up to 80% token reduction on long, static videos but doesn't establish theoretical bounds.
- Why unresolved: The paper demonstrates practical effectiveness but doesn't explore the theoretical maximum reduction possible on ideal cases.
- What evidence would resolve it: Analysis of RLT's performance on artificially generated videos with varying degrees of redundancy, identifying the point of diminishing returns.

### Open Question 4
- Question: How does RLT's threshold parameter τ interact with different video compression algorithms?
- Basis in paper: [explicit] The paper uses τ = 0.1 as a default but doesn't explore interaction with video compression artifacts.
- Why unresolved: The paper assumes clean video input but doesn't examine how compression artifacts might affect RLT's similarity comparisons.
- What evidence would resolve it: Experiments varying τ across videos with different compression levels (H.264, H.265, VP9) to find optimal thresholds for each.

### Open Question 5
- Question: Can RLT be extended to handle camera motion more effectively?
- Basis in paper: [explicit] The paper acknowledges RLT's limitation with camera motion, noting "in a video with constant camera motion, few tokens will be removed."
- Why unresolved: The paper identifies this limitation but doesn't propose solutions or explore potential modifications to handle motion.
- What evidence would resolve it: Modified RLT algorithm incorporating motion estimation that maintains or improves speed benefits on camera-motion videos.

## Limitations
- Effectiveness depends heavily on video content having temporal redundancy; highly dynamic videos with rapid motion see minimal token reduction
- The method's performance on domains beyond action recognition (surveillance, animation, sports) remains untested
- Claims of "no tuning required" are not thoroughly validated across diverse video characteristics and compression artifacts

## Confidence
- **High Confidence**: The core technical mechanism of RLT (temporal patch comparison and run-length encoding) is clearly described and algorithmally sound. Speed measurements showing reduced token counts and faster throughput are straightforward to verify.
- **Medium Confidence**: Accuracy preservation claims are supported by experiments on two datasets, but generalizability to other video domains and tasks remains uncertain. Limited ablations on how different video characteristics affect the speedup-accuracy tradeoff.
- **Low Confidence**: Claims of compatibility with "other methods" and "no tuning for different datasets" are not thoroughly validated. Brief mention of combining with random masking but lacks systematic analysis of parameter sensitivity.

## Next Checks
1. **Cross-domain validation**: Apply RLT to videos with varying temporal characteristics (sports, surveillance, animation) and measure both speedup and accuracy degradation across multiple task types beyond action recognition.
2. **Hardware implementation profiling**: Profile the actual GPU implementation of the similarity comparison and mask generation steps across different hardware configurations to verify that the reported negligible overhead is consistently achievable.
3. **Parameter sensitivity analysis**: Systematically vary the similarity threshold τ and run-length encoding parameters across different video characteristics to determine whether "no tuning" is truly achievable or if dataset-specific optimization provides significant benefits.