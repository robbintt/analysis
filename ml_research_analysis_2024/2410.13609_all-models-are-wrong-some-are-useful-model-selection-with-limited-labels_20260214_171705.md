---
ver: rpa2
title: 'All models are wrong, some are useful: Model Selection with Limited Labels'
arxiv_id: '2410.13609'
source_url: https://arxiv.org/abs/2410.13609
tags:
- best
- labels
- selector
- learning
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MODEL SELECTOR, a framework for efficient
  model selection among pretrained classifiers using limited labeled data. The core
  idea involves selecting the most informative unlabeled examples to label, maximizing
  expected information gain about the best model.
---

# All models are wrong, some are useful: Model Selection with Limited Labels

## Quick Facts
- arXiv ID: 2410.13609
- Source URL: https://arxiv.org/abs/2410.13609
- Reference count: 40
- This paper introduces MODEL SELECTOR, a framework for efficient model selection among pretrained classifiers using limited labeled data.

## Executive Summary
This paper addresses the challenge of model selection when labeled data is scarce but pretrained classifiers are abundant. The authors propose MODEL SELECTOR, a framework that strategically selects the most informative unlabeled examples to label, maximizing expected information gain about the best model. By characterizing the relationship between models and true labels through a single-parameter model and using a greedy mutual information-based selection policy, the framework significantly reduces labeling costs while maintaining selection accuracy.

## Method Summary
MODEL SELECTOR operates by first defining a probabilistic model that captures the relationship between the best model and true labels. It then uses mutual information to quantify the expected information gain from labeling specific examples. The framework employs a greedy policy to select the most informative unlabeled examples for labeling, iteratively updating its belief about which model is best. This approach allows for efficient model selection with minimal labeled data by focusing labeling efforts on the most informative examples rather than random sampling or uncertainty-based approaches.

## Key Results
- Reduces labeling costs by up to 94.15% compared to strongest baseline in identifying the best model
- Achieves up to 72.41% reduction in labeling costs when selecting a near-best model within 1% accuracy of the best
- Consistently outperforms adapted baselines like uncertainty sampling, margin sampling, active model comparison, and variance minimization across 18 model collections and 16 datasets

## Why This Works (Mechanism)
The framework works by leveraging the availability of unlabeled data and pretrained classifiers to strategically select which examples to label. By using mutual information to quantify the expected information gain from labeling each example, the method can identify which examples will be most informative for distinguishing between candidate models. The single-parameter model provides a computationally efficient way to characterize the relationship between model predictions and true labels, while the greedy selection policy ensures practical implementation despite the theoretical complexity of the optimal selection problem.

## Foundational Learning

**Mutual Information**: A measure of the amount of information obtained about one random variable through observing another random variable. Needed to quantify the expected information gain from labeling specific examples. Quick check: Verify that the mutual information calculation correctly captures the reduction in uncertainty about the best model.

**Bayesian Model Selection**: A framework for choosing between models based on their posterior probabilities given observed data. Needed to update beliefs about which model is best as more labeled data becomes available. Quick check: Confirm that the posterior updates follow Bayesian principles and properly incorporate new evidence.

**Information Gain**: The expected reduction in entropy or uncertainty from observing a particular variable. Needed to identify which examples will be most informative for model selection. Quick check: Validate that examples with high information gain truly provide more insight into model differences.

## Architecture Onboarding

**Component Map**: Pretrained Models -> Mutual Information Calculator -> Greedy Selector -> Labeled Dataset -> Model Evaluator -> Best Model Selection

**Critical Path**: The framework iteratively selects examples to label based on maximum expected information gain, labels them, updates the model selection posterior, and repeats until confident in the best model selection.

**Design Tradeoffs**: The greedy approach trades optimality for computational efficiency, while the single-parameter model trades expressiveness for tractability. These choices enable practical implementation but may miss optimal selections in complex scenarios.

**Failure Signatures**: Poor performance when candidate models are poorly calibrated, when the single-parameter model assumption breaks down, or when unlabeled data is not representative of the true distribution.

**First Experiments**:
1. Verify mutual information calculations on simple synthetic datasets with known ground truth
2. Test greedy selection policy on small model collections with controlled label noise
3. Validate posterior updates follow Bayesian principles on toy examples

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes pretrained classifiers are available and well-calibrated, which may not hold in all scenarios
- The single-parameter model may not capture complex relationships in all domains
- Greedy selection may miss optimal choices due to myopic decision-making
- Computational overhead for large-scale applications not thoroughly discussed

## Confidence

**High confidence**: Core performance improvements are well-supported by extensive experiments across multiple datasets and model collections.

**Medium confidence**: Generalizability claims are limited by assumptions about model calibration, single-parameter model validity, and unlabeled data representativeness.

## Next Checks

1. **Cross-domain validation**: Test MODEL SELECTOR on datasets from entirely different domains (e.g., medical imaging, scientific simulations) to assess generalizability beyond the current experimental scope.

2. **Noisy label robustness**: Evaluate the framework's performance when candidate models produce noisy or unreliable predictions, simulating real-world scenarios where pretrained models may be poorly calibrated.

3. **Scalability analysis**: Conduct experiments measuring computational overhead and selection efficiency as the number of candidate models and dataset size scale to millions of unlabeled examples.