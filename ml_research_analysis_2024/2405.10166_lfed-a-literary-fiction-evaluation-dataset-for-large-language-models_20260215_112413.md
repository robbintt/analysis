---
ver: rpa2
title: 'LFED: A Literary Fiction Evaluation Dataset for Large Language Models'
arxiv_id: '2405.10166'
source_url: https://arxiv.org/abs/2405.10166
tags:
- question
- dataset
- llms
- questions
- fiction
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LFED, a Literary Fiction Evaluation Dataset
  for assessing large language models (LLMs) on long fiction comprehension and reasoning.
  The dataset includes 95 literary fictions and 1,304 questions across 8 categories,
  covering a wide range of topics and centuries.
---

# LFED: A Literary Fiction Evaluation Dataset for Large Language Models

## Quick Facts
- arXiv ID: 2405.10166
- Source URL: https://arxiv.org/abs/2405.10166
- Reference count: 0
- Primary result: ChatGPT achieves only 57.08% accuracy on Chinese literary fiction comprehension under zero-shot setting

## Executive Summary
This paper introduces LFED, a Literary Fiction Evaluation Dataset designed to assess large language models (LLMs) on long fiction comprehension and reasoning. The dataset contains 95 Chinese literary fictions and 1,304 multiple-choice questions across 8 categories, covering a wide range of topics and centuries. Through crowdsourcing and expert review, the questions test various reasoning capabilities including fact understanding, logical reasoning, and counterfactual analysis. Experiments show that current LLMs struggle significantly with this task, with ChatGPT achieving only 57.08% accuracy under zero-shot setting, highlighting the dataset's potential as a challenging benchmark.

## Method Summary
The LFED dataset was created through a multi-stage process involving fiction selection, crowdsourced question creation, and expert review. The dataset includes 95 Chinese literary fictions (originals or translations) with 1,304 multiple-choice questions spanning 8 reasoning categories. Evaluation was conducted using zero-shot and few-shot settings with 6 different LLMs (ChatGPT, ChatGLM-6B, BLOOM-560M, BLOOM-1B7, BELLE-7B-0.2M, BELLE-7B-2M) with both short and long prompts. The questions were annotated through crowdsourcing followed by expert validation to ensure quality and category correctness.

## Key Results
- ChatGPT achieves only 57.08% accuracy on LFED under zero-shot setting
- Performance varies significantly across question categories, with counterfactual reasoning being particularly challenging
- Longer prompts generally improve performance compared to short prompts
- All evaluated LLMs show substantial difficulty with literary fiction comprehension tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Literary fiction comprehension requires long-context reasoning across narrative elements (character relationships, plot events, counterfactuals).
- Mechanism: By curating 95 literary fictions in Chinese with 1,304 multiple-choice questions spanning 8 categories, the dataset forces LLMs to perform deep contextual inference rather than surface-level pattern matching.
- Core assumption: Question diversity and fiction length create challenges that current LLMs cannot easily overcome with memorized or shallow reasoning patterns.
- Evidence anchors:
  - [abstract]: "demonstrates that these models face considerable challenges in effectively addressing questions related to literary fictions, with ChatGPT reaching only 57.08% under the zero-shot setting."
  - [section 4.2]: Performance differences across question types (e.g., counterfactual reasoning) and prompts (long vs short) show reasoning difficulty varies by cognitive demand.
  - [corpus]: No direct correlation with citation counts or H-indices suggests novelty and lack of prior strong benchmarks.
- Break condition: If a model achieves >85% accuracy, the dataset may no longer be sufficiently challenging.

### Mechanism 2
- Claim: Expert-annotated questions with rigorous quality control reduce noise and increase dataset reliability for evaluation.
- Mechanism: Two-stage annotation (crowdsourcing + expert review) ensures questions are contextually accurate, category-correct, and free of ambiguity.
- Core assumption: High-quality human judgment can eliminate errors in question taxonomy classification and answer validity.
- Evidence anchors:
  - [section 3.3]: "We design a fine-grained and strict procedure for data annotation, which ensures the quality of LFED."
  - [section 3.3]: Agreement rates for category correctness (87.92%) and counterfactual alignment (98.33%) indicate strong consistency.
  - [corpus]: Lack of explicit dataset citations suggests novelty; however, low average citations in neighbors may indicate a small field or emerging area.
- Break condition: If expert disagreement rises above 20%, the annotation process may introduce bias or inconsistency.

### Mechanism 3
- Claim: Long fictions (100K+ characters) exceed current LLM context windows, forcing reliance on learned knowledge and generalization rather than full document parsing.
- Mechanism: By selecting novels longer than typical context windows, the dataset ensures LLMs cannot simply "read" the entire text during inference.
- Core assumption: Exceeding context window forces models to use learned representations and reasoning over narrative summaries or key events.
- Evidence anchors:
  - [section 3.1]: "The majority of selected fictions (i.e., 64 fictions) are very long, containing more than 100K Chinese characters (longer than the context window of most LLMs)."
  - [section 4.2]: Performance degrades on longer texts and counterfactual reasoning, suggesting reasoning over incomplete text.
  - [corpus]: No strong prior evidence in neighbors; this is a novel evaluation strategy.
- Break condition: If models gain access to long-context capabilities (>100K tokens), this mechanism weakens.

## Foundational Learning

- Concept: Question taxonomy design and classification
  - Why needed here: Ensures each question tests a specific reasoning skill, enabling fine-grained performance analysis
  - Quick check question: Can you map a sample question to the correct of the 8 categories without ambiguity?

- Concept: Annotation quality control in crowdsourced datasets
  - Why needed here: Prevents noisy or ambiguous questions from undermining evaluation validity
  - Quick check question: If an expert rejects a question, can you identify whether it's due to category misclassification or factual error?

- Concept: Prompt engineering for LLM evaluation (short vs long prompts)
  - Why needed here: Different prompts can influence model performance; understanding this impact is key for fair benchmarking
  - Quick check question: Does switching from short to long prompt improve or degrade performance on counterfactual reasoning questions?

## Architecture Onboarding

- Component map: Fiction collection -> Crowdsourced question creation -> Expert review -> Dataset assembly -> Prompt generation -> LLM inference -> Answer extraction -> Result aggregation
- Critical path: Fiction selection -> Question creation -> Expert validation -> Evaluation runs -> Analysis
- Design tradeoffs:
  - Fiction length vs. evaluation feasibility: Longer texts increase difficulty but may reduce question quality
  - Expert review vs. speed: Rigorous review ensures quality but slows dataset creation
  - Prompt complexity vs. model comprehension: Longer prompts may guide better reasoning but risk overfitting
- Failure signatures:
  - High disagreement in expert review -> Annotation guidelines unclear
  - Consistent performance drop on certain categories -> Question taxonomy flawed
  - No performance variation across prompt types -> Prompt engineering ineffective
- First 3 experiments:
  1. Run zero-shot evaluation with short prompt on all models; record per-category accuracy
  2. Repeat with long prompt; compare performance shifts
  3. Run few-shot evaluation (1-5 shots) on the most challenging categories; analyze learning curves

## Open Questions the Paper Calls Out

- What is the impact of different question formats on the performance of LLMs in evaluating long literary fiction comprehension?
- How does the performance of LLMs on LFED correlate with their performance on other NLP benchmarks?
- What are the specific challenges that LLMs face in comprehending and reasoning over long literary fictions?

## Limitations

- Limited scope to Chinese literary fiction may restrict generalizability to other languages and cultural contexts
- The dataset's effectiveness depends on the assumption that longer texts genuinely exceed LLM context windows, which may change as models evolve
- Expert review quality is high but may introduce cultural or interpretive biases in question validation
- Performance metrics are based on multiple-choice format, which may not fully capture nuanced comprehension capabilities

## Confidence

- **High confidence**: The dataset creation methodology (fiction selection, question taxonomy, expert review process) is well-documented and methodologically sound
- **Medium confidence**: The claim that LLMs struggle with literary fiction comprehension is supported by results, but performance may improve rapidly as models advance
- **Low confidence**: The assertion that longer texts force reliance on learned knowledge rather than full document parsing is plausible but not directly tested

## Next Checks

1. Cross-linguistic validation: Evaluate the same question types and taxonomy on English literary fiction to test cultural and language generalizability
2. Context window stress test: Systematically evaluate models with varying context windows (including those exceeding 100K tokens) to quantify the impact of text length on performance
3. Human benchmark comparison: Establish human performance baselines on the same questions to calibrate the difficulty level and validate the dataset's discriminative power