---
ver: rpa2
title: Causal Estimation of Memorisation Profiles
arxiv_id: '2406.04327'
source_url: https://arxiv.org/abs/2406.04327
tags:
- memorisation
- training
- estimator
- counterfactual
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new method for estimating counterfactual
  memorisation in language models, based on the difference-in-differences design from
  econometrics. The method characterises a model's memorisation profile by observing
  its behaviour on a small set of training instances throughout training, requiring
  only log-likelihood measurements.
---

# Causal Estimation of Memorisation Profiles

## Quick Facts
- arXiv ID: 2406.04327
- Source URL: https://arxiv.org/abs/2406.04327
- Authors: Pietro Lesci; Clara Meister; Thomas Hofmann; Andreas Vlachos; Tiago Pimentel
- Reference count: 40
- Primary result: Introduces difference-in-differences method to estimate counterfactual memorisation in language models using only log-likelihood measurements

## Executive Summary
This paper presents a novel approach to estimate counterfactual memorisation in language models by leveraging difference-in-differences (DiD) design from econometrics. The method characterises a model's memorisation profile by observing its behaviour on a small set of training instances throughout training, requiring only log-likelihood measurements. Experiments with the Pythia model suite reveal that memorisation is stronger and more persistent in larger models, is influenced by data order and learning rate, and exhibits stable trends across model sizes, making larger model memorisation predictable from smaller ones.

## Method Summary
The method estimates counterfactual memorisation by computing the difference-in-differences between performance changes on trained versus validation instances across training checkpoints. For each treatment step (macro-batch of training data), the estimator calculates how much better the model performs on trained instances compared to what would have happened without training, using validation instances as counterfactual controls. The approach requires only log-likelihood measurements from existing checkpoints, avoiding expensive retraining while providing statistically principled estimates of memorisation dynamics.

## Key Results
- Memorisation is stronger and more persistent in larger models, with correlation coefficients indicating predictable relationships across scales
- Data order and learning rate significantly influence memorisation patterns
- DiD estimates capture nuanced memorisation effects that simpler difference estimators miss
- The method achieves computational efficiency by requiring only log-likelihood measurements from existing checkpoints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The difference-in-differences (DiD) estimator provides unbiased estimates of counterfactual memorisation by controlling for time-invariant confounders via parallel trends assumption.
- Mechanism: DiD compares the change in performance on trained vs. validation instances over time. If validation instances would have followed similar performance trends to trained ones in absence of training, then the difference in their performance changes isolates the memorisation effect.
- Core assumption: Parallel trends assumption - in absence of training, expected change in model performance across checkpoints would be the same regardless of treatment.
- Evidence anchors:
  - [abstract] "we characterise a model's memorisation profile--its memorisation trends across training--by only observing its behaviour on a small set of instances throughout training"
  - [section] "Difference-in-Differences Estimator... This statistical estimand thus identifies τg,c... under Assumps. 2 and 3"
  - [corpus] Weak - corpus mentions counterfactual reasoning but not specifically DiD methodology
- Break condition: If validation and training instances have systematically different performance trends unrelated to training (e.g., different data distributions or difficulty levels).

### Mechanism 2
- Claim: The method enables estimation of memorisation profiles without expensive retraining by leveraging existing checkpoints.
- Mechanism: By computing performance differences between checkpoints before and after training on specific batches, the method reconstructs counterfactual outcomes using only observational data from available checkpoints.
- Core assumption: No anticipation assumption - training has no effect before it happens.
- Evidence anchors:
  - [abstract] "proposes a new, principled, and efficient method to estimate memorisation based on the difference-in-differences design from econometrics"
  - [section] "Our first approach to estimate memorisation is straightforward and only requires the observed outcomes of a held-out validation set"
  - [corpus] Weak - corpus discusses counterfactual reasoning but doesn't specifically address checkpoint-based estimation efficiency
- Break condition: If model checkpoints are too sparse to capture meaningful performance changes, or if checkpoint creation introduces systematic biases.

### Mechanism 3
- Claim: The method generalises across model scales, allowing prediction of larger model memorisation from smaller ones.
- Mechanism: Memorisation profiles show stable trends across model sizes, with correlation coefficients indicating predictable relationships between model scales.
- Core assumption: Memorisation dynamics are scale-invariant in their fundamental patterns.
- Evidence anchors:
  - [abstract] "memorisation... has stable trends across model sizes, thus making memorisation in larger models predictable from smaller ones"
  - [section] "memorisation profiles are stable across model sizes; thus, we can predict memorisation in larger models from the memorisation observed in smaller ones"
  - [corpus] Weak - corpus mentions counterfactual reasoning across scales but doesn't specifically address scale predictability
- Break condition: If larger models exhibit fundamentally different memorisation mechanisms or if training instabilities in smaller models break the correlation pattern.

## Foundational Learning

- Concept: Potential outcomes framework
  - Why needed here: Provides formal notation to represent both observed and counterfactual outcomes needed for causal memorisation estimation
  - Quick check question: What is the key difference between Yc(x; g) and Yc(x; ∞) in the notation?

- Concept: Difference-in-differences design
  - Why needed here: Enables identification of causal effects using observational data by comparing trends over time between treated and control groups
  - Quick check question: What are the two key assumptions required for DiD identification?

- Concept: Parallel trends assumption
  - Why needed here: Critical assumption that allows using validation instances as counterfactual controls by assuming similar performance trends in absence of training
  - Quick check question: How does the parallel trends assumption differ from the i.i.d. dataset sampling assumption?

## Architecture Onboarding

- Component map: Subsampling → Evaluation → DiD computation → Statistical inference → Visualization
- Critical path: Subsampling → Evaluation → DiD computation → Statistical inference → Visualization
- Design tradeoffs:
  - Subsampling rate vs. estimator variance: Higher sampling reduces variance but increases computational cost
  - Checkpoint frequency vs. temporal resolution: More checkpoints capture finer memorisation dynamics but require more storage
  - Performance metric choice: Sequence log-likelihood captures nuanced effects but may be noisier than accuracy
- Failure signatures:
  - All estimates insignificant: Likely due to insufficient sampling or too coarse performance metric
  - Asymmetric patterns across treatment steps: May indicate data ordering effects or learning rate schedule impacts
  - Unexpected negative memorisation: Could indicate model forgetting or issues with parallel trends assumption
- First 3 experiments:
  1. Verify DiD estimator with synthetic data where ground truth memorisation is known
  2. Test sensitivity to subsampling rate by varying number of instances per macro-batch
  3. Compare DiD estimates with naive difference estimator to validate assumption requirements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the DiD estimator of memorisation perform equally well across different model architectures (e.g., transformers vs. other architectures) and modalities (e.g., vision vs. language)?
- Basis in paper: [inferred] The paper demonstrates the DiD estimator on Pythia transformers for language modeling, but notes the method "can be trivially applied to any neural model and input modality."
- Why unresolved: The paper only empirically tests the method on one architecture (transformers) and one modality (language). Different architectures or modalities might exhibit different training dynamics that could violate the parallel trends or no anticipation assumptions.
- What evidence would resolve it: Applying the DiD estimator to memorisation analysis in other architectures (e.g., CNNs, RNNs) and modalities (e.g., image, audio) and comparing the results to the transformer language model case.

### Open Question 2
- Question: How robust is the DiD estimator to violations of the parallel trends assumption when the training and validation distributions differ in systematic ways?
- Basis in paper: [explicit] The paper notes that the parallel trends assumption is "strictly weaker" than the i.i.d. assumption and "only requires that the training and validation sets follow similar trends on average."
- Why unresolved: The paper assumes parallel trends hold but doesn't test scenarios where the training and validation sets might systematically differ (e.g., different document sources, different preprocessing). The estimator's performance in such cases is unknown.
- What evidence would resolve it: Systematic experiments where training and validation sets are constructed to differ in known ways (e.g., different domains, different deduplication) and measuring how these differences affect DiD estimator accuracy.

### Open Question 3
- Question: What is the relationship between the granularity of performance metrics (e.g., token-level vs. sequence-level) and the sensitivity of memorisation detection?
- Basis in paper: [explicit] The paper shows memorisation profiles using different metrics (token accuracy, token rank, sequence log-likelihood) and notes that "finer-grained metrics—like sequence log-likelihood—allow us to detect smaller memorisation effects."
- Why unresolved: While the paper demonstrates that metric choice affects detection sensitivity, it doesn't establish a quantitative relationship or determine which metric is optimal for different research questions about memorisation.
- What evidence would resolve it: Controlled experiments varying metric granularity while holding other factors constant, measuring detection sensitivity for different types and magnitudes of memorisation effects.

## Limitations
- Parallel trends assumption is difficult to verify empirically and may be violated if training and validation sets have systematically different characteristics
- Method requires sufficient checkpoints to capture meaningful performance changes, which may not be available for all training regimes
- Relationship between log-likelihood changes and actual memorisation remains somewhat indirect

## Confidence
High confidence: The methodological framework of using difference-in-differences for counterfactual estimation in this context is sound and well-established in econometrics.

Medium confidence: The empirical findings about memorisation being stronger in larger models and showing stable trends across scales are supported by the data but could benefit from additional validation.

Low confidence: The practical significance of the quantified memorisation levels and their relationship to actual privacy risks or model behaviour in deployment contexts is not fully established.

## Next Checks
1. Implement a formal test for the parallel trends assumption by examining performance differences in early checkpoints where treatment effects should be minimal, and use permutation tests to assess whether observed trends are significantly different between treatment and control groups.

2. Validate the DiD estimator using synthetic data where ground truth memorisation effects can be controlled, comparing DiD estimates against known true values across different sampling rates and checkpoint frequencies.

3. Test the scale-predictability hypothesis by training models at intermediate sizes (between the Pythia scales used) and measuring whether memorisation profiles follow the predicted patterns, examining the correlation structure across a broader range of model scales.