---
ver: rpa2
title: A Multi-Domain Multi-Task Approach for Feature Selection from Bulk RNA Datasets
arxiv_id: '2405.02534'
source_url: https://arxiv.org/abs/2405.02534
tags:
- features
- feature
- selection
- domain
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel multi-domain multi-task learning (MDMTL)
  approach for feature selection from bulk RNA sequencing data. The method leverages
  two datasets from mouse host immune response to Salmonella infection, with data
  collected from spleen and liver tissues serving as two distinct domains.
---

# A Multi-Domain Multi-Task Approach for Feature Selection from Bulk RNA Datasets

## Quick Facts
- arXiv ID: 2405.02534
- Source URL: https://arxiv.org/abs/2405.02534
- Reference count: 40
- A novel MDMTL approach identifies novel biomarkers in bulk RNA data that are undetectable by single-domain analysis

## Executive Summary
This paper introduces a multi-domain multi-task learning (MDMTL) approach for feature selection from bulk RNA sequencing data, specifically applied to mouse immune response to Salmonella infection across spleen and liver tissues. The method employs domain-specific variational autoencoders with a shared classifier and sparsification layer, trained jointly to reconstruct input data, perform classification, and promote sparsity in features. The approach successfully identifies novel biomarkers whose signals are too weak to be captured by analyzing domains individually, demonstrating superior performance compared to single-domain methods.

## Method Summary
The MDMTL approach uses a neural network architecture consisting of domain-specific variational autoencoders (VAEs) that reconstruct sparsified inputs while aligning latent embeddings, a shared classifier for distinguishing susceptible from tolerant/resistant phenotypes, and a sparsification layer with ℓ1-norm penalty to promote feature selection. The network is trained with a weighted combination of reconstruction loss, classification loss, KL divergence (for VAE regularization), and sparsity loss. Feature importance is determined by the frequency of non-zero weights across multiple training runs, enabling robust identification of discriminative biomarkers that emerge from cross-domain signal consistency.

## Key Results
- MDMTL approach identifies novel biomarkers whose signals are too weak to be captured by single-domain analysis
- The method achieves excellent classification results between susceptible and tolerant or resistant phenotypes
- Some biomarkers are only present in the across-domain experiment, not in single-domain analyses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint training of domain-specific VAEs with a shared classifier enables discovery of biomarkers only discriminative when both domains are considered.
- Mechanism: Reconstructing sparsified inputs in each domain while aligning embeddings through shared classification amplifies weak but consistent signals across tissues, allowing them to emerge as top discriminative features.
- Core assumption: Weak biomarker signals in individual domains can combine into strong discriminative signals when analyzed jointly, and VAE reconstruction preserves these subtle features.
- Break condition: If domain alignment in latent space fails, the shared classifier cannot leverage cross-domain signal consistency, and weak features remain undetected.

### Mechanism 2
- Claim: ℓ1-norm penalty on input weights enables effective feature selection while avoiding computational issues of ℓ0-norm.
- Mechanism: Penalizing ℓ1-norm of sparse layer weights during training zeros out irrelevant genes; frequency of non-zero weights across runs provides robust importance scores.
- Core assumption: ℓ1-norm minimization approximates intractable ℓ0-norm well for high-dimensional biological data, and repeated sampling improves stability.
- Break condition: If ℓ1 penalty is too weak, irrelevant features remain; if too strong, true biomarkers are suppressed.

### Mechanism 3
- Claim: Multi-task learning balances reconstruction, classification, and sparsity objectives, preventing any single task from dominating training.
- Mechanism: Weighted sum of reconstruction loss (MSE), classification loss (log loss), KL divergence (variational regularization), and sparsity loss (ℓ1 norm) ensures each task contributes without overwhelming others.
- Core assumption: Fixed weight hyperparameters adequately balance competing objectives across all experimental conditions.
- Break condition: If weights are poorly tuned, one task (e.g., reconstruction) may dominate, degrading feature selection or classification performance.

## Foundational Learning

- Concept: Domain alignment in latent space
  - Why needed here: Ensures corresponding biological states are represented similarly across spleen and liver, allowing shared classifier to learn cross-tissue patterns.
  - Quick check question: If you visualize PCA of latent embeddings from both domains, what should you observe for good alignment?

- Concept: Variational autoencoder reconstruction
  - Why needed here: VAE must faithfully reconstruct sparsified input to preserve biological signal while learning compressed representation useful for classification.
  - Quick check question: What happens to latent space distribution if KL divergence term is omitted?

- Concept: Frequency-based feature importance aggregation
  - Why needed here: Single-run feature selection is unstable; aggregating across multiple random initializations and data splits yields more reliable biomarkers.
  - Quick check question: How does the "elbow method" help decide which features to keep after frequency counting?

## Architecture Onboarding

- Component map: Input layer → Shared Sparsification Layer (ℓ1-penalized) → Domain-specific VAEs (Encoder → 128D latent → Decoder) → Shared Classifier (5-layer FC → 2D → 1D sigmoid) → Loss aggregation
- Critical path: Sparsification → VAE reconstruction → Latent alignment → Classification → Sparsity update (via ℓ1 gradient)
- Design tradeoffs:
  - Using VAEs instead of plain autoencoders adds stochasticity and regularization but increases training time
  - Fixed loss weights simplify tuning but may not adapt to varying task difficulty across experiments
  - ℓ1 sparsity is computationally efficient but may miss truly sparse solutions compared to ℓ0
- Failure signatures:
  - If classification accuracy plateaus early, check if sparsity loss is overwhelming other terms
  - If reconstruction loss remains high, the sparsification mask may be too aggressive
  - If PCA shows poor cross-domain clustering, the shared classifier may not be learning aligned embeddings
- First 3 experiments:
  1. Train with α=10, β=1e-4, γ=1, θ=1e-4 for 20k epochs; monitor sparsity and classification curves
  2. After training, apply elbow method to frequency distribution; zero out weights below threshold
  3. Compare frequency-ranked features to single-domain results; identify "None" category features unique to MDMT

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the MDMTL approach be further improved to better handle cases where highly-weighted features extracted from single domain experiments are not captured in the respective across-domain experiments?
- Basis in paper: [inferred] The paper mentions that some highly-weighted features extracted from one domain experiments are not captured in respective across-domain experiments, potentially indicating a need for improved robustness or suggesting differences in infection manifestations across tissues.
- Why unresolved: The paper acknowledges this observation but does not provide a definitive explanation or solution for addressing this discrepancy.
- What evidence would resolve it: Conducting additional experiments with other datasets to determine if this pattern persists, and investigating potential modifications to the MDMTL architecture or training process to improve feature alignment across domains.

### Open Question 2
- Question: What are the specific biological implications and functional roles of the novel biomarkers identified by the MDMTL approach that are not present in single-domain experiments?
- Basis in paper: [explicit] The paper states that the MDMTL approach identifies novel biomarkers whose signals appear to be amplified by the multi-domain approach, and some of these biomarkers do not appear in either single experiment.
- Why unresolved: While the paper demonstrates the ability of the MDMTL approach to identify these novel biomarkers, it does not explore their biological significance or functional roles in detail.
- What evidence would resolve it: Conducting follow-up biological studies to investigate the specific functions and pathways associated with these novel biomarkers, and comparing their expression patterns and effects across different tissues and infection states.

### Open Question 3
- Question: How can the MDMTL approach be extended to handle more than two domains, and what are the potential challenges and limitations in scaling up to multiple domains?
- Basis in paper: [inferred] The current MDMTL approach is designed for two domains (spleen and liver), and the paper does not discuss its applicability or performance when dealing with more than two domains.
- Why unresolved: The paper focuses on demonstrating the effectiveness of the MDMTL approach for two domains, but does not explore its scalability or limitations when applied to multiple domains.
- What evidence would resolve it: Extending the MDMTL architecture and training process to handle multiple domains, and conducting experiments to evaluate its performance, robustness, and potential challenges when dealing with a larger number of domains.

## Limitations
- The specific weight hyperparameters were fixed without systematic sensitivity analysis, raising questions about generalizability
- The method's performance advantage relies on detecting weak but consistent signals, but the frequency of this scenario in practice is not quantified
- The claim of discovering "novel" biomarkers lacks validation against known biological markers or independent experimental confirmation

## Confidence
- High confidence: The technical feasibility of the MDMT architecture combining VAEs, shared classifier, and ℓ1 sparsity for feature selection from bulk RNA data
- Medium confidence: The claim that MDMT can identify biomarkers undetectable by single-domain analysis, based on the presented experiments
- Medium confidence: The overall superiority of MDMT for classification tasks between susceptible and tolerant/resistant phenotypes

## Next Checks
1. Conduct hyperparameter sensitivity analysis by varying α, β, γ, θ across a grid and measuring impact on both classification accuracy and feature selection stability
2. Perform ablation studies comparing MDMT to: (a) single-domain VAE+classifier, (b) non-variational autoencoder baseline, and (c) traditional statistical feature selection methods like limma or edgeR
3. Validate the biological relevance of top-ranked features by comparing against known Salmonella response pathways and conducting enrichment analysis for immune-related gene sets