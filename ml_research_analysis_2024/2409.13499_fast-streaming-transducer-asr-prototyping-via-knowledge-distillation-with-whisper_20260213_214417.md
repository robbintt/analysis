---
ver: rpa2
title: Fast Streaming Transducer ASR Prototyping via Knowledge Distillation with Whisper
arxiv_id: '2409.13499'
source_url: https://arxiv.org/abs/2409.13499
tags:
- data
- training
- speech
- supervised
- streaming
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that streaming Transformer-Transducer (TT)
  ASR models can be trained from scratch using only pseudo-labeled (PL) data generated
  by Whisper models, even on consumer GPUs. This one-stage approach avoids the need
  for large supervised datasets and extensive computational resources.
---

# Fast Streaming Transducer ASR Prototyping via Knowledge Distillation with Whisper

## Quick Facts
- arXiv ID: 2409.13499
- Source URL: https://arxiv.org/abs/2409.13499
- Reference count: 40
- One-line primary result: Streaming TT models can be trained from scratch using only Whisper-generated pseudo-labels on consumer GPUs, with shallow fusion and named entity biasing improving WERs.

## Executive Summary
This paper demonstrates that streaming Transformer-Transducer (TT) ASR models can be trained from scratch using only pseudo-labeled data generated by Whisper models, even on consumer GPUs. By using WhisperX for efficient pseudo-label generation, applying heuristics to filter noisy/hallucinated outputs, and leveraging chunk-wise training for streaming, the authors achieve competitive performance on 6 CommonVoice languages. Decoding improvements via shallow fusion of n-gram LMs and contextual biasing with named entities consistently reduce word error rates (WERs), with the largest gains seen when using weaker pseudo-labels.

## Method Summary
The method involves generating pseudo-labels from Whisper models (tiny, base, small, medium, large-v3) via WhisperX pipeline, filtering out noisy and hallucinated outputs using heuristics based on repeated unigrams, word length, and word ratio, then training Zipformer TT models from scratch using these pseudo-labels with chunk-wise training for streaming capability. The models are evaluated on CommonVoice test sets across 13 streaming configurations, with shallow fusion of n-gram LMs and named entity biasing applied during decoding to improve WERs. The approach is validated across 6 languages (CA, EN, DE, FR, ES, IT) and includes ablation studies on supervised data regularization and pseudo-label quality.

## Key Results
- TT models trained from scratch with Whisper-generated pseudo-labels achieve competitive WERs on 6 CommonVoice languages
- Shallow fusion of n-gram LMs and named entity biasing consistently improves WERs, especially with weaker pseudo-labels
- Streaming chunk-wise training enables low-latency inference while maintaining accuracy
- Adding small amounts of supervised data helps when pseudo-label quality is low

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pseudo-labeled data from Whisper can replace supervised data for training streaming Transformer-Transducer models.
- Mechanism: Whisper's large-scale training provides high-quality ASR outputs that can be used as labels for training smaller TT models, enabling one-stage training without supervised data.
- Core assumption: Whisper's outputs are sufficiently accurate to serve as training labels for streaming TT models.
- Evidence anchors: [abstract] "streaming Transformer-Transducer (TT) models can be trained from scratch in consumer and accessible GPUs in their entirety with pseudo-labeled (PL) speech from foundational speech models (FSM)." [section] "We demonstrate that TT models can be trained from scratch without supervised data, even with very noisy PLs."

### Mechanism 2
- Claim: Heuristics can effectively filter out noisy and hallucinated pseudo-labels.
- Mechanism: Applying filters based on repeated unigrams, word length, and word ratio removes low-quality pseudo-labels, improving TT model training.
- Core assumption: Hallucinations and noise in Whisper outputs follow detectable patterns that can be filtered.
- Evidence anchors: [abstract] "We propose multiple heuristics to filter out hallucinated PLs." [section] "We developed multiple data selection heuristics (H) to filter out noisy and hallucinated PLs."

### Mechanism 3
- Claim: Shallow fusion of n-gram LMs and contextual biasing with named entities consistently improves WERs.
- Mechanism: Integrating external language models and named entity lists during decoding corrects errors and biases predictions toward more likely sequences.
- Core assumption: External language models and named entities provide complementary information to the TT model's predictions.
- Evidence anchors: [abstract] "decoding improvements via shallow fusion of n-gram LMs and contextual biasing with named entities consistently reduce word error rates (WERs)." [section] "Leveraging more text data and context information with language model and keywords integration can considerably improve ASR performance."

## Foundational Learning

- Concept: Knowledge Distillation
  - Why needed here: Understanding how knowledge from a large teacher model (Whisper) can be transferred to a smaller student model (TT) is crucial for pseudo-labeling.
  - Quick check question: What is the difference between sequence-level and posterior-level knowledge distillation?

- Concept: Transformer-Transducer Architecture
  - Why needed here: Knowing how TT models work, including their encoder, predictor, and joint networks, is essential for understanding their training and streaming capabilities.
  - Quick check question: How does the transducer decoder naturally support streaming decoding?

- Concept: Language Model Integration (Shallow Fusion)
  - Why needed here: Understanding how external LMs can be integrated during decoding to improve ASR performance is key for the shallow fusion experiments.
  - Quick check question: What is the role of the interpolation parameter λ in shallow fusion?

## Architecture Onboarding

- Component map: Whisper (Teacher Model) → WhisperX Pipeline → Pseudo-labels → TT Model (Student) → Streaming Decoding with Shallow Fusion
- Critical path: Pseudo-label generation → Data filtering → TT model training → Decoding with shallow fusion
- Design tradeoffs: Using larger Whisper models yields better pseudo-labels but increases computational cost; more aggressive filtering removes noise but may also discard valid data; streaming decoding with smaller chunk sizes improves latency but may degrade accuracy
- Failure signatures: High WERs despite training indicate poor quality pseudo-labels or ineffective filtering; model convergence issues may stem from noisy data or insufficient regularization; decoding performance degradation with shallow fusion suggests conflicts between TT model and LM predictions
- First 3 experiments: 1) Train TT model with pseudo-labels from different Whisper model sizes to assess quality impact; 2) Apply data filtering heuristics and measure WER improvement; 3) Implement shallow fusion with n-gram LM and evaluate WER reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do proposed data selection heuristics (H1-H4) perform on spontaneous speech vs read speech?
- Basis in paper: [explicit] Heuristics designed for CommonVoice read speech; limitations section notes potential domain shift to spontaneous speech
- Why unresolved: No experiments conducted on spontaneous speech datasets; only read speech (CommonVoice) tested
- What evidence would resolve it: Comparative experiments applying same heuristics to spontaneous speech datasets (e.g., TED-LIUM, Switchboard) measuring WER changes

### Open Question 2
- Question: What is the optimal computational budget allocation between PL generation vs supervised data regularization?
- Basis in paper: [inferred] Call-center experiment shows trade-off between PL data size and supervised data; no systematic study of budget allocation
- Why unresolved: Single fixed budget experiment; no ablation varying budget split between PL vs supervised data
- What evidence would resolve it: Grid search experiments varying total budget and split ratio, measuring WER and training time

### Open Question 3
- Question: Does shallow fusion decoding overhead impact streaming latency in practice?
- Basis in paper: [explicit] Shallow fusion improves WER but limitations section notes missing execution time measurements
- Why unresolved: Only mentions theoretical concerns about NN-LM size; no empirical latency measurements on streaming models
- What evidence would resolve it: Streaming inference benchmarks comparing baseline vs shallow-fused models with different LM sizes, measuring real-time factor

## Limitations

- The approach's reliance on Whisper quality creates potential failure modes when dealing with domain-specific hallucinations or when filtering heuristics are insufficient
- The paper doesn't fully explore the trade-off between filtering aggressiveness and data retention, leaving optimal parameter settings uncertain
- Performance gains from shallow fusion and named entity biasing lack detailed ablation studies to isolate their individual contributions

## Confidence

**High Confidence**: The core claim that TT models can be trained from scratch using Whisper-generated pseudo-labels is well-supported by experimental results across 6 languages and 13 decoding configurations.

**Medium Confidence**: The effectiveness of data filtering heuristics and shallow fusion improvements, while demonstrated, has some uncertainty around optimal parameter settings and generalizability to languages/datasets beyond CommonVoice.

**Low Confidence**: The claim about achieving competitive performance without any supervised data is partially supported, but the paper acknowledges that adding small amounts of supervised data helps in cases with low-quality pseudo-labels.

## Next Checks

1. **Ablation on Filtering Heuristics**: Systematically remove each filtering heuristic (H1-H4) individually to quantify their specific contributions to WER improvement and identify which are most critical for maintaining data quality without excessive data loss.

2. **Robustness to Whisper Model Size**: Conduct experiments using only whisper-tiny or whisper-base models (without mixing with larger models) to assess the lower bound of pseudo-label quality needed for successful TT training, and identify failure modes when Whisper outputs are too noisy.

3. **Cross-Dataset Generalization**: Apply the complete pipeline (pseudo-label generation → filtering → TT training → shallow fusion) to a non-CommonVoice dataset with different acoustic characteristics (e.g., TED-LIUM or LibriSpeech) to evaluate robustness and identify any dataset-specific limitations in the approach.