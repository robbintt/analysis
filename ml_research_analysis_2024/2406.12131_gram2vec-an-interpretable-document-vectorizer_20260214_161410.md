---
ver: rpa2
title: 'Gram2Vec: An Interpretable Document Vectorizer'
arxiv_id: '2406.12131'
source_url: https://arxiv.org/abs/2406.12131
tags:
- author
- gram
- features
- feature
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: GRAM2Vec embeds documents into higher-dimensional space using normalized
  frequencies of grammatical features, providing inherent interpretability compared
  to neural embeddings. It extracts features like letters, punctuation, emojis, POS
  tags, morphology, dependencies, syntactic constructions, and function words.
---

# Gram2Vec: An Interpretable Document Vectorizer
## Quick Facts
- arXiv ID: 2406.12131
- Source URL: https://arxiv.org/abs/2406.12131
- Reference count: 5
- Primary result: Achieved 27.6% EER for authorship attribution alone, improved to 15.1% EER when combined with neural LUAR model

## Executive Summary
GRAM2Vec is a document vectorization method that creates interpretable document representations by using normalized frequencies of grammatical features. Unlike neural embedding approaches that produce opaque vector representations, GRAM2Vec extracts features including letters, punctuation, emojis, POS tags, morphology, dependencies, syntactic constructions, and function words. When applied to authorship attribution tasks, GRAM2Vec achieved a 27.6% equal error rate on its own, which while higher than the 15.5% achieved by the neural LUAR model, provided complementary information. Combining GRAM2Vec with LUAR improved overall performance to 15.1% EER, suggesting that the grammatical features capture stylistic information not present in neural embeddings.

## Method Summary
GRAM2Vec transforms documents into vector representations using normalized frequencies of various grammatical features. The system extracts multiple linguistic features including letters, punctuation, emojis, part-of-speech tags, morphological information, dependency structures, syntactic constructions, and function words. These features are then normalized to create document vectors that can be used for downstream tasks like authorship attribution. The key innovation is that these vectors are inherently interpretable, as each dimension corresponds to a specific grammatical feature whose contribution can be directly examined and explained.

## Key Results
- GRAM2Vec achieved 27.6% equal error rate (EER) for authorship attribution when used alone
- Neural LUAR model achieved 15.5% EER, outperforming GRAM2Vec individually
- Combining GRAM2Vec with LUAR improved performance to 15.1% EER, demonstrating complementary information capture

## Why This Works (Mechanism)
GRAM2Vec works by leveraging the hypothesis that individual authors have distinctive grammatical patterns that can be captured through feature frequencies. By extracting normalized frequencies of grammatical elements like POS tags, syntactic constructions, and function words, the method creates document representations that reflect an author's stylistic fingerprint. The interpretability advantage comes from the direct mapping between vector dimensions and specific grammatical features, allowing users to examine which features contributed most to classification decisions. This contrasts with neural embeddings where the relationship between input features and output decisions is opaque.

## Foundational Learning
- Grammatical feature extraction - Needed to capture linguistic patterns that characterize writing style; Quick check: Verify feature extraction correctly identifies POS tags, dependencies, and syntactic constructions
- Document normalization techniques - Required to make feature frequencies comparable across documents of different lengths; Quick check: Ensure normalization produces consistent vector magnitudes
- Authorship attribution evaluation metrics - Essential for measuring model performance; Quick check: Confirm EER calculations follow standard methodology

## Architecture Onboarding
Component map: Text documents -> Feature extraction -> Normalization -> Vector representation -> Classification
Critical path: Feature extraction → Normalization → Vector representation
Design tradeoffs: Interpretable but potentially less accurate than neural methods vs. accurate but opaque neural embeddings
Failure signatures: Poor performance when stylistic features are insufficient to distinguish authors; failure to extract relevant grammatical features
First experiments: 1) Test feature extraction on sample documents, 2) Validate normalization across document lengths, 3) Evaluate classification performance on small dataset

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Performance gap between GRAM2Vec (27.6% EER) and neural LUAR model (15.5% EER) indicates lower accuracy when used alone
- Limited validation of actual interpretability benefits - while theoretically interpretable, user studies are needed to confirm practical utility
- Evaluation restricted to English authorship attribution, raising questions about generalizability to other languages and domains

## Confidence
High confidence in technical implementation of GRAM2Vec and its ability to produce interpretable document vectors. Medium confidence in authorship attribution performance results and claimed interpretability benefits, as these require more extensive validation with real-world users. Low confidence in generalizability across different domains and languages given the limited evaluation scope.

## Next Checks
1) Conduct user studies to empirically validate whether feature weights help users understand and trust authorship attribution decisions
2) Test GRAM2Vec's performance and interpretability on multiple languages and domains beyond authorship attribution
3) Compare GRAM2Vec against other interpretable embedding methods on the same authorship attribution task to establish relative advantages and limitations