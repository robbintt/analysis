---
ver: rpa2
title: 'Ask-EDA: A Design Assistant Empowered by LLM, Hybrid RAG and Abbreviation
  De-hallucination'
arxiv_id: '2406.06575'
source_url: https://arxiv.org/abs/2406.06575
tags: []
core_contribution: Ask-EDA is a 24x7 AI design assistant for electronic design engineers
  that combines a large language model with hybrid retrieval-augmented generation
  and abbreviation de-hallucination techniques. It addresses the challenge of efficiently
  finding relevant information for design tasks by retrieving and augmenting LLM responses
  with curated design documents and an abbreviation dictionary.
---

# Ask-EDA: A Design Assistant Empowered by LLM, Hybrid RAG and Abbreviation De-hallucination

## Quick Facts
- arXiv ID: 2406.06575
- Source URL: https://arxiv.org/abs/2406.06575
- Authors: Luyao Shi; Michael Kazda; Bradley Sears; Nick Shropshire; Ruchir Puri
- Reference count: 17
- Ask-EDA achieves over 40% improvement in recall on general design questions and over 70% improvement on abbreviation resolution compared to baseline methods

## Executive Summary
Ask-EDA is a 24/7 AI design assistant for electronic design engineers that combines a large language model with hybrid retrieval-augmented generation and abbreviation de-hallucination techniques. The system addresses the challenge of efficiently finding relevant information for design tasks by retrieving and augmenting LLM responses with curated design documents and an abbreviation dictionary. Using a hybrid search combining dense and sparse retrieval methods, Ask-EDA demonstrates significant improvements in recall metrics across three evaluation datasets. The system is deployed via Slack interface, allowing engineers to receive accurate, context-aware responses to their design queries.

## Method Summary
The system ingests curated design documents using LangChain loaders, chunks them into 2048-token segments, and creates dense and sparse embeddings for hybrid retrieval. A hybrid search engine combines sentence transformers (dense) with BM25 (sparse) using reciprocal rank fusion to retrieve relevant context. An abbreviation de-hallucination component extracts abbreviations from queries, retrieves their full names from a curated dictionary, and injects this knowledge into the LLM prompt. The system uses Granite-13b or Llama2-13b with a carefully crafted system prompt to generate responses. Evaluation on three datasets (q2a-100, cmds-100, abbr-100) demonstrates significant improvements in recall compared to baseline methods.

## Key Results
- Hybrid RAG achieved over 40% improvement in recall on the q2a-100 dataset and over 60% on the cmds-100 dataset compared to no RAG
- Abbreviation de-hallucination provided over 70% recall improvement on the abbr-100 dataset
- The system successfully handles both general design questions and technical command queries with improved accuracy
- Ask-EDA is deployed as a Slack interface for real-world use by design engineers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid retrieval combining dense (semantic) and sparse (exact match) methods improves recall for design-related queries.
- Mechanism: Dense retrieval captures semantic similarity while sparse retrieval ensures exact term matching; their combination via reciprocal rank fusion leverages complementary strengths.
- Core assumption: Design queries contain both semantic meaning and technical terms requiring exact matching.
- Evidence anchors:
  - [abstract]: "We developed a hybrid search engine that leverages the strengths of both dense and sparse retrieval algorithms"
  - [section II-B2]: Describes combining sentence transformers (dense) with BM25 (sparse) using reciprocal rank fusion
  - [corpus]: No direct evidence; only related work on hybrid retrieval systems

### Mechanism 2
- Claim: Abbreviation de-hallucination improves recall on abbreviation resolution by providing explicit abbreviation knowledge to the LLM.
- Mechanism: Extracts abbreviation terms from query and context, retrieves their full names and descriptions from a curated dictionary, and injects this knowledge into the prompt before the user query.
- Core assumption: LLMs will use explicitly provided abbreviation knowledge when generating responses.
- Evidence anchors:
  - [abstract]: "ADH yields over a 70% enhancement in Recall on the abbr-100 dataset"
  - [section II-C]: Details the abbreviation de-hallucination component and its integration into the prompt
  - [corpus]: No direct evidence; corpus contains related RAG systems but not abbreviation-specific ones

### Mechanism 3
- Claim: Adding relevant context via RAG improves LLM response quality for design questions.
- Mechanism: Retrieves relevant text chunks from curated design documents using hybrid search, ranks them via reciprocal rank fusion, and presents the top results as context in the LLM prompt.
- Core assumption: Providing relevant context enables the LLM to generate more accurate responses than relying solely on pre-training knowledge.
- Evidence anchors:
  - [abstract]: "hybrid RAG offers over a 40% improvement in Recall on the q2a-100 dataset and over a 60% improvement on the cmds-100 dataset compared to not using RAG"
  - [section II-B]: Details the RAG pipeline from ingestion to retrieval
  - [corpus]: No direct evidence; corpus contains related RAG systems but not design-specific ones

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: RAG addresses LLM hallucination by providing current, relevant context from external knowledge sources
  - Quick check question: How does RAG differ from fine-tuning an LLM on domain-specific data?

- Concept: Hybrid search combining dense and sparse retrieval
  - Why needed here: Design queries require both semantic understanding and exact term matching for technical concepts
  - Quick check question: When would dense retrieval alone be insufficient for design queries?

- Concept: Reciprocal rank fusion for combining retrieval results
  - Why needed here: RRF provides a principled way to combine rankings from different retrieval methods
  - Quick check question: What parameter in RRF controls the balance between high and low ranking results?

## Architecture Onboarding

- Component map:
  - Document ingestion pipeline (LangChain loaders, chunking, embeddings)
  - Hybrid database (ChromaDB for dense vectors, BM25 index for sparse)
  - Hybrid search engine (sentence transformer + BM25 + RRF)
  - Abbreviation de-hallucination module (dictionary lookup and prompt injection)
  - LLM generation (Granite-13b or Llama2-13b with system prompt)
  - Slack interface (API integration and feedback collection)

- Critical path: User query → hybrid search → abbreviation de-hallucination → prompt construction → LLM generation → response

- Design tradeoffs:
  - Chunk size (2048 tokens) balances context granularity with retrieval efficiency
  - Using reciprocal rank fusion vs. other combination methods
  - Fixed abbreviation dictionary vs. dynamic extraction methods

- Failure signatures:
  - Poor recall indicates issues with retrieval relevance or RRF parameters
  - Hallucinated abbreviations suggest dictionary incompleteness or prompt formatting issues
  - Low F1 scores may indicate LLM struggles with context extraction

- First 3 experiments:
  1. Compare hybrid RAG performance against dense-only and sparse-only baselines on q2a-100
  2. Test abbreviation de-hallucination on a subset of abbr-100 with manual verification
  3. Evaluate different RRF k values (60 used) on hybrid retrieval effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the abbreviation de-hallucination (ADH) component be further improved to achieve 100% recall on the abbr-100 dataset?
- Basis in paper: Explicit
- Why unresolved: The paper states that even with ADH, neither Granite-13b-chat-v2.1 nor Llama2-13b-chat achieved a 1.0 recall score on the abbr-100 dataset. The authors conclude that some intrinsic limitations in LLMs, exacerbated by complex RAG-based contexts, prevent them from successfully recalling all abbreviations.
- What evidence would resolve it: Experiments with different ADH techniques, such as using a larger or more comprehensive abbreviation dictionary, incorporating contextual information, or fine-tuning the LLM on abbreviation-related tasks.

### Open Question 2
- Question: How can the hybrid RAG technique be further improved to outperform both sparse-only and dense-only RAG on all datasets and evaluation metrics?
- Basis in paper: Explicit
- Why unresolved: The paper shows that hybrid RAG outperforms sparse-only and dense-only RAG on the q2a-100 and cmds-100 datasets in terms of recall. However, on the cmds-100 dataset, hybrid RAG does not outperform sparse-only RAG in terms of F1 score. The authors suggest exploring fine-tuning more sophisticated sparse and dense retrieval models to enhance RAG even further.
- What evidence would resolve it: Experiments with different hybrid RAG techniques, such as adjusting the weights of dense and sparse retrieval, using different retrieval models, or incorporating additional information sources.

### Open Question 3
- Question: How can reinforcement learning from human feedback (RLHF) be effectively applied to align the chat agent more closely with human preferences?
- Basis in paper: Explicit
- Why unresolved: The paper mentions the intention to leverage RLHF to align the chat agent more closely with human preferences, based on the feedback data collected from the Slack GUI. However, the paper does not provide details on how RLHF will be implemented or its potential impact on the chat agent's performance.
- What evidence would resolve it: Experiments with different RLHF techniques, such as reward modeling, preference learning, or human-in-the-loop fine-tuning, and their impact on the chat agent's performance and user satisfaction.

## Limitations
- The evaluation is conducted on curated datasets specifically constructed for this work rather than real-world usage data
- Performance comparisons against commercial solutions like Amazon CodeWhisperer are limited to stated capabilities rather than empirical head-to-head testing
- The paper reports improvements in recall metrics but does not provide qualitative analysis of response quality or user satisfaction

## Confidence

- **High confidence**: The hybrid RAG mechanism combining dense and sparse retrieval demonstrably improves recall metrics over baseline methods, supported by quantitative results across multiple datasets.
- **Medium confidence**: The abbreviation de-hallucination component's 70% recall improvement is well-documented, though the specific dictionary coverage and handling of edge cases is not fully characterized.
- **Low confidence**: Claims about real-world effectiveness and superiority over existing commercial solutions lack empirical validation beyond metric comparisons on curated datasets.

## Next Checks

1. **Real-world deployment test**: Deploy Ask-EDA with a small group of design engineers for one month, collecting both quantitative metrics and qualitative feedback on response usefulness and accuracy in actual design scenarios.

2. **Robustness evaluation**: Test the system's performance on adversarial queries containing rare abbreviations, ambiguous terminology, and complex multi-step design problems not represented in the curated datasets.

3. **Comparative benchmark**: Conduct head-to-head evaluation against at least two commercial design assistants (e.g., Amazon CodeWhisperer, GitHub Copilot) using identical queries and human evaluation of response quality, not just automated metrics.