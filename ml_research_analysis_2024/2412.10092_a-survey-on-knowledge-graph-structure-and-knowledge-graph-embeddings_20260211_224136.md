---
ver: rpa2
title: A Survey on Knowledge Graph Structure and Knowledge Graph Embeddings
arxiv_id: '2412.10092'
source_url: https://arxiv.org/abs/2412.10092
tags:
- they
- kgem
- performance
- kgems
- link
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first comprehensive survey examining how
  Knowledge Graph (KG) structure and Knowledge Graph Embedding Model (KGEM) hyperparameters
  influence link prediction performance. It systematically reviews 22 studies analyzing
  the relationship between KG structural features (node degree, relationship frequency,
  node-relationship and node-node co-frequency) and KGEM performance.
---

# A Survey on Knowledge Graph Structure and Knowledge Graph Embeddings

## Quick Facts
- arXiv ID: 2412.10092
- Source URL: https://arxiv.org/abs/2412.10092
- Authors: Jeffrey Sardina; John D. Kelleher; Declan O'Sullivan
- Reference count: 23
- Primary result: This paper presents the first comprehensive survey examining how Knowledge Graph (KG) structure and Knowledge Graph Embedding Model (KGEM) hyperparameters influence link prediction performance.

## Executive Summary
This survey systematically reviews 22 studies analyzing the relationship between KG structural features and KGEM performance. It identifies consistent findings across multiple works that KGEMs exhibit degree-biased learning, where higher-degree nodes are learned more effectively than lower-degree nodes. The survey also documents that optimal KGEM hyperparameters are both KG and KGEM-dependent, and that different negative sampling strategies perform better depending on graph connectivity. The authors highlight three major open research directions: studying how KG structure interacts with hyperparameter preference, examining the role of ontological properties, and establishing structurally-controlled benchmark KGs.

## Method Summary
The survey conducts a comprehensive literature review of 22 studies that examine the relationship between KG structure and KGEM performance. It systematically categorizes and analyzes findings across four structural metrics: node degree, relationship frequency, node-relationship co-frequency, and node-node co-frequency. The review examines how these structural features influence link prediction performance and interact with KGEM hyperparameters including learning rate, batch size, number of negative samples, and negative sampling strategy. The authors synthesize findings from multiple studies using different KGEM architectures (TransE, DistMult, ComplEx, etc.) and benchmark datasets (FB15k, FB15k-237, WN18, WN18RR, YAGO3-10).

## Key Results
- KGEMs exhibit consistent degree-biased learning, with higher-degree nodes being learned more effectively than lower-degree nodes
- Optimal KGEM hyperparameters are both KG and KGEM-dependent, with no universally optimal configuration
- Negative sampling strategy performance depends on graph connectivity, particularly node degree and node-relation co-frequency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Degree-biased learning is a consistent phenomenon across KGEMs, where higher-degree nodes are learned more effectively than lower-degree nodes.
- Mechanism: KGEMs learn from observed triples, so nodes appearing in more triples (higher degree) have more information available for embedding optimization, leading to more accurate representations.
- Core assumption: The number of observed triples containing a node is proportional to the quality of its learned embedding.
- Evidence anchors:
  - [abstract] "KGEMs exhibit degree-biased learning, with higher-degree nodes being learned more effectively than lower-degree nodes."
  - [section II] "Rossi et al. (2020) [...] indicate that, on both TransE and DistMult on all datasets tested, higher degree nodes are learned substantially better than lower-degree nodes."
  - [corpus] Weak: corpus neighbors focus on related KGEM topics but don't directly address degree bias mechanism.
- Break Condition: If a KGEM explicitly incorporates degree-balancing or uses uniform negative sampling strategies that mitigate this bias.

### Mechanism 2
- Claim: Hyperparameter performance is both KG and KGEM dependent, with no universally optimal configuration.
- Mechanism: Different KGEMs have different inductive biases and optimization landscapes, and these interact with KG-specific features like degree distribution and relationship frequency to determine optimal hyperparameters.
- Core assumption: The structure of the loss surface for a given KGEM-KG pair is unique and affects how hyperparameters like learning rate and batch size influence convergence.
- Evidence anchors:
  - [abstract] "optimal KGEM hyperparameters are both KG and KGEM-dependent"
  - [section III] "Ali et al. (2022) [...] The optimal hyperparameters they found for each KG-KGEM pair indicate that optimal hyperparameters are a function of the the KG being used and the KGEM being used."
  - [corpus] Weak: corpus neighbors do not provide evidence on hyperparameter dependence.
- Break Condition: If a new study demonstrates that a small set of hyperparameters consistently performs well across diverse KGEM-KG combinations.

### Mechanism 3
- Claim: Negative sampling strategy performance depends on graph connectivity, particularly node degree and node-relation co-frequency.
- Mechanism: Negative sampling strategies that rely on finding valid corruptions (e.g., pseudo-typed sampling) require sufficient co-occurrence of nodes and relations; sparse regions of the graph cannot support these strategies effectively.
- Core assumption: The effectiveness of a negative sampler is directly related to the availability of valid negative examples in the graph structure.
- Evidence anchors:
  - [section III] "Kotnis et al. (2017) [...] find that how effective various negative samplers are [...] is based on [...] Relationship Frequency [...] and Node-Relationship Co-Frequency."
  - [section III] "They specifically highlight that low node-relation co-frequency in FB15k results in poor performance of pseudo-typed sampling, as low node-relation co-frequency means that there are not enough options for negative generation."
  - [corpus] Weak: corpus neighbors do not address negative sampling strategies.
- Break Condition: If a negative sampling strategy is developed that is agnostic to graph connectivity and performs well across all graph structures.

## Foundational Learning

- Concept: Knowledge Graph Structure Metrics
  - Why needed here: Understanding the four core metrics (degree, relationship frequency, node-relationship co-frequency, node-node co-frequency) is essential for interpreting how KG structure influences KGEM performance.
  - Quick check question: Can you explain the difference between node degree and node-relationship co-frequency?

- Concept: Link Prediction Task
  - Why needed here: KGEMs are primarily used for link prediction, so understanding this task is fundamental to grasping why structure and hyperparameters matter.
  - Quick check question: In the link prediction task, what are you trying to predict given a subject, object, and relationship?

- Concept: Hyperparameter Optimization
  - Why needed here: The survey emphasizes that optimal hyperparameters are KG and KGEM dependent, making it crucial to understand how to search for and evaluate different hyperparameter configurations.
  - Quick check question: Why is it problematic to use the same hyperparameters for all KGEMs across different KGs?

## Architecture Onboarding

- Component map:
  Knowledge Graph (KG) -> KGEM -> Link Prediction Module -> Structure Analysis Module -> Hyperparameter Search

- Critical path:
  1. Load KG and compute structural features (degree, frequency metrics)
  2. Select KGEM and define hyperparameter search space
  3. Train KGEM with different hyperparameter configurations
  4. Evaluate link prediction performance for each configuration
  5. Analyze the relationship between structure, hyperparameters, and performance

- Design tradeoffs:
  - Comprehensive vs. focused analysis: The survey covers many structural features and hyperparameters but doesn't deeply analyze interactions between them
  - Benchmark diversity vs. control: Using standard benchmarks provides comparability but may not allow controlled structural analysis
  - Negative sampling strategy: More sophisticated strategies may be better but require sufficient graph connectivity

- Failure signatures:
  - Consistently poor performance on low-degree nodes suggests degree bias in the KGEM
  - Suboptimal performance despite extensive hyperparameter search may indicate KGEM limitations for the given structure
  - High variance in performance across different negative sampling strategies suggests sensitivity to graph connectivity

- First 3 experiments:
  1. Replicate Rossi et al. (2020) analysis: Train TransE and DistMult on FB15k, FB15k-237, WN18, and WN18RR, then analyze link prediction performance as a function of node degree
  2. Conduct a grid search over negative sampling strategies and number of negatives per positive for a single KGEM-KG pair, then analyze how strategy performance correlates with graph connectivity metrics
  3. Train the same KGEM with identical hyperparameters on two KGs with different degree distributions, then compare performance and analyze which structural features correlate with performance differences

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific KG structural features (e.g., node degree distribution, relationship frequency, node-relationship co-frequency) interact with KGEM hyperparameter preferences to determine link prediction performance?
- Basis in paper: [explicit] The paper identifies this as an open research direction, noting that while studies have shown relationships between structure and performance, and between hyperparameters and performance, few have examined how KG structure interacts with hyperparameter preference.
- Why unresolved: Most existing research has studied these factors independently rather than examining their interactions.
- What evidence would resolve it: Systematic experiments testing various KGEMs on structurally-controlled KGs with comprehensive hyperparameter searches, analyzing which structural features predict optimal hyperparameter choices.

### Open Question 2
- Question: How do ontological properties of KGs (such as frequencies of transitive/symmetric/asymmetric relationships, node/relationship typing, and type hierarchies) interact with KG structure, hyperparameter preference, and link prediction performance?
- Basis in paper: [explicit] The paper explicitly identifies this as an open direction, noting that it does not consider ontological properties which are core to KG data representation.
- Why unresolved: Most existing research focuses on structural features without considering the semantic/ontological aspects of KGs.
- What evidence would resolve it: Experiments varying both structural and ontological properties of KGs, testing how these combinations affect KGEM performance and hyperparameter optimization.

### Open Question 3
- Question: What are the optimal desiderata for structurally-controlled benchmark KGs, and how can we establish a standardized library for such KGs?
- Basis in paper: [explicit] The paper identifies the lack of diverse, structurally-controlled benchmark KGs as a major limitation and proposes this as an open research direction.
- Why unresolved: While some work exists on structure-controlled KG generation (e.g., PyGraft), there is no consensus on what constitutes an ideal benchmark KG.
- What evidence would resolve it: Development of standardized guidelines for creating structurally-controlled KGs, along with a community-accepted benchmark library (similar to how PyKEEN serves as a KGEM library).

## Limitations
- Reliance on secondary analysis of 22 studies rather than direct experimental validation
- Limited empirical validation of claims about negative sampling strategy effectiveness
- Focus on structural features without considering ontological properties of KGs

## Confidence
- Degree-biased learning findings: High confidence due to consistent replication across multiple studies
- Hyperparameter dependency claims: Medium confidence given study-specific optimization results
- Negative sampling effectiveness: Low confidence due to limited empirical validation across diverse graph structures

## Next Checks
1. Conduct controlled experiments varying only node degree while holding other factors constant to isolate degree bias effects
2. Perform systematic hyperparameter sweeps across multiple KGEM-KG pairs to quantify the degree of KGEM-KG interaction effects
3. Evaluate negative sampling strategy performance on synthetically generated KGs with precisely controlled connectivity patterns to establish causal relationships between structure and sampling effectiveness