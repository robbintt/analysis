---
ver: rpa2
title: 'Distributed Inference on Mobile Edge and Cloud: A Data-Cartography based Clustering
  Approach'
arxiv_id: '2412.16616'
source_url: https://arxiv.org/abs/2412.16616
tags:
- samples
- mobile
- cost
- cloud
- edge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes DIMEC-DC, a distributed inference framework
  for deploying large DNNs across mobile, edge, and cloud devices. The key challenge
  is determining the computational complexity of incoming samples to allocate them
  to appropriate devices.
---

# Distributed Inference on Mobile Edge and Cloud: A Data-Cartography based Clustering Approach

## Quick Facts
- arXiv ID: 2412.16616
- Source URL: https://arxiv.org/abs/2412.16616
- Reference count: 33
- Primary result: >43% cost reduction with <0.5% accuracy drop vs cloud-only inference

## Executive Summary
This paper introduces DIMEC-DC, a distributed inference framework that leverages data cartography to classify samples by computational complexity and route them to mobile, edge, or cloud devices accordingly. The key innovation is using confidence and variance metrics computed across training epochs to create pools of easy, moderate, and hard samples. By processing easy samples locally, moderate samples on edge, and only hard samples on cloud, DIMEC-DC achieves significant cost savings while maintaining accuracy comparable to cloud-only inference.

## Method Summary
DIMEC-DC uses data cartography to analyze training dynamics of each sample, computing confidence (mean model probability of true label across epochs) and variance (spread of that probability) to classify samples as easy, moderate, or hard. These classifications determine routing: easy samples processed on mobile, moderate on edge, and hard on cloud. The method creates pools using validation data, classifies incoming samples by distance to pool averages, and optionally updates pools dynamically during inference to handle distribution shifts. Experiments on GLUE datasets demonstrate over 43% cost reduction while maintaining less than 0.5% accuracy drop compared to cloud-only inference.

## Key Results
- DIMEC-DC achieves over 43% cost reduction compared to cloud-only inference
- Maintains less than 0.5% accuracy drop versus cloud-only baseline
- Outperforms Random, DeeBERT, AdaEE, and I-SplitEE baselines in both cost and accuracy metrics
- Dynamic pool updates improve robustness to domain shifts in incoming data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DIMEC-DC uses data cartography to classify incoming samples into easy, moderate, and hard based on confidence and variance computed across training epochs.
- Mechanism: The method computes two metrics for each sample: (1) confidence = mean model probability of the true label across epochs, and (2) variance = spread of that probability across epochs. Samples are then grouped into pools based on thresholds α and β.
- Core assumption: The confidence and variance of validation samples during training are indicative of their computational complexity during inference.
- Evidence anchors:
  - [abstract] "DIMEC-DC leverages Data Cartography to assess sample complexity by analyzing confidence and variance across training epochs, classifying samples as easy, moderate, or hard."
  - [section] "We propose a method to determine the complexity of each sample based on data cartography [15]."
- Break condition: If the distribution of test samples shifts significantly from the validation set used to create pools, the thresholds may misclassify samples, leading to incorrect routing and accuracy loss.

### Mechanism 2
- Claim: By routing samples to the appropriate device (mobile for easy, edge for moderate, cloud for hard), DIMEC-DC reduces overall inference cost while maintaining accuracy.
- Mechanism: Easy samples processed on mobile incur only local computation cost, moderate samples offloaded to edge incur a small communication cost plus computation, and only hard samples go to cloud, avoiding unnecessary expensive cloud processing.
- Core assumption: The cost model (processing costs λm, λe, offloading costs o1, o2, cloud fee γ) accurately reflects real-world resource usage.
- Evidence anchors:
  - [abstract] "DIMEC-DC achieves over 43% cost reduction while maintaining less than 0.5% accuracy drop compared to cloud-only inference."
  - [section] "Our method effectively balances the accuracy-efficiency trade-off, significantly enhancing efficiency while maintaining accuracy comparable to that of the final layer."
- Break condition: If the cost structure changes (e.g., mobile battery depletion or edge node congestion), the fixed thresholds may no longer yield optimal cost savings.

### Mechanism 3
- Claim: The adaptive version of DIMEC-DC dynamically updates pool averages during inference to handle domain shifts in incoming data.
- Mechanism: As each test sample arrives, its embedding is compared to current pool averages, classified into easy/medium/hard, and then used to update the corresponding pool average for future samples.
- Core assumption: Incremental updates of pool averages will track gradual shifts in data distribution without causing instability in classification.
- Evidence anchors:
  - [section] "Adaptive: In fixed inference, the pools are created using the validation dataset, however during test time there might be a shift in the dataset distribution. For such cases, we dynamically update the pool averages such that the distribution shift can be properly captured."
- Break condition: If the distribution shift is abrupt or large, the pool averages may lag, causing misclassification until they adapt.

## Foundational Learning

- Concept: Data cartography and training dynamics
  - Why needed here: Understanding how confidence and variance across epochs characterize sample difficulty is central to DIMEC-DC's classification logic.
  - Quick check question: If a sample has high confidence but high variance across epochs, which pool does it belong to and why?
- Concept: Distributed inference and split computing
  - Why needed here: The paper assumes knowledge of deploying DNN layers across mobile, edge, and cloud, and the cost implications of offloading.
  - Quick check question: In a split computing setup, what factors influence the choice of split layer between mobile and edge?
- Concept: BERT model architecture and embedding layers
  - Why needed here: The method relies on extracting embeddings from the BERT model for sample classification and routing.
  - Quick check question: Why is the embedding layer sufficient for classification in DIMEC-DC instead of full DNN inference?

## Architecture Onboarding

- Component map:
  - Training phase: BERT backbone + exit classifiers at layers m and n + pool creation using validation data.
  - Inference phase: Embedding extraction on mobile + distance calculation to pool averages + sample routing decision + dynamic pool update (adaptive mode).
  - Cost model: Processing costs (λm, λe), offloading costs (o1, o2), cloud fee (γ).
- Critical path:
  1. Load pre-trained BERT with exit classifiers.
  2. Train on 80% of data, validate on 10%, test on 10%.
  3. Create pools on validation data using confidence/variance thresholds.
  4. During inference, extract embedding → classify → route → update pools (if adaptive).
- Design tradeoffs:
  - Fixed vs. adaptive pool averages: Fixed is simpler but less robust to distribution shift; adaptive adds overhead but improves accuracy under drift.
  - Choice of m and n layers: Balances local processing capability vs. offloading cost; too small leads to high cloud usage, too large underutilizes edge.
  - Threshold selection (α, β): Must maximize reward function balancing confidence and cost; poorly chosen thresholds degrade both cost and accuracy.
- Failure signatures:
  - Accuracy drops > 0.5% indicate misclassification due to poor thresholds or distribution shift.
  - Cost increases suggest unnecessary offloading (e.g., easy samples sent to cloud).
  - Runtime slowdown may occur if pool updates are too frequent in adaptive mode.
- First 3 experiments:
  1. Run DIMEC-DC on a single GLUE task (e.g., SST-2) with fixed pool averages; measure accuracy and cost vs. cloud-only baseline.
  2. Introduce controlled domain shift in test data; compare fixed vs. adaptive inference performance.
  3. Vary cost parameters (λm, λe, o1, o2, γ) and observe changes in routing decisions and overall cost.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but raises implicit considerations about scalability, generalization to other architectures, and robustness to domain shifts.

## Limitations

- The exact numerical values of cost parameters (λm, λe, oe, oc, γ) are not specified, only their relative ordering, limiting precise reproduction of cost savings.
- The search space and optimization method for selecting thresholds α and β are not fully detailed, introducing potential variability in pool creation quality.
- Performance under abrupt or large domain shifts is not thoroughly evaluated, particularly the lag in dynamic pool updates.

## Confidence

- High Confidence: The mechanism of using confidence and variance across epochs for sample classification (Mechanism 1) is well-grounded in data cartography literature.
- Medium Confidence: The cost reduction claims (>43%) are supported by experiments but depend on unspecified cost values and real-world cost structures.
- Medium Confidence: The adaptive inference mechanism is theoretically sound but lacks detailed empirical validation under various distribution shift scenarios.

## Next Checks

1. **Replicate Cost Analysis**: Implement the full cost model with assumed values for λm, λe, oe, oc, γ; verify cost savings claimed in the paper under controlled parameter variations.
2. **Test Pool Robustness**: Create pools using the described α/β threshold selection; evaluate classification accuracy and cost under synthetic distribution shifts in the test data.
3. **Baseline Comparison**: Implement and compare DIMEC-DC against the stated baselines (Random, DeeBERT, AdaEE, I-SplitEE) on at least one GLUE task to confirm relative performance.