---
ver: rpa2
title: Self-Improvement Programming for Temporal Knowledge Graph Question Answering
arxiv_id: '2404.01720'
source_url: https://arxiv.org/abs/2404.01720
tags:
- prog-tqa
- time
- questions
- temporal
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of answering temporal questions
  over temporal knowledge graphs (TKGs) by proposing a novel semantic-parsing-based
  approach called Prog-TQA. Prog-TQA leverages the in-context learning ability of
  large language models (LLMs) to generate program drafts with designed temporal operators,
  aligns these drafts to TKGs, and executes them to obtain answers.
---

# Self-Improvement Programming for Temporal Knowledge Graph Question Answering

## Quick Facts
- arXiv ID: 2404.01720
- Source URL: https://arxiv.org/abs/2404.01720
- Reference count: 0
- Key outcome: Achieves up to 50.4% improvement on Hits@1 for multiple-constraint questions on MultiTQ and 3.5% on complex questions on CronQuestions

## Executive Summary
This paper tackles the challenge of answering temporal questions over temporal knowledge graphs (TKGs) by proposing Prog-TQA, a semantic-parsing-based approach that leverages large language models' in-context learning ability. The method generates program drafts with temporal operators, aligns them to TKGs through similarity-based linking, and executes them to obtain answers. To further enhance temporal question understanding, Prog-TQA incorporates a self-improvement strategy that iteratively refines the LLM using high-quality self-generated programs as weak supervision.

## Method Summary
Prog-TQA is a two-stage framework that first generates executable program drafts from natural language questions using LLM in-context learning, then executes these programs over temporal knowledge graphs to obtain answers. The approach introduces temporal operators in KoPL to handle time constraints like "before," "after," "first," and "last." A self-improvement strategy uses gold answers as weak supervision to filter correct programs, which are then used to iteratively fine-tune the LLM. The linking module employs similarity-based entity/relation alignment rather than simple fuzzy matching, retrieving top-k similar facts based on embedded question and fact representations.

## Key Results
- Achieves up to 50.4% improvement on Hits@1 for multiple-constraint questions on MultiTQ dataset
- Shows 3.5% improvement on complex questions on CronQuestions dataset
- Demonstrates effectiveness of self-improvement strategy in bootstrapping LLM comprehension of temporal questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM's in-context learning ability can generate executable program drafts from a few examples
- Mechanism: ICL allows the model to map question semantics to KoPL functions without fine-tuning on large labeled datasets
- Core assumption: The LLM's semantic understanding generalizes from a small set of annotated examples to unseen temporal questions
- Evidence anchors:
  - [abstract] "Prog-TQA leverages the ICL ability of the LLM to parse questions and generate primarily program drafts according to a few question-program examples."
  - [section] "In the draft generation module, Prog-TQA leverages the ICL ability of the LLM to parse questions and generate primarily program drafts according to a few question-program examples."
  - [corpus] Weak - corpus contains related TKGQA work but no direct ICL mechanism description

### Mechanism 2
- Claim: Self-improvement strategy bootstraps the LLM's ability to handle complex temporal questions
- Mechanism: Gold answers serve as weak supervision to filter correct programs, which are then used to iteratively fine-tune the LLM
- Core assumption: High-quality self-generated programs are available and their correctness can be reliably determined from gold answers
- Evidence anchors:
  - [abstract] "To enhance the ability to understand questions, Prog-TQA is further equipped with a self-improvement strategy to effectively bootstrap LLMs using high-quality self-generated drafts."
  - [section] "Algorithm 1 shows the pseudocode for the self-improvement process... Using gold answers as weak supervision to assess programs."
  - [corpus] Weak - mentions self-improvement in corpus but lacks detail on weak supervision use

### Mechanism 3
- Claim: Similarity-based linking module improves entity/relation alignment accuracy compared to fuzzy matching
- Mechanism: Embeds question and fact sentences, selects top-k similar facts, then links mentions to entities/relations in those facts
- Core assumption: Core semantics of the question appear together in a small set of facts, making similarity-based retrieval effective
- Evidence anchors:
  - [section] "The linking module utilizes a similarity-based method... the main entities and relations in the question semantics tend to appear in the same few fact quadruples."
  - [section] "Unlike existing methods that use a simple fuzzy matching approach, the linking module utilizes a similarity-based method..."
  - [corpus] Weak - no explicit linking mechanism details in corpus

## Foundational Learning

- Concept: Temporal operators in KoPL
  - Why needed here: KoPL lacks built-in temporal operators, so custom operators are needed to express time constraints like "before", "first", etc.
  - Quick check question: What KoPL function would you use to filter facts occurring after a specific time?

- Concept: In-context learning
  - Why needed here: Avoids costly fine-tuning by leveraging LLM's ability to learn from a few annotated examples
  - Quick check question: How many examples are used per question category in Prog-TQA?

- Concept: Weak supervision
  - Why needed here: Gold answers provide indirect supervision for program correctness when labeled programs are scarce
  - Quick check question: What criterion determines if a generated program is "correct" in the self-improvement process?

## Architecture Onboarding

- Component map: Draft Generation Module -> Linking Module -> Execution Module
- Critical path: 1. Input question → Draft Generation → Linking → Execution → Answer
               2. Self-improvement runs separately, using answers to filter and fine-tune
- Design tradeoffs:
  - Few-shot vs. full fine-tuning: Few-shot reduces annotation cost but may limit complex reasoning
  - Post-processing: Increases recall (Hits@10) but can reduce precision (Hits@1)
  - r-relation copies: Improves recall but introduces redundancy
- Failure signatures:
  - Low Hits@1 but high Hits@10: Post-processing generating many candidates
  - Poor performance on multi-constraint questions: LLM failing to capture compound time constraints
  - Unexecutable drafts: Linking module failing to resolve entities/relations
- First 3 experiments:
  1. Run draft generation on a simple single-constraint question and verify the generated KoPL draft structure
  2. Test the linking module on a question with annotated entities/relations and check if correct facts are retrieved
  3. Execute a manually verified correct draft on a small TKG and confirm the answer matches expectations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Prog-TQA scale with increasingly complex temporal questions involving more than two time constraints?
- Basis in paper: Explicit. The paper mentions that Prog-TQA achieves significant improvement on multiple-constraint questions, but does not provide results for questions with more than two constraints.
- Why unresolved: The experiments were conducted on datasets with a maximum of two time constraints per question. The paper does not explore the model's performance on questions with more complex temporal reasoning.
- What evidence would resolve it: Experiments on a dataset with questions involving three or more time constraints, comparing Prog-TQA's performance to other methods.

### Open Question 2
- Question: What is the impact of using different types of LLMs (e.g., encoder-decoder vs. decoder-only) on the performance of Prog-TQA?
- Basis in paper: Inferred. The paper uses vicuna-13B, a decoder-only LLM, but does not explore the impact of using other types of LLMs.
- Why unresolved: The choice of LLM could potentially impact the model's ability to understand and generate temporal programs. The paper does not investigate this aspect.
- What evidence would resolve it: Experiments comparing the performance of Prog-TQA using different types of LLMs, such as encoder-decoder models like T5 or BART.

### Open Question 3
- Question: How does the self-improvement strategy in Prog-TQA handle spurious programs, and what is the impact of these spurious programs on the model's performance?
- Basis in paper: Explicit. The paper mentions the existence of spurious programs and their potential impact on the model's performance, but does not provide a detailed analysis of how the self-improvement strategy handles them.
- Why unresolved: The paper does not provide a quantitative analysis of the number of spurious programs generated during the self-improvement process and their impact on the model's performance.
- What evidence would resolve it: A detailed analysis of the number of spurious programs generated during the self-improvement process and their impact on the model's performance, along with strategies to mitigate their impact.

## Limitations
- The similarity-based linking module may struggle with sparse or noisy temporal facts where core semantics are distributed across many unrelated facts
- The self-improvement strategy's effectiveness depends on reliable filtering of correct programs, which is not fully specified
- Few-shot ICL may have limited ability to capture complex compositional time constraints beyond the examples provided

## Confidence
- High confidence in the overall framework design combining ICL, linking, and execution modules for TKGQA
- Medium confidence in the self-improvement strategy's contribution to performance gains
- Medium confidence in the effectiveness of similarity-based linking over fuzzy matching
- Low confidence in the exact performance numbers without access to full implementation details

## Next Checks
1. Test draft generation on diverse temporal questions (single-constraint, multi-constraint, complex) to verify the LLM's ability to generate executable KoPL programs with correct temporal operators and parameters
2. Evaluate the similarity-based linking on a held-out set of questions with annotated entities/relations to measure precision and recall of fact retrieval compared to fuzzy matching baselines
3. Run ablation studies to quantify the contribution of self-improvement by comparing performance with and without iterative fine-tuning, and analyze the quality distribution of self-generated programs used for training