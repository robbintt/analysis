---
ver: rpa2
title: 'Castor: Competing shapelets for fast and accurate time series classification'
arxiv_id: '2403.13176'
source_url: https://arxiv.org/abs/2403.13176
tags:
- time
- series
- castor
- distance
- shapelets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Castor, a novel time series classification
  method that uses competing shapelets to transform time series data into a discriminative
  feature representation. The key innovation is organizing shapelets into groups with
  varying dilation and allowing them to compete over the temporal context to construct
  a diverse feature representation.
---

# Castor: Competing shapelets for fast and accurate time series classification

## Quick Facts
- arXiv ID: 2403.13176
- Source URL: https://arxiv.org/abs/2403.13176
- Authors: Isak Samsten; Zed Lee
- Reference count: 13
- Primary result: Castor outperforms Rocket and MultiRocket, two leading random convolution-based classifiers, as well as other shapelet-based methods while maintaining computational efficiency

## Executive Summary
Castor introduces a novel time series classification method that uses competing shapelets organized into groups with varying dilation to create discriminative feature representations. The key innovation is allowing shapelets to compete over temporal context, enabling transitions between distance-based and dictionary-based transformation behaviors. This approach yields classifiers that are significantly more accurate than state-of-the-art methods while maintaining computational efficiency.

## Method Summary
Castor transforms time series data using shapelets sampled from training data, organized into groups with shared dilation factors. Each group contains multiple shapelets that compete to represent time series segments, with the most informative shapelet selected per time step. The method computes three distance-based features (minimum distance, maximum distance, and occurrence frequency) for each shapelet, creating a rich feature representation. Exponential dilation across groups captures multi-scale temporal patterns, and the entire transformation can be efficiently computed using padded dilated distance profiles.

## Key Results
- Castor achieves superior accuracy compared to Rocket and MultiRocket on 112 benchmark datasets
- The method maintains computational efficiency through the use of padded dilated distance profiles
- An ablation study identifies optimal default hyperparameters (g=128 groups, k=16 shapelets per group, l=9 shapelet length)
- Competing shapelets within groups create more discriminative feature representations than non-competing approaches

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Castor's use of competing shapelets within groups leads to more discriminative feature representations than non-competing approaches.
- **Mechanism**: By organizing shapelets into groups with shared dilation, each time step is represented by the most informative shapelet within the group. This competition ensures that each feature captures the strongest local pattern, filtering out redundant or less discriminative shapelets.
- **Core assumption**: Competition among shapelets within a group leads to better representation than using all shapelets independently.
- **Evidence anchors**:
  - [abstract]: "organizes shapelets into groups with varying dilation and allows the shapelets to compete over the time context"
  - [section]: "we enable the transformation to transition between levels of competition, resulting in methods that more closely resemble distance-based transformations or dictionary-based transformations"
  - [corpus]: Weak correlation; neighboring papers focus on shapelet-based explainability rather than competitive representation
- **Break Condition**: If the competition mechanism is disabled (using independent shapelets instead), predictive performance degrades significantly as shown in ablation study.

### Mechanism 2
- **Claim**: The combination of dilated shapelets with multiple distance-based features (min, max, and occurrence) captures both presence and absence of patterns more effectively than single-metric approaches.
- **Mechanism**: For each shapelet, three features are computed: minimum distance (presence), maximum distance (absence), and frequency of occurrence. This multi-metric approach encodes both when patterns match and when they don't, providing richer discriminatory information.
- **Core assumption**: Different distance metrics capture complementary aspects of time series characteristics.
- **Evidence anchors**:
  - [abstract]: "constructs a diverse feature representation"
  - [section]: "features based on the distance profile values for each shapelet within the group"
  - [corpus]: Missing; no direct evidence in neighboring papers about multi-metric distance features
- **Break Condition**: Removing any of the three distance features (min, max, or occurrence) reduces accuracy, as shown in ablation study.

### Mechanism 3
- **Claim**: Exponential dilation combined with group-based organization creates scale-invariant features that capture temporal patterns at multiple resolutions.
- **Mechanism**: By using exponentially increasing dilation factors (d = 2^e) across groups, the method captures patterns at different temporal scales. The group organization ensures that each scale has dedicated shapelets, preventing interference between different resolutions.
- **Core assumption**: Temporal patterns in time series exist at multiple scales and benefit from multi-resolution analysis.
- **Evidence anchors**:
  - [abstract]: "organizes shapelets into groups with varying dilation"
  - [section]: "DST represents the current state-of-the-art predictive performance for shapelet-based classifiers" and "integrates the concept of dilation"
  - [corpus]: Missing; neighboring papers don't discuss dilation strategies
- **Break Condition**: If dilation is disabled (using only non-dilated shapelets), the method loses its ability to capture multi-scale patterns and performance drops.

## Foundational Learning

- **Concept**: Distance profiles and their computation
  - Why needed here: The entire Castor transformation relies on computing distance profiles between shapelets and time series segments. Understanding this is fundamental to grasping how features are extracted.
  - Quick check question: What is the computational complexity of computing a distance profile between a shapelet of length l and a time series of length m?

- **Concept**: Z-normalization of time series
  - Why needed here: Shapelets are z-normalized with probability ρnorm to ensure scale invariance. This preprocessing step is critical for the method's robustness to amplitude variations.
  - Quick check question: Why is z-normalization applied probabilistically rather than to all shapelets in Castor?

- **Concept**: Convolution and its relationship to distance-based transformations
  - Why needed here: While Castor uses distance measures instead of dot products, understanding convolution helps in comparing it to methods like Rocket and Hydra, which use similar sliding-window approaches but with different operators.
  - Quick check question: How does the distance-based convolution in Castor differ fundamentally from the dot-product convolution used in Rocket?

## Architecture Onboarding

- **Component map**: Shapelet sampling module -> Distance profile computation engine -> Competition mechanism -> Feature extraction layer -> Ridge classification
- **Critical path**: Shapelet sampling → Distance profile computation → Competition mechanism → Feature extraction → Ridge classification
- **Design tradeoffs**:
  - Competition vs. independence: More competition (fewer groups, more shapelets per group) vs. more independence (more groups, fewer shapelets per group)
  - Hard vs. soft counting: Hard counting is faster but potentially less informative than soft counting
  - Z-normalization probability: Affects scale invariance vs. computational efficiency
- **Failure signatures**:
  - Low accuracy with high variance across folds suggests poor shapelet sampling or inappropriate hyperparameter settings
  - Extremely slow training indicates inefficient distance profile computation or excessive number of shapelets
  - Memory issues occur when g×k×E becomes too large for available RAM
- **First 3 experiments**:
  1. Run Castor with g=128, k=16, l=9 on a small dataset to verify basic functionality and compare against DST baseline
  2. Test different competition levels (vary g and k while keeping g×k constant) to find optimal balance for a specific dataset
  3. Compare hard vs. soft counting strategies on the development set to determine which yields better accuracy for the target domain

## Open Questions the Paper Calls Out
None

## Limitations
- Performance is highly sensitive to hyperparameter choices (g, k, dilation range)
- Random shapelet sampling may miss domain-specific patterns
- Method may not generalize well to very short or extremely long time series
- Memory footprint can be significant for large datasets

## Confidence
- **High Confidence**: The superiority of Castor over Rocket and MultiRocket is well-supported by empirical results across 112 datasets, with statistical significance demonstrated through rigorous testing.
- **Medium Confidence**: The claim about Castor's ability to transition between distance-based and dictionary-based transformations is theoretically sound but would benefit from additional visualization of learned features across different competition levels.
- **Low Confidence**: The assertion that competing shapelets provide more discriminative features than non-competing approaches needs further validation, as the ablation study focuses on hyperparameter effects rather than comparing competitive vs. non-competitive architectures directly.

## Next Checks
1. **Scale sensitivity test**: Evaluate Castor's performance on time series with significantly different lengths (both much shorter and much longer than the tested datasets) to verify multi-scale pattern capture claims.
2. **Competition mechanism ablation**: Implement a variant where shapelets within groups do not compete (using all shapelets independently) and compare performance against the competitive version across diverse datasets.
3. **Memory efficiency analysis**: Systematically measure and optimize the memory footprint of Castor for very large datasets, particularly examining the impact of different group configurations on RAM usage during training.