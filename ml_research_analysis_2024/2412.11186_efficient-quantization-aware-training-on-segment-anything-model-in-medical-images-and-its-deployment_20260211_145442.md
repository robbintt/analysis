---
ver: rpa2
title: Efficient Quantization-Aware Training on Segment Anything Model in Medical
  Images and Its Deployment
arxiv_id: '2412.11186'
source_url: https://arxiv.org/abs/2412.11186
tags:
- quantized
- training
- inference
- image
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a quantization-aware training pipeline for
  the Segment Anything Model (SAM) tailored to medical images. The authors address
  the challenge of high computational costs during inference by quantizing LiteMedSAM
  using quantization-aware training (QAT) with the Brevitas framework.
---

# Efficient Quantization-Aware Training on Segment Anything Model in Medical Images and Its Deployment

## Quick Facts
- arXiv ID: 2412.11186
- Source URL: https://arxiv.org/abs/2412.11186
- Authors: Haisheng Lu; Yujie Fu; Fan Zhang; Le Zhang
- Reference count: 17
- Primary result: Quantized LiteMedSAM achieves 3x faster inference while maintaining competitive DSC scores

## Executive Summary
This paper addresses the computational challenges of medical image segmentation by proposing a quantization-aware training pipeline for LiteMedSAM. The authors develop a comprehensive approach that optimizes data preprocessing, implements quantization-aware training using Brevitas, and deploys the model using OpenVINO for efficient inference. The method specifically targets modality imbalance issues common in medical imaging datasets. Through extensive experiments on CT and MRI images, the quantized model demonstrates significant inference speed improvements (reducing CT inference time from 38.78s to 11.78s) while maintaining competitive Dice Similarity Coefficient scores compared to the baseline model.

## Method Summary
The proposed quantization-aware training pipeline follows a systematic approach to optimize LiteMedSAM for medical image segmentation. The method begins with optimized data preprocessing to handle diverse medical imaging modalities and address potential modality imbalance. Quantization-aware training is then implemented using the Brevitas framework, which enables the model to learn quantized weights during training rather than applying post-training quantization. The training process involves using a carefully selected subset of data to balance different modalities and ensure robust performance across imaging types. Finally, the quantized model is deployed using OpenVINO, enabling efficient inference on resource-constrained platforms while maintaining accuracy comparable to the full-precision baseline.

## Key Results
- Inference time reduced from 38.78s to 11.78s for CT images
- Maintains competitive Dice Similarity Coefficient (DSC) scores compared to baseline
- Successfully addresses modality imbalance through subset-based training approach
- Achieves efficient deployment using OpenVINO with minimal accuracy degradation

## Why This Works (Mechanism)
Quantization-aware training allows the model to learn quantized weights during training, which reduces the precision of weights and activations while preserving model accuracy. By using Brevitas framework during training, the model becomes robust to quantization errors that would typically degrade performance in post-training quantization. The subset-based training approach effectively balances different medical imaging modalities, preventing any single modality from dominating the learning process. OpenVINO deployment further optimizes the quantized model for efficient inference on target hardware platforms.

## Foundational Learning
- **Quantization-aware training**: Simulates quantization during training to make model robust to quantization errors - needed to prevent accuracy drop during deployment
- **Brevitas framework**: PyTorch extension for building quantized models - needed for seamless integration with existing PyTorch workflows
- **OpenVINO toolkit**: Intel's optimization and deployment toolkit - needed for efficient inference on various hardware platforms
- **Dice Similarity Coefficient**: Metric for evaluating segmentation accuracy - needed for medical image segmentation evaluation
- **Modality imbalance**: Uneven distribution of different imaging types in training data - needed to address for robust multi-modal performance
- **LiteMedSAM**: Lightweight version of Segment Anything Model for medical images - needed to reduce computational complexity for medical applications

## Architecture Onboarding
- **Component map**: Data Preprocessing -> Quantization-Aware Training (Brevitas) -> OpenVINO Deployment
- **Critical path**: Optimized preprocessing ensures balanced modality representation → QAT with Brevitas learns quantized weights → OpenVINO deployment enables efficient inference
- **Design tradeoffs**: Lower bit-width quantization improves speed but may reduce accuracy; subset-based training balances modalities but may limit generalization
- **Failure signatures**: Performance degradation with underrepresented modalities; accuracy loss with aggressive quantization; deployment failures with incompatible hardware
- **First experiments**: 1) Test quantization with different bit-widths to find optimal balance; 2) Validate modality balancing with various subset sizes; 3) Benchmark OpenVINO deployment across different hardware platforms

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation scope to CT and MRI images with minimal examples per modality
- Methodological details on data balancing and subset selection lack specificity
- Quantization parameters (bit-width, layer-wise strategy) not fully specified
- Generalizability across diverse medical imaging modalities remains unverified

## Confidence
- Core methodology (quantization-aware training): High
- Implementation details and performance claims: Medium
- Generalizability across medical imaging scenarios: Low

## Next Checks
1. Replicate quantization pipeline across diverse medical imaging modalities (ultrasound, X-ray, PET) to verify cross-modality performance
2. Conduct ablation studies varying quantization bit-width and layer-specific strategies to determine optimal configuration
3. Compare inference speed and accuracy against alternative optimization methods (pruning, knowledge distillation)