---
ver: rpa2
title: Real-time and Continuous Turn-taking Prediction Using Voice Activity Projection
arxiv_id: '2401.04868'
source_url: https://arxiv.org/abs/2401.04868
tags:
- turn-taking
- prediction
- voice
- input
- real-time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a real-time continuous turn-taking prediction
  system using voice activity projection (VAP). The VAP model directly maps dialogue
  stereo audio to future voice activities using contrastive predictive coding and
  transformer networks.
---

# Real-time and Continuous Turn-taking Prediction Using Voice Activity Projection

## Quick Facts
- arXiv ID: 2401.04868
- Source URL: https://arxiv.org/abs/2401.04868
- Reference count: 19
- Primary result: VAP model achieves 76.16% balanced accuracy for turn-taking prediction with 1-second input context while operating in real-time on CPU (real-time factor 0.73)

## Executive Summary
This paper presents a real-time continuous turn-taking prediction system using Voice Activity Projection (VAP). The VAP model directly maps dialogue stereo audio to future voice activities using contrastive predictive coding and transformer networks. Experiments demonstrate that with 1-second input context length, the model achieves 76.16% balanced accuracy for turn-taking prediction while maintaining real-time performance on CPU. The system can predict whether speakers will speak within the next 600ms or 2000ms without requiring utterance-level segmentation.

## Method Summary
The VAP model combines a Contrastive Predictive Coding (CPC) encoder with self-attention and cross-attention transformers to process raw stereo audio and predict future voice activity. The architecture uses a GRU-based CPC encoder to extract representations from audio sequences, followed by self-attention transformers for each speaker channel and cross-attention transformers to model inter-speaker interactions. The model is trained with multitask learning including both VAP and voice activity detection (VAD) objectives. Audio is processed at 50 frames per second with context lengths ranging from 0.1s to 20s.

## Key Results
- Achieves 76.16% balanced accuracy for turn-taking prediction with 1-second input context
- Maintains real-time performance with CPU real-time factor of 0.73
- Performance remains stable for input lengths above 0.3 seconds
- Model operates without utterance-level segmentation or pre-trained features

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VAP model directly maps dialogue stereo audio to future voice activities in real-time without utterance-level segmentation
- Mechanism: Uses CPC and transformer networks to process raw stereo audio and predict future voice activity bins
- Core assumption: Raw audio contains sufficient information for turn-taking dynamics
- Evidence: Abstract states direct mapping using CPC and transformer networks; architecture described in section 2

### Mechanism 2
- Claim: Transformer input sequence length can be reduced to 1 second without significant accuracy loss
- Mechanism: GRU inside CPC encoder captures long-term dependencies, allowing transformer to focus on short-term context
- Core assumption: CPC encoder's GRU has learned sufficient audio sequence representation
- Evidence: Section 3 shows balanced accuracy at 76.16% for 1.0s vs 74.20% for 20.0s; performance degrades below 0.3s

### Mechanism 3
- Claim: VAP output can be simplified using probability sums over time bins
- Mechanism: Sums probabilities for 0-200ms and 200-600ms bins, then applies softmax for turn prediction
- Core assumption: Summed probabilities capture sufficient turn-taking information
- Evidence: Section 2 describes probability summation approach; section 3 mentions calculating p_now(s) over time

## Foundational Learning

- Concept: Contrastive Predictive Coding (CPC)
  - Why needed here: Provides unsupervised representation learning that captures temporal dependencies in raw audio
  - Quick check question: What is the main advantage of using CPC over supervised feature extraction for turn-taking prediction?

- Concept: Transformer self-attention and cross-attention mechanisms
  - Why needed here: Captures intra-speaker patterns (self-attention) and inter-speaker interactions (cross-attention)
  - Quick check question: How does the cross-attention transformer differ from the self-attention transformers in the VAP architecture?

- Concept: Voice activity detection (VAD) integration
  - Why needed here: VAD provides binary voiced/unvoiced classification that forms basis for VAP's future activity prediction
  - Quick check question: Why is VAD included as a multitask learning objective alongside VAP in the model architecture?

## Architecture Onboarding

- Component map: Audio input → CPC encoder → Self-attention transformers (2 channels) → Cross-attention transformer → Linear layers (VAD + VAP tasks)

- Critical path: Stereo audio signals → CPC encoder → Self-attention transformers → Cross-attention transformer → Linear layers → Turn-taking prediction

- Design tradeoffs:
  - Input sequence length vs. real-time performance: Shorter sequences enable real-time processing but may lose context
  - Model complexity vs. inference speed: More transformer layers improve accuracy but increase latency
  - Multitask learning vs. single-task focus: VAD helps VAP but adds computational overhead

- Failure signatures:
  - High CPU usage with poor real-time performance indicates need for sequence length reduction
  - Low balanced accuracy suggests insufficient model capacity or poor CPC representations
  - Inaccurate predictions during pauses may indicate need for longer context

- First 3 experiments:
  1. Vary input sequence length (0.1s to 20s) and measure balanced accuracy and real-time factor
  2. Test model with different transformer layer configurations (varying depth and attention heads)
  3. Evaluate performance on different languages (English, Mandarin, Japanese) to validate multilingual capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the VAP model's performance degrade when input sequence length falls below 0.3 seconds, and what is the theoretical minimum input length for maintaining acceptable accuracy?
- Basis in paper: Section 3 mentions performance degradation below 0.3 seconds without quantifying extent or minimum threshold
- Why unresolved: Paper only mentions degradation occurs below 0.3 seconds without specific accuracy drop metrics
- What evidence would resolve it: Controlled experiments varying input lengths from 0.05 to 0.3 seconds with detailed accuracy metrics

### Open Question 2
- Question: How does the VAP model perform in multilingual settings with languages not included in current training datasets?
- Basis in paper: Paper mentions models for English and Mandarin Chinese yield similar results but doesn't discuss other languages
- Why unresolved: Only demonstrates performance on three specific languages without testing other language families
- What evidence would resolve it: Evaluation on diverse languages including Romance, Slavic, and low-resource language families

### Open Question 3
- Question: How does the VAP model's real-time performance scale when deployed on resource-constrained edge devices or mobile platforms?
- Basis in paper: Demonstrates real-time performance on Intel Xeon Gold 6128 CPU but doesn't explore less powerful hardware
- Why unresolved: Only reports performance on high-end server CPU without investigating edge devices or mobile processors
- What evidence would resolve it: Benchmarking across different hardware platforms including mobile processors and edge devices

## Limitations

- Limited validation across diverse dialogue scenarios and languages beyond the three tested
- No testing of model robustness under adverse conditions (noise, compression, different audio qualities)
- Performance boundaries in non-dyadic conversations or with more than two participants unexplored

## Confidence

**High Confidence**: The core claim that VAP can achieve real-time turn-taking prediction with balanced accuracy around 76% for 1-second input context is well-supported by experimental results and CPU inference timing data.

**Medium Confidence**: The claim that CPC encoder's GRU captures sufficient long-term dependencies to allow transformer to focus on short-term context is plausible but relies on indirect evidence.

**Low Confidence**: The assertion that probability summation over specific time bins captures sufficient information for turn-taking prediction lacks direct experimental validation.

## Next Checks

1. Conduct experiments varying input context from 0.1s to 5s on the same corpus, measuring both balanced accuracy and real-time factor, focusing on identifying the exact breaking point where accuracy degradation begins.

2. Train and evaluate the VAP model separately on each language corpus (Japanese, English, Mandarin) with identical hyperparameters to compare balanced accuracy and inference speed across languages.

3. Evaluate model performance with degraded audio quality (additive noise, compression artifacts), different sampling rates, and non-dyadic conversations to establish practical deployment boundaries.