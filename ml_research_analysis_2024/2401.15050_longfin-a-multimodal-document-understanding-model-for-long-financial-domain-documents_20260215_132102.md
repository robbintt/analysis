---
ver: rpa2
title: 'LongFin: A Multimodal Document Understanding Model for Long Financial Domain
  Documents'
arxiv_id: '2401.15050'
source_url: https://arxiv.org/abs/2401.15050
tags: []
core_contribution: This paper introduces LongFin, a multimodal document understanding
  model designed to handle long financial documents, and LongForms, a new dataset
  that replicates industrial challenges in the financial domain. The authors address
  the limitations of existing datasets and models, which primarily focus on short,
  single-page documents, by creating a dataset that consists of multi-page financial
  forms and a model that can process up to 4K tokens.
---

# LongFin: A Multimodal Document Understanding Model for Long Financial Domain Documents

## Quick Facts
- arXiv ID: 2401.15050
- Source URL: https://arxiv.org/abs/2401.15050
- Authors: Ahmed Masry; Amir Hajian
- Reference count: 10
- One-line primary result: LongFin achieves F1 score of 0.7436 on LongForms dataset, outperforming LayoutLMv3 (0.6997) and LiLT (0.6567)

## Executive Summary
This paper introduces LongFin, a multimodal document understanding model designed to handle long financial documents, and LongForms, a new dataset that replicates industrial challenges in the financial domain. The authors address the limitations of existing datasets and models, which primarily focus on short, single-page documents, by creating a dataset that consists of multi-page financial forms and a model that can process up to 4K tokens. The LongFin model is built upon LiLT and Longformer, incorporating sliding window local attention and interval-based global attention mechanisms to effectively handle long contexts. The model is evaluated on both existing single-page benchmarks (FUNSD and CORD) and the newly proposed LongForms dataset, demonstrating superior performance on the latter while maintaining comparable results on the former.

## Method Summary
LongFin combines Longformer's text encoder with LiLT's layout encoder, using sliding window local attention and interval-based global attention mechanisms to handle long financial documents up to 4096 tokens. The model uses normalized bounding box coordinates for layout embeddings and a BiACM layer for bidirectional text-layout communication. It was pretrained on IIT-CDIP with OCR-IDL annotations and fine-tuned on the LongForms dataset using a Masked Visual-Language Modeling task with learning rate 2e-5, batch size 4 for 6000 steps on A100 GPU.

## Key Results
- LongFin achieves F1 score of 0.7436 on LongForms dataset, outperforming LayoutLMv3 (0.6997) and LiLT (0.6567)
- On FUNSD dataset, LongFin achieves F1 score of 0.6961, comparable to existing models
- The model demonstrates superior performance on long financial documents while maintaining effectiveness on single-page documents

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LongFin extends token capacity to 4K tokens by combining LiLT's layout-aware design with Longformer's sliding window and interval-based global attention.
- Mechanism: The text encoder uses local sliding window attention (512 tokens) and global attention at fixed intervals (every 100 tokens), reducing computational complexity from O(n²) to O(n) while maintaining long-range dependencies.
- Core assumption: The chosen interval size (100) provides sufficient global context coverage for financial document understanding.
- Evidence anchors:
  - [abstract] "incorporating sliding window local attention and interval-based global attention mechanisms"
  - [section] "The attention mechanism in LongFin incorporates two types of attention: local attention and global attention. The local attention employs a sliding window approach, where each token attends to the 512 local tokens surrounding it. On the other hand, the global attention involves a set of global tokens, selected at intervals of 100."
  - [corpus] Weak evidence - corpus shows related work on long-context models but no direct comparison of interval sizes.

### Mechanism 2
- Claim: Layout encoder uses normalized bounding box coordinates to create spatial embeddings that capture document structure.
- Mechanism: Each word's bounding box (x0, y0, x1, y1) is normalized to [0,1000], then each coordinate gets an embedding, concatenated, and projected via linear layer to create layout embeddings.
- Core assumption: Bounding box normalization and coordinate embedding sufficiently capture spatial relationships across multi-page documents.
- Evidence anchors:
  - [section] "To generate the layout embedding for each word, each coordinate in the normalized bounding box is used to obtain an embedding vector. The different coordinates' embedding vectors are then concatenated and projected using a linear layer."
  - [abstract] "incorporating sliding window local attention and interval-based global attention mechanisms"
  - [corpus] Weak evidence - corpus mentions multimodal approaches but lacks specific layout encoding details.

### Mechanism 3
- Claim: BiACM layer enables bidirectional communication between text and layout encoders without detaching text encoder gradients.
- Mechanism: BiACM adds attention scores from both encoders, allowing layout encoder to receive text context while maintaining end-to-end training.
- Core assumption: Removing detach operation (unlike LiLT) works for English-only focus and improves training efficiency.
- Evidence anchors:
  - [section] "In LiLT, a detach operation is applied to the scores generated by the text encoder before passing them to the layout encoder. This detachment prevents the layout encoder from backpropagating into the text encoder during pretraining... we have chosen to remove the detach operation to expedite pretraining, given our limited computational resources."
  - [abstract] "incorporating sliding window local attention and interval-based global attention mechanisms"
  - [corpus] Weak evidence - corpus shows document understanding models but no specific BiACM mechanism comparison.

## Foundational Learning

- Concept: Tokenization and positional embeddings
  - Why needed here: LongFin processes up to 4096 tokens, requiring proper tokenization and positional information for both text and layout
  - Quick check question: How does Longformer's position embedding initialization work when extending from 512 to 4096 tokens?

- Concept: Attention mechanisms (local vs global)
  - Why needed here: The model needs to balance local context processing with long-range dependencies in multi-page documents
  - Quick check question: What is the computational complexity difference between sliding window attention and full attention?

- Concept: Multimodal integration (text + layout)
  - Why needed here: Financial documents contain both textual content and spatial layout information that must be processed together
  - Quick check question: How do the text and layout embeddings combine in the BiACM layer?

## Architecture Onboarding

- Component map: Text encoder (Longformer-based) → Layout encoder (LiLT-based) → BiACM layer → Output head
- Critical path: Input text and bounding boxes → Token and layout embeddings → Text and layout encoders → BiACM fusion → NER predictions
- Design tradeoffs: Linear scaling vs. quadratic complexity, interval-based vs. random global tokens, detach vs. no-detach in BiACM
- Failure signatures: Poor performance on long tables (indicating insufficient context), degradation on short documents (interval size too large), layout information loss (bounding box normalization issues)
- First 3 experiments:
  1. Test on single-page FUNSD dataset to verify short-context performance matches baselines
  2. Evaluate on LongForms dataset with varying interval sizes (50, 100, 200) to find optimal global token spacing
  3. Compare with and without detach operation in BiACM to measure impact on convergence and final performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LongFin vary when processing documents with different layouts (e.g., forms with tables vs. forms with mostly text)?
- Basis in paper: [explicit] The authors mention that LongFin incorporates layout embeddings and attention mechanisms to handle diverse layouts in financial documents.
- Why unresolved: The paper does not provide specific performance comparisons for different layout types within the LongForms dataset.
- What evidence would resolve it: Detailed performance metrics of LongFin on different layout types (e.g., tables, text-heavy, mixed layouts) within the LongForms dataset.

### Open Question 2
- Question: What is the impact of increasing the maximum token length beyond 4K on the performance of LongFin, especially for documents with tens of pages?
- Basis in paper: [inferred] The authors acknowledge that LongFin's maximum input length of 4096 tokens might not accommodate certain financial documents that contain tens of pages.
- Why unresolved: The paper does not explore the performance of LongFin with extended token lengths or alternative architectures like relative position embeddings.
- What evidence would resolve it: Experiments comparing LongFin's performance with extended token lengths (e.g., 8K, 16K) and architectures using relative position embeddings on very long documents.

### Open Question 3
- Question: How does LongFin's performance on multilingual documents compare to its performance on English documents?
- Basis in paper: [explicit] The authors state that LongFin was trained and evaluated on the English language only and plan to extend it to support multiple languages in the future.
- Why unresolved: The paper does not provide any data on LongFin's performance on non-English documents.
- What evidence would resolve it: Evaluation of LongFin's performance on multilingual datasets and comparison with its performance on English documents.

## Limitations

- Dataset Representation: The LongForms dataset is limited to SEC Form 10-Q documents, which may not represent the full diversity of financial documents encountered in industrial settings.
- Interval Size Optimization: The choice of interval size (100) for global attention tokens is critical but not extensively validated across different document types.
- Language Generalization: The model's focus on English-only documents limits its applicability in multilingual financial contexts.

## Confidence

- High Confidence: The local and global attention mechanisms are well-established and their effectiveness in handling long contexts is supported by the model's performance on the LongForms dataset.
- Medium Confidence: The layout encoder's bounding box normalization is effective but may face challenges with documents of unusual layouts or scaling.
- Medium Confidence: The BiACM layer's removal of the detach operation improves training efficiency but may introduce instability in text encoder pretraining, requiring further validation.

## Next Checks

1. **Interval Size Sensitivity Analysis**: Conduct experiments with different interval sizes (e.g., 50, 100, 200) to determine the optimal global token spacing for various document lengths and types.
2. **Cross-Dataset Performance Evaluation**: Test LongFin on a broader range of financial documents, including those from different industries or regions, to assess its generalizability beyond SEC Form 10-Q documents.
3. **Multilingual Capability Testing**: Evaluate the model's performance on non-English financial documents to identify any degradation in performance and assess the impact of the BiACM layer's detach operation removal on multilingual contexts.