---
ver: rpa2
title: Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion
  Models
arxiv_id: '2410.02416'
source_url: https://arxiv.org/abs/2410.02416
tags:
- diffusion
- ours
- guidance
- saturation
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Classifier-free guidance is essential for improving image quality
  in diffusion models but causes oversaturation and artifacts at high guidance scales.
  This work analyzes the CFG update rule and decomposes it into parallel and orthogonal
  components relative to the conditional model prediction, showing that the parallel
  component causes oversaturation while the orthogonal component enhances image quality.
---

# Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models

## Quick Facts
- arXiv ID: 2410.02416
- Source URL: https://arxiv.org/abs/2410.02416
- Reference count: 40
- Primary result: Classifier-free guidance causes oversaturation at high guidance scales; APG reduces this while maintaining/improving image quality metrics

## Executive Summary
This paper addresses the critical issue of oversaturation and artifacts that occur when using high guidance scales in classifier-free guidance (CFG) for diffusion models. The authors decompose the CFG update into parallel and orthogonal components relative to the conditional prediction, showing that the parallel component primarily causes oversaturation while the orthogonal component enhances image quality. They propose Adaptive Projected Guidance (APG), which down-weights the parallel component and introduces rescaling and reverse momentum inspired by gradient ascent. APG significantly reduces oversaturation while maintaining or improving FID, recall, and saturation metrics across multiple models including Stable Diffusion and EDM2.

## Method Summary
The paper introduces Adaptive Projected Guidance (APG) to address oversaturation in diffusion models at high guidance scales. APG modifies the standard CFG update by first decomposing it into parallel and orthogonal components relative to the conditional model prediction. The parallel component is down-weighted using a parameter η, while the orthogonal component is preserved. Additionally, APG incorporates rescaling to constrain update norms within a sphere of radius r, inspired by normalized gradient ascent. Finally, reverse momentum with negative β is applied to push away from previous update directions and reduce drift. This method adds negligible computational overhead and serves as a plug-and-play replacement for CFG.

## Key Results
- APG reduces oversaturation while maintaining or improving FID scores compared to CFG across multiple models
- The method significantly improves recall (diversity) metrics while maintaining similar precision to CFG
- APG enables higher guidance scales without the typical artifacts, improving text rendering consistency in Stable Diffusion 3
- Compatible with various samplers, distilled models, and methods like CADS and interval guidance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The parallel component of CFG update is the main source of oversaturation and unrealistic artifacts at high guidance scales.
- Mechanism: CFG update is decomposed into two components: parallel and orthogonal to the conditional model prediction. The parallel component, when amplified by high guidance scales, pushes pixel values toward extremes (gain effect), increasing saturation and contrast. The orthogonal component primarily improves image quality and detail.
- Core assumption: The decomposition into parallel and orthogonal components accurately captures the effects of CFG update on the final image.
- Evidence anchors:
  - [abstract]: "We first decompose the update term in CFG into parallel and orthogonal components with respect to the conditional model prediction and observe that the parallel component primarily causes oversaturation, while the orthogonal component enhances image quality."
  - [section 4]: "We show that the CFG update rule can be decomposed into two components, one that is parallel to the conditional model prediction, and one that is orthogonal to this prediction."
  - [corpus]: Weak - no direct corpus evidence on this specific decomposition mechanism, though EP-CFG and CFG++ also address oversaturation issues.

### Mechanism 2
- Claim: Rescaling the CFG update direction controls large update norms and prevents significant drifts in the sampling process.
- Mechanism: Inspired by the connection between CFG and gradient ascent, the update direction is constrained to lie within a sphere of radius r. This prevents large updates from causing drift and maintains stability in the sampling process.
- Core assumption: The CFG update can be interpreted as a gradient ascent step, and constraining the update norm is beneficial for stability.
- Evidence anchors:
  - [abstract]: "Additionally, we draw a connection between CFG and gradient ascent and introduce a new rescaling and momentum method for the CFG update rule based on this insight."
  - [section 4]: "Inspired by this interpretation and normalized gradient ascent, we rescale the CFG update rule at each step to regulate the impact of each update."
  - [corpus]: Weak - no direct corpus evidence on this specific rescaling mechanism, though normalized gradient ascent is a known technique.

### Mechanism 3
- Claim: Reverse momentum pushes the model away from previous CFG update directions, focusing on the current update direction and reducing drift.
- Mechanism: A negative momentum term (β < 0) is applied to the CFG update direction. This introduces a repulsive effect between consecutive updates, down-weighting components already present in previous steps.
- Core assumption: The negative momentum effectively reduces drift and improves image quality without introducing instability.
- Evidence anchors:
  - [abstract]: "For the momentum term, unlike with traditional optimization, we apply a negative value to introduce a repulsive effect between consecutive updates, effectively down-weighting components already present in previous steps."
  - [section 4]: "Unlike standard optimization methods, we use a negative momentum strength β < 0. Intuitively, this pushes the model away from previous CFG update directions and encourages the model to focus more on the current update direction."
  - [corpus]: Weak - no direct corpus evidence on this specific reverse momentum mechanism.

## Foundational Learning

- Concept: Diffusion models and denoising
  - Why needed here: Understanding how diffusion models work and how they denoise images is crucial for understanding the CFG update and its effects.
  - Quick check question: What is the forward process in a diffusion model, and how does the denoising process work?

- Concept: Classifier-free guidance (CFG)
  - Why needed here: CFG is the core method being modified, so understanding its update rule and effects is essential.
  - Quick check question: How does CFG modify the denoiser's output at each sampling step, and what are its effects on image quality and diversity?

- Concept: Orthogonal projection
  - Why needed here: The decomposition of the CFG update into parallel and orthogonal components relies on orthogonal projection.
  - Quick check question: How is orthogonal projection used to decompose a vector into parallel and orthogonal components?

## Architecture Onboarding

- Component map: Diffusion model -> CFG update -> APG modification (projection, rescaling, reverse momentum) -> Updated noisy image
- Critical path: The sampling process involves querying the diffusion model at each step to predict noise or denoised samples, computing the CFG update, modifying it with APG, and updating the noisy image until the final image is generated.
- Design tradeoffs: The main tradeoff is between image quality and oversaturation. CFG improves image quality but causes oversaturation at high guidance scales. APG aims to retain the quality benefits of CFG while reducing oversaturation. Another tradeoff is between computational cost and performance. APG adds negligible overhead but may slightly increase sampling time.
- Failure signatures: If APG causes instability or degrades image quality, it may be due to incorrect implementation of the projection, rescaling, or reverse momentum. If APG does not reduce oversaturation, it may be due to incorrect hyperparameters or an issue with the decomposition of the CFG update.
- First 3 experiments:
  1. Implement the decomposition of the CFG update into parallel and orthogonal components and visualize their effects on a simple image.
  2. Implement the rescaling of the CFG update and compare its effects on image quality and stability with standard CFG.
  3. Implement the reverse momentum and compare its effects on image quality and drift with standard CFG and APG without reverse momentum.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal guidance scale range for APG across different diffusion model architectures?
- Basis in paper: [explicit] The paper demonstrates that APG enables higher guidance scales without oversaturation, but does not provide specific optimal ranges for different models.
- Why unresolved: The paper shows APG works across various models but only tests a few guidance scale values per model. The relationship between guidance scale and APG performance across the full range remains unexplored.
- What evidence would resolve it: Systematic testing of APG across a wide range of guidance scales (e.g., 1-50) for each model architecture, identifying the point where additional quality gains plateau or quality metrics start degrading.

### Open Question 2
- Question: How does APG affect the diversity-quality trade-off differently than CFG across various guidance scales?
- Basis in paper: [explicit] The paper notes that CFG reduces diversity and that APG can improve diversity (higher recall), but doesn't provide a detailed analysis of how this trade-off changes with guidance scale.
- Why unresolved: While the paper shows APG improves both quality and diversity compared to CFG, it doesn't quantify how this balance shifts as guidance scale increases or how it compares to other diversity-enhancing methods.
- What evidence would resolve it: Detailed plots showing the precision-recall-FID trade-off curve for APG and CFG across multiple guidance scales, along with comparisons to other diversity methods like CADS and IG.

### Open Question 3
- Question: What is the theoretical explanation for why APG improves text rendering consistency in Stable Diffusion 3?
- Basis in paper: [explicit] The paper observes that APG improves text spelling in Stable Diffusion 3 but doesn't provide a theoretical explanation for this specific improvement.
- Why unresolved: The paper demonstrates the empirical result but doesn't explore why the orthogonal component projection specifically benefits text rendering, which involves different visual features than general image generation.
- What evidence would resolve it: Analysis of how the orthogonal projection affects different visual feature channels, particularly those relevant to text rendering, or experiments isolating which component of APG (projection, rescaling, or momentum) is responsible for the text quality improvement.

## Limitations

- The connection between CFG and gradient ascent lacks rigorous theoretical justification, relying on intuitive analogies rather than proven equivalences
- The reverse momentum mechanism (β < 0) introduces an optimization approach that hasn't been extensively studied in diffusion contexts
- The paper shows APG works across multiple models but doesn't deeply analyze when the parallel component is most problematic or whether the decomposition captures all oversaturation sources

## Confidence

- **High confidence**: The empirical results showing APG reduces oversaturation while maintaining/improving FID and recall across multiple models
- **Medium confidence**: The decomposition mechanism explaining why parallel components cause oversaturation, as the theoretical justification is intuitive but not rigorously proven
- **Medium confidence**: The gradient ascent connection and reverse momentum mechanism, as these are inspired analogies rather than proven equivalences

## Next Checks

1. **Theoretical validation**: Formally prove or disprove the gradient ascent interpretation of CFG and analyze under what conditions the parallel component dominates oversaturation
2. **Ablation study**: Systematically vary η, r, and β parameters across a wider range to identify optimal settings and failure modes for different model architectures
3. **Generalization test**: Apply APG to conditional image-to-image tasks and inpainting to verify the method's effectiveness beyond text-to-image generation