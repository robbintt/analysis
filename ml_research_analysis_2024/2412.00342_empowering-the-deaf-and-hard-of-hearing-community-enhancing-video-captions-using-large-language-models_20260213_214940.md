---
ver: rpa2
title: 'Empowering the Deaf and Hard of Hearing Community: Enhancing Video Captions
  Using Large Language Models'
arxiv_id: '2412.00342'
source_url: https://arxiv.org/abs/2412.00342
tags:
- captions
- caption
- video
- llms
- speech
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving video caption accuracy
  for the Deaf and Hard of Hearing (DHH) community by leveraging Large Language Models
  (LLMs) to correct errors in Automatic Speech Recognition (ASR) outputs. The authors
  curated a dataset of 52 YouTube videos across diverse domains and used it to evaluate
  a pipeline that employs ChatGPT-3.5 and Llama2-13B to enhance caption quality.
---

# Empowering the Deaf and Hard of Hearing Community: Enhancing Video Captions Using Large Language Models

## Quick Facts
- arXiv ID: 2412.00342
- Source URL: https://arxiv.org/abs/2412.00342
- Reference count: 40
- One-line primary result: ChatGPT-3.5 significantly reduces Word Error Rate (WER) from 23.07% to 9.75%, achieving high BLEU and ROUGE scores for improved caption accuracy.

## Executive Summary
This paper addresses the challenge of improving video caption accuracy for the Deaf and Hard of Hearing (DHH) community by leveraging Large Language Models (LLMs) to correct errors in Automatic Speech Recognition (ASR) outputs. The authors curated a dataset of 52 YouTube videos across diverse domains and used it to evaluate a pipeline that employs ChatGPT-3.5 and Llama2-13B to enhance caption quality. Results show that ChatGPT-3.5 significantly reduces Word Error Rate (WER) from 23.07% to 9.75%, a 57.72% improvement, and achieves high BLEU (0.85) and ROUGE scores (ROUGE-1: 0.98, ROUGE-2: 0.97), indicating superior accuracy and contextual relevance compared to original ASR captions. While Llama2-13B performs well in some aspects, it shows higher WER (24.42%) due to language improvements that deviate from ground truth.

## Method Summary
The study employs zero-shot prompting to correct ASR-generated captions from 52 YouTube videos using ChatGPT-3.5 and Llama2-13B. The pipeline takes ASR output as input, applies carefully crafted prompts to instruct the LLM not to alter word order while correcting errors, and evaluates the corrected captions against manually created ground truth using WER, BLEU, and ROUGE metrics. The dataset spans diverse domains including education, cooking, travel, entertainment, and news, with videos ranging from 1-3 minutes in duration.

## Key Results
- ChatGPT-3.5 reduces WER from 23.07% to 9.75% (57.72% improvement) while maintaining high BLEU (0.85) and ROUGE scores
- Llama2-13B achieves lower WER (24.42%) due to language improvements that deviate from ground truth
- Both models effectively correct homophones and contextual errors in ASR outputs
- ChatGPT-3.5 demonstrates superior contextual understanding and grammatical coherence compared to Llama2-13B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs correct ASR errors by leveraging contextual understanding beyond word-level matching.
- Mechanism: The model uses surrounding text to infer the correct word, resolving homophones and contextual ambiguities that ASR misses.
- Core assumption: LLM training data includes sufficient diversity to generalize to ASR errors.
- Evidence anchors:
  - [abstract] "ChatGPT-3.5 significantly reduces Word Error Rate (WER) from 23.07% to 9.75%"
  - [section 4.1] "Both LLMs, ChatGPT-3.5 and Llama2-13B, effectively corrected homophones and inaccurate words in the input captions"
- Break condition: If ASR errors are too domain-specific or rare in LLM training data, contextual correction may fail.

### Mechanism 2
- Claim: LLMs improve grammatical coherence and language quality while preserving original meaning.
- Mechanism: The model applies syntactic and semantic corrections without altering the intended word sequence, improving readability.
- Core assumption: Correction prompts are precise enough to constrain the LLM to preserve the original structure.
- Evidence anchors:
  - [abstract] "ChatGPT-3.5 shows an approximate 57.72% improvement in WER"
  - [section 3.3] "We prompt LLMs to correct input inaccurate captions without altering the word sequence"
- Break condition: If prompt precision is lost, LLM may rewrite captions, changing meaning or word order.

### Mechanism 3
- Claim: LLMs generalize across linguistic variations, handling accents, dialects, and code-switching in captions.
- Mechanism: Zero-shot prompting allows LLMs to adapt to varied speech inputs without retraining, correcting pronunciation-based errors.
- Core assumption: Zero-shot prompting is sufficient for handling multilingual and dialectal variation.
- Evidence anchors:
  - [section 2.3] "LLMs, trained on diverse datasets, adapt to various accents and dialects, enhancing transcription accuracy"
  - [section 4.1] Preliminary experiments show both models can handle mixed-language inputs
- Break condition: If input speech patterns are outside the LLM's training distribution, accuracy may degrade.

## Foundational Learning

- Concept: Word Error Rate (WER) and its calculation.
  - Why needed here: To measure how many words the LLM corrects versus the original ASR output.
  - Quick check question: How does WER treat insertions versus substitutions versus deletions?

- Concept: Zero-shot prompting.
  - Why needed here: The pipeline relies on instructing LLMs to correct captions without fine-tuning.
  - Quick check question: What distinguishes zero-shot from few-shot prompting in this context?

- Concept: BLEU and ROUGE metrics.
  - Why needed here: To evaluate not just word accuracy but also n-gram overlap and structural coherence.
  - Quick check question: Why might BLEU score lower even if WER improves significantly?

## Architecture Onboarding

- Component map: ASR output (text) → Prompt construction → LLM inference (ChatGPT-3.5 or Llama2-13B) → Corrected caption (text)
- Critical path: Fetch ASR caption → Apply zero-shot correction prompt → Compare to ground truth using WER/BLEU/ROUGE
- Design tradeoffs:
  - Using ChatGPT-3.5 offers higher accuracy but at greater cost and dependency risk; Llama2-13B is more self-contained but less precise.
  - Prompt constraints preserve meaning but may limit correction flexibility.
- Failure signatures:
  - Increased WER despite LLM use: Likely prompt drift or LLM hallucinating corrections.
  - BLEU drops but WER improves: LLM is making semantic corrections that change n-gram matches.
- First 3 experiments:
  1. Run pipeline on a single video with known ASR errors; compare manual corrections to LLM output.
  2. Vary prompt wording (e.g., add domain hints) and measure impact on WER/BLEU.
  3. Test Llama2-13B vs ChatGPT-3.5 on a subset to quantify performance gaps.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LLMs (beyond GPT-3.5 and Llama2-13B) compare in correcting video captions for the DHH community?
- Basis in paper: [explicit] The paper compares GPT-3.5 and Llama2-13B, but suggests future work could explore "more advanced prompt engineering techniques" and "experimenting with more advanced prompt engineering techniques, such as context-aware prompting."
- Why unresolved: The paper only tests two specific LLM models and does not explore the full range of available LLMs or their potential for caption correction.
- What evidence would resolve it: A comprehensive study comparing the performance of multiple LLM models (e.g., GPT-4, Claude, PaLM) on the same dataset using the same evaluation metrics (WER, BLEU, ROUGE).

### Open Question 2
- Question: How effective are LLM-based caption correction systems in real-time applications?
- Basis in paper: [inferred] The paper focuses on improving the accuracy of video captions, but does not address the real-time aspect of caption correction, which is crucial for live events or real-time communication.
- Why unresolved: The paper does not evaluate the latency or computational efficiency of the LLM-based caption correction system.
- What evidence would resolve it: Testing the LLM-based caption correction system on real-time video streams and measuring the latency and accuracy of the corrected captions.

### Open Question 3
- Question: How can LLM-based caption correction systems be adapted to handle code-switching and multilingual content?
- Basis in paper: [explicit] The paper mentions that "YouTube’s ASR system feature currently handles only one language at a time, which presents a significant limitation" and suggests that "Curating a dataset to address this challenge requires using tools such as Whisper by OpenAI, designed to handle multiple languages within a single audio stream."
- Why unresolved: The paper only provides preliminary experiments on code-switching and does not explore the full potential of LLMs in handling multilingual content.
- What evidence would resolve it: A study evaluating the performance of LLM-based caption correction systems on a diverse set of multilingual videos, including those with code-switching, and measuring the accuracy of the corrected captions in different languages.

## Limitations
- The study relies on a relatively small dataset of 52 YouTube videos, which may not fully represent real-world captioning challenges.
- The performance gap between ChatGPT-3.5 and Llama2-13B raises questions about generalizability to other LLM architectures.
- The study does not address potential biases in the dataset or how well the approach scales to longer videos or live captioning scenarios.

## Confidence
- **High confidence** in the methodology and evaluation framework (WER, BLEU, ROUGE metrics are standard and well-established)
- **Medium confidence** in the comparative performance results between ChatGPT-3.5 and Llama2-13B, given the small sample size
- **Low confidence** in the generalizability of results to other domains or longer-form content

## Next Checks
1. **Dataset expansion validation**: Test the pipeline on a larger, more diverse dataset (e.g., 200+ videos across additional domains and longer durations) to assess scalability and robustness.
2. **Cross-LLM comparison**: Evaluate additional LLM architectures (e.g., GPT-4, Claude) using the same prompting strategy to determine if results are model-specific or generalizable.
3. **Real-world deployment test**: Implement the pipeline in a live captioning scenario (e.g., educational lectures or live events) to measure performance under time constraints and assess user feedback from the DHH community.