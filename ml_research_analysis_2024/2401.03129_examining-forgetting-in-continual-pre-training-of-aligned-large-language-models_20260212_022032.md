---
ver: rpa2
title: Examining Forgetting in Continual Pre-training of Aligned Large Language Models
arxiv_id: '2401.03129'
source_url: https://arxiv.org/abs/2401.03129
tags:
- pre-training
- language
- chinese
- continual
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work examines the catastrophic forgetting phenomenon that
  occurs during continual pre-training on an existing fine-tuned large language model
  (LLM). Specifically, we investigate the impact of continual pre-training on a Llama-2-7b-chat
  model using a Traditional Chinese corpus.
---

# Examining Forgetting in Continual Pre-training of Aligned Large Language Models

## Quick Facts
- **arXiv ID**: 2401.03129
- **Source URL**: https://arxiv.org/abs/2401.03129
- **Reference count**: 40
- **Primary result**: Continual pre-training on aligned LLMs leads to catastrophic forgetting, with insufficient mitigation from simple methods like freezing layers or adapters.

## Executive Summary
This work investigates catastrophic forgetting during continual pre-training of aligned large language models, specifically examining a Llama-2-7b-chat model fine-tuned with a Traditional Chinese corpus. The study evaluates changes across output format, knowledge, and reliability dimensions, finding that addressing forgetting is non-trivial and that simple intervention methods prove insufficient. Notably, the research observes increased repetition problems and format drift toward Traditional Chinese outputs, while knowledge retention remains largely unaffected but reliability declines.

## Method Summary
The researchers conducted continual pre-training on an existing fine-tuned Llama-2-7b-chat model using a Traditional Chinese corpus. They evaluated the pre-trained model across multiple dimensions including output format consistency, knowledge retention, and reliability metrics. The study tested various intervention strategies such as layer freezing and adapter-based approaches to mitigate catastrophic forgetting. Comparative analysis was performed between the original fine-tuned model and the continually pre-trained versions to quantify changes in performance characteristics.

## Key Results
- Catastrophic forgetting persists during continual pre-training despite simple intervention methods
- Models show increased repetition problems and format drift toward Traditional Chinese outputs
- Knowledge retention remains largely unaffected while reliability metrics decline

## Why This Works (Mechanism)
Continual pre-training on aligned LLMs causes catastrophic forgetting due to interference between new task optimization and previously learned alignment patterns. The model's parameters, optimized for balanced multi-task performance including safety and alignment constraints, get overwritten by task-specific patterns in the new corpus. This overwrites the delicate equilibrium established during initial alignment, particularly affecting output format consistency and reliability measures while leaving factual knowledge relatively intact.

## Foundational Learning
- **Catastrophic forgetting**: When neural networks learn new tasks, previously learned information can be rapidly overwritten - critical to understand why simple continual training fails
- **Model alignment**: Fine-tuned models incorporate safety and behavioral constraints that can be disrupted during further training - explains why knowledge persists but reliability drops
- **Parameter interference**: Different training objectives create conflicting gradient updates that compete for parameter space - underlies why freezing layers or adapters alone are insufficient
- **Format consistency**: Models develop stable output patterns during alignment that can be disrupted by domain-specific training - explains the Traditional Chinese format drift
- **Knowledge vs. behavior dissociation**: Factual information stored in attention patterns may be more robust than behavioral constraints - accounts for differential effects on knowledge versus reliability
- **Repetition amplification**: Training on domain-specific corpora can reinforce generation patterns that lead to increased repetition - explains the observed repetition problem

## Architecture Onboarding

**Component Map**
Llama-2-7b-chat (aligned model) -> Continual pre-training on Traditional Chinese corpus -> Evaluation across format, knowledge, reliability dimensions

**Critical Path**
The critical path involves the interaction between alignment parameters and new corpus patterns. During continual training, gradient updates from the Traditional Chinese corpus propagate through the network, overwriting alignment-related parameters before reaching output layers that determine format and reliability characteristics.

**Design Tradeoffs**
The study implicitly reveals the tension between maintaining alignment integrity and adapting to new domains. Simple intervention methods like layer freezing preserve some alignment characteristics but limit adaptation capacity, while adapters provide minimal protection against forgetting. The choice of intervention method involves balancing adaptation needs against catastrophic forgetting risk.

**Failure Signatures**
The primary failure signatures observed are: (1) increased generation of Traditional Chinese text despite input language, (2) higher frequency of repetitive output patterns, and (3) degraded reliability metrics while maintaining knowledge accuracy. These failures manifest as behavioral drift rather than knowledge loss.

**First 3 Experiments**
1. Compare format consistency metrics between original and continually pre-trained models across multiple input languages
2. Measure repetition frequency and pattern diversity in generated outputs
3. Evaluate reliability metrics (toxicity, factuality, coherence) while controlling for knowledge-based tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation conducted on single model architecture and language variant, limiting generalizability
- No statistical significance testing reported for performance differences between models
- Methodology lacks detailed explanation of specific evaluation metrics and protocols

## Confidence

**High confidence**: The observation that continual pre-training on aligned LLMs leads to format drift (increased Traditional Chinese generation) is well-supported by the experimental setup.

**Medium confidence**: The finding that catastrophic forgetting persists despite intervention methods (freezing layers, adapters) is plausible but would benefit from more rigorous ablation studies.

**Medium confidence**: The claim about declining reliability requires more detailed metrics and statistical validation to be considered robust.

## Next Checks
1. Replicate the experiments across multiple base model architectures (Mistral, Gemma, etc.) and different model scales (1B, 13B, 70B parameters) to assess generalizability of forgetting effects.

2. Conduct statistical significance testing on all reported performance differences between original and continually pre-trained models using paired statistical tests appropriate for the evaluation metrics.

3. Implement controlled ablation studies testing different continual pre-training strategies (elastic weight consolidation, rehearsal-based methods, selective parameter updates) to identify which intervention methods show promise for mitigating forgetting.