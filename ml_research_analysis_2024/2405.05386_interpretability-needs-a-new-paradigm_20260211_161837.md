---
ver: rpa2
title: Interpretability Needs a New Paradigm
arxiv_id: '2405.05386'
source_url: https://arxiv.org/abs/2405.05386
tags:
- paradigm
- interpretability
- explanations
- arxiv
- paradigms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that interpretability research needs new paradigms
  beyond the current intrinsic and post-hoc approaches. The authors claim that neither
  paradigm adequately ensures faithfulness of explanations - the intrinsic paradigm's
  architectural constraints don't guarantee faithful explanations, while the post-hoc
  paradigm struggles to provide faithful explanations for black-box models.
---

# Interpretability Needs a New Paradigm

## Quick Facts
- arXiv ID: 2405.05386
- Source URL: https://arxiv.org/abs/2405.05386
- Reference count: 40
- Primary result: Current interpretability paradigms (intrinsic and post-hoc) are inadequate for ensuring faithful explanations, requiring new approaches

## Executive Summary
This paper argues that interpretability research needs new paradigms beyond the current intrinsic and post-hoc approaches. The authors claim that neither paradigm adequately ensures faithfulness of explanations - the intrinsic paradigm's architectural constraints don't guarantee faithful explanations, while the post-hoc paradigm struggles to provide faithful explanations for black-box models. They propose three emerging paradigms: learn-to-faithfully-explain, faithfulness-measurable models, and self-explaining models, which could better address the challenge of ensuring faithfulness while maintaining model performance. The paper emphasizes that vigilance regarding faithfulness is crucial even with these new approaches to avoid repeating past mistakes where explanations appeared convincing but were ultimately unfaithful to the model's behavior.

## Method Summary
The paper presents a conceptual framework analyzing the limitations of current interpretability paradigms and proposing three new approaches. Rather than introducing specific algorithms or models, it provides a theoretical analysis of why existing methods fall short and outlines potential directions for future research. The three proposed paradigms - learn-to-faithfully-explain (optimizing models so explanation methods become faithful), faithfulness-measurable models (designing models to enable easy measurement of explanation faithfulness), and self-explaining models (where models directly output both predictions and explanations) - represent conceptual frameworks rather than concrete implementations. The paper serves as a roadmap for future interpretability research rather than presenting a specific technical contribution.

## Key Results
- Current interpretability paradigms (intrinsic and post-hoc) fail to adequately ensure faithful explanations
- Neither architectural constraints (intrinsic) nor post-hoc methods can reliably produce faithful explanations for black-box models
- Three new paradigms are proposed to address these limitations: learn-to-faithfully-explain, faithfulness-measurable models, and self-explaining models

## Why This Works (Mechanism)
The paper's mechanism rests on recognizing that faithfulness in interpretability requires fundamentally rethinking how models are designed and trained, rather than treating interpretability as an afterthought or architectural constraint. By proposing paradigms that integrate faithfulness into the core model design or optimization process, the authors argue that more reliable explanations can be achieved.

## Foundational Learning

**Faithfulness in interpretability**: The property that an explanation accurately represents the model's actual decision-making process. Why needed: Without faithfulness, explanations may be convincing but misleading. Quick check: Can you distinguish between an explanation that matches the model's actual behavior versus one that appears plausible but is incorrect?

**Intrinsic vs. post-hoc interpretability**: Intrinsic approaches build interpretability into the model architecture itself, while post-hoc approaches analyze trained models to generate explanations. Why needed: Understanding these paradigms helps identify their limitations. Quick check: Can you explain why architectural constraints don't guarantee faithful explanations?

**Self-explaining models**: Models that directly output both predictions and explanations. Why needed: This paradigm could eliminate the gap between model behavior and explanations. Quick check: How would a model need to be structured to provide both predictions and faithful explanations simultaneously?

## Architecture Onboarding

**Component map**: Input data -> Model architecture -> Prediction output -> Explanation method (current paradigms) OR Input data -> Faithfulness-optimized architecture -> Prediction + Explanation (new paradigms)

**Critical path**: The flow from input to explanation is critical - any break in faithfulness between model behavior and explanation output undermines the entire interpretability goal.

**Design tradeoffs**: Current paradigms tradeoff between model performance and interpretability, while proposed paradigms aim to integrate both. The challenge is maintaining competitive performance while ensuring faithfulness.

**Failure signatures**: Explanations that appear plausible but don't reflect actual model behavior; explanations that are technically faithful but so complex they're unusable; models that sacrifice too much performance for interpretability.

**First experiments**:
1. Implement a simple learn-to-faithfully-explain model on a toy dataset to test if optimizing for faithful explanations improves faithfulness metrics
2. Create a faithfulness-measurable model variant of a standard architecture and compare explanation quality against the original
3. Develop a prototype self-explaining model for a simple classification task to evaluate the feasibility of joint prediction-explanation outputs

## Open Questions the Paper Calls Out

None

## Limitations
- The proposed new paradigms remain largely theoretical without concrete implementation details
- Minimal empirical evidence that these approaches can deliver faithful explanations while maintaining model performance
- No systematic demonstration that current paradigm limitations actually manifest in real-world applications
- Vague guidance on how to practically evaluate or enforce faithfulness in new paradigms

## Confidence
- Claim that current paradigms are inadequate: Medium (sound reasoning but weak empirical foundation)
- Claim that new paradigms could better address faithfulness: Medium (theoretical promise but no validation)
- Claim that vigilance regarding faithfulness is crucial: High (theoretically sound and practically important)

## Next Checks
1. Develop and test specific implementations of at least one proposed paradigm on standard interpretability benchmarks to measure both faithfulness and performance trade-offs
2. Conduct systematic experiments comparing explanations from current paradigms versus proposed approaches to quantify improvements in faithfulness
3. Create standardized evaluation protocols for measuring faithfulness that can be applied across different paradigms to enable fair comparisons