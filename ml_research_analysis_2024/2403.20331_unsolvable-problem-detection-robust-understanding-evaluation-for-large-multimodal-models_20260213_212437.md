---
ver: rpa2
title: 'Unsolvable Problem Detection: Robust Understanding Evaluation for Large Multimodal
  Models'
arxiv_id: '2403.20331'
source_url: https://arxiv.org/abs/2403.20331
tags:
- llav
- answer
- image
- questions
- standard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Unsolvable Problem Detection (UPD), a novel
  task to evaluate the robust understanding capability of Large Multimodal Models
  (LMMs). UPD assesses whether LMMs can withhold answers when encountering unsolvable
  problems in Multiple-Choice Question Answering (MCQA), covering three settings:
  Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible
  Visual Question Detection (IVQD).'
---

# Unsolvable Problem Detection: Robust Understanding Evaluation for Large Multimodal Models

## Quick Facts
- arXiv ID: 2403.20331
- Source URL: https://arxiv.org/abs/2403.20331
- Reference count: 37
- Primary result: Most state-of-the-art LMMs struggle significantly with unsolvable problem detection despite strong performance on standard benchmarks

## Executive Summary
This paper introduces Unsolvable Problem Detection (UPD) as a novel task to evaluate the robust understanding capability of Large Multimodal Models (LMMs). UPD tests whether models can withhold answers when encountering unsolvable problems in Multiple-Choice Question Answering, covering three settings: Absent Answer Detection, Incompatible Answer Set Detection, and Incompatible Visual Question Detection. The authors construct the MM-UPD Bench benchmark based on MMBench, enabling fine-grained evaluation across 18 ability dimensions. Experiments reveal that most LMMs, despite adequate performance on existing benchmarks, struggle significantly with UPD, indicating a novel aspect of trustworthiness overlooked by current evaluations.

## Method Summary
The authors construct MM-UPD Bench by filtering MMBench to exclude image-agnostic questions, then adapting problems for three UPD settings: masking correct answers for Absent Answer Detection, shuffling answer sets for Incompatible Answer Set Detection, and pairing incompatible images with questions for Incompatible Visual Question Detection. Evaluation uses CircularEval and GPT-4o-mini for answer matching, applying the same protocol for standard and UPD settings with additional options and instructions. The study tests multiple open-source and closed-source LMMs using standard inference with greedy search, computing Standard, UPD, and Dual accuracy for each model.

## Key Results
- Most LMMs achieve high standard accuracy (>70%) but significantly lower UPD accuracy, revealing superficial understanding
- No correlation between standard benchmark performance and UPD performance, indicating UPD measures a distinct capability
- Open-source models lag behind closed-source models in UPD performance, though larger open-source models can mitigate this gap
- Chain-of-thought and self-reflection prompting improve performance for LMMs with LLM capability bottlenecks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: UPD evaluates true comprehension by testing refusal capability on unsolvable problems
- Mechanism: LMMs must withhold answers when encountering unsolvable MCQA problems (absent answers, incompatible answer sets, or incompatible image-question pairs)
- Core assumption: Standard MCQA accuracy does not guarantee true understanding; refusal ability is a stronger indicator of comprehension
- Evidence anchors:
  - [abstract] "UPD assesses the LMM's ability to withhold answers when encountering unsolvable problems of MCQA, verifying whether the model truly understands the answer."
  - [section] "A model that effectively rejects unsolvable problems while accurately solving standard solvable problems can be regarded as truly understanding them."
  - [corpus] Weak: No direct corpus evidence linking refusal ability to comprehension
- Break condition: If refusal capability is not correlated with actual reasoning ability, or if models learn to refuse without understanding

### Mechanism 2
- Claim: Fine-grained ability diagnosis reveals specific weaknesses in LMMs
- Mechanism: By categorizing unsolvable problems across 18 ability dimensions, UPD identifies which specific reasoning skills LMMs lack
- Core assumption: Different LMMs have different bottlenecks in visual understanding versus language comprehension
- Evidence anchors:
  - [section] "Our fine-grained ability analysis revealed that even closed-source models such as GPT-4o exhibit weaknesses in specific abilities."
  - [section] "Performance trends vary across abilities" and Figure 4 showing ability-wise analysis
  - [corpus] Weak: Limited corpus evidence on ability-specific weakness diagnosis
- Break condition: If performance differences across abilities are due to dataset artifacts rather than genuine capability differences

### Mechanism 3
- Claim: Chain-of-thought and self-reflection improve performance for LMMs with LLM capability bottlenecks
- Mechanism: Prompting strategies that enhance reasoning help LMMs better identify unsolvable problems when their language comprehension is the limiting factor
- Core assumption: Some LMMs struggle with refusal due to inadequate language reasoning rather than visual understanding limitations
- Evidence anchors:
  - [section] "we revealed that whether the bottleneck lies in the LLM's refusal capability or its visual understanding depends on the specific LMM."
  - [section] "For LMMs where the bottleneck is in the LLM's refusal capability, we observed performance improvements with LLM-driven approaches such as chain-of-thought and self-reflection."
  - [corpus] Weak: Limited corpus evidence on effectiveness of these prompting strategies for unsolvable problem detection
- Break condition: If these prompting strategies do not generalize beyond the specific LMMs tested

## Foundational Learning

- Concept: Out-of-distribution detection
  - Why needed here: UPD extends OOD detection concepts to multimodal reasoning, where models must identify unsolvable inputs
  - Quick check question: What distinguishes a solvable problem from an unsolvable one in the context of UPD?

- Concept: Multimodal reasoning evaluation
  - Why needed here: Understanding how visual and language components interact is crucial for interpreting UPD results
  - Quick check question: How does UPD differ from standard VQA evaluation in terms of what it measures?

- Concept: Instruction following and refusal capability
  - Why needed here: UPD tests whether models can follow instructions to refuse answering when appropriate
  - Quick check question: Why might a model correctly answer standard questions but fail at UPD tasks?

## Architecture Onboarding

- Component map:
  - Image and text question parsing -> Visual understanding module -> Language reasoning module -> Refusal decision module -> Output generation

- Critical path:
  1. Parse image and question
  2. Analyze compatibility between image, question, and answer options
  3. Make refusal decision if any component is incompatible
  4. Generate appropriate response

- Design tradeoffs:
  - Fine-grained vs. coarse-grained unsolvable problem detection
  - Dataset size vs. evaluation reliability
  - General capability vs. UPD-specific specialization

- Failure signatures:
  - High standard accuracy but low UPD accuracy indicates superficial understanding
  - Selective failure on specific ability dimensions reveals targeted weaknesses
  - Performance gap between open-source and closed-source models suggests training differences

- First 3 experiments:
  1. Test base model performance on AAD, IASD, and IVQD to identify initial bottlenecks
  2. Apply chain-of-thought prompting to determine if language reasoning is limiting factor
  3. Test self-reflection prompting to evaluate meta-reasoning capabilities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the correlation between standard benchmark performance and UPD performance vary across different model architectures and training paradigms?
- Basis in paper: Explicit - The paper states "there is little correlation between the performance on the existing MMBench and MM-UPD Bench" and this is a key finding
- Why unresolved: The paper only shows correlation coefficients but doesn't analyze how different architectural choices (vision encoders, instruction tuning methods, fine-tuning strategies) or training paradigms (supervised vs self-supervised, size of pretraining data) affect this correlation
- What evidence would resolve it: Systematic experiments comparing correlation coefficients across different model families, training regimes, and architectural variations while controlling for model size

### Open Question 2
- Question: What is the optimal instruction tuning strategy for improving UPD performance while maintaining general task capabilities?
- Basis in paper: Explicit - The paper explores instruction tuning with different ratios of Standard, AAD, and IVQD data, finding a 6:2:2 ratio optimal for UPD but noting it may degrade general performance
- Why unresolved: The paper only tests a limited set of ratios and data sizes, and doesn't explore more sophisticated approaches like curriculum learning, adaptive weighting, or task-specific fine-tuning strategies
- What evidence would resolve it: Comprehensive ablation studies across a wider range of ratios, data sizes, and fine-tuning strategies, with systematic evaluation on both UPD and general benchmarks

### Open Question 3
- Question: How does the difficulty and heterogeneity of different UPD tasks (AAD, IASD, IVQD) vary across different ability categories?
- Basis in paper: Explicit - The paper shows performance varies by ability in Fig. 4 and notes "the ease of withholding responses varies by ability"
- Why unresolved: The paper provides ability-wise analysis for only two models (InternVL2-40B and GPT-4o) and doesn't systematically characterize which ability types are most/least susceptible to different types of unsolvable problems
- What evidence would resolve it: Large-scale analysis across all abilities and models to identify patterns in which ability categories are most vulnerable to each type of unsolvable problem

### Open Question 4
- Question: How can open-source LMMs be improved to match the refusal capabilities of closed-source models?
- Basis in paper: Explicit - The paper finds "a significant performance gap between open-source LMMs and closed-source LMMs" and notes this is partly due to closed-source models being trained for refusal
- Why unresolved: The paper only explores basic chain-of-thought and self-reflection approaches, without investigating more sophisticated refusal training techniques, reward modeling, or synthetic data generation for refusal capabilities
- What evidence would resolve it: Systematic comparison of different refusal training techniques on open-source models, including synthetic data generation, reinforcement learning from human feedback, and model merging approaches

## Limitations
- Reliance on GPT-4o-mini for answer matching may introduce bias toward models with similar architectural characteristics
- Automated dataset filtering may miss edge cases, potentially affecting benchmark reliability
- Limited evidence directly linking refusal behavior to genuine comprehension versus learned patterns

## Confidence

- **High Confidence:** The experimental finding that most LMMs struggle with UPD tasks despite strong performance on standard benchmarks
- **Medium Confidence:** The claim that UPD performance does not correlate with standard benchmark performance
- **Medium Confidence:** The assertion that chain-of-thought and self-reflection strategies improve performance for LMMs with LLM capability bottlenecks

## Next Checks

1. Conduct correlation analysis between standard benchmark performance and UPD performance across all tested models to quantify the relationship (or lack thereof) between these metrics
2. Perform human evaluation of a sample of UPD predictions to verify whether model refusals align with genuine understanding versus learned patterns
3. Test the UPD benchmark with models that have been specifically fine-tuned on refusal capabilities to determine whether performance improvements stem from enhanced understanding or merely better instruction following