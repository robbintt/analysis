---
ver: rpa2
title: 'Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era of
  Foundation Models'
arxiv_id: '2407.07035'
source_url: https://arxiv.org/abs/2407.07035
tags:
- navigation
- language
- conference
- learning
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey reviews vision-and-language navigation (VLN) in the
  era of foundation models, organizing challenges and solutions into three main areas:
  learning a world model to represent visual environments, learning a human model
  to interpret instructions, and learning a VLN agent for reasoning and planning.
  It highlights how foundation models like large language models and vision-language
  models have advanced VLN by enabling better generalization, handling ambiguous instructions,
  and improving planning and reasoning.'
---

# Vision-and-Language Navigation Today and Tomorrow: A Survey in the Era of Foundation Models

## Quick Facts
- **arXiv ID:** 2407.07035
- **Source URL:** https://arxiv.org/abs/2407.07035
- **Authors:** Yue Zhang; Ziqiao Ma; Jialu Li; Yanyuan Qiao; Zun Wang; Joyce Chai; Qi Wu; Mohit Bansal; Parisa Kordjamshidi
- **Reference count:** 40
- **Primary result:** This survey reviews vision-and-language navigation (VLN) in the era of foundation models, organizing challenges and solutions into three main areas: learning a world model to represent visual environments, learning a human model to interpret instructions, and learning a VLN agent for reasoning and planning.

## Executive Summary
This survey comprehensively examines vision-and-language navigation (VLN) in the context of foundation models, which include large language models and vision-language models. The paper systematically organizes the field's challenges and solutions into three interconnected components: world model learning for environment representation, human model learning for instruction interpretation, and VLN agent learning for reasoning and planning. It demonstrates how foundation models have significantly advanced VLN capabilities, particularly in generalization, handling ambiguous instructions, and improving planning and reasoning. The survey also identifies current limitations and future research directions, including real-world deployment challenges and the need for more realistic, dynamic benchmarks.

## Method Summary
The survey employs a comprehensive literature review methodology, synthesizing recent advances in vision-and-language navigation with a particular focus on the transformative impact of foundation models. The authors organize existing research into three main categories: world model learning (environment representation), human model learning (instruction understanding), and VLN agent learning (reasoning and planning). They analyze how foundation models like large language models and vision-language models have addressed traditional VLN challenges such as generalization, instruction ambiguity, and reasoning complexity. The paper also evaluates current limitations and proposes future directions, emphasizing the gap between research benchmarks and real-world deployment scenarios.

## Key Results
- Foundation models have significantly advanced VLN by enabling better generalization across environments and handling ambiguous instructions more effectively.
- The integration of vision-language models and large language models has improved VLN agents' reasoning and planning capabilities.
- Current limitations include the gap between research benchmarks and real-world deployment, as well as the need for more dynamic and realistic evaluation environments.

## Why This Works (Mechanism)
Foundation models work for VLN by leveraging their large-scale pretraining on diverse vision-language data, which provides rich semantic understanding and reasoning capabilities. Large language models excel at processing and interpreting natural language instructions, handling ambiguity, and reasoning about navigation goals. Vision-language models like CLIP can effectively bridge visual perception and language understanding, creating robust representations of the environment. When integrated into VLN systems, these models can generalize better across different environments, handle complex and ambiguous instructions, and perform more sophisticated reasoning about navigation paths. The pretraining on vast datasets allows these models to capture subtle semantic relationships between visual scenes and language, which is crucial for understanding instructions in diverse environments.

## Foundational Learning
- **Environment representation**: Understanding and encoding visual scenes into navigable representations. Why needed: VLN requires agents to understand spatial relationships and object locations in 3D environments. Quick check: Can the model distinguish between similar-looking rooms or identify navigable paths?
- **Instruction parsing**: Converting natural language instructions into actionable navigation goals. Why needed: Instructions often contain ambiguous or context-dependent language that requires sophisticated interpretation. Quick check: Can the model handle references like "the red chair by the window" in different environments?
- **Cross-modal alignment**: Aligning visual features with language semantics. Why needed: VLN requires tight coupling between what the agent sees and what it's told to do. Quick check: Does the model correctly associate visual landmarks with their textual descriptions?
- **Reasoning and planning**: Developing strategies to reach navigation goals through sequential decision-making. Why needed: Navigation often requires multi-step reasoning and path planning. Quick check: Can the model handle instructions requiring backtracking or exploring alternative routes?
- **Generalization**: Adapting to new environments and instruction styles. Why needed: Real-world deployment requires handling previously unseen scenarios. Quick check: How does performance degrade when tested on environments different from training data?
- **Handling ambiguity**: Dealing with unclear or incomplete instructions. Why needed: Natural language instructions often contain implicit information or ambiguity. Quick check: Can the model resolve references like "go there" or "move closer" without explicit context?

## Architecture Onboarding

**Component map:** Vision encoder -> Cross-modal fusion -> Language understanding -> Reasoning module -> Action selection -> Environment feedback

**Critical path:** Instruction understanding → Reasoning and planning → Action execution → Environment observation → Model update

**Design tradeoffs:**
1. **End-to-end vs modular approaches**: End-to-end models offer simplicity but may lack interpretability, while modular approaches provide flexibility but increase complexity
2. **Model size vs inference speed**: Larger foundation models provide better performance but may be impractical for real-time deployment
3. **Generalization vs specialization**: Models trained on diverse data generalize better but may underperform on domain-specific tasks

**Failure signatures:**
- Getting stuck in local minima when instructions are ambiguous
- Over-reliance on visual features at the expense of language understanding
- Failure to handle out-of-distribution environments
- Inability to recover from navigation errors

**First 3 experiments to run:**
1. Test the model on environments with varying degrees of visual similarity to training data to assess generalization capabilities
2. Evaluate performance on instructions with different levels of ambiguity and complexity
3. Measure the impact of different cross-modal fusion strategies on navigation success rates

## Open Questions the Paper Calls Out
The paper identifies several open questions for future research, including: How can foundation models be effectively adapted for real-world VLN deployment considering computational constraints? What benchmarks are needed to evaluate VLN in more realistic and dynamic environments? How can we better integrate multiple foundation models to create more robust VLN systems? What are the best approaches for handling long-term navigation tasks that require memory and planning over extended time horizons?

## Limitations
- The rapid evolution of foundation models may quickly make specific technical details outdated
- The survey's organization into three main areas may oversimplify the complex interactions between world models, human models, and VLN agents
- Discussion of limitations lacks specific quantitative benchmarks and comparative analyses of different foundation model approaches

## Confidence
- **High confidence:** The general framework of organizing VLN challenges and solutions into world models, human models, and agent learning
- **Medium confidence:** The assertion that foundation models have significantly advanced VLN capabilities
- **Medium confidence:** The identification of future directions including real-world deployment and dynamic environments

## Next Checks
1. Conduct a systematic review of recent VLN benchmarks to quantify the performance improvements attributable to foundation models, comparing against traditional approaches using standardized metrics.
2. Implement a controlled experiment testing the integration of multiple foundation models (e.g., combining a vision-language model with a large language model) in VLN tasks to validate claims about their synergistic effects.
3. Develop a case study examining the challenges of adapting a specific foundation model (e.g., CLIP or GPT-4) for real-world VLN deployment, documenting the technical hurdles and performance trade-offs encountered.