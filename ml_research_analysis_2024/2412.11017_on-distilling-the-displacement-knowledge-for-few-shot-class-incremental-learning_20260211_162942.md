---
ver: rpa2
title: On Distilling the Displacement Knowledge for Few-Shot Class-Incremental Learning
arxiv_id: '2412.11017'
source_url: https://arxiv.org/abs/2412.11017
tags:
- knowledge
- learning
- distillation
- classes
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses catastrophic forgetting in Few-Shot Class-Incremental
  Learning (FSCIL) by proposing a Displacement Knowledge Distillation (DKD) method.
  Unlike traditional knowledge distillation that measures sample similarity, DKD computes
  displacement vectors between samples to capture both distance and angular information,
  preserving more structural information.
---

# On Distilling the Displacement Knowledge for Few-Shot Class-Incremental Learning

## Quick Facts
- arXiv ID: 2412.11017
- Source URL: https://arxiv.org/abs/2412.11017
- Authors: Pengfei Fang; Yongchun Qin; Hui Xue
- Reference count: 40
- Primary result: Achieves SOTA performance with 2.62% average accuracy gain in FSCIL

## Executive Summary
This paper addresses catastrophic forgetting in Few-Shot Class-Incremental Learning (FSCIL) by proposing a Displacement Knowledge Distillation (DKD) method. Unlike traditional knowledge distillation that measures sample similarity, DKD computes displacement vectors between samples to capture both distance and angular information, preserving more structural information. The authors introduce a Dual Distillation Network (DDNet) framework that applies Individual Knowledge Distillation (IKD) to base classes and DKD to novel classes, addressing the performance gap between them. An instance-aware sample selector dynamically adjusts branch weights during inference.

## Method Summary
The proposed method introduces Displacement Knowledge Distillation (DKD) as an alternative to traditional knowledge distillation approaches in FSCIL. DKD computes displacement vectors between samples rather than relying on similarity measures, capturing both distance and angular information to preserve more structural information about the feature space. The Dual Distillation Network (DDNet) framework combines IKD for base classes with DKD for novel classes, addressing the performance gap between them. An instance-aware sample selector dynamically adjusts branch weights during inference based on the similarity between base and novel class features, with DKD branch weight decreasing as similarity increases.

## Key Results
- Achieves state-of-the-art performance with an average accuracy gain of 2.62% across CIFAR-100, miniImageNet, and CUB-200 datasets
- Demonstrates improved robustness against outliers compared to existing methods
- Shows effectiveness of DKD in preserving structural information through displacement vectors rather than similarity measures

## Why This Works (Mechanism)
The displacement-based knowledge distillation captures both distance and angular information between samples, which traditional similarity-based methods cannot fully represent. By computing displacement vectors rather than relying solely on feature similarity, the method preserves more structural information about the feature space. The dual distillation approach addresses the inherent performance gap between base and novel classes in FSCIL by applying different distillation strategies optimized for each class type.

## Foundational Learning
- **Knowledge Distillation**: Essential for transferring knowledge from old models to new ones in incremental learning; quick check: verify distillation loss computation
- **Catastrophic Forgetting**: The core problem being addressed where models lose previously learned knowledge when learning new classes; quick check: measure performance degradation on base classes
- **Few-Shot Learning**: The scenario where only limited examples are available for new classes; quick check: verify sample count per class
- **Feature Displacement**: The core innovation measuring vector differences between samples rather than similarities; quick check: validate displacement vector computation
- **Instance-Aware Weighting**: Dynamic adjustment of distillation branch weights based on sample similarity; quick check: verify weight computation logic

## Architecture Onboarding

**Component Map**: Base Feature Extractor -> IKD Branch -> DKD Branch -> Instance-Aware Selector -> Final Prediction

**Critical Path**: Input -> Feature Extractor -> Dual Distillation Branches -> Instance-Aware Weighting -> Output

**Design Tradeoffs**: The dual-branch approach increases computational complexity but provides better adaptation to different class types. The instance-aware selector adds inference overhead but improves accuracy by dynamically adjusting to sample characteristics.

**Failure Signatures**: Performance degradation on base classes indicates insufficient IKD strength; poor novel class performance suggests DKD branch issues; inconsistent instance weighting may cause unstable predictions.

**First 3 Experiments**: 
1. Validate displacement vector computation compared to similarity-based approaches
2. Test instance-aware selector weight adjustments on synthetic data
3. Compare base vs novel class performance with different distillation strengths

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Performance improvements are most pronounced on CIFAR-100, with more modest gains on miniImageNet and CUB-200
- Experimental validation relies heavily on synthetic few-shot scenarios created by subsampling from standard datasets
- The claims about improved robustness to outliers could benefit from more rigorous statistical analysis

## Confidence

**High confidence**: The technical implementation of DKD and DDNet framework
**Medium confidence**: The claims about structural information preservation through displacement vectors
**Medium confidence**: The reported performance improvements, particularly on CIFAR-100
**Low confidence**: The generalization of results to truly few-shot scenarios and real-world applications

## Next Checks
1. Test the method on truly few-shot datasets (e.g., Omniglot, mini-ImageNet) with genuine few-shot learning scenarios rather than subsampled standard datasets
2. Conduct ablation studies to quantify the specific contribution of DKD versus IKD components and the instance-aware sample selector
3. Perform long-term incremental learning experiments to evaluate the method's performance over many incremental steps and assess potential degradation over time