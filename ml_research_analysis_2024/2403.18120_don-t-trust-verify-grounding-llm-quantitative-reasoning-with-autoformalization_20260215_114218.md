---
ver: rpa2
title: 'Don''t Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization'
arxiv_id: '2403.18120'
source_url: https://arxiv.org/abs/2403.18120
tags:
- formal
- statement
- informal
- solution
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper presents a method to improve the reliability of quantitative
  reasoning in large language models (LLMs) by grounding their outputs in formal theorem
  proving environments. The approach, called Don''t Trust: Verify (DTV), leverages
  the autoformalization capability of LLMs to translate natural language problem statements
  and solutions into formal mathematical representations.'
---

# Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization

## Quick Facts
- arXiv ID: 2403.18120
- Source URL: https://arxiv.org/abs/2403.18120
- Reference count: 40
- Method improves LLM quantitative reasoning by verifying solutions with formal theorem proving, achieving over 12% improvement on GSM8K

## Executive Summary
This paper presents a method to improve the reliability of quantitative reasoning in large language models (LLMs) by grounding their outputs in formal theorem proving environments. The approach, called Don't Trust: Verify (DTV), leverages autoformalization to translate natural language problem statements and solutions into formal mathematical representations, which are then verified using automated theorem provers. This allows incorrect solutions to be identified and rejected, consistently outperforming vanilla majority voting across multiple datasets and model sizes. The method is lightweight, requiring no additional training or fine-tuning, and is complementary to existing prompting methods for improving LLM reasoning.

## Method Summary
The method generates 64 informal solutions per problem using few-shot prompting with a base LLM, then performs statement formalization by translating natural language problem statements into Isabelle formal statements using few-shot prompting. For each solution, formal solution sketches are generated and verified using Isabelle's automated theorem prover, filtering out unverified solutions. The approach operates without additional training or fine-tuning, relying solely on the autoformalization capability of LLMs to create formal representations that can be checked by theorem provers.

## Key Results
- 12.4% improvement over vanilla majority voting on GSM8K dataset
- Average improvements of 3.2-5.2% on MATH (Prealgebra, Algebra, Number Theory subsets) and MultiArith datasets
- Outperforms both few-shot baselines and majority voting with or without few-shot demonstrations
- Lightweight approach requiring no additional training or fine-tuning

## Why This Works (Mechanism)
The method works by creating a formal verification layer between informal LLM outputs and their acceptance as correct answers. By translating problem statements and solutions into formal mathematical representations, the approach enables automated theorem provers to check the logical validity of solutions. This catches incorrect reasoning paths that might still arrive at plausible-sounding but wrong answers. The formal verification acts as a safety net that filters out solutions with flawed logic, even when those solutions appear reasonable in natural language form.

## Foundational Learning
- **Autoformalization**: The process of translating natural language mathematics into formal logical statements; needed to bridge between LLM outputs and theorem prover inputs; quick check: verify that formal statements preserve semantic meaning of original problems
- **Automated theorem proving**: Using software to mechanically verify mathematical proofs; needed to provide objective correctness validation beyond LLM self-assessment; quick check: test prover success rate on known correct/incorrect solutions
- **Few-shot prompting**: Providing example demonstrations within prompts to guide LLM behavior; needed to generate both informal solutions and formal translations without fine-tuning; quick check: measure solution quality with varying numbers of examples
- **Majority voting ensembles**: Aggregating multiple model outputs to improve reliability; needed as baseline comparison and for filtering verified solutions; quick check: compare accuracy vs. number of solutions generated
- **Formal solution sketches**: Structured representations of solution steps in formal logic; needed to enable theorem provers to verify reasoning steps; quick check: verify that sketches are both expressive enough and provable
- **Isabelle/HOL**: A proof assistant and theorem prover for higher-order logic; needed as the formal verification backend; quick check: measure automation success rate on translated problems

## Architecture Onboarding

**Component Map:**
Problem Statement -> Few-shot Solution Generation -> Autoformalization -> Formal Verification -> Filtered Solutions -> Majority Voting

**Critical Path:**
The critical path flows from problem statement through solution generation, autoformalization, and formal verification before reaching the final filtering and voting stage. Each component must succeed for a solution to be accepted, making autoformalization and verification the key bottlenecks.

**Design Tradeoffs:**
The approach trades computational overhead (theorem proving is expensive) for reliability gains. It also accepts that some correct solutions may be rejected if they cannot be formalized or verified, prioritizing precision over recall. The method avoids training/fine-tuning costs but depends heavily on few-shot prompting quality.

**Failure Signatures:**
- High verification failure rate indicates poor autoformalization quality or overly complex formal representations
- Low improvement over baselines suggests the theorem prover is not catching meaningful errors or the informal solutions are already highly accurate
- Inconsistent results across datasets may indicate domain-specific limitations in the formal representation approach

**3 First Experiments to Run:**
1. Measure autoformalization success rate and fidelity on held-out problems to establish baseline translation quality
2. Compare verification accuracy on known correct vs. incorrect solutions to calibrate prover sensitivity
3. Test the impact of varying the number of generated solutions (e.g., 16, 32, 64) on final accuracy to find optimal tradeoff

## Open Questions the Paper Calls Out
None

## Limitations
- Autoformalization quality is critical and limits verification capability - approximately one-third of solutions cannot be verified due to formalization failures
- The approach may not generalize well beyond arithmetic word problems to more complex mathematical domains
- Computational overhead of theorem proving may limit practical deployment despite no training requirements

## Confidence
- **High confidence**: Verification via theorem proving consistently improves accuracy over vanilla majority voting across multiple datasets
- **Medium confidence**: The approach is complementary to existing prompting methods, though direct comparisons to state-of-the-art techniques are limited
- **Medium confidence**: No additional training is required, though the method depends on few-shot prompting capabilities that may implicitly require mathematical pretraining

## Next Checks
1. Systematically evaluate the faithfulness of autoformalized statements by having human experts verify whether formal representations accurately capture the meaning of their informal counterparts
2. Test the approach on non-arithmetic quantitative reasoning problems (e.g., calculus, probability) to assess cross-domain generalization
3. Measure the computational overhead of the verification process relative to solution generation time to determine practical deployment feasibility