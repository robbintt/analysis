---
ver: rpa2
title: 'Question: How do Large Language Models perform on the Question Answering tasks?
  Answer:'
arxiv_id: '2412.12893'
source_url: https://arxiv.org/abs/2412.12893
tags:
- llms
- answer
- questions
- question
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares large language models (LLMs) to fine-tuned
  models on the SQuAD2 question-answering dataset, focusing on single-inference prompting
  for unanswerable questions. The authors evaluate models including GPT-4 Turbo, various
  LLaMA variants, and fine-tuned models (Flan-T5, DistilBERT, RoBERTa) using standard
  metrics (EM, F1) and extended analysis including Levenshtein distance and interrogative
  pronoun breakdown.
---

# Question: How do Large Language Models perform on the Question Answering tasks? Answer:

## Quick Facts
- arXiv ID: 2412.12893
- Source URL: https://arxiv.org/abs/2412.12893
- Reference count: 39
- Large language models (LLMs) generally underperform fine-tuned models on SQuAD2 but show superior generalization to out-of-distribution QA datasets

## Executive Summary
This paper evaluates large language models against fine-tuned models on the SQuAD2 question-answering dataset, focusing on a single-inference prompting strategy for handling both answerable and unanswerable questions. The authors find that while fine-tuned models outperform LLMs on the in-distribution task, recent LLMs like LLaMA-3.1-70B achieve competitive performance and demonstrate superior generalization to five out-of-distribution QA datasets. The study proposes a prompting approach that eliminates the need for separate unanswerable detection passes, reducing computational overhead while maintaining accuracy.

## Method Summary
The study compares out-of-the-box LLMs (GPT-4 Turbo, LLaMA variants) against fine-tuned models (Flan-T5, DistilBERT, RoBERTa) on SQuAD2 using a single-inference prompting strategy. Models are evaluated using standard metrics (EM, F1) plus extended analysis including Levenshtein distance and interrogative pronoun breakdown. The approach involves prompting LLMs to output "unanswerable" for questions without answers in the context, eliminating the need for separate unanswerable detection. Performance is tested on SQuAD2 and five additional out-of-distribution QA datasets (NewsQA, SearchQA, TriviaQA, HotpotQA, SQuAD1.1).

## Key Results
- Fine-tuned models outperform LLMs on SQuAD2 (FT-LM F1: 74.55% vs LLaMA-3.1-70B-Instruct F1: 66.03%)
- LLaMA-3.1-70B-Instruct achieves the second-best F1 score among all models tested
- LLMs show better generalization, with LLaMA-3.1-70B outperforming fine-tuned models on 3 of 5 OOD datasets
- Single-inference prompting reduces computational overhead while maintaining competitive accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Single-inference prompting can handle both answerable and unanswerable questions without the computational overhead of a two-step approach.
- Mechanism: The prompt explicitly instructs the model to evaluate context and reply "unanswerable" if no answer can be derived, eliminating the need for a separate unanswerable detection pass.
- Core assumption: LLMs will reliably follow the instruction to only output "unanswerable" when appropriate and avoid verbose or explanatory responses.
- Evidence anchors:
  - [abstract]: "We propose a prompting style which aims to elicit the same ability without the need for double inference, saving compute time and resources."
  - [section 3.2]: "To handle these questions, we prompted the LLM to answer with only the string 'unanswerable' for unanswerable questions."
- Break condition: If the LLM ignores the instruction and produces extraneous text or attempts to explain why the question is unanswerable, the single-inference approach fails to save computation and may introduce noise into evaluation.

### Mechanism 2
- Claim: Recent LLMs (particularly LLaMA-3.1-70B) can rival fine-tuned models on in-distribution SQuAD2 tasks when using optimized prompts.
- Mechanism: Instruction-tuned LLMs leverage extensive pretraining and few-shot learning capabilities to perform competitively on the QA task without task-specific fine-tuning.
- Core assumption: The instruction-tuning and prompt design sufficiently guide the model to produce concise, context-accurate answers matching the evaluation format.
- Evidence anchors:
  - [abstract]: "Our results show that smaller, fine-tuned models outperform current State-Of-The-Art (SOTA) LLMs on the fine-tuned task, but recent SOTA models are able to close this gap on the out-of-distribution test..."
  - [section 4.1]: "LLaMA-3.1-70B-Instruction achieved the second-best F1-score."
- Break condition: If the prompt fails to constrain the model's output format (e.g., includes extraneous tokens), the evaluation metrics will penalize otherwise semantically correct answers.

### Mechanism 3
- Claim: LLMs show better generalization to out-of-distribution QA datasets compared to fine-tuned models.
- Mechanism: Because LLMs were not adapted to the specific SQuAD2 distribution, their pretraining allows them to transfer knowledge more effectively to new QA formats and domains.
- Core assumption: The diversity and scale of LLM pretraining provides broader generalization capabilities than task-specific fine-tuning on a single dataset.
- Evidence anchors:
  - [abstract]: "Our results show that smaller, fine-tuned models outperform current State-Of-The-Art (SOTA) LLMs on the fine-tuned task, but recent SOTA models are able to close this gap on the out-of-distribution test and even outperform the fine-tuned models on 3 of the 5 tested QA datasets."
  - [section 5]: "LLaMA-3.1-70B Instruct was able to outperform the FT-LMs... in both F1 and EM-score on three of the five datasets."
- Break condition: If the out-of-distribution datasets share significant structural similarities with SQuAD2, the generalization advantage may diminish or reverse.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: CoT prompting helps LLMs break down the QA task into logical steps, improving performance on complex reasoning required for both answerable and unanswerable questions.
  - Quick check question: How does telling the model to "think step-by-step" influence its ability to correctly identify unanswerable questions?

- Concept: Levenshtein distance as an evaluation metric
  - Why needed here: Standard exact match metrics can be too strict for LLM outputs that may be semantically correct but syntactically different; Levenshtein distance provides a more nuanced view of answer quality.
  - Quick check question: Why might allowing 8-10 character changes significantly improve EM scores for LLaMA-3.1-70B but not for fine-tuned models?

- Concept: Prompt engineering for instruction-following models
  - Why needed here: Effective prompts must constrain LLM behavior to produce concise answers matching evaluation requirements, avoiding verbose or explanatory responses that would be penalized.
  - Quick check question: What specific prompt instructions are necessary to prevent LLMs from including extraneous tokens like "Sure, your answer is"?

## Architecture Onboarding

- Component map: Prompt template with persona and CoT instructions -> LLM inference engine (various LLaMA and GPT-4 models) -> Answer post-processing for extraneous token removal -> Evaluation pipeline using SQuAD2 metrics plus Levenshtein distance analysis -> OOD testing framework with 5 external QA datasets
- Critical path: Prompt generation → LLM inference → Answer post-processing → Metric calculation → Result aggregation
- Design tradeoffs: Using single-inference prompting reduces computational cost but requires more sophisticated prompt design; post-processing is necessary for fair comparison but adds complexity; Levenshtein distance provides nuanced evaluation but increases computation time.
- Failure signatures: Poor performance on unanswerable questions (hallucinations), extraneous tokens in LLM outputs, significant performance gaps between in-distribution and OOD testing, inconsistent results across interrogative pronouns.
- First 3 experiments:
  1. Compare single-inference vs two-step inference approaches on unanswerable questions to quantify computational savings and accuracy trade-offs.
  2. Test different prompt variations (without CoT, with different persona descriptions) on LLaMA-3.1-70B to identify optimal prompt structure.
  3. Implement and evaluate different post-processing strategies (regex-based vs model-specific) to determine the best balance between simplicity and effectiveness.

## Open Questions the Paper Calls Out

## Open Question 1
- Question: What specific architectural or training differences in LLaMA-3.1-70B enabled it to achieve competitive performance with fine-tuned models on SQuAD2?
- Basis in paper: Explicit - The paper identifies LLaMA-3.1-70B as a notable exception that achieved the second-highest F1 score, trailing only RoBERTa
- Why unresolved: The paper does not analyze what specific improvements in LLaMA-3.1-70B (compared to earlier LLaMA versions) contributed to this performance leap
- What evidence would resolve it: A comparative analysis of LLaMA-3.1-70B's architecture, training methodology, and pretraining data composition versus previous LLaMA models

## Open Question 2
- Question: How does the single-inference prompting strategy's performance vary across different question types and difficulty levels?
- Basis in paper: Inferred - The paper proposes a single-inference prompting strategy to handle both answerable and unanswerable questions but doesn't analyze its effectiveness breakdown by question type
- Why unresolved: While the paper evaluates overall performance and interrogative pronoun breakdown, it doesn't specifically assess how the single-inference strategy performs across different question difficulty levels or types
- What evidence would resolve it: A detailed analysis of the single-inference prompt's performance on easy versus hard questions, or answerable versus unanswerable questions specifically

## Open Question 3
- Question: What is the computational cost trade-off between the single-inference prompting strategy and traditional two-step approaches across different model sizes?
- Basis in paper: Explicit - The paper states that the single-inference approach "saves compute time and resources" but doesn't provide quantitative comparisons
- Why unresolved: The paper claims reduced computational costs but doesn't provide specific metrics or comparisons between the single-inference approach and traditional two-step methods
- What evidence would resolve it: Empirical measurements of inference time, computational resources, and cost comparisons between single-inference and two-step approaches across various model sizes

## Open Question 4
- Question: How do the Levenshtein distance improvements translate to actual user-perceived answer quality improvements?
- Basis in paper: Explicit - The paper introduces Levenshtein distance as an extended evaluation metric but doesn't connect it to user experience or practical applications
- Why unresolved: While the paper shows that some models have smaller Levenshtein distances from ground truth, it doesn't demonstrate how this translates to meaningful differences in user-perceived answer quality
- What evidence would resolve it: Human evaluation studies comparing user satisfaction with answers that have varying Levenshtein distances from ground truth

## Limitations
- The study relies on post-processing to normalize LLM outputs, and differences in tokenization and normalization procedures between model types could influence the results
- The absolute performance comparisons between LLMs and fine-tuned models may be affected by the lack of exact prompt specifications
- The generalization findings, while promising, are based on only five out-of-distribution datasets that may not represent all possible QA task variations

## Confidence
- High confidence: The single-inference approach clearly eliminates the need for separate unanswerable detection passes, providing computational efficiency
- Medium confidence: The generalization findings are promising but based on a limited set of out-of-distribution datasets
- Low confidence: The absolute performance comparisons may be influenced by post-processing differences and lack of exact prompt specifications

## Next Checks
1. Implement and compare different post-processing strategies (regex-based vs model-specific) to determine the best balance between simplicity and effectiveness for ensuring fair comparison between LLM and fine-tuned model outputs.
2. Test different prompt variations (without CoT, with different persona descriptions) on LLaMA-3.1-70B to identify optimal prompt structure and isolate the contribution of each prompt component to overall performance.
3. Conduct an ablation study on the single-inference prompting approach by systematically removing the instruction to output "unanswerable" and measuring the impact on both answerable and unanswerable question performance.