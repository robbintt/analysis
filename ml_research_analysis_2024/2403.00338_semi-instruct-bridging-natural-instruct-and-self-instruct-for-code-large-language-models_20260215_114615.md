---
ver: rpa2
title: 'Semi-Instruct: Bridging Natural-Instruct and Self-Instruct for Code Large
  Language Models'
arxiv_id: '2403.00338'
source_url: https://arxiv.org/abs/2403.00338
tags:
- code
- data
- test
- cases
- codes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Semi-Instruct, a method that combines natural-instruct
  and self-instruct paradigms to improve code large language models for program synthesis.
  Natural-instruct provides diverse and correct codes but lacks proper instruction-code
  pairs and has improper formats.
---

# Semi-Instruct: Bridging Natural-Instruct and Self-Instruct for Code Large Language Models

## Quick Facts
- **arXiv ID**: 2403.00338
- **Source URL**: https://arxiv.org/abs/2403.00338
- **Reference count**: 0
- **Primary result**: Semi-Instruct significantly outperforms both natural-instruct and self-instruct paradigms on HumanEval dataset, with performance steadily improving as data scale increases.

## Executive Summary
This paper addresses the limitations of existing code instruction tuning paradigms for large language models by proposing Semi-Instruct, which bridges natural-instruct and self-instruct approaches. Natural-instruct provides diverse and correct codes but lacks proper instruction-code pairs and has improper formats, while self-instruct generates proper paired data but suffers from low diversity and uncertain code correctness. Semi-Instruct converts improper codes from natural-instruct into proper instruction-code pairs using a method similar to self-instruct, then verifies correctness by constructing test cases through executing original code to get outputs. Experiments on HumanEval demonstrate significant performance improvements over both baseline methods.

## Method Summary
Semi-Instruct combines the strengths of natural-instruct and self-instruct paradigms to improve code large language models for program synthesis. The method first converts improper codes from natural-instruct data into proper instruction-code pairs by generating instructions for each code snippet. It then constructs test cases by generating inputs and executing the original correct code to obtain outputs, avoiding unreliable LLM-generated outputs. The resulting instruction-code pairs are ranked by the number of passing test cases to create an implicit difficulty curriculum. Finally, diverse and correct instruction-code pairs are retained for instruction tuning, and the performance steadily improves as data scale increases.

## Key Results
- Semi-Instruct significantly outperforms both natural-instruct and self-instruct paradigms on HumanEval dataset
- Performance steadily improves as data scale increases, with consistent gains across different dataset sizes
- Combining Semi-Instruct with self-instruct data provides complementary benefits rather than redundancy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Executing original code on generated test inputs provides correct outputs without needing LLM generation.
- Mechanism: By running the known-correct original code on the generated inputs, the system extracts outputs directly, avoiding the unreliable LLM-generated outputs that often fail on complex tasks.
- Core assumption: The original code is functionally correct and deterministic given the inputs.
- Evidence anchors:
  - [abstract] "To verify the correctness of generated codes, we design a novel way to construct test cases by generating cases' inputs and executing correct codes from natural-instruct to get outputs."
  - [section] "Unlike generated outputs, executed outputs are inherently correct due to the correctness of original code."
- Break condition: Original code contains bugs, non-determinism, or external dependencies that affect output consistency.

### Mechanism 2
- Claim: Sorting data by number of passing test cases creates an implicit difficulty curriculum.
- Mechanism: Instructions that generate more test cases are easier, as the LLM better understands their constraints. Sorting from most to fewest test cases enables progressive learning from simple to complex tasks.
- Core assumption: The number of generated test cases correlates with instruction complexity and model understanding.
- Evidence anchors:
  - [abstract] "Finally, diverse and correct instruction-code pairs are retained for instruction tuning."
  - [section] "Intuitively, the quality of generated inputs depends on to what extent the LLMs understand the instruction...We rank the data in reverse order by the number of test cases so that the model can learn incrementally."
- Break condition: Test case generation becomes too sparse to provide meaningful difficulty distinctions.

### Mechanism 3
- Claim: Combining Semi-Instruct data with Self-Instruct data improves performance beyond either alone.
- Mechanism: Semi-Instruct provides diverse, correct code from natural sources while Self-Instruct provides proper formatting and instruction-code pairs. The combination captures both strengths without the individual weaknesses.
- Core assumption: The distributions of Semi-Instruct and Self-Instruct data are complementary rather than redundant.
- Evidence anchors:
  - [abstract] "Furthermore, the performance steadily improves as data scale increases."
  - [section] "We observe that combining SI with NI resulted in a decline in performance. The distinct distributions of these two datasets likely cause this outcome."
- Break condition: Data distributions become too similar, causing redundancy rather than complementarity.

## Foundational Learning

- **Concept**: Test case generation and validation
  - Why needed here: The paper relies on generating inputs and executing original code to get outputs, avoiding unreliable LLM-generated outputs.
  - Quick check question: Why does executing original code on inputs provide more reliable outputs than having the LLM generate outputs directly?

- **Concept**: Curriculum learning principles
  - Why needed here: The paper sorts data by test case count to create an implicit difficulty progression for model training.
  - Quick check question: How does the number of test cases relate to instruction difficulty in this context?

- **Concept**: Data diversity and redundancy management
  - Why needed here: The paper must balance diversity from natural sources with the proper formatting from self-generated data while avoiding duplicates.
  - Quick check question: What metrics does the paper use to filter out similar instructions and maintain diversity?

## Architecture Onboarding

- **Component map**: Data generation pipeline → Test case construction → Code validation → Difficulty ranking → Model training
- **Critical path**: Original code → Instruction generation → Input generation → Output extraction via execution → Refined code validation → Ranking → Training
- **Design tradeoffs**: Balance between data diversity (from natural sources) and code correctness/consistency (from self-generated data)
- **Failure signatures**: Performance plateaus despite more data, inconsistent p@1 scores across data scales, high ROUGE-L similarity scores between instructions
- **First 3 experiments**:
  1. Generate a small batch of Semi-Instruct data and verify that executing original code produces correct outputs
  2. Test the sorting mechanism by comparing model performance with and without test case-based ordering
  3. Validate the combination approach by training on Semi-Instruct alone vs. combined with Self-Instruct data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Semi-Instruct scale when applied to larger datasets beyond 70k entries?
- Basis in paper: [explicit] The paper discusses performance improvements up to 70k data entries but does not explore beyond this scale.
- Why unresolved: The experiments are limited to datasets ranging from 10k to 70k entries, leaving the scalability of Semi-Instruct on larger datasets unexplored.
- What evidence would resolve it: Conducting experiments with datasets larger than 70k entries and analyzing the performance metrics such as p@1, p@10, and p@100 to determine if the trend of improvement continues or plateaus.

### Open Question 2
- Question: What is the impact of Semi-Instruct on the diversity of generated code compared to other methods?
- Basis in paper: [explicit] The paper mentions that Semi-Instruct inherits the diversity of codes from NI and improves diversity metrics compared to SI.
- Why unresolved: While the paper shows improvements in diversity metrics, it does not provide a detailed analysis of the types of diversity (e.g., syntactic, semantic) and how Semi-Instruct specifically enhances these aspects.
- What evidence would resolve it: Conducting a detailed analysis of the diversity of generated code, including syntactic and semantic diversity, and comparing it with other methods like NI and SI.

### Open Question 3
- Question: How does the ranking of data by the number of test cases affect the learning efficiency of the model?
- Basis in paper: [explicit] The paper introduces ranking data by the number of test cases as a measure of difficulty and claims it improves learning efficiency.
- Why unresolved: The paper suggests that ranking by test cases is beneficial but does not provide empirical evidence on how it specifically affects learning efficiency compared to other ranking methods.
- What evidence would resolve it: Conducting experiments where data is ranked by different criteria (e.g., problem complexity, code length) and comparing the learning efficiency and performance metrics to determine the optimal ranking method.

### Open Question 4
- Question: What are the limitations of using original codes from NI to generate test cases, and how can these be addressed?
- Basis in paper: [explicit] The paper mentions that executing original codes on generated inputs can construct test cases but also highlights potential runtime errors and mismatches between instructions and codes.
- Why unresolved: The paper does not explore the limitations of this approach in detail or propose solutions to address these issues.
- What evidence would resolve it: Analyzing the types and frequencies of runtime errors and mismatches in test case generation and developing strategies to mitigate these issues, such as improving input generation or enhancing code execution frameworks.

## Limitations
- The execution-based validation mechanism assumes original code is functionally correct and deterministic, which may not hold for complex or non-deterministic programs
- The difficulty ranking based on test case count is an intuitive heuristic that lacks theoretical grounding in code complexity measurement
- The complementary data distribution assumption between Semi-Instruct and Self-Instruct data needs more rigorous validation, particularly regarding potential redundancy

## Confidence
- **High confidence**: The core Semi-Instruct methodology combining natural-instruct and self-instruct paradigms is well-specified and the experimental setup on HumanEval is reproducible.
- **Medium confidence**: The effectiveness of test case-based validation and difficulty ranking mechanisms, as the paper provides reasonable justification but limited ablation studies.
- **Low confidence**: The assumption that combining Semi-Instruct with Self-Instruct data provides complementary benefits, as the paper only briefly mentions this observation without detailed analysis.

## Next Checks
1. **Execute original code validation**: Select 100 random instructions from natural-instruct data, run the Semi-Instruct pipeline to generate test cases, and verify that executing original code on generated inputs produces correct outputs as claimed.

2. **Difficulty ranking ablation**: Train two models on the same Semi-Instruct data - one sorted by test case count and one in random order - and compare their HumanEval performance to validate the curriculum learning hypothesis.

3. **Data distribution analysis**: Calculate pairwise ROUGE-L similarity scores between instructions in Semi-Instruct, Self-Instruct, and their combination to quantify distributional overlap and validate the complementarity assumption.