---
ver: rpa2
title: Findings of the IWSLT 2024 Evaluation Campaign
arxiv_id: '2411.05088'
source_url: https://arxiv.org/abs/2411.05088
tags:
- translation
- language
- speech
- data
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The IWSLT 2024 evaluation campaign included 7 shared tasks covering
  speech-to-text translation, simultaneous translation, automatic subtitling, speech-to-speech
  translation, dubbing, low-resource translation, and Indic language translation.
  The campaign attracted 18 teams who submitted 69 systems across 8 language pairs.
---

# Findings of the IWSLT 2024 Evaluation Campaign

## Quick Facts
- **arXiv ID:** 2411.05088
- **Source URL:** https://arxiv.org/abs/2411.05088
- **Reference count:** 40
- **Primary result:** HW-TSC dominated most tasks using cascade systems, while CMU won human evaluation for English→German

## Executive Summary
The IWSLT 2024 evaluation campaign featured 7 shared tasks across speech-to-text translation, simultaneous translation, automatic subtitling, speech-to-speech translation, dubbing, low-resource translation, and Indic language translation. The campaign attracted 18 teams who submitted 69 systems across 8 language pairs. Cascade architectures continued to outperform end-to-end models in speech translation tasks, with HW-TSC emerging as the dominant participant across multiple tasks. The introduction of a subtitle compression subtrack revealed new challenges in maintaining translation quality while adhering to subtitle constraints.

## Method Summary
The evaluation campaign employed a variety of tasks with different data conditions and evaluation metrics. Offline speech-to-text translation used cascade systems combining ASR and MT components. Simultaneous translation required systems to start translating before audio completion. Automatic subtitling introduced a new compression subtrack alongside standard subtitling. Speech-to-speech translation extended the scope to audio output. Low-resource translation focused on languages with limited parallel data. Indic language translation targeted English↔Indian language pairs. Evaluation metrics included BLEU, COMET, chrF for translation quality, and human evaluation for subjective assessment.

## Key Results
- HW-TSC won the offline speech-to-text translation task for English→German, Chinese, and Japanese using cascade systems
- CMU achieved the best human evaluation results for English→German speech translation
- The low-resource translation task showed marked improvements over 2023, with BLEU scores above 20 for 5 of 8 language pairs
- NICT won the Indic language translation task using pseudo translation data for their end-to-end approach
- FBK-AI4CDIR achieved the best overall SubER scores in the automatic subtitling task using a direct speech-to-text approach

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Cascade systems continue to outperform end-to-end models in offline speech translation tasks.
- **Mechanism:** Cascade systems combine separate ASR and MT components, allowing each to specialize and optimize independently. This specialization enables better handling of domain-specific challenges such as spontaneous speech, background noise, and accented input.
- **Core assumption:** The separation of ASR and MT components allows for more effective optimization than joint end-to-end training.
- **Evidence anchors:**
  - [abstract] "The offline speech-to-text translation task saw all systems use a cascade approach, with HW-TSC winning in English→German, Chinese, and Japanese..."
  - [section] "The results show that it is essential to perform a human evaluation since no automatic metric, at the moment, can predict the performance of the individual systems well."
- **Break condition:** When end-to-end models achieve sufficient quality to match or exceed cascade performance across all domains and conditions, or when computational constraints favor end-to-end approaches.

### Mechanism 2
- **Claim:** Large language models provide significant performance gains when integrated into speech translation systems.
- **Mechanism:** LLMs enhance translation quality by providing better context understanding, handling linguistic nuances, and improving output coherence through fine-tuning on task-specific data.
- **Core assumption:** LLMs can be effectively fine-tuned for speech translation tasks while maintaining their general language understanding capabilities.
- **Evidence anchors:**
  - [abstract] "The low-resource translation task showed marked improvements over 2023, with the best systems achieving BLEU scores above 20 for 5 of 8 language pairs."
  - [section] "HW-TSC submitted three primary systems for each data condition, and both the unconstrained (U) and the constrained +LLM (C+) models have a noticeable gain over the constrained model (C)."
- **Break condition:** When the benefits of LLM integration are outweighed by computational costs or when task-specific models achieve comparable performance without LLM dependency.

### Mechanism 3
- **Claim:** Human evaluation remains essential for speech translation assessment due to limitations of automatic metrics.
- **Mechanism:** Human evaluators can assess translation quality, naturalness, and domain-specific challenges that automatic metrics cannot capture, particularly in speech translation where context and delivery affect meaning.
- **Core assumption:** Human judgment provides more reliable assessment of speech translation quality than current automatic metrics.
- **Evidence anchors:**
  - [abstract] "The simultaneous speech-to-text translation task was dominated by HW-TSC, with CMU and NAIST close behind."
  - [section] "The results show that it is essential to perform a human evaluation since no automatic metric, at the moment, can predict the performance of the individual systems well."
- **Break condition:** When automatic metrics achieve sufficient correlation with human judgments across all relevant dimensions of speech translation quality.

## Foundational Learning

- **Concept:** Speech-to-text translation pipeline architecture
  - Why needed here: Understanding the cascade vs. end-to-end debate requires knowledge of how speech translation systems process audio input through transcription to translation output.
  - Quick check question: What are the key differences between cascade and end-to-end speech translation architectures?

- **Concept:** Automatic evaluation metrics for machine translation
  - Why needed here: The paper discusses multiple metrics (BLEU, COMET, chrF) and their limitations, requiring understanding of how these metrics work and what they measure.
  - Quick check question: How do BLEU and COMET differ in their approach to evaluating translation quality?

- **Concept:** Data augmentation techniques in low-resource settings
  - Why needed here: The low-resource translation track discusses various data augmentation approaches, requiring understanding of how these techniques improve model performance with limited data.
  - Quick check question: What are common data augmentation strategies used in low-resource machine translation?

## Architecture Onboarding

- **Component map:** Audio input → ASR transcription → MT translation → (Optional LLM refinement) → Output text/speech
- **Critical path:** Audio input → ASR transcription → MT translation → (Optional LLM refinement) → Output text/speech
- **Design tradeoffs:**
  - Cascade vs. end-to-end: Specialization vs. integration, computational efficiency vs. potential quality gains
  - LLM integration: Performance improvement vs. computational cost and complexity
  - Data conditions: Constrained vs. unconstrained training, impact on model performance and deployment

- **Failure signatures:**
  - Poor ASR quality propagating through cascade
  - Translation errors due to domain mismatch or insufficient training data
  - LLM integration causing unexpected behavior or quality degradation
  - Subtitle compliance issues in automatic subtitling tasks

- **First 3 experiments:**
  1. Compare cascade vs. end-to-end performance on TED talks domain using constrained data
  2. Evaluate LLM integration impact by comparing constrained vs. constrained+LLM systems on the same dataset
  3. Test data augmentation techniques on low-resource language pairs to measure performance improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural improvements or innovations are needed for end-to-end speech translation systems to consistently outperform cascaded approaches across diverse domains and language pairs?
- Basis in paper: [inferred] The paper notes that despite advancements, all systems in the 2024 IWSLT offline task used cascade architectures, with no direct systems submitted. This suggests that end-to-end approaches still lag behind cascades in effectiveness.
- Why unresolved: The paper doesn't analyze why end-to-end systems are not yet competitive or what specific bottlenecks prevent their adoption. It only observes the trend without exploring underlying technical limitations.
- What evidence would resolve it: A detailed ablation study comparing end-to-end and cascade systems on the same data conditions, identifying specific failure modes (e.g., robustness to disfluencies, domain adaptation, or alignment accuracy) that cascades handle better.

### Open Question 2
- Question: How can automatic evaluation metrics be improved to better correlate with human judgments across different speech translation domains and accent variations?
- Basis in paper: [explicit] The paper highlights significant discrepancies between automatic metrics (BLEU, COMET) and human evaluation (Direct Assessment) rankings, especially in the English→German offline task and the accent test set. It notes that no automatic metric reliably predicts system performance across domains.
- Why unresolved: While the paper observes the poor correlation, it doesn't propose solutions or analyze which linguistic phenomena (e.g., disfluencies, named entities, prosodic features) contribute most to metric failures.
- What evidence would resolve it: A systematic analysis correlating metric failures with specific error types identified in human evaluations, followed by development and testing of domain-adaptive or multi-metric evaluation frameworks.

### Open Question 3
- Question: What are the key factors that enable successful low-resource speech translation for typologically diverse languages, and how can these be generalized across language families?
- Basis in paper: [inferred] The low-resource task showed marked improvements, with BLEU scores above 20 for 5 of 8 language pairs. However, performance varied significantly (e.g., Maltese-English at 58.9 BLEU vs. Tamasheq-French at 8.83 BLEU), suggesting some languages benefit more from current approaches than others.
- Why unresolved: The paper reports results but doesn't analyze what makes certain language pairs (like Maltese-English) more amenable to current methods versus others (like Tamasheq-French). It doesn't identify transferable strategies across language families.
- What evidence would resolve it: Comparative analysis of successful systems across language pairs, identifying common techniques (e.g., data augmentation, pseudo-translation, adapter-based fine-tuning) and their effectiveness relative to language family characteristics (e.g., morphology, orthography, available resources).

## Limitations

- The exact reasons for cascade superiority over end-to-end approaches are not fully characterized
- The correlation between automatic metrics and human judgments is not comprehensively analyzed
- The impact of language pair characteristics on system performance across tasks is not systematically explored

## Confidence

- **High confidence:** Cascade systems outperforming end-to-end models in offline tasks, LLM integration providing measurable benefits, human evaluation necessity
- **Medium confidence:** Relative performance differences between teams across language pairs, effectiveness of data augmentation techniques
- **Low confidence:** Specific reasons for automatic metric limitations, precise impact of language characteristics on system performance

## Next Checks

1. Conduct ablation studies comparing cascade and end-to-end performance on specific linguistic phenomena (word order, morphology, idiomatic expressions) to identify systematic failure modes
2. Perform correlation analysis between automatic metrics and human judgments across all tasks to identify which metrics best predict human preferences for different language pairs and domains
3. Test the robustness of winning systems across out-of-domain data and noisy conditions to assess real-world deployment viability beyond the controlled evaluation setting