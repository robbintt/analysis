---
ver: rpa2
title: 'Hybrid Primal Sketch: Combining Analogy, Qualitative Representations, and
  Computer Vision for Scene Understanding'
arxiv_id: '2407.04859'
source_url: https://arxiv.org/abs/2407.04859
tags:
- sketch
- visual
- forbus
- learning
- representations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the Hybrid Primal Sketch (HPS), a framework
  that combines computer vision components with qualitative visual representation
  and analogical learning to create human-like visual scene understanding. The HPS
  uses an ensemble of computer vision algorithms to generate "glyphs" - visual entities
  consisting of digital ink and conceptual labels - which are then processed by CogSketch
  to produce relational scene representations.
---

# Hybrid Primal Sketch: Combining Analogy, Qualitative Representations, and Computer Vision for Scene Understanding

## Quick Facts
- arXiv ID: 2407.04859
- Source URL: https://arxiv.org/abs/2407.04859
- Reference count: 6
- Primary result: Achieves 85.03% accuracy on MNIST with only 500 training examples using analogical learning

## Executive Summary
The Hybrid Primal Sketch (HPS) framework combines computer vision algorithms with qualitative spatial representations and analogical learning to create human-like visual scene understanding. The system uses an ensemble of vision components to generate "glyphs" - visual entities with digital ink and conceptual labels - which are processed by CogSketch to produce relational scene representations. This approach enables data-efficient learning that requires only one pass through training data, compared to 20+ epochs for deep learning baselines, while producing interpretable models that explain classification decisions.

## Method Summary
HPS processes images through an ensemble of vision components (edge finders, recognizers, segmenters) that output glyphs in a digital ink format. CogSketch converts these glyphs into qualitative shape representations and computes spatial relationships between entities. These representations are then fed into SAGE, an analogical learning system that incrementally builds generalization pools by comparing new examples to existing concepts. The framework includes a hierarchical extension (PHAL) that decomposes sketches into multiple levels of edge cycles, training separate generalization pools per level and combining evidence across levels for classification.

## Key Results
- Achieved 85.03% accuracy on MNIST sketch recognition with only 500 training examples
- Obtained 52.38% recall@50 on the VRD visual relationship detection dataset
- Demonstrated 75% accuracy on human action recognition from Kinect video data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Hybrid Primal Sketch achieves data efficiency by leveraging analogical generalization over qualitative visual representations rather than deep learning's brute-force parameter fitting.
- Mechanism: CogSketch converts ensemble-generated glyphs into relational scene representations that SAGE can incrementally generalize from, requiring only one pass through training data versus 20+ epochs for deep learning baselines.
- Core assumption: Qualitative representations capture the essential structure of visual concepts in a form amenable to analogical reasoning.
- Evidence anchors:
  - [abstract]: "These descriptions can be used in analogical reasoning and learning. Analogical generalization provides incremental, data-efficient learning of inspectable models, producing performance on a par with, and sometimes better than, deep learning systems."
  - [section 4.1]: "We note that with fewer than 1,000 training examples, LeNet-5 performs at chance, thereby demonstrating the data-efficiency of analogical learning."
  - [corpus]: Weak - no direct citations found in neighbor papers about analogical learning efficiency.
- Break condition: If qualitative representations fail to capture task-relevant visual structure, analogical generalization cannot produce accurate concepts from limited examples.

### Mechanism 2
- Claim: Hierarchical decomposition via edge cycles enables better generalization by capturing multi-scale structure.
- Mechanism: PHAL decomposes sketches into 3 levels of edge cycles, training separate generalization pools per level, then combining evidence across levels for classification.
- Core assumption: Visual concepts have inherent hierarchical structure that can be exploited for recognition.
- Evidence anchors:
  - [section 4.1]: "PHAL has been tested on two datasets to date... On this dataset we used 10 CPUs to encode sketches and just one to perform the hierarchical analogical learning – No GPUs or TPUs required, thereby demonstrating training efficiency as well."
  - [section 4.1]: "PHAL does best of all, handily beating traditional analogy" on the Coloring Book Object dataset.
  - [corpus]: Weak - no neighbor papers directly address hierarchical visual decomposition for recognition.
- Break condition: If visual concepts lack consistent hierarchical structure, the decomposition may introduce noise rather than useful multi-scale information.

### Mechanism 3
- Claim: The glyph-based intermediate representation bridges the semantic gap between low-level vision and high-level reasoning.
- Mechanism: Ensemble components output glyphs (digital ink + conceptual label) which CogSketch processes using human-like qualitative representations, enabling inspection and explanation of classifications.
- Core assumption: Glyphs provide a cognitive-level representation that maintains spatial and conceptual information needed for reasoning.
- Evidence anchors:
  - [section 3]: "Our hypothesis is that the notion of glyph in open-domain sketch understanding... provides that output format... Thus the elements of the ensemble produce glyphs for CogSketch, which then analyzes them in the same ways that it analyzes human-drawn sketches."
  - [section 4.2]: "Because the visual relationships computed by CogSketch are inspired by human vision, the generalizations produced tend to be interpretable."
  - [corpus]: Weak - no neighbor papers discuss glyph-based representations as bridges between vision and reasoning.
- Break condition: If the glyph representation loses critical information during ensemble processing, subsequent qualitative analysis cannot recover it.

## Foundational Learning

- Concept: Qualitative spatial representations (e.g., RCC8)
  - Why needed here: These provide the symbolic vocabulary for describing relationships between visual entities in a way that supports analogical reasoning and learning.
  - Quick check question: What are the eight relations in RCC8 and what topological relationships do they capture?

- Concept: Structure-mapping theory and analogical generalization
  - Why needed here: The learning mechanism relies on comparing new examples to existing generalization pools via structural similarity, updating probabilities of statements within concepts.
  - Quick check question: How does SAGE handle disjunctive concepts and what happens when a new example retrieves an outlier?

- Concept: Ensemble-based perception
  - Why needed here: The HPS framework combines multiple vision components (edge finders, recognizers, segmenters) to produce robust glyph representations, leveraging the strengths of each type.
  - Quick check question: What are the advantages and disadvantages of edge finders versus recognizers versus segmenters in the HPS ensemble?

## Architecture Onboarding

- Component map: Image → Ensemble (Canny, MASK-RCNN, SAM, Potrace) → Glyphs → CogSketch → Qualitative representations → SAGE generalization pools → Classification
- Critical path: Image → Ensemble → Glyphs → CogSketch → Qualitative representations → SAGE generalization → Classification
- Design tradeoffs: Interpretability and data efficiency vs. raw performance ceiling; CPU-only training vs. GPU acceleration; incremental learning vs. batch optimization
- Failure signatures: Poor glyph generation from ensemble, loss of spatial information in CogSketch decomposition, inability of SAGE to form useful generalizations from limited data
- First 3 experiments:
  1. MNIST sketch recognition with Potrace encoding to verify basic analogical learning pipeline
  2. Visual relationship detection on VRD dataset to test natural image processing capabilities
  3. TU Berlin sketch dataset with PHAL to validate hierarchical decomposition benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific mechanisms determine how the ensemble of visual components in HPS automatically selects which algorithms to apply for a given image and task context?
- Basis in paper: [explicit] The paper states "Given an image and a task context, which algorithms should be used? Currently this is a procedural parameter set by the experimenter" and mentions plans to make ensemble operations more automatic.
- Why unresolved: The paper only mentions plans to develop automatic control mechanisms but does not describe what these mechanisms would be or how they would work.
- What evidence would resolve it: Experimental results showing HPS automatically selecting appropriate algorithms for different image types and tasks, along with technical descriptions of the decision-making process.

### Open Question 2
- Question: How does HPS handle the trade-off between top-down conceptual constraints and bottom-up visual processing in diagram understanding, particularly when dealing with errors in automatic segmentation?
- Basis in paper: [explicit] The paper discusses this trade-off in the context of diagram understanding, stating "A tradeoff we will explore involves the relative roles of conceptual and visual constraints in the interpretation process."
- Why unresolved: The paper identifies this as an area of exploration but provides no data or analysis of how this trade-off should be managed or what the optimal balance might be.
- What evidence would resolve it: Empirical studies comparing different weighting schemes between conceptual and visual constraints, with quantitative results showing which approaches work best for different types of diagrams.

### Open Question 3
- Question: What are the specific neural implementations or biological plausibility of the symbolic representations used in HPS, particularly in relation to the original Primal Sketch's biological hypotheses?
- Basis in paper: [explicit] The paper notes that while the HPS builds on Marr's work, it is "agnostic as to where and how" symbolic representations are produced in biological systems, unlike the original Primal Sketch which focused on biological plausibility.
- Why unresolved: The paper deliberately avoids addressing biological plausibility, focusing instead on practical functionality, but this leaves open questions about how well the HPS aligns with human visual processing.
- What evidence would resolve it: Neuroscientific studies comparing HPS representations to brain activity patterns during visual processing, or detailed models showing how HPS components could be implemented in neural architectures.

## Limitations

- Evaluation limited to small-scale datasets rather than industrial benchmarks like ImageNet or COCO
- Minimal error analysis showing which specific visual concepts or relationship types the system struggles with
- No systematic comparison of CPU-only training against optimized deep learning pipelines on identical hardware

## Confidence

- **Medium**: Data efficiency claims (single-pass training vs. 20+ epochs) - supported by MNIST results but not tested across diverse datasets or compared to optimized deep learning training schedules
- **Medium**: Training efficiency (CPU-only operation) - demonstrated on Sketchy dataset but no systematic comparison to deep learning baselines under identical hardware constraints  
- **Medium**: Interpretability of models - qualitative assertion supported by the inspection capability but lacking quantitative or comparative evaluation
- **High**: Glyph representation bridging capability - well-supported by the architectural description and multiple domain demonstrations
- **Low**: Hierarchical decomposition benefits - limited to two datasets with minimal analysis of when PHAL outperforms flat analogical learning

## Next Checks

1. **Scalability test**: Evaluate HPS on ImageNet or COCO datasets to assess whether the data efficiency advantage persists with orders of magnitude more training examples and classes. Measure both absolute performance and training time compared to deep learning baselines.

2. **Interpretability benchmark**: Conduct systematic comparison of HPS interpretability against deep learning models with attention visualization, feature attribution methods, or natural language explanations. Use standardized metrics for model transparency and user studies for human interpretability.

3. **Error analysis study**: Perform detailed analysis of classification errors across all three demonstrated domains (sketches, natural images, actions). Identify failure modes related to glyph generation, qualitative representation limitations, or analogical generalization failures.