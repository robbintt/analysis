---
ver: rpa2
title: Challenging Low Homophily in Social Recommendation
arxiv_id: '2401.14606'
source_url: https://arxiv.org/abs/2401.14606
tags:
- uni00000013
- uni00000011
- social
- uni00000014
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of low preference-aware homophily
  in social recommendation, where social relations may not effectively reflect user
  preferences for items. The authors propose a data-centric framework called Social
  Heterophily-alleviating Rewiring (SHaRe) to enhance existing graph-based social
  recommendation models.
---

# Challenging Low Homophily in Social Recommendation

## Quick Facts
- arXiv ID: 2401.14606
- Source URL: https://arxiv.org/abs/2401.14606
- Authors: Wei Jiang; Xinyi Gao; Guandong Xu; Tong Chen; Hongzhi Yin
- Reference count: 40
- One-line primary result: SHaRe improves social recommendation by 3.138% in Recall@10, 2.804% in Precision@10, and 2.783% in NDCG@10 by addressing low preference-aware homophily through graph rewiring and contrastive learning

## Executive Summary
This paper addresses the challenge of low preference-aware homophily in social recommendation, where social relations may not effectively reflect user preferences for items. The authors propose a data-centric framework called Social Heterophily-alleviating Rewiring (SHaRe) to enhance existing graph-based social recommendation models. SHaRe employs Social Graph Rewiring to retain reliable social relations by cutting unreliable edges and adding highly homophilic ones, and integrates a Homophilic Relation Augmentation method using contrastive learning to refine user representations. Experiments on three real-world datasets (LastFM, Douban, and Yelp) demonstrate that SHaRe consistently improves the performance of state-of-the-art social recommendation models.

## Method Summary
SHaRe is a data-centric framework that enhances graph-based social recommendation models by addressing low preference-aware homophily. It consists of two main components: Social Graph Rewiring (SGR) and Homophilic Relation Augmentation (HRA). SGR computes user similarity from the interaction graph using cosine similarity, then removes edges with non-positive similarity and adds edges between users with top similarity scores to create a more preference-aligned social graph. HRA uses contrastive learning with homophily-based positive/negative sampling to refine user representations, optimizing both recommendation and contrastive tasks jointly. The framework starts with a warm-up period (first 10 epochs) using the original social graph, then executes SGR once per epoch thereafter, combining with backbone models like DiffNet, MHCN, or LightGCN+social.

## Key Results
- SHaRe achieves average improvements of 3.138% in Recall@10, 2.804% in Precision@10, and 2.783% in NDCG@10 over state-of-the-art models
- The framework consistently improves performance across three real-world datasets (LastFM, Douban, and Yelp) with varying homophily ratios
- SHaRe demonstrates effectiveness even under different graph-wise homophily ratios, validating its robustness and adaptability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph rewiring with similarity-based edge filtering improves social recommendation accuracy.
- Mechanism: The framework calculates user similarity from the interaction graph using cosine similarity, then removes edges with non-positive similarity and adds edges between users with top similarity scores. This creates a social graph more aligned with user preferences.
- Core assumption: User preferences reflected in interaction patterns are more reliable than direct social connections for recommendation.
- Evidence anchors:
  - [abstract] "We adopt Graph Rewiring technique to capture and add highly homophilic social relations, and cut low homophilic (or heterophilic) relations."
  - [section 3.1] "Based on the similarity ùëê (ùëñ,ùëó ) for each user-user pair, we rewire the original social graph by adding highly homophilic edges and cutting redundant edges."
- Break condition: If the interaction graph is too sparse to compute reliable similarity scores, or if the similarity metric doesn't capture preference similarity well.

### Mechanism 2
- Claim: Contrastive learning with homophily-based positive/negative sampling refines user representations.
- Mechanism: The framework selects positive samples as user pairs with high edge-wise homophily ratios above a threshold ùúñ, and negative samples as pairs with low homophily. It then uses InfoNCE loss to maximize consistency between positive pairs while minimizing correlation with negative pairs.
- Core assumption: Users with high homophily ratios have similar preferences and should have similar representations.
- Evidence anchors:
  - [abstract] "we integrate a contrastive learning method into the training of SHaRe, aiming to calibrate the user representations for enhancing the result of Graph Rewiring."
  - [section 3.2] "We select the user pairs with high edge-wise homophily ratios as the positive pairs and let the low homophilic pairs be negative pairs."
- Break condition: If the homophily ratio threshold ùúñ is poorly chosen, leading to noisy positive/negative samples that don't reflect true preference similarity.

### Mechanism 3
- Claim: Joint learning with recommendation and contrastive tasks provides better optimization than sequential training.
- Mechanism: The framework optimizes both the recommendation loss Lrec and the contrastive loss Lcl simultaneously using a weighted combination L = Lrec + ùúÜLcl, where ùúÜ controls the contrastive learning magnitude.
- Core assumption: The contrastive learning auxiliary task provides useful regularization that improves the primary recommendation task.
- Evidence anchors:
  - [abstract] "Experiments on real-world datasets show that the proposed framework not only exhibits enhanced performances across varying homophily ratios but also improves the performance of existing state-of-the-art (SOTA) social recommendation models."
  - [section 3.2] "In SHaRe, we adopt a joint learning strategy to optimize the model, including two tasks: Top-N item recommendation and HRA."
- Break condition: If ùúÜ is poorly tuned, causing either task to dominate and prevent effective learning of the other.

## Foundational Learning

- Concept: Graph neural networks for recommendation
  - Why needed here: The framework builds on graph-based social recommendation models that use GNNs to propagate information through user-item and user-user graphs
  - Quick check question: What are the key differences between message passing in homogeneous graphs versus bipartite interaction graphs?

- Concept: Homophily and heterophily in graphs
  - Why needed here: The framework addresses low preference-aware homophily, where social connections don't reflect user preferences, and aims to increase homophily through rewiring
  - Quick check question: How does edge-wise homophily ratio differ from graph-wise homophily ratio, and why is this distinction important?

- Concept: Contrastive learning for representation refinement
  - Why needed here: The framework uses contrastive learning to calibrate user representations by maximizing consistency between users with similar preferences
  - Quick check question: What is the role of the temperature parameter ùúè in the InfoNCE loss function?

## Architecture Onboarding

- Component map: Social Graph Rewiring (SGR) ‚Üí Homophilic Relation Augmentation (HRA) ‚Üí Backbone social recommendation model ‚Üí Joint loss computation
- Critical path: User embeddings ‚Üí Similarity computation ‚Üí Edge cutting/adding ‚Üí Rewired graph ‚Üí Backbone model ‚Üí Embeddings ‚Üí Loss computation ‚Üí Parameter updates
- Design tradeoffs: SGR introduces noise risk that HRA mitigates; joint learning vs. sequential training; warm-up strategy vs. immediate rewiring
- Failure signatures: Degraded performance on datasets with already high homophily; sensitivity to parameter ùúÅ and ùúÜ; increased training time with multi-SGR strategy
- First 3 experiments:
  1. Test SHaRe with different backbone models (DiffNet, MHCN, LightGCN+social) on LastFM dataset to verify general applicability
  2. Vary the sampling threshold ùúÅ from 0.2 to 0.8 to find optimal positive sample selection
  3. Compare SHaRe with and without HRA component to measure contribution of contrastive learning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the SHaRe framework perform under extremely low homophily ratios, such as those found in highly heterophilic social networks?
- Basis in paper: [inferred] The paper demonstrates SHaRe's effectiveness across varying homophily ratios, but does not specifically test extremely low homophily scenarios.
- Why unresolved: The paper's experiments focus on moderate homophily levels, and extreme cases may present unique challenges not addressed.
- What evidence would resolve it: Testing SHaRe on datasets with known extremely low homophily ratios and comparing its performance to vanilla models would provide clarity.

### Open Question 2
- Question: What is the impact of different edge weighting strategies in the rewired social graph on the recommendation performance?
- Basis in paper: [explicit] The paper mentions using normalized cosine similarity for edge weights in the rewired graph, but does not explore alternative weighting strategies.
- Why unresolved: The paper does not investigate how different edge weighting methods might affect the model's ability to capture reliable social relations.
- What evidence would resolve it: Experimenting with various edge weighting schemes and evaluating their impact on recommendation accuracy would provide insights.

### Open Question 3
- Question: How does the SHaRe framework scale with increasing graph size and sparsity in real-world applications?
- Basis in paper: [inferred] The paper evaluates SHaRe on three datasets but does not explicitly address scalability concerns or performance on very large or sparse graphs.
- Why unresolved: The scalability of SHaRe in handling massive or highly sparse graphs is not thoroughly explored, which is crucial for practical deployment.
- What evidence would resolve it: Conducting experiments on larger datasets with varying levels of sparsity and analyzing computational efficiency would help determine scalability limits.

## Limitations

- The framework's effectiveness depends on proper tuning of sampling threshold Œ∂ and contrastive learning weight Œª, which may vary across datasets and require careful hyperparameter optimization
- The Social Graph Rewiring mechanism could potentially introduce noise if the similarity computation from the interaction graph is unreliable due to sparsity, particularly in cold-start scenarios
- The warm-up strategy assumes initial learning from the original social graph is beneficial, though this assumption may not hold for all backbone models or datasets

## Confidence

- High confidence in the core mechanism: Graph rewiring to align social relations with user preferences based on similarity from interaction patterns
- Medium confidence in the joint learning effectiveness: While the framework claims improvements, the specific contribution of each component (SGR vs HRA) to the overall performance gain requires further ablation studies
- Medium confidence in generalization: The framework shows consistent improvements across three datasets, but performance on datasets with very different characteristics (e.g., higher initial homophily) remains untested

## Next Checks

1. Conduct ablation studies on each component (SGR without HRA, HRA without SGR) to quantify individual contributions to performance gains
2. Test SHaRe's effectiveness on a dataset with inherently high preference-aware homophily to determine if the framework provides benefits beyond datasets with low homophily
3. Evaluate the framework's sensitivity to the warm-up strategy by comparing performance when rewiring starts from epoch 1 versus epoch 10 across different backbone models