---
ver: rpa2
title: 'LightHouse: A Survey of AGI Hallucination'
arxiv_id: '2401.06792'
source_url: https://arxiv.org/abs/2401.06792
tags:
- arxiv
- language
- large
- hallucination
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive survey of hallucinations in
  Artificial General Intelligence (AGI), categorizing them into three types: Conflict
  in Intrinsic Knowledge of Models, Factual Conflict in Information Forgetting and
  Updating, and Conflict in Multimodal Fusion. It examines the emergence of hallucinations
  due to training data distribution, timeliness of information, and ambiguity in different
  modalities.'
---

# LightHouse: A Survey of AGI Hallucination

## Quick Facts
- arXiv ID: 2401.06792
- Source URL: https://arxiv.org/abs/2401.06792
- Reference count: 18
- Key outcome: Comprehensive survey categorizing AGI hallucinations into three types and providing mitigation strategies across data preparation, training, and inference stages.

## Executive Summary
This survey provides a systematic examination of hallucinations in Artificial General Intelligence systems, identifying three primary categories: conflicts arising from intrinsic knowledge, factual conflicts due to knowledge forgetting and updating failures, and multimodal fusion conflicts. The paper explores how hallucinations emerge from training data distribution shifts, knowledge base staleness, and modality integration challenges. It reviews mitigation strategies including reinforcement learning with human feedback, fine-tuning approaches, and post-hoc explanations, while evaluating hallucination detection methods through rule-based, large model-based, and human-based approaches.

## Method Summary
The survey synthesizes existing research on AGI hallucinations through comprehensive literature review, organizing findings into a structured framework covering hallucination categorization, emergence mechanisms, mitigation strategies, and evaluation methods. The methodology involves analyzing the relationships between training data distribution, knowledge retention, multimodal integration, and hallucination occurrence, while examining various intervention points across the model development pipeline.

## Key Results
- Identified three distinct types of AGI hallucinations: intrinsic knowledge conflicts, factual conflicts from information forgetting/updating, and multimodal fusion conflicts
- Mapped hallucination emergence to training data distribution shifts, knowledge base timeliness issues, and modality integration challenges
- Catalogued mitigation strategies across data preparation, model training, and inference/post-processing stages
- Evaluated hallucination detection approaches including rule-based, large model-based, and human-based methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hallucinations arise when the model's output distribution diverges from the training data distribution, causing conflicts with intrinsic knowledge.
- Mechanism: Training data distribution shifts alter the model's learned representations, leading to outputs that do not match real-world facts or prompt expectations.
- Core assumption: The model's generalization ability is sensitive to the alignment between training data distribution and real-world data distribution.
- Evidence anchors:
  - [abstract] "extensive research (Shen et al., 2021) indicates that there is a bias between the output distribution of the models and the distribution of the training data itself."
  - [section 3.1] "The generalization ability of the model such as overfitting and underfitting to specific data distributions can adversely impact model function, occasionally leading to hallucinations in the model's outputs."
  - [corpus] Weak evidence: No corpus papers directly address training data distribution's impact on hallucination; must infer from broader ML generalization literature.

### Mechanism 2
- Claim: Factual conflicts occur when models fail to update their knowledge base with new information, leading to outdated outputs.
- Mechanism: Models "forget" previously learned facts or fail to incorporate new information during fine-tuning, causing factual inconsistencies.
- Core assumption: Models have limited ability to retain and update knowledge similar to human short-term and long-term memory systems.
- Evidence anchors:
  - [abstract] "Factual Conflict primarily arises when models fail to retain previously acquired factual knowledge and are unable to assimilate new information."
  - [section 3.2] "Lin et al. (2023b) has highlighted specific instances where foundational models, during the fine-tuning process, tend to sacrifice their general applicability in favor of becoming more specialized for particular tasks."
  - [corpus] Weak evidence: No corpus papers specifically address knowledge updating failures; inference based on catastrophic forgetting literature.

### Mechanism 3
- Claim: Multimodal fusion conflicts occur when different modality representations interact incorrectly, causing hallucinations.
- Mechanism: Integration of different modality features through adapter methods can introduce errors from pre-training stages of different modalities, leading to incorrect outputs.
- Core assumption: Modality integration through simple fusion methods is insufficient for complex multimodal understanding.
- Evidence anchors:
  - [abstract] "Many existing multimodal large models integrate information from different modalities using an adapter method... The fusion at the hidden state can easily induce errors from the pre-training stage of different modalities."
  - [section 2.3] "In the realm of audio-language models, the issue of 'where is speaking part' emerges as a notable challenge, which can be categorized under this type of hallucination."
  - [corpus] Weak evidence: No corpus papers directly address multimodal fusion conflicts; inference from multimodal model architecture literature.

## Foundational Learning

- Concept: Distribution shift and generalization
  - Why needed here: Understanding how training data distribution affects model outputs is fundamental to grasping why hallucinations occur.
  - Quick check question: If a model is trained on data from 2020, what happens when asked about events in 2024?

- Concept: Knowledge retention and catastrophic forgetting
  - Why needed here: Models must balance between retaining old knowledge and learning new information, similar to human memory systems.
  - Quick check question: Why does fine-tuning a model on specialized data sometimes hurt performance on general tasks?

- Concept: Multimodal representation learning
  - Why needed here: Different modalities have different feature spaces and integration methods that can introduce conflicts.
  - Quick check question: Why might an image-text model confuse text in an image with knowledge from the accompanying text?

## Architecture Onboarding

- Component map: Definition -> Emergence -> Mitigation -> Evaluation -> Discourse -> Future Outlook
- Critical path: Understanding hallucination types (section 2) -> recognizing emergence causes (section 3) -> implementing mitigation strategies (section 4) -> evaluating effectiveness (section 5) -> discussing broader implications (section 6)
- Design tradeoffs: Comprehensive coverage vs. depth in each area; balancing technical detail with accessibility for different audiences
- Failure signatures: Confusion between hallucination types, overlooking emergence causes, focusing only on one mitigation stage, neglecting evaluation methods
- First 3 experiments:
  1. Reproduce the hallucination categorization from section 2 using a small set of multimodal model outputs.
  2. Test knowledge updating by fine-tuning a model on new information and measuring retention of old knowledge.
  3. Implement a simple multimodal fusion method and observe how modality conflicts create hallucinations.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can AGI models be designed to effectively balance hallucinations and creativity in their outputs?
- Basis in paper: [explicit] The paper discusses the challenge of balancing hallucination and creativity in model outputs, particularly in the context of AGI.
- Why unresolved: The paper acknowledges the importance of this balance but does not provide a clear methodology for achieving it. The trade-off between hallucination and creativity is complex and context-dependent, requiring further research.
- What evidence would resolve it: Development of a framework or set of guidelines that AGI models can use to determine when to prioritize creativity over accuracy, and vice versa, would provide a concrete solution to this issue.

### Open Question 2
- Question: What are the most effective strategies for updating AGI models' knowledge bases to prevent factual conflicts while minimizing catastrophic forgetting?
- Basis in paper: [explicit] The paper highlights the problem of factual conflicts due to outdated information and catastrophic forgetting in AGI models.
- Why unresolved: While the paper mentions the importance of knowledge updating, it does not provide specific strategies for achieving this without losing previously learned information.
- What evidence would resolve it: Empirical studies demonstrating the effectiveness of different knowledge updating techniques, such as elastic weight consolidation or learning without forgetting, in maintaining both new and old information in AGI models.

### Open Question 3
- Question: How can AGI models be evaluated for hallucinations in multimodal contexts, and what metrics are most appropriate for assessing the severity and impact of these hallucinations?
- Basis in paper: [explicit] The paper discusses the complexity of hallucinations in multimodal AGI and the need for effective evaluation methods.
- Why unresolved: The paper identifies the challenge of evaluating hallucinations in multimodal contexts but does not propose specific evaluation metrics or methodologies.
- What evidence would resolve it: Development and validation of a comprehensive evaluation framework that includes both quantitative and qualitative metrics for assessing hallucinations in AGI models across different modalities, such as language, vision, and audio.

## Limitations

- The causal mechanisms explaining hallucination emergence rely heavily on inference from related domains rather than direct AGI-specific validation
- Weak corpus evidence for many claims, particularly regarding training data distribution impacts and multimodal fusion conflicts
- Limited empirical validation of proposed hallucination categorization framework

## Confidence

- High Confidence: The survey's categorization framework and systematic organization of existing hallucination research is well-structured and comprehensive.
- Medium Confidence: The identification of mitigation strategies (RLHF, fine-tuning, post-hoc explanations) is supported by multiple sources, though effectiveness varies by context.
- Low Confidence: The causal mechanisms explaining why hallucinations emerge (distribution shifts, knowledge updating failures, multimodal fusion conflicts) rely on inference from related domains with limited AGI-specific validation.

## Next Checks

1. Empirical validation of hallucination types: Conduct controlled experiments testing whether the three proposed hallucination categories (intrinsic knowledge conflicts, factual conflicts, multimodal fusion conflicts) can be reliably distinguished and reproduced in current multimodal models.

2. Distribution shift impact measurement: Design experiments comparing hallucination rates when models are trained on time-bound datasets versus continuously updated data, directly testing the training data distribution hypothesis.

3. Knowledge retention benchmarking: Implement systematic fine-tuning experiments to quantify how knowledge specialization affects hallucination frequency on both in-domain and out-of-domain tasks, validating the claimed tradeoff between specialization and general applicability.