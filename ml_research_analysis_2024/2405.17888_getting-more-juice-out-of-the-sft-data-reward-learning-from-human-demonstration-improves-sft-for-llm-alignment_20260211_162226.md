---
ver: rpa2
title: 'Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration
  Improves SFT for LLM Alignment'
arxiv_id: '2405.17888'
source_url: https://arxiv.org/abs/2405.17888
tags:
- reward
- data
- learning
- algorithm
- spin
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new approach to fine-tuning large language
  models (LLMs) using Inverse Reinforcement Learning (IRL) to learn a reward model
  from demonstration data, which improves alignment over standard supervised fine-tuning.
  Instead of directly imitating human demonstrations, the method jointly learns a
  reward function and a policy by framing the problem as a maximum likelihood IRL
  formulation.
---

# Getting More Juice Out of the SFT Data: Reward Learning from Human Demonstration Improves SFT for LLM Alignment

## Quick Facts
- arXiv ID: 2405.17888
- Source URL: https://arxiv.org/abs/2405.17888
- Reference count: 40
- Primary result: Reward-learning fine-tuning methods outperform standard SFT, improving average scores from 59.47% to 61.03% on the Open LLM Leaderboard

## Executive Summary
This paper introduces a novel approach to fine-tuning large language models using Inverse Reinforcement Learning (IRL) to learn reward models from demonstration data, improving alignment beyond standard supervised fine-tuning. The method frames the problem as jointly learning a reward function and policy model through maximum likelihood IRL, with two proposed algorithms: explicit Reward-learning Fine-tune (RFT) and Implicit Reward-learning Fine-tune (IRFT). The IRFT algorithm connects to recent self-play fine-tuning methods like SPIN. Empirically, both 1B and 7B models fine-tuned with these methods show consistent improvements across multiple benchmarks, demonstrating the effectiveness of learning reward functions from human demonstrations.

## Method Summary
The paper proposes two algorithms for reward-learning fine-tuning: RFT (explicit reward learning) and IRFT (implicit reward learning). Both methods leverage IRL techniques to jointly learn a reward model and a policy model from demonstration data. The algorithms operate by generating responses, collecting rewards, and updating both the reward model and policy model iteratively. The IRFT algorithm is particularly noteworthy as it connects to self-play fine-tuning methods. The training procedure involves setting up the environment with DeepSpeed ZeRO-3, FlashAttention-2, and TRL package, preparing datasets (Anthropic-HH for pythia models and UltraChat200k for zephyr models), and training for 2 epochs after each generation process.

## Key Results
- Average scores improved from 59.47% to 61.03% on HuggingFace's Open LLM Leaderboard
- Both 1B and 7B parameter models showed consistent performance gains over standard SFT
- RFT and IRFT algorithms demonstrated convergence to stationary points of the IRL objective

## Why This Works (Mechanism)
The approach works by framing the alignment problem as an IRL task where the model learns to infer the underlying reward function from human demonstrations rather than directly imitating them. This allows the model to capture the implicit preferences and values embedded in the demonstration data. By jointly optimizing the reward model and policy, the method creates a feedback loop that progressively improves both components. The implicit version (IRFT) is particularly effective as it learns the reward function through the optimization dynamics themselves, similar to how self-play methods discover optimal strategies.

## Foundational Learning
- **Inverse Reinforcement Learning (IRL)**: Learning reward functions from expert demonstrations; needed to capture implicit preferences beyond direct imitation; quick check: verify the IRL objective is correctly formulated
- **Maximum Likelihood IRL**: Formulating the problem as maximizing the likelihood of observed demonstrations under the learned reward model; needed for principled optimization; quick check: confirm the likelihood function is properly defined
- **Self-Play Fine-tuning**: Iterative generation and evaluation process; needed for the connection between IRFT and methods like SPIN; quick check: verify the generation frequency parameter is appropriately set

## Architecture Onboarding
- **Component Map**: Human Demonstrations -> IRL Algorithm (RFT/IRFT) -> Reward Model + Policy Model -> Fine-tuned LLM
- **Critical Path**: Data preparation → Algorithm implementation → Model training → Evaluation
- **Design Tradeoffs**: Explicit reward learning (RFT) vs. implicit reward learning (IRFT); explicit provides interpretability while implicit may be more stable
- **Failure Signatures**: Non-convergence if generation frequency is too high/low; memory issues with larger models; poor performance if reward model doesn't capture demonstration patterns
- **First Experiments**: 1) Test RFT with a simple synthetic dataset to verify basic functionality; 2) Compare convergence patterns between RFT and IRFT on the same data; 3) Evaluate the impact of generation frequency on final performance

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis assumes infinite capacity models, which may not hold in practice
- Empirical improvements are modest (approximately 2 percentage points)
- Performance on non-English languages and specialized domains remains unexplored

## Confidence
- High confidence in algorithmic correctness of RFT and IRFT formulations
- Medium confidence in empirical results due to potential benchmark overfitting
- Low confidence in generalization to models outside 1B-7B parameter range

## Next Checks
1. Conduct ablation studies varying generation frequency T and comparing convergence between RFT and IRFT
2. Test proposed methods on broader range of model sizes to verify scalability
3. Evaluate performance on out-of-distribution datasets not included in original benchmark suite