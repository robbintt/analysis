---
ver: rpa2
title: 'Perceiver-Prompt: Flexible Speaker Adaptation in Whisper for Chinese Disordered
  Speech Recognition'
arxiv_id: '2406.09873'
source_url: https://arxiv.org/abs/2406.09873
tags:
- speech
- speaker
- recognition
- whisper
- dysarthric
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of recognizing Chinese dysarthric
  speech by introducing Perceiver-Prompt, a speaker adaptation method for the Whisper
  model using P-Tuning. The method first fine-tunes Whisper using LoRA, then employs
  a trainable Perceiver to generate fixed-length speaker prompts from variable-length
  inputs, improving model recognition of Chinese dysarthric speech.
---

# Perceiver-Prompt: Flexible Speaker Adaptation in Whisper for Chinese Disordered Speech Recognition

## Quick Facts
- arXiv ID: 2406.09873
- Source URL: https://arxiv.org/abs/2406.09873
- Reference count: 0
- Primary result: Achieves 13.04% relative CER reduction over fine-tuned Whisper on Chinese dysarthric speech

## Executive Summary
This paper addresses the challenge of recognizing Chinese dysarthric speech by introducing Perceiver-Prompt, a speaker adaptation method for the Whisper model using P-Tuning. The method first fine-tunes Whisper using LoRA, then employs a trainable Perceiver to generate fixed-length speaker prompts from variable-length inputs, improving model recognition of Chinese dysarthric speech. Experiments on a Chinese dysarthric speech dataset demonstrate consistent improvements in recognition performance with Perceiver-Prompt, achieving a relative reduction of up to 13.04% in Character Error Rate (CER) over the fine-tuned Whisper.

## Method Summary
The proposed method consists of two main stages: first, Whisper is fine-tuned on dysarthric speech data using LoRA for efficient parameter adaptation; second, a trainable Perceiver is used with P-Tuning to generate fixed-length speaker prompts from variable-length historical utterances. These prompts are concatenated with input embeddings during inference to provide speaker-specific context. The approach is evaluated on a Chinese dysarthric speech dataset with patients exhibiting different severity levels of dysarthria, showing significant improvements especially for severe cases.

## Key Results
- Achieves 13.04% relative CER reduction over fine-tuned Whisper on Chinese dysarthric speech
- On most severe dysarthric speech (FDA severity F.1), achieves 51.38% relative CER reduction
- Lowest CER of 6.0% achieved using 5 historical utterances per speaker configuration
- Consistently outperforms baseline Whisper across different dysarthria severity levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Perceiver-Prompt generates fixed-length speaker embeddings from variable-length dysarthric utterances, enabling speaker adaptation in Whisper.
- Mechanism: The trainable Perceiver encoder maps a sequence of variable-length historical utterances from the same speaker into a fixed-length prompt vector. This prompt is concatenated with the Whisper input embeddings, providing speaker-specific context to the model.
- Core assumption: Dysarthric speech from the same speaker shares consistent articulatory characteristics that can be captured by a fixed-length embedding, even if individual utterances vary in length.
- Evidence anchors:
  - [abstract] "integrate a trainable Perceiver to generate fixed-length speaker prompts from variable-length inputs"
  - [section] "The Perceiver-Prompt is a sequence of vectors derived by a P-tuning trained Perceiver using data from the same speaker"

### Mechanism 2
- Claim: P-Tuning allows efficient fine-tuning of the Perceiver-Prompt without modifying the large Whisper model parameters.
- Mechanism: By freezing Whisper and training only the Perceiver and its linear transformation, the method achieves speaker adaptation with minimal parameter changes. This reduces computational cost and avoids catastrophic forgetting.
- Core assumption: The large-scale Whisper model retains general speech representations that can be adapted with small speaker-specific modifications rather than full fine-tuning.
- Evidence anchors:
  - [section] "we employ LoRA to fine-tune the medium-sized Whisper model" and "Subsequently, parameters of the Perceiver-Prompt is trained with P-tuning by fixing the parameters of Whisper"

### Mechanism 3
- Claim: Using multiple historical utterances per speaker improves the quality of the speaker prompt and adaptation performance.
- Mechanism: By concatenating several past utterances from the same speaker as input to the Perceiver, the model captures a more stable and representative speaker profile, which leads to better adaptation especially for severe dysarthric cases.
- Core assumption: Dysarthric speakers exhibit consistent patterns across multiple utterances that can be aggregated into a robust speaker representation.
- Evidence anchors:
  - [section] "For instance, employing 5 historical utterances from the same speaker (Conf.8) to generate Speaker Prompt achieves the lowest 5.3% CER for the most severe F.1"
  - [section] "incorporation of more historical information enhances speaker discrimination"

## Foundational Learning

- Concept: **Low-Rank Adaptation (LoRA)**
  - Why needed here: LoRA allows efficient fine-tuning of the large Whisper model on limited dysarthric speech data without full parameter updates, reducing computational cost.
  - Quick check question: How does LoRA modify model parameters during fine-tuning, and why is this more efficient than full fine-tuning?

- Concept: **P-Tuning with Continuous Prompts**
  - Why needed here: P-Tuning replaces discrete prompt engineering with trainable continuous embeddings, enabling flexible and data-driven speaker prompt generation.
  - Quick check question: What is the role of the Prompt Encoder in P-Tuning, and how does it differ from manual prompt design?

- Concept: **Perceiver Architecture**
  - Why needed here: The Perceiver can handle variable-length input sequences and produce fixed-length outputs, making it ideal for generating speaker prompts from variable-length dysarthric utterances.
  - Quick check question: How does the Perceiver maintain permutation invariance while producing fixed-length outputs from sequences?

## Architecture Onboarding

- Component map:
  Whisper (frozen base model) -> LoRA adapters (fine-tuned) -> Perceiver (speaker prompt generator) -> Linear transform (maps output to embedding space) -> Prompt concatenation layer (adds to input embeddings)

- Critical path:
  1. Preprocess dysarthric utterances and resample to 16 kHz
  2. Fine-tune Whisper with LoRA on dysarthric dataset
  3. Train Perceiver-Prompt with P-Tuning using speaker-labeled data
  4. During inference, generate speaker prompt from target speaker's history
  5. Concatenate prompt and run through adapted Whisper

- Design tradeoffs:
  - Using fixed-length prompts simplifies integration but may lose fine-grained speaker details
  - Concatenating at input vs encoder block changes adaptation granularity
  - Joint training with FDA severity adds supervision but increases complexity

- Failure signatures:
  - High CER persists even after adaptation → prompts not capturing speaker characteristics
  - Training instability or divergence → incorrect prompt encoder configuration or learning rate
  - Poor generalization to new speakers → overfitting to training speaker prompts

- First 3 experiments:
  1. Validate baseline Whisper fine-tuning on dysarthric data without any speaker adaptation
  2. Test Perceiver-Prompt with single utterance per speaker and compare to baseline
  3. Vary the number of historical utterances used (1, 3, 5) and measure CER on severe dysarthric cases

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Perceiver-Prompt method perform when using speaker-specific prompts generated from a larger number of historical utterances (e.g., 10 or more)?
- Basis in paper: [inferred] The paper experimented with using up to 5 historical utterances, but the impact of using more is not explored.
- Why unresolved: The paper does not provide results for using a larger number of historical utterances, leaving the potential benefits of more extensive speaker-specific prompt generation unknown.
- What evidence would resolve it: Experiments comparing Perceiver-Prompt performance using different numbers of historical utterances (e.g., 1, 3, 5, 10, 15) would clarify the optimal number of utterances for generating effective speaker prompts.

### Open Question 2
- Question: Can the Perceiver-Prompt method be effectively applied to languages other than Chinese and English, particularly those with very different phonetic structures?
- Basis in paper: [explicit] The paper focuses on Chinese dysarthric speech recognition and uses Whisper, which is trained on multilingual data including Mandarin.
- Why unresolved: The paper does not investigate the method's effectiveness on other languages, especially those with significantly different phonetic structures from Chinese or English.
- What evidence would resolve it: Applying the Perceiver-Prompt method to dysarthric speech datasets in various languages (e.g., Arabic, Japanese, Swahili) and comparing its performance to the baseline would demonstrate its cross-linguistic applicability.

### Open Question 3
- Question: What is the impact of using different auxiliary tasks (e.g., speaker classification, FDA severity classification) in joint training on the Perceiver-Prompt's performance for dysarthric speech recognition?
- Basis in paper: [explicit] The paper mentions using FDA score regression and speaker classification as auxiliary tasks, but does not provide a detailed comparison of their effectiveness.
- Why unresolved: The paper only briefly mentions the use of auxiliary tasks without a comprehensive analysis of their impact on the Perceiver-Prompt's performance.
- What evidence would resolve it: Experiments comparing the Perceiver-Prompt's performance when trained with different auxiliary tasks (e.g., speaker classification, FDA severity classification, both) would clarify the most effective approach for joint training.

## Limitations

- Dataset Specificity: The method is evaluated only on a single Chinese dysarthric dataset, limiting generalizability claims to other languages or dysarthria types.
- Training Complexity: The two-stage training process (LoRA fine-tuning followed by P-Tuning) adds implementation complexity and may require careful hyperparameter tuning.
- Computational Overhead: While parameter-efficient, the Perceiver-Prompt generation adds inference-time computation that is not fully characterized.

## Confidence

- High Confidence: The basic methodology of using Perceiver-Prompt for speaker adaptation is technically sound and well-explained; the experimental setup and dataset descriptions are clear and reproducible; the relative CER improvements (13.04% overall, 51.38% for severe cases) are specifically quantified and reported.
- Medium Confidence: The effectiveness of P-Tuning for Whisper adaptation in dysarthric speech tasks, while plausible, lacks direct experimental validation within the paper; the choice of historical utterance count (5 utterances) as optimal is based on empirical results but the sensitivity analysis could be more thorough; the claim that this is the "first work on speaker adaptation for Chinese dysarthric speech recognition" requires verification of the broader literature.
- Low Confidence: The generalization capability to speakers outside the training set is not demonstrated; the computational efficiency gains from LoRA + Perceiver-Prompt versus full fine-tuning are not quantified; the ablation study on prompt length and dimensionality is incomplete.

## Next Checks

1. **Cross-Speaker Generalization Test**: Evaluate the model on speakers completely held out from training to assess true adaptation capability versus overfitting to training speakers.

2. **Ablation on Prompt Architecture**: Systematically vary the Perceiver output dimension and prompt length to identify the minimum effective configuration and understand the parameter sensitivity.

3. **Real-Time Performance Analysis**: Measure inference latency and computational requirements compared to baseline Whisper and full fine-tuning approaches to validate practical deployment feasibility.