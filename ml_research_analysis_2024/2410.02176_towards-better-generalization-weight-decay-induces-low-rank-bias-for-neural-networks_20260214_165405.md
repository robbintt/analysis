---
ver: rpa2
title: 'Towards Better Generalization: Weight Decay Induces Low-rank Bias for Neural
  Networks'
arxiv_id: '2410.02176'
source_url: https://arxiv.org/abs/2410.02176
tags:
- neural
- generalization
- rank
- batch
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how weight decay induces low-rank bias in neural
  networks. The authors prove that training a ReLU network with SGD and weight decay
  leads to weight matrices that are approximately rank-two (or rank-one with constant
  regularization).
---

# Towards Better Generalization: Weight Decay Induces Low-rank Bias for Neural Networks

## Quick Facts
- arXiv ID: 2410.02176
- Source URL: https://arxiv.org/abs/2410.02176
- Reference count: 40
- Proves weight decay induces low-rank bias in ReLU networks, improving generalization bounds

## Executive Summary
This paper establishes that weight decay acts as an implicit regularizer inducing low-rank structure in neural network weight matrices. The authors prove that training ReLU networks with SGD and weight decay produces weight matrices that are approximately rank-two (or rank-one with constant regularization). They demonstrate that this low-rank bias leads to improved generalization error bounds, reducing from O(√mn ln m ln N/N) to O(√((m+n) ln m ln N)/N). Empirical results on California housing regression and MNIST classification tasks validate the theory, showing that larger weight decay coefficients produce lower-rank weight matrices and better generalization performance.

## Method Summary
The authors analyze training of fully connected ReLU networks with SGD and weight decay regularization. They establish theoretical bounds showing that weight decay induces low-rank structure in weight matrices, specifically rank-two approximations for non-constant regularization and rank-one for constant regularization. The analysis leverages empirical risk minimization framework and establishes sample complexity bounds that improve when exploiting the induced low-rank structure. The theoretical analysis is complemented by experiments on simple regression and classification tasks to validate the low-rank bias phenomenon.

## Key Results
- Weight decay induces rank-two (or rank-one with constant regularization) structure in weight matrices
- Generalization error bounds improve from O(√mn ln m ln N/N) to O(√((m+n) ln m ln N)/N)
- Larger weight decay coefficients produce lower-rank weight matrices and better generalization
- Empirical validation on California housing and MNIST tasks supports theoretical predictions

## Why This Works (Mechanism)
Weight decay acts as an implicit regularizer that constrains the optimization landscape during training. By penalizing the magnitude of weights, it forces the network to find solutions in a lower-dimensional subspace, effectively inducing low-rank structure in the weight matrices. This reduced complexity directly translates to better generalization performance through the improved sample complexity bounds. The mechanism is particularly pronounced in ReLU networks due to their piecewise linear nature and the interaction between weight decay and the activation function's geometry.

## Foundational Learning

**Weight decay (L2 regularization)**: Adding λ||w||² to the loss function to penalize large weights. Needed to understand the core mechanism being studied. Quick check: Verify that weight decay term appears in the optimization objective.

**ReLU activation**: f(x) = max(0,x) activation function. Needed because the analysis specifically targets ReLU networks. Quick check: Confirm ReLU is used throughout the experiments.

**Empirical risk minimization (ERM)**: Minimizing training loss over a hypothesis class. Needed as the theoretical framework for generalization analysis. Quick check: Identify the ERM formulation in the theoretical section.

**Sample complexity bounds**: Bounds on the number of samples needed for generalization. Needed to quantify the improvement from low-rank bias. Quick check: Compare the pre- and post-improvement bounds in the results.

**Rank decomposition**: Expressing a matrix as a product of lower-rank matrices. Needed to understand how low-rank structure is characterized. Quick check: Verify the rank-two decomposition formula in the theory section.

## Architecture Onboarding

**Component map**: Data -> Fully connected ReLU network with weight decay -> Trained weights with low-rank structure -> Improved generalization

**Critical path**: Training with weight decay → Low-rank weight matrices → Improved sample complexity bounds → Better generalization performance

**Design tradeoffs**: The analysis trades mathematical tractability (rank-two approximation) for generality, focusing on ReLU networks while acknowledging limitations for other activation functions. The theoretical improvement comes at the cost of requiring specific weight decay coefficients.

**Failure signatures**: If the low-rank bias doesn't manifest, we would expect: (1) weight matrices maintaining high rank, (2) generalization bounds not improving as predicted, (3) weight decay not correlating with rank reduction in experiments.

**First experiments**: 
1. Train ReLU network on synthetic data with varying weight decay coefficients and measure weight matrix rank
2. Compare generalization performance between standard and low-rank regularized networks
3. Validate the rank-two approximation accuracy on trained weight matrices

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Analysis restricted to ReLU networks and does not extend to other activation functions
- Rank-two approximation assumption may be overly restrictive for deeper or more complex architectures
- Experimental validation limited to simple tasks (housing regression, MNIST) without testing on modern deep learning challenges

## Confidence

**High**: Theoretical analysis of weight decay inducing low-rank bias is mathematically sound given the assumptions; empirical validation on tested tasks supports the theoretical predictions.

**Medium**: Applicability to other network types beyond ReLU and generalization to more complex datasets and architectures.

**Low**: Claims about practical implications in real-world deep learning scenarios and industrial applications.

## Next Checks

1. Test the low-rank bias phenomenon on deeper networks (more than 2 layers) and modern architectures like ResNets or Transformers

2. Validate the theory on more challenging datasets like CIFAR-10/100 or ImageNet

3. Investigate whether the low-rank bias persists when using different activation functions beyond ReLU