---
ver: rpa2
title: 'TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese'
arxiv_id: '2401.16640'
source_url: https://arxiv.org/abs/2401.16640
tags:
- arxiv
- language
- training
- preprint
- portuguese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces TeenyTinyLlama, a pair of compact language
  models (160M and 460M parameters) trained from scratch in Brazilian Portuguese.
  The models were developed under a limited computational budget of 500 USD and trained
  on a dataset of approximately 6.2 billion tokens, including web data and instruction-following
  demonstrations.
---

# TeenyTinyLlama: open-source tiny language models trained in Brazilian Portuguese

## Quick Facts
- arXiv ID: 2401.16640
- Source URL: https://arxiv.org/abs/2401.16640
- Reference count: 14
- Primary result: Developed 160M and 460M parameter Brazilian Portuguese models under $500 budget

## Executive Summary
This study introduces TeenyTinyLlama, a pair of compact language models specifically trained for Brazilian Portuguese. The models were developed under strict computational constraints, utilizing a Llama 2-based architecture and custom tokenizer trained on 6.2 billion tokens of web data and instruction demonstrations. The resulting models achieve perplexity scores comparable to larger models while consuming minimal energy during training.

## Method Summary
The authors developed two language models (160M and 460M parameters) using a Llama 2-based architecture with custom tokenizer optimization for Brazilian Portuguese. Training was conducted on a dataset of approximately 6.2 billion tokens, including web data and instruction-following demonstrations, with a total computational budget of $500. The models were evaluated across multiple multilingual benchmarks and downstream tasks, demonstrating competitive performance particularly in the ARC-Challenge where the 460M model outperformed other models of similar size.

## Key Results
- Achieved perplexity scores comparable to larger models despite smaller size
- 460M model outperformed other models in ARC-Challenge benchmark
- Total energy consumption equivalent to 185-kilometer car ride
- All components released under Apache 2.0 license with public access to code and datasets

## Why This Works (Mechanism)
The models' success stems from efficient architecture design optimized for Brazilian Portuguese, careful dataset curation balancing web data and instruction demonstrations, and strategic resource allocation within the computational budget constraint. The custom tokenizer enables better tokenization of Portuguese-specific linguistic features, while the focused training approach allows for effective knowledge transfer within the target language domain.

## Foundational Learning

**Language Model Architecture**: Understanding transformer-based architectures is essential for comprehending how these models process sequential data and generate text. Quick check: Can you explain the difference between encoder-only, decoder-only, and encoder-decoder architectures?

**Tokenization**: Critical for efficient text processing and vocabulary management. Quick check: How does byte-pair encoding work in tokenizer design?

**Perplexity**: Key metric for evaluating language model quality. Quick check: What does a lower perplexity score indicate about model performance?

**Computational Efficiency**: Important for understanding trade-offs in model size versus performance. Quick check: How does model size impact inference speed and memory requirements?

**Benchmark Evaluation**: Essential for assessing model capabilities across different tasks. Quick check: What are the key differences between GLUE, SuperGLUE, and other language understanding benchmarks?

## Architecture Onboarding

Component Map: Data Collection -> Tokenizer Training -> Model Architecture Design -> Training -> Evaluation -> Release

Critical Path: Data Collection → Tokenizer Training → Model Architecture Design → Training → Evaluation

Design Tradeoffs: The study prioritized computational efficiency over maximum parameter count, opting for smaller models that could be trained within budget constraints while maintaining competitive performance.

Failure Signatures: Potential issues include overfitting on limited training data, suboptimal tokenization for rare Portuguese words, and benchmark-specific performance limitations.

First Experiments:
1. Verify tokenizer performance on Portuguese text segmentation tasks
2. Test basic text generation capabilities with temperature scaling
3. Evaluate perplexity on held-out validation data

## Open Questions the Paper Calls Out
None

## Limitations
- Computational budget constraint may have limited hyperparameter optimization
- Dataset documentation lacks detail about domain balance and potential biases
- Energy consumption estimates rely on proxy calculations rather than direct measurements

## Confidence

| Claim Type | Confidence Level |
|------------|------------------|
| Model Architecture and Training Methodology | High |
| Performance Comparisons | Medium |
| Broader Claims about Model Superiority | Low |

## Next Checks

1. Conduct human evaluation studies comparing model outputs across multiple Brazilian Portuguese tasks
2. Perform ablation studies to determine optimal model size for specific downstream applications
3. Validate energy consumption estimates through direct power monitoring during training