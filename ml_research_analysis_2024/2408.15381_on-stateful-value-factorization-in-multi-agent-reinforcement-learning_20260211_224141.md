---
ver: rpa2
title: On Stateful Value Factorization in Multi-Agent Reinforcement Learning
arxiv_id: '2408.15381'
source_url: https://arxiv.org/abs/2408.15381
tags:
- state
- qmix
- value
- joint
- qplex
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies a theoretical gap between stateless theory\
  \ and stateful practice in value factorization methods for MARL. It proves that\
  \ using state in QMIX and QPLEX does not bias action selection, but QPLEX\u2019\
  s stateful implementation loses full expressiveness."
---

# On Stateful Value Factorization in Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2408.15381
- Source URL: https://arxiv.org/abs/2408.15381
- Authors: Enrico Marchesini; Andrea Baisero; Rupali Bhati; Christopher Amato
- Reference count: 30
- Key finding: Stateful QMIX/QPLEX does not bias action selection, but loses full IGM expressiveness; DuelMIX restores it and outperforms baselines in SMACLite and Box Pushing.

## Executive Summary
This paper investigates the impact of stateful information in value factorization methods for multi-agent reinforcement learning. The authors prove that incorporating state into QMIX and QPLEX does not bias action selection but can reduce the expressiveness of the factorization. To address this, they propose DuelMIX, which learns separate value and advantage streams per agent and combines them using a weighted history mixer. DuelMIX achieves full IGM expressiveness and demonstrates superior performance in Box Pushing and SMACLite benchmarks, while also revealing that random noise or zero vectors can serve as effective centralized information.

## Method Summary
The authors first analyze the theoretical implications of stateful versus stateless value factorization in MARL. They prove that using state in QMIX and QPLEX does not bias action selection, but QPLEX's stateful implementation loses full IGM expressiveness. To restore this expressiveness, they introduce DuelMIX, which decomposes the joint Q-value into per-agent value and advantage streams, combines them via a weighted history mixer, and aggregates them into a centralized value. This architecture maintains monotonicity for IGM while allowing full representational capacity. Empirically, DuelMIX is tested in Box Pushing and SMACLite, where it outperforms QMIX and QPLEX in returns, sample efficiency, and optimal coordination.

## Key Results
- DuelMIX restores full IGM expressiveness lost in stateful QPLEX by learning separate value and advantage streams.
- In Box Pushing, DuelMIX learns optimal coordination where QMIX and QPLEX fail.
- In SMACLite, DuelMIX achieves higher returns and better sample efficiency than QMIX and QPLEX.
- Surprisingly, random noise or zero vectors as centralized information perform comparably or better than state.

## Why This Works (Mechanism)
DuelMIX works by explicitly separating per-agent value and advantage streams, which allows the architecture to fully represent the joint action space while maintaining monotonicity for IGM. The weighted history mixer aggregates these streams in a way that preserves the expressiveness lost in stateful QPLEX. By not relying on state for centralized information, DuelMIX avoids the representational bottleneck and enables more effective coordination in multi-agent tasks.

## Foundational Learning
- **IGM (Individual-Global-Max)**: Ensures that the optimal joint action can be recovered by selecting each agent's best action independently. Needed to guarantee that the factorization supports decentralized execution. Quick check: Verify that argmax_a Q_tot(a) = sum_i argmax_{a_i} Q_i(a_i).
- **Value-Advantage Decomposition**: Separates the Q-value into a state-independent value and a state-dependent advantage. Needed to enable full expressiveness while maintaining monotonicity. Quick check: Confirm that Q = V + A and that A is zero-mean.
- **Monotonicity in QMIX/QPLEX**: Ensures that the joint Q-value is monotonic in each agent's Q-value, enabling IGM. Needed for compatibility with decentralized execution. Quick check: Verify that ∂Q_tot/∂Q_i ≥ 0 for all i.
- **Stateful vs. Stateless Factorization**: Stateful methods incorporate global state into the mixing network, potentially reducing expressiveness. Needed to understand the trade-off between representational capacity and coordination. Quick check: Compare the representational power of stateful and stateless mixers on simple coordination tasks.
- **Weighted History Mixer**: Aggregates value and advantage streams using learned weights over time. Needed to maintain full expressiveness while allowing adaptive coordination. Quick check: Ensure that the mixer weights are bounded and do not cause instability.

## Architecture Onboarding
- **Component Map**: Agent Q-networks -> Value & Advantage Streams -> Weighted History Mixer -> Centralized Q-value -> Action Selection
- **Critical Path**: Agent observations → Q-networks → value/advantage streams → mixer → joint Q → greedy action selection
- **Design Tradeoffs**: DuelMIX trades increased model complexity (separate streams, history mixer) for full IGM expressiveness and better coordination. The use of stateful information is optional and can be replaced with noise or zeros without loss of performance.
- **Failure Signatures**: Poor coordination in tasks requiring complex joint action selection; instability in learned mixer weights; overfitting to training environments.
- **First Experiments**:
  1. Verify that DuelMIX recovers optimal joint actions in a simple coordination task (e.g., Box Pushing).
  2. Test the impact of replacing state with noise or zeros in the mixer on coordination performance.
  3. Compare sample efficiency and final returns of DuelMIX against QMIX and QPLEX in SMACLite.

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical claims are well-supported, but empirical validation is limited to small-scale environments.
- Results are confined to relatively simple tasks; scalability to complex, high-dimensional domains remains unproven.
- The finding that noise or zeros can replace state raises questions about the practical value of stateful information, but ablation studies are insufficient to fully explain this phenomenon.

## Confidence
- High: Theoretical claims regarding bias in stateful QMIX/QPLEX are well-supported by proofs.
- Medium: Empirical results show strong performance, but are limited in scope and complexity.
- Low: Broader implications for complex domains and the role of stateful information are not fully established.

## Next Checks
1. Test DuelMIX in large-scale SMAC maps and StarCraft II micromanagement tasks to assess scalability and real-world applicability.
2. Conduct ablation studies isolating the impact of stateful vs. stateless centralized information across diverse environment complexities.
3. Evaluate DuelMIX's robustness to noise and sparse rewards in cooperative multi-agent settings to verify stability of learned value-advantage decomposition.