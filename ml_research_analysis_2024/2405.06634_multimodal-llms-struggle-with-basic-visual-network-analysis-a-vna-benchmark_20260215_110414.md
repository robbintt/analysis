---
ver: rpa2
title: 'Multimodal LLMs Struggle with Basic Visual Network Analysis: a VNA Benchmark'
arxiv_id: '2405.06634'
source_url: https://arxiv.org/abs/2405.06634
tags:
- graph
- tasks
- degree
- graphs
- llava
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces the first benchmark for zero-shot Visual
  Network Analysis (VNA), evaluating GPT-4 and LLaVa on 5 tasks related to core graph
  theory concepts: identifying nodes of maximal degree, determining structural balance
  of signed triads, and counting graph components. The tasks are designed to be solvable
  by counting specific elements within graph visualizations.'
---

# Multimodal LLMs Struggle with Basic Visual Network Analysis: a VNA Benchmark

## Quick Facts
- arXiv ID: 2405.06634
- Source URL: https://arxiv.org/abs/2405.06634
- Reference count: 17
- Key outcome: VLMs achieve only 51-67% accuracy on basic graph analysis tasks, with LLaVa often fabricating graph descriptions

## Executive Summary
This paper introduces the first zero-shot benchmark for Visual Network Analysis (VNA), evaluating GPT-4 and LLaVa on fundamental graph theory tasks. The benchmark tests models' ability to identify nodes of maximal degree, determine structural balance of signed triads, and count graph components through visual analysis. Despite human-level performance on professional exams, both models struggle significantly with basic network visualization tasks. GPT-4 achieves highest accuracy of 67% on isolate counting and 51% on structural balance tasks, while LLaVa performs particularly poorly, often generating fabricated graph descriptions rather than analyzing actual visual content.

## Method Summary
The researchers created a benchmark of 500 graph visualization images testing five core VNA tasks: identifying nodes of maximal degree, determining structural balance of signed triads, counting graph components, and related counting problems. Both GPT-4 and LLaVa were evaluated using zero-shot prompting without task-specific fine-tuning. Performance was measured using accuracy metrics and similarity scores comparing model responses to ground truth answers. The evaluation focused on simple graph structures designed to be solvable through basic counting of visual elements.

## Key Results
- GPT-4 achieved highest accuracy of 67% on isolate counting tasks
- GPT-4 scored only 51% on structural balance determination
- LLaVa frequently fabricated graph descriptions rather than analyzing visual content
- Both models showed significant difficulty with basic graph theory concepts despite strong performance on professional exams

## Why This Works (Mechanism)
The benchmark reveals fundamental limitations in how current VLMs process visual network information. The zero-shot evaluation approach demonstrates that models lack implicit graph theory knowledge needed for visual network analysis. Performance degradation occurs when models must translate visual graph representations into structural understanding, suggesting current architectures struggle with visual-symbolic reasoning for network concepts.

## Foundational Learning

**Graph Theory Fundamentals**: Understanding nodes, edges, degree, components, and signed networks - needed to interpret what the tasks are measuring and why certain visual patterns matter; quick check: can describe the difference between connected and disconnected graphs.

**Visual Network Analysis**: Process of extracting structural information from graph visualizations - needed to understand the benchmark's scope and methodology; quick check: can explain how to count components in a simple graph image.

**Multimodal Model Architecture**: Integration of vision and language processing - needed to contextualize model capabilities and limitations; quick check: can describe how vision-language models differ from pure text models.

## Architecture Onboarding

**Component Map**: Input Image -> Vision Encoder -> Fusion Module -> Language Model -> Output Response

**Critical Path**: Vision encoder processes visual features, which are then fused with language context through cross-attention mechanisms before being processed by the language model for final response generation.

**Design Tradeoffs**: Zero-shot evaluation vs. fine-tuned performance, simple vs. complex graph structures, automated vs. human evaluation methods.

**Failure Signatures**: Fabricated graph descriptions, incorrect counting of visual elements, failure to recognize basic structural patterns, inconsistent responses to similar visual inputs.

**First Experiments**:
1. Test model performance on progressively more complex graph structures
2. Evaluate impact of few-shot prompting on task performance
3. Compare automated metrics with human expert evaluation

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark only tests 500 images across 5 tasks, potentially missing real-world complexity
- Zero-shot evaluation may underestimate models' true capabilities with appropriate prompting
- Only two models tested, limiting generalizability to broader VLM landscape
- Focus on simple graph structures may not reflect practical network analysis needs

## Confidence

**High Confidence** - Both GPT-4 and LLaVa struggle with basic visual network analysis tasks is well-supported by systematic evaluation showing clear performance gaps.

**Medium Confidence** - VLMs struggle specifically with graph theory concepts requires more nuanced interpretation as underlying cause is not definitively established.

**Low Confidence** - Assertion that models "fail to learn basic visual graph analysis skills" may be premature given zero-shot evaluation limitations.

## Next Checks

1. Conduct human evaluation study where domain experts manually review model outputs to identify error sources.

2. Test additional VLMs including LLaVa-NeXT, Qwen-VL, and Claude-3 to establish performance consistency.

3. Implement prompt engineering experiments with few-shot examples and chain-of-thought prompting to distinguish capability limitations from evaluation methodology constraints.