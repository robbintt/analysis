---
ver: rpa2
title: What Makes An Expert? Reviewing How ML Researchers Define "Expert"
arxiv_id: '2411.00179'
source_url: https://arxiv.org/abs/2411.00179
tags:
- knowledge
- experts
- expertise
- expert
- development
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically reviewed 112 ML publications to understand
  how researchers define and engage "experts" in ML development. The authors found
  that expertise is often left undefined and is rarely sought outside of formal education
  and professional certification.
---

# What Makes An Expert? Reviewing How ML Researchers Define "Expert"

## Quick Facts
- **arXiv ID**: 2411.00179
- **Source URL**: https://arxiv.org/abs/2411.00179
- **Authors**: Mark DÃ­az; Angela DR Smith
- **Reference count**: 24
- **Primary result**: ML researchers rarely define expertise and primarily engage experts for data annotation, often overlooking experiential and marginalized knowledge forms.

## Executive Summary
This systematic review of 112 ML publications reveals that researchers frequently leave "expertise" undefined while predominantly engaging experts for data annotation, validation, and end-user evaluations. The study found that formal education and professional certification are the most commonly cited expertise criteria, with experiential and marginalized knowledge forms rarely sought. Experts are largely excluded from algorithm design and problem formulation stages, reflecting deskilling practices where complex expertise is reduced to modular, decontextualized knowledge contributions.

## Method Summary
The researchers conducted a systematic review using keyword searches on dblp.org across top ML venues (NeurIPS, ICML, AAAI, CHI, FAccT) to identify papers mentioning "expert", "expertise", "domain expert", or "non-expert". Two researchers iteratively coded papers for explicit expertise definitions, development phases, and expert roles. The analysis identified patterns in how expertise is defined (often implicitly or through formal credentials), which development phases involve experts (predominantly data-related activities), and what roles experts play (primarily data providers rather than system designers).

## Key Results
- Expertise remains undefined in 51 out of 112 analyzed papers
- 84 instances of expert engagement focused on data annotation activities
- Only 4 instances of expert involvement in problem formulation stages
- Formal education and professional certification cited as expertise criteria in 30 papers
- Non-traditional expertise forms rarely sought or valued in ML development

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Expertise is often treated as modular and decontextualized knowledge that can be extracted from experts and applied in ML systems.
- Mechanism: Researchers engage experts primarily as data providers or informants, focusing on capturing discrete facts or annotations rather than integrating broader experiential knowledge.
- Core assumption: Expert knowledge can be reduced to textbook-style information that is universally applicable across contexts.
- Evidence anchors:
  - [abstract] "expertise is often undefined and forms of knowledge outside of formal education and professional certification are rarely sought"
  - [section] "The process of working with domain experts to develop AI systems has been acknowledged as a challenge for several reasons, including the learning curve that data scientists and ML experts face in familiarizing themselves with a new domain"
  - [corpus] Weak - corpus mentions related work on human-in-the-loop and expert knowledge extraction but does not directly address decontextualization
- Break condition: If experts are engaged beyond data provision, such as in problem formulation or algorithm design, or if experiential knowledge is explicitly valued and integrated.

### Mechanism 2
- Claim: Recognition of expertise confers power and shapes which knowledge is legitimized in ML development.
- Mechanism: By identifying certain individuals as "experts," researchers implicitly prioritize their perspectives and values, potentially marginalizing alternative forms of expertise.
- Core assumption: Expertise labels direct attention to specific sources as knowledgeable and canonical, influencing system development priorities.
- Evidence anchors:
  - [abstract] "the label 'expert' directs attention to whose knowledge should be upheld as canonical"
  - [section] "Recognition of expertise confers power and influence upon those acknowledged"
  - [corpus] Weak - corpus does not directly address power dynamics in expertise recognition
- Break condition: If multiple perspectives are actively sought and documented, or if expertise definitions are explicitly broadened to include diverse knowledge sources.

### Mechanism 3
- Claim: Deskilling occurs when experts are engaged in ways that ignore or devalue their broader knowledge and skills.
- Mechanism: Experts are often reduced to providing ground truth annotations or specific domain facts without involvement in system design or problem formulation.
- Core assumption: Expert input beyond data provision is not valued or sought in ML development processes.
- Evidence anchors:
  - [abstract] "experts were predominantly engaged in data annotation, validation, and end-user evaluations, with limited involvement in algorithm design or problem formulation"
  - [section] "In ML development, deskilling has affected a variety of experts, ranging from farmers to medical doctors"
  - [corpus] Weak - corpus does not directly address deskilling but mentions expert roles in development
- Break condition: If experts are meaningfully involved in shaping system goals, design decisions, or problem formulation.

## Foundational Learning

- Concept: Knowledge as socially and culturally constituted
  - Why needed here: Understanding that expertise is not just individual knowledge but shaped by social contexts helps explain why ML systems may miss important contextual factors
  - Quick check question: How might an expert's decision-making be influenced by their social and organizational context beyond formal training?

- Concept: Epistemic trespassing and epistemological discrimination
  - Why needed here: Recognizing how traditional expertise definitions can marginalize alternative knowledge sources is crucial for understanding the paper's call for broader expertise recognition
  - Quick check question: What are potential consequences of relying on a limited set of experts whose knowledge may not represent the full range of perspectives in a domain?

- Concept: Datafication and its limitations
  - Why needed here: Understanding how knowledge becomes quantified and integrated into ML systems helps explain the challenges of capturing complex, experiential expertise
  - Quick check question: What aspects of expert knowledge might be lost when transformed into quantified data for ML systems?

## Architecture Onboarding

- Component map:
  - Data Collection Module: Handles expert data gathering and annotation processes
  - Knowledge Representation Layer: Stores and structures expert knowledge for ML use
  - Expert Engagement Interface: Manages interactions with experts at different development stages
  - Documentation System: Records expertise definitions, engagement rationales, and contribution details
  - Validation Pipeline: Tests and evaluates expert input integration into ML systems

- Critical path:
  1. Define expertise needs and criteria for the specific ML task
  2. Engage experts through appropriate channels (data provision, consultation, design input)
  3. Document expertise sources and engagement rationales
  4. Integrate expert knowledge into ML system components
  5. Validate expert input effectiveness through testing and evaluation

- Design tradeoffs:
  - Depth vs. breadth of expert engagement: More comprehensive engagement provides richer knowledge but increases time and resource costs
  - Standardization vs. flexibility: Formalized processes ensure consistency but may limit capturing unique expert insights
  - Transparency vs. efficiency: Detailed documentation supports reproducibility but adds overhead to development

- Failure signatures:
  - Over-reliance on single expertise source leading to biased or incomplete knowledge representation
  - Experts providing input without understanding system goals, resulting in misaligned contributions
  - Documentation gaps making it difficult to assess expertise validity or reproduce results

- First 3 experiments:
  1. Test different expertise definition criteria (education vs. experience vs. community recognition) on a small ML task to compare outcomes
  2. Implement a prototype documentation system for tracking expertise engagement and assess its impact on transparency and reproducibility
  3. Design an experiment comparing ML system performance when experts are engaged only for data provision versus broader involvement in problem formulation and design

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific impacts of epistemic trespassing in machine learning development, and how can they be measured?
- Basis in paper: [explicit] The paper discusses epistemic trespassing as a form of epistemological discrimination where stakeholders possessing nontraditional and minoritized forms of knowledge are sidelined in favor of individuals whose expertise primarily resides in another domain.
- Why unresolved: The paper highlights the issue but does not provide specific metrics or methods to measure the impacts of epistemic trespassing.
- What evidence would resolve it: Empirical studies comparing the outcomes of projects that include diverse expertise versus those that rely solely on traditional experts, measuring aspects such as innovation, inclusivity, and fairness.

### Open Question 2
- Question: How can participatory AI methodologies be optimized to ensure equitable representation of marginalized expertise?
- Basis in paper: [explicit] The paper emphasizes the need for broader recognition of expertise, including experiential and marginalized knowledge, and suggests that participatory AI should acknowledge the expertise of marginalized communities.
- Why unresolved: While the paper calls for expanded participation, it does not detail specific methodologies or frameworks to ensure equitable representation.
- What evidence would resolve it: Case studies or pilot programs demonstrating successful integration of marginalized expertise into AI development, with metrics on participation and impact.

### Open Question 3
- Question: What are the best practices for documenting the expertise and knowledge sources involved in machine learning projects?
- Basis in paper: [explicit] The paper discusses the importance of transparency and documentation of expert engagement to support responsible AI development and improve scientific reproducibility.
- Why unresolved: The paper suggests the need for documentation but does not provide a detailed framework or best practices for how this should be implemented.
- What evidence would resolve it: A comprehensive guideline or framework for documenting expertise in ML projects, validated by industry standards or academic research.

## Limitations
- Review focused only on ML publications from major venues, potentially missing relevant work from other fields or less prominent conferences
- Coding process may have missed nuanced definitions of expertise due to the implicit nature of many expertise claims
- Analysis captures formal research practices but may not reflect real-world ML development workflows

## Confidence
- **High**: The finding that expertise is often undefined in ML literature (51 out of 112 papers)
- **Medium**: The characterization of deskilling practices, based on limited direct evidence but supported by related work
- **Medium**: The claim that non-traditional expertise forms are rarely sought, given the narrow focus on published research

## Next Checks
1. **Expand corpus scope**: Conduct a similar review across different publication venues (industry white papers, technical reports, interdisciplinary journals) to assess if expertise patterns differ outside mainstream ML venues.

2. **Field validation**: Interview ML practitioners about their actual expert engagement practices to compare with published descriptions, particularly focusing on problem formulation and algorithm design involvement.

3. **Expert knowledge integration test**: Design a controlled experiment comparing ML system performance when experts are engaged only for data annotation versus broader involvement in system design, measuring both technical outcomes and knowledge representation quality.