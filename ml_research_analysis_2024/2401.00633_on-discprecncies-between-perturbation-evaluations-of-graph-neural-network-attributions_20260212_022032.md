---
ver: rpa2
title: On Discprecncies between Perturbation Evaluations of Graph Neural Network Attributions
arxiv_id: '2401.00633'
source_url: https://arxiv.org/abs/2401.00633
tags:
- network
- graph
- attribution
- edges
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the performance of different graph neural
  network (GNN) attribution methods by retraining networks on important and unimportant
  edges as identified by the attributions. The core idea is to evaluate if the remaining
  edges are generalizable by the network.
---

# On Discprecncies between Perturbation Evaluations of Graph Neural Network Attributions

## Quick Facts
- arXiv ID: 2401.00633
- Source URL: https://arxiv.org/abs/2401.00633
- Reference count: 18
- Primary result: GNN attribution methods perform variably across datasets; GNNExplainer shows no advantage over random edge selection

## Executive Summary
This paper investigates the reliability of graph neural network attribution methods by retraining networks on edges identified as important versus unimportant. The study reformulates previous retraining frameworks to address methodological issues and proposes guidelines for proper analysis. Through extensive experiments on four attribution methods across five datasets, the authors demonstrate that attribution performance is highly dependent on the specific network and dataset combination, challenging the use of retraining as a universal benchmark for attribution quality.

## Method Summary
The authors evaluate GNN attribution methods by removing edges deemed important or unimportant based on attribution scores, then retraining the network on the remaining edges. They propose a reformulated retraining framework that addresses issues with previous formulations, including proper handling of edge sparsity and generalization assessment. The framework tests whether remaining edges after perturbation are sufficient for the network to maintain performance. Four attribution methods (GradCAM, GNNExplainer, PGExplainer, SubgraphX) are evaluated across five graph datasets using GCN-based architectures, with experiments measuring performance degradation after edge removal and subsequent retraining.

## Key Results
- Attribution methods show inconsistent performance across different datasets and networks
- GNNExplainer performs similarly to random edge selection in many cases
- No single attribution method consistently outperforms others across all tested scenarios
- Retraining evaluation reveals dataset-specific properties rather than universal attribution quality
- The study concludes that retraining cannot serve as a generalized benchmark but is useful for specific network-dataset combinations

## Why This Works (Mechanism)
The retraining framework works by creating a controlled perturbation environment where the true importance of edges can be empirically tested. By removing edges based on attribution scores and retraining, the method directly measures whether the attributions identify genuinely critical edges for model performance. This approach bypasses the need for ground truth explanations by using downstream task performance as the evaluation metric, providing a task-specific measure of attribution quality.

## Foundational Learning
**Graph Neural Networks**: Neural networks that operate on graph-structured data by propagating and aggregating information through edges and nodes.
*Why needed*: Understanding GNN architecture is essential for grasping how edge perturbations affect model performance.
*Quick check*: Can you explain how message passing works in GCNs?

**Attribution Methods**: Techniques that identify which input features (edges, nodes, or node features) contribute most to a model's prediction.
*Why needed*: These are the methods being evaluated for their ability to identify truly important edges.
*Quick check*: What's the difference between gradient-based and perturbation-based attribution methods?

**Edge Sparsity**: The ratio of removed edges to total edges in the graph.
*Why needed*: Proper handling of sparsity is crucial for meaningful retraining experiments.
*Quick check*: How does edge sparsity affect the generalization capability of GNNs?

## Architecture Onboarding

**Component Map**: Attribution Method -> Edge Importance Scoring -> Edge Removal -> Network Retraining -> Performance Evaluation

**Critical Path**: The attribution method generates importance scores → edges are removed based on these scores → network is retrained on remaining edges → performance is measured and compared to baseline

**Design Tradeoffs**: The study balances between thorough evaluation across multiple datasets and methods versus computational cost of retraining multiple networks. Using GCN as a base architecture provides consistency but may limit generalizability to other GNN architectures.

**Failure Signatures**: Attribution methods failing to outperform random selection indicate either poor quality of attributions or dataset characteristics that make edge importance less critical for performance.

**First Experiments**:
1. Replicate main experiments with additional GCN-based architectures
2. Test attribution methods on a simple synthetic dataset with known ground truth
3. Evaluate attribution stability across multiple retraining runs with the same settings

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to four attribution methods and five datasets
- Focus on edge perturbation may miss other important aspects of attribution quality
- Results may not generalize to attention-based or transformer-style GNN architectures
- Computational cost of retraining limits scalability to larger datasets or more methods

## Confidence
| Claim | Confidence |
|-------|------------|
| Attribution performance varies by dataset | Medium |
| GNNExplainer performs similarly to random | Medium |
| Retraining evaluation is not a universal benchmark | Medium |

## Next Checks
1. Test the same attribution methods on additional real-world graph datasets (e.g., citation networks, social networks) to assess robustness of findings
2. Compare attribution performance when using alternative perturbation strategies (node-level, feature-level) alongside edge perturbation
3. Evaluate attribution methods on GNN architectures beyond GCN-based models, including attention-based and graph transformer networks