---
ver: rpa2
title: General2Specialized LLMs Translation for E-commerce
arxiv_id: '2403.03689'
source_url: https://arxiv.org/abs/2403.03689
tags: []
core_contribution: This paper addresses the challenge of translating e-commerce text,
  which contains domain-specific terminology and unique writing styles that are not
  well handled by general-purpose neural machine translation (NMT) models. The authors
  propose a two-step fine-tuning approach called G2ST that adapts large language models
  (LLMs) to the e-commerce domain.
---

# General2Specialized LLMs Translation for E-commerce

## Quick Facts
- arXiv ID: 2403.03689
- Source URL: https://arxiv.org/abs/2403.03689
- Reference count: 19
- Primary result: Proposed G2ST approach achieves SacreBLEU score of 39.85 and Rouge scores of 76.89/60.86/75.74 on e-commerce translation task

## Executive Summary
This paper addresses the challenge of translating e-commerce text, which contains domain-specific terminology and unique writing styles that are not well handled by general-purpose neural machine translation (NMT) models. The authors propose a two-step fine-tuning approach called G2ST that adapts large language models (LLMs) to the e-commerce domain. First, they expand the vocabulary with e-commerce-specific terms and fine-tune the model on bilingual term pairs. Then, they fine-tune on a parallel corpus of e-commerce text. Additionally, they incorporate self-contrastive learning to improve robustness and semantic representation. Evaluations on a real e-commerce dataset show that G2ST significantly outperforms state-of-the-art NMT models like LLaMA, Qwen, and GPT-4.

## Method Summary
The authors propose a two-step fine-tuning paradigm called G2ST for adapting general LLMs to specialized e-commerce translation. First, they expand the tokenizer vocabulary from 2,575 to 7,000 Chinese characters to incorporate e-commerce-specific terms, then fine-tune on 20k bilingual term pairs to learn domain terminology. Second, they fine-tune on 7k annotated e-commerce parallel corpus to adapt to e-commerce writing styles. Throughout training, they apply self-contrastive learning (R-Drop) by running two forward passes with dropout and minimizing the KL divergence between outputs to enhance robustness.

## Key Results
- G2ST achieves SacreBLEU score of 39.85 on e-commerce translation task
- Rouge-1, Rouge-2, and Rouge-L scores reach 76.89, 60.86, and 75.74 respectively
- Significantly outperforms baseline models including LLaMA, Qwen, and GPT-4
- Demonstrates effectiveness of vocabulary expansion and two-step fine-tuning approach

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain vocabulary expansion reduces Out-Of-Vocabulary (OOV) failures by adding missing characters to the tokenizer's vocabulary.
- Mechanism: The model's tokenizer is expanded from 2,575 to 7,000 Chinese characters, incorporating e-commerce-specific terms. This ensures domain-specific words are tokenized correctly rather than broken into unknown tokens.
- Core assumption: Missing characters in the tokenizer's vocabulary are the primary cause of OOV errors in e-commerce text.
- Evidence anchors:
  - [abstract] "we augment the tokenizer's vocabulary with prevalent e-commerce Chinese characters, mitigating the OOV problem in this domain."
  - [section] "Transformer-based models... often struggle with out-of-vocabulary (OOV) issues in character-centric languages like Chinese... We augment the tokenizer's vocabulary with prevalent e-commerce Chinese characters, mitigating the OOV problem in this domain."
  - [corpus] Weak - corpus lacks specific OOV rate metrics before/after expansion.

### Mechanism 2
- Claim: Two-step fine-tuning aligns model behavior with e-commerce writing style and terminology.
- Mechanism: First fine-tune on bilingual term pairs to learn domain-specific vocabulary semantics. Second fine-tune on e-commerce parallel corpus to learn the stacked keyword writing pattern.
- Core assumption: Domain adaptation requires both vocabulary alignment and stylistic adaptation.
- Evidence anchors:
  - [abstract] "we propose a two-step fine-tuning paradigm (named G2ST) with self-contrastive semantic enhancement to transfer one general NMT model to the specialized NMT model for e-commerce."
  - [section] "The first step of fine-tuning is employed to learn the semantics of newly added Chinese characters... The second step of fine-tuning is leveraged to learn the writing formulas of the e-commerce experts using the parallel corpus."
  - [corpus] Weak - corpus lacks intermediate step performance metrics.

### Mechanism 3
- Claim: Self-contrastive learning reduces prediction inconsistency between dropout passes, improving robustness.
- Mechanism: Two forward passes with dropout produce distributions P1 and P2. Minimizing KL divergence between them regularizes the model to produce consistent predictions.
- Core assumption: Dropout-induced prediction variance is harmful to translation quality.
- Evidence anchors:
  - [abstract] "Self-contrastive learning is employed to enhance the robustness and lexical representation capability of the model."
  - [section] "To enhance the output representation, we utilize R-Drop... We feed ð‘¥ð‘– to go through the forward pass of the network twice and get two distributions... the total KL loss is as follow..."
  - [corpus] Weak - corpus lacks dropout variance metrics or ablation results for R-Drop.

## Foundational Learning

- Concept: Tokenization and subword encoding
  - Why needed here: Chinese e-commerce text contains domain-specific terms that standard BPE tokenization may split incorrectly, causing semantic loss.
  - Quick check question: How does BPE tokenization handle rare or compound domain-specific words in Chinese?

- Concept: Domain adaptation in NMT
  - Why needed here: General NMT models trained on web data lack exposure to e-commerce-specific terminology and writing patterns, leading to poor translation quality.
  - Quick check question: What is the difference between general-domain and domain-specific parallel corpora in terms of vocabulary and sentence structure?

- Concept: Contrastive learning and regularization
  - Why needed here: Self-contrastive learning reduces inconsistency between different dropout passes, improving model robustness during inference.
  - Quick check question: How does minimizing KL divergence between two dropout-perturbed predictions improve model generalization?

## Architecture Onboarding

- Component map:
  Tokenizer vocabulary expansion module (7,000 chars) -> Two-step fine-tuning pipeline (term pairs â†’ parallel corpus) -> Self-contrastive learning module (R-Drop with KL divergence) -> Base NMT backbone (LLM or STM)

- Critical path:
  1. Expand tokenizer vocabulary with e-commerce characters.
  2. Fine-tune on bilingual term pairs to learn domain-specific semantics.
  3. Fine-tune on e-commerce parallel corpus to adapt to writing style.
  4. Apply self-contrastive learning during fine-tuning for robustness.

- Design tradeoffs:
  - Vocabulary size vs. model efficiency: Larger vocab reduces OOV but increases memory usage.
  - Fine-tuning data size vs. overfitting: Small e-commerce datasets risk overfitting; larger datasets improve generalization.
  - Self-contrastive weight (Î±) vs. convergence speed: Higher Î± enforces consistency but may slow training.

- Failure signatures:
  - High OOV rate after vocab expansion â†’ missing critical characters.
  - No improvement after fine-tuning â†’ domain shift too small or dataset too small.
  - Degraded performance with contrastive loss â†’ Î± too high or dropout too aggressive.

- First 3 experiments:
  1. Measure OOV rate before and after tokenizer expansion on e-commerce test set.
  2. Compare BLEU scores after each fine-tuning step (term pairs â†’ parallel corpus).
  3. Ablate self-contrastive learning by training with and without KL loss.

## Open Questions the Paper Calls Out
- How does the G2ST approach perform on other specialized domains beyond e-commerce, such as legal or medical translation?
- What is the impact of the self-contrastive learning component on the model's ability to handle out-of-domain data?
- How scalable is the G2ST approach for extremely large language models (e.g., those with trillions of parameters)?
- What are the specific characteristics of the e-commerce parallel corpus that contribute to the model's improved performance?

## Limitations
- No ablation studies isolating individual mechanism contributions (vocabulary expansion, two-step fine-tuning, self-contrastive learning)
- Evaluation limited to single domain (e-commerce) and language pair (Chinese-English)
- Self-contrastive learning implementation details unclear (KL divergence weighting, dropout configurations)
- No empirical evidence of vocabulary expansion's actual impact on OOV reduction

## Confidence
- High confidence: Two-step fine-tuning approach is theoretically sound and reported BLEU/ROUGE scores are reasonable for domain adaptation
- Medium confidence: Vocabulary expansion will reduce OOV errors, but magnitude depends on overlap with original tokenizer coverage
- Low confidence: Self-contrastive learning contribution is least validated with no ablation results showing impact on e-commerce translation quality

## Next Checks
1. Measure out-of-vocabulary rate on e-commerce test data before and after tokenizer expansion to quantify actual impact
2. Conduct component ablation study comparing base model vs. vocabulary expansion only vs. two-step fine-tuning only vs. full G2ST
3. Apply G2ST pipeline to different domain (medical or legal text) to test cross-domain transferability and generalization beyond e-commerce