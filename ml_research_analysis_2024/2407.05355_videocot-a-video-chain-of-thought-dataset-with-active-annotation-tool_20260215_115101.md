---
ver: rpa2
title: 'VideoCoT: A Video Chain-of-Thought Dataset with Active Annotation Tool'
arxiv_id: '2407.05355'
source_url: https://arxiv.org/abs/2407.05355
tags:
- video
- videocot
- dataset
- datasets
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an active learning-based automatic annotation
  tool to generate video chain-of-thought (CoT) datasets, addressing the challenge
  of expensive and unreliable manual or machine-only annotation for complex video
  reasoning tasks. The tool iteratively refines prompts and generates CoT rationales
  using a combination of machine-generated data and human expert refinement, guided
  by a multi-dimensional quality scoring function.
---

# VideoCoT: A Video Chain-of-Thought Dataset with Active Annotation Tool

## Quick Facts
- **arXiv ID**: 2407.05355
- **Source URL**: https://arxiv.org/abs/2407.05355
- **Reference count**: 12
- **Primary result**: Active learning-based annotation tool generates VideoCoT datasets that improve MLLM performance by up to +63% on open-ended video QA tasks.

## Executive Summary
This paper addresses the challenge of creating high-quality video Chain-of-Thought (CoT) datasets for training MultiModal Large Language Models (MLLMs). The proposed active learning framework combines machine-generated CoT with human expert refinement to iteratively improve dataset quality while reducing annotation costs. The approach generates three datasets: VideoCoT, TopicQA, and TopicCoT, demonstrating significant performance gains when used to train MLLMs on open-ended video reasoning tasks.

## Method Summary
The method employs an active learning paradigm where a prompt generator (Qwen-1.8B) creates initial prompts for LLMs (GPT-4) to generate CoT rationales. A multi-dimensional quality scoring function evaluates generated CoT across six dimensions (perplexity, background, temporal changes, spatial objects, relations, summary) with a threshold of 0.9. Low-scoring samples undergo human expert refinement, and the refined examples retrain the prompt generator. This iterative process produces high-quality CoT datasets that are then used to train MLLMs including mPLUG-Owl, VisualGLM, and mini-GPT4 for improved video understanding and reasoning.

## Key Results
- MLLMs trained on VideoCoT datasets achieve up to +63% accuracy improvements on open-ended video QA tasks compared to baseline models
- The active learning approach reduces human annotation workload while maintaining high dataset quality
- Three datasets constructed: VideoCoT (11K samples), TopicQA (11K samples), and TopicCoT (3K samples)
- Quality scores consistently above 0.9 threshold after human refinement iterations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Active learning with human experts iteratively improves prompt quality for CoT generation.
- Mechanism: The system generates initial prompts, evaluates their output quality using a multi-dimensional scoring function, and then refines low-scoring prompts with human expert input. These refined examples are used to train the prompt generation model for better future outputs.
- Core assumption: Human experts can reliably identify and correct key flaws in machine-generated CoT, and these corrections can be generalized to improve the prompt generation model.
- Evidence anchors: [abstract]: "Active learning is an interactive strategy between the model and human experts, in this way, the workload of human labeling can be reduced and the quality of the dataset can be guaranteed."
- Break condition: If human experts cannot consistently identify specific flaws in CoT (e.g., logical gaps, missing spatial/temporal relationships), or if the prompt generator cannot learn generalizable patterns from corrections, the iterative improvement will stall.

### Mechanism 2
- Claim: Multi-dimensional quality scoring enables automated filtering of low-quality CoT before human review.
- Mechanism: The system evaluates generated CoT across six dimensions (perplexity, background description, temporal changes, spatial objects, relations, summary) to assign a composite score. CoT below threshold (0.9) is sent for human refinement.
- Core assumption: These six dimensions comprehensively capture quality attributes needed for effective video CoT, and their weighted combination provides reliable quality indicator.
- Evidence anchors: [section]: "To achieve this, we design a scoring function SvCoT that automatically evaluates from six dimensions, i.e., perplexity Sppl, background Sbac, temporal changes Stem, spatial objects Sspa, relations Srel, summary Ssum."
- Break condition: If scoring function fails to detect critical quality issues (e.g., hallucinations, logical inconsistencies) or produces too many false positives/negatives, human effort will be wasted or quality will suffer.

### Mechanism 3
- Claim: CoT training significantly improves MLLM reasoning ability for video understanding tasks.
- Mechanism: Models trained on VideoCoT and TopicCoT datasets with CoT rationales show substantial accuracy improvements (up to +63%) over baseline models on open-ended video QA tasks.
- Core assumption: Step-by-step reasoning captured in CoT provides models with explicit reasoning patterns that generalize to unseen video understanding tasks.
- Evidence anchors: [abstract]: "When used to train MLLMs (mPLUG-Owl, VisualGLM, mini-GPT4), models achieve significant performance gains in open-ended video QA tasks, with accuracy improvements up to +63% over baseline models."
- Break condition: If CoT patterns are too dataset-specific and don't generalize to new video domains, or if models simply memorize reasoning steps without understanding underlying concepts, performance gains will not transfer.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: Video understanding requires complex multi-step reasoning about spatial relationships, temporal changes, and object interactions that simple answer generation cannot capture.
  - Quick check question: What are the three key benefits of CoT for video understanding mentioned in the introduction?

- Concept: Active learning paradigm
  - Why needed here: Manual annotation of video CoT is prohibitively expensive, while fully automated generation produces unreliable results due to hallucination issues.
  - Quick check question: How does active learning reduce human annotation workload while maintaining dataset quality?

- Concept: Multi-dimensional quality scoring
  - Why needed here: Need automated way to filter low-quality CoT before expensive human review, while capturing specific attributes that make video CoT effective.
  - Quick check question: What are the six dimensions used in the quality scoring function?

## Architecture Onboarding

- Component map: Video source (Kinetics-700) → Prompt generator (Qwen-1.8B) → LLM for CoT generation (GPT-4) → Quality scoring module (six-dimensional evaluation) → Human expert refinement interface → Training pipeline for MLLMs

- Critical path: Video → Prompt Generation → CoT Generation → Quality Scoring → Human Refinement (if needed) → Dataset Construction → MLLM Training → Evaluation

- Design tradeoffs:
  - Using GPT-4 for CoT generation provides high-quality outputs but adds API costs and dependency
  - Six-dimensional scoring is comprehensive but requires careful weight tuning
  - Human-in-the-loop approach ensures quality but limits scalability

- Failure signatures:
  - Low-quality prompts consistently failing quality threshold (>50% rejection rate)
  - Human experts unable to identify consistent patterns in low-quality CoT
  - MLLM performance not improving despite dataset quality

- First 3 experiments:
  1. Run prompt generator on small video sample without human refinement to establish baseline quality
  2. Test quality scoring function on human-annotated CoT to validate dimension weights
  3. Train a small MLLM on a subset of VideoCoT and evaluate on a held-out validation set to confirm CoT effectiveness

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but identifies several limitations and areas for future work:

- Scalability constraints due to funding limitations restricting expert annotation invitations
- Training resource restrictions limiting application to larger models
- Need for more comprehensive evaluation across diverse video domains

## Limitations

- The active learning loop implementation lacks transparency in critical details about prompt generator training and iteration stopping criteria
- Dependency on GPT-4 for CoT generation raises reproducibility and scalability concerns
- Human-in-the-loop approach, while ensuring quality, creates questions about scalability to larger video datasets

## Confidence

- **High confidence**: The core mechanism of active learning with human expert refinement for video CoT generation is well-described and technically sound. The multi-dimensional quality scoring approach is clearly specified.
- **Medium confidence**: The claimed performance improvements (+63%) are based on experiments with specific MLLMs, but the methodology lacks sufficient detail for full reproducibility.
- **Low confidence**: The scalability analysis and long-term sustainability of the human-in-the-loop approach are not addressed, leaving questions about practical deployment.

## Next Checks

1. **Reproduce the quality scoring function**: Implement the six-dimensional scoring independently and validate it against a small set of human-annotated CoT samples to verify the dimension weights and scoring thresholds.

2. **Ablation study on CoT contribution**: Train the same MLLMs on the same videos without CoT rationales to quantify exactly how much of the performance improvement comes from the CoT training versus other factors like base model quality or prompt engineering.

3. **Scalability stress test**: Run the active learning loop on progressively larger subsets of the video dataset to measure how the proportion of samples requiring human refinement changes, and estimate the human annotation cost for scaling to the full dataset.