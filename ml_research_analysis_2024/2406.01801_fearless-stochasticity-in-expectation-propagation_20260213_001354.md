---
ver: rpa2
title: Fearless Stochasticity in Expectation Propagation
arxiv_id: '2406.01801'
source_url: https://arxiv.org/abs/2406.01801
tags:
- update
- used
- updates
- parameters
- respect
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Expectation propagation (EP) can perform approximate inference
  in complex probabilistic models by evaluating moments under tilted distributions,
  which are often estimated using Monte Carlo (MC) sampling. However, naive MC estimation
  leads to unstable updates due to bias from nonlinear transformations.
---

# Fearless Stochasticity in Expectation Propagation

## Quick Facts
- arXiv ID: 2406.01801
- Source URL: https://arxiv.org/abs/2406.01801
- Reference count: 40
- Primary result: EP-η and EP-µ variants are more robust to Monte Carlo noise than standard EP

## Executive Summary
Expectation propagation (EP) performs approximate inference by matching moments under tilted distributions, but naive Monte Carlo estimation introduces bias that destabilizes updates. This paper shows that EP updates are equivalent to natural-gradient-based optimization, and leverages this insight to develop two new variants—EP-η and EP-µ—that are significantly more robust to stochastic noise. These methods achieve stability and sample efficiency using as few as a single Monte Carlo sample per update, without requiring debiasing estimators or sample thinning. Experiments demonstrate consistent speed-accuracy improvements across hierarchical logistic regression, cosmic radiation modeling, and neural response modeling tasks.

## Method Summary
The authors reinterpret EP's moment-matching updates as natural-gradient optimization of a variational objective, which reveals why noisy Monte Carlo estimates cause instability. They develop EP-η, which performs natural gradient descent in the space of natural parameters, yielding unbiased updates. They also develop EP-µ, which uses smaller step sizes in the mean parameter space to reduce bias more quickly than the loss function decreases. Both variants maintain stability with minimal sampling and show improved speed-accuracy trade-offs compared to standard EP and related methods like stochastic natural gradient EP.

## Key Results
- EP-η and EP-µ remain stable with just a single Monte Carlo sample per update
- Both methods consistently reach target accuracy faster than EP and SNEP across three different models
- EP-η shows superior sample efficiency due to unbiased updates, while EP-µ offers easier tuning through its smaller step sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EP updates are equivalent to natural-gradient-based optimization of a variational objective
- Mechanism: The natural gradient interpretation reveals that moment estimates are converted to natural parameters via nonlinear maps (∇A*(.)), introducing bias when estimates are noisy
- Core assumption: The equivalence between EP updates and natural gradient descent holds for the specific tilted distributions used in EP
- Evidence anchors:
  - [abstract]: "EP's moment-matching updates are equivalent to natural-gradient-based optimization of a variational objective"
  - [section 3.1]: "Several EP variants...differ only in how they perform the inner optimisation in (3) with respect to {λi}i"
  - [corpus]: Weak evidence

### Mechanism 2
- Claim: EP-η and EP-µ reduce bias in updates, making them more robust to Monte Carlo noise
- Mechanism: EP-η uses unbiased natural gradient updates in parameter space, while EP-µ uses smaller step sizes that reduce bias faster than loss decreases
- Core assumption: Bias from nonlinear mapping of noisy moment estimates is the primary source of instability
- Evidence anchors:
  - [abstract]: "These new variants...do not rely on the use of debiasing estimators"
  - [section 3.2]: "The unbiased updates of EP-η allow it to be more sample-efficient"
  - [corpus]: Weak evidence

### Mechanism 3
- Claim: EP-η and EP-µ are more sample-efficient due to reduced bias
- Mechanism: Lower bias allows fewer samples per update while maintaining progress toward optimal solution
- Core assumption: Bias is a significant source of error that can be reduced by using more samples, but small initial bias requires fewer samples
- Evidence anchors:
  - [abstract]: "They remain stable and are most sample-efficient when estimated with just a single sample"
  - [section 3.2]: "EP-η can use smaller number of samples per iteration"
  - [corpus]: Weak evidence

## Foundational Learning

- Concept: Exponential families of distributions
  - Why needed here: EP assumes approximating distribution belongs to tractable exponential family; properties used to derive updates and analyze bias
  - Quick check question: What are the key properties of exponential families that make them useful for EP?

- Concept: Natural gradient descent
  - Why needed here: Natural gradient interpretation is key insight enabling EP-η and EP-µ development
  - Quick check question: How does natural gradient descent differ from standard gradient descent, and why is it useful for optimization in distribution spaces?

- Concept: Monte Carlo estimation
  - Why needed here: EP uses Monte Carlo to approximate moments under tilted distributions; understanding error sources is crucial
  - Quick check question: What are the main sources of error in Monte Carlo estimation, and how can they be reduced?

## Architecture Onboarding

- Component map: p0 (base distribution) -> pi (tilted distributions) -> λi (site parameters) -> updates (adjust site parameters based on moments)
- Critical path: Sequence of updates adjusting site parameters based on moments under tilted distributions
- Design tradeoffs: Accuracy vs computational cost; EP-η and EP-µ improve accuracy without significant cost increase but introduce new tradeoffs in parameterisation or step size choice
- Failure signatures: Poor tilted distribution approximation by exponential family, inappropriate step size, excessive Monte Carlo noise; EP-η and EP-µ may fail if natural gradient interpretation breaks down or bias reduction ineffective
- First 3 experiments:
  1. Implement EP with Monte Carlo estimation and observe bias in updates
  2. Implement EP-η and compare bias and sample efficiency to EP
  3. Implement EP-µ and compare bias and sample efficiency to EP and EP-η

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would EP-η and EP-µ perform on models with very high-dimensional latent variables (d > 1000) where computational cost of forward/backward mappings becomes significant?
- Basis in paper: [explicit] Authors note computational costs for MVN families with dense covariance are O(d³), suggest diagonal covariance for very large d
- Why unresolved: Experiments used moderate-dimensional models (d up to 65), scaling behavior for very high dimensions untested
- What evidence would resolve it: Experiments on models with d > 1000 comparing EP-η, EP-µ, and EP variants, measuring convergence speed and approximation quality

### Open Question 2
- Question: Can efficiency of EP variants be improved by using knowledge of current approximation p to guide sampling from tilted distributions pi?
- Basis in paper: [explicit] Authors mention in Section 6 that using p to set MCMC hyperparameters directly, or to re-initialize MCMC chains, could improve performance
- Why unresolved: Experiments used standard NUTS without leveraging current approximation for guidance
- What evidence would resolve it: Experiments comparing standard NUTS sampling vs. guided sampling using p, measuring NUTS steps and wall-clock time for convergence

### Open Question 3
- Question: How do EP-η and EP-µ compare to other black-box inference methods (e.g., stochastic variational inference, normalizing flows) on models where posterior is highly non-Gaussian?
- Basis in paper: [inferred] Authors compare EP variants to CVI in moderately non-Gaussian setting, but don't compare to other black-box methods
- Why unresolved: Comparison with CVI shows EP-η can capture non-Gaussianity better, but unclear how it compares to other modern black-box methods
- What evidence would resolve it: Experiments on models with highly non-Gaussian posteriors (e.g., multimodal, heavy-tailed) comparing EP-η, EP-µ, and other black-box methods (SVI, normalizing flows), measuring KL divergence and computational cost

## Limitations
- Theoretical analysis assumes idealized conditions that may not hold in practice
- Limited testing on non-hierarchical models
- Potential sensitivity to NUTS sampler configuration not fully explored

## Confidence
- Natural gradient interpretation of EP: Medium
- Single-sample efficiency claims: Medium
- Empirical performance improvements: High

## Next Checks
1. Test EP-η and EP-µ on non-hierarchical models (e.g., Gaussian process regression) to verify generality
2. Conduct ablation studies varying the number of MC samples systematically across different noise levels
3. Compare against alternative debiasing approaches (like control variates) under controlled bias conditions