---
ver: rpa2
title: 'MoNDE: Mixture of Near-Data Experts for Large-Scale Sparse Models'
arxiv_id: '2405.18832'
source_url: https://arxiv.org/abs/2405.18832
tags:
- monde
- expert
- memory
- experts
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MoNDE introduces a near-data computing approach for efficient inference
  of large-scale sparse MoE language models. By leveraging the skewed token distribution
  across experts, MoNDE processes cold experts directly within host memory using NDP
  units, transferring only small activations instead of massive expert parameters.
---

# MoNDE: Mixture of Near-Data Experts for Large-Scale Sparse Models

## Quick Facts
- arXiv ID: 2405.18832
- Source URL: https://arxiv.org/abs/2405.18832
- Reference count: 16
- Key result: Up to 7.5× and 3.7× speedup for encoder and decoder operations respectively over state-of-the-art parameter offloading frameworks

## Executive Summary
MoNDE introduces a near-data computing approach for efficient inference of large-scale sparse MoE language models. By leveraging the skewed token distribution across experts, MoNDE processes cold experts directly within host memory using NDP units, transferring only small activations instead of massive expert parameters. This Activation Movement strategy significantly reduces data movement overhead compared to traditional parameter offloading. Additionally, MoNDE employs a load-balancing scheme that distributes hot and cold experts between GPU and NDP units to maximize hardware utilization.

## Method Summary
MoNDE addresses the memory bottleneck in MoE inference by introducing an NDP device that processes cold experts near their parameter storage. The system uses activation movement (AMove) instead of parameter movement (PMove), transferring only small activations between GPU and NDP memory. A load-balancing algorithm assigns compute-intensive hot experts to GPU and memory-intensive cold experts to NDP based on the ratio of PCIe bandwidth to NDP memory bandwidth. The NDP device contains 64 processing elements organized as 4×4 systolic arrays for efficient matrix operations.

## Key Results
- Achieves up to 7.5× and 3.7× speedup for encoder and decoder operations respectively over parameter offloading frameworks
- Reduces data movement volume by replacing massive expert parameter transfers with small activation transfers
- Shows minimal area overhead with 3.0mm² for 64 NDP processing units

## Why This Works (Mechanism)

### Mechanism 1
Cold experts in MoE layers have very few routed tokens, making their compute-to-memory ratio low and thus better suited for near-data processing. By moving only the small activations from GPU to NDP memory and keeping the large expert parameters resident in NDP memory, MoNDE reduces data movement volume and allows NDP to compute cold experts efficiently. This relies on the assumption that most experts receive very few tokens while only a few receive many tokens.

### Mechanism 2
Using activation movement instead of parameter movement reduces communication overhead due to the difference in data size between small activations and large expert parameters. Activations (2 × B × S × dmodel) are much smaller than parameters (2 × E × dmodel × dff), so transferring activations instead of parameters reduces data movement volume and latency. This advantage holds when batch size and sequence length are small relative to the number of experts.

### Mechanism 3
Load balancing between GPU and NDP based on expert compute intensity reduces overall latency by assigning hot experts to GPU and cold experts to NDP. The algorithm calculates an optimal number of hot experts (H) to assign to GPU based on the ratio of PCIe bandwidth to NDP memory bandwidth and the number of activated experts, then processes experts concurrently on both devices. This requires accurate approximation of expert compute intensity based on token count.

## Foundational Learning

- Concept: Mixture-of-Experts (MoE) architecture
  - Why needed here: Understanding how MoE layers work is fundamental to grasping why MoNDE's approach is beneficial
  - Quick check question: In an MoE layer with 128 experts and top-2 routing, how many experts will process each token?

- Concept: Near-Data Processing (NDP) and CXL
  - Why needed here: MoNDE leverages NDP units within CXL memory to process experts near where their parameters reside
  - Quick check question: What is the primary advantage of using CXL for NDP compared to traditional PCIe-attached memory?

- Concept: Compute-to-memory ratio and operational intensity
  - Why needed here: MoNDE exploits the low operational intensity of cold experts to justify processing them in NDP rather than GPU
  - Quick check question: If an expert receives 2 tokens with dmodel=1024, would this computation be compute-bound or memory-bound on a typical GPU?

## Architecture Onboarding

- Component map:
  Host CPU with CXL controller -> GPU with CUDA cores -> MoNDE NDP device with CXL interface -> Host memory (DRAM)

- Critical path:
  1. Token routing through gating network
  2. Data transfer of input activations to MoNDE (AMove)
  3. Expert computation in MoNDE NDP units
  4. Data transfer of output activations back to GPU (AMove)
  5. Expert combination and forwarding to next layer
  The critical path varies depending on whether experts are hot (GPU) or cold (NDP)

- Design tradeoffs:
  - NDP unit complexity vs. area overhead: MoNDE uses 64 units of 4×4 systolic arrays (3.0mm² area) to balance compute capability with area cost
  - Memory bandwidth vs. performance: Higher memory bandwidth in MoNDE improves performance for cold experts but increases power consumption
  - Load balancing threshold (H) vs. hardware utilization: The optimal H value balances GPU and NDP utilization but requires runtime tuning

- Failure signatures:
  - Poor performance when expert token distribution is uniform (no cold experts)
  - Latency increase when batch size grows large (activations become comparable to parameters)
  - Underutilization when H is poorly tuned (too many or too few experts on GPU)
  - Memory bandwidth saturation in MoNDE device

- First 3 experiments:
  1. Measure expert token distribution on a sample dataset to verify the cold/hot expert skew assumption
  2. Profile parameter movement vs. activation movement data volumes for different model configurations
  3. Implement and test the load balancing algorithm with different H values to find the optimal threshold

## Open Questions the Paper Calls Out

### Open Question 1
How does MoNDE performance scale with varying ratios of hot to cold experts in different MoE architectures? The paper only evaluates on specific models with their inherent skew ratios without exploring a broader design space.

### Open Question 2
What are the memory bandwidth requirements for MoNDE to outperform multi-GPU parameter sharding approaches? The paper mentions bandwidth sensitivity but doesn't provide a clear threshold where MoNDE becomes superior.

### Open Question 3
How does MoNDE handle expert computation for very large embedding dimensions (d_model > 2048) where matrix operations become extremely wide? The current design uses 4×256 matrix operations, but very large d_model values would require different tiling strategies.

### Open Question 4
What is the impact of varying batch sizes on the effectiveness of the GPU-MoNDE load-balancing algorithm? The paper shows throughput comparisons but doesn't explain how the H value selection algorithm performs across different batch sizes.

## Limitations
- The cold/hot expert skew assumption may not generalize across different MoE models and datasets
- NDP hardware overhead (3.0mm²) and power consumption are not fully characterized
- The load balancing algorithm's effectiveness depends on accurate approximation of expert compute intensity

## Confidence

**High Confidence** (Empirical validation provided, mechanism well-understood):
- Activation movement strategy reduces data transfer volume compared to parameter movement for typical MoE configurations
- CXL-based NDP architecture can physically process computations near memory
- Load balancing between GPU and NDP is technically feasible with current hardware

**Medium Confidence** (Theoretical justification with limited empirical validation):
- Cold/hot expert skew exists in Switch Transformer models for specific datasets
- The load balancing algorithm optimally distributes experts between GPU and NDP
- 7.5× and 3.7× speedups are achievable for the specific models tested

**Low Confidence** (Significant assumptions, minimal validation):
- Cold/hot expert skew generalizes across different MoE models and datasets
- NDP hardware overhead remains minimal as model scales increase
- The solution remains effective when batch sizes or sequence lengths grow significantly

## Next Checks

1. **Expert Distribution Validation**: Measure the token distribution across experts for multiple MoE models (different sizes, routing strategies, and datasets) to empirically verify the cold/hot skew assumption. This should include Switch Transformers of various sizes, different routing algorithms (e.g., GShard, BASE), and diverse datasets to ensure the assumption holds beyond the specific configuration tested.

2. **Load Balancing Sensitivity Analysis**: Systematically vary the H threshold across different expert distributions, batch sizes, and model configurations to identify when the load balancing algorithm breaks down. This should include testing with uniform token distributions, different bandwidth ratios, and varying numbers of activated experts to understand the algorithm's robustness.

3. **Scalability and Overhead Characterization**: Characterize the NDP hardware overhead (area, power, bandwidth requirements) as model scales increase to experts (e.g., 1024, 2048 experts) and validate that the solution remains effective. This should include measuring the impact on overall system power efficiency and comparing the area overhead against the performance gains across different NDP designs and expert counts.