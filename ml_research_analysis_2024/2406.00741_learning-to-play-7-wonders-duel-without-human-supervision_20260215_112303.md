---
ver: rpa2
title: Learning to Play 7 Wonders Duel Without Human Supervision
arxiv_id: '2406.00741'
source_url: https://arxiv.org/abs/2406.00741
tags: []
core_contribution: This paper introduces ZeusAI, an AI system capable of playing 7
  Wonders Duel at the level of top human players. The authors develop ZeusAI using
  Deep Reinforcement Learning without any supervision from human games or strategies,
  inspired by the AlphaZero algorithm.
---

# Learning to Play 7 Wonders Duel Without Human Supervision

## Quick Facts
- arXiv ID: 2406.00741
- Source URL: https://arxiv.org/abs/2406.00741
- Reference count: 4
- ZeusAI achieves 68.4% win rate against top human players in 7 Wonders Duel

## Executive Summary
This paper introduces ZeusAI, an AI system that plays 7 Wonders Duel at the level of top human players without using any human game data for training. The system is built on a Deep Reinforcement Learning framework inspired by AlphaZero, using Monte Carlo Tree Search combined with a Transformer neural network to evaluate game states and guide policy decisions. ZeusAI not only matches known expert strategies but also discovers novel approaches, demonstrating the potential of self-supervised learning in complex strategy games.

## Method Summary
The authors developed ZeusAI using a Deep Reinforcement Learning approach that combines Monte Carlo Tree Search (MCTS) with a Transformer neural network. The system learns entirely through self-play, without any human supervision or game data. The MCTS algorithm uses the Transformer model to evaluate board states and guide move selection, while the neural network is trained to predict both the value of positions and the probability of winning from different moves. This architecture allows the AI to discover both known and novel strategies through extensive self-play training.

## Key Results
- ZeusAI achieves a 68.4% win rate against top human players
- The system discovers novel strategies not previously known to human experts
- Proposed rule variants reduce first-player advantage from 66.8% to 51.6% while removing only 4 cards from a 66-card deck

## Why This Works (Mechanism)
ZeusAI's success stems from its ability to learn optimal strategies through pure self-play, leveraging the power of Monte Carlo Tree Search to explore the game tree while the Transformer network provides sophisticated evaluation of board states. The combination allows for both deep exploration of possible moves and accurate assessment of position values, enabling the discovery of complex strategic patterns that emerge from the game's mechanics rather than being explicitly programmed or learned from human examples.

## Foundational Learning
- **Monte Carlo Tree Search**: Needed for systematic exploration of game tree; quick check: verifies move selection balances exploration/exploitation
- **Transformer Neural Networks**: Required for capturing complex game state relationships; quick check: validates attention mechanisms learn meaningful patterns
- **Self-Play Reinforcement Learning**: Essential for learning without human data; quick check: confirms learning progresses through competition against itself
- **Game State Evaluation**: Critical for determining position value; quick check: ensures accurate assessment of winning probabilities
- **Policy Network**: Necessary for move probability estimation; quick check: validates reasonable move distribution outputs

## Architecture Onboarding
Component map: Game State -> Transformer Network -> MCTS -> Move Selection
Critical path: Game State evaluation flows through Transformer to MCTS, which generates move probabilities and values that guide both gameplay and training
Design tradeoffs: Pure self-play vs. human data usage; computational cost of MCTS vs. performance gains
Failure signatures: Overfitting to self-play patterns; excessive computational requirements; inability to generalize to human strategies
First experiments:
1. Test Transformer network on simplified board states
2. Validate MCTS performance on known game positions
3. Evaluate self-play learning curve on smaller game variants

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Limited human trial sample size (only 10 players tested)
- Computational resource requirements not fully disclosed
- Proposed rule variants lack empirical validation beyond AI internal testing

## Confidence
- ZeusAI's competitive performance against human players: Medium
- Discovery of novel strategies: Medium
- Effectiveness of proposed rule variants: Low
- Computational resource requirements: Low

## Next Checks
1. Conduct larger-scale human trials with diverse skill levels to validate ZeusAI's competitive performance claims against a broader population of players
2. Perform ablation studies to determine the individual contributions of MCTS, Transformer architecture, and training methodology to overall performance
3. Test the proposed rule variants in actual human gameplay sessions to verify that the first-player advantage reduction observed by the AI translates to real-world play dynamics