---
ver: rpa2
title: 'Assessing Adversarial Robustness of Large Language Models: An Empirical Study'
arxiv_id: '2405.02764'
source_url: https://arxiv.org/abs/2405.02764
tags:
- adversarial
- robustness
- language
- llms
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the adversarial robustness of large language
  models (LLMs) using a novel white-box attack approach based on geometric adversarial
  attacks. The method leverages output logits and gradients to generate adversarial
  examples through word replacement while maintaining semantic similarity.
---

# Assessing Adversarial Robustness of Large Language Models: An Empirical Study

## Quick Facts
- **arXiv ID**: 2405.02764
- **Source URL**: https://arxiv.org/abs/2405.02764
- **Reference count**: 40
- **Key outcome**: LLMs are vulnerable to adversarial attacks with accuracy drops of 15-80%, with decoder-only architectures showing better robustness than encoder-decoder models

## Executive Summary
This study evaluates the adversarial robustness of large language models using a novel white-box geometric attack approach that leverages output logits and gradients to generate semantically similar word replacements. The method successfully creates adversarial examples that cause significant accuracy drops across five text classification datasets. Experiments with models ranging from 60M to 13B parameters reveal that larger models generally show improved robustness up to a saturation point, while decoder-only architectures demonstrate better resilience than encoder-decoder models.

## Method Summary
The paper proposes a geometric adversarial attack methodology that computes gradients of the generation loss with respect to word embeddings to identify the most influential words for prediction. These gradients guide the selection of semantically similar word replacements that maximize projection onto the gradient direction while maintaining semantic similarity. The attack is evaluated across five text classification datasets using T5, OPT, and Llama models with various fine-tuning techniques including LoRA and quantization. The framework measures accuracy, attack success rate, and replacement rate to assess model robustness comprehensively.

## Key Results
- LLMs show vulnerability to adversarial attacks with accuracy drops ranging from 15% to 80% depending on model size and dataset
- Decoder-only architectures (OPT, Llama) demonstrate better robustness than encoder-decoder models (T5)
- Model size generally improves robustness up to a saturation point, beyond which additional parameters provide diminishing returns
- Fine-tuning techniques like LoRA and quantization do not significantly affect adversarial resilience

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Geometric adversarial attacks exploit gradients of output logits to identify word embeddings most influential to model predictions.
- Mechanism: The attack computes gradients of the loss with respect to word embeddings, then uses these gradients to find semantically similar word replacements that maximize projection onto the gradient direction.
- Core assumption: Model gradients reliably indicate which words contribute most to prediction confidence, and small semantic perturbations can cause large classification shifts.
- Evidence anchors:
  - [abstract] "The method leverages output logits and gradients to generate adversarial examples through word replacement while maintaining semantic similarity."
  - [section] "We commence by calculating the gradients of the generation loss Lᵢ with respect to the embeddings eᵢ of input sentence Sᵢ."
  - [corpus] Weak - no direct evidence found in related papers.

### Mechanism 2
- Claim: Model size improves adversarial robustness up to a saturation point, after which additional parameters provide diminishing returns.
- Mechanism: Larger models develop more nuanced language understanding, making them less sensitive to small input perturbations. However, this benefit plateaus as models reach sufficient capacity.
- Core assumption: Parameter count correlates with model's ability to distinguish between legitimate input variations and adversarial perturbations.
- Evidence anchors:
  - [abstract] "Model size generally improves robustness up to a saturation point"
  - [section] "As the size of the T5 model increases, its ASR gradually decreases. This suggests that larger models, with more parameters, tend to have a deeper understanding of language."
  - [corpus] Weak - no direct evidence found in related papers.

### Mechanism 3
- Claim: Decoder-only architectures (OPT, Llama) demonstrate better robustness than encoder-decoder models (T5) under synonym substitution attacks.
- Mechanism: The architectural difference affects how models process and represent semantic information, making decoder-only models more resilient to word-level perturbations.
- Core assumption: The architectural distinction creates fundamental differences in how models handle semantic preservation during adversarial attacks.
- Evidence anchors:
  - [abstract] "Decoder-only architectures (OPT, Llama) demonstrate better robustness than encoder-decoder models (T5)"
  - [section] "It is obvious that the T5 model's ASR and replacement rate are significantly higher than those of OPT and Llama. This indicated that Decoder-only Causal LMs have higher robustness against encoder-decoder architectures"
  - [corpus] Weak - no direct evidence found in related papers.

## Foundational Learning

- Concept: Gradient-based optimization and backpropagation
  - Why needed here: Understanding how gradients are computed and used to identify influential words is crucial for comprehending the attack mechanism
  - Quick check question: Can you explain how the gradient of the loss with respect to embeddings indicates which words are most important for the model's prediction?

- Concept: Word embeddings and semantic similarity
  - Why needed here: The attack relies on finding semantically similar word replacements, requiring understanding of how words are represented as vectors
  - Quick check question: How does cosine similarity between word embeddings help ensure semantic preservation during adversarial attacks?

- Concept: Adversarial machine learning fundamentals
  - Why needed here: The study evaluates model robustness against adversarial attacks, requiring knowledge of attack types and evaluation metrics
  - Quick check question: What is the difference between white-box and black-box adversarial attacks, and why is white-box access important for this study?

## Architecture Onboarding

- Component map: Dataset loading -> Model initialization -> Fine-tuning pipeline -> Adversarial attack generation -> Evaluation framework -> Results analysis

- Critical path:
  1. Load pre-trained model and prepare dataset
  2. Apply fine-tuning technique
  3. Compute baseline accuracy
  4. Generate adversarial examples using geometric attack
  5. Evaluate model performance on adversarial examples
  6. Record and compare metrics across different configurations

- Design tradeoffs:
  - Memory vs. precision: int8 quantization reduces memory usage but may affect model performance
  - Speed vs. effectiveness: More attack iterations improve attack quality but increase computation time
  - Model size vs. robustness: Larger models generally show better robustness but require more resources

- Failure signatures:
  - NaN or infinite values in gradient computation
  - Empty candidate word sets during attack generation
  - Memory overflow during model loading or fine-tuning
  - Unexpected drops in accuracy unrelated to adversarial attacks

- First 3 experiments:
  1. Verify baseline functionality by running a simple classification task with T5-60m on IMDB dataset
  2. Test the adversarial attack pipeline by generating examples for a single sentence and checking semantic similarity
  3. Compare baseline and attacked accuracy for OPT-1.3b on SST-2 dataset to validate the complete evaluation pipeline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do reinforcement learning with human feedback (RLHF) and other alignment techniques impact the adversarial robustness of large language models?
- Basis in paper: [inferred] The paper mentions that future research could explore models like RLHF within their adversarial robustness framework, suggesting this has not been studied.
- Why unresolved: The paper focuses on pre-trained and fine-tuned LLMs without alignment techniques. RLHF and similar methods could potentially improve robustness but their effects remain unknown.
- What evidence would resolve it: Comparative experiments testing RLHF-aligned models against their base models using the same adversarial attack methodology, measuring differences in attack success rates and accuracy drops.

### Open Question 2
- Question: Does model parallelism approach affect the adversarial robustness of large language models during training and inference?
- Basis in paper: [inferred] The paper suggests future research could explore model parallelism approaches within their framework, indicating this factor has not been examined.
- Why unresolved: All experiments in the paper use standard single-node training/inference setups. Model parallelism could introduce architectural differences that affect gradient flow and attack surfaces.
- What evidence would resolve it: Systematic comparison of model robustness between standard and model-parallel implementations of the same model architecture, using identical datasets and attack methods.

### Open Question 3
- Question: How does the complexity of adversarial attacks evolve with increasingly sophisticated generation methods, and what are the limits of current robustness evaluation approaches?
- Basis in paper: [explicit] The paper states that "the evolution of more complex adversarial attacks promises deeper insights into LLM strengths and weaknesses," indicating this as an open research direction.
- Why unresolved: The paper uses a geometric attack method based on word substitution, which may not represent the full spectrum of potential attack strategies against LLMs.
- What evidence would resolve it: Development and testing of multi-stage attack methods that combine word substitution with prompt engineering, semantic manipulation, or other novel techniques, compared against current approaches.

### Open Question 4
- Question: What is the relationship between model size saturation in accuracy and robustness, and does this relationship vary across different architectural families?
- Basis in paper: [inferred] The paper observes that accuracy tends to saturate after a certain model size threshold, but does not explicitly analyze whether robustness also saturates or if the relationship differs across T5, OPT, and Llama architectures.
- Why unresolved: The paper provides size-related observations but does not systematically analyze saturation points or architectural differences in the robustness-accuracy relationship.
- What evidence would resolve it: Extended experiments across more model sizes per architecture, with detailed analysis of accuracy and robustness trends to identify saturation points and architectural variations.

## Limitations
- The study's empirical scope is limited to five specific text classification datasets, which may not generalize to other NLP tasks or domains
- The geometric adversarial attack methodology lacks comprehensive hyperparameter documentation, making exact reproduction challenging
- Architectural comparisons between encoder-decoder and decoder-only models rely on indirect evidence without mechanistic explanation

## Confidence
**High Confidence Claims:**
- Large language models are demonstrably vulnerable to adversarial attacks, with accuracy drops ranging from 15% to 80%
- Model size generally improves adversarial robustness up to a saturation point
- The geometric attack methodology effectively generates semantically similar adversarial examples

**Medium Confidence Claims:**
- Decoder-only architectures (OPT, Llama) show better robustness than encoder-decoder models (T5)
- Fine-tuning techniques like LoRA and quantization do not significantly affect adversarial resilience
- The relationship between model size and robustness follows a predictable pattern

**Low Confidence Claims:**
- The specific mechanisms explaining architectural differences in robustness
- The universality of the saturation point across different model families and tasks
- The long-term effectiveness of geometric attacks against evolving defensive techniques

## Next Checks
1. **Hyperparameter Sensitivity Analysis**: Systematically vary key attack parameters (gradient thresholds, similarity constraints, candidate filtering) to determine their impact on attack success rates and semantic preservation.

2. **Cross-Domain Generalization Test**: Apply the same evaluation framework to non-text-classification tasks (such as question answering or summarization) using the same model architectures.

3. **Architectural Ablation Study**: Create controlled experiments comparing different architectural components (attention mechanisms, positional encoding, decoder structures) while holding model size constant.