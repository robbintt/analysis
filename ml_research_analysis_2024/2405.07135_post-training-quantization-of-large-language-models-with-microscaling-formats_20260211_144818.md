---
ver: rpa2
title: Post Training Quantization of Large Language Models with Microscaling Formats
arxiv_id: '2405.07135'
source_url: https://arxiv.org/abs/2405.07135
tags:
- quantization
- gptq
- smoothquant
- formats
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies the interaction of three post-training quantization\
  \ (PTQ) algorithms\u2014SmoothQuant, AWQ, and GPTQ\u2014applied to large language\
  \ models (LLMs). The authors extend these algorithms to support microscaling (MX)\
  \ formats, which use shared scaling factors across blocks of values, improving memory\
  \ efficiency over fixed-point formats."
---

# Post Training Quantitation of Large Language Models with Microscaling Formats

## Quick Facts
- **arXiv ID**: 2405.07135
- **Source URL**: https://arxiv.org/abs/2405.07135
- **Reference count**: 40
- **Primary result**: Combining AWQ and GPTQ algorithms synergistically improves PTQ accuracy for LLMs, especially at 4-bit and 3-bit quantization, with MXINT formats outperforming fixed-point formats.

## Executive Summary
This paper investigates the application of three post-training quantization (PTQ) algorithms—SmoothQuant, AWQ, and GPTQ—to large language models (LLMs) with microscaling (MX) formats. MX formats use shared scaling factors across blocks of values, improving memory efficiency over traditional fixed-point formats. The authors systematically evaluate these combinations on Llama2, Llama3.1, and Qwen2 models, measuring perplexity on WikiText-2 and C4 datasets, and accuracy on eight zero-shot commonsense reasoning tasks. Results show that combining AWQ and GPTQ is synergistic, particularly for aggressive quantization (4-bit and 3-bit), while MXINT formats outperform fixed-point formats at the same bit-width. For example, quantizing Llama2-7B to MXINT4 with AWQ and GPTQ achieves a perplexity of 5.53, close to the baseline. The best results are obtained using MXINT4-AWQ-GPTQ, enabling 4-bit weights and 8-bit activations with minimal accuracy loss.

## Method Summary
The authors extend SmoothQuant, AWQ, and GPTQ algorithms to support microscaling (MX) formats, which share scaling factors across blocks of values. They systematically evaluate these combinations on Llama2, Llama3.1, and Qwen2 models, measuring perplexity on WikiText-2 and C4 datasets, and accuracy on eight zero-shot commonsense reasoning tasks. The study quantifies the impact of combining algorithms (AWQ + GPTQ) and microscaling formats on quantization accuracy, especially at aggressive bit-widths (4-bit and 3-bit). MXINT formats are shown to outperform fixed-point formats at the same bit-width, enabling 4-bit weights and 8-bit activations with minimal accuracy loss.

## Key Results
- Combining AWQ and GPTQ algorithms is synergistic, especially for aggressive quantization (4-bit and 3-bit).
- MXINT formats outperform fixed-point formats at the same bit-width.
- Quantizing Llama2-7B to MXINT4 with AWQ and GPTQ achieves a perplexity of 5.53, close to the baseline.
- The best results are obtained using MXINT4-AWQ-GPTQ, enabling 4-bit weights and 8-bit activations with minimal accuracy loss.

## Why This Works (Mechanism)
None provided.

## Foundational Learning
- **Post-training quantization (PTQ)**: Quantization applied after model training to reduce model size and accelerate inference. PTQ is needed because full quantization-aware training is resource-intensive and often impractical for large models. Quick check: Verify PTQ preserves model accuracy by comparing quantized and full-precision outputs on validation data.
- **Microscaling (MX) formats**: Quantization formats that use shared scaling factors across blocks of values, improving memory efficiency over fixed-point formats. MX formats are needed to reduce memory footprint and enable efficient deployment on resource-constrained hardware. Quick check: Measure memory savings and accuracy trade-offs when switching from fixed-point to MX formats.
- **AWQ (Adaptive Weight Quantization)**: A PTQ algorithm that optimizes quantization by analyzing the weight distribution and adjusting scaling factors per layer or block. AWQ is needed to minimize quantization error and preserve model accuracy. Quick check: Compare AWQ against uniform quantization by measuring perplexity on benchmark datasets.
- **GPTQ (Group-wise PTQ)**: A PTQ algorithm that groups weights and applies quantization per group, reducing quantization error. GPTQ is needed to further improve accuracy, especially at low bit-widths. Quick check: Evaluate GPTQ's impact on model accuracy at 4-bit and 3-bit quantization.
- **SmoothQuant**: A PTQ algorithm that smooths the weight distribution before quantization, reducing quantization noise. SmoothQuant is needed to stabilize quantization for models with highly skewed weight distributions. Quick check: Assess SmoothQuant's effectiveness by measuring quantization error on models with varying weight distributions.

## Architecture Onboarding
- **Component map**: LLM model (Llama2, Llama3.1, Qwen2) -> PTQ algorithm (SmoothQuant, AWQ, GPTQ) -> Microscaling format (MXINT) -> Quantized weights/activations
- **Critical path**: Quantize model weights using MXINT-AWQ-GPTQ -> Deploy with 4-bit weights and 8-bit activations -> Measure perplexity and accuracy on benchmark datasets
- **Design tradeoffs**: Memory efficiency (MX formats) vs. quantization accuracy (AWQ/GPTQ combinations); aggressive quantization (4-bit/3-bit) vs. model accuracy
- **Failure signatures**: Large increase in perplexity or accuracy drop when switching from fixed-point to MX formats; model instability or degraded performance with aggressive quantization
- **First experiments**: 1) Quantize Llama2-7B to MXINT4 with AWQ only; 2) Quantize Llama2-7B to MXINT4 with GPTQ only; 3) Combine AWQ and GPTQ for MXINT4 quantization and compare results

## Open Questions the Paper Calls Out
None provided.

## Limitations
- Evaluation is limited to Llama2, Llama3.1, and Qwen2 model families, which may not generalize to other LLM architectures or sizes.
- Focus on perplexity and zero-shot commonsense reasoning tasks does not address downstream task performance or robustness to adversarial inputs.
- No ablation studies on microscaling granularity or block size, limiting insights into scalability and efficiency trade-offs.
- Memory savings and inference latency from MX formats are not quantified in terms of actual hardware utilization.

## Confidence
- **High**: Combining AWQ and GPTQ algorithms is synergistic and improves quantization accuracy, especially at aggressive bit-widths, as supported by direct quantitative comparisons and multiple datasets.
- **Medium**: MXINT formats consistently outperform fixed-point formats at the same bit-width, based on tested model families and tasks, but may not generalize to all architectures or applications.
- **Low**: Claims about practical deployment benefits (such as inference speed or memory usage) are not directly measured or reported.

## Next Checks
1. Extend experiments to additional LLM architectures and sizes (e.g., Mistral, BLOOM) to assess generalizability.
2. Measure actual hardware inference latency and memory usage for MXINT vs fixed-point formats under realistic deployment scenarios.
3. Evaluate downstream task performance and robustness to adversarial inputs to better understand real-world applicability.