---
ver: rpa2
title: Federated Automated Feature Engineering
arxiv_id: '2412.04404'
source_url: https://arxiv.org/abs/2412.04404
tags:
- features
- feature
- each
- autofe
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces three novel algorithms for automated feature
  engineering (AutoFE) in federated learning settings: horizontal, vertical, and hybrid.
  The horizontal algorithm allows each client to perform local AutoFE, then the server
  aggregates and selects the best features using a communication-efficient hyperband-inspired
  method.'
---

# Federated Automated Feature Engineering

## Quick Facts
- arXiv ID: 2412.04404
- Source URL: https://arxiv.org/abs/2412.04404
- Authors: Tom Overman; Diego Klabjan
- Reference count: 25
- Primary result: Horizontal federated AutoFE achieves test scores close to centralized AutoFE baseline

## Executive Summary
This paper introduces three novel algorithms for automated feature engineering (AutoFE) in federated learning settings: horizontal, vertical, and hybrid. The horizontal algorithm allows each client to perform local AutoFE, then the server aggregates and selects the best features using a communication-efficient hyperband-inspired method. The vertical and hybrid algorithms use homomorphic encryption to safely combine features across clients before selection. Experiments on two datasets show that the horizontal algorithm achieves test scores close to the centralized AutoFE baseline, demonstrating that federated AutoFE can perform comparably to centralized approaches while preserving privacy.

## Method Summary
The paper proposes three federated AutoFE algorithms for different federated learning settings. The horizontal algorithm executes local AutoFE on each client, shares only feature string representations to the server, then uses successive halving for efficient feature selection. The vertical and hybrid algorithms employ BFV homomorphic encryption to combine encrypted features across clients, adding Laplace noise for differential privacy before selecting important features. All algorithms use FedAvg for federated training with a Lasso downstream model and AutoFeat as the base AutoFE method.

## Key Results
- Horizontal federated AutoFE achieves test scores comparable to centralized AutoFE baseline
- Feature string representation enables privacy-preserving feature selection without raw data exchange
- Homomorphic encryption enables safe feature combination in vertical and hybrid settings

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The horizontal algorithm preserves feature utility while maintaining privacy by only sharing feature string representations rather than raw data.
- Mechanism: Each client performs local AutoFE independently, then sends only the engineered feature strings (e.g., "log(X001**2/X002)") to the server. The server aggregates these strings and uses a hyperband-inspired successive halving method to select the best features based on federated training performance.
- Core assumption: The string representation of engineered features is sufficient for the server to reconstruct the numerical features on each client without compromising data privacy.
- Evidence anchors:
  - [abstract] "Each client sends the feature string representation of the engineered features to the server. It is important to note that no data is actually sent to the server, only the string representation of the engineered feature such as log(X001**2/X002)."
  - [section] "Then, each client sends the feature string representation of the engineered features to the server. It is important to note that no data is actually sent to the server, only the string representation of the engineered feature such as log(X001**2/X002)."
  - [corpus] Weak evidence - no direct corpus papers discuss this specific string-sharing mechanism.
- Break condition: If feature string representations are not deterministic or if parsing the strings becomes computationally expensive, the mechanism fails.

### Mechanism 2
- Claim: Homomorphic encryption enables safe feature combination across clients in vertical and hybrid settings without exposing raw data.
- Mechanism: Clients encrypt their feature vectors using BFV homomorphic encryption, send them to the server, which then performs combinations of these encrypted features. Laplace noise is added to the encrypted combinations before sending back to clients for decryption.
- Core assumption: BFV homomorphic encryption can handle the required operations (addition and multiplication) while preserving privacy, and the added noise maintains differential privacy.
- Evidence anchors:
  - [abstract] "The vertical and hybrid algorithms use homomorphic encryption to safely combine features across clients before selection."
  - [section] "This is why we first find the feature importance of each feature held on the client, and only the most important features on this client are combined with the most important features on other clients."
  - [corpus] No direct corpus papers discuss this specific BFV-based approach for AutoFE.
- Break condition: If homomorphic encryption operations become too computationally expensive or if the Laplace noise significantly degrades feature utility.

### Mechanism 3
- Claim: Successive halving with varying communication rounds efficiently identifies high-quality features while minimizing communication overhead.
- Mechanism: The algorithm generates random feature masks with varying sparsity levels, trains models with limited communication rounds for each mask, keeps the top-performing masks, and iteratively increases communication rounds until convergence.
- Core assumption: The relationship between communication rounds and feature quality is monotonic, such that features performing well with fewer rounds will also perform well with more rounds.
- Evidence anchors:
  - [abstract] "the server aggregates and selects the best features using a communication-efficient hyperband-inspired method."
  - [section] "Our approach is inspired by Hyperband [6] where successive halving is used with varying resource allocations (in this case the resource is communications between client and server)."
  - [corpus] The corpus contains related work on federated learning and feature engineering but no specific papers on this successive halving approach for AutoFE.
- Break condition: If the feature quality does not improve monotonically with communication rounds, or if the halving ratio eliminates potentially good features too early.

## Foundational Learning

- Concept: Federated Learning Basics
  - Why needed here: Understanding the three FL settings (horizontal, vertical, hybrid) is crucial for implementing the correct algorithm variant.
  - Quick check question: In which FL setting do clients have different features but the same samples?

- Concept: Homomorphic Encryption Fundamentals
  - Why needed here: BFV encryption is used in vertical and hybrid algorithms, requiring understanding of encryption/decryption and noise management.
  - Quick check question: What operations are preserved by BFV homomorphic encryption schemes?

- Concept: Feature Engineering Principles
  - Why needed here: The algorithms build on AutoFE concepts like feature combinations, transformations, and importance scoring.
  - Quick check question: Why might engineered features perform better than raw features in machine learning models?

## Architecture Onboarding

- Component map:
  - Client-side: Local AutoFE execution, feature string generation/parsing, feature reconstruction from strings
  - Server-side: Feature string aggregation, mask generation, successive halving execution, model training orchestration
  - Communication layer: String exchange, encrypted feature exchange (vertical/hybrid), model weights exchange

- Critical path:
  1. Local AutoFE on each client
  2. String representation exchange
  3. Server aggregation and mask generation
  4. Federated model training with successive halving
  5. Feature selection and final model training

- Design tradeoffs:
  - String representation vs. raw data exchange: Privacy preserved but parsing overhead introduced
  - Homomorphic encryption vs. secure aggregation: Better privacy but higher computational cost
  - Successive halving vs. full grid search: Communication efficiency vs. potential suboptimal feature selection

- Failure signatures:
  - Horizontal: Poor performance when feature strings are too complex to parse or when local AutoFE algorithms are not well-suited to local data distributions
  - Vertical/Hybrid: High latency due to encryption/decryption overhead or significant performance degradation from added Laplace noise
  - All settings: Communication bottlenecks when number of engineered features or clients is very large

- First 3 experiments:
  1. Baseline test: Run centralized AutoFE and compare performance metrics to establish ground truth
  2. Communication sensitivity test: Vary the number of communication rounds in successive halving to find optimal tradeoff
  3. Privacy-utility tradeoff analysis: Measure performance degradation as Laplace noise scale parameter increases in vertical/hybrid settings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of federated AutoFE algorithms scale with increasing numbers of clients in the horizontal setting?
- Basis in paper: [inferred] The paper only tested with 8 clients and did not explore scenarios with larger client populations.
- Why unresolved: The current experiments provide limited evidence on how the algorithm performs as the number of clients grows, which is crucial for real-world federated learning applications.
- What evidence would resolve it: Conducting experiments with varying numbers of clients (e.g., 10, 50, 100) and measuring the impact on communication overhead, model performance, and convergence time would provide insights into scalability.

### Open Question 2
- Question: How do federated AutoFE algorithms perform under non-IID data distributions across clients?
- Basis in paper: [inferred] The paper tested only with IID data distributions, but real-world federated learning scenarios often involve non-IID data.
- Why unresolved: The paper's results are based on IID data, which does not reflect the complexities of real-world federated learning environments where data distributions can vary significantly across clients.
- What evidence would resolve it: Testing the algorithms under various non-IID data distributions (e.g., different feature distributions, label distributions) and comparing the performance to the IID case would demonstrate the robustness of the algorithms.

### Open Question 3
- Question: What is the impact of different AutoFE algorithms on the performance of federated AutoFE in the horizontal setting?
- Basis in paper: [inferred] The paper used AutoFeat as the base AutoFE algorithm but did not explore the effects of using different AutoFE methods.
- Why unresolved: The choice of AutoFE algorithm could significantly influence the quality of engineered features and the overall performance of federated AutoFE, but this has not been explored.
- What evidence would resolve it: Experimenting with various AutoFE algorithms (e.g., OpenFE, IIFE, EAAFE) in the federated setting and comparing their performance would reveal the optimal choices for different scenarios.

## Limitations
- Narrow experimental scope: Only two datasets tested with 8 clients in IID settings
- No empirical validation of vertical and hybrid algorithms
- Limited hyperparameter sensitivity analysis

## Confidence
- Core performance claims: Medium confidence
- Privacy guarantees of string-sharing mechanism: Low confidence
- Homomorphic encryption implementation feasibility: Low confidence

## Next Checks
1. Implement and benchmark the vertical and hybrid algorithms on the same datasets to validate the theoretical privacy-utility tradeoffs claimed
2. Conduct security analysis of the feature string representation approach to quantify potential information leakage
3. Perform extensive hyperparameter sensitivity analysis on successive halving parameters across diverse dataset sizes and client counts