---
ver: rpa2
title: 'Cross Entropy versus Label Smoothing: A Neural Collapse Perspective'
arxiv_id: '2402.03979'
source_url: https://arxiv.org/abs/2402.03979
tags:
- loss
- neural
- smoothing
- label
- collapse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the convergence properties of label smoothing
  (LS) and cross-entropy (CE) losses through the lens of neural collapse. Using the
  unconstrained feature model, the authors derive closed-form solutions for global
  minimizers under both losses and show that LS loss has a smaller condition number
  in the optimization landscape around the global minimum.
---

# Cross Entropy versus Label Smoothing: A Neural Collapse Perspective

## Quick Facts
- arXiv ID: 2402.03979
- Source URL: https://arxiv.org/abs/2402.03979
- Reference count: 40
- Key outcome: Label smoothing loss exhibits a more well-conditioned optimization landscape around the global minimum compared to cross-entropy, leading to faster convergence to neural collapse solutions and improved generalization.

## Executive Summary
This paper analyzes the convergence properties of label smoothing (LS) and cross-entropy (CE) losses through the lens of neural collapse. Using the unconstrained feature model, the authors derive closed-form solutions for global minimizers under both losses and show that LS loss has a smaller condition number in the optimization landscape around the global minimum. Empirically, models trained with LS loss converge faster to neural collapse solutions, achieving stronger levels of both NC1 (within-class variability) and NC2 (simplex ETF structure). The LS loss implicitly enforces equal logits among non-target classes, promoting better generalization. Theoretical analysis reveals that LS loss results in a more well-conditioned optimization landscape, explaining the observed accelerated convergence.

## Method Summary
The authors use the Unconstrained Feature Model (UFM) to analyze the global minimizers of both CE and LS losses, deriving closed-form solutions and comparing the condition numbers of their Hessian matrices. They conduct empirical experiments on CIFAR-10, CIFAR-100, and STL-10 datasets using ResNet-18/50 architectures, training for 800 epochs with SGD optimizer, weight decay, and multi-step learning rate decay. Neural collapse metrics (NC1, NC2, NC3) and expected calibration error (ECE) are computed throughout training to compare the behavior of CE and LS losses. The impact of different label smoothing values (δ) on model performance is also studied.

## Key Results
- Label smoothing loss exhibits a more well-conditioned optimization landscape around the global minimum, leading to faster convergence to neural collapse solutions.
- Models trained with LS loss achieve stronger levels of NC1 (lower within-class variability) and NC2 (simplex ETF structure) compared to CE loss.
- LS loss implicitly enforces equal logits among non-target classes, promoting better generalization.
- LS loss leads to improved model calibration by implicitly regularizing the weights and features during training, mitigating overconfidence.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Label smoothing loss results in a more well-conditioned optimization landscape around the global minimum, leading to faster convergence.
- Mechanism: The condition number of the Hessian matrix (ratio of largest to smallest eigenvalue) is smaller under label smoothing loss compared to cross-entropy loss, indicating a more favorable optimization landscape.
- Core assumption: The condition number of the Hessian is a key factor in determining convergence rate.
- Evidence anchors:
  - [abstract] "Our mathematical analysis reveals that LS exhibits a more well-conditioned landscape around the global minimum, which facilitates the faster convergence observed in our empirical study."
  - [section] "Within the UFM context, our mathematical analysis reveals that LS exhibits a more well-conditioned landscape around the global minimum, which facilitates the faster convergence observed in our empirical study."
  - [corpus] Weak evidence. The corpus contains papers on related topics (e.g., "The Persistence of Neural Collapse Despite Low-Rank Bias", "Neural Collapse for Cross-entropy Class-Imbalanced Learning with Unconstrained ReLU Feature Model") but no direct evidence supporting the claim about condition number and convergence rate.
- Break condition: If the relationship between condition number and convergence rate is not as assumed, or if the Hessian analysis is not valid for the specific problem.

### Mechanism 2
- Claim: Label smoothing loss implicitly enforces equal logits among non-target classes, promoting better generalization.
- Mechanism: Label smoothing loss includes an inductive bias towards equalizing the logits of non-target classes, which aligns with the definition of NC2 and promotes the development of maximally separable features and classifiers.
- Core assumption: Equalizing the logits of non-target classes leads to better generalization.
- Evidence anchors:
  - [abstract] "The LS loss implicitly enforces equal logits among non-target classes, promoting better generalization."
  - [section] "This implies that LS loss reaches its minimum only if the predicted probabilities for non-target classes are all equal. Thus, LS loss implicitly includes an inductive bias towards equalizing the logits of the non-target classes, which is not present in CE loss."
  - [corpus] Weak evidence. The corpus contains papers on related topics (e.g., "Calibrated Language Models and How to Find Them with Label Smoothing") but no direct evidence supporting the claim about the inductive bias and generalization.
- Break condition: If equalizing the logits of non-target classes does not lead to better generalization, or if the inductive bias is not present in the label smoothing loss.

### Mechanism 3
- Claim: Label smoothing loss leads to improved model calibration by implicitly regularizing the weights and features during training.
- Mechanism: Label smoothing loss results in significantly smaller norms for the classification weight vectors and the last-layer features, mitigating the overconfidence issue in the model and improving calibration.
- Core assumption: Smaller norms for the classification weight vectors and the last-layer features lead to improved model calibration.
- Evidence anchors:
  - [abstract] "Through comprehensive empirical analysis, we demonstrate an excessive level of NC1 can make the model overconfident in its predictions, even when they are incorrect. Models trained under LS loss exhibit improved calibration by implicitly regularizing the weights and features during training."
  - [section] "By increasing the norm of the classification weight vectors or the feature embeddings, we amplify the logits Z, thereby increasing the confidence of the network's predictions. Notably, the cross-entropy loss is minimized when ∥Z∥ → ∞. In contrast, models trained with LS loss exhibit significantly smaller norms for the classification weight vectors and the last-layer features, as illustrated in the rightmost column of Figure 1. The lower norms effectively mitigate the overconfidence issue in the model."
  - [corpus] Weak evidence. The corpus contains papers on related topics (e.g., "Calibrated Language Models and How to Find Them with Label Smoothing") but no direct evidence supporting the claim about the regularization effect and calibration.
- Break condition: If smaller norms for the classification weight vectors and the last-layer features do not lead to improved model calibration, or if the regularization effect is not present in the label smoothing loss.

## Foundational Learning

- Concept: Neural Collapse (NC)
  - Why needed here: Understanding NC is crucial for interpreting the results of this paper, as the authors analyze the convergence properties of label smoothing and cross-entropy losses through the lens of NC.
  - Quick check question: What are the key properties of neural collapse, and how do they relate to the behavior of deep neural networks during training?

- Concept: Unconstrained Feature Model (UFM)
  - Why needed here: The UFM is used in this paper to derive closed-form solutions for the global minimizers under both label smoothing and cross-entropy losses, and to analyze the optimization landscape around these solutions.
  - Quick check question: How does the UFM simplify the analysis of deep neural networks, and what are its key assumptions and limitations?

- Concept: Condition Number of the Hessian Matrix
  - Why needed here: The condition number of the Hessian matrix is used in this paper to analyze the convergence properties of label smoothing and cross-entropy losses, with a smaller condition number indicating a more well-conditioned optimization landscape and faster convergence.
  - Quick check question: How is the condition number of the Hessian matrix calculated, and what does it tell us about the optimization landscape and convergence rate?

## Architecture Onboarding

- Component map: Deep Neural Network (DNN) with feature extractor and linear classifier -> Loss function (label smoothing or cross-entropy) -> Unconstrained Feature Model (UFM) for analysis -> Neural Collapse (NC) framework for interpretation
- Critical path:
  1. Define the problem and the UFM
  2. Derive closed-form solutions for the global minimizers under both label smoothing and cross-entropy losses
  3. Analyze the optimization landscape around these solutions using the condition number of the Hessian matrix
  4. Conduct empirical experiments to validate the theoretical findings
- Design tradeoffs:
  - Complexity of the UFM vs. accuracy of the analysis
  - Choice of loss function (label smoothing vs. cross-entropy) and its impact on convergence and generalization
  - Tradeoff between model calibration and convergence rate
- Failure signatures:
  - Divergence or slow convergence during training
  - Poor generalization on test data
  - Overconfidence in predictions
- First 3 experiments:
  1. Implement the UFM and derive the closed-form solutions for the global minimizers under both label smoothing and cross-entropy losses
  2. Calculate the condition number of the Hessian matrix around these solutions and compare the results
  3. Conduct empirical experiments on a small dataset (e.g., MNIST) to validate the theoretical findings and observe the behavior of label smoothing and cross-entropy losses during training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mathematical relationship between the smoothing parameter δ and the conditioning number of the Hessian matrix?
- Basis in paper: [explicit] The paper states that as δ increases, pt decreases, leading to a smaller condition number, but does not provide a closed-form expression for this relationship.
- Why unresolved: The paper provides empirical evidence and theoretical bounds but does not derive an explicit formula for how the conditioning number depends on δ.
- What evidence would resolve it: A mathematical derivation showing the exact dependence of the conditioning number on δ would resolve this question.

### Open Question 2
- Question: How does the choice of δ affect the trade-off between model calibration and neural collapse properties?
- Basis in paper: [inferred] The paper discusses the impact of δ on NC1, NC2, and model calibration, but does not provide a systematic study of the trade-offs involved.
- Why unresolved: While the paper presents empirical results for specific values of δ, it does not explore the full range of δ or provide a theoretical framework for understanding the trade-offs.
- What evidence would resolve it: A comprehensive study varying δ across its full range, combined with theoretical analysis of the trade-offs, would resolve this question.

### Open Question 3
- Question: Can the insights from the unconstrained feature model be extended to more complex deep neural network architectures?
- Basis in paper: [explicit] The paper uses the unconstrained feature model as a simplification to analyze the convergence properties of CE and LS losses.
- Why unresolved: The unconstrained feature model is a simplification, and it is unclear how well the insights generalize to more complex architectures.
- What evidence would resolve it: Empirical studies on a variety of deep neural network architectures, combined with theoretical analysis extending the unconstrained feature model, would resolve this question.

### Open Question 4
- Question: How does the choice of loss function (CE vs. LS) interact with other regularization techniques, such as dropout or weight decay?
- Basis in paper: [inferred] The paper focuses on the comparison between CE and LS losses, but does not explore their interaction with other regularization techniques.
- Why unresolved: The paper does not provide empirical or theoretical analysis of how CE and LS losses interact with other regularization methods.
- What evidence would resolve it: Empirical studies comparing the performance of CE and LS losses with and without other regularization techniques would resolve this question.

## Limitations

- The theoretical analysis relies on the Unconstrained Feature Model, which may not fully capture the complexities of practical deep neural networks.
- The empirical validation is based on a relatively narrow set of experiments, focusing on specific datasets and architectures.
- The study does not explore the impact of different label smoothing values (δ) on model performance comprehensively.

## Confidence

- Mechanism 1 (Condition Number and Convergence Rate): Medium confidence
- Mechanism 2 (Equal Logits and Generalization): Medium confidence
- Mechanism 3 (Regularization and Calibration): Low confidence

## Next Checks

1. Extend the theoretical analysis to other models and architectures, such as Vision Transformers, to assess the generalizability of the findings.
2. Conduct a more comprehensive study of the impact of different label smoothing values (δ) on model performance, including their effects on convergence, generalization, and calibration.
3. Validate the findings on larger and more diverse datasets, such as ImageNet, to assess the robustness of the conclusions across different domains and scales.