---
ver: rpa2
title: Video Quality Assessment Based on Swin TransformerV2 and Coarse to Fine Strategy
arxiv_id: '2401.08522'
source_url: https://arxiv.org/abs/2401.08522
tags:
- quality
- assessment
- video
- loss
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a no-reference video quality assessment method
  using Swin TransformerV2 and a coarse-to-fine contrastive strategy. The model incorporates
  an enhanced spatial perception module pre-trained on multiple image quality datasets
  and a lightweight temporal fusion module for spatiotemporal feature extraction.
---

# Video Quality Assessment Based on Swin TransformerV2 and Coarse to Fine Strategy

## Quick Facts
- arXiv ID: 2401.08522
- Source URL: https://arxiv.org/abs/2401.08522
- Authors: Zihao Yu; Fengbin Guan; Yiting Lu; Xin Li; Zhibo Chen
- Reference count: 10
- Primary result: Achieves PLCC 0.924 and SRCC 0.903 on compressed video quality assessment

## Executive Summary
This paper presents a no-reference video quality assessment method that leverages Swin TransformerV2 for spatial feature extraction and introduces a coarse-to-fine contrastive strategy to handle videos with varying compression bitrates. The approach combines pre-training on multiple image quality datasets with a novel two-stage contrastive learning framework that first separates videos by bitrate groups and then maintains discriminative ability within each group. Experimental results demonstrate strong performance on compressed video quality assessment tasks while maintaining computational efficiency through the Swin TransformerV2 architecture.

## Method Summary
The method employs Swin TransformerV2 as a local-level spatial feature extractor pre-trained on image quality assessment datasets including CLIVE, LIVE, KonIQ-10k, and KADID-10K. A lightweight temporal fusion module performs spatiotemporal feature extraction, followed by a quality-aware fully connected layer. The key innovation is a coarse-to-fine contrastive strategy combining group contrastive loss for bitrate discrimination and rank loss for within-bitrate quality discrimination. The model uses pseudo-labels created from VMAF scores and is trained with a batch size of 8 on NVIDIA 1080Ti GPU.

## Key Results
- Achieves PLCC of 0.924 and SRCC of 0.903 on compressed video quality assessment
- Swin TransformerV2 provides efficient local spatial feature extraction while maintaining computational efficiency
- Coarse-to-fine contrastive strategy enables effective handling of videos with varying compression bitrates
- Pre-training on multiple BIQA datasets enhances spatial feature extraction capability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Swin TransformerV2 provides efficient local-level spatial feature extraction while maintaining computational efficiency
- Mechanism: The shifted window mechanism partitions images into non-overlapping windows, allowing self-attention computation within local regions. This hierarchical approach captures multi-scale spatial features while keeping computational complexity manageable compared to global attention.
- Core assumption: Local self-attention windows are sufficient to capture the spatial distortions relevant to video quality assessment
- Evidence anchors:
  - [abstract]: "This model implements Swin Transformer V2 as a local-level spatial feature extractor"
  - [section]: "In the spatial feature extraction part, we feed the sampled frames into SwinTransformerV2 [1]. SwinTransformerV2 is capable of obtaining local features of video frames at different levels."

### Mechanism 2
- Claim: The coarse-to-fine contrastive strategy enables effective handling of videos with varying compression bitrates
- Mechanism: Group contrastive loss creates separation between different bitrate groups by maximizing inter-group distances and minimizing intra-group distances. Rank loss within each bitrate group ensures the model maintains discriminative ability to distinguish subtle quality differences at the same compression level.
- Core assumption: Videos with different bitrates exhibit distinct feature distributions that can be separated through contrastive learning
- Evidence anchors:
  - [abstract]: "To accommodate compressed videos of varying bitrates, we incorporate a coarse-to-fine contrastive strategy to enrich the model's capability to discriminate features from videos of different bitrates"
  - [section]: "The Group Contrastive loss minimizes the feature similarity within the same bitrate group and maximizes the feature similarity of the different bitrate groups"

### Mechanism 3
- Claim: Pre-training on multiple image quality assessment datasets enhances spatial feature extraction capability
- Mechanism: Transfer learning from diverse BIQA datasets provides the model with rich prior knowledge about image quality features, allowing it to better recognize and quantify distortions in video frames without requiring extensive video-specific training data.
- Core assumption: Image quality features are transferable to video quality assessment tasks
- Evidence anchors:
  - [abstract]: "We pre-train the spatial module on datasets such as CLIVE[3], LIVE[4], KonIQ-10k[5], and KADID-10K[6]"
  - [section]: "We pre-trained the spatial module on datasets such as CLIVE[3], LIVE[4], KonIQ-10k[5], and KADID-10K[6]. The diversity and extensiveness of these datasets provide our model with a wealth of training samples for enhancing image-level perception."

## Foundational Learning

- Concept: Swin Transformer architecture and shifted window mechanism
  - Why needed here: Understanding how Swin TransformerV2 partitions images into windows and shifts them across layers is crucial for grasping why it's effective for spatial feature extraction in video quality assessment
  - Quick check question: What is the computational complexity difference between global self-attention and windowed self-attention in Swin Transformers?

- Concept: Contrastive learning and loss functions
  - Why needed here: The coarse-to-fine strategy relies on understanding group contrastive loss and rank loss, including how temperature parameters and margin values affect feature separation
  - Quick check question: How does the temperature parameter τ in the group contrastive loss affect the hardness of negative samples?

- Concept: Transfer learning and pre-training strategies
  - Why needed here: The model's performance depends on effective pre-training on BIQA datasets, requiring understanding of how knowledge transfers from image to video quality assessment
  - Quick check question: What factors determine whether pre-trained features from image datasets will generalize to video quality assessment tasks?

## Architecture Onboarding

- Component map: Spatial feature extraction module (Swin TransformerV2) → Feature pooling and fusion → Quality-aware fully connected layer → Coarse-to-fine contrastive strategy (group contrastive loss + rank loss) → Temporal fusion module → Final quality prediction
- Critical path: Frame sampling → Spatial feature extraction → Temporal fusion → Quality prediction
- Design tradeoffs: The choice of Swin TransformerV2 balances computational efficiency with feature extraction capability, while the coarse-to-fine strategy adds complexity but enables bitrate-aware quality assessment
- Failure signatures: Poor PLCC/SRCC scores, especially when testing across different bitrates; model failing to distinguish videos within the same bitrate group
- First 3 experiments:
  1. Test the spatial module independently on a BIQA dataset to verify pre-training effectiveness
  2. Evaluate the impact of removing group contrastive loss while keeping rank loss to measure coarse-level bitrate discrimination
  3. Assess temporal fusion module performance by comparing with and without temporal modeling on a video quality dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the coarse-to-fine contrastive strategy perform on other VQA datasets beyond compressed videos?
- Basis in paper: [explicit] The authors note their strategy is "helpful for VQA tasks on compressed videos at different bit rates" but do not test on broader VQA datasets
- Why unresolved: The experiments only evaluate performance on compressed video quality assessment using pseudo-labels from VMAF, without testing on standard VQA datasets with human ratings
- What evidence would resolve it: Direct evaluation on established VQA databases like LIVE Video Quality Database or CSIQ-Video using human-rated scores

### Open Question 2
- Question: What is the optimal balance between group contrastive loss and rank loss parameters (λ1 and λ2) for different compression levels?
- Basis in paper: [inferred] The authors use fixed values for λ1 and λ2 in their experiments but do not explore sensitivity to these hyperparameters
- Why unresolved: The paper does not report ablation studies or sensitivity analysis on the loss weight parameters
- What evidence would resolve it: Systematic evaluation showing performance variation with different λ1/λ2 ratios across various compression scenarios

### Open Question 3
- Question: How does the temporal fusion module's performance compare to alternative lightweight architectures for spatiotemporal feature extraction?
- Basis in paper: [explicit] The authors state they use "a lightweight temporal fusion module" but do not compare against other temporal architectures
- Why unresolved: The paper does not provide ablation studies comparing different temporal fusion approaches
- What evidence would resolve it: Head-to-head comparison of the proposed temporal transformer with alternatives like 3D convolutions or other lightweight temporal modules on compressed video quality assessment tasks

## Limitations
- Reliance on VMAF pseudo-labels for training creates uncertainty about whether the model learns perceptual quality or just VMAF's specific metric
- Swin TransformerV2's local window approach may miss global spatial distortions that span across window boundaries
- Coarse-to-fine contrastive strategy assumes bitrate variations create distinct feature distributions, which may not hold for all compression artifacts

## Confidence

- **High confidence**: Swin TransformerV2's effectiveness as a spatial feature extractor (well-established architecture with proven results)
- **Medium confidence**: Pre-training on BIQA datasets transferring to video quality assessment (reasonable assumption but not extensively validated)
- **Medium confidence**: Coarse-to-fine contrastive strategy improving bitrate-aware quality assessment (plausible mechanism but dependent on specific dataset characteristics)

## Next Checks

1. Cross-dataset validation: Test the pre-trained spatial module on multiple BIQA datasets to verify transfer learning effectiveness before video quality assessment
2. Ablation study on contrastive losses: Systematically remove group contrastive loss and rank loss separately to quantify their individual contributions to performance
3. Window size sensitivity analysis: Evaluate model performance across different Swin TransformerV2 window sizes to determine optimal local context for quality assessment