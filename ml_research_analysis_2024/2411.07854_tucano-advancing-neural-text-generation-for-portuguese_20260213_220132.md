---
ver: rpa2
title: 'Tucano: Advancing Neural Text Generation for Portuguese'
arxiv_id: '2411.07854'
source_url: https://arxiv.org/abs/2411.07854
tags:
- arxiv
- language
- portuguese
- training
- preprint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study presents Tucano, a new set of resources aimed at advancing
  neural text generation for Portuguese. We introduce GigaVerbo, a deduplicated corpus
  of 200 billion tokens, and develop Tucano, a series of decoder-transformer models
  trained on this corpus.
---

# Tucano: Advancing Neural Text Generation for Portuguese

## Quick Facts
- **arXiv ID**: 2411.07854
- **Source URL**: https://arxiv.org/abs/2411.07854
- **Reference count**: 40
- **Primary result**: Tucano models outperform or match other Portuguese and multilingual models of similar size across benchmarks using a 200B token corpus

## Executive Summary
This study introduces Tucano, a new suite of resources for advancing neural text generation in Portuguese. The authors develop GigaVerbo, a massive 200 billion token corpus derived from multiple Portuguese datasets, and train a series of decoder-transformer models (Tucano) on this data. The models demonstrate equal or superior performance compared to other Portuguese and multilingual language models of similar size across various benchmarks. The work also introduces learned filters for data quality improvement and provides critical assessment of existing Portuguese language model benchmarks.

## Method Summary
The authors created GigaVerbo by deduplicating and filtering a 145 million document corpus (780 GB) from multiple Portuguese sources including monoHPLT-PT, CrawlPT, Wikipedia, and others. They implemented a learned filter using GPT-4o to remove low-quality samples, retaining approximately 65% of the data. Tucano models were built using a decoder-only transformer architecture based on Llama, trained with causal language modeling objectives using AdamW optimization. The models were evaluated across multiple benchmarks including ENEM, BLUEX, ASSIN2, and others using a custom evaluation harness.

## Key Results
- Tucano models achieve equal or superior performance to other Portuguese and multilingual models of similar size across multiple benchmarks
- Larger models exhibit more significant reduction in loss and perplexity during training
- Learned filters successfully identified and removed approximately 50 million low-quality samples from the corpus

## Why This Works (Mechanism)

### Mechanism 1: Model Scaling
Larger models trained on more tokens show reduced loss and perplexity across training. As model capacity and training data increase, the model can better capture linguistic patterns, leading to improved generalization and lower perplexity. This relationship is monotonic and predictable based on scaling laws.

### Mechanism 2: Learned Data Filtering
Learned filters improve data quality by removing low-quality samples, leading to better model performance. The model learns to distinguish high-quality text from low-quality text, allowing the dataset to be filtered before training, which enhances the signal-to-noise ratio.

### Mechanism 3: Custom Tokenization
Custom tokenizers tailored for Portuguese improve model efficiency and performance compared to generic multilingual tokenizers. Domain-specific tokenizers can better compress Portuguese text, reducing the number of tokens required to represent the same amount of information, which improves model efficiency and potentially performance.

## Foundational Learning

- **Concept: Scaling Laws**
  - Why needed here: Understanding how model performance scales with size and data is crucial for interpreting the results and designing future experiments.
  - Quick check question: What is the relationship between model size, data quantity, and performance according to scaling laws?

- **Concept: Data Filtering and Quality**
  - Why needed here: The effectiveness of the learned filter in improving data quality is a key factor in the success of the model.
  - Quick check question: How does the learned filter distinguish between high-quality and low-quality text?

- **Concept: Tokenization**
  - Why needed here: The choice of tokenizer significantly impacts model efficiency and performance, especially for low-resource languages like Portuguese.
  - Quick check question: How does a custom tokenizer tailored for Portuguese compare to generic multilingual tokenizers in terms of compression efficiency?

## Architecture Onboarding

- **Component map**: Data preprocessing -> Tokenizer training -> Model training -> Evaluation and analysis
- **Critical path**: GigaVerbo corpus creation → Learned filtering → Tucano tokenizer training → Decoder-only transformer model training → Benchmark evaluation
- **Design tradeoffs**:
  - Larger models require more computational resources but offer better performance
  - Custom tokenizers improve efficiency but require domain expertise to develop
  - Filtering data improves quality but risks removing valuable information if not done carefully
- **Failure signatures**:
  - Model fails to converge: Check learning rate, batch size, and gradient accumulation settings
  - High perplexity: Investigate data quality, tokenizer efficiency, and model architecture
  - Poor performance on benchmarks: Evaluate data filtering, benchmark relevance, and potential overfitting
- **First 3 experiments**:
  1. Train a small model (160m parameters) on the filtered GigaVerbo dataset and evaluate its performance on a subset of benchmarks
  2. Compare the Tucano tokenizer to a generic multilingual tokenizer on a sample of Portuguese text to measure compression efficiency
  3. Implement the learned filter on a small sample of GigaVerbo and manually verify its accuracy in identifying high-quality text

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of Tucano models change when evaluated on domain-specific benchmarks (e.g., legal, biomedical, technical) that were not part of the pretraining data? The paper evaluates Tucano on general Portuguese benchmarks but does not explore performance on specialized domains despite mentioning domain-specific models like BioBERTpt and PetroBERT.

### Open Question 2
What is the impact of different filtering approaches (e.g., heuristic vs. learned filters) on model performance and training efficiency? The paper implements a learned filter using GPT-4o but does not compare its effectiveness to other filtering methods or analyze its impact on training outcomes.

### Open Question 3
How does the size and quality of pretraining data interact with model architecture choices (e.g., number of layers, attention mechanisms) to influence performance on low-resource languages? The paper scales up pretraining data to 200B tokens and uses standard transformer architectures, but does not explore how different architectural choices interact with data scaling.

## Limitations

- **Data Quality Impact**: The paper lacks quantitative analysis showing how filtering affected final model performance, with no direct validation against models trained on unfiltered data.
- **Benchmark Representativeness**: Current Portuguese benchmarks have limitations including dataset size constraints and translation quality issues, with evaluation relying on both native and translated benchmarks.
- **Reproducibility Constraints**: Computational requirements for training larger models (8-96 A100 GPUs) create significant barriers to independent verification, and ablation studies are absent.

## Confidence

- **High Confidence**: Claims about model scaling behavior are well-supported by training curves and align with established scaling laws
- **Medium Confidence**: Performance comparisons against other models are credible but the impact of preprocessing choices on these results remains unverified
- **Low Confidence**: Claims about the effectiveness of the learned filter and custom tokenizer improvements lack direct experimental validation

## Next Checks

1. **Filter Impact Validation**: Train two versions of Tucano-160m - one on the full GigaVerbo corpus and one on the filtered subset - then compare performance across the same benchmark suite to quantify the filter's actual contribution to model quality.

2. **Tokenizer Efficiency Benchmark**: Conduct a controlled comparison where identical Portuguese text is tokenized using both the Tucano tokenizer and a generic multilingual tokenizer, measuring compression ratios, token counts, and downstream perplexity on a held-out validation set.

3. **Benchmark Translation Quality Assessment**: Evaluate the translation quality of translated English benchmarks used in the study by having native Portuguese speakers rate translation fidelity, then analyze whether performance differences correlate with translation quality variations.