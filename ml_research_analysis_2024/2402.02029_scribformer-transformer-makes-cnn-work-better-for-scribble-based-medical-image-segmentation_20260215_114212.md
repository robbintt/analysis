---
ver: rpa2
title: 'ScribFormer: Transformer Makes CNN Work Better for Scribble-based Medical
  Image Segmentation'
arxiv_id: '2402.02029'
source_url: https://arxiv.org/abs/2402.02029
tags:
- segmentation
- image
- medical
- transformer
- unet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a new CNN-Transformer hybrid solution for scribble-supervised
  medical image segmentation called ScribFormer. The main challenge addressed is learning
  global shape information from limited scribble annotations using CNN's local receptive
  field.
---

# ScribFormer: Transformer Makes CNN Work Better for Scribble-based Medical Image Segmentation

## Quick Facts
- **arXiv ID:** 2402.02029
- **Source URL:** https://arxiv.org/abs/2402.02029
- **Reference count:** 40
- **Primary result:** Achieves Dice scores of 88.8%, 83.9%, and 83.3% on ACDC, MSCMRseg, and HeartUII datasets respectively, outperforming state-of-the-art scribble-supervised methods and even some fully-supervised approaches

## Executive Summary
This paper proposes ScribFormer, a novel hybrid CNN-Transformer architecture for scribble-based medical image segmentation that addresses the challenge of learning global shape information from limited scribble annotations. The method combines a CNN branch for local feature extraction, a Transformer branch for capturing global contextual relationships, and an attention-guided class activation map (ACAM) branch for improved feature localization. Through extensive experiments on three public medical imaging datasets, ScribFormer demonstrates superior performance compared to existing scribble-supervised methods while even surpassing some fully-supervised approaches.

## Method Summary
ScribFormer employs a hybrid architecture that integrates CNN and Transformer components through Feature Coupling Units (FCUs) to fuse local and global features at multiple scales. The model uses partial cross-entropy loss for scribble supervision and incorporates dynamic pseudo-label mixing to leverage complementary strengths of both branches. An ACAM branch with channel and spatial attention modules generates attention-guided maps to address the partial activation issue of traditional CAMs, with ACAM-consistency loss regularizing shallow layers based on deep layer supervision.

## Key Results
- Achieves Dice scores of 88.8%, 83.9%, and 83.3% on ACDC, MSCMRseg, and HeartUII datasets respectively
- Outperforms state-of-the-art scribble-supervised methods by significant margins
- Even surpasses some fully-supervised methods in segmentation accuracy
- Demonstrates the effectiveness of combining local CNN features with global Transformer representations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The hybrid CNN-Transformer architecture overcomes CNN's local receptive field limitation by fusing local features with global representations.
- Mechanism: CNN branch captures local spatial details while Transformer branch captures global contextual relationships through self-attention; FCU (Feature Coupling Units) aligns and combines these features at multiple scales.
- Core assumption: Global context is essential for scribble-supervised segmentation because scribbles provide incomplete boundary information.
- Evidence anchors:
  - [abstract] "The Transformer branch captures global representations while the CNN branch provides local features."
  - [section] "The hybrid CNN and Transformer branches... fuse the local features learned from CNN with the global representations obtained from Transformers, which can effectively overcome limitations of existing scribble-supervised segmentation methods."
- Break condition: If the global context captured by Transformer does not improve segmentation accuracy over CNN alone.

### Mechanism 2
- Claim: ACAM branch addresses partial activation issue of CAMs by generating attention-guided maps that restore missed activation regions.
- Mechanism: ACAMs combine channel attention modulation and spatial attention modulation to identify both major and minor features across all encoding layers, then use ACAM-consistency loss to regularize shallow layers based on deep layer supervision.
- Core assumption: Traditional CAMs only highlight the most discriminative part of objects, missing other relevant regions.
- Evidence anchors:
  - [abstract] "ACAM branch assists in unifying the shallow convolution features and the deep convolution features to improve model's performance further."
  - [section] "ACAMs-consistency is employed to penalize inconsistent feature maps from different convolution layers, in which the low-level ACAMs are regularized by the high-level ACAM generated from feature of last CNN-Transformer branch layer."
- Break condition: If ACAMs do not show more complete activation regions compared to traditional CAMs.

### Mechanism 3
- Claim: Dynamic pseudo-label mixing leverages complementary strengths of CNN and Transformer branches.
- Mechanism: CNN prediction and Transformer prediction are dynamically mixed with varying weights (α, β) to generate hard pseudo labels, which are then used to supervise both branches separately.
- Core assumption: CNN and Transformer branches capture different aspects of the segmentation task, and their combination improves robustness.
- Evidence anchors:
  - [section] "we further explore their outputs to boost the model training... pseudo label generated by mixing two predictions dynamically for pseudo-supervised learning."
- Break condition: If fixed pseudo-labels or single-branch supervision performs equally well.

## Foundational Learning

- **Vision Transformer architecture**
  - Why needed here: Understanding how self-attention captures global context is crucial for grasping why the Transformer branch helps overcome CNN's local receptive field limitation.
  - Quick check question: What is the key difference between how CNN and Transformer process spatial information?

- **Partial cross-entropy loss**
  - Why needed here: This loss function is essential for scribble-supervised learning as it ignores unlabeled pixels in scribble annotations.
  - Quick check question: How does partial cross-entropy loss differ from standard cross-entropy loss in terms of handling unlabeled pixels?

- **Class Activation Maps (CAMs)**
  - Why needed here: Understanding CAM limitations (partial activation) explains why ACAMs were developed as an improvement.
  - Quick check question: What is the main limitation of traditional CAMs that ACAMs aim to address?

## Architecture Onboarding

- **Component map:** Input → CNN Encoder (ResNet-style blocks) → FCU → Transformer Encoder (MHSA+MLP) → FCU → CNN Decoder (UNet-style) → Output
  - Parallel ACAM branch: CNN features → Channel/Spatial attention modules → ACAM generation → Consistency loss
  - Scribble supervision: Partial cross-entropy on both CNN and Transformer predictions
  - Pseudo-supervision: Dynamic mixing of predictions → Hard pseudo labels → Dice loss

- **Critical path:** CNN Encoder → FCU → Transformer Encoder → FCU → CNN Decoder (for final segmentation output)

- **Design tradeoffs:**
  - Higher parameter count and computational complexity vs. improved segmentation accuracy
  - Using Transformer adds global context understanding but increases model size and inference time
  - ACAM branch adds regularization but requires additional computation

- **Failure signatures:**
  - If ACAMs don't show more complete activation than standard CAMs, the ACAM branch may be ineffective
  - If pseudo-label mixing doesn't improve performance, the dynamic weight strategy may need adjustment
  - If CNN branch performs similarly to hybrid, Transformer may not be adding value

- **First 3 experiments:**
  1. Compare CNN-only baseline vs. hybrid CNN-Transformer model to verify global context helps
  2. Compare with/without ACAM branch to validate its contribution to performance
  3. Test different values for pseudo-label mixing weights (α, β) to find optimal configuration

## Open Questions the Paper Calls Out
None

## Limitations
- Relies heavily on dataset-specific results without extensive ablation studies on ACAM mechanism or pseudo-label mixing strategy
- Computational overhead of hybrid architecture not thoroughly discussed regarding inference time and memory requirements
- Claim that global context from Transformers is essential needs further validation through controlled experiments

## Confidence
- **High Confidence:** The hybrid CNN-Transformer architecture improves over pure CNN baselines for scribble-supervised segmentation
- **Medium Confidence:** ACAMs provide meaningful improvement over standard CAMs
- **Medium Confidence:** Dynamic pseudo-label mixing provides complementary supervision benefits

## Next Checks
1. Conduct controlled experiments isolating the contribution of each component (CNN branch, Transformer branch, ACAM branch) through systematic ablations
2. Evaluate inference time and memory overhead compared to CNN-only baselines to assess practical deployment considerations
3. Test model robustness on datasets with varying scribble quality and density to verify generalizability beyond current evaluation scenarios