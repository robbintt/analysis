---
ver: rpa2
title: Large Language Model Pruning
arxiv_id: '2406.00030'
source_url: https://arxiv.org/abs/2406.00030
tags:
- pruning
- proposed
- information
- neurons
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work proposes a pruning method for large language models\
  \ (LLMs) based on mutual information (MI) estimation to identify and remove redundant\
  \ neurons in the fully connected layers of the feedforward network. The method uses\
  \ a matrix-based R\xE9nyi's \u03B1-order entropy estimator with a well-tuned kernel\
  \ width parameter to compute MI between hidden neurons accurately."
---

# Large Language Model Pruning

## Quick Facts
- arXiv ID: 2406.00030
- Source URL: https://arxiv.org/abs/2406.00030
- Reference count: 40
- This work proposes a mutual information-based pruning method for large language models that identifies and removes redundant neurons in fully connected layers without retraining.

## Executive Summary
This paper introduces an unsupervised pruning method for large language models (LLMs) based on mutual information (MI) estimation to identify redundant neurons in fully connected layers. The approach uses a matrix-based Rényi's α-order entropy estimator with a well-tuned kernel width parameter to accurately compute MI between hidden neurons, and employs an alternative clustering strategy to scale the pruning process for large models. The method requires no retraining and demonstrates competitive performance with supervised pruning methods on various GLUE benchmark tasks while maintaining effectiveness for both small and large models.

## Method Summary
The proposed method estimates mutual information between neurons in fully connected layers using a matrix-based Rényi's α-order entropy estimator. A kernel width parameter is tuned using a combination of Scott's rule and kernel alignment. For large models, the method employs clustering with MDS projection to group highly correlated neurons. The pruning process identifies neurons with high MI (indicating redundancy) and removes them, preserving model performance while reducing size. The approach is unsupervised and does not require retraining, distinguishing it from supervised pruning methods.

## Key Results
- MI-based pruning effectively identifies redundant neurons while preserving model performance on GLUE tasks
- The method outperforms existing unsupervised pruning approaches and maintains competitive performance with supervised methods
- MI-based pruning shows particular sensitivity and effectiveness for small models compared to large models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mutual Information (MI) estimation accurately identifies redundant neurons by measuring shared information between hidden layer activations.
- Mechanism: MI between two neurons quantifies how much knowing one neuron's activation reduces uncertainty about the other. High MI implies redundancy, so one can be pruned.
- Core assumption: MI estimation is accurate enough to distinguish truly redundant neurons from those with similar but non-redundant functions.
- Evidence anchors:
  - [abstract] "A mutual information-based estimation is adopted to find neurons with redundancy to eliminate."
  - [section] "We adopt mutual information to measure the relationship between features. Based on the result, we prune features with a certain level of redundancy."
  - [corpus] Weak evidence - related papers discuss pruning but don't specifically validate MI accuracy.
- Break condition: If MI estimation fails to capture non-linear dependencies or is too noisy, pruning decisions become unreliable.

### Mechanism 2
- Claim: Matrix-based Rényi's α-order entropy estimator scales MI computation for large models without estimating full probability distributions.
- Mechanism: Instead of binning or kNN methods, the Rényi entropy is computed via eigenvalues of kernel matrices, avoiding the curse of dimensionality.
- Core assumption: The kernel width parameter σ can be accurately estimated for each neuron using a hybrid of Scott's rule and kernel alignment.
- Evidence anchors:
  - [section] "Specifically, a matrix-based Rényi'sα-order entropy estimator can be used to estimate Rényi's entropy in FC layers..."
  - [section] "To estimate the MI between Zk and Zℓ, we compute I(Zk; Zℓ) = Sα(A) + Sα(B) − Sα(A, B), where A and B are Gram matrices..."
  - [corpus] No direct evidence - corpus neighbors don't discuss Rényi entropy specifically.
- Break condition: If kernel width estimation is poor, MI values become inaccurate, leading to incorrect pruning.

### Mechanism 3
- Claim: Clustering strategy with MDS projection enables scalable pruning by grouping highly correlated neurons and selecting representatives.
- Mechanism: Pairwise MI values are converted to distances, MDS finds coordinates in low-dimensional space, and clustering groups neurons with similar information content.
- Core assumption: MDS embedding preserves MI-based similarity structure well enough for effective clustering.
- Evidence anchors:
  - [section] "The mutual information is used to decide a metric for the clustering procedure. Given the pairwise distances, we utilize the multidimensional scaling (MDS) [54] to find coordinates..."
  - [section] "In the MDS-projected space, two close-by features Zk and Zℓ implies that they share larger mutual information I(Zk; Zℓ)."
  - [corpus] Weak evidence - clustering in pruning is mentioned but not validated with MDS specifically.
- Break condition: If MDS embedding distorts relationships, clustering will group dissimilar neurons, causing poor pruning quality.

## Foundational Learning

- Concept: Mutual Information and Information Theory
  - Why needed here: MI is the core metric for identifying redundant neurons; understanding its properties is essential for interpreting pruning decisions.
  - Quick check question: What does it mean if two neurons have high mutual information, and how does this relate to redundancy?

- Concept: Kernel Methods and Reproducing Kernel Hilbert Spaces (RKHS)
  - Why needed here: The Rényi entropy estimator relies on kernel matrices; understanding kernel methods is necessary to grasp how high-dimensional MI is computed efficiently.
  - Quick check question: How does the choice of kernel (e.g., RBF) and its width parameter affect the MI estimation in this context?

- Concept: Dimensionality Reduction and MDS
  - Why needed here: MDS is used to project high-dimensional MI relationships into a space suitable for clustering; understanding MDS helps explain why clustering works.
  - Quick check question: What does it mean if two neurons are close in the MDS-projected space, and how does this relate to their MI?

## Architecture Onboarding

- Component map: Pre-trained LLM -> MI computation pipeline -> Clustering/MDS -> Pruning selection -> Pruned LLM
- Critical path: 1) Fine-tune pre-trained LLM on target task 2) Collect activation values from FFN layers 3) Compute pairwise MI between neurons using Rényi entropy estimator 4) Apply clustering/MDS to group similar neurons 5) Select representative neurons per cluster, prune others 6) Evaluate pruned model on GLUE benchmark tasks
- Design tradeoffs:
  - Unsupervised vs. supervised pruning: No labels needed, but potentially less precise than supervised methods
  - Retraining-free vs. retraining-based: Faster, but may sacrifice some accuracy recovery
  - Matrix-based Rényi entropy vs. other MI estimators: Scales better, but depends on kernel width estimation quality
- Failure signatures:
  - Large drop in accuracy after pruning → MI estimation or clustering failed to preserve essential neurons
  - No significant compression achieved → MI threshold too high or clustering too conservative
  - Unstable results across runs → Randomness in MDS or neuron selection not properly controlled
- First 3 experiments:
  1. Run MI estimation on a small subset of BERT-tiny activations to verify computation works and check MI values for known redundant neurons.
  2. Apply clustering with MDS on a toy example (e.g., 2D synthetic data) to confirm MDS preserves similarity and clustering groups similar points.
  3. Prune BERT-tiny on a single GLUE task with 10% compression, measure accuracy drop, and compare to random pruning baseline.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed MI-based pruning method scale to models significantly larger than BERT-tiny, such as GPT-3 or LLaMA, in terms of computational efficiency and effectiveness?
- Basis in paper: [inferred] The paper discusses the proposed method's effectiveness on BERT-tiny and mentions the desire to test it on larger models in the future, indicating that scalability to much larger models remains an open question.
- Why unresolved: The experiments conducted were limited to BERT-tiny, which is a relatively small model compared to state-of-the-art LLMs. The computational challenges and effectiveness of the MI-based approach on extremely large models with billions of parameters are yet to be explored.
- What evidence would resolve it: Conducting experiments on larger LLMs like GPT-3, GPT-4, or LLaMA variants and comparing the pruning results (in terms of model size reduction, computational efficiency, and task performance) with those obtained on BERT-tiny would provide insights into the scalability of the proposed method.

### Open Question 2
- Question: What is the optimal choice of α for the matrix-based Rényi's α-order entropy estimator across different LLM architectures and tasks, and how sensitive is the pruning performance to variations in α?
- Basis in paper: [explicit] The paper mentions that α = 1.01 performs exceptionally well for the tested tasks and compression rates, but also suggests that the optimal α might vary depending on the specific model and task.
- Why unresolved: While the paper provides evidence for α = 1.01 being a good choice for the tested scenarios, it does not comprehensively explore the sensitivity of the pruning performance to different α values across various LLM architectures and tasks. The impact of α on the MI estimation accuracy and subsequent pruning effectiveness remains to be fully understood.
- What evidence would resolve it: Conducting extensive experiments by varying α across a wide range of values and testing on diverse LLM architectures and tasks would help determine the optimal α and quantify the sensitivity of the pruning performance to α variations.

### Open Question 3
- Question: How does the proposed MI-based pruning method compare to other state-of-the-art pruning techniques, such as magnitude-based pruning or structured pruning, in terms of final model size, computational efficiency, and task performance?
- Basis in paper: [inferred] The paper compares the proposed method to a few other pruning techniques, including weight-magnitude pruning and KCM, but does not provide a comprehensive comparison with all state-of-the-art methods.
- Why unresolved: While the paper demonstrates the superiority of the proposed method over some existing techniques, a thorough comparison with all major pruning approaches is necessary to establish its relative effectiveness and efficiency. The trade-offs between model size reduction, computational overhead, and task performance across different pruning methods remain to be fully evaluated.
- What evidence would resolve it: Conducting extensive experiments that compare the proposed MI-based pruning method with a wide range of state-of-the-art techniques, including magnitude-based pruning, structured pruning, and other MI-based approaches, on various LLM architectures and tasks would provide a comprehensive understanding of its relative effectiveness and efficiency.

## Limitations

- Critical implementation details are underspecified, particularly the exact matrix-based Rényi entropy computation and kernel width parameter estimation
- The clustering strategy for large models is described as "alternative" but not detailed, limiting reproducibility
- The 10% relative FLOPs compression target seems modest compared to the "huge" parameter reduction claimed in the abstract

## Confidence

**High Confidence**: The core conceptual framework (MI-based pruning, unsupervised approach, GLUE benchmark evaluation) is well-articulated and methodologically sound.

**Medium Confidence**: The empirical results showing competitive performance with supervised methods are convincing for the tasks and models tested, though the modest compression ratios limit generalizability.

**Low Confidence**: The scalability claims and the effectiveness of the clustering/MDS strategy for large models cannot be fully verified without implementation details.

## Next Checks

1. **Implementation Verification**: Reproduce the MI estimation pipeline on a small BERT-tiny activation dataset to verify that the Rényi entropy estimator produces reasonable values and that kernel width tuning works as described.

2. **Clustering Effectiveness**: Test the MDS + clustering approach on a controlled synthetic dataset where neuron relationships are known, to verify that the method correctly groups highly correlated neurons and preserves essential information.

3. **Compression-Performance Tradeoff**: Systematically vary the compression ratio (not just 10%) and measure the accuracy drop curve, comparing against both random pruning and the reported results to validate the claimed efficiency advantage.