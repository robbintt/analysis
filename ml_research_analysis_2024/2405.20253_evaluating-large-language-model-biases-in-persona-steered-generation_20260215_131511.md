---
ver: rpa2
title: Evaluating Large Language Model Biases in Persona-Steered Generation
arxiv_id: '2405.20253'
source_url: https://arxiv.org/abs/2405.20253
tags:
- believes
- stances
- more
- personas
- stance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Large language models (LLMs) are increasingly used to generate
  open-ended text reflecting diverse viewpoints. This study investigates how well
  LLMs can be steered towards multifaceted personas, particularly those with incongruous
  traits (e.g., a political liberal who supports increased military spending).
---

# Evaluating Large Language Model Biases in Persona-Steered Generation

## Quick Facts
- arXiv ID: 2405.20253
- Source URL: https://arxiv.org/abs/2405.20253
- Authors: Andy Liu; Mona Diab; Daniel Fried
- Reference count: 40
- Large language models are 9.7% less steerable towards incongruous personas than congruous ones, with RLHF fine-tuning improving steerability but reducing semantic diversity.

## Executive Summary
This study investigates how well large language models (LLMs) can generate text that reflects multifaceted personas, particularly those with incongruous traits. The research finds that LLMs are significantly less steerable towards incongruous personas, often defaulting to stereotypical stances associated with demographics. While models fine-tuned with Reinforcement Learning from Human Feedback (RLHF) show improved steerability, especially for liberal and female-associated stances, this comes at the cost of reduced semantic diversity. The study highlights the importance of evaluating models in open-ended generation tasks to uncover biases not apparent in multiple-choice settings.

## Method Summary
The study constructs personas from Pew Research Center survey data, combining demographics with stances that diverge between groups. Six high-level demographics are used: liberal/conservative, white/black, and male/female. Four model families (Llama-2-Chat, Tulu-2, GPT-3.5-Turbo) with different fine-tuning methods (RLHF, DPO, SFT) are evaluated. For each persona, 50 statements are generated using specified prompts and temperatures. Steerability is measured by the percentage of statements agreeing with the target stance, evaluated by GPT-4 or human annotators. Additional metrics include Individuation, Exaggeration, Entailment Diversity, and Semantic Diversity to assess the richness of generated content.

## Key Results
- LLMs are 9.7% less steerable towards incongruous personas than congruous ones.
- RLHF fine-tuned models show higher steerability towards liberal and female-associated stances but suffer up to 58.2% reduction in semantic diversity.
- Multiple-choice task performance does not reliably predict steerability in open-ended generation.

## Why This Works (Mechanism)
The study leverages the inherent biases in LLMs when generating text for personas with incongruous traits. By using survey-derived stances that diverge between demographic groups, the research exposes how models default to stereotypical associations. The steerability metric quantifies how well models align with target stances, while diversity metrics reveal the trade-off between adherence to persona and semantic richness. The use of GPT-4 for evaluation ensures consistency, though human annotation provides a complementary check.

## Foundational Learning
- **Steerability**: Ability of LLMs to generate text aligned with specified personas; critical for assessing bias in open-ended tasks.
- **Incongruous Personas**: Personas combining demographics with stances that contradict typical associations; used to test model flexibility.
- **RLHF Fine-tuning**: Method to improve model alignment with human preferences; improves steerability but reduces diversity.
- **Semantic Diversity**: Measures the variety of viewpoints in generated text; essential for capturing nuanced human opinions.
- **Multiple-choice vs. Open-ended Evaluation**: Highlights limitations of closed-form tasks in revealing generation biases.

## Architecture Onboarding
- **Component Map**: Pew Survey Data -> Persona Construction -> Prompt Generation -> LLM Generation -> Steerability Evaluation -> Diversity Metrics
- **Critical Path**: Persona Construction -> Prompt Generation -> LLM Generation -> Steerability Evaluation
- **Design Tradeoffs**: Higher steerability (via RLHF) vs. lower semantic diversity; multiple-choice tasks vs. open-ended evaluation.
- **Failure Signatures**: Defaulting to stereotypical stances for incongruous personas; reduced diversity in RLHF models.
- **First Experiments**: 1) Generate statements for incongruous personas and measure steerability; 2) Compare diversity metrics between RLHF and non-RLHF models; 3) Evaluate if multiple-choice performance predicts open-ended steerability.

## Open Questions the Paper Calls Out
None

## Limitations
- The study uses a limited set of demographics and stances, potentially missing complex intersectional identities.
- GPT-4 annotation introduces potential biases, and human annotation was limited, raising questions about reliability.
- The impact of prompt templates and generation hyperparameters on results is not explored.

## Confidence
- **High confidence**: LLMs are less steerable towards incongruous personas, and RLHF improves steerability for liberal/female stances.
- **Medium confidence**: The trade-off between steerability and semantic diversity is significant but may vary across models and datasets.
- **Low confidence**: Conclusions about the importance of open-ended evaluation and nuanced representations need further validation.

## Next Checks
1. Expand demographic and stance coverage to include intersectional identities and diverse data sources.
2. Compare annotation methods (human vs. GPT-4) to assess reliability and generalizability.
3. Explore the impact of varying prompt templates and generation hyperparameters on steerability and diversity.