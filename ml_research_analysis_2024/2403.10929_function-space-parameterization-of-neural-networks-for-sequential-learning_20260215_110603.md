---
ver: rpa2
title: Function-space Parameterization of Neural Networks for Sequential Learning
arxiv_id: '2403.10929'
source_url: https://arxiv.org/abs/2403.10929
tags:
- learning
- data
- dual
- laplace
- sparse
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a function-space representation of neural
  networks called Sparse Function-space Representation (SFR) to address challenges
  in sequential learning. SFR converts trained neural networks into sparse Gaussian
  processes using a dual parameterization, enabling efficient incorporation of new
  data and retention of prior knowledge without retraining.
---

# Function-space Parameterization of Neural Networks for Sequential Learning

## Quick Facts
- arXiv ID: 2403.10929
- Source URL: https://arxiv.org/abs/2403.10929
- Authors: Aidan Scannell, Riccardo Mereu, Paul Chang, Ella Tamir, Joni Pajarinen, Arno Solin
- Reference count: 40
- Primary result: Sparse Function-space Representation (SFR) converts trained neural networks into sparse Gaussian processes for efficient sequential learning

## Executive Summary
This paper introduces Sparse Function-space Representation (SFR), a method that converts trained neural networks into sparse Gaussian processes using a dual parameterization. SFR addresses key challenges in sequential learning by enabling efficient incorporation of new data and retention of prior knowledge without retraining. The approach leverages sparse inducing points to summarize all training data while maintaining computational efficiency. Through experiments on image classification, continual learning, and reinforcement learning tasks, SFR demonstrates superior performance compared to traditional methods like Laplace approximations and subset-based Gaussian processes.

## Method Summary
SFR operates by linearizing a trained neural network around its MAP weights using the Neural Tangent Kernel (NTK), then computing dual parameters (α, β) that represent the function-space posterior. These dual parameters are projected onto a sparse set of inducing points, creating a compact representation that captures information from all training data. New data can be incorporated through simple additive updates to the dual parameters, avoiding full retraining. The method scales efficiently by working in function space rather than weight space, enabling uncertainty quantification and continual learning capabilities.

## Key Results
- SFR outperforms Laplace approximations and subset-based Gaussian processes on CIFAR-10 and Fashion-MNIST classification tasks
- Achieves state-of-the-art performance on Split-MNIST and Split-FashionMNIST continual learning benchmarks
- Demonstrates superior sample efficiency in model-based reinforcement learning through uncertainty-guided exploration
- Scales to datasets with over 1 million points while maintaining computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SFR's dual parameterization enables scalable uncertainty quantification by capturing information from all training data at a sparse set of inducing points
- Mechanism: The dual parameter formulation computes α and β values that represent the posterior without requiring the full NxN covariance matrix, allowing sparsification while retaining global data information
- Core assumption: Dual parameters accurately capture posterior uncertainty structure when projected onto inducing points
- Evidence anchors: Abstract states dual parameterization scales function-space methods to large datasets via sparsification; section shows dual parameters enable leveraging any sparsification method
- Break condition: If dual parameters fail to capture full data information when projected, uncertainty estimates would degrade

### Mechanism 2
- Claim: SFR can incorporate new data without retraining by updating dual parameters directly
- Mechanism: New data dual parameters can be computed and added to existing sparse dual parameters using simple additive updates, avoiding full retraining
- Core assumption: Kernel (NTK) remains accurate enough for new data points outside original training distribution
- Evidence anchors: Abstract states dual parameterization enables new data incorporation without retraining; section demonstrates efficient incorporation when access to previous data is lost
- Break condition: If kernel accuracy degrades significantly outside training distribution, dual updates become ineffective

### Mechanism 3
- Claim: SFR provides better continual learning through function-space regularization using sparse dual parameters
- Mechanism: Regularizer uses sparse dual parameters to maintain compact representation of previous tasks' knowledge without storing full data
- Core assumption: Sparse dual parameterization sufficiently captures task-specific information for regularization
- Evidence anchors: Experiments demonstrate knowledge retention in continual learning; section shows SFR approximates sparse function-space VCL with regularizer keeping function values close to MAP of previous tasks
- Break condition: If sparse dual parameters don't capture sufficient task information, catastrophic forgetting occurs

## Foundational Learning

- Concept: Gaussian Processes and function-space representation
  - Why needed here: SFR converts neural networks to function space using GP-like formulations, requiring understanding of GP fundamentals
  - Quick check question: What is the key advantage of function-space representation over weight-space for uncertainty quantification?

- Concept: Neural Tangent Kernel (NTK) and linearization
  - Why needed here: SFR linearizes neural networks around MAP weights using NTK to create kernel function for function-space representation
  - Quick check question: How does the NTK relate to the Jacobian of the neural network output with respect to weights?

- Concept: Sparse variational inference and inducing points
  - Why needed here: SFR uses sparse GP techniques with inducing points to scale to large datasets while maintaining computational efficiency
  - Quick check question: What is the computational complexity reduction achieved by using inducing points instead of full GP?

## Architecture Onboarding

- Component map: Neural Network (weight space) → Linearization (NTK) → Dual Parameters (α, β) → Sparse GP (function space)
- Critical path: 1) Train NN to obtain MAP weights 2) Compute Jacobian at MAP weights 3) Calculate NTK kernel matrix 4) Compute dual parameters (α, β) 5) Project dual parameters onto inducing points 6) Make predictions using sparse GP formulation
- Design tradeoffs: Computational efficiency vs. accuracy in inducing point selection; sparsity level (M) vs. uncertainty quantification quality; linearization accuracy vs. simplicity of function-space representation
- Failure signatures: Poor uncertainty calibration (dual parameters not capturing true posterior); degrading performance with fewer inducing points; catastrophic forgetting in continual learning tasks; inefficient dual updates for new data incorporation
- First 3 experiments: 1) Simple 1D regression to verify uncertainty quantification matches full GP 2) UCI classification benchmark to test scalability and accuracy 3) Continual learning experiment on Split-MNIST to validate knowledge retention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SFR's performance scale when using larger neural network architectures (e.g., deeper CNNs) on high-dimensional image datasets?
- Basis in paper: [explicit] The paper demonstrates SFR's effectiveness on CIFAR-10 and Fashion-MNIST with CNNs, but does not explore scaling to larger architectures
- Why unresolved: Experiments focused on standard CNN architectures without exploring limits of SFR's scalability
- What evidence would resolve it: Experiments comparing SFR's performance across various CNN depths and widths on benchmark image datasets

### Open Question 2
- Question: Can SFR's dual parameterization be extended to non-stationary kernel functions for improved uncertainty quantification in complex domains?
- Basis in paper: [inferred] Paper notes SFR learns covariance structure and can handle non-stationary functions, but does not explore this explicitly
- Why unresolved: Paper does not investigate alternative kernel formulations or their impact on SFR's performance
- What evidence would resolve it: Experiments comparing SFR with different kernel formulations (e.g., non-stationary kernels) on tasks requiring complex uncertainty quantification

### Open Question 3
- Question: How does SFR's computational efficiency compare to other sparse GP methods when scaling to extremely large datasets (e.g., millions of data points)?
- Basis in paper: [explicit] Paper demonstrates SFR's efficiency on dataset with over 1 million points, but lacks comprehensive comparison with other sparse GP methods
- Why unresolved: Paper lacks direct comparison of SFR's computational efficiency with other sparse GP methods on extremely large datasets
- What evidence would resolve it: Benchmarking SFR against other sparse GP methods on datasets with millions of data points, comparing training and inference times

## Limitations

- Scalability boundary remains untested at extreme scales (>1M samples) where inducing point selection becomes increasingly challenging
- NTK-based linearization assumes lazy training regime, potentially degrading for networks that learn feature representations outside NTK regime
- Effectiveness on domains outside standard vision benchmarks (time series, graphs, multimodal data) is unclear

## Confidence

- High Confidence: Dual parameterization enables function-space representation; sparsification through inducing points maintains computational efficiency; additive dual updates enable new data incorporation
- Medium Confidence: Superiority over Laplace approximations on CIFAR-10; continual learning performance on Split-MNIST; reinforcement learning sample efficiency
- Low Confidence: Claims about "state-of-the-art" continual learning performance; generalizability to other neural architectures beyond standard CNNs; performance on real-world noisy datasets with distribution shift

## Next Checks

1. Scale Boundary Test: Evaluate SFR on ImageNet-scale datasets (1M+ samples) to identify practical scalability limit and measure computational overhead compared to standard deep learning approaches

2. Kernel Regime Validation: Test SFR on architectures known to operate outside NTK regime (e.g., ResNet with skip connections) and compare performance with weight-space Bayesian methods to quantify linearization assumption's impact

3. Cross-Domain Robustness: Apply SFR to time series forecasting (UCI datasets) and graph neural networks (Cora citation network) to assess whether function-space representation transfers across input modalities or requires domain-specific kernel adaptations