---
ver: rpa2
title: 'TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks'
arxiv_id: '2412.14161'
source_url: https://arxiv.org/abs/2412.14161
tags:
- tasks
- agent
- task
- theagentcompany
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces TheAgentCompany, a benchmark for evaluating
  AI agents on consequential real-world work tasks by simulating a software development
  company environment with diverse internal platforms. The benchmark uses a checkpoint-based
  evaluation scheme with both deterministic and LLM-based evaluators, testing agents
  across 175 tasks that span software engineering, project management, data science,
  and other professional roles.
---

# TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks

## Quick Facts
- arXiv ID: 2412.14161
- Source URL: https://arxiv.org/abs/2412.14161
- Reference count: 40
- Agents can autonomously complete 30.3% of tasks with best model (Gemini 2.5 Pro), 7.4% with best open-weight model

## Executive Summary
This paper introduces TheAgentCompany, a comprehensive benchmark for evaluating AI agents on consequential real-world work tasks by simulating a software development company environment. The benchmark features 175 tasks across diverse professional domains including software engineering, project management, data science, and administrative roles. Using a checkpoint-based evaluation system with both deterministic and LLM-based evaluators, the study tests twelve foundation models through OpenHands and OWL-RolePlay frameworks, revealing significant performance gaps between top-performing models and the challenging nature of workplace automation tasks.

## Method Summary
TheAgentCompany evaluates LLM agents on 175 real-world work tasks in a self-contained software development company simulation using internal web platforms (GitLab, OwnCloud, Plane, RocketChat). Tasks are divided into checkpoints with partial credit scoring, and agents use OpenHands CodeAct framework with browsing capabilities and OWL-RolePlay framework to interact with simulated colleagues. The evaluation employs both deterministic checkers for simple tasks and LLM-based evaluators for complex ones, measuring success through full completion scores, partial completion scores, step counts, and API costs across twelve foundation models.

## Key Results
- Best-performing model (Gemini 2.5 Pro) achieves 30.3% full task completion and 39.3% partial completion score
- Strongest open-weight model (Llama 3.1 405B) reaches only 7.4% success rate
- Agent performance varies significantly across task types, with software engineering tasks showing higher success rates than project management and admin tasks
- RocketChat and OwnCloud platforms present the greatest challenges, with agents struggling most on tasks involving social interaction and complex web interfaces

## Why This Works (Mechanism)

### Mechanism 1
The benchmark's checkpoint-based evaluation scheme enables granular assessment of agent capabilities by dividing tasks into intermediate milestones with partial credit scoring, providing more nuanced measurement than binary pass/fail metrics.

### Mechanism 2
The simulated colleague communication system tests agents' ability to interact with humans in workplace settings through LLM-powered NPCs that simulate human colleagues for realistic workplace interactions.

### Mechanism 3
The self-hosted, reproducible environment ensures consistent benchmark evaluation across different models and time periods by creating a completely self-contained environment using open-source software that eliminates external dependencies.

## Foundational Learning

- Concept: Checkpoint-based evaluation systems
  - Why needed here: The benchmark uses sophisticated scoring that awards partial credit for intermediate task progress, requiring understanding of how to design granular evaluation metrics
  - Quick check question: What are the two scoring metrics used in TheAgentCompany and how do they differ in what they measure?

- Concept: Multi-agent interaction frameworks
  - Why needed here: The benchmark incorporates simulated colleagues using Sotopia platform, requiring knowledge of how to design AI agents that can communicate with other agents in realistic ways
  - Quick check question: How do agents in TheAgentCompany interact with simulated colleagues and what platforms enable this interaction?

- Concept: Self-contained environment design
  - Why needed here: The benchmark requires building a complete, reproducible environment with multiple internal platforms, necessitating understanding of how to create isolated but realistic testing environments
  - Quick check question: What are the four main internal platforms in TheAgentCompany environment and what real-world equivalents do they represent?

## Architecture Onboarding

- Component map: Evaluation environment (GitLab, OwnCloud, Plane, RocketChat) -> Agent framework (OpenHands or OWL-RolePlay) -> Evaluation system (checkpoints, deterministic and LLM-based evaluators)
- Critical path: Agent receives task → Initializes environment → Executes subtasks using browser, terminal, and code tools → Interacts with simulated colleagues if needed → Submits final output → Evaluation system checks checkpoints → Scores partial and full completion → Records results with step count and cost
- Design tradeoffs: Trades real-world complexity for reproducibility by using open-source internal platforms instead of actual commercial services, ensuring consistency but potentially missing some real-world nuances
- Failure signatures: Common failures include getting stuck in browser navigation loops, failing to communicate effectively with simulated colleagues, making incorrect assumptions about task requirements, and creating fake shortcuts when uncertain
- First 3 experiments:
  1. Run a simple software engineering task (like cloning a repository and running a script) to verify basic agent-environment interaction works correctly
  2. Test a task requiring simulated colleague interaction to validate communication system functionality
  3. Execute a multi-platform task combining GitLab and OwnCloud operations to verify cross-platform coordination

## Open Questions the Paper Calls Out

Open Question 1
- Question: What is the relative importance of social interaction versus technical interface complexity in limiting LLM agent performance on real-world work tasks?
- Basis in paper: Inferred from observation that agents struggle most with RocketChat (social interaction) and ownCloud (complex UI)
- Why unresolved: Paper presents these as separate challenges but does not experimentally disentangle their relative contributions
- What evidence would resolve it: Controlled experiments comparing agent performance on tasks with identical technical complexity but varying social interaction requirements

Open Question 2
- Question: How does the performance of LLM agents on TheAgentCompany tasks compare to actual human professionals performing the same tasks?
- Basis in paper: Explicit statement that "due to resource limitations we were not able to perform this comparison"
- Why unresolved: Benchmark creators acknowledge this gap but lack resources to collect human baseline data
- What evidence would resolve it: Empirical studies measuring task completion times, success rates, and quality metrics for human professionals versus LLM agents on identical tasks

Open Question 3
- Question: What is the relationship between task complexity (number of steps/checkpoints) and LLM agent success rates, and at what point does performance degrade exponentially?
- Basis in paper: Inferred from discussion of "long-horizon tasks" and observation that difficult long-horizon tasks are still beyond current systems
- Why unresolved: Paper provides overall statistics but does not analyze correlation between task length/complexity and success rates in detail
- What evidence would resolve it: Statistical analysis of success rates as a function of task length, number of checkpoints, or required number of tool interactions

## Limitations

- Controlled environment may not fully capture real-world workplace complexity and unpredictability despite using open-source equivalents of business tools
- Effectiveness of LLM-powered simulated colleagues in accurately representing human workplace communication remains uncertain
- Checkpoint-based evaluation system assumes intermediate progress correlates with meaningful task completion, which may not always hold true

## Confidence

- High confidence: Technical implementation and reproducibility mechanisms
- Medium confidence: Ability to assess real-world task completion capabilities
- Low confidence: Social interaction evaluation results with simulated colleagues

## Next Checks

1. Validate simulated colleague realism through human evaluation studies comparing LLM-powered NPC responses to actual human workplace communication patterns
2. Test real-world generalizability by applying benchmark methodology to actual commercial platforms rather than self-hosted equivalents
3. Analyze checkpoint alignment through expert review of checkpoint definitions across tasks to verify meaningful contribution to task completion