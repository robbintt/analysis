---
ver: rpa2
title: Unraveling the Impact of Heterophilic Structures on Graph Positive-Unlabeled
  Learning
arxiv_id: '2405.19919'
source_url: https://arxiv.org/abs/2405.19919
tags:
- graph
- learning
- nodes
- heterophilic
- positive
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses a key challenge in graph-based Positive-Unlabeled
  (PU) learning: the presence of heterophilic structures, where positive and negative
  nodes are connected, which undermines Class-Prior Estimation (CPE) and PU classification.
  The authors propose Graph PU Learning with Label Propagation Loss (GPL), a method
  that reduces heterophilic edge weights using a Label Propagation Loss (LPL) that
  relies only on observed positive nodes.'
---

# Unraveling the Impact of Heterophilic Structures on Graph Positive-Unlabeled Learning

## Quick Facts
- arXiv ID: 2405.19919
- Source URL: https://arxiv.org/abs/2405.19919
- Authors: Yuhao Wu; Jiangchao Yao; Bo Han; Lina Yao; Tongliang Liu
- Reference count: 40
- Primary result: Proposed GPL method achieves F1 scores up to 82.3% on Cora and 48.0% on Cornell, reducing class prior estimation error by up to 17%

## Executive Summary
This paper addresses a critical challenge in graph-based Positive-Unlabeled (PU) learning: heterophilic structures where positive and negative nodes are connected, which undermines Class-Prior Estimation (CPE) and PU classification. The authors propose Graph PU Learning with Label Propagation Loss (GPL), a method that reduces heterophilic edge weights using a Label Propagation Loss (LPL) that relies only on observed positive nodes. LPL minimizes the likelihood of positive nodes being misclassified by label propagation, thereby strengthening homophilic edges and reducing heterophilic ones. The optimization is formulated as bilevel: an inner loop that reduces heterophily via LPL, and an outer loop that trains a classifier using the refined graph. Empirically, GPL significantly outperforms existing methods across multiple datasets with varying heterophily ratios, achieving F1 scores up to 82.3% on Cora and 48.0% on Cornell, and reducing class prior estimation error by up to 17%. Ablation studies confirm the effectiveness of LPL, bilevel optimization, and node selection.

## Method Summary
GPL addresses graph PU learning by first reducing heterophilic edge weights through Label Propagation Loss (LPL), which uses only observed positive nodes to identify and weaken edges connecting different classes. The method employs bilevel optimization where the inner loop optimizes the adjacency matrix to reduce heterophily while the outer loop trains a GNN classifier using the refined graph structure and estimated class prior. LPL minimizes the likelihood of positive nodes being misclassified through label propagation, effectively strengthening homophilic edges and weakening heterophilic ones. The class prior is estimated using the refined graph, and the GNN classifier is trained with this estimated prior. This iterative process creates a feedback loop where better graph structure leads to better classifier predictions, which in turn helps identify more accurate positive and negative nodes for the next iteration of structure optimization.

## Key Results
- GPL achieves F1 scores up to 82.3% on Cora and 48.0% on Cornell datasets
- Class prior estimation error reduced by up to 17% compared to baseline methods
- Outperforms existing methods including GCN+TED, GCN+NNPU, and PU-GNN across multiple datasets with varying heterophily ratios
- Ablation studies confirm the effectiveness of LPL, bilevel optimization, and node selection mechanisms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing heterophilic edge weights improves class-prior estimation accuracy.
- Mechanism: The Label Propagation Loss (LPL) uses label propagation to identify edges that connect nodes of different classes (heterophilic edges). By minimizing the likelihood that positive nodes are misclassified as negative through label propagation, LPL reduces the weights of heterophilic edges while strengthening homophilic edges. This creates a clearer separation between positive and negative node distributions in the embedding space.
- Core assumption: Label propagation can effectively distinguish between homophilic and heterophilic edges using only observed positive nodes.
- Evidence anchors:
  - [abstract]: "GPL considers learning from PU nodes along with an intermediate heterophily reduction, which helps mitigate the negative impact of the heterophilic structure."
  - [section]: "optimizing LPL results in strengthening the weights of homophilic edges, i.e, edges linking same-class nodes, while simultaneously reducing the weights of heterophilic edges, relying solely on the observed positive nodes."
  - [corpus]: Weak - related papers focus on heterophily in general GNNs but don't specifically address PU learning scenarios.

### Mechanism 2
- Claim: Bilevel optimization enables simultaneous graph structure refinement and classifier training.
- Mechanism: The inner loop optimizes the adjacency matrix to reduce heterophily using LPL, while the outer loop trains the GNN classifier using the refined graph structure and estimated class prior. This creates a feedback loop where better graph structure leads to better classifier predictions, which in turn helps identify more accurate positive and negative nodes for the next iteration of structure optimization.
- Core assumption: The bilevel optimization converges to a stable solution where both graph structure and classifier are mutually improved.
- Evidence anchors:
  - [abstract]: "We formulate this procedure as a bilevel optimization that reduces heterophily in the inner loop and efficiently learns a classifier in the outer loop."
  - [section]: "This entire procedure can be elegantly formulated as a bi-level optimization: In the inner loop, the identified P and N nodes... are fixed while the graph structure is optimized to maximize label propagation within the same category. In the outer loop, while maintaining a constant optimized graph structure and utilizing CPE, a binary classifier is strategically trained..."
  - [corpus]: Weak - related papers discuss bilevel optimization for GNNs but not specifically in the context of PU learning with heterophily.

### Mechanism 3
- Claim: Theoretical guarantees exist for improved class-prior estimation and classification when heterophily is reduced.
- Mechanism: Theorem 4.4 proves that reducing heterophily tightens the estimation error bound for class priors, while Theorem 4.5 shows that heterophilic edges cause positive and negative node embeddings to converge, making classification harder. By reducing heterophilic edge weights, GPL ensures better separation between positive and negative nodes in the embedding space.
- Core assumption: The theoretical analysis conditions (such as smoothness and convexity assumptions) hold in practice.
- Evidence anchors:
  - [abstract]: "The classifier trained through graph PU learning, in turn, helps refine this structure with the aid of more accurate prediction labels."
  - [section]: "Theorem 4.4 (Estimation Error). Let the class prior can be estimated by ˆπp = min c∈[0,1] Q A
u (c)/Q A
p (c) and the graph structure can be optimized through ˆA = arg minA LLPA(A)(K). Then, the upper bound of the estimation error with optimized graph structure ˆA is lower than ordinary graph structure A."
  - [corpus]: Weak - related papers discuss heterophily's impact on GNNs but don't provide specific theoretical guarantees for PU learning scenarios.

## Foundational Learning

- Concept: Positive-Unlabeled (PU) Learning
  - Why needed here: The paper addresses the specific challenge of applying PU learning to graph data where positive nodes are labeled but negative nodes are unknown, requiring specialized techniques to handle the missing label information.
  - Quick check question: What is the key difference between PU learning and semi-supervised learning in terms of available label information?

- Concept: Graph Neural Networks (GNNs) and Message Passing
  - Why needed here: GPL uses GNNs as the backbone classifier and relies on message passing through the graph structure, which is directly affected by heterophilic edges.
  - Quick check question: How does the presence of heterophilic edges affect the message passing process in standard GNNs?

- Concept: Heterophily vs. Homophily in Graphs
  - Why needed here: The core contribution of GPL is addressing heterophily, where connected nodes tend to have different labels, which violates assumptions in standard PU learning methods.
  - Quick check question: What is the impact of high heterophily ratio on the performance of standard GNNs?

## Architecture Onboarding

- Component map:
  Input: Graph structure (adjacency matrix), node features, observed positive nodes -> LPL module: Label propagation loss computation and optimization -> CPE module: Class-prior estimation using refined graph structure -> GNN classifier: Binary classification model trained on refined graph -> Bilevel optimizer: Coordinates inner (graph structure) and outer (classifier) loops

- Critical path:
  1. Initialize graph structure and GNN parameters
  2. Inner loop: Optimize adjacency matrix using LPL based on current positive/negative node assignments
  3. Estimate class prior using refined graph structure
  4. Outer loop: Train GNN classifier using estimated class prior and refined graph
  5. Repeat until convergence

- Design tradeoffs:
  - LPL iteration count (K): Too few iterations may not adequately reduce heterophily; too many may overfit
  - Learning rates: Inner loop (graph structure) vs outer loop (classifier) need careful balancing
  - Positive node selection: How many positive nodes to use for initial LPL computation

- Failure signatures:
  - If F1 score plateaus early: Graph structure optimization may be stuck in local minimum
  - If class prior estimation error remains high: LPL may not be effectively identifying heterophilic edges
  - If performance degrades on homophilic datasets: Over-aggressive heterophily reduction

- First 3 experiments:
  1. Test GPL on Cora dataset with varying heterophily ratios to verify the core mechanism
  2. Compare GPL with standard GCN+TED baseline to isolate the impact of heterophily reduction
  3. Perform ablation study removing LPL to quantify its contribution to overall performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of GPL scale with increasing graph size and node count in real-world scenarios?
- Basis in paper: [explicit] The paper mentions that GPL is tested on datasets with varying node and edge counts, but doesn't explore scaling behavior.
- Why unresolved: The paper only evaluates GPL on datasets with up to 19,717 nodes, and scaling to larger graphs with millions of nodes is not addressed.
- What evidence would resolve it: Empirical results showing GPL's performance and computational efficiency on large-scale graph datasets with millions of nodes and edges.

### Open Question 2
- Question: Can GPL be extended to handle multi-class graph PU learning, where there are more than two classes in the data?
- Basis in paper: [inferred] The paper focuses on binary classification, but real-world applications often involve multiple classes. The method could potentially be generalized.
- Why unresolved: The current formulation of GPL is designed for binary classification, and adapting it to multi-class scenarios requires significant modifications to the loss functions and optimization procedures.
- What evidence would resolve it: A modified version of GPL that can handle multi-class graph PU learning, with experimental results demonstrating its effectiveness on datasets with more than two classes.

### Open Question 3
- Question: How does GPL perform when the positive and negative classes have significantly different class prior probabilities?
- Basis in paper: [explicit] The paper mentions that GPL is robust to different ratios of labeled nodes, but doesn't explore scenarios with highly imbalanced class priors.
- Why unresolved: The current experiments focus on cases where the positive and negative classes have similar class prior probabilities, but real-world scenarios often involve highly imbalanced classes.
- What evidence would resolve it: Empirical results showing GPL's performance on datasets with highly imbalanced class priors, and a comparison with existing methods that are specifically designed for imbalanced data.

## Limitations

- The theoretical analysis relies on smoothness and convexity assumptions that may not hold in practice, particularly for sparse graphs with high heterophily ratios
- The effectiveness of LPL in distinguishing heterophilic edges depends heavily on the quality of initial positive node labels, and performance may degrade significantly if these labels are noisy or limited
- The bilevel optimization framework requires careful hyperparameter tuning, and the interaction between inner loop (graph structure) and outer loop (classifier) optimization is not fully characterized

## Confidence

- **High confidence** in the empirical results showing GPL outperforms baselines across multiple datasets, as this is directly measurable and reproducible
- **Medium confidence** in the theoretical error bounds, as they depend on assumptions that need verification in real-world scenarios
- **Medium confidence** in the mechanism explanation, as the ablation studies support the core claims but don't fully isolate all contributing factors

## Next Checks

1. Conduct stress tests on GPL with varying percentages of observed positive nodes (e.g., 10%, 30%, 50%) to evaluate robustness to label sparsity
2. Perform runtime complexity analysis comparing GPL to baseline methods, particularly focusing on the overhead introduced by bilevel optimization
3. Test GPL on graphs with different heterophily ratios to verify the theoretical claim that performance improves as heterophily increases relative to homophily