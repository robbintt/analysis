---
ver: rpa2
title: Impact of Adversarial Attacks on Deep Learning Model Explainability
arxiv_id: '2412.11119'
source_url: https://arxiv.org/abs/2412.11119
tags:
- adversarial
- explanation
- images
- learning
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how adversarial attacks affect the explainability
  of deep learning models using techniques like GradCAM, SmoothGrad, and LIME. The
  authors generate adversarial images using FGSM and BIM methods, which significantly
  reduce model accuracy from 89.94% to 58.73% and 45.50% respectively, yet explanation
  metrics (IoU and RMSE) show negligible changes.
---

# Impact of Adversarial Attacks on Deep Learning Model Explainability

## Quick Facts
- arXiv ID: 2412.11119
- Source URL: https://arxiv.org/abs/2412.11119
- Authors: Gazi Nazia Nur; Mohammad Ahnaf Sadat
- Reference count: 7
- Key outcome: Adversarial attacks significantly reduce model accuracy while explanation metrics (IoU, RMSE) show negligible changes, suggesting current metrics may not detect adversarial perturbations.

## Executive Summary
This paper investigates how adversarial attacks affect the explainability of deep learning models using techniques like GradCAM, SmoothGrad, and LIME. The authors generate adversarial images using FGSM and BIM methods, which significantly reduce model accuracy from 89.94% to 58.73% and 45.50% respectively, yet explanation metrics (IoU and RMSE) show negligible changes. This suggests current explainability metrics may not be sensitive enough to detect adversarial perturbations. The study highlights a gap in benchmarks for evaluating adversarial attacks on explanation methods and proposes future work to include more attack methods, metrics, and datasets for comprehensive evaluation.

## Method Summary
The study uses a subset of ImageNet validation dataset (classes 1-40 and 80-120, 5 images per class) with ground truth explanations generated using SAM. A pre-trained EfficientNetV2B0 model classifies original and adversarial images (generated using FGSM and BIM attacks). Explanations are generated using GradCAM, SmoothGrad, and LIME, and evaluated using IoU and RMSE metrics. The pipeline is repeated for both original and adversarial images to compare changes in accuracy and explanation metrics.

## Key Results
- Model accuracy drops significantly from 89.94% to 58.73% (FGSM) and 45.50% (BIM) under adversarial attacks
- IoU and RMSE metrics for explanation methods remain largely unchanged despite accuracy drops
- SmoothGrad achieves lower RMSE than GradCAM and LIME due to variance reduction through averaging
- GradCAM consistently yields the highest IoU due to its spatial localization capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adversarial attacks can significantly reduce deep learning model accuracy while leaving explainability metrics like IoU and RMSE largely unchanged.
- Mechanism: Adversarial noise reduces model confidence in correct predictions by shifting decision boundaries, but the spatial structure of feature importance maps (as captured by GradCAM, SmoothGrad, LIME) remains relatively stable, so IoU and RMSE between these maps and ground truth stay high.
- Core assumption: The ground truth masks generated by SAM represent the "true" explanation, and small perturbations do not materially alter the distribution of high-activation pixels.
- Evidence anchors:
  - [abstract] states "substantial decline in model accuracy... with accuracies dropping from 89.94% to 58.73% and 45.50% under FGSM and BIM attacks, respectively. Despite these declines in accuracy, the explanation of the models measured by metrics such as Intersection over Union (IoU) and Root Mean Square Error (RMSE) shows negligible changes."
  - [section] "However, we noted no significant changes in the IoU and RMSE metrics for the explanations after the attacks, suggesting that these explanation methods with the combination of IoU and RMSE metrics are not effective in discerning adversarial influences."
- Break condition: If adversarial perturbations systematically shift feature activation patterns in ways that the explanation methods highlight, then IoU/RMSE would change noticeably.

### Mechanism 2
- Claim: SmoothGrad achieves lower RMSE than GradCAM or LIME because it averages multiple noisy saliency maps, reducing variance in pixel importance estimates.
- Mechanism: By adding Gaussian noise to inputs and averaging resulting saliency maps, SmoothGrad smooths out spurious high-magnitude gradients, yielding a more stable, lower-variance explanation.
- Core assumption: Multiple noisy passes suppress transient, high-variance activations that would otherwise inflate RMSE.
- Evidence anchors:
  - [section] "SmoothGrad... is a gradient-based technique designed to refine the interpretability of neural network decisions by producing smoother saliency maps."
  - [section] Table 2 shows SmoothGrad consistently has the lowest RMSE (36.21%, 36.46%, 36.66%) compared to GradCAM (37.40%, 37.95%, 38.43%) and LIME (43.28%, 43.59%, 44.37%).
- Break condition: If the noise level or number of samples is too low, the variance reduction effect diminishes and RMSE will increase.

### Mechanism 3
- Claim: GradCAM consistently yields the highest IoU because it leverages spatial localization of class-discriminative regions in feature maps, aligning better with ground truth masks.
- Mechanism: GradCAM computes gradients of the class score with respect to feature maps and applies a weighted combination of these maps, directly producing a spatial heatmap that aligns with where the model "looks" for the class.
- Core assumption: Ground truth masks from SAM correctly capture the spatial extent of the object, and GradCAM's weighted combination preserves this spatial structure.
- Evidence anchors:
  - [section] "GradCAM computes the gradient of the target class score (the output before activation) with respect to the feature maps of a convolutional layer... These weights are then used to create a weighted combination of the feature maps, producing a heatmap."
  - [section] Table 1 shows GradCAM consistently has the highest IoU (34.66%, 33.67%, 32.75%) compared to SmoothGrad (19.83%, 19.63%, 19.46%) and LIME (24.78%, 24.57%, 24.63%).
- Break condition: If GradCAM is applied to layers too deep or too shallow, the localization signal may degrade, lowering IoU.

## Foundational Learning

- Concept: Gradient-based saliency maps (GradCAM, SmoothGrad)
  - Why needed here: These techniques generate pixel importance heatmaps that can be quantitatively compared to ground truth masks via IoU/RMSE.
  - Quick check question: What is the role of global average pooling in GradCAM?
- Concept: Adversarial example generation (FGSM, BIM)
  - Why needed here: These methods introduce imperceptible perturbations that drastically reduce model accuracy, allowing testing of explanation robustness under attack.
  - Quick check question: How does BIM differ from FGSM in terms of perturbation application?
- Concept: IoU and RMSE metrics for explanation comparison
  - Why needed here: These metrics provide quantitative measures of how well explanation heatmaps align with ground truth masks, enabling systematic benchmarking.
  - Quick check question: Why might IoU be more sensitive than RMSE for spatial alignment tasks?

## Architecture Onboarding

- Component map: Dataset -> SAM (ground truth masks) -> EfficientNetV2B0 -> Explanation methods (GradCAM, SmoothGrad, LIME) -> IoU/RMSE comparison -> Adversarial attack (FGSM, BIM) -> Repeat pipeline
- Critical path: Ground truth generation -> Model inference -> Explanation generation -> Metric computation
- Design tradeoffs: Using pre-trained EfficientNetV2B0 trades some model capacity for faster, more stable explanations. Selecting only images with clear SAM masks reduces dataset size but improves ground truth quality.
- Failure signatures: If IoU stays high but accuracy drops, explanation methods may be insensitive to adversarial perturbations. If RMSE varies widely across methods, some explanations are unstable or noisy.
- First 3 experiments:
  1. Run the pipeline on the 189-image subset without any attack to establish baseline IoU/RMSE
  2. Apply FGSM with ε=0.025 and measure changes in accuracy and IoU/RMSE for each explanation method
  3. Apply BIM with ε=0.025 over 10 iterations and repeat the same measurements

## Open Questions the Paper Calls Out
None

## Limitations
- The study relies on ground truth masks from SAM, which may not perfectly align with true object boundaries
- The subset of ImageNet used (189 images) is relatively small, potentially limiting generalizability
- The analysis does not explore adaptive attacks specifically designed to degrade explanation methods

## Confidence

**Major Uncertainties and Limitations:**
- The study relies on ground truth masks from SAM, which may not perfectly align with the true object boundaries, introducing noise into IoU/RMSE metrics. Additionally, the subset of ImageNet used (189 images) is relatively small, potentially limiting generalizability to other datasets or attack types. The analysis does not explore adaptive attacks specifically designed to degrade explanation methods, leaving open the possibility that current metrics are insufficient against more sophisticated adversarial strategies.

**Confidence Labels:**
- High confidence in the observation that adversarial attacks significantly reduce model accuracy while explanation metrics remain stable, as this is directly supported by the experimental results and tables provided.
- Medium confidence in the mechanism that IoU/RMSE are not sensitive to adversarial perturbations, as this conclusion depends on the assumption that SAM masks are accurate and representative of true explanations.
- Low confidence in the broader implication that all explainability metrics are inherently insensitive to adversarial attacks, given the limited scope of attacks and metrics evaluated.

## Next Checks
1. Validate SAM-generated masks against human-annotated ground truth on a subset of images to quantify alignment error and its impact on IoU/RMSE
2. Test additional adaptive attacks (e.g., targeted adversarial attacks on explanation outputs) to determine if IoU/RMSE become more sensitive under more sophisticated adversarial strategies
3. Expand the evaluation to include more diverse datasets (e.g., COCO, medical imaging) and attack methods (e.g., CW, PGD) to assess the robustness of the findings across different domains and adversarial scenarios