---
ver: rpa2
title: Tokenisation is NP-Complete
arxiv_id: '2412.15210'
source_url: https://arxiv.org/abs/2412.15210
tags:
- tokenisation
- symbols
- which
- problem
- will
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proves the NP-completeness of two variants of tokenisation,\
  \ defined as the problem of compressing a dataset to at most \u03B4 symbols. The\
  \ variants are direct tokenisation, which finds a vocabulary directly, and bottom-up\
  \ tokenisation, which selects a sequence of merge operations."
---

# Tokenisation is NP-Complete
## Quick Facts
- arXiv ID: 2412.15210
- Source URL: https://arxiv.org/abs/2412.15210
- Reference count: 40
- Primary result: NP-completeness proven for two tokenisation variants (direct and bottom-up)

## Executive Summary
This paper establishes that tokenisation - the process of compressing a dataset to at most δ symbols - is NP-complete for two variants: direct tokenisation (finding a vocabulary directly) and bottom-up tokenisation (selecting a sequence of merge operations). The authors prove NP-hardness by reducing from the max-2-SAT problem, demonstrating that finding optimal tokenisers is computationally intractable. The theoretical results imply that approximate algorithms should be prioritized over exact methods in practical applications.

## Method Summary
The authors employ a reduction from max-2-SAT to establish NP-hardness for both direct and bottom-up tokenisation variants. They define tokenisation as the problem of compressing a dataset to at most δ symbols, with direct tokenisation involving vocabulary selection and bottom-up tokenisation involving merge operation sequences. The reduction demonstrates how instances of max-2-SAT can be transformed into tokenisation problems, establishing the computational equivalence between the two problem classes.

## Key Results
- Direct tokenisation (finding optimal vocabulary) is NP-complete
- Bottom-up tokenisation (selecting optimal merge operations) is NP-complete
- Both variants can be reduced from max-2-SAT in polynomial time
- Optimal tokenisation is unlikely to be efficiently computable, suggesting approximate methods should be used

## Why This Works (Mechanism)
The NP-completeness is established through polynomial-time reduction from max-2-SAT, a known NP-complete problem. The reduction constructs tokenisation instances that encode satisfiability constraints, such that solving the tokenisation problem would yield a solution to the original max-2-SAT instance. This reduction preserves the essential computational difficulty while mapping logical constraints to vocabulary construction constraints.

## Foundational Learning
- Computational complexity theory (why needed: to establish NP-completeness; quick check: understand P vs NP, NP-hard, NP-complete definitions)
- Max-2-SAT problem (why needed: serves as the source problem for reduction; quick check: understand 2-SAT and maximum satisfiability concepts)
- Tokenisation algorithms (why needed: to understand the problem being analyzed; quick check: know BPE, WordPiece, and vocabulary construction basics)
- Reduction techniques (why needed: method for proving NP-hardness; quick check: understand how problems can be transformed while preserving computational complexity)
- Compression theory (why needed: to understand the connection between tokenisation and compression; quick check: understand basic compression metrics and objectives)

## Architecture Onboarding
The theoretical framework consists of two main components: the problem definition and the reduction proof. The critical path follows from defining the tokenisation variants, establishing membership in NP, and then proving NP-hardness through reduction. Design tradeoffs involve choosing between exact optimization (computationally intractable) and approximate methods (practically feasible). Failure signatures would manifest as inability to find optimal solutions within reasonable timeframes for non-trivial instances. First experiments should include: (1) implementing the max-2-SAT reduction for small instances, (2) testing heuristic tokenisation against optimal solutions on toy datasets, and (3) analyzing the gap between theoretical complexity and empirical performance.

## Open Questions the Paper Calls Out
None

## Limitations
- Reduction from max-2-SAT may not capture real-world tokenisation complexity
- Theoretical model assumes binary vocabularies differing from practical implementations
- Focuses on exact optimization rather than approximate methods actually used in practice
- Theoretical findings require empirical validation in actual NLP pipelines

## Confidence
Theoretical framework: High
Practical implications: Medium
Compression relationship: Medium

## Next Checks
1. Implement empirical experiments comparing optimal tokenisation (using exact methods for small instances) versus heuristic approaches on standard NLP datasets to quantify the performance gap suggested by the theoretical results.

2. Conduct a systematic analysis of how well the binary vocabulary model captures the behavior of actual tokenisers like BPE, WordPiece, or SentencePiece across different languages and domains.

3. Perform average-case complexity analysis by generating random instances of the tokenisation problems and measuring the actual computational effort required, comparing this with the worst-case theoretical bounds.