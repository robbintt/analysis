---
ver: rpa2
title: A Comparative Study on Annotation Quality of Crowdsourcing and LLM via Label
  Aggregation
arxiv_id: '2401.09760'
source_url: https://arxiv.org/abs/2401.09760
tags:
- labels
- crowd
- label
- datasets
- workers
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compares the annotation quality between crowdsourcing
  and Large Language Models (LLMs) by leveraging existing crowdsourcing datasets.
  It evaluates both individual label quality and aggregated label quality, proposing
  a hybrid Crowd-LLM label aggregation method.
---

# A Comparative Study on Annotation Quality of Crowdsourcing and LLM via Label Aggregation

## Quick Facts
- arXiv ID: 2401.09760
- Source URL: https://arxiv.org/abs/2401.09760
- Reference count: 27
- High-quality LLM workers (e.g., ChatGPT) improve aggregated label quality when added to crowdsourcing datasets

## Executive Summary
This study compares annotation quality between crowdsourcing and Large Language Models (LLMs) by evaluating individual label quality and aggregated label quality across multiple datasets. The research introduces a hybrid Crowd-LLM label aggregation method that combines crowd and LLM labels to improve overall annotation quality. Results demonstrate that adding high-quality LLM workers (specifically ChatGPT) to existing crowdsourcing datasets enhances the aggregated label quality beyond what either approach achieves alone. The study uses standard label aggregation models including Majority Vote, DS, and GLAD to evaluate and combine labels from both sources.

## Method Summary
The study leverages existing crowdsourcing datasets (RTE, QUIZ/ENG, QUIZ/CHI, QUIZ/ITM, QUIZ/MED, QUIZ/POK, QUIZ/SCI) containing text contents, individual crowd labels, and categorical labels. Two LLM workers are evaluated: ChatGPT (high-quality) and Vicuna (normal), with temperature parameters t={0, 0.5, 1}. The research evaluates individual label quality for both LLM workers and applies three label aggregation models (Majority Vote, DS, GLAD) to crowd labels only, LLM labels only, and hybrid labels combining both sources. The primary metric is accuracy of aggregated labels compared to estimated true labels.

## Key Results
- Adding high-quality LLM labels (ChatGPT) to crowdsourcing datasets improves aggregated label quality beyond LLM-only or crowd-only approaches
- Standard LLM workers (Vicuna) do not enhance label quality when added to crowdsourcing datasets
- Advanced aggregation models like GLAD outperform simpler models like Majority Vote in combining crowd and LLM labels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: High-quality LLM workers can enhance aggregated label quality when added to crowdsourcing datasets
- Mechanism: Good LLM workers (e.g., ChatGPT) provide accurate labels that improve the estimated true labels through aggregation models like GLAD, DS, or MV
- Core assumption: The LLM worker's individual label quality is higher than the average crowd worker, and the aggregation model can effectively integrate these labels
- Evidence anchors:
  - [abstract] "Results show that adding high-quality LLM labels (e.g., ChatGPT) to crowdsourcing datasets improves the overall aggregated label quality, surpassing the quality of LLM labels alone."
  - [section] "In addition, we propose a Crowd-LLM hybrid label aggregation method from both the individual crowd and LLM labels, and verify the performance."
  - [corpus] Weak - no direct corpus evidence for this specific claim; relies on experimental results
- Break Condition: If the LLM worker's label quality is not significantly better than average crowd workers, or if the aggregation model fails to properly weight LLM contributions

### Mechanism 2
- Claim: Normal LLM workers (e.g., Vicuna) do not enhance label quality when added to crowdsourcing datasets
- Mechanism: Normal LLM workers provide labels with quality comparable to or worse than average crowd workers, which does not improve the aggregated label quality
- Core assumption: The normal LLM worker's individual label quality is not better than the mean and median of crowd workers
- Evidence anchors:
  - [abstract] "However, standard LLM workers (e.g., Vicuna) do not enhance label quality."
  - [section] "A normal LLM worker (Vicuna) is worse than the mean and median of crowd workers; although it is free, a normal LLM worker is still not good enough to replace the crowd workers."
  - [corpus] Weak - no direct corpus evidence for this specific claim; relies on experimental results
- Break Condition: If the normal LLM worker's label quality improves through better prompts or fine-tuning, or if the aggregation model can effectively leverage lower-quality labels

### Mechanism 3
- Claim: Using advanced aggregation models like GLAD improves label quality more than simpler models like MV
- Mechanism: Advanced aggregation models model worker ability and instance difficulty, leading to better estimation of true labels compared to simple majority voting
- Core assumption: The aggregation model can accurately estimate worker ability and instance difficulty from the label data
- Evidence anchors:
  - [section] "In both Table 3 and 4, GLAD performs better than MV and DS, and using advanced aggregation models is recommended."
  - [section] "GLAD [10]: This is one typical method of modeling worker ability and instance difficulty."
  - [corpus] Weak - no direct corpus evidence for this specific claim; relies on experimental results
- Break Condition: If the label data is insufficient to estimate worker ability and instance difficulty accurately, or if the aggregation model is too complex for the task

## Foundational Learning

- Concept: Label Aggregation Models
  - Why needed here: Understanding how different aggregation models work is crucial for evaluating their performance in combining crowd and LLM labels
  - Quick check question: What is the main difference between MV, DS, and GLAD aggregation models?
- Concept: Worker Ability and Instance Difficulty
  - Why needed here: Knowing how worker ability and instance difficulty affect label quality is important for selecting appropriate aggregation models
  - Quick check question: How does GLAD model worker ability and instance difficulty differently from DS?
- Concept: LLM Label Quality
  - Why needed here: Understanding the factors that affect LLM label quality is essential for selecting good LLM workers to enhance aggregated labels
  - Quick check question: What factors influence the quality of LLM-generated labels?

## Architecture Onboarding

- Component map: Datasets -> LLM Workers (ChatGPT, Vicuna) -> Aggregation Models (MV, DS, GLAD) -> Aggregated Labels -> Evaluation
- Critical path: Datasets → LLM Workers → Aggregation Models → Aggregated Labels → Evaluation
- Design tradeoffs:
  - Using high-quality LLM workers improves aggregated label quality but may be more expensive
  - Using normal LLM workers is cheaper but may not enhance label quality
  - Advanced aggregation models perform better but are more complex to implement
- Failure signatures:
  - Aggregated label quality does not improve when adding LLM labels
  - Aggregation models fail to converge or produce unreasonable results
  - LLM workers generate labels that are inconsistent with the dataset's format or requirements
- First 3 experiments:
  1. Evaluate individual label quality of ChatGPT and Vicuna on a subset of the datasets
  2. Compare aggregated label quality using MV, DS, and GLAD on crowd labels only
  3. Compare aggregated label quality using MV, DS, and GLAD on hybrid crowd and LLM labels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the costs and benefits of using high-quality LLM workers (e.g., ChatGPT) compare to traditional quality control methods (e.g., qualification tests) for selecting high-ability crowd workers?
- Basis in paper: [explicit] The paper suggests that if quality control methods are used to select high-ability crowd workers, crowd workers are still possibly better than good LLM workers, but it does not provide a detailed cost-benefit analysis of using LLMs versus quality control methods
- Why unresolved: The paper does not provide a detailed analysis of the costs and benefits of using LLMs compared to traditional quality control methods
- What evidence would resolve it: A comprehensive study comparing the costs and benefits of using high-quality LLM workers versus traditional quality control methods for selecting high-ability crowd workers

### Open Question 2
- Question: How do different types of labels (e.g., numerical, pairwise comparison, triplet comparison, text sequence) affect the performance of LLM workers compared to crowd workers?
- Basis in paper: [explicit] The paper acknowledges that it only covers categorical labels and mentions other types of labels that are not yet included, but it does not provide any analysis or comparison of LLM performance across different label types
- Why unresolved: The paper does not include a comparative study of LLM performance across different label types
- What evidence would resolve it: A study that evaluates and compares the performance of LLM workers and crowd workers across different types of labels

### Open Question 3
- Question: How does the quality of LLM-generated labels affect the performance of label aggregation models when combined with crowd labels?
- Basis in paper: [explicit] The paper shows that adding good LLM labels to existing crowdsourcing datasets can enhance the quality of aggregated labels, but it does not provide a detailed analysis of how the quality of LLM-generated labels specifically affects the performance of label aggregation models
- Why unresolved: The paper does not provide a detailed analysis of the relationship between the quality of LLM-generated labels and the performance of label aggregation models
- What evidence would resolve it: A study that systematically varies the quality of LLM-generated labels and evaluates the impact on the performance of label aggregation models

## Limitations

- The study relies entirely on experimental results rather than established theoretical foundations or extensive prior literature for its claims about LLM label quality and aggregation effectiveness
- Specific implementation details of the aggregation models and exact LLM prompting strategies are not fully specified, limiting reproducibility
- The evaluation focuses primarily on accuracy metrics without examining other quality dimensions like consistency, bias, or cost-effectiveness
- The selection of datasets and their representativeness for different domains remains unclear

## Confidence

- **High Confidence:** The observation that high-quality LLM workers (ChatGPT) can improve aggregated label quality when added to crowdsourcing datasets is well-supported by experimental results
- **Medium Confidence:** The finding that normal LLM workers (Vicuna) do not enhance label quality has experimental support but may be sensitive to prompting strategies and temperature settings
- **Medium Confidence:** The claim that advanced aggregation models like GLAD outperform simpler models like MV is supported by results but depends on data characteristics

## Next Checks

1. Conduct ablation studies to isolate the impact of LLM worker quality versus aggregation model sophistication on overall label quality
2. Test the robustness of findings across additional datasets and domains, particularly those with different label distributions and complexity levels
3. Evaluate cost-benefit tradeoffs by incorporating worker costs and aggregation computation time into the quality assessment framework