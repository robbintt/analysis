---
ver: rpa2
title: Evolving Benchmark Functions to Compare Evolutionary Algorithms via Genetic
  Programming
arxiv_id: '2403.14146'
source_url: https://arxiv.org/abs/2403.14146
tags:
- functions
- benchmark
- algorithms
- distance
- fitness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel approach to automatically generate
  optimization benchmark functions using Genetic Programming (GP). The key idea is
  to evolve functions that maximize the difference in solution distributions between
  two given optimization algorithms, as measured by Wasserstein distance.
---

# Evolving Benchmark Functions to Compare Evolutionary Algorithms via Genetic Programming

## Quick Facts
- arXiv ID: 2403.14146
- Source URL: https://arxiv.org/abs/2403.14146
- Authors: Yifan He; Claus Aranha
- Reference count: 20
- Key outcome: GP-evolved benchmark functions differentiate algorithm pairs better than CEC2005 benchmarks in decision space using Wasserstein distance fitness.

## Executive Summary
This paper proposes using Genetic Programming to automatically generate optimization benchmark functions that maximize the difference in solution distributions between two given optimization algorithms. The approach uses MAP-Elites to maintain diversity in the generated functions based on fitness landscape features like Fitness Distance Correlation and neutrality. Experiments show that the evolved benchmarks can better differentiate between two Differential Evolution configurations and between SHADE and CMA-ES compared to CEC2005 benchmarks, with statistical significance in decision space separation.

## Method Summary
The method uses Genetic Programming to evolve mathematical functions as benchmark problems, with fitness measured by the Wasserstein distance between solution distributions of two optimizers. MAP-Elites maintains a diverse archive of elite functions partitioned by phenotypic descriptors (FDC, neutrality, best-fitness-equality). The GP uses Koza's tree-based representation with specific function and terminal sets, evolving 2D functions that are then extended to 10D by summing 1D slices. Two optimizer pairs (DE configurations and SHADE/CMA-ES) are tested against both evolved and CEC2005 benchmarks.

## Key Results
- Evolved functions achieved larger mean Wasserstein distance values than CEC2005 benchmarks in differentiating DE configurations (statistically significant)
- Heatmaps showed algorithm differences vary across different landscape feature regions
- Generated functions performed comparably to CEC2005 benchmarks in differentiating algorithms in objective space
- MAP-Elites maintained diversity across FDC and neutrality dimensions in the archive

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GP-generated benchmark functions can differentiate between two optimization algorithms better than human-made functions in decision space.
- Mechanism: The fitness measure uses Wasserstein distance between solution distributions from each optimizer, maximizing the statistical divergence of their sampled points.
- Core assumption: Larger Wasserstein distance in decision space implies more distinct algorithmic behaviors.
- Evidence anchors:
  - [abstract] "The functions produced by our approach separate both algorithm pairs better than the CEC2005 benchmarks in the decision space, and hold a comparable performance in differentiating algorithms in the objective space."
  - [section IV-A] "Table II shows the largest Wasserstein distance... differentiate the two settings of DE better than the CEC2005 benchmark functions in the training phase."
  - [corpus] Weak/no direct evidence; corpus papers focus on evolving optimizers, not benchmarks, so limited overlap.
- Break condition: If algorithms produce similar solution distributions regardless of function landscape, Wasserstein distance will be low and the GP cannot evolve discriminative functions.

### Mechanism 2
- Claim: MAP-Elites selection using landscape features (FDC, neutrality) produces a diverse set of benchmark functions with varying discriminative power.
- Mechanism: Phenotype descriptors partition the archive into cells; only the best function per cell is kept, ensuring coverage of different FDC/neutrality regions.
- Core assumption: Different algorithm pairs exhibit different sensitivity to FDC and neutrality; thus, diversifying these features yields functions that highlight algorithmic differences.
- Evidence anchors:
  - [abstract] "Additionally, we use MAP-Elites to both enhance the search power of the GP and also illustrate how the difference between optimizers changes by various landscape features."
  - [section III-C] "In our proposed method, the MAP-Elites maintains a three-dimensional archive... Functions with FDC and neutrality near 0 have higher Wasserstein distance values."
  - [corpus] Weak overlap; neighbor papers do not explicitly discuss MAP-Elites or landscape-feature-based archive diversity.
- Break condition: If all generated functions end up in the same phenotype cell, diversity is lost and the archive fails to cover discriminative regions.

### Mechanism 3
- Claim: Evolving 2D functions and then extending them to higher dimensions preserves their discriminative properties.
- Mechanism: The 10D function is constructed as a sum of 1D slices of the 2D function, maintaining relative differences in decision space.
- Core assumption: Local landscape characteristics in 2D translate to analogous characteristics in higher dimensions under this extension scheme.
- Evidence anchors:
  - [section IV-A] "We create 10-dimensional functions h10 based on these functions h using the following approach: h10(x1, x2, ..., x10) = 1/9 Σi=1^9 h(xi, xi+1)"
  - [section IV-A] "functions generated by GP hold a larger mean value of ∆x... with statistical significance"
  - [corpus] No direct evidence; neighbor papers do not discuss dimensional extension of evolved functions.
- Break condition: If the dimensional extension distorts the original fitness landscape enough that algorithmic differences vanish, the test performance will not match training results.

## Foundational Learning

- Concept: Wasserstein distance as a measure of distribution difference.
  - Why needed here: It quantifies how differently two optimizers explore the search space, which is the core fitness signal for evolving benchmarks.
  - Quick check question: What does a larger Wasserstein distance between two solution sets indicate about the optimizers' behaviors?

- Concept: Fitness Distance Correlation (FDC) and neutrality as landscape metrics.
  - Why needed here: They serve as phenotypic descriptors in MAP-Elites to ensure generated functions cover diverse deceptive and neutral regions, which can stress different algorithms differently.
  - Quick check question: How do low FDC and high neutrality values affect the difficulty of a landscape for local search methods?

- Concept: MAP-Elites archive partitioning and elitism.
  - Why needed here: It maintains a diverse set of elite functions across predefined phenotype intervals, preventing premature convergence to a single landscape type.
  - Quick check question: What would happen if the archive had only one phenotype descriptor instead of three?

## Architecture Onboarding

- Component map: GP tree generator -> fitness evaluator (Wasserstein distance) -> MAP-Elites archive (FDC, neutrality, best-fitness-equality) -> selection/crossover/mutation -> next generation
- Critical path: Function generation -> solution sampling by both optimizers -> Wasserstein distance computation -> archive insertion -> reproduction
- Design tradeoffs: Using 2D functions speeds training but may limit realism; high-dimensional extension validates scalability but adds computational cost. Simple GP operators keep evolution fast but may restrict functional expressiveness.
- Failure signatures: Low archive diversity (many empty cells), stagnant Wasserstein distances across generations, or test performance not matching training performance.
- First 3 experiments:
  1. Run GP with only FDC as phenotype descriptor; compare archive diversity and final discriminative power against the three-descriptor setup.
  2. Replace Wasserstein distance with Kolmogorov-Smirnov statistic; observe changes in evolved function characteristics and algorithm separation.
  3. Test alternative dimensional extension methods (e.g., tensor product) and measure whether discriminative power in higher dimensions is preserved.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed method be generalized to generate benchmark functions for comparing more than two algorithms simultaneously?
- Basis in paper: [inferred] The paper focuses on comparing two algorithms and mentions future work to "explore other possible indicators for the phenotypical diversity in MAP-Elites" but does not discuss multi-algorithm comparison.
- Why unresolved: The current method uses Wasserstein distance between two solution sets, which would need modification for multiple algorithms. The fitness landscape metrics used for diversity might not capture differences relevant to multiple algorithms.
- What evidence would resolve it: Experiments showing the method can generate functions that differentiate between three or more algorithms, with statistical analysis of the results.

### Open Question 2
- Question: How sensitive is the quality of generated benchmarks to the choice of landscape metrics used in MAP-Elites?
- Basis in paper: [explicit] The paper states "one of our future work is to explore other possible indicators for the phenotypical diversity in MAP-Elites" and uses FDC and neutrality as phenotypic descriptors.
- Why unresolved: The paper only tests two metrics (FDC and neutrality) and does not compare their effectiveness to other potential metrics. The choice of metrics could significantly impact the diversity and quality of generated functions.
- What evidence would resolve it: Comparative experiments using different landscape metrics (e.g., ruggedness, epistasis) in MAP-Elites, showing how the choice affects the generated benchmarks' ability to differentiate algorithms.

### Open Question 3
- Question: Can the proposed method generate benchmark functions that are more representative of real-world optimization problems?
- Basis in paper: [explicit] The paper cites literature [13] stating that existing benchmarks are "not close to real-world cases" and aims to automatically design benchmarks.
- Why unresolved: The generated functions are mathematical compositions that may not capture the complexity and characteristics of real-world problems. The method's ability to produce practically relevant benchmarks is not validated.
- What evidence would resolve it: Analysis comparing the landscape features of generated functions to those of real-world problems, and experiments showing algorithms performing on generated benchmarks also perform well on real-world problems.

### Open Question 4
- Question: How does the proposed method compare to existing techniques for generating synthetic benchmark functions, such as instance space analysis or surrogate function generation?
- Basis in paper: [explicit] The paper contrasts its approach with prior work [14], [15] on generating diverse or surrogate functions, but does not directly compare performance.
- Why unresolved: The paper only compares generated functions to CEC2005 benchmarks, not to other synthetic benchmark generation methods. The advantages or disadvantages of the proposed method relative to alternatives are unclear.
- What evidence would resolve it: Experiments comparing the proposed method's generated benchmarks to those from instance space analysis [14] and surrogate function generation [15] in terms of differentiating algorithms and representing problem characteristics.

## Limitations

- Small-scale tests with only two DE configurations and two optimizer pairs limit generalizability
- Dimensional extension from 2D to 10D uses heuristic construction without validation of property preservation
- Archive diversity maintenance is assumed but not empirically validated through coverage analysis
- Wasserstein distance as sole fitness signal may miss other relevant behavioral differences between optimizers

## Confidence

- Mechanism 1 (Wasserstein-based fitness): Medium - supported by described results but lacks statistical detail and broader algorithm coverage
- Mechanism 2 (MAP-Elites diversity): Medium - method is described clearly, but diversity outcomes are not empirically demonstrated
- Mechanism 3 (dimensional extension): Low - the construction method is stated but no validation that discriminative properties are preserved

## Next Checks

1. Run the full GP-MAP-Elites pipeline with statistical significance testing (e.g., Wilcoxon rank-sum) to quantify performance differences between evolved and CEC2005 benchmarks across multiple optimizer pairs.
2. Perform ablation studies: evolve benchmarks using only FDC or only neutrality as phenotype descriptors, and compare archive diversity and discriminative power against the full three-descriptor setup.
3. Validate the dimensional extension by testing evolved 2D functions directly in 10D without summation, or by using alternative extension methods (e.g., tensor products), and compare algorithm separation performance.