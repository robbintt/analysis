---
ver: rpa2
title: 'Beyond Unimodal Learning: The Importance of Integrating Multiple Modalities
  for Lifelong Learning'
arxiv_id: '2405.02766'
source_url: https://arxiv.org/abs/2405.02766
tags:
- learning
- multimodal
- after
- modalities
- audio
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper demonstrates that integrating multiple modalities (audio
  and visual) significantly improves continual learning performance by reducing catastrophic
  forgetting and enhancing model stability and plasticity. The proposed method, SAMM,
  leverages relational structural similarities between data points in each modality
  to align and consolidate modality-specific representations, resulting in improved
  knowledge transfer and retention.
---

# Beyond Unimodal Learning: The Importance of Integrating Multiple Modalities for Lifelong Learning

## Quick Facts
- arXiv ID: 2405.02766
- Source URL: https://arxiv.org/abs/2405.02766
- Reference count: 40
- Primary result: SAMM achieves up to 37.8% mean accuracy on Seq-VGGSound, outperforming unimodal approaches in continual learning

## Executive Summary
This paper addresses catastrophic forgetting in continual learning by proposing a multimodal approach that integrates audio and visual information. The authors introduce SAMM (Semantic-Aware Multimodal method), which leverages relational structural similarities between modalities to align and consolidate representations, resulting in improved knowledge transfer and retention. The method enables both single- and multimodal inference by dynamically weighing modality contributions based on confidence scores. On the MMCL benchmark, SAMM demonstrates significant improvements over unimodal approaches, achieving up to 37.8% mean accuracy on Seq-VGGSound.

## Method Summary
SAMM employs experience replay with reservoir sampling to maintain a buffer of past samples, while consistency regularization and feature alignment losses ensure knowledge retention across tasks. The approach uses separate ResNet18 encoders for audio and visual modalities, with FiLM-based fusion to create multimodal representations. A key innovation is the use of distance-wise relation knowledge distillation to align the pairwise distance structures between data points in each modality, preserving semantic relationships. The method also implements dynamic multimodal inference through temperature-scaled confidence weighting of modality-specific classifiers, allowing the model to adaptively rely on the most informative modality for each sample.

## Key Results
- SAMM achieves up to 37.8% mean accuracy on Seq-VGGSound, outperforming unimodal baselines
- Multimodal training significantly reduces catastrophic forgetting compared to single-modality approaches
- Dynamic multimodal inference with confidence weighting improves performance under noisy or missing modalities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Integrating multiple modalities reduces catastrophic forgetting by creating a more robust and generalized representation space that is less sensitive to modality-specific shifts.
- Mechanism: Multimodal training exposes the model to complementary views of the same task, allowing it to form a representation that is not overly dependent on any single modality's idiosyncrasies. When a shift occurs in one modality (e.g., visual changes from day to night), the other modality (e.g., audio) can provide a stable reference point, maintaining performance across tasks.
- Core assumption: The complementary modalities exhibit varying degrees of robustness to different types of distribution shifts, and their integration leads to a representation that captures the invariant aspects of the task.
- Evidence anchors:
  - [abstract] "leveraging multiple views and complementary information from multiple modalities enables the model to learn more accurate and robust representations. This makes the model less vulnerable to modality-specific regularities and considerably mitigates forgetting."
  - [section] "different modalities might exhibit different behavior and sensitivity in terms of shifts in representations depending upon the nature of domain shift. For instance, moving from daylight scenarios to nighttime would incur a greater shift in the visual domain compared to audio, whereas moving from an indoor to an outdoor setting may incur a higher shift in the audio domain."
  - [corpus] "Continual Learning for Multiple Modalities" - explicitly targets the multimodal CL gap.
- Break condition: If the modalities are highly correlated and share the same failure modes, the robustness benefit diminishes or disappears.

### Mechanism 2
- Claim: The relational structural alignment between modalities ensures that the fused representation preserves semantic relationships across tasks, improving knowledge transfer and retention.
- Mechanism: By aligning the pairwise distance structures between data points in each modality, the model enforces a consistent semantic geometry across both modality-specific and fused spaces. This alignment acts as a form of regularization that preserves knowledge when consolidating across tasks.
- Core assumption: The relational structure (pairwise distances) between data points is a meaningful proxy for semantic similarity and can be aligned across modalities without loss of task-relevant information.
- Evidence anchors:
  - [abstract] "propose a method for integrating and aligning the information from different modalities by utilizing the relational structural similarities between the data points in each modality."
  - [section] "we employ the distance-wise relation knowledge distillation loss... which encourages a similar pairwise relationship structure in the different modalities by penalizing distance differences between their individual output representation spaces."
  - [corpus] "ConSurv: Multimodal Continual Learning for Survival Analysis" - uses modality fusion for CL, supporting the general approach.
- Break condition: If the modalities are semantically misaligned (e.g., audio unrelated to visual content), enforcing alignment could distort the representation space and hurt performance.

### Mechanism 3
- Claim: Dynamic multimodal inference, where predictions are weighted by modality confidence, improves performance in the presence of noisy or missing modalities.
- Mechanism: By calibrating modality-specific classifiers and using softmax confidence scores to weight their outputs, the model can adaptively rely more on the informative modality for each sample. This allows graceful degradation when one modality is corrupted or absent.
- Core assumption: Modality confidence scores are good proxies for informativeness and that calibration via temperature scaling yields well-calibrated probabilities.
- Evidence anchors:
  - [abstract] "Our method sets a strong baseline that enables both single- and multimodal inference."
  - [section] "we use a weighted ensemble of the classifiers based on the softmax confidence score... At the end of each task, we calibrate the classifiers using temperature scaling... This provides us with a simple and effective approach for leveraging different modalities based on their quality of signal."
  - [corpus] Weak - no direct evidence in corpus neighbors for confidence-based weighting in multimodal CL.
- Break condition: If the confidence scores are poorly calibrated or if both modalities are noisy simultaneously, the weighting could amplify errors.

## Foundational Learning

- Concept: Catastrophic forgetting in neural networks
  - Why needed here: Understanding why models lose performance on earlier tasks when trained on new ones is central to the problem being solved.
  - Quick check question: What happens to the weights of a network trained on task A when it is then trained on task B without any mitigation strategy?

- Concept: Multimodal learning and representation fusion
  - Why needed here: The method relies on integrating audio and visual information; knowing how different fusion strategies work is critical.
  - Quick check question: What are the trade-offs between early, mid, and late fusion of audio and visual features?

- Concept: Experience replay and episodic memory in CL
  - Why needed here: The approach builds on replay-based CL; understanding how buffer sampling and rehearsal work is key to grasping the method.
  - Quick check question: How does reservoir sampling ensure a uniform representation of past tasks in a fixed-size buffer?

## Architecture Onboarding

- Component map:
  - Audio encoder (ResNet18 variant, 1 input channel) -> Visual encoder (ResNet18, 4 frames input) -> Multimodal fusion (FiLM-based affine transformation) -> Modality-specific classifiers (audio, visual, multimodal) -> Memory buffer (reservoir-sampled episodic replay) -> Alignment module (distance-wise relation distillation)

- Critical path:
  1. Encode audio and visual inputs into modality-specific features.
  2. Apply consistency regularization using stored logits.
  3. Compute and apply relational alignment loss.
  4. Fuse features and update all classifier heads.
  5. Store current logits and samples in buffer.

- Design tradeoffs:
  - Buffer size vs. forgetting: Larger buffers reduce forgetting but increase memory and compute.
  - Alignment strength (β) vs. flexibility: Stronger alignment preserves knowledge but may hinder adaptation.
  - Modality weighting vs. robustness: Confidence-based weighting improves performance under noise but requires good calibration.

- Failure signatures:
  - If modality alignment is too strong, the model may struggle to adapt to new tasks.
  - If buffer sampling is biased, the model may overfit to recent tasks.
  - If confidence calibration is poor, dynamic inference may degrade performance.

- First 3 experiments:
  1. Train unimodal ER baselines (audio, visual) and compare forgetting curves.
  2. Enable multimodal training without alignment; measure gains over unimodal.
  3. Add alignment loss; ablate β to find optimal strength.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do multimodal learning approaches scale when integrating more than two modalities (e.g., adding language or haptic feedback) in continual learning settings?
- Basis in paper: [explicit] The paper focuses on audio-visual integration and mentions the potential for extending to additional modalities like language, but does not empirically test this.
- Why unresolved: The study only evaluates two modalities (audio and visual), leaving open questions about the effectiveness of multimodal learning with more diverse input types.
- What evidence would resolve it: Experimental results comparing continual learning performance across models trained with two, three, or more modalities, particularly in benchmarks like MMCL extended to include language or other sensory inputs.

### Open Question 2
- Question: What is the impact of varying the sampling frequency of individual modalities on the stability-plasticity tradeoff in multimodal continual learning?
- Basis in paper: [explicit] The paper includes experiments with different audio sampling frequencies (10kHz vs. 20kHz) and observes performance changes, but does not deeply analyze how sampling frequency affects the balance between stability and plasticity.
- Why unresolved: While performance differences are noted, the study does not systematically explore how sampling frequency interacts with the model's ability to retain old knowledge while learning new tasks.
- What evidence would resolve it: A detailed ablation study measuring stability and plasticity metrics across a range of sampling frequencies for each modality, with corresponding analysis of knowledge retention and task performance.

### Open Question 3
- Question: How does the proposed SAMM method perform under non-uniform data distributions and class imbalances compared to other multimodal continual learning approaches?
- Basis in paper: [explicit] The paper evaluates SAMM on uniform and long-tail distributions in the GCIL-VGGSound scenario, but does not compare it to other state-of-the-art multimodal continual learning methods under these conditions.
- Why unresolved: The study establishes SAMM's effectiveness against unimodal baselines but lacks direct comparisons with competing multimodal methods in challenging data regimes.
- What evidence would resolve it: Head-to-head comparisons of SAMM with other multimodal continual learning approaches (e.g., MWC, ER-AML) on the same non-uniform and imbalanced datasets, reporting stability, plasticity, and overall task performance.

## Limitations

- The empirical validation is confined to synthetic class-incremental splits on a single multimodal dataset (VGGSound), limiting generalizability to real-world continual learning scenarios with domain shifts and class imbalances
- The method's performance gains over unimodal baselines may partly stem from the specific choice of dataset and task structure rather than universal multimodal benefits
- The dynamic inference component's effectiveness relies on well-calibrated confidence scores, but the paper does not validate the calibration quality or robustness to systematic confidence miscalibration

## Confidence

- **High confidence**: The core claim that multimodal integration reduces catastrophic forgetting is well-supported by empirical results and aligns with established principles of representation robustness
- **Medium confidence**: The relational alignment mechanism's effectiveness is demonstrated, but its general applicability across diverse multimodal tasks remains to be validated
- **Medium confidence**: The dynamic inference approach is conceptually sound, but its practical benefits depend on the quality of confidence calibration, which is not thoroughly evaluated

## Next Checks

1. Evaluate SAMM on a broader range of continual learning scenarios, including domain-incremental and task-incremental settings, and on datasets with different modalities (e.g., text-vision, sensor data)
2. Conduct ablation studies to quantify the individual contributions of the three key mechanisms (consistency regularization, feature alignment, and dynamic inference) and test their robustness to hyperparameter variations
3. Assess the quality of confidence calibration in the dynamic inference component using proper scoring rules (e.g., expected calibration error) and test performance under systematic confidence miscalibration