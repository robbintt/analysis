---
ver: rpa2
title: How to unlearn a learned Machine Learning model ?
arxiv_id: '2410.09935'
source_url: https://arxiv.org/abs/2410.09935
tags:
- data
- unwanted
- wanted
- will
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an algorithm for unlearning undesired data
  from trained machine learning models using a new objective function called Ethical
  MSE (EMSE). The method maximizes error on unwanted data while minimizing it on desired
  data, effectively making the model "forget" the unwanted data while maintaining
  or improving performance on desired data.
---

# How to unlearn a learned Machine Learning model ?

## Quick Facts
- arXiv ID: 2410.09935
- Source URL: https://arxiv.org/abs/2410.09935
- Reference count: 4
- Primary result: Introduces Ethical MSE (EMSE) objective function for unlearning unwanted data while maintaining performance on desired data

## Executive Summary
This paper addresses the problem of unlearning undesired data from trained machine learning models, proposing an algorithm that maximizes error on unwanted data while minimizing it on desired data. The approach introduces a novel objective function called Ethical MSE (EMSE) that enables selective "forgetting" of specific data points. The method was tested on polynomial regression models using both synthetic and real datasets, demonstrating significant improvements in model performance on desired data after unlearning.

The proposed unlearning algorithm operates by introducing a parameter μ that controls the trade-off between maximizing error on unwanted data and minimizing error on desired data. This allows the model to effectively remove the influence of unwanted data from its learned parameters while potentially improving performance on the remaining data. The approach introduces a new metric called Fair R-squared to measure the model's ability to represent desired data while ignoring unwanted data, providing a quantitative measure of the unlearning effectiveness.

## Method Summary
The paper proposes an unlearning algorithm based on a new objective function called Ethical MSE (EMSE). The algorithm works by optimizing the model parameters to simultaneously minimize error on desired data and maximize error on unwanted data. The EMSE objective function is defined as:

$$\text{EMSE} = \frac{1}{n_w} \sum_{i \in \text{wanted}} (y_i - \hat{y}_i)^2 + \mu \cdot \frac{1}{n_u} \sum_{j \in \text{unwanted}} (y_j - \hat{y}_j)^2$$

where $\mu$ is a parameter that controls the trade-off between the two terms. The algorithm iteratively updates the model parameters using gradient descent to minimize this objective function. The paper demonstrates the approach on polynomial regression models using both synthetic and real datasets, showing that the unlearning process can significantly improve model performance on desired data while effectively removing the influence of unwanted data.

## Key Results
- R-squared improved from 0.43 to 0.98 for desired data after unlearning in one test case
- The algorithm demonstrated effectiveness even with large amounts of unwanted data, showing R-squared improvements from 0.28 to 0.96 in another case
- Introduction of Fair R-squared metric to measure model's performance in representing desired data while ignoring unwanted data

## Why This Works (Mechanism)
The algorithm works by explicitly optimizing for two competing objectives: minimizing error on desired data while maximizing error on unwanted data. This dual-objective optimization creates a mechanism where the model parameters are adjusted to "forget" the unwanted data by increasing prediction errors on those points, while simultaneously improving performance on the desired data. The parameter μ controls the trade-off between these two objectives, allowing for flexible control over the unlearning process.

## Foundational Learning

1. **Ethical MSE (EMSE) Objective Function** - Needed to create a mathematical framework for unlearning unwanted data while maintaining performance on desired data. Quick check: Verify that the objective function correctly balances the two competing terms through appropriate weighting.

2. **Polynomial Regression Models** - Required as the base model architecture for demonstrating the unlearning algorithm. Quick check: Ensure polynomial degree selection is appropriate for the complexity of the underlying data patterns.

3. **Fair R-squared Metric** - Introduced to quantify the model's ability to represent desired data while ignoring unwanted data. Quick check: Validate that Fair R-squared correlates with intuitive measures of unlearning effectiveness.

4. **Gradient Descent Optimization** - Used to iteratively update model parameters to minimize the EMSE objective function. Quick check: Monitor convergence behavior and ensure stable parameter updates during optimization.

5. **Independence Assumption** - Critical assumption that wanted and unwanted data are independent, which affects the theoretical validity of the unlearning approach. Quick check: Test whether violation of this assumption leads to degraded unlearning performance.

## Architecture Onboarding

**Component Map:**
Data Preprocessing -> EMSE Objective Function -> Gradient Descent Optimization -> Model Parameters -> Evaluation Metrics

**Critical Path:**
The critical path involves the iterative optimization loop where the EMSE objective function is evaluated, gradients are computed, and model parameters are updated. This loop continues until convergence or a stopping criterion is met.

**Design Tradeoffs:**
The primary design tradeoff is the selection of the μ parameter, which controls the balance between forgetting unwanted data and maintaining performance on desired data. Higher values of μ prioritize unlearning but may degrade overall model performance, while lower values preserve performance but may be less effective at removing unwanted data influence.

**Failure Signatures:**
The algorithm may fail when the independence assumption between wanted and unwanted data is violated, leading to degraded performance on desired data during the unlearning process. Additionally, inappropriate selection of the μ parameter can result in either ineffective unlearning or significant performance degradation.

**3 First Experiments:**
1. Test the algorithm on a simple synthetic dataset where wanted and unwanted data are clearly separable to establish baseline effectiveness.
2. Vary the μ parameter systematically to understand its impact on the trade-off between unlearning effectiveness and model performance.
3. Introduce correlation between wanted and unwanted data to test the robustness of the algorithm when the independence assumption is violated.

## Open Questions the Paper Calls Out
None

## Limitations
- The algorithm assumes independence between wanted and unwanted data, which may not hold in real-world scenarios
- Currently tested only on polynomial regression models with relatively small sample sizes
- Ethical implications of deliberately degrading model performance on certain data are not fully explored
- Fair R-squared metric requires further validation and comparison with established fairness metrics

## Confidence
- **Medium** confidence in the claimed R-squared improvements from 0.43 to 0.98 and 0.28 to 0.96, as results are based on limited testing scenarios
- **High** confidence in EMSE objective function's effectiveness for polynomial regression models, but **Low** confidence for broader machine learning applications
- **Medium** confidence in the practical applicability due to the independence assumption limitation

## Next Checks
1. Test the unlearning algorithm on deep neural networks and other non-linear model architectures to assess generalizability beyond polynomial regression
2. Evaluate the algorithm on datasets where wanted and unwanted data are correlated to test the independence assumption
3. Conduct a comparative analysis between EMSE and established machine unlearning methods using standardized benchmark datasets and metrics