---
ver: rpa2
title: 'Bias Similarity Measurement: A Black-Box Audit of Fairness Across LLMs'
arxiv_id: '2410.12010'
source_url: https://arxiv.org/abs/2410.12010
tags:
- bias
- similarity
- llms
- across
- output
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces Bias Similarity Measurement (BSM), a framework
  that treats fairness as a relational property between models by unifying scalar,
  distributional, behavioral, and representational signals into a single similarity
  space. The authors evaluated 30 LLMs on over 1M prompts, revealing that instruction
  tuning primarily increases abstention rather than altering internal representations,
  and that small models gain little accuracy and can become less fair under forced
  choice.
---

# Bias Similarity Measurement: A Black-Box Audit of Fairness Across LLMs

## Quick Facts
- arXiv ID: 2410.12010
- Source URL: https://arxiv.org/abs/2410.12010
- Reference count: 40
- Primary result: Introduces BSM framework measuring fairness as relational property between models, revealing instruction tuning increases abstention without altering internal representations

## Executive Summary
This work introduces Bias Similarity Measurement (BSM), a framework that treats fairness as a relational property between models by unifying scalar, distributional, behavioral, and representational signals into a single similarity space. The authors evaluated 30 LLMs on over 1M prompts, revealing that instruction tuning primarily increases abstention rather than altering internal representations, and that small models gain little accuracy and can become less fair under forced choice. Open-weight models can match or exceed proprietary systems. Family signatures diverge: Gemma favors refusal, LLaMA 3.1 approaches neutrality with fewer refusals, and all converge toward abstention-heavy behavior. Gemma 3 Instruct matches GPT-4-level fairness at far lower cost, while Gemini's heavy abstention suppresses utility. BSM provides an auditing workflow for procurement, regression testing, and lineage screening, and extends to code and multilingual settings, reframing fairness as comparative bias similarity rather than isolated scores.

## Method Summary
The authors developed BSM to measure bias similarity across LLMs using a black-box evaluation approach. They used 10 models including Llama-2-7b, Llama-3-8B, Alpaca 7B, Vicuna-7b-v1.5, and Gemma variants, evaluating them on BBQ and UnQover datasets containing 4k-150k+ bias questions across 9 dimensions. Models were prompted with two-shot manner for BBQ and zero-shot for UnQover, then analyzed using four metrics: accuracy on disambiguated-context questions, histogram distributions, cosine distance, and Jensen-Shannon divergence. The framework compares output distributions to assess functional similarity in bias behavior across model families.

## Key Results
- Instruction tuning primarily increases abstention without significantly altering output distributions or internal representations
- Models within the same family show divergent bias behaviors, challenging assumptions about architectural lineage predicting similarity
- Open-weight models can match or exceed proprietary systems in fairness performance while offering cost advantages
- Family signatures diverge significantly: Gemma favors refusal, LLaMA 3.1 approaches neutrality, and all converge toward abstention-heavy behavior
- Gemma 3 Instruct achieves GPT-4-level fairness at far lower cost, while Gemini's heavy abstention reduces utility

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BSM reframes fairness as a relational property between models by combining scalar, distributional, behavioral, and representational signals into a unified similarity space
- Mechanism: By evaluating models across multiple dimensions (accuracy, histogram distributions, cosine distance, and JS divergence), BSM captures both performance-level and deeper behavioral similarity, revealing how biases persist or diverge across model families
- Core assumption: Bias is not an isolated property but a comparative one that can be measured by how similarly different models respond to the same bias-inducing prompts
- Evidence anchors: Abstract statement about unifying multiple signals into similarity space; use of four metrics for bias assessment

### Mechanism 2
- Claim: Instruction tuning primarily enforces abstention rather than altering internal representations, as evidenced by output distribution similarity between base and instruction-tuned models
- Mechanism: Fine-tuning shifts model behavior toward safer, more cautious responses (more "unknown" answers) without changing the underlying token distribution patterns significantly
- Core assumption: Instruction tuning is surface-level alignment that affects response style more than the internal knowledge or distributional tendencies
- Evidence anchors: Finding that fine-tuning doesn't significantly modify output distributions; observation that small models gain little accuracy under forced choice

### Mechanism 3
- Claim: Models within the same family do not necessarily produce similar output distributions, challenging the assumption that shared architecture implies shared bias behavior
- Mechanism: Different training corpora and fine-tuning strategies cause models with shared lineage to diverge in bias expression, as shown by significant cosine distance and JS divergence between Llama and Gemma families
- Core assumption: Model family membership is not a reliable predictor of bias similarity without considering training data and fine-tuning differences
- Evidence anchors: Finding that LLMs within the same family tree don't produce similar output distributions; suggestion that addressing bias in one model has limited implications for others in the same family

## Foundational Learning

- Concept: Bias benchmarking methodology (BBQ, UnQover datasets)
  - Why needed here: These datasets provide structured, multi-dimensional bias prompts necessary for comparative fairness evaluation across models
  - Quick check question: What are the key differences between BBQ and UnQover in terms of prompt structure and answer options?

- Concept: Similarity metrics (cosine distance, JS divergence)
  - Why needed here: These metrics quantify distributional similarity between model outputs, enabling relational fairness comparisons rather than isolated scores
  - Quick check question: How does cosine distance differ from JS divergence in measuring output distribution similarity?

- Concept: Instruction tuning and its effects
  - Why needed here: Understanding how fine-tuning changes (or doesn't change) model behavior is crucial for interpreting BSM results and their implications for bias mitigation
  - Quick check question: Why might instruction tuning increase abstention without significantly altering internal representations?

## Architecture Onboarding

- Component map: Data ingestion -> Bias prompt generation -> Model evaluation -> Similarity metric computation -> Visualization/analysis
- Critical path: Prompt generation -> Model inference -> Output distribution collection -> Similarity calculation -> Comparative analysis
- Design tradeoffs: Accuracy vs. comprehensiveness (using fewer bias dimensions for consistency), computational cost vs. dataset size (1M+ prompts), model accessibility (black-box vs. white-box evaluation)
- Failure signatures: High similarity scores between unrelated models (indicates metric issues), inconsistent results across dimensions (suggests dataset or prompt problems), low abstention in instruction-tuned models (suggests evaluation setup issues)
- First 3 experiments:
  1. Replicate accuracy comparisons between base and instruction-tuned models on BBQ dataset to verify fine-tuning effects
  2. Compute cosine distance between Llama2 and Llama3 across all bias dimensions to confirm family divergence
  3. Compare output distributions for ambiguous vs. disambiguated contexts to test dataset sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model size (parameter count) affect bias similarity across LLM families?
- Basis in paper: The paper focused exclusively on 7B parameter models and noted that comparing different sizes within the same family could provide insights into how scaling affects performance and bias behavior
- Why unresolved: The study was limited to 7B parameter models and did not examine how model size influences bias similarity or fairness
- What evidence would resolve it: Systematic comparison of models with varying parameter counts (e.g., Llama2 7B vs Llama2 70B) across multiple bias dimensions and similarity metrics

### Open Question 2
- Question: Does instruction tuning create consistent behavioral shifts across all LLM families or only specific ones?
- Basis in paper: The paper found that Llama3-chat and Gemma2-it showed noticeable behavioral shifts in the religion dimension, while other fine-tuned models showed minimal changes
- Why unresolved: The study identified exceptions to the general finding that fine-tuning doesn't significantly modify output distributions, but didn't explain why these exceptions occurred
- What evidence would resolve it: Analysis of instruction tuning effects across multiple families with consistent methodology, including examination of training data differences and instruction-following capabilities

### Open Question 3
- Question: Can bias similarity measurement frameworks detect unauthorized model reuse or copyright infringement?
- Basis in paper: The paper mentions that model similarity detection has applications in preventing illegal model reuse that infringes copyright, and discusses functional similarity measures for black-box models
- Why unresolved: While the paper established BSM as an auditing tool, it didn't specifically test its effectiveness for detecting model theft or unauthorized copying
- What evidence would resolve it: Testing BSM on known cases of model reuse, comparing similarity scores between original and copied models, and establishing thresholds for identifying unauthorized replication

## Limitations
- The framework relies on distributional similarity metrics without direct analysis of attention patterns or internal activations that would confirm claimed mechanisms
- The comparative framework assumes that similar output distributions indicate similar bias behavior, which may not hold if models arrive at the same answers through different reasoning processes
- The evaluation focuses on multiple-choice bias detection tasks, which may not fully capture the range of real-world fairness concerns

## Confidence
- High confidence: The finding that instruction tuning increases abstention without significantly changing accuracy is well-supported by the empirical data across multiple model families
- Medium confidence: The claim that models within the same family produce divergent output distributions is supported, but the interpretation of this as evidence against family-based bias mitigation requires additional validation
- Medium confidence: The assertion that open-weight models can match proprietary systems in fairness performance is based on the specific bias dimensions tested and may not generalize to all fairness criteria

## Next Checks
1. Replicate the cosine distance and JS divergence calculations between base and instruction-tuned versions of the same model using a different random seed to verify result stability
2. Extend the evaluation to include additional bias dimensions beyond the four common ones to test whether the observed patterns hold across a broader range of fairness concerns
3. Conduct ablation studies by systematically removing different similarity metrics (accuracy, histogram, cosine distance, JS divergence) to determine which contribute most to the overall similarity assessments