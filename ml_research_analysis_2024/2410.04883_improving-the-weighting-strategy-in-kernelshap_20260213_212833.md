---
ver: rpa2
title: Improving the Weighting Strategy in KernelSHAP
arxiv_id: '2410.04883'
source_url: https://arxiv.org/abs/2410.04883
tags:
- shapley
- values
- coalitions
- strategy
- strategies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper improves the KernelSHAP approximation method for Shapley
  value explanations in machine learning models. The core contribution is replacing
  the stochastic coalition sampling weights in KernelSHAP with deterministic ones,
  significantly reducing variance in the Shapley value approximations.
---

# Improving the Weighting Strategy in KernelSHAP

## Quick Facts
- **arXiv ID**: 2410.04883
- **Source URL**: https://arxiv.org/abs/2410.04883
- **Reference count**: 40
- **Primary result**: Deterministic weighting in KernelSHAP reduces variance by 5-50% compared to stochastic sampling

## Executive Summary
This paper improves the KernelSHAP approximation method for Shapley value explanations in machine learning models by replacing stochastic coalition sampling weights with deterministic ones. The core contribution is a new weighting strategy that conditions on coalitions being sampled at least once, significantly reducing variance in the Shapley value approximations. Through extensive numerical experiments on both simulated and real-world data, the authors demonstrate that their new methods consistently outperform existing approaches, with the best-performing PySHAP* c-kernel strategy reducing the required number of contribution function evaluations by 5% to 50% compared to standard KernelSHAP implementation.

## Method Summary
The authors introduce several new weighting strategies for KernelSHAP that replace the stochastic Shapley kernel weights with deterministic conditional expectations. The key innovation is calculating E[TL(S)|TL(S)≥1] = LpS/(1-(1-pS)L) for each coalition S, where TL(S) is the number of times coalition S is sampled. They implement paired sampling strategies that sample coalitions and their complements together to exploit negative correlation, and semi-deterministic inclusion strategies (PySHAP and PySHAP*) that deterministically include the most important coalitions while sampling the remainder. The methods are evaluated through Monte Carlo experiments comparing mean absolute error (MAE) between exact and approximate Shapley values across different correlation structures and feature dimensions.

## Key Results
- The PySHAP* c-kernel strategy reduces the number of coalitions needed by 5-50% compared to standard KernelSHAP
- Paired c-kernel consistently outperforms both unique and paired sampling strategies
- PySHAP* c-kernel achieves up to 50% reduction in contribution function evaluations while maintaining same accuracy
- Performance improvements are consistent across Gaussian simulations and real-world Red Wine dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deterministic weights reduce variance compared to stochastic sampling frequencies
- Mechanism: By conditioning on coalitions being sampled at least once and using the expected sampling frequency, the method replaces a stochastic weight (proportional to TL(S)) with its deterministic conditional expectation E[TL(S)|TL(S)≥1)
- Core assumption: The sampling distribution of coalitions follows the Shapley kernel weights pS, and the number of samples L is sufficiently large to make the conditional expectation stable
- Evidence anchors:
  - [abstract]: "replaces the stochastic weights with deterministic ones to reduce the variance"
  - [section 3.3]: Derives E[TL(S)|TL(S)≥1] = LpS/(1-(1-pS)L) and applies it to paired c-kernel strategy
  - [corpus]: No direct evidence found in corpus; weak anchor
- Break condition: If sampling is too sparse (L small) or if coalition sampling deviates significantly from the assumed Shapley kernel distribution, the conditional expectation may not stabilize and variance reduction may not occur

### Mechanism 2
- Claim: Paired sampling reduces variance by creating negative correlation between complementary coalitions
- Mechanism: When sampling coalition S and its complement S together, their contribution function values v(S) and v(S) become negatively correlated, which reduces the overall variance in the weighted least squares solution
- Core assumption: The contribution functions v(S) and v(S) for complementary coalitions exhibit negative correlation in their evaluations
- Evidence anchors:
  - [section 3.2]: "paired sampling reduces the variance of the Shapley value approximations compared to the unique strategy due to negative correlation between the v(S) and v(S) values"
  - [section 3.1]: References [8,12] demonstrating variance reduction from paired sampling
  - [corpus]: No direct evidence found in corpus; weak anchor
- Break condition: If the model predictions or data structure create positive correlation between v(S) and v(S), or if pairing is broken (as in PySHAP for even M), the variance reduction effect disappears

### Mechanism 3
- Claim: Semi-deterministic inclusion of most important coalitions improves accuracy while reducing sampling burden
- Mechanism: PySHAP deterministically includes all coalitions of sizes less than P and greater than M-P based on expected sampling thresholds, ensuring the most influential coalitions are always present while only sampling the remaining ones
- Core assumption: Coalitions of extreme sizes (very small or very large) contribute more significantly to Shapley values and should be included deterministically
- Evidence anchors:
  - [section 3.4]: Describes PySHAP's deterministic inclusion of coalitions when "the number of remaining coalitions to sample Ncoal exceeds the expected number of coalitions needed to sample all coalitions of these sizes"
  - [section 4.1]: Shows PySHAP-based strategies have "jumps at Ncoal values where additional coalition sizes are deterministically included"
  - [corpus]: No direct evidence found in corpus; weak anchor
- Break condition: If the most important coalitions are not necessarily of extreme sizes, or if the deterministic threshold selection P is poorly chosen for the specific problem, accuracy gains may be limited

## Foundational Learning

- Concept: KernelSHAP approximation framework
  - Why needed here: This paper builds directly on KernelSHAP by modifying its weighting strategy, so understanding the original framework is essential
  - Quick check question: How does KernelSHAP approximate Shapley values using a weighted least squares problem, and what role do the Shapley kernel weights play?

- Concept: Shapley value axioms and efficiency property
  - Why needed here: The paper evaluates approximation accuracy using MAE between exact and approximate Shapley values, which requires understanding what exact Shapley values should sum to (the efficiency property)
  - Quick check question: What does the efficiency axiom state about the relationship between Shapley values and the prediction f(x*)?

- Concept: Variance reduction techniques in Monte Carlo estimation
  - Why needed here: The core contribution involves variance reduction through deterministic weighting, which relies on understanding variance properties in sampling-based estimators
  - Quick check question: How does paired sampling reduce variance in Monte Carlo estimation through negative correlation?

## Architecture Onboarding

- **Component map**: Coalition sampling -> Weight calculation -> Weighted least squares (Equation 6) -> Shapley values
- **Critical path**: Sampling coalitions → Calculating weights (wS) → Computing weighted least squares solution (Equation 6) → Outputting Shapley values
- **Design tradeoffs**: Stochastic weights provide unbiased estimates but high variance; deterministic weights reduce variance but may introduce bias if conditioning is incorrect; paired sampling adds complexity but improves accuracy; semi-deterministic inclusion reduces sampling time but may miss important coalitions
- **Failure signatures**: High MAE values indicate poor approximation; erratic jumps in MAE suggest incorrect deterministic thresholds; non-convergence as Ncoal→2M indicates implementation issues
- **First 3 experiments**:
  1. Compare MAE for unique vs paired vs paired c-kernel on M=10 Gaussian data with ρ=0.5 and Ncoal=100-1000
  2. Test PySHAP vs PySHAP* vs PySHAP* c-kernel on the same setup to verify pairing correction
  3. Measure runtime reduction when using PySHAP* c-kernel vs original KernelSHAP on M=20 linear model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the proposed c-kernel weighting strategy perform when applied to global Shapley values (e.g., SAGE values) or Shapley interaction values (e.g., KernelSHAP-IQ)?
- Basis in paper: [explicit] The paper mentions these applications as future directions in the conclusion section.
- Why unresolved: The paper only tested the methods on local feature attributions, not on these alternative Shapley value formulations.
- What evidence would resolve it: Numerical experiments comparing the c-kernel approach to existing methods for SAGE and KernelSHAP-IQ across various datasets and model types.

### Open Question 2
- Question: Would the PySHAP* c-kernel strategy maintain its superior performance if the initial coalition sampling were replaced with stratified sampling based on coalition size?
- Basis in paper: [inferred] The authors suggest this as an interesting direction for future research in the conclusion.
- Why unresolved: The paper only tested the method with random coalition sampling, not stratified sampling.
- What evidence would resolve it: Numerical experiments comparing the performance of PySHAP* c-kernel with random sampling versus stratified sampling across various datasets.

### Open Question 3
- Question: Can theoretical guarantees be derived for the variance reduction achieved by the c-kernel weighting strategy compared to the original KernelSHAP approach?
- Basis in paper: [explicit] The authors mention in the conclusion that theoretical insights into the weighting strategies would be valuable.
- Why unresolved: The paper only provides empirical evidence of performance improvements, not theoretical analysis.
- What evidence would resolve it: Mathematical proofs or bounds showing the variance reduction achieved by the c-kernel weights under various conditions.

## Limitations

- The variance reduction claims are primarily demonstrated on simulated data with controlled correlation structures, and real-world performance may vary with complex feature interactions.
- The conditional expectation weight correction assumes sufficient sampling frequency, but the paper lacks theoretical bounds on when the approximation breaks down for small sample sizes.
- The PySHAP semi-deterministic inclusion strategy uses heuristic threshold selection that may not generalize well across different problem domains.

## Confidence

- **High Confidence**: The mechanism of deterministic weight calculation reducing variance compared to stochastic sampling (Mechanism 1) - supported by the mathematical derivation and consistent experimental results across multiple datasets and model types.
- **Medium Confidence**: The paired sampling variance reduction claim (Mechanism 2) - while the theoretical basis is sound, the paper lacks direct empirical validation of negative correlation between v(S) and v(S) values in the experimental results.
- **Medium Confidence**: The PySHAP semi-deterministic inclusion strategy (Mechanism 3) - the approach is well-motivated but the selection of threshold P appears heuristic, and the paper does not explore sensitivity to different P values.

## Next Checks

1. **Small Sample Stability Test**: Evaluate the variance reduction performance of paired c-kernel and PySHAP* c-kernel when using very small numbers of samples (L=10, 20, 50) to determine the minimum sample size required for the deterministic weight correction to stabilize.

2. **Correlation Structure Sensitivity**: Test the proposed methods on datasets with known feature correlations (positive, negative, non-linear) to quantify how deviations from the assumed independence affect the variance reduction benefits.

3. **Real-World Application Benchmark**: Implement the new weighting strategies on a diverse set of real-world datasets (tabular, image, text) to validate the generalizability of the 5-50% reduction claims beyond the controlled Gaussian and Red Wine experiments.