---
ver: rpa2
title: 'Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation
  to Unseen Initial States'
arxiv_id: '2402.07875'
source_url: https://arxiv.org/abs/2402.07875
tags:
- states
- initial
- training
- gradient
- extrapolation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how policy gradient methods for learning
  controllers in Linear Quadratic Regulator (LQR) problems generalize to unseen initial
  states. The authors analyze underdetermined LQR settings where the training objective
  admits multiple global minimizers, and show that the extent of extrapolation depends
  critically on the degree of exploration induced by the system when commencing from
  initial states seen in training.
---

# Implicit Bias of Policy Gradient in Linear Quadratic Control: Extrapolation to Unseen Initial States

## Quick Facts
- arXiv ID: 2402.07875
- Source URL: https://arxiv.org/abs/2402.07875
- Reference count: 40
- This paper investigates how policy gradient methods for learning controllers in Linear Quadratic Regulator (LQR) problems generalize to unseen initial states.

## Executive Summary
This paper analyzes the extrapolation capabilities of policy gradient methods in linear quadratic control settings. The authors focus on underdetermined LQR problems where multiple global optima exist for the training objective, and investigate how the choice of training initial states affects generalization to unseen initial states. They show that the degree of exploration induced by the system dynamics plays a critical role in determining whether policy gradient methods can extrapolate beyond the training distribution. The theoretical analysis reveals that unlike in supervised learning, policy gradient does not implicitly minimize the Euclidean norm of the controller parameters, making the analysis more complex.

## Method Summary
The authors study policy gradient methods for learning controllers in Linear Quadratic Regulator (LQR) problems. They analyze settings where the training objective admits multiple global minimizers due to underdetermined systems. The key insight is that the exploration induced by the system when starting from training initial states determines the extrapolation capability. The authors prove theoretical bounds on extrapolation performance based on the controllability Gramian structure and provide empirical validation using both LQR problems and nonlinear systems with neural network controllers.

## Key Results
- In underdetermined LQR settings, the extent of extrapolation depends critically on the degree of exploration induced by the system when commencing from training initial states
- Insufficient exploration leads to no extrapolation beyond training initial states, while certain exploration-inducing settings enable near-perfect extrapolation
- Policy gradient does not implicitly minimize the Euclidean norm of controller parameters, unlike in supervised learning
- Experiments confirm theoretical findings in both LQR problems and extend to nonlinear systems with neural network controllers

## Why This Works (Mechanism)
The mechanism relies on the interplay between the controllability Gramian (which captures how system states evolve from initial conditions) and the structure of the policy gradient optimization landscape. When the training initial states induce sufficient exploration (i.e., the controllability Gramian has certain properties), the policy gradient dynamics can identify controllers that generalize well to unseen initial states. The key is that the optimization landscape contains a global optimum that both minimizes training loss and extrapolates well, and the policy gradient method can find this optimum when exploration is adequate.

## Foundational Learning
- **Linear Quadratic Regulator (LQR) theory**: Understanding the mathematical formulation of optimal control problems with linear dynamics and quadratic costs is essential for grasping the problem setup
- **Controllability Gramian**: This matrix captures how system states evolve from initial conditions and is crucial for analyzing exploration properties
- **Policy gradient methods**: The optimization algorithms used to learn controllers from data
- **Implicit bias in optimization**: How optimization algorithms can favor certain solutions among multiple global optima
- **Linear system theory**: The underlying mathematical framework for analyzing system dynamics and controller properties

## Architecture Onboarding
Component map: Training initial states -> System dynamics -> Controllability Gramian -> Policy gradient optimization -> Controller performance
Critical path: The analysis traces how the choice of training initial states affects the controllability Gramian, which in turn influences which global optimum the policy gradient method finds, ultimately determining extrapolation performance.
Design tradeoffs: The paper reveals a fundamental tradeoff between exploration (needed for extrapolation) and optimization efficiency - more exploration can help generalization but may make optimization more challenging.
Failure signatures: Lack of extrapolation to unseen initial states indicates insufficient exploration during training, which can be diagnosed by examining the controllability Gramian structure.
First experiments: 1) Vary training initial states in a simple LQR problem and measure extrapolation performance, 2) Compare policy gradient with explicit regularization methods in the same settings, 3) Test the theory on a nonlinear control task with a neural network controller.

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis relies heavily on linearized dynamics and quadratic cost assumptions
- Exploration analysis depends critically on controllability Gramian structure, which may not capture real-world system complexities
- Assumes known system dynamics for theoretical analysis, simplifying real-world implementation challenges

## Confidence
**Major claim confidence:**
- Theoretical extrapolation bounds: Medium confidence (proofs are rigorous but assumptions are restrictive)
- Empirical generalization to nonlinear systems: Low-Medium confidence (limited to small-scale examples)
- Exploration-induced extrapolation mechanism: High confidence (well-supported theoretically and empirically)

## Next Checks
1. Test extrapolation bounds in partially observable or stochastic LQR variants to assess robustness to uncertainty
2. Evaluate neural network controllers on larger-scale nonlinear control tasks (e.g., quadrotor or robotic arm control) with systematic variation of training initial states
3. Compare policy gradient extrapolation with explicit regularization methods (L2 regularization, dropout) in the same LQR settings to quantify the implicit bias advantage/disadvantage