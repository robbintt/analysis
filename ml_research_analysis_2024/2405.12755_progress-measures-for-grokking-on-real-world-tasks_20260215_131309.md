---
ver: rpa2
title: Progress Measures for Grokking on Real-world Tasks
arxiv_id: '2405.12755'
source_url: https://arxiv.org/abs/2405.12755
tags:
- grokking
- weight
- measures
- weights
- progress
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper challenges the prevalent hypothesis that L2 weight
  norms explain grokking by showing generalization can occur outside the expected
  low-norm range. The author proposes three new progress measures: activation sparsity
  (fraction of near-zero activations), absolute weight entropy (spread of weight magnitudes),
  and approximate local circuit complexity (KL divergence when perturbing weights).'
---

# Progress Measures for Grokking on Real-world Tasks

## Quick Facts
- arXiv ID: 2405.12755
- Source URL: https://arxiv.org/abs/2405.12755
- Authors: Satvik Golechha
- Reference count: 39
- One-line primary result: Proposes three new progress measures (activation sparsity, absolute weight entropy, and approximate local circuit complexity) that correlate better with grokking than weight norms alone.

## Executive Summary
This paper challenges the prevailing hypothesis that L2 weight norms explain grokking by demonstrating that generalization can occur even when weight norms increase. The author introduces three new progress measures—activation sparsity, absolute weight entropy, and approximate local circuit complexity—which show stronger correlation with grokking than traditional weight norm metrics. Through experiments on MNIST with a 3-layer MLP and LSTMs on IMDb, the work shows these measures provide a more nuanced understanding of the generalization process in deep learning models.

## Method Summary
The study employs a 3-layer MLP with ReLU activations trained on MNIST using AdamW optimizer with weight decay. A modified loss function (cross-entropy plus L2 penalty) is used to induce grokking while weight norms increase. Three new progress measures are implemented: activation sparsity (fraction of near-zero activations), absolute weight entropy (spread of weight magnitudes), and approximate local circuit complexity (KL divergence when perturbing weights). The approach is validated across different architectures including LSTMs on IMDb dataset.

## Key Results
- Grokking occurs even when L2 weight norms increase, challenging the weight norm hypothesis
- Activation sparsity, weight entropy, and circuit complexity correlate better with generalization than weight norms alone
- Results hold across different model architectures including MLPs and LSTMs
- Dropout eliminates grokking, suggesting specific training dynamics are required

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Weight norms alone do not cause grokking; the dynamics of weight distribution and activation patterns are more predictive.
- Mechanism: When a model groks, the L2 norm may increase due to the loss landscape, but other measures like activation sparsity and weight entropy decrease, indicating that the model is concentrating useful features and pruning redundant pathways.
- Core assumption: The cross-entropy loss drives the weights toward configurations that balance fitting training data and maintaining generalizable representations.
- Evidence anchors:
  - [abstract] The paper explicitly challenges the weight norm hypothesis by showing grokking occurs outside the "goldilocks zone" and proposes three alternative measures that correlate better with generalization.
  - [section] "We employ a clever trick to elicit grokking while the L2 norm of the weights increases during generalization" and show that activation sparsity and weight entropy still align with generalization even as norms rise.
  - [corpus] Related work discusses information-theoretic progress measures and the need for richer metrics beyond norms, aligning with this mechanism.
- Break condition: If activation sparsity or weight entropy do not decrease when grokking occurs, the mechanism would fail to explain the generalization dynamics.

### Mechanism 2
- Claim: Activation sparsity reflects the model's transition from memorization to generalization.
- Mechanism: During overfitting, many neurons fire actively, but as grokking begins, the network prunes unnecessary activations, increasing sparsity. This sparsity indicates that the model is relying on a smaller, more generalizable set of features.
- Core assumption: The training process encourages neurons to become inactive when their contributions are redundant or harmful to generalization.
- Evidence anchors:
  - [abstract] "Activation sparsity... demonstrate a stronger correlation with grokking in real-world datasets compared to weight norms."
  - [section] "Activation sparsity measures how many activations are near zero, indicating how many neurons are effectively turned 'off', with a higher sparsity implying that fewer neurons are actively contributing to the output."
  - [corpus] Studies cited observe increases in activation sparsity before increasing test loss, supporting the link between sparsity and generalization.
- Break condition: If sparsity does not plateau or increase before generalization, the link between sparsity and grokking is weakened.

### Mechanism 3
- Claim: Lower weight entropy and approximate local circuit complexity indicate a more robust and generalizable model.
- Mechanism: As training progresses, the weight distribution becomes more concentrated (lower entropy), and the model's output becomes less sensitive to small perturbations in weights (lower KL divergence). These indicate stable internal representations that generalize better.
- Core assumption: Generalization requires the model to settle into a region of parameter space where small changes do not drastically alter predictions.
- Evidence anchors:
  - [abstract] "absolute weight entropy... and approximate local circuit complexity... demonstrate a stronger correlation with grokking... compared to weight norms."
  - [section] Explains that "weight entropy measures the uncertainty or spread of the weight distribution, and a lower entropy is indicative of a compact and concentrated weight distribution, a sign of robust generalization."
  - [corpus] Entropy-based regularization methods are used to stabilize neural network training, supporting the role of entropy in generalization.
- Break condition: If weight entropy does not decrease or circuit complexity does not drop when grokking occurs, the mechanism would not hold.

## Foundational Learning

- Concept: Understanding of grokking phenomenon
  - Why needed here: The paper builds on the concept of grokking as delayed generalization after overfitting, and challenges existing explanations.
  - Quick check question: What is grokking, and how does it differ from typical overfitting?

- Concept: Familiarity with neural network training dynamics
  - Why needed here: The mechanisms rely on how weight updates, activation patterns, and entropy evolve during training.
  - Quick check question: How do weight decay and initialization affect the training trajectory?

- Concept: Basic information theory (entropy, KL divergence)
  - Why needed here: The proposed progress measures use entropy and KL divergence to quantify network complexity and robustness.
  - Quick check question: What does a lower entropy of a distribution signify about its concentration?

## Architecture Onboarding

- Component map: Input -> 3-layer MLP with ReLU activations -> Output classification
- Critical path: Data flows from input through layers, with gradients computed via cross-entropy loss. The key is monitoring the proposed measures alongside standard metrics (train/test accuracy, weight norm).
- Design tradeoffs: Using higher weight penalties or dropout can alter grokking behavior. Choosing the layer for circuit complexity and the threshold for activation sparsity are hyperparameters.
- Failure signatures: Grokking does not occur (test accuracy stays low), measures do not align with generalization, or adding dropout eliminates grokking.
- First 3 experiments:
  1. Replicate the MNIST experiment with the modified loss to see weight norms increase but grokking still occurs.
  2. Measure activation sparsity and weight entropy during training to confirm they correlate with generalization.
  3. Apply dropout and observe whether grokking disappears, as predicted.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the exact mathematical relationship between the three proposed progress measures (activation sparsity, absolute weight entropy, and approximate local circuit complexity) and the generalization performance of neural networks exhibiting grokking?
- Basis in paper: [explicit] The paper shows these measures correlate with grokking but does not establish their precise mathematical relationship to generalization
- Why unresolved: The paper demonstrates correlation but doesn't provide theoretical derivation of how these measures quantitatively relate to generalization bounds or performance metrics
- What evidence would resolve it: Formal mathematical proofs or extensive empirical studies showing the precise functional relationship between these measures and test accuracy/performance metrics across diverse architectures and datasets

### Open Question 2
- Question: Can the proposed progress measures be incorporated into the training objective as differentiable regularizers to control or accelerate grokking?
- Basis in paper: [explicit] The conclusion mentions this as a potential future direction but doesn't explore it
- Why unresolved: The paper only observes these measures during training but doesn't investigate whether optimizing them directly affects grokking dynamics
- What evidence would resolve it: Experiments showing that adding these measures as differentiable loss terms either accelerates generalization, changes grokking dynamics, or prevents overfitting

### Open Question 3
- Question: Why does adding dropout eliminate grokking in the experimental setup, and what does this reveal about the mechanism behind grokking?
- Basis in paper: [explicit] Appendix C shows dropout eliminates grokking but doesn't explain the underlying mechanism
- Why unresolved: The paper observes the effect but doesn't provide theoretical explanation for why dropout prevents the delayed generalization phenomenon
- What evidence would resolve it: Analysis of how dropout affects the evolution of the proposed progress measures during training, and comparison with theoretical models of grokking mechanisms

### Open Question 4
- Question: Are there fundamental differences in grokking dynamics between algorithmic tasks and real-world datasets beyond what can be explained by the proposed progress measures?
- Basis in paper: [inferred] The paper contrasts algorithmic and real-world grokking but doesn't systematically compare them
- Why unresolved: The paper focuses on real-world datasets and doesn't perform controlled comparisons with algorithmic tasks using the same measures
- What evidence would resolve it: Direct comparison of grokking on algorithmic tasks using the proposed measures versus traditional measures, identifying any systematic differences in dynamics or required conditions

## Limitations

- Limited task diversity: Experiments are primarily conducted on MNIST and IMDb datasets, which may not capture the full spectrum of real-world complexity.
- Mechanistic opacity: While the proposed measures correlate with grokking, the exact causal mechanisms linking them to generalization are not fully established.
- Hyperparameter sensitivity: The effectiveness of the new measures may depend on choices like the activation sparsity threshold and the fraction of weights perturbed for circuit complexity.

## Confidence

- **High confidence**: The claim that weight norms alone do not cause grokking is strongly supported by the experimental evidence showing grokking with increasing norms.
- **Medium confidence**: The assertion that activation sparsity and weight entropy correlate better with generalization than weight norms is plausible but requires further validation on diverse tasks.
- **Medium confidence**: The proposed progress measures are useful for understanding grokking dynamics, but their robustness across architectures and datasets needs more extensive testing.

## Next Checks

1. **Cross-task generalization**: Validate the proposed measures on a broader set of real-world datasets (e.g., CIFAR-10, language modeling tasks) to assess their robustness and generalizability.
2. **Architectural robustness**: Test the measures on different model architectures (e.g., CNNs, transformers) to determine if the findings hold beyond MLPs and LSTMs.
3. **Causal analysis**: Conduct ablation studies or intervention experiments (e.g., forcing specific sparsity or entropy levels) to establish causal links between the proposed measures and generalization performance.