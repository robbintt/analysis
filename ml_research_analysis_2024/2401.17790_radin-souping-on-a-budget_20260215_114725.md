---
ver: rpa2
title: 'RADIN: Souping on a Budget'
arxiv_id: '2401.17790'
source_url: https://arxiv.org/abs/2401.17790
tags: []
core_contribution: This paper proposes RADIN, a method to approximate the performance
  of model soups using averaged ensemble logits. The key insight is that the first-order
  Taylor expansion of the loss for model soups and ensemble logits are equivalent,
  allowing RADIN to rank soup candidates quickly without full inference.
---

# RADIN: Souping on a Budget

## Quick Facts
- arXiv ID: 2401.17790
- Source URL: https://arxiv.org/abs/2401.17790
- Authors: Thibaut Menes; Olivier Risser-Maroix
- Reference count: 0
- Primary result: RADIN achieves competitive model soup performance at lower computational cost using ensemble logit approximation

## Executive Summary
This paper introduces RADIN, a method for efficient model soup selection that leverages the equivalence between first-order Taylor expansions of model soups and averaged ensemble logits. The approach enables rapid candidate ranking without full inference, followed by selective full evaluation of top candidates. Experiments on CIFAR-10 and ImageNet demonstrate that RADIN can match or exceed greedy model soup performance while significantly reducing computational requirements, with improvements up to 4% at B=1 on ImageNet.

## Method Summary
RADIN employs a two-step approach to model soup selection. First, it approximates the performance of candidate soups using averaged ensemble logits, which are mathematically equivalent to model soups under first-order Taylor expansion of the loss function. This allows rapid ranking of numerous candidate subsets without full model soup inference. Second, the top-ranked candidates undergo full evaluation to select the optimal soup. The method uses Monte-Carlo sampling for candidate generation and incorporates a λ parameter to control prior strength favoring larger model subsets. This combination enables efficient exploration of the model soup space while maintaining approximation accuracy.

## Key Results
- RADIN achieves comparable or better performance than greedy model soups at significantly reduced computational cost
- On ImageNet with B=1, RADIN shows up to 4% improvement over baseline methods
- The method successfully scales to ensembles of up to 13 CLIP ViT-B/32 models
- Computational efficiency gains are particularly pronounced for smaller budgets (B=1, B=2)

## Why This Works (Mechanism)
The core insight is that model soups and averaged ensemble logits share identical first-order Taylor expansions of their loss functions. This mathematical equivalence means that for small perturbations around the initialization point, the two approaches produce indistinguishable gradients. By ranking candidates based on ensemble logits rather than full soup inference, RADIN exploits this approximation to dramatically reduce computational requirements while preserving selection quality.

## Foundational Learning
- **First-order Taylor expansion**: Approximates non-linear functions using linear terms around a point - needed to establish mathematical equivalence between soups and ensemble logits
- **Quick check**: Verify that f(x+h) ≈ f(x) + f'(x)·h for small h using simple functions

- **Model ensembling**: Combining multiple trained models' predictions - fundamental to understanding soup behavior
- **Quick check**: Compare ensemble accuracy vs individual model accuracy on a validation set

- **Greedy optimization**: Iterative selection of best immediate option - used as baseline for comparison
- **Quick check**: Implement greedy subset selection and verify it produces monotonic improvements

- **Monte-Carlo sampling**: Random selection method for exploring solution space - used for candidate generation
- **Quick check**: Verify that increasing samples improves candidate diversity

## Architecture Onboarding

**Component Map**: Candidate Generator -> Ensemble Logit Ranker -> Full Evaluation Selector -> Best Soup Output

**Critical Path**: The performance bottleneck occurs during the ensemble logit ranking phase, as this requires forward passes through all candidate subsets. The full evaluation phase is only applied to top candidates, making it less critical for overall efficiency.

**Design Tradeoffs**: RADIN trades approximation accuracy for computational efficiency. The first-order Taylor assumption may break down for highly non-linear loss landscapes or when models exhibit strong interaction effects. The Monte-Carlo sampling approach is simple but may miss optimal subsets compared to more sophisticated search algorithms.

**Failure Signatures**: Poor performance occurs when: 1) models in the ensemble have conflicting gradients causing the first-order approximation to fail, 2) the λ parameter is poorly tuned leading to suboptimal subset selection, or 3) the number of Monte-Carlo samples is insufficient to explore the solution space adequately.

**First Experiments**:
1. Verify Taylor expansion equivalence on a simple linear regression task
2. Compare ensemble logit rankings against full soup evaluations on a small subset
3. Test sensitivity to λ parameter by running with λ=0 and λ=1 on CIFAR-10

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of λ in RADIN's prior affect the trade-off between exploration of diverse subsets and exploitation of higher-performing subsets?
- Basis in paper: The paper mentions using a λ term to control the strength of the prior that favors solutions averaging the maximum number of models, and compares results with λ set to 0 and 1
- Why unresolved: The paper only explores two values of λ (0 and 1) and does not investigate how different λ values impact the exploration-exploitation balance or the overall performance of RADIN
- What evidence would resolve it: Experiments varying λ across a wider range and analyzing the resulting performance and diversity of selected subsets

### Open Question 2
- Question: Can more sophisticated subset selection algorithms (e.g., genetic algorithms, reinforcement learning) outperform the naive Monte-Carlo sampling used in RADIN's candidate generation?
- Basis in paper: The paper mentions that different methods are possible for finding the best subset of models, ranging from simple ones like Monte-Carlo sampling to more sophisticated ones, but only uses Monte-Carlo sampling in experiments
- Why unresolved: The paper does not compare RADIN's performance when using different subset selection algorithms, leaving the question of whether more advanced methods could improve results
- What evidence would resolve it: Experiments using alternative subset selection algorithms within RADIN's framework and comparing their performance to Monte-Carlo sampling

### Open Question 3
- Question: How does RADIN's performance scale with larger ensembles of models (e.g., N > 13) and more complex architectures (e.g., larger CLIP models)?
- Basis in paper: The paper only tests RADIN on ensembles of up to 13 CLIP ViT-B/32 models, and does not investigate its performance on larger ensembles or more complex architectures
- Why unresolved: The paper's experiments are limited to a specific model size and ensemble size, so it is unclear how RADIN would perform in more demanding scenarios
- What evidence would resolve it: Experiments applying RADIN to larger ensembles and more complex architectures, and analyzing its performance and scalability

## Limitations
- The first-order Taylor expansion approximation may break down for highly non-linear loss landscapes
- Experimental validation is limited to vision tasks, leaving uncertainty about generalization to other domains
- The method's performance depends on the quality of the ensemble logit approximation, which may degrade with model heterogeneity

## Confidence
- Mathematical equivalence proof: High
- Computational efficiency claims: High
- Performance improvement claims: Medium (task-specific)
- Scalability to larger ensembles: Low (not experimentally validated)

## Next Checks
1. Test RADIN's approximation quality across different neural network architectures beyond standard vision models
2. Evaluate the method's robustness to model heterogeneity (mixing architectures or training paradigms)
3. Benchmark against alternative soup approximation methods like feature-space averaging or low-rank approximations to establish relative advantages