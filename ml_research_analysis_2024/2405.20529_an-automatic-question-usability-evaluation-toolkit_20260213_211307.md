---
ver: rpa2
title: An Automatic Question Usability Evaluation Toolkit
arxiv_id: '2405.20529'
source_url: https://arxiv.org/abs/2405.20529
tags:
- questions
- evaluation
- mcqs
- question
- criteria
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study demonstrates that traditional linguistic quality metrics
  (perplexity, diversity, grammatical error, cognitive complexity, and answerability)
  fail to detect deeper flaws in multiple-choice questions (MCQs) and often misclassify
  flawed questions as high quality. The authors introduce SAQUET, an automated MCQ
  evaluation toolkit based on the Item-Writing Flaws (IWF) rubric, leveraging GPT-4,
  word embeddings, and NLP techniques to identify 19 types of question design flaws.
---

# An Automatic Question Usability Evaluation Toolkit

## Quick Facts
- arXiv ID: 2405.20529
- Source URL: https://arxiv.org/abs/2405.20529
- Reference count: 28
- 94% accuracy in detecting item-writing flaws across 271 MCQs

## Executive Summary
This study demonstrates that traditional linguistic quality metrics fail to detect deeper flaws in multiple-choice questions and often misclassify flawed questions as high quality. The authors introduce SAQUET, an automated MCQ evaluation toolkit based on the Item-Writing Flaws (IWF) rubric, leveraging GPT-4, word embeddings, and NLP techniques to identify 19 types of question design flaws. Tested across 271 MCQs from five domains, SAQUET achieved an overall accuracy of over 94% in detecting flaws and a 75.3% match rate with human evaluations for acceptability classification. The findings highlight the limitations of existing metrics and showcase SAQUET's potential to improve MCQ quality assessment.

## Method Summary
SAQUET is an open-source toolkit that combines rule-based techniques, NLP methods, and GPT-4 interventions to automatically detect 19 item-writing flaws in multiple-choice questions. The toolkit uses text-matching for clearly definable criteria, word embeddings and transformers for context-dependent criteria, and GPT-4 for nuanced analysis requiring understanding of question intent. The evaluation was conducted on 271 MCQs from five domains (Chemistry, Statistics, Computer Science, Humanities, Healthcare) annotated by human experts with IWF criteria.

## Key Results
- SAQUET achieved over 94% accuracy in detecting flaws identified by human evaluators
- Exact match ratio of 38% across all 19 criteria with Hamming Loss of 5.9%
- 75.3% match rate with human evaluations for acceptability classification
- Traditional metrics (perplexity, diversity, grammatical error, cognitive complexity, answerability) failed to correlate with pedagogical quality

## Why This Works (Mechanism)

### Mechanism 1
The combination of rule-based and GPT-4-based detection methods in SAQUET provides higher accuracy than either method alone. Rule-based techniques handle clearly definable criteria (e.g., "None of the Above", "All of the Above") with high precision, while GPT-4 handles context-dependent criteria (e.g., "Absolute Terms", "More Than One Correct") where nuanced understanding is required.

### Mechanism 2
SAQUET's domain-agnostic design allows it to evaluate MCQs across diverse educational fields without requiring domain-specific training data. By using general NLP techniques (word embeddings, NER, transformer models) rather than domain-specific training, SAQUET can apply the same evaluation criteria across Chemistry, Statistics, Computer Science, Humanities, and Healthcare.

### Mechanism 3
SAQUET achieves higher accuracy than traditional metrics by focusing on pedagogical quality rather than just readability or linguistic features. Traditional metrics focus on surface-level features, while SAQUET evaluates deeper question design flaws that affect educational effectiveness.

## Foundational Learning

- Concept: Item-Writing Flaws (IWF) rubric
  - Why needed here: Understanding the 19 criteria is essential for comprehending what SAQUET evaluates and how it classifies MCQs
  - Quick check question: Which IWF criterion addresses questions where the correct answer is noticeably longer than distractors?
  - Answer: Longest Option Correct

- Concept: Multi-label classification
  - Why needed here: SAQUET performs classification across 19 binary labels for each MCQ, requiring understanding of evaluation metrics like Hamming Loss and micro-averaged F1
  - Quick check question: What evaluation metric measures the average proportion of incorrect labels in multi-label classification?
  - Answer: Hamming Loss

- Concept: NLP evaluation metrics vs pedagogical assessment
  - Why needed here: Understanding why traditional metrics fail helps appreciate SAQUET's novel approach
  - Quick check question: Which traditional metric measures how well a language model predicts question text based on training data?
  - Answer: Perplexity

## Architecture Onboarding

- Component map: MCQ → Text parser → Rule engine (8 criteria) → NLP extraction layer (5 criteria) → GPT-4 integration layer (6 criteria) → Classification output
- Critical path: MCQ → Text parser → Rule engine (if applicable) → NLP layer (if applicable) → GPT-4 (if applicable) → Classification output
- Design tradeoffs:
  - Speed vs accuracy: GPT-4 calls provide higher accuracy but increase latency and cost
  - False positives vs false negatives: SAQUET prioritizes identifying more flaws (false positives) over missing flaws (false negatives)
  - Generalizability vs specificity: Domain-agnostic design sacrifices some domain-specific accuracy for broader applicability
- Failure signatures:
  - High false positive rate on "More Than One Correct" criterion indicates GPT-4 reliability issues
  - Consistent misclassification of True/False questions as Longest Option Correct suggests rule threshold problems
  - Poor performance on Computer Science MCQs with code syntax indicates NLP tool limitations with technical content
- First 3 experiments:
  1. Run SAQUET on a small set of hand-crafted MCQs with known IWFs to verify each detection criterion individually
  2. Compare SAQUET classifications with human evaluations on a subset to establish baseline accuracy
  3. Test SAQUET's performance across different domains using the provided dataset to identify domain-specific weaknesses

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability to larger, more diverse question banks remains untested
- Reliance on GPT-4 introduces potential variability due to API updates and cost constraints
- Evaluation focuses on detecting flaws rather than providing actionable remediation guidance

## Confidence
- High confidence: SAQUET's technical architecture and implementation are sound
- Medium confidence: Performance metrics are reliable for the specific dataset used
- Medium confidence: The claim that traditional metrics fail to detect pedagogical flaws is well-supported

## Next Checks
1. Test SAQUET on a larger, more diverse corpus of MCQs (minimum 1000 questions across additional domains) to assess scalability and generalizability
2. Conduct A/B testing comparing SAQUET evaluations with human expert assessments on a blind sample of questions to verify consistency
3. Implement a longitudinal study tracking MCQ quality improvements over time when using SAQUET for iterative question refinement