---
ver: rpa2
title: 'An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge
  Model is not a General Substitute for GPT-4'
arxiv_id: '2403.02839'
source_url: https://arxiv.org/abs/2403.02839
tags:
- evaluation
- judge
- fine-tuned
- figure
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether fine-tuned judge models can serve
  as general substitutes for GPT-4 in evaluating LLMs. The authors conduct an empirical
  study on four representative fine-tuned judge models (JudgeLM, PandaLM, Auto-J,
  Prometheus) and compare their performance to GPT-4 across various evaluation schemes
  and datasets.
---

# An Empirical Study of LLM-as-a-Judge for LLM Evaluation: Fine-tuned Judge Model is not a General Substitute for GPT-4

## Quick Facts
- **arXiv ID**: 2403.02839
- **Source URL**: https://arxiv.org/abs/2403.02839
- **Reference count**: 10
- **Primary result**: Fine-tuned judge models are severely limited compared to GPT-4 for LLM evaluation, showing poor generalization, bias toward superficial qualities, and inability to benefit from prompting strategies

## Executive Summary
This paper investigates whether fine-tuned judge models can serve as general substitutes for GPT-4 in evaluating LLMs. The authors conduct an empirical study on four representative fine-tuned judge models (JudgeLM, PandaLM, Auto-J, Prometheus) and compare their performance to GPT-4 across various evaluation schemes and datasets. Key findings show that while fine-tuned judge models achieve high performance on in-domain test sets, they exhibit significant limitations compared to GPT-4 in several dimensions: poor generalizability to unseen evaluation schemes, severe bias toward superficial qualities like verbosity and fluency, underperformance on aspect-specific evaluation tasks, and inability to benefit from prompting strategies like Chain-of-Thought or In-Context Learning.

## Method Summary
The study evaluates four fine-tuned judge models across multiple dimensions: cross-scheme generalization (testing models on evaluation schemes not seen during training), bias analysis using adversarial testsets, aspect-specific evaluation on fine-grained tasks, and prompt engineering experiments with Chain-of-Thought and In-Context Learning strategies. The authors use publicly released checkpoints of the fine-tuned models and conduct extensive comparisons with GPT-4 across standard evaluation datasets and custom test sets designed to reveal specific limitations.

## Key Results
- Fine-tuned judge models perform poorly on evaluation schemes not seen during training, while GPT-4 maintains consistent performance across different schemes
- Fine-tuned models are severely biased toward superficial qualities like verbosity and fluency, leading to preference for incorrect but well-formatted answers
- Fine-tuned models cannot benefit from prompting strategies like Chain-of-Thought or In-Context Learning, while GPT-4 shows significant improvements with these techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuned judge models become task-specific classifiers that are overfitted to their training data
- Mechanism: Fine-tuning process transforms general foundation models into specialized classifiers by fitting them to a fixed prompt template and evaluation scheme
- Core assumption: The model's general instruction-following ability is lost during fine-tuning because it's optimized for a single task
- Evidence anchors:
  - [abstract] "We also reveal that the fine-tuned judge model inherently operates as a task-specific classifier"
  - [section 3] "Combining all the limitations revealed in our experiments, we would like to claim that after the fine-tuning process on a single task, the judge model has degenerated into a task-specific classifier"
  - [corpus] Weak correlation - only 1/5 related papers mention task-specificity directly

### Mechanism 2
- Claim: Fine-tuned models are severely biased toward superficial qualities like verbosity and fluency
- Mechanism: Models learn to associate well-formatted, verbose responses with high scores during training, leading to preference for incorrect but well-presented answers
- Core assumption: Training data contains responses where surface-level quality correlates with correctness, causing the model to learn this spurious correlation
- Evidence anchors:
  - [section 2.2] "The fine-tuned judge model is biased towards superficial quality" and "they are severely biased toward superficial quality such as formality or verbosity"
  - [table 4] Fine-tuned models perform worse than random guess on LLMBar adversarial testsets
  - [corpus] No direct evidence in related papers about verbosity bias specifically

### Mechanism 3
- Claim: Fine-tuned models cannot benefit from prompting strategies like Chain-of-Thought or In-Context Learning
- Mechanism: The models have lost their general instruction-following ability and cannot adapt to new prompting strategies introduced during inference
- Core assumption: The fine-tuning process creates a rigid model architecture that only responds to the exact prompt template it was trained on
- Evidence anchors:
  - [section 2.4] "while the close-sourced proprietary models are improved by a large margin through both prompt engineering strategies, the fine-tuned judges hardly benefit from these strategies"
  - [table 7] Fine-tuned models show minimal or negative improvements with CoT and ICL
  - [corpus] No related papers discuss prompting strategy limitations in fine-tuned judge models

## Foundational Learning

- Concept: Evaluation scheme generalization
  - Why needed here: Understanding why models perform poorly on evaluation schemes not seen during training
  - Quick check question: What happens to a model's performance when evaluated on a scheme different from its training scheme?

- Concept: Bias in machine learning models
  - Why needed here: Recognizing how models can learn spurious correlations between surface features and quality
  - Quick check question: How can training data distribution lead to bias toward superficial qualities?

- Concept: Prompt engineering and in-context learning
  - Why needed here: Understanding the capabilities of foundation models that are lost during fine-tuning
  - Quick check question: What makes foundation models responsive to prompting strategies like CoT and ICL?

## Architecture Onboarding

- Component map: Foundation model (Vicuna, LLaMA2-chat, etc.) → Task-specific classification head → Fixed prompt template → Evaluation scheme
- Critical path: Training data → Fine-tuning process → Task-specific classifier → Evaluation limitations
- Design tradeoffs: General instruction-following ability vs. task-specific performance; flexibility vs. specialization
- Failure signatures: Poor performance on unseen evaluation schemes; bias toward superficial qualities; inability to benefit from prompting strategies
- First 3 experiments:
  1. Test fine-tuned model on evaluation schemes not seen during training to verify scheme-specific limitations
  2. Evaluate model on adversarial testsets (like LLMBar) to measure superficial quality bias
  3. Apply Chain-of-Thought prompting to fine-tuned model to test for prompt engineering benefits

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of fine-tuned judge models change when evaluated on multi-turn conversations versus single-turn evaluations?
- Basis in paper: [explicit] The paper mentions evaluating models on MT-Bench (Zheng et al., 2023), which is described as a multi-turn meta-evaluation dataset, and shows results in Table 2 comparing performance on MT-Bench versus single-turn schemes
- Why unresolved: While the paper provides accuracy and F1 scores for multi-turn evaluation, it does not deeply analyze the specific challenges or degradation patterns that occur when fine-tuned models handle conversational context across multiple turns
- What evidence would resolve it: A detailed breakdown of performance across different conversation lengths, analysis of how context retention affects scoring consistency, and comparison of single-turn versus multi-turn error patterns

### Open Question 2
- Question: Can fine-tuned judge models be improved through domain-specific adaptation after initial fine-tuning, and what are the limits of such adaptation?
- Basis in paper: [inferred] The paper discusses that fine-tuned judge models are task-specific classifiers that are overfitted to their training data, and mentions that increasing fine-tuning data could possibly mitigate some limitations, suggesting adaptation might be possible
- Why unresolved: The paper does not explore whether continued training on new domains or tasks can extend the capabilities of fine-tuned judges, or whether there are diminishing returns or catastrophic forgetting issues
- What evidence would resolve it: Experimental results showing performance gains (or lack thereof) when fine-tuned models are further adapted to new evaluation schemes or domains, including retention of original capabilities

### Open Question 3
- Question: What is the relationship between model size and the severity of limitations in fine-tuned judge models?
- Basis in paper: [explicit] The paper presents results for different model sizes (7B vs 13B parameters) in Table 2, showing variations in performance across models like JudgeLM-7B, PandaLM-7B, Auto-J-13B, and Prometheus-13B
- Why unresolved: While the paper compares different sized models, it does not systematically analyze whether larger models exhibit proportionally less bias, better generalization, or greater adaptability to prompting strategies
- What evidence would resolve it: A scaling analysis showing how each limitation (generalization, bias, aspect-specific evaluation, prompting benefits) changes as a function of model parameter count across a wider range of sizes

## Limitations

- Study based on a limited set of four fine-tuned models, which may not capture the full spectrum of fine-tuning approaches
- Bias analysis results could be influenced by the specific adversarial datasets used
- Prompt engineering experiments tested only two strategies (CoT and ICL), leaving open whether other prompting techniques might yield different results

## Confidence

- **High Confidence**: Fine-tuned models perform poorly on evaluation schemes not seen during training
- **Medium Confidence**: Fine-tuned models exhibit bias toward superficial qualities
- **Medium Confidence**: Fine-tuned models cannot benefit from prompt engineering strategies

## Next Checks

1. **Cross-domain Generalization Test**: Evaluate fine-tuned judge models on LLM evaluation tasks from entirely different domains (e.g., medical, legal, or technical writing) to assess whether the scheme-specific limitations persist across domain boundaries

2. **Fine-tuning Ablation Study**: Systematically vary fine-tuning parameters including training duration, data diversity, and prompt template complexity to identify which factors most strongly contribute to the loss of generalization ability and instruction-following capacity

3. **Alternative Prompting Strategy Exploration**: Test a broader range of prompting techniques including chain-of-density prompting, step-back prompting, and adaptive prompting strategies to determine if any approaches can successfully restore the general instruction-following capabilities in fine-tuned models