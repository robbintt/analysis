---
ver: rpa2
title: Unsupervised Morphological Tree Tokenizer
arxiv_id: '2406.15245'
source_url: https://arxiv.org/abs/2406.15245
tags:
- vocabulary
- composition
- language
- word
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an unsupervised morphological tree tokenizer
  that induces character-level structures of words to improve tokenization quality.
  The method uses a composition model with a MorphOverriding mechanism to handle morpheme
  indecomposability, trained with self-supervised objectives.
---

# Unsupervised Morphological Tree Tokenizer

## Quick Facts
- arXiv ID: 2406.15245
- Source URL: https://arxiv.org/abs/2406.15245
- Reference count: 40
- Outperforms BPE and WordPiece on morphological segmentation tasks (37.9% vs 19.5% accuracy on Morpho dataset)

## Executive Summary
This paper introduces TreeTok, an unsupervised morphological tree tokenizer that induces character-level structures of words to improve tokenization quality. The method uses a composition model with a MorphOverriding mechanism to handle morpheme indecomposability, trained with self-supervised objectives. TreeTok segments words through top-down vocabulary matching based on induced parse trees. Experiments show the approach outperforms BPE and WordPiece on morphological segmentation tasks and language modeling tasks while producing more compact vocabularies and shorter average token lengths.

## Method Summary
TreeTok consists of a composition model with MorphOverriding mechanism and a tree-based tokenization algorithm. The composition model uses a 4-layer Transformer with 128-dimensional embeddings and 4 attention heads, trained with two self-supervised objectives: intra-word auto-encoding loss and inter-word auto-regression loss. The MorphOverriding mechanism ensures morpheme indecomposability by mixing or overriding composition vectors with morpheme embeddings when subwords match the morpheme vocabulary. TreeTok constructs vocabulary through a tree-based BPE variant and prunes it to 30,000 tokens, then segments words using top-down matching on induced parse trees with post-processing merge optimization.

## Key Results
- TreeTok achieves 37.9% morphological segmentation accuracy on Morpho dataset vs 19.5% for BPE and 27.3% for WordPiece
- TreeTok produces better perplexity (107.26 vs 107.76) and BLEU scores in language modeling tasks
- TreeTok creates more compact vocabularies with shorter average token lengths compared to BPE and WordPiece

## Why This Works (Mechanism)

### Mechanism 1
MorphOverriding enables morpheme indecomposability by mixing or overriding composition vectors with morpheme embeddings when a subword matches the morpheme vocabulary. When a subword span (i,j) hits the external morpheme vocabulary V, the model computes its representation using both its component pairs and the corresponding morpheme embedding, allowing the representation to disentangle from component composition. Morphemes are the smallest meaning-bearing units and should not be decomposed further, unlike general subword units.

### Mechanism 2
Tree-based top-down vocabulary matching produces more morphologically aligned tokenization than bottom-up merging approaches. Instead of applying learned merge operations sequentially, TreeTok parses words into binary trees and matches subwords top-down, allowing pruning of intermediate "junk" tokens while preserving complete morphemes. Morphological structure can be effectively captured through binary tree representations that reflect constituent boundaries.

### Mechanism 3
Joint training with both auto-encoding and auto-regression losses enables the model to capture both intra-word compositionality and inter-word contextual dependencies. Auto-encoding loss predicts characters/morphemes from context within words while auto-regression loss predicts next words in sequences, providing complementary signals for structure induction. Morphological structure induction benefits from both local composition patterns and global contextual information.

## Foundational Learning

- **Character-level composition models and inside-outside algorithms**: Needed for TreeTok to induce character-level structures of words, requiring understanding of how to compose character representations into hierarchical structures. Quick check: How does the inside pass compute constituent representations differently from standard transformers?

- **Morphological segmentation and morpheme boundaries**: Needed for the tokenizer to recognize morpheme boundaries to avoid breaking them, requiring understanding of morphological theory and unsupervised segmentation methods. Quick check: What distinguishes morphemes from general subword units in terms of compositionality?

- **Self-supervised learning objectives for structure induction**: Needed for training objectives that capture both intra-word composition and inter-word context without labeled data. Quick check: How do auto-encoding and auto-regression objectives complement each other in structure induction?

## Architecture Onboarding

- **Component map**: Composition model (character-level transformer with MorphOverriding) → Parse tree generation → Top-down vocabulary matching → Post-processing merge optimization
- **Critical path**: Training composition model with dual objectives → Inducing parse trees → Tokenizing words via top-down matching → Vocabulary construction with pruning
- **Design tradeoffs**: MorphOverriding vs pure composition, tree-based vs linear matching, compact vs comprehensive vocabularies
- **Failure signatures**: Poor morpheme recall (MorphOverriding issues), excessive fragmentation (tree quality issues), suboptimal compression (vocabulary pruning issues)
- **First 3 experiments**:
  1. Train composition model with/without MorphOverriding on Morpho dataset and measure morpheme recall
  2. Compare tree-based top-down matching vs bottom-up BPE segmentation on compound word datasets
  3. Vary vocabulary size and pruning rate to optimize R'enyi efficiency and perplexity trade-off

## Open Questions the Paper Calls Out

### Open Question 1
What is the optimal vocabulary size for the heuristic morpheme vocabulary when using different morphological complexity levels across languages? The paper shows performance varies with vocabulary size and notes "the optimal size of an external vocabulary should be neither too large nor too small" (Figure 3), but doesn't provide specific optimal sizes for different languages. This remains unresolved because the paper only tests English and Chinese, which have very different morphological structures. Empirical studies testing different vocabulary sizes on morphologically diverse languages (e.g., Turkish, Finnish, Mandarin, English) would establish a relationship between morphological complexity and optimal vocabulary size.

### Open Question 2
How does the MorphOverriding mechanism's effectiveness vary across different types of morphological phenomena (e.g., compounding vs inflection vs derivation)? The ablation study shows "Removing MorphOverriding from the model results in a significant decrease of around 50% in performance on the decompounding task" but doesn't analyze other morphological phenomena separately. This remains unresolved because the paper combines all morphological phenomena in evaluation but doesn't quantify how MorphOverriding specifically helps with different types of morphology. Detailed analysis breaking down performance by morphological phenomenon type with and without MorphOverriding would quantify its differential impact.

### Open Question 3
What is the theoretical relationship between the compactness of induced tree structures and downstream task performance? The paper shows TreeTok produces "more compact vocabularies and shorter average token lengths" and correlates this with better perplexity and BLEU scores, but doesn't establish a formal relationship. This remains unresolved because while correlations are shown empirically, the paper doesn't provide theoretical justification for why structural compactness should improve performance or quantify this relationship. Formal analysis linking structural complexity measures to downstream task metrics would resolve this.

## Limitations

- Model architecture details lack precise implementation specifications for the pruned deep inside-outside encoder and MorphOverriding mechanism integration
- Hyperparameter sensitivity is not well-characterized, particularly for vocabulary construction parameters like threshold values and pruning rates
- Evaluation dataset scope is limited to English morphological segmentation tasks, with unproven generalization to morphologically rich languages

## Confidence

- **High Confidence**: Claims about TreeTok's superior performance compared to BPE and WordPiece on the specific evaluation datasets (37.9% vs 19.5% accuracy, perplexity 107.26 vs 107.76) are well-supported by experimental results
- **Medium Confidence**: Claims about MorphOverriding mechanism's effectiveness in preserving morpheme indecomposability are supported by comparative results but lack detailed ablation studies isolating this mechanism's contribution
- **Low Confidence**: Claims about the method's generalization to morphologically diverse languages beyond English are not adequately supported by the current experimental scope

## Next Checks

1. **Ablation Study on MorphOverriding**: Train composition models with and without MorphOverriding on Morpho dataset, measuring morpheme recall rates and segmentation accuracy to isolate the mechanism's contribution. This will validate whether the claimed indecomposability preservation is truly due to MorphOverriding.

2. **Cross-Linguistic Evaluation**: Apply TreeTok to morphologically rich languages (e.g., Turkish, Finnish, Arabic) using publicly available morphological segmentation datasets to assess generalization beyond English. This will test the method's robustness across different morphological typologies.

3. **Vocabulary Construction Sensitivity Analysis**: Systematically vary vocabulary construction parameters (BPE merge operations, pruning thresholds, morpheme vocabulary size) and measure impacts on segmentation quality, R'enyi efficiency, and perplexity to understand hyperparameter sensitivity and identify optimal configurations.