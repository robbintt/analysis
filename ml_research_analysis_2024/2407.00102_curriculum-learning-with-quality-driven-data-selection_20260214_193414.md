---
ver: rpa2
title: Curriculum Learning with Quality-Driven Data Selection
arxiv_id: '2407.00102'
source_url: https://arxiv.org/abs/2407.00102
tags:
- data
- should
- instruction
- learning
- quality
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of selecting high-quality multimodal
  instruction data for training large language models with vision capabilities. The
  authors propose a novel data selection methodology that leverages both image-text
  correlation (using CLIP scores) and model perplexity (measured through loss) to
  create a two-dimensional representation space for data quality assessment.
---

# Curriculum Learning with Quality-Driven Data Selection

## Quick Facts
- arXiv ID: 2407.00102
- Source URL: https://arxiv.org/abs/2407.00102
- Reference count: 40
- Primary result: 5% dataset selection outperforms full dataset training

## Executive Summary
This paper addresses the challenge of selecting high-quality multimodal instruction data for training large language models with vision capabilities. The authors propose a novel data selection methodology that leverages both image-text correlation (using CLIP scores) and model perplexity (measured through loss) to create a two-dimensional representation space for data quality assessment. By analyzing this space, they identify that different task types produce distinct data quality distributions and that higher-quality data clusters in specific regions of this space.

The paper introduces a curriculum learning strategy that progressively trains models on increasingly high-quality subsets of data, starting from simpler examples and advancing to more complex ones. Experiments on the LLaVA-v1.5 architecture demonstrate that using only 5% of the original dataset (approximately 7,000 samples) through their quality-driven selection method outperforms models trained on the full dataset, achieving state-of-the-art results across five benchmarks including VQA-v2, GQA, VisWiz, ScienceQA-IMG, and TextVQA.

## Method Summary
The methodology centers on a two-dimensional quality assessment framework that combines CLIP-based image-text correlation scores with model perplexity measurements to evaluate instruction data quality. The authors analyze this quality space to identify patterns in data distribution across different task types, discovering that high-quality data clusters in specific regions. They then implement a curriculum learning approach that progressively trains models starting with lower-complexity, higher-quality data and advances to more challenging examples. The selection process identifies that approximately 5% of the original dataset represents the highest quality instruction pairs, which when used exclusively for training, yields superior performance compared to using the complete dataset.

## Key Results
- 5% dataset selection (approximately 7,000 samples) outperforms full dataset training
- State-of-the-art results achieved across five benchmarks: VQA-v2, GQA, VisWiz, ScienceQA-IMG, and TextVQA
- 4.7 point improvement on GQA and 2.0 point improvement on VQA-v2 in LoRA fine-tuning setting
- Quality-driven selection method shows particular effectiveness in low-resource training scenarios

## Why This Works (Mechanism)
The effectiveness stems from the quality-driven selection mechanism that identifies and prioritizes instruction data with optimal image-text alignment and appropriate complexity levels. By using CLIP scores to measure semantic correlation between images and text, and perplexity to assess instruction difficulty, the method creates a quality space where high-value data points can be identified and progressively incorporated into training. This curriculum approach allows models to first master simpler, well-aligned examples before tackling more complex instruction-response pairs, leading to more efficient learning and better generalization.

## Foundational Learning
- **Multimodal instruction tuning**: Why needed - to align vision and language representations in large models; Quick check - verify cross-modal attention mechanisms are properly activated
- **CLIP-based correlation metrics**: Why needed - to quantitatively assess image-text semantic alignment; Quick check - validate CLIP score distributions across different data quality levels
- **Curriculum learning progression**: Why needed - to optimize learning trajectories from simple to complex examples; Quick check - monitor loss curves during progressive training stages
- **Perplexity-based quality assessment**: Why needed - to measure instruction complexity and response predictability; Quick check - compare perplexity distributions between selected and rejected data
- **LoRA fine-tuning methodology**: Why needed - to efficiently adapt large models to new instruction data; Quick check - verify adapter weight updates during training
- **Multimodal benchmark evaluation**: Why needed - to assess vision-language model performance across diverse tasks; Quick check - ensure consistent evaluation protocols across all benchmarks

## Architecture Onboarding

**Component Map:**
Data Quality Assessment -> Quality Space Analysis -> Curriculum Learning Progression -> Model Training

**Critical Path:**
Data collection → CLIP score computation → Perplexity measurement → Quality space mapping → Subset selection → Curriculum-based training → Evaluation

**Design Tradeoffs:**
- Quality vs. quantity: Prioritizing high-quality data reduces dataset size but improves performance
- Complexity vs. accessibility: Balancing instruction difficulty for optimal learning progression
- Computational cost vs. selection accuracy: Trade-off between thorough quality assessment and training efficiency
- Generalization vs. specialization: Risk of overfitting to selected quality regions

**Failure Signatures:**
- Poor performance despite quality selection indicates inadequate quality metrics
- Training instability suggests inappropriate curriculum progression
- Benchmark results inconsistent with quality space predictions reveal metric limitations
- No improvement over full dataset training suggests selection criteria are too restrictive

**First Experiments:**
1. Compare model performance using different quality thresholds (10%, 5%, 1% of data)
2. Evaluate ablation study: CLIP scores only vs. perplexity only vs. combined quality assessment
3. Test different curriculum progression strategies: random vs. quality-based ordering

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to LLaVA-v1.5 architecture, limiting generalizability to other multimodal LLM designs
- CLIP-based correlation metrics may not fully capture semantic alignment quality and could be sensitive to specific model versions
- Perplexity-based quality assessment assumes lower perplexity correlates with higher data quality, which may not hold for all instruction types

## Confidence

**High confidence:**
- The core methodology of using two-dimensional quality space for data selection and the empirical results showing performance improvements

**Medium confidence:**
- The generalizability of results across different architectures and datasets
- The interpretation that higher-quality data clusters in specific regions of the quality space

## Next Checks
1. Replicate the experiments using different multimodal LLM architectures (e.g., LLaVA-NeXT, Qwen-VL) to assess generalizability
2. Test the quality-driven selection method on multiple dataset splits and alternative multimodal instruction datasets to verify robustness
3. Conduct ablation studies comparing different curriculum progression strategies (e.g., random vs. quality-based ordering, varying the quality thresholds)