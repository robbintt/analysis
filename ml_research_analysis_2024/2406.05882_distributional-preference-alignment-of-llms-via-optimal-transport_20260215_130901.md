---
ver: rpa2
title: Distributional Preference Alignment of LLMs via Optimal Transport
arxiv_id: '2406.05882'
source_url: https://arxiv.org/abs/2406.05882
tags:
- alignment
- preference
- unpaired
- optimal
- paired
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel distributional preference alignment
  method for large language models (LLMs) using optimal transport. The key idea is
  to align LLMs on unpaired preference data by making the reward distribution of positive
  samples stochastically dominant over negative samples.
---

# Distributional Preference Alignment of LLMs via Optimal Transport

## Quick Facts
- arXiv ID: 2406.05882
- Source URL: https://arxiv.org/abs/2406.05882
- Reference count: 40
- Key outcome: AOT achieves state-of-the-art alignment results on 7B models, with Merlinite-7B scoring 31.3% on AlpacaEval

## Executive Summary
This paper introduces AOT (Alignment via Optimal Transport), a novel method for distributional preference alignment of LLMs that enforces first-order stochastic dominance of positive sample rewards over negative samples. The approach casts alignment as a one-dimensional optimal transport problem with a smooth, convex cost, enabling efficient computation via sorting on empirical measures. The method is evaluated across multiple alignment datasets and achieves state-of-the-art results in the 7B model family, particularly excelling on AlpacaEval with a score of 31.3%.

## Method Summary
AOT aligns LLMs by making the reward distribution of positive samples stochastically dominant over negative samples through a convex relaxation of first-order stochastic dominance. The method solves an optimal transport problem with a smooth, convex cost that penalizes violations of this dominance relationship. Due to the one-dimensional nature of the problem, the OT cost has a closed-form solution via sorting empirical measures, enabling efficient computation and gradient-based optimization. The approach is evaluated on various alignment datasets including UltraFeedback, PKU BeaverTails, and HelpSteer, using models like Merlinite-7B, Zephyr-7B, and OpenCode-7B.

## Key Results
- Merlinite-7B aligned with AOT achieves 31.3% on AlpacaEval, outperforming DPO, KTO, and IPO
- AOT demonstrates state-of-the-art performance in the 7B family of models across Open LLM Benchmarks
- The method shows strong results on multiple alignment datasets including UltraFeedback and PKU BeaverTails

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AOT achieves distributional preference alignment by enforcing first-order stochastic dominance of the reward distribution of positive samples over negative samples.
- Mechanism: The method casts the alignment problem as a one-dimensional optimal transport (OT) problem with a smooth and convex cost function. This OT formulation relaxes the hard stochastic dominance constraint into a differentiable objective that can be optimized via sorting on empirical measures.
- Core assumption: The reward distributions are one-dimensional and can be ordered via stochastic dominance; the cost function h is convex and Lipschitz.
- Evidence anchors:
  - [abstract] "AOT aligns LLMs on unpaired preference data by making the reward distribution of the positive samples stochastically dominant in the first order on the distribution of negative samples."
  - [section 3] "We introduce a convex relaxation of this first-order stochastic dominance and cast it as an optimal transport problem with a smooth and convex cost."
  - [corpus] Weak evidence: corpus lacks direct discussion of first-order stochastic dominance in LLM alignment; most related works focus on pointwise or pairwise preferences.
- Break condition: If the reward distributions are not one-dimensional or if the cost function is not convex/Lipschitz, the OT formulation breaks down and the closed-form sorting solution no longer applies.

### Mechanism 2
- Claim: The AOT objective can be optimized efficiently via sorting on empirical measures due to the one-dimensional nature of the problem.
- Mechanism: For fixed θ, the optimal transport cost OTh(μUθ, μVθ) has a closed-form solution given by sorting the samples and pairing them in order. This enables efficient computation of gradients via sorting (hard or soft) rather than solving the full OT problem.
- Core assumption: The reward distributions are supported on bounded intervals and have densities (atomless).
- Evidence anchors:
  - [section 3] "Thanks to the one-dimensional nature of the resulting optimal transport problem and the convexity of the cost, it has a closed-form solution via sorting on empirical measures."
  - [section 6] "The AOT cost can be cast as a one-dimensional optimal transport problem with a smooth and convex cost that penalizes violations of the dominance of the chosen on rejected marginals."
  - [corpus] No direct evidence in corpus about sorting-based OT solutions for distributional alignment; related works focus on robust reward modeling or diffusion models.
- Break condition: If the reward distributions are not one-dimensional or if they have atoms (discrete support), the sorting-based solution no longer applies and the OT problem must be solved numerically.

### Mechanism 3
- Claim: AOT enjoys parametric statistical rates (n^{-1/2}) due to the smoothness and convexity of the OT cost.
- Mechanism: The dual formulation of the OT problem reveals it as a min-max game between the policy and a c-concave potential function. The Rademacher complexity of the hypothesis class can be bounded using the Lipschitz and convexity properties of the cost, leading to parametric convergence rates.
- Core assumption: The reward function is bounded, the policy class is differentiable, and the cost function h is convex and Lipschitz.
- Evidence anchors:
  - [section 4] "We analyze the sample complexity of AOT by considering the dual of the OT problem and show that it converges at the parametric rate."
  - [section 4] "Under Assumptions 1, 2 and 3 we have: E OTh((r ◦ πˆθn)♯µ+, (r ◦ πˆθn)♯µ−) − OTh((r ◦ πθ∗)♯µ+, (r ◦ πθ∗)♯µ−) ≲ n− 1/2."
  - [corpus] No direct evidence in corpus about parametric rates for distributional alignment; related works focus on robust reward modeling or diffusion models.
- Break condition: If the cost function is not smooth or convex, or if the policy class is too complex, the statistical rates degrade and the convergence may be slower than parametric.

## Foundational Learning

- Concept: First-order stochastic dominance
  - Why needed here: AOT relies on enforcing that the reward distribution of positive samples stochastically dominates that of negative samples. This requires understanding the definition and properties of stochastic dominance.
  - Quick check question: Given two random variables X and Y, what condition must hold for X to stochastically dominate Y in the first order?

- Concept: Optimal transport theory
  - Why needed here: AOT casts the alignment problem as a one-dimensional optimal transport problem. Understanding the dual formulation, c-concave functions, and the properties of OT costs is crucial for analyzing the method.
  - Quick check question: What is the dual representation of the optimal transport problem with a convex cost function h?

- Concept: Rademacher complexity and statistical learning theory
  - Why needed here: The analysis of AOT's sample complexity relies on bounding the Rademacher complexity of the hypothesis class. Understanding how to bound covering numbers and use Dudley's entropy integral is essential.
  - Quick check question: How does the Lipschitz property of the cost function h affect the covering number of the class of c-concave functions?

## Architecture Onboarding

- Component map: LLM policy πθ -> Reference policy πref -> Reward function r -> Sorting algorithm -> Loss function h -> Optimizer
- Critical path:
  1. Compute rewards for positive and negative samples under current policy
  2. Sort rewards to obtain order statistics
  3. Compute AOT loss using sorted rewards and loss function h
  4. Backpropagate gradients through sorting (hard or soft)
  5. Update policy parameters using optimizer

- Design tradeoffs:
  - Hard vs. soft sorting: Hard sorting is more efficient but has discontinuous gradients; soft sorting is differentiable but introduces regularization bias
  - Choice of loss function h: Different losses (hinge, logistic, squared) trade off between sparsity of violations and gradient signal
  - Batch size: Larger batches provide better estimates of order statistics but increase memory usage

- Failure signatures:
  - Training instability or divergence: May indicate issues with sorting implementation or choice of loss function
  - Poor alignment performance: Could be due to insufficient batch size, inappropriate loss function, or mismatch between paired and unpaired settings
  - Slow convergence: May suggest the need for larger batch sizes or different optimization hyperparameters

- First 3 experiments:
  1. Implement AOT with hard sorting and hinge loss on a small paired dataset to verify basic functionality
  2. Compare AOT with DPO on the same dataset to assess alignment performance
  3. Experiment with different loss functions (hinge, logistic, squared) to understand their impact on alignment quality and convergence

## Open Questions the Paper Calls Out
- How does AOT perform on larger models beyond the 7B family?
- What is the impact of different loss functions (e.g., logistic vs. hinge) on AOT's performance in various domains?
- How does AOT's performance scale with the size of the preference dataset?

## Limitations
- Evaluation focuses primarily on 7B models, limiting generalizability to larger model families
- Computational efficiency claims lack detailed analysis of memory and runtime scaling for larger models
- Performance on truly open-ended generation tasks remains unclear, as evaluations focus on multiple-choice benchmarks

## Confidence
- **High Confidence**: The core mathematical formulation of AOT and its closed-form sorting solution are well-established and rigorously proven
- **Medium Confidence**: The sample complexity analysis and statistical rates are theoretically sound but rely on strong assumptions about reward functions and policy classes
- **Medium Confidence**: The empirical results show strong performance on specific benchmarks but the evaluation protocol may favor distributional alignment methods

## Next Checks
1. Evaluate AOT on diverse model families (Llama, Mistral) and reward models (GPT-3.5, Claude) to assess generalizability across different LLM architectures and reward functions
2. Test AOT on long-form generation tasks (story completion, code generation) beyond the current multiple-choice and short dialogue focus to validate real-world applicability
3. Systematically vary the loss function h, sorting mechanism (hard vs soft), and batch sizes to quantify their impact on alignment performance and identify potential failure modes