---
ver: rpa2
title: 'A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms'
arxiv_id: '2409.16694'
source_url: https://arxiv.org/abs/2409.16694
tags:
- quantization
- arxiv
- preprint
- llms
- zhang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey systematically examines low-bit quantization methods
  for large language models (LLMs), covering fundamental concepts, system implementations,
  and algorithmic strategies. The paper introduces low-bit number formats (e.g., floating-point,
  integer, binarized), quantization granularity (tensor-wise, token-wise, channel-wise,
  etc.), and dynamic vs.
---

# A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms

## Quick Facts
- arXiv ID: 2409.16694
- Source URL: https://arxiv.org/abs/2409.16694
- Reference count: 23
- Systematic examination of low-bit quantization methods for large language models covering fundamentals, systems, and algorithms

## Executive Summary
This survey provides a comprehensive overview of low-bit quantization techniques for large language models (LLMs), examining the fundamental concepts, system implementations, and algorithmic strategies. The paper systematically categorizes quantization approaches based on number formats, granularity levels, and dynamic versus static methods. It reviews both quantization-aware training (QAT) and post-training quantization (PTQ) frameworks, with particular emphasis on PTQ techniques including equivalent transformations, compensation methods, and mixed-precision strategies. The survey also explores emerging quantization forms and their applications to multimodal and mixture-of-experts architectures.

## Method Summary
The survey synthesizes existing literature on low-bit quantization methods for LLMs without presenting novel experimental results. It organizes the field into three main categories: basics (number formats and granularity), systems (hardware frameworks and platforms), and algorithms (QAT, PTQ techniques, and advanced strategies). The methodology involves comprehensive literature review and systematic categorization of approaches, with analysis focused on describing established methods rather than validating their relative effectiveness through new experiments.

## Key Results
- Equivalent transformation and compensation methods effectively mitigate quantization errors in low-bit settings
- Mixed-precision approaches achieve favorable balance between accuracy and compression efficiency
- Advanced quantization techniques show promise for multimodal and mixture-of-experts architectures

## Why This Works (Mechanism)
The survey's comprehensive approach works by systematically organizing the fragmented landscape of low-bit quantization research into coherent categories and frameworks. By distinguishing between fundamental concepts (number formats, granularity), system implementations (hardware support), and algorithmic strategies (QAT vs PTQ), the paper creates a structured taxonomy that enables practitioners to navigate the field effectively. The emphasis on equivalent transformation techniques (shifting, scaling, rotation) and compensation methods addresses the core challenge of quantization error management, while the exploration of mixed-precision and combined compression strategies demonstrates how different approaches can be synergistically combined to optimize the accuracy-efficiency trade-off.

## Foundational Learning
- **Low-bit number formats** (floating-point, integer, binarized) - Why needed: Determines the precision and range constraints for quantization; Quick check: Verify that chosen format supports required dynamic range for activation distributions
- **Quantization granularity** (tensor-wise, token-wise, channel-wise) - Why needed: Affects the trade-off between quantization error and computational efficiency; Quick check: Test different granularities on representative layers to measure accuracy impact
- **Dynamic vs. static quantization** - Why needed: Determines whether quantization parameters adapt during inference; Quick check: Compare inference latency and accuracy between dynamic and static approaches on target hardware
- **Quantization-aware training (QAT) vs. post-training quantization (PTQ)** - Why needed: Represents fundamental trade-off between training time and accuracy preservation; Quick check: Measure accuracy drop between QAT and PTQ for identical bit-widths
- **Outlier management techniques** - Why needed: Critical for preventing catastrophic accuracy loss in low-bit quantization; Quick check: Identify outlier activation distributions and test various compensation strategies
- **Mixed-precision quantization** - Why needed: Enables optimization by allocating higher precision to sensitive layers; Quick check: Profile layer sensitivity to determine optimal precision allocation

## Architecture Onboarding

**Component Map:** Input -> Pre-processing -> Quantization Module -> Computation Layer -> De-quantization -> Output

**Critical Path:** Input tensor → Quantization (granularity selection) → Low-bit computation → De-quantization → Output activation

**Design Tradeoffs:** Precision vs. efficiency (lower bits reduce memory/computation but increase quantization error), granularity vs. adaptability (finer granularity reduces error but increases overhead), dynamic vs. static parameters (adaptability vs. runtime cost)

**Failure Signatures:** Accuracy degradation concentrated in outlier activations, layer-specific performance collapse, hardware-specific quantization artifacts, loss of model calibration

**First Experiments:**
1. Baseline accuracy comparison between full-precision and 8-bit quantization on a small transformer layer
2. Granularity sensitivity analysis using tensor-wise vs. channel-wise quantization on representative attention blocks
3. Outlier detection and compensation method comparison on activation distributions from multiple LLM layers

## Open Questions the Paper Calls Out
None

## Limitations
- Claims about method effectiveness primarily based on cited works rather than direct experimental validation
- Many algorithmic strategies lack systematic comparison under unified benchmarks
- Hardware-specific optimizations may not translate across different platforms

## Confidence
- Methodological coverage: Medium
- Describing established quantization frameworks: High
- Claims about relative effectiveness: Low

## Next Checks
1. Conduct controlled experiments comparing tensor-wise, token-wise, and channel-wise quantization granularities on identical LLM architectures using standardized benchmarks
2. Systematically evaluate equivalent transformation techniques (shifting, scaling, rotation) against compensation methods for outlier management in low-bit settings
3. Benchmark mixed-precision quantization approaches against pure low-bit methods across multiple hardware platforms to verify claimed accuracy-efficiency trade-offs