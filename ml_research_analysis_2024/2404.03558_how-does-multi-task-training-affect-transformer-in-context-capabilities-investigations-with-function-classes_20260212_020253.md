---
ver: rpa2
title: How does Multi-Task Training Affect Transformer In-Context Capabilities? Investigations
  with Function Classes
arxiv_id: '2404.03558'
source_url: https://arxiv.org/abs/2404.03558
tags:
- learning
- curriculum
- tasks
- function
- mixed
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores how multi-task training affects transformer
  in-context learning (ICL) capabilities. The authors investigate curriculum learning
  strategies for training transformers on multiple function classes to improve data
  efficiency and convergence.
---

# How does Multi-Task Training Affect Transformer In-Context Capabilities? Investigations with Function Classes

## Quick Facts
- arXiv ID: 2404.03558
- Source URL: https://arxiv.org/abs/2404.03558
- Authors: Harmon Bhasin; Timothy Ossowski; Yiqiao Zhong; Junjie Hu
- Reference count: 10
- Primary result: Multi-task training with mixed curriculum improves transformer ICL, achieving comparable performance with 1/9 of training data

## Executive Summary
This paper investigates how multi-task training strategies affect transformer in-context learning (ICL) capabilities through experiments on function classes. The authors propose three curriculum learning approaches - sequential, mixed, and random - for training transformers on multiple function classes simultaneously. Their experiments demonstrate that the mixed curriculum significantly outperforms single-task training, achieving comparable performance while using only 1/9 of the training data. The approach also enables convergence on difficult function classes where single-task models fail, suggesting that multi-task training can enhance both efficiency and capability of transformer ICL.

## Method Summary
The authors design an experimental framework using synthetic function classes (linear, quadratic, cubic) to study ICL under different curriculum strategies. They implement three training curricula: sequential (one function class at a time), mixed (all function classes simultaneously), and random (shuffled order). Models are trained on varying amounts of data and evaluated on their ability to generalize to unseen function instances. Attention analysis is used to identify specific heads responsible for ICL, with particular focus on "retrospective" heads that consistently appear across tasks in the mixed curriculum model. The experimental setup allows precise control over training conditions and clear measurement of ICL capabilities.

## Key Results
- Mixed curriculum achieves comparable performance to single-task training using only 1/9 of the training data
- Mixed curriculum enables convergence on difficult function classes where single-task models fail
- Specific "retrospective" attention heads remain consistent across tasks in mixed curriculum models, suggesting a stable ICL mechanism

## Why This Works (Mechanism)
The paper demonstrates that multi-task training with a mixed curriculum creates a more efficient learning process by exposing the model to diverse function classes simultaneously. This exposure appears to develop more robust and generalizable ICL capabilities compared to sequential or single-task approaches. The attention analysis reveals that retrospective heads - attention heads that look back at previous context - play a crucial role in the ICL mechanism, and these heads remain stable across different tasks in the mixed curriculum model.

## Foundational Learning
- In-context learning: The ability of transformers to learn from input context without parameter updates during inference
  * Why needed: Core concept being studied
  * Quick check: Can model generalize to new instances within trained function classes
- Curriculum learning: Training strategy that controls the order and mixture of tasks presented during training
  * Why needed: Three different curricula are compared for effectiveness
  * Quick check: Compare sequential vs mixed vs random training orders
- Function classes: Mathematical function families (linear, quadratic, cubic) used as synthetic tasks
  * Why needed: Controlled experimental setup for studying ICL
  * Quick check: Can model extrapolate beyond training range within function class
- Attention mechanisms: How transformers process input through self-attention
  * Why needed: Used to analyze which components enable ICL
  * Quick check: Identify retrospective heads that remain consistent across tasks
- Data efficiency: Ratio of performance to training data required
  * Why needed: Mixed curriculum claims 1/9 data reduction
  * Quick check: Performance curves across different training set sizes
- Model convergence: Whether training successfully completes and generalizes
  * Why needed: Mixed curriculum enables convergence on difficult tasks
  * Quick check: Compare success rates across different curricula

## Architecture Onboarding

Component Map:
Input Functions -> Transformer Encoder -> Attention Heads -> ICL Output

Critical Path:
Input sequence → Self-attention layers → Retrospective heads activation → Context processing → Output prediction

Design Tradeoffs:
- Single-task: Simpler training, potentially better for individual tasks but less efficient
- Sequential curriculum: Gradual learning but may not capture task relationships
- Mixed curriculum: More complex training but better efficiency and convergence

Failure Signatures:
- Single-task models failing to converge on difficult function classes
- Random curriculum showing inconsistent performance
- Models without retrospective heads showing poor ICL capability

First Experiments:
1. Train single-task models on each function class separately
2. Implement mixed curriculum training across all function classes
3. Compare attention patterns between successful and failed models

## Open Questions the Paper Calls Out
None

## Limitations
- Results are based on simple mathematical functions and may not generalize to complex real-world tasks
- The study does not establish causal mechanisms for why mixed curriculum specifically enables better ICL performance
- Computational efficiency claims may be dependent on specific experimental conditions and may not translate to larger-scale architectures

## Confidence

| Claim | Confidence |
|-------|------------|
| Multi-task training with mixed curriculum improves ICL efficiency and capability | High |
| Mixed curriculum enables convergence on difficult function classes | High |
| Retrospective heads are responsible for ICL mechanism | Medium |
| 1/9 data reduction translates to practical benefits | Low |

## Next Checks
1. Test curriculum strategies on more complex function classes or non-mathematical tasks to assess generalizability
2. Conduct ablation studies that systematically disable or modify retrospective heads to establish causal relationships
3. Evaluate computational efficiency gains on larger-scale transformer architectures and longer sequence lengths