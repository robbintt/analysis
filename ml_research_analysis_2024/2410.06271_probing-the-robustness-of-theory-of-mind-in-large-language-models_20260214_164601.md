---
ver: rpa2
title: Probing the Robustness of Theory of Mind in Large Language Models
arxiv_id: '2410.06271'
source_url: https://arxiv.org/abs/2410.06271
tags:
- accuracy
- tasks
- dataset
- llms
- complexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel Theory of Mind (ToM) benchmarking
  dataset with 68 tasks across 10 complexity classes, designed to probe LLM robustness
  in social reasoning. The authors evaluate four state-of-the-art open-source LLMs
  on this dataset and Kosinski's baseline.
---

# Probing the Robustness of Theory of Mind in Large Language Models

## Quick Facts
- **arXiv ID**: 2410.06271
- **Source URL**: https://arxiv.org/abs/2410.06271
- **Reference count**: 21
- **Key outcome**: Models achieve >50% turn accuracy but only 0-4.4% goal accuracy, suggesting limited ToM capabilities

## Executive Summary
This work introduces a novel Theory of Mind (ToM) benchmarking dataset with 68 tasks across 10 complexity classes, designed to probe LLM robustness in social reasoning. The authors evaluate four state-of-the-art open-source LLMs on this dataset and Kosinski's baseline. While models achieve over 50% turn accuracy, goal accuracy remains extremely low (0–4.4%), indicating limited ToM capabilities. The "automatic change knowledge" class proves most challenging, with models struggling to track agents' assumptions about unobserved environmental changes. Performance drops are also observed for preposition replacements and transparent container scenarios. The results suggest current LLMs lack robust ToM, though they learn partial linguistic patterns. The dataset enables future research into ToM challenges and potential improvements through methods like Chain-of-Thought prompting.

## Method Summary
The authors construct a novel Theory of Mind benchmark with 68 tasks spanning 10 complexity classes, ranging from simple false-belief scenarios to multi-agent belief tracking. Four open-source LLMs are evaluated using zero-shot prompting with multiple-choice questions. Performance is measured through turn accuracy (tracking state changes) and goal accuracy (inferring agent intentions). The dataset includes controlled variations in prepositions, container transparency, and knowledge states. Baseline comparisons use Kosinski's established ToM test. Results are analyzed across complexity classes to identify specific ToM challenges where models struggle most.

## Key Results
- Models achieve >50% turn accuracy but only 0–4.4% goal accuracy across all tested LLMs
- "Automatic change knowledge" class shows highest difficulty, with models failing to track unobserved environmental changes
- Performance drops significantly for preposition replacements (under vs over) and transparent container scenarios
- Current LLMs demonstrate ability to learn partial linguistic patterns but lack robust social reasoning capabilities

## Why This Works (Mechanism)
The study leverages controlled task variations to isolate specific ToM challenges and measure model performance across different complexity levels. By using binary multiple-choice format with explicit turn and goal accuracy metrics, the evaluation provides granular insights into whether models can track state changes versus understanding deeper agent intentions. The 10-class complexity structure allows systematic probing of model limitations across increasingly sophisticated social reasoning scenarios.

## Foundational Learning
- **Theory of Mind concepts**: Understanding agents' beliefs, desires, and intentions different from one's own
  - Why needed: Core framework for evaluating social reasoning capabilities
  - Quick check: Can models predict behavior based on false beliefs?

- **False-belief scenarios**: Tasks where agents hold incorrect assumptions about reality
  - Why needed: Classic ToM test case distinguishing humans from simpler AI
  - Quick check: Does model predict agent's action based on their mistaken belief?

- **Belief tracking**: Maintaining accurate mental models of multiple agents' knowledge states
  - Why needed: Essential for multi-agent social reasoning
  - Quick check: Can model update beliefs when agents gain or lose information?

## Architecture Onboarding

### Component Map
Input text → Task parser → LLM → Answer classifier → Turn accuracy / Goal accuracy metrics

### Critical Path
Task generation → Model prompting → Answer prediction → Accuracy calculation → Complexity class analysis

### Design Tradeoffs
- Binary multiple-choice format enables clear metrics but may oversimplify ToM complexity
- Zero-shot prompting tests inherent capabilities but may underestimate potential with better prompting
- Open-source models ensure reproducibility but limit comparison with proprietary systems

### Failure Signatures
- High turn accuracy but low goal accuracy indicates surface-level pattern matching without deep understanding
- Specific struggles with "automatic change knowledge" suggest limitations in tracking unobserved state changes
- Preposition sensitivity reveals linguistic rather than conceptual reasoning

### 3 First Experiments
1. Replicate findings with larger task set (200+ tasks) to improve statistical power
2. Test proprietary models (GPT-4, Claude) using identical protocols
3. Apply Chain-of-Thought prompting to assess if reasoning chains improve performance

## Open Questions the Paper Calls Out
- Why do models struggle specifically with "automatic change knowledge" class?
- Do current LLMs truly understand social reasoning or merely learn superficial linguistic patterns?
- Can few-shot or Chain-of-Thought prompting improve ToM performance?
- Are limitations due to training data, architecture, or fundamental model capabilities?

## Limitations
- Small task count (68 total) may limit generalizability across broader ToM landscape
- Binary multiple-choice format may not capture full spectrum of ToM capabilities
- Zero-shot prompting without exploring alternative prompting strategies

## Confidence
- High confidence: Task construction methodology and dataset creation
- Medium confidence: Comparative performance analysis between models
- Low confidence: Explanations for why certain ToM classes prove more difficult

## Next Checks
1. Replicate findings with a larger task set (200+ tasks) to improve statistical power and generalizability
2. Test proprietary models (GPT-4, Claude) using identical evaluation protocols
3. Conduct ablation studies on task components (prepositions, container types) to isolate which features most impact performance