---
ver: rpa2
title: Cross-lingual neural fuzzy matching for exploiting target-language monolingual
  corpora in computer-aided translation
arxiv_id: '2401.08374'
source_url: https://arxiv.org/abs/2401.08374
tags:
- translation
- proposals
- neuromatch
- used
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces a method to exploit target-language monolingual
  corpora in computer-aided translation (CAT) tools based on translation memories
  (TM). The core idea is to use cross-lingual sentence embeddings to retrieve translation
  proposals from monolingual corpora and a neural model to estimate their post-editing
  effort.
---

# Cross-lingual neural fuzzy matching for exploiting target-language monolingual corpora in computer-aided translation

## Quick Facts
- arXiv ID: 2401.08374
- Source URL: https://arxiv.org/abs/2401.08374
- Reference count: 15
- Primary result: Method increases useful translation proposals by ~10 percentage points compared to using TMs alone

## Executive Summary
This paper introduces a novel approach to enhance computer-aided translation (CAT) tools by integrating target-language monolingual corpora alongside traditional translation memories (TMs). The core innovation lies in using cross-lingual sentence embeddings (LaBSE) to retrieve translation proposals from monolingual corpora and a neural model (COMET-based) to estimate post-editing effort. Experiments on four language pairs demonstrate that this method significantly increases the amount of useful translation proposals compared to TM-only approaches, with human evaluation confirming the automatic results.

## Method Summary
The approach combines cross-lingual sentence embeddings (LaBSE) with a neural fuzzy match score (neuroFMS) to retrieve and rank translation proposals from both TMs and monolingual corpora. LaBSE embeddings enable retrieval of target-language sentences similar to source sentences without requiring parallel text. A COMET-based neural model then estimates a fuzzy match score for each candidate, allowing unified ranking and post-editing effort estimation. The method was evaluated on four language pairs using DGT-TM as TM, DGT-TM 2020 as test set, and EurLex as monolingual corpus.

## Key Results
- The proposed method increases useful translation proposals (TER < 0.4) by approximately 10 percentage points compared to TM-only approaches
- Human evaluation confirms automatic results, showing that proposals retrieved with the new method are more useful than automatic evaluation indicates
- The approach works across four language pairs (EN-ES, EN-DE, EN-CS, EN-FI) using a unified architecture

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual sentence embeddings allow retrieval of translation proposals from monolingual corpora without needing parallel source sentences.
- Mechanism: LaBSE encoder projects source and target sentences into a shared semantic space; cosine similarity between embeddings approximates translation equivalence.
- Core assumption: Sentences with high cosine similarity in the shared space are mutual translations.
- Evidence anchors:
  - [abstract] "Our approach relies on cross-lingual sentence embeddings to retrieve translation proposals from TL monolingual corpora"
  - [section] "LaBSE is a neural architecture that consists of two paired BERT encoders with shared weights... Cosine similarity can then be used to compare them."
  - [corpus] Corpus neighbor titles show cross-lingual retrieval works on related tasks, though specific proof for LaBSE in CAT context is indirect.
- Break condition: If semantic drift occurs between source and target embedding spaces, or if target sentences are too dissimilar in meaning despite lexical overlap.

### Mechanism 2
- Claim: A COMET-based model can estimate a fuzzy match score (neuroFMS) between a source sentence and a target-only sentence.
- Mechanism: COMET takes source and target embeddings, passes them through a feed-forward network to predict a score in [0,1] approximating conventional FMS.
- Core assumption: Learned mapping from embedding features to FMS generalizes from training pairs (source, target, FMS) to unseen source-target pairs.
- Evidence anchors:
  - [abstract] "our neural model to estimate their post-editing effort"
  - [section] "NeuroFMS is trained on a data set in which each instance consists of a source sentence s′, a translation proposal t, and the FMS between s′ and the source sentence s paired with t"
  - [corpus] Limited corpus evidence; only the AutoDesk dataset exists out-of-domain for training, so generalization is inferred rather than proven.
- Break condition: If the COMET model overfits to training domain or fails to capture semantic differences critical for FMS prediction.

### Mechanism 3
- Claim: neuroMatch and neuroFMS together enable CAT tools to combine proposals from TM and monolingual corpora seamlessly.
- Mechanism: neuroMatch retrieves proposals from both sources; neuroFMS estimates FMS-like scores for all proposals, allowing unified ranking and post-editing effort estimation.
- Core assumption: neuroFMS scores are comparable across proposals from different sources, enabling fair comparison.
- Evidence anchors:
  - [abstract] "our neural model for estimating the post-editing effort enables the combination of translation proposals obtained from monolingual corpora and from TMs in the usual way"
  - [section] "We then compute a form of FMS, which we term as neuroFMS... so that it can be used to estimate the post-editing effort"
  - [corpus] Indirect evidence from related work; no direct corpus proof that neuroFMS merges scores without bias.
- Break condition: If neuroFMS systematically biases scores for monolingual vs. TM proposals, leading to suboptimal ranking.

## Foundational Learning

- Concept: Cross-lingual sentence embeddings (LaBSE)
  - Why needed here: To measure semantic similarity between source and target sentences without relying on parallel text.
  - Quick check question: What property of LaBSE makes it suitable for finding translations across languages?

- Concept: Fuzzy match score (FMS) and its estimation
  - Why needed here: Translators use FMS to estimate post-editing effort; neuroFMS replicates this for monolingual proposals.
  - Quick check question: Why can't we use cosine similarity directly as a substitute for FMS?

- Concept: COMET architecture for quality estimation
  - Why needed here: To learn a mapping from sentence embeddings to a score mimicking FMS, enabling ranking and effort estimation.
  - Quick check question: What input features does COMET use to estimate FMS-like scores?

## Architecture Onboarding

- Component map:
  LaBSE encoder -> Faiss index -> neuroMatch pipeline -> neuroFMS model -> TER evaluation

- Critical path:
  1. Precompute LaBSE embeddings for all target sentences in TM and monolingual corpus.
  2. For each source sentence, compute its LaBSE embedding.
  3. Use Faiss to retrieve top-k target sentences by cosine similarity (neuroMatch).
  4. For each candidate, run COMET-based neuroFMS to get a fuzzy match score.
  5. Rank candidates by neuroFMS and filter by threshold.

- Design tradeoffs:
  - Embedding computation is expensive up front but enables fast retrieval.
  - neuroFMS is slower than cosine similarity but provides usable FMS-like scores.
  - Training neuroFMS requires labeled (source, target, FMS) pairs; out-of-domain models are more flexible but less accurate.

- Failure signatures:
  - Low retrieval recall: embeddings poorly aligned, Faiss index not large enough.
  - Poor FMS estimation: COMET model underfits or overfits to training domain.
  - Unreliable ranking: neuroFMS scores inconsistent across sources.

- First 3 experiments:
  1. Verify LaBSE embeddings produce high cosine similarity for known translation pairs in a small bilingual sample.
  2. Train neuroFMS on a small in-domain dataset and check correlation with true FMS on held-out pairs.
  3. Run neuroMatch on a subset of the TM only, compare recall and precision against FMS.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the quality of translation proposals be improved when using LaBSE embeddings, considering that some useful proposals may be discarded due to differences in phrasing compared to the reference translation?
- Basis in paper: [inferred] The paper mentions that using TER against an independent reference may underestimate the quality of translation proposals, as valid proposals could be discarded if the reference is formulated differently.
- Why unresolved: The paper suggests that using human TER (HTER) would be a better option, but producing such a dataset would be prohibitively expensive for the size of the test sets used.
- What evidence would resolve it: A human evaluation using HTER to compare translation proposals with their post-edited versions would provide a more accurate assessment of the quality of translation proposals retrieved using LaBSE embeddings.

### Open Question 2
- Question: How can the proposed approach be adapted to languages not covered by LaBSE or XLM-RoBERTa models?
- Basis in paper: [explicit] The paper mentions that the models used are pre-trained for a high number of languages, but there are languages not covered by these models.
- Why unresolved: The paper does not provide a solution for applying the proposed approach to languages not covered by LaBSE or XLM-RoBERTa.
- What evidence would resolve it: A study that evaluates the performance of the proposed approach using alternative cross-lingual sentence embedding models for languages not covered by LaBSE or XLM-RoBERTa would demonstrate the adaptability of the approach to a wider range of languages.

### Open Question 3
- Question: How can the combination of neuroMatch and neuroFMS be optimized to improve the retrieval and ranking of translation proposals?
- Basis in paper: [inferred] The paper mentions that further combination of neuroMatch and neuroFMS could be explored, such as using neuroMatch to retrieve the n-best translation proposals and then using neuroFMS to keep only the best one.
- Why unresolved: The paper does not provide an evaluation of different combination strategies for neuroMatch and neuroFMS.
- What evidence would resolve it: An experiment that compares the performance of different combination strategies for neuroMatch and neuroFMS in terms of the quality and usefulness of the retrieved translation proposals would identify the optimal approach for integrating these models.

## Limitations

- The evaluation relies on TER < 0.4 as a proxy for usefulness, which may not fully capture human judgment of translation quality
- Human evaluation was conducted only for EN-ES, leaving uncertainty about generalizability to other language pairs
- Limited training data available for neuroFMS, with only one out-of-domain dataset (Autodesk) for most language pairs

## Confidence

- Confidence in claimed ~10 percentage point improvement: Medium (evaluation relies on TER < 0.4 proxy; limited human evaluation; no ablation studies)
- Confidence in cross-lingual sentence embeddings mechanism: High (LaBSE's established performance in related tasks)
- Confidence in neuroFMS as reliable fuzzy match estimator: Medium (limited training data; absence of in-domain training for most language pairs)

## Next Checks

1. **Ablation study**: Run experiments with neuroMatch alone (using cosine similarity instead of neuroFMS) to quantify the specific contribution of the COMET-based scoring model to overall performance gains.

2. **Cross-lingual embedding validation**: Test LaBSE embedding quality by measuring translation retrieval accuracy on a small gold-standard parallel dataset for each language pair, comparing against baseline methods like MUSE or LASER.

3. **Generalization assessment**: Conduct human evaluation for all four language pairs to verify that the EN-ES results generalize, particularly for non-European language pairs (CS, FI) where cross-lingual embedding performance may differ.