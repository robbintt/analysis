---
ver: rpa2
title: Fast unsupervised ground metric learning with tree-Wasserstein distance
arxiv_id: '2411.07432'
source_url: https://arxiv.org/abs/2411.07432
tags:
- distance
- metric
- tree
- ground
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a computationally efficient method for unsupervised\
  \ ground metric learning using tree-Wasserstein distance (TWD) to approximate the\
  \ 1-Wasserstein distance. The key innovation is learning tree metrics on samples\
  \ (features) via power iterations, with complexity reduced from O(n\u2075) to O(n\xB3\
  )."
---

# Fast unsupervised ground metric learning with tree-Wasserstein distance

## Quick Facts
- arXiv ID: 2411.07432
- Source URL: https://arxiv.org/abs/2411.07432
- Reference count: 18
- Primary result: Tree-WSV achieves comparable or better clustering accuracy than Sinkhorn regularization on genomics datasets while being significantly faster, with complexity reduced from O(n⁵) to O(n³).

## Executive Summary
This paper introduces a computationally efficient method for unsupervised ground metric learning using tree-Wasserstein distance (TWD) to approximate 1-Wasserstein distance. The key innovation is learning tree metrics on samples and features via power iterations, reducing computational complexity from O(n⁵) to O(n³). The authors prove that tree structure can be chosen flexibly without constraining approximation quality, and provide a fast recursive algorithm for basis vector computation. Experiments demonstrate superior performance on single-cell RNA sequencing data, achieving comparable or better clustering accuracy than Sinkhorn regularization with significantly lower runtime.

## Method Summary
The method learns unsupervised ground metrics by approximating Wasserstein distances using tree structures. It constructs trees on samples and features via ClusterTree, computes basis vectors for pairwise leaf paths (using sparse SVD for small datasets or a recursive algorithm for large ones), and iteratively solves for edge weights using power iterations. Meta-iterations improve tree construction by using learned metrics from previous iterations. The approach achieves O(n³) complexity compared to O(n⁵) for full Wasserstein singular vectors, enabling application to large genomics datasets while maintaining clustering accuracy.

## Key Results
- Tree-WSV outperforms Sinkhorn entropy regularization in accuracy on toy data while being significantly faster
- On PBMC 3k, Neurons V1, and HLCA datasets (5k-10k samples), Tree-WSV achieves comparable or better clustering performance with runtime improvements
- Meta-iterations using learned metrics to construct new trees improve silhouette scores beyond single-pass performance
- The method scales efficiently to large datasets where traditional WSV approaches become computationally prohibitive

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The tree-Wasserstein distance (TWD) provides a low-rank approximation of the full Wasserstein singular vector (WSV) problem that preserves the essential geometry of the data.
- Mechanism: By embedding samples and features on trees and computing distances via tree metrics, the algorithm captures the key pairwise relationships while reducing computational complexity from O(n⁵) to O(n³). The tree structure ensures that the rank of the pairwise path matrix equals the number of edges minus one (or one less if there's a degree-2 root), guaranteeing a complete solution for the weight vector.
- Core assumption: The tree structure can flexibly represent the underlying data geometry without constraining the richness of the approximation up to the number of edge weights.
- Evidence anchors:
  - [abstract]: "We prove that the initial tree structure can be chosen flexibly, since tree geometry does not constrain the richness of the approximation up to the number of edge weights."
  - [section 2.2]: "Theorem 2.2 proves that we can choose trees for which the rank of Y′ is N−1 (or M−1, respectively), ensuring that the problem is well-posed."
  - [corpus]: The corpus shows related work on tree-Wasserstein distances and hierarchical representation learning, supporting the theoretical foundation of this approach.
- Break condition: If the tree structure is chosen such that the rank of the pairwise path matrix is less than N−1 (or M−1), the linear system may have multiple solutions or no unique solution, breaking the method.

### Mechanism 2
- Claim: The recursive algorithm for computing the basis set of pairwise leaf-to-leaf paths enables efficient computation of the TWD matrix without explicitly constructing the full n²×N matrix.
- Mechanism: The constructive proof of Theorem 2.2 provides a recursive method that directly computes a basis set from just the N×n tree parameter matrix. This avoids the computational bottleneck of constructing and decomposing the full pairwise path tensor, which doesn't scale well with large n, m.
- Core assumption: The recursive algorithm correctly identifies a linearly independent set of vectors that span the space of all pairwise leaf-to-leaf paths on the tree.
- Evidence anchors:
  - [section 2.3.1]: "Because this operation requires computing a large tensor Y reshaped into a long rectangular matrix of size N×n², the sparse method does not scale well with large n, m. In this case, we instead created a recursive algorithm as suggested from the constructive proof of Theorem 2.2."
  - [section 2.2]: "This proof suggests an efficient recursive algorithm that can also be used to find the basis set for Y′, which we call U."
  - [corpus]: The corpus includes related work on tree-based methods and hierarchical representation learning, supporting the viability of recursive approaches for tree computations.
- Break condition: If the recursion depth exceeds practical limits or the algorithm fails to correctly identify the basis vectors, the method will produce incorrect TWD computations.

### Mechanism 3
- Claim: Meta-iterations using the learned ground metric to construct new trees iteratively improve the clustering quality beyond what's achievable in a single pass.
- Mechanism: Initial trees constructed via ClusterTree use Euclidean distances, which may not capture the optimal geometry for the data. By using the ground metric outputs from previous iterations as the distance metric for initial tree construction, the algorithm creates deeper trees that better reflect the learned sample-feature relationships, leading to improved silhouette scores.
- Core assumption: The ground metric learned in one iteration provides meaningful information about the data structure that can be used to improve tree construction in subsequent iterations.
- Evidence anchors:
  - [section 4.2]: "We noted that silhouette scores for one iteration of the Tree-WSV algorithm – while quick to run – were low across datasets... To account for this, we re-ran the full unsupervised TWD ground metric algorithm using the ground metric outputs from the previous run as the distance metric for the initial tree construction."
  - [section 4]: "Results at 4 iterations and best (max 15 iterations) are summarised in Table 1."
  - [corpus]: The corpus shows related work on hierarchical and iterative learning methods, supporting the concept of using learned metrics to improve subsequent representations.
- Break condition: If the meta-iteration process fails to converge or if the improvement plateaus early, the additional computational cost may not justify the marginal gains in clustering quality.

## Foundational Learning

- Concept: Optimal Transport (OT) and Wasserstein Distance
  - Why needed here: The paper builds on Wasserstein singular vectors (WSV) as a foundation for unsupervised ground metric learning. Understanding OT provides the geometric framework for comparing probability distributions.
  - Quick check question: What is the computational complexity of computing the full Wasserstein distance between two histograms with n samples?

- Concept: Tree Metrics and Tree-Wasserstein Distance
  - Why needed here: The core innovation uses trees to approximate Wasserstein distances. Understanding how tree metrics work and their computational advantages is crucial.
  - Quick check question: How does the computational complexity of Tree-Wasserstein distance compare to sliced-Wasserstein distance?

- Concept: Singular Value Decomposition (SVD) and Power Iterations
  - Why needed here: The algorithm uses power iterations to find Wasserstein singular vectors. Understanding SVD helps grasp how the iterative method converges to meaningful solutions.
  - Quick check question: In the context of WSV, what does the singular value represent in terms of the relationship between samples and features?

## Architecture Onboarding

- Component map:
  Data preprocessing -> Tree construction (ClusterTree) -> Basis computation -> Power iterations -> Meta-iterations -> Evaluation

- Critical path:
  1. Construct initial trees using ClusterTree
  2. Compute basis set for pairwise leaf paths
  3. Initialize random weight vectors
  4. Run power iterations to solve for w vectors
  5. Use learned metrics to construct new trees
  6. Repeat until convergence or max iterations

- Design tradeoffs:
  - Tree depth vs. accuracy: Deeper trees (fewer children per node) provide more accurate approximations but increase computational cost
  - Basis computation method: Sparse SVD is faster for small datasets but doesn't scale; recursive algorithm scales better but may have implementation complexity
  - Number of meta-iterations: More iterations can improve clustering but increase runtime significantly

- Failure signatures:
  - Poor clustering results: May indicate inappropriate tree structure or insufficient meta-iterations
  - Slow convergence: Could suggest issues with basis computation or weight initialization
  - Memory errors on large datasets: May indicate need for the recursive basis algorithm instead of sparse SVD

- First 3 experiments:
  1. Run Tree-WSV on the PBMC 3k dataset with default parameters and compare silhouette scores to Euclidean baseline
  2. Vary the minimum children parameter in ClusterTree (k=3, k=4, k=5) and measure impact on accuracy vs. runtime
  3. Compare the recursive basis algorithm vs. sparse SVD on a small dataset (n<100) to verify correctness before scaling up

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of initial tree structure (via ClusterTree) affect the convergence and final quality of the learned ground metric?
- Basis in paper: [inferred] The paper mentions that initial tree structure is calculated via ClusterTree using Euclidean distances, and that meta-iterations with re-constructed trees based on previous weight matrices improved silhouette scores.
- Why unresolved: While the paper shows that meta-iterations improve performance, it doesn't systematically analyze how different initial tree structures affect convergence speed or final metric quality.
- What evidence would resolve it: Comparative experiments varying the initial tree construction method (different distance metrics, different k values in ClusterTree) and measuring their impact on convergence rate and final clustering performance.

### Open Question 2
- Question: What is the theoretical relationship between the rank of the tree path matrix Y and the quality of the learned ground metric approximation?
- Basis in paper: [explicit] Theorem 2.2 proves that rank(Y) = N-1 or N-2 depending on tree structure, and the paper notes that rank correlates with accuracy.
- Why unresolved: The paper observes a correlation but doesn't establish a rigorous theoretical bound on approximation quality as a function of rank.
- What evidence would resolve it: A mathematical proof or empirical study demonstrating the relationship between rank deficiency and approximation error in the learned ground metric.

### Open Question 3
- Question: How does the proposed Tree-WSV method perform on other types of biological data beyond single-cell RNA sequencing, such as proteomics or spatial transcriptomics?
- Basis in paper: [inferred] The paper demonstrates success on scRNA-seq data but doesn't explore other biological data modalities.
- Why unresolved: The method is presented as a general unsupervised ground metric learning approach, but its performance characteristics on other data types remain unknown.
- What evidence would resolve it: Benchmarking Tree-WSV on proteomics, spatial transcriptomics, or other omics datasets, comparing performance to both traditional methods and WSV approaches.

## Limitations
- The meta-iteration approach lacks theoretical convergence guarantees despite showing empirical improvements
- The recursive basis computation algorithm has implementation complexity that could introduce subtle errors
- Performance on non-biological datasets remains unexplored, limiting generalizability claims

## Confidence
- High confidence: The theoretical foundations (Theorem 2.2 on flexible tree structure, rank guarantees) are well-established and clearly proven
- Medium confidence: The empirical improvements over baselines are demonstrated but could benefit from more diverse datasets and statistical significance testing
- Medium confidence: The scalability claims are supported by runtime experiments but would benefit from testing on even larger datasets (>10k samples)

## Next Checks
1. **Convergence analysis**: Systematically measure silhouette score improvements across meta-iterations to identify optimal iteration count and verify convergence behavior
2. **Statistical validation**: Perform significance testing comparing Tree-WSV against Sinkhorn regularization across multiple random seeds and dataset splits
3. **Generalization testing**: Apply the method to non-biological datasets (e.g., image features, text embeddings) to assess broader applicability beyond genomics