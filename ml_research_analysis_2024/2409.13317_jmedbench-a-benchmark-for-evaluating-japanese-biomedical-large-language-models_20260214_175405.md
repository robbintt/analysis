---
ver: rpa2
title: 'JMedBench: A Benchmark for Evaluating Japanese Biomedical Large Language Models'
arxiv_id: '2409.13317'
source_url: https://arxiv.org/abs/2409.13317
tags:
- japanese
- biomedical
- llms
- datasets
- tasks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces JMedBench, a benchmark for evaluating Japanese
  biomedical large language models (LLMs). The benchmark consists of 20 Japanese biomedical
  datasets across five tasks: multi-choice question-answering (MCQA), named entity
  recognition (NER), machine translation (MT), document classification (DC), and semantic
  text similarity (STS).'
---

# JMedBench: A Benchmark for Evaluating Japanese Biomedical Large Language Models

## Quick Facts
- **arXiv ID**: 2409.13317
- **Source URL**: https://arxiv.org/abs/2409.13317
- **Reference count**: 32
- **Key outcome**: Introduces JMedBench, a benchmark evaluating Japanese biomedical LLMs across 20 datasets in 5 task types, revealing that models with strong Japanese language understanding and biomedical knowledge perform best, while non-specialized LLMs can still achieve competitive results.

## Executive Summary
This paper presents JMedBench, a comprehensive benchmark for evaluating Japanese biomedical large language models (LLMs). The benchmark encompasses 20 Japanese biomedical datasets spanning five core NLP tasks: multi-choice question-answering, named entity recognition, machine translation, document classification, and semantic text similarity. Eight representative LLMs were systematically evaluated, including general and biomedical models across Japanese and other languages. The results demonstrate that LLMs with strong Japanese language capabilities and rich biomedical knowledge achieve superior performance on Japanese biomedical tasks. Interestingly, the study finds that LLMs not specifically designed for Japanese biomedical domains can still perform competitively, highlighting the importance of cross-lingual transfer learning and the need for further refinement in certain Japanese biomedical tasks.

## Method Summary
The authors developed JMedBench by curating 20 Japanese biomedical datasets across five task categories. The evaluation framework tested eight representative LLMs including general and biomedical models in both Japanese and other languages. Each model was assessed using standardized metrics appropriate to each task type, with results analyzed to identify performance patterns and task-specific challenges. The evaluation tools and datasets were made publicly available to support reproducibility and future research.

## Key Results
- Japanese biomedical LLMs outperform general models on Japanese biomedical tasks
- Models with better Japanese language understanding achieve higher scores
- Non-biomedical LLMs can perform surprisingly well on certain Japanese biomedical tasks
- Document classification and semantic text similarity tasks show particular room for improvement

## Why This Works (Mechanism)
The evaluation framework works by providing standardized, task-specific metrics that reveal how different models handle the intersection of Japanese language processing and biomedical domain knowledge. The benchmark design allows for direct comparison across model types by controlling for task difficulty and dataset characteristics.

## Foundational Learning
- **Japanese NLP**: Why needed - Japanese has unique linguistic features requiring specialized processing; Quick check - Can the model handle Japanese tokenization and morphological analysis correctly
- **Biomedical terminology**: Why needed - Medical terms often differ from general language; Quick check - Can the model correctly interpret and generate medical terminology
- **Cross-lingual transfer**: Why needed - Models trained on one language may transfer to Japanese; Quick check - Does the model maintain performance when processing Japanese biomedical text
- **Task-specific metrics**: Why needed - Different NLP tasks require different evaluation approaches; Quick check - Are the metrics appropriate for each task type
- **Benchmark standardization**: Why needed - Enables fair comparison across diverse models; Quick check - Are evaluation conditions consistent across all models

## Architecture Onboarding

**Component Map**: Data curation -> Task classification -> Model selection -> Evaluation execution -> Result analysis -> Tool publication

**Critical Path**: The evaluation pipeline flows from dataset preparation through model execution to result aggregation, with each stage building on the previous one. The critical path is: Data preparation → Model evaluation → Performance analysis → Result reporting

**Design Tradeoffs**: The benchmark balances comprehensiveness with practicality by selecting 20 datasets across 5 task types rather than attempting exhaustive coverage. This approach provides broad insight while remaining manageable for evaluation purposes.

**Failure Signatures**: Models may fail due to insufficient Japanese language understanding, lack of biomedical domain knowledge, or task-specific limitations. Japanese general models may struggle with biomedical terminology, while biomedical models may have poor Japanese language processing.

**First Experiments**:
1. Evaluate a single model across all five task types to establish baseline performance patterns
2. Compare Japanese general models against biomedical models on a shared task to identify knowledge gaps
3. Test cross-lingual transfer by evaluating non-Japanese models on Japanese biomedical datasets

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark scope limited to five specific task types, potentially missing other critical biomedical NLP applications
- Evaluation includes only eight representative LLMs, which may not capture the full landscape of available models
- Results focus primarily on performance metrics without extensive error analysis to understand failure modes

## Confidence
- **High confidence**: Japanese language understanding and biomedical knowledge correlate with better performance
- **Medium confidence**: Non-specialized LLMs can perform well on Japanese biomedical tasks
- **Low confidence**: Claims about specific areas needing improvement due to limited task and model sample

## Next Checks
1. Expand evaluation to include additional biomedical NLP tasks such as information extraction from clinical notes and question answering from medical literature
2. Conduct detailed error analysis across different model types to identify systematic weaknesses and failure patterns
3. Test model performance on progressively more challenging subsets within each task category to establish performance ceilings and limitations