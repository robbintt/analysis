---
ver: rpa2
title: Multimodal Pragmatic Jailbreak on Text-to-image Models
arxiv_id: '2409.19149'
source_url: https://arxiv.org/abs/2409.19149
tags:
- image
- text
- unsafe
- prompt
- safe
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces a novel jailbreak technique that exploits\
  \ the interaction between generated images and visual text, enabling diffusion models\
  \ to produce unsafe content even when each modality is safe in isolation. The authors\
  \ propose the MPUP dataset of 1,400 multimodal pragmatic unsafe prompts and benchmark\
  \ nine state-of-the-art T2I models, revealing that all models suffer from such jailbreak\
  \ with rates ranging from 10% to 70%, with DALL\xB7E 3 showing the highest vulnerability."
---

# Multimodal Pragmatic Jailbreak on Text-to-image Models

## Quick Facts
- arXiv ID: 2409.19149
- Source URL: https://arxiv.org/abs/2409.19149
- Reference count: 40
- One-line primary result: All nine tested T2I models are vulnerable to multimodal jailbreak attacks, with success rates ranging from 10% to 70%

## Executive Summary
This work introduces a novel jailbreak technique that exploits the interaction between generated images and visual text, enabling diffusion models to produce unsafe content even when each modality is safe in isolation. The authors propose the MPUP dataset of 1,400 multimodal pragmatic unsafe prompts and benchmark nine state-of-the-art T2I models, revealing that all models suffer from such jailbreak with rates ranging from 10% to 70%, with DALL·E 3 showing the highest vulnerability. Experiments show that current safety filters fail to effectively detect this multimodal unsafe content. The jailbreak arises from the models' ability to render visual text, which can originate from training data containing unsafe multimodal examples. The authors also explore mitigation strategies, including advanced prompting techniques and latent space fine-tuning, but find these methods inadequate for fully addressing the issue.

## Method Summary
The paper introduces a multimodal pragmatic jailbreak technique that exploits the interaction between visual text and image content in text-to-image models. The authors create the MPUP dataset using GPT-4 to generate 1,400 multimodal unsafe prompts across four categories: hate speech, physical harm, fraud, and pornography. They benchmark nine T2I models (seven open-source and two closed-source) using ASR measured by GPT-4o. The study evaluates safety classifiers including word blocklists, semantic similarity scores, LLM-based classifiers, and adapted CLIP classifiers. They also analyze training data for multimodal unsafe examples and test mitigation strategies including advanced prompting and latent space fine-tuning.

## Key Results
- All nine T2I models tested show vulnerability to multimodal jailbreak attacks with ASR ranging from 10% to 70%
- DALL·E 3 exhibits the highest vulnerability at 70% ASR across all categories
- Current safety filters (keyword blocklists, CLIP classifiers, NSFW detectors) fail to effectively detect multimodal unsafe content
- Models with higher visual text rendering accuracy show higher jailbreak success rates
- Mitigation strategies including advanced prompting and latent space fine-tuning show limited effectiveness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal pragmatic jailbreak exploits semantic misalignment between image and visual text content, where each modality alone is safe but their combination produces unsafe meaning.
- Mechanism: Diffusion models' text rendering capability allows them to generate legible visual text in images. When this visual text interacts with the image content through figurative language (metaphor, sarcasm, allusion), it creates new unsafe meanings that bypass single-modality safety filters.
- Core assumption: Diffusion models trained on image-text pairs containing visual text can render legible text that interacts semantically with image content to create multimodal unsafe content.
- Evidence anchors:
  - [abstract] "introduces a novel type of jailbreak, which triggers T2I models to generate the image with visual text, where the image and the text, although considered to be safe in isolation, combine to form unsafe content"
  - [section 3.1] "The format of the prompt is '<image-generation prompt>, with a sign that says, '<visual text prompt>'"
  - [corpus] Weak - no direct evidence of this specific jailbreak mechanism in related papers
- Break condition: If safety filters incorporate multimodal reasoning or if models lose their ability to render legible visual text.

### Mechanism 2
- Claim: Training data containing multimodal unsafe examples provides the foundation for jailbreak generation.
- Mechanism: The LAION dataset contains images with visual text where the combination of image and text is unsafe, even though each component alone may be safe. Diffusion models learn to reproduce these multimodal unsafe combinations.
- Core assumption: Training data includes image-text pairs where visual text and image content combine to create unsafe meanings, and models learn to reproduce these combinations.
- Evidence anchors:
  - [section 6.1] "LAION does include data pairs of images with visual text and corresponding captions" and "multimodal pragmatic jailbreak in diffusion models can arise from an apparent capability of generating correct visual text in images without deep semantic understanding"
  - [corpus] Moderate - related work shows T2I models can reproduce unsafe content from training data
- Break condition: If training data is filtered to remove multimodal unsafe examples or if models are fine-tuned to recognize unsafe combinations.

### Mechanism 3
- Claim: Single-modality safety filters fail because they cannot reason about multimodal interactions.
- Mechanism: Current safety filters operate independently on text prompts and generated images. They lack the capability to analyze how visual text interacts with image content to create unsafe meanings, allowing multimodal jailbreaks to bypass detection.
- Core assumption: Safety filters are designed to detect unsafe content in individual modalities rather than analyzing multimodal interactions.
- Evidence anchors:
  - [abstract] "various filters such as keyword blocklists, customized prompt filters, and NSFW image filters, are commonly employed to mitigate these risks... they fail to work against our jailbreak"
  - [section 5.3] "current classifiers and mitigation methods are inadequate for effectively identifying such complex unsafe content"
  - [corpus] Strong - related papers discuss limitations of unimodal safety filters
- Break condition: If safety systems incorporate multimodal reasoning capabilities or cross-modal analysis.

## Foundational Learning

- Concept: Diffusion models and latent space generation
  - Why needed here: Understanding how T2I models generate images through denoising processes is crucial for understanding jailbreak mechanisms
  - Quick check question: How does a diffusion model transform random noise into a coherent image through the denoising process?

- Concept: Visual text rendering in diffusion models
  - Why needed here: The jailbreak relies on models' ability to generate legible text within images, which is an emergent capability in diffusion models
  - Quick check question: What architectural features enable diffusion models to render readable text in generated images?

- Concept: Multimodal semantics and figurative language
  - Why needed here: The jailbreak exploits how visual text interacts with image content through metaphor, sarcasm, and allusion to create unsafe meanings
  - Quick check question: How can the same visual text have different meanings when combined with different image contexts?

## Architecture Onboarding

- Component map: Text encoder -> Diffusion UNet -> Text renderer -> Safety filters -> Multimodal evaluator
- Critical path: Prompt → Text encoding → Latent diffusion → Image generation → Visual text rendering → Safety evaluation
- Design tradeoffs:
  - Model size vs. safety: Larger models show higher jailbreak rates due to better text rendering
  - Filter granularity vs. usability: Stricter filters may block legitimate content
  - Training data diversity vs. safety: More diverse data increases both capability and risk
- Failure signatures:
  - High OCR accuracy correlates with high jailbreak success rates
  - Models with better text rendering capabilities show more multimodal jailbreaks
  - Safety filters with high precision on single modalities fail on multimodal content
- First 3 experiments:
  1. Measure OCR accuracy across different T2I models and correlate with jailbreak rates
  2. Test safety filter effectiveness on isolated vs. combined modalities
  3. Analyze training data for multimodal unsafe examples and their impact on model behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do multimodal jailbreak prompts interact with different types of visual text rendering errors, and what is the relationship between OCR accuracy and attack success rates?
- Basis in paper: [explicit] The paper analyzes OCR accuracy for both full and substring texts across nine T2I models and observes that models with higher substring OCR accuracy tend to have higher multimodal jailbreak rates.
- Why unresolved: The paper notes that ASR does not reach 100% even for OCR-correct images, suggesting other factors beyond text rendering quality influence jailbreak success. The relationship between rendering quality, ASR, and evaluation model limitations remains unclear.
- What evidence would resolve it: Systematic analysis of ASR distributions across different OCR accuracy bins, combined with human evaluation of rendering quality and evaluation model error analysis, would clarify how text rendering quality impacts jailbreak success.

### Open Question 2
- Question: What specific characteristics of training data contribute to multimodal jailbreak vulnerability, beyond the presence of visual text in captions?
- Basis in paper: [explicit] The paper investigates training data by sampling LAION-400M and finding rates of visual text presence (R1=25.2%) and multimodal unsafe content (R4=1.6%), but acknowledges these metrics alone don't fully explain jailbreak susceptibility.
- Why unresolved: The analysis identifies the presence of visual text in training data but doesn't explore deeper patterns like frequency of specific unsafe themes, multimodal interaction examples, or the role of visual-text-image relationships in training data.
- What evidence would resolve it: Detailed analysis of training data distributions showing frequencies of different unsafe themes, multimodal interaction patterns, and correlations between specific visual-text combinations and jailbreak susceptibility would clarify training data's role.

### Open Question 3
- Question: How effective would multimodal safety classifiers be compared to unimodal approaches, and what architectural approaches might enable effective multimodal detection?
- Basis in paper: [inferred] The paper benchmarks unimodal filters (keyword blocklists, CLIP classifiers, NSFW detectors) and finds them inadequate, suggesting that current methods fail because they cannot capture interactions between modalities.
- Why unresolved: While the paper demonstrates unimodal filter limitations, it doesn't explore or propose specific multimodal classifier architectures or evaluate their potential effectiveness against the jailbreak.
- What evidence would resolve it: Development and evaluation of multimodal classifier architectures (e.g., fusion models, joint embedding spaces) against the MPUP dataset would demonstrate whether multimodal approaches could effectively detect these jailbreaks.

## Limitations
- The MPUP dataset may not capture full diversity of multimodal jailbreak scenarios and relies on GPT-4 for prompt generation, introducing potential biases
- Results focus on text-to-image models; effectiveness on video generation or multimodal models remains untested
- Proposed mitigation strategies show limited effectiveness but evaluation timeframe and methodology are not fully detailed

## Confidence
- High Confidence: Core mechanism of multimodal jailbreak exploiting visual text-image interaction is well-supported across nine T2I models
- Medium Confidence: Hypothesis that training data enables jailbreak is supported by LAION analysis but lacks quantitative causation evidence
- Low Confidence: Precise threshold of visual text rendering accuracy correlating with jailbreak success is not clearly established; DALL·E 3 vulnerability claim may be influenced by API constraints

## Next Checks
1. Implement and evaluate a multimodal safety filter that analyzes the interaction between visual text and image content rather than processing modalities independently, testing against the MPUP dataset
2. Conduct systematic analysis of the LAION dataset to quantify prevalence of multimodal unsafe examples and their correlation with jailbreak success rates across different T2I models
3. Extend benchmark to include newer architectures and models with different text rendering capabilities to understand relationship between architectural design and jailbreak vulnerability