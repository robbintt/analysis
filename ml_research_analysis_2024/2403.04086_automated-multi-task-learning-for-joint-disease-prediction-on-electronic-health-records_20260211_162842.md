---
ver: rpa2
title: Automated Multi-Task Learning for Joint Disease Prediction on Electronic Health
  Records
arxiv_id: '2403.04086'
source_url: https://arxiv.org/abs/2403.04086
tags:
- task
- search
- tasks
- learning
- grouping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces AutoDP, an automated multi-task learning
  framework for joint disease prediction on electronic health records. The framework
  addresses limitations in existing multi-task learning approaches by simultaneously
  optimizing task grouping and model architectures.
---

# Automated Multi-Task Learning for Joint Disease Prediction on Electronic Health Records

## Quick Facts
- arXiv ID: 2403.04086
- Source URL: https://arxiv.org/abs/2403.04086
- Authors: Suhan Cui; Prasenjit Mitra
- Reference count: 40
- Primary result: AutoDP framework achieves 13.43% A VP and 2.80% ROC gains on MIMIC-IV for 25 disease predictions

## Executive Summary
This paper introduces AutoDP, an automated multi-task learning framework that jointly optimizes task grouping and neural architecture for disease prediction from electronic health records. The framework addresses the limitation of manual task grouping in existing MTL approaches by employing surrogate model-based optimization with progressive sampling to efficiently explore the joint space of task combinations and architectures. Evaluated on MIMIC-IV dataset for 25 diseases, AutoDP demonstrates significant performance improvements over both hand-crafted and automated baselines, while maintaining computational feasibility through its optimization strategy.

## Method Summary
AutoDP employs a surrogate model-based optimization approach with progressive sampling to efficiently search the joint space of task combinations and neural architectures. The framework uses a greedy search algorithm to derive near-optimal solutions from the large search space, avoiding exhaustive enumeration. The method simultaneously optimizes task grouping and model architectures through iterative refinement, leveraging surrogate models to predict performance of candidate configurations before full training. The progressive sampling strategy helps focus computational resources on promising regions of the search space.

## Key Results
- AutoDP achieves average per-task gains of 13.43% in A VP compared to existing methods
- AutoDP achieves average per-task gains of 2.80% in ROC AUC compared to existing methods
- Outperforms both hand-crafted and automated baselines while maintaining feasible computational costs

## Why This Works (Mechanism)
The framework works by addressing the fundamental limitation in multi-task learning where task grouping is typically predetermined. By automating the discovery of optimal task groupings through surrogate model-based optimization, AutoDP can identify synergies between related diseases that may not be apparent through manual grouping. The progressive sampling strategy ensures computational efficiency by focusing resources on promising configurations, while the greedy search algorithm provides a practical way to navigate the large joint space of task combinations and architectures.

## Foundational Learning

**Surrogate Model Optimization**: Uses predictive models to estimate performance of candidate configurations without full training. Needed to make the large search space computationally tractable. Quick check: Verify the surrogate model's prediction accuracy against actual training results.

**Progressive Sampling**: Iteratively refines the search space by focusing on promising regions. Needed to balance exploration and exploitation in the search process. Quick check: Track how quickly the sampling converges to optimal solutions.

**Multi-Task Learning**: Trains multiple related tasks simultaneously to share knowledge and improve generalization. Needed to leverage correlations between disease predictions. Quick check: Measure performance gains from task sharing versus individual task training.

**Greedy Search Algorithm**: Iteratively builds solutions by making locally optimal choices. Needed to find near-optimal solutions in large search spaces. Quick check: Compare greedy results against known optimal solutions on smaller problem instances.

## Architecture Onboarding

**Component Map**: EHR Data -> Task Grouping Module -> Architecture Search Module -> Surrogate Model -> Progressive Sampling -> Model Training -> Performance Evaluation

**Critical Path**: The critical path flows from EHR data through task grouping optimization to final model training and evaluation. The surrogate model and progressive sampling components are essential for computational efficiency.

**Design Tradeoffs**: The framework trades exhaustive search completeness for computational feasibility. The surrogate model introduces approximation error but enables exploration of a much larger search space than would be possible with full training of all candidates.

**Failure Signatures**: Poor surrogate model accuracy would lead to wasted computational resources on suboptimal configurations. Inadequate progressive sampling might miss optimal solutions in unexplored regions. Suboptimal greedy search could converge to local optima.

**First Experiments**: 1) Validate surrogate model accuracy on a small subset of configurations. 2) Test progressive sampling convergence on synthetic task grouping problems. 3) Benchmark greedy search against random search on toy MTL problems.

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on surrogate model-based optimization may not capture full complexity of search space
- Evaluation conducted only on MIMIC-IV dataset, limiting generalizability
- Statistical significance of performance improvements not explicitly tested

## Confidence
The major claims are rated as **Medium** confidence. The methodology is sound and well-documented, but the limited scope of evaluation and lack of statistical validation reduce confidence in the generalizability of results.

## Next Checks
1. Evaluate AutoDP on at least two additional EHR datasets from different healthcare systems to assess generalizability
2. Perform statistical tests on performance metrics across multiple data splits to confirm significance of improvements
3. Provide detailed documentation of computational resources required for the entire AutoDP pipeline