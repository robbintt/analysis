---
ver: rpa2
title: Erasure Coded Neural Network Inference via Fisher Averaging
arxiv_id: '2409.01420'
source_url: https://arxiv.org/abs/2409.01420
tags:
- neural
- coded
- networks
- coding
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of erasure coding for neural network
  inference, aiming to construct a coded model whose output is a linear combination
  of multiple neural network outputs. The authors formulate this as a KL barycenter
  problem and propose a practical algorithm called COIN that leverages the diagonal
  Fisher information to create a coded model that approximately outputs the desired
  linear combination of outputs.
---

# Erasure Coded Neural Network Inference via Fisher Averaging

## Quick Facts
- arXiv ID: 2409.01420
- Source URL: https://arxiv.org/abs/2409.01420
- Reference count: 40
- Primary result: COIN achieves >97.5% normalized decoding accuracy with <2.5% accuracy loss compared to individual models, using <1% of training data

## Executive Summary
This paper addresses the problem of erasure coding for neural network inference by constructing a coded model whose output is a linear combination of multiple neural network outputs. The authors formulate this as a KL barycenter problem and propose COIN, a practical algorithm that leverages diagonal Fisher information to create an efficient coded model. COIN significantly outperforms baseline methods in decoding accuracy while requiring minimal computational resources, using less than 1% of training data and avoiding expensive ensemble distillation procedures.

## Method Summary
COIN reformulates the coding objective as a KL barycenter problem where the goal is to find a distribution that minimizes the weighted KL divergence from multiple neural network output distributions. The method approximates the Fisher information matrix with its diagonal and uses a closed-form solution to compute the coded model parameters. This approach replaces iterative training with a single matrix inversion step, achieving high compute efficiency. The algorithm requires only a small fraction of training data (200 samples) to estimate the diagonal Fisher and derive the coded parameters analytically.

## Key Results
- COIN achieves average normalized decoding accuracies greater than 97.5% in some cases
- The method results in less than 2.5% loss in accuracy compared to individual models
- COIN requires less than 1% of the training data and avoids expensive ensemble distillation procedures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: COIN approximates a KL barycenter to produce a coded model whose output is a linear combination of given neural network outputs
- Mechanism: Reformulates coding objective as KL barycenter problem, approximates Fisher information matrix with diagonal, uses closed-form solution
- Core assumption: Diagonal Fisher approximation is sufficient for non-linear neural networks
- Evidence anchors: [abstract] "proposes a practical algorithm COIN that leverages the diagonal Fisher information"; [section] "approximation for KL divergence between pθi and pθ"; [corpus] Weak or missing direct evidence that diagonal Fisher works well for non-linear networks

### Mechanism 2
- Claim: COIN avoids overfitting by using only a small fraction of training data (less than 1%)
- Mechanism: Uses fixed number of samples (P=200) to compute diagonal Fisher and derive coded parameters analytically
- Core assumption: Small number of samples is sufficient to accurately estimate diagonal Fisher
- Evidence anchors: [abstract] "method only requires less than 1% of training data"; [section] "P to be about 200 to get good approximation"; [corpus] Weak or missing direct evidence that 200 samples are sufficient

### Mechanism 3
- Claim: COIN's compute efficiency comes from replacing iterative training with single matrix inversion
- Mechanism: Coded model parameters computed directly via closed-form expression involving diagonal Fisher matrices
- Core assumption: Fisher-weighted averaging formula yields good approximation without iterative optimization
- Evidence anchors: [abstract] "significantly higher than other baselines while being extremely compute-efficient"; [section] "Equation (15) outlines how to compute θ∗ such that fθ∗(x) ≈PN i=1 βifθi(x)"; [corpus] Weak or missing direct evidence comparing runtime to ensemble distillation

## Foundational Learning

- Concept: KL divergence and KL barycenter
  - Why needed here: Coding objective reformulated as finding distribution minimizing weighted KL divergence from multiple neural network output distributions
  - Quick check question: What is the KL divergence between two Gaussian distributions with same covariance but different means?

- Concept: Fisher information matrix
  - Why needed here: Encodes curvature of log-likelihood and approximates KL divergence between neural network output distributions
  - Quick check question: How is Fisher information matrix related to gradient of log-likelihood with respect to model parameters?

- Concept: Diagonal approximation of matrices
  - Why needed here: Computing full Fisher matrix is expensive, so diagonal used as computationally efficient approximation
  - Quick check question: When is diagonal approximation of matrix a good approximation of full matrix?

## Architecture Onboarding

- Component map: Pre-trained neural network parameters (θ1, θ2, ..., θN) and coding weights (β1, β2, ..., βN) -> Fisher estimation (diagonal approximation) -> closed-form parameter computation -> coded model parameters (θc) and decoding function

- Critical path:
  1. Compute diagonal Fisher matrices for each pre-trained model using P samples
  2. Apply Fisher-weighted averaging formula to compute θc
  3. Use coded model and decoding function to recover individual model outputs

- Design tradeoffs:
  - Sample size vs. accuracy: Larger P improves Fisher estimation but increases compute cost
  - Diagonal vs. full Fisher: Diagonal is faster but may lose information
  - Coding weights: Choice of β affects trade-off between individual model accuracy and overall robustness

- Failure signatures:
  - Low normalized decoding accuracy despite high compute efficiency
  - Unstable or NaN values in coded model parameters (likely due to ill-conditioned Fisher matrices)
  - Large gap between train and test decoding accuracy (overfitting)

- First 3 experiments:
  1. Implement Fisher diagonal estimation and verify it matches gradient outer product on small network
  2. Test closed-form parameter computation on simple linear model and verify it produces correct linear combination
  3. Run full pipeline on small dataset (e.g., MNIST) and compare decoding accuracy to baselines using same P samples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does COIN performance scale when coding over more than two neural networks (general k > 2)?
- Basis in paper: [explicit] "we focus on the Nc = 1 case and leave extensions to general k as future work" in Section I
- Why unresolved: Authors explicitly state this as future work and only demonstrate results for n = 2 models
- What evidence would resolve it: Experimental results showing normalized decoding accuracy for COIN with 3 or more models, comparing against baselines

### Open Question 2
- Question: How does COIN perform with different neural network architectures beyond ResNet50, such as transformers?
- Basis in paper: [explicit] Section VI mentions "Directions for future work include characterizing the performance on a wider range of model architectures such as transformers"
- Why unresolved: All experiments use ResNet50 pretrained on ImageNet, limiting generalizability to other architectures
- What evidence would resolve it: Experiments applying COIN to transformer-based models or other architectures, measuring normalized decoding accuracy across different model types

### Open Question 3
- Question: What is the optimal number of training samples P needed for COIN to achieve good performance across different datasets?
- Basis in paper: [explicit] Section V mentions using P = 200 samples (less than 1% of data) and Appendix VIII shows ablation study on P's effect
- Why unresolved: Paper only tests one specific value of P and shows results for one dataset combination in ablation study
- What evidence would resolve it: Systematic experiments varying P across multiple datasets and measuring trade-off between performance and sample efficiency

## Limitations
- The diagonal Fisher approximation may not capture full geometry of non-linear neural networks, though not validated against full Fisher computation
- The choice of 200 samples (P=200) for Fisher estimation is not rigorously justified - no sensitivity ablation studies
- Results primarily shown for ResNet50 on vision datasets; generalization to other architectures and domains remains unclear

## Confidence
- High confidence: Compute efficiency claims and NDA metric computation (directly measured)
- Medium confidence: General mechanism of using Fisher-weighted averaging for coded models (theoretically sound but diagonal approximation unverified)
- Low confidence: Claims about avoiding overfitting and superiority over ensemble distillation (lacking direct comparisons)

## Next Checks
1. Implement COIN with full Fisher matrix computation and compare NDA results to diagonal approximation to quantify information loss
2. Run ablation studies varying P (number of samples for Fisher estimation) from 50 to 1000 to determine sensitivity and minimum viable sample size
3. Test COIN on different architecture (e.g., Vision Transformer) and dataset type (e.g., NLP) to evaluate generalizability beyond ResNet50 on vision tasks