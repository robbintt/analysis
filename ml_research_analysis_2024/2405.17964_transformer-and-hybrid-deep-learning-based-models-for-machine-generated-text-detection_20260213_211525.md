---
ver: rpa2
title: Transformer and Hybrid Deep Learning Based Models for Machine-Generated Text
  Detection
arxiv_id: '2405.17964'
source_url: https://arxiv.org/abs/2405.17964
tags:
- subtask
- text
- used
- figure
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents the UniBuc - NLP team''s approach to the SemEval
  2024 Task 8: Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated
  Text Detection. The team explored transformer-based and hybrid deep learning architectures
  for three subtasks: binary classification (human vs.'
---

# Transformer and Hybrid Deep Learning Based Models for Machine-Generated Text Detection

## Quick Facts
- **arXiv ID:** 2405.17964
- **Source URL:** https://arxiv.org/abs/2405.17964
- **Reference count:** 6
- **Primary result:** Achieved 2nd place in SemEval 2024 Task 8 Subtask B with 86.95% accuracy

## Executive Summary
This paper presents the UniBuc - NLP team's approach to the SemEval 2024 Task 8, focusing on detecting machine-generated text across three challenging subtasks. The team developed transformer-based and hybrid deep learning architectures to tackle binary classification, multi-class classification, and token-level classification tasks. Their transformer-based model achieved strong performance in subtask B, placing second out of 77 teams, demonstrating the effectiveness of their approach for identifying the specific generator model. However, the models showed significant overfitting in subtasks A and C, highlighting the challenges of this task and the need for further optimization.

## Method Summary
The approach combines transformer-based and hybrid deep learning architectures. For transformer models, the team used RoBERTa-base as a feature extractor, concatenating either the last 4 layers or just the last layer, followed by fully connected layers. They employed a Head-First Fine-Tuning (HeFit) strategy with two training phases: an initial freeze phase where transformer weights are not updated, followed by a fine-tuning phase where selected layers are updated with smaller learning rates. For the hybrid approach, they combined character-level CNN features with word-level embeddings and Bidirectional LSTM layers, primarily used for the token-level classification task. The models were trained on the M4 dataset with a maximum sequence length of 512 tokens.

## Key Results
- Transformer-based model achieved 86.95% accuracy in subtask B, placing 2nd out of 77 teams
- Models showed significant overfitting in subtasks A and C, with performance degradation on validation data
- Performance dropped substantially for texts longer than 500-1500 tokens due to the 512 token limit
- The hybrid model for token-level classification (subtask C) experienced drastic overfitting on the dataset

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Transformer-based architectures can extract high-level contextual features that distinguish human and AI-generated patterns
- **Mechanism:** Uses pre-trained transformer models as feature extractors, freezing most layers initially to preserve general linguistic features, then fine-tuning selected layers for task-specific detection
- **Core assumption:** Lower transformer layers capture general token-level features while higher layers capture more contextualized patterns useful for distinguishing human vs. machine text
- **Evidence anchors:**
  - [abstract] "For subtask B, our transformer-based model achieved a strong second-place out of 77 teams with an accuracy of 86.95%, demonstrating the architecture's suitability for this task."
  - [section 3.1.2] "From our experiments, concatenating the last 4 layers and using only the last layer from the transformer proved to give the best results."
  - [corpus] Weak - no direct corpus evidence for this specific mechanism

### Mechanism 2
- **Claim:** The Head-First Fine-Tuning (HeFit) strategy prevents overfitting by first freezing transformer weights and only updating fully connected layers, then gradually fine-tuning selected layers
- **Mechanism:** Training is split into two phases - an initial freeze phase where only fully connected layers are updated, followed by a fine-tuning phase where selected transformer layers are updated with smaller learning rates
- **Core assumption:** Gradual adaptation of transformer weights allows the model to first learn task-specific patterns without destroying general linguistic knowledge
- **Evidence anchors:**
  - [section 3.1.3] "Using this strategy, we are not only using less resources, but we can also preserve the more general information of the transformer (freezing lower layers) and updating information that is most relevant to the downstream task (fine-tuning selected upper layers)."
  - [section 4.1] "Our error analysis revealed that overfitting remains a primary challenge, despite our initial precautions."
  - [corpus] Weak - no direct corpus evidence for this specific fine-tuning mechanism

### Mechanism 3
- **Claim:** The hybrid architecture combining character-level CNN features with word-level embeddings and LSTM layers is effective for token-level classification tasks like detecting transitions between human and machine-generated text
- **Mechanism:** Character-level features capture morphological patterns, word embeddings capture semantic meaning, and LSTM processes sequential dependencies to identify transition points
- **Core assumption:** Machine-generated text has distinctive character-level patterns and transition points that can be detected through multi-level feature extraction
- **Evidence anchors:**
  - [section 3.2] "This model was mainly used for the subtask C, which we treated as a token classification task."
  - [section 4.3] "Our results on the subtask C show that the model architecture we chose alongside the hyperparameters overfitted drastically on this dataset."
  - [corpus] Weak - no direct corpus evidence for this specific hybrid mechanism

## Foundational Learning

- **Concept:** Transformer architecture fundamentals (self-attention, positional encoding, layer stacking)
  - **Why needed here:** Understanding how transformers process text is crucial for selecting appropriate layers for feature extraction and designing fine-tuning strategies
  - **Quick check question:** How do different transformer layers capture different levels of linguistic information, and why would this matter for detecting machine-generated text?

- **Concept:** Transfer learning and fine-tuning strategies
  - **Why needed here:** The paper relies heavily on pre-trained models and specific fine-tuning approaches to adapt them for the detection task
  - **Quick check question:** What are the trade-offs between freezing all layers vs. fine-tuning all layers vs. selective fine-tuning strategies?

- **Concept:** Overfitting detection and prevention
  - **Why needed here:** The paper explicitly mentions overfitting as a primary challenge, particularly in subtasks A and C
  - **Quick check question:** How can you distinguish between genuine model performance degradation and overfitting based on training vs. validation metrics?

## Architecture Onboarding

- **Component map:** Input text → Tokenization (512 max tokens) → Transformer feature extraction (selected layers) → Dropout → Fully connected layers (2 layers: [512, 128] for subtask B) → Output layer (6 classes for subtask B) → Softmax
- **Critical path:** Text preprocessing → Feature extraction → Classification head → Prediction
- **Design tradeoffs:** Longer sequence length could improve performance but requires more computational resources; more layers for fine-tuning could improve accuracy but increases overfitting risk
- **Failure signatures:** Overfitting (high training accuracy but low validation/test accuracy), underfitting (poor performance across all metrics), class imbalance issues (systematic under-prediction of certain classes)
- **First 3 experiments:**
  1. Test different transformer base models (RoBERTa, BERT, DeBERTa) with frozen layers and only fully connected heads to establish baseline performance
  2. Experiment with different fine-tuning strategies (freeze k epochs, then fine-tune n layers) with varying learning rates to find optimal balance
  3. Test different sequence truncation strategies (head only, tail only, head+tail, hierarchical) to determine best approach for handling long texts within 512 token limit

## Open Questions the Paper Calls Out

- **Open Question 1:** Would increasing the maximum sequence length beyond 512 tokens improve model performance for detecting machine-generated text?
  - **Basis in paper:** [explicit] The authors mention that their models performed worst for texts with 500-1500 tokens, attributing this to the 512 token limit of their transformer models, and state that with more powerful machines they plan to increase the token length to 1024.
  - **Why unresolved:** The authors did not have sufficient computational resources to test longer sequences, leaving the actual impact of increased sequence length unknown.
  - **What evidence would resolve it:** Training and evaluating the same models with sequence lengths of 1024+ tokens on the same datasets would provide direct evidence of performance improvements.

- **Open Question 2:** Does using an ensemble of specialized models trained on text from individual generators (e.g., separate models for ChatGPT, Cohere, etc.) outperform a single general model for multi-class classification of machine-generated text?
  - **Basis in paper:** [explicit] The authors propose this as future work, noting that each specialized model would become adept at discerning text from its corresponding generator.
  - **Why unresolved:** The authors did not implement this approach, so comparative performance data is unavailable.
  - **What evidence would resolve it:** Implementing the ensemble approach and comparing its performance to the single-model approach on the same test data would provide a direct answer.

- **Open Question 3:** Can large language models (LLMs) like Mistral/Mixtral or Solar effectively detect machine-generated text using zero-shot or few-shot learning approaches?
  - **Basis in paper:** [explicit] The authors plan to investigate the efficacy of LLMs with zero-shot or few-shot learning for this task.
  - **Why unresolved:** This approach has not yet been tested by the authors, so its effectiveness remains unknown.
  - **What evidence would resolve it:** Testing LLMs on the same datasets using zero-shot and few-shot learning protocols and comparing results to traditional fine-tuned transformer models would provide evidence of their effectiveness.

## Limitations

- Models showed significant overfitting in subtasks A and C, limiting their effectiveness for binary and token-level classification tasks
- Performance degraded substantially for texts longer than 500-1500 tokens due to the 512 token limit, creating a U-shaped accuracy curve
- Limited details about CNN-character level feature implementation and exact hyperparameters for the hybrid model make complete reproduction challenging

## Confidence

**High Confidence:**
- Transformer-based models can effectively detect machine-generated text in multi-class classification tasks (subtask B)
- The two-phase fine-tuning strategy (HeFit) is effective at preventing overfitting when properly implemented
- Character-level features combined with word embeddings can capture distinctive patterns in machine-generated text

**Medium Confidence:**
- The specific architecture choices (last 4 layers vs. last layer only) are optimal for this task
- The observed overfitting issues can be resolved through less fine-tuning and increased sequence length
- The performance degradation at longer sequence lengths is primarily due to the 512 token limit rather than fundamental architectural limitations

**Low Confidence:**
- The hybrid architecture is fundamentally unsuitable for token-level classification tasks
- The proposed solutions (less fine-tuning, longer sequences) will definitively resolve the overfitting issues
- The 86.95% accuracy in subtask B represents a ceiling for transformer-based approaches on this problem

## Next Checks

1. **Cross-dataset validation:** Test the trained models on independent datasets beyond the M4 competition data to assess generalizability and determine if the strong performance in subtask B is dataset-specific or represents genuine capability.

2. **Sequence length sensitivity analysis:** Systematically evaluate model performance across varying sequence lengths (256, 512, 1024, 2048 tokens) to quantify the impact of the 512 token limit and determine if the U-shaped performance curve persists with architectural modifications.

3. **Fine-tuning strategy ablation study:** Conduct controlled experiments comparing different fine-tuning approaches (freeze all, fine-tune all, selective fine-tuning with varying learning rates) to isolate the specific factors contributing to overfitting in subtasks A and C and validate whether the proposed solutions would be effective.