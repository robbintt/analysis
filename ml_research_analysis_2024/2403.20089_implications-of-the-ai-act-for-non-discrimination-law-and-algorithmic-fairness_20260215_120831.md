---
ver: rpa2
title: Implications of the AI Act for Non-Discrimination Law and Algorithmic Fairness
arxiv_id: '2403.20089'
source_url: https://arxiv.org/abs/2403.20089
tags:
- fairness
- non-discrimination
- data
- bias
- discrimination
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper bridges non-discrimination law and algorithmic fairness,
  addressing their misalignment. It highlights how the EU's AI Act could link these
  fields by introducing fairness requirements at the design stage of AI models.
---

# Implications of the AI Act for Non-Discrimination Law and Algorithmic Fairness

## Quick Facts
- arXiv ID: 2403.20089
- Source URL: https://arxiv.org/abs/2403.20089
- Reference count: 15
- Key outcome: This paper bridges non-discrimination law and algorithmic fairness, addressing their misalignment through the EU's AI Act.

## Executive Summary
This paper examines how the EU's AI Act could bridge the gap between non-discrimination law and algorithmic fairness by introducing fairness requirements at the design stage of AI models. The authors identify key challenges in defining appropriate fairness metrics and determining when biases lead to discrimination, while proposing the application of technical fairness metrics to legal contexts. They demonstrate how statistical hypothesis testing, particularly z-tests, can detect bias but emphasize the need for clear standards in bias detection methods. The paper also explores how the AI Act's provisions for processing special categories of personal data enable bias detection while balancing privacy concerns.

## Method Summary
The paper employs a theoretical analysis combining legal interpretation of the AI Act with technical discussions of algorithmic fairness metrics and statistical hypothesis testing. The authors use z-tests to illustrate how sample size and acceptance rates impact bias detection capabilities, demonstrating the sensitivity of these tests to different parameters. They analyze the AI Act's provisions, particularly Art. 10(5), to show how legal frameworks can enable technical bias detection while addressing privacy concerns.

## Key Results
- The AI Act bridges non-discrimination law and algorithmic fairness by shifting responsibilities to the design stage of AI models
- Statistical hypothesis testing (z-tests) can detect bias, but error rates depend critically on sample size and acceptance rates
- The AI Act enables processing of special categories data for bias detection, resolving the tension between fairness and privacy requirements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The AI Act enables bias detection and correction by allowing the processing of special categories of personal data for bias mitigation in high-risk AI systems.
- Mechanism: The AI Act explicitly permits the processing of special categories of personal data (e.g., race, gender) under Art. 10(5) for the purpose of bias detection and correction, which is normally restricted under GDPR Art. 9. This legal exception enables the collection of sensitive data needed to measure and mitigate discriminatory outcomes.
- Core assumption: The legal framework allows for exceptions to data protection rules when there is a substantial public interest in preventing discrimination.
- Evidence anchors:
  - [abstract]: "The AI Act seeks to mitigate this tension by broadening the scope of lawful data processing. Art. 10(5) AI Act states that [...] providers of such systems may exceptionally process special categories of personal data referred to in Art. 9(1) [GDPR]."
  - [section]: "The AI Act seeks to mitigate this tension by broadening the scope of lawful data processing. Art. 10(5) AI Act states that [...] providers of such systems may exceptionally process special categories of personal data referred to in Art. 9(1) [GDPR]."
  - [corpus]: "It's complicated. The relationship of algorithmic fairness and non-discrimination provisions for high-risk systems in the EU AI Act" (no direct evidence on data processing exception, but relevant to the legal framing).
- Break condition: If courts interpret "substantial public interest" narrowly or if data protection authorities restrict the scope of this exception, the mechanism fails.

### Mechanism 2
- Claim: The AI Act shifts non-discrimination responsibilities from ex-post litigation to ex-ante design requirements, bridging the gap between non-discrimination law and algorithmic fairness.
- Mechanism: By requiring high-risk AI systems to undergo bias detection and correction during the design phase (Art. 10(2)f), the AI Act moves the focus from individual litigation after deployment to proactive fairness interventions during model development. This aligns technical fairness practices with legal non-discrimination obligations.
- Core assumption: The AI Act's design requirements are enforceable and translate into concrete technical standards for fairness.
- Evidence anchors:
  - [abstract]: "The AI Act might present a tremendous step towards bridging these two approaches by shifting non-discrimination responsibilities into the design stage of AI models."
  - [section]: "The AI Actprospectively demands fairness interventions by implementing non-discrimination requirements at the stage of model design."
  - [corpus]: "It's complicated. The relationship of algorithmic fairness and non-discrimination provisions for high-risk systems in the EU AI Act" (discusses the alignment of AI Act provisions with non-discrimination law).
- Break condition: If enforcement mechanisms are weak or if the AI Act's requirements are too vague to be operationalized, the mechanism fails.

### Mechanism 3
- Claim: Statistical hypothesis testing (e.g., z-tests) can be used to detect bias in AI systems, but the choice of test and error rates must be carefully calibrated to avoid false positives and false negatives.
- Mechanism: The paper demonstrates that z-tests can detect violations of demographic parity, but the probability of type 2 errors (failing to detect a violation) depends on sample size, acceptance rates, and the magnitude of disparities. This highlights the need for clear standards in bias detection methods.
- Core assumption: Statistical tests are a valid method for detecting bias in AI systems.
- Evidence anchors:
  - [abstract]: "The authors also illustrate the impact of acceptance rates and sample sizes on bias detection using z-tests, emphasizing the need for clear standards in bias detection methods."
  - [section]: "To test the hypothesis of compliance with demographic parity, we are interested in the test's error rates, i.e., falsely detecting a violation (type 1 error) or the likelihood of failing to detect a violation (type 2 error)."
  - [corpus]: No direct evidence on statistical testing methods, but "Formalising Anti-Discrimination Law in Automated Decision Systems" may discuss related technical approaches.
- Break condition: If the statistical tests are not appropriate for the context or if the error rates are not well-defined, the mechanism fails.

## Foundational Learning

- Concept: Statistical hypothesis testing (e.g., z-tests)
  - Why needed here: To detect violations of fairness metrics like demographic parity.
  - Quick check question: What is the difference between a type 1 error and a type 2 error in hypothesis testing?

- Concept: Legal vs. technical fairness
  - Why needed here: To understand how the AI Act bridges the gap between legal non-discrimination law and technical algorithmic fairness.
  - Quick check question: How does the AI Act's approach to fairness differ from traditional non-discrimination law?

- Concept: Data protection law (GDPR)
  - Why needed here: To understand the tension between fairness and privacy in AI systems and how the AI Act resolves it.
  - Quick check question: What are the implications of GDPR Art. 9 for bias detection and correction in AI systems?

## Architecture Onboarding

- Component map:
  - Legal framework (AI Act, GDPR, non-discrimination law)
  - Technical fairness metrics (e.g., demographic parity, equalized odds)
  - Bias detection methods (e.g., z-tests)
  - Bias correction techniques (e.g., reweighting, adversarial debiasing)

- Critical path:
  1. Identify high-risk AI systems subject to the AI Act.
  2. Determine the appropriate fairness metrics for the context.
  3. Collect and process special categories of personal data for bias detection.
  4. Apply statistical tests to detect bias.
  5. Implement bias correction techniques if necessary.
  6. Document compliance with the AI Act's requirements.

- Design tradeoffs:
  - Fairness vs. privacy: Balancing the need for sensitive data to detect bias with data protection requirements.
  - Precision vs. recall: Choosing fairness metrics and detection methods that minimize false positives and false negatives.
  - Ex-ante vs. ex-post: Shifting from individual litigation to proactive design interventions.

- Failure signatures:
  - High type 1 error rates: Too many false positives in bias detection.
  - High type 2 error rates: Too many false negatives in bias detection.
  - Non-compliance with data protection laws: Illegal processing of special categories of personal data.
  - Lack of enforceability: Vague or unenforceable AI Act requirements.

- First 3 experiments:
  1. Test the impact of sample size and acceptance rates on type 2 error rates using z-tests.
  2. Evaluate the effectiveness of different bias correction techniques (e.g., reweighting, adversarial debiasing) on a synthetic dataset.
  3. Analyze the trade-offs between fairness and privacy in a real-world high-risk AI system (e.g., hiring or credit scoring).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific technical fairness metrics should be used to evaluate bias in AI systems, and how should they be selected based on the intended purpose of the AI system?
- Basis in paper: [explicit] The paper discusses the challenge of defining appropriate fairness metrics, stating "the concept of technical fairness metrics begs the question of which one(s) may be 'appropriate for the intended purpose of the AI system'" (Art. 10(2)f AI Act).
- Why unresolved: The paper highlights the difficulty in selecting the right fairness metric for different contexts and the need for guidance in this area. It suggests that a single standard of fairness may not be achievable due to the heterogeneity of social contexts.
- What evidence would resolve it: A comprehensive framework or guidelines for selecting appropriate fairness metrics based on the specific context and purpose of the AI system, along with empirical studies demonstrating the effectiveness of these metrics in different scenarios.

### Open Question 2
- Question: How can the AI Act effectively balance the need for bias detection and correction with data protection requirements, particularly regarding the processing of special categories of personal data?
- Basis in paper: [explicit] The paper discusses the tension between fairness and privacy, stating "The AI Act seeks to mitigate this tension by broadening the scope of lawful data processing" and "balancing the public and private interests regarding non-discrimination and privacy will inevitably lead to intricate trade-offs."
- Why unresolved: The paper acknowledges the need to balance fairness and privacy concerns but does not provide specific guidance on how to achieve this balance in practice.
- What evidence would resolve it: Clear guidelines and case studies demonstrating how the AI Act can be implemented to achieve both bias detection/correction and data protection, along with empirical evidence on the effectiveness of these approaches.

### Open Question 3
- Question: What standards should be established for the error rates of statistical tests used in bias detection, and how should these standards vary based on sample size and acceptance rates?
- Basis in paper: [explicit] The paper discusses the impact of sample size and acceptance rates on the effectiveness of z-tests in detecting bias, stating "Our example highlights the need for guidance in selecting appropriate tests and specifying standards for the error rates of tests utilized in bias detection."
- Why unresolved: The paper demonstrates the sensitivity of z-tests to sample size and acceptance rates but does not provide specific standards or guidelines for acceptable error rates in different scenarios.
- What evidence would resolve it: Empirical studies determining appropriate error rate thresholds for different sample sizes and acceptance rates, along with guidelines for selecting and interpreting statistical tests in bias detection.

## Limitations
- The practical implementation and enforcement of the AI Act's provisions remain untested
- The translation between technical fairness metrics and legal discrimination contexts may face significant challenges
- The analysis relies on idealized assumptions about data quality and system design

## Confidence
Medium confidence due to uncertainties in legal interpretation, enforcement mechanisms, and the practical application of statistical testing methods to real-world AI systems.

## Next Checks
1. Conduct empirical analysis of actual AI Act implementation cases to verify how Art. 10(5) exceptions are being applied in practice
2. Test the proposed z-test methodology with real-world datasets from high-risk AI systems to assess Type I and Type II error rates in practice
3. Analyze case law from EU member states to determine how courts are interpreting the relationship between technical fairness metrics and legal discrimination standards