---
ver: rpa2
title: 'PaCoST: Paired Confidence Significance Testing for Benchmark Contamination
  Detection in Large Language Models'
arxiv_id: '2406.18326'
source_url: https://arxiv.org/abs/2406.18326
tags:
- contamination
- data
- arxiv
- benchmark
- confidence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of detecting benchmark contamination
  in large language models (LLMs), where training data may unintentionally include
  evaluation datasets, leading to inflated benchmark scores. The authors propose PaCoST,
  a Paired Confidence Significance Testing method that rephrases each benchmark instance
  and compares the model's confidence on original versus rephrased questions using
  statistical significance testing.
---

# PaCoST: Paired Confidence Significance Testing for Benchmark Contamination Detection in Large Language Models

## Quick Facts
- arXiv ID: 2406.18326
- Source URL: https://arxiv.org/abs/2406.18326
- Reference count: 27
- Primary result: Proposes PaCoST method that detects benchmark contamination in LLMs by comparing confidence on original vs. paraphrased questions using statistical significance testing

## Executive Summary
This paper addresses the critical problem of benchmark contamination in large language models, where training data may unintentionally include evaluation datasets, leading to inflated and unreliable benchmark scores. The authors propose PaCoST (Paired Confidence Significance Testing), a novel method that constructs paraphrased versions of benchmark questions and compares the model's confidence on original versus rephrased questions using statistical significance testing. The core assumption is that models exhibit higher confidence when answering questions they have been trained on. Experimental results show that PaCoST effectively detects contamination in intentionally poisoned models and finds suspected contamination in almost all tested open-source models and benchmarks, suggesting that current LLM evaluation practices are unreliable due to widespread benchmark contamination.

## Method Summary
PaCoST works through a three-step process: First, each benchmark instance is rephrased using a powerful LLM (Llama-2-Chat-7B) to create counterpart questions with the same meaning but different wording. Second, model confidence is estimated using the P(True) method, where the model is asked whether its answer is correct, providing a calibrated confidence measure that addresses overconfidence issues in direct probability outputs. Third, paired samples t-test is performed on confidence scores between original and rephrased questions to determine statistical significance (p < 0.05 indicates contamination). The method demonstrates stability across different sample sizes (100-1000) and successfully detects contamination in controlled experiments with intentionally poisoned models.

## Key Results
- PaCoST successfully detects contamination in intentionally poisoned models where 1000 out of 1400 samples were contaminated
- The method shows stable performance across sample sizes ranging from 100 to 1000 instances without generating false positives or negatives
- PaCoST finds suspected contamination in almost all tested open-source models and benchmarks, suggesting widespread benchmark contamination issues

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The model demonstrates higher confidence when answering questions it has been trained on.
- Mechanism: PaCoST constructs paraphrased versions of benchmark questions and compares the model's confidence on original versus rephrased questions using statistical significance testing.
- Core assumption: Models exhibit statistically significant differences in confidence levels between original and rephrased versions of questions depending on whether they were trained on those questions.
- Evidence anchors:
  - [abstract] "Our method constructs a counterpart for each piece of data with the same distribution, and performs statistical analysis of the corresponding confidence to test whether the model is significantly more confident under the original benchmark."
  - [section 4.3] "We use Paired Samples T-test to perform statistical analysis... If p < 0.05, we can confidently reject null hypothesis and choose the alternative hypothesis, which means the model is statistically significantly more confident when answering the original questions and this provides evidence for potential contamination."

### Mechanism 2
- Claim: P(True) confidence estimation method provides more reliable confidence scores than direct probability output.
- Mechanism: Instead of using raw probability distributions from the model's output, P(True) asks the model to verify whether its own answer is correct, providing a calibrated confidence measure.
- Core assumption: Direct probability outputs from LLMs suffer from overconfidence issues, making them unreliable for contamination detection.
- Evidence anchors:
  - [section 4.2] "Using probability distribution of the original output (P(M(x)|x, M) to estimate confidence often leads to overconfidence issues, resulting in unnaturally high confidence scores."
  - [section 4.2] "Therefore, we ultimately choose P(True) for its simplicity and effectiveness."

### Mechanism 3
- Claim: Statistical significance testing can reliably detect contamination patterns across different sample sizes.
- Mechanism: Paired t-tests compare confidence differences between original and rephrased questions across multiple samples to determine if contamination exists.
- Core assumption: Statistical tests can identify systematic differences in confidence patterns that indicate training on benchmark data.
- Evidence anchors:
  - [section 5.1] "Our method works properly with sample sizes ranging from 100 to 1000, without generating any false positives or false negatives."
  - [section 5.1] "This demonstrates the stability of our method across different sample sizes and highlights that it only requires a subset of the dataset to effectively detect contamination."

## Foundational Learning

- Concept: Statistical significance testing (Paired t-test)
  - Why needed here: To determine whether observed differences in confidence between original and rephrased questions are statistically significant rather than random variation
  - Quick check question: What p-value threshold is used to determine significant contamination?

- Concept: Confidence estimation in language models
  - Why needed here: To measure the model's certainty in its answers rather than just correctness, which is key for detecting contamination
  - Quick check question: Why does the paper prefer P(True) over direct probability outputs?

- Concept: Paraphrasing as data augmentation
  - Why needed here: To create comparable question pairs that share meaning but differ in surface form, allowing detection of training effects
  - Quick check question: What evaluation metrics are used to ensure paraphrasing quality?

## Architecture Onboarding

- Component map: Benchmark questions → Paraphrase generation → Original answer generation → Confidence estimation (original) → Rephrased answer generation → Confidence estimation (rephrased) → Statistical test
- Critical path: Question → Paraphrase generation → Original answer generation → Confidence estimation (original) → Rephrased answer generation → Confidence estimation (rephrased) → Statistical test
- Design tradeoffs: Using P(True) for confidence provides better calibration but requires additional model queries compared to direct probability extraction
- Failure signatures: High p-values despite contamination (false negatives), low p-values without contamination (false positives), poor paraphrasing quality affecting results
- First 3 experiments:
  1. Test contamination detection on intentionally poisoned models with known contamination
  2. Vary sample sizes to validate statistical stability across different dataset sizes
  3. Test different rephrasing models to ensure method robustness to paraphrasing quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum dataset size required for PaCoST to maintain statistical reliability while detecting benchmark contamination?
- Basis in paper: [inferred] from stability experiments showing performance across 100-1000 samples, but stating "datasets with fewer than 100 samples are rare, making the analysis of such scenarios less relevant"
- Why unresolved: The paper does not systematically test sample sizes below 100, which could be relevant for smaller benchmarks or when computational resources are limited
- What evidence would resolve it: Systematic experiments testing PaCoST performance on datasets ranging from 10 to 100 samples, measuring false positive/negative rates and statistical power

### Open Question 2
- Question: How does PaCoST perform when detecting contamination in multilingual benchmarks across different language families?
- Basis in paper: [inferred] from the focus on English benchmarks and the use of Llama-2-Chat-7B for rephrasing, with no mention of multilingual testing
- Why unresolved: The paper only tests English-language benchmarks and does not explore whether the method generalizes to other languages or requires language-specific adjustments
- What evidence would resolve it: Experiments applying PaCoST to benchmarks in multiple languages (e.g., XSum, MLQA, or multilingual MMLU) with appropriate rephrasing models for each language

### Open Question 3
- Question: Can PaCoST detect contamination when only a small fraction (e.g., 1-5%) of a benchmark is present in the training data?
- Basis in paper: [inferred] from the controlled experiments using 1000 out of 1400 samples for contamination, but no testing of lower contamination rates
- Why unresolved: The paper tests contamination with 1000/1400 samples (~71%) but does not explore detection sensitivity at lower contamination thresholds that might be more realistic
- What evidence would resolve it: Experiments systematically varying contamination rates from 1% to 50% of benchmark samples to determine the minimum detectable contamination level

### Open Question 4
- Question: How does the choice of rephrase model (Mp) affect PaCoST's contamination detection accuracy when using models with different capabilities or architectures?
- Basis in paper: [explicit] from the statement "We do not discuss samples with fewer than 100 instances for two reasons" and experiments showing stability between Llama and Mistral for rephrasing
- Why unresolved: While the paper shows PaCoST works with Llama and Mistral, it does not explore a broader range of rephrase models, including smaller models or those with different architectures
- What evidence would resolve it: Comparative experiments using rephrase models ranging from small (1B parameters) to large (70B parameters), including different architectures (decoder-only, encoder-decoder) and capabilities

### Open Question 5
- Question: What is the impact of different prompt engineering strategies on PaCoST's confidence estimation and contamination detection performance?
- Basis in paper: [inferred] from the use of a specific P(True) prompt and the observation that LLMs are sensitive to prompts, but no systematic exploration of prompt variations
- Why unresolved: The paper uses a fixed P(True) prompt but does not investigate how different prompt formulations, temperature settings, or few-shot examples might affect the reliability of contamination detection
- What evidence would resolve it: Ablation studies testing different prompt formulations, temperature values, and few-shot examples to measure their impact on detection accuracy and false positive/negative rates

## Limitations

- The method's effectiveness heavily depends on paraphrasing quality, which could introduce artifacts if rephrased questions deviate significantly from original meaning
- The statistical significance testing approach assumes normal distribution of confidence differences, which may not always be valid
- The reliance on a fixed p-value threshold of 0.05 without addressing multiple comparison corrections could lead to false positives

## Confidence

- High Confidence: The core mechanism that models exhibit different confidence patterns on training versus non-training data, and that statistical significance testing can detect these differences
- Medium Confidence: The claim that PaCoST can reliably detect contamination across all tested open-source models and benchmarks
- Low Confidence: The assertion that current LLM evaluation practices are "unreliable" due to widespread contamination

## Next Checks

1. **Cross-Architecture Validation**: Test PaCoST on models with different architectural designs (transformers, recurrent networks, etc.) and training approaches (supervised, RLHF, etc.) to verify that confidence differences are consistent across model types and not specific to the tested architectures.

2. **Controlled Contamination Analysis**: Systematically vary the degree of contamination (10%, 25%, 50%, 75%) in models with known benchmark data and measure how detection sensitivity changes. This would establish the method's detection threshold and help quantify the relationship between contamination levels and confidence differences.

3. **Benchmark Transferability Test**: Apply PaCoST to models trained on one benchmark and tested on different, semantically related benchmarks to determine whether the method can distinguish between genuine generalization and contamination from semantically similar training data.