---
ver: rpa2
title: Imbalanced Graph-Level Anomaly Detection via Counterfactual Augmentation and
  Feature Learning
arxiv_id: '2407.11082'
source_url: https://arxiv.org/abs/2407.11082
tags:
- samples
- graph
- datasets
- features
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of imbalanced graph-level anomaly
  detection (GLAD), where rare anomalous graphs lead models to learn normal patterns
  predominantly, reducing detection performance. The authors propose IGAD-CF, a method
  that combines counterfactual augmentation with adaptive feature learning.
---

# Imbalanced Graph-Level Anomaly Detection via Counterfactual Augmentation and Feature Learning

## Quick Facts
- **arXiv ID:** 2407.11082
- **Source URL:** https://arxiv.org/abs/2407.11082
- **Reference count:** 40
- **Primary result:** Achieves up to 99.71% AUC on AIDS dataset using counterfactual augmentation and adaptive feature learning for imbalanced graph-level anomaly detection

## Executive Summary
This paper addresses the challenge of imbalanced graph-level anomaly detection (GLAD), where rare anomalous graphs lead models to predominantly learn normal patterns, reducing detection performance. The authors propose IGAD-CF, a method that combines counterfactual augmentation with adaptive feature learning to balance the dataset and improve anomaly detection. The approach generates synthetic anomalous graphs by perturbing normal graphs using counterfactual learning, then applies a node feature learning module that combines node features with degree attributes for richer representations. An adaptive weight learning module assigns importance-based weights to different features, resulting in significant improvements over baseline methods across eight public datasets and real brain disorder datasets.

## Method Summary
IGAD-CF addresses imbalanced GLAD through a three-stage approach. First, counterfactual augmentation generates synthetic anomalous graphs by perturbing normal graphs, balancing the dataset. Second, a node feature learning module combines node features with degree attributes to create richer representations. Third, an adaptive weight learning module assigns importance-based weights to different features. The method leverages graph neural networks to learn representations from the augmented dataset, with the adaptive weights allowing the model to focus on the most discriminative features for anomaly detection. The overall architecture integrates these components to create a robust system for detecting rare anomalies in graph-structured data.

## Key Results
- Achieves up to 99.71% AUC on AIDS dataset and 87.95% AUC on BZR dataset
- Demonstrates significant improvements over baseline methods across eight public datasets
- Shows effectiveness on real brain disorder datasets, validating practical applicability

## Why This Works (Mechanism)
The method works by addressing the fundamental challenge of imbalanced learning in graph anomaly detection. Counterfactual augmentation generates synthetic anomalies that expose the model to a more balanced representation of normal and anomalous patterns during training. The node feature learning module enriches the graph representations by incorporating structural information (node degrees) alongside node features, creating more discriminative embeddings. The adaptive weight learning module dynamically assigns importance to different features based on their relevance to anomaly detection, allowing the model to focus on the most informative aspects of the graph structure. This multi-pronged approach ensures that the model learns to distinguish anomalies from normal patterns despite their rarity in the training data.

## Foundational Learning
- **Counterfactual augmentation** - Generating synthetic anomalies by perturbing normal graphs; needed to balance the training dataset and expose the model to diverse anomaly patterns; quick check: verify synthetic anomalies capture true anomaly characteristics
- **Graph neural networks** - Learning node and graph representations from structural and feature information; needed to encode graph topology and attributes into meaningful embeddings; quick check: ensure GNN layers capture both local and global graph structure
- **Adaptive feature weighting** - Dynamically assigning importance weights to different features during training; needed to focus learning on the most discriminative features for anomaly detection; quick check: validate weight distributions reflect feature importance for anomalies

## Architecture Onboarding

**Component Map:** Data Preprocessing -> Counterfactual Augmentation -> Node Feature Learning -> Adaptive Weight Learning -> Graph Neural Network -> Anomaly Detection

**Critical Path:** The critical path flows from counterfactual augmentation through node feature learning to adaptive weight learning, as these components directly address the imbalance problem and feature representation challenges that are central to the method's effectiveness.

**Design Tradeoffs:** The approach trades computational complexity for improved detection performance, as counterfactual augmentation and adaptive weighting add overhead to training. The method assumes that synthetic anomalies generated through perturbation capture real anomaly characteristics, which may not always hold. The adaptive weighting mechanism adds flexibility but may overfit to specific feature importance patterns in the training data.

**Failure Signatures:** The method may fail when perturbation strategies for counterfactual augmentation do not accurately represent true anomalies, leading to spurious patterns. Performance may degrade if the adaptive weighting mechanism incorrectly assigns importance to irrelevant features. The approach could also struggle with very large graphs where computational complexity becomes prohibitive.

**3 First Experiments:**
1. Test counterfactual augmentation effectiveness by comparing performance with and without synthetic anomaly generation
2. Evaluate the contribution of node degree features by comparing models with and without degree information in the feature learning module
3. Assess adaptive weight learning by comparing performance with fixed versus learned feature weights

## Open Questions the Paper Calls Out
None

## Limitations
- Performance heavily depends on the quality of synthetic anomalous graphs generated through counterfactual augmentation
- Assumes feature importance can be effectively captured through learned weights, which may not hold for all graph structures
- Evaluation focuses primarily on AUC scores, which may not capture practical performance where false positives and negatives have different costs

## Confidence
- **High Confidence:** Core methodology combining counterfactual augmentation with adaptive feature learning is technically sound and experimental results are reproducible
- **Medium Confidence:** Robustness across different graph structures and domains is supported but generalizability to completely unseen scenarios remains uncertain
- **Low Confidence:** Claims of consistently high performance (99.71% AUC) across all imbalanced GLAD scenarios may be overstated due to data characteristic variations

## Next Checks
1. Conduct detailed ablation studies to quantify individual contributions of counterfactual augmentation, node feature learning, and adaptive weight learning modules
2. Evaluate model performance on datasets with varying imbalance ratios and different anomaly types to assess generalizability
3. Analyze computational complexity and runtime performance compared to baseline methods, particularly for larger graphs