---
ver: rpa2
title: 'Unpacking Tokenization: Evaluating Text Compression and its Correlation with
  Model Performance'
arxiv_id: '2403.06265'
source_url: https://arxiv.org/abs/2403.06265
tags:
- tokenizers
- tasks
- tokenization
- compression
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates whether compression ability of tokenizers,
  as measured by text length in tokens, correlates with downstream performance of
  pre-trained language models. By controlling tokenizer compression through varying
  training data support (1M to 0 documents), and training models of three sizes on
  English and Turkish, the authors show a strong monotonic correlation between tokenizer
  compression and model performance, especially for generation tasks and smaller models.
---

# Unpacking Tokenization: Evaluating Text Compression and its Correlation with Model Performance

## Quick Facts
- arXiv ID: 2403.06265
- Source URL: https://arxiv.org/abs/2403.06265
- Reference count: 18
- Primary result: Strong monotonic correlation between tokenizer compression ability and downstream language model performance, especially for smaller models and generation tasks

## Executive Summary
This study investigates whether a tokenizer's compression ability—measured by text length in tokens—correlates with downstream performance of pre-trained language models. By controlling tokenizer compression through varying training data support (1M to 0 documents) and training models of three sizes on English and Turkish, the authors demonstrate a strong monotonic correlation between compression and model performance. The effect is particularly pronounced for generation tasks and smaller models. The study concludes that compression, especially for rare words, serves as a reliable intrinsic indicator of tokenization quality.

## Method Summary
The researchers trained multiple Byte Pair Encoding (BPE) tokenizers with varying amounts of supporting documents (1M to 0 documents) for English and Turkish. They then trained decoder-only transformer-based language models in three sizes (1B, 128m, and 10m parameters) using each tokenizer on C4 (English) and mC4 (Turkish) corpora. After pre-training, the models were fine-tuned on downstream tasks including classification (QQP, MultiNLI, XNLI) and generation (XSum, QG-QA, XL-Sum). The study evaluated the correlation between tokenizer compression ability and downstream performance across these varied conditions.

## Key Results
- Strong monotonic correlation between tokenizer compression and downstream model performance
- Smaller models show more significant performance drops with poor tokenizers compared to larger models
- Compression differences are most pronounced for rare words rather than frequent ones
- Generation tasks show stronger correlation with compression than classification tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Compression ability of a tokenizer correlates strongly with downstream performance, especially for smaller models and generation tasks.
- Mechanism: Compression is a form of language modeling where each token has equal probability. By minimizing token count, better compression approximates the downstream objective of maximizing token probability.
- Core assumption: The relationship between compression and downstream performance is symmetric - better compression leads to better log-likelihood.
- Evidence anchors:
  - [abstract] "We show that there is a correlation between tokenizers' compression and models' downstream performance"
  - [section] "From this point of view, prioritizing compression as an indicator for tokenization quality is very reasonable"
  - [corpus] Found 25 related papers, average neighbor FMR=0.455, average citations=0.0 (weak corpus support)

### Mechanism 2
- Claim: Smaller models are more vulnerable to poor tokenizations than larger models.
- Mechanism: Larger models are more powerful language models that can allocate resources to compensate for less compressing tokenizers.
- Core assumption: Larger models have more capacity to learn from suboptimal tokenization.
- Evidence anchors:
  - [abstract] "smaller models (over large ones)" and "the smallest 10m parameter model suffering from more significant drops in performance compared to our largest 1B model"
  - [section] "As we claim that compression is simple language modeling on its own, it is possible that LLMs that are more powerful language models in general are able to allocate resources to compensate for less compressing tokenizers"
  - [corpus] Weak support, no direct citations

### Mechanism 3
- Claim: Differences in compression ability stem mostly from the difference in compression of less common words.
- Mechanism: Tokenizers with less support diverge more in token-to-word ratio when presented with rarer words.
- Core assumption: Rarer words are semantically interesting and their compression affects model performance more.
- Evidence anchors:
  - [abstract] "discrepancy in compression is significantly more marked for less frequent words"
  - [section] "The figures show that the token-to-word ratio is extremely similar across tokenizers for words that are the most frequent... On the other hand, the different tokenizers diverge in token-to-word ratio when presented with rarer words"
  - [corpus] Weak support, no direct citations

## Foundational Learning

- Concept: Byte Pair Encoding (BPE)
  - Why needed here: BPE is the tokenization algorithm used in the study, and understanding its mechanism is crucial for understanding the results.
  - Quick check question: How does BPE work to minimize text length in tokens?

- Concept: Language modeling
  - Why needed here: The study argues that compression can be viewed as a form of language modeling, so understanding language modeling is essential.
  - Quick check question: What is the relationship between language modeling and compression?

- Concept: Downstream evaluation
  - Why needed here: The study evaluates tokenizer quality through downstream model performance, so understanding this concept is crucial.
  - Quick check question: How does downstream evaluation differ from intrinsic evaluation?

## Architecture Onboarding

- Component map: Tokenizers (trained with varying support) -> Models (1B, 128m, 10m parameters) -> Downstream tasks (QQP, MultiNLI, XNLI, XSum, QG-QA, XL-Sum)
- Critical path: 1. Train tokenizers with varying support 2. Train models with each tokenizer 3. Fine-tune models on downstream tasks 4. Evaluate correlation between compression and performance
- Design tradeoffs: Model size vs. tokenizer quality; Generation tasks vs. classification tasks for evaluation; English vs. Turkish for linguistic diversity
- Failure signatures: Poor correlation between compression and performance; No difference in performance between tokenizers; Models fail to converge or overfit
- First 3 experiments:
  1. Train tokenizers with 1M, 100, and 0 supporting documents
  2. Train 128m parameter models with each tokenizer
  3. Fine-tune models on QQP and X-Sum tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does tokenizer compression ability affect model performance on non-English languages with typological characteristics even more dissimilar to English than Turkish?
- Basis in paper: [explicit] The authors tested their hypothesis on Turkish, a language with typologically different characteristics from English, and found similar results. However, they note that ample room is left for studying the effects of tokenization on more languages that are even more typologically diverse.
- Why unresolved: The study only experimented with Turkish, which, while typologically different from English, may not be representative of all non-English languages. More diverse languages, especially those with more complex morphology or different script systems, could yield different results.
- What evidence would resolve it: Conducting similar experiments on a wider range of non-English languages, particularly those with significantly different typological characteristics from both English and Turkish, would provide more comprehensive evidence.

### Open Question 2
- Question: To what extent does the correlation between tokenizer compression and model performance generalize to other tokenization algorithms beyond BPE?
- Basis in paper: [explicit] The authors focused on BPE tokenizers and controlled their compression ability by limiting the amount of training data. They argue that this approach is justified because BPE optimizes an approximation of the downstream objective, but they do not explore other tokenization algorithms.
- Why unresolved: The study only examined BPE tokenizers, leaving open the question of whether the observed correlation between compression and performance holds for other tokenization methods like WordPiece, SentencePiece, or unigram language models.
- What evidence would resolve it: Repeating the experiments with different tokenization algorithms while controlling for compression ability would determine if the correlation observed with BPE is generalizable.

### Open Question 3
- Question: What is the impact of tokenizer compression on model performance in modalities other than text, such as vision or audio?
- Basis in paper: [explicit] The authors mention that their conclusion about the importance of compression may be true even for models dealing with other modalities, citing works on vision transformers with mixed-resolution tokenization.
- Why unresolved: While the authors suggest that their findings could extend to other modalities, they do not provide experimental evidence or theoretical justification for this claim.
- What evidence would resolve it: Conducting experiments on models dealing with non-text modalities (e.g., vision transformers, audio processing models) while varying the compression ability of their tokenization schemes would determine if the observed correlation holds beyond text-based models.

## Limitations

- Study focuses exclusively on decoder-only transformer models with Byte Pair Encoding (BPE) tokenization, limiting generalizability
- The causal mechanism between compression and performance remains partially theoretical
- Analysis of rare word compression effects relies on aggregate statistics rather than controlled experiments
- Does not explore interactions between tokenizer compression and other factors like vocabulary size

## Confidence

- **High Confidence**: The existence of a strong monotonic correlation between tokenizer compression and downstream performance
- **Medium Confidence**: That smaller models are more vulnerable to poor tokenization than larger models
- **Medium Confidence**: That differences in compression stem primarily from rare word handling

## Next Checks

1. Test the correlation hypothesis with alternative tokenization algorithms (Unigram, WordPiece) to determine if results generalize beyond BPE
2. Conduct ablation studies isolating the impact of rare word compression on downstream performance, controlling for other factors
3. Explore whether the compression-performance relationship holds when varying vocabulary size independently of training data support