---
ver: rpa2
title: 'VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to
  Speech Synthesizers'
arxiv_id: '2406.05370'
source_url: https://arxiv.org/abs/2406.05370
tags:
- speech
- vall-e
- code
- codec
- sequence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces VALL-E 2, a neural codec language model
  that achieves human parity zero-shot text-to-speech synthesis (TTS) for the first
  time. Building on VALL-E, VALL-E 2 incorporates two key improvements: repetition
  aware sampling, which enhances decoding stability by adaptively switching between
  random and nucleus sampling based on token repetition; and grouped code modeling,
  which partitions codec codes into groups to shorten sequence length, boost inference
  speed, and mitigate long context modeling issues.'
---

# VALL-E 2: Neural Codec Language Models are Human Parity Zero-Shot Text to Speech Synthesizers

## Quick Facts
- **arXiv ID**: 2406.05370
- **Source URL**: https://arxiv.org/abs/2406.05370
- **Reference count**: 9
- **Primary result**: Achieves human parity zero-shot text-to-speech synthesis for the first time

## Executive Summary
VALL-E 2 is a neural codec language model that achieves human parity in zero-shot text-to-speech synthesis. Building on VALL-E, it incorporates two key improvements: repetition aware sampling for enhanced decoding stability and grouped code modeling for faster inference and better long context handling. Trained on the Libriheavy dataset, VALL-E 2 demonstrates superior performance on LibriSpeech and VCTK datasets in robustness, naturalness, and speaker similarity, surpassing previous systems and even ground truth samples in certain metrics.

## Method Summary
VALL-E 2 builds upon the neural codec language model framework of VALL-E, introducing two significant improvements: repetition aware sampling and grouped code modeling. The model is trained on the large-scale Libriheavy dataset and evaluated on LibriSpeech and VCTK datasets. These enhancements address key challenges in zero-shot TTS, including decoding stability, inference speed, and long context modeling, resulting in a system that achieves human parity performance across multiple evaluation metrics.

## Key Results
- Achieved human parity zero-shot TTS performance on LibriSpeech and VCTK datasets
- Outperformed previous systems in robustness, naturalness, and speaker similarity
- Surpassed ground truth samples in key metrics such as WER, DNSMOS, and subjective evaluations

## Why This Works (Mechanism)
VALL-E 2 achieves human parity through its neural codec language model architecture, which learns to generate speech from text by modeling discrete acoustic tokens. The model leverages the power of large-scale pretraining on diverse speech data to capture rich acoustic patterns and speaker characteristics. By using a codec-based approach, VALL-E 2 can efficiently represent and generate high-quality speech with minimal artifacts. The two key innovations - repetition aware sampling and grouped code modeling - further enhance the model's ability to produce stable, natural-sounding speech across diverse inputs and contexts.

## Foundational Learning
- **Neural Codec Language Models**: These models learn to generate speech by predicting discrete acoustic tokens, enabling efficient representation and synthesis of high-quality audio. Understanding this concept is crucial as it forms the basis of VALL-E 2's approach to zero-shot TTS.
- **Zero-shot TTS**: This refers to the ability to generate speech for speakers or styles not seen during training. It's important because VALL-E 2 aims to achieve human parity in this challenging setting, demonstrating the model's generalization capabilities.
- **Repetition Aware Sampling**: This technique adaptively switches between random and nucleus sampling based on token repetition, improving decoding stability. It's a key innovation in VALL-E 2 that addresses common issues in autoregressive speech generation.
- **Grouped Code Modeling**: This approach partitions codec codes into groups to shorten sequence length and boost inference speed. It's crucial for mitigating long context modeling issues and enabling efficient real-time speech synthesis.

## Architecture Onboarding

**Component Map**: Text Input -> Tokenizer -> Neural Codec Language Model -> Codec Decoder -> Speech Output

**Critical Path**: The critical path in VALL-E 2 involves text tokenization, encoding through the neural codec language model, and decoding to speech output. The model's efficiency and quality depend on the interplay between these components, particularly the codec-based representation and the improvements in sampling and code modeling.

**Design Tradeoffs**: VALL-E 2 prioritizes achieving human parity in zero-shot TTS over computational efficiency. The use of a large-scale dataset (Libriheavy) and the implementation of grouped code modeling reflect a tradeoff between model capacity and inference speed. The repetition aware sampling technique balances between diversity and stability in speech generation.

**Failure Signatures**: Potential failure modes include mispronunciations for rare words or names, difficulty handling extremely long or complex sentences, and challenges in maintaining speaker consistency across very long utterances. The model may also struggle with highly expressive or emotional speech that requires nuanced prosodic control.

**First Experiments**: 1) Evaluate VALL-E 2's performance on out-of-domain text to assess generalization capabilities. 2) Compare the model's output with ground truth samples for speaker similarity and naturalness. 3) Test the model's robustness by synthesizing speech with challenging linguistic features (e.g., homophones, homographs).

## Open Questions the Paper Calls Out
None

## Limitations
- The claim of human parity relies heavily on subjective evaluation results, which may be influenced by sample selection and rater bias
- Generalizability to diverse, real-world applications remains untested beyond LibriSpeech and VCTK datasets
- The computational requirements and inference speed improvements are not thoroughly quantified for practical deployment scenarios

## Confidence
- **High**: VALL-E 2 outperforms previous TTS systems on LibriSpeech and VCTK datasets in objective metrics (WER, DNSMOS)
- **Medium**: VALL-E 2 achieves human parity based on subjective evaluations, though evaluation methodology transparency is limited
- **Medium**: Technical improvements (repetition aware sampling, grouped code modeling) enhance stability and efficiency, but individual contributions are not fully isolated

## Next Checks
1. Conduct cross-dataset evaluation to assess generalizability beyond LibriSpeech and VCTK, including diverse speaker demographics and acoustic environments
2. Perform detailed ablation studies to quantify the individual contributions of repetition aware sampling and grouped code modeling to overall performance
3. Evaluate real-time inference speed and computational requirements for practical deployment scenarios, comparing against established TTS systems