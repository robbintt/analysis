---
ver: rpa2
title: 'AiGAS-dEVL: An Adaptive Incremental Neural Gas Model for Drifting Data Streams
  under Extreme Verification Latency'
arxiv_id: '2407.05379'
source_url: https://arxiv.org/abs/2407.05379
tags: []
core_contribution: The paper introduces AiGAS-dEVL, an adaptive incremental neural
  gas model designed for classification in drifting data streams with extreme verification
  latency, where labeled data is unavailable. AiGAS-dEVL leverages Growing Neural
  Gas (GNG) to characterize and track evolving data distributions, using a matching
  and projection mechanism to adapt to gradual concept drift.
---

# AiGAS-dEVL: An Adaptive Incremental Neural Gas Model for Drifting Data Streams under Extreme Verification Latency

## Quick Facts
- arXiv ID: 2407.05379
- Source URL: https://arxiv.org/abs/2407.05379
- Authors: Maria Arostegi; Miren Nekane Bilbao; Jesus L. Lobo; Javier Del Ser
- Reference count: 40
- The paper introduces AiGAS-dEVL, an adaptive incremental neural gas model designed for classification in drifting data streams with extreme verification latency, where labeled data is unavailable.

## Executive Summary
This paper introduces AiGAS-dEVL, an adaptive incremental neural gas model for classification in drifting data streams with extreme verification latency (EVL), where labeled data is unavailable during the unsupervised period. The model leverages Growing Neural Gas (GNG) to characterize and track evolving data distributions without labels, using a matching and projection mechanism to adapt to gradual concept drift. AiGAS-dEVL predicts labels for unlabeled data based on nearest neighbor assignments to GNG nodes, which are updated incrementally as new data arrives.

Experiments on synthetic and real-world datasets demonstrate that AiGAS-dEVL outperforms state-of-the-art methods, achieving superior adaptability and competitive performance in terms of prequential error and macro F1 score, particularly for non-corpuscular data shapes and complex drift dynamics.

## Method Summary
AiGAS-dEVL addresses the challenge of classification in drifting data streams with extreme verification latency by using Growing Neural Gas (GNG) to incrementally characterize data distribution over time. The model employs a matching and projection mechanism based on rigid transformations to track gradual concept drift. Labels for new GNG nodes are assigned through nearest-neighbor voting from the previous node distribution, and incoming data instances are classified using nearest-neighbor voting on projected nodes. The approach is evaluated using macro F1 score and prequential error on synthetic and real-world datasets.

## Key Results
- AiGAS-dEVL outperforms state-of-the-art methods in extreme verification latency scenarios
- Superior adaptability to non-corpuscular data shapes and complex drift dynamics
- Competitive performance in terms of prequential error and macro F1 score

## Why This Works (Mechanism)

### Mechanism 1
AiGAS-dEVL uses Growing Neural Gas (GNG) to incrementally characterize data distribution over time, enabling concept drift tracking without labels. GNG nodes adapt to incoming stream data by competitive Hebbian learning, forming a topology-preserving map. Labels for new nodes are inferred by nearest-neighbor matching to previous node distributions. Core assumption: Gradual concept drift allows GNG nodes to be mapped between consecutive batches using rigid transformations (translation + rotation).

### Mechanism 2
AiGAS-dEVL assigns labels to GNG nodes in unlabeled streams by nearest-neighbor voting from the previous node distribution. For each new GNG node, find its K nearest neighbors among the projected nodes from the previous batch; the majority label among these neighbors becomes the new node's label. Core assumption: The nearest-neighbor relationship between consecutive node distributions remains meaningful under gradual drift.

### Mechanism 3
AiGAS-dEVL adapts to evolving data distributions by estimating and applying a rigid transformation between consecutive GNG node sets. Compute minimum-cost assignment between node sets, then fit translation vector t and rotation matrix R to align previous and current nodes. Apply this transform to project previous nodes onto the current space for classification. Core assumption: Concept drift dynamics are well-approximated by rigid transformations (translation + rotation).

## Foundational Learning

- Concept: Growing Neural Gas (GNG)
  - Why needed here: GNG provides incremental, topology-preserving clustering that adapts to evolving data distributions without requiring pre-defined cluster numbers or shapes.
  - Quick check question: What are the two key parameters GNG updates during node adaptation, and how do they differ from standard competitive learning?

- Concept: Minimum-cost assignment (rectangular assignment problem)
  - Why needed here: Maps nodes from one batch to the next to establish correspondence for drift estimation and label propagation.
  - Quick check question: In the assignment matrix A, what constraints ensure each node in the new batch is matched to at most one node in the previous batch?

- Concept: Rigid point set registration (Kabsch algorithm)
  - Why needed here: Estimates the geometric transformation (rotation + translation) between consecutive node distributions to project nodes forward in time.
  - Quick check question: What is the computational complexity of the Kabsch algorithm for aligning two point sets of size G?

## Architecture Onboarding

- Component map:
  - Data Ingestion -> GNG Core -> Label Assignment -> Drift Estimation -> Projection -> Classification

- Critical path:
  1. Receive batch xb
  2. Predict labels for xb using projected nodes from b-1
  3. Update GNG with xb to generate new nodes
  4. Assign labels to new nodes via NN voting
  5. Compute assignment matrix between previous and current nodes
  6. Estimate rigid transformation (R, t)
  7. Project previous nodes using (R, t)
  8. Output predictions; repeat

- Design tradeoffs:
  - Rigid transformation assumption simplifies computation but limits adaptability to non-linear drifts
  - Single GNG instance reduces memory but may underrepresent minority classes in unbalanced data
  - Nearest-neighbor voting is interpretable but sensitive to concept collisions

- Failure signatures:
  - Sudden performance drops coinciding with concept collisions in feature space
  - Gradual performance degradation when drift becomes non-rigid
  - Label propagation errors when concepts from different classes occupy same regions

- First 3 experiments:
  1. Run AiGAS-dEVL on a synthetic rectilinear drift dataset; verify that rigid transformation accurately tracks node movement and maintains high classification accuracy.
  2. Introduce a sudden non-rigid drift (e.g., rotation + scaling) in a controlled dataset; measure performance drop and identify when rigid projection fails.
  3. Test on an imbalanced dataset; adjust initial GNG node count using the Î¾ parameter and observe impact on minority class representation and overall accuracy.

## Open Questions the Paper Calls Out

### Open Question 1
How can AiGAS-dEVL be adapted to handle non-linear and non-rigid concept drift dynamics beyond the current rigid transformation approach? The paper identifies that the current rigid transformation model for concept drift may not suffice for non-linear warps in the evolution of GNG concepts, suggesting the need for alternative projections such as polynomial warping or thin plate spline transformations. Experimental results demonstrating improved performance using alternative non-rigid transformation models would resolve this.

### Open Question 2
How can AiGAS-dEVL incorporate a memory mechanism to improve the assignment of labels to newly discovered GNG nodes, particularly in scenarios where concepts belonging to different classes coexist in the same feature space? The paper identifies the lack of memory in the assignment of labels to new GNG nodes as a limitation, noting that this can lead to mismatches between concepts and labels. Empirical results showing that incorporating a memory mechanism leads to more accurate label assignments would resolve this.

### Open Question 3
How can AiGAS-dEVL autonomously tune the trade-off between stability and plasticity in its adaptation mechanisms to optimize performance across varying drift dynamics and noise levels? The paper discusses the need for a balance between stability (robustness against small drifts and noise) and plasticity (adaptability to sharply changing concepts) in the assignment of labels to GNG nodes, suggesting this as a future research priority. Experimental results demonstrating that an autonomous tuning mechanism leads to consistently improved performance would resolve this.

## Limitations
- The rigid transformation assumption may not hold for non-linear or non-rigid drift dynamics, limiting adaptability.
- Label propagation through nearest-neighbor voting can lead to error propagation when concepts collide or overlap between batches.
- Scalability concerns arise as computational complexity of minimum-cost assignment and rigid transformation estimation increases with node count.

## Confidence

- High Confidence: The use of GNG for incremental clustering and the overall architecture of AiGAS-dEVL are well-specified and grounded in established methods.
- Medium Confidence: The effectiveness of the rigid transformation projection mechanism is supported by experiments but may not generalize to all drift types.
- Low Confidence: The robustness of label propagation under concept collisions and the model's behavior on imbalanced datasets are not thoroughly validated.

## Next Checks

1. Test on non-rigid drift: Introduce controlled non-linear drift (e.g., scaling + rotation) in a synthetic dataset and measure the performance drop to quantify the impact of the rigid transformation assumption.
2. Evaluate collision resilience: Create a dataset where concepts from different classes collide in the feature space; observe how errors propagate and whether the model can recover without explicit collision detection.
3. Assess scalability: Benchmark AiGAS-dEVL on high-dimensional data streams with increasing numbers of GNG nodes to identify performance bottlenecks in the assignment and transformation steps.