---
ver: rpa2
title: Large Language Model for Participatory Urban Planning
arxiv_id: '2402.17161'
source_url: https://arxiv.org/abs/2402.17161
tags:
- residents
- planning
- area
- areas
- plan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a large language model (LLM)-based framework
  for participatory urban planning that simulates human-like agents to collaboratively
  generate land-use plans. The method uses an LLM planner to propose initial plans
  and LLM-resident agents with diverse profiles to discuss and provide feedback on
  local needs.
---

# Large Language Model for Participatory Urban Planning

## Quick Facts
- arXiv ID: 2402.17161
- Source URL: https://arxiv.org/abs/2402.17161
- Reference count: 40
- Key outcome: LLM-based framework simulates human-like agents to generate land-use plans, achieving up to 78.7% satisfaction and 79.0% inclusion metrics in Beijing regions

## Executive Summary
This paper introduces a large language model (LLM)-based framework for participatory urban planning that simulates human-like agents to collaboratively generate land-use plans. The method uses an LLM planner to propose initial plans and LLM-resident agents with diverse profiles to discuss and provide feedback on local needs. A fishbowl discussion mechanism enables efficient multi-round communication among thousands of residents. Experiments on two Beijing regions show the method outperforms baselines in residents' satisfaction (up to 78.7%) and inclusion (up to 79.0%) while maintaining competitive service accessibility and ecology metrics. The approach demonstrates how LLMs can effectively incorporate diverse resident needs into urban planning decisions.

## Method Summary
The framework partitions urban regions into communities and generates 1000 LLM-resident agents with diverse profiles based on real demographic data. The planner (GPT-4V) observes the entire region and proposes an initial land-use plan, while residents (GPT-3.5-turbo) discuss in a fishbowl mechanism over multiple rounds. After each discussion round, the planner revises the plan based on aggregated feedback. The process iterates through all communities, with evaluation metrics including service accessibility, ecology, resident satisfaction, and inclusion of marginalized groups' needs.

## Key Results
- Outperformed baselines in satisfaction metrics (up to 78.7%) and inclusion (up to 79.0%)
- Maintained competitive service accessibility and ecology metrics
- Demonstrated effectiveness of multi-agent collaboration with diverse resident profiles
- Showed fishbowl discussion mechanism enables efficient multi-round communication

## Why This Works (Mechanism)

### Mechanism 1
The fishbowl discussion mechanism enables efficient multi-round communication among thousands of residents without exceeding context limits. Residents are divided into inner and outer circles where only inner circle participants discuss each round. After each round, roles are randomly exchanged and discussion history is summarized. This reduces the number of active participants per round while maintaining broad participation over multiple rounds. The core assumption is that summarization can effectively preserve discussion content without losing critical feedback, and role rotation ensures fair participation.

### Mechanism 2
Multi-agent collaboration with LLM residents allows the system to incorporate diverse resident needs into urban planning decisions. LLM agents are created with diverse profiles (age, gender, family size, special backgrounds) and asked to express needs. The planner then revises plans based on aggregated resident feedback, balancing conflicting needs through discussion. The core assumption is that LLM role-play can effectively simulate realistic human preferences and that diverse profiles lead to meaningfully different expressed needs.

### Mechanism 3
The planner's global perspective combined with residents' local perspectives creates better land-use plans than either approach alone. The planner has global view of entire region and expert knowledge, while residents only see their immediate neighborhood and express local needs. Through iterative discussion and revision, local needs are incorporated into globally coherent plans. The core assumption is that local knowledge from residents contains information the planner cannot easily derive from global analysis alone.

## Foundational Learning

- **Large Language Models as role-play agents**
  - Why needed here: The system needs to simulate thousands of residents with diverse characteristics and preferences without requiring actual human participants
  - Quick check question: What prompt engineering technique ensures LLM agents maintain consistent profiles throughout the discussion?

- **Multi-round deliberative processes**
  - Why needed here: Single-round feedback would be insufficient for balancing diverse and potentially conflicting resident needs across a large community
  - Quick check question: How does the system determine when to stop discussion rounds?

- **Need-aware vs need-agnostic metrics**
  - Why needed here: Different evaluation criteria are required to assess whether plans satisfy both general accessibility requirements and specific resident preferences
  - Quick check question: What would happen to satisfaction metrics if the system only optimized for need-agnostic metrics?

## Architecture Onboarding

- **Component map:** GPT-4V planner agent (global view, multimodal input) -> GPT-3.5-turbo resident agents (local views, diverse profiles) -> Fishbowl discussion orchestrator (manages round-based communication) -> Plan revision engine (incorporates feedback into land-use decisions) -> Evaluation metrics module (computes service, ecology, satisfaction, inclusion)

- **Critical path:** 1. Planner generates initial land-use plan 2. Residents provide neighborhood observations and needs 3. Fishbowl discussion occurs over multiple rounds 4. Planner revises plan based on aggregated feedback 5. Process repeats for each community 6. Final plan evaluated on all metrics

- **Design tradeoffs:** Using separate LLM models (GPT-4V vs GPT-3.5-turbo) balances capability needs with cost; Fishbowl discussion trades depth of individual participation for breadth of community coverage; Role-play simplification enables scalability but may miss nuanced human behaviors

- **Failure signatures:** Satisfaction metrics remain low despite discussion (feedback not being incorporated); Discussion summaries become too generic (loss of critical information); Plan revisions oscillate without convergence (poor aggregation of conflicting feedback)

- **First 3 experiments:** 1. Test fishbowl mechanism with small resident set to verify role rotation and summarization work correctly 2. Validate planner's ability to incorporate specific resident feedback into plan revisions 3. Measure sensitivity of satisfaction metrics to different numbers of discussion rounds

## Open Questions the Paper Calls Out

### Open Question 1
How does the framework scale to regions with significantly larger populations (e.g., 100,000+ residents) and more complex urban structures? The paper only tests on two regions with 1000 agents each and doesn't provide evidence of performance on larger scales.

### Open Question 2
How robust is the framework to variations in the quality and quantity of input data (e.g., demographics, land use requirements)? The paper mentions agent profiles are generated from census data but doesn't explore the impact of data quality or quantity on performance.

### Open Question 3
How does the framework handle conflicts between residents' needs that cannot be easily resolved through discussion (e.g., opposing views on land use)? The paper mentions the fishbowl discussion balances needs but doesn't address irreconcilable conflicts.

## Limitations

- Reliance on synthetic LLM residents rather than actual human participants introduces uncertainty about how well simulated preferences match real-world behavior
- Study limited to two specific regions in Beijing with particular demographic characteristics, raising questions about generalizability to other urban contexts
- Computational costs of running thousands of LLM agents for multiple discussion rounds could be prohibitive for real-world implementation

## Confidence

- **High Confidence:** Fishbowl discussion mechanism effectively manages context limits while enabling broad participation; experimental setup and evaluation metrics are clearly specified
- **Medium Confidence:** Multi-agent collaboration framework successfully incorporates diverse resident needs into planning decisions; depends on validity of LLM role-play
- **Low Confidence:** Framework's generalizability to other urban contexts and scalability for real-world implementation; limited discussion of performance in different settings

## Next Checks

1. **Validation against human participants:** Conduct a small-scale study comparing LLM-resident feedback with actual human resident preferences in the same geographic areas to quantify how well synthetic agents capture real-world needs.

2. **Cross-context transferability test:** Apply the framework to a different urban region with distinct demographic characteristics, geographic constraints, and planning challenges to identify robust versus context-dependent components.

3. **Scalability and cost analysis:** Measure actual computational costs and processing times for running the full framework with varying numbers of residents and discussion rounds, developing estimates for city-wide implementation.