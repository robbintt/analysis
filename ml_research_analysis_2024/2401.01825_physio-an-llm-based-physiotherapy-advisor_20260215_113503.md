---
ver: rpa2
title: 'Physio: An LLM-Based Physiotherapy Advisor'
arxiv_id: '2401.01825'
source_url: https://arxiv.org/abs/2401.01825
tags:
- physio
- https
- answer
- condition
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Physio, a chat-based application for physical
  rehabilitation that leverages retrieval-augmented generation to provide trustworthy
  medical advice. The system combines a language model with a curated knowledge base
  containing reliable health sources to generate initial diagnoses with proper citations,
  recommend rehabilitation exercises, and suggest over-the-counter medications for
  symptom relief.
---

# Physio: An LLM-Based Physiotherapy Advisor

## Quick Facts
- arXiv ID: 2401.01825
- Source URL: https://arxiv.org/abs/2401.01825
- Reference count: 18
- Primary result: Chat-based physiotherapy advisor using retrieval-augmented generation with trustworthy medical sources

## Executive Summary
Physio is a chat-based application for physical rehabilitation that combines large language models with retrieval-augmented generation to provide trustworthy medical advice. The system processes user queries to generate initial diagnoses with proper citations, recommend rehabilitation exercises, and suggest over-the-counter medications for symptom relief. By grounding LLM responses in a curated knowledge base of reliable health sources, Physio addresses the hallucination problem common in generative models while maintaining the conversational benefits of chat interfaces.

## Method Summary
The system implements a data pipeline that validates user input, identifies medical conditions using LLM prompts, retrieves relevant documents from a curated knowledge base using BM25 ranking, generates responses conditioned on retrieved documents, and extracts exercise and medication recommendations from indexed collections. The knowledge base contains information from reliable sources including Rehab Hero, Mayo Clinic, NHS, and OrthoInfo, indexed in MongoDB collections for exercises, webpages, and medications. The system leverages GPT-4 for both validation and generation tasks while maintaining proper citations for all medical information.

## Key Results
- Successfully combines LLM capabilities with retrieval-augmented generation for trustworthy medical advice
- Implements proper citation system linking responses to reliable health sources
- Provides comprehensive rehabilitation support including condition identification, exercise recommendations, and over-the-counter medication suggestions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Retrieval-augmented generation grounds the model's responses in verifiable medical sources, reducing hallucination risk.
- Mechanism: The system retrieves relevant documents from a curated knowledge base using BM25 ranking, then conditions the LLM's response on those documents through prompt engineering.
- Core assumption: The retrieved documents contain accurate, relevant information that the LLM can effectively synthesize into coherent responses.
- Evidence anchors:
  - [abstract] "Physio can leverage the power of generative models for language processing while also conditioning its response on dependable and verifiable sources"
  - [section 2.3] "With the linked condition one can retrieve the list of documents related to that condition from the webpages collection. Among the pages available, we employ the BM25 retrieval model to search and rank them based on their relevance to the user's input."
- Break condition: If the knowledge base lacks coverage for certain conditions, the retrieval step may return irrelevant or no documents, forcing the system to fall back on less reliable generation.

### Mechanism 2
- Claim: The data pipeline validates input queries to ensure they are both English and physiotherapy-related before processing.
- Mechanism: A validation prompt template is used to assess whether the user's input meets these criteria, with the LLM providing a boolean response that can be programmatically interpreted.
- Core assumption: The LLM can reliably distinguish between relevant and irrelevant queries, and between English and non-English text.
- Evidence anchors:
  - [section 2.2] "The initial step in this pipeline is to verify if it is an English physiotherapy-related prompt. This validation is achieved by using the LM and a predefined validation prompt template that assesses whether the user's input is related to physiotherapy and written in English."
- Break condition: The validation prompt may produce false positives or negatives if the LLM misclassifies queries, especially with ambiguous or poorly phrased inputs.

### Mechanism 3
- Claim: The system can recommend appropriate exercises and over-the-counter medications based on the identified condition.
- Mechanism: After identifying the condition, the system queries its indexed collections for relevant exercises and medication information, using exact matching, alias matching, and fuzzy matching strategies.
- Core assumption: The indexed collections contain comprehensive and accurate information about exercises and medications for various conditions.
- Evidence anchors:
  - [section 2.1] "The information obtained was then indexed in a MongoDB database composed of three collections: exercises, webpages, and medications."
  - [section 2.2] "For the linked condition we randomly sample up to five exercises to be presented in the web interface."
- Break condition: If the condition identification step fails or if the database lacks entries for certain conditions, the system may provide no recommendations or incorrect ones.

## Foundational Learning

- Concept: Retrieval-augmented generation (RAG)
  - Why needed here: RAG is essential for grounding the LLM's responses in reliable medical sources, addressing the hallucination problem common in generative models.
  - Quick check question: How does the system ensure that the LLM's responses are based on actual medical information rather than fabricated content?

- Concept: Natural language processing for medical query understanding
  - Why needed here: The system must accurately interpret user queries about physical conditions and symptoms to provide relevant advice.
  - Quick check question: What techniques does the system use to identify the specific medical condition mentioned in a user's query?

- Concept: Knowledge base construction and indexing
  - Why needed here: A well-structured knowledge base is crucial for the retrieval component to function effectively and provide relevant information.
  - Quick check question: What criteria were used to select and validate the sources included in the knowledge base?

## Architecture Onboarding

- Component map:
  Frontend web interface -> Validation module -> Condition identification module -> Retrieval module -> Answer generation module -> Exercise and medication extraction modules -> MongoDB database -> OpenAI GPT-4 API

- Critical path: User query → Validation → Condition Identification → Retrieval → Answer Generation → Exercise/Medication Extraction → Response Delivery

- Design tradeoffs:
  - Using GPT-4 vs. open-source models for better generation quality but higher cost
  - Random sampling of exercises vs. ranking by relevance or difficulty
  - Limiting medication recommendations to over-the-counter options for safety

- Failure signatures:
  - Empty or irrelevant responses when knowledge base lacks coverage
  - Incorrect condition identification leading to wrong advice
  - Validation module rejecting valid queries or accepting invalid ones
  - LLM generating hallucinated content despite retrieval

- First 3 experiments:
  1. Test the validation module with a diverse set of queries (English/non-English, relevant/irrelevant) to assess accuracy and identify edge cases.
  2. Evaluate the condition identification accuracy by providing the system with known conditions and checking if it correctly identifies them.
  3. Measure the relevance of retrieved documents by comparing BM25 rankings against human judgments for a sample of queries.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of references to include in Physio's responses for balancing user trust and information overload?
- Basis in paper: [explicit] The paper states "determining the optimal number of references to include in the final answer is not trivial" and mentions they use a heuristic based on the number of generated sentences, but note this is "ongoing research."
- Why unresolved: The authors acknowledge their heuristic approach is not optimal and explicitly mention this as an area for improvement.
- What evidence would resolve it: User studies comparing different reference quantities in responses, measuring user trust, comprehension, and satisfaction across various reference-to-sentence ratios.

### Open Question 2
- Question: How does Physio's retrieval-augmented generation approach compare to standard GPT-4 in terms of diagnostic accuracy and user satisfaction for physical rehabilitation queries?
- Basis in paper: [inferred] The paper emphasizes Physio's trustworthiness through citation and retrieval-augmented generation, but doesn't provide comparative performance metrics against standard GPT-4.
- Why unresolved: The paper doesn't include any comparative evaluation between Physio and standard GPT-4 responses, only describing the architecture.
- What evidence would resolve it: Head-to-head comparison studies measuring diagnostic accuracy, completeness of recommendations, and user satisfaction scores between Physio and standard GPT-4 responses to identical queries.

### Open Question 3
- Question: How effective is Physio's fuzzy matching for medication recommendations when exact matches are not found in the database?
- Basis in paper: [explicit] The paper mentions they use "fuzzy matching" as a fallback for medication searches after exact matching fails, but provides no evaluation of this approach.
- Why unresolved: The paper describes the fuzzy matching approach but doesn't evaluate its accuracy or potential for recommending inappropriate medications.
- What evidence would resolve it: Analysis of fuzzy matching outcomes showing precision, recall, and clinical safety metrics, including false positive rates for inappropriate medication recommendations.

## Limitations

- System reliability depends heavily on the quality and coverage of the curated knowledge base, with no clear validation process described
- The validation module's effectiveness is uncertain due to unspecified prompt templates and lack of reported accuracy metrics
- Exercise and medication recommendation accuracy and safety have not been validated through clinical review or user studies

## Confidence

- High confidence: Core retrieval-augmented generation mechanism (Mechanism 1)
- Medium confidence: Input validation mechanism (Mechanism 2) and exercise/medication recommendation system (Mechanism 3)

## Next Checks

1. Conduct systematic testing of the validation module with diverse query sets to measure false positive and false negative rates, then refine the prompt template based on observed failure patterns.

2. Perform gap analysis on the knowledge base by systematically testing condition identification across a comprehensive list of common physiotherapy conditions, documenting where the system fails and expanding source coverage accordingly.

3. Implement a human-in-the-loop review process for a sample of generated exercise and medication recommendations, having licensed physiotherapists verify the appropriateness and safety of the advice for various conditions.