---
ver: rpa2
title: Short-term Inland Vessel Trajectory Prediction with Encoder-Decoder Models
arxiv_id: '2406.02770'
source_url: https://arxiv.org/abs/2406.02770
tags:
- prediction
- trajectory
- encoder-decoder
- transformer
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates encoder-decoder models for predicting short-term
  inland vessel trajectories. It compares LSTM and transformer architectures, reformulating
  trajectory prediction as a classification problem and incorporating river-specific
  features (radii) via a context vector.
---

# Short-term Inland Vessel Trajectory Prediction with Encoder-Decoder Models

## Quick Facts
- arXiv ID: 2406.02770
- Source URL: https://arxiv.org/abs/2406.02770
- Reference count: 21
- Primary result: LSTM encoder-decoder with context outperforms transformer and context-agnostic variants for inland vessel trajectory prediction

## Executive Summary
This paper investigates encoder-decoder models for short-term inland vessel trajectory prediction, comparing LSTM and transformer architectures on Danube river data. The key innovation is reformulating trajectory prediction as a classification problem and incorporating river-specific features (radii) via a context vector. Results show the LSTM encoder-decoder with context achieves the lowest displacement errors, with a mean Euclidean error of 120.61 m after 15 minutes. While the transformer is computationally cheaper, it is outperformed by the LSTM model for the given data. The classification reformulation and inclusion of contextual information significantly improve accuracy compared to context-agnostic variants.

## Method Summary
The method reformulates short-term vessel trajectory prediction as a classification problem, discretizing distance and course over ground (COG) changes into classes. AIS data is preprocessed to extract sequences of traveled distances and COG changes, with river radii used to generate context vectors. Two encoder-decoder architectures are implemented: LSTM and transformer, both incorporating the classification reformulation and context vectors. Models are trained using cross-entropy loss on sequences with varying observation and prediction horizons, with performance evaluated using mean Euclidean distance between predicted and target positions.

## Key Results
- LSTM encoder-decoder with context outperforms transformer encoder-decoder and context-agnostic variants
- Classification reformulation enables uncertainty estimation and reduces error accumulation
- Context vector incorporating river radii significantly improves prediction accuracy in constrained inland waterways
- Achieved mean Euclidean error of 120.61 m after 15 minutes of prediction

## Why This Works (Mechanism)

### Mechanism 1
Reformulating regression as classification enables transformer to handle discrete trajectory steps and reduce error accumulation by outputting probability distributions rather than continuous values, allowing sampling of multiple possible future paths and uncertainty estimation.

### Mechanism 2
Incorporating river-specific context (radii) via context vector significantly improves accuracy by providing information about river curvature that constrains vessel movement, allowing the model to learn waterway-specific navigation patterns.

### Mechanism 3
LSTM encoder-decoder architecture outperforms transformer for this task because LSTM's sequential processing and memory capabilities may be better suited for temporal dependencies in vessel movement data, despite transformer's computational efficiency.

## Foundational Learning

- Concept: Encoder-decoder architecture for sequence-to-sequence modeling
  - Why needed here: Vessel trajectories are temporal sequences requiring models that can encode input sequences and decode future predictions
  - Quick check question: What is the fundamental difference between how encoder-decoder models and standard RNNs handle multi-step prediction?

- Concept: Classification vs. regression problem formulation
  - Why needed here: Reformulation enables uncertainty estimation and potentially reduces error accumulation in multi-step predictions
  - Quick check question: Why might a classification approach be preferable to regression for multi-step trajectory prediction?

- Concept: Attention mechanisms and positional encoding
  - Why needed here: Transformers use attention to capture relationships between sequence elements, while positional encoding preserves temporal information
  - Quick check question: How do positional encodings in transformers differ from the inherent sequential processing in RNNs?

## Architecture Onboarding

- Component map: AIS data → preprocessing → embeddings → context vector → transformer → classification → loss computation → backprop
- Critical path: AIS data → preprocessing → embeddings → context vector → transformer → classification → loss computation → backprop
- Design tradeoffs:
  - Classification vs. regression: Classification enables uncertainty estimation but introduces discretization error
  - LSTM vs. Transformer: LSTM performs better but is computationally more expensive
  - Context inclusion: Improves accuracy but requires additional data processing
- Failure signatures:
  - High prediction error after few time steps: May indicate model overfitting or inappropriate discretization
  - Model not learning: Check data preprocessing, embedding dimensions, or learning rate
  - Context vector not helping: Verify river radii accuracy or consider additional context features
- First 3 experiments:
  1. Test baseline models (average velocity models) to establish minimum performance threshold
  2. Compare LSTM encoder-decoder with and without context vector to quantify context benefit
  3. Test different discretization resolutions for distance and COG changes to find optimal balance between accuracy and error

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of transformer-based encoder-decoder models for vessel trajectory prediction change with different hyperparameter optimizations?
- Basis in paper: The paper mentions that future studies can investigate the improvement of the computationally more efficient transformer, e.g. through further hyper-parameter optimization.
- Why unresolved: The study only tested a specific set of hyperparameters for the transformer model, and it is unclear how further optimization would affect its performance relative to the LSTM encoder-decoder.
- What evidence would resolve it: Systematic hyperparameter tuning experiments comparing different transformer architectures and their impact on prediction accuracy and computational cost.

### Open Question 2
- Question: What is the impact of incorporating additional river-specific information, such as water levels and flow velocities, into the context vector on the accuracy of vessel trajectory predictions?
- Basis in paper: The paper suggests that future studies can use additional river-specific information in the context representation to further increase prediction accuracy.
- Why unresolved: The study only considered river radii as the river-specific feature, and the effect of other features on prediction accuracy is unknown.
- What evidence would resolve it: Experiments incorporating various combinations of river-specific features into the context vector and measuring their impact on prediction accuracy.

### Open Question 3
- Question: How does the use of beam search decoding instead of greedy decoding affect the uncertainty estimation and overall prediction accuracy of the classification-based trajectory prediction models?
- Basis in paper: The paper mentions that sampling output sequences would be required to get an estimate of the model's uncertainty, and that substituting the greedy decoding method with a beam search decoding could possibly lead to better results.
- Why unresolved: The study used greedy decoding for simplicity, and the potential benefits of beam search decoding on uncertainty estimation and prediction accuracy are not explored.
- What evidence would resolve it: Comparative experiments using both greedy and beam search decoding methods, measuring their impact on prediction accuracy and uncertainty estimation.

## Limitations

- Analysis based on single river (Danube) dataset, raising generalizability questions to other waterways
- Classification reformulation introduces discretization error (maximum ~25m after 20 minutes, ~70m after 40 minutes)
- River radii context assumes vessels follow predictable paths, which may not hold for all navigation scenarios
- Computational efficiency claims for transformers lack specific runtime comparisons and hardware requirements

## Confidence

**High Confidence**: LSTM encoder-decoder with context outperforms other variants is supported by direct experimental results showing lower validation error and faster learning rates.

**Medium Confidence**: LSTM superiority over transformer may be architecture-dependent and could change with different hyperparameters or modifications.

**Low Confidence**: Claims about general applicability to other inland waterways or vessel types are not substantiated due to single-dataset focus.

## Next Checks

1. **Generalizability Test**: Validate models on multiple river datasets with varying characteristics to assess whether LSTM+context advantage holds across different inland waterway environments.

2. **Discretization Error Analysis**: Conduct detailed analysis of classification discretization error distribution across different prediction horizons and vessel speeds.

3. **Context Feature Expansion**: Experiment with additional context features beyond river radii (water levels, flow velocities, traffic density) to determine if current context vector is capturing all relevant navigational constraints.