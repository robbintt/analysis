---
ver: rpa2
title: 'LOTUS: Improving Transformer Efficiency with Sparsity Pruning and Data Lottery
  Tickets'
arxiv_id: '2405.00906'
source_url: https://arxiv.org/abs/2405.00906
tags:
- data
- sparsity
- lottery
- pruning
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LOTUS (Lottery Transformers with Ultra Sparsity),
  a method that combines data lottery ticket selection with sparsity pruning to accelerate
  vision transformer training. The approach first identifies informative data subsets
  using attention maps to select high-attention patches, then applies a two-step pruning
  process (Essential Sparsity followed by Instant Soup Pruning) to remove redundant
  model parameters.
---

# LOTUS: Improving Transformer Efficiency with Sparsity Pruning and Data Lottery Tickets

## Quick Facts
- arXiv ID: 2405.00906
- Source URL: https://arxiv.org/abs/2405.00906
- Reference count: 5
- Primary result: Achieves 79% accuracy at 30% sparsity on CIFAR-10 using combined data lottery tickets and sparsity pruning

## Executive Summary
LOTUS introduces a novel approach to improving transformer efficiency by combining data lottery ticket selection with sparsity pruning. The method first identifies informative data subsets through attention-based patch selection, then applies a two-step pruning process (Essential Sparsity followed by Instant Soup Pruning) to remove redundant parameters. The approach demonstrates accelerated convergence when fine-tuned on lottery data tickets, reaching near-SOTA performance by the fifth epoch on CIFAR-10. However, the combined approach shows significant limitations with accuracy dropping to approximately 50% when both components are used together, indicating potential issues with aggressive pruning removing crucial weights.

## Method Summary
LOTUS operates through a two-stage process: first, it identifies lottery data tickets by analyzing attention maps to select high-attention patches that are most informative for training. Second, it applies a two-step pruning strategy - Essential Sparsity to remove redundant weights based on importance scores, followed by Instant Soup Pruning for further parameter reduction. The method claims to maintain performance while achieving significant sparsity, though the combined approach shows concerning accuracy degradation when both components are applied simultaneously.

## Key Results
- Achieves 79% accuracy at 30% sparsity on CIFAR-10 with ViT model
- Demonstrates rapid convergence, reaching near-SOTA performance by fifth epoch when fine-tuned on lottery data tickets
- Combined ISSP approach results in accuracy dropping to approximately 50%, suggesting aggressive pruning removes crucial weights

## Why This Works (Mechanism)
The method leverages attention maps to identify the most informative data samples (lottery tickets) that contribute most to model learning. By focusing training on these high-value samples, the model can achieve comparable performance with fewer training iterations. The sparsity pruning component then removes redundant parameters that don't contribute significantly to the model's predictive capability. The combination aims to reduce both computational cost and memory requirements while maintaining accuracy, though the aggressive pruning appears to compromise performance in the combined approach.

## Foundational Learning

- Vision Transformers (ViT): Replace CNNs with self-attention mechanisms for image classification - needed for understanding the base architecture being optimized
- Lottery Ticket Hypothesis: Identifies sparse subnetworks that can be trained in isolation to match full network performance - crucial for understanding data selection methodology
- Model Pruning: Removes redundant parameters while preserving performance - essential for grasping the sparsity component
- Attention Mechanisms: Compute relationships between input elements to identify important features - fundamental to lottery ticket selection process

## Architecture Onboarding

Component Map: Input Data -> Attention Analysis -> Lottery Ticket Selection -> Model Training -> Essential Sparsity Pruning -> Instant Soup Pruning -> Final Model

Critical Path: The lottery ticket selection process is critical as it determines which data samples receive priority during training. The pruning steps must preserve parameters essential for lottery ticket effectiveness.

Design Tradeoffs: The method trades potential accuracy for efficiency gains. Aggressive pruning may remove critical parameters needed for lottery ticket effectiveness, while conservative pruning may not achieve desired efficiency improvements.

Failure Signatures: The 50% accuracy drop when combining both components indicates the pruning strategy is removing essential parameters. This suggests the pruning criteria may not be aligned with lottery ticket preservation.

First Experiments:
1. Run ablation studies comparing performance with only data selection, only pruning, and combined approach
2. Test lottery ticket selection effectiveness on varying dataset complexities
3. Evaluate pruning sensitivity by adjusting pruning ratios and observing accuracy impacts

## Open Questions