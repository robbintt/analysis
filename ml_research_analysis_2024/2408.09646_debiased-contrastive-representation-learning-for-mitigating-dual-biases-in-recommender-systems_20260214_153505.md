---
ver: rpa2
title: Debiased Contrastive Representation Learning for Mitigating Dual Biases in
  Recommender Systems
arxiv_id: '2408.09646'
source_url: https://arxiv.org/abs/2408.09646
tags:
- dclmdb
- popularity
- user
- bias
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses dual biases\u2014popularity and conformity\u2014\
  in recommender systems that skew recommendations toward popular items and user conformity\
  \ rather than true preferences. The authors model the data generation process with\
  \ a causal graph and propose DCLMDB, a framework using debiased contrastive learning\
  \ to disentangle popularity (Z) and conformity (W) influences from user-item interactions."
---

# Debiased Contrastive Representation Learning for Mitigating Dual Biases in Recommender Systems

## Quick Facts
- arXiv ID: 2408.09646
- Source URL: https://arxiv.org/abs/2408.09646
- Reference count: 35
- DCLMDB achieves up to 35% improvement in Recall@20 over baselines

## Executive Summary
This paper addresses dual biases—popularity and conformity—in recommender systems that skew recommendations toward popular items and user conformity rather than true preferences. The authors model the data generation process with a causal graph and propose DCLMDB, a framework using debiased contrastive learning to disentangle popularity (Z) and conformity (W) influences from user-item interactions. The method learns independent embeddings via contrastive objectives, ensuring these biases do not distort recommendations. Evaluated on Movielens-10M and Netflix datasets, DCLMDB outperforms baselines across Recall, HR, and NDCG metrics, achieving up to 35% improvement in Recall@20 and maintaining stable debiasing effects across different recommendation list sizes.

## Method Summary
DCLMDB uses a causal graph to model the data generation process and identify confounders (popularity Z and conformity W). The framework employs contrastive learning to learn two separate embeddings—Z for popularity and W for conformity—that are pushed away from the base user (U) and item (I) embeddings. A Bayesian Personalized Ranking (BPR) loss guides the embeddings to optimize for click prediction while the contrastive terms act as regularizers to enforce independence. The final loss combines BPR with contrastive regularizers: LDCLMDB = α · LBP R + β · (Lu + Li). This approach ensures that recommendations are not unduly influenced by either popularity or conformity.

## Key Results
- DCLMDB outperforms baselines (MF, LightGCN, IPS variants, CausE, DICE, DCCL) across Recall, HR, and NDCG metrics
- Achieves up to 35% improvement in Recall@20 compared to standard MF and LightGCN
- Maintains stable debiasing effects under varying Top-K conditions, showing consistent performance across different recommendation list sizes
- Demonstrates superior performance even with limited intervention data (0%, 10%, 20% of training set)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DCLMDB disentangles popularity and conformity biases from user-item embeddings using contrastive learning
- Mechanism: The framework learns two separate embeddings—Z (popularity) and W (conformity)—and uses contrastive objectives to push these away from the base user and item embeddings (U and I)
- Core assumption: Popularity and conformity can be effectively modeled as latent confounders in the causal graph and separated via contrastive learning
- Evidence anchors: [abstract] "both popularity bias and conformity bias are handled in the model training process by contrastive learning to ensure that user choices and recommended items are not unduly influenced by conformity and popularity." [section] "We employ contrastive representation learning (Schroff, Kalenichenko, and Philbin 2015) to learn the embeddings Z and W derived from the latent spaces of items and users, guided by the proposed causal graph in Fig. 2 (b) to effectively mitigate the dual biases in recommender systems."
- Break condition: If popularity and conformity cannot be cleanly separated into latent factors, or if the contrastive objective does not sufficiently decorrelate Z and W from U and I, the debiasing will fail

### Mechanism 2
- Claim: DCLMDB preserves ranking performance by combining contrastive debiasing with a standard BPR ranking loss
- Mechanism: The contrastive terms (Lu and Li) act as regularizers to enforce independence, while the BPR loss guides the embeddings to optimize for click prediction
- Core assumption: The BPR loss can steer the contrastive debiasing in the correct direction without undermining the recommendation task
- Evidence anchors: [abstract] "Fine-tuning Z and W to address the dual biases by applying back-door adjustment (Pearl 2009) improves the implementation of the click prediction task and enhances recommendation accuracy." [section] "We use the Bayesian Personalised Ranking (BPR) loss function to guide the direction of the optimisation of the two embeddings W and Z, as well as to achieve the main task (click prediction) in the recommender system."
- Break condition: If the BPR loss dominates and overwhelms the contrastive regularizers, the debiasing effect will be lost; if the contrastive terms dominate, ranking performance may degrade

### Mechanism 3
- Claim: DCLMDB is robust to different recommendation list sizes because the debiasing is applied at the embedding level rather than by reweighting items after scoring
- Mechanism: By removing bias at the representation level, the model consistently avoids recommending popular or conformist items regardless of the Top-K cutoff
- Core assumption: The embedding-level debiasing generalizes across different values of K
- Evidence anchors: [section] "DCLMDB not only excels in eradicating popularity bias and conformity bias but also maintains stable debiasing effects under varying Top-K conditions." [section] "This further demonstrates that the strategy of simultaneous debiasing on both the user side and the item side is effective and can make the debiasing effect more stable."
- Break condition: If the embedding-level debiasing does not generalize across different Top-K cutoffs, the debiasing effect will degrade for larger lists

## Foundational Learning

- Concept: Causal graphs and do-calculus
  - Why needed here: The causal graph models the data generation process and identifies confounders (popularity Z and conformity W). Do-calculus provides the theoretical justification for cutting the edges that cause bias
  - Quick check question: In the causal graph, what do the edges Z→I and W→U represent, and why must they be cut?

- Concept: Contrastive learning
  - Why needed here: Contrastive learning is used to push apart embeddings that represent biased factors (Z and W) from those representing user preferences (U) and item features (I)
  - Quick check question: How do the contrastive objectives Lu and Li ensure that Z and W are independent of I and U?

- Concept: BPR loss for ranking
  - Why needed here: BPR loss is used to optimize the model for click prediction while maintaining ranking performance
  - Quick check question: Why is BPR loss combined with the contrastive regularizers instead of using only contrastive loss?

## Architecture Onboarding

- Component map: Backbone recommender (MF or LightGCN) -> Contrastive debiasing module (learns Z and W embeddings) -> BPR loss for ranking
- Critical path: The forward pass computes base embeddings via the backbone, then the contrastive module generates debiased embeddings Z and W, and the final score is computed using both the base and debiased embeddings. The loss is the sum of BPR and the contrastive regularizers
- Design tradeoffs: Using contrastive learning adds computational overhead but provides a principled way to separate biases. The tradeoff is between debiasing effectiveness and model complexity. The choice of backbone (MF vs. LightGCN) affects the expressiveness and efficiency of the model
- Failure signatures: If the model overfits to the contrastive terms, ranking performance may degrade. If the contrastive terms are too weak, debiasing will be ineffective. If the embeddings Z and W do not capture the bias factors well, the debiasing will fail
- First 3 experiments:
  1. Train DCLMDB with only the contrastive terms (no BPR) and evaluate debiasing on a synthetic dataset where popularity and conformity are known
  2. Train DCLMDB with only BPR loss (no contrastive terms) and compare to a standard MF/LightGCN baseline to confirm that the contrastive terms are necessary for debiasing
  3. Vary the strengths of the contrastive regularizers (β) and observe the tradeoff between debiasing effectiveness and ranking performance on Movielens-10M

## Open Questions the Paper Calls Out

- Open Question 1: How does DCLMDB's debiasing performance scale when the proportion of intervened training data approaches zero?
  - Basis in paper: [explicit] The authors mention that CausE cannot function without intervention data and compare DCLMDB's performance with 0%, 10%, and 20% intervention data
  - Why unresolved: The experiments only tested up to 20% intervention data, leaving the lower bound of DCLMDB's effectiveness unclear
  - What evidence would resolve it: Experiments showing DCLMDB's performance metrics (Recall, HR, NDCG) across a continuous range of intervention data proportions from 0% to 20% with fine-grained intervals

- Open Question 2: What is the computational overhead of DCLMDB compared to baseline methods in terms of training time and inference latency?
  - Basis in paper: [inferred] The paper emphasizes DCLMDB's effectiveness but does not discuss computational efficiency relative to other methods
  - Why unresolved: The authors focused on accuracy metrics but omitted runtime performance, which is critical for practical deployment
  - What evidence would resolve it: Detailed benchmarking of training time per epoch and inference latency per recommendation across all compared methods on the same hardware

- Open Question 3: How does DCLMDB perform on datasets with different bias distributions, such as those with extreme long-tail item popularity?
  - Basis in paper: [explicit] The authors mention that methods like IPS show inconsistent performance across different datasets (Movielens-10M vs Netflix)
  - Why unresolved: Experiments were limited to two datasets, and the paper doesn't explore how DCLMDB handles varying degrees of popularity bias concentration
  - What evidence would resolve it: Experiments on synthetic datasets with controlled popularity distributions (e.g., Zipf distributions with different skew parameters) and real-world datasets known for extreme long-tail characteristics

## Limitations
- The paper does not fully detail the exact implementation of causal graph edge cutting or the PNSM negative sampling strategy, which could impact reproducibility
- Evaluation focuses on offline metrics (Recall, HR, NDCG, IOU) but does not address online A/B testing, which is critical for validating real-world effectiveness
- The model's robustness to different recommendation list sizes is claimed but not extensively validated across a wide range of K values

## Confidence

**High:** The theoretical foundation of using causal graphs and contrastive learning to disentangle biases is well-supported. The ablation studies and comparison with baselines provide strong empirical evidence for the model's effectiveness.

**Medium:** The claim that DCLMDB outperforms all baselines across all metrics is robust, but the exact magnitude of improvement may vary depending on hyperparameter tuning and dataset characteristics.

**Low:** The paper does not provide sufficient details on the implementation of the causal graph edge cutting and PNSM strategy, which could lead to discrepancies in reproduction.

## Next Checks

1. Implement and test the exact causal graph edge cutting mechanism and PNSM negative sampling strategy to ensure faithful reproduction
2. Conduct online A/B testing to validate the model's effectiveness in real-world scenarios and compare it with offline metrics
3. Evaluate DCLMDB on a wider range of datasets, including those with noisy or sparse interactions, to assess its robustness and generalization