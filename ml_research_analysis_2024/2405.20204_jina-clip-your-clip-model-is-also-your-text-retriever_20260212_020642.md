---
ver: rpa2
title: 'Jina CLIP: Your CLIP Model Is Also Your Text Retriever'
arxiv_id: '2405.20204'
source_url: https://arxiv.org/abs/2405.20204
tags:
- text
- clip
- arxiv
- stage
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Jina CLIP presents a novel multi-task contrastive training approach
  that enables a single model to perform both text-image and text-text retrieval tasks
  at state-of-the-art levels. Traditional CLIP models excel at multimodal tasks but
  underperform on text-only retrieval compared to specialized text embedding models,
  creating inefficiencies for systems needing both capabilities.
---

# Jina CLIP: Your CLIP Model Is Also Your Text Retriever

## Quick Facts
- arXiv ID: 2405.20204
- Source URL: https://arxiv.org/abs/2405.20204
- Authors: Andreas Koukounas, Georgios Mastrapas, Michael Günther, Bo Wang, Scott Martens, Isabelle Mohr, Saba Sturua, Mohammad Kalim Akram, Joan Fontanals Martínez, Saahil Ognawala, Susana Guzman, Maximilian Werk, Nan Wang, Han Xiao
- Reference count: 27
- Key outcome: Single model achieves 80.31% Recall@5 on text-image retrieval and 60.12% on MTEB text tasks

## Executive Summary
Jina CLIP addresses a fundamental limitation of traditional CLIP models: while excelling at text-image retrieval, they underperform on text-only tasks compared to specialized text embedding models. The paper introduces a novel multi-task contrastive training approach that enables a single model to achieve state-of-the-art performance on both modalities. By jointly optimizing text-image and text-text matching through a three-stage training pipeline with long synthetic captions and hard negatives, jina-clip-v1 matches specialized models on both tasks while offering significant efficiency gains for real-world applications.

## Method Summary
The method employs a three-stage contrastive training pipeline that combines text-image and text-text retrieval tasks. Stage 1 uses LAION-400M and 40 text-pair datasets with 32,768 batch size for joint optimization. Stage 2 introduces long synthetic captions from ShareGPT4V to improve handling of longer text contexts. Stage 3 incorporates hard negatives from text triplet datasets (MSMarco, Natural Questions, HotpotQA, NLI) to refine text-text retrieval capabilities. The model uses JinaBERT (BERT variant with AliBi for longer contexts) for text encoding and EV A02 for image encoding, with joint InfoNCE loss functions for both modalities.

## Key Results
- Achieves 80.31% average Recall@5 on CLIP Benchmark text-image retrieval (comparable to EV A-CLIP)
- Reaches 60.12% average on MTEB text retrieval tasks (matching top-tier text embedding models)
- Improves text-text retrieval performance by 22% over other CLIP models
- Demonstrates unified model can replace separate specialized models for different modalities

## Why This Works (Mechanism)

### Mechanism 1
Multi-task contrastive training enables unified performance by jointly optimizing text-image and text-text matching through a three-stage pipeline with large-scale datasets, long synthetic captions, and hard negatives. The core assumption is that a unified representation space can effectively support both cross-modal and within-modal retrieval tasks. Evidence from section 4 shows the joint loss function combining InfoNCE losses for both task types. Break condition: If the unified space cannot capture nuances of both relationships or if one task dominates training.

### Mechanism 2
Long synthetic captions improve handling of longer text contexts by introducing an intermediate training stage. The core assumption is that longer inputs contain richer information capturable with appropriate training data. Section 4 explains the intermediate stage for training on GPT4v-generated long captions, with section 5 showing improved text retrieval performance. Break condition: If synthetic data quality is poor or model cannot generalize to real-world long text.

### Mechanism 3
Hard negatives in final stage significantly improve text-text retrieval by using text retrieval models to select challenging negatives for triplet text corpus. The core assumption is that challenging negative examples during training improve distinction between relevant and irrelevant pairs. Section 4 describes using hard negatives in stage 3, with section 5 demonstrating competitive MTEB performance. Break condition: If negative selection introduces bias or model becomes too conservative in similarity scores.

## Foundational Learning

- **Concept: Contrastive learning**
  - Why needed here: The entire training approach relies on contrastive learning to align representations of different modalities and text pairs.
  - Quick check question: Can you explain the difference between positive and negative pairs in contrastive learning and why they're important?

- **Concept: Multi-task learning**
  - Why needed here: The model needs to learn multiple related tasks (text-image and text-text retrieval) simultaneously to achieve unified performance.
  - Quick check question: What are the potential benefits and challenges of multi-task learning compared to single-task approaches?

- **Concept: Information retrieval metrics**
  - Why needed here: Understanding metrics like Recall@5, nDCG, and Spearman correlation is crucial for evaluating the model's performance on different tasks.
  - Quick check question: How do Recall@5 and nDCG@10 differ in their evaluation of retrieval system performance?

## Architecture Onboarding

- **Component map**: Text input → JinaBERT (with AliBi) → Embedding → Joint loss function → Parameter update; Image input → EV A02 → Embedding → Joint loss function → Parameter update
- **Critical path**: Text/image input → Encoder → Joint embedding space → Loss computation → Parameter update
- **Design tradeoffs**: Unified model vs. separate specialized models, synthetic vs. real data for long captions, hard negatives vs. random negatives
- **Failure signatures**: Poor performance on one task type, overfitting to synthetic data, unstable training due to hard negative selection
- **First 3 experiments**:
  1. Train stage 1 only and evaluate text-text retrieval performance to establish baseline
  2. Add stage 2 with long captions and measure improvement on text retrieval tasks
  3. Incorporate stage 3 with hard negatives and compare final performance against stage 2 results

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of Jina CLIP vary across different languages beyond English?
- **Basis in paper**: [explicit] The paper states the model is "currently limited to English-language texts due to limited multilingual resources" and mentions future work on multilingual contexts.
- **Why unresolved**: No data or analysis provided on non-English text performance.
- **What evidence would resolve it**: Testing and reporting performance on multilingual text retrieval and embedding tasks compared to multilingual baselines.

### Open Question 2
- **Question**: What is the impact of using AI-generated long captions on the model's performance and generalization?
- **Basis in paper**: [explicit] Paper notes AI-generated long captions were used due to limited public data availability and states investigating AI-generated data impact is outside scope.
- **Why unresolved**: Paper acknowledges use but doesn't analyze or quantify its effect or potential biases.
- **What evidence would resolve it**: Systematic study comparing performance with human-written vs. AI-generated captions, including performance differences and bias analysis.

### Open Question 3
- **Question**: How does the three-stage training approach compare to alternative training strategies for achieving similar performance?
- **Basis in paper**: [inferred] Paper presents novel three-stage method and demonstrates effectiveness but doesn't compare to other strategies or ablate stage contributions.
- **Why unresolved**: Paper doesn't explore alternative training methodologies or isolate effects of individual stages.
- **What evidence would resolve it**: Comparative experiments using different training strategies and ablation studies removing or modifying stages.

## Limitations

- Reliance on synthetic data for long captions may not fully capture real-world text complexity
- Hard negative selection process introduces potential biases not fully characterized
- Performance on specialized text tasks beyond general retrieval remains unclear

## Confidence

- **High Confidence**: Multi-task contrastive training with joint loss functions is well-supported by experimental results showing competitive performance on both task types.
- **Medium Confidence**: Specific architecture choices (JinaBERT with AliBi, EV A02) and their contribution to overall performance, as alternative configurations weren't thoroughly explored.
- **Low Confidence**: Long-term generalization to unseen domains and tasks beyond CLIP Benchmark and MTEB, given reliance on synthetic data for training.

## Next Checks

1. Conduct ablation studies removing each training stage to quantify specific contribution of long captions and hard negatives to final performance.
2. Test the model on specialized text tasks not included in MTEB benchmark to assess versatility beyond general retrieval.
3. Evaluate model's performance on real-world long-form text inputs versus synthetic data used in training to assess potential domain shift issues.