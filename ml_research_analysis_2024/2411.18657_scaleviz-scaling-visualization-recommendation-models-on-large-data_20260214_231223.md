---
ver: rpa2
title: 'ScaleViz: Scaling Visualization Recommendation Models on Large Data'
arxiv_id: '2411.18657'
source_url: https://arxiv.org/abs/2411.18657
tags:
- features
- dataset
- datasets
- vis-rec
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the scalability problem in automated visualization
  recommendation (Vis-Rec) models, which rely on computing large numbers of expensive
  statistics from datasets. For large datasets, this computation becomes prohibitively
  expensive, making these models impractical for real-world use.
---

# ScaleViz: Scaling Visualization Recommendation Models on Large Data

## Quick Facts
- arXiv ID: 2411.18657
- Source URL: https://arxiv.org/abs/2411.18657
- Authors: Ghazi Shazan Ahmad; Shubham Agarwal; Subrata Mitra; Ryan Rossi; Manav Doshi; Vibhor Porwal; Syam Manoj Kumar Paila
- Reference count: 17
- One-line primary result: Achieves up to 10X speedup in visualization recommendation generation while maintaining small error margins on large datasets

## Executive Summary
ScaleViz addresses the scalability challenge in automated visualization recommendation (Vis-Rec) models, which become computationally prohibitive on large datasets due to the need to compute numerous expensive statistics. The framework uses reinforcement learning to learn which subset of statistics is most important for a given dataset and time budget, avoiding the need to compute all statistics. ScaleViz profiles computational costs using polynomial regression, trains an RL agent to select features that minimize error within budget constraints, and demonstrates significant speed improvements across multiple real-world datasets while maintaining recommendation quality.

## Method Summary
ScaleViz profiles the computational cost of features across dataset sizes using polynomial regression to extrapolate costs to full dataset size. It then trains a reinforcement learning agent using double deep Q-learning with experience replay to learn feature acquisition under budget constraints. The agent incrementally builds feature subsets by selecting features that maximize reward (prediction improvement per unit cost) until the budget is exhausted. For inference on new datasets, the trained RL agent selects the most important features within the given time budget, and only these selected features are computed to generate visualization recommendations.

## Key Results
- Achieves up to 10X speedup in generating visualization recommendations compared to baseline methods
- Maintains small error margins while significantly reducing computation time
- Feature selection is shown to be dataset-specific with low overlap between selected features across different datasets
- Demonstrates improved scalability as dataset size increases, with required budget saturating at a fraction of original model computation time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reinforcement learning can learn dataset-specific feature importance without explicit labels
- Mechanism: The RL agent receives rewards based on prediction score changes when selecting features, allowing it to learn which features improve model performance most efficiently under budget constraints
- Core assumption: Statistical feature importance is dataset-dependent rather than model-architecture-dependent
- Evidence anchors:
  - [abstract] "The feature selection is shown to be dataset-specific, with low overlap between selected features across datasets"
  - [section] "At every step t, the agent selects a feature ij and masks that feature as selected. The agent moves to the next state... The reward for an action ak,t is calculated as the absolute change in the score before and after acquiring the feature, ij with a penalty of c"
- Break condition: If feature importance were primarily determined by model architecture rather than data distribution, RL training would converge to similar feature sets across datasets

### Mechanism 2
- Claim: Polynomial regression can accurately predict computational costs across dataset sizes
- Mechanism: The cost profiler collects feature computation times on small samples and uses polynomial regression to extrapolate costs to full dataset size
- Core assumption: Feature computation time scales polynomially with dataset size
- Evidence anchors:
  - [section] "For other features, assuming polynomial growth of feature costs with dataset size (as proved in [16])"
  - [section] "The Cost Profiler module profiles the computational cost of calculating statistics for each statistics that are needed by the generalized model on a few samples of data of different sizes"
- Break condition: If computation time scaling is non-polynomial (e.g., exponential or discontinuous), regression predictions become unreliable

### Mechanism 3
- Claim: Sequential feature selection under budget constraints optimizes trade-off between computation time and recommendation accuracy
- Mechanism: The RL agent incrementally builds feature subsets, maximizing reward (prediction improvement per unit cost) until budget exhaustion
- Core assumption: Optimal feature subsets can be built incrementally without backtracking
- Evidence anchors:
  - [abstract] "ScaleViz achieves up to 10X speedup in generating visualization recommendations while maintaining small error margins"
  - [section] "Each episode consists of the agent choosing the important subset of features for a sample Sk... The action ak,t of the agent is to select a feature which has not been masked in the feature set"
- Break condition: If optimal feature subsets require complex interactions that can't be discovered incrementally, greedy selection may miss important combinations

## Foundational Learning

- Concept: Reinforcement learning fundamentals (Q-learning, experience replay, exploration-exploitation trade-off)
  - Why needed here: The core mechanism uses RL to learn feature selection policies
  - Quick check question: What happens if the exploration rate is set too low during training?

- Concept: Polynomial regression and cost modeling
  - Why needed here: Essential for predicting feature computation costs without running expensive calculations
  - Quick check question: How would you verify that polynomial regression is an appropriate model for your feature costs?

- Concept: Visualization recommendation pipeline architecture
  - Why needed here: Understanding the baseline models (VizML, MLVR) is crucial for contextualizing ScaleViz's improvements
  - Quick check question: What are the typical stages in a visualization recommendation system?

## Architecture Onboarding

- Component map: Cost Profiler -> RL Agent -> Base Vis-Rec Model -> Inference Pipeline
- Critical path: Cost profiling → RL training → Feature selection → Base model inference
- Design tradeoffs:
  - Feature granularity: Column-level vs. aggregated features
  - Sample size selection: Larger samples improve cost estimation but increase profiling time
  - RL architecture: Simple Q-networks vs. more complex architectures for feature selection
- Failure signatures:
  - High error rates despite feature selection: May indicate insufficient training or poor cost estimation
  - No speedup compared to baselines: Suggests budget constraints are too tight or feature selection is ineffective
  - RL agent not converging: Could indicate poor reward design or exploration-exploitation imbalance
- First 3 experiments:
  1. Profile costs on small sample: Verify polynomial regression accurately predicts costs across dataset sizes
  2. RL training with synthetic data: Test feature selection policy before applying to real datasets
  3. Baseline comparison: Run ScaleViz vs. random/greedy selection on small dataset to validate speedup claims

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several remain unresolved based on the content:

## Limitations
- Polynomial regression cost model may not generalize well to datasets with highly irregular computation patterns
- Framework's effectiveness depends on the quality of base Vis-Rec models, which may not be well-calibrated
- The RL framework requires accurate cost profiling as a prerequisite for effective feature selection

## Confidence
- High confidence: The fundamental problem of scalability in Vis-Rec models is well-established, and the polynomial regression cost modeling approach is mathematically sound for the feature types described
- Medium confidence: The RL-based feature selection approach shows promise, but the generalization across diverse dataset types and sizes needs further validation
- Medium confidence: The claimed 10X speedup is impressive, but the error margins relative to the baseline models could be more thoroughly characterized across different data distributions

## Next Checks
1. Cost profiling robustness test: Validate polynomial regression predictions across a wider range of dataset cardinalities and column types, particularly focusing on features with non-polynomial scaling behavior
2. Cross-dataset generalization: Evaluate ScaleViz's performance when trained on one dataset type and applied to another, measuring both speedup and accuracy degradation
3. Budget sensitivity analysis: Systematically vary the time budget constraints to identify the minimum budget required for acceptable recommendation quality across different dataset sizes