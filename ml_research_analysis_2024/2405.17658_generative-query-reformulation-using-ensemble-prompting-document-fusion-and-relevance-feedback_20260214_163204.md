---
ver: rpa2
title: Generative Query Reformulation Using Ensemble Prompting, Document Fusion, and
  Relevance Feedback
arxiv_id: '2405.17658'
source_url: https://arxiv.org/abs/2405.17658
tags:
- query
- retrieval
- language
- search
- ndcg
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces generative query reformulation methods that
  leverage ensemble prompting, document fusion, and relevance feedback to improve
  retrieval effectiveness. The proposed GenQREnsemble and GenQRFusion methods generate
  multiple query reformulations using paraphrastic instructions and combine them via
  appending or document fusion.
---

# Generative Query Reformulation Using Ensemble Prompting, Document Fusion, and Relevance Feedback

## Quick Facts
- arXiv ID: 2405.17658
- Source URL: https://arxiv.org/abs/2405.17658
- Authors: Kaustubh D. Dhole; Ramraj Chandradevan; Eugene Agichtein
- Reference count: 16
- Key outcome: Up to 18% relative gains on nDCG@10 in pre-retrieval settings and 9% in post-retrieval settings, outperforming previous state-of-the-art results

## Executive Summary
This paper introduces generative query reformulation methods that leverage ensemble prompting, document fusion, and relevance feedback to improve retrieval effectiveness. The proposed GenQREnsemble and GenQRFusion methods generate multiple query reformulations using paraphrastic instructions and combine them via appending or document fusion. Their post-retrieval variants incorporate relevance feedback from documents or human users. Experiments on four IR benchmarks show significant improvements, with up to 18% relative gains on nDCG@10 in pre-retrieval settings and 9% in post-retrieval settings, outperforming previous state-of-the-art results. Analysis reveals benefits from increasing feedback documents, using domain-specific instructions, and filtering reformulations for better interpretability.

## Method Summary
The paper proposes ensemble-based query reformulation using LLMs with paraphrased instructions, document fusion techniques, and relevance feedback integration. The approach generates multiple reformulations from diverse paraphrased versions of a base instruction, combines them through either appending keywords or fusing retrieval results, and optionally incorporates feedback documents to refine reformulations. The method operates in zero-shot settings without requiring labeled training data, using FlanT5 and Llama-2 models to generate reformulations that are then evaluated on multiple IR benchmarks using nDCG@10, RR, and P@10 metrics.

## Key Results
- Up to 18% relative improvements on nDCG@10 in pre-retrieval settings compared to state-of-the-art methods
- Up to 9% relative improvements in post-retrieval settings when incorporating relevance feedback
- Significant performance gains from increasing feedback documents from 1 to 5, with diminishing returns beyond that
- Better interpretability achieved through GPT-4-based filtering of query reformulations

## Why This Works (Mechanism)

### Mechanism 1
Paraphrasing the base QR instruction into multiple diverse instructions allows the LLM to generate varied keyword sets, capturing different aspects of the query intent. Each paraphrased instruction serves as a distinct "view" on the query, prompting the model to produce semantically related but lexically diverse keywords. This variation exploits the model's sensitivity to prompt wording.

### Mechanism 2
Combining multiple keyword sets through either appending or document fusion improves retrieval effectiveness compared to a single reformulation. Appending merges all generated keywords into one expanded query, while fusion runs each keyword set as a separate query and merges their results. Both methods aggregate multiple evidence sources.

### Mechanism 3
Incorporating relevance feedback (pseudo or human) into the instruction context improves reformulations by grounding them in actual document content. Prepending a context string with feedback documents to the paraphrased instructions guides the LLM to generate reformulations that align with relevant content.

## Foundational Learning

- Concept: Zero-shot prompting with LLMs
  - Why needed here: The methods rely on generating query reformulations without any labeled training data, using only natural language instructions.
  - Quick check question: Can you explain the difference between zero-shot and few-shot prompting in the context of query reformulation?

- Concept: Query reformulation and vocabulary mismatch
  - Why needed here: QR aims to expand or paraphrase queries to better match document vocabulary, addressing the vocabulary mismatch problem in IR.
  - Quick check question: What is the vocabulary mismatch problem and how does query reformulation help solve it?

- Concept: Ensemble methods in NLP
  - Why needed here: The core contribution is using multiple paraphrased instructions to create an ensemble of reformulations, improving over single-instruction approaches.
  - Quick check question: How does ensembling multiple prompts or reformulations improve performance compared to a single prompt?

## Architecture Onboarding

- Component map: Base QR instruction → Paraphrasing step → N paraphrased instructions → Each instruction + query → LLM generator → N keyword sets → Append/Fusion logic → Reformulated query(s) → Retrieval engine → Ranked documents → (Optional) Feedback documents → Context prepending → RF variants

- Critical path: Paraphrase instructions → Generate keyword sets → Combine via append/fusion → Execute retrieval → (Optional) Rerank with MonoT5

- Design tradeoffs:
  - Number of instructions N vs. latency: More instructions improve performance but increase generation time
  - Append vs. fusion: Append is simpler but fusion may better handle diverse evidence
  - Feedback integration: Oracle feedback shows potential but real feedback may be noisy

- Failure signatures:
  - Performance degrades when paraphrased instructions produce too similar keyword sets
  - Fusion fails if document score distributions are incompatible
  - RF variants degrade if feedback documents are irrelevant

- First 3 experiments:
  1. Test single instruction vs. N=2 paraphrased instructions to verify ensemble benefit
  2. Compare append vs. fusion combination strategies on a small benchmark
  3. Evaluate effect of adding 1 vs. 5 feedback documents in RF variants

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of GenQREnsemble scale with the number of instructions (N) beyond 10? Is there a point of diminishing returns? The paper mentions that performance improves with more instructions, but only evaluates up to N=10.

### Open Question 2
How do different domain-specific instructions affect the performance of GenQREnsemble on various datasets? Are there certain domains where domain-specific instructions provide significant improvements? The paper demonstrates the benefits of using domain-specific instructions for DBPedia Entity Retrieval and mentions the potential for further improvements.

### Open Question 3
How does the filtering of query reformulations affect the interpretability and retrieval performance across different LLMs and datasets? The paper demonstrates the benefits of filtering reformulations using GPT-4 and mentions the potential for further improvements in interpretability.

## Limitations
- Performance sensitivity to paraphrasing quality - if paraphrased instructions don't yield meaningfully distinct keywords, ensemble benefits disappear
- Reliance on oracle feedback settings that may not translate to real-world noisy feedback scenarios
- Computational overhead from GPT-4-based interpretability filtering that may not generalize across domains

## Confidence

- High confidence: The ensemble prompting mechanism and its basic effectiveness
- Medium confidence: The oracle feedback results
- Medium confidence: The document fusion effectiveness

## Next Checks

1. Test instruction diversity by measuring keyword overlap between paraphrased instructions - if overlap exceeds 70%, ensemble benefit may be artifactual
2. Validate RF variants with noisy vs. oracle feedback to measure real-world degradation in performance
3. Conduct computational cost analysis comparing single vs. ensemble approaches to quantify latency-accuracy tradeoff