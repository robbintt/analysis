---
ver: rpa2
title: Active Legibility in Multiagent Reinforcement Learning
arxiv_id: '2410.20954'
source_url: https://arxiv.org/abs/2410.20954
tags:
- agents
- agent
- legibility
- maal
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Multiagent Active Legibility (MAAL), a novel
  framework designed to enhance collaboration in multiagent reinforcement learning
  (MARL) by improving action legibility. The core idea is to allow agents to convey
  their intentions more clearly, reducing ambiguity for observers.
---

# Active Legibility in Multiagent Reinforcement Learning

## Quick Facts
- arXiv ID: 2410.20954
- Source URL: https://arxiv.org/abs/2410.20954
- Authors: Yanyu Liu; Yinghui Pan; Yifeng Zeng; Biyang Ma; Doshi Prashant
- Reference count: 40
- One-line primary result: MAAL improves multiagent collaboration by enhancing action legibility through KL-divergence based reward shaping, achieving higher success rates and faster goal recognition.

## Executive Summary
This paper introduces Multiagent Active Legibility (MAAL), a novel framework that enhances collaboration in multiagent reinforcement learning (MARL) by improving action legibility. MAAL achieves this through a reward shaping mechanism that minimizes the Kullback-Leibler divergence between an agent's true goal and the observer's prediction, effectively guiding agents to take more informative actions. The framework was evaluated on two problem domains: Lead-Follow Maze and Simple Navigation, showing significant improvements in both speed and accuracy of goal recognition compared to several MARL baselines.

## Method Summary
MAAL uses reward shaping with KL divergence gain to encourage legible behavior in multiagent settings. The framework integrates with various RL algorithms through legibility-weighted rewards, where agents use plan recognition (Bayesian learning or LSTM) to infer others' goals. The core mechanism involves augmenting the reward function with the difference in KL-divergence between the observer's belief and the agent's true goal before and after an action, encouraging actions that reduce ambiguity in goal prediction.

## Key Results
- MAAL+QL achieved a success rate of 0.89±0.05 in the Lead-Follow Maze, outperforming IQL, VDN, QMIX, and MADDPG baselines.
- MAAL reduced prediction time ratio by nearly half, indicating faster goal identification.
- These improvements translated into higher task completion rates and shorter training times, demonstrating MAAL's effectiveness in enhancing multiagent collaboration.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KL-divergence based reward shaping improves action legibility in MARL.
- Mechanism: The reward function is augmented with the difference in KL-divergence between the observer's belief and the agent's true goal before and after an action is executed. This encourages actions that reduce ambiguity in goal prediction.
- Core assumption: The observer can model the agent's policy and update beliefs in real-time using Bayesian inference or neural networks like LSTM.
- Evidence anchors:
  - [abstract]: "MAAL achieves this through a reward shaping mechanism that minimizes the Kullback-Leibler divergence between an agent's true goal and the observer's prediction"
  - [section]: "We utilize the reward shaping technique to enable the agent's behavior to be legible in solving MDP with the new objective function in Eq. 6. We define the KL-divergence Gain (KLG), denoted by ∆ DKL(Ai), as the difference of DKL(ˆb i ||gi) before and after the action ai t−1 is executed in the state st−1."
- Break condition: If the observer's belief model is inaccurate or the belief update mechanism is too slow, the reward shaping signal may mislead the agent rather than improve legibility.

### Mechanism 2
- Claim: Plan recognition via Bayesian update or LSTM enables real-time goal inference by observers.
- Mechanism: Observers update their belief over the agent's goals based on observed actions and states using Bayesian inference or LSTM networks that process sequential data.
- Core assumption: Observers have access to the agent's observations and actions, and the goal space is discrete and known.
- Evidence anchors:
  - [abstract]: "MAAL significantly improved both the speed and accuracy of goal recognition compared to several MARL baselines"
  - [section]: "Additionally, neural networks, like Recurrent Neural Networks (RNN) or Long Short-Term Memory (LSTM), can be incorporated as I i due to their ability to process sequential data."
- Break condition: If the state-action space is continuous and high-dimensional, belief updates become computationally expensive, limiting real-time inference.

### Mechanism 3
- Claim: The LI-POMDP framework unifies legibility and interactive modeling for MARL.
- Mechanism: The framework extends POMDP with legibility rewards and belief modeling, allowing agents to optimize for both task completion and legibility.
- Core assumption: The problem can be decomposed into sub-goals, and each agent's goal is known to itself but not to others.
- Evidence anchors:
  - [abstract]: "The framework was evaluated on two problem domains: Lead-Follow Maze and Simple Navigation"
  - [section]: "Definition 1. For a subject agent Ai ∈ {A1, ..., AN }, a legible I-POMDP (LI-POMDP) is defined as: LI-P OM DPi = {S, A, T i, Oi, Ωi, Ri, G, B, I i, P i, Ri}"
- Break condition: If the number of agents or goals increases, the belief space grows exponentially, making real-time inference intractable.

## Foundational Learning

- Concept: KL-divergence and its role in measuring belief divergence
  - Why needed here: KL-divergence quantifies the difference between the observer's belief and the agent's true goal, which is used to shape rewards and improve legibility.
  - Quick check question: What is the formula for KL-divergence between two probability distributions P and Q?

- Concept: Bayesian belief updating in sequential decision making
  - Why needed here: Observers use Bayesian updates to infer the agent's goals from observed actions and states, which is critical for real-time plan recognition.
  - Quick check question: How does Bayesian updating work in POMDPs?

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The LI-POMDP framework extends POMDPs to include legibility rewards and belief modeling, which is essential for understanding the theoretical foundation of MAAL.
  - Quick check question: What are the key components of a POMDP?

## Architecture Onboarding

- Component map: State → Policy → Action → State transition → Belief update → KL-gain → Reward shaping → Policy update
- Critical path: The agent's policy generates an action based on the current state, which leads to a state transition. The observer then updates their belief about the agent's goal, calculates the KL-divergence gain, and shapes the reward accordingly.
- Design tradeoffs:
  - Computational complexity vs. real-time inference (Bayesian vs. LSTM)
  - Reward shaping weight (β) vs. task completion vs. legibility
  - Discrete vs. continuous state-action space (Q-learning vs. DQN/DDPG)
- Failure signatures:
  - High KL-divergence gain but low task completion → β too high
  - Belief update too slow → Use LSTM or reduce state space
  - Agent cycles between states → Reward shaping breaks completeness
- First 3 experiments:
  1. Implement MAAL with Q-learning in a simple grid world with 2 agents and 2 goals. Measure PCR and PTR.
  2. Compare MAAL+QL vs. IQL in the Lead-Follow Maze domain. Plot success rate and training time.
  3. Implement MAAL+DQN in the Simple Navigation domain with 3 agents and 6 landmarks. Measure reward and convergence speed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MAAL perform in environments with more than three agents or with varying levels of agent heterogeneity?
- Basis in paper: [explicit] The paper mentions the need for further testing in more challenging environments like StarCraft Multiagent Challenge, indicating current limitations in testing with larger or more complex agent setups.
- Why unresolved: The current experiments focus on scenarios with up to three agents, which may not fully capture the scalability and robustness of MAAL in larger or more diverse multiagent systems.
- What evidence would resolve it: Conducting experiments in environments with more agents or heterogeneous agent types, comparing MAAL's performance against MARL baselines, and analyzing the impact of increased complexity on legibility and collaboration.

### Open Question 2
- Question: How does the choice of plan recognition method (e.g., Bayesian learning vs. LSTM) affect the performance and computational efficiency of MAAL?
- Basis in paper: [explicit] The paper uses both Bayesian learning and LSTM for plan recognition in different experiments, suggesting a need to understand their relative impacts on MAAL's effectiveness.
- Why unresolved: The paper does not provide a direct comparison of different plan recognition methods within the MAAL framework, leaving uncertainty about the optimal choice for various scenarios.
- What evidence would resolve it: Implementing and comparing multiple plan recognition methods within MAAL across different environments, measuring both performance metrics (e.g., success rate, training time) and computational costs.

### Open Question 3
- Question: What is the optimal balance between legibility and task efficiency in MAAL, and how does it vary across different tasks and environments?
- Basis in paper: [explicit] The paper discusses the trade-off between legibility and task completion, noting that higher legibility weights can reduce task efficiency, but does not explore this balance systematically.
- Why unresolved: The experiments primarily focus on specific legibility weights without exploring a broader range or task-specific optimizations, leaving questions about generalizability.
- What evidence would resolve it: Conducting a systematic study of legibility weights across diverse tasks and environments, analyzing the impact on both legibility and task efficiency, and identifying patterns or guidelines for optimal weight selection.

## Limitations

- Unknown neural network architectures for belief estimation models in continuous state spaces, particularly for the LSTM models used in the Simple Navigation domain.
- Optimal value for the legibility reward weight β is not clearly established, with the paper noting that tuning this parameter is crucial for balancing legibility and task completion.
- Computational complexity of real-time Bayesian belief updates in high-dimensional state spaces, particularly as the number of agents or goals increases.

## Confidence

- High confidence in the core mechanism of KL-divergence based reward shaping for improving legibility
- Medium confidence in the effectiveness of LSTM-based plan recognition, due to limited architectural details
- Medium confidence in the LI-POMDP framework's scalability, as the paper only demonstrates results with 2-3 agents

## Next Checks

1. Implement MAAL with different values of β (legibility reward weight) and measure the tradeoff between legibility and task completion in the Lead-Follow Maze environment.
2. Compare the performance of Bayesian vs. LSTM-based belief estimation in the Simple Navigation domain to determine which method is more effective in continuous state spaces.
3. Test MAAL with a larger number of agents (4+) and goals to evaluate the scalability of the framework and identify potential computational bottlenecks.