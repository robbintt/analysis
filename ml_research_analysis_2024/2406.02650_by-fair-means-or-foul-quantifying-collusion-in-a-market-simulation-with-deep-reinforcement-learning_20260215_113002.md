---
ver: rpa2
title: 'By Fair Means or Foul: Quantifying Collusion in a Market Simulation with Deep
  Reinforcement Learning'
arxiv_id: '2406.02650'
source_url: https://arxiv.org/abs/2406.02650
tags:
- agents
- price
- pricing
- prices
- profit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study employs deep reinforcement learning (DRL) agents in a
  market simulation to explore whether autonomous pricing algorithms can converge
  to collusive outcomes without explicit communication. Agents use Proximal Policy
  Optimization (PPO) and Deep Q-Networks (DQN) in a Bertrand-style oligopoly model,
  adjusting prices based on discrete action spaces derived from logarithmic distributions.
---

# By Fair Means or Foul: Quantifying Collusion in a Market Simulation with Deep Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2406.02650
- **Source URL**: https://arxiv.org/abs/2406.02650
- **Reference count**: 7
- **Primary result**: DRL agents consistently converge to supracompetitive prices and profits approaching monopoly levels, even without explicit communication

## Executive Summary
This study investigates whether deep reinforcement learning (DRL) agents can achieve collusive pricing outcomes in a market simulation without explicit communication. Using Proximal Policy Optimization (PPO) and Deep Q-Networks (DQN) in a Bertrand-style oligopoly model, the research demonstrates that autonomous pricing agents consistently converge to supracompetitive prices and profits exceeding competitive benchmarks. The findings are robust across variations in agent count (3 and 5 agents), bias parameters (µ = 0, 0.5, 1), and observation scope (full vs. restricted price visibility), suggesting algorithmic collusion is an emergent property of DRL pricing agents in oligopolistic markets.

## Method Summary
The researchers created a market simulation where DRL agents compete in a Bertrand oligopoly, adjusting prices based on discrete action spaces derived from logarithmic distributions. Agents use either PPO or DQN algorithms to maximize profits through iterative pricing decisions. The simulation varies market conditions including the number of competing agents, price observation capabilities, and bias parameters that influence initial pricing strategies. Collusion is measured through average profit gains relative to competitive benchmarks, with values exceeding 0.7 indicating high collusion intensity. The discrete action space constrains price choices while maintaining sufficient granularity for meaningful competition.

## Key Results
- DRL agents consistently converge to supracompetitive prices and profits exceeding competitive benchmarks across all experimental configurations
- Collusion intensity (∆ > 0.7) indicates high collusion across different numbers of agents and observation scopes
- Some runs achieve outcomes beyond monopoly levels (∆ > 1), suggesting coordination beyond traditional collusive behavior
- Oscillation patterns in pricing emerge even when competitor prices are hidden, indicating implicit coordination mechanisms

## Why This Works (Mechanism)
DRL agents learn optimal pricing strategies through repeated interactions with the market environment, where they receive feedback on profit outcomes from their pricing decisions. The reinforcement learning framework enables agents to discover collusive equilibria without explicit communication by rewarding strategies that maintain supracompetitive prices. When multiple agents independently learn to avoid price wars and maintain higher prices, they collectively achieve monopoly-like profits. The discrete action space creates a structured decision environment where agents can develop stable pricing patterns that resist deviation, while the iterative nature of the simulation allows for the emergence of coordinated behavior through mutual adaptation.

## Foundational Learning

**Reinforcement Learning Basics**: Understanding how agents learn through reward maximization is essential to grasp why DRL can discover collusive strategies. *Why needed*: The entire mechanism relies on agents optimizing for profits through trial and error. *Quick check*: Can explain the difference between policy-based (PPO) and value-based (DQN) approaches.

**Bertrand Competition Model**: The classic economic framework for price competition in oligopolies provides the theoretical foundation for interpreting DRL outcomes. *Why needed*: Without understanding the competitive benchmark, collusion cannot be properly measured. *Quick check*: Can describe why Bertrand competition typically leads to competitive pricing.

**Collusion Detection Metrics**: Measuring collusion through profit gains relative to competitive benchmarks requires understanding how to quantify anti-competitive behavior. *Why needed*: The study's main findings depend on interpreting ∆ values. *Quick check*: Can explain why ∆ > 0.7 indicates high collusion intensity.

**Discrete vs Continuous Action Spaces**: The choice of logarithmic discrete pricing actions shapes the range of possible strategies and emergent behaviors. *Why needed*: The action space constrains what agents can learn and how coordination emerges. *Quick check*: Can compare implications of discrete versus continuous pricing in market simulations.

## Architecture Onboarding

**Component Map**: Environment -> Agents (PPO/DQN) -> Market Simulation -> Profit Feedback -> Agent Update

**Critical Path**: Agent selects price → Market determines demand and profits → Profit feedback sent to agent → Agent updates policy/value function → Next pricing decision

**Design Tradeoffs**: Discrete action spaces provide computational efficiency and structured exploration but may limit pricing granularity compared to continuous spaces. Symmetric agent architectures simplify analysis but reduce realism compared to heterogeneous firm capabilities.

**Failure Signatures**: Lack of convergence to supracompetitive prices, oscillation without profit gains, or failure to maintain stable pricing patterns would indicate unsuccessful collusion. Agents that consistently undercut competitors or fail to learn profitable strategies represent unsuccessful collusion outcomes.

**First Experiments**:
1. Run baseline simulation with 3 agents using PPO to establish baseline collusion metrics
2. Test DQN agents under restricted observation conditions to verify collusion without price visibility
3. Vary bias parameter µ to examine how initial conditions affect convergence to collusive equilibria

## Open Questions the Paper Calls Out
None

## Limitations
- Discrete logarithmic action space may constrain emergent behaviors compared to continuous pricing
- Specific demand function parameters (fixed elasticity of -1.5) may not reflect real-world market conditions
- Symmetric agents with identical learning architectures differ from heterogeneous real-world markets

## Confidence

**Core claim (DRL agents achieve collusive outcomes)**: High
**Specific profit measures and collusion intensity metrics**: Medium
**Oscillation patterns as evidence of implicit coordination**: Medium

## Next Checks

1. Test algorithm performance under continuous action spaces to verify whether discrete pricing constraints influence collusion patterns
2. Introduce asymmetric demand functions and heterogeneous agent architectures to assess robustness to market heterogeneity
3. Implement alternative collusion detection metrics (such as cross-correlation analysis of price movements) to confirm implicit coordination beyond profit-based measures