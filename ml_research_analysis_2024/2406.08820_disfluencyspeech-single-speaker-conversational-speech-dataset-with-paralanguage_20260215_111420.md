---
ver: rpa2
title: DisfluencySpeech -- Single-Speaker Conversational Speech Dataset with Paralanguage
arxiv_id: '2406.08820'
source_url: https://arxiv.org/abs/2406.08820
tags:
- speech
- dataset
- transcript
- audio
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DisfluencySpeech, a single-speaker, studio-quality
  English speech dataset with paralanguage annotations derived from the Switchboard
  corpus. The dataset includes nearly 10 hours of utterances with detailed annotations
  for disfluencies (filled pauses, explicit editing terms, discourse markers, restarts)
  and non-speech sounds (laughter, sighs).
---

# DisfluencySpeech -- Single-Speaker Conversational Speech Dataset with Paralanguage

## Quick Facts
- arXiv ID: 2406.08820
- Source URL: https://arxiv.org/abs/2406.08820
- Authors: Kyra Wang; Dorien Herremans
- Reference count: 26
- Key outcome: Single-speaker conversational speech dataset with paralanguage annotations for predictive TTS synthesis research

## Executive Summary
This paper introduces DisfluencySpeech, a single-speaker, studio-quality English speech dataset with paralanguage annotations derived from the Switchboard corpus. The dataset includes nearly 10 hours of utterances with detailed annotations for disfluencies (filled pauses, explicit editing terms, discourse markers, restarts) and non-speech sounds (laughter, sighs). Three different transcripts are provided at varying levels of information removal to support research on predictive TTS synthesis of paralanguage. The authors trained benchmark Transformer TTS models on each transcript and evaluated them using MCD and CER metrics. While the model trained on the full transcript achieved reasonable MCD (3.68 dB), CER was high (15.01%) due to alignment issues. Models trained on more minimal transcripts failed to converge, highlighting the brittleness of Transformers to missing transcript information.

## Method Summary
The dataset consists of nearly 10 hours of utterances derived from the Switchboard corpus, recorded by a single speaker in a professional studio. Three transcript variants are provided: Transcript A (full with non-speech events removed), Transcript B (A with filled pauses, editing terms, and discourse markers removed), and Transcript C (B with false starts removed). Benchmark Transformer TTS models were trained using the fairseq S2 speech synthesis toolkit with LJ Speech hyperparameters for 30,000 steps on a Tesla V100-DGXS GPU. Models were evaluated using Mean Cepstral Distortion (MCD) and Character Error Rate (CER) metrics.

## Key Results
- Transformer model trained on full transcript achieved MCD of 3.68 dB and CER of 15.01%
- Models trained on minimal transcripts (B, C) failed to converge with CER values of 60.07% and 55.66% respectively
- Dataset successfully demonstrates controlled ablation of disfluency modeling complexity for TTS research

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Three transcript levels enable controlled ablation of disfluency modeling complexity for TTS
- **Mechanism**: By progressively removing non-sentence elements across transcripts A→B→C, the dataset allows researchers to isolate the contribution of each paralanguage component to model performance
- **Core assumption**: Disfluencies and non-speech sounds contribute differently to semantic interpretation
- **Evidence anchors**: Abstract and section descriptions of three transcript variants
- **Break condition**: If removing disfluencies doesn't affect semantic interpretation or synthesis quality

### Mechanism 2
- **Claim**: Single-speaker dataset overcomes speaker variability limitations in multi-speaker datasets for conversational TTS
- **Mechanism**: Having one speaker recreate 10 hours of expressive utterances eliminates inter-speaker variability, allowing TTS models to focus on learning paralanguage patterns
- **Core assumption**: Speaker consistency is more valuable than diversity for learning paralanguage patterns
- **Evidence anchors**: Abstract and section discussing multi-speaker dataset limitations
- **Break condition**: If paralanguage patterns vary significantly across speakers

### Mechanism 3
- **Claim**: Studio-quality recordings with controlled environment ensure consistent acoustic conditions for TTS training
- **Mechanism**: Recording in an acoustically-treated professional studio creates consistent acoustic signature across all utterances
- **Core assumption**: Acoustic consistency across recordings is more important than naturalistic recording conditions
- **Evidence anchors**: Section describing controlled recording environment
- **Break condition**: If TTS models can handle acoustic variability effectively

## Foundational Learning

- **Concept**: Transcript normalization and disfluency annotation
  - Why needed here: The dataset requires understanding how different levels of transcript information affect TTS synthesis
  - Quick check question: What's the difference between transcript A (full annotations) and transcript C (minimal) in terms of disfluency content?

- **Concept**: Forced alignment and phonetic representation
  - Why needed here: The dataset provides MFA resources and requires understanding how audio-to-text alignment works for training TTS models
  - Quick check question: How does Montreal Forced Aligner handle disfluent speech compared to clean speech?

- **Concept**: Transformer-based TTS architecture limitations
  - Why needed here: The benchmark results show Transformers struggle with missing transcript information
  - Quick check question: Why did Transformer models trained on transcripts B and C fail to converge while transcript A succeeded?

## Architecture Onboarding

- **Component map**: Audio recording → Transcript generation with three variants → Forced alignment processing → TTS model training → Evaluation with MCD/CER metrics → Analysis of alignment failures
- **Critical path**: Studio recording → Transcript variant creation → Forced alignment → Transformer model training → MCD/CER evaluation
- **Design tradeoffs**: Studio-quality single-speaker recording provides consistency but lacks speaker diversity; three transcript variants enable controlled ablation but increase complexity
- **Failure signatures**: High CER values (15.01% for transcript A, 55-60% for transcripts B/C) indicate alignment failures; MCD values around 3.7-5.3 dB show phonetic similarity issues
- **First 3 experiments**:
  1. Train baseline Transformer on transcript A and evaluate MCD/CER to establish reference performance
  2. Train Transformer on transcript B and C, observe convergence failures and analyze alignment issues
  3. Implement monotonic alignment constraint and retrain on transcript C to test if alignment issues can be resolved

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the lack of spontaneous speech in DisfluencySpeech affect the performance of TTS models trained on it compared to models trained on spontaneous speech datasets?
- Basis in paper: [explicit] The authors acknowledge that the dataset was built by reading aloud an existing corpus, which is a form of acting rather than spontaneous speech production
- Why unresolved: The paper does not provide empirical data comparing model performance on spontaneous versus acted speech datasets
- What evidence would resolve it: A comparative study evaluating TTS model performance on DisfluencySpeech versus a dataset of spontaneous conversational speech with similar annotations

### Open Question 2
- Question: Can TTS models trained on DisfluencySpeech's most minimal transcript (Transcript C) learn to generate semantically appropriate disfluencies and non-speech sounds through auxiliary tasks or multi-task learning?
- Basis in paper: [explicit] The authors observed that the Transformer model trained on Transcript A could implicitly learn to generate non-speech sounds despite the lack of explicit annotations
- Why unresolved: The benchmark models failed to converge on Transcripts B and C, and the paper does not explore auxiliary task or multi-task learning approaches
- What evidence would resolve it: Training and evaluating TTS models using multi-task learning or auxiliary tasks with DisfluencySpeech transcripts

### Open Question 3
- Question: What is the impact of removing contextual information by joining Switchboard subutterances on the semantic interpretation and generation of paralanguage in TTS models?
- Basis in paper: [explicit] The authors note that joining subutterances removes the context of interleaving speech between speakers
- Why unresolved: The paper does not investigate the effects of this limitation on TTS model performance
- What evidence would resolve it: An ablation study comparing TTS model performance on DisfluencySpeech versus a version where subutterances are kept separate

### Open Question 4
- Question: How do more recent TTS architectures with hard monotonic alignment enforcement perform on DisfluencySpeech compared to the Transformer model used in the benchmark?
- Basis in paper: [explicit] The authors mention that the Transformer model's attention mechanism struggled with missing transcript information
- Why unresolved: The benchmark only used a basic Transformer model without advanced alignment techniques
- What evidence would resolve it: Training and evaluating modern TTS architectures on DisfluencySpeech and comparing their performance to the baseline Transformer model

## Limitations
- Single-speaker nature limits generalizability to real-world multi-speaker conversational scenarios
- High CER values indicate fundamental alignment issues suggesting current architecture limitations
- Studio-quality recording environment may not represent acoustic variability found in natural conversational settings

## Confidence
- **High**: The dataset successfully provides studio-quality single-speaker recordings with three levels of transcript annotation
- **Medium**: The controlled ablation approach through transcript variants is methodologically sound
- **Low**: The benchmark results demonstrating Transformer limitations are reliable indicators of fundamental architectural issues

## Next Checks
1. Test the dataset with alternative TTS architectures (e.g., non-autoregressive models, flow-based models) to determine if the alignment failures are specific to Transformers or inherent to the disfluent speech challenge
2. Conduct cross-speaker validation by training models on DisfluencySpeech and evaluating on multi-speaker conversational datasets to assess generalization capabilities
3. Implement and evaluate acoustic enhancement techniques to determine if the high-quality recording environment artificially improves performance compared to more realistic recording conditions