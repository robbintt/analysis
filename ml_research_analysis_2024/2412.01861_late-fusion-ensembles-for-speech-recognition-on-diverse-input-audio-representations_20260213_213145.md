---
ver: rpa2
title: Late fusion ensembles for speech recognition on diverse input audio representations
arxiv_id: '2412.01861'
source_url: https://arxiv.org/abs/2412.01861
tags:
- speech
- recognition
- e-branchformer
- ensemble
- fusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the performance of E-Branchformer models
  in Automatic Speech Recognition (ASR) when trained on diverse audio representations.
  The authors explore alternative features such as Gammatone filterbanks, Constant-Q
  transform, Modified Group Delay spectrograms, and Symlets, alongside standard Mel
  spectrograms and MFCCs.
---

# Late fusion ensembles for speech recognition on diverse input audio representations

## Quick Facts
- arXiv ID: 2412.01861
- Source URL: https://arxiv.org/abs/2412.01861
- Reference count: 40
- Primary result: E-Branchformer ensembles trained on diverse audio representations achieve 1%-14% relative improvement over state-of-the-art models

## Executive Summary
This paper investigates the performance of E-Branchformer models in Automatic Speech Recognition (ASR) when trained on diverse audio representations. The authors explore alternative features such as Gammatone filterbanks, Constant-Q transform, Modified Group Delay spectrograms, and Symlets, alongside standard Mel spectrograms and MFCCs. They propose a generalized decoding method for late fusion ensembles that combines predictions from models trained on different representations using weighted combinations of CTC and attention decoder scores. Experiments on four benchmark datasets (Librispeech, Aishell, Gigaspeech, TEDLIUMv2) demonstrate that ensembles of E-Branchformers trained on diverse representations achieve significant improvements over state-of-the-art models, with the diversity analysis revealing that different models capture complementary information.

## Method Summary
The method involves training multiple E-Branchformer models independently on different audio representations (MEL, MODGD, GAMMA, CQT, MFCC, SYMLET) and combining their predictions through late fusion using a generalized decoding approach. During inference, CTC and attention decoder scores from all models are weighted and combined stepwise, allowing parallel processing. The E-Branchformer architecture's branch structure enables effective learning from diverse input types while maintaining recognition capability. The ensemble leverages the complementary strengths of different acoustic feature types to reduce overall word error rates.

## Key Results
- Ensembles of E-Branchformers trained on diverse representations achieve 1%-14% relative improvement over state-of-the-art models
- MODGD features can outperform MEL features in some cases
- Diversity analysis shows different models capture complementary information, reducing WER by up to 12.3% compared to individual models
- Late fusion provides computational efficiency through parallelizable base model training and inference

## Why This Works (Mechanism)

### Mechanism 1
Diverse audio representations provide complementary information that reduces recognition errors. Different acoustic feature types capture distinct aspects of speech signals (temporal vs. frequency resolution, phase vs. magnitude information), and when combined through late fusion, these complementary strengths offset each other's weaknesses. The core assumption is that error patterns of models trained on different representations are sufficiently uncorrelated to enable error correction through ensembling.

### Mechanism 2
Late fusion ensembles maintain model independence while enabling coordinated predictions. Each base model processes its input representation independently through parallel training, and during inference, predictions are combined stepwise using weighted combinations of CTC and attention decoder scores, preserving computational efficiency. The core assumption is that parallel training and inference scales well without introducing harmful correlations between models.

### Mechanism 3
The E-Branchformer architecture's expressiveness enables effective learning from diverse representations. The branch structure, with separate paths for local and global dependencies, can adapt to capture features specific to different audio representations while maintaining overall recognition capability. The core assumption is that the E-Branchformer architecture is sufficiently flexible to learn optimal representations from diverse input types.

## Foundational Learning

- Concept: Audio signal representations and their properties
  - Why needed here: Understanding how different representations capture speech characteristics is essential for selecting complementary features and interpreting model behavior
  - Quick check question: What are the key differences between Mel spectrograms, Gammatone filterbanks, and Modified Group Delay features in terms of frequency resolution and noise robustness?

- Concept: Transformer-based ASR architectures
  - Why needed here: The E-Branchformer is a complex architecture that combines attention mechanisms with convolutional processing; understanding its components is crucial for troubleshooting and optimization
  - Quick check question: How does the E-Branchformer's branch structure differ from standard Conformer architecture, and what advantages does this provide for diverse input representations?

- Concept: Ensemble methods and diversity theory
  - Why needed here: The effectiveness of late fusion ensembles depends on understanding diversity metrics and combination strategies to maximize complementary information

- Concept: Sequence-to-sequence decoding with CTC and attention
  - Why needed here: The generalized decoding method combines multiple scorers; understanding this process is essential for implementing and debugging the fusion system
  - Quick check question: How does the weighted combination of CTC and attention scores work in the generalized decoding approach, and what role does the λ parameter play?

## Architecture Onboarding

- Component map: Frontend (audio feature extraction) -> Model (E-Branchformer) -> Training (independent per representation) -> Fusion (late fusion with weighted CTC/attention scores) -> Decoding (generalized beam search)

- Critical path: Audio preprocessing → feature extraction → independent E-Branchformer training per representation → parallel inference across all models → score combination using generalized decoding → final output sequence generation

- Design tradeoffs: Feature selection (more diverse features → better complementarity but higher computational cost), Model size (larger models capture more nuances but increase training/inference time), Fusion strategy (simple weighted averaging vs. more complex combination methods), Beam size (larger beams improve accuracy but increase computation)

- Failure signatures: High WER on specific subsets (feature mismatch or representation bias), No improvement from ensemble (insufficient diversity or correlated errors), Training instability (hyperparameter conflicts across representations), Slow inference (inefficient parallelization or oversized models)

- First 3 experiments: Train individual E-Branchformers on MEL and MODGD representations, compare baseline WERs; Implement late fusion between MEL and MODGD models, measure ensemble improvement; Add GAMMA representation to ensemble, verify incremental gains and diversity metrics

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed generalized decoding method perform compared to other late fusion techniques on ASR tasks? The paper proposes a generalized decoding method but doesn't provide comprehensive comparison with other late fusion techniques.

### Open Question 2
Can the proposed ensemble method be effectively combined with semi-supervised learning techniques like pseudolabeling? The paper doesn't explore potential benefits of combining the ensemble method with semi-supervised learning.

### Open Question 3
How does the diversity of the base models in the ensemble affect the overall performance? While the paper discusses diversity importance, it doesn't provide quantitative analysis of how different levels of diversity impact ensemble performance.

## Limitations

- Exact implementation details for Modified Group Delay feature extraction remain unspecified
- Weighted combination coefficients for late fusion are vaguely described without specific values
- Diversity analysis lacks rigorous quantification beyond simple difficulty measures
- Performance improvements on benchmark datasets may not translate to real-world deployment scenarios

## Confidence

- **High Confidence**: Late fusion ensembles achieve 1%-14% relative improvement over state-of-the-art models
- **Medium Confidence**: MODGD features can outperform MEL features in some cases
- **Low Confidence**: Diversity analysis reveals complementary information capture with up to 12.3% WER reduction

## Next Checks

1. Compute formal diversity metrics (Q-statistic, disagreement measure) between model predictions to quantify actual complementarity between different representations

2. Conduct controlled experiments varying MODGD feature extraction parameters to determine sensitivity and identify optimal configurations

3. Evaluate ensemble performance when trained on one dataset and tested on another to assess robustness across different acoustic conditions and speaking styles