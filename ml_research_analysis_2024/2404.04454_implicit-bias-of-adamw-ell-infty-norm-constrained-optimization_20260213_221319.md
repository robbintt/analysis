---
ver: rpa2
title: 'Implicit Bias of AdamW: $\ell_\infty$ Norm Constrained Optimization'
arxiv_id: '2404.04454'
source_url: https://arxiv.org/abs/2404.04454
tags:
- norm
- learning
- descent
- lemma
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes the implicit bias of AdamW, a popular optimization\
  \ algorithm for deep learning. The key insight is that AdamW implicitly performs\
  \ constrained optimization with respect to the \u2113\u221E norm."
---

# Implicit Bias of AdamW: $\ell_\infty$ Norm Constrained Optimization

## Quick Facts
- arXiv ID: 2404.04454
- Source URL: https://arxiv.org/abs/2404.04454
- Reference count: 40
- Key outcome: AdamW implicitly performs $\ell_\infty$ norm constrained optimization when converging with non-increasing learning rate schedule whose partial sum diverges.

## Executive Summary
This paper analyzes the implicit bias of AdamW, showing it performs constrained optimization with respect to the $\ell_\infty$ norm. The key insight is that AdamW can be viewed as a smoothed version of SignGD, which is normalized steepest descent under $\ell_\infty$ norm. When AdamW converges with appropriate learning rate schedules, it must converge to KKT points of the original loss under the constraint that the $\ell_\infty$ norm of parameters is bounded by the inverse of the weight decay factor. The paper also provides convergence analysis for normalized steepest descent with weight decay for convex loss, achieving a rate of $O(H/(T\lambda^2))$.

## Method Summary
The paper analyzes AdamW through its connection to normalized steepest descent (NSD) with weight decay (WD). By viewing Adam as a smoothed SignGD, the authors establish that AdamW implicitly performs Frank-Wolfe optimization over an $\ell_\infty$ norm ball when certain conditions are met. The analysis combines this theoretical framework with empirical validation on language modeling tasks, comparing AdamW against Adam with $\ell_2$ regularization.

## Key Results
- AdamW implicitly performs $\ell_\infty$ norm constrained optimization when converging with non-increasing learning rate schedule whose partial sum diverges
- For convex loss functions, normalized steepest descent with weight decay converges to constrained minimizer at rate $O(H/(T\lambda^2))$
- The average update size of AdamW is bounded by 1 when $\beta_1 = \beta_2$, and by a logarithmic factor when $\beta_1 < \beta_2$

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: AdamW implicitly performs constrained optimization under $\ell_\infty$ norm when it converges with a non-increasing learning rate schedule whose partial sum diverges.
- **Mechanism**: AdamW can be viewed as a smoothed version of SignGD, which is the normalized steepest descent with respect to $\ell_\infty$ norm. When AdamW converges under the specified learning rate conditions, the parameter iterates enter and remain within the $\ell_\infty$ norm ball of radius $1/\lambda$ (where $\lambda$ is the weight decay factor). This behavior is equivalent to Frank-Wolfe optimization over this constraint set.
- **Core assumption**: The learning rate schedule $\{\eta_t\}$ is non-increasing, $\sum_{t=1}^{\infty} \eta_t = \infty$, and $\beta_1 \leq \beta_2 < 1$.
- **Evidence anchors**:
  - [abstract]: "if AdamW converges with a non-increasing learning rate schedule whose partial sum diverges, it must converge to a KKT point of the original loss under the constraint that the $\ell_\infty$ norm of the parameter is bounded by the inverse of the weight decay factor."
  - [section 3.2]: "For any continuously differentiable function $L: \mathbb{R}^d \to \mathbb{R}$, initialization $x_0$, and learning rate schedule $\{\eta_t\}_{t=1}^{\infty}$ such that $\sum_{t=1}^{\infty} \eta_t = \infty$, if the iterates of NSD-WD $\{x_t\}_{t=0}^{\infty}$ on $L$ converges to some $x_{\infty}$, then $x_{\infty}$ is a KKT point..."
- **Break condition**: If $\beta_1 > \beta_2$, the iterates can converge outside the $\ell_\infty$ norm ball (as shown in Appendix B.3).

### Mechanism 2
- **Claim**: The average update size of AdamW is bounded by 1 when $\beta_1 = \beta_2$, and by a logarithmic factor when $\beta_1 < \beta_2$.
- **Mechanism**: The key technical insight is that despite individual updates potentially exceeding norm 1, the weighted average of updates (weighted by learning rates) remains bounded. This is proven through careful analysis of the moving averages and gradient terms in the AdamW update rule.
- **Core assumption**: The learning rate schedule is non-increasing and the gradient terms satisfy certain boundedness conditions.
- **Evidence anchors**:
  - [section 4.1]: "Lemma 4.2. Given any $\beta_1 \leq \beta_2 < 1$, suppose scalar sequences $\{v_t\}_{t=0}^{\infty}$ and $\{g_t\}_{t=1}^{\infty}$ satisfy that $v_0 \geq 0$, $v_1 > 0$ and $v_t - \beta_2 v_{t-1} \geq (1 - \beta_2)g_t^2$ for $t \geq 1$..."
  - [section 4]: "The main technical difficulty here lies in the proof of the third property. This is because for any single $t$, $\|\Delta_t\|$ could be larger than 1, which is different from the case of NSD-WD."
- **Break condition**: If the learning rate is not non-increasing, or if the boundedness conditions on gradients are violated.

### Mechanism 3
- **Claim**: For convex loss functions, normalized steepest descent with weight decay (NSD-WD) converges to the constrained minimizer at a rate of $O(H/(T\lambda^2))$.
- **Mechanism**: When the iterates enter the norm ball of radius $1/\lambda$, the NSD-WD algorithm becomes equivalent to Frank-Wolfe optimization. The convergence rate depends on the smoothness $H$ of the loss with respect to the chosen norm.
- **Core assumption**: The loss function is convex and has $H$-Lipschitz gradient with respect to the chosen norm.
- **Evidence anchors**:
  - [section 3.1]: "Theorem 3.5. Define $B = \max \{\|x_0\|, 1/\lambda\}$. For NSD-WD with learning rate schedule $\eta_t = 2/(\lambda(t+1))$, we have $L(x_t) - L(x^*) \leq 2H(1+\lambda B)^2/(t+2)\lambda^2$ for $t \geq 1$."
  - [section 3.1]: "We also provide a convergence analysis for normalized steepest descent with weight decay for convex loss, where the suboptimality against the constrained minimizer in norm ball of radius $1/\lambda$ vanishes is $O(H/T\lambda^2)$."
- **Break condition**: If the loss function is non-convex, the algorithm converges to KKT points rather than the global minimizer.

## Foundational Learning

- **Concept**: KKT (Karush-Kuhn-Tucker) conditions
  - Why needed here: The paper's main theoretical results characterize convergence to KKT points of constrained optimization problems. Understanding KKT conditions is essential to interpret what the algorithm converges to.
  - Quick check question: What are the three main conditions that define a KKT point for the problem $\min_{\|x\|_\infty \leq 1/\lambda} L(x)$?

- **Concept**: Normalized steepest descent
  - Why needed here: AdamW is shown to be a smoothed version of normalized steepest descent with respect to $\ell_\infty$ norm. The paper builds its analysis on properties of normalized steepest descent algorithms.
  - Quick check question: How does normalized steepest descent differ from standard gradient descent in terms of the direction it selects?

- **Concept**: Frank-Wolfe algorithm
  - Why needed here: The paper establishes an equivalence between normalized steepest descent with weight decay and Frank-Wolfe when iterates are within the constraint set. This equivalence is crucial for the convergence analysis.
  - Quick check question: What is the key difference between Frank-Wolfe updates and standard gradient descent updates?

## Architecture Onboarding

- **Component map**: Algorithm 1 (AdamW with decoupled weight decay) -> $\beta_1 \leq \beta_2 < 1$ condition -> $\ell_\infty$ norm constraint -> KKT point convergence
- **Critical path**: Ensuring the learning rate schedule satisfies $\sum_{t=1}^{\infty} \eta_t = \infty$ while being non-increasing, and that $\beta_1 \leq \beta_2 < 1$. These conditions are necessary for the theoretical guarantees to hold.
- **Design tradeoffs**: Using AdamW with $\ell_\infty$ implicit constraints trades off between the adaptive learning rates of Adam and the regularization benefits of weight decay. The choice of $\beta_1$ and $\beta_2$ affects whether the $\ell_\infty$ norm constraint is effectively enforced.
- **Failure signatures**: If $\beta_1 > \beta_2$, the iterates may converge outside the $\ell_\infty$ norm ball. If the learning rate schedule doesn't satisfy the divergence condition, convergence to KKT points is not guaranteed. If the gradients are too noisy, the theoretical bounds may not provide tight predictions.
- **First 3 experiments**:
  1. Implement the synthetic quadratic loss function from Equation 3 and compare $\ell_\infty$ vs $\ell_2$ normalized steepest descent with weight decay.
  2. Run AdamW on the PTB language modeling task with $\beta_1 = \beta_2 = 0.99$ and verify that the $\ell_\infty$ norm of parameters stays below $1/\lambda$.
  3. Create a counter-example by setting $\beta_1 > \beta_2$ and show that the iterates converge outside the $\ell_\infty$ norm ball.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does AdamW's implicit bias towards $\ell_\infty$ norm constrained optimization manifest in high-dimensional deep learning models beyond the synthetic example and PTB experiments?
- Basis in paper: [explicit] The paper states AdamW implicitly performs constrained optimization with respect to the $\ell_\infty$ norm, and provides theoretical and empirical evidence for this claim.
- Why unresolved: The experiments are limited to a specific language modeling task and a synthetic quadratic function. It's unclear how general this implicit bias is across different architectures, datasets, and loss functions.
- What evidence would resolve it: Empirical studies on a diverse set of deep learning tasks (e.g., image classification, object detection, generative modeling) comparing AdamW with other optimizers and analyzing the $\ell_\infty$ norm of the learned parameters.

### Open Question 2
- Question: What are the specific advantages of AdamW's implicit $\ell_\infty$ norm constraint compared to explicit $\ell_2$ regularization or other regularization techniques in terms of optimization efficiency and generalization?
- Basis in paper: [explicit] The paper mentions that AdamW achieves better optimization and generalization over Adam with $\ell_2$ regularization, but the theoretical understanding of this advantage is limited.
- Why unresolved: The paper provides theoretical justification for AdamW's implicit bias, but doesn't directly compare its practical benefits to other regularization methods.
- What evidence would resolve it: Ablation studies on the impact of different regularization techniques on training dynamics, convergence speed, and test performance across various deep learning tasks.

### Open Question 3
- Question: How does the implicit $\ell_\infty$ norm constraint of AdamW interact with other architectural components like normalization layers, skip connections, and attention mechanisms?
- Basis in paper: [inferred] The paper doesn't discuss the interaction of AdamW's implicit bias with specific architectural elements. However, it's known that these components can affect the effective learning rate and optimization dynamics.
- Why unresolved: The implicit bias might be influenced by the presence of these components, leading to different optimization trajectories and generalization properties.
- What evidence would resolve it: Empirical studies on architectures with and without these components, comparing AdamW's performance and analyzing the $\ell_\infty$ norm of the learned parameters.

## Limitations

- The theoretical results rely on full-batch optimization and may not directly translate to stochastic settings with mini-batch training.
- The analysis assumes specific conditions on learning rates and hyperparameters ($\beta_1 \leq \beta_2 < 1$) that may not always hold in practice.
- The experimental validation is limited to a specific language modeling task and doesn't fully explore the KKT point convergence behavior across diverse deep learning problems.

## Confidence

- **High confidence**: The characterization of AdamW as a smoothed SignGD and the resulting $\ell_\infty$ norm constraint when $\beta_1 \leq \beta_2 < 1$. This is well-supported by the theoretical analysis and experimental validation.
- **Medium confidence**: The convergence rate $O(H/(T\lambda^2))$ for convex loss functions. While the proof is rigorous, it depends on the full-batch assumption and specific learning rate schedules.
- **Medium confidence**: The experimental results showing $\ell_\infty$ norm constraints on PTB language modeling. The experiments are well-designed but limited in scope.

## Next Checks

1. **Test the $\beta_1 > \beta_2$ break condition**: Implement AdamW with $\beta_1 = 0.99$, $\beta_2 = 0.9$ and verify that the $\ell_\infty$ norm of parameters exceeds $1/\lambda$, confirming the theoretical prediction.

2. **Evaluate stochastic settings**: Run the same experiments with mini-batch optimization to assess how gradient noise affects the $\ell_\infty$ norm constraint and convergence behavior.

3. **Test non-convex landscapes**: Apply the analysis to a non-convex synthetic loss function and verify that AdamW converges to KKT points rather than global minima, as predicted by the theory.