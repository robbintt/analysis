---
ver: rpa2
title: 'Word2World: Generating Stories and Worlds through Large Language Models'
arxiv_id: '2405.06686'
source_url: https://arxiv.org/abs/2405.06686
tags:
- uni00000013
- uni00000011
- uni00000052
- world
- uni00000057
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Word2World, a system that leverages large
  language models (LLMs) to procedurally generate playable 2D game levels from stories
  without task-specific fine-tuning. The core idea is to break down the complex task
  of level generation into multiple steps: creating a story, extracting relevant information
  (characters, tiles, goals), and iteratively refining the world design based on LLM
  evaluations and conventional PCG checks.'
---

# Word2World: Generating Stories and Worlds through Large Language Models

## Quick Facts
- **arXiv ID**: 2405.06686
- **Source URL**: https://arxiv.org/abs/2405.06686
- **Reference count**: 6
- **Primary result**: Word2World generates playable 2D game levels from stories using LLMs without task-specific fine-tuning, achieving 90% playability with largest models performing best.

## Executive Summary
Word2World introduces a novel approach to procedural content generation that leverages large language models to create playable 2D game levels from stories. The system decomposes the complex task of level generation into sequential sub-tasks: story creation, information extraction (characters, tiles, goals), and iterative refinement. By combining LLM-based evaluations with conventional PCG checks, Word2World produces coherent worlds that align with generated narratives. The method was validated through ablation studies and tested across multiple LLMs, demonstrating that the largest models achieve the best results on coherence, novelty, and agent rewards.

## Method Summary
Word2World breaks down level generation into multiple steps using LLMs without task-specific fine-tuning. The process starts with generating a story, then extracting structured information including characters, tiles, and goals. This information guides iterative world generation rounds, where LLM evaluations and conventional PCG checks (like A* pathfinding) provide feedback for refinement. After world generation, tiles are retrieved from curated datasets using DistilBERT embeddings and cosine similarity to ensure semantic alignment with the story. The system uses multiple LLM calls per world to improve controllability and coherence, with each step feeding context to the next.

## Key Results
- Achieved 90% playability across generated worlds
- claude-3-opus-20240229 performed best on coherence, novelty, and LLM agent rewards
- Ablation studies confirmed importance of each step: Word2World outperformed direct generation and variants omitting goal extraction or important tiles
- Generated diverse worlds with varying sizes and patterns while maintaining narrative alignment

## Why This Works (Mechanism)

### Mechanism 1
Decomposing level generation into sequential sub-tasks (story → information extraction → iterative refinement) improves LLM controllability and coherence. By splitting the complex task into smaller, well-defined steps, each step can be prompted with focused context. The iterative rounds allow feedback to refine the world design progressively, mitigating the known issue of LLMs producing incoherent or unplayable content when given large, unconstrained generation tasks.

### Mechanism 2
Iterative refinement with LLM evaluation and conventional PCG checks produces playable and novel worlds. After initial world generation, the system evaluates the world using both LLM-based coherence checks and conventional PCG methods (e.g., A* pathfinding). Results are fed back into the next round, guiding the LLM to improve problematic areas (e.g., unreachable objectives, lack of novelty). This hybrid evaluation loop ensures both playability and diversity.

### Mechanism 3
Using similarity-based tile retrieval from a curated dataset ensures visual coherence and semantic alignment with the generated story. After the world is generated as a tile map, each tile description is embedded using DistilBERT and matched against a curated dataset of tile images and descriptions via cosine similarity. This ensures that the final world uses tiles that semantically fit the story and extracted descriptions, enhancing coherence.

## Foundational Learning

- **Large Language Models (LLMs) and zero-shot/few-shot capabilities**: Word2World relies on LLMs to generate stories, extract structured information, and iteratively refine world designs without task-specific fine-tuning. *Quick check: Can you explain how zero-shot learning allows an LLM to perform a task it was not explicitly trained on, using only a prompt?*

- **Procedural Content Generation (PCG) and evaluation metrics**: Word2World uses PCG techniques (e.g., A* pathfinding) to ensure generated worlds are playable and novel, and evaluates coherence via LLM-based prompts. *Quick check: What is the purpose of using both LLM-based and conventional PCG methods for evaluation in Word2World?*

- **Text-to-image embedding and similarity search**: Word2World retrieves tiles from a dataset by embedding descriptions and finding the most similar match, ensuring semantic alignment between story elements and visual tiles. *Quick check: How does using cosine similarity between embeddings help in retrieving the most semantically appropriate tile for a given description?*

## Architecture Onboarding

- **Component map**: LLM instances (story creation, information extraction, world generation, evaluation) → Tile datasets (environment/interactive tiles and character tiles) → Tile retrieval system (DistilBERT embeddings + cosine similarity) → Evaluation system (LLM-based coherence checks + conventional PCG checks like A* pathfinding) → Iterative refinement loop

- **Critical path**: 1. Generate story via LLM 2. Extract characters, tiles, goals, important/walkable/interactive tiles via LLM 3. Generate world environment via LLM 4. Place characters and interactive objects via LLM 5. Fix tile map algorithmically (row padding, duplicate removal) 6. Evaluate world (LLM coherence + PCG playability) 7. Iterate (if rounds remain) with feedback 8. Retrieve tiles via similarity search 9. Assemble final world

- **Design tradeoffs**: Using multiple LLM calls per world increases cost and latency but improves controllability and coherence. Relying on external LLM APIs limits local deployment and increases operational cost. The curated tile dataset limits expressiveness to available tiles but ensures semantic alignment.

- **Failure signatures**: High incoherence between story and world → likely issue in information extraction or tile retrieval. Low playability rates → likely issue in world generation or missing goal extraction. Excessive token usage or API errors → likely issue in prompt design or LLM limits. Tile scaling issues → dataset or retrieval mismatch.

- **First 3 experiments**: 1. Run ablation: direct-generation (no steps) vs Word2World (full pipeline) to verify step-wise improvement in coherence and playability. 2. Vary number of iterative rounds (0, 1, 3) to quantify impact of refinement on world quality. 3. Test different LLMs (claude-3-opus-20240229, gpt-4-turbo-2024-04-09) to compare performance on coherence, novelty, and agent rewards.

## Open Questions the Paper Calls Out

### Open Question 1
How can Word2World be extended to support 3D games or non-top-down perspectives while maintaining coherence and playability? The paper lists "Word2World is designed only for top-down games" as a key limitation and discusses extending it to 2D platformers or 3D games as a future direction. The current pipeline relies on 2D tile-based representations and 2D pathfinding (A* agent). Extending to 3D would require fundamentally different spatial representations, navigation systems, and possibly different LLM prompting strategies.

### Open Question 2
What is the optimal balance between world generation parameters (story paragraphs, objectives, important tiles) to maximize both novelty and playability across different game genres? The paper shows correlation between these parameters and evaluation metrics, but notes that increasing them doesn't necessarily improve coherence or character tile accuracy. The current approach uses empirical parameter tuning without a systematic method for genre-specific optimization or adaptive parameter selection.

### Open Question 3
How does the cost-effectiveness of Word2World compare to traditional procedural content generation methods when scaled to commercial game development? The paper notes that one run costs $0.5-$1 per generation and cannot run fully on local hardware due to API dependencies. There's no comparison of generation speed, quality, or cost per tile/level against traditional PCG algorithms, despite this being listed as a limitation alongside the lack of open-source LLMs.

## Limitations
- Restricted to top-down 2D games with scaling issues for larger objects
- Relies on external LLM APIs, introducing cost ($0.5-$1 per generation) and reproducibility constraints
- Curated tile dataset limits expressiveness and may constrain world diversity

## Confidence
- **High confidence**: Decomposition approach improves controllability and coherence, supported by ablation studies
- **Medium confidence**: Iterative refinement with dual evaluation reliably produces playable worlds, though specific contribution of each evaluation type isn't fully isolated
- **Medium confidence**: Similarity-based tile retrieval ensures semantic alignment, but effectiveness depends on dataset completeness and embedding quality

## Next Checks
1. **Ablation study with open-source LLMs**: Replace proprietary LLM APIs with open-source models (e.g., Llama 3) to assess whether performance depends on model size/complexity or the Word2World pipeline itself.

2. **Human evaluation of coherence**: Conduct user studies comparing Word2World worlds against ablation variants to validate whether LLM-based coherence metrics align with human perception of story-world alignment.

3. **Dataset scaling experiment**: Test tile retrieval with expanded/curated datasets (10x larger) to quantify impact on visual coherence and semantic alignment, and identify diminishing returns.