---
ver: rpa2
title: 'Temporally Consistent Dynamic Scene Graphs: An End-to-End Approach for Action
  Tracklet Generation'
arxiv_id: '2412.02808'
source_url: https://arxiv.org/abs/2412.02808
tags:
- temporal
- scene
- computer
- pages
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of generating temporally consistent
  action tracklets from video sequences, extending scene graph representations beyond
  individual frames to capture dynamic interactions over time. The proposed Temporally
  Consistent Dynamic Scene Graphs (TCDSG) framework introduces two key innovations:
  (1) a sequence-level bipartite matching objective that maintains stable query-triplet
  assignments across frames, and (2) temporally conditioned decoder queries that incorporate
  LSTM-based feedback from previous frames.'
---

# Temporally Consistent Dynamic Scene Graphs: An End-to-End Approach for Action Tracklet Generation

## Quick Facts
- arXiv ID: 2412.02808
- Source URL: https://arxiv.org/abs/2412.02808
- Reference count: 40
- This paper introduces TCDSG, achieving 39.1% temporal recall@50 on Action Genome, a 20.5% improvement over state-of-the-art methods

## Executive Summary
This paper addresses the challenge of generating temporally consistent action tracklets from video sequences by extending scene graph representations beyond individual frames. The proposed TCDSG framework introduces two key innovations: sequence-level bipartite matching that maintains stable query-triplet assignments across frames, and temporally conditioned decoder queries that incorporate LSTM-based feedback from previous frames. These modifications enable end-to-end tracking of subject-object relationships without requiring post-processing. Evaluated on Action Genome, OpenPVSG, and MEVA datasets, TCDSG achieves state-of-the-art temporal recall@k performance, with tR@50 improving from 18.6% to 39.1% on Action Genome.

## Method Summary
TCDSG extends transformer-based detection frameworks (DETR/Deformable DETR) with two key innovations: (1) sequence-level bipartite matching that caches ground-truth triplets and adds large penalties for reassigning them to different queries across frames, and (2) temporally conditioned decoder queries that incorporate LSTM-processed output embeddings from previous frames as input queries for the current frame. The method operates on video sequences with shared ResNet-50 backbone, parallel object and relationship decoder branches, and a combined loss function incorporating spatial, label, consistency, and reference point terms. Training uses AdamW optimizer for 20 epochs on 8 A100 GPUs with BLoad for efficient distributed learning on variable-length videos.

## Key Results
- TCDSG achieves tR@50 of 39.1% on Action Genome, improving from 18.6% baseline
- Temporal recall improvements come with slight frame-level recall reduction (from 30.6% to 29.4% for R@50)
- Method runs at ~20 fps using < 2 GB GPU memory at inference
- New annotations provided for MEVA dataset with persistent object IDs for long-range evaluation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal bipartite matching preserves query-triplet identities across frames by penalizing identity switches
- Mechanism: The algorithm caches ground-truth triplets when first assigned to decoder queries and adds a large penalty (λco) to any assignment that reassigns these triplets to different queries in subsequent frames
- Core assumption: Stable query-triplet bindings across frames will reduce tracklet fragmentation without requiring post-processing
- Evidence anchors:
  - [abstract] "a temporal bipartite matching strategy maintains stable query assignments across frames"
  - [section 3.4] "we cache each ground-truth triplet ⟨s, o, rs,o⟩ the first time it appears and bind it to the decoder query to which it was matched"
  - [section 3.4] "any assignment that would re-map this previously locked triplet to a different query receives a large penalty λco"
- Break condition: If objects frequently enter/exit the scene (especially same-class objects), the caching mechanism may incorrectly preserve assignments across distinct instances, leading to identity errors.

### Mechanism 2
- Claim: Temporally conditioned decoder queries incorporate LSTM-based feedback from previous frames to maintain temporal context
- Mechanism: Input queries at frame t are computed as a function of both learnable embeddings and LSTM-processed output embeddings from frame t-1, creating a cross-attentional feedback loop
- Core assumption: Maintaining an updated state through LSTM layers enables the model to retain temporal information and improve prediction stability
- Evidence anchors:
  - [abstract] "adaptive decoder queries augmented with inter-frame feedback inject temporal context directly into decoding"
  - [section 3.3] "the input queries at frame t are a function of both learnable embeddings and the output embeddings from frame t − 1"
  - [section 3.3] "a simple LSTM layer connecting t − 1 to t, ensuring that we maintain an updated state after each frame gets processed"
- Break condition: If the temporal dynamics change rapidly or the LSTM state becomes outdated, the feedback may introduce noise rather than useful context, degrading performance.

### Mechanism 3
- Claim: Sequence-level bipartite matching optimizes over evolving data trajectories rather than individual frames
- Mechanism: Instead of per-frame Hungarian matching, the objective is redefined to operate across time, persistently binding triplets to decoder queries throughout the sequence
- Core assumption: Optimizing over trajectories rather than isolated frames will produce more temporally coherent predictions
- Evidence anchors:
  - [abstract] "a sequence-level bipartite matching objective that maintains stable query-triplet assignments across frames"
  - [section 3.4] "Instead of optimizing over per-frame entities, we optimize over evolving data trajectories"
  - [section 3.4] "This sequence-aware formulation is implemented as a lightweight extension of Hungarian matching"
- Break condition: If the video contains long-term occlusions or interactions that change over extended periods, strict trajectory optimization may force incorrect persistent assignments.

## Foundational Learning

- Concept: Transformer-based detection frameworks (DETR, Deformable DETR)
  - Why needed here: TCDSG builds upon transformer architectures for both object detection and temporal tracking
  - Quick check question: How does DETR differ from traditional object detection pipelines in terms of end-to-end processing?

- Concept: Hungarian matching algorithm
  - Why needed here: The temporal bipartite matching mechanism extends the standard Hungarian matching used in DETR
  - Quick check question: What is the computational complexity of the Hungarian algorithm and how does it scale with the number of objects?

- Concept: LSTM (Long Short-Term Memory) networks
  - Why needed here: LSTM layers process temporal feedback from previous frames to maintain state across the video sequence
  - Quick check question: How do LSTM gates (input, forget, output) help in maintaining temporal information across frames?

## Architecture Onboarding

- Component map: Shared ResNet-50 backbone → Object branch (transformer) + Relationship branch (transformer) → Temporally Conditioned Queries (with LSTM) → Sequence-Level Bipartite Matching → Loss computation
- Critical path: Backbone → Query conditioning (LSTM feedback) → Dual-branch decoding → Temporal matching → Loss computation
- Design tradeoffs:
  - Frame-level accuracy vs. temporal consistency (slight R@K reduction for improved tR@K)
  - Memory overhead of caching assignments vs. computational simplicity
  - Static queries vs. adaptive queries (trade-off between stability and flexibility)
- Failure signatures:
  - High temporal recall but low frame-level recall suggests overly strict temporal constraints
  - Identity switches in crowded scenes indicate caching mechanism limitations
  - Degraded performance on long videos suggests LSTM state degradation
- First 3 experiments:
  1. Run baseline without temporal matching to establish frame-level performance
  2. Enable temporal matching with varying λco penalties to find optimal balance
  3. Test different LSTM hidden state sizes to optimize temporal context retention

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does TCDSG performance scale with video sequence length, particularly for videos exceeding 5 minutes?
- Basis in paper: [inferred] The paper mentions MEVA videos can be up to 5 minutes long and notes "long unbroken sequences expose transient query switches" but doesn't provide systematic analysis of performance degradation with increasing sequence length
- Why unresolved: The paper only briefly mentions the subsampling analysis showing tR@K improves with sparser sampling, but doesn't investigate the fundamental limits of temporal consistency maintenance across progressively longer sequences
- What evidence would resolve it: Systematic evaluation showing tR@K degradation curves across video lengths from seconds to multiple minutes/hours, identifying at what point temporal consistency fundamentally breaks down

### Open Question 2
- Question: Can TCDSG's temporal consistency mechanism be effectively extended to cross-camera tracking scenarios with non-overlapping fields of view?
- Basis in paper: [explicit] The authors state "we plan to explore Re-identification (ReID) capabilities across distinct cameras by having an additional ReID block" suggesting this is a planned future direction but not yet implemented
- Why unresolved: The current TCDSG framework operates within single video sequences and the paper only proposes but does not demonstrate cross-camera ReID extension
- What evidence would resolve it: Implementation and evaluation showing consistent query-triplet assignment maintenance across camera boundaries, with quantitative comparison of tR@K before and after ReID module integration

### Open Question 3
- Question: What is the impact of temporal consistency enforcement on computational efficiency and inference speed in real-time applications?
- Basis in paper: [inferred] The paper mentions TCDSG runs at "∼20 fps using < 2 GB GPU memory at inference" but doesn't analyze the computational overhead introduced by the sequence-level bipartite matching and LSTM feedback mechanisms
- Why unresolved: While the paper provides inference speed metrics, it doesn't break down the contribution of temporal consistency components to overall computational cost or compare against baseline framewise approaches
- What evidence would resolve it: Detailed profiling showing per-frame computational cost with and without temporal consistency mechanisms, including memory usage patterns and latency measurements across different video resolutions and frame rates

## Limitations

- Performance on highly dynamic scenes with frequent occlusions, camera motion, or significant appearance changes remains untested
- The caching mechanism for maintaining query-triplet assignments could struggle with same-class object interactions in crowded scenes, potentially leading to identity switches
- Temporal recall improvements come at the cost of some frame-level detection accuracy, indicating a fundamental trade-off between temporal consistency and per-frame performance

## Confidence

- **High**: Temporal bipartite matching mechanism and its implementation
- **Medium**: LSTM-based query conditioning effectiveness
- **Medium**: Overall state-of-the-art performance claims

## Next Checks

1. **Robustness testing**: Evaluate TCDSG performance on videos with significant camera motion and varying illumination conditions to assess real-world applicability
2. **Scalability analysis**: Test the method on longer video sequences (beyond 20 frames) to examine LSTM state degradation and caching mechanism limitations
3. **Ablation study**: Systematically vary the temporal consistency penalty λco to quantify the trade-off between temporal recall and frame-level detection accuracy across different application scenarios