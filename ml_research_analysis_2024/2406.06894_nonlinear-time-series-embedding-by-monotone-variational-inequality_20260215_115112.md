---
ver: rpa2
title: Nonlinear time-series embedding by monotone variational inequality
arxiv_id: '2406.06894'
source_url: https://arxiv.org/abs/2406.06894
tags: []
core_contribution: The paper proposes a method for unsupervised low-dimensional representation
  learning of nonlinear time series using monotone variational inequalities. The method
  assumes sequences arise from a common domain but each follows its own autoregressive
  model, encoded via low-rank regularization.
---

# Nonlinear time-series embedding by monotone variational inequality

## Quick Facts
- arXiv ID: 2406.06894
- Source URL: https://arxiv.org/abs/2406.06894
- Authors: Jonathan Y. Zhou; Yao Xie
- Reference count: 40
- The method achieves 78.8% average accuracy on UCR time series datasets with 400s runtime, outperforming simple baselines and performing close to deep learning methods like TS2Vec but using only 37% of the runtime.

## Executive Summary
This paper proposes a method for unsupervised low-dimensional representation learning of nonlinear time series using monotone variational inequalities. The approach assumes sequences arise from a common domain but each follows its own autoregressive model, encoded via low-rank regularization. By casting the problem as a convex matrix parameter recovery problem using monotone VIs, the method learns both the geometry of the entire domain and faithful individual sequence dynamics. The method shows competitive performance on real-world time-series data, symbolic text modeling, and RNA sequence clustering.

## Method Summary
The method casts nonlinear time series embedding as a convex matrix parameter recovery problem using monotone variational inequalities. It assumes sequences arise from a common domain but with individual autoregressive dynamics, enforced through low-rank regularization on the parameter matrix. The approach uses stochastic approximation of the monotone VI field for scalability, with nuclear norm regularization to promote low-rank structure. After solving the VI, singular value decomposition extracts low-dimensional embeddings for each sequence while preserving the shared domain geometry.

## Key Results
- Achieves 78.8% average accuracy on UCR time series classification datasets with 400s runtime
- Outperforms simple baselines and performs close to deep learning methods like TS2Vec while using only 37% of the runtime
- Successfully applies to symbolic text modeling and RNA sequence clustering tasks

## Why This Works (Mechanism)

### Mechanism 1
Low-rank constraint across sequences enforces sharing of domain geometry while preserving individual dynamics. The matrix B contains all sequence parameters stacked column-wise. Enforcing a nuclear norm constraint on B biases the solution toward low-rank structure. The rank-r SVD B = UΣV* yields columns of ΣV* as low-dimensional embeddings for each sequence and columns of U as shared basis for the domain. Core assumption: Observed sequences arise from a common domain but with individual autoregressive dynamics related through low-rank regularization.

### Mechanism 2
Monotone variational inequalities enable convex formulation of nonlinear autoregressive parameter recovery. By modeling E[xi,t|Ht] = η(Riξi,t) with monotone link η, the problem of finding parameters becomes finding zeros of a monotone vector field Ψ(B) constructed from the model and observations. This allows use of efficient first-order methods (mirror-prox, accelerated mirror descent) with provable convergence. Core assumption: The link function η is monotone, which preserves convexity of the variational inequality formulation.

### Mechanism 3
Stochastic approximation of the monotone VI field enables scalable learning on long sequences. Instead of computing the full VI field over entire sequences, random sub-windows of length G are sampled and averaged to approximate Ψ. This reduces per-iteration cost from O(TNC²d) to O(GNC²d) while maintaining convergence under bounded variance. Core assumption: The VI field is Lipschitz continuous and the stochastic approximation has bounded variance.

## Foundational Learning

- Concept: Singular Value Decomposition (SVD) and its role in low-rank matrix recovery.
  - Why needed here: SVD decomposes B into shared basis (U) and sequence embeddings (ΣV*), enabling the low-rank constraint and interpretation of the learned representation.
  - Quick check question: What do the columns of U, Σ, and V* represent in the SVD of the parameter matrix B?

- Concept: Monotone operator theory and variational inequalities.
  - Why needed here: Monotone VIs generalize convex optimization and allow efficient solution methods for the nonlinear autoregressive parameter recovery problem.
  - Quick check question: What property must the link function η satisfy for the VI formulation to preserve convexity?

- Concept: Mirror descent and proximal algorithms for constrained optimization.
  - Why needed here: These first-order methods solve the nuclear norm constrained VI efficiently, with each iteration requiring SVD and prox-mapping.
  - Quick check question: How does the prox-mapping with nuclear norm constraint relate to singular value thresholding?

## Architecture Onboarding

- Component map: Data preprocessing -> VI field construction -> Optimization core -> Post-processing -> Downstream tasks
- Critical path: 1. Encode raw sequences into multichannel format. 2. Construct measurement operator A and its adjoint A*. 3. Run accelerated mirror-prox with stochastic VI approximation for N iterations. 4. Compute SVD of final B to obtain embeddings. 5. Apply downstream ML task (clustering, classification, etc.).
- Design tradeoffs:
  - Nuclear norm penalty λ: Larger λ enforces stronger low-rank structure but may underfit individual dynamics; smaller λ allows more flexibility but risks overfitting noise.
  - Sub-window length G: Larger G gives better VI field approximation but increases per-iteration cost; smaller G speeds up iterations but may increase variance.
  - Number of iterations N: More iterations improve convergence but increase runtime; too few may underfit.
  - Choice of monotone link η: Must match data type (linear for real-valued, softmax for categorical, etc.) to preserve convexity.
- Failure signatures:
  - High reconstruction error despite low nuclear norm: Low-rank constraint too strong, underfitting.
  - Poor clustering/classification performance: Embeddings not capturing meaningful structure, possibly due to inappropriate λ or η.
  - Slow convergence or divergence: Stochastic approximation variance too high, or step sizes inappropriate.
  - Ill-conditioned SVD: Parameter matrix B poorly conditioned, may indicate numerical issues or inappropriate model assumptions.
- First 3 experiments:
  1. Synthetic autoregressive sequences with known low-rank structure: Recover parameters with varying λ, check reconstruction error and rank of B.
  2. UCR time series classification: Embed sequences, apply k-NN/SVM, compare ARI/NMI/accuracy against baselines.
  3. Symbolic sequence clustering: Embed gene or text sequences, apply UMAP, visually inspect cluster separation.

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of the method vary when applied to time series with different types of nonlinear dynamics beyond autoregressive models? The paper only demonstrates the method on autoregressive models with different link functions (linear, softmax, exponential), but doesn't explore other types of nonlinear dynamics that might be present in real-world time series data.

### Open Question 2
How does the choice of the nuclear norm regularization parameter λ affect the trade-off between learning the common global structure and individual sequence dynamics? While the paper mentions this trade-off, it doesn't provide a systematic study of how different values of λ affect the learned representations and downstream task performance.

### Open Question 3
How does the method scale to very large collections of time series, both in terms of the number of sequences and the length of individual sequences? The paper mentions runtime considerations but doesn't provide experiments or analysis on the method's scalability to very large datasets.

## Limitations
- Assumes all sequences share a common domain through low-rank regularization, but provides limited empirical validation of when this assumption breaks down
- Uses fixed window length G=800 without systematic exploration of how window size affects convergence or embedding quality
- Claims about handling symbolic sequences lack thorough analysis of encoding strategies and their impact

## Confidence

- **High confidence**: The convex formulation via monotone VI and nuclear norm regularization is mathematically sound. The connection between SVD and low-rank embedding extraction is well-established.
- **Medium confidence**: The empirical results on UCR datasets are promising, but comparisons focus on simple baselines rather than state-of-the-art time series methods beyond TS2Vec.
- **Low confidence**: Claims about scalability to long sequences and symbolic data lack comprehensive validation across diverse data types and sequence lengths.

## Next Checks

1. **Domain heterogeneity test**: Generate synthetic datasets where some sequences share a common domain while others do not. Measure how performance degrades as domain heterogeneity increases.

2. **Window size sensitivity**: Systematically vary the sub-window length G from 100 to 2000 observations. Plot convergence speed and final embedding quality (ARI/NMI) against window size.

3. **Symbolic sequence ablation**: Compare performance on gene sequences using one-hot encoding versus the power set approach. Measure impact on VI field computation and downstream clustering accuracy.