---
ver: rpa2
title: Can Large Language Models Automatically Score Proficiency of Written Essays?
arxiv_id: '2403.06149'
source_url: https://arxiv.org/abs/2403.06149
tags:
- prompt
- llama
- task
- chatgpt
- essay
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the ability of large language models (LLMs)
  to automatically score proficiency of written essays. The authors test two popular
  LLMs, ChatGPT and Llama, on the ASAP dataset, which contains 8 tasks and 12978 essays.
---

# Can Large Language Models Automatically Score Proficiency of Written Essays?

## Quick Facts
- **arXiv ID:** 2403.06149
- **Source URL:** https://arxiv.org/abs/2403.06149
- **Reference count:** 0
- **Primary result:** ChatGPT achieves QWK score of 0.606; Llama achieves 0.562 on ASAP dataset

## Executive Summary
This paper investigates whether large language models (LLMs) can automatically score proficiency of written essays. The authors test ChatGPT and Llama on the ASAP dataset containing 8 tasks and 12,978 essays. They design four prompts with incremental task elaboration and evaluate performance using quadratic weighted kappa (QWK) scores. While LLMs lag behind state-of-the-art models in scoring accuracy, they provide useful feedback that could enhance essay quality and assist both teachers and students.

## Method Summary
The study employs prompt engineering with four progressively complex prompts (simple prompt, rubric-included, one-shot example, chat format) to maximize LLM performance on AES tasks. Essays are preprocessed by replacing anonymization tokens. The ASAP dataset with 8 tasks and 12,978 essays serves as the evaluation corpus. Performance is measured using QWK scores comparing LLM predictions against human ratings, with additional analysis of feedback quality provided by the models.

## Key Results
- ChatGPT achieved peak QWK score of 0.606, while Llama achieved 0.562
- Prompt effectiveness varies by LLM and task category (persuasive, narrative, source-dependent)
- Despite lower scoring accuracy than SOTA models, LLMs provide actionable feedback for essay improvement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Incremental prompt engineering improves LLM performance in AES
- Mechanism: Each successive prompt adds more context (rubric, one-shot example, chat format) which helps LLMs better understand scoring criteria
- Core assumption: LLMs benefit from structured, context-rich instructions that clarify the task and evaluation criteria
- Evidence anchors: [abstract] prompt-engineering tactics led to enhancements reaching QWK scores of 0.606 and 0.562

### Mechanism 2
- Claim: Prompt effectiveness varies by LLM and task category
- Mechanism: Different LLMs respond uniquely to prompt structures; task type influences which prompt format yields best results
- Core assumption: The internal architecture and training data of each LLM affect how it processes and utilizes prompt information
- Evidence anchors: [section] ChatGPT's best prompts varied by category (persuasive, narrative, source-dependent), while Llama showed different performance patterns

### Mechanism 3
- Claim: LLMs can provide useful feedback even when scoring accuracy is low
- Mechanism: LLMs leverage their language generation capabilities to offer constructive feedback on essay quality, aiding student improvement
- Core assumption: The ability to generate coherent text translates into the ability to identify and articulate areas for improvement
- Evidence anchors: [abstract] LLMs have an advantage in terms of generated feedback despite lower scoring prediction accuracy

## Foundational Learning

- Concept: Quadratic Weighted Kappa (QWK) as an evaluation metric
  - Why needed here: QWK measures the agreement between predicted and actual scores, accounting for chance agreement, which is crucial for assessing AES performance
  - Quick check question: What does a QWK score of 0.606 indicate about the model's performance compared to human raters?

- Concept: Prompt engineering techniques
  - Why needed here: Effective prompts are essential for guiding LLMs to perform AES tasks accurately by providing clear instructions and context
  - Quick check question: How does adding a one-shot example to a prompt potentially improve the LLM's performance?

- Concept: Differences between holistic and trait-based scoring
  - Why needed here: Understanding these scoring levels is important because LLMs may perform differently when evaluating overall essay quality versus specific writing traits
  - Quick check question: Why might an LLM struggle more with trait-based scoring than holistic scoring?

## Architecture Onboarding

- Component map: Essay text, task prompt, scoring rubric -> LLM inference with engineered prompts -> Predicted score, feedback commentary -> QWK score calculation, feedback analysis

- Critical path: 1. Preprocess essay (anonymization token replacement) 2. Select appropriate prompt based on LLM and task category 3. Generate LLM response (score and feedback) 4. Parse JSON output 5. Calculate QWK score against human ratings

- Design tradeoffs: Prompt complexity vs. token limits and cost, Model accuracy vs. feedback quality, Task specificity vs. generalizability across essay types

- Failure signatures: Low QWK scores indicating poor scoring alignment, Inconsistent scores across different prompts for same essay, Feedback that is vague or not actionable

- First 3 experiments: 1. Test baseline performance with minimal prompt (Prompt A) across all tasks to establish initial QWK scores 2. Implement rubric-included prompt (Prompt B) for source-dependent tasks to assess impact of explicit scoring criteria 3. Add one-shot example to prompt (Prompt C) for persuasive tasks to evaluate learning from examples

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific characteristics of the task category that influence the effectiveness of prompt engineering for AES?
- Basis in paper: The paper states that the best prompt for ChatGPT and Llama varies across different task categories (persuasive, source-dependent, narrative) and provides some examples
- Why unresolved: The paper provides some examples but doesn't offer a comprehensive analysis of the underlying factors that determine prompt effectiveness for each task category
- What evidence would resolve it: A detailed analysis of the task characteristics (e.g., essay length, prompt complexity, source material availability) and their correlation with prompt effectiveness for each task category

### Open Question 2
- Question: How does the performance of LLMs for AES compare to human raters in terms of consistency and reliability?
- Basis in paper: The paper mentions that the average agreement score between human raters in the ASAP dataset is 0.75, and it also analyzes the consistency of LLM performance across different prompts
- Why unresolved: While the paper provides some insights into LLM consistency, it doesn't directly compare LLM performance to human raters in terms of overall reliability and consistency
- What evidence would resolve it: A direct comparison of LLM performance (e.g., QWK scores) with human rater performance on the same set of essays, along with an analysis of inter-rater reliability

### Open Question 3
- Question: What are the key factors that contribute to the underperformance of LLMs in AES compared to task-specific SOTA models?
- Basis in paper: The paper discusses the significant performance gap between LLMs and SOTA models for AES, both holistically and per trait
- Why unresolved: The paper doesn't provide a detailed analysis of the specific factors that lead to this performance gap, such as the limitations of LLM training data, the complexity of essay scoring, or the lack of task-specific fine-tuning
- What evidence would resolve it: A comprehensive analysis of the LLM training data, the AES task requirements, and the performance of task-specific fine-tuned LLMs on the same dataset

## Limitations

- QWK scores of 0.606 (ChatGPT) and 0.562 (Llama) are notably lower than state-of-the-art AES models
- The paper lacks detailed information about the exact prompt formulations, making precise replication difficult
- Feedback quality analysis is somewhat anecdotal, relying on a few example essays rather than systematic evaluation

## Confidence

- **High Confidence:** The observation that prompt engineering improves LLM performance in AES tasks is well-supported by the structured experimentation across four prompt variants
- **Medium Confidence:** The claim that different LLMs respond differently to prompt structures and task categories is supported by comparative results but would benefit from more extensive testing across additional models and tasks
- **Low Confidence:** The assertion that LLMs provide useful feedback despite scoring limitations is based on limited qualitative examples rather than systematic feedback quality assessment

## Next Checks

1. **Prompt Reconstruction Validation:** Reconstruct the exact prompt formulations using the incremental description provided (simple prompt → rubric-included → one-shot example → chat format) and verify whether the reported QWK scores of 0.606 and 0.562 can be reproduced across all 8 tasks in the ASAP dataset

2. **Feedback Quality Assessment:** Implement a systematic evaluation of the feedback generated by LLMs, using human raters to assess whether the feedback is specific, actionable, and correlates with actual essay improvements, rather than relying on anecdotal examples

3. **Cross-Model Generalization Test:** Test additional LLMs beyond ChatGPT and Llama (such as Claude or specialized smaller models) using the same prompt engineering approach to determine whether the observed pattern of prompt effectiveness varies by model architecture or is consistent across different LLMs