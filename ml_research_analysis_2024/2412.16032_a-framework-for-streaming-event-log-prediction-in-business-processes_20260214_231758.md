---
ver: rpa2
title: A Framework for Streaming Event-Log Prediction in Business Processes
arxiv_id: '2412.16032'
source_url: https://arxiv.org/abs/2412.16032
tags:
- data
- learning
- streaming
- prediction
- event
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a Python-based framework for event-log prediction
  in streaming mode, enabling predictions while data is being generated by a business
  process. The framework allows for easy integration of streaming algorithms, including
  language models like n-grams and LSTMs, and for combining these predictors using
  ensemble methods.
---

# A Framework for Streaming Event-Log Prediction in Business Processes

## Quick Facts
- arXiv ID: 2412.16032
- Source URL: https://arxiv.org/abs/2412.16032
- Authors: Benedikt Bollig; Matthias Függer; Thomas Nowak
- Reference count: 40
- Primary result: Framework enables streaming event-log predictions with ensemble methods outperforming LSTMs

## Executive Summary
This paper presents a Python-based framework for event-log prediction in streaming mode, enabling real-time predictions while data is being generated by business processes. The framework supports various language models including n-grams, LSTMs, and ensemble methods, and allows for easy integration of new streaming algorithms. Experiments on seven real-world process-mining datasets demonstrate that while LSTMs achieve the best performance in batch mode, n-gram models often provide comparable accuracy with the advantage of immediate prediction capability in streaming scenarios.

## Method Summary
The framework implements a streaming architecture where events are processed incrementally, updating model states in real-time rather than reprocessing entire datasets. Base models include frequency prefix trees (FPT), n-gram models (1-8), and bags, combined through ensemble methods using soft/hard voting and adaptive voting. LSTMs are implemented with 2 hidden layers of 128 units, embedding dimension 50, and batch size 8. The framework uses LogicSponge for parallel execution and supports both batch (70% train, 15% validation, 15% test) and streaming modes. Performance is evaluated using prediction accuracy and latency metrics across seven process-mining datasets.

## Key Results
- N-gram models achieve accuracy close to LSTMs in batch mode and often outperform them in streaming mode due to immediate prediction capability
- Ensemble methods combining basic models can outperform individual LSTMs by leveraging complementary strengths
- The framework's streaming architecture enables efficient real-time predictions without the overhead of batch processing or full retraining

## Why This Works (Mechanism)

### Mechanism 1
- Claim: N-gram models perform close to LSTMs in batch mode and often outperform them in streaming mode due to immediate prediction capability without waiting for sufficient training data.
- Mechanism: N-grams make predictions based on immediate historical frequency counts without requiring iterative training. This allows them to start producing sensible predictions as soon as events arrive, while LSTMs need sufficient data to train and optimize their internal state representations.
- Core assumption: Streaming predictions need to be made early and often, before sufficient data accumulates for complex models to learn effectively.
- Evidence anchors:
  - [abstract] "there is often an n-gram whose accuracy comes very close" and "LSTMs generally lack accuracy in the early stages of a streaming prediction run, while basic methods make sensible predictions immediately"
  - [section] "In streaming mode, LSTMs appear to struggle during the initial stage of a stream. As illustrated in Figure 2a for the sepsis dataset, which depicts the accuracy evolution for various models, LSTM performance is initially well below that of other base models, catching up only later"
  - [corpus] Weak evidence - corpus neighbors focus on related process mining topics but don't directly address the streaming vs batch performance comparison
- Break condition: If the data stream has very long historical dependencies that require deep learning to capture, or if the early prediction accuracy is not critical for the business use case.

### Mechanism 2
- Claim: Ensemble methods combining multiple base models (FPT, bag, n-grams) can outperform individual LSTMs by leveraging complementary strengths of different approaches.
- Mechanism: Soft voting aggregates probability distributions from multiple models, allowing each model to contribute its specialized knowledge. This combines the immediate responsiveness of n-grams with the global pattern recognition of FPTs and bags, creating a more robust predictor than any single approach.
- Core assumption: Different base models capture different aspects of the process behavior, and combining them provides better coverage than any individual model.
- Evidence anchors:
  - [abstract] "Combining basic models in ensemble methods can even outperform LSTMs"
  - [section] "Ensemble methods can outperform LSTMs on several datasets. Notably, the average latency (the total time required for making one prediction and one update) of base models is significantly lower that of LSTMs"
  - [corpus] Weak evidence - corpus doesn't contain specific information about ensemble methods outperforming LSTMs
- Break condition: If computational resources are severely constrained, as ensemble methods require maintaining multiple models simultaneously, or if the individual models have highly correlated errors that ensemble methods cannot mitigate.

### Mechanism 3
- Claim: The framework's streaming architecture with incremental updates enables efficient real-time predictions without the overhead of batch processing or full retraining.
- Mechanism: The framework processes events one by one, updating model states incrementally rather than reprocessing entire datasets. This allows predictions to be made immediately upon receiving new events while maintaining model accuracy through continuous learning.
- Core assumption: Business processes generate data continuously and predictions need to be made in real-time without waiting for batch windows or complete retraining cycles.
- Evidence anchors:
  - [abstract] "enabling predictions while data is being generated by a business process" and "Training is conducted online, i.e., in production"
  - [section] "In streaming mode, we did not add stop-symbols to mark the end of a sequence, as adding them to pre-mortem data is unsuitable unless explicitly present in the dataset" and "For querying an LSTM, we parse the current sequence through the model and retrieve the corresponding outcome"
  - [corpus] Weak evidence - corpus neighbors discuss related process mining topics but don't specifically address streaming architecture or incremental updates
- Break condition: If the event stream has very high volume or velocity that exceeds the framework's processing capacity, or if the business process requires batch-level accuracy that streaming predictions cannot achieve.

## Foundational Learning

- Concept: Event log structure and case-based processing
  - Why needed here: Understanding how event logs are structured as sequences of activities per case is fundamental to implementing the prediction framework correctly
  - Quick check question: What is the difference between an event log and an event stream in the context of process mining?

- Concept: Language models and probabilistic prediction functions
  - Why needed here: The framework uses various language models (n-grams, LSTMs, FPTs) that all implement probabilistic prediction functions mapping historical sequences to probability distributions over next activities
  - Quick check question: How does a prediction function p: Σ* → ∆(Σstop) differ from a simple lookup table?

- Concept: Streaming vs batch learning paradigms
  - Why needed here: The framework supports both modes, and understanding their tradeoffs is crucial for choosing the right approach for different business scenarios
  - Quick check question: What is the key difference between how batch and streaming models handle the training/test data split?

## Architecture Onboarding

- Component map: Data Source -> Model Manager -> Predictor -> Evaluator -> Results Sink
- Critical path: 1) Data stream creation from event log 2) Model initialization and state tracking 3) Incremental event processing and model updates 4) Prediction generation for each active case 5) Performance evaluation and logging
- Design tradeoffs: Memory vs accuracy (storing full histories vs fixed-size n-grams), Latency vs complexity (simple models with low latency vs complex models with higher accuracy), Online vs offline processing (real-time predictions vs batch analysis)
- Failure signatures: Memory leaks (increasing memory usage over time during streaming), Prediction drift (accuracy degradation as process behavior changes), Latency spikes (sudden increases in prediction/update time)
- First 3 experiments: 1) Run baseline FPT model on a small dataset to verify framework setup 2) Compare 3-gram vs 5-gram performance on the same dataset 3) Implement and test soft voting ensemble with FPT + bag + 3-gram models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal combination of n-gram models (i.e., specific values of n) for ensemble methods in streaming event-log prediction?
- Basis in paper: [explicit] The paper notes that the best combinations of n-grams in ensemble methods are not determined and suggests investigating whether distributing them sparsely or clustering them is preferable.
- Why unresolved: While the paper demonstrates that ensemble methods combining different n-grams outperform individual models, it does not identify which specific combinations of n values yield optimal performance across different datasets.
- What evidence would resolve it: Systematic experiments testing all combinations of n-gram models (e.g., n=1 through n=8) across multiple datasets to identify which specific combinations consistently produce the best accuracy in both batch and streaming modes.

### Open Question 2
- Question: How can LSTM models be adapted to perform better in the early stages of streaming prediction when data is sparse?
- Basis in paper: [explicit] The paper explicitly states that LSTMs "generally lack accuracy in the early stages of a streaming prediction run" while basic methods make sensible predictions immediately.
- Why unresolved: The paper identifies this as a limitation but does not propose solutions for improving early-stage LSTM performance in streaming contexts.
- What evidence would resolve it: Development and testing of LSTM architectures or training strategies specifically designed for early-stage prediction accuracy, potentially including techniques like transfer learning, curriculum learning, or adaptive learning rates that adjust based on available data volume.

### Open Question 3
- Question: Can fallback strategies be optimized beyond the simple threshold-based approach described in the paper?
- Basis in paper: [explicit] The paper mentions that fallback algorithms show "promising potential" but require more detailed experiments with "more sophisticated fallback strategies" that might use statistical tests.
- Why unresolved: The current fallback method uses a fixed threshold (minimum number of visits) but doesn't explore more nuanced decision criteria for when to switch between models.
- What evidence would resolve it: Comparative experiments testing various fallback strategies including confidence-based switching, dynamic threshold adjustment, or model confidence metrics that determine when to fall back to simpler models.

## Limitations
- LSTM implementation details for streaming mode are not fully specified, particularly regarding sequence batching and update frequency
- Specific configurations and weight assignments for adaptive voting ensemble methods are insufficiently detailed
- The framework's performance under high-volume or high-velocity event streams is not thoroughly evaluated

## Confidence
- **High Confidence**: The general framework architecture and base model implementations (n-grams, FPT, bag) are well-specified and should be reproducible. The core claim that n-grams provide immediate predictions while LSTMs require training time is well-supported by the evidence provided.
- **Medium Confidence**: The ensemble methods' superiority over individual LSTMs is supported but could benefit from more detailed implementation specifications. The latency measurements appear reasonable but may vary based on hardware and specific implementation choices.
- **Low Confidence**: The exact LSTM streaming implementation details and the adaptive voting mechanism specifications are insufficient for complete reproducibility without additional experimentation.

## Next Checks
1. **Implement streaming LSTM with variable update frequencies** to test whether the early-stage accuracy issues persist across different batching strategies and update intervals.
2. **Test ensemble robustness with different base model combinations** beyond the ones reported, particularly focusing on understanding which model combinations provide the most complementary predictions.
3. **Measure memory usage and computational overhead** of the framework in production scenarios to validate the claimed efficiency advantages over batch processing approaches.