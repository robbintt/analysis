---
ver: rpa2
title: 'Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning'
arxiv_id: '2402.14963'
source_url: https://arxiv.org/abs/2402.14963
tags:
- answer
- llms
- reasoning
- language
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Mirror, a multiple-perspective self-reflection
  method for knowledge-rich reasoning. The key idea is to guide large language models
  (LLMs) through a reasoning loop using a Navigator to generate diverse question-oriented
  directions and a Reasoner to produce responses, with rewards based on diversity
  of generated directions and agreement among strategically induced perturbations
  in responses.
---

# Mirror: A Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning

## Quick Facts
- arXiv ID: 2402.14963
- Source URL: https://arxiv.org/abs/2402.14963
- Reference count: 16
- One-line primary result: Mirror achieves over 15% average improvement over baselines on knowledge-rich reasoning tasks

## Executive Summary
This paper introduces Mirror, a self-reflection method for knowledge-rich reasoning that guides large language models (LLMs) through a reasoning loop using a Navigator to generate diverse question-oriented directions and a Reasoner to produce responses. The method employs rewards based on diversity of generated directions and agreement among strategically induced perturbations in responses, addressing the limitations of LLMs in self-assessment and feedback generation. Experiments on five reasoning datasets demonstrate that Mirror significantly outperforms recent unsupervised self-refinement methods.

## Method Summary
Mirror implements a multiple-perspective self-reflection approach where a Navigator generates K diverse directions per iteration, and a Reasoner produces responses based on each direction. The system uses Monte Carlo Tree Search (MCTS) with Upper Confidence bounds applied to Trees (UCT) to explore diverse reasoning trajectories while balancing exploitation and exploration. Responses are evaluated using inter-consistency (agreement across different directions) and intra-consistency (self-consistency of individual responses) as unsupervised proxies for correctness, with diversity rewards incorporated to prevent reasoning loops.

## Key Results
- Mirror achieves an average improvement of over 15% compared to recent unsupervised self-refinement methods
- The diversity-based reward and answer assessment strategy effectively address limitations of LLMs in self-assessment
- Question-oriented directions from the Navigator outperform generic reflection instructions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multiple-perspective self-assessment improves knowledge-rich reasoning by leveraging consistency among diverse directions
- Mechanism: The Navigator generates K diverse directions per iteration, and the Reasoner produces responses based on each direction. Inter-consistency and intra-consistency are used to assess correctness without ground truth
- Core assumption: Diverse directions will lead to varied but informative reasoning paths, and consistency among these paths indicates correctness
- Evidence anchors: Abstract mentions "agreement among strategically induced perturbations in responses"; Section 4.2 describes the Navigator generating K directions for diverse perspectives

### Mechanism 2
- Claim: Diversity-based reward prevents LLMs from getting stuck in reasoning loops by encouraging exploration of new answer spaces
- Mechanism: A diversity reward is incorporated into the MCTS framework, rewarding nodes where the predicted answer differs from the parent node's answer
- Core assumption: LLM responses to diverse directions will produce different answers, and this diversity indicates productive exploration
- Evidence anchors: Abstract mentions "diverse yet plausibly reliable reasoning trajectory"; Section 4.3 explains promoting diversity between parent and child nodes

### Mechanism 3
- Claim: Question-oriented directions provide more effective feedback than generic instructions for self-correction
- Mechanism: The Navigator generates specific directions tailored to each question, providing explicit clues about key elements and reasoning paths
- Core assumption: LLMs can better identify and correct errors when given specific guidance about what to reconsider
- Evidence anchors: Abstract mentions "multiple-perspective clues"; Section 5.2 shows GenerativeDirect boosts performance compared to CoT

## Foundational Learning

- Concept: Monte Carlo Tree Search (MCTS) with Upper Confidence bounds applied to Trees (UCT)
  - Why needed here: Provides a framework for exploring diverse reasoning trajectories while balancing exploitation of promising paths with exploration of new directions
  - Quick check question: How does UCT balance exploration vs exploitation in tree search, and why is this particularly important for self-reflection where ground truth isn't available?

- Concept: Self-consistency as a proxy for correctness assessment
  - Why needed here: Since LLMs cannot reliably self-assess correctness without ground truth, self-consistency provides an unsupervised signal for when a response is likely correct
  - Quick check question: What correlation was observed between self-consistency confidence and actual accuracy in the experiments, and how does this justify using consistency as a stopping criterion?

- Concept: In-Context Learning (ICL) sensitivity to prompt positioning and content
  - Why needed here: The effectiveness of generated directions depends on how well LLMs follow instructions, which varies based on prompt structure
  - Quick check question: According to the findings, how did repositioning the reflection instruction affect the percentage of changed samples, and what does this imply about ICL?

## Architecture Onboarding

- Component map: Question -> Navigator (generates K directions) -> Reasoner (produces responses) -> MCTS Framework (manages tree search) -> Consistency Assessment (inter/intra-consistency) -> Reward Function (diversity + consistency rewards)

- Critical path: 1) Receive question and initial attempt, 2) Navigator generates K diverse directions, 3) Reasoner generates responses for each direction, 4) Assess intra-consistency; if above threshold, accept answer, 5) If not, assess inter-consistency; if above threshold, accept answer, 6) If neither threshold met, continue with MCTS search using diversity and consistency rewards, 7) Return answer with highest consistency score

- Design tradeoffs: K directions vs computational cost (more directions provide better coverage but increase inference time), Diversity threshold vs exploration (higher thresholds ensure genuine exploration but may miss valid paths), Consistency threshold vs accuracy (lower thresholds accept more answers but risk including incorrect ones)

- Failure signatures: Consistently low diversity across iterations, High inter-consistency but low accuracy, Reasoner ignoring directions and generating identical responses, Thresholds too high/low causing premature stopping or infinite loops

- First 3 experiments: 1) Implement basic MCTS with diversity reward only - measure if diversity alone improves over CoT baseline, 2) Add consistency assessment with fixed thresholds - test if stopping criteria improve efficiency, 3) Compare question-specific vs generic directions - validate that Navigator provides meaningful guidance beyond simple reflection prompts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal size of the search space (number of directions generated) for balancing computational efficiency and performance improvement in Mirror?
- Basis in paper: [inferred] The paper discusses the impact of varying the number of generated directions (Num) on task performance and the presence of correct answers in the tree
- Why unresolved: The paper does not explicitly state an optimal number of directions, only showing the relationship between Num and performance across different models and datasets
- What evidence would resolve it: Conducting additional experiments to systematically test a wider range of Num values and identifying the point of diminishing returns in performance improvement relative to computational cost

### Open Question 2
- Question: How does the performance of Mirror compare to state-of-the-art methods that utilize external resources or labeled datasets for training critic modules?
- Basis in paper: [explicit] The paper mentions that Mirror achieves superior performance compared to recent unsupervised self-refinement methods, but does not directly compare to methods using external resources or labeled data
- Why unresolved: The paper focuses on unsupervised methods and does not include a comprehensive comparison with supervised or semi-supervised approaches
- What evidence would resolve it: Conducting experiments to directly compare Mirror's performance against methods like Refiner (Peng et al., 2023) or CRITIC (Gou et al., 2023a) on the same datasets

### Open Question 3
- Question: How does the choice of the threshold T0 for self-consistency impact Mirror's performance, and is there an optimal value that generalizes across different models and datasets?
- Basis in paper: [explicit] The paper mentions that different values of T0 (0.8 for GPT-3.5, 0.5 for Llama and Vicuna) were used based on validation results, but does not explore the impact of varying this threshold
- Why unresolved: The paper does not provide a systematic analysis of how different T0 values affect performance or whether a single optimal value exists
- What evidence would resolve it: Conducting experiments with a range of T0 values for each model and dataset to determine the sensitivity of performance to this hyperparameter and identify any generalizable optimal settings

### Open Question 4
- Question: How does Mirror's performance scale with the size of the underlying language model, and are there diminishing returns for larger models?
- Basis in paper: [inferred] The paper evaluates Mirror on GPT-3.5 (175B parameters), Llama2-13B, and Vicuna-13B, showing consistent improvements over baselines, but does not explicitly analyze the relationship between model size and performance gains
- Why unresolved: The paper does not provide a detailed analysis of how the improvements from Mirror scale with model size or whether larger models benefit proportionally more or less from the approach
- What evidence would resolve it: Conducting experiments with a wider range of model sizes, including both smaller and larger models than those tested, to quantify the relationship between model size and performance improvement from Mirror

## Limitations

- The method's performance may not generalize to domains where consistency does not correlate with correctness
- Computational cost increases with the number of directions (K) generated per iteration
- The quality and diversity of generated directions are not rigorously evaluated

## Confidence

- Mechanism 1 (Multiple-perspective consistency): Medium confidence - empirical results support the approach, but the underlying assumption that consistency equals correctness is not universally validated
- Mechanism 2 (Diversity-based rewards): Medium confidence - the diversity metric shows improvement, but the paper doesn't sufficiently address whether increased diversity correlates with better reasoning quality
- Mechanism 3 (Question-oriented directions): Medium confidence - specific directions show improvement over generic reflection, but quality evaluation is limited

## Next Checks

1. Cross-domain validation: Test Mirror on reasoning tasks outside MMLU and FEVER to verify if the self-consistency proxy remains reliable when moving beyond general knowledge and fact verification tasks

2. Consistency vs correctness correlation analysis: Conduct a detailed study examining how often self-consistency accurately predicts correctness across different question types and difficulty levels, identifying where the proxy breaks down

3. Diversity quality assessment: Implement a human evaluation to determine whether the diversity in generated directions represents genuinely different reasoning approaches or superficial variations that don't improve reasoning quality