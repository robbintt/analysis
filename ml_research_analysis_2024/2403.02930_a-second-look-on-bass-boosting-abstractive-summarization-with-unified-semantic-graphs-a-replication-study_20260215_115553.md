---
ver: rpa2
title: A Second Look on BASS -- Boosting Abstractive Summarization with Unified Semantic
  Graphs -- A Replication Study
arxiv_id: '2403.02930'
source_url: https://arxiv.org/abs/2403.02930
tags:
- graph
- bass
- replication
- information
- original
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a detailed replication study of the BASS framework,
  an abstractive summarization system that uses Unified Semantic Graphs (USGs) to
  guide the summarization process. The authors implement the BASS framework from scratch,
  encountering several challenges and discrepancies between their implementation and
  the original paper.
---

# A Second Look on BASS -- Boosting Abstractive Summarization with Unified Semantic Graphs -- A Replication Study

## Quick Facts
- arXiv ID: 2403.02930
- Source URL: https://arxiv.org/abs/2403.02930
- Authors: Osman Alperen Koraş; Jörg Schlötterer; Christin Seifert
- Reference count: 40
- Replicated model falls short of original results, even below reported baselines

## Executive Summary
This paper presents a detailed replication study of the BASS framework, which uses Unified Semantic Graphs (USGs) to guide abstractive summarization. The authors implement BASS from scratch and conduct an ablation study to isolate the impact of architectural adaptations and USGs on performance. They find that their replicated models perform worse than the original results, even falling below the baselines reported in the original paper. The authors attribute this discrepancy primarily to undertraining due to smaller batch sizes and question the effectiveness of the proposed model adaptations and USGs.

## Method Summary
The authors implemented the BASS framework from scratch, creating Unified Semantic Graphs from dependency parse trees and co-reference chains, then integrating these into a transformer architecture with a graph encoder and PageRank-based attention propagation. They conducted an ablation study comparing: (1) basic transformer encoder-decoder (RTS2S), (2) transformer with graph encoder and cross-attention without USG structure (exRTS2S), and (3) full BASS with USG src graphs. The study used the BigPatent dataset with CoreNLP for preprocessing, and evaluated performance using ROUGE scores and BERTScore F1.

## Key Results
- Replicated models achieved significantly lower ROUGE and BERTScore scores than the original BASS paper, even falling below the reported baseline performance
- Ablation study showed slight improvements in BERTScore but mixed ROUGE results when comparing exRTS2S to RTS2S
- BASS model was undertrained due to batch size constraints (8 vs. estimated 32-64), completing only 0.5 epochs versus the expected 5.8 epochs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph structure injection improves transformer summarization by providing explicit semantic relationships
- Mechanism: BASS constructs Unified Semantic Graphs from dependency parse trees and co-reference chains, then uses a graph encoder with attention propagation to inject this structured information into the transformer decoder
- Core assumption: The semantic relationships captured in USGs are complementary to the learned representations in transformers
- Evidence anchors: [abstract]: "uses Unified Semantic Graphs (USGs) to guide the summarization process"; [section]: "the graph encoder 6○, which is a standard two-layered transformer encoder using the adjacency matrix of the graph as attention mask"; [corpus]: Weak evidence - replication study found no clear performance improvement, suggesting this mechanism may not work as expected
- Break condition: If USG construction fails to capture meaningful semantic relationships, or if graph propagation introduces noise instead of useful signal

### Mechanism 2
- Claim: Architectural adaptations like graph-encoder, token-to-node alignment, and graph-propagated attention improve summarization
- Mechanism: BASS adds specialized components (graph encoder, cross-attention with PageRank propagation, residual dropout) to standard transformer architecture to better incorporate graph information
- Core assumption: These architectural modifications effectively leverage graph structure information
- Evidence anchors: [abstract]: "conduct an ablation study to examine BASS' architectural adaptations"; [section]: "a six-layered decoder without any graph components (i.e., omitting 3○, 5○ – 8○)"; [corpus]: Mixed evidence - ablation study showed slight improvements in BERTScore but mixed ROUGE results, suggesting partial effectiveness
- Break condition: If architectural overhead outweighs any benefits from graph information, or if components introduce architectural conflicts

### Mechanism 3
- Claim: Proper pre-processing and graph construction are critical for downstream performance
- Mechanism: The paper uses CoreNLP for linguistic analysis (POS tagging, co-reference resolution, dependency parsing) to create USGs, with careful chunking to handle memory constraints
- Core assumption: Quality of linguistic analysis directly impacts quality of USGs and downstream performance
- Evidence anchors: [section]: "We used the latest CoreNLP library (v4.5.2) and had the authors confirm a configuration"; [section]: "We chunk source documents into blocks of sentences with approx. 500 words"; [corpus]: Evidence shows pre-processing challenges (20% documents had runtime/memory issues), suggesting this is a critical but fragile step
- Break condition: If pre-processing fails to handle edge cases or if graph construction introduces errors through merging rules

## Foundational Learning

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: Understanding the base model to appreciate what BASS adds
  - Quick check question: What is the difference between self-attention and cross-attention in transformers?

- Concept: Graph neural networks and graph attention
  - Why needed here: The graph encoder and propagation mechanisms are based on GNN concepts
  - Quick check question: How does PageRank-based attention propagation differ from standard attention?

- Concept: Dependency parsing and semantic role labeling
  - Why needed here: USG construction relies on accurate linguistic analysis
  - Quick check question: What information is captured in dependency parse trees that might be useful for summarization?

## Architecture Onboarding

- Component map: Pre-processing (CoreNLP pipeline) → Graph Construction (USG creation) → Token-to-Node Alignment → Graph Encoder → Cross-attention with PageRank propagation → Residual Dropout → Text Encoder → Transformer Decoder

- Critical path: Pre-processing → Graph Construction → Graph Encoder → Cross-attention → Decoder output
- Design tradeoffs: Increased model complexity and parameter count vs. potential performance gains from structured information
- Failure signatures: Undertraining (batch size issues), graph construction errors, architectural conflicts, tokenization mismatches
- First 3 experiments:
  1. Replicate the basic transformer encoder-decoder (RTS2S) to establish baseline performance
  2. Add graph encoder and cross-attention without USG structure (exRTS2S) to isolate architectural impact
  3. Use USG src graphs with full BASS architecture to test complete system

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural differences between the replication and the original BASS model account for the performance gap?
- Basis in paper: [explicit] The authors note that their replicated model ended up having approximately 205M trainable parameters, which is around 2% larger than the reported 201M parameters in the original BASS paper.
- Why unresolved: The paper does not provide detailed architectural specifications of the original BASS model, making it difficult to pinpoint the exact differences.
- What evidence would resolve it: Access to the original BASS model's architecture details or a side-by-side comparison of the replication and original model components.

### Open Question 2
- Question: How does the effectiveness of the Unified Semantic Graphs (USGs) compare to other graph-based approaches in abstractive summarization?
- Basis in paper: [inferred] The authors' ablation study shows that using USGs does not significantly improve performance compared to models without graph information.
- Why unresolved: The paper does not compare the performance of USGs to other graph-based methods or provide insights into why USGs may be less effective.
- What evidence would resolve it: Comparative studies evaluating USGs against alternative graph-based approaches in terms of summarization quality and computational efficiency.

### Open Question 3
- Question: What are the implications of the discrepancies between the paper's description and the provided Java source code for graph construction?
- Basis in paper: [explicit] The authors note that the Java source code provided by the original authors for graph construction differs algorithmically from the paper's description.
- Why unresolved: The paper does not explore the potential impact of these discrepancies on the overall performance of the BASS model.
- What evidence would resolve it: Analysis of the impact of the discrepancies on the generated USGs and their downstream performance in the summarization task.

### Open Question 4
- Question: How does the choice of batch size affect the training and performance of the BASS model?
- Basis in paper: [explicit] The authors mention that they were unable to determine the batch size used in the original work and had to use a batch size of 48 per step due to hardware constraints.
- Why unresolved: The paper does not investigate the impact of different batch sizes on the model's performance or training dynamics.
- What evidence would resolve it: Systematic experiments varying the batch size and analyzing its effects on training stability, convergence, and final summarization quality.

## Limitations
- Undertraining due to batch size constraints (8 vs. estimated 32-64) resulted in only 0.5 epochs completed versus expected 5.8 epochs
- Implementation discrepancies in USG construction algorithm between paper description and provided Java source code
- Potential dataset variations using non-preprocessed BigPatent with CoreNLP v4.5.2 versus original preprocessing

## Confidence

**Low Confidence**: Claims about BASS's effectiveness relative to the original paper are not supported due to the undertraining issue. The performance gap between replication and original results cannot be attributed solely to the method's ineffectiveness.

**Medium Confidence**: The replication successfully implements the BASS framework architecture and demonstrates that the proposed components (graph encoder, graph-propagated attention, residual dropout) can be integrated into a transformer model. The ablation study methodology is sound and provides insights into individual component contributions.

**High Confidence**: The critical importance of providing complete technical details in research papers is well-established by this replication effort. The challenges encountered (batch size configuration, USG construction rules, tokenization handling) demonstrate systemic issues in reproducible research.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically test different batch sizes (8, 16, 32, 64) while keeping other hyperparameters constant to isolate the impact of batch size on convergence and final performance.

2. **Complete USG Construction Verification**: Implement and validate the full USG construction algorithm with all linguistic rules for node merging, then compare the resulting graphs against the original paper's descriptions or examples.

3. **Tokenization Consistency Check**: Verify that the tokenization pipeline produces identical token-graph node alignments as the original implementation, particularly for edge cases involving punctuation and decimals.