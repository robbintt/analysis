---
ver: rpa2
title: Generalizing Visual Question Answering from Synthetic to Human-Written Questions
  via a Chain of QA with a Large Language Model
arxiv_id: '2401.06400'
source_url: https://arxiv.org/abs/2401.06400
tags:
- questions
- coqah
- question
- template-based
- answer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoQAH, a method that enables visual question
  answering (VQA) models trained on synthetic template-based data to generalize to
  complex, human-written questions. CoQAH uses a large language model (LLM) to interact
  with a template-based VQA model in a chain-of-qa fashion, asking a series of template-based
  questions to gather relevant visual information and derive logical answers for human-written
  questions.
---

# Generalizing Visual Question Answering from Synthetic to Human-Written Questions via a Chain of QA with a Large Language Model

## Quick Facts
- arXiv ID: 2401.06400
- Source URL: https://arxiv.org/abs/2401.06400
- Reference count: 27
- Primary result: Introduces CoQAH method that achieves state-of-the-art performance on generalizing VQA from synthetic to human-written questions

## Executive Summary
This paper introduces CoQAH, a novel approach for generalizing visual question answering (VQA) models trained on synthetic template-based data to answer complex human-written questions. The method employs a large language model (LLM) to interact with a template-based VQA model in a chain-of-qa fashion, asking a series of template-based questions to gather relevant visual information and derive logical answers for human-written questions. CoQAH demonstrates superior performance compared to general vision-language models, VQA models, and medical foundation models without requiring fine-tuning, achieving state-of-the-art results on both 3D-rendered images (CLEVR/CLEVR-Human) and medical imaging datasets (MIMIC-Diff-VQA/VQA-RAD/SLAKE).

## Method Summary
CoQAH addresses the challenge of generalizing VQA models from synthetic template-based questions to complex human-written questions by leveraging a large language model (LLM) to interact with a template-based VQA model in a chain-of-qa fashion. The method works by decomposing a complex human-written question into a series of simpler, template-based questions that can be answered by the existing VQA model. The LLM acts as an intermediary, asking these template-based questions to gather relevant visual information from the VQA model, then synthesizing the collected information to provide a final answer to the original complex question. This approach enables the VQA model to handle questions that go beyond its original training distribution without requiring additional fine-tuning.

## Key Results
- Achieves 74.3% accuracy on CLEVR-Human, surpassing MDETR (59.9%) and GPT-4-Vision (60.1%)
- Reports highest accuracy in closed-form questions on VQA-RAD (67.5%) and SLAKE (73.9%)
- Achieves highest LA VE GP T âˆ’4 scores in open-form questions on VQA-RAD (0.302) and SLAKE (0.425)

## Why This Works (Mechanism)
The paper demonstrates that CoQAH effectively bridges the gap between synthetic and human-written questions by decomposing complex queries into simpler, template-based components that can be answered by existing VQA models. The LLM's ability to reason about visual information and generate appropriate follow-up questions enables the system to gather relevant information incrementally. This chain-of-qa approach allows the VQA model to access information beyond its original training distribution while maintaining its strengths in template-based reasoning. The method's success suggests that combining LLMs' reasoning capabilities with specialized VQA models can create more robust and generalizable visual question-answering systems.

## Foundational Learning
1. Chain-of-Thought Reasoning: Breaking down complex problems into sequential steps
   - Why needed: Enables systematic approach to complex visual questions
   - Quick check: Can follow logical reasoning chain from start to finish

2. Template-based VQA: Using structured question formats for visual reasoning
   - Why needed: Provides consistent framework for visual understanding
   - Quick check: Questions follow predictable syntactic and semantic patterns

3. Large Language Model Integration: Using LLMs for reasoning and question generation
   - Why needed: Leverages LLMs' natural language understanding and reasoning capabilities
   - Quick check: LLM can generate appropriate follow-up questions based on context

4. Visual Question Decomposition: Breaking complex questions into simpler components
   - Why needed: Makes complex questions tractable for template-based models
   - Quick check: Each component question can be answered independently

5. Cross-domain Generalization: Applying models across different visual domains
   - Why needed: Tests robustness of approach beyond training data
   - Quick check: Method works consistently across synthetic and medical images

6. Closed-form vs Open-form Questions: Different types of answer formats
   - Why needed: Evaluates system's ability to handle various question types
   - Quick check: Can generate both structured and free-form responses appropriately

## Architecture Onboarding

**Component Map:**
Human Question -> LLM (Chain-of-QA) -> Template-based VQA Model -> Visual Information -> LLM Synthesis -> Final Answer

**Critical Path:**
The critical path flows from the human-written question through the LLM's chain-of-qa process to the template-based VQA model, then back through the LLM for synthesis and final answer generation. The LLM acts as the central coordinator, determining which template-based questions to ask and how to combine the resulting information.

**Design Tradeoffs:**
The approach trades off the need for fine-tuning the VQA model against the dependency on LLM quality and reasoning capabilities. While avoiding fine-tuning saves resources and maintains the VQA model's specialized capabilities, it introduces potential bottlenecks in the LLM's ability to generate appropriate follow-up questions and synthesize answers. The method also requires careful prompt engineering to ensure the LLM asks relevant template-based questions.

**Failure Signatures:**
- LLM fails to decompose complex questions appropriately
- Template-based VQA model cannot answer certain question types
- LLM synthesizes incorrect conclusions from collected information
- Chain-of-qa becomes too long or circular
- Domain mismatch between training data and test scenarios

**First 3 Experiments:**
1. Test CoQAH on a held-out set of human-written questions from CLEVR-Human to validate generalization capability
2. Compare performance with and without the LLM component to isolate its contribution
3. Evaluate CoQAH on additional visual domains (e.g., natural images) to assess generalizability

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on availability of template-based synthetic data for training VQA models
- Performance primarily demonstrated on specific datasets (CLEVR and medical imaging), raising questions about generalizability to other visual domains
- Method depends on quality and reasoning capabilities of underlying LLM, which may vary across different models
- Absolute performance on human-written questions (74.3% on CLEVR-Human) still leaves room for improvement

## Confidence
High confidence in: Technical feasibility of using LLMs to interact with template-based VQA models in chain-of-qa fashion; Experimental methodology and dataset choices
Medium confidence in: State-of-the-art performance claims; Need for ablation studies to isolate CoQAH's contribution
Low confidence in: Generalizability to domains outside synthetic question-answering datasets and medical imaging

## Next Checks
1. Test CoQAH on additional visual domains (e.g., natural images, video understanding) to assess generalizability beyond synthetic and medical datasets
2. Conduct ablation study comparing CoQAH with and without LLM component to quantify specific contribution of chain-of-qa approach
3. Evaluate robustness of CoQAH to different LLM choices and parameter settings to understand sensitivity to these factors