---
ver: rpa2
title: 'SMART: Submodular Data Mixture Strategy for Instruction Tuning'
arxiv_id: '2403.08370'
source_url: https://arxiv.org/abs/2403.08370
tags:
- tasks
- arxiv
- task
- data
- mixture
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of balancing task proportions
  in instruction tuning of large language models, where determining the optimal mixture
  of tasks and instances is currently done through manual tuning or practitioner intuition.
  The proposed method, SMART, formulates this as a sequence of two cardinality-constrained
  submodular maximization problems to select representative tasks and non-redundant
  instances.
---

# SMART: Submodular Data Mixture Strategy for Instruction Tuning

## Quick Facts
- **arXiv ID**: 2403.08370
- **Source URL**: https://arxiv.org/abs/2403.08370
- **Reference count**: 40
- **Primary result**: SMART achieves up to 4.4% higher exact match on MMLU and BBH benchmarks compared to traditional mixing baselines

## Executive Summary
This paper addresses the challenge of balancing task proportions in instruction tuning of large language models, where current approaches rely on manual tuning or practitioner intuition. The authors propose SMART, a method that formulates task and instance selection as a sequence of cardinality-constrained submodular maximization problems. By treating task selection as a submodular optimization problem, SMART can automatically identify representative tasks and non-redundant instances without requiring extensive manual experimentation.

The approach demonstrates significant performance improvements over traditional proportional and equal mixing baselines, with Llama-2-7B achieving up to 4.4% higher exact match scores on standard benchmarks. Additionally, SMART enables effective instruction tuning with as few as 16 representative tasks instead of the full 1840 tasks, potentially reducing computational costs while maintaining or improving performance. The method uses ROUGE-1 scores and log-probabilities to measure task importance and instance similarity, respectively.

## Method Summary
SMART formulates instruction tuning as a two-stage submodular optimization problem. First, it selects a representative subset of tasks from a large collection using submodular maximization based on task importance scores derived from ROUGE-1 metrics. Second, for each selected task, it identifies non-redundant instances using submodular optimization with similarity measures based on log-probabilities. The method treats both stages as cardinality-constrained submodular maximization problems, allowing for efficient selection of diverse and representative data while maintaining computational tractability. Grid search experiments revealed that Graph Cut is optimal for task subset selection while Facility Location works best for instance selection when using the full task budget.

## Key Results
- Llama-2-7B achieves up to 4.4% higher exact match on MMLU and BBH benchmarks compared to proportional and equal mixing baselines
- SMART enables effective instruction tuning with as few as 16 representative tasks instead of the full 1840 tasks
- Grid search reveals Graph Cut optimal for task subset selection and Facility Location optimal for instance selection

## Why This Works (Mechanism)
The submodular optimization framework enables efficient selection of diverse and representative data by capturing diminishing returns in task importance and instance similarity. By treating task selection as a submodular problem, SMART can automatically identify tasks that provide the most information gain while avoiding redundancy. The two-stage approach ensures both task-level diversity and instance-level non-redundancy, addressing the limitations of uniform mixing strategies that treat all tasks and instances equally.

## Foundational Learning
- **Submodular optimization**: Functions that exhibit diminishing returns, enabling efficient selection of diverse subsets (needed for scalable data selection, quick check: verify submodularity property)
- **ROUGE-1 scoring**: Measures text similarity based on n-gram overlap, used to evaluate task importance (needed for quantifying task relevance, quick check: validate ROUGE-1 scores correlate with task quality)
- **Log-probability similarity**: Measures instance redundancy based on model confidence scores (needed for identifying non-redundant instances, quick check: confirm log-probabilities capture semantic similarity)

## Architecture Onboarding
**Component Map**: Task Collection -> Task Selection (Graph Cut) -> Instance Selection (Facility Location) -> Instruction Tuning

**Critical Path**: The core workflow involves calculating ROUGE-1 scores for all tasks, running Graph Cut optimization to select representative tasks, then applying Facility Location to select non-redundant instances within each task before instruction tuning.

**Design Tradeoffs**: SMART trades computational overhead of submodular optimization for improved performance and reduced dataset size, versus simpler but less effective uniform mixing strategies.

**Failure Signatures**: Poor performance may indicate inadequate task importance scoring, suboptimal submodular function choice, or insufficient task diversity in the initial collection.

**First Experiments**:
1. Validate ROUGE-1 scores correlate with downstream task performance
2. Test different submodular functions (Graph Cut vs. Facility Location) on a small task subset
3. Compare instance selection quality using log-probability vs. other similarity measures

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Submodular optimization assumes task importance can be captured through ROUGE-1 scores and log-probabilities, which may not fully represent complex task relationships
- Experimental validation limited to Llama-2-7B models and specific benchmarks (MMLU and BBH), raising generalizability concerns
- Computational overhead of calculating ROUGE scores and log-probabilities for large task collections not thoroughly analyzed

## Confidence
- **SMART outperforms baselines (High confidence)**: Supported by concrete numerical improvements (4.4% EM gain) across multiple experiments
- **SMART works with 16 tasks (Medium confidence)**: Demonstrated through ablation studies, but "effective" performance threshold not clearly defined
- **Graph Cut optimal for tasks, Facility Location for instances (Medium confidence)**: Based on grid search results, but search space and hyperparameter ranges not fully specified

## Next Checks
1. Test SMART's performance on different model architectures (e.g., OPT, BLOOM) and additional benchmarks beyond MMLU and BBH
2. Conduct comprehensive analysis of computational overhead, including wall-clock time and memory usage for calculating submodular scores
3. Evaluate robustness to noisy or adversarial data by introducing corrupted instances and measuring performance degradation