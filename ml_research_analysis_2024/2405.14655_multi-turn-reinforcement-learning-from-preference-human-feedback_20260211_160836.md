---
ver: rpa2
title: Multi-turn Reinforcement Learning from Preference Human Feedback
arxiv_id: '2405.14655'
source_url: https://arxiv.org/abs/2405.14655
tags:
- policy
- teacher
- student
- preference
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper extends reinforcement learning from human feedback (RLHF)
  to multi-turn conversations, where feedback compares entire dialogues rather than
  individual actions. The authors propose Multi-turn Preference Optimization (MTPO),
  a mirror-descent-based algorithm that converges to a Nash equilibrium in the preference-based
  setting, and prove its theoretical guarantees.
---

# Multi-turn Reinforcement Learning from Preference Human Feedback

## Quick Facts
- arXiv ID: 2405.14655
- Source URL: https://arxiv.org/abs/2405.14655
- Reference count: 40
- Key outcome: Extends RLHF to multi-turn conversations using conversation-level preference feedback

## Executive Summary
This paper addresses the challenge of applying reinforcement learning from human feedback (RLHF) to multi-turn conversations, where traditional RLHF methods only handle single-turn interactions. The authors develop Multi-turn Preference Optimization (MTPO), a novel algorithm that uses conversation-level preference feedback to optimize dialogue policies. The approach treats entire conversations as actions and applies mirror-descent optimization to find Nash equilibria in the preference-based setting. The method is validated in an Education Dialogue environment, demonstrating significant improvements over single-turn RLHF baselines.

## Method Summary
The authors propose MTPO, which extends single-turn RLHF to multi-turn conversations by treating entire dialogues as actions. The algorithm uses mirror-descent optimization to find Nash equilibria in the preference-based setting, where feedback compares whole conversations rather than individual actions. For implementation, they develop a deep RL variant that trains large language models using conversation-level preferences. The method estimates value functions for entire conversations and updates policies based on these estimates, handling the challenge that feedback is only available at the conversation level rather than for individual turns.

## Key Results
- MTPO significantly outperforms single-turn RLHF baselines in an Education Dialogue environment
- The method achieves comparable performance to reward-based RL even when using only preference signals
- MTPO demonstrates the advantage of conversation-level feedback for optimizing multi-turn interactions
- The approach converges to a Nash equilibrium in the preference-based setting, as proven theoretically

## Why This Works (Mechanism)
MTPO works by treating multi-turn conversations as single actions and optimizing policies based on conversation-level preference feedback. The mirror-descent algorithm finds Nash equilibria in this setting, allowing the policy to improve based on holistic feedback about dialogue quality. The value estimation captures the expected preference score for entire conversations, enabling gradient-based policy updates that account for the sequential nature of dialogue. By operating at the conversation level rather than turn level, MTPO can optimize for long-term dialogue outcomes rather than just immediate responses.

## Foundational Learning
1. **Mirror Descent Optimization** - Why needed: Provides the theoretical foundation for converging to Nash equilibria in non-convex settings; Quick check: Verify convergence guarantees in multi-agent games
2. **Preference-based RL vs Reward-based RL** - Why needed: Preference feedback is often more practical than explicit rewards; Quick check: Compare sample efficiency of preference vs reward signals
3. **Value Estimation for Sequences** - Why needed: Enables policy updates based on conversation-level feedback; Quick check: Validate that estimated values correlate with human preferences
4. **Multi-turn Policy Optimization** - Why needed: Single-turn methods cannot capture long-term dialogue dynamics; Quick check: Measure performance degradation when applying single-turn methods to multi-turn tasks
5. **Language Model Fine-tuning with Preferences** - Why needed: Allows integration with existing LLM architectures; Quick check: Monitor KL divergence between base and fine-tuned models
6. **Nash Equilibrium in Policy Optimization** - Why needed: Provides stable solution concept for multi-agent settings; Quick check: Test stability under different preference distributions

## Architecture Onboarding

Component Map: Human Preferences -> Value Model -> Policy Network -> Generated Dialogues -> Preference Comparator

Critical Path: Value Model estimates conversation preferences → Policy Network generates dialogues → Human provides preferences → Value Model updates → Policy Network updates

Design Tradeoffs: Turn-level vs token-level value estimation (computational efficiency vs granularity), KL regularization (stability vs flexibility), preference quality vs quantity

Failure Signatures: Preference model overfitting to training data, policy collapse to degenerate strategies, value estimation bias in long conversations

First Experiments: 1) Validate value model correlates with human preferences on held-out conversations, 2) Test policy updates improve preference scores on validation set, 3) Measure convergence speed vs single-turn baselines

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MTPO's performance scale with model size and in more complex environments beyond the current T5-based models and simple prompts?
- Basis in paper: The paper acknowledges limitations regarding the use of relatively small T5-based models and prompt-based environments, stating "This work presents a proof of concept for the potential of MTPO to improve existing single-turn techniques. Our experimental setup might be limited by the relatively small T5-based models and the use of prompt-based environments."
- Why unresolved: The experiments were conducted on small T5 models (770M and 3B parameters) and simplified environments. The authors explicitly state this is a proof of concept and suggest applications to state-of-the-art models as future work.
- What evidence would resolve it: Experiments applying MTPO to larger language models (e.g., 70B+ parameter models) and more complex, realistic environments with richer interaction dynamics.

### Open Question 2
- Question: What is the optimal way to combine turn-level and token-level value estimates in the auto-regressive case?
- Basis in paper: "Finally, when the policy is an auto-regressive language model, which generates actions token-by-token until an end-of-sequence signal is generated, we use a turn-level value (and not a token-level value as done in [Stiennon et al., 2020]). That is, the value model gets as input a state represented by a sequence of tokens, and outputs a single scalar value instead of a scalar value for each token in the action sequence. This is justified by our analysis which treats whole turns as single actions. We leave the many ways to combine turn-level and token-level values for future research."
- Why unresolved: The paper explicitly leaves this as future research, noting that their current implementation uses turn-level values but acknowledges there are "many ways to combine turn-level and token-level values."
- What evidence would resolve it: Comparative experiments testing different value estimation approaches (pure turn-level, pure token-level, and hybrid approaches) across various domains and model sizes.

### Open Question 3
- Question: Does the KL regularization term KL(πθt∣∣πθt−1) provide significant benefits for stability and performance that would justify its computational cost?
- Basis in paper: "For simplicity and computational efficiency, we implement a policy-gradient (PG) based approach and ignore the MD stability term KL(πθt∣∣πθt−1), similarly to the implementation of the Nash-MD algorithm. We justify this simplification with the fact that the KL regularization w.r.t. the fixed reference policy µ already provides stability to our online algorithm... Nevertheless, we believe that this additional MD penalty should contribute to the performance and stability of the algorithm, as shown in [Tomar et al., 2020], and we leave this for further research."
- Why unresolved: The authors explicitly chose to omit this term for computational efficiency but acknowledge it might provide benefits, leaving this as an open question for future research.
- What evidence would resolve it: Empirical comparison of MTPO with and without the additional MD penalty across multiple domains, measuring both performance and training stability metrics.

## Limitations
- The theoretical framework assumes access to "accurate" human preferences, which may not hold in practice where preferences can be inconsistent, noisy, or influenced by factors beyond dialogue quality
- The Education Dialogue environment represents a narrow domain that may not capture the complexity and diversity of real-world multi-turn interactions
- The paper does not address how preference feedback quality degrades over extended conversations or how to handle conflicting preferences from multiple users

## Confidence

High confidence in the theoretical contributions regarding the mirror-descent algorithm and Nash equilibrium convergence properties, as these are mathematically formalized.

Medium confidence in the experimental results due to the controlled nature of the Education Dialogue environment and the limited scope of evaluation metrics.

Low confidence in the generalizability of findings to open-domain conversations, commercial applications, or scenarios with unreliable or inconsistent human feedback.

## Next Checks

1. Test MTPO across diverse multi-turn domains (e.g., customer service, negotiation, creative collaboration) to evaluate robustness beyond educational contexts
2. Implement human preference noise injection in experiments to measure algorithm performance degradation under realistic feedback conditions
3. Compare MTPO against single-turn RLHF variants in large-scale human preference studies with thousands of conversation pairs to validate statistical significance of improvements