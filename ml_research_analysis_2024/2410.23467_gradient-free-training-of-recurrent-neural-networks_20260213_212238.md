---
ver: rpa2
title: Gradient-free training of recurrent neural networks
arxiv_id: '2410.23467'
source_url: https://arxiv.org/abs/2410.23467
tags:
- training
- koopman
- networks
- neural
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of training recurrent neural
  networks (RNNs) for time-dependent problems, particularly those involving chaotic
  dynamics, where backpropagation through time often suffers from exploding or vanishing
  gradients. The proposed solution combines random feature networks with Koopman operator
  theory, avoiding gradient-based training entirely.
---

# Gradient-free training of recurrent neural networks

## Quick Facts
- arXiv ID: 2410.23467
- Source URL: https://arxiv.org/abs/2410.23467
- Reference count: 40
- Primary result: Gradient-free training of RNNs using random feature networks and Koopman operator theory achieves comparable or better forecasting accuracy than backpropagation while reducing training time by orders of magnitude.

## Executive Summary
This paper addresses the challenge of training recurrent neural networks (RNNs) for time-dependent problems, particularly chaotic dynamics, where backpropagation through time suffers from exploding or vanishing gradients. The authors propose a gradient-free approach that combines random feature networks with Koopman operator theory, sampling hidden layer parameters at random and constructing outer weights using extended dynamic mode decomposition (EDMD). Computational experiments on time series, chaotic systems (Lorenz, Rössler), and real-world weather data show this method achieves comparable or better forecasting accuracy while reducing training time by orders of magnitude compared to gradient-based methods like shPLRNN.

## Method Summary
The method samples hidden layer parameters (weights and biases) from data-dependent distributions based on pairwise state differences, then computes outer weights K, B, and C using extended dynamic mode decomposition (EDMD), a Koopman operator approximation technique. This bypasses backpropagation entirely, with the hidden layer parameters constructed from pairs of data points with direction along the data manifold. The approach interprets the linear recurrence as a finite-dimensional approximation of the Koopman operator, enabling stability analysis through eigenvalue inspection and providing convergence guarantees in the limit of infinite network width.

## Key Results
- Gradient-free training achieves comparable or better forecasting accuracy than shPLRNN, ESN, and LSTM baselines
- Training time reduced by orders of magnitude compared to gradient-based methods
- Koopman connection enables stability analysis through eigenvalue inspection of learned matrices
- Method successfully handles chaotic systems (Lorenz, Rössler) and real-world weather data forecasting

## Why This Works (Mechanism)

### Mechanism 1
Gradient-free training avoids exploding/vanishing gradients by replacing iterative optimization with random feature sampling plus linear solver. Hidden layer parameters are sampled from data-dependent distributions, then outer weights are computed via least-squares using Koopman-based EDMD, bypassing backpropagation entirely.

### Mechanism 2
Koopman operator theory provides structure and interpretability to RNN outer layers, enabling stability analysis via eigenvalues. By interpreting the linear recurrence as a finite-dimensional approximation of the Koopman operator, spectral properties of the learned matrix K directly inform stability and dynamics.

### Mechanism 3
Sampling from data-dependent distributions improves accuracy and interpretability over data-agnostic distributions. Weights are constructed from pairs of data points with direction along the data manifold, biasing neurons toward informative directions and avoiding uniform random sampling pitfalls.

## Foundational Learning

- **Koopman operator theory**: Provides mathematical foundation for interpreting RNNs as linear evolution in a lifted space, enabling convergence proofs and stability analysis. Quick check: What does the Koopman operator evolve in time—the state or observables of the state?
- **Extended Dynamic Mode Decomposition (EDMD)**: Finite-dimensional approximation method used to compute outer weights K, B, and C from sampled data. Quick check: In EDMD, what does the matrix K approximate when applied to the dictionary of observables?
- **Random feature networks and sampling strategies**: Technique to generate hidden layer parameters without gradient descent, crucial for gradient-free approach. Quick check: How does the sampling scheme ensure that neurons align with the data manifold?

## Architecture Onboarding

- **Component map**: Input → Random feature layer (sampled weights/biases) → Lifted space → Linear Koopman step (K, B) → Projection (C) → Output
- **Critical path**:
  1. Sample weights/biases from data-dependent distributions
  2. Apply hidden layer activation to input and state
  3. Solve linear system (via EDMD) for K, B, and C
  4. Combine into recurrent step: h_t = C(Kσ(W h_{t-1}+b) + Bσ(W_x x + b_x))
- **Design tradeoffs**:
  - More neurons → better approximation but larger linear system and memory use
  - Narrower networks → faster training but risk underfitting dynamics
  - Data-dependent sampling vs. uniform → better coverage but needs representative data
- **Failure signatures**:
  - High training error: likely insufficient neurons or poor sampling coverage
  - Unstable eigenvalues in K: may indicate poor approximation or chaotic dynamics beyond model capacity
  - Slow convergence of EDMD: may be due to ill-conditioned data matrices or too few samples
- **First 3 experiments**:
  1. Van der Pol oscillator with full state observation, 80 tanh neurons, check training MSE and eigenvalue stability
  2. Lorenz system, 200 tanh neurons, compare EKL to ESN baseline
  3. Weather data, time-delay embedding, 256 tanh neurons, predict temperature 1 day ahead, compare to LSTM

## Open Questions the Paper Calls Out

- **Can the Koopman operator approach extend to high-dimensional data like images or natural language?**: Current challenges include applying the method to high-dimensional data as the sampling scheme used to construct hidden weights is not currently useful for constructing parameters for such architectures, though it is an intriguing challenge to work towards sampling them.

- **How can the theoretical results be extended to controlled systems with inputs?**: Remaining challenges in theoretical work include extending the convergence theory to controlled systems, as well as bridging Koopman theory for continuous dynamical systems and Neural ODEs.

- **What is the optimal balance between the number of neurons and the number of data points to minimize computational complexity?**: The complexity of solving the linear system depends cubically on the minimum number of neurons and the number of data points, leading to rapid growth in computational time and memory demands if both grow together.

## Limitations
- Theoretical convergence guarantees rely on infinite-width approximations, but finite-width behavior in chaotic regimes is less characterized
- Sampling from data-dependent distributions requires representative data; extrapolation to unseen regimes may fail
- Koopman operator approximations via EDMD can be sensitive to dictionary choice and matrix conditioning

## Confidence
- Mechanism 1 (gradient-free training avoiding gradient issues): High
- Mechanism 2 (Koopman theory enabling stability analysis): Medium
- Mechanism 3 (data-dependent sampling improving accuracy): Low

## Next Checks
1. Validate convergence rates for EDMD in high-dimensional chaotic systems versus random feature sampling
2. Test extrapolation performance on out-of-distribution initial conditions and parameter regimes
3. Benchmark computational scaling with increasing hidden layer width and system dimension