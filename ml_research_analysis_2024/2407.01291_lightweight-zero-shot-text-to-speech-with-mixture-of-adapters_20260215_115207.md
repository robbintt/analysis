---
ver: rpa2
title: Lightweight Zero-shot Text-to-Speech with Mixture of Adapters
arxiv_id: '2407.01291'
source_url: https://arxiv.org/abs/2407.01291
tags: []
core_contribution: This paper proposes a lightweight zero-shot text-to-speech (TTS)
  method using a mixture of adapters (MoA) to enable high-quality speech synthesis
  across thousands of speakers with minimal additional parameters. The method incorporates
  MoA modules into the decoder and variance adapter of a non-autoregressive TTS model,
  selecting appropriate adapters based on speaker embeddings.
---

# Lightweight Zero-shot Text-to-Speech with Mixture of Adapters

## Quick Facts
- arXiv ID: 2407.01291
- Source URL: https://arxiv.org/abs/2407.01291
- Authors: Kenichi Fujita; Takanori Ashihara; Marc Delcroix; Yusuke Ijima
- Reference count: 0
- Primary result: Achieves better performance than baseline with less than 40% of parameters at 1.9× faster inference speed while maintaining high naturalness and similarity in subjective evaluations

## Executive Summary
This paper proposes a lightweight zero-shot text-to-speech (TTS) method using mixture of adapters (MoA) to enable high-quality speech synthesis across thousands of speakers with minimal additional parameters. The method incorporates MoA modules into the decoder and variance adapter of a non-autoregressive TTS model, selecting appropriate adapters based on speaker embeddings. Experiments show that the proposed method achieves better performance than the baseline with less than 40% of parameters at 1.9 times faster inference speed, while maintaining high naturalness and similarity in subjective evaluations.

## Method Summary
The proposed method uses a FastSpeech2 backbone with mixture of adapters (MoA) modules inserted into the decoder and variance adapter (pitch, energy, duration predictors). MoA consists of multiple lightweight bottleneck adapters and a trainable gating network that selects appropriate adapters based on speaker embeddings extracted from reference speech using an SSL model (HuBERT). Two MoA types are explored: sparse (8 adapters with top-3 sampling) and dense (3 adapters with no sampling). The entire model is jointly trained with multi-task objectives including MSE losses and importance loss for balanced adapter weights. HiFi-GAN is used for waveform generation without fine-tuning.

## Key Results
- MoA-based models achieve better performance than baseline models with less than 40% of parameters
- Inference speed is 1.9 times faster compared to dense MoA models
- Subjective evaluations show comparable or superior naturalness and similarity to baselines
- Proposed(s) model significantly outperforms S and M/S models for professional speakers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixture of Adapters (MoA) modules enhance the TTS model's ability to adapt to diverse speakers with minimal additional parameters.
- Mechanism: The MoA modules consist of multiple lightweight bottleneck adapters and a trainable gating network that selects appropriate adapters based on speaker embeddings. This allows the model to dynamically configure itself depending on speaker characteristics.
- Core assumption: Speaker embeddings can effectively route the input through appropriate adapters to capture speaker-specific features.
- Evidence anchors:
  - [abstract]: "These modules enhance the ability to adapt a wide variety of speakers in a zero-shot manner by selecting appropriate adapters associated with speaker characteristics on the basis of speaker embeddings."
  - [section]: "The MoA module is expressed as follows: MoA(x, xe) = x + Σgi(xe) · Adapteri(x)"
  - [corpus]: Weak evidence - related papers mention adapters but do not specifically discuss mixture of adapters or gating based on speaker embeddings.
- Break condition: If speaker embeddings are not discriminative enough to route to appropriate adapters, or if adapters are not specialized enough to capture speaker characteristics.

### Mechanism 2
- Claim: Sparse gating in MoA modules improves performance while maintaining inference efficiency.
- Mechanism: During training, a large number of adapters are used with sparse gating (only top-k weights are non-zero). At inference, only the selected adapters are used, reducing computational cost while maintaining expressiveness.
- Core assumption: The sparse gating mechanism can effectively select the most relevant adapters without losing important information.
- Evidence anchors:
  - [section]: "In the former, there were 8 adapters (N=8), and the k in top-k sampling was set to 3, while in the latter, N and k were both set to 3"
  - [section]: "Comparing the results from non-professional and professional speakers... Proposed(s) exhibited superior performance compared with S and M/S"
  - [corpus]: Weak evidence - related papers mention adapters but do not specifically discuss sparse gating in the context of MoA.
- Break condition: If the top-k selection is not optimal, leading to loss of important adapter contributions, or if the sparse gating mechanism becomes too complex to train effectively.

### Mechanism 3
- Claim: Joint training of the entire model including SSL-based embedding extractor with MoA modules leads to better speaker embeddings for TTS.
- Mechanism: By training the TTS model and embedding module together, suitable speaker embeddings are obtained that are optimized for the TTS task.
- Core assumption: The joint training process can effectively align the speaker embedding space with the requirements of the TTS model.
- Evidence anchors:
  - [section]: "As both the TTS model and embedding module are jointly trained, suitable speaker embeddings for the TTS model are obtained from the embedding module."
  - [section]: "Through the training process, HuBERT remained frozen. For equitable comparison, all other models trained without MoA modules underwent a total training duration of 800K steps."
  - [corpus]: Weak evidence - related papers mention SSL-based embeddings but do not discuss joint training with MoA modules.
- Break condition: If the joint training process leads to overfitting or if the SSL model (HuBERT) is not sufficiently expressive to capture speaker characteristics.

## Foundational Learning

- Concept: Mixture of Experts (MoE) and its variants
  - Why needed here: Understanding MoE provides the theoretical foundation for why MoA works - it's about routing inputs through specialized sub-networks
  - Quick check question: How does MoE differ from traditional ensemble methods in terms of parameter efficiency and routing mechanisms?

- Concept: Adapter-based fine-tuning
  - Why needed here: MoA builds on adapter concepts - understanding how adapters work in NLP helps grasp how they're applied here
  - Quick check question: What are the key differences between adapter-based fine-tuning and full fine-tuning in terms of parameter efficiency?

- Concept: Self-supervised learning for speaker representation
  - Why needed here: The paper uses SSL-based embeddings (HuBERT) - understanding SSL is crucial for grasping how speaker characteristics are extracted
  - Quick check question: How does SSL-based speaker embedding extraction differ from traditional x-vector or d-vector approaches in terms of data requirements and performance?

## Architecture Onboarding

- Component map:
  - Encoder (FFT blocks) -> Decoder (FFT blocks with MoA modules) -> Variance adapter (Pitch, Energy, Duration predictors with MoA) -> SSL-based embedding extractor (HuBERT + embedding module) -> Gating network (selects adapters based on speaker embeddings) -> Adapter pool (N bottleneck adapters per MoA module)

- Critical path:
  1. Extract speaker embedding from reference speech
  2. Feed linguistic features and speaker embedding into encoder
  3. Process through encoder FFT blocks
  4. Route through decoder FFT blocks with MoA selection
  5. Generate mel-spectrogram
  6. Vocode to waveform

- Design tradeoffs:
  - MoA vs full model expansion: MoA provides better parameter efficiency but may have routing complexity
  - Dense vs sparse MoA: Dense has more expressiveness but higher inference cost; sparse is more efficient but may lose some information
  - Joint vs separate training: Joint training optimizes embeddings for TTS but may be harder to train

- Failure signatures:
  - Poor speaker similarity: Check if gating network is routing correctly or if adapters are not specialized enough
  - Quality degradation: Verify that MoA insertion isn't disrupting the original model's learning
  - Slow inference: Ensure sparse gating is working as intended and not selecting too many adapters

- First 3 experiments:
  1. Ablation study: Remove MoA modules and compare with baseline to confirm their contribution
  2. Adapter analysis: Examine adapter weights across speakers to verify specialization
  3. Routing visualization: Visualize gating network outputs to understand how speakers are being routed through different adapters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method scale when applied to larger TTS models with different architectures, such as VALL-E?
- Basis in paper: [explicit] The paper mentions that future work includes applying the proposed method to larger TTS models including different structure models such as VALL-E, suggesting this is an open question.
- Why unresolved: The paper does not provide experimental results or analysis on how the method performs with larger models or different architectures.
- What evidence would resolve it: Experimental results comparing the proposed method's performance on larger models like VALL-E versus smaller models, and analysis of any changes in effectiveness or efficiency.

### Open Question 2
- Question: What is the impact of the proposed method on speech synthesis quality for professional versus non-professional speakers?
- Basis in paper: [explicit] The paper notes that professional speakers generally have more dynamic speaking styles, which are more challenging to reproduce, and that there were fewer professional speakers in the training data. It also mentions that the proposed method showed significant improvement for professional speakers in subjective evaluations.
- Why unresolved: While the paper provides some comparison between professional and non-professional speakers, it does not deeply analyze why the method performs better for professional speakers or what specific aspects of the speech are improved.
- What evidence would resolve it: Detailed analysis of speech synthesis quality metrics (e.g., MCD, RMSE of F0) broken down by professional and non-professional speakers, and qualitative analysis of the synthesized speech samples.

### Open Question 3
- Question: How does the choice of adapter size and number of adapters affect the performance of the proposed method?
- Basis in paper: [explicit] The paper mentions that the bottleneck size of the adapters was 96 and that there were 8 adapters in the sparse version with k=3. It also notes that having a larger number of adapters and selecting them through sparse gating may lead to better speaker representation.
- Why unresolved: The paper does not explore the impact of varying the adapter size or the number of adapters on the performance of the proposed method. It only mentions the specific values used in their experiments.
- What evidence would resolve it: A systematic study varying the adapter size and number of adapters, with corresponding performance metrics (e.g., MCD, RMSE of F0) and computational efficiency measures (e.g., inference speed).

## Limitations

- The evaluation uses a proprietary Japanese speech database, limiting reproducibility and generalizability to other languages and datasets
- Only three baseline models are compared against, without benchmarking against recent adapter-based TTS approaches mentioned in the literature
- The sparse gating mechanism's effectiveness in selecting genuinely distinct and complementary adapters is not thoroughly validated through detailed analysis

## Confidence

**High Confidence**: The parameter efficiency claims (40% reduction with better performance) and inference speed improvements (1.9× faster) are well-supported by the experimental results and align with the theoretical advantages of MoA modules.

**Medium Confidence**: The naturalness and similarity improvements are demonstrated through subjective evaluations, but the relatively small listener pools (10 listeners for AB tests, 7 for XAB tests) and limited number of test samples (64 speakers) reduce confidence in the generalizability of these results.

**Low Confidence**: Claims about the "lightweight" nature of the approach are relative - while MoA adds minimal parameters compared to full model expansion, the base FastSpeech2 models still require substantial parameters (14M-151M range). The practical deployment advantages over other adapter-based approaches are not fully quantified.

## Next Checks

1. **Adapter Specialization Analysis**: Conduct a detailed analysis of the adapter weight distributions across different speaker groups (professional vs non-professional, gender, age) to verify that the MoA modules are learning genuinely specialized representations rather than redundant functions.

2. **Cross-Dataset Generalization Test**: Evaluate the trained models on a different zero-shot TTS benchmark (such as VCTK or LibriTTS) to assess whether the joint SSL-TTS training approach provides benefits beyond the specific Japanese dataset used in training.

3. **Robustness Under Limited Data**: Test the system's performance when fine-tuning adapters with limited adaptation data (5, 10, 20 utterances) per speaker to better understand the practical trade-offs between zero-shot and few-shot adaptation scenarios.