---
ver: rpa2
title: 'Spirit LM: Interleaved Spoken and Written Language Model'
arxiv_id: '2402.05755'
source_url: https://arxiv.org/abs/2402.05755
tags:
- speech
- text
- tokens
- pirit
- spirit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SPIRIT LM, a multimodal language model that
  can generate both text and speech in a cross-modal manner. The model is based on
  a 7B pretrained text language model (LLaMA 2) that is continuously trained on text
  and speech units.
---

# Spirit LM: Interleaved Spoken and Written Language Model

## Quick Facts
- **arXiv ID**: 2402.05755
- **Source URL**: https://arxiv.org/abs/2402.05755
- **Reference count**: 35
- **Primary result**: Introduces Spirit LM, a multimodal language model that generates both text and speech with cross-modal capabilities

## Executive Summary
Spirit LM is a novel multimodal language model that can generate both text and speech in an interleaved manner. Built on a 7B parameter LLaMA 2 backbone, it is continuously trained on aligned text and speech sequences at the word level. The model comes in two variants: BASE using phonetic speech units (HuBERT), and EXPRESSIVE which adds pitch and style units to capture speech expressivity. The key innovation is the interleaved training approach that allows the model to learn aligned representations between speech and text units while maintaining semantic abilities and expressive speech generation capabilities.

## Method Summary
The Spirit LM architecture builds upon a pretrained 7B parameter LLaMA 2 text language model. During training, text and speech sequences are interleaved at the word level, allowing the model to learn aligned representations between speech and text units. The model uses discrete speech units extracted from a HuBERT model for the BASE variant, while the EXPRESSIVE variant incorporates additional pitch and style units to capture prosodic features. This continuous training approach enables the model to handle both modalities while maintaining the semantic capabilities of the original text model and adding expressive speech generation abilities.

## Key Results
- Spirit LM successfully generates both text and speech with cross-modal capabilities
- The model preserves sentiment across modalities as measured by the newly introduced Speech-Text Sentiment Preservation (STSP) benchmark
- Demonstrates few-shot learning abilities across multiple tasks including ASR, TTS, and speech classification

## Why This Works (Mechanism)
The interleaved training approach at the word level allows Spirit LM to learn aligned representations between speech and text units without requiring explicit alignment supervision. By continuously training on both modalities, the model can leverage the semantic understanding from the pretrained text model while learning the acoustic patterns and expressivity of speech. The use of discrete speech units (HuBERT for BASE, plus pitch/style for EXPRESSIVE) provides a structured representation that the model can effectively process alongside text tokens.

## Foundational Learning
- **Discrete speech units**: Why needed - To represent speech in a form compatible with language model architectures; Quick check - Verify that HuBERT units capture phonetic information effectively
- **Interleaved sequence training**: Why needed - To learn aligned representations between speech and text without explicit alignment supervision; Quick check - Confirm that word-level interleaving preserves semantic relationships
- **Cross-modal few-shot learning**: Why needed - To demonstrate the model's ability to transfer knowledge between modalities; Quick check - Test performance on held-out ASR and TTS tasks with limited examples
- **Expressivity modeling**: Why needed - To capture prosodic features like pitch and speaking style in speech generation; Quick check - Compare expressive vs. neutral speech samples for naturalness
- **Sentiment preservation**: Why needed - To evaluate whether the model maintains emotional content across modalities; Quick check - Measure sentiment scores consistency between input and output
- **Multimodal evaluation metrics**: Why needed - To comprehensively assess performance across both text and speech domains; Quick check - Validate that metrics capture relevant aspects of cross-modal quality

## Architecture Onboarding

**Component Map**: Text tokens -> LLaMA 2 backbone -> Speech units (HuBERT) + Pitch/Style units (EXPRESSIVE) -> Cross-modal output

**Critical Path**: Training data preparation (text+speech interleaving) -> Continuous training on LLaMA 2 -> Speech unit extraction (HuBERT/pitch/style) -> Cross-modal generation capabilities

**Design Tradeoffs**: Unified model vs. specialized ASR/TTS systems - offers flexibility and cross-modal capabilities but may sacrifice some performance compared to task-specific models; Word-level interleaving vs. sentence-level - enables finer alignment but may be more computationally intensive

**Failure Signatures**: Degradation in performance on longer sequences due to memory constraints; Loss of expressivity in speech generation when using only phonetic units; Difficulty handling diverse accents or languages not well-represented in training data

**First 3 Experiments**:
1. Evaluate cross-modal generation quality by prompting with text and measuring the semantic and stylistic consistency of generated speech
2. Test few-shot learning capabilities by providing minimal examples for ASR, TTS, and classification tasks
3. Compare sentiment preservation between Spirit LM and baseline models using the STSP benchmark

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Performance trade-offs compared to specialized ASR and TTS models
- Limited evaluation primarily to English data, with unclear performance on multilingual or accented speech
- Potential degradation on longer sequences due to the interleaved training approach

## Confidence

| Claim | Confidence |
|-------|------------|
| Cross-modal generation capabilities | High |
| Few-shot learning across modalities | Medium |
| Expressive speech generation | Medium-High |

## Next Checks
1. Benchmark Spirit LM against specialized ASR and TTS models on standard evaluation datasets to quantify the performance trade-offs of the unified approach
2. Evaluate the model's performance and robustness on multilingual data, including low-resource languages and diverse accents
3. Conduct extensive user studies to assess the perceived naturalness and expressivity of the generated speech, particularly for the EXPRESSIVE variant