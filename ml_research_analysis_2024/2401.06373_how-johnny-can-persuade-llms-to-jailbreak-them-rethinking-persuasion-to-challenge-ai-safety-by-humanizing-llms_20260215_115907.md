---
ver: rpa2
title: 'How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge
  AI Safety by Humanizing LLMs'
arxiv_id: '2401.06373'
source_url: https://arxiv.org/abs/2401.06373
tags:
- persuasion
- harmful
- jailbreak
- redacted
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how persuasion can be used to jailbreak
  large language models (LLMs) by treating them as human-like communicators rather
  than mere instruction followers. The authors propose a persuasion taxonomy derived
  from social science research, which is then used to automatically generate persuasive
  adversarial prompts (PAP) to jailbreak LLMs.
---

# How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs

## Quick Facts
- arXiv ID: 2401.06373
- Source URL: https://arxiv.org/abs/2401.06373
- Reference count: 40
- Key outcome: Persuasion techniques can effectively jailbreak LLMs with over 92% success rate

## Executive Summary
This paper introduces a novel perspective on jailbreaking large language models (LLMs) by treating them as human-like communicators rather than mere instruction followers. The authors propose a persuasion taxonomy derived from social science research and use it to automatically generate persuasive adversarial prompts (PAP) that consistently jailbreak LLMs across various risk categories. The approach demonstrates superior performance compared to recent algorithm-focused attacks and highlights the inadequacy of current defenses against such persuasion-based jailbreaks.

## Method Summary
The method involves developing a persuasion taxonomy with 13 strategies and 40 techniques based on social science research. A persuasive paraphraser (GPT-3.5) is fine-tuned on manually crafted PAP training data to automatically generate persuasive adversarial prompts. These PAP are then evaluated using a GPT-4 Judge to assess their effectiveness in jailbreaking target LLMs. The approach iteratively refines PAP based on successful examples and applies different persuasion techniques to probe various LLM architectures.

## Key Results
- PAP consistently achieves over 92% attack success rate on Llama-2-7b Chat, GPT-3.5, and GPT-4
- The method outperforms recent algorithm-focused attacks in jailbreak effectiveness
- Current defenses are inadequate against PAP, highlighting the need for more fundamental mitigation strategies

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed method treats LLMs as human-like communicators rather than mere instruction followers, exploiting their susceptibility to persuasive communication.
- Mechanism: By applying a persuasion taxonomy derived from social science research, the method generates persuasive adversarial prompts (PAP) that effectively jailbreak LLMs by appealing to their human-like understanding of nuanced interpersonal influence.
- Core assumption: LLMs can understand and respond to persuasive communication in a way that bypasses their safety guardrails, even when the underlying intent remains harmful.
- Evidence anchors:
  - [abstract]: "We introduce a new perspective on jailbreaking LLMs as human-like communicators to explore this overlooked intersection between everyday language interaction and AI safety."
  - [section]: "Our approach innovatively treats LLMs as human-like communicators and grounds our study on a taxonomy informed by decades of social science research on human communication."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.374, average citations=0.0. Top related titles: Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak Prevention for Large Language Models, MetaBreak: Jailbreaking Online LLM Services via Special Token Manipulation, How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States.
- Break condition: The mechanism would break if LLMs were trained specifically to recognize and resist persuasive communication patterns, or if their safety mechanisms were designed to detect and block such nuanced human-like interactions.

### Mechanism 2
- Claim: The persuasion taxonomy provides a systematic framework for generating interpretable PAP that can consistently jailbreak LLMs across various risk categories.
- Mechanism: The taxonomy categorizes 40 persuasion techniques into 13 strategies, which are then used to automatically generate PAP that bypass LLM safety guardrails by framing harmful queries in a persuasive context.
- Core assumption: The systematic application of persuasion techniques through the taxonomy can generate PAP that are both interpretable and effective in jailbreaking LLMs.
- Evidence anchors:
  - [abstract]: "We propose a persuasion taxonomy derived from decades of social science research. Then, we apply the taxonomy to automatically generate interpretable persuasive adversarial prompts (PAP) to jailbreak LLMs."
  - [section]: "Our taxonomy, detailed in Table 1, classifies 40 persuasion techniques into 13 broad strategies based on extensive social science research across psychology, communication, sociology, marketing, and so on."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.374, average citations=0.0. Top related titles: Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak Prevention for Large Language Models, MetaBreak: Jailbreaking Online LLM Services via Special Token Manipulation, How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States.
- Break condition: This mechanism would fail if the generated PAP became too formulaic or if the LLMs were trained to recognize and resist the specific persuasion techniques outlined in the taxonomy.

### Mechanism 3
- Claim: The iterative refinement of PAP based on successful examples significantly improves jailbreak success rates across different LLM architectures.
- Mechanism: By fine-tuning a Persuasive Paraphraser on successful PAP examples and iteratively applying different persuasion techniques, the method can jailbreak even more advanced LLMs like GPT-4 with high success rates.
- Core assumption: The iterative refinement process, which mimics human behavior in refining effective prompts, can overcome the safety measures of various LLM architectures.
- Evidence anchors:
  - [abstract]: "We mimic human users and fine-tune a more targeted Persuasive Paraphraser on these successful PAP, to refine the jailbreak. Then we iteratively apply different persuasion techniques to generate PAP and perform a more in-depth probe on LLMs."
  - [section]: "In real-world jailbreaks, users will refine effective prompts to improve the jailbreak process. So after identifying successful PAP in the broad scan step, we mimic human users and fine-tune a more targeted Persuasive Paraphraser on these successful PAP, to refine the jailbreak."
  - [corpus]: Found 25 related papers (using 8). Average neighbor FMR=0.374, average citations=0.0. Top related titles: Retrieval-Augmented Defense: Adaptive and Controllable Jailbreak Prevention for Large Language Models, MetaBreak: Jailbreaking Online LLM Services via Special Token Manipulation, How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States.
- Break condition: This mechanism would break if the iterative refinement process became too predictable or if the LLMs were trained to recognize and resist patterns in iteratively refined prompts.

## Foundational Learning

- Concept: Persuasion techniques and their application in human communication
  - Why needed here: The method relies on understanding and applying various persuasion techniques to generate effective PAP. Without knowledge of these techniques, one cannot effectively use the persuasion taxonomy to jailbreak LLMs.
  - Quick check question: Can you name and briefly describe at least three persuasion techniques from the taxonomy and explain how they might be used to bypass LLM safety guardrails?

- Concept: Taxonomy-based systematic approach to problem-solving
  - Why needed here: The method uses a taxonomy to systematically categorize and apply persuasion techniques. Understanding how to use and apply taxonomies is crucial for effectively implementing this approach.
  - Quick check question: How would you use a taxonomy to categorize and systematically apply different strategies to solve a complex problem?

- Concept: Fine-tuning language models for specific tasks
  - Why needed here: The method involves fine-tuning a Persuasive Paraphraser on successful PAP examples. Understanding the principles and techniques of fine-tuning is essential for effectively implementing this part of the method.
  - Quick check question: What are the key considerations when fine-tuning a language model for a specific task, and how would you approach fine-tuning a model to generate persuasive adversarial prompts?

## Architecture Onboarding

- Component map:
  1. Persuasion Taxonomy: A framework categorizing 40 persuasion techniques into 13 strategies
  2. Persuasive Paraphraser: A fine-tuned language model that generates PAP based on the taxonomy
  3. GPT-4 Judge: An evaluation system that assesses the harmfulness of LLM outputs
  4. Target LLMs: The models being tested for jailbreak vulnerability
  5. Defense Mechanisms: Various strategies to mitigate PAP effectiveness

- Critical path:
  1. Develop and validate the persuasion taxonomy
  2. Train the Persuasive Paraphraser on successful PAP examples
  3. Generate PAP for target LLMs using the fine-tuned paraphraser
  4. Evaluate the effectiveness of PAP using the GPT-4 Judge
  5. Analyze results and iterate on the approach

- Design tradeoffs:
  - Interpretability vs. effectiveness: The taxonomy-based approach prioritizes interpretable PAP over potentially more effective but less interpretable methods
  - Generalization vs. specificity: The method aims to generalize across different LLM architectures while still being effective against specific models
  - Safety vs. utility: The research aims to expose vulnerabilities while minimizing real-world harm

- Failure signatures:
  - Low PAP Success Ratio across multiple risk categories
  - Inability to generate interpretable PAP that maintain harmful intent
  - Failure to jailbreak even basic LLM architectures
  - Overlaps with existing attack methods, reducing the novelty of the approach

- First 3 experiments:
  1. Validate the persuasion taxonomy by manually generating PAP for a small set of harmful queries and testing them against a basic LLM
  2. Fine-tune the Persuasive Paraphraser on a diverse set of successful PAP examples and evaluate its ability to generate effective PAP for new harmful queries
  3. Conduct a broad scan across multiple risk categories and LLM architectures to assess the generalizability of the PAP approach

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the interplay between specific persuasion techniques and different risk categories impact the effectiveness of PAPs?
- Basis in paper: Explicit
- Why unresolved: While the paper identifies trends in susceptibility across risk categories and effectiveness of persuasion techniques, it does not provide a detailed analysis of the underlying mechanisms driving these interactions.
- What evidence would resolve it: A deeper analysis of the linguistic features, contextual factors, and model biases that contribute to the varying effectiveness of PAPs across different risk categories and persuasion techniques.

### Open Question 2
- Question: How do multi-turn persuasive dialogues compare to single-turn PAPs in terms of jailbreak success rates?
- Basis in paper: Inferred
- Why unresolved: The paper primarily focuses on single-turn PAPs and acknowledges the potential for increased effectiveness with multi-turn interactions, but does not empirically test this hypothesis.
- What evidence would resolve it: Experiments comparing the success rates of single-turn PAPs versus multi-turn persuasive dialogues in jailbreaking LLMs across various risk categories.

### Open Question 3
- Question: What are the specific linguistic cues and keywords within PAPs that make them effective in persuading LLMs?
- Basis in paper: Inferred
- Why unresolved: While the paper identifies the importance of persuasive techniques, it does not provide a detailed linguistic analysis of the PAPs themselves to identify the specific features that contribute to their effectiveness.
- What evidence would resolve it: A comprehensive linguistic analysis of successful PAPs to identify common patterns, keywords, and rhetorical devices that make them persuasive to LLMs.

### Open Question 4
- Question: How do different model architectures and training datasets influence the susceptibility of LLMs to PAPs?
- Basis in paper: Inferred
- Why unresolved: The paper tests PAPs on a limited set of models and does not explore how variations in architecture, training data, or fine-tuning approaches might impact susceptibility.
- What evidence would resolve it: Experiments testing PAPs on a wider range of LLM architectures, trained on diverse datasets, and fine-tuned with different safety measures to assess the impact on jailbreak success rates.

### Open Question 5
- Question: What are the ethical implications of using persuasion techniques to jailbreak LLMs, and how can we develop safeguards against malicious use?
- Basis in paper: Explicit
- Why unresolved: While the paper acknowledges the potential for misuse, it does not delve into the ethical considerations or propose specific safeguards against malicious actors using PAPs.
- What evidence would resolve it: A thorough ethical analysis of the use of persuasion techniques in jailbreaking LLMs, along with proposals for safeguards such as detection mechanisms, content moderation, or user education.

## Limitations
- The generalizability of the persuasion taxonomy and PAP approach to other LLM architectures beyond the tested models (Llama-2-7b Chat, GPT-3.5, GPT-4) is uncertain.
- The long-term effectiveness of PAP against evolving LLM safety measures and defenses is unclear.
- The potential for PAP to be detected and blocked by advanced monitoring systems is not thoroughly explored.

## Confidence
- High confidence: The effectiveness of PAP in jailbreaking the tested LLM architectures (Llama-2-7b Chat, GPT-3.5, GPT-4).
- Medium confidence: The generalizability of the persuasion taxonomy and PAP approach to other LLM architectures.
- Low confidence: The long-term effectiveness of PAP against evolving LLM safety measures and defenses.

## Next Checks
1. Evaluate the effectiveness of PAP against a broader range of LLM architectures, including those with more advanced safety measures.
2. Assess the robustness of PAP to various monitoring and detection systems designed to identify and block jailbreak attempts.
3. Investigate the potential for developing more advanced defenses specifically targeting PAP and persuasion-based jailbreak techniques.