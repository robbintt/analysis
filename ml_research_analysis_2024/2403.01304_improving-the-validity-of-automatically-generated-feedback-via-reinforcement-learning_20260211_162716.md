---
ver: rpa2
title: Improving the Validity of Automatically Generated Feedback via Reinforcement
  Learning
arxiv_id: '2403.01304'
source_url: https://arxiv.org/abs/2403.01304
tags:
- feedback
- gpt-4
- answer
- incorrect
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the challenge of automatically generating pedagogically
  valid feedback for incorrect student responses in math. They propose a rubric-based
  evaluation framework and show that GPT-4 can effectively use it to annotate both
  human-written and LLM-generated feedback with high agreement to human annotators.
---

# Improving the Validity of Automatically Generated Feedback via Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2403.01304
- **Source URL**: https://arxiv.org/abs/2403.01304
- **Reference count**: 39
- **Key outcome**: The authors address the challenge of automatically generating pedagogically valid feedback for incorrect student responses in math. They propose a rubric-based evaluation framework and show that GPT-4 can effectively use it to annotate both human-written and LLM-generated feedback with high agreement to human annotators. Using GPT-4's annotations, they construct a preference dataset and employ direct preference optimization (DPO) to fine-tune Llama 2 for feedback generation. Experiments on a math multiple-choice dataset demonstrate that their method significantly improves feedback correctness and alignment compared to baselines, approaching GPT-4's performance with a much smaller model.

## Executive Summary
This paper tackles the challenging problem of generating pedagogically valid feedback for incorrect student responses in mathematics. The authors develop a rubric-based evaluation framework and demonstrate that GPT-4 can effectively use this rubric to annotate both human-written and LLM-generated feedback with high agreement to human annotators. Using GPT-4's annotations, they create a preference dataset and employ direct preference optimization (DPO) to fine-tune Llama 2 for feedback generation. The method significantly improves feedback correctness and alignment compared to baselines, approaching GPT-4's performance while using a much smaller model.

## Method Summary
The authors propose a three-stage approach to improve automatically generated feedback. First, they create an augmented dataset using LLM-generated feedback with varying quality levels. Second, they employ GPT-4 to annotate feedback according to a rubric evaluating correctness, revealing, suggestion, diagnostic, and positive aspects. Third, they use these annotations to construct preference pairs and fine-tune Llama 2 using direct preference optimization. The framework specifically targets middle school-level math multiple-choice questions, using mismatched feedback as hard negatives to improve model discrimination. The approach addresses the challenge of providing pedagogically valid feedback that correctly identifies student errors and offers meaningful guidance.

## Key Results
- GPT-4 achieves 76% agreement with human annotators when evaluating feedback quality using the proposed rubric
- DPO fine-tuning with rubric-based preferences significantly improves feedback correctness and alignment compared to baseline models
- The approach approaches GPT-4's performance while using a much smaller Llama 2 model
- Including mismatched feedback as hard negatives substantially increases the correctness of generated feedback

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 can reliably annotate feedback quality using the rubric
- Mechanism: GPT-4 uses chain-of-thought prompting to answer rubric questions, producing binary labels for correctness, revealing, suggestion, diagnostic, and positive aspects
- Core assumption: GPT-4's judgments correlate with human annotations and capture rubric criteria effectively
- Evidence anchors:
  - [abstract] "GPT-4 is able to effectively use it to annotate human-written and LLM-generated feedback"
  - [section 4.4] "GPT-4 generally agrees with human annotations, with an average accuracy of 76% across labels"
  - [corpus] Weak - only 5 related papers found, none specifically validating GPT-4 rubric annotation
- Break condition: GPT-4's accuracy falls below acceptable threshold, or agreement with human annotators drops significantly

### Mechanism 2
- Claim: Direct Preference Optimization (DPO) aligns Llama 2 outputs with rubric-defined quality
- Mechanism: DPO trains Llama 2 to generate feedback that GPT-4 prefers based on rubric scores, using preference pairs from augmented dataset
- Core assumption: GPT-4's rubric scores provide meaningful preference information for training
- Evidence anchors:
  - [abstract] "we use GPT-4's annotations to create preferences over feedback pairs in an augmented dataset for training via direct preference optimization (DPO)"
  - [section 4.3] "DPO (Score + Mismatch) significantly improves the feedback scores compared to baselines"
  - [corpus] Weak - related papers focus on RL for code/RTL generation, not educational feedback alignment
- Break condition: DPO fails to improve rubric scores beyond baseline, or generates hallucinated or incorrect feedback

### Mechanism 3
- Claim: Including mismatched feedback as hard negatives improves model discrimination
- Mechanism: Feedback written for other incorrect answers in same question serves as challenging negative examples that force model to distinguish subtle differences
- Core assumption: Mismatched feedback shares surface features with correct feedback but lacks target relevance
- Evidence anchors:
  - [section 3.3] "these mismatched feedback are excellent hard negatives since it is hard for algorithms to distinguish between them and good feedback"
  - [section 4.3] "including the mismatched feedback messages substantially increases the correctness of generated feedback"
  - [corpus] Missing - no direct corpus evidence for hard negative effectiveness in educational feedback
- Break condition: Mismatched feedback becomes too dissimilar to provide useful training signal, or model overfits to specific question patterns

## Foundational Learning

- Concept: Rubric-based evaluation of educational feedback
  - Why needed here: Provides structured criteria for assessing feedback quality beyond simple correctness
  - Quick check question: What are the five rubric aspects used to evaluate feedback?

- Concept: Reinforcement learning for text generation
  - Why needed here: Allows training model to optimize for complex, multi-dimensional feedback quality criteria
  - Quick check question: What is the difference between DPO and standard RL approaches?

- Concept: Data augmentation through LLM-generated samples
  - Why needed here: Creates diverse training data including both high and low quality examples
  - Quick check question: How does the augmented dataset improve over using only human-written feedback?

## Architecture Onboarding

- Component map: GPT-4 rubric annotation → Augmented dataset construction → DPO training pipeline → Llama 2 feedback generation
- Critical path: Feedback generation → GPT-4 evaluation → Preference pair creation → DPO model training → Final feedback output
- Design tradeoffs: GPT-4 evaluation provides strong supervision but introduces cost and potential bias vs. human-only evaluation
- Failure signatures: Low rubric scores, high hallucination rates, failure to identify student misconceptions, overly generic feedback
- First 3 experiments:
  1. Evaluate GPT-4 rubric annotation agreement with human annotators on sampled feedback
  2. Compare DPO training with and without mismatched feedback examples
  3. Test feedback generation on held-out questions and evaluate with GPT-4 rubric

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the proposed feedback generation framework be effectively applied to other subjects beyond mathematics, such as programming or language learning?
- Basis in paper: [explicit] The paper states, "We ground our work in math education but note that our framework could potentially be generalized to other subjects, such as programming or language learning."
- Why unresolved: The authors have not conducted experiments or provided evidence to demonstrate the framework's effectiveness in other domains. They only mention the possibility of generalization.
- What evidence would resolve it: Conducting experiments to apply the framework to feedback generation in programming or language learning, and comparing the results to those obtained in the math domain.

### Open Question 2
- Question: How can the framework be extended to provide personalized feedback tailored to individual students' knowledge levels and learning styles?
- Basis in paper: [inferred] The authors mention the need for personalized feedback in the context of open-ended questions, stating, "we can consider tailoring feedback to each student according to their knowledge levels...since student errors can likely be detected from these responses."
- Why unresolved: The paper does not provide any specific methods or experiments for implementing personalized feedback. It only suggests this as a potential direction for future work.
- What evidence would resolve it: Developing and testing methods to adapt the feedback generation process based on individual student profiles, such as their prior knowledge, learning preferences, and error patterns.

### Open Question 3
- Question: Can the evaluation rubric and GPT-4-based annotation method be further improved to better capture the nuances of pedagogically valid feedback?
- Basis in paper: [explicit] The authors acknowledge that GPT-4 can struggle to identify when feedback inaccurately addresses student errors or provides invalid suggestions, and that human annotators have lower agreement on more subjective labels like positivity.
- Why unresolved: While the authors propose a rubric and use GPT-4 for annotation, they recognize limitations in the current approach. They suggest that additional prompt engineering or tools like self-reflection and code execution could help resolve these issues, but do not provide concrete solutions or experimental results.
- What evidence would resolve it: Conducting a thorough analysis of the rubric's effectiveness, gathering feedback from human experts, and experimenting with different prompt engineering techniques or external tools to improve the accuracy and consistency of GPT-4's annotations.

## Limitations

- The study heavily relies on GPT-4 for both feedback evaluation and preference generation, which may introduce systematic bias despite 76% agreement with human annotations
- The dataset size of 1,418 questions and focus on single-step multiple-choice problems limits generalizability to more complex mathematical domains
- The fixed rubric dimensions may not capture all aspects of pedagogically valuable feedback, particularly nuanced instructional qualities

## Confidence

- **High confidence**: The effectiveness of DPO fine-tuning for improving feedback quality compared to baseline models
- **Medium confidence**: GPT-4's ability to reliably evaluate feedback quality across all rubric dimensions
- **Low confidence**: The scalability of this approach to more complex mathematical problems and broader educational contexts

## Next Checks

1. **Human evaluation validation**: Conduct blind human evaluations comparing feedback from the DPO model, GPT-4, and baseline models across all rubric dimensions to verify GPT-4's evaluation reliability and identify potential systematic biases in automated assessment.

2. **Cross-domain generalization test**: Evaluate the fine-tuned model on math problems from different educational levels (e.g., high school algebra, calculus) and problem types (e.g., word problems, proofs) to assess generalizability beyond the middle school multiple-choice domain.

3. **Ablation study on rubric dimensions**: Perform controlled experiments removing individual rubric components (e.g., diagnostic vs. suggestion) to quantify their relative importance in generating pedagogically effective feedback and identify potential overfitting to specific evaluation criteria.