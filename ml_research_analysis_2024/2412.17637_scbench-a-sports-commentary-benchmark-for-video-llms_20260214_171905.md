---
ver: rpa2
title: 'SCBench: A Sports Commentary Benchmark for Video LLMs'
arxiv_id: '2412.17637'
source_url: https://arxiv.org/abs/2412.17637
tags:
- commentary
- video
- sports
- evaluation
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SCBench, a new benchmark for evaluating video
  Large Language Models (LLMs) on sports commentary generation. Current benchmarks
  focus on simple videos and QA tasks, failing to assess models' abilities in generating
  in-depth, temporally coherent, and emotionally expressive commentary.
---

# SCBench: A Sports Commentary Benchmark for Video LLMs

## Quick Facts
- arXiv ID: 2412.17637
- Source URL: https://arxiv.org/abs/2412.17637
- Authors: Kuangzhi Ge; Lingjun Chen; Kevin Zhang; Yulin Luo; Tianyu Shi; Liaoyuan Fan; Xiang Li; Guanqun Wang; Shanghang Zhang
- Reference count: 7
- Primary result: Introduced SCBench benchmark with 5,775 sports video clips for evaluating video LLM commentary generation capabilities

## Executive Summary
SCBench is a new benchmark for evaluating video Large Language Models (LLMs) on sports commentary generation. Current benchmarks focus on simple videos and QA tasks, failing to assess models' abilities in generating in-depth, temporally coherent, and emotionally expressive commentary. To address this, the authors propose CommentarySet, a dataset of 5,775 high-quality sports video clips spanning six sports, each annotated with professional commentary and six-dimensional labels. They also introduce SCORES, a novel evaluation metric, and a GPT-based evaluation method.

## Method Summary
The paper introduces SCBench, a comprehensive benchmark for evaluating video LLMs on sports commentary generation. The authors created CommentarySet, a dataset of 5,775 high-quality sports video clips across six sports, each annotated with professional commentary and six-dimensional labels covering key events, technical details, background information, tactics, match situations, and emotional expression. They developed SCORES, a novel evaluation metric specifically designed for sports commentary quality assessment, and implemented a GPT-based evaluation method to automate scoring. The benchmark includes experiments showing that current state-of-the-art video LLMs, including InternVL-Chat-2, achieve only moderate performance (5.44/10), highlighting the challenge of this task.

## Key Results
- Current video LLMs achieve only 5.44/10 on SCBench, demonstrating the challenge of sports commentary generation
- Fine-tuning methods like Chain-of-Thought improve performance by up to 8%
- Models still significantly lag behind human-level quality in generating in-depth, temporally coherent, and emotionally expressive commentary
- The benchmark successfully reveals limitations of existing models in handling complex visual understanding tasks

## Why This Works (Mechanism)
The benchmark addresses a critical gap in video LLM evaluation by focusing on complex, temporally extended tasks that require deep understanding of sports dynamics, emotional expression, and tactical analysis. By providing professionally annotated commentary with multiple dimensions of evaluation, SCBench forces models to demonstrate capabilities beyond simple visual recognition or short-term reasoning. The combination of human-annotated ground truth, SCORES metric, and GPT-based evaluation creates a comprehensive assessment framework that captures both technical accuracy and qualitative aspects of commentary generation.

## Foundational Learning
- Video LLM evaluation metrics: Why needed - Existing benchmarks focus on simple tasks; quick check - Verify metric captures temporal coherence and emotional expression
- Sports-specific visual understanding: Why needed - Sports require complex temporal and contextual reasoning; quick check - Test model on different sports with varying complexity
- Commentary quality assessment: Why needed - Subjective aspects require multi-dimensional evaluation; quick check - Compare human vs GPT-based scoring consistency

## Architecture Onboarding

Component Map:
CommentarySet (5,775 clips) -> SCORES metric -> GPT-based evaluation -> Model benchmarking

Critical Path:
Video clips → Annotation pipeline → Quality control → SCORES calculation → Model evaluation

Design Tradeoffs:
- Expert annotation vs. scalability: High-quality expert annotations ensure benchmark quality but limit dataset size
- Automated vs. human evaluation: GPT-based evaluation enables scalability but may miss nuanced quality aspects
- Single-sport vs. multi-sport coverage: Including six sports provides broader evaluation but increases annotation complexity

Failure Signatures:
- Low temporal coherence scores indicate models struggle with understanding event sequences
- Poor emotional expression scores suggest models cannot capture commentary sentiment
- Low tactical analysis scores reveal limitations in understanding strategic elements

First Experiments:
1. Run baseline models on single-sport subsets to identify specific sport-related challenges
2. Compare SCORES metric results with human evaluation on sample clips
3. Test Chain-of-Thought prompting across different model architectures

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset creation relies on crowdworkers followed by expert refinement, potentially introducing annotation inconsistencies
- SCORES metric and GPT-based evaluation may not fully capture nuanced aspects of emotional expression and tactical analysis
- Benchmark focuses on six specific sports, limiting generalizability to other sports or commentary styles
- Fine-tuning improvements (up to 8%) may not generalize across different model architectures

## Confidence
- Dataset quality and annotation process: Medium
- SCORES metric effectiveness: Medium
- Overall benchmark validity: High
- Model performance comparisons: High
- Fine-tuning results: Medium

## Next Checks
1. Conduct human evaluation studies with professional sports commentators to validate the SCORES metric and GPT-based evaluation results
2. Test model performance on additional sports and different commentary styles to assess generalizability
3. Compare model-generated commentary against human-generated commentary on the same test set to establish stronger baselines