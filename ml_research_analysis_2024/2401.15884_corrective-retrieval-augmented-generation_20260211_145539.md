---
ver: rpa2
title: Corrective Retrieval Augmented Generation
arxiv_id: '2401.15884'
source_url: https://arxiv.org/abs/2401.15884
tags:
- retrieval
- knowledge
- generation
- retrieved
- documents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the problem of improving the robustness of retrieval-augmented
  generation (RAG) when the retriever returns inaccurate results. The proposed Corrective
  Retrieval Augmented Generation (CRAG) method addresses this by introducing a lightweight
  retrieval evaluator that assesses the quality of retrieved documents and triggers
  different knowledge retrieval actions (Correct, Incorrect, Ambiguous) based on confidence
  scores.
---

# Corrective Retrieval Augmented Generation

## Quick Facts
- arXiv ID: 2401.15884
- Source URL: https://arxiv.org/abs/2401.15884
- Reference count: 25
- This paper proposes CRAG, which improves RAG robustness by 4.4-20.0% accuracy on four datasets through a lightweight retrieval evaluator and web search integration.

## Executive Summary
This paper addresses the critical limitation of retrieval-augmented generation systems when retrievers return inaccurate or irrelevant documents. The proposed Corrective Retrieval Augmented Generation (CRAG) method introduces a lightweight retrieval evaluator that assesses document quality and triggers appropriate knowledge retrieval actions. When retrievals are deemed low-quality, CRAG employs web searches as a complementary knowledge source and uses a decompose-then-recompose algorithm to refine documents. Experiments demonstrate significant performance improvements over standard RAG and state-of-the-art Self-RAG across both short and long-form generation tasks.

## Method Summary
CRAG introduces a multi-stage approach to improve RAG robustness when retrievers return inaccurate results. The system first employs a lightweight retrieval evaluator that assigns confidence scores to retrieved documents, classifying them as Correct, Incorrect, or Ambiguous. Based on these classifications, CRAG triggers different knowledge retrieval actions: for low-quality retrievals, it performs web searches to obtain complementary information, then applies a decompose-then-recompose algorithm to refine the retrieved documents. This approach maintains the efficiency of standard RAG while providing corrective mechanisms when the retriever fails, resulting in more accurate and reliable generated responses across diverse generation tasks.

## Key Results
- CRAG improves standard RAG performance by 4.4-20.0% accuracy across four datasets
- State-of-the-art Self-RAG accuracy improves by 4.0-36.9% with CRAG integration
- Demonstrated effectiveness on both short-form and long-form generation tasks
- Shows adaptability and generalizability across diverse generation scenarios

## Why This Works (Mechanism)
CRAG works by creating a feedback loop between retrieval quality assessment and knowledge acquisition. The lightweight retrieval evaluator provides confidence scores that enable the system to distinguish between reliable and unreliable retrievals. When retrievals are poor, CRAG doesn't simply fail but instead activates compensatory mechanisms through web search integration. The decompose-then-recompose algorithm ensures that even when web searches return noisy information, the system can extract and integrate only the most relevant knowledge. This multi-layered approach addresses the fundamental limitation of traditional RAG systems that assume retrieval quality is guaranteed.

## Foundational Learning
- Retrieval augmented generation fundamentals: Understanding how RAG systems combine retrieval and generation is essential for grasping CRAG's improvements. Quick check: Can you explain how traditional RAG differs from CRAG in handling poor retrievals?
- Confidence scoring mechanisms: The lightweight retrieval evaluator relies on confidence scoring to classify document quality. Quick check: What are the three classification categories (Correct, Incorrect, Ambiguous) and how do they trigger different actions?
- Web search integration strategies: CRAG uses web searches as a fallback mechanism, requiring understanding of how external knowledge sources can complement document retrieval. Quick check: How does CRAG ensure web search results are relevant and not noisy?
- Document refinement techniques: The decompose-then-recompose algorithm is central to CRAG's ability to improve low-quality retrievals. Quick check: What are the key steps in the decomposition and recomposition process?

## Architecture Onboarding
- Component map: Retriever -> Retrieval Evaluator -> Decision Module -> (Generator OR Web Search -> Document Refiner -> Generator)
- Critical path: Document retrieval → Quality assessment → Action selection → Knowledge integration → Text generation
- Design tradeoffs: CRAG trades additional computation (web search, document refinement) for improved accuracy and robustness, maintaining efficiency for high-quality retrievals while providing corrective mechanisms for poor ones
- Failure signatures: Performance degradation when web searches return irrelevant information or when the decompose-then-recompose algorithm fails to extract useful knowledge from noisy sources
- First experiments: 1) Test retrieval evaluator accuracy on benchmark datasets, 2) Evaluate web search integration effectiveness with controlled noise injection, 3) Measure document refinement performance on synthetically corrupted documents

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Evaluation limited to four datasets with controlled conditions, raising questions about real-world deployment robustness
- Performance under noisy web search conditions and dynamic document collections not thoroughly tested
- Decomposition and recomposition algorithm lacks detailed error analysis for edge cases with contradictory web search results

## Confidence
- High confidence: CRAG improves RAG performance by 4.4-20.0% accuracy on tested datasets
- Medium confidence: Generalizability across diverse generation tasks (limited to four specific datasets)
- Medium confidence: Adaptability to truly diverse, real-world knowledge bases (needs validation)

## Next Checks
1. Conduct ablation studies to isolate the contribution of each CRAG component (retrieval evaluator, web search integration, document refinement)
2. Test CRAG's performance under adversarial conditions with deliberate misinformation or highly noisy web search results
3. Evaluate CRAG on continuously updating knowledge bases to measure adaptation to evolving information landscapes without retraining