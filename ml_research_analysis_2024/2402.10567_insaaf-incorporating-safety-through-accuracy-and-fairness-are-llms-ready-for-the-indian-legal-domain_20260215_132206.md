---
ver: rpa2
title: 'InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready
  for the Indian Legal Domain?'
arxiv_id: '2402.10567'
source_url: https://arxiv.org/abs/2402.10567
tags:
- identity
- legal
- llama
- finetuning
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper introduces InSaAF, a framework to evaluate Large Language\
  \ Models (LLMs) for safety and fairness in the Indian legal domain. It constructs\
  \ a synthetic dataset for Binary Statutory Reasoning tasks and proposes the \u03B2\
  -weighted Legal Safety Score (LSS\u03B2) to assess LLMs' performance, balancing\
  \ fairness and accuracy."
---

# InSaAF: Incorporating Safety through Accuracy and Fairness | Are LLMs ready for the Indian Legal Domain?

## Quick Facts
- arXiv ID: 2402.10567
- Source URL: https://arxiv.org/abs/2402.10567
- Reference count: 40
- Introduces InSaAF framework for evaluating LLM safety and fairness in Indian legal domain

## Executive Summary
This paper introduces InSaAF, a framework designed to evaluate Large Language Models (LLMs) for safety and fairness specifically within the Indian legal domain. The authors construct a synthetic dataset for Binary Statutory Reasoning tasks and propose a β-weighted Legal Safety Score (LSSβ) to measure model performance, balancing fairness and accuracy metrics. Through experiments on LLaMA and LLaMA-2 models, they demonstrate that finetuning with specialized legal datasets improves LSSβ scores, suggesting enhanced safety and usability for legal applications. The study emphasizes the potential of targeted finetuning to mitigate bias and improve model reliability in high-stakes legal contexts.

## Method Summary
The InSaAF framework evaluates LLM safety and fairness in the Indian legal domain through a multi-component approach. The methodology begins with synthetic dataset construction for Binary Statutory Reasoning tasks, enabling controlled testing of model behavior across various legal scenarios. The β-weighted Legal Safety Score (LSSβ) is introduced as a composite metric that balances fairness and accuracy considerations. The experimental pipeline involves testing LLaMA and LLaMA-2 models on these tasks, followed by finetuning with specialized legal datasets to assess improvements in safety metrics. The framework systematically measures how finetuning affects both the accuracy of legal reasoning and the fairness of outcomes across different demographic or case characteristics.

## Key Results
- Finetuning with specialized legal datasets significantly improves LSSβ scores for LLaMA and LLaMA-2 models
- InSaAF framework successfully quantifies the trade-off between fairness and accuracy in legal reasoning tasks
- Synthetic dataset generation enables systematic evaluation of bias patterns in legal decision-making

## Why This Works (Mechanism)
The framework works by creating controlled environments where LLM biases can be systematically identified and measured. By using synthetic legal scenarios, the approach isolates specific decision points where fairness issues might arise, allowing for targeted evaluation of model behavior. The β-weighted scoring mechanism captures the essential tension between achieving accurate legal outcomes and ensuring equitable treatment across different case characteristics. Finetuning on specialized legal data appears to recalibrate model parameters to better handle domain-specific nuances while maintaining safety considerations.

## Foundational Learning
- Binary Statutory Reasoning: Understanding how models interpret and apply legal statutes to specific fact patterns - needed to construct evaluation tasks, quick check through case analysis
- Fairness Metrics in Legal AI: Familiarity with demographic parity, equal opportunity, and related fairness concepts - needed to evaluate bias, quick check through metric comparison
- Synthetic Data Generation: Knowledge of controlled dataset creation methods - needed for reproducible evaluation, quick check through generation validation
- Composite Scoring Systems: Understanding of weighted metric combinations - needed to interpret LSSβ, quick check through sensitivity analysis

## Architecture Onboarding

Component Map:
Synthetic Dataset Generation -> Binary Statutory Reasoning Tasks -> β-weighted Legal Safety Score Calculation -> Finetuning Pipeline -> Safety Performance Evaluation

Critical Path:
Dataset creation → Task definition → Score computation → Model training → Performance assessment

Design Tradeoffs:
- Synthetic vs. real data: Controlled evaluation vs. ecological validity
- Binary vs. multi-class reasoning: Simplified measurement vs. real-world complexity
- Fixed β weighting vs. adaptive scoring: Consistent evaluation vs. context-sensitive assessment

Failure Signatures:
- Low accuracy despite high fairness scores indicating overly cautious models
- High accuracy with poor fairness suggesting biased optimization
- Inconsistent LSSβ across similar scenarios revealing scoring instability

First 3 Experiments:
1. Test LSSβ sensitivity to different β values to understand fairness-accuracy trade-offs
2. Evaluate model performance on edge cases not present in training data
3. Compare finetuned vs. baseline models on progressively complex legal reasoning tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Synthetic dataset may not fully capture the complexity of real legal reasoning scenarios
- β-weighted scoring approach may oversimplify the nuanced relationship between fairness and accuracy
- Results are limited to LLaMA and LLaMA-2 architectures, limiting generalizability

## Confidence

**Major Claims and Confidence:**

- **LLM Safety and Fairness Evaluation Framework (High Confidence)**: The methodological approach for constructing evaluation datasets and scoring mechanisms is well-documented and reproducible. The synthetic dataset generation process is transparent, though its representativeness of actual legal scenarios remains uncertain.

- **Finetuning Improves LSSβ Performance (Medium Confidence)**: While results show improvements in LSSβ scores after finetuning, the magnitude of improvement and its practical significance for real-world deployment are unclear. The study does not address whether finetuned models maintain general legal reasoning capabilities or develop narrow task-specific responses.

- **Bias Mitigation Through Finetuning (Medium Confidence)**: The paper suggests finetuning reduces bias, but the analysis of which specific biases are addressed and whether new biases might emerge is limited. The evaluation focuses on binary outcomes rather than continuous or multi-dimensional fairness measures.

## Next Checks
1. Conduct experiments on additional model architectures (GPT, BERT variants, and domain-specific legal models) to assess whether LSSβ improvements are architecture-dependent or generalize across different LLM families.

2. Validate findings using real Indian legal cases rather than synthetic data to test whether observed safety and fairness improvements translate to authentic legal reasoning scenarios with complex factual patterns.

3. Implement longitudinal testing to evaluate whether LSSβ improvements persist across different β values and whether finetuned models maintain consistent safety-performance trade-offs when applied to evolving legal questions and precedents.