---
ver: rpa2
title: On the Robustness of Editing Large Language Models
arxiv_id: '2402.05827'
source_url: https://arxiv.org/abs/2402.05827
tags:
- knowledge
- editing
- language
- edited
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the robustness of large language model (LLM)
  editing methods when applied in realistic communication scenarios. Experiments show
  that edited models frequently exhibit confusion and hallucination when interacting
  with related knowledge, and their performance significantly declines when faced
  with rephrased prompts involving complex contexts or different question formats.
---

# On the Robustness of Editing Large Language Models
## Quick Facts
- arXiv ID: 2402.05827
- Source URL: https://arxiv.org/abs/2402.05827
- Authors: Xinbei Ma; Tianjie Ju; Jiyang Qiu; Zhuosheng Zhang; Hai Zhao; Lifeng Liu; Yulong Wang
- Reference count: 40
- Primary result: Edited LLMs frequently exhibit confusion and hallucination when interacting with related knowledge, and their performance significantly declines with rephrased prompts involving complex contexts or different question formats.

## Executive Summary
This paper evaluates the robustness of large language model editing methods when applied in realistic communication scenarios. Through systematic experiments, the authors demonstrate that edited models struggle with related knowledge during multi-turn dialogues and fail to generalize edited knowledge to rephrased prompts with complex contexts. The study reveals a fundamental gap between current editing methods and practical LLM applications, showing that more popular knowledge (measured by frequency, connection, and co-occurrence) is particularly resistant to editing and more vulnerable to rephrasing attacks.

## Method Summary
The study evaluates four editing methods (ROME, MEMIT, SERAC, IKE) on Llama-2-7B-chat and Llama-13B models using benchmarks like CounterFact, zsRE, and MQUAKE-T. The authors design attacking prompts that include lengthy contexts (related, noisy, simulated dialogue) and different question formats (cloze, reference resolution). They measure knowledge popularity through frequency, connection, and co-occurrence metrics, and conduct human evaluations to assess confusion and hallucination. The experiments compare editing success rates across direct and rephrased prompts, and analyze the correlation between knowledge popularity and editing robustness.

## Key Results
- Edited models exhibit confusion and hallucination in 38% of samples during multi-turn dialogues with related knowledge
- Rephrased prompts with complex contexts and different formats cause significant drops in editing performance
- More popular knowledge (higher frequency, connection, co-occurrence) is harder to edit effectively and more vulnerable to rephrasing attacks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Edited LLMs suffer from confusion and hallucination when interacting with related knowledge in realistic multi-turn dialogues.
- **Mechanism:** Editing a knowledge triplet (s, r, o → o′) alters the model's response to the direct prompt, but does not erase the original memory. During dialogue, related neighbor knowledge (k2) can trigger the original knowledge (o), causing the model to revert to the original answer and generate contradictory statements.
- **Core assumption:** The parametric knowledge in LLMs is stored in an interconnected manner, not as independent isolated facts.
- **Evidence anchors:**
  - [abstract] "Experiments show that edited models frequently exhibit confusion and hallucination when interacting with related knowledge"
  - [section 4.3] "38% samples revert to the original answer o during the dialogue"
- **Break condition:** If the editing method could completely erase the original knowledge and create an isolated memory for the new knowledge.

### Mechanism 2
- **Claim:** Rephrased prompts with complex contexts and different question formats significantly reduce editing performance.
- **Mechanism:** When prompts are rephrased to include lengthy contexts (related, noisy, simulated dialogue) or changed to cloze format or reference resolution, the model's ability to recall the edited knowledge (o′) is compromised. The complex inputs can trigger retrieval of the original knowledge (o) or generate hallucinated content.
- **Core assumption:** LLMs rely on the exact prompt structure for knowledge retrieval, and complex inputs can bypass the editing scope.
- **Evidence anchors:**
  - [abstract] "their performance significantly declines when faced with rephrased prompts involving complex contexts or different question formats"
  - [section 5.3] "The success on direct prompts keeps high scores and gentle decreases on the three measurements. Much more significant drops appear on the rephrased prompts"
- **Break condition:** If the editing method could generalize the edited knowledge to all semantically equivalent expressions.

### Mechanism 3
- **Claim:** More popular knowledge (higher frequency, connection, co-occurrence) is harder to edit effectively due to stronger parametric memory.
- **Mechanism:** Knowledge that is frequently visited (high frequency), has many connections in the knowledge graph (high connection), or co-occurs with other entities (high co-occurrence) has stronger parametric memory in the LLM. This makes it more resistant to editing, as the original knowledge is more deeply embedded.
- **Core assumption:** The strength of parametric memory is proportional to the popularity of the knowledge.
- **Evidence anchors:**
  - [abstract] "Further analysis shows that more popular knowledge—measured by frequency, connection, and co-occurrence—is better memorized, easier to recall, and harder to edit effectively"
  - [section 6.2] "Editing more popular knowledge is more vulnerable to rephrasing"
- **Break condition:** If the editing method could overcome the strength of parametric memory regardless of knowledge popularity.

## Foundational Learning

- **Concept:** Parametric knowledge storage in LLMs
  - **Why needed here:** Understanding how LLMs store knowledge is crucial for understanding why editing is difficult and why edited knowledge can be vulnerable.
  - **Quick check question:** How does the parametric knowledge storage mechanism in LLMs contribute to the vulnerability of edited knowledge?

- **Concept:** Knowledge graph and entity relationships
  - **Why needed here:** The concept of knowledge graphs and entity relationships is important for understanding the measurement of knowledge popularity (connection and co-occurrence) and how related knowledge can trigger confusion in edited LLMs.
  - **Quick check question:** How do the connections and co-occurrences of entities in a knowledge graph affect the editing robustness of LLMs?

- **Concept:** Prompt engineering and context influence
  - **Why needed here:** Understanding how different prompt formats and contexts influence the model's response is crucial for designing effective attacking prompts and evaluating the robustness of editing methods.
  - **Quick check question:** How do complex contexts and different question formats affect the model's ability to recall edited knowledge?

## Architecture Onboarding

- **Component map:** Llama-2-7B/13B model -> Editing methods (ROME, MEMIT, SERAC, IKE) -> Evaluation pipeline -> Robustness testing
- **Critical path:** Edit base LLM -> Generate attacking prompts with complex contexts -> Evaluate performance on rephrased prompts -> Analyze vulnerabilities
- **Design tradeoffs:** Balancing editing success rate with robustness to various inputs - methods with high direct prompt success may be more vulnerable to complex inputs
- **Failure signatures:** Confusion and hallucination in multi-turn dialogues, significant performance drops on rephrased prompts, reversion to original knowledge, difficulty editing popular knowledge
- **First 3 experiments:**
  1. Test edited model's performance on direct prompts to establish baseline editing success rate
  2. Generate attacking prompts with complex contexts (related, noisy, simulated dialogue) and different formats (cloze, reference resolution) to evaluate robustness
  3. Measure knowledge popularity of edited facts and analyze correlation with editing robustness

## Open Questions the Paper Calls Out
The paper identifies several open questions, including: what are the fundamental mechanisms causing edited models to revert to original knowledge when interacting with related knowledge; how different knowledge popularity metrics interact to affect editing robustness and whether there are threshold effects; and what is the optimal balance between context length and editing robustness across different editing methods.

## Limitations
- The analysis of confusion and hallucination relies heavily on the CounterFact benchmark, which may not represent the full diversity of real-world conversational scenarios
- The correlation between knowledge popularity and editing difficulty, while statistically significant, may not capture all relevant factors influencing editing success
- The paper does not explore potential interactions between different popularity metrics or identify critical threshold values where editing becomes significantly more difficult

## Confidence
- High confidence: The observation that edited LLMs struggle with rephrased prompts and complex contexts is well-supported by experimental results across multiple benchmarks and editing methods
- Medium confidence: The mechanism explaining confusion through interconnected knowledge storage is plausible but requires further validation with alternative experimental designs
- Medium confidence: The relationship between knowledge popularity and editing difficulty is statistically significant but may not be causal in all cases

## Next Checks
1. Cross-benchmark validation: Replicate robustness experiments on additional editing benchmarks beyond CounterFact and zsRE to verify pattern consistency across different knowledge domains
2. Controlled knowledge isolation test: Design experiment isolating specific knowledge triplets to test whether observed confusion stems from interconnected memory storage
3. Human evaluation expansion: Conduct comprehensive human evaluation with diverse annotators to assess severity and frequency of confusion and hallucination across different dialogue scenarios