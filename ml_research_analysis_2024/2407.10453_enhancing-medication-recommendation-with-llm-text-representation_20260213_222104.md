---
ver: rpa2
title: Enhancing Medication Recommendation with LLM Text Representation
arxiv_id: '2407.10453'
source_url: https://arxiv.org/abs/2407.10453
tags:
- representation
- text
- medical
- medication
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of medication recommendation
  using only structured medical codes, leaving unstructured data like clinical notes
  underutilized. To enhance performance, it proposes a method that extracts text representations
  from large language models (LLMs) applied to clinical notes, then combines these
  with medical code embeddings.
---

# Enhancing Medication Recommendation with LLM Text Representation

## Quick Facts
- arXiv ID: 2407.10453
- Source URL: https://arxiv.org/abs/2407.10453
- Reference count: 0
- Primary result: LLM text representation combined with medical code embeddings improves medication recommendation performance across seven base models

## Executive Summary
This paper addresses the challenge of medication recommendation by proposing a method that leverages both structured medical codes and unstructured clinical notes. The approach extracts text representations from LLM hidden states (specifically layer 16 of Mistral-7B-Instruct) and combines them with medical code embeddings. Tested across seven existing medication recommendation models on MIMIC-III and CYCH datasets, the method shows that LLM text representation alone achieves comparable performance to medical code representation, and the combined representation notably improves the G-BERT model's performance. This demonstrates effective utilization of both structured and unstructured medical data for medication recommendations.

## Method Summary
The method extracts text representations from clinical notes by chunking them, passing through Mistral-7B-Instruct, and averaging the hidden states from layer 16. These text embeddings are then combined with medical code embeddings (using either addition or concatenation based on each base model's architecture) and fed into seven existing medication recommendation models (G-BERT, GAMENet, SafeDrug, COGNet, GSVEMed, SHAPE, StratMed). The combined representations are evaluated on MIMIC-III and CYCH datasets using Jaccard Similarity, F1-Score, PR-AUC, and DDI Rate metrics.

## Key Results
- LLM text representation alone achieves comparable performance to medical code representation in several base models
- Combined text and code representation notably improves G-BERT model performance on both MIMIC-III and CYCH datasets
- The method demonstrates effective utilization of both structured and unstructured medical data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM hidden states from layer 16 capture useful medication recommendation signals
- Mechanism: LLM is used as feature extractorâ€”clinical notes are chunked, passed through Mistral-7B-Instruct, and hidden states from layer 16 are averaged to produce dense text representation
- Core assumption: Middle layers of LLMs retain meaningful semantic and domain-specific knowledge even without fine-tuning
- Evidence anchors: "LLM text representation alone can even demonstrate a comparable ability to the medical code representation alone"

### Mechanism 2
- Claim: Combining text and code representations improves model performance over code-only inputs
- Mechanism: After generating text embeddings via LLM, they are either added or concatenated to medical code embeddings
- Core assumption: Structured medical codes and unstructured clinical notes contain complementary information
- Evidence anchors: "This method can be applied to several existing base models we selected and improve medication recommendation performance"

### Mechanism 3
- Claim: LLM text representation alone can match or exceed medical code-only performance in certain models
- Mechanism: By feeding only LLM-extracted text embeddings into base models (without medical codes), the system still achieves reasonable prediction performance
- Core assumption: Clinical notes contain sufficient diagnostic and treatment information to infer medication needs
- Evidence anchors: "LLM text representation alone can even demonstrate a comparable ability to the medical code representation alone"

## Foundational Learning

- Concept: Large Language Model (LLM) hidden states as feature extractors
  - Why needed here: The study uses LLM hidden states to transform unstructured clinical notes into dense embeddings that can be combined with structured medical code embeddings
  - Quick check question: What part of the LLM output is used for text representation, and why is averaging chosen over other pooling methods?

- Concept: Graph Neural Networks (GNNs) for medical code embedding
  - Why needed here: Many base models (e.g., GSVEMed, G-BERT) use GNNs to learn relationships among medical codes and visits
  - Quick check question: How does the adjacency matrix in GNN-based models influence the aggregation of visit representations?

- Concept: Multi-label classification with combined loss functions
  - Why needed here: Medication recommendation is a multi-label task; understanding binary cross-entropy and multi-label margin loss is necessary to evaluate combined representations
  - Quick check question: What is the role of the weighting parameter ð›¼ in the combined loss function, and how does it affect model training?

## Architecture Onboarding

- Component map: Clinical notes -> Mistral-7B-Instruct -> Hidden states (layer 16) -> Average pooling -> Text embeddings -> [Addition/Concatenation] -> Medical code embeddings -> Base models -> Medication predictions

- Critical path:
  1. Chunk clinical notes â†’ pass through LLM â†’ extract hidden states
  2. Average hidden states â†’ reduce dimension (if needed)
  3. Combine with medical code embeddings per base model design
  4. Feed combined representation into base model's encoder/decoder
  5. Predict medications and evaluate with Jaccard, F1, PR-AUC

- Design tradeoffs:
  - Using middle-layer hidden states vs. final-layer states: Middle layers preserve more general semantic features; final layers may be more task-specific but less generalizable
  - Averaging vs. more complex pooling: Averaging is computationally efficient but may lose local context
  - Addition vs. concatenation for fusion: Addition keeps dimensionality low; concatenation may capture richer interactions but increases model complexity

- Failure signatures:
  - Performance degrades when using only text representation â†’ LLM hidden states lack domain specificity
  - No improvement when combining text and codes â†’ base model cannot effectively process heterogeneous embeddings
  - High DDI rates in SafeDrug/StratMed â†’ DDI constraint conflicts with text-derived predictions

- First 3 experiments:
  1. Evaluate base models using only medical codes (C) as a performance baseline
  2. Evaluate base models using only LLM text representation (T) to test standalone effectiveness
  3. Evaluate base models using combined text and code representation (C+T) to measure improvement

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLM text representation compare when using different layers of the LLM's hidden states for extraction?
- Basis in paper: The paper mentions that the 16th layer was chosen based on previous studies but does not explore other layers or compare their performance
- Why unresolved: The paper only tests one specific layer (16th) and does not provide a systematic comparison of different layers' effectiveness for medication recommendation
- What evidence would resolve it: A comprehensive ablation study comparing the performance of text representations extracted from multiple layers of the LLM, including both early, middle, and late layers, across all base models and datasets

### Open Question 2
- Question: What is the impact of chunk size on the quality of text representation extraction and subsequent medication recommendation performance?
- Basis in paper: The paper mentions that clinical notes are split into chunks to fit within the LLM's context window, but it does not explore how different chunk sizes affect the final performance
- Why unresolved: The paper uses a fixed chunking approach without investigating the optimal chunk size or the trade-off between chunk size and context preservation
- What evidence would resolve it: Experiments varying the chunk size parameter across a range of values and measuring the resulting medication recommendation performance, potentially including metrics on context retention and information loss

### Open Question 3
- Question: How does the proposed method perform on datasets with longer clinical notes or different types of unstructured data beyond discharge summaries?
- Basis in paper: The paper uses MIMIC-III and CYCH datasets, which have specific note lengths and structures, but it does not test the method's generalizability to other types of clinical notes or longer documents
- Why unresolved: The paper does not explore the method's robustness to variations in note length, structure, or content type (e.g., admission notes, progress notes, or radiology reports)
- What evidence would resolve it: Testing the method on additional datasets with varying note lengths, structures, and content types, and comparing performance across these different scenarios

## Limitations
- The study relies on averaging middle-layer LLM hidden states without demonstrating why layer 16 specifically captures medication-relevant signals better than other layers
- The averaging method may lose important sequential information from chunked clinical notes
- The paper does not provide detailed implementation specifications for how text representations are combined with each base model's architecture

## Confidence
- High confidence: The general finding that combining text and code representations improves medication recommendation performance is well-supported by experimental results across multiple models and datasets
- Medium confidence: The claim that LLM text representation alone can match medical code representation is supported but may depend heavily on the specific base model architecture and dataset characteristics
- Medium confidence: The mechanism by which middle-layer hidden states capture useful signals is theoretically sound but lacks direct experimental validation for the specific layer choice

## Next Checks
1. Systematically test text representation performance using different LLM hidden layers (e.g., layers 8, 12, 16, 20) to identify optimal layer selection and validate the claim about layer 16's effectiveness

2. Compare averaging against other pooling techniques (max pooling, attention-based pooling) for aggregating chunked text representations to determine if the chosen method is optimal

3. Evaluate model performance using synthetic clinical notes with varying levels of medical terminology and coherence to determine how sensitive the LLM text representation is to note quality and whether the observed improvements hold across different note quality scenarios