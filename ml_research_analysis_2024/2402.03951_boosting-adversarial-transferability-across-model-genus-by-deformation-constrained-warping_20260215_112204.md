---
ver: rpa2
title: Boosting Adversarial Transferability across Model Genus by Deformation-Constrained
  Warping
arxiv_id: '2402.03951'
source_url: https://arxiv.org/abs/2402.03951
tags: []
core_contribution: This paper tackles the challenge of creating adversarial examples
  that can transfer effectively across different model genera (e.g., CNNs and Transformers),
  which has not been well-explored in prior work. The proposed Deformation-Constrained
  Warping Attack (DeCoWA) introduces a novel input transformation called Deformation-Constrained
  Warping (DeCoW) that applies elastic deformation to input data while constraining
  the magnitude and direction of warping to preserve global semantics.
---

# Boosting Adversarial Transferability across Model Genus by Deformation-Constrained Warping

## Quick Facts
- arXiv ID: 2402.03951
- Source URL: https://arxiv.org/abs/2402.03951
- Reference count: 19
- One-line primary result: DeCoWA achieves significant improvements in cross-model genus adversarial transferability using elastic deformation with semantic constraints

## Executive Summary
This paper addresses the challenge of creating adversarial examples that can transfer effectively across different model genera, particularly between CNNs and Transformers. The proposed Deformation-Constrained Warping Attack (DeCoWA) introduces a novel input transformation technique that applies elastic deformation while preserving global semantics. By constraining the warping strength and direction, DeCoWA increases local detail diversity in augmented inputs, encouraging surrogate models to rely on more invariant features. Experiments demonstrate substantial improvements in transferability across image classification, video recognition, and audio recognition tasks.

## Method Summary
DeCoWA is built on MI-FGSM as the base attack method, enhanced with a novel input augmentation technique called Deformation-Constrained Warping (DeCoW). The method applies elastic deformation to input data using thin-plate spline interpolation while constraining the warping strength and direction through an adaptive control strategy. Multiple transformations (N=15) are applied to create diverse augmented inputs, which are used to compute adversarial gradients. The approach maintains semantic preservation while increasing local feature diversity, improving gradient diversity and thus transferability across different model architectures.

## Key Results
- Achieves classification accuracy reductions of over 14.65% on average compared to previous best methods when attacking ViT models using CNN surrogates
- Shows strong performance improvements across image classification, video recognition, and audio recognition tasks
- Demonstrates effective cross-model genus transferability while maintaining performance in homologous model genus attacks

## Why This Works (Mechanism)

### Mechanism 1
Elastic deformation preserves global semantics while increasing local detail diversity. DeCoW applies elastic deformation with constraints on warping strength and direction to avoid severe distortion of global semantics, increasing local feature diversity which is crucial for cross-model genus transferability.

### Mechanism 2
Constrained deformation encourages surrogate models to rely on more invariant features. By forcing models to learn features that are robust to the controlled deformations, the approach reduces overfitting to specific local patterns that may not generalize across model architectures.

### Mechanism 3
Multi-transform augmentation enhances gradient diversity. Applying multiple warping transformations creates diverse augmented inputs, which increases the diversity of gradients and improves transferability by making the adversarial examples more robust across different model architectures.

## Foundational Learning

- Concept: Adversarial transferability
  - Why needed here: Understanding how adversarial examples generated on one model can fool other models is fundamental to this research
  - Quick check question: What is the difference between white-box and black-box adversarial attacks?

- Concept: Input transformation for transferability
  - Why needed here: The core technique relies on input transformations to improve transferability across different model architectures
  - Quick check question: How do input transformations typically improve adversarial transferability?

- Concept: Elastic deformation and thin-plate splines
  - Why needed here: The core technique relies on elastic deformation using thin-plate spline interpolation
  - Quick check question: How does thin-plate spline interpolation differ from other interpolation methods?

## Architecture Onboarding

- Component map: Input → DeCoW transformation → Adaptive control optimization → Multiple transformation aggregation → Attack loss calculation
- Critical path: Input → DeCoW transformation → Adaptive control optimization → Multiple transformation aggregation → Attack loss calculation
- Design tradeoffs:
  - Deformation strength vs. semantic preservation
  - Number of transformations vs. computational cost
  - Adaptive control learning rate vs. convergence stability
- Failure signatures:
  - Excessive deformation causing semantic loss
  - Insufficient gradient diversity
  - Poor convergence of adaptive control
- First 3 experiments:
  1. Test basic DeCoW transformation on a simple CNN model to verify local detail enhancement
  2. Implement adaptive control strategy and verify semantic preservation
  3. Combine with MI-FGSM and test cross-model genus transferability on image classification task

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of DeCoWA compare to other input transformation methods when attacking models trained on different data distributions? The paper mentions future work on cross-data distribution attacks but provides no experimental results for this scenario.

### Open Question 2
What is the optimal number of control points and learning rate for the Deformation-Constrained Warping method? The paper sets specific values (N=15, M=9, β=0.02) but doesn't explore sensitivity to these hyperparameters.

### Open Question 3
How does DeCoWA perform against more diverse model architectures beyond CNNs and Vision Transformers? The paper focuses specifically on CNN vs. ViT transferability without exploring other architectures like RNNs or GNNs.

## Limitations

- The computational overhead of multiple transformations (N=15) may limit practical deployment in resource-constrained scenarios
- Performance on audio and video tasks is based on fewer experiments compared to the extensive image classification evaluation
- The paper lacks ablation studies on deformation constraint strength, making it difficult to determine the optimal balance between local detail enhancement and semantic preservation

## Confidence

- High Confidence: The basic premise that cross-model genus transferability is an important and under-explored problem; experimental methodology using standard benchmarks; general improvement over baseline methods
- Medium Confidence: The specific mechanism of Deformation-Constrained Warping improving transferability through local detail diversity; the adaptive control strategy's effectiveness; the claimed superiority over state-of-the-art methods
- Low Confidence: The scalability of the approach to extremely large models; the computational efficiency for real-world deployment; the robustness of results across diverse datasets beyond the tested benchmarks

## Next Checks

1. **Ablation Study on Deformation Constraints**: Systematically vary the deformation constraint strength (β) and measure its impact on both semantic preservation (via attention maps) and transferability performance to identify the optimal balance.

2. **Cross-Domain Generalization Test**: Evaluate DeCoWA's performance when the surrogate and target models are trained on completely different datasets (e.g., using a model trained on CIFAR-10 to attack a model trained on ImageNet) to assess robustness beyond in-domain scenarios.

3. **Computational Overhead Analysis**: Measure the wall-clock time and memory usage of DeCoWA compared to baseline methods across different hardware configurations to quantify the practical deployment cost of the multiple transformation approach.