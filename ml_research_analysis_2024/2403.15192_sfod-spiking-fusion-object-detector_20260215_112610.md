---
ver: rpa2
title: 'SFOD: Spiking Fusion Object Detector'
arxiv_id: '2403.15192'
source_url: https://arxiv.org/abs/2403.15192
tags:
- spiking
- detection
- snns
- rate
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces SFOD (Spiking Fusion Object Detector), the
  first SNN-based object detection model for event cameras that incorporates multi-scale
  feature fusion. The authors address the challenge of processing sparse, asynchronous
  event data by proposing a Spiking Fusion Module that fuses feature maps from different
  scales in both spatial and temporal domains.
---

# SFOD: Spiking Fusion Object Detector

## Quick Facts
- arXiv ID: 2403.15192
- Source URL: https://arxiv.org/abs/2403.15192
- Reference count: 40
- Primary result: SFOD achieves 93.7% accuracy on NCAR classification and 32.1% mAP on GEN1 object detection

## Executive Summary
This paper introduces SFOD, the first Spiking Neural Network (SNN) model for event camera-based object detection that incorporates multi-scale feature fusion. The authors address the challenges of processing sparse, asynchronous event data by proposing a Spiking Fusion Module that fuses feature maps from different scales in both spatial and temporal domains. Their comprehensive analysis reveals that Spiking Rate Decoding combined with MSE loss yields optimal performance for SNNs on event camera data. On the NCAR classification dataset, SFOD achieves state-of-the-art accuracy of 93.7%, while on the GEN1 object detection dataset, it reaches an mAP of 32.1%, nearly doubling the performance of previous SNN-based approaches.

## Method Summary
SFOD processes event camera data encoded as voxel cubes (5 time bins, 2 micro time bins, 4 channels) through a Spiking DenseNet backbone with PLIF neurons. The key innovation is the Spiking Fusion Module, which fuses multi-scale feature maps using transposed convolution for upsampling, concatenation for combining features, and SEW Res Blocks for refinement. For decoding, SFOD employs Spiking Rate Decoding with MSE loss for classification and Focal Loss for detection. The model is trained using AdamW optimizer with cosine learning rate scheduling on NCAR (30 epochs) and GEN1 (50 epochs) datasets.

## Key Results
- Achieves 93.7% classification accuracy on NCAR dataset
- Reaches 32.1% mAP on GEN1 object detection dataset
- Nearly doubles the performance of previous SNN-based approaches
- Maintains comparable model size and firing rate (14.70%) to existing methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Spiking Fusion Module effectively addresses the sparsity and asynchrony of event data by fusing multi-scale feature maps in both spatial and temporal domains.
- Mechanism: The module upsamples lower-resolution feature maps using transposed convolution, concatenates them with higher-resolution maps, and then refines the fused features through a Spiking Pyramid Extraction Submodule (SPES) that uses SEW Res Blocks.
- Core assumption: Transposed convolution preserves the binary nature of SNNs better than bilinear interpolation while maintaining effective spatial upsampling.
- Evidence anchors: [abstract] "design a Spiking Fusion Module, achieving the first-time fusion of feature maps from different scales in SNNs applied to event cameras"; [section] "we opt for transposed convolution... for image upsampling. This approach not only preserves the characteristics of SNNs but also exhibits adaptability to spiking data"

### Mechanism 2
- Claim: Spiking Rate Decoding with MSE loss provides superior performance compared to Spiking Count Decoding with CE loss for SNNs on event camera data.
- Mechanism: Spiking Rate Decoding normalizes spike counts by time, producing outputs in [0,1] that match prediction ranges. MSE directly computes loss from these normalized values without requiring softmax transformation.
- Core assumption: The normalized output range of Spiking Rate Decoding aligns better with prediction targets than the [0,T] range of Spiking Count Decoding.
- Evidence anchors: [abstract] "Spiking Rate Decoding combined with MSE loss yields optimal performance"; [section] "we achieve state-of-the-art classification results based on SNNs, achieving 93.7% accuracy on the NCAR dataset" using this combination

### Mechanism 3
- Claim: SEW Res Blocks in SPES improve multi-scale feature extraction by introducing identity mapping that stabilizes gradients while maintaining SNN characteristics.
- Mechanism: SEW Res Blocks provide residual connections that help gradients flow through the network, while their spiking-aware design maintains the binary computation nature of SNNs.
- Core assumption: The limited number of SEW Res Block layers (six 3x3 convolutions) keeps additional non-spiking computations minimal while providing sufficient gradient stabilization.
- Evidence anchors: [section] "the identity mapping introduced by SEW Res Block can notably elevate model performance" and "rarely occur in our experiments"; [section] "compared to the SFOD-B, the SFOD-R... significantly improves the mAP by 2.2 points"

## Foundational Learning

- Concept: Event camera data encoding and voxel cube representation
  - Why needed here: SFOD processes event camera data, which requires understanding how events are converted to a format suitable for SNN processing
  - Quick check question: How does the voxel cube representation handle the temporal dimension of event data, and what parameters control its resolution?

- Concept: Spiking neuron models (PLIF) and membrane potential dynamics
  - Why needed here: The backbone uses PLIF neurons, and understanding their behavior is crucial for implementing and debugging the network
  - Quick check question: What is the role of the membrane time constant τ in PLIF neurons, and how does it affect spike generation?

- Concept: Object detection metrics (mAP, AP@0.5:0.95, AP@0.5)
  - Why needed here: SFOD's performance is evaluated using these metrics, requiring understanding of what they measure and how they're computed
  - Quick check question: How does the IoU threshold affect mAP calculation, and why might different thresholds be used for different evaluation scenarios?

## Architecture Onboarding

- Component map: Event data → Voxel cube → Backbone feature extraction → Spiking Fusion Module → SSD detection head → Output predictions

- Critical path: Event data → Voxel cube → Backbone feature extraction → Spiking Fusion Module → SSD detection head → Output predictions

- Design tradeoffs:
  - Spatial resolution vs. temporal information: Using voxel cubes with micro time bins balances these competing needs
  - Fusion strategy: Concatenation preserves binary nature vs. element-wise summation that might disrupt it
  - Backbone depth: Deeper networks capture more context but increase firing rate and complexity

- Failure signatures:
  - Low mAP with high firing rate: May indicate inefficient feature fusion or poor decoding strategy
  - High mAP but very high firing rate: Backbone or fusion module may be too complex
  - Training instability: Could result from inappropriate loss function choice or gradient issues in SEW Res Blocks

- First 3 experiments:
  1. Verify voxel cube encoding works correctly by visualizing the 3D representation of simple event sequences
  2. Test Spiking Fusion Module independently by feeding pre-computed feature maps and checking if upsampling and concatenation preserve spatial relationships
  3. Evaluate different decoding strategies (Count vs Rate) with MSE loss on a small subset of NCAR to confirm the claimed performance difference

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions, but several areas remain unexplored:

### Open Question 1
- Question: How does the Spiking Fusion Module's performance scale with more complex backbone architectures beyond DenseNet121-24?
- Basis in paper: [inferred] The paper evaluates different backbone architectures but focuses primarily on DenseNet variants. It states "when employing a consistent growth rate, an increase in network depth leads to a decline in mAP" but doesn't explore more complex architectures like ResNets or EfficientNets.
- Why unresolved: The paper limits its backbone architecture exploration to DenseNet variants, leaving open whether the Spiking Fusion Module would benefit from or be compatible with other state-of-the-art backbone designs.
- What evidence would resolve it: Direct experimental comparison of SFOD with Spiking Fusion Module using various backbone architectures (e.g., ResNet, EfficientNet, MobileNet) on the GEN1 dataset, measuring mAP, parameters, and firing rate.

### Open Question 2
- Question: What is the optimal number of micro time bins (n) for the voxel cube representation, and how does this hyperparameter affect detection performance?
- Basis in paper: [explicit] The paper mentions "we code all samples with voxel cube, using a T-value of 5 and a micro time bin of 2" but doesn't explore the sensitivity of performance to this hyperparameter.
- Why unresolved: The voxel cube temporal discretization is a critical design choice that could significantly impact the model's ability to capture temporal information in event data, but the paper treats it as a fixed parameter without exploring its effects.
- What evidence would resolve it: Systematic ablation studies varying the number of micro time bins (n) from 1 to 8 while keeping other parameters constant, measuring resulting mAP, accuracy, and firing rate on both NCAR and GEN1 datasets.

### Open Question 3
- Question: How does the SFOD's performance compare to transformer-based approaches that incorporate temporal information directly, such as RVT [15]?
- Basis in paper: [explicit] The paper compares SFOD to RVT but notes "our model not only has fewer parameters but also demonstrates significant advantages in both energy consumption and computation speed" without directly comparing detection accuracy on the same metrics.
- Why unresolved: While the paper provides comparative metrics, it doesn't present a head-to-head accuracy comparison under identical evaluation conditions, making it unclear whether SFOD's advantages in efficiency come at the cost of detection performance.
- What evidence would resolve it: Direct experimental comparison of SFOD and RVT on the GEN1 dataset using identical evaluation protocols, reporting mAP@0.5:0.95, mAP@0.5, inference time, energy consumption, and parameter count.

## Limitations

- Architecture details of Spiking DenseNet and Spiking Fusion Module are incompletely specified
- Performance claims rely on specific dataset processing choices that may not generalize
- The claim of "first" for multi-scale fusion in SNNs is limited to event camera object detection context

## Confidence

- **High**: Spiking Rate Decoding + MSE loss performance benefit (supported by empirical results and intuitive explanation)
- **Medium**: Multi-scale fusion module effectiveness (mechanism is plausible but architecture details are sparse)
- **Medium**: mAP improvement over previous SNN methods (direct comparison provided but implementation details uncertain)

## Next Checks

1. Implement the voxel cube encoding and verify that event temporal information is preserved correctly across the 5 time bins and 2 micro time bins
2. Test the Spiking Fusion Module independently with synthetic feature maps to confirm that transposed convolution and concatenation preserve spatial relationships and that SEW Res Blocks improve performance as claimed
3. Validate the Spiking Rate Decoding implementation by comparing Count vs Rate decoding on NCAR classification, ensuring the normalized [0,1] output range matches predictions correctly