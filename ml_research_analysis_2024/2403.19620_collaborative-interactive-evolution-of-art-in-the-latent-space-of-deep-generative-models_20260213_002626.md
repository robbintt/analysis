---
ver: rpa2
title: Collaborative Interactive Evolution of Art in the Latent Space of Deep Generative
  Models
arxiv_id: '2403.19620'
source_url: https://arxiv.org/abs/2403.19620
tags:
- images
- evolution
- interactive
- collaborative
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores the use of evolutionary algorithms to navigate
  the latent space of Creative Adversarial Networks (CANs) for generating art images.
  The approach combines automatic aesthetic evaluation using Neural Image Assessment
  (NIMA) and collaborative interactive human evaluation to guide the evolution process.
---

# Collaborative Interactive Evolution of Art in the Latent Space of Deep Generative Models

## Quick Facts
- arXiv ID: 2403.19620
- Source URL: https://arxiv.org/abs/2403.19620
- Authors: Ole Hall; Anil Yaman
- Reference count: 40
- Primary result: Evolutionary algorithms guided by human collaborative feedback produce significantly more attractive art images than random generation in the latent space of Creative Adversarial Networks.

## Executive Summary
This study explores the use of evolutionary algorithms to navigate the latent space of Creative Adversarial Networks (CANs) for generating art images. The approach combines automatic aesthetic evaluation using Neural Image Assessment (NIMA) and collaborative interactive human evaluation to guide the evolution process. A local search mutation operator is also employed to improve image quality. Results show that both automatic and collaborative interactive evolution can generate increasingly attractive art images, but only the collaborative interactive evolution produced significantly more attractive images compared to random generation. The study highlights the potential of evolutionary algorithms for art generation and the importance of human input in guiding the creative process.

## Method Summary
The method uses evolutionary algorithms to explore the latent space of a trained Creative Adversarial Network (CAN) on the WikiArts dataset. The approach encodes latent vectors as individuals in a population of size 15, evolved over 25 generations. Fitness evaluation combines automatic aesthetic scoring via NIMA and collaborative human ratings (1-10) from five participants. The evolutionary process employs uniform crossover (50% probability, exchanging 25% of genes) and local search mutation using a (1+1) evolution strategy with 100 steps. Diversity preservation is enforced through a similarity threshold of 25, with random immigrants inserted when needed. Images are upscaled using LapSRN for human evaluation.

## Key Results
- Both automatic (NIMA) and collaborative interactive evolution can generate increasingly attractive art images.
- Only collaborative interactive evolution produced significantly more attractive images compared to random generation (p < 0.001).
- Human participants preferred evolved images over random images in 60% of pairwise comparisons.
- Local search mutation based on automatic aesthetic evaluation did not lead to improvements beyond random level in human-perceived quality.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Evolutionary algorithms can navigate the latent space of Creative Adversarial Networks (CANs) to discover novel and aesthetically pleasing art images.
- Mechanism: By encoding latent vectors as individuals in an evolutionary algorithm, crossover and mutation operators generate new candidate images. These are evaluated using automatic aesthetic metrics (NIMA) and/or human collaborative feedback to guide the evolution toward increasingly attractive outputs.
- Core assumption: The latent space of a well-trained CAN contains a diverse set of visually interesting images, and the fitness functions (automatic or human) can meaningfully distinguish between more and less attractive ones.
- Evidence anchors:
  - [abstract] "both automatic and collaborative interactive evolution can generate increasingly attractive art images"
  - [section] "The results show that this methodology is generally suitable for exploring the latent space of possible art images in a GAN, and it is able to create increasingly attractive images."
  - [corpus] No direct corpus evidence; claim relies on internal results.
- Break condition: If the latent space is too sparse or the fitness evaluation fails to capture human aesthetic preferences, evolution will not converge to more attractive images.

### Mechanism 2
- Claim: Human collaborative feedback is crucial for guiding evolution toward images perceived as more attractive than random generation.
- Mechanism: Multiple participants rate images independently, and the average score serves as fitness. This collaborative evaluation accounts for subjectivity and steers evolution toward consensus-preferred aesthetics.
- Core assumption: Human aesthetic preferences are sufficiently aligned across participants to allow meaningful collective guidance, and the collaborative evaluation captures this alignment.
- Evidence anchors:
  - [abstract] "human guidance is crucial, as only the collaborative interactive evolution produced significantly more attractive images compared to random generation"
  - [section] "human participants preferred the obtained images on average in 60% of the cases... significantly preferred over random images ( T = 185, p < .001)"
  - [corpus] No corpus evidence; relies on study's own evaluation data.
- Break condition: If participants' preferences are too divergent or random, the collaborative fitness signal becomes noisy and evolution fails to improve image attractiveness.

### Mechanism 3
- Claim: Local search mutation (intelligent mutation) using automatic aesthetic evaluation does not reliably improve human-perceived image quality.
- Mechanism: A (1+1) evolution strategy mutates a latent vector and keeps the better of parent/offspring based on NIMA scores, aiming to locally optimize aesthetics before human evaluation.
- Core assumption: Automatic aesthetic metrics (NIMA) correlate with human aesthetic judgment, so local search improves images in ways humans will appreciate.
- Evidence anchors:
  - [section] "the evaluation by human participants revealed that the local search did not lead to improvements beyond a random level"
  - [section] "The mean value of 51% shows that the results are preferred only in slightly more than half of the cases, which corresponds to random level."
  - [corpus] No corpus evidence; finding is specific to this study's setup.
- Break condition: If NIMA and human judgment diverge significantly, local search optimizes for the wrong objective and fails to improve perceived quality.

## Foundational Learning

- Concept: Generative Adversarial Networks (GANs) and their latent space
  - Why needed here: The entire approach relies on navigating the latent space of a trained GAN to generate novel images; understanding GAN mechanics is essential.
  - Quick check question: What is the relationship between latent vectors and generated images in a GAN?

- Concept: Evolutionary algorithms (EA) and their operators
  - Why needed here: The method uses EA to explore the latent space; knowing how selection, crossover, and mutation work is critical for implementation and tuning.
  - Quick check question: How does stochastic universal sampling (SUS) differ from roulette wheel selection?

- Concept: Aesthetic evaluation metrics (e.g., NIMA)
  - Why needed here: Automatic fitness evaluation depends on these metrics; understanding their training and limitations is key to interpreting results.
  - Quick check question: On what type of data was NIMA trained, and how might that affect its performance on art images?

## Architecture Onboarding

- Component map:
  - Trained CAN generator (genotype-to-phenotype mapping) -> Latent vector population (individuals) -> Evolutionary operators (crossover, mutation with local search) -> Fitness evaluation (automatic NIMA, collaborative human ratings) -> Diversity preservation (similarity threshold, random immigrants) -> Upscaling (LapSRN for display) -> Evaluation pipeline (pairwise human comparisons)

- Critical path:
  1. Initialize random latent vectors
  2. Generate images via CAN generator
  3. Evaluate fitness (automatic or human)
  4. Select parents (SUS)
  5. Apply crossover/mutation (including local search)
  6. Enforce diversity preservation
  7. Repeat until termination

- Design tradeoffs:
  - Population size vs. user fatigue: small (15) to reduce human evaluation burden
  - Local search generations vs. diversity: 100 generations trades off exploration for exploitation
  - Similarity threshold: set to 25, balancing diversity with avoiding too many random immigrants

- Failure signatures:
  - Fitness plateaus early: may indicate local optima or poor fitness signal
  - High diversity loss: similarity threshold too strict or insufficient random immigrants
  - Random immigrants consistently rated above average: suggests population is converging to suboptimal regions

- First 3 experiments:
  1. Verify CAN generator produces diverse art images from random latent vectors
  2. Test automatic aesthetic evaluation (NIMA) on a small set of generated images for consistency
  3. Run a short pilot with human participants to calibrate collaborative evaluation process and timing

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of mutation strategy (local search) affect the evolution of art images compared to other mutation strategies?
- Basis in paper: [explicit] The study mentions using a local search mutation operator based on an aesthetic measure, but its effectiveness compared to other mutation strategies is not explicitly tested.
- Why unresolved: The study focuses on comparing automatic and collaborative interactive evolution using the local search mutation strategy, without exploring alternative mutation strategies.
- What evidence would resolve it: Comparing the evolution of art images using different mutation strategies (e.g., random mutation, gradient-based mutation) would provide insights into the impact of mutation strategy on the final results.

### Open Question 2
- Question: What is the impact of the population size and number of generations on the evolution of art images?
- Basis in paper: [explicit] The study uses a population size of 15 and evolves it over 25 generations, but the impact of varying these parameters is not explored.
- Why unresolved: The study does not investigate how different population sizes and number of generations affect the evolution process and the final results.
- What evidence would resolve it: Conducting experiments with different population sizes and number of generations would reveal the optimal settings for the evolution of art images.

### Open Question 3
- Question: How does the diversity preservation mechanism affect the exploration of the latent space and the final results?
- Basis in paper: [explicit] The study introduces a diversity preservation mechanism to avoid user fatigue and accelerate exploration of the latent space, but its impact on the final results is not explicitly tested.
- Why unresolved: The study does not investigate how the diversity preservation mechanism affects the exploration of the latent space and the quality of the final art images.
- What evidence would resolve it: Comparing the evolution of art images with and without the diversity preservation mechanism would reveal its impact on the exploration of the latent space and the final results.

## Limitations
- Results are limited to a single CAN architecture trained on WikiArts dataset with only five human participants.
- No comparison against alternative generative models (StyleGAN, diffusion models) or other evolutionary strategies.
- Local search mutation effectiveness is only tested with one strategy and hyperparameter setting (100 steps).

## Confidence

- High: The experimental methodology is clearly described and reproducible.
- Medium: The core result that collaborative human evaluation outperforms automatic evaluation is well-supported.
- Low: Generalizability to other GAN architectures, datasets, or evolutionary strategies.

## Next Checks

1. **Replication with alternative GANs**: Repeat the experiment using StyleGAN2 or diffusion models to test whether results hold across different latent space geometries.

2. **Automated metric ablation study**: Test multiple automatic aesthetic metrics (e.g., LPIPS, FID-based aesthetics) to identify whether NIMA is uniquely ineffective or if the issue is more general.

3. **Larger-scale human evaluation**: Expand participant pool to 20+ individuals with diverse art backgrounds to test robustness of collaborative preferences and measure inter-rater reliability.