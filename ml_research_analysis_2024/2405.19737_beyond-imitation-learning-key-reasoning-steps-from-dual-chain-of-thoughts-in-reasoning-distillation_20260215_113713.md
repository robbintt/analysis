---
ver: rpa2
title: 'Beyond Imitation: Learning Key Reasoning Steps from Dual Chain-of-Thoughts
  in Reasoning Distillation'
arxiv_id: '2405.19737'
source_url: https://arxiv.org/abs/2405.19737
tags:
- cots
- reasoning
- steps
- dual
- edit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of distilling reasoning capabilities
  from large language models (LLMs) to smaller language models (SLMs) in a way that
  captures key reasoning steps rather than just imitating reasoning style. The proposed
  method, EDIT (mistakE-Driven key reasonIng step distillaTion), generates dual Chain-of-Thought
  (CoT) data with similar intermediate reasoning but different conclusions, then uses
  minimum edit distance to identify and optimize for key reasoning steps during training.
---

# Beyond Imitation: Learning Key Reasoning Steps from Dual Chain-of-Thoughts in Reasoning Distillation

## Quick Facts
- arXiv ID: 2405.19737
- Source URL: https://arxiv.org/abs/2405.19737
- Authors: Chengwei Dai; Kun Li; Wei Zhou; Songlin Hu
- Reference count: 40
- Key outcome: EDIT achieves 4.7% higher accuracy than standard CoT distillation by focusing on key reasoning steps identified through dual CoT analysis

## Executive Summary
This paper addresses the challenge of distilling reasoning capabilities from large language models (LLMs) to smaller language models (SLMs) in a way that captures key reasoning steps rather than just imitating reasoning style. The proposed method, EDIT (mistakE-Driven key reasonIng step distillaTion), generates dual Chain-of-Thought (CoT) data with similar intermediate reasoning but different conclusions, then uses minimum edit distance to identify and optimize for key reasoning steps during training. Experiments show EDIT outperforms standard CoT distillation baselines, achieving 4.7% higher accuracy on average across in-domain and out-of-domain reasoning benchmarks. The method is particularly effective when the dual CoT pairs contain logical errors rather than knowledge or calculation errors. EDIT also generates higher-quality CoTs with more correct key reasoning steps, as verified by GPT-4 evaluation.

## Method Summary
EDIT works by first generating Chain-of-Thought (CoT) data from teacher LLMs, then creating dual CoT pairs where two responses share similar intermediate reasoning steps but arrive at different conclusions. The method uses minimum edit distance to identify which reasoning steps are truly key (those that differ between the dual CoTs), then optimizes the student model specifically for these key steps while de-emphasizing non-critical steps. This is implemented through a two-stage training process: initial supervised fine-tuning on correct CoTs, followed by Key Reasoning Steps Learning (KRSL) that applies weighted loss functions to prioritize key reasoning steps. The approach draws an analogy to human learning, where analyzing mistakes alongside correct solutions reveals crucial steps leading to success or failure.

## Key Results
- EDIT outperforms standard CoT distillation by 4.7% average accuracy across reasoning benchmarks
- EDIT shows particular effectiveness when dual CoT pairs contain logical errors rather than knowledge/calculation errors
- GPT-4 evaluation shows EDIT generates higher-quality CoTs with more correct key reasoning steps
- Performance improvements are consistent across both in-domain (BBH) and out-of-domain (BB-sub, AGIEval, ARC-E, ARC-C) benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: EDIT improves SLM reasoning by focusing learning on key reasoning steps rather than full CoT imitation
- **Mechanism**: By generating dual CoT pairs and using minimum edit distance to identify key reasoning steps, EDIT optimizes likelihood specifically for these critical steps during training
- **Core assumption**: A small proportion (~4.7%) of reasoning steps in CoTs are truly key steps that impact conclusions
- **Evidence anchors**: [abstract]: "CoTs consist mainly of simple reasoning forms, with a small proportion (≈ 4.7%) of key reasoning steps that truly impact conclusions"
- **Break condition**: If dual CoTs cannot be generated with sufficiently similar intermediate reasoning steps

### Mechanism 2
- **Claim**: Learning from mistakes through dual CoTs helps students understand why certain reasoning paths lead to correct vs incorrect conclusions
- **Mechanism**: By exposing students to both correct and wrong CoTs with similar intermediate steps but different conclusions, EDIT helps students learn to avoid errors in key reasoning steps
- **Core assumption**: Analyzing mistakes according to correct solutions reveals crucial steps leading to successes or failures
- **Evidence anchors**: [abstract]: "Drawing an analogy to human learning, where analyzing mistakes according to correct solutions often reveals the crucial steps leading to successes or failures"
- **Break condition**: If dual CoTs contain too many confounding factors or if mistakes are not representative

### Mechanism 3
- **Claim**: EDIT generates higher-quality CoTs with more correct key reasoning steps compared to standard CoT distillation
- **Mechanism**: By optimizing for key reasoning steps specifically and learning from both correct and wrong reasoning paths, EDIT produces CoTs that better reflect actual reasoning needed to solve problems
- **Core assumption**: CoTs that correctly execute key reasoning steps are higher quality than those that merely imitate teacher style
- **Evidence anchors**: [abstract]: "Further analysis shows that EDIT can generate high-quality CoTs with more correct key reasoning steps"
- **Break condition**: If GPT-4 evaluation shows no improvement in CoT quality despite EDIT training

## Foundational Learning

- **Concept**: Minimum edit distance algorithm
  - Why needed here: To identify key reasoning steps by finding minimal changes needed to transform correct CoTs into wrong ones
  - Quick check question: If correct CoT is "Step 1. X, Step 2. Y, Step 3. Z" and wrong CoT is "Step 1. X, Step 2. W, Step 3. Z", which step would edit distance identify as key?

- **Concept**: Supervised fine-tuning with weighted loss functions
  - Why needed here: To optimize likelihood specifically for key reasoning steps identified through edit distance
  - Quick check question: How would you modify standard cross-entropy loss to weight certain tokens more heavily based on their importance?

- **Concept**: Chain-of-Thought prompting and reasoning extraction
  - Why needed here: To generate initial CoT data from teacher LLMs that serves as foundation for EDIT's dual CoT generation
  - Quick check question: What is the difference between CoT prompting and standard prompting in terms of output structure?

## Architecture Onboarding

- **Component map**: Question → Teacher LLM CoT generation → Dual CoT creation → EDIT training → Student SLM with improved reasoning
- **Critical path**: Question → Teacher LLM CoT generation → Dual CoT creation → EDIT training → Student SLM with improved reasoning
- **Design tradeoffs**: 
  - More complex training pipeline vs simpler standard CoT distillation
  - Higher computational cost for dual CoT generation vs potential accuracy gains
  - Reliance on teacher LLM mistakes vs using only correct examples
- **Failure signatures**: 
  - EDIT performs worse than standard CoT distillation
  - Student model fails to generate CoTs at all
  - EDIT only improves on in-domain tasks but not out-of-domain
- **First 3 experiments**:
  1. Run EDIT on a small subset of BBH with a simple baseline to verify training works
  2. Compare CoT quality scores from GPT-4 between EDIT and Std-CoT on the same small dataset
  3. Test whether edit distance correctly identifies key steps by manually verifying a sample of dual CoTs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of EDIT vary when using different teacher LLM models (e.g., GPT-4 vs ChatGPT) as the source of CoTs and dual CoTs?
- Basis in paper: [inferred] The paper uses ChatGPT as the teacher model but notes this was due to cost considerations rather than optimal choice
- Why unresolved: The paper acknowledges that using GPT-4 could better demonstrate EDIT's effectiveness but doesn't actually test this comparison due to resource constraints
- What evidence would resolve it: Experiments comparing EDIT performance when using GPT-4 vs ChatGPT as the teacher model

### Open Question 2
- Question: What is the optimal balance between correct and wrong CoTs data for maximizing EDIT's performance?
- Basis in paper: [explicit] The paper shows that using both correct and wrong CoTs is beneficial, but doesn't systematically explore the optimal ratio between them
- Why unresolved: The current implementation uses all available correct and wrong CoTs without investigating whether different proportions might yield better results
- What evidence would resolve it: Controlled experiments varying the ratio of correct to wrong CoTs while measuring performance on reasoning benchmarks

### Open Question 3
- Question: How does EDIT perform on reasoning tasks that require domain-specific knowledge beyond general reasoning capabilities?
- Basis in paper: [inferred] The experiments focus on general reasoning benchmarks, but the paper doesn't test EDIT on specialized domains like legal reasoning or medical diagnosis
- Why unresolved: The paper demonstrates EDIT's effectiveness on general reasoning but doesn't explore its applicability to specialized knowledge domains
- What evidence would resolve it: Testing EDIT on domain-specific reasoning benchmarks and comparing performance against standard CoT distillation methods

### Open Question 4
- Question: How sensitive is EDIT to the quality of the dual CoTs generation process?
- Basis in paper: [explicit] The paper notes that the quality of dual CoTs is crucial and explores different mistake patterns, but doesn't investigate the impact of prompt quality or generation reliability
- Why unresolved: The current implementation uses specific prompts to generate dual CoTs, but doesn't examine how variations in prompt design affect outcomes
- What evidence would resolve it: Experiments systematically varying the dual CoTs generation prompts and measuring resulting performance differences

## Limitations

- Core mechanism relies heavily on minimum edit distance algorithm, but paper provides minimal detail on handling complex reasoning structures
- Dual CoT generation depends on teacher LLM mistakes, introducing variability and potential bias in training data
- Quality evaluation using GPT-4 introduces circularity concerns, as same type of model evaluates outputs from method designed to improve similar models
- Lack of ablation studies to isolate contribution of each component (dual CoTs, edit distance weighting, mistake learning) to overall performance improvement

## Confidence

**High confidence**: The general framework of using dual CoTs and weighted optimization for key reasoning steps is technically sound and implementable. The experimental methodology for accuracy benchmarking is standard and reliable.

**Medium confidence**: The claim that EDIT outperforms standard CoT distillation by 4.7% average accuracy is supported by experimental results, but underlying reasons for this improvement are not fully validated. The claim about generating higher-quality CoTs is partially supported but relies on potentially biased evaluation methods.

**Low confidence**: The assertion that key reasoning steps comprise only ~4.7% of total steps appears to be dataset-specific rather than a universal principle. The effectiveness of learning from mistakes as a general mechanism for improving reasoning is not sufficiently demonstrated across diverse reasoning domains.

## Next Checks

1. **Ablation study validation**: Implement and test versions of EDIT with individual components disabled (no dual CoTs, no edit distance weighting, no mistake learning) to quantify the contribution of each mechanism to performance improvements.

2. **Cross-task generalization test**: Apply EDIT to reasoning tasks from domains not represented in the training data (e.g., medical diagnosis, legal reasoning, or scientific hypothesis generation) to evaluate whether the method generalizes beyond specific problem types.

3. **Alternative evaluation methodology**: Replace GPT-4-based quality assessment with human expert evaluation of CoT quality, or develop a benchmark dataset with ground-truth key reasoning steps to directly measure whether EDIT actually identifies and optimizes the correct key steps.