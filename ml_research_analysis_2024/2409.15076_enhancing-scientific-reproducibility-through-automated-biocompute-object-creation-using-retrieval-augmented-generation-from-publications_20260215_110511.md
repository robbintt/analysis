---
ver: rpa2
title: Enhancing Scientific Reproducibility Through Automated BioCompute Object Creation
  Using Retrieval-Augmented Generation from Publications
arxiv_id: '2409.15076'
source_url: https://arxiv.org/abs/2409.15076
tags: []
core_contribution: This paper presents the BCO assistant, an automated tool for creating
  BioCompute Objects (BCOs) from scientific publications using Retrieval-Augmented
  Generation (RAG) with large language models. The tool addresses the challenge of
  retroactive documentation compliance by extracting relevant information from papers
  and associated GitHub repositories through optimized retrieval processes, including
  two-pass retrieval with re-ranking.
---

# Enhancing Scientific Reproducibility Through Automated BioCompute Object Creation Using Retrieval-Augmented Generation from Publications

## Quick Facts
- arXiv ID: 2409.15076
- Source URL: https://arxiv.org/abs/2409.15076
- Authors: Sean Kim; Raja Mazumder
- Reference count: 35
- Automated BCO creation from publications using RAG and LLM

## Executive Summary
This paper introduces the BCO assistant, an automated tool that creates BioCompute Objects (BCOs) from scientific publications using Retrieval-Augmented Generation (RAG) with large language models. The tool addresses the challenge of retroactive documentation compliance by extracting relevant information from papers and associated GitHub repositories through optimized retrieval processes. By employing carefully engineered prompts for each BCO domain and automated evaluation capabilities, the system significantly reduces the time and effort required for bioinformatics documentation while maintaining IEEE BCO standard compliance.

## Method Summary
The BCO assistant uses a two-pass retrieval system with re-ranking to extract relevant information from scientific publications and associated GitHub repositories. The system employs domain-specific prompt engineering for each BCO domain and integrates large language models for generating compliant documentation. The methodology includes automated evaluation capabilities to assess the quality and completeness of generated BCOs, with the goal of enhancing scientific reproducibility through AI-assisted documentation and knowledge extraction.

## Key Results
- Automated creation of BCOs from publications using RAG with LLMs
- Two-pass retrieval system with re-ranking improves information extraction
- Domain-specific prompt engineering ensures IEEE BCO standard compliance
- Automated evaluation capabilities assess BCO quality and completeness

## Why This Works (Mechanism)
The system leverages RAG to combine document retrieval with LLM generation capabilities, allowing it to extract relevant information from publications and repositories while maintaining context awareness. The two-pass retrieval with re-ranking improves the precision of information extraction by first gathering broad context and then focusing on the most relevant details for each BCO domain.

## Foundational Learning
- **RAG Architecture**: Why needed - Combines retrieval with generation for context-aware outputs; Quick check - Verify retrieval results before generation
- **Two-pass Retrieval**: Why needed - Improves precision by first gathering broad context then focusing on details; Quick check - Compare single-pass vs two-pass retrieval effectiveness
- **Domain-specific Prompt Engineering**: Why needed - Ensures compliance with BCO standards across different workflow domains; Quick check - Validate prompts with BCO standard documentation
- **Automated Evaluation**: Why needed - Enables systematic assessment of generated BCO quality; Quick check - Test evaluation metrics against manually created BCOs
- **GitHub Repository Integration**: Why needed - Provides access to actual code and execution details; Quick check - Verify repository access and parsing functionality
- **LLM-based Generation**: Why needed - Transforms extracted information into structured BCO format; Quick check - Validate output format against BCO schema

## Architecture Onboarding
**Component Map**: Publication Corpus -> Two-pass Retriever -> GitHub Repository Fetcher -> LLM Generator -> BCO Output -> Automated Evaluator

**Critical Path**: Publication retrieval → Information extraction → Prompt engineering → LLM generation → BCO validation

**Design Tradeoffs**: 
- Balance between retrieval precision and computational cost
- Tradeoff between prompt specificity and generalization across domains
- Balance between automated evaluation automation and human oversight

**Failure Signatures**:
- Poor retrieval results leading to incomplete BCOs
- LLM generation failures producing non-compliant BCOs
- GitHub repository access issues preventing code integration
- Automated evaluation missing critical BCO components

**3 First Experiments**:
1. Test retrieval accuracy on sample publications with known BCO requirements
2. Validate LLM generation against sample BCO templates
3. Evaluate automated assessment accuracy using manually created BCO benchmarks

## Open Questions the Paper Calls Out
None

## Limitations
- Validation relied on only three manually created BCOs from a single conference
- Evaluation framework lacked independent verification by external reviewers
- Performance in edge cases with ambiguous methodology descriptions not addressed
- GitHub repository integration assumes standardized code organization practices

## Confidence
- High confidence: Technical implementation of two-pass retrieval system with re-ranking
- Medium confidence: Tool's potential to reduce documentation time based on evaluation framework
- Low confidence: Claims about maintaining IEEE BCO standard compliance require broader validation

## Next Checks
1. Conduct a blind evaluation where independent domain experts assess BCOs generated by the tool against manually created BCOs using established BCO validation criteria
2. Test the system on a diverse corpus of publications spanning multiple computational biology domains, including those with varying levels of methodological detail and code organization
3. Evaluate system performance on publications containing incomplete or ambiguous methodology descriptions to assess robustness in real-world scenarios