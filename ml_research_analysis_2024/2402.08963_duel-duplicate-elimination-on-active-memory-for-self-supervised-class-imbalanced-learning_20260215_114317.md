---
ver: rpa2
title: 'DUEL: Duplicate Elimination on Active Memory for Self-Supervised Class-Imbalanced
  Learning'
arxiv_id: '2402.08963'
source_url: https://arxiv.org/abs/2402.08963
tags:
- memory
- data
- learning
- class
- duel
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes a framework for addressing class imbalance in
  self-supervised learning by introducing an active memory system that filters redundant
  data. The DUEL framework employs a duplicate elimination policy inspired by human
  working memory to maintain diversity in stored samples, thereby improving representation
  robustness.
---

# DUEL: Duplicate Elimination on Active Memory for Self-Supervised Class-Imbalanced Learning

## Quick Facts
- **arXiv ID**: 2402.08963
- **Source URL**: https://arxiv.org/abs/2402.08963
- **Reference count**: 40
- **Primary result**: DUEL significantly improves self-supervised learning under class imbalance by filtering redundant data from active memory, boosting linear probing accuracy especially under extreme imbalance.

## Executive Summary
This work addresses class imbalance in self-supervised learning (SSL) by introducing DUEL, a framework that actively manages a memory buffer to filter redundant (dominant-class) samples. Inspired by human working memory, DUEL employs a duplicate elimination policy that incrementally increases class distribution entropy in memory, thereby improving representation robustness. The approach integrates Hebbian Metric Learning (HML) to optimize feature representations without explicit labels, enforcing both intra-class compactness and inter-class separation. Empirical results on CIFAR-10, STL-10, and ImageNet-LT demonstrate consistent improvements over baseline SSL methods, particularly under severe imbalance.

## Method Summary
DUEL builds on SSL frameworks (MoCo, SimCLR) by introducing an active memory system with a duplicate elimination policy. The policy replaces the most duplicated data—identified by lowest distinctiveness information—with new samples to enhance memory diversity. HML is used to align representations by jointly optimizing a feature extractor and memory to minimize intra-class similarity and maximize inter-class distinctiveness. The method trains with InfoNCE loss, incorporating both batch and memory-based negative samples. Theoretical analysis bounds the ideal HML objective with an empirical loss, showing that an optimal memory approximating the oracle data preserves feature extractor optimality.

## Key Results
- DUEL variants (D-MoCo, D-SimCLR) achieve higher linear probing accuracies on CIFAR-10 and STL-10 under extreme imbalance (e.g., 75% dominant class).
- The framework consistently improves performance on the long-tailed ImageNet-LT dataset.
- DUEL enhances distinctiveness of negative samples while preserving intra-class compactness.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: DUEL prevents class imbalance degradation by actively filtering duplicated (dominant-class) samples from memory.
- **Mechanism**: The DUEL policy removes the most duplicated element in the active memory—i.e., the element with lowest distinctiveness information—and replaces it with new data. This incrementally increases the entropy of the class distribution in memory, reducing overrepresentation of dominant classes.
- **Core assumption**: Lower distinctiveness information correlates with higher duplication in the memory.
- **Evidence anchors**: [abstract] "DUEL policy, which replaces the most duplicated data with new samples, aims to enhance the distinctiveness information in the memory"; [section] "J = arg min Id(xj; f, Mπ)" — directly defines the replacement criterion.
- **Break condition**: If distinctiveness information does not monotonically increase with replacements, the filtering may become ineffective.

### Mechanism 2
- **Claim**: Hebbian Metric Learning (HML) aligns representations by minimizing mutual duplication probability within classes while maximizing distinctiveness across classes.
- **Mechanism**: HML jointly optimizes a feature extractor and memory to minimize Ih (Hebbian information, intra-class similarity) and maximize Id (distinctiveness information, inter-class diversity). This enforces both intra-class compactness and inter-class separation.
- **Core assumption**: Mutual duplication probability can be defined without explicit class labels and still enforce semantic separation.
- **Evidence anchors**: [section] "Hebbian information Ih(f; D) is defined as mean information of mutual duplication probability with positive samples"; [section] "distinctiveness information Id(xi; f, D) is estimated information of the proportion of class ci from the distribution D".
- **Break condition**: If the metric used for mutual duplication probability fails to reflect true semantic similarity, optimization may collapse representations.

### Mechanism 3
- **Claim**: Memory-integrated HML generalizes to class-imbalanced environments by approximating the ideal HML objective with an empirical distribution and active memory.
- **Mechanism**: By bounding the ideal HML loss (LHML) with an empirical loss (LM-HML) that includes a memory term, and showing that an optimal memory approximating the oracle data preserves the optimality of the feature extractor, the framework maintains performance under imbalance.
- **Core assumption**: An optimal memory M* ≈ D exists and can be approximated in practice.
- **Evidence anchors**: [section] "Theorem 1 (Optimality of M-HML). Assume that an optimal memory M∗ ≃ D exists."; [section] "The bound contains two terms: |λ · Ih(f; D′) − Ih(f; D)| and |Id(f; D′, M) − Id(f; D)|".
- **Break condition**: If the memory cannot approximate the oracle distribution (e.g., too small or poor replacement policy), the bound fails.

## Foundational Learning

- **Concept**: Self-supervised learning (SSL)
  - **Why needed here**: SSL avoids costly labels while still learning discriminative representations; DUEL builds on SSL frameworks like MoCo and SimCLR.
  - **Quick check question**: What is the role of negative samples in InfoNCE-based SSL, and how does memory augmentation change it?

- **Concept**: Working memory in cognitive science
  - **Why needed here**: DUEL is inspired by the central executive system's inhibition of dominant information to maintain diversity.
  - **Quick check question**: How does distinctiveness information in DUEL relate to the cognitive notion of information maximization in working memory?

- **Concept**: Metric learning
  - **Why needed here**: HML reframes representation learning as a metric learning problem without explicit labels, optimizing similarity metrics directly.
  - **Quick check question**: In HML, how are mutual duplication probabilities used to define positive and negative pairs without class labels?

## Architecture Onboarding

- **Component map**: Feature extractor (fθ) -> Projection head -> Memory (M) -> DUEL policy (πDUEL) -> Loss (InfoNCE)
- **Critical path**:
  1. Sample minibatch and augment.
  2. Compute embeddings for batch and sampled memory negatives.
  3. Update feature extractor via InfoNCE loss.
  4. Update memory using DUEL policy (replace lowest Id element).
  5. Repeat until convergence.
- **Design tradeoffs**:
  - Memory size vs. computational cost: Larger memory improves approximation of D but increases O(K²) cost.
  - Replacement frequency vs. stability: Frequent replacement increases diversity but may destabilize training.
  - Stop-gradient on memory samples vs. end-to-end learning: Improves efficiency but may limit adaptation.
- **Failure signatures**:
  - Performance plateaus early: May indicate memory not being updated effectively or feature extractor not adapting.
  - Accuracy drops in later training: Possible overfitting or poor distinctiveness signal in memory.
  - High VRAM usage with no gain: Memory too large relative to batch size; diminishing returns.
- **First 3 experiments**:
  1. Run D-MoCo on CIFAR-10 with ρmax = 0.75; measure linear probing accuracy vs. baseline MoCo.
  2. Visualize class distribution entropy in memory over training steps; verify DUEL increases entropy.
  3. Ablation: Replace DUEL policy with random replacement; compare downstream accuracy and distinctiveness metrics.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. However, based on the content, potential open questions include:

- How does the DUEL policy perform under extreme class imbalance (e.g., one class representing 99% of the data)?
- What is the optimal memory size for the DUEL framework in different class-imbalanced settings?
- How does the DUEL framework compare to traditional class-imbalance techniques like reweighting and resampling?

## Limitations

- The effectiveness of DUEL hinges on the assumption that distinctiveness information reliably captures semantic diversity and that removing the minimum Id element monotonically improves class balance.
- The working memory inspiration is conceptually appealing but lacks empirical grounding in how closely DUEL mirrors cognitive processes.
- The HML framework relies on theoretical bounds that may not hold tightly in practice.

## Confidence

- **High**: Empirical improvements on CIFAR-10 and STL-10 with clear baselines and metrics.
- **Medium**: Theoretical justification of HML and memory bounds; requires deeper inspection of assumptions.
- **Low**: Cognitive analogy to working memory and distinctiveness; more metaphor than measurable mechanism.

## Next Checks

1. Test whether DUEL memory entropy correlates with downstream task performance across multiple imbalance ratios.
2. Evaluate robustness by replacing DUEL policy with a diversity-driven sampling baseline (e.g., core-set selection) and compare gains.
3. Inspect learned embeddings for class overlap and intra-class variance to verify HML’s claimed effects.