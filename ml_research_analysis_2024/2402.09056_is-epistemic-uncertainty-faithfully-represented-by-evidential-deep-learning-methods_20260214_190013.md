---
ver: rpa2
title: Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning
  Methods?
arxiv_id: '2402.09056'
source_url: https://arxiv.org/abs/2402.09056
tags:
- uncertainty
- distribution
- epistemic
- learning
- second-order
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether evidential deep learning methods
  faithfully represent epistemic uncertainty by comparing them to a theoretically
  defined "reference distribution." Through novel theoretical analysis and experiments
  on classification and regression tasks, it demonstrates that second-order risk minimization
  methods do not capture epistemic uncertainty in a quantitatively faithful manner.
  The authors show that these methods can converge to degenerate distributions (like
  Dirac deltas) and that the learned parameters are often unidentifiable, leading
  to arbitrary epistemic uncertainty estimates.
---

# Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?

## Quick Facts
- arXiv ID: 2402.09056
- Source URL: https://arxiv.org/abs/2402.09056
- Authors: Mira Jürgens; Nis Meinert; Viktor Bengs; Eyke Hüllermeier; Willem Waegeman
- Reference count: 32
- Primary result: Evidential deep learning methods fail to quantitatively capture epistemic uncertainty despite good relative uncertainty performance

## Executive Summary
This paper investigates whether evidential deep learning methods faithfully represent epistemic uncertainty by comparing them to a theoretically defined "reference distribution." Through novel theoretical analysis and experiments on classification and regression tasks, it demonstrates that second-order risk minimization methods do not capture epistemic uncertainty in a quantitatively faithful manner. The authors show that these methods can converge to degenerate distributions (like Dirac deltas) and that the learned parameters are often unidentifiable, leading to arbitrary epistemic uncertainty estimates. While these methods perform well for relative uncertainty comparisons in downstream tasks, they fail to provide accurate absolute uncertainty quantification.

## Method Summary
The authors develop a theoretical framework that defines a reference distribution representing true epistemic uncertainty. They compare this reference distribution against the output distributions of evidential deep learning methods using second-order risk minimization. The comparison is performed both theoretically and empirically across classification and regression tasks. The theoretical analysis proves that second-order methods cannot match the reference distribution under any regularization scheme, while experiments demonstrate that learned parameters are often unidentifiable and can converge to degenerate distributions. The framework uses statistical tests and likelihood-based metrics to quantify the faithfulness of uncertainty estimates.

## Key Results
- Second-order risk minimization methods can converge to degenerate distributions like Dirac deltas, failing to capture meaningful epistemic uncertainty
- The learned parameters in evidential methods are often unidentifiable, resulting in arbitrary epistemic uncertainty estimates
- While evidential methods perform well for relative uncertainty comparisons in downstream tasks, they fail to provide accurate absolute uncertainty quantification

## Why This Works (Mechanism)
The core mechanism behind the failure of evidential deep learning methods lies in the mismatch between the optimization objective and the true epistemic uncertainty. Second-order risk minimization optimizes for specific distributional properties that do not align with the reference distribution representing true epistemic uncertainty. The theoretical framework reveals that the optimization landscape allows for multiple parameter configurations that achieve similar training performance but produce vastly different uncertainty estimates. This leads to unidentifiability issues where the learned parameters are arbitrary with respect to the true epistemic uncertainty, resulting in poor quantitative faithfulness even when relative uncertainty comparisons appear reasonable.

## Foundational Learning
- **Reference distribution concept**: Why needed - provides ground truth for epistemic uncertainty; Quick check - verify it represents true model uncertainty given data
- **Second-order risk minimization**: Why needed - the optimization framework used by evidential methods; Quick check - understand how it differs from first-order methods
- **Identifiability in Bayesian inference**: Why needed - explains why learned parameters can be arbitrary; Quick check - assess whether parameters are uniquely determined by data
- **Degenerate distributions**: Why needed - shows failure modes of evidential methods; Quick check - verify convergence to Dirac deltas or similar distributions
- **Relative vs absolute uncertainty**: Why needed - distinguishes performance metrics; Quick check - test both types of uncertainty in downstream tasks
- **Statistical testing for distribution comparison**: Why needed - quantifies faithfulness of uncertainty estimates; Quick check - apply tests to compare learned vs reference distributions

## Architecture Onboarding

Component map: Input data -> Evidential model -> Second-order loss -> Parameter optimization -> Output distribution -> Uncertainty quantification

Critical path: The critical path is the optimization process where parameters are learned through second-order risk minimization. This path determines the output distribution's shape and thus the quality of uncertainty quantification. Any issues in parameter identifiability or convergence directly impact the final uncertainty estimates.

Design tradeoffs: The main tradeoff is between computational efficiency and faithfulness of uncertainty quantification. Evidential methods offer tractable uncertainty estimates but sacrifice quantitative accuracy. Alternative approaches like sampling-based Bayesian methods are more faithful but computationally expensive.

Failure signatures: Key failure modes include convergence to degenerate distributions (Dirac deltas), unidentifiable parameters leading to arbitrary uncertainty estimates, and good relative uncertainty performance masking poor absolute uncertainty quantification.

First experiments:
1. Compare uncertainty estimates from evidential methods against analytical reference distributions on simple synthetic datasets
2. Test identifiability by training the same model with different random seeds and comparing learned parameters
3. Evaluate both relative and absolute uncertainty performance on a standard benchmark dataset

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions beyond its main findings about the limitations of evidential deep learning methods.

## Limitations
- Theoretical analysis assumes certain regularity conditions that may not hold in practice
- Experiments focus on specific architectures and datasets, limiting generalization
- Does not fully explore potential mitigation strategies or alternative regularization approaches

## Confidence
High: The theoretical framework is sound and well-developed
Medium: Experimental evidence supports findings but is limited in scope
Medium: The core mechanism explaining failure modes is clearly articulated

## Next Checks
1. Test the proposed framework on larger-scale real-world datasets, particularly in safety-critical applications
2. Investigate whether alternative regularization schemes or architectural modifications can improve faithfulness
3. Evaluate the impact of different training procedures and hyperparameter choices on the faithfulness of uncertainty estimates