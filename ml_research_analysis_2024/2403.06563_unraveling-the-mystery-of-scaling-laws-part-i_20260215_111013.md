---
ver: rpa2
title: 'Unraveling the Mystery of Scaling Laws: Part I'
arxiv_id: '2403.06563'
source_url: https://arxiv.org/abs/2403.06563
tags:
- loss
- training
- size
- batch
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper aims to establish a more precise and practical derivation
  of scaling laws for large language models. The authors confirm that the original
  scaling law formulas from [KMH+20] remain valid when scaling models up to 33B parameters,
  but the constant coefficients vary significantly with experiment setup.
---

# Unraveling the Mystery of Scaling Laws: Part I

## Quick Facts
- arXiv ID: 2403.06563
- Source URL: https://arxiv.org/abs/2403.06563
- Reference count: 8
- This paper provides step-by-step instructions to estimate scaling law constants from small models and accurately predicts various attributes for models up to 33B parameters.

## Executive Summary
This paper aims to establish a more precise and practical derivation of scaling laws for large language models. The authors confirm that the original scaling law formulas from [KMH+20] remain valid when scaling models up to 33B parameters, but the constant coefficients vary significantly with experiment setup. They provide step-by-step instructions to estimate all constant terms by training on models with only 1M-60M parameters. Using these estimated formulas, they accurately predict various attributes for models with up to 33B parameters, including the minimum possible test loss, the minimum required training steps and processed tokens to achieve a specific loss, the critical batch size at any loss value, and the complete test loss trajectory with arbitrary batch size. The paper showcases the capability to predict training behavior before training starts, enabling more principled and efficient development of large language models.

## Method Summary
The authors propose a method to estimate scaling law constants by training a series of small models (1M-60M parameters) and measuring their convergence behavior. They estimate constants for model size (Nc, αN), dataset size (Sc, αS), and batch size (B*, αB) using linear regression and contour line fitting. These constants are then used in the scaling law formulas to predict loss trajectories, critical batch sizes, and other training attributes for larger models up to 33B parameters. The approach involves training small models with varying parameter counts and batch sizes, then applying mathematical frameworks to derive predictions for larger models.

## Key Results
- Scaling law functional forms remain valid when scaling from 1M to 33B parameters, but constant coefficients vary significantly with experiment setup
- Loss trajectories for 33B parameter models can be predicted with 90-95% accuracy using constants estimated from models as small as 1M parameters
- Critical batch size that optimizes time/computation tradeoff can be analytically computed from loss value using derived formula Bcrit(L) = tr(HΣ)/(GT HG)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The original scaling law formulations remain valid when scaling up to 33B parameters, but the constant coefficients vary significantly with experiment setup.
- Mechanism: The scaling law's functional form (power-law relationship) is robust across model sizes, but the constants depend on factors like data distribution, context length, tokenization, and model configurations. These constants can be accurately estimated by training small models (1M-60M parameters) and extrapolating.
- Core assumption: The power-law relationship itself is invariant to model size; only the constants change with experimental conditions.
- Evidence anchors:
  - [abstract] "we confirm that the scaling law formulations proposed in the original OpenAI paper remain valid when scaling the model size up to 33 billion, but the constant coefficients in these formulas vary significantly with the experiment setup"
  - [section 2] "scaling laws [KMH+20] draw the following correlation: L(N) = (Nc/N)^αN"
  - [corpus] Found 25 related papers with average FMR=0.432, suggesting active research but no direct contradicting evidence
- Break condition: If the underlying data manifold or task fundamentally changes (e.g., switching from language modeling to computer vision), the power-law form itself may break down.

### Mechanism 2
- Claim: The test loss trajectory can be predicted accurately by estimating constants from small models and applying them to large models.
- Mechanism: By training a series of small models with different parameter counts and measuring their convergence loss, we can estimate the constants Nc and αN. Similarly, by training with varying batch sizes, we can estimate B* and αB. These constants allow us to predict loss trajectories for much larger models without actually training them.
- Core assumption: The training dynamics observed in small models are representative of those in large models when properly scaled.
- Evidence anchors:
  - [section 3.1] "we can obtain k equations in the form of αN log Nc − αN log Ni − Li = 0"
  - [section 4] "The actual and predicted loss trajectories closely align, especially after the initial warm-up stage"
  - [corpus] No direct evidence, but scaling law research generally supports this mechanism
- Break condition: If hyperparameters or architectural changes fundamentally alter the optimization landscape in ways not captured by simple scaling.

### Mechanism 3
- Claim: The critical batch size Bcrit(L) that optimizes the time/computation tradeoff can be analytically computed from the loss value.
- Mechanism: Through mathematical derivation, the authors show that Bcrit(L) = tr(HΣ)/(GT HG), where H is the Hessian, Σ is the gradient covariance, and G is the true gradient. This allows for principled batch size selection without exhaustive tuning.
- Core assumption: The relationship between batch size and training dynamics follows predictable mathematical patterns.
- Evidence anchors:
  - [section 3.3] "we can derive the critical batch size at L which minimizes the trade-off between time (S/Smin) and computation (E/Emin)"
  - [section 4.1] "Table 1 The estimated constant values in scaling-law formulas on the C4 training data"
  - [corpus] Weak evidence; this is a novel contribution not widely validated in existing literature
- Break condition: If the assumption of smooth loss landscape breaks down (e.g., with highly non-convex objectives or poor initialization).

## Foundational Learning

- Concept: Power-law relationships and their mathematical properties
  - Why needed here: The entire scaling law framework is built on power-law relationships between loss and various factors like model size, dataset size, and compute.
  - Quick check question: If L ∝ N^α, what happens to L when N is doubled and α = -0.1?

- Concept: Gradient descent dynamics and batch size effects
  - Why needed here: Understanding how batch size affects convergence rate and the relationship between noisy gradients and true gradients is crucial for deriving the critical batch size formula.
  - Quick check question: In the limit of infinite batch size, what does stochastic gradient descent become?

- Concept: Hyperparameter optimization and learning rate scheduling
  - Why needed here: The paper emphasizes that hyperparameters like learning rate affect convergence rate but not final converged loss, and optimal learning rates scale with model size.
  - Quick check question: If a model with N parameters uses learning rate η, what learning rate should a model with 10N parameters use according to scaling laws?

## Architecture Onboarding

- Component map: Small model training pipeline -> Constant estimation module -> Scaling law prediction engine -> Large model validation
- Critical path: Estimate constants from small models → Apply mathematical framework → Generate predictions → Validate on actual large model training (if resources permit)
- Design tradeoffs: Small model training is computationally cheap but may miss some large-model-specific phenomena; more extensive small model training improves prediction accuracy but increases upfront costs
- Failure signatures: Predictions consistently overestimate convergence speed; actual loss trajectories show different functional forms than predicted; critical batch size estimates don't match empirical findings
- First 3 experiments:
  1. Train 7 models with parameters 1M, 10M, 100M, 1B, 3B, 10B, 30B on C4 dataset until convergence to estimate Nc and αN
  2. Train a 10M parameter model with varying batch sizes to estimate B* and αB using contour line fitting
  3. Use estimated constants to predict loss trajectory of a 2B model and compare with actual training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do scaling laws change when using mixture-of-experts (MoE) architectures compared to dense architectures?
- Basis in paper: [explicit] The paper discusses that mixture-of-experts architectures have different parameter interactions, stating "In MoE architectures, each input interacts with only a subset of the network’s parameters – chosen independently for each datapoint."
- Why unresolved: The paper acknowledges that MoE architectures would likely impact the scaling law formulas, but does not provide specific formulas or experiments for MoE architectures.
- What evidence would resolve it: Deriving and validating scaling law formulas specifically for MoE architectures through experiments comparing MoE and dense models across different parameter counts.

### Open Question 2
- Question: How can context length be incorporated as a parameter in scaling law formulas?
- Basis in paper: [explicit] The paper notes that "the context length significantly influences the values of the constant terms in scaling-law formulas" and suggests that "it would be possible to include the context length directly as a parameter of the formulas."
- Why unresolved: The paper mentions the potential for including context length but does not provide a concrete method or experimental validation of how to incorporate it into the existing scaling law framework.
- What evidence would resolve it: Developing a modified set of scaling law formulas that include context length as a parameter and validating these through experiments with varying context lengths.

### Open Question 3
- Question: How do scaling laws apply to models trained on multiple datasets with different distributions?
- Basis in paper: [explicit] The paper discusses the challenge of determining the optimal mix ratio from multiple data sources and suggests that scaling laws could provide insights by "predicting the test loss trajectory of large models on each individual data source."
- Why unresolved: While the paper suggests that scaling laws could help in determining the optimal mix ratio, it does not provide a concrete method or experimental validation of how to apply scaling laws to multi-dataset scenarios.
- What evidence would resolve it: Developing a framework for applying scaling laws to multi-dataset training scenarios and validating it through experiments with models trained on different combinations of datasets.

## Limitations
- The constant estimation procedure may not generalize perfectly to different data distributions, tokenization schemes, or model architectures beyond the tested experimental setup
- The critical batch size formula and its practical utility for hyperparameter selection hasn't been extensively validated across diverse training scenarios
- The assumption that optimal learning rates scale proportionally with model size may break down for very large models or different optimizer configurations

## Confidence
- **High Confidence**: The core claim that scaling law functional forms (power-law relationships) remain valid when scaling from small to large models is well-supported by extensive experimental evidence across the 1M-33B parameter range
- **Medium Confidence**: The specific constant values and their estimation procedures are reasonably well-validated within the tested experimental setup, but may not generalize perfectly to different data distributions, tokenization schemes, or model architectures
- **Low Confidence**: The critical batch size formula and its practical utility for hyperparameter selection hasn't been extensively validated across diverse training scenarios

## Next Checks
1. **Cross-dataset validation**: Apply the estimated scaling law constants to predict training behavior on a completely different dataset (e.g., multilingual corpus, code-only dataset) and compare predicted vs actual loss trajectories
2. **Architecture ablation study**: Systematically vary architectural components (attention mechanisms, feed-forward network sizes, activation functions) in the small models used for constant estimation and measure how this affects prediction accuracy for large models
3. **Early-stage prediction accuracy**: Focus specifically on the initial warm-up phase of training where the paper acknowledges predictions are less accurate. Train a series of models with different initialization schemes and measure whether the constant estimation procedure can be adapted to improve early-stage predictions