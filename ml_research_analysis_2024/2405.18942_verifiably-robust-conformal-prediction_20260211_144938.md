---
ver: rpa2
title: Verifiably Robust Conformal Prediction
arxiv_id: '2405.18942'
source_url: https://arxiv.org/abs/2405.18942
tags:
- vrcp
- prediction
- robust
- coverage
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes Verifiably Robust Conformal Prediction (VRCP),\
  \ a novel framework that leverages neural network verification to construct adversarially\
  \ robust prediction sets for both classification and regression tasks. Unlike prior\
  \ methods limited to \u21132-norm bounded perturbations and classification, VRCP\
  \ supports arbitrary \u2113p-norm perturbations and extends to regression settings."
---

# Verifiably Robust Conformal Prediction

## Quick Facts
- arXiv ID: 2405.18942
- Source URL: https://arxiv.org/abs/2405.18942
- Reference count: 13
- Key outcome: Novel framework using neural verification to construct adversarially robust prediction sets for both classification and regression tasks

## Executive Summary
This paper introduces Verifiably Robust Conformal Prediction (VRCP), a framework that leverages neural network verification to construct prediction sets that maintain coverage guarantees under adversarial perturbations. Unlike prior methods limited to ℓ2-norm bounded perturbations and classification tasks, VRCP supports arbitrary ℓp-norm perturbations and extends to regression settings. The framework introduces two variants: VRCP-I (verification at inference time) and VRCP-C (verification at calibration time), achieving strong empirical performance on CIFAR10, CIFAR100, TinyImageNet, and deep reinforcement learning regression tasks.

## Method Summary
VRCP constructs prediction sets by solving a verification problem that ensures coverage guarantees hold under adversarial perturbations. For classification, the framework computes lower bounds on softmax scores to determine which classes remain plausible under worst-case perturbations. For regression, it computes lower bounds on the probability that predictions fall within specified intervals. The two variants differ in when verification occurs: VRCP-I performs verification during inference for each test point, while VRCP-C performs verification during calibration on the training set. Both approaches eliminate the need for holdout sets, score function transformations, or probabilistic smoothing.

## Key Results
- Maintains above-nominal coverage under adversarial attacks while producing more efficient and informative prediction sets
- Extends conformal prediction to arbitrary ℓp-norm perturbations and regression tasks
- Achieves significant efficiency improvements over state-of-the-art methods without requiring holdout sets or score transformations

## Why This Works (Mechanism)
VRCP leverages formal verification techniques to guarantee that prediction sets remain valid under worst-case adversarial perturbations. By computing lower bounds on model outputs or probabilities under perturbation constraints, the framework can mathematically prove that coverage guarantees hold even when inputs are adversarially manipulated. This approach provides deterministic robustness guarantees rather than probabilistic ones.

## Foundational Learning
- Conformal prediction fundamentals: Why needed - provides distribution-free uncertainty quantification; Quick check - verify coverage guarantee holds marginally at target level
- Neural network verification basics: Why needed - enables formal robustness guarantees; Quick check - confirm verification solver can handle model architecture
- ℓp-norm bounded perturbations: Why needed - defines adversarial threat model; Quick check - validate perturbation bounds are appropriate for data domain

## Architecture Onboarding
Component map: Input → Verification Solver → Prediction Set Construction → Output

Critical path: For each input, compute adversarial bounds → verify coverage condition → construct minimal valid prediction set

Design tradeoffs: VRCP-I trades inference time for calibration efficiency, while VRCP-C does the opposite. No holdout set requirement vs. potential calibration quality trade-offs.

Failure signatures: Verification timeouts indicate overly conservative bounds; coverage drops suggest insufficient perturbation budget; overly large prediction sets indicate loose bounds.

First experiments:
1. Verify basic coverage guarantee on clean data
2. Test prediction set efficiency compared to standard conformal methods
3. Evaluate adversarial robustness under ℓ∞ perturbations

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- Computational efficiency concerns with neural verification, particularly for large models and datasets
- Empirical evaluation limited to specific architectures and may not generalize broadly
- Theoretical guarantees assume exact verification, which relies on approximations in practice

## Confidence
High: Theoretical framework is well-developed and logically sound
Medium: Empirical results are promising but may not capture all real-world scenarios
Low: Scalability claims and performance under adaptive adversaries require further validation

## Next Checks
1. Evaluate VRCP's performance on larger-scale models (Vision Transformers, LLMs) and datasets to assess scalability claims
2. Conduct stress tests with adaptive adversaries targeting the verification mechanism
3. Validate framework performance across diverse data modalities and under distribution shifts