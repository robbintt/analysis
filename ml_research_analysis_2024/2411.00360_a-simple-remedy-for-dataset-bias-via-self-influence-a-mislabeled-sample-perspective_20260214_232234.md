---
ver: rpa2
title: 'A Simple Remedy for Dataset Bias via Self-Influence: A Mislabeled Sample Perspective'
arxiv_id: '2411.00360'
source_url: https://arxiv.org/abs/2411.00360
tags:
- samples
- bias-conflicting
- sample
- bias
- self-influence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new perspective on dataset bias by linking
  it to mislabeled sample detection. The authors propose Bias-Conditioned Self-Influence
  (BCSI), which adapts Self-Influence (SI) from mislabeled sample detection to identify
  bias-conflicting samples by restricting models to focus on malignant bias early
  in training.
---

# A Simple Remedy for Dataset Bias via Self-Influence: A Mislabeled Sample Perspective

## Quick Facts
- arXiv ID: 2411.00360
- Source URL: https://arxiv.org/abs/2411.00360
- Reference count: 40
- Proposes BCSI to detect bias-conflicting samples and improve debiasing through pivotal subset fine-tuning

## Executive Summary
This paper introduces Bias-Conditioned Self-Influence (BCSI), a method that adapts Self-Influence from mislabeled sample detection to identify bias-conflicting samples in biased datasets. By computing self-influence using models trained with Generalized Cross Entropy (GCE) for only a few epochs, BCSI effectively detects samples that conflict with spurious correlations. The method constructs a pivotal subset of these bias-conflicting samples and fine-tunes biased models using this subset, achieving state-of-the-art accuracy on CIFAR10C and improving performance across various bias severities.

## Method Summary
The method consists of three main components: (1) BCSI computation using early-epoch models with GCE loss to detect bias-conflicting samples, (2) pivotal subset construction via top-k selection and intersection across random initializations to increase detection purity, and (3) fine-tuning with dual loss functions (pivotal subset cross-entropy + counterweight loss on remaining samples) to rectify biased models. The approach exploits the empirical observation that models learn bias before task-relevant features during early training, making early epoch models more effective at detecting bias-conflicting samples.

## Key Results
- BCSI outperforms standard Self-Influence in detecting bias-conflicting samples across diverse datasets
- Fine-tuning with BCSI-constructed pivotal subsets achieves state-of-the-art accuracy on CIFAR10C
- Method improves performance across various bias severities (0.5% to 90%) and is complementary to existing debiasing techniques

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BCSI outperforms SI in detecting bias-conflicting samples by restricting the model to focus on malignant bias early in training
- Mechanism: BCSI computes self-influence using a model trained with GCE for only 5 epochs, forcing the model to prioritize learning bias-aligned samples before task-relevant features emerge
- Core assumption: Malignant bias is learned before task-relevant features during early training stages
- Evidence anchors: Abstract mentions BCSI outperforms SI; section explains using heavily biased early-epoch models; corpus papers don't directly validate BCSI's early-training mechanism

### Mechanism 2
- Claim: The pivotal subset constructed via BCSI contains a higher proportion of bias-conflicting samples
- Mechanism: BCSI ranks training samples by influence scores, selecting top-k per class and using intersection across multiple random initializations to increase purity
- Core assumption: Bias-conflicting samples consistently receive high BCSI scores across different random initializations
- Evidence anchors: Abstract mentions pivotal subset construction; section explains intersection approach; corpus papers don't validate intersection for BCSI

### Mechanism 3
- Claim: Fine-tuning with pivotal subset and counterweight loss effectively rectifies biased models
- Mechanism: Fine-tuning uses cross-entropy loss on pivotal subset and counterweight cross-entropy loss on remaining training data
- Core assumption: Pivotal subset contains enough bias-conflicting samples to shift model focus from spurious correlations
- Evidence anchors: Abstract mentions fine-tuning achieves SotA accuracy; section describes dual loss approach; corpus papers don't validate counterweight loss approach

## Foundational Learning

- Concept: Influence Functions and Self-Influence
  - Why needed here: BCSI builds on influence function theory to detect bias-conflicting samples by measuring how removing a sample affects its own prediction
  - Quick check question: What does a high self-influence score indicate about a training sample's relationship to the learned model?

- Concept: Bias-Conflicting vs Bias-Aligned Samples
  - Why needed here: The method fundamentally relies on distinguishing between samples that contradict versus reinforce spurious correlations
  - Quick check question: How does a bias-conflicting sample differ from a bias-aligned sample in terms of its relationship to task-relevant features and bias attributes?

- Concept: Early Training Dynamics and Bias Learning
  - Why needed here: BCSI exploits that models learn bias before task-relevant features, making early epoch models more effective for detecting bias-conflicting samples
  - Quick check question: Why does computing self-influence on early epoch models improve detection of bias-conflicting samples compared to converged models?

## Architecture Onboarding

- Component map: BCSI computation -> pivotal subset construction -> fine-tuning
- Critical path: BCSI quality directly affects pivotal set purity and fine-tuning effectiveness
- Design tradeoffs: Early epoch computation trades detection accuracy for efficiency; intersection trades sample quantity for quality; counterweight loss trades pure bias-conflicting learning for robustness
- Failure signatures: Poor BCSI separation indicates early feature learning; low pivotal set purity suggests wrong sample detection; fine-tuning failure suggests insufficient bias-conflicting samples
- First 3 experiments:
  1. Run BCSI on CIFAR10C with varying epoch values (1, 3, 5, 10) to find optimal early-training cutoff
  2. Test pivotal set construction with different intersection counts (1, 2, 3) on CIFAR10C to measure detection precision vs set size tradeoff
  3. Apply full pipeline to highly biased dataset (CIFAR10C 5%) comparing ERM + fine-tuning vs BCSI + fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific conditions make BCSI most effective for detecting bias-conflicting samples across different dataset types?
- Basis in paper: Explicit mention that BCSI works better in early training epochs with GCE training
- Why unresolved: Paper identifies optimal conditions but lacks comprehensive analysis of dataset characteristics affecting BCSI effectiveness
- What evidence would resolve it: Systematic experiments varying dataset characteristics and comparing BCSI performance across these variations

### Open Question 2
- Question: How does the size of the pivotal set affect debiasing performance across different bias severities?
- Basis in paper: Inferred from ablation study mentioning k=100 but suggesting different sizes might be optimal
- Why unresolved: Paper tests fixed k=100 but doesn't systematically explore size-performance tradeoff across bias severities
- What evidence would resolve it: Experiments varying k across a range of values for datasets with different bias severities

### Open Question 3
- Question: Can BCSI be effectively adapted for debiasing tasks in domains beyond computer vision, such as natural language processing?
- Basis in paper: Explicit mention of NLP datasets but only using existing debiasing methods as baselines
- Why unresolved: Paper demonstrates effectiveness in vision but doesn't explore BCSI's applicability to NLP debiasing
- What evidence would resolve it: Applying BCSI methodology to NLP debiasing tasks and comparing performance to existing NLP methods

### Open Question 4
- Question: What are the computational trade-offs between BCSI and other influence function estimation methods in large-scale applications?
- Basis in paper: Inferred from mentions of last-layer approximation and ablation studies
- Why unresolved: Paper acknowledges computational considerations but lacks detailed analysis of scalability and resource requirements
- What evidence would resolve it: Systematic benchmarking of BCSI against other influence estimation methods on increasingly large datasets and models

## Limitations

- Mechanism dependency on early bias learning: BCSI fundamentally relies on the empirical observation that models learn bias before task-relevant features, which may not generalize universally
- Intersection-based pivotal set construction: While improving purity, intersection reduces pivotal set size, potentially limiting effectiveness for datasets with very low bias-conflicting sample proportions
- Computational overhead: Computing influence functions remains expensive compared to standard training, requiring multiple random initializations and repeated computations

## Confidence

- High confidence: Empirical results showing BCSI outperforming standard SI across multiple datasets with consistent quantitative improvements
- Medium confidence: Mechanism explanation for why early training with GCE improves detection, relying on empirical observation about training dynamics
- Medium confidence: Claim that BCSI is complementary to existing debiasing techniques, demonstrated through compatibility but not extensive combination testing

## Next Checks

1. Systematically vary the training epoch for BCSI computation (1, 3, 5, 10 epochs) on CIFAR10C to empirically validate that separation between bias-conflicting and bias-aligned samples peaks at early epochs

2. Test pivotal set construction with different intersection counts (1, 2, 3) on datasets with varying bias severities to measure the tradeoff between detection precision and pivotal set size

3. Apply the complete BCSI pipeline to datasets using different model architectures (CNNs, transformers) to verify method effectiveness isn't architecture-specific