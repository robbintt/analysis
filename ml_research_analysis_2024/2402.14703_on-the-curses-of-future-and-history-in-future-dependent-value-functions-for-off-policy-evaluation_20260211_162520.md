---
ver: rpa2
title: On the Curses of Future and History in Future-dependent Value Functions for
  Off-policy Evaluation
arxiv_id: '2402.14703'
source_url: https://arxiv.org/abs/2402.14703
tags:
- coverage
- which
- learning
- assumption
- belief
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of off-policy evaluation (OPE)
  in partially observable environments, where traditional approaches suffer from exponential
  dependence on the horizon. The authors build on recent work on future-dependent
  value functions (FDVFs) but identify critical issues with existing boundedness guarantees
  that could still scale exponentially.
---

# On the Curses of Future and History in Future-dependent Value Functions for Off-policy Evaluation

## Quick Facts
- arXiv ID: 2402.14703
- Source URL: https://arxiv.org/abs/2402.14703
- Reference count: 40
- Key outcome: Introduces polynomial bounds for off-policy evaluation in POMDPs through novel coverage assumptions

## Executive Summary
This paper addresses the challenge of off-policy evaluation (OPE) in partially observable environments, where traditional approaches suffer from exponential dependence on the horizon. The authors build on recent work on future-dependent value functions (FDVFs) but identify critical issues with existing boundedness guarantees that could still scale exponentially. They introduce two novel coverage assumptions - outcome coverage and belief coverage - that enable polynomial bounds on key quantities and avoid the exponential scaling that plagued previous approaches.

## Method Summary
The authors develop a framework for OPE in POMDPs by introducing future-dependent value functions that leverage both future observations and historical information to break the curse of horizon. They establish two coverage conditions: outcome coverage, which measures how well the future reveals latent states, and belief coverage, which measures how well histories reveal latent states. Under these conditions, they prove that FDVFs can be constructed with bounded range and provide finite-sample guarantees for estimation algorithms. The work also discovers a new algorithm analogous to marginalized importance sampling for MDPs that achieves complementary properties.

## Key Results
- Establishes polynomial error bounds for OPE in POMDPs under outcome and belief coverage assumptions
- Provides finite-sample guarantees with error bounds depending polynomially on horizon H rather than exponentially
- Develops a marginalized importance sampling analog for POMDPs with complementary theoretical properties
- Proves that under coverage conditions, FDVFs can be constructed with bounded range

## Why This Works (Mechanism)
The mechanism works by breaking the curse of horizon through the introduction of future-dependent value functions that can capture the essential information about latent states from both future observations and historical data. The coverage assumptions ensure that this information is sufficiently revealed to enable polynomial bounds, rather than requiring exponential memory to track all possible histories.

## Foundational Learning

**Partially Observable MDPs (POMDPs)**: Models where agents cannot directly observe the true state but receive observations. Needed to understand the fundamental challenge being addressed. Quick check: Can you explain how POMDPs differ from MDPs in terms of information structure?

**Future-dependent Value Functions (FDVFs)**: Value functions that depend on future observations rather than just current state. Needed to understand the core technical innovation. Quick check: How does conditioning on future observations help break the curse of horizon?

**Outcome Coverage**: A condition measuring how well future observations reveal latent states. Needed to understand one of the key theoretical assumptions. Quick check: Can you explain why this condition is necessary for polynomial bounds?

**Belief Coverage**: A condition measuring how well historical information reveals latent states. Needed to understand the complementary theoretical assumption. Quick check: How does this differ from outcome coverage and why is both needed?

## Architecture Onboarding

**Component map**: POMDP environment -> Future-dependent value function estimator -> OPE estimate

**Critical path**: Coverage conditions verification -> FDVF construction -> Finite-sample estimation -> Error bound derivation

**Design tradeoffs**: The paper trades off strict theoretical conditions (coverage assumptions) for polynomial rather than exponential bounds. This requires verifying conditions that may be challenging in practice but enables tractable OPE.

**Failure signatures**: If coverage conditions are violated, the bounds degrade to exponential scaling. If the FDVF estimator is misspecified, the OPE estimates may be biased.

**3 first experiments**:
1. Verify coverage conditions in simple POMDP environments (Hallway, Tag)
2. Compare FDVF-based OPE against baseline methods under varying coverage quality
3. Test finite-sample performance of the marginalized importance sampling analog

## Open Questions the Paper Calls Out

The paper identifies several open questions including: how to efficiently verify coverage conditions in practice, whether weaker conditions could yield similar results, and how to extend the approach to function approximation settings.

## Limitations

- Coverage assumptions may be challenging to verify in practice
- Proposed algorithm lacks comprehensive experimental validation
- Computational complexity beyond asymptotic considerations remains unexplored

## Confidence

**High confidence**: Theoretical claims regarding polynomial bounds under coverage assumptions
**Medium confidence**: Practical applicability of coverage assumptions
**Medium confidence**: Performance of proposed algorithm in real-world settings
**Medium confidence**: Finite-sample guarantees in complex POMDP environments

## Next Checks

1. Implement and test the proposed algorithm on benchmark POMDP environments (e.g., Hallway, Tag) to empirically verify polynomial error bounds.

2. Develop methods to estimate or verify outcome and belief coverage conditions from data, and test their effectiveness in practice.

3. Compare the proposed approach against existing OPE methods (e.g., Doubly Robust, Fitted Q-Evaluation) in partially observable settings to quantify practical performance gains.