---
ver: rpa2
title: 'OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language
  Foundation Models'
arxiv_id: '2402.10670'
source_url: https://arxiv.org/abs/2402.10670
tags: []
core_contribution: OpenFMNav tackles open-set zero-shot object navigation by leveraging
  foundation models. It uses LLMs to extract proposed objects from free-form natural
  language instructions and VLMs to detect candidate objects from scenes, building
  a Versatile Semantic Score Map (VSSM).
---

# OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models

## Quick Facts
- arXiv ID: 2402.10670
- Source URL: https://arxiv.org/abs/2402.10670
- Reference count: 14
- Primary result: Achieves 54.9% success rate and 0.244 SPL on HM3D ObjectNav benchmark

## Executive Summary
OpenFMNav introduces a novel approach to open-set zero-shot object navigation by leveraging vision-language foundation models. The method enables agents to navigate to objects specified in free-form natural language instructions, even when those objects were not present in the training data. By combining LLM-based semantic reasoning with VLM-based object discovery, OpenFMNav builds a Versatile Semantic Score Map that guides exploration and exploitation in unknown environments. The system demonstrates strong performance on the HM3D ObjectNav benchmark and validates its generalizability through real robot demonstrations.

## Method Summary
OpenFMNav tackles open-set zero-shot object navigation by first using ProposeLLM to extract semantic intent from free-form instructions, identifying possible objects that satisfy the user's demand. DiscoverVLM then actively detects candidate objects from the scene using GPT-4V, updating the Versatile Semantic Score Map (VSSM). PerceptVLM segments objects from RGB images using prompts, while ReasonLLM conducts common-sense reasoning on the VSSM to score frontiers and guide exploration. The system uses Fast Marching Method (FMM) control policy to generate low-level actions toward selected frontier goals, enabling effective navigation to open-set objects.

## Key Results
- Achieves 54.9% success rate and 0.244 SPL on HM3D ObjectNav benchmark
- Outperforms all strong baselines in open-set zero-shot object navigation
- Demonstrates real robot deployment validating open-set-ness and generalizability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs extract semantic intent from free-form instructions, enabling open-set object detection
- Mechanism: ProposeLLM uses Chain-of-Thought prompting to parse natural language and output possible objects with attributes matching user intent
- Core assumption: LLMs can reason about implicit user needs and map them to concrete object candidates using learned world knowledge
- Evidence anchors: [abstract] mentions LLM reasoning abilities for instruction parsing; [section 3.2] describes ProposeLLM extracting proposal objects

### Mechanism 2
- Claim: VLMs discover novel objects in the scene, enriching semantic map beyond training set categories
- Mechanism: DiscoverVLM (GPT-4V) actively segments and identifies objects in RGB frames, adding them to discovered objects list
- Core assumption: VLMs trained on diverse data can generalize to unseen object categories and recognize them in context
- Evidence anchors: [abstract] mentions VLM generalizability for candidate object detection; [section 3.2] describes DiscoverVLM discovering novel objects

### Mechanism 3
- Claim: ReasonLLM scores frontiers using semantic context, enabling common-sense guided exploration
- Mechanism: ReasonLLM uses Chain-of-Thought reasoning to assign scores [0,1] indicating likelihood of containing the goal based on semantic information
- Core assumption: LLMs can map semantic context to numeric likelihoods based on learned world knowledge
- Evidence anchors: [abstract] mentions common sense reasoning on VSSM; [section 3.3] describes ReasonLLM rating frontiers

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: Enables LLMs to decompose complex reasoning tasks into explicit steps, improving accuracy
  - Quick check question: What is the benefit of using CoT prompting in the ProposeLLM when given "I need to wash my hands"?

- Concept: Zero-shot object detection via VLMs
  - Why needed here: Allows system to detect objects outside training set, crucial for open-set navigation
  - Quick check question: How does the DiscoverVLM handle an object like "robot arm" that wasn't in the training vocabulary?

- Concept: Semantic mapping with confidence scores
  - Why needed here: Enables life-long learning and dynamic updates as new objects are discovered
  - Quick check question: Why does OpenFMNav use confidence scores instead of binary labels in the VSSM?

## Architecture Onboarding

- Component map: Instruction → ProposeLLM → DiscoverVLM (optional) → PerceptVLM → VSSM → ReasonLLM → Frontier selection → FMM control → Action
- Critical path: Instruction → ProposeLLM → DiscoverVLM (optional) → PerceptVLM → VSSM → ReasonLLM → Frontier selection → FMM control → Action
- Design tradeoffs:
  - Open-set vs. close-set: Open-set allows arbitrary objects but requires more robust VLMs; close-set is faster but less flexible
  - Map updating vs. static: Dynamic VSSM supports life-long learning but adds computational cost
  - LLM frequency: Frequent LLM calls improve reasoning but increase latency and cost
- Failure signatures:
  - High collision rate → control policy or VSSM mapping issue
  - Low success rate → poor frontier scoring or object detection
  - Timeouts → inefficient exploration or incorrect frontier selection
- First 3 experiments:
  1. Test ProposeLLM with simple vs. complex instructions to measure parsing accuracy
  2. Validate DiscoverVLM's ability to detect out-of-vocabulary objects in sample images
  3. Evaluate ReasonLLM's frontier scoring consistency across varied semantic contexts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance scale with size of dataset used to train foundation models?
- Basis in paper: [inferred] Foundation models are pre-trained on vast amounts of data, endowing them with exceptional generalizability
- Why unresolved: Paper does not provide experiments or analysis on how performance changes with dataset size
- What evidence would resolve it: Experiments comparing performance using foundation models trained on datasets of varying sizes

### Open Question 2
- Question: How does OpenFMNav handle situations where proposed objects are not present in the environment?
- Basis in paper: [explicit] DiscoverVLM can actively discover novel objects from the scene and update proposal objects
- Why unresolved: Paper does not provide details on handling situations where proposed objects cannot be discovered
- What evidence would resolve it: Experiments where proposed objects are not present in environment

### Open Question 3
- Question: How does performance compare when instructions contain ambiguous or contradictory information?
- Basis in paper: [inferred] Humans often provide free-form instructions with open-set objects, which can be ambiguous
- Why unresolved: Paper does not provide experiments on ambiguous or contradictory instructions
- What evidence would resolve it: Experiments comparing performance on ambiguous or contradictory instructions

## Limitations

- Computational overhead from frequent LLM calls may limit real-world deployment
- Performance depends heavily on quality of foundation model reasoning and object detection
- Scalability to environments with significantly more object categories than HM3D remains uncertain

## Confidence

- High confidence: Core mechanism of LLMs extracting semantic intent and VLMs detecting novel objects
- Medium confidence: Frontier scoring mechanism and common-sense reasoning effectiveness
- Low confidence: Computational overhead, real-world feasibility, and scalability to larger object vocabularies

## Next Checks

1. Conduct ablation studies isolating contribution of each foundation model component to success rate
2. Test OpenFMNav on more diverse dataset with expanded object vocabulary beyond HM3D
3. Measure and report computational latency and cost per navigation episode to assess real-world feasibility