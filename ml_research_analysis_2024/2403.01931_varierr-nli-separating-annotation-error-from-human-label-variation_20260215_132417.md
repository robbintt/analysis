---
ver: rpa2
title: 'VariErr NLI: Separating Annotation Error from Human Label Variation'
arxiv_id: '2403.01931'
source_url: https://arxiv.org/abs/2403.01931
tags:
- label
- human
- error
- labels
- errors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of distinguishing annotation
  errors from valid human label variation in NLP datasets. The authors propose a novel
  two-round annotation procedure for the NLI task, where annotators provide labels
  with explanations and then judge the validity of these label-explanation pairs.
---

# VariErr NLI: Separating Annotation Error from Human Label Variation

## Quick Facts
- arXiv ID: 2403.01931
- Source URL: https://arxiv.org/abs/2403.01931
- Reference count: 30
- Primary result: A novel two-round annotation procedure distinguishes annotation errors from valid human label variation in NLI datasets

## Executive Summary
This paper addresses the challenge of distinguishing annotation errors from valid human label variation in NLP datasets. The authors propose a novel two-round annotation procedure for the NLI task, where annotators provide labels with explanations and then judge the validity of these label-explanation pairs. This methodology is used to create the VARI ERR dataset, containing 7,574 validity judgments on 1,933 explanations for 500 NLI items. The authors evaluate various automatic error detection methods and large language models on this dataset, finding that existing AED methods significantly underperform compared to GPT models and human heuristics.

## Method Summary
The authors develop a two-round annotation procedure for NLI. In Round 1, four annotators label each item with explanations for their choices. In Round 2, annotators anonymously judge the validity of each label-explanation pair. Labels are classified as errors if all explanations are not self-validated, and as valid human label variation if at least one explanation is peer-validated. The VARI ERR dataset contains 7,574 validity judgments on 1,933 explanations for 500 NLI items.

## Key Results
- Existing AED methods significantly underperform compared to GPT models and human heuristics
- GPT-4 performs best among all methods but still falls short of human performance
- The results highlight the need for improved AED methods that can effectively separate errors from valid human label variation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining explanations with validity judgments enables distinguishing errors from valid label variation
- Mechanism: Round 1 annotators provide explanations for their labels, creating ecologically valid justifications. Round 2 annotators judge these explanations' validity, allowing identification of labels that lack valid justification as errors
- Core assumption: Explanations capture the annotator's true reasoning process and can be evaluated by other annotators for validity
- Evidence anchors:
  - [abstract]: "We propose a 2-round annotation procedure with annotators explaining each label and subsequently judging the validity of label-explanation pairs."
  - [section]: "Validity judgment mirrors conventional annotation adjudication in that annotators view NLI labels and explanations from each other. However, this information is delivered anonymously to annotators to reduce group dynamics."
  - [corpus]: Weak - no direct corpus evidence that explanations improve error detection beyond simple label counts
- Break condition: If explanations are post-hoc rationalizations rather than genuine reasoning, or if annotators cannot distinguish valid from invalid explanations, the mechanism fails

### Mechanism 2
- Claim: Self-validation allows annotators to correct their own errors when exposed to peer annotations
- Mechanism: Annotators judge their own label-explanation pairs in Round 2, allowing them to recognize when their initial annotation was incorrect given the full context of all annotations for that item
- Core assumption: Annotators can recognize their own errors when presented with the full set of peer annotations for the same item
- Evidence anchors:
  - [section]: "Importantly, in our Round 2 annotation, a label-explanation pair might be considered wrong in retrospect by the annotator who wrote it after reading all label-explanation pairs given to that item by four annotators."
  - [section]: "We define an NLI label as an error if all label-explanation pairs are not self-validated."
  - [corpus]: Weak - no direct corpus evidence showing self-correction rates
- Break condition: If annotators cannot recognize their own errors even when exposed to peer annotations, or if they become defensive and never reject their own work

### Mechanism 3
- Claim: Peer validation provides a robust signal for error detection by aggregating multiple independent judgments
- Mechanism: Each label-explanation pair receives judgments from three other annotators, and pairs are considered valid if they receive majority approval from peers
- Core assumption: Multiple independent annotators can reliably identify invalid explanations and errors when judging peers' work
- Evidence anchors:
  - [section]: "A label-explanation pair given by annotator a in Round 1 is peer-validated iff the majority (≥2) of the other annotators A/{a} approves the pair in Round 2."
  - [section]: "Peer sum sums all 'yes' judgments across multiple explanations on the same label, and Peeravg sums 'yes' judgments within each explanation and then averages across explanations within the label."
  - [corpus]: Weak - no direct corpus evidence showing inter-annotator agreement on validity judgments
- Break condition: If annotators have systematic biases or if the task is too subjective for reliable peer judgment, the mechanism fails

## Foundational Learning

- Concept: Annotation error detection in NLP
  - Why needed here: This work builds on extensive prior research in annotation error detection, which has focused on either post-hoc mining of errors or synthetic noise injection
  - Quick check question: What is the key limitation of post-hoc mining approaches for annotation error detection?
- Concept: Human label variation in NLP tasks
  - Why needed here: Understanding that valid disagreement exists is crucial for this work's approach of separating errors from variation rather than simply flagging all disagreement
  - Quick check question: How does human label variation differ from simple annotation noise in NLP tasks?
- Concept: Two-round annotation methodology
  - Why needed here: The methodology relies on having two distinct annotation phases - one for initial labeling with explanations, and one for validity judgment
  - Quick check question: What is the key difference between this two-round approach and conventional annotation adjudication?

## Architecture Onboarding

- Component map: Round 1 annotation pipeline -> Round 2 validation pipeline -> AED model evaluation framework -> Data statistics module
- Critical path: Round 1 → Round 2 → AED evaluation → dataset release
- Design tradeoffs:
  - Expert annotators (4) vs. crowd workers (100+): higher quality but smaller scale
  - Explanations required vs. optional: richer data but increased annotation burden
  - Anonymous validation vs. transparent discussion: reduces bias but loses collaborative benefits
- Failure signatures:
  - Low inter-annotator agreement on validity judgments indicates the task may be too subjective
  - GPT models performing similarly to random suggests the explanations may not be useful signals
  - High correlation between AED methods and label counts suggests methods may be capturing popularity rather than error
- First 3 experiments:
  1. Compare self-validation rates across different label types to identify which labels are most prone to self-correction
  2. Analyze the correlation between explanation quality scores and error detection performance
  3. Test whether training AED models on subsets of VARI ERR with different levels of HLV improves generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop a more accurate method to distinguish annotation errors from valid human label variation?
- Basis in paper: [explicit] The paper highlights the need for improved automatic error detection (AED) methods that can effectively separate errors from valid human label variation, as existing methods significantly underperform compared to GPT models and human heuristics
- Why unresolved: While the authors propose a novel two-round annotation procedure and create the VARI ERR dataset, the effectiveness of current AED methods remains limited, and there is a need for further research to develop more accurate techniques
- What evidence would resolve it: A comprehensive evaluation of new AED methods on the VARI ERR dataset, demonstrating their ability to accurately identify errors while preserving valid human label variation, would help resolve this question

### Open Question 2
- Question: What are the characteristics of the top-ranking items that are neither errors nor human label variation (HLV)?
- Basis in paper: [explicit] The authors observe that training-dynamics-based methods (MA and DM) assign a significant proportion of items that are neither errors nor HLV to the top 100 rankings, suggesting a need to identify the characteristics of these items
- Why unresolved: The paper does not provide a detailed analysis of the characteristics of these items, leaving the question of what distinguishes them from errors and HLV unanswered
- What evidence would resolve it: A thorough analysis of the linguistic features, semantic properties, or other relevant characteristics of the top-ranking items that are neither errors nor HLV would help resolve this question

### Open Question 3
- Question: How can explanations provided by annotators be leveraged to improve automatic error detection (AED) performance?
- Basis in paper: [explicit] The authors note that GPT models, which have access to explanations, outperform other methods that do not use explanations, suggesting the potential value of incorporating explanations into AED techniques
- Why unresolved: The paper does not explore the specific ways in which explanations could be utilized to enhance AED performance, leaving the question of how to effectively leverage this information unanswered
- What evidence would resolve it: Developing and evaluating AED methods that incorporate explanations, either by using them to compute training dynamics or by directly modeling the validity of explanations for labels, would help resolve this question

## Limitations

- The methodology may not generalize to other NLP tasks beyond NLI
- Reliance on expert annotators (4 per item) raises questions about scalability
- The relatively small dataset size (500 items) may limit the generalizability of findings

## Confidence

Medium. The experimental results show clear differences between error detection methods, with GPT models outperforming traditional AED approaches. However, the small dataset size and specific NLI domain context mean these findings should be validated across broader settings before being generalized.

## Next Checks

1. Apply the two-round annotation methodology to a different NLP task (e.g., sentiment analysis or coreference resolution) to test cross-task generalizability
2. Compare error detection performance when using crowd workers versus expert annotators on the same items to assess the impact of annotator expertise
3. Test whether explanations from GPT-4 or other LLMs can substitute for human explanations in the validation pipeline, enabling more scalable error detection