---
ver: rpa2
title: 'DOMBA: Double Model Balancing for Access-Controlled Language Models via Minimum-Bounded
  Aggregation'
arxiv_id: '2408.11121'
source_url: https://arxiv.org/abs/2408.11121
tags:
- exposure
- access
- domba
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DOMBA addresses the problem of training language models on access-controlled
  data without exposing sensitive information to unauthorized users. The core method
  trains two models on separate data partitions and aggregates their probability distributions
  using a min-bounded function, ensuring that information known to only one model
  is unlikely to be generated.
---

# DOMBA: Double Model Balancing for Access-Controlled Language Models via Minimum-Bounded Aggregation

## Quick Facts
- arXiv ID: 2408.11121
- Source URL: https://arxiv.org/abs/2408.11121
- Authors: Tom Segal; Asaf Shabtai; Yuval Elovici
- Reference count: 5
- Primary result: DOMBA achieves better utility-security trade-offs than existing methods for access-controlled language models

## Executive Summary
DOMBA addresses the challenge of training language models on access-controlled data without exposing sensitive information to unauthorized users. The method trains two separate models on different data partitions and aggregates their probability distributions using a min-bounded function, ensuring that information known to only one model is unlikely to be generated. This approach provides theoretical security guarantees while maintaining high utility performance.

## Method Summary
DOMBA trains two language models on separate data partitions and aggregates their probability distributions using a minimum-bounded function. The aggregation ensures that tokens only appear in the final model's output if both constituent models assign high probability to them, effectively preventing information leakage from either model individually. The method provides theoretical security guarantees through this bounded aggregation approach while maintaining utility comparable to standard language models.

## Key Results
- DOMBA achieves orders-of-magnitude improvement in exposure metrics compared to baseline methods
- Maintains perplexity scores comparable to non-secure language models
- Provides better utility-security trade-offs than existing access-controlled training approaches

## Why This Works (Mechanism)
DOMBA's effectiveness stems from the min-bounded aggregation function that combines probability distributions from two separately trained models. By only allowing tokens with high probability in both models to appear in the final output, the method ensures that information unique to one data partition cannot easily leak. This creates a natural access control mechanism where sensitive information present in only one partition has minimal chance of being generated.

## Foundational Learning
- **Min-bounded aggregation**: Mathematical operation combining probability distributions; needed to ensure information from one model cannot dominate output; quick check: verify aggregation preserves total probability mass
- **Access-controlled training**: Process of training models on sensitive data while preventing unauthorized access; needed to enable secure deployment of models trained on restricted datasets; quick check: confirm access policies are correctly enforced
- **Probability distribution aggregation**: Combining multiple probability distributions into one; needed to merge information from separate models while maintaining security guarantees; quick check: validate aggregated distribution sums to 1
- **Exposure metrics**: Quantitative measures of information leakage; needed to evaluate security effectiveness of access control methods; quick check: ensure exposure calculations are consistent across experiments
- **Perplexity measurement**: Standard metric for evaluating language model quality; needed to assess utility impact of security mechanisms; quick check: verify perplexity calculations match standard implementations

## Architecture Onboarding

**Component map**: Data partitioner -> Model 1 trainer -> Model 2 trainer -> Min-bounded aggregator -> Final model

**Critical path**: The aggregation step is the core innovation where security guarantees are enforced. The min-bounded function takes the probability distributions from both models and produces a final distribution that only allows tokens with high probability in both inputs.

**Design tradeoffs**: The approach trades some model expressiveness for security guarantees. While the min-bounded aggregation provides strong theoretical security, it may limit the model's ability to generate diverse outputs compared to standard training methods.

**Failure signatures**: If the aggregation function is not properly implemented, sensitive information could leak through one of the models. Additionally, if the data partitioning is not done carefully, the security guarantees may not hold.

**First experiments**:
1. Verify that the min-bounded aggregation preserves total probability mass (sums to 1)
2. Test that information present in only one data partition has low probability in the final model
3. Measure perplexity and exposure metrics on a small dataset to validate the utility-security trade-off

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical reliance on min-bounded aggregation may not perfectly prevent all information leakage in practice
- Real-world deployment could face challenges from model memorization and adversarial attacks
- Experiments focus on perplexity and exposure metrics but don't evaluate downstream task performance

## Confidence

**High**: DOMBA's core methodology and aggregation approach are well-defined and theoretically grounded

**Medium**: The utility-security trade-off improvements shown in experiments, as these depend on specific dataset characteristics

**Medium**: The claim about orders-of-magnitude improvement in exposure metrics, pending independent replication

## Next Checks
1. Conduct adversarial testing with fine-tuning attacks to evaluate whether malicious users can recover sensitive information from the aggregated model
2. Test DOMBA's performance across diverse downstream tasks (classification, generation, etc.) to verify that utility gains translate to practical applications
3. Evaluate the approach with larger language models (beyond 125M parameters) and more complex access control scenarios involving multiple data partitions