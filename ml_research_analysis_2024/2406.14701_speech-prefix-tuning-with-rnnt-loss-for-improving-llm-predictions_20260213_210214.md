---
ver: rpa2
title: Speech Prefix-Tuning with RNNT Loss for Improving LLM Predictions
arxiv_id: '2406.14701'
source_url: https://arxiv.org/abs/2406.14701
tags:
- speech
- prefixlm
- rnnt
- language
- prefix-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving ASR performance
  when applying large language models (LLMs) to speech recognition. The core method
  introduces RNNT loss for speech prefix-tuning, optimizing speech prefixes with ASR
  loss to learn better speech features and reduce LLM hallucinations.
---

# Speech Prefix-Tuning with RNNT Loss for Improving LLM Predictions

## Quick Facts
- arXiv ID: 2406.14701
- Source URL: https://arxiv.org/abs/2406.14701
- Authors: Murali Karthick Baskar; Andrew Rosenberg; Bhuvana Ramabhadran; Neeraj Gaur; Zhong Meng
- Reference count: 0
- Key outcome: Speech prefix-tuning with RNNT loss achieves 12% relative WER improvement over fine-tuned LLM baseline on 10 Indic languages; frozen LLM with speech prefix-tuning and langID soft prompting achieves 31% relative improvement over basic soft-prompting prefixLM.

## Executive Summary
This paper addresses the challenge of applying large language models (LLMs) to automatic speech recognition (ASR), specifically tackling LLM hallucinations and code-switching errors in multilingual settings. The authors propose speech prefix-tuning with RNNT loss, optimizing speech prefixes with ASR loss to learn better speech features and reduce hallucinations. Additionally, language-based soft prompting is introduced to enhance frozen LLM performance. Experiments on 10 Indic languages demonstrate significant improvements over both fine-tuned and frozen LLM baselines.

## Method Summary
The method involves optimizing speech prefixes with RNNT loss to improve ASR performance when applying LLMs to speech recognition. RNNT loss trains the speech encoder and prefix embeddings jointly to align speech features with text predictions, providing richer context to the LLM. Language ID (langID) soft prompting is proposed to further improve frozen LLM performance by conditioning prefixes on language-specific embeddings. The approach is evaluated on 10 Indic languages using YouTube longform data, with WER as the primary metric.

## Key Results
- Speech prefix-tuning with RNNT loss achieves 12% relative WER improvement over fine-tuned LLM baseline
- Frozen LLM with speech prefix-tuning and langID soft prompting achieves 31% relative WER improvement over basic soft-prompting prefixLM
- Average WER across 10 Indic languages reduced to 29.1% with combined approach

## Why This Works (Mechanism)

### Mechanism 1
RNNT loss optimizes speech prefix tokens to learn better speech-to-text alignments, reducing LLM hallucinations. The RNNT loss trains the speech encoder and prefix embeddings jointly to align speech features with text predictions, providing richer context to the LLM.

### Mechanism 2
Language ID (langID) soft prompting improves frozen LLM performance by conditioning prefixes on language-specific embeddings. langID embeddings select language-specific soft prompts that condition the LLM's predictions, stabilizing multilingual ASR.

### Mechanism 3
Combining speech prefix-tuning with langID soft prompting is additive, bridging the gap between frozen and fine-tuned LLM performance. Speech prefix-tuning enhances speech feature learning while langID soft prompting provides language context, together improving frozen LLM performance to near fine-tuned levels.

## Foundational Learning

- **RNN Transducer (RNNT) Loss**: Provides robust alignment between speech and text for ASR; how does RNNT loss differ from CTC loss in handling speech-to-text alignment?
- **Prefix-Tuning**: Optimizes only prefix embeddings for lightweight adaptation; what are the advantages of prefix-tuning over full fine-tuning in terms of computational efficiency?
- **Language ID (langID) Embeddings**: Conditions LLM on language-specific information for multilingual ASR; how do langID embeddings influence the selection of soft prompts?

## Architecture Onboarding

- **Component map**: Speech Encoder -> LLM -> RNNT Decoder -> Prefix Embeddings -> langID Embeddings
- **Critical path**: 1) Input speech sequence encoded by USM, 2) Encoded speech fed as prefix to LLM, 3) LLM processes prefix and generates text predictions, 4) RNNT decoder combines LLM outputs with previous predictions, 5) RNNT loss optimizes speech encoder, prefix embeddings, and optionally LLM
- **Design tradeoffs**: Frozen vs. Fine-tuned LLM (cost vs. performance), RNNT vs. CTC Loss (complexity vs. alignment quality)
- **Failure signatures**: High insertion rates (insufficient speech context), code-switching errors (inadequate language conditioning), degradation in multilingual settings (langID or prefix-tuning limitations)
- **First 3 experiments**: 1) Evaluate baseline PrefixLM with frozen LLM, 2) Implement speech prefix-tuning with RNNT loss, 3) Introduce langID soft prompting to frozen LLM

## Open Questions the Paper Calls Out

### Open Question 1
Does RNNT loss for speech prefix-tuning scale effectively to larger LLM architectures (e.g., LLaMA-65B, GPT-3)? The paper tests with 128M and 500M parameter LLMs but does not explore larger models.

### Open Question 2
How does the effectiveness of RNNT loss for speech prefix-tuning compare to other ASR losses (e.g., CTC, MMI) in terms of WER reduction and hallucination control? The paper compares RNNT to CTC but does not explore other losses.

### Open Question 3
Can langID soft prompting be extended to multi-script or code-mixed languages without explicit langID supervision? The paper uses explicit langID embeddings but does not address scenarios where langID is unavailable.

### Open Question 4
What is the impact of RNNT-based speech prefix-tuning on low-resource languages where labeled data is scarce? The paper focuses on 10 Indic languages with substantial training data but does not address low-resource scenarios.

## Limitations
- Exact implementation details of joint loss function and RNNT integration not fully specified
- Procedure for creating and incorporating language ID embeddings is unclear
- Study limited to 10 Indic languages, raising questions about generalizability

## Confidence
- **High Confidence**: RNNT loss effectiveness for speech prefix optimization, WER improvements over fine-tuned LLM baseline
- **Medium Confidence**: Additive benefits of combining speech prefix-tuning and langID soft prompting, improvements with frozen LLM
- **Low Confidence**: Specific mechanisms of langID soft prompting impact, generalizability to non-Indic languages

## Next Checks
1. Conduct ablation studies to isolate contributions of RNNT loss and langID soft prompting
2. Evaluate proposed method on diverse languages with different linguistic properties
3. Perform detailed error analysis comparing proposed approach with strong baselines