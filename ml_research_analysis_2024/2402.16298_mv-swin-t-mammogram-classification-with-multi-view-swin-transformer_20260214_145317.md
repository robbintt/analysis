---
ver: rpa2
title: 'MV-Swin-T: Mammogram Classification with Multi-view Swin Transformer'
arxiv_id: '2402.16298'
source_url: https://arxiv.org/abs/2402.16298
tags:
- views
- multi-view
- attention
- image
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a transformer-based multi-view network (MV-Swin-T)
  for mammogram classification that processes ipsilateral views together to capture
  inter-view correlations lost in single-view approaches. The core method is a novel
  Multi-headed Dynamic Attention (MDA) block that performs self and cross-view attention
  within local windows, enabling coherent information transfer between CC and MLO
  views at the spatial feature map level.
---

# MV-Swin-T: Mammogram Classification with Multi-view Swin Transformer

## Quick Facts
- arXiv ID: 2402.16298
- Source URL: https://arxiv.org/abs/2402.16298
- Authors: Sushmita Sarker; Prithul Sarker; George Bebis; Alireza Tavakkoli
- Reference count: 0
- One-line primary result: MV-Swin-T achieves 71.37% AUC and 68.63% accuracy on CBIS-DDSM (mass only) and 96.08% AUC and 95.50% accuracy on VinDr-Mammo, outperforming single-view Swin Transformer baselines

## Executive Summary
This paper proposes MV-Swin-T, a transformer-based multi-view network for mammogram classification that processes ipsilateral CC and MLO views together to capture inter-view correlations. The core innovation is a Multi-headed Dynamic Attention (MDA) block that performs self and cross-view attention within local windows, enabling coherent information transfer between views at the spatial feature map level. Experiments demonstrate that early fusion after the second stage, combined with the Omni-Attention transformer blocks, significantly outperforms single-view approaches on two benchmark datasets.

## Method Summary
MV-Swin-T is a transformer-based architecture that processes ipsilateral CC and MLO mammogram views using Omni-Attention blocks containing Multi-headed Dynamic Attention (MDA) modules. The MDA blocks perform self-attention within each view and cross-view attention between views within local windows, enabling dynamic feature weighting and cross-view information transfer. The network fuses the two views after the second stage via concatenation, then processes the fused features through standard Swin Transformer blocks. The model is trained with Adam optimizer (learning rate 0.0001, weight decay 0.001) using binary cross-entropy loss, with early stopping and learning rate scheduling.

## Key Results
- MV-Swin-T achieves 71.37% AUC and 68.63% accuracy on CBIS-DDSM (mass only)
- MV-Swin-T achieves 96.08% AUC and 95.50% accuracy on VinDr-Mammo
- Outperforms single-view Swin Transformer baselines on both datasets
- Early fusion after stage 2 yields optimal performance compared to other fusion timings

## Why This Works (Mechanism)

### Mechanism 1
The Multi-headed Dynamic Attention (MDA) block enables coherent cross-view information transfer by performing self and cross-view attention within local windows, allowing the model to dynamically focus on relevant spatial regions across CC and MLO views. Within each window, query, key, and value matrices are computed for both views. For self-attention, all matrices come from the same view; for cross-view attention, the key matrix comes from the other view while query and value come from the original view. The resulting attention maps are concatenated, preserving spatial correspondence.

### Mechanism 2
Early fusion after the second stage (rather than late fusion) improves contextual understanding while reducing model complexity and preventing overfitting. By concatenating CC and MLO features after the second stage and passing them through a fully connected layer, the network benefits from multi-view context before deeper stages, reducing parameter count compared to fusing at every stage.

### Mechanism 3
The shifted window multi-head dynamic attention (SW-MDA) introduces cross-window connections that capture long-range dependencies between adjacent spatial regions, overcoming the local-only limitation of fixed windows. After processing with regular windows (W-MDA), the SW-MDA shifts the window partitioning by half a window size, allowing features in adjacent windows to interact and build broader spatial context.

## Foundational Learning

- **Concept:** Swin Transformer architecture and shifted window mechanism
  - Why needed here: MV-Swin-T builds directly on Swin Transformer, replacing MSA modules with MDA modules while preserving the hierarchical shifted window structure.
  - Quick check question: How does the shifted window mechanism in Swin Transformer enable cross-window connections without increasing computational cost?

- **Concept:** Multi-view medical image analysis and clinical workflow
  - Why needed here: Understanding that radiologists analyze ipsilateral views together for tumor detection explains why multi-view fusion is clinically motivated and why the model focuses on CC and MLO views of the same breast.
  - Quick check question: Why does the paper focus on ipsilateral views rather than bilateral views for breast cancer classification?

- **Concept:** Attention mechanisms and dynamic feature weighting
  - Why needed here: The MDA block uses attention to dynamically weight features from both views, allowing the model to emphasize relevant regions for cancer detection rather than treating all locations equally.
  - Quick check question: In the MDA block, what is the difference between self-attention and cross-view attention in terms of how query, key, and value matrices are sourced?

## Architecture Onboarding

- **Component map:** Input (CC and MLO views) → Stage 1-2 (W-MDA and SW-MDA modules) → Concatenate + FC → Stage 3-4 (Swin blocks) → Classification

- **Critical path:** W-MDA → SW-MDA → Concatenate → FC → Swin blocks → Classification

- **Design tradeoffs:**
  - Early fusion reduces parameters but may lose view-specific details
  - 384x384 image size improves boundary detection but increases computation
  - Concatenation vs weighted addition for attention fusion: concatenation performed better but increases feature dimensionality

- **Failure signatures:**
  - Overfitting with small datasets → try earlier fusion or stronger regularization
  - Misaligned cross-view attention → check input registration quality
  - Poor cross-window connections → verify shifted window partitioning is correct

- **First 3 experiments:**
  1. Ablation: Replace MDA blocks with standard Swin MSA blocks to confirm benefit of cross-view attention
  2. Fusion timing: Test fusion after stage 1, 2, and 3 to validate stage 2 is optimal
  3. Attention fusion method: Compare concatenation vs weighted addition with various weight values

## Open Questions the Paper Calls Out

### Open Question 1
How does the proposed MV-Swin-T architecture scale to larger datasets beyond CBIS-DDSM and VinDr-Mammo, and what impact does this have on generalization and performance? The authors recognize the need to explore scalability but have only tested the model on two datasets, limiting understanding of generalization capabilities.

### Open Question 2
What is the impact of using different image sizes (e.g., 224×224 vs. 384×384) on the performance of the MV-Swin-T architecture, and how does this affect the model's ability to capture mass boundaries? While the authors have experimented with different image sizes, they have not provided a detailed analysis of how image size affects performance or boundary detection ability.

### Open Question 3
How does the proposed MV-Swin-T architecture compare to other transformer-based models for multi-view mammogram analysis, such as those using cross-view transformers or hybrid models combining transformers and CNNs? The paper mentions that transformer application in multi-view mammogram analysis remains relatively uncharted territory, suggesting a lack of comparison with other transformer-based approaches.

## Limitations
- Limited external validation beyond two specific datasets (CBIS-DDSM and VinDr-Mammo)
- Assumes sufficient spatial alignment between CC and MLO views, which may not hold in clinical practice
- Early fusion strategy after stage 2 lacks theoretical justification for why this specific timing is superior

## Confidence

- **High confidence:** Swin Transformer baseline comparisons, implementation of shifted window mechanism
- **Medium confidence:** Multi-view fusion benefits (limited to specific datasets and view types)
- **Low confidence:** Generalization to other imaging modalities or view combinations (bilateral vs ipsilateral)

## Next Checks

1. Test MV-Swin-T on additional mammogram datasets (e.g., INbreast, DDSM) to assess generalization across different acquisition protocols and populations
2. Evaluate model performance when CC and MLO views are artificially misaligned to quantify robustness to registration errors
3. Compare MV-Swin-T against traditional CNN-based multi-view approaches (e.g., ResNeXt with early fusion) to isolate the benefit of transformer architecture versus multi-view fusion itself