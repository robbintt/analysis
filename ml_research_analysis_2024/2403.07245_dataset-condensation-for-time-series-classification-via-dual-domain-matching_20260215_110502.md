---
ver: rpa2
title: Dataset Condensation for Time Series Classification via Dual Domain Matching
arxiv_id: '2403.07245'
source_url: https://arxiv.org/abs/2403.07245
tags:
- data
- dataset
- time
- training
- domain
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes CondTSC, a novel framework for time series dataset
  condensation that leverages both time and frequency domains. The key idea is to
  generate a condensed synthetic dataset by matching surrogate objectives across multiple
  views and domains.
---

# Dataset Condensation for Time Series Classification via Dual Domain Matching

## Quick Facts
- **arXiv ID**: 2403.07245
- **Source URL**: https://arxiv.org/abs/2403.07245
- **Reference count**: 40
- **Primary result**: Proposed CondTSC framework achieves state-of-the-art performance in time series dataset condensation, with 61.38% accuracy on HAR dataset using only 0.1% of original data size.

## Executive Summary
This paper introduces CondTSC, a novel framework for time series dataset condensation that leverages both time and frequency domains. The key innovation is matching surrogate objectives across multiple views and domains to generate a condensed synthetic dataset. By employing multi-view data augmentation in the frequency domain, dual domain training, and dual objectives matching, CondTSC significantly outperforms existing baselines in classification accuracy when training on condensed data. The framework demonstrates that synthetic datasets can preserve essential patterns for time series classification while achieving substantial data compression.

## Method Summary
CondTSC generates condensed synthetic datasets through a three-module framework: (1) Multi-view Data Augmentation using frequency-domain augmentations (low-pass filtering, Fourier transform magnitude/phase perturbations) to create multiple views of the synthetic data; (2) Dual Domain Training that trains separate networks on both time and frequency representations for N iterations on synthetic data and M iterations on real data; (3) Dual Objectives Matching that aligns gradients and embedding distributions between synthetic and real data across both domains. The framework initializes synthetic data using K-means centroids of real data classes and updates the synthetic dataset through gradient-based optimization to match surrogate objectives in both domains.

## Key Results
- CondTSC achieves 61.38% accuracy on HAR dataset with just 0.1% of original data size (compared to 93.14% with full data)
- Outperforms existing baselines including random selection, K-means, and clustering-based methods across five benchmark datasets
- Condensed datasets demonstrate good generalization properties and conform to original data distribution
- Maintains competitive performance across different network architectures (CNN, ResNet, TCN, LSTM, Transformer, GRU)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Matching surrogate objectives in both time and frequency domains preserves essential information for time series classification.
- Mechanism: The framework aligns training dynamics of synthetic and real data by minimizing differences in gradients and embedding distributions across dual domains.
- Core assumption: Time series data contains critical patterns in both domains, and matching them captures these patterns.
- Evidence anchors:
  - [abstract]: "our proposed framework aims to generate a condensed dataset that matches the surrogate objectives in both the time and frequency domains."
  - [section 4.4]: "By matching these surrogate objectives, the losses computed across all views can be back-propagated to update the synthetic dataset S accordingly."
  - [corpus]: Weak evidence. Corpus focuses on graph condensation and multi-modal settings, not directly on dual-domain time series matching.
- Break condition: If the frequency domain does not contain separable patterns, the dual-domain matching would not improve performance.

### Mechanism 2
- Claim: Multi-view data augmentation in the frequency domain enriches the synthetic dataset and strengthens surrogate objective matching.
- Mechanism: Sequential augmentations (LPF, FTPP, FTMP) project synthetic data into multiple frequency-enhanced spaces, increasing robustness and representativeness.
- Core assumption: Frequency-domain augmentations squeeze class boundaries, making synthetic data more effective.
- Evidence anchors:
  - [abstract]: "CondTSC employs multi-view data augmentation in the frequency domain..."
  - [section 4.2]: "This approach enriches data samples and strengthens the matching process of surrogate objectives, resulting in synthetic data that is more robust and representative."
  - [corpus]: Weak evidence. Corpus mentions augmentations for graphs and images, but not specifically for time series in frequency domain.
- Break condition: If augmentations introduce noise that obscures real patterns, performance degrades.

### Mechanism 3
- Claim: Initializing synthetic data with K-means centroids of real data classes improves condensation performance.
- Mechanism: Using class centroids provides a structured starting point rather than random initialization, which is ineffective for time series.
- Core assumption: Class centroids capture representative time series patterns better than random samples.
- Evidence anchors:
  - [section 4.1]: "empirical evidence suggests that random initialization results in poor performance when condensing time-series datasets."
  - [table 5]: Shows that K-means initialization leads to better final performance than random initialization.
  - [corpus]: Weak evidence. No corpus papers discuss initialization strategies for time series condensation.
- Break condition: If centroids are not representative (e.g., in highly imbalanced datasets), initialization quality drops.

## Foundational Learning

- Concept: Fourier Transform and frequency domain analysis
  - Why needed here: Time series have periodic patterns best captured in frequency domain; essential for dual-domain matching.
  - Quick check question: What does a Fourier Transform do to a time series signal?

- Concept: Dataset condensation via gradient and embedding matching
  - Why needed here: Core technique for compressing data while preserving training dynamics; necessary for efficient synthetic data generation.
  - Quick check question: How does matching gradients help in dataset condensation?

- Concept: Multi-view data augmentation techniques
  - Why needed here: Augments data in multiple frequency-enhanced spaces to improve synthetic data robustness and matching.
  - Quick check question: Why might frequency-domain augmentations be more effective for time series than spatial augmentations?

## Architecture Onboarding

- Component map:
  - Multi-view Data Augmentation (LPF → FTPP → FTMP)
  - Dual Domain Training (time & frequency networks)
  - Dual Objectives Matching (gradient + embedding space matching)
  - Synthetic data initialization via K-means

- Critical path: Augmentation → Dual Domain Training → Dual Objectives Matching → Update synthetic data

- Design tradeoffs:
  - Augmentations increase diversity but add computational cost.
  - Dual-domain training doubles network parameters but captures richer information.
  - K-means initialization is faster than random but may miss rare patterns.

- Failure signatures:
  - Poor performance despite training → likely augmentation or initialization issue.
  - Slow convergence → may need to adjust learning rate or iteration counts.
  - Overfitting to training data → reduce augmentation or increase synthetic data size.

- First 3 experiments:
  1. Test initialization: Compare random vs K-means initialization on a small dataset.
  2. Validate augmentation: Check if frequency augmentations improve separability.
  3. Baseline comparison: Run CondTSC vs random selection on HAR dataset with small sample size.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does CondTSC perform on datasets with significantly different characteristics, such as very long time series or highly non-stationary data?
- Basis in paper: [inferred] The paper evaluates CondTSC on five benchmark datasets with relatively short time series (ranging from 96 to 5120 data points). It would be valuable to assess its performance on datasets with very long time series or highly non-stationary data.
- Why unresolved: The current experiments do not cover datasets with extreme characteristics that might challenge the assumptions or design choices of CondTSC.
- What evidence would resolve it: Testing CondTSC on a diverse range of time series datasets with varying lengths, stationarity, and other characteristics would provide insights into its robustness and generalizability.

### Open Question 2
- Question: Can the effectiveness of CondTSC be further improved by incorporating additional frequency domain information or advanced frequency analysis techniques?
- Basis in paper: [explicit] The paper highlights the importance of the frequency domain in time series analysis and proposes incorporating frequency domain information through data augmentation and dual domain training. However, it does not explore the full potential of frequency domain analysis.
- Why unresolved: The current approach uses basic frequency domain techniques, and there might be more sophisticated methods that could enhance the performance of CondTSC.
- What evidence would resolve it: Investigating the impact of incorporating advanced frequency analysis techniques, such as wavelet transforms or more complex spectral analysis, on the performance of CondTSC would provide insights into potential improvements.

### Open Question 3
- Question: How does the performance of CondTSC scale with the size of the condensed dataset, and what is the optimal trade-off between dataset size and performance?
- Basis in paper: [explicit] The paper shows that CondTSC outperforms baselines even with a very small condensed dataset (0.1% of the original size). However, it does not explore the relationship between dataset size and performance in detail.
- Why unresolved: Understanding how the performance of CondTSC scales with the size of the condensed dataset is crucial for practical applications, as it would help determine the optimal trade-off between dataset size and performance.
- What evidence would resolve it: Conducting experiments with varying sizes of condensed datasets and analyzing the performance trade-offs would provide insights into the optimal dataset size for different applications.

## Limitations
- Dual-domain matching relies heavily on assumption that frequency domain contains separable patterns complementary to time domain
- Effectiveness of frequency-domain augmentations depends on specific parameter choices not fully specified
- K-means initialization may not generalize well to highly imbalanced datasets where centroids don't represent minority classes

## Confidence
- **High confidence**: Claims about CondTSC outperforming baseline methods on standard metrics (classification accuracy) when tested on multiple benchmark datasets with controlled synthetic data sizes.
- **Medium confidence**: Claims about dual-domain matching mechanism working because it captures essential patterns in both domains - while supported by experimental results, the mechanistic explanation could benefit from more ablation studies isolating time vs frequency contributions.
- **Medium confidence**: Claims about multi-view augmentation enriching synthetic data - supported by methodology description but lacking direct comparison with single-view augmentation baselines.

## Next Checks
1. **Domain contribution isolation**: Run ablation experiments comparing CondTSC variants that match only time domain, only frequency domain, and both domains to quantify the contribution of each component.
2. **Augmentation sensitivity analysis**: Test CondTSC with different combinations of frequency augmentations disabled to determine which augmentations are critical for performance and whether they introduce robustness or merely add noise.
3. **Initialization robustness testing**: Evaluate CondTSC initialization strategies across datasets with varying class distributions (balanced vs imbalanced) to determine when K-means initialization fails and whether alternative strategies would be more effective.