---
ver: rpa2
title: 'FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in
  Semantic Dataset Deduplication'
arxiv_id: '2404.16123'
source_url: https://arxiv.org/abs/2404.16123
tags:
- fairdedup
- semdedup
- data
- dataset
- gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how deduplication affects the fairness of Vision-Language
  Pre-trained (VLP) models. The authors find that deduplication can reinforce biases
  related to gender, skin tone, and age in VLP models trained on large-scale datasets.
---

# FairDeDup: Detecting and Mitigating Vision-Language Fairness Disparities in Semantic Dataset Deduplication

## Quick Facts
- **arXiv ID**: 2404.16123
- **Source URL**: https://arxiv.org/abs/2404.16123
- **Reference count**: 40
- **Primary result**: FairDeDup improves fairness metrics in VLP models by boosting underrepresented sensitive concepts during deduplication

## Executive Summary
This paper investigates how deduplication processes in vision-language pretraining datasets can inadvertently reinforce demographic biases related to gender, skin tone, and age. The authors demonstrate that standard deduplication algorithms like SemDeDup, while reducing redundancy, can amplify existing biases by disproportionately removing samples representing underrepresented groups. To address this issue, they propose FairDeDup, a modification to SemDeDup that incorporates fairness considerations by boosting the representation of underrepresented sensitive concepts during the deduplication process. The method shows consistent improvements in fairness metrics while maintaining zero-shot performance on CLIP benchmarks.

## Method Summary
The authors propose FairDeDup as a simple modification to the SemDeDup algorithm that incorporates fairness considerations during dataset deduplication. The method works by identifying sensitive attributes (gender, skin tone, age) in the dataset and adjusting the deduplication process to boost the representation of underrepresented concepts. During the deduplication process, FairDeDup modifies the similarity scoring mechanism to penalize the removal of samples from underrepresented groups, effectively preserving more diverse representations. The approach is evaluated on the FairFace and FACET datasets, demonstrating improved fairness metrics while maintaining zero-shot performance on CLIP benchmarks.

## Key Results
- FairDeDup consistently improves fairness metrics compared to SemDeDup on FairFace and FACET datasets
- The method maintains zero-shot performance on CLIP benchmarks while improving demographic representation
- Standard deduplication was shown to reinforce existing biases related to gender, skin tone, and age in VLP models

## Why This Works (Mechanism)
FairDeDup works by modifying the deduplication scoring function to account for demographic representation. When evaluating whether to remove a duplicate sample, the algorithm considers not just semantic similarity but also the demographic attributes of the samples. If a sample belongs to an underrepresented group, the algorithm assigns a lower penalty for keeping that sample, effectively preserving more diverse representations in the dataset. This approach ensures that the deduplication process doesn't disproportionately remove samples from minority groups while still maintaining the core functionality of reducing redundancy.

## Foundational Learning
- **Vision-Language Pre-training (VLP)**: Joint training of vision and language models on paired image-text data; needed to understand the context of bias in multimodal models; quick check: models like CLIP learn visual concepts through language supervision.
- **Dataset Deduplication**: Process of removing redundant or near-duplicate samples from training data; needed to understand why and how bias can be amplified; quick check: deduplication typically uses semantic similarity to identify duplicates.
- **Sensitive Attribute Detection**: Methods for identifying demographic attributes like gender, skin tone, and age in images; needed to enable fairness-aware processing; quick check: FairFace and FACET datasets provide annotations for these attributes.
- **Fairness Metrics in Vision-Language Models**: Quantitative measures of bias in multimodal models; needed to evaluate the effectiveness of fairness interventions; quick check: metrics include equal opportunity difference and demographic parity across attribute groups.
- **Zero-shot Transfer Learning**: Ability of models to perform tasks without task-specific fine-tuning; needed to evaluate performance trade-offs; quick check: CLIP demonstrates strong zero-shot capabilities on various vision tasks.

## Architecture Onboarding

**Component Map**: Input dataset -> Attribute detection -> Similarity scoring (FairDeDup) -> Filtered dataset

**Critical Path**: The deduplication process is the critical component where fairness is enforced. FairDeDup modifies the similarity scoring between image-text pairs by incorporating demographic representation considerations, making this the key algorithmic innovation.

**Design Tradeoffs**: The main tradeoff is between maintaining dataset quality (removing true duplicates) and preserving demographic diversity. FairDeDup must balance these competing objectives by adjusting the weight given to fairness considerations versus pure semantic similarity.

**Failure Signatures**: Potential failures include overcorrection leading to retention of near-duplicates, or insufficient fairness improvement if the attribute detection is noisy. The method may also struggle with intersectional attributes or attributes not present in the detection system.

**First Experiments**: 
1. Apply FairDeDup to a standard dataset and measure changes in demographic attribute distribution
2. Evaluate zero-shot performance on CLIP benchmarks before and after FairDeDup processing
3. Compare fairness metrics (e.g., demographic parity, equal opportunity) between FairDeDup and standard SemDeDup outputs

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions beyond the general need for more comprehensive fairness evaluations across different model architectures and datasets.

## Limitations
- The study focuses primarily on CLIP-based models, limiting generalizability to other VLP architectures
- Evaluation relies on specific demographic attributes from FairFace and FACET datasets, potentially missing broader fairness issues
- The evaluation framework uses limited fairness metrics and prompt-based assessments, which may not capture all real-world deployment scenarios

## Confidence

**High confidence**: The observation that deduplication can reinforce existing biases in VLP models is well-supported by empirical evidence.

**Medium confidence**: The effectiveness of FairDeDup in improving fairness metrics is demonstrated, but generalizability across different model architectures and datasets remains uncertain.

**Medium confidence**: The zero-shot performance maintenance claim is supported, though the evaluation scope is limited to CLIP benchmarks.

## Next Checks

1. Evaluate FairDeDup across multiple VLP architectures beyond CLIP to assess generalizability of fairness improvements.

2. Test the algorithm's effectiveness on additional demographic attributes and cultural contexts not covered in FairFace and FACET datasets.

3. Conduct ablation studies to quantify the trade-offs between fairness improvements and potential performance degradation on specific downstream tasks.