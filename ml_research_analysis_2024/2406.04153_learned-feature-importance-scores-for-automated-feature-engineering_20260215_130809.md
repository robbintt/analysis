---
ver: rpa2
title: Learned Feature Importance Scores for Automated Feature Engineering
arxiv_id: '2406.04153'
source_url: https://arxiv.org/abs/2406.04153
tags: []
core_contribution: AutoMAN automates feature engineering by learning feature importance
  masks to explore a human-expert-curated transform space without explicitly manifesting
  all transform-feature combinations. It uses local masks to select features per transform
  and a global mask to select across all transformed features, optimized end-to-end
  with the downstream task objective.
---

# Learned Feature Importance Scores for Automated Feature Engineering

## Quick Facts
- arXiv ID: 2406.04153
- Source URL: https://arxiv.org/abs/2406.04153
- Reference count: 5
- Primary result: AutoMAN outperforms baselines on diverse datasets with 7.5x performance gains on smaller datasets and significantly lower latency

## Executive Summary
AutoMAN introduces a novel approach to automated feature engineering by learning feature importance masks instead of explicitly generating all possible transform-feature combinations. The method uses local masks to select relevant features for each transform and a global mask to select across all transformed features, optimizing end-to-end with the downstream task objective. AutoMAN demonstrates superior performance on diverse datasets, particularly on smaller datasets where it achieves 7.5x improvement, while maintaining significantly lower latency than alternatives. The approach extends naturally to time series data through learnable temporal masks.

## Method Summary
AutoMAN automates feature engineering through a masking-based approach that learns feature importance scores for selecting transforms. The method employs local masks for each transform function that select the most relevant input features via softmax-based attention, applies the transforms, concatenates all transformed features, then applies a global mask to select the final set of features for prediction. The entire pipeline is optimized end-to-end with the downstream task objective, avoiding the need to explicitly manifest all possible transform-feature combinations. For time series data, temporal masks are learned to select relevant temporal windows. The approach is theoretically motivated by the Stone-Weierstrass theorem, showing that a small set of functions can approximate arbitrary continuous transforms.

## Key Results
- AutoMAN achieves 7.5x performance gains on smaller datasets compared to baselines
- Demonstrates significantly lower latency than alternative automated feature engineering methods
- Outperforms competitors on diverse datasets including tabular, time series, and multi-modal data
- Shows consistent performance improvements particularly for MLP predictors

## Why This Works (Mechanism)
AutoMAN's effectiveness stems from learning which features are most relevant for each transform rather than applying all transforms to all features. The masking mechanism allows the model to discover meaningful feature combinations without the computational burden of explicitly generating every possible transform-feature pair. By optimizing masks end-to-end with the task objective, AutoMAN learns to select transforms that are genuinely useful for the specific prediction task rather than following generic heuristics.

## Foundational Learning
**Mask-based feature selection** - Why needed: Avoids combinatorial explosion of transform-feature combinations; Quick check: Verify softmax gradients flow properly through mask selection
**Stone-Weierstrass approximation** - Why needed: Provides theoretical justification for using limited transform functions; Quick check: Confirm transform space covers required function classes
**End-to-end optimization** - Why needed: Ensures transforms are learned for the specific task, not generic feature engineering; Quick check: Monitor loss convergence with and without task supervision

## Architecture Onboarding

**Component Map**
Input features -> Local masks -> Transform functions -> Concatenation -> Global mask -> Predictor MLP -> Task loss

**Critical Path**
Input features → Local mask selection → Transform application → Global mask selection → Predictor → Loss computation

**Design Tradeoffs**
- Mask-based selection vs explicit combination generation (memory vs completeness)
- Number of selected features per mask (performance vs computational cost)
- Fixed transform set vs learned transforms (simplicity vs flexibility)

**Failure Signatures**
- Vanishing gradients in mask learning (check softmax temperature and gradient flow)
- Memory explosion from large feature spaces (verify top-k selection implementation)
- Poor performance on larger datasets (feature space may already be saturated)

**First 3 Experiments**
1. Test mask learning with synthetic data where ground truth feature importance is known
2. Compare memory usage and runtime with and without masking on increasing dataset sizes
3. Validate Stone-Weierstrass approximation by testing with varying numbers of transform functions

## Open Questions the Paper Calls Out
1. How can the initial set of transform functions be optimized automatically rather than relying on human expert curation?
2. How can AutoMAN be extended to other data modalities beyond time series, such as images or text?
3. Why does the performance improvement from AutoMAN's engineered features diminish on larger datasets compared to smaller ones?
4. How does the choice of the number of features selected by the local mask affect AutoMAN's performance and scalability?
5. How does the performance of AutoMAN compare to other automated feature engineering methods when using different predictor models?

## Limitations
- Does not provide specific implementation details for transform functions and masking mechanisms
- Claims about linear scaling are not empirically validated across varying dataset sizes
- Relies on human-curated transform function sets rather than automated discovery

## Confidence
- Core method claims (learned masks for feature selection): Medium
- Empirical results showing performance improvements: High
- Theoretical motivation via Stone-Weierstrass: High
- Linear scaling claims: Low (not empirically validated)

## Next Checks
1. Implement the masking mechanism with top-k selection during training and verify gradient flow through softmax operations
2. Test memory scaling with increasing feature and transform combinations to confirm linear scaling claims
3. Compare against a simple baseline that applies all transforms to all features (baseline implementation details would be needed)