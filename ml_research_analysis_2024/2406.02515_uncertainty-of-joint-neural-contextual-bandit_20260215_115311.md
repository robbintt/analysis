---
ver: rpa2
title: Uncertainty of Joint Neural Contextual Bandit
arxiv_id: '2406.02515'
source_url: https://arxiv.org/abs/2406.02515
tags: []
core_contribution: This paper analyzes uncertainty in joint neural contextual bandit
  models for large-scale recommendation systems. It addresses the challenge of balancing
  exploration and exploitation by understanding how uncertainty depends on model architecture
  and data.
---

# Uncertainty of Joint Neural Contextual Bandit

## Quick Facts
- arXiv ID: 2406.02515
- Source URL: https://arxiv.org/abs/2406.02515
- Reference count: 15
- Primary result: Uncertainty σ in joint neural contextual bandit scales as √(F/N) where F is last hidden layer size and N is training data size

## Executive Summary
This paper analyzes uncertainty in joint neural contextual bandit models for large-scale recommendation systems. The authors derive theoretical relationships showing uncertainty is proportional to the square root of the last hidden layer size F and inversely proportional to the square root of training data size N. They validate these findings empirically using real industrial data, demonstrating that uncertainty approximately follows σ ∝ C√(F/N) where C is a heuristic scaling factor. This work provides practical guidance for hyperparameter tuning in production recommendation systems.

## Method Summary
The paper analyzes a joint neural contextual bandit model that combines an MLP preprocessing layer with a linear bandit module. The method involves deriving theoretical bounds on the uncertainty term σ based on feature distribution assumptions and approximation techniques. The uncertainty is computed from the posterior covariance in the linear bandit module, and the authors show it scales with √(F/N) where F is the MLP output dimension and N is the number of training samples. The analysis relies on assumptions about feature distributions being approximately i.i.d. normal and the MLP output variance being uniform across dimensions.

## Key Results
- Uncertainty σ is proportional to √(F/N) where F is last hidden layer size and N is training data size
- Empirical validation on real industrial data confirms theoretical scaling relationships
- Provides practical guidance for setting exploration-exploitation balance parameter α in production systems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Uncertainty decreases proportionally to the square root of the number of training data points (N).
- Mechanism: The uncertainty term σ is derived from the quadratic form f^T(A + 'I)^(-1)f, where A = XX^T is the centroid matrix. As N increases, the eigenvalue of (A + 'I)^(-1) becomes approximately 1/(N + ')', which scales the uncertainty inversely with √N.
- Core assumption: The feature vector f consists of a common centroid x shared across data points and a variation component n that is i.i.d. normal. The variation term dominates the uncertainty when N is large.
- Evidence anchors:
  - [abstract]: "Our analysis reveals that α demonstrates an approximate square root relationship with the size of the last hidden layer F and inverse square root relationship with the amount of training data N, i.e., σ∝ √(F/N)."
  - [section]: "When our training data amount N is huge such as thousands or even millions Eq. 15 approximately reduces to σ ∝ 1/√N."
  - [corpus]: No direct evidence found; the corpus papers focus on different bandit formulations (e.g., clustering, combinatorial), not the joint neural model.
- Break condition: If the variation component n is not i.i.d. or the centroid assumption breaks (e.g., highly non-stationary features), the √N scaling may fail.

### Mechanism 2
- Claim: Uncertainty increases proportionally to the square root of the last hidden layer size (F).
- Mechanism: The MLP output layer size F determines the dimensionality of the feature vector f. The quadratic form f^T(A + 'I)^(-1)f scales with F because the variation component n has F dimensions, and its norm contributes proportionally to √F to the uncertainty.
- Core assumption: The variation n is distributed across F independent dimensions, and the centroid component is normalized (||x||^2 = 1). The matrix (A + 'I)^(-1) is approximately diagonal in the variation subspace.
- Evidence anchors:
  - [abstract]: "σ demonstrates an approximate square root relationship with the size of the last hidden layer F... i.e., σ∝ √(F/N)."
  - [section]: "Further, the argument vector f has shape F, and as a result its corresponding quadratic form f^T(A + 'I)^(-1)f in nature is proportional to the magnitude F. That is, the σ^2 is approximately proportional to the MLP output layer size: σ ∝ √F."
  - [corpus]: No direct evidence found; corpus neighbors do not analyze the effect of hidden layer size on uncertainty scaling.
- Break condition: If the feature dimensions are highly correlated or the variation is not uniformly distributed across dimensions, the √F scaling may not hold.

### Mechanism 3
- Claim: The uncertainty term σ is approximately proportional to C√(F/N), where C is a heuristic scaling factor.
- Mechanism: Combining the √F growth from the feature dimension and the 1/√N decay from the data size, the overall uncertainty scales as √(F/N). The constant C captures the variance of the MLP output distribution, which is not analytically tractable due to the non-linear transformations.
- Core assumption: The MLP is well-trained and the last hidden layer outputs are approximately i.i.d. normal. The constant C can be estimated empirically from offline data.
- Evidence anchors:
  - [abstract]: "Our analysis reveals that α demonstrates an approximate square root relationship with the size of the last hidden layer F and inverse square root relationship with the amount of training data N, i.e., σ∝ √(F/N)."
  - [section]: "Due to the fact that the MLP as a deep learning module not being transparent, we do not have variance information of the distribution of the MLP output. In practice, we can introduce a heuristic scaling hyper-parameter C and get σ ∝ C√(F/N)."
  - [corpus]: No direct evidence found; corpus neighbors do not address the scaling of uncertainty in joint neural bandits.
- Break condition: If the MLP output distribution is highly non-normal or the scaling factor C is not stable across different data regimes, the approximation may break.

## Foundational Learning

- Concept: Contextual bandit algorithms and the exploration-exploitation tradeoff.
  - Why needed here: The joint neural contextual bandit model outputs both a predicted reward µ and an uncertainty σ, which are combined as µ + ασ to balance exploration and exploitation. Understanding how α interacts with σ is critical for hyperparameter tuning.
  - Quick check question: What is the role of the hyperparameter α in the score µ + ασ, and how does it affect the exploration-exploitation balance?

- Concept: Linear Bayesian bandits and posterior inference.
  - Why needed here: The joint neural model uses a linear bandit module on top of the MLP features, with posterior inference yielding µw and ㉧w. Deriving the uncertainty requires understanding the posterior covariance structure.
  - Quick check question: How is the posterior covariance ㉧w computed in a Bayesian linear bandit, and why does it depend on the feature matrix F and regularization?

- Concept: Central limit theorem and feature distribution assumptions.
  - Why needed here: The paper assumes the MLP output features are approximately i.i.d. normal due to the central limit theorem, which justifies the analytical derivations of the uncertainty scaling.
  - Quick check question: Under what conditions does the central limit theorem apply to the outputs of a deep neural network, and why is this assumption critical for the analysis?

## Architecture Onboarding

- Component map: MLP module -> Linear bandit module -> Training loop -> Serving layer
- Critical path: 1. Forward pass through MLP to get f. 2. Compute µ = f^T µw and σ = √(f^T ㉧w f). 3. Combine into score µ + ασ. 4. Select top items by score.
- Design tradeoffs:
  - Joint vs. disjoint: Joint model trains one MLP for all items, avoiding per-item matrices but requiring careful uncertainty scaling.
  - Exploration strategy: α controls exploration; setting it too high leads to random recommendations, too low leads to premature convergence.
  - Feature size F: Larger F increases model capacity but also increases σ ∝ √F, requiring smaller α.
- Failure signatures:
  - High variance in σ across similar items: May indicate unstable feature extraction or insufficient regularization.
  - σ not decreasing with more data: Could mean the variation component n is too large or the centroid assumption is invalid.
  - Poor exploration-exploitation balance: α not tuned to the actual scale of σ; may need to adjust C or F.
- First 3 experiments:
  1. Vary F (32, 64, 128, 256) and plot σ vs. √F to verify the scaling relationship.
  2. Vary N (log scale) and plot σ vs. 1/√N to verify the inverse scaling.
  3. Sweep α over a range and measure cumulative regret or click-through rate to find the optimal balance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the variance of the MLP output layer's feature distribution affect the uncertainty scaling factor C in ‡ ≈ C√(F/N)?
- Basis in paper: [inferred] The paper mentions that due to the MLP being a "black box," the variance information of the MLP output distribution is unknown, and a heuristic scaling factor C is introduced.
- Why unresolved: The paper does not provide a method to estimate or derive the value of C from the model architecture or training data.
- What evidence would resolve it: Empirical studies varying the MLP architecture (number of layers, activation functions, etc.) and measuring the resulting uncertainty to establish a relationship with C.

### Open Question 2
- Question: How does the assumption of uniform variance across all dimensions of the MLP output (i.e., ‡1 = ... = ‡F = σ) affect the accuracy of the uncertainty estimation?
- Basis in paper: [explicit] The paper states, "Without loss of generality and for simplicity, our analysis assumes the variance in each MLP output dimension is uniform."
- Why unresolved: The paper acknowledges this assumption but does not explore how relaxing it might impact the uncertainty formula or its relationship with F and N.
- What evidence would resolve it: Experiments comparing the uncertainty behavior under different variance assumptions (uniform vs. non-uniform) across MLP output dimensions.

### Open Question 3
- Question: What is the impact of feature correlation (non-identical columns in F) on the uncertainty estimation, and how can this be accounted for in the formula?
- Basis in paper: [explicit] The paper notes, "Note we cannot directly reuse the logic in Eq. 11 ≥ Eq. 15 to conclude that ‡ is only related to N but unrelated to F, because F does not have identical columns (while X does)."
- Why unresolved: The paper explains why the simple case (f = x) doesn't apply but doesn't provide a complete derivation for the general case with correlated features.
- What evidence would resolve it: Mathematical analysis or experiments quantifying the effect of feature correlation on the uncertainty term and proposing a modified formula.

## Limitations
- The theoretical analysis relies on simplifying assumptions about feature distributions and the non-transparent nature of deep neural networks.
- Empirical validation is limited to specific industrial datasets without comprehensive ablation studies across diverse recommendation domains.
- The analysis assumes stationary feature distributions, potentially limiting applicability to dynamic recommendation environments.

## Confidence
- Theoretical scaling relationships (σ ∝ √F/√N): Medium confidence
- Empirical validation on industrial data: Medium confidence
- Practical applicability across recommendation domains: Low confidence

## Next Checks
1. Conduct controlled experiments varying both F and N systematically to verify the combined scaling relationship σ ∝ √F/√N across multiple recommendation datasets.
2. Perform sensitivity analysis on the regularization parameter ε to understand its impact on the uncertainty scaling and identify optimal settings for different data regimes.
3. Test the framework's robustness to non-stationary features by introducing concept drift in the training data and measuring the stability of the uncertainty estimates.