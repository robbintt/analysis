---
ver: rpa2
title: Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image
  Generation
arxiv_id: '2403.07605'
source_url: https://arxiv.org/abs/2403.07605
tags:
- prompts
- negative
- image
- negopt
- prompt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors propose NegOpt, a method for optimizing negative prompts
  in text-to-image generation. They address the challenge of manual and tedious negative
  prompt creation by fine-tuning a sequence-to-sequence model and applying reinforcement
  learning to maximize a reward function based on image quality metrics.
---

# Optimizing Negative Prompts for Enhanced Aesthetics and Fidelity in Text-To-Image Generation

## Quick Facts
- **arXiv ID**: 2403.07605
- **Source URL**: https://arxiv.org/abs/2403.07605
- **Reference count**: 5
- **Key outcome**: NegOpt achieves 25% improvement in Inception Score compared to baselines

## Executive Summary
This paper introduces NegOpt, a method for optimizing negative prompts in text-to-image generation. The authors address the challenge of manual and tedious negative prompt creation by developing a two-phase approach combining supervised fine-tuning (SFT) on a novel dataset called Negative Prompts DB with reinforcement learning (RL). NegOpt fine-tunes a sequence-to-sequence model to generate negative prompts that maximize a reward function based on image quality metrics including aesthetics, fidelity, and alignment. The method demonstrates significant improvements over baselines while maintaining the balance between different quality metrics.

## Method Summary
NegOpt employs a two-phase approach: first, a T5 sequence-to-sequence model is fine-tuned on the Negative Prompts DB dataset using supervised learning to learn the mapping between normal prompts and their corresponding negative prompts. Second, the model is further optimized using reinforcement learning with the PPO algorithm to maximize a weighted reward function combining aesthetics, fidelity, and alignment scores. The reward function is defined as r(p,i) = α·saesthetics(i) + β·salignment(p,i) + γ·sfidelity(i), with weights set to prioritize aesthetics. The method is evaluated using Stable Diffusion as the image generator and various quality metrics including Inception Score, CLIP Score, and Aesthetics Score.

## Key Results
- NegOpt achieves a 25% improvement in Inception Score compared to baseline approaches
- The method learns better negative prompts than ground truth, surpassing human-generated prompts in the test set
- NegOpt improves aesthetics and fidelity without sacrificing alignment between prompts and generated images

## Why This Works (Mechanism)

### Mechanism 1
The combined SFT+RL approach improves both fidelity (Inception Score) and aesthetics while maintaining alignment. SFT provides a strong initial negative prompt generation capability by learning from curated negative prompt pairs, then RL fine-tunes the model to maximize a reward function that explicitly weights aesthetics, fidelity, and alignment according to the desired priorities. Core assumption: The reward function coefficients (α=5, β=1, γ=1) correctly reflect the relative importance of aesthetics, alignment, and fidelity for the task.

### Mechanism 2
NegOpt learns better negative prompts than ground truth because it can leverage patterns in the training data more efficiently than humans. Humans creating negative prompts are essentially performing manual optimization, which is limited by cognitive constraints and individual biases. NegOpt, through machine learning, can identify and exploit subtle patterns across a large dataset that humans might miss. Core assumption: The training data contains generalizable patterns that can be learned to improve negative prompt generation beyond what individual human creators could achieve.

### Mechanism 3
The seq2seq model architecture is effective for negative prompt generation because it can capture the relationship between normal prompts and their corresponding negative prompts. The seq2seq model learns to map input normal prompts to output negative prompts by identifying patterns in the training data where certain prompt characteristics correspond to desirable or undesirable image qualities. Core assumption: There is a learnable mapping between normal prompts and effective negative prompts that can be captured by a seq2seq model.

## Foundational Learning

- **Reinforcement Learning and Policy Gradient Methods**: RL is used to fine-tune the seq2seq model to maximize the reward function based on image quality metrics, allowing for targeted optimization of specific metrics. Quick check: What is the difference between policy gradient methods and value-based methods in reinforcement learning?

- **Sequence-to-Sequence Models and Fine-tuning**: The seq2seq model is the core architecture for generating negative prompts, and fine-tuning it on the Negative Prompts DB dataset is essential for its effectiveness. Quick check: What are the advantages and disadvantages of using a pre-trained seq2seq model versus training one from scratch for this task?

- **Image Quality Metrics (Inception Score, CLIP Score, Aesthetics Score)**: These metrics are used to evaluate the performance of the model and define the reward function for RL, so understanding their strengths and limitations is crucial. Quick check: How does the Inception Score differ from the Fréchet Inception Distance (FID) in evaluating image generation quality?

## Architecture Onboarding

- **Component map**: Negative Prompts DB -> T5 seq2seq model -> Stable Diffusion -> CLIP model -> Aesthetics predictor -> Reward function -> PPO
- **Critical path**: 1. Fine-tune T5 on Negative Prompts DB (SFT phase) 2. Generate negative prompts for SFT test split 3. Generate images using Stable Diffusion with normal and negative prompts 4. Calculate reward based on image quality metrics 5. Fine-tune T5 using PPO to maximize reward (RL phase) 6. Evaluate final model on test set using Inception Score, CLIP Score, and Aesthetics Score
- **Design tradeoffs**: SFT-only vs. SFT+RL: SFT-only provides a strong initial model but lacks the ability to fine-tune for specific metrics, while SFT+RL allows for targeted optimization but requires more computational resources and careful tuning of reward function coefficients. Reward function weighting: The choice of α, β, and γ coefficients determines the relative importance of aesthetics, alignment, and fidelity, which can significantly impact the final model performance. Dataset size and quality: The effectiveness of the model depends on the size and quality of the Negative Prompts DB dataset, with larger and more diverse datasets generally leading to better performance.
- **Failure signatures**: Low Inception Score: Indicates poor image quality or lack of diversity in generated images. Low CLIP Score: Indicates poor alignment between prompts and generated images. Low Aesthetics Score: Indicates unappealing or low-quality images according to human preferences. High variance in metrics across different seeds: Indicates instability or lack of robustness in the model.
- **First 3 experiments**: 1. Train SFT-only model and evaluate on test set to establish baseline performance 2. Train SFT+RL model with different reward function coefficient combinations to find optimal weighting 3. Evaluate the final SFT+RL model on a held-out validation set to assess generalization and compare to baselines

## Open Questions the Paper Calls Out

### Open Question 1
How does the relative weighting of α, β, and γ in the reward function affect the balance between aesthetics, alignment, and fidelity? Basis: The paper states "we prioritize aesthetics by setting α = 5, β = 1, and γ = 1" and mentions "we can preferentially optimize the metrics most important to us." Unresolved because the paper only reports results for one specific weighting and does not explore how different weightings affect the outcomes. Evidence: A systematic study varying α, β, and γ values and reporting the resulting performance on all three metrics would show how the balance shifts with different weightings.

### Open Question 2
Can NegOpt be extended to optimize for additional image quality metrics beyond aesthetics, fidelity, and alignment? Basis: The paper mentions that "with NegOpt we can preferentially optimize the metrics most important to us" and that the reward function is a weighted sum of three terms, suggesting a modular design that could accommodate more metrics. Unresolved because the paper only implements and evaluates three specific metrics. Evidence: Demonstrating the extension of NegOpt to include additional metrics (e.g., color harmony, composition, or specific style attributes) and showing improved performance on these new metrics would validate the framework's extensibility.

### Open Question 3
How does NegOpt perform when applied to different text-to-image generation models beyond Stable Diffusion? Basis: The paper states "We use T5 as our seq2seq language model and Stable Diffusion as our image generator" and "we run Stable Diffusion for 25 steps with a guidance scale of 7.5." Unresolved because the paper only evaluates NegOpt with Stable Diffusion and T5. Evidence: Applying NegOpt to other popular text-to-image models (e.g., DALL-E, Midjourney) and reporting performance metrics would demonstrate the method's generalizability across different generation architectures.

### Open Question 4
What is the impact of dataset size and quality on the performance of NegOpt? Basis: The paper mentions constructing "Negative Prompts DB, a publicly available dataset of negative prompts" and using subsets of this dataset for SFT and RL training. Unresolved because while the paper reports results using a specific dataset size, it does not explore how varying the size or quality of the training data affects NegOpt's performance. Evidence: Conducting experiments with different dataset sizes and comparing performance would show how data quantity impacts results. Additionally, comparing results using datasets of varying quality would demonstrate the importance of data quality.

## Limitations
- The effectiveness of the SFT+RL approach compared to SFT-only is not directly demonstrated as the authors only report SFT+RL results
- The choice of reward function coefficients (α=5, β=1, γ=1) is not justified or explored systematically
- The dataset creation process relies on user-generated content from Playground AI, which may introduce biases or quality variations

## Confidence

- **High confidence**: The claim that NegOpt achieves a 25% improvement in Inception Score compared to baselines is supported by direct experimental results reported in the paper.
- **Medium confidence**: The claim that NegOpt learns better negative prompts than ground truth is supported by quantitative metrics, but the mechanism (that the model can leverage patterns in training data more efficiently than humans) is not directly validated.
- **Low confidence**: The claim that the seq2seq model architecture is particularly effective for negative prompt generation, as the paper does not compare to other architectures or provide ablation studies.

## Next Checks

1. **Ablation Study**: Conduct experiments comparing SFT-only vs. SFT+RL models to quantify the contribution of the RL phase to overall performance improvements.
2. **Reward Function Sensitivity**: Systematically explore different combinations of α, β, and γ coefficients to determine their impact on the balance between aesthetics, fidelity, and alignment.
3. **Dataset Quality Analysis**: Perform a detailed analysis of the Negative Prompts DB dataset to identify potential biases or quality issues that could affect model performance, including manual inspection of a random sample of prompt pairs.