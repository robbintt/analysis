---
ver: rpa2
title: 'Improving Adversarial Data Collection by Supporting Annotators: Lessons from
  GAHD, a German Hate Speech Dataset'
arxiv_id: '2403.19559'
source_url: https://arxiv.org/abs/2403.19559
tags:
- hate
- speech
- examples
- annotators
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors introduce GAHD, a German adversarial hate speech dataset
  created through dynamic adversarial data collection. They propose supporting annotators
  with diverse strategies (translated examples, newspaper sentences, contrastive examples)
  to create more effective adversarial examples efficiently.
---

# Improving Adversarial Data Collection by Supporting Annotators: Lessons from GAHD, a German Hate Speech Dataset

## Quick Facts
- **arXiv ID**: 2403.19559
- **Source URL**: https://arxiv.org/abs/2403.19559
- **Reference count**: 40
- **Primary result**: German adversarial hate speech dataset (GAHD) improves model robustness by 18-20 percentage points

## Executive Summary
This paper introduces GAHD, a German hate speech dataset created through dynamic adversarial data collection (DADC). The authors propose supporting annotators with diverse strategies—translated examples, newspaper sentences, and contrastive examples—to efficiently create effective adversarial examples. The resulting dataset contains ~11k examples with 42.4% labeled as hate speech and demonstrates significant improvements in hate speech detection model robustness across both in-domain and out-of-domain evaluations.

## Method Summary
The authors conducted four rounds of dynamic adversarial data collection with varying annotator support strategies. They began with existing German hate speech datasets to train an initial target model (gelectra-large), then iteratively collected adversarial examples through the Dynabench platform. Each round employed a different support method: free creation (R1), translated examples (R2), newspaper validation (R3), and contrastive examples (R4). Examples were validated by annotators and disagreements resolved by experts before being added to the training data.

## Key Results
- GAHD improves hate speech detection model robustness by 18-20 percentage points on macro F1
- Mixing multiple annotator support strategies yields the most effective dataset
- State-of-the-art models still struggle on GAHD, demonstrating its effectiveness
- 42.4% of GAHD examples are labeled as hate speech

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic adversarial data collection (DADC) improves model robustness by forcing the model to learn from examples it misclassifies.
- Mechanism: Annotators create examples that trick the current target model. When these examples are added to training data and the model is retrained, it learns to handle previously unseen patterns and edge cases.
- Core assumption: The model's prediction errors reveal its decision boundaries and weaknesses.
- Evidence anchors:
  - [abstract] "Evaluat-ing the target model after each round demonstrates large improvements in model robustness, with almost 20 percentage point increases in macro F1 on the GAHD test split (in-domain), and German HateCheck test suite (out-of-domain)"
  - [section 4.1] "we train gelectra-large on the web-sourced datasets from Section 3.1, and add the training splits of each round incrementally. We use macro F1 to measure performance."

### Mechanism 2
- Claim: Supporting annotators with multiple strategies leads to more diverse and effective adversarial examples.
- Mechanism: Different support methods (free creation, translated examples, newspaper validation, contrastive examples) encourage annotators to explore various ways to trick the model, increasing diversity.
- Core assumption: Annotators have limited creativity and benefit from external inspiration and validation.
- Evidence anchors:
  - [abstract] "we use a new strategy in each round to support annotators in finding diverse adversarial examples, in a time-efficient manner"
  - [section 3] "For R2, we provide the annotators with English-to-German translated adversarial examples as candidates to validate or reject, and as a way to inspire new, derived examples."

### Mechanism 3
- Claim: Mixing multiple rounds with different data collection strategies is more effective than using a single round.
- Mechanism: Each round contributes unique examples that target different model weaknesses. Combining them creates a more comprehensive training set.
- Core assumption: Different support methods lead to examples that are complementary in their difficulty and coverage.
- Evidence anchors:
  - [section 4.2] "we observe that the manually created examples from R1 and R4 have more positive effects on performance than the collected and validated examples from R2 and R3. Examples from these two rounds have mixed effects when used in isolation from the other rounds. However, combining data from all four rounds yields by far the best results"
  - [section 4.1] "Finally, including all GAHD rounds in the training ("R4") leads to an increase of 18 to 20 percentage points on GAHD and HateCheck."

## Foundational Learning

- **Concept: Dynamic Adversarial Data Collection (DADC)**
  - Why needed here: DADC is the core methodology for creating adversarial examples that improve model robustness.
  - Quick check question: What is the difference between DADC and static adversarial data collection?

- **Concept: Contrastive Examples**
  - Why needed here: Creating contrastive examples helps the model learn the boundary between hate speech and not-hate speech by showing minimal changes that flip the label.
  - Quick check question: How do contrastive examples differ from regular adversarial examples?

- **Concept: Inter-Annotator Agreement**
  - Why needed here: High inter-annotator agreement indicates that annotators have a consistent understanding of the hate speech definition, which is crucial for dataset quality.
  - Quick check question: Why is it important to resolve disagreements between annotators?

## Architecture Onboarding

- **Component map**: Annotator -> Target Model -> Dynabench Interface -> Validation -> Training Data Integration
- **Critical path**: Annotator creates example → Target model predicts → Annotator validates → Expert resolves disagreements → Example added to training data → Model retrained
- **Design tradeoffs**: More annotators increase diversity but also increase coordination complexity. More support methods increase efficiency but may reduce annotator creativity.
- **Failure signatures**: Low inter-annotator agreement, model not improving after adding new examples, annotators only creating easy examples.
- **First 3 experiments**:
  1. Evaluate the target model on a held-out test set after each round to measure improvement.
  2. Compare the effectiveness of different support methods by isolating their contributions.
  3. Analyze the types of examples that the model still struggles with after training on GAHD.

## Open Questions the Paper Calls Out

The paper doesn't explicitly call out open questions but the methodology raises several important considerations. The authors demonstrate that their approach improves model robustness but don't fully explore whether different cultural contexts within German-speaking countries influence interpretation and annotation of hate speech, or how the approach would scale to other languages and domains.

## Limitations

- Dataset size (11k examples) is relatively modest for deep learning applications
- Study relies on a single target model architecture (gelectra-large) for adversarial example generation
- Paper doesn't fully explore whether different model architectures would yield different patterns of vulnerability
- Limited exploration of cultural context variations across German-speaking countries

## Confidence

- **High Confidence**: The core claim that GAHD improves hate speech detection model robustness by 18-20 percentage points on both in-domain and out-of-domain tests. This is directly supported by the reported F1 scores and experimental setup.
- **Medium Confidence**: The claim that mixing multiple annotation strategies is superior to any single strategy. While supported by the ablation study, the paper doesn't explore whether certain strategies are more effective for specific types of hate speech or model vulnerabilities.
- **Medium Confidence**: The finding that contrastive examples from R4 are particularly effective. This is based on ablation studies but could be influenced by the specific examples chosen or the order of rounds.

## Next Checks

1. **Architecture Sensitivity Analysis**: Test whether the same adversarial examples trick different transformer architectures (BERT, RoBERTa, mBERT) to determine if the dataset reveals fundamental model weaknesses or architecture-specific vulnerabilities.

2. **Generalizability Across Languages**: Apply the same four-round annotation strategy with different support methods to create adversarial datasets for other languages to test whether the methodology transfers across linguistic contexts.

3. **Long-term Model Adaptation**: Retrain models on GAHD and evaluate performance after 3-6 months on fresh adversarial examples to assess whether the improvements represent genuine learning of hate speech patterns or overfitting to specific attack patterns.