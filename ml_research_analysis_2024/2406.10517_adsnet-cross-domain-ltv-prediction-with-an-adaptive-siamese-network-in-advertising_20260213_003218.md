---
ver: rpa2
title: 'ADSNet: Cross-Domain LTV Prediction with an Adaptive Siamese Network in Advertising'
arxiv_id: '2406.10517'
source_url: https://arxiv.org/abs/2406.10517
tags:
- data
- network
- domain
- gain
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of data sparsity in lifetime
  value (LTV) prediction for advertising systems. The authors propose ADSNet, an adaptive
  cross-domain transfer learning framework that leverages external data to enhance
  the LTV prediction model.
---

# ADSNet: Cross-Domain LTV Prediction with an Adaptive Siamese Network in Advertising

## Quick Facts
- **arXiv ID**: 2406.10517
- **Source URL**: https://arxiv.org/abs/2406.10517
- **Reference count**: 40
- **Key outcome**: ADSNet improves GINI by 2% and increases online LTV by 3.47% and GMV by 3.89% compared to other methods

## Executive Summary
This paper addresses the challenge of data sparsity in lifetime value (LTV) prediction for advertising systems by proposing ADSNet, an adaptive cross-domain transfer learning framework. The method leverages external data through a pseudo-siamese network architecture with a gain evaluation strategy that selectively transfers beneficial information while rejecting noisy samples. A domain adaptation module further bridges the distribution gap between internal and external domains. Extensive offline and online experiments demonstrate significant performance improvements, with the gain evaluation strategy identified as particularly critical for mitigating negative transfer effects.

## Method Summary
ADSNet is a cross-domain transfer learning framework for LTV prediction that combines a pseudo-siamese network architecture with gain evaluation and domain adaptation modules. The model employs ordinal classification to handle the complex multi-modal distribution of LTV values. During training, a vanilla network learns from internal data while a gain network learns from both internal and external data, with the gain evaluation strategy calculating information gain to reject negative samples. The domain adaptation module uses knowledge distillation to reduce distribution distance between domains, and an iterative alignment strategy prevents network divergence through periodic parameter synchronization.

## Key Results
- ADSNet achieves 2% improvement in GINI coefficient compared to baseline methods
- Online A/B testing shows 3.47% increase in LTV and 3.89% increase in GMV
- Ablation study confirms the critical importance of the gain evaluation strategy for negative transfer mitigation
- Significant performance improvements observed across different sample size intervals

## Why This Works (Mechanism)

### Mechanism 1: Gain Evaluation Strategy
The gain evaluation strategy enables selective transfer of beneficial external data while rejecting noisy samples, mitigating negative transfer. The pseudo-siamese network architecture creates two parallel models: a vanilla network trained only on internal data and a gain network trained on both internal and external data. By comparing their performance on internal samples, the gain evaluation strategy calculates information gain for each external sample. Samples with negative gain (ùëäùê∫ < 0) are rejected, ensuring only beneficial external knowledge is transferred.

### Mechanism 2: Domain Adaptation Module
The Domain Adaptation Module bridges different domains by reducing distribution distance and enhancing representation space consistency. The adapter layer uses knowledge distillation through Mean Squared Error (MSE) to constrain distributions between the vanilla and gain networks at both embedding and tower layers. This creates a bridge that aligns the feature representations across domains, reducing the domain shift.

### Mechanism 3: Iterative Alignment Strategy
The iterative alignment strategy prevents divergence between the gain and vanilla networks during training. The training process is divided into two stages: a warmup stage where only the vanilla network is trained on internal data, followed by a joint training stage where both networks are trained with periodic parameter synchronization. This ensures the gain network doesn't diverge too far from the vanilla network, maintaining the effectiveness of the gain evaluation strategy.

## Foundational Learning

- **Cross-domain transfer learning**: Needed because advertising platforms have sparse internal LTV data, so external data from other platforms can provide additional samples to improve prediction accuracy. Quick check: What is the primary challenge in cross-domain transfer learning that ADSNet addresses?

- **Negative transfer**: Needed because simply combining internal and external data can introduce noise and harm model performance when the domains have different distributions. ADSNet specifically addresses this by rejecting negative gain samples. Quick check: How does ADSNet identify and reject samples that would cause negative transfer?

- **Ordinal classification for regression**: Needed because LTV follows a complex multi-modal distribution with many zero values and standardized purchase tiers. Ordinal classification divides this into sub-distributions and models the cumulative distribution function, which is more aligned with the sequential nature of purchase amounts. Quick check: How does ordinal classification transform the LTV prediction problem compared to direct regression?

## Architecture Onboarding

- **Component map**: Feature encoding (FwFM) -> Expert layer (PLE) -> Tower layer (probability + amount prediction) -> Multi-granularity prediction, with pseudo-siamese network and domain adaptation module for cross-domain learning

- **Critical path**: Feature encoding ‚Üí Expert layer ‚Üí Tower layer ‚Üí Multi-granularity prediction, with the pseudo-siamese network and domain adaptation module providing cross-domain learning capabilities

- **Design tradeoffs**: Complexity vs. performance (pseudo-siamese architecture adds complexity but enables effective cross-domain learning), parameter synchronization frequency (balancing alignment and independent learning), external data selection (sensitivity of gain evaluation strategy)

- **Failure signatures**: Poor internal data performance despite good external integration (ineffective gain evaluation), degradation with external data (insufficient negative transfer mitigation), high computational cost with marginal gains (domain adaptation overhead)

- **First 3 experiments**: 1) Implement vanilla network backbone on internal data only to establish baseline, 2) Add gain network with external data but without gain evaluation to measure data volume vs. quality impact, 3) Implement full ADSNet with gain evaluation and domain adaptation to measure negative transfer mitigation effectiveness

## Open Questions the Paper Calls Out

- **Generalizability to other advertising tasks**: How does ADSNet perform on tasks beyond LTV prediction like CTR or CVR prediction? The authors suggest extending their approach but haven't tested it on these tasks.

- **Balance between internal and external data**: What is the optimal balance between internal and external data contributions in the gain evaluation strategy, and how sensitive is the model to this balance?

- **Comparison with other alignment methods**: How does the iterative alignment strategy compare to other alignment methods in terms of effectiveness and computational efficiency?

## Limitations

- Implementation details for gain evaluation strategy and domain adaptation module are not fully specified, making exact replication challenging
- Limited analysis of computational overhead from the pseudo-siamese architecture and iterative alignment strategy
- No comprehensive study on the trade-off between external data volume and negative transfer risk
- External data source characteristics and similarity metrics to internal data are not explicitly defined

## Confidence

- **High Confidence**: The ordinal classification approach for LTV prediction and the general pseudo-siamese network architecture
- **Medium Confidence**: The effectiveness of the gain evaluation strategy in rejecting negative gain samples, based on reported 2% GINI improvement
- **Low Confidence**: The specific implementation details of the domain adaptation module and its contribution to performance gains

## Next Checks

1. Implement the gain evaluation strategy independently and validate its ability to distinguish between beneficial and harmful external samples using synthetic data with known domain shift

2. Conduct controlled experiments varying the synchronization frequency in the iterative alignment strategy to identify optimal parameter updates

3. Test the domain adaptation module's effectiveness by measuring distribution distance reduction between vanilla and gain networks using domain divergence metrics like MMD (Maximum Mean Discrepancy)