---
ver: rpa2
title: A Career Interview Dialogue System using Large Language Model-based Dynamic
  Slot Generation
arxiv_id: '2412.16943'
source_url: https://arxiv.org/abs/2412.16943
tags:
- system
- slots
- dialogue
- slot
- information
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a slot-filling dialogue system for career
  interviews using LLM-based dynamic slot generation. The proposed method dynamically
  generates new slots during conversation using LLMs, with an abductive reasoning
  process to create more relevant slots.
---

# A Career Interview Dialogue System using Large Language Model-based Dynamic Slot Generation

## Quick Facts
- arXiv ID: 2412.16943
- Source URL: https://arxiv.org/abs/2412.16943
- Reference count: 11
- This study developed a slot-filling dialogue system for career interviews using LLM-based dynamic slot generation

## Executive Summary
This paper presents a novel approach to career interview dialogue systems that leverages large language models for dynamic slot generation. The system is designed to collect comprehensive information about nursing staff careers to prepare for actual interviews with nursing managers. By using abductive reasoning in conjunction with LLM-based slot generation, the system can identify latent concerns and generate more relevant probing questions, ultimately collecting more check items and achieving higher naturalness scores compared to baseline methods.

## Method Summary
The proposed system uses a slot-filling dialogue structure enhanced with LLM-based dynamic slot generation. It begins with a self-assessment questionnaire completed by the user, followed by small talk and interview phases. During the interview, the system dynamically generates new slots based on conversation context using abductive reasoning to infer underlying causes from user statements. The LLM generates questions to fill these slots, and the system creates a structured report for nursing managers. Experiments compared the proposed method against a baseline using a user simulator with 16 nurse personas, evaluating information collection and dialogue naturalness.

## Key Results
- Collected 2.8 check items versus 2.0 for baseline method
- Achieved higher naturalness scores (5.23 vs 4.90 for baseline in processing effort)
- Successfully identified latent concerns like resignation intentions through abductive reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Abductive reasoning enables the system to infer latent user concerns and generate more relevant slots.
- Mechanism: The system uses abductive reasoning to identify surprising facts (C) in user utterances and infers potential underlying causes (A). It then generates slots to explore these inferred causes.
- Core assumption: User utterances contain implicit information that can be inferred through abductive reasoning.
- Evidence anchors:
  - [abstract]: "we incorporate abduction into the slot generation process to enable more appropriate and effective slot generation"
  - [section]: "We believe Takanashi's findings can be used to identify significant potential risks in a user's career... By asking questions to fill this 'dissatisfaction with night shifts' slot, it is expected that significant potential risks, such as resignation due to dissatisfaction with night shifts, can be identified."
  - [corpus]: Weak - no direct corpus evidence found

### Mechanism 2
- Claim: Dynamic slot generation allows the system to collect more comprehensive information than fixed slot sets.
- Mechanism: The system generates new slots during conversation based on context, allowing it to probe topics not covered by the initial slot set.
- Core assumption: User information needs vary and cannot be fully captured by a predefined slot set.
- Evidence anchors:
  - [abstract]: "We therefore propose a method that leverages large language models (LLMs) to dynamically generate new slots according to the flow of the dialogue"
  - [section]: "Conventional slot-filling-based interview dialogue systems operate using a predefined set of slots, which presents two problems. The first is the limited ability to collect information, as the system can only inquire about information related to the predefined slots."
  - [corpus]: Weak - no direct corpus evidence found

### Mechanism 3
- Claim: The slot-filling dialogue structure enables systematic information collection for career interview preparation.
- Mechanism: The system uses slot-filling to organize collected information into structured categories, making it easier for nursing managers to understand user concerns.
- Core assumption: Structured information presentation improves report usability for nursing managers.
- Evidence anchors:
  - [abstract]: "We developed a dialogue system for pre-interviews to collect information on staff careers in preparation for actual interviews and share this information as a report with nursing managers."
  - [section]: "The system begins by conducting a dialogue in the small talk phase... Once the interview phase is complete, the system generates a report based on the information collected from the user."
  - [corpus]: Weak - no direct corpus evidence found

## Foundational Learning

- Concept: Abductive reasoning
  - Why needed here: Enables the system to infer underlying causes from observed user statements, generating more relevant probing questions
  - Quick check question: What is the difference between abductive reasoning and deductive reasoning?

- Concept: Slot-filling dialogue systems
  - Why needed here: Provides the basic framework for systematic information collection in career interviews
  - Quick check question: How does slot-filling differ from open-ended conversation in dialogue systems?

- Concept: Large language model prompting
  - Why needed here: Allows dynamic slot generation and question creation based on conversation context
  - Quick check question: What are the key components of an effective LLM prompt for slot generation?

## Architecture Onboarding

- Component map:
  - User interface (dialogue frontend) -> Dialogue manager (state transition network) -> LLM modules (slot-filling, slot-generation, question-generation, user simulation) -> Report generator -> Self-assessment questionnaire processor

- Critical path:
  1. User completes self-assessment questionnaire
  2. System enters small talk phase
  3. System transitions to interview phase
  4. LLM performs slot-filling and slot generation
  5. System generates questions based on slots
  6. User responds
  7. Repeat steps 4-6 until termination condition
  8. System generates report

- Design tradeoffs:
  - Fixed vs. dynamic slots: Fixed slots provide consistency but miss context-specific information; dynamic slots are more flexible but risk generating irrelevant slots
  - Abduction vs. no abduction: Abduction generates more targeted questions but may create unclear reasoning paths for users
  - Prompt length vs. relevance: Longer prompts provide more context but may reduce LLM response quality

- Failure signatures:
  - Excessive slot generation leading to long prompts
  - Irrelevant slot generation causing unnatural questions
  - Poor slot-filling accuracy resulting in incomplete information
  - User simulator generating unnatural utterances

- First 3 experiments:
  1. Compare information collection between baseline and proposed methods with same user simulator personas
  2. Test different prompt structures for slot generation to optimize relevance
  3. Evaluate user experience with and without abductive reasoning explanations

## Open Questions the Paper Calls Out
None

## Limitations
- The user simulator cannot fully replicate the complexity of real human conversations, creating uncertainty about real-world performance
- Exact prompt templates used for slot generation are not provided, making faithful reproduction challenging
- Evaluation focuses on quantitative metrics and subjective naturalness scores but lacks qualitative analysis of dialogue content and user satisfaction

## Confidence
- Dynamic slot generation effectiveness: Medium
- Abductive reasoning improvement: Medium
- Systematic information collection: Medium

## Next Checks
1. Conduct a pilot study with actual nursing staff and managers to validate the system's performance with real users and assess the practical utility of collected information.

2. Implement A/B testing comparing the proposed system against a human-conducted career interview to measure relative effectiveness in identifying latent concerns and overall user satisfaction.

3. Perform ablation studies testing different components of the system (abductive reasoning, dynamic slot generation) with varying prompt structures and context windows to optimize performance and identify the most critical elements for success.