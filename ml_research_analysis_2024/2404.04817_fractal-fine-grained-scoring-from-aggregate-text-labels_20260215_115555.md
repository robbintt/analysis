---
ver: rpa2
title: 'FRACTAL: Fine-Grained Scoring from Aggregate Text Labels'
arxiv_id: '2404.04817'
source_url: https://arxiv.org/abs/2404.04817
tags:
- learning
- labels
- bagloss
- dataset
- label
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FRACTAL, a method to disaggregate response-level
  labels into sentence-level pseudo-labels for fine-grained scoring in text generation
  tasks. FRACTAL leverages multiple instance learning (MIL) and learning from label
  proportions (LLP) techniques, augmented with prior information such as document-sentence
  cosine similarity, to train specialized models for sentence-level scoring.
---

# FRACTAL: Fine-Grained Scoring from Aggregate Text Labels

## Quick Facts
- arXiv ID: 2404.04817
- Source URL: https://arxiv.org/abs/2404.04817
- Reference count: 22
- Primary result: FRACTAL achieves comparable performance to models trained on fine-grained human annotations by disaggregating response-level labels into sentence-level pseudo-labels.

## Executive Summary
FRACTAL addresses the challenge of obtaining fine-grained sentence-level scores for text generation tasks when only aggregate response-level labels are available. The method combines multiple instance learning (MIL) and learning from label proportions (LLP) techniques with task-specific prior information to train models that can accurately score individual sentences within responses. By using differentiable approximations of aggregation functions and a maximum likelihood pseudo-labeling strategy, FRACTAL generates consistent sentence-level pseudo-labels that enable training specialized scoring models. Extensive experiments across six datasets and four tasks demonstrate that FRACTAL significantly outperforms several strong baselines and achieves performance close to models trained with full fine-grained supervision.

## Method Summary
FRACTAL operates by first training a model using a bag-loss function that computes aggregate predictions for response bags using differentiable approximations of MIN, MAX, or AVG functions, then minimizing the difference between this aggregate and the response-level label. Prior information such as document-sentence cosine similarity and inter-sentence correlation is incorporated through additional loss terms. The method then applies maximum likelihood pseudo-labeling (PsLab) to generate consistent sentence-level labels that satisfy bag constraints. These pseudo-labels are used to retrain the model, creating a self-training loop that progressively improves sentence-level scoring accuracy. The approach is flexible enough to handle different aggregation functions and priors across various NLP tasks including retrieval, question answering, summarization, and math reasoning.

## Key Results
- FRACTAL achieves comparable performance to supervised models trained on fine-grained human annotations across all six datasets
- The method outperforms strong baselines including BagLoss and response-level models on sentence-level metrics (AUC-ROC, MAE, MSE)
- Task-specific priors and the PsLab pseudo-labeling strategy provide consistent performance gains across different aggregation functions and NLP tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FRACTAL improves sentence-level scoring by combining bag-loss optimization with sentence-level priors that guide the model toward more accurate local predictions.
- Mechanism: The bag-loss function computes an aggregate prediction for each response using differentiable approximations of aggregation functions (MIN, MAX, AVG), then minimizes the difference between this aggregate and the response-level label. Priors like document-sentence cosine similarity and inter-sentence correlation are incorporated via additional loss terms that penalize deviations from expected relationships, effectively injecting structured knowledge into the training process.
- Core assumption: The aggregation function (e.g., AVG, MIN) accurately reflects how response-level labels are derived from underlying sentence-level quality distributions, and that sentence-level priors are correlated with true sentence quality.
- Evidence anchors:
  - [abstract] "Our approach leverages multiple instance learning (MIL) and learning from label proportions (LLP) techniques in conjunction with prior information (e.g., document-sentence cosine similarity) to train a specialized model for sentence-level scoring."
  - [section 4.1] "The total loss is a combination of bag and prior losses: Ltot = λLtotbag + λ1Ltotprior1 + λ2Ltotprior2"
  - [corpus] Weak: No direct corpus evidence for this specific aggregation function claim; relies on MIL/LLP literature.
- Break condition: If the aggregation function poorly represents the true relationship between sentence and response quality, or if priors are uncorrelated with sentence quality, performance degrades significantly.

### Mechanism 2
- Claim: Pseudo-labeling using maximum likelihood estimation (PsLab) refines sentence-level predictions to be consistent with bag labels, enabling self-training that improves model accuracy.
- Mechanism: For binary labels and MIN aggregation, PsLab assigns label 1 to all sentences if the bag label is 1. If the bag label is 0, it assigns 0 to at least one sentence (the one with minimum predicted score) and 1 to others, ensuring the minimum prediction matches the bag label. This calibrated pseudo-label set is then used to retrain the model.
- Core assumption: The model's predicted probabilities accurately reflect the true underlying label distribution, making maximum likelihood assignments valid.
- Evidence anchors:
  - [section 4.3] "PsLab outputs the maximum likelihood valid configuration of labels i.e., with at least one 0-label. This is can be efficiently done via the following algorithm"
  - [abstract] "We also employ techniques which use model predictions to pseudo-label the train-set at the sentence-level for model training to further improve performance."
  - [corpus] Weak: No direct corpus evidence; method described in paper text.
- Break condition: If initial model predictions are systematically biased or if the aggregation function doesn't align with PsLab's assumptions, pseudo-labeling can propagate errors.

### Mechanism 3
- Claim: FRACTAL generalizes across diverse NLP tasks by adapting aggregation functions and priors to task-specific characteristics.
- Mechanism: Different tasks use different aggregation functions (AVG for QA preference, MAX for retrieval, MIN for summarization) and task-specific priors (knowledge passage-sentence similarity for QA, query-sentence similarity for retrieval, document-sentence similarity for summarization). This flexibility allows the same core framework to address varied objectives.
- Core assumption: The choice of aggregation function and priors can be mapped appropriately to each task's evaluation criteria.
- Evidence anchors:
  - [section 5] "We use QA-Feedback dataset... The aggregation function used isAVG" and "The FiRA dataset... employ MAX as the aggregation function"
  - [table 1] Shows task-specific priors and aggregation functions
  - [corpus] Weak: No direct corpus evidence; relies on paper's experimental setup.
- Break condition: If a task's evaluation criteria don't align well with available aggregation functions or if suitable priors cannot be constructed, the framework's effectiveness diminishes.

## Foundational Learning

- Concept: Multiple Instance Learning (MIL) and Learning from Label Proportions (LLP)
  - Why needed here: These frameworks provide the theoretical foundation for training models when only aggregated labels are available, which is exactly the problem FRACTAL addresses.
  - Quick check question: Can you explain the difference between MIL (where bag label is MIN/MAX of instance labels) and LLP (where bag label is average of instance labels)?

- Concept: Differentiable approximations of discrete aggregation functions
  - Why needed here: Standard MIN/MAX operations are non-differentiable, preventing gradient-based optimization. Approximations like log-sum-exp enable backpropagation through the aggregation step.
  - Quick check question: Why can't we use standard MIN or MAX operations directly in our loss function when training with gradient descent?

- Concept: Maximum likelihood estimation in constrained label spaces
- Why needed here: PsLab uses MLE to find the most probable label configuration consistent with bag constraints, which is essential for generating useful pseudo-labels.
  - Quick check question: Given a bag with label 0 and three sentences with predicted probabilities 0.1, 0.3, 0.8, what would PsLab assign as the final labels?

## Architecture Onboarding

- Component map:
  - Sentence-T5 Large encoder → generates embeddings for text components
  - 2-hidden layer MLP (73728 parameters) → predicts sentence-level scores
  - Loss computation module → combines bag-loss and prior loss terms
  - PsLab module → generates pseudo-labels for self-training
  - Training loop → orchestrates minibatch processing and weight updates

- Critical path: Embedding generation → Score prediction → Loss computation (bag + priors) → Backpropagation → Weight update. The most time-critical components are the encoder and MLP inference during training.

- Design tradeoffs: Using sentence-level priors improves accuracy but adds computational overhead. The choice between AVG, MIN, and MAX aggregation depends on task characteristics—AVG may be too smooth for tasks needing strict thresholds, while MIN/MAX may be too brittle.

- Failure signatures:
  - Poor performance across all tasks → Check aggregation function approximation and prior quality
  - Good bag-level performance but poor sentence-level → PsLab may be generating poor pseudo-labels
  - Slow convergence → Review learning rate and batch size; check gradient flow through differentiable approximations

- First 3 experiments:
  1. Train BagLoss baseline without priors on a small subset of one dataset to verify basic functionality
  2. Add cosine similarity prior and observe improvement on sentence-level metrics
  3. Apply PsLab pseudo-labeling and compare against supervised baseline to validate self-training effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of differentiable approximation function for MIN (e.g., Mult, GM, tf.reduce_min) impact the performance of FRACTAL across different tasks and datasets?
- Basis in paper: [explicit] The paper discusses various differentiable approximations to MIN, including Mult, GM, and tf.reduce_min, and provides results comparing their performance on the WikiCatSum dataset.
- Why unresolved: While the paper presents results for a single dataset, it does not explore the impact of the choice of approximation function across all tasks and datasets considered.
- What evidence would resolve it: Conducting experiments using different differentiable approximations for MIN across all datasets and tasks in the study would provide insights into the generalizability and impact of this choice on FRACTAL's performance.

### Open Question 2
- Question: How does the performance of FRACTAL compare to other methods for fine-grained scoring, such as those based on reinforcement learning or generative models?
- Basis in paper: [inferred] The paper focuses on evaluating FRACTAL against baselines like BagLoss and response-level models but does not compare it to other fine-grained scoring methods.
- Why unresolved: The paper does not explore the landscape of existing fine-grained scoring techniques beyond the baselines mentioned, leaving the relative performance of FRACTAL in comparison to other methods unclear.
- What evidence would resolve it: Implementing and evaluating FRACTAL against other state-of-the-art fine-grained scoring methods, such as those based on reinforcement learning or generative models, would provide a comprehensive comparison of its performance.

### Open Question 3
- Question: How does the incorporation of additional prior information, such as task-specific features or linguistic knowledge, affect the performance of FRACTAL?
- Basis in paper: [explicit] The paper discusses the use of priors like cosine similarity and correlation between sentences, but does not explore the impact of incorporating other types of prior information.
- Why unresolved: While the paper demonstrates the effectiveness of certain priors, it does not investigate the potential benefits of incorporating additional task-specific or linguistic knowledge as priors.
- What evidence would resolve it: Conducting experiments that incorporate various types of prior information, such as task-specific features or linguistic knowledge, and evaluating their impact on FRACTAL's performance would provide insights into the benefits of leveraging diverse prior information.

## Limitations
- The effectiveness of FRACTAL depends heavily on the availability and quality of suitable prior information, which may not be readily available for all domains
- Error propagation from initial model predictions through PsLab can amplify biases if the starting model is poorly calibrated
- The method's performance is sensitive to the choice of aggregation function and priors, requiring careful task-specific tuning

## Confidence
- **High confidence**: The MIL/LLP theoretical foundation and differentiable approximation techniques are well-established in the literature. The experimental results showing performance improvements over baselines are clearly demonstrated.
- **Medium confidence**: The PsLab pseudo-labeling mechanism is sound in theory, but its effectiveness depends on initial model quality which isn't thoroughly validated across different starting conditions.
- **Medium confidence**: Task-specific adaptations (different aggregation functions and priors) show promise but the selection criteria for these choices could benefit from more systematic analysis.

## Next Checks
1. **Cross-dataset generalization test**: Train FRACTAL on one task (e.g., retrieval) and evaluate on a different task (e.g., QA) to assess whether the learned disaggregation generalizes across domains.
2. **Initial model quality ablation**: Systematically vary the quality of the initial model used for PsLab and measure the impact on final performance to quantify error propagation effects.
3. **Prior sensitivity analysis**: Conduct a comprehensive study removing or replacing task-specific priors with random/noisy priors to establish the true contribution of each prior type to overall performance.