---
ver: rpa2
title: LLMs Can Evolve Continually on Modality for X-Modal Reasoning
arxiv_id: '2410.20178'
source_url: https://arxiv.org/abs/2410.20178
tags:
- modality
- learning
- modalities
- arxiv
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PathWeave, a flexible and scalable framework
  for continual learning on modality in multi-modal large language models (MLLMs).
  The core method introduces an Adapter-in-Adapter (AnA) framework with uni-modal
  and cross-modal adapters, along with an MoE-based gating module, to enable efficient
  modality alignment and collaboration during incremental learning.
---

# LLMs Can Evolve Continually on Modality for X-Modal Reasoning

## Quick Facts
- arXiv ID: 2410.20178
- Source URL: https://arxiv.org/abs/2410.20178
- Reference count: 40
- Primary result: PathWeave achieves comparable performance to state-of-the-art MLLMs while reducing parameter training burdens by 98.73%

## Executive Summary
This paper introduces PathWeave, a novel framework for continual learning on modality in multi-modal large language models (MLLMs). The core innovation is an Adapter-in-Adapter (AnA) architecture that enables efficient expansion to new modalities using only uni-modal data, eliminating the need for joint-modal pretraining or fine-tuning. The framework integrates uni-modal and cross-modal adapters with an MoE-based gating module to facilitate modality alignment and collaboration during incremental learning. The authors establish a challenging benchmark called Continual Learning of Modality (MCL) and demonstrate that PathWeave significantly reduces parameter training burdens while maintaining competitive performance.

## Method Summary
PathWeave employs an Adapter-in-Adapter (AnA) framework that uses two types of adapters: uni-modal adapters that capture modality-specific knowledge and are frozen after training, and cross-modal adapters formed by inserting in-adapters into previously learned uni-modal adapters. An MoE-based gating module dynamically weights adapter contributions based on input modality to prevent interference between different modalities. The framework enables continual learning on uni-modal data sequentially, eliminating the need for joint-modal pretraining or fine-tuning. The method is evaluated on a newly established MCL benchmark with five distinct modalities: image, video, audio, depth, and point cloud.

## Key Results
- PathWeave reduces parameter training burdens by 98.73% compared to traditional approaches
- Achieves comparable performance to state-of-the-art MLLMs on the MCL benchmark
- Successfully demonstrates continual learning capability without catastrophic forgetting across multiple modalities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The Adapter-in-Adapter (AnA) framework enables efficient modality expansion by freezing uni-modal adapters after training and only training cross-modal adapters for new modalities.
- Mechanism: The framework uses two types of adapters: uni-modal adapters that capture modality-specific knowledge and are frozen after training, and cross-modal adapters formed by inserting in-adapters into previously learned uni-modal adapters. This allows new modalities to leverage historical knowledge without retraining old adapters.
- Core assumption: Modality-specific knowledge can be effectively captured and frozen in uni-modal adapters, and cross-modal adapters can learn to integrate this knowledge with new modalities through in-adapter insertion.
- Evidence anchors:
  - [abstract]: "a novel Adapter-in-Adapter (AnA) framework is introduced, in which uni-modal and cross-modal adapters are seamlessly integrated to facilitate efficient modality alignment and collaboration"
  - [section 3.3]: "The uni-modal adapters are implemented in parallel in the pretrained Q-Former for new modal alignment... Meanwhile, the cross-modal adapters ( ˆAm) are constructed by inserting a set of in-adapters ( {F m i }m−1 i=1 ) into previously learned uni-adapters"
  - [corpus]: Weak - No direct corpus evidence supporting this specific adapter mechanism
- Break condition: If uni-modal adapters fail to capture modality-specific knowledge effectively, or if cross-modal adapters cannot integrate historical knowledge with new modalities, the incremental learning would fail.

### Mechanism 2
- Claim: The MoE-based gating module dynamically weights adapter contributions based on input modality, preventing interference between different modalities.
- Mechanism: An MoE (Mixture of Experts) gating module computes weights for each adapter path based on input embeddings, allowing the model to selectively emphasize relevant modality knowledge while suppressing interfering information from other modalities.
- Core assumption: Different modalities have distinct characteristics that can be identified by the gating module, and appropriate weighting can prevent negative interference between modalities.
- Evidence anchors:
  - [abstract]: "an MoE-based gating module is applied between two types of adapters to further enhance the multimodal interaction"
  - [section 3.3]: "We propose an MoE-based gating module between cross-modal and uni-modal adapters for adaptive multimodal integration... Our MoE-based gating Gm automatically assigns weights of paths P m of different cross-modal adapters and uni-modal adapter"
  - [corpus]: Weak - No direct corpus evidence supporting this specific MoE gating mechanism
- Break condition: If the gating module cannot accurately identify modality characteristics or if inappropriate weighting causes performance degradation, the multimodal integration would fail.

### Mechanism 3
- Claim: Continual learning on uni-modal data eliminates the need for joint-modal pretraining or fine-tuning, significantly reducing computational costs while maintaining performance.
- Mechanism: By training on uni-modal data sequentially and leveraging the AnA framework, the model can expand to new modalities without requiring access to all historical data or performing expensive joint-modal optimization.
- Core assumption: Sequential training on uni-modal data with proper adapter mechanisms can achieve comparable performance to joint-modal training without the computational overhead.
- Evidence anchors:
  - [abstract]: "enabling their expansion to new modalities using uni-modal data, without executing joint-modal pretraining" and "PathWeave performs comparably to state-of-the-art MLLMs while concurrently reducing parameter training burdens by 98.73%"
  - [section 3.1]: "We introduce this concept into MLLMs to form an incremental training strategy on uni-modal data called Continual Learning on Modality (MCL), eliminating the necessity of modal-specific pertaining and joint-modal datasets"
  - [corpus]: Weak - No direct corpus evidence supporting this specific continual learning approach
- Break condition: If sequential uni-modal training cannot capture the necessary cross-modal interactions, or if the performance gap between this approach and joint-modal training becomes unacceptable.

## Foundational Learning

- Concept: Adapter-based transfer learning (e.g., LoRA)
  - Why needed here: Provides the foundation for the AnA framework's parameter-efficient modality expansion
  - Quick check question: How does LoRA reduce trainable parameters while maintaining performance compared to full fine-tuning?

- Concept: Continual learning concepts (catastrophic forgetting, plasticity-stability tradeoff)
  - Why needed here: Essential for understanding how the AnA framework maintains historical knowledge while learning new modalities
  - Quick check question: What are the main approaches to mitigate catastrophic forgetting in continual learning?

- Concept: Multimodal learning fundamentals (cross-modal alignment, modality-specific feature extraction)
  - Why needed here: Critical for understanding how different modalities are processed and integrated in the PathWeave framework
  - Quick check question: How do different modalities typically differ in their feature extraction and representation requirements?

## Architecture Onboarding

- Component map: Raw modality data → Encoder → Q-Former with AnA → MoE gating → LLM → Text responses
- Critical path: Modality data → Encoder → Q-Former with AnA → MoE gating → LLM → Output
- Design tradeoffs:
  - Parameter efficiency vs. performance: Using adapters reduces parameters but may limit capacity
  - Sequential vs. joint training: Uni-modal training is more efficient but may miss cross-modal interactions
  - Adapter freezing vs. updating: Freezing prevents forgetting but may limit adaptation
- Failure signatures:
  - Performance degradation on historical modalities indicates catastrophic forgetting
  - Poor performance on new modalities suggests inadequate adapter learning
  - Inconsistent outputs across modalities may indicate gating module issues
- First 3 experiments:
  1. Test basic adapter functionality: Train on image modality, then test on same modality to verify basic learning
  2. Test incremental learning: Train on image, then video, then test on both to verify no forgetting
  3. Test gating effectiveness: Compare performance with and without MoE gating on multimodal inputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the PathWeave framework scale to support more than five modalities while maintaining performance?
- Basis in paper: [inferred] The paper mentions that a limitation is that only five modalities were explored and that the implicit interaction between modalities cannot accomplish cross-modal joint language reasoning tasks in an incremental manner.
- Why unresolved: The paper only tested the framework on five modalities and did not explore its performance or limitations when scaled to more diverse or numerous modalities.
- What evidence would resolve it: Testing the framework on a broader range of modalities (e.g., adding text, tactile, or temperature data) and evaluating performance degradation or improvement would provide clarity.

### Open Question 2
- Question: Can the MoE-based gating module be optimized to handle modalities with highly imbalanced data distributions?
- Basis in paper: [explicit] The paper discusses the use of an MoE-based gating module to adaptively integrate multimodal information but does not address scenarios with imbalanced data.
- Why unresolved: The paper does not provide experiments or analysis on how the gating module performs when some modalities have significantly more data than others.
- What evidence would resolve it: Experiments comparing the gating module's performance on balanced vs. imbalanced datasets would clarify its robustness.

### Open Question 3
- Question: What is the impact of the order in which modalities are introduced on the final model performance?
- Basis in paper: [inferred] The paper introduces modalities in a specific sequence (image, video, audio, depth, 3D) but does not explore how changing this order affects performance.
- Why unresolved: The paper does not provide ablation studies or comparisons to determine if the sequence of modality introduction influences the model's ability to learn or generalize.
- What evidence would resolve it: Testing the framework with different modality sequences and comparing performance metrics would reveal the impact of introduction order.

### Open Question 4
- Question: How does the Adapter-in-Adapter (AnA) framework perform in real-time applications where latency is critical?
- Basis in paper: [explicit] The paper highlights the efficiency of the AnA framework in reducing parameter training burdens but does not address real-time performance or latency.
- Why unresolved: The paper focuses on training efficiency but does not discuss inference speed or real-time applicability.
- What evidence would resolve it: Benchmarking the framework's inference latency on a real-time system would provide insights into its practical usability.

## Limitations
- Limited evaluation scope to five specific modalities, with unclear scalability to more diverse or numerous modalities
- Sequential training approach may miss complex cross-modal interactions that emerge from joint training
- Framework's performance in real-time applications with latency constraints has not been evaluated

## Confidence

- **High Confidence**: The basic Adapter-in-Adapter architecture and its parameter efficiency benefits are well-established concepts in the literature, and the framework's core design follows established patterns.
- **Medium Confidence**: The claim about achieving comparable performance to state-of-the-art models is supported by benchmark results, but the evaluation scope is limited to specific baselines and datasets.
- **Low Confidence**: The assertion that the MoE-based gating module effectively prevents interference between modalities lacks direct empirical validation beyond performance metrics.

## Next Checks

1. **Generalization Test**: Evaluate PathWeave on additional MLLM architectures and real-world multimodal datasets beyond the MCL benchmark to assess robustness and generalizability of the performance claims.

2. **Cross-Modal Interaction Analysis**: Conduct ablation studies comparing PathWeave's performance against joint-modal training on tasks requiring complex cross-modal reasoning to quantify potential limitations of the sequential training approach.

3. **Scalability Assessment**: Test the framework's performance and computational efficiency when expanding to 10+ modalities to identify potential bottlenecks or degradation in the AnA mechanism's effectiveness.