---
ver: rpa2
title: Whose wife is it anyway? Assessing bias against same-gender relationships in
  machine translation
arxiv_id: '2401.04972'
source_url: https://arxiv.org/abs/2401.04972
tags:
- relationships
- bias
- gender
- social
- same-gender
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study assesses bias against same-gender relationships in\
  \ machine translation (MT) systems. We generate a dataset of sentence templates\
  \ in French, Italian, and Spanish describing relationships, filled with occupation\
  \ nouns and relationship targets (friend, fianc\xE9(e), husband/wife)."
---

# Whose wife is it anyway? Assessing bias against same-gender relationships in machine translation

## Quick Facts
- arXiv ID: 2401.04972
- Source URL: https://arxiv.org/abs/2401.04972
- Authors: Ian Stewart; Rada Mihalcea
- Reference count: 14
- Key outcome: Machine translation systems consistently fail to accurately translate sentences about same-gender relationships, with accuracy varying by occupation characteristics

## Executive Summary
This study investigates bias against same-gender relationships in machine translation systems across French, Italian, and Spanish. The researchers generated a dataset of relationship-describing sentences using occupation nouns and relationship targets, then tested three popular MT services (Google, Amazon, Microsoft) on their ability to correctly translate gender-marked possessive pronouns. They found consistent bias against same-gender relationships, with lower translation accuracy compared to different-gender relationships. The bias was particularly pronounced for high-income, high-female-representation, and older-age occupations. Logistic regression analysis revealed that these social factors correlate with decreased likelihood of correct pronoun prediction for same-gender sentences, highlighting the need for more balanced training data in MT systems.

## Method Summary
The researchers created a dataset of ~3000 sentences per language using templates like "OCCUPATION RELATIONSHIP-VERB HIS/HER RELATIONSHIP-TARGET" filled with occupation nouns and relationship targets (friend, fiancÃ©(e), husband/wife). They tested Google, Amazon, and Microsoft MT services on translating these sentences to English, measuring accuracy by comparing the gender of possessive pronouns in English output with the subject gender in the source language. They then performed logistic regression analysis using US labor statistics for income, female representation, and age of occupations to identify social correlates of the observed bias.

## Key Results
- Three popular MT services consistently failed to accurately translate sentences about same-gender relationships
- Translation accuracy for same-gender relationships was significantly lower than for different-gender relationships across all tested systems
- Lower accuracy for same-gender translations correlated with higher income, higher female representation, and older age of occupations
- Amazon MT showed the highest accuracy for same-gender relationships, but the gap between same-gender and different-gender accuracy remained substantial

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Machine translation systems systematically fail to preserve same-gender relationship information when translating from gender-marked languages to English
- Mechanism: The MT systems appear to prioritize fluent, high-likelihood English output over faithful translation of gender information, defaulting to different-gender relationship constructions
- Core assumption: The MT training data contains strong statistical associations between certain occupations and different-gender relationships
- Evidence anchors:
  - [abstract] "We find that three popular MT services consistently fail to accurately translate sentences concerning relationships between entities of the same gender"
  - [section 3.2] "We count a translation as correct if the gender of the English possessive pronoun in the translated sentence matches the gender of the subject noun in the source language sentence"
  - [corpus] Weak evidence - only 5 related papers found, suggesting limited prior research on same-gender relationship bias in MT

### Mechanism 2
- Claim: Accuracy for same-gender relationship translation decreases with higher income, female representation, and age of occupations
- Mechanism: MT systems have learned social conservative norms from training data that associate certain professional contexts with traditional different-gender relationships
- Core assumption: Training data reflects societal biases where high-status occupations are more commonly discussed in the context of different-gender relationships
- Evidence anchors:
  - [section 3.2.1] "We find that a lower likelihood of correct subject-gender prediction for occupations that had a higher income, a higher female representation, and higher age"
  - [section 3.2] "Occupations with higher income tend to see a very low accuracy for same-gender translations (e.g. 'judge,' 15% accuracy)"
  - [corpus] No direct evidence in corpus neighbors about income or occupation-specific bias patterns

### Mechanism 3
- Claim: Different MT models exhibit varying levels of same-gender relationship bias due to differences in training data distribution
- Mechanism: Proprietary data collection practices lead to different representations of same-gender relationships in model training corpora
- Core assumption: The variation in model performance indicates that some training datasets contain more balanced representations of relationships than others
- Evidence anchors:
  - [section 3.2] "Out of all the models, the Amazon MT model has the highest accuracy for same-gender relationships, but the gap between same-gender and different-gender relationships remains substantial"
  - [section 3.2] "The considerable variation among models suggests substantial differences in the training data distribution, perhaps due to proprietary data collection practices"
  - [corpus] Weak evidence - corpus neighbors focus on gender bias generally but not model-specific variation

## Foundational Learning

- Concept: Grammatical gender in Romance languages
  - Why needed here: The study relies on Spanish, French, and Italian having grammatical gender to create test sentences that mark gender explicitly
  - Quick check question: How do Spanish nouns mark gender, and how does this differ from English?

- Concept: Possessive pronoun agreement in translation
  - Why needed here: The evaluation metric depends on whether the translated English pronoun matches the gender of the subject in the source language
  - Quick check question: What determines whether "his" or "her" should be used in the English translation of a sentence from a gender-marked language?

- Concept: Logistic regression for bias analysis
  - Why needed here: The study uses logistic regression to identify social correlates of translation bias across different occupations
  - Quick check question: What does the coefficient for "Income" in the logistic regression tell us about the relationship between occupation income and translation accuracy?

## Architecture Onboarding

- Component map: Generated sentences -> MT system translation -> accuracy evaluation -> social correlates analysis
- Critical path: Generated sentences -> translation API calls -> pronoun accuracy checking -> statistical analysis
- Design tradeoffs: Simple template-based generation vs. more naturalistic sentence construction; focus on grammatical gender vs. social gender identity
- Failure signatures: Systematic misgendering of relationship pronouns; higher error rates for certain occupation types; model-specific variation in accuracy
- First 3 experiments:
  1. Test the same template approach with additional relationship types (e.g., "X argued with Y") to verify the bias generalizes beyond romantic contexts
  2. Create a balanced training dataset with equal same-gender and different-gender relationship examples and retrain a small MT model to test if the bias can be reduced
  3. Implement a diagnostic tool that flags potential same-gender relationship misgendering in real-time MT output for user review

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions but acknowledges several limitations in its analysis, particularly around the focus on grammatical gender rather than social gender identity and the limitation to one direction of translation.

## Limitations
- The study relies on template-generated sentences rather than naturalistic language, which may not fully capture the complexity of real-world translation scenarios
- The analysis is limited to three specific Romance languages and may not generalize to other language families or non-gender-marked languages
- The evaluation metric focuses solely on possessive pronoun accuracy, potentially missing other forms of gender-related translation errors

## Confidence
- High confidence in the existence of systematic bias against same-gender relationships across all tested MT systems
- Medium confidence in the correlation between occupation characteristics (income, female representation, age) and translation accuracy
- Low confidence in the specific mechanism explaining why certain occupation types show higher bias, as the study cannot directly observe training data composition

## Next Checks
1. Replicate the study using a balanced training corpus where same-gender and different-gender relationship examples are equally represented across all occupation types to test if the bias can be eliminated
2. Conduct human evaluation of translation outputs to verify that automated pronoun accuracy checking correctly identifies the source of translation errors
3. Test the same template approach with additional Romance languages (Portuguese, Romanian) and non-Romance languages to determine if the bias patterns are language-specific or universal across gender-marked languages