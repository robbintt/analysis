---
ver: rpa2
title: 'PlaMo: Plan and Move in Rich 3D Physical Environments'
arxiv_id: '2406.18237'
source_url: https://arxiv.org/abs/2406.18237
tags: []
core_contribution: This paper introduces PlaMo, a method that combines path planning
  and motion control for animating humanoid characters in complex 3D environments.
  The approach uses a hierarchical architecture where a high-level path planner generates
  feasible 3D trajectories considering terrain, obstacles, and character capabilities,
  while a low-level reinforcement learning controller executes these plans through
  realistic physical motion.
---

# PlaMo: Plan and Move in Rich 3D Physical Environments

## Quick Facts
- arXiv ID: 2406.18237
- Source URL: https://arxiv.org/abs/2406.18237
- Authors: Assaf Hallak; Gal Dalal; Chen Tessler; Kelly Guo; Shie Mannor; Gal Chechik
- Reference count: 13
- Primary result: Hierarchical path planning + RL motion control achieves 0.20-1.44m path displacement error across diverse terrains

## Executive Summary
This paper presents PlaMo, a hierarchical system for animating humanoid characters in complex 3D environments. The approach combines a high-level path planner that generates terrain-aware trajectories with a low-level RL motion controller that executes realistic physical movement. The method demonstrates successful navigation across multiple locomotion types (walking, running, crawling, crouching) in randomized scenes with obstacles and varying terrain.

## Method Summary
PlaMo employs a two-tier architecture where an A* search-based path planner generates feasible 3D trajectories considering terrain, obstacles, and character capabilities. The planner uses path refinement and motion-aware speed control via quadratic programming to ensure trajectories respect the character's performance envelope. A reinforcement learning controller trained with rewards combining path-following accuracy and motion capture style similarity executes the planned paths. The system handles diverse locomotion types across various terrains while maintaining realistic physical motion.

## Key Results
- Achieves mean path displacement errors of 0.20-1.44m depending on terrain complexity and locomotion type
- Successfully navigates randomized complex scenes with multiple landmarks and obstacles
- Demonstrates robust performance across four locomotion types: walking, running, crawling, and crouching
- Shows strong path-following accuracy while maintaining realistic character motion

## Why This Works (Mechanism)
The hierarchical approach works by separating high-level geometric planning from low-level physical execution. The path planner can optimize for terrain feasibility and efficiency without being constrained by the complexities of physical motion, while the RL controller focuses purely on generating realistic motion without needing to solve global planning. This decomposition allows each component to specialize in its domain, with the planner ensuring geometric feasibility and the controller ensuring physical plausibility.

## Foundational Learning
- **A* path planning**: Essential for finding shortest feasible paths in 3D environments; quick check: verify admissibility and consistency of heuristic
- **Quadratic programming for speed control**: Needed to smoothly modulate character speed along paths while respecting terrain constraints; quick check: validate solution feasibility and convergence
- **Reinforcement learning for motion control**: Required to learn complex physical interactions from experience; quick check: monitor reward convergence and motion quality metrics
- **Hierarchical control**: Separates planning from execution to handle complexity; quick check: ensure effective communication between layers
- **Terrain-aware planning**: Incorporates ground height and slope into path feasibility; quick check: validate terrain classification accuracy
- **Motion capture integration**: Provides realistic movement patterns for training; quick check: verify style preservation metrics

## Architecture Onboarding

**Component Map:**
Path Planner (A* + refinement) -> Speed Controller (QP) -> Motion Controller (RL) -> Character Physics

**Critical Path:**
Path planning → Speed optimization → Motion execution → Physics simulation

**Design Tradeoffs:**
- Planning granularity vs. computational efficiency
- Motion realism vs. path-following accuracy
- Pre-trained models vs. adaptability to new characters
- Reward shaping complexity vs. training stability

**Failure Signatures:**
- Large path displacement errors indicate planner-controller mismatch
- Unrealistic motion suggests RL training issues or reward misalignment
- Getting stuck indicates path planning failures or infeasible trajectories
- Excessive computation time suggests optimization bottlenecks

**First 3 Experiments:**
1. Basic navigation on flat terrain with simple obstacles
2. Multi-terrain navigation testing terrain adaptation
3. Complex scene navigation with multiple landmarks and obstacles

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Limited evaluation in highly dynamic or time-critical scenarios
- Reliance on pre-trained motion models may restrict adaptability
- Focus on path-following accuracy without comprehensive energy efficiency analysis
- Potential scalability concerns with increasing environment complexity

## Confidence

**High confidence:**
- Technical implementation and core algorithmic approach
- Performance in controlled experimental scenarios

**Medium confidence:**
- Generalization to diverse, unseen environments
- Real-time performance in highly dynamic scenarios

## Next Checks
1. Test system robustness with moving obstacles and dynamic environmental changes
2. Evaluate performance across significantly different character morphologies and physics parameters
3. Measure computational latency and scalability with increasing environment complexity and path length