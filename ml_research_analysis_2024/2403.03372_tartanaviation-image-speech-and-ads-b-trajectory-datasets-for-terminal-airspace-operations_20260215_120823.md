---
ver: rpa2
title: 'TartanAviation: Image, Speech, and ADS-B Trajectory Datasets for Terminal
  Airspace Operations'
arxiv_id: '2403.03372'
source_url: https://arxiv.org/abs/2403.03372
tags:
- data
- aircraft
- tartanaviation
- dataset
- ads-b
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: TartanAviation is a multi-modal dataset for terminal airspace operations,
  containing 3.1M images, 3374 hours of ATC speech, and 661 days of ADS-B trajectory
  data. It captures diverse aircraft operations, seasons, and weather conditions at
  towered and non-towered airports.
---

# TartanAviation: Image, Speech, and ADS-B Trajectory Datasets for Terminal Airspace Operations

## Quick Facts
- arXiv ID: 2403.03372
- Source URL: https://arxiv.org/abs/2403.03372
- Reference count: 19
- Primary result: Multi-modal dataset with 3.1M images, 3374 hours of ATC speech, and 661 days of ADS-B trajectory data for terminal airspace operations

## Executive Summary
TartanAviation is a comprehensive multi-modal dataset designed to advance AI/ML research in aviation, particularly for autonomous aircraft and air traffic control systems. The dataset captures diverse terminal airspace operations through synchronized image, speech, and ADS-B trajectory data collected at regional towered and non-towered airports. With over 3 million images, thousands of hours of air traffic control communications, and extensive trajectory data, TartanAviation provides researchers with rich, real-world data to develop and validate models for object detection, trajectory forecasting, and speech-to-intent prediction. The dataset's diversity in aircraft types, weather conditions, and airport operations makes it particularly valuable for developing robust AI systems that can operate in varied real-world scenarios.

## Method Summary
TartanAviation was collected using coordinated setups at multiple regional airports within the Greater Pittsburgh area. Three main data collection components work in tandem: Sony IMX 264 cameras with NVIDIA Xavier AGX for vision data, Stratux ADS-B receivers for trajectory tracking, and Bearcat SR30C radio receivers for air traffic control speech communications. The system uses ADS-B-triggered recording logic with an 8km fence for vision and 10km fence for speech, ensuring temporal alignment across all modalities. Data was collected across different seasons and weather conditions to capture operational diversity. The raw data was processed through synchronization, calibration, and formatting pipelines before being made available through high-throughput servers. The complete codebase for data collection and preprocessing is open-sourced to facilitate reproducibility and extension.

## Key Results
- Provides 3.1 million images, 3374 hours of ATC speech, and 661 days of ADS-B trajectory data
- Captures diverse aircraft operations across multiple towered and non-towered regional airports
- Includes vision data for object detection, trajectory data for forecasting, and speech data for communication analysis
- All data collected with proper authorization and processed for reliability and synchronization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-modal data improves situational awareness in terminal airspace by providing complementary perspectives (visual, auditory, trajectory).
- Mechanism: Concurrent collection of image, speech, and ADS-B trajectory data captures different aspects of aircraft operations, allowing AI models to learn richer representations of the airport environment.
- Core assumption: The three data modalities are sufficiently correlated to enable cross-modal learning and validation.
- Evidence anchors:
  - [abstract] states the dataset "provides a holistic view of the airport environment by concurrently collecting image, speech, and ADS-B trajectory data"
  - [section] mentions the data was collected "using setups installed inside airport boundaries" to capture diverse operations
- Break condition: If the modalities are poorly synchronized or the correlations between them are weak, the benefit of multi-modal learning diminishes significantly.

### Mechanism 2
- Claim: Real-world data from regional airports provides diversity that commercial airport datasets lack.
- Mechanism: By collecting data at towered and non-towered regional airports, the dataset captures a wider variety of aircraft types, operations, and weather conditions than datasets focused only on major commercial airports.
- Core assumption: Regional airports have sufficiently different operational characteristics compared to major commercial airports.
- Evidence anchors:
  - [abstract] states the dataset focuses on "smaller regional airports within the Greater Pittsburgh area" and that these airports "serve a multitude of different aircraft and mission profiles, providing a richer and more diverse data stream"
  - [section] contrasts this with prior datasets that "focus on large commercial airports"
- Break condition: If regional airports don't actually exhibit significantly different characteristics than major airports for the target applications, the diversity benefit is reduced.

### Mechanism 3
- Claim: Open-sourcing both data and collection code lowers barriers to entry for aviation AI research.
- Mechanism: Providing not just the curated dataset but also the code used to collect and pre-process it enables researchers to understand data provenance, replicate methods, and extend the dataset.
- Core assumption: The provided code is well-documented and usable by the research community.
- Evidence anchors:
  - [abstract] states "we also open-source the code-base used to collect and pre-process the dataset, further enhancing accessibility and usability"
  - [section] notes the data is available "on high throughput servers without a paywall or any other access restrictions"
- Break condition: If the code is poorly documented, buggy, or difficult to set up, the open-source benefit is significantly reduced.

## Foundational Learning

- Concept: Automatic Dependent Surveillance-Broadcast (ADS-B) technology
  - Why needed here: ADS-B provides the trajectory data component of the multi-modal dataset, tracking aircraft positions in real-time
  - Quick check question: What frequency bands does ADS-B use to transmit aircraft position data?

- Concept: Computer vision for object detection in aviation contexts
  - Why needed here: The image dataset enables research on vision-based detect-and-avoid systems for autonomous aircraft
  - Quick check question: What challenges make detecting aircraft in terminal airspace images particularly difficult?

- Concept: Air traffic control speech communications protocols
  - Why needed here: The speech dataset captures radio communications between pilots and controllers, enabling speech-to-intent research
  - Quick check question: What is the typical frequency range for aviation radio communications?

## Architecture Onboarding

- Component map: Vision (Sony IMX 264 cameras with NVIDIA Xavier AGX) -> ADS-B receivers (Stratux) -> Speech (Bearcat SR30C radio receiver) -> Coordinated recording logic
- Critical path: ADS-B data triggers all three recording modalities, ensuring temporal alignment. The vision system uses an 8km fence, speech uses a 10km fence.
- Design tradeoffs: Using regional airports provides diversity but may limit applicability to major airport operations. Open-sourcing code increases accessibility but requires maintenance.
- Failure signatures: Missing or corrupted data files, mis-calibrated camera projections leading to inaccurate bounding boxes, unsynchronized timestamps between modalities.
- First 3 experiments:
  1. Validate data synchronization by checking timestamp alignment between modalities for sample recordings
  2. Test vision model performance on the provided Pytorch dataloaders with bounding box labels
  3. Analyze trajectory data diversity by plotting aircraft position histograms and comparing to Figure 4 in the paper

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the diversity of aircraft types and weather conditions in TartanAviation impact the performance of AI models trained on the dataset?
- Basis in paper: [explicit] The dataset captures diverse aircraft operations, seasons, aircraft types, and weather conditions.
- Why unresolved: The paper does not provide empirical results on how model performance varies with different aircraft types and weather conditions.
- What evidence would resolve it: Conducting experiments to evaluate AI model performance across different subsets of the dataset categorized by aircraft type and weather conditions.

### Open Question 2
- Question: What are the limitations of the current multi-modal approach in TartanAviation for improving air traffic control systems?
- Basis in paper: [inferred] The dataset aims to provide a holistic view of terminal airspace operations across various data modalities.
- Why unresolved: The paper does not discuss the limitations or challenges of integrating multi-modal data for air traffic control applications.
- What evidence would resolve it: Analyzing case studies or simulations where multi-modal data integration leads to improved or hindered air traffic control outcomes.

### Open Question 3
- Question: How can the TartanAviation dataset be used to advance fully or partially autonomous AAM agents?
- Basis in paper: [explicit] The dataset is vital for integrating AI and machine learning technologies into air traffic control systems and advancing autonomous aircraft.
- Why unresolved: The paper does not provide specific examples or methodologies for using the dataset to develop autonomous AAM agents.
- What evidence would resolve it: Developing and testing autonomous AAM agent prototypes using the dataset and evaluating their performance in simulated or real-world scenarios.

## Limitations

- Regional airport focus may limit generalizability to major hub operations despite providing operational diversity
- Coverage gaps exist for aircraft operating near but outside the 8km visual and 10km speech fences
- 4K camera resolution may struggle with small aircraft at maximum range despite being high resolution
- Speech dataset captures only pilot-to-ATC communications, missing potentially valuable controller-to-controller coordination data

## Confidence

- Multi-modal data benefits (High): Well-supported by concurrent collection methodology and clear evidence of diverse operational capture
- Regional airport diversity (Medium): Supported by airport selection rationale but requires empirical validation of diversity benefits
- Open-source accessibility (Medium): Code availability is confirmed, but usability depends on documentation quality not evaluated in the paper
- Data quality and synchronization (High): Explicit mention of validation procedures and time synchronization mechanisms

## Next Checks

1. Conduct temporal alignment validation across all three modalities for randomly sampled recordings to verify the 1Hz ADS-B trigger mechanism works consistently
2. Test model generalization by training on Pittsburgh regional airports and evaluating on a held-out major commercial airport dataset (if available)
3. Assess coverage completeness by calculating the percentage of total operations captured within the 8km visual and 10km speech fences versus total airport traffic volume