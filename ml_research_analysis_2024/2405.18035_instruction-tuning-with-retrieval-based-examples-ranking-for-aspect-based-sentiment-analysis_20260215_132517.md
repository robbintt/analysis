---
ver: rpa2
title: Instruction Tuning with Retrieval-based Examples Ranking for Aspect-based Sentiment
  Analysis
arxiv_id: '2405.18035'
source_url: https://arxiv.org/abs/2405.18035
tags:
- examples
- sentiment
- output
- example
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a retrieval-based example ranking method for
  instruction tuning in aspect-based sentiment analysis (ABSA). The approach uses
  a language model (LM) to score candidate examples based on their log-likelihood
  for a given target sample, then selects top-k examples as positive and bottom-k
  as negative for contrastive learning.
---

# Instruction Tuning with Retrieval-based Examples Ranking for Aspect-based Sentiment Analysis

## Quick Facts
- arXiv ID: 2405.18035
- Source URL: https://arxiv.org/abs/2405.18035
- Reference count: 27
- Primary result: Achieves F1 scores of 79.93-83.85 for aspect term extraction, accuracy of 91.47-97.47% for aspect term sentiment classification, and F1 scores of 73.41-81.49% for aspect sentiment pair extraction across four benchmark datasets

## Executive Summary
This paper proposes a retrieval-based example ranking method for instruction tuning in aspect-based sentiment analysis (ABSA). The approach uses a language model to score candidate examples based on their log-likelihood for a given target sample, then selects top-k examples as positive and bottom-k as negative for contrastive learning. An alternating training schema is proposed to train both the retriever and LM simultaneously. Experiments on three ABSA subtasks across four benchmark datasets show the proposed method outperforms various strong baseline models.

## Method Summary
The method employs a retriever that uses a language model (T5-based encoder-decoder) to score candidate examples based on log-likelihood of the target output given the example and task definition. For each target sample, the retriever selects top-k candidates as positive examples and bottom-k as negative examples for contrastive learning. The model uses an alternating training schema where the retriever is trained using the LM as scorer, then the LM is fine-tuned using examples selected by the updated retriever. The instruction template includes task definition, examples, and input text to guide the LM generation.

## Key Results
- Outperforms various strong baseline models across all three ABSA subtasks
- Achieves F1 scores of 79.93-83.85 for aspect term extraction (ATE)
- Achieves accuracy of 91.47-97.47% for aspect term sentiment classification (ATSC)
- Achieves F1 scores of 73.41-81.49% for aspect sentiment pair extraction (ASPE)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The retriever selects examples based on the LM's log-likelihood scoring, aligning retrieval with the generative objective.
- Mechanism: For each candidate example, the LM computes the log-likelihood of the target output given the example and task definition. Examples are ranked by this score, and top-k become positive, bottom-k negative for contrastive learning.
- Core assumption: The LM's likelihood estimate correlates with how useful an example is for generating the target output.
- Evidence anchors:
  - [abstract] "the retriever evaluates the importance of candidate examples based on the log-likelihood of the LM. This retrieval goal is consistent with the generative objective of the LM."
  - [section 3.2] "The LM adopted the encoder-decoder architecture of T5, which scores each candidate independently. The scoring function is the log-likelihood of output ys, which is consistent with the autoregressive decoding objective of the LM"
  - [corpus] Weak - no corpus evidence directly comparing this to traditional similarity metrics.
- Break condition: If the LM's likelihood score poorly correlates with example usefulness for the specific ABSA task, retrieval quality degrades.

### Mechanism 2
- Claim: Alternating training between retriever and LM improves performance by allowing each to adapt to the other's strengths.
- Mechanism: The retriever is trained using the LM as scorer; then the LM is fine-tuned using examples selected by the updated retriever. This iterative process continues for several steps.
- Core assumption: Retriever and LM can mutually improve through this iterative adaptation process.
- Evidence anchors:
  - [section 3.4] "we adopted an alternating training schema for the retriever and language models... the finetuned LM in the t−1 step is used as a scoring model to train the retriever in the t step. The LM in the t step is finetuned by the instruction generated by the updated retriever."
  - [section 4.6] "The results demonstrate the effectiveness of each part of the proposed method. For further analysis, the performance decrease in the term w/o alternating training suggests that alternating training schema can better narrow the gap between the retriever and LM."
  - [corpus] Weak - no direct comparison to non-alternating training on the same model architecture.
- Break condition: If the retriever and LM's objectives diverge significantly, alternating training may cause performance degradation rather than improvement.

### Mechanism 3
- Claim: Instruction templates with task definitions and examples improve LM generation quality for ABSA.
- Mechanism: For each input, the retriever selects top-k examples to form an instruction template that includes task definition, examples, and the input text. The LM generates output conditioned on this full prompt.
- Core assumption: Providing relevant examples and clear task definitions helps the LM understand the specific ABSA task requirements.
- Evidence anchors:
  - [section 3.1] "Figure 2 shows the instruction prompt template, which comprises a task definition, examples, and input text."
  - [section 4.4] "The model performance was measured using F1 scores for the ATE and ASPE tasks and accuracy for the ATSC task. Table 1 provides the details of the data distribution."
  - [corpus] Moderate - several related papers (InstructABSA) use similar template-based instruction tuning, providing indirect support.
- Break condition: If the examples selected are not representative or the template structure is suboptimal, generation quality may suffer.

## Foundational Learning

- Concept: Contrastive learning for retriever training
  - Why needed here: To train the retriever to distinguish useful examples (positive) from less useful ones (negative) based on LM scores
  - Quick check question: What is the objective function used to train the retriever in this paper?

- Concept: Alternating training schema
  - Why needed here: To allow simultaneous improvement of both retriever and LM by having each adapt to the other's outputs
  - Quick check question: How many steps of alternating training were used in the experiments?

- Concept: Instruction tuning with in-context examples
  - Why needed here: To provide the LM with task-specific context that improves generation quality for ABSA tasks
  - Quick check question: What are the three components of the instruction template used in this paper?

## Architecture Onboarding

- Component map: Retriever (T5 encoder) -> LM (T5 encoder-decoder) -> Alternating training controller
- Critical path: For each training sample → Retriever selects candidates → LM scores candidates → Contrastive loss updates retriever → Retriever selects top-k examples → LM fine-tuned on instruction template
- Design tradeoffs: Retriever uses only a subset (ratio r) of training data to reduce computation; LM uses both for scoring and generation, avoiding additional encoders
- Failure signatures: Poor retrieval quality manifests as inconsistent performance across datasets; alternating training failure shows as performance worse than non-alternating approach
- First 3 experiments:
  1. Run retriever-only retrieval and examine top-1 example selection quality on a validation set
  2. Test LM generation with fixed examples vs retrieved examples on a small validation set
  3. Run full alternating training for 1-2 steps and measure performance change

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed method perform on aspect-based sentiment analysis tasks in languages other than English, such as Russian, French, or Chinese?
- Basis in paper: [explicit] The paper mentions that the proposed method is only experimented on the English dataset and acknowledges that its performance in other languages remains unknown.
- Why unresolved: The paper does not provide any experimental results or analysis for languages other than English.
- What evidence would resolve it: Conducting experiments on multilingual datasets and comparing the performance of the proposed method across different languages would provide evidence to resolve this question.

### Open Question 2
- Question: How does the proposed method handle mixed language and multilingual datasets in aspect-based sentiment analysis tasks?
- Basis in paper: [explicit] The paper mentions that the proposed method is only experimented on the English dataset and acknowledges that its performance on mixed language and multilingual datasets remains unknown.
- Why unresolved: The paper does not provide any experimental results or analysis for mixed language and multilingual datasets.
- What evidence would resolve it: Conducting experiments on mixed language and multilingual datasets and comparing the performance of the proposed method with other methods would provide evidence to resolve this question.

### Open Question 3
- Question: What is the impact of using different numbers of examples (k) on the performance of the proposed method in aspect-based sentiment analysis tasks?
- Basis in paper: [explicit] The paper mentions that the choice of the number k of examples constrains the performance of the model and explores the impact of using different numbers of examples on the overall performance.
- Why unresolved: The paper does not provide a definitive answer on the optimal number of examples for different test inputs.
- What evidence would resolve it: Conducting experiments with varying numbers of examples for different test inputs and analyzing the performance of the proposed method would provide evidence to resolve this question.

## Limitations

- Experimental validation is limited to only four benchmark datasets from Semeval challenges, which may not represent the full diversity of ABSA scenarios
- The alternating training schema lacks comprehensive ablation analysis and direct comparisons to alternative training strategies on identical model architectures
- The method's generalizability to other NLP tasks beyond ABSA is not explored

## Confidence

High confidence: The overall effectiveness of the retrieval-based example ranking approach for ABSA tasks, supported by consistent performance improvements across all three subtasks and four datasets.

Medium confidence: The superiority of the alternating training schema, as the evidence is limited to ablation studies without direct comparisons to alternative training strategies on the same model architecture.

Medium confidence: The efficiency claims regarding using the LM for both scoring and generation, as the computational savings are stated but not quantitatively validated against alternative architectures that might use separate models.

## Next Checks

1. **Ablation on alternating training steps**: Systematically vary the number of alternating training steps (1, 2, 3, 5) and measure the performance impact on each ABSA subtask to identify the optimal number of iterations and understand the convergence behavior.

2. **Cross-domain generalization test**: Apply the trained model to an out-of-domain ABSA dataset (e.g., different product or service domains) to evaluate how well the retrieval-based approach generalizes beyond the four benchmark datasets used in the paper.

3. **Comparison with non-retrieval baselines**: Implement and compare against strong non-retrieval instruction tuning approaches (like InstructABSA) using identical model architectures and training procedures to isolate the specific contribution of the retrieval component.