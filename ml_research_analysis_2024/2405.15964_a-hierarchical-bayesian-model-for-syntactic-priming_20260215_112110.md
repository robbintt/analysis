---
ver: rpa2
title: A hierarchical Bayesian model for syntactic priming
arxiv_id: '2405.15964'
source_url: https://arxiv.org/abs/2405.15964
tags:
- priming
- syntactic
- effect
- verb
- lexical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study proposes a hierarchical Bayesian model (HBM) to explain
  three empirical properties of syntactic priming: lexical boost, inverse frequency
  effect, and asymmetrical decay. The model represents syntactic knowledge at two
  levels: verb-specific biases and an abstract global bias, updated through Bayesian
  inference.'
---

# A hierarchical Bayesian model for syntactic priming

## Quick Facts
- arXiv ID: 2405.15964
- Source URL: https://arxiv.org/abs/2405.15964
- Reference count: 11
- Primary result: Hierarchical Bayesian model explains lexical boost, inverse frequency effect, and asymmetrical decay in syntactic priming through implicit learning mechanisms

## Executive Summary
This study proposes a hierarchical Bayesian model (HBM) to explain three empirical properties of syntactic priming: lexical boost, inverse frequency effect, and asymmetrical decay. The model represents syntactic knowledge at two levels: verb-specific biases and an abstract global bias, updated through Bayesian inference. In simulations using materials from Pickering and Branigan (1998), the HBM successfully captures all three properties, showing that implicit learning alone can account for phenomena typically explained by residual activation. The lexical boost is interpreted as lexical transfer, where learning on one verb generalizes to others, while asymmetrical decay is explained by interference-based unlearning—higher-level abstract knowledge is more stable and decays slower than verb-specific knowledge. The results suggest that syntactic priming has a lexical basis, with abstract priming emerging from verb-specific effects.

## Method Summary
The hierarchical Bayesian model represents syntactic knowledge through verb-specific and global decision biases, updated via Bayesian inference based on prime exposure. The model was tested through two simulations: (1) reproducing priming effects with verb overlap manipulation, and (2) testing decay patterns with post-priming interference. The model uses a prior dataset of 100 verb-construction pairs and samples from this distribution to simulate language processing. Bayesian updating occurs at both levels of the hierarchy, with lower-level verb-specific statistics being more susceptible to interference from post-priming data while higher-level abstract statistics remain more stable due to aggregation across multiple verbs.

## Key Results
- The HBM successfully captures all three empirical properties of syntactic priming in simulations
- Lexical boost emerges as a natural consequence of hierarchical Bayesian updating and lexical transfer
- Inverse frequency effect is explained through Bayesian prediction error proportional to prior probability
- Asymmetrical decay occurs due to differential stability between verb-specific and abstract knowledge levels

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Lexical boost arises from lexical transfer via hierarchical Bayesian updating
- Mechanism: The model represents syntactic knowledge at two levels: verb-specific biases (lower level) and abstract global bias (higher level). When a verb-specific bias is updated by exposure, the change propagates upward to update the global bias, which then influences other verbs' biases through Bayesian inference
- Core assumption: Syntactic priming operates primarily at the verb-specific level, with abstract priming emerging as a secondary effect
- Evidence anchors:
  - [abstract]: "The model represents syntactic knowledge in a hierarchical structure of syntactic statistics, where a lower level represents the verb-specific biases of syntactic decisions, and a higher level represents the abstract bias as an aggregation of verb-specific biases"
  - [section]: "The hierarchical structure of the generative model means that experience with one verb can cause changes not only to that verb's decision bias, but also to the global decision bias, which in turn affects other verbs"
  - [corpus]: Weak evidence - corpus shows related work on lexical-driven priming but no direct evidence for this specific hierarchical mechanism
- Break condition: If priming occurs without any verb-specific learning, or if verb-independent priming exists that cannot be explained by aggregated verb-specific effects

### Mechanism 2
- Claim: Inverse frequency effect results from Bayesian prediction error proportional to prior probability
- Mechanism: Less frequent constructions have lower prior probability, creating larger prediction errors when encountered. Bayesian updating responds more strongly to larger prediction errors, resulting in stronger priming effects for infrequent structures
- Core assumption: The magnitude of learning updates is proportional to prediction error magnitude
- Evidence anchors:
  - [abstract]: "infrequent constructions or co-occurrences cause larger prediction errors and thus stronger priming effect"
  - [section]: "the priming effect size is proportional to the prediction error when processing the prime"
  - [corpus]: Moderate evidence - corpus shows inverse frequency is a well-documented phenomenon, though the specific Bayesian error-based explanation is less established
- Break condition: If frequency effects persist when prediction errors are controlled for, or if priming strength doesn't correlate with surprisal

### Mechanism 3
- Claim: Asymmetrical decay occurs due to interference-based unlearning with differential stability between levels
- Mechanism: Lower-level verb-specific statistics are more susceptible to interference from post-priming data due to sparsity, while higher-level abstract statistics are more stable due to aggregation across many verbs. This creates faster decay for lexical boost (lower-level) compared to abstract priming (higher-level)
- Core assumption: Memory interference, not temporal decay, drives unlearning, and hierarchical levels have different interference susceptibilities
- Evidence anchors:
  - [section]: "The lower-level statistics, due to the sparsity of data, can be fairly susceptible to a small amount of incoming data. The higher-level statistics, in contrast, is much more stable as it collects information from all the lower-level statistics"
  - [section]: "This interference-based unlearning naturally provides a mechanism for asymmetrical decay in HBM"
  - [corpus]: Weak evidence - corpus mentions interference-based decay but doesn't specifically connect it to hierarchical Bayesian models
- Break condition: If decay patterns don't match interference predictions, or if temporal factors dominate over interference

## Foundational Learning

- Concept: Bayesian belief updating
  - Why needed here: The model uses Bayesian inference to update syntactic knowledge based on experience, with the magnitude of updates proportional to prediction error
  - Quick check question: If you observe a rare construction, will the Bayesian update be larger or smaller than for a common construction?

- Concept: Hierarchical modeling
  - Why needed here: The model represents knowledge at multiple levels of abstraction, allowing generalization from verb-specific to abstract syntactic patterns
  - Quick check question: In a two-level hierarchy, if you learn something about one verb, which other verbs' knowledge might be affected and how?

- Concept: Prediction error as learning signal
  - Why needed here: The model's learning is driven by the discrepancy between expected and observed linguistic events, with larger errors producing stronger learning
  - Quick check question: If you expect a construction with probability 0.9 but observe it 50% of the time, is the prediction error large or small?

## Architecture Onboarding

- Component map: Prior dataset generator -> Bayesian inference engine -> Marginalization layer -> Effect measurement
- Critical path: Prior generation → Prime exposure → Bayesian update → Marginalization → Effect measurement
- Design tradeoffs: The model trades computational simplicity for explanatory power - it can explain three phenomena with one mechanism but requires maintaining hierarchical representations and performing marginalization
- Failure signatures: If the model fails to capture lexical boost, check the prior distribution; if it fails to capture asymmetrical decay, check the interference mechanism; if it fails to capture inverse frequency, check the prediction error calculation
- First 3 experiments:
  1. Test lexical transfer by priming with one verb and measuring effects on untrained verbs
  2. Test inverse frequency by varying prime frequency and measuring priming strength
  3. Test asymmetrical decay by measuring priming strength after post-priming interference at different levels

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the model account for the complete loss of lexical boost observed in some empirical studies?
- Basis in paper: [explicit] The authors note that while the model captures the qualitative pattern that lexical boost decays faster than verb-independent priming, it cannot reproduce the complete loss of lexical boost observed in studies like Hartsuiker et al. (2008).
- Why unresolved: The current hierarchical Bayesian model always predicts a stronger priming effect with verb overlap, even after decay, which contradicts empirical findings showing complete loss of the lexical boost benefit.
- What evidence would resolve it: Empirical data demonstrating conditions under which lexical boost completely disappears versus conditions where it persists would help refine the model. Additionally, testing whether adding a mechanism for complete unlearning of verb-specific biases could better match the empirical data.

### Open Question 2
- Question: How can the model be extended to handle filler items that involve different constructions than the primed structure?
- Basis in paper: [explicit] The authors acknowledge that sampling from prior data in post-priming stages has limitations, especially when intervening filler items in experiments involve constructions different from those represented in the model's statistics.
- Why unresolved: The current model doesn't explicitly include a network of constructions and their relationships, making it difficult to account for decay when post-priming trials involve apparently unrelated constructions.
- What evidence would resolve it: Empirical data on how different types of filler items affect the decay of syntactic priming, and testing whether models with explicit construction networks better capture the observed decay patterns would be informative.

### Open Question 3
- Question: What is the mechanism for long-term persistence of syntactic priming across different time scales and experimental contexts?
- Basis in paper: [explicit] The authors discuss the difficulty of the implicit learning account in explaining long-term priming, which has been considered a major limitation compared to residual activation accounts.
- Why unresolved: While the hierarchical Bayesian model can capture some aspects of decay, it doesn't fully explain how syntactic priming persists across different time scales and contexts, particularly in comprehension where verb overlap seems necessary for reliable effects.
- What evidence would resolve it: Longitudinal studies tracking syntactic priming effects across extended time periods and diverse contexts, and testing whether incorporating mechanisms for long-term memory consolidation improves model predictions would be valuable.

## Limitations

- The model cannot reproduce the complete loss of lexical boost observed in some empirical studies with filler conditions
- The interference-based decay mechanism lacks direct empirical validation in the syntactic priming literature
- The hierarchical structure may oversimplify the complex interaction between lexical and abstract representations

## Confidence

- High confidence: The model's ability to capture the inverse frequency effect through Bayesian prediction error - this follows directly from well-established principles of Bayesian updating
- Medium confidence: The explanation of lexical boost through hierarchical transfer - while the mechanism is sound, the assumption that abstract priming emerges entirely from verb-specific effects requires further validation
- Low confidence: The interference-based explanation for asymmetrical decay - this mechanism is the most speculative and least directly supported by existing evidence

## Next Checks

1. Test whether the model can reproduce the complete loss of lexical boost observed in filler conditions by incorporating realistic post-priming data distributions
2. Validate the interference-based decay mechanism by comparing predicted decay patterns against empirical data from experiments with controlled post-priming exposure
3. Examine whether the model can account for priming effects in verb-independent constructions where hierarchical transfer should not apply