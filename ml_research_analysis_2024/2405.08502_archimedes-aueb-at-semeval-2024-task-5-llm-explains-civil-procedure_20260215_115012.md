---
ver: rpa2
title: 'Archimedes-AUEB at SemEval-2024 Task 5: LLM explains Civil Procedure'
arxiv_id: '2405.08502'
source_url: https://arxiv.org/abs/2405.08502
tags:
- question
- table
- legal
- federal
- explanation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work presents a system for the SemEval-2024 Task 5 on legal
  argument reasoning in civil procedure. The approach uses a small LLM fine-tuned
  on Chain-of-Thought explanations and synthetic data generated by a larger teacher
  LLM.
---

# Archimedes-AUEB at SemEval-2024 Task 5: LLM explains Civil Procedure

## Quick Facts
- arXiv ID: 2405.08502
- Source URL: https://arxiv.org/abs/2405.08502
- Reference count: 11
- Primary result: Ranked 15th in SemEval-2024 Task 5 using fine-tuned Llama-2-7B with Chain-of-Thought explanations grounded in human analyses

## Executive Summary
This work presents a system for legal argument reasoning in civil procedure that uses a small LLM fine-tuned on Chain-of-Thought explanations and synthetic data generated by a larger teacher LLM. The approach grounds explanations in authentic human analyses rather than the teacher's internal knowledge, producing more reliable reasoning signals. A novel mutation method creates synthetic examples by transforming existing instances while preserving core legal concepts. The system outperformed its teacher model and produced explanations aligned with expert analyses, achieving 15th place in the competition.

## Method Summary
The method uses GPT-3.5 to generate Chain-of-Thought explanations refined by human experts, creating a grounding in authentic legal reasoning rather than teacher LLM knowledge. A novel mutation method generates synthetic examples by transforming existing instances while preserving core legal concepts. The system fine-tunes Llama-2-7B on combined original and synthetic data with explanations, achieving improved performance through this approach. Manual evaluation by legal experts assessed explanation quality and identified reasoning errors stemming from either model limitations or gaps in legal knowledge.

## Key Results
- Ranked 15th in SemEval-2024 Task 5 competition
- Outperformed the teacher model (GPT-3.5) on the legal reasoning task
- Generated explanations aligned with expert analyses and identified reasoning errors
- Demonstrated effectiveness of grounding explanations in human analyses rather than teacher LLM knowledge

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning on human-guided explanations grounded in authentic legal analyses produces more reliable reasoning signals than using explanations directly derived from the teacher LLM's internal knowledge.
- Mechanism: The small student LLM learns to generate explanations that align with expert legal reasoning rather than potentially inconsistent or hallucinated reasoning from the teacher LLM. This grounding provides a more stable and accurate training signal.
- Core assumption: Human expert analyses represent the gold standard for legal reasoning quality, and explanations aligned with these analyses will produce better downstream reasoning than explanations generated purely by the teacher LLM.
- Evidence anchors:
  - [abstract]: "Contrary to previous work, our explanations are not directly derived from the teacher's internal knowledge. Instead they are grounded in authentic human analyses, therefore delivering a superior reasoning signal."
  - [section 3.3]: "We refine the expert's analysis with the assistance of GPT-3.5 to align it with our desired properties" and "CoT explanations that will be leveraged for fine-tuning must adhere to a uniform and formal style"

### Mechanism 2
- Claim: The consistency filter in the Multiple Choice Mutation method significantly improves synthetic data quality by eliminating examples where the teacher LLM gives inconsistent answers across prompt stages.
- Mechanism: By requiring the teacher LLM to give the same correct answer in both prompt stages, the method filters out synthetic examples where the teacher model was uncertain or contradictory, reducing noise in the training data.
- Core assumption: Consistency in the teacher LLM's responses across different prompting approaches indicates higher confidence and correctness, making those examples more valuable for training.
- Evidence anchors:
  - [section 3.4]: "if the option chosen in the first stage is not same as the option chosen in the second stage, meaning that GPT-3.5 answered inconsistently this question, the example is discarded"
  - [section 5.3]: "MCM's performance drops significantly (4 percentage points) without consistency filtering (no fltr, see Table 7), highlighting its value in discarding poor synthetic examples"

### Mechanism 3
- Claim: The mutation method generates synthetic examples that preserve the core legal reasoning patterns while introducing new factual scenarios, allowing the student LLM to generalize better to unseen legal situations.
- Mechanism: By transforming existing examples into new scenarios that require similar legal reasoning but with different facts, the model learns to apply legal principles across various contexts rather than memorizing specific instances.
- Core assumption: Legal reasoning patterns are transferable across different factual scenarios, so training on mutated examples that preserve reasoning structure while varying facts improves generalization.
- Evidence anchors:
  - [abstract]: "a new 'mutation' method generates artificial data instances inspired from existing ones"
  - [section 3.4]: "Our goal with this technique is to generate artificial data that demand similar reasoning skills and legal knowledge to the original ones"

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: Legal argument reasoning requires multiple logical steps and consideration of various legal principles. CoT explanations help the model break down complex reasoning into traceable steps.
  - Quick check question: Can you explain why a federal court in Rhode Island would apply Vermont law in a case where the accident happened in Vermont?

- Concept: Fine-tuning vs. few-shot prompting
  - Why needed here: The task requires specialized legal reasoning that few-shot prompting with large models cannot achieve as effectively as fine-tuning a smaller model on domain-specific data with explanations.
  - Quick check question: Why might fine-tuning a Llama-2-7B model on legal explanations outperform using GPT-4 with few-shot prompting for this task?

- Concept: Data augmentation through synthetic generation
  - Why needed here: The original dataset is relatively small (666 training samples), so synthetic data generation helps expand the training set while maintaining the complexity and reasoning requirements of legal questions.
  - Quick check question: How does the consistency filter in the mutation method help ensure that synthetic examples are reliable training data?

## Architecture Onboarding

- Component map: Teacher LLM (GPT-3.5) -> Consistency filter -> Fine-tuning pipeline -> Legal expert evaluation
- Critical path: Teacher LLM generates explanations → Consistency filter validates examples → Fine-tuning pipeline trains student model → Legal experts evaluate explanation quality
- Design tradeoffs: Using a smaller student model (Llama-2-7B) instead of a larger model allows for more efficient fine-tuning and deployment, but may limit maximum performance compared to larger models. The choice to ground explanations in human analyses rather than teacher LLM knowledge trades some scalability for improved reasoning quality.
- Failure signatures: Performance drops when synthetic data quality is poor (inconsistent teacher responses), when explanations are not properly aligned with human analyses, or when the fine-tuning process overfits to synthetic examples.
- First 3 experiments:
  1. Fine-tune Llama-2-7B on original training data without any explanations to establish baseline performance
  2. Fine-tune on original data extended with human-guided explanations (HGE) to measure impact of explanation grounding
  3. Fine-tune on HGE data plus synthetic data from MCM to measure impact of synthetic data augmentation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the "mutation" method affect the model's ability to generalize to unseen legal concepts?
- Basis in paper: [explicit] The paper describes a novel "mutation" method that generates synthetic data by transforming existing instances while preserving core legal concepts.
- Why unresolved: While the paper shows that the mutation method improves performance, it does not investigate whether this improvement translates to better generalization on legal concepts not present in the training data.
- What evidence would resolve it: Experiments comparing the model's performance on a held-out set of legal concepts not seen during training, with and without the mutation method.

### Open Question 2
- Question: What is the optimal amount of synthetic data to generate using the multiple choice mutation (MCM) method?
- Basis in paper: [inferred] The paper explores the effect of using more synthetic data from MCM but finds that after a certain point, additional data does not improve F1 score and may even decrease it.
- Why unresolved: The paper does not determine the optimal amount of synthetic data to generate for maximum performance improvement.
- What evidence would resolve it: A systematic study varying the amount of synthetic data generated by MCM and measuring its impact on model performance.

### Open Question 3
- Question: How does the quality of human-guided explanations (HGE) impact the model's performance compared to explanations generated solely by the teacher LLM?
- Basis in paper: [explicit] The paper uses HGE, which are refined expert analyses, and compares its performance to a baseline without HGE.
- Why unresolved: While the paper shows that HGE improves performance, it does not compare this improvement to using explanations generated solely by the teacher LLM without expert refinement.
- What evidence would resolve it: Experiments comparing the model's performance when fine-tuned on explanations generated solely by the teacher LLM versus HGE.

## Limitations
- Manual evaluation by legal experts covered only a small subset of test cases, potentially missing broader performance patterns
- Effectiveness of mutation method heavily depends on teacher LLM response quality, which may vary across legal domains
- Ranking of 15th place demonstrates competitiveness but doesn't establish superiority over all alternative approaches

## Confidence
- High confidence: The mechanism of explanation grounding (Mechanism 1) and the consistency filter's impact on synthetic data quality (Mechanism 2) are well-supported by the evidence provided in the paper.
- Medium confidence: The mutation method's effectiveness in improving generalization (Mechanism 3) is supported by performance metrics but would benefit from more extensive qualitative analysis.
- Medium confidence: The overall ranking performance (15th place) demonstrates the approach's competitiveness but doesn't fully establish superiority over all alternative methods.

## Next Checks
1. Conduct systematic manual evaluation of model explanations across all test cases, not just a subset, to better understand performance patterns and error types.
2. Test the approach on legal questions from different jurisdictions or legal domains to assess generalizability beyond civil procedure in the US context.
3. Compare performance using explanations directly from the teacher LLM versus the grounded human analyses to quantify the exact contribution of the grounding approach.