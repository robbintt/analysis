---
ver: rpa2
title: Human-AI collectives produce the most accurate differential diagnoses
arxiv_id: '2406.14981'
source_url: https://arxiv.org/abs/2406.14981
tags:
- llms
- human
- medical
- base
- diagnoses
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether combining human expertise with
  large language models (LLMs) can improve diagnostic accuracy in open-ended medical
  problems. The authors developed a hybrid collective intelligence system that integrates
  differential diagnoses from 40,762 physicians with outputs from five state-of-the-art
  LLMs across 2,133 medical cases.
---

# Human-AI collectives produce the most accurate differential diagnoses

## Quick Facts
- arXiv ID: 2406.14981
- Source URL: https://arxiv.org/abs/2406.14981
- Reference count: 40
- Key outcome: Hybrid collectives of physicians and LLMs outperform both individual humans and AI models in differential diagnosis accuracy

## Executive Summary
This study investigates whether combining human expertise with large language models (LLMs) can improve diagnostic accuracy in open-ended medical problems. The authors developed a hybrid collective intelligence system that integrates differential diagnoses from 40,762 physicians with outputs from five state-of-the-art LLMs across 2,133 medical cases. Using a weighted aggregation approach based on performance in a training fold, they found that hybrid collectives consistently outperformed both individual physicians and LLMs, as well as physician-only and LLM-only collectives. The improvement was particularly notable because physicians compensated for LLM errors when LLMs failed to include correct diagnoses.

## Method Summary
The study combined differential diagnoses from 40,762 physicians with outputs from five LLMs (Claude 3 Opus, GPT-4, Gemini Pro 1.0, Llama 2 70B, Mistral Large) across 2,133 medical cases from the Human Diagnosis Project. Raw text diagnoses were normalized and mapped to standardized SNOMED CT concepts. A weighted majority voting ensemble (WMVE) approach dynamically assigned weights to each diagnostician based on their performance in training folds. The system used five-fold cross-validation with 1/r scoring rule for aggregation, evaluating top-5, top-3, top-1 accuracy and mean reciprocal rank (MRR).

## Key Results
- Hybrid collectives consistently outperformed individual physicians, individual LLMs, and physician-only or LLM-only collectives across all evaluation metrics
- The improvement was particularly notable because physicians compensated for LLM errors when LLMs failed to include correct diagnoses
- Results held across various medical specialties and levels of professional experience

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hybrid collectives outperform both individual humans and LLMs because humans compensate for LLM errors when LLMs fail to include correct diagnoses.
- Mechanism: Physicians and LLMs make different types of errors that are not highly correlated. When LLMs miss correct diagnoses, physicians often provide them, and vice versa, allowing error diversity to reduce overall error rates through aggregation.
- Core assumption: Physicians and LLMs produce independent and complementary errors across the vast solution space of differential diagnoses.
- Evidence anchors:
  - [abstract] "physicians compensated for LLM errors when LLMs failed to include correct diagnoses"
  - [section] "Crucially, when LLMs did not list the correct diagnosis at all (range across LLMs: 34%–54%; right-most columns), individual humans did mention it in a substantial number of cases (range across LLMs: 30%–38%; right-most columns excluding bottom-right cells), most frequently ranking it first (range across LLMs: 20%–27%, top-right cells)."
  - [corpus] Weak - corpus contains related studies on hybrid intelligence but no direct evidence for this specific error complementarity mechanism.
- Break condition: If physicians and LLMs start making correlated errors (e.g., both missing the same rare diagnoses), the complementarity advantage would diminish.

### Mechanism 2
- Claim: Weighted aggregation of human and LLM inputs improves performance over simple majority voting when there are substantial differences in individual accuracy.
- Mechanism: The Weighted Majority Voting Ensemble (WMVE) approach dynamically adjusts weights for each diagnostician based on their performance in the training fold, giving more weight to more accurate contributors.
- Core assumption: Performance differences between humans and LLMs are measurable and stable enough to inform weighting during cross-validation.
- Evidence anchors:
  - [section] "we used the Weighted Majority Voting Ensemble (WMVE) approach described in [43] to determine weights for LLMs and humans"
  - [section] "weights were determined on one-fifth of the cases and calculated for each configuration"
  - [corpus] Weak - corpus contains related ensemble methods but no specific evidence for WMVE application to hybrid human-AI collectives.
- Break condition: If performance differences between humans and LLMs are not stable across folds or if the training set is too small to reliably estimate weights.

### Mechanism 3
- Claim: Mapping raw text diagnoses to standardized SNOMED CT concepts enables principled aggregation of open-ended responses from humans and LLMs.
- Mechanism: The text normalization and matching pipeline converts diverse free-text diagnoses into unique, comparable identifiers, allowing aggregation algorithms to combine responses meaningfully.
- Core assumption: SNOMED CT contains comprehensive coverage of medical concepts and synonyms that can map to the diverse diagnostic language used by humans and LLMs.
- Evidence anchors:
  - [section] "we developed a method and processing pipeline that leveraged the comprehensive SNOMED CT healthcare terminology"
  - [section] "Applying this approach, as described in [36], produced a match for 90% of the correct case diagnoses"
  - [corpus] Weak - corpus contains related medical ontology work but no specific evidence for this SNOMED CT matching pipeline.
- Break condition: If SNOMED CT coverage is incomplete for rare diseases or if the matching algorithm fails on novel terminology patterns.

## Foundational Learning

- Concept: Cross-validation methodology
  - Why needed here: To evaluate model performance on unseen data and prevent overfitting when selecting prompts and calculating weights
  - Quick check question: What portion of the data is used for training versus testing in a 5-fold cross-validation setup?

- Concept: Weighted ensemble methods
  - Why needed here: To combine predictions from multiple diagnosticians with different accuracy levels
  - Quick check question: How does the WMVE algorithm update weights based on individual performance?

- Concept: Natural language processing for medical terminology
  - Why needed here: To standardize diverse diagnostic language into comparable concepts for aggregation
  - Quick check question: What are the two main steps in the SNOMED CT matching pipeline described?

## Architecture Onboarding

- Component map: Data ingestion -> LLM processing -> Text normalization -> Weight calculation -> Aggregation -> Evaluation
- Critical path: Case vignette → LLM prompt generation → response processing → SNOMED CT matching → weighted aggregation → performance evaluation
- Design tradeoffs:
  - Prompt complexity vs. response quality: More complex prompts may yield better answers but increase computational cost
  - SNOMED CT matching accuracy vs. coverage: Stricter matching may miss some diagnoses while looser matching may introduce errors
  - Weight calculation vs. sample size: More data for weight calculation improves reliability but reduces training data
- Failure signatures:
  - Low top-1 accuracy despite high top-5 accuracy suggests aggregation is spreading correct diagnoses too thin
  - Sudden performance drops when adding new LLMs may indicate correlated errors
  - SNOMED CT matching failures may manifest as missing diagnoses in collective output
- First 3 experiments:
  1. Run cross-validation with equal weights instead of WMVE to establish baseline performance
  2. Test different prompt configurations on a subset of cases to find optimal prompt structure
  3. Evaluate SNOMED CT matching accuracy on a manually labeled subset of diagnoses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the hybrid human-AI collective approach maintain its diagnostic accuracy advantage when applied to cases with more complex or rare medical conditions that may be underrepresented in the training data?
- Basis in paper: [inferred] The paper mentions that the Human Dx case selection procedure may have excluded very difficult or rare cases, suggesting this could be a limitation.
- Why unresolved: The study's dataset was selected by an expert panel and may not represent the full range of medical complexity encountered in real-world practice.
- What evidence would resolve it: Testing the hybrid approach on a more diverse and representative set of cases, including rare conditions and complex presentations, would show whether the accuracy advantage persists.

### Open Question 2
- Question: How does the hybrid human-AI approach affect the likelihood of arriving at beneficial versus harmful treatments, and how does this vary based on cultural, regional, and institutional contexts?
- Basis in paper: [explicit] The paper explicitly states that their analyses do not consider the consequences of treatments implied by diagnoses and suggests this as future research direction.
- Why unresolved: The study focused solely on diagnostic accuracy without examining downstream treatment implications or contextual factors.
- What evidence would resolve it: Conducting a longitudinal study tracking patient outcomes based on diagnoses from hybrid versus traditional approaches across different healthcare systems would provide answers.

### Open Question 3
- Question: To what extent does the integration of humans and LLMs mitigate or amplify biases present in both parties, particularly regarding race-based medicine and health disparities?
- Basis in paper: [explicit] The paper acknowledges that LLMs have been shown to perpetuate race-based medicine and suggests this as an important area for future research.
- Why unresolved: The study did not directly examine fairness and equity outcomes of the hybrid approach.
- What evidence would resolve it: Analyzing diagnostic outcomes and treatment recommendations across different demographic groups using the hybrid approach would reveal whether biases are reduced or amplified.

## Limitations
- Reliance on SNOMED CT matching introduces uncertainty about coverage for rare diseases and novel diagnostic terms
- WMVE weight calculation depends on stable performance differences that may not hold across different medical specialties
- Study uses a specific dataset (Human Dx) that may not represent all medical diagnostic contexts

## Confidence

- **High Confidence**: The core finding that hybrid collectives outperform individual contributors is well-supported by cross-validation results across multiple metrics
- **Medium Confidence**: The mechanism explanation (error complementarity between humans and LLMs) is plausible but requires further testing to confirm it's the primary driver
- **Low Confidence**: The specific performance thresholds and weight calculations may not generalize to different datasets or medical contexts without re-validation

## Next Checks
1. **Error Correlation Analysis**: Quantify the correlation between physician and LLM errors across different medical specialties to confirm the complementarity mechanism proposed.
2. **Cross-Dataset Validation**: Test the hybrid collective approach on an independent medical dataset with different case distributions to assess generalizability.
3. **Prompt Sensitivity Testing**: Systematically vary LLM prompts and few-shot examples to determine how sensitive performance is to prompt engineering choices.