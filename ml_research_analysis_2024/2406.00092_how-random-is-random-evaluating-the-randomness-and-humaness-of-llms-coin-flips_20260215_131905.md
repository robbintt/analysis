---
ver: rpa2
title: How Random is Random? Evaluating the Randomness and Humaness of LLMs' Coin
  Flips
arxiv_id: '2406.00092'
source_url: https://arxiv.org/abs/2406.00092
tags:
- llama
- random
- randomness
- human
- flip
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates how Large Language Models (LLMs) approach
  randomness and where they fail, through the lens of generating binary random sequences.
  The authors found that GPT-4 and Llama-3 exhibit and exacerbate nearly every human
  bias in this context, while GPT-3.5 exhibits more random behavior.
---

# How Random is Random? Evaluating the Randomness and Humaness of LLMs' Coin Flips

## Quick Facts
- **arXiv ID**: 2406.00092
- **Source URL**: https://arxiv.org/abs/2406.00092
- **Reference count**: 40
- **Primary result**: GPT-4 and Llama-3 exhibit and exacerbate human biases in random binary sequence generation, while GPT-3.5 shows more random behavior.

## Executive Summary
This paper investigates how Large Language Models approach randomness by asking them to generate binary coin flip sequences. The authors found that GPT-4 and Llama-3 exhibit strong human-like biases including heads bias, alternation preference, and run avoidance, while GPT-3.5 produces more genuinely random sequences. The study reveals a fundamental dichotomy in LLMs: they either produce truly random outputs or exhibit exaggerated human biases, with important implications for their use in tasks requiring randomness or human behavior prediction.

## Method Summary
The authors prompted three LLMs (GPT-3.5, GPT-4, and Llama-3) to generate sequences of 20 coin flips at various temperature settings (0.0 to 1.5). They analyzed the generated sequences for basic statistics (heads/tails distribution, first flip bias) and pattern frequencies (alternations, runs, n-grams). A LASSO model was trained to predict the 8th flip given the previous 7 flips, with mean squared error used as a measure of randomness. The LLM-generated sequences were compared against both random sequences and human-generated random sequences.

## Key Results
- GPT-4 and Llama-3 show strong bias toward heads in single flips and the first flip of sequences
- All models exhibit excessive alternation bias, producing more alternating patterns than expected from true randomness
- GPT-3.5 achieves higher mean squared error in prediction tasks, indicating more random behavior
- Temperature affects randomness, with higher temperatures generally producing more random sequences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs inherit and amplify human biases in random binary sequence generation due to training on human-generated text data.
- Mechanism: During training, LLMs learn patterns from human text where randomness is often poorly generated (e.g., avoiding long runs, favoring alternations). These learned patterns persist and are amplified when LLMs generate random sequences.
- Core assumption: The text corpus contains sufficient examples of human-generated "random" sequences that the LLM can learn these patterns.
- Evidence anchors:
  - [abstract]: "LLMs are supplied with human data and prone to human biases."
  - [section]: "machines have not only learned human biases in their dealing with randomness, but they have exacerbated this bias to be worse than humans in a large variety of ways."
  - [corpus]: Weak - no direct evidence in neighbor papers about LLM training data composition or bias amplification.
- Break condition: If the training corpus lacks sufficient examples of human-generated random sequences, or if the LLM architecture has strong regularization against pattern replication.

### Mechanism 2
- Claim: Temperature controls in LLMs affect the degree of randomness in generated sequences, with lower temperatures producing more deterministic (biased) outputs.
- Mechanism: Temperature scales the logits before softmax sampling. Lower temperatures make the distribution peakier, favoring high-probability tokens. For randomness tasks, this means the LLM consistently picks patterns it's learned (e.g., alternations, avoiding runs).
- Core assumption: The LLM's output distribution is sensitive to temperature scaling in a way that affects pattern selection.
- Evidence anchors:
  - [section]: "Higher temperature, therefore, seems likely to give more random (and thus less consistent) results."
  - [section]: "we test for temperatures from 0 to 1.5" and observe pattern changes.
  - [corpus]: Weak - neighbor papers don't discuss temperature's role in randomness quality.
- Break condition: If the task is too simple for temperature to matter, or if the LLM uses deterministic sampling regardless of temperature.

### Mechanism 3
- Claim: Different LLM architectures (GPT-3.5 vs GPT-4 vs Llama-3) exhibit varying degrees of humanness in randomness generation due to architectural and training differences.
- Mechanism: Each model's architecture and training procedure creates different inductive biases. GPT-3.5 shows more "random" behavior (higher MSE) while GPT-4 and Llama-3 amplify human biases more strongly.
- Core assumption: Architectural differences between models lead to different learned representations of randomness patterns.
- Evidence anchors:
  - [abstract]: "GPT-4 and Llama-3 exhibit and exacerbate nearly every human bias we test in this context, but GPT-3.5 exhibits more random behavior."
  - [section]: "GPT 3.5, in line with its previous results, does achieve very high error rates of nearly 0.25" while other models achieve lower MSE.
  - [corpus]: Weak - neighbor papers don't compare randomness across different LLM architectures.
- Break condition: If the observed differences are due to random variation rather than systematic architectural differences.

## Foundational Learning

- Concept: Human biases in random sequence generation
  - Why needed here: Understanding these biases is essential to recognize when LLMs are exhibiting similar patterns and to measure how they differ from true randomness.
  - Quick check question: What are the two most common human biases in generating random binary sequences mentioned in the paper?

- Concept: Temperature scaling in language models
  - Why needed here: Temperature controls the randomness of outputs, which is critical for understanding how LLMs generate sequences and how to manipulate their behavior.
  - Quick check question: How does decreasing temperature affect the probability distribution over tokens in an LLM?

- Concept: N-grams and pattern analysis in sequences
  - Why needed here: Analyzing n-grams helps quantify how sequences deviate from randomness and identify specific biases like alternation preferences or run avoidance.
  - Quick check question: What n-gram pattern do humans typically overuse when generating random sequences, and how does this compare to true randomness?

## Architecture Onboarding

- Component map: LLM -> Prompt -> Temperature -> Token sampling -> Sequence generation -> Pattern analysis -> MSE calculation -> Correlation analysis
- Critical path: Prompt generation -> LLM response -> Sequence parsing -> Statistical analysis pipeline
- Design tradeoffs: Temperature vs. determinism, computational cost of pattern analysis vs. accuracy, model choice (GPT-3.5 more random vs. GPT-4 more biased)
- Failure signatures: Consistently biased outputs (e.g., excessive heads), low MSE indicating non-randomness, pattern repetition at low temperatures
- First 3 experiments:
  1. Test single coin flip across temperature range (0.0 to 1.5) to verify heads bias and temperature effects
  2. Generate 20-flip sequences at multiple temperatures and analyze alternation distribution vs. expected 3.5
  3. Train LASSO model to predict next flip and measure MSE at different temperatures to quantify randomness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the fundamental source of LLMs' exacerbation of human biases in random binary sequence generation?
- Basis in paper: [explicit] The paper discusses that LLMs not only replicate but exacerbate human biases in random sequence generation, but the specific source of this exacerbation is not definitively determined.
- Why unresolved: The paper suggests that the exacerbation could be due to either additional machine bias injected into the randomness or the amplification of human bias present in the training data, but does not conclusively determine which factor is more significant.
- What evidence would resolve it: Comparative analysis of LLMs trained on different datasets with varying levels of human bias, and controlled experiments isolating the effects of model architecture versus training data on bias amplification.

### Open Question 2
- Question: How does the temperature parameter affect the randomness and humanness of LLMs' output in random binary sequence generation?
- Basis in paper: [explicit] The paper observes that temperature affects the randomness of LLMs' output, with higher temperatures generally leading to more random sequences, but the relationship between temperature and the humanness of the output is not fully explored.
- Why unresolved: While the paper notes changes in randomness with temperature, it does not comprehensively analyze how temperature influences the specific human biases observed in the output.
- What evidence would resolve it: Systematic experiments varying temperature across a wider range and analyzing the impact on specific human biases in the generated sequences.

### Open Question 3
- Question: Can LLMs be trained or fine-tuned to generate truly random sequences, or is there an inherent limitation due to their training on human data?
- Basis in paper: [inferred] The paper suggests that LLMs, due to their training on human data, inherently carry and exacerbate human biases, raising the question of whether it is possible to overcome this limitation.
- Why unresolved: The paper does not explore potential methods for training LLMs to generate truly random sequences, nor does it address whether the limitation is inherent or can be mitigated through specific techniques.
- What evidence would resolve it: Experiments with LLMs trained on datasets specifically designed to minimize human bias in random sequence generation, or techniques such as adversarial training to encourage randomness.

## Limitations
- The study relies on comparing LLM outputs to human-generated random sequences, but the quality and characteristics of these human baselines are not thoroughly documented.
- While temperature effects are discussed, the specific mechanisms by which temperature influences pattern generation across different architectures remain unclear.
- The analysis focuses on a specific task (coin flips) and extrapolation to general scenarios requiring randomness requires additional validation.

## Confidence

- **High Confidence**: The observation that LLMs exhibit systematic biases in coin flip generation, particularly the heads bias and alternation preference. These patterns are consistent across multiple temperature settings and clearly deviate from true randomness.
- **Medium Confidence**: The claim that GPT-3.5 is fundamentally more random than GPT-4 and Llama-3. While the MSE data supports this, alternative explanations such as different temperature scaling implementations or training objectives could contribute to the observed differences.
- **Low Confidence**: The broader implications for LLM use in tasks requiring randomness or predicting human behavior. The study focuses on a specific task (coin flips) and extrapolation to general scenarios requires additional validation.

## Next Checks
1. **Reproduce human baseline comparison**: Generate fresh human-generated random sequences using the same prompts and analyze whether the LLM biases remain consistent relative to contemporary human performance.

2. **Cross-model temperature calibration**: Test identical temperature values across all three models with finer granularity (0.1 increments) to determine if temperature effects are consistent or model-specific.

3. **Sequence length sensitivity**: Repeat the analysis with varying sequence lengths (10, 20, 50 flips) to assess whether observed biases scale or change with sequence duration, particularly for the alternation and run-length patterns.