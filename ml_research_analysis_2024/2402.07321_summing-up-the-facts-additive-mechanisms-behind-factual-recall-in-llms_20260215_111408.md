---
ver: rpa2
title: 'Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs'
arxiv_id: '2402.07321'
source_url: https://arxiv.org/abs/2402.07321
tags:
- heads
- subject
- relation
- head
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how transformer-based language models
  store and retrieve factual knowledge, focusing on the task of factual recall where
  models must surface known facts in response to prompts like "Fact: The Colosseum
  is in the country of". Through mechanistic interpretability techniques, the authors
  find that factual recall is implemented additively through multiple distinct mechanisms
  that constructively interfere on the correct answer.'
---

# Summing Up the Facts: Additive Mechanisms Behind Factual Recall in LLMs

## Quick Facts
- arXiv ID: 2402.07321
- Source URL: https://arxiv.org/abs/2402.07321
- Reference count: 40
- Primary result: Factual recall in transformers is implemented through four additive mechanisms that constructively interfere on correct answers

## Executive Summary
This paper investigates how transformer-based language models store and retrieve factual knowledge through the lens of factual recall tasks. The authors discover that models solve these tasks additively using four distinct mechanisms: subject heads, relation heads, mixed heads, and MLP boosting. Rather than relying on a single complex circuit, the model employs multiple independent pathways that each contribute positively to the correct logit, with their sum producing robust performance. The work extends direct logit attribution to attribute attention head outputs to individual source tokens, enabling detailed mechanistic analysis of how these components interact.

## Method Summary
The authors analyze factual recall by constructing a hand-written dataset of approximately 106 prompts across 10 relations, then apply mechanistic interpretability techniques including extended direct logit attribution (DLA) that attributes attention head outputs to individual source tokens. They use logit lens analysis to examine how residual stream activations contribute to final outputs, and perform ablation studies to test mechanism independence. The analysis focuses on identifying which attention heads contribute to factual recall and how their contributions decompose by source token (subject vs relation).

## Key Results
- Four distinct mechanisms contribute additively to factual recall: subject heads, relation heads, mixed heads, and MLPs
- Subject heads extract attributes about the subject while relation heads extract attributes about the relation
- Mixed heads combine both approaches and attend to both subject and relation tokens
- MLPs boost relation attributes, with relation heads being more prominent in later layers
- The additive combination of mechanisms is more robust than any individual mechanism alone

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Models store factual knowledge through enrichment of subject representations in early MLP layers, which simultaneously look up all known facts about the subject before seeing which relation is requested.
- Mechanism: MLP layers act as key-value memories that enrich subject representations by gathering all related facts into the residual stream, creating an enriched representation that later layers can query.
- Core assumption: The model uses a unidirectional circuit where subject enrichment occurs before relation processing, and this enrichment is sufficient to support later extraction mechanisms.
- Evidence anchors:
  - [abstract]: "Meng et al. (2023a) find an important role of early MLP layers is to enrich the internal representations of subjects (The Colosseum), through simultaneously looking up all known facts, and storing them in activations on the final subject token."
  - [section 1.1]: "Meng et al. (2023a) found a separate function of MLP layers: to enrich the representations in the residual stream of subjects with facts for the model to later use"
  - [corpus]: Weak - corpus papers focus on multilingual or cross-architecture aspects rather than the enrichment mechanism itself
- Break condition: If enrichment occurs after relation processing, or if enrichment is distributed across layers rather than concentrated in early MLPs

### Mechanism 2
- Claim: Factual recall is implemented additively through multiple distinct mechanisms that constructively interfere on the correct answer.
- Mechanism: Four independent mechanisms (subject heads, relation heads, mixed heads, MLPs) each contribute positively to the correct logit through separate pathways, with their sum being more robust than any individual mechanism.
- Core assumption: Models prefer additive computation through multiple independent circuits rather than relying on a single complex mechanism.
- Evidence anchors:
  - [abstract]: "We find that the mechanistic story behind factual recall is more complex than previously thought. It comprises several distinct, independent, and qualitatively different mechanisms that additively combine, constructively interfering on the correct attribute."
  - [section 1.1]: "Our core contribution in this work is showing that models primarily solve factual recall tasks additively."
  - [section 3.1]: "Individual subject heads extract specific attributes about subjects in some set S ∩ C by attending from END to SUBJECT"
- Break condition: If any single mechanism dominates performance, or if mechanisms interfere destructively rather than constructively

### Mechanism 3
- Claim: Direct Logit Attribution (DLA) can be extended to attribute attention head outputs to individual source tokens, enabling disentanglement of mixed head contributions.
- Mechanism: By recognizing that attention head outputs are weighted sums over source positions, DLA can be split into contributions from different token groups (SUBJECT vs RELATION), revealing separate additive contributions.
- Core assumption: The residual stream accumulates outputs additively, and attention outputs can be decomposed into source-specific components.
- Evidence anchors:
  - [abstract]: "We extend the method of direct logit attribution to attribute an attention head's output to individual source tokens."
  - [section 2.4]: "DLA by source token group is an extension to the DLA technique through the further insight that attention head outputs are a weighted sum of outputs corresponding to distinct attention source position"
  - [section 3.3]: "From SUBJECT, they extract the correct attribute a more than other attributes from R. From RELATION, they extract many attributes in R"
- Break condition: If attention outputs cannot be cleanly decomposed by source position, or if LayerNorm interactions prevent clean attribution

## Foundational Learning

- Concept: Residual stream accumulation and linearity
  - Why needed here: Understanding that the residual stream is a cumulative sum of model component outputs is fundamental to DLA and analyzing additive mechanisms
  - Quick check question: If residual stream at layer l is z_l = z_{l-1} + a_l + m_l, what is the total contribution of all components to the final output?

- Concept: Linear map from residual stream to logits
  - Why needed here: The approximately linear relationship between residual activations and output logits enables techniques like logit lens and DLA
  - Quick check question: If unembedding matrix is W_U, how do you compute logits from residual stream activations?

- Concept: Attention as weighted sum over source positions
  - Why needed here: Recognizing that attention head outputs are sums over key positions allows decomposition of DLA by source token
  - Quick check question: Given attention probabilities attn and value vectors, how do you express attention output as sum over source positions?

## Architecture Onboarding

- Component map: Early MLP layers → Subject enrichment → Multiple attention mechanisms (subject heads, relation heads, mixed heads) → MLP boosting → Output logits
- Critical path: Subject enrichment (MLP layers) → Information movement to END token → Multiple attention-based extraction mechanisms → MLP boosting → Logit computation
- Design tradeoffs: Additive mechanisms provide robustness but require more parameters and computation vs single complex mechanism; unidirectional enrichment vs bidirectional lookup
- Failure signatures: Poor performance on factual recall tasks, inability to generalize "A is B" to "B is A" (reversal curse), over-reliance on single mechanism
- First 3 experiments:
  1. Apply DLA to individual MLP layers on END token to verify enrichment mechanism
  2. Perform attention knockout experiments to test independence of subject vs relation mechanisms
  3. Compare performance with and without mixed head contributions to demonstrate additive nature

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why do transformer models prefer additive computation through multiple independent circuits rather than relying on a single complex mechanism?
- Basis in paper: Explicit - The authors state "We do not explain why the additive mechanism is preferred, but speculate that compounding evidence through several simple circuits is significantly easier for models."
- Why unresolved: The paper only provides speculation without experimental evidence. The authors note that additive computation may be easier for models but don't test this hypothesis or compare it to alternative computational strategies.
- What evidence would resolve it: Comparative experiments showing performance, training efficiency, or generalization differences between additive vs single-circuit approaches for factual recall tasks.

### Open Question 2
- Question: How do MLP layers interact with relation heads to boost relation attributes, and do they compose with relation information directly or through relation head outputs?
- Basis in paper: Explicit - The authors note "We hypothesize MLPs either compose with relation heads, or with relation information directly" but don't investigate this further.
- Why unresolved: The paper only analyzes MLP direct effects on logits and doesn't investigate whether MLPs work independently or in conjunction with relation heads.
- What evidence would resolve it: Causal intervention experiments that isolate MLP effects from relation head effects, and vice versa, to determine the compositional relationship.

### Open Question 3
- Question: How does subject-relation propagation vary across different model scales, and why is it more prominent in larger models?
- Basis in paper: Explicit - The authors note "Preliminary results suggest this latter finding is less true in larger models" regarding relation heads not depending on subject, and mention "We do not rigorously study" subject-relation propagation.
- Why unresolved: The paper only briefly touches on this phenomenon and doesn't systematically study how it changes with model size or why larger models implement more sophisticated circuits.
- What evidence would resolve it: Comparative analysis of subject-relation propagation mechanisms across different model scales, with ablation studies to identify the computational benefits of more sophisticated propagation in larger models.

## Limitations

- The analysis is based on a hand-crafted dataset of approximately 106 prompts, which may not capture the full complexity of real-world factual knowledge retrieval
- The extended DLA technique relies on assumptions about attention output decomposition that may not hold under all conditions due to LayerNorm and residual connections
- Generalizability to other transformer architectures (LLaMA, PaLM, Claude) and tasks remains uncertain without cross-architecture validation

## Confidence

- Core findings about four distinct mechanisms: Medium
- Claims about additive computation preference: Low
- Generalizability across models and tasks: Low
- Connection to reversal curse: Low

## Next Checks

1. **Cross-architecture validation**: Apply the same mechanistic analysis to at least two different transformer architectures (e.g., LLaMA and PaLM) to test whether the four-mechanism additive pattern holds consistently across models.

2. **Noise robustness test**: Systematically inject noise into individual mechanism components (subject heads, relation heads, etc.) and measure whether the additive combination truly provides robustness as claimed, or whether some mechanisms are more critical than others.

3. **Real-world generalization**: Test the mechanisms on naturally occurring factual queries from benchmarks like KILT or NaturalQuestions to verify that the hand-crafted prompt analysis generalizes to realistic usage patterns.