---
ver: rpa2
title: Are We Really Achieving Better Beyond-Accuracy Performance in Next Basket Recommendation?
arxiv_id: '2405.01143'
source_url: https://arxiv.org/abs/2405.01143
tags:
- items
- accuracy
- metrics
- basket
- beyond-accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The authors examine whether improving beyond-accuracy metrics\
  \ (e.g., fairness and diversity) in next basket recommendation (NBR) is possible\
  \ without sacrificing accuracy. They identify a potential \u201Cshort-cut\u201D\
  \ strategy: sacrificing exploration accuracy and using explore items to optimize\
  \ beyond-accuracy metrics, while maintaining high accuracy via repeat items."
---

# Are We Really Achieving Better Beyond-Accuracy Performance in Next Basket Recommendation?

## Quick Facts
- arXiv ID: 2405.01143
- Source URL: https://arxiv.org/abs/2405.01143
- Reference count: 40
- The authors propose TREx, a two-step framework that separately models repeat and explore items to achieve state-of-the-art accuracy and improved beyond-accuracy metrics

## Executive Summary
This paper investigates whether improvements in beyond-accuracy metrics (fairness and diversity) in next basket recommendation can be achieved without sacrificing accuracy. The authors identify a potential "short-cut" strategy where models sacrifice exploration accuracy but optimize beyond-accuracy metrics using explore items, while maintaining high accuracy through repeat items. They propose TREx, a two-step framework that separately models repeat and explore items, achieving strong empirical results. The framework uses a simple repetition module based on item features and user repurchase frequency for accuracy, and two exploration modules for fairness or diversity optimization. Experiments on two datasets show TREx achieves state-of-the-art accuracy and improved beyond-accuracy metrics on most measures, except those strongly connected to accuracy like logRUR.

## Method Summary
The authors propose TREx, a two-step framework that separates the modeling of repeat and explore items in next basket recommendation. The framework first identifies which items in a basket are likely repeats versus explorations. For repeat items, it uses a simple repetition module based on item features and user repurchase frequency to maintain high accuracy. For explore items, it employs two different exploration modules - one optimized for fairness and another for diversity. This separation allows the model to optimize beyond-accuracy metrics on explore items without compromising the accuracy achieved through repeat item prediction.

## Key Results
- TREx achieves state-of-the-art accuracy performance across multiple datasets
- The framework improves beyond-accuracy metrics (fairness and diversity) on most measures
- Performance on accuracy-related beyond-accuracy metrics like logRUR remains comparable to baselines
- The "short-cut" strategy of sacrificing exploration accuracy while optimizing beyond-accuracy metrics proves effective under current evaluation paradigms

## Why This Works (Mechanism)
The mechanism works because it exploits the distinction between repeat and explore items in user behavior. By recognizing that users often have predictable repeat purchase patterns, the framework can maintain high accuracy through simple repetition modeling while using the more flexible exploration modules to optimize fairness and diversity. This separation allows the model to satisfy accuracy requirements through the repeat component while pushing beyond-accuracy optimization through the explore component, effectively decoupling these two objectives that are often in tension.

## Foundational Learning
- **Next basket recommendation (NBR)**: Predicting the next set of items a user will purchase together; needed to understand the specific recommendation context being studied; quick check: verify the definition includes basket-level predictions rather than individual items
- **Beyond-accuracy metrics**: Evaluation measures like fairness and diversity that go beyond traditional accuracy metrics; needed to understand the paper's contribution beyond standard recommendation evaluation; quick check: confirm these metrics capture different aspects of recommendation quality
- **Repeat vs explore items**: Classification of items as either predictable repurchases or novel discoveries; needed to understand the core insight about separating these behaviors; quick check: validate that this distinction aligns with observed user purchase patterns
- **Evaluation paradigm limitations**: Recognition that current evaluation methods may not adequately capture the trade-offs between accuracy and beyond-accuracy objectives; needed to understand why the "short-cut" strategy works; quick check: examine whether existing benchmarks properly test exploration accuracy

## Architecture Onboarding

**Component Map**: User features -> Repeat/Explore Classifier -> [Repeat Module] -> [Explore Module (Fairness)] and [Explore Module (Diversity)] -> Combined Predictions

**Critical Path**: The critical path flows from user features through the repeat/explore classifier to either the repeat module (for accuracy) or the appropriate explore module (for fairness/diversity optimization), with final predictions being combined from both paths.

**Design Tradeoffs**: The main tradeoff is between model complexity and performance separation - by splitting repeat and explore modeling, the framework simplifies each component but requires accurate classification of item types. This design assumes clean separation is possible and beneficial, which may not hold in all scenarios.

**Failure Signatures**: The framework may fail when users don't exhibit clear repeat/explore patterns, when the classifier misidentifies item types, or when exploration accuracy is actually important to users. Poor performance on logRUR suggests limitations when accuracy and beyond-accuracy metrics are strongly correlated.

**3 First Experiments**: 
1. Test TREx's performance when exploration accuracy is explicitly weighted in the objective function
2. Evaluate the framework on cold-start scenarios where repeat patterns are unreliable
3. Conduct ablation studies comparing TREx with unified models that don't separate repeat and explore items

## Open Questions the Paper Calls Out
The paper acknowledges that while the "short-cut" strategy works under current evaluation paradigms, it may not be desirable if exploration accuracy is important. This raises questions about the appropriateness of current evaluation methods and whether the framework's approach truly represents progress in beyond-accuracy optimization or merely exploits evaluation limitations.

## Limitations
- The framework's reliance on repeat item modeling may not generalize well to cold-start scenarios or users with less predictable purchase patterns
- The assumption that repeat and explore items can be cleanly separated may oversimplify complex user behavior
- The "short-cut" strategy of sacrificing exploration accuracy may not be acceptable in real-world applications where discovery is valued
- The approach doesn't explicitly test scenarios where exploration accuracy matters, limiting understanding of its trade-offs

## Confidence
- **High confidence**: Empirical results showing TREx's superior performance on standard metrics across multiple datasets
- **Medium confidence**: The claim about "short-cut" strategy being a fundamental limitation of current evaluation paradigms
- **Medium confidence**: The conclusion that TREx represents a reasonable approach to beyond-accuracy optimization given the acknowledged trade-offs

## Next Checks
1. Conduct ablation studies specifically measuring the impact of TREx's exploration modules on exploration accuracy, comparing against baselines that maintain exploration accuracy while optimizing beyond-accuracy metrics.
2. Test TREx's performance in cold-start scenarios where repeat patterns are less reliable, to evaluate whether the framework's reliance on repeat item modeling creates vulnerabilities.
3. Implement a user study to assess whether improvements in beyond-accuracy metrics translate to perceived recommendation quality, particularly for the exploration component.