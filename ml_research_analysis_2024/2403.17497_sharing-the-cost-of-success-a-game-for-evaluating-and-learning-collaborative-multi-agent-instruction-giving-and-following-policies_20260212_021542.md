---
ver: rpa2
title: 'Sharing the Cost of Success: A Game for Evaluating and Learning Collaborative
  Multi-Agent Instruction Giving and Following Policies'
arxiv_id: '2403.17497'
source_url: https://arxiv.org/abs/2403.17497
tags:
- guide
- follower
- piece
- neural
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoGRIP, a collaborative reference game where
  two agents must coordinate to select a target piece from a board of distractors.
  The game includes a scoring system that rewards successful completion while penalizing
  high effort, encouraging cost-sharing between agents.
---

# Sharing the Cost of Success: A Game for Evaluating and Learning Collaborative Multi-Agent Instruction Giving and Following Policies

## Quick Facts
- arXiv ID: 2403.17497
- Source URL: https://arxiv.org/abs/2403.17497
- Reference count: 0
- Key outcome: Neural agents paired with heuristic partners achieve high success rates but with significantly higher joint effort than optimal heuristic pairings in a collaborative reference game.

## Executive Summary
This paper introduces CoGRIP, a collaborative reference game where two agents must coordinate to select a target piece from a board of distractors. The game includes a scoring system that rewards successful completion while penalizing high effort, encouraging cost-sharing between agents. The authors develop neural policies using PPO and bootstrap them with heuristic partners based on human-human interaction analysis. They show that neural agents paired with heuristic partners achieve high success rates and learn to reduce joint effort over time, though they still fall short of the efficiency of a well-optimized heuristic pairing.

## Method Summary
The authors create CoGRIP, a collaborative reference game where two agents (guide and follower) must select a target piece from a board of 4-16 distractors. The game uses a scoring system that rewards successful completion while penalizing effort costs for both agents. Neural policies are trained using Proximal Policy Optimization (PPO) with neural agents paired with heuristic partners, bootstrapped from human-human interaction analysis. The agents operate in a partially observable environment where the guide sees a limited view and the follower sees an overview plus instructions. Training occurs for 5 million steps on 12×12 boards, with evaluation against validation sets every 100k steps.

## Key Results
- Neural agents paired with heuristic partners achieve high success rates (mSR > 0.9)
- Neural-heuristic pairs show significantly higher joint effort than optimal heuristic-heuristic pairs
- Agents learn to reduce joint effort over training, with clear trends in mean episode length and mean joint effort
- Neural agents converge to "Guide A" strategy but show trends toward "Guide M" strategy over time

## Why This Works (Mechanism)
The game design creates a natural incentive structure where success requires coordination while efficiency demands cost-sharing. The scoring system penalizes both failure and excessive effort, forcing agents to balance accuracy with communication efficiency. By bootstrapping neural agents with heuristic partners based on human interaction patterns, the learning process starts from reasonable initial strategies rather than random exploration. The partial observability setting (guide with limited view, follower with overview) creates a natural division of labor that encourages informative communication.

## Foundational Learning
- **Proximal Policy Optimization (PPO)**: Why needed - Stable policy gradient method that handles continuous action spaces; Quick check - Verify training stability through consistent reward curves
- **Collaborative multi-agent reinforcement learning**: Why needed - Agents must learn to coordinate rather than compete; Quick check - Test coordination through success rate metrics
- **Bootstrapping from heuristic policies**: Why needed - Provides structured initial learning rather than random exploration; Quick check - Compare learning curves with and without heuristic bootstrapping
- **Effort-based reward shaping**: Why needed - Encourages efficient communication rather than verbose descriptions; Quick check - Monitor joint effort reduction over training
- **Partially observable Markov decision process (POMDP)**: Why needed - Realistic setting where agents have incomplete information; Quick check - Verify success rates under different observation settings
- **Curriculum learning through board complexity**: Why needed - Gradually increases task difficulty to support learning; Quick check - Test performance across different board sizes

## Architecture Onboarding

**Component map**: Game environment -> CNN visual encoder -> LSTM instruction encoder -> MLP policy network -> Action selection -> Heuristic partner

**Critical path**: Visual observation → CNN → LSTM → MLP → Action → Environment feedback → Reward calculation

**Design tradeoffs**: The paper chooses heuristic bootstrapping over pure self-play to provide structured initial learning, accepting that neural agents won't learn to coordinate with other neural agents. The effort-based reward shaping prioritizes efficiency but may limit exploration of more communicative strategies.

**Failure signatures**: Neural agents converging to "remote control" strategies with high success rates but excessive joint effort indicates insufficient exploration of collaborative strategies. Poor generalization to larger board sizes suggests overfitting to the 12×12 training environment.

**First experiments**:
1. Train neural guide with heuristic follower, evaluate success rate and joint effort over training steps
2. Compare neural-heuristic pairs against heuristic-heuristic baseline on validation set
3. Test neural agent generalization on board sizes not seen during training

## Open Questions the Paper Calls Out

**Open Question 1**: How would the neural agents perform if the effort costs for different actions were adjusted or based on a more complex cognitive model?
- Basis in paper: The current effort model is a simplified approximation based on empirical observations
- Why unresolved: More nuanced effort costs could significantly impact agent behavior and learning
- What evidence would resolve it: Experiments with alternative effort cost models comparing resulting agent strategies and performance metrics

**Open Question 2**: Can the neural agents learn to produce more diverse and human-like language strategies beyond the "Guide A" and "Guide M" strategies observed?
- Basis in paper: Current neural agents exhibit limited language diversity
- Why unresolved: Further research is needed to encourage more varied and human-like language production
- What evidence would resolve it: Analysis of language produced by neural agents in extended training scenarios compared to human-human interaction data

**Open Question 3**: How would the neural agents perform in a more visually complex environment with realistic 3D objects and scenes?
- Basis in paper: Current study uses simplified visual domain, acknowledging need for realistic environments
- Why unresolved: Unclear how learned strategies would generalize to more complex visual scenarios
- What evidence would resolve it: Implementation in photo-realistic environment evaluating performance on tasks with increased visual complexity

## Limitations
- Neural agents paired with heuristic partners rather than other neural agents limits applicability to fully autonomous multi-agent systems
- Performance metrics based on average results may mask individual failure cases
- Simplified visual domain (abstract shapes) may not generalize to realistic visual environments
- Limited exploration of language diversity in neural agent strategies

## Confidence
- Neural agent success rates when paired with heuristic partners: **High**
- Observed reduction in joint effort over training: **High**
- Comparison between neural-heuristic and heuristic-heuristic pairs: **Medium**
- Generalization to different board sizes: **Low**

## Next Checks
1. Test neural agents against other neural agents rather than heuristic partners to verify collaborative capability in fully learned systems
2. Evaluate performance on board sizes not seen during training to assess generalization
3. Conduct ablation studies on the heuristic bootstrap approach to quantify its contribution to final performance