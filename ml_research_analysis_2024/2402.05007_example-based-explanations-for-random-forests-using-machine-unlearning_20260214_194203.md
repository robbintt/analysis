---
ver: rpa2
title: Example-based Explanations for Random Forests using Machine Unlearning
arxiv_id: '2402.05007'
source_url: https://arxiv.org/abs/2402.05007
tags:
- data
- subset
- subsets
- fairness
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents FairDebugger, a system that generates example-based
  explanations for random forest classifiers to identify training data subsets responsible
  for fairness violations. The system leverages machine unlearning to estimate the
  effect of removing data subsets on model fairness, and uses a lattice-based search
  method to efficiently find the most influential subsets.
---

# Example-based Explanations for Random Forests using Machine Unlearning

## Quick Facts
- **arXiv ID**: 2402.05007
- **Source URL**: https://arxiv.org/abs/2402.05007
- **Reference count**: 40
- **Primary result**: Identifies training data subsets responsible for fairness violations in random forests using machine unlearning, with up to 100% parity reduction identification

## Executive Summary
This paper presents FairDebugger, a system that generates example-based explanations for random forest classifiers to identify training data subsets responsible for fairness violations. The approach leverages machine unlearning through DaRE-RF, a random forest variant that supports efficient data removal, to estimate the effect of removing data subsets on model fairness without full retraining. Using a lattice-based search method combined with apriori pruning, FairDebugger efficiently finds the most influential subsets causing fairness violations.

The system was evaluated on three real-world datasets (German Credit, Adult Income, and Stop-Question-Frisk) and demonstrated the ability to identify subsets responsible for substantial fractions of model bias, with some cases showing up to 100% parity reduction. The approach is particularly effective for datasets with up to 600,000 instances, though runtime increases quadratically with dataset size. The explanations generated are consistent with findings from prior fairness studies, validating the method's effectiveness.

## Method Summary
FairDebugger operates by first training a random forest classifier using DaRE-RF, which supports efficient data removal through specialized mechanisms that avoid full retraining. The system then employs a lattice-based search strategy to explore the space of possible training data subsets, using the apriori algorithm to prune unpromising candidates early in the search process. For each candidate subset, FairDebugger estimates the impact on fairness metrics by leveraging DaRE-RF's unlearning capabilities rather than retraining from scratch. This allows the system to efficiently identify subsets whose removal would significantly improve model fairness, providing interpretable explanations for fairness violations rooted in specific training data patterns.

## Key Results
- Successfully identified training subsets responsible for up to 100% parity reduction in model fairness
- Demonstrated consistent results across three diverse real-world datasets (German Credit, Adult Income, Stop-Question-Frisk)
- Achieved efficient search through lattice-based methods with apriori pruning, though runtime scales quadratically with dataset size
- Generated explanations that align with findings from prior fairness studies, validating the approach's effectiveness

## Why This Works (Mechanism)
FairDebugger works by exploiting the relationship between training data composition and model fairness through efficient data removal rather than retraining. The DaRE-RF mechanism allows approximate estimation of how removing specific data subsets affects fairness metrics, enabling rapid exploration of the search space. The lattice-based approach systematically explores subset combinations while apriori pruning eliminates candidates unlikely to yield significant improvements, making the search tractable even for large datasets.

## Foundational Learning
**Machine Unlearning**: The ability to efficiently remove the influence of training data without full retraining is essential for FairDebugger's scalability. Quick check: Verify that DaRE-RF can approximate removal effects within acceptable accuracy bounds compared to full retraining.

**Lattice-based Search**: This structured approach to exploring subset combinations ensures systematic coverage of the search space while enabling effective pruning strategies. Quick check: Confirm that the lattice structure maintains completeness while avoiding redundant explorations.

**Apriori Pruning**: This algorithm identifies and eliminates candidate subsets unlikely to yield significant fairness improvements, dramatically reducing computational requirements. Quick check: Validate that pruning decisions maintain a high recall of truly influential subsets.

## Architecture Onboarding

**Component Map**: Data -> DaRE-RF Trainer -> FairDebugger Engine -> Subset Evaluator -> Explanation Generator -> Results

**Critical Path**: The most time-consuming step is the lattice-based search through candidate subsets, with each evaluation requiring DaRE-RF's unlearning operations. The apriori pruning stage is critical for maintaining tractability as dataset size increases.

**Design Tradeoffs**: The system trades some accuracy in unlearning approximation (versus full retraining) for significant gains in computational efficiency. This enables exploration of larger search spaces but may miss subtle effects requiring exact retraining.

**Failure Signatures**: Poor performance may manifest as failure to identify influential subsets (overly aggressive pruning), excessive runtime (insufficient pruning), or explanations inconsistent with known dataset biases (inaccurate unlearning approximations).

**First Experiments**:
1. Run on a small synthetic dataset with known bias sources to verify correct identification
2. Compare DaRE-RF unlearning accuracy against full retraining on a validation subset
3. Profile runtime scaling with dataset size to verify quadratic complexity

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Quadratic runtime scaling with dataset size limits scalability to very large datasets
- Current evaluation focuses only on demographic parity, not other fairness metrics
- Effectiveness depends on the accuracy of DaRE-RF's unlearning approximations
- May miss subtle bias sources requiring exact retraining for detection

## Confidence

| Claim | Confidence |
|-------|------------|
| FairDebugger effectively identifies influential training subsets | High |
| Runtime complexity analysis is accurate | Medium |
| Results generalize to other tree-based models or fairness metrics | Low |

## Next Checks
1. Evaluate FairDebugger's performance on additional fairness metrics (e.g., equalized odds, predictive parity) beyond demographic parity
2. Test scalability on datasets exceeding 600,000 instances to better characterize the quadratic runtime behavior
3. Assess the approach's effectiveness when applied to other tree-based models like gradient boosting machines or decision trees