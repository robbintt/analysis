---
ver: rpa2
title: 'CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large Language
  Models'
arxiv_id: '2406.12257'
source_url: https://arxiv.org/abs/2406.12257
tags:
- clean
- backdoor
- attacks
- llms
- reference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CLEAN GEN, an effective decoding strategy to
  mitigate backdoor attacks for generation tasks in large language models (LLMs).
  The key insight is that backdoored LLMs assign significantly higher probabilities
  to tokens representing attacker-desired contents when the input prompt contains
  the trigger.
---

# CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models

## Quick Facts
- arXiv ID: 2406.12257
- Source URL: https://arxiv.org/abs/2406.12257
- Reference count: 40
- CleanGen achieves lower attack success rates than five baseline defenses while maintaining LLM helpfulness and minimal computational overhead

## Executive Summary
This paper presents CleanGen, an effective decoding strategy to mitigate backdoor attacks for generation tasks in large language models (LLMs). The key insight is that backdoored LLMs assign significantly higher probabilities to tokens representing attacker-desired contents when the input prompt contains the trigger. Leveraging this observation, CleanGen identifies suspicious tokens favored by the attacker and replaces them with tokens generated by another LLM that is not compromised by the same attacker. Experimental results show that CleanGen effectively mitigates five state-of-the-art backdoor attacks while maintaining the helpfulness of the LLMs for benign user queries and introducing minimal computational overhead.

## Method Summary
CleanGen is a decoding strategy that mitigates backdoor attacks by leveraging probability discrepancies between a backdoored target model and a clean reference model. For each token generated by the target model, CleanGen calculates a suspicion score based on the probability difference compared to the reference model. Tokens exceeding a threshold are flagged as suspicious and replaced with tokens generated by the reference model. The method uses configurable parameters for the suspicion score threshold and prediction horizon to balance effectiveness and efficiency.

## Key Results
- CleanGen achieves lower attack success rates compared to five baseline defenses across five state-of-the-art backdoor attacks
- Maintains MT-Bench scores close to baseline levels, preserving LLM helpfulness for benign queries
- Introduces minimal computational overhead with average token generation time ratio remaining low

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Backdoored LLMs assign significantly higher probabilities to tokens representing attacker-desired contents when the input prompt contains the trigger.
- Mechanism: CleanGen identifies tokens where the target model's probability is significantly higher than the reference model's probability, flagging these as suspicious and replacing them.
- Core assumption: The probability discrepancy between backdoored and clean models is sufficient to detect backdoor-influenced tokens.
- Evidence anchors: [abstract] "Our insight behind CLEAN GEN is that compared to other LLMs, backdoored LLMs assign significantly higher probabilities to tokens representing the attacker-desired contents."

### Mechanism 2
- Claim: CleanGen replaces suspicious tokens with those generated by a reference model not compromised by the same attacker.
- Mechanism: By using a reference model that wasn't trained on the same poisoned data, CleanGen ensures that replaced tokens don't contain attacker-desired content.
- Core assumption: The reference model is sufficiently clean to generate appropriate replacement tokens.
- Evidence anchors: [abstract] "CLEAN GEN identifies suspicious tokens favored by the attacker and replace them with tokens generated by another LLM that is not compromised by the same attacker"

### Mechanism 3
- Claim: CleanGen maintains helpfulness by only replacing a small fraction of tokens when no trigger is present.
- Mechanism: CleanGen uses a threshold parameter to minimize false positives, ensuring that benign prompts are largely unaffected.
- Core assumption: The suspicion score threshold can be set to distinguish between trigger-influenced and normal token generation.
- Evidence anchors: [section 5.2] "The results show that the values of q for benign queries consistently remains below 0.05"

## Foundational Learning

- Concept: Token probability distributions in LLMs
  - Why needed here: Understanding how LLMs assign probabilities to tokens is crucial for grasping CleanGen's detection mechanism
  - Quick check question: How does an LLM typically generate the next token in a sequence?

- Concept: Backdoor attacks in LLMs
  - Why needed here: Knowing how backdoors work helps understand what CleanGen is defending against
  - Quick check question: What is the primary goal of a backdoor attack in an LLM?

- Concept: Decoding strategies in LLMs
  - Why needed here: CleanGen is a decoding strategy, so understanding common decoding approaches is important
  - Quick check question: What is the difference between greedy decoding and beam search?

## Architecture Onboarding

- Component map: Target model -> Suspicion score calculator -> Token replacement mechanism -> Reference model -> Final token sequence
- Critical path:
  1. Target model generates token probabilities
  2. Reference model generates reference probabilities
  3. Suspicion scores are calculated
  4. Suspicious tokens are identified and replaced
  5. Final token sequence is generated
- Design tradeoffs:
  - Higher threshold α reduces false positives but may miss some backdoor tokens
  - Larger prediction horizon k improves efficiency but may reduce precision
  - More capable reference model improves replacement quality but increases computational overhead
- Failure signatures:
  - High attack success rate despite CleanGen deployment
  - Significant drop in helpfulness scores
  - Consistently high fraction of replaced tokens in benign prompts
- First 3 experiments:
  1. Test CleanGen with varying threshold α values on a simple backdoor attack
  2. Compare CleanGen's performance with different reference model capabilities
  3. Measure the impact of prediction horizon k on both effectiveness and efficiency

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions.

## Limitations
- Limited generalizability to backdoor attacks not included in the evaluation
- Potential vulnerability to sophisticated attacks that don't create significant probability differences
- Dependency on the quality and independence of the reference model

## Confidence

- **High Confidence:** The basic mechanism of using probability discrepancy for detection is sound and well-explained.
- **Medium Confidence:** The effectiveness claims for mitigating the five specific backdoor attacks are supported by experiments, but the results may not generalize to all possible attack variants or model architectures.
- **Medium Confidence:** The claims about maintaining helpfulness while mitigating attacks are supported by MT-Bench scores, but the evaluation doesn't cover task-specific performance degradation.
- **Low Confidence:** The paper doesn't provide sufficient analysis of potential failure modes, such as false positive rates in benign contexts or scenarios where the reference model might also be compromised.

## Next Checks
1. **Threshold Sensitivity Analysis:** Systematically vary the suspicion score threshold α across multiple orders of magnitude to quantify the tradeoff between attack mitigation and false positive rates.
2. **Cross-Attack Generalization:** Test CleanGen against backdoor attacks not included in the original evaluation to assess its generalizability.
3. **Reference Model Dependency Analysis:** Evaluate CleanGen's performance when using reference models of varying quality/capability, including cases where the reference model is also partially compromised.