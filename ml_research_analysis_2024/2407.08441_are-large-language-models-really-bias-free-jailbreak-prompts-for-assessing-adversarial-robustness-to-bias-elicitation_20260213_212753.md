---
ver: rpa2
title: Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing
  Adversarial Robustness to Bias Elicitation
arxiv_id: '2407.08441'
source_url: https://arxiv.org/abs/2407.08441
tags:
- safety
- bias
- arxiv
- language
- prompts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates the presence of biases in large language
  models (LLMs) by using a two-step methodology that first evaluates bias-specific
  safety scores and then applies jailbreak prompts to assess adversarial robustness.
  Experiments conducted on various LLMs, including GPT-3.5 Turbo, Gemini Pro, and
  Llama 3.70B, revealed that while some models like Llama 3.70B and Gemini Pro initially
  achieved high safety scores, they were still vulnerable to jailbreak attacks, resulting
  in significant safety reductions.
---

# Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation

## Quick Facts
- arXiv ID: 2407.08441
- Source URL: https://arxiv.org/abs/2407.08441
- Authors: Riccardo Cantini; Giada Cosenza; Alessio Orsino; Domenico Talia
- Reference count: 40
- Key outcome: No LLM was entirely safe from bias elicitation attacks, with safety scores dropping below 0.5 after adversarial testing

## Executive Summary
This study investigates biases in large language models by testing their robustness against jailbreak prompts designed to elicit biased responses. Using a two-step methodology that first evaluates baseline safety scores and then applies adversarial jailbreak techniques, the researchers found that even models with high initial safety scores were vulnerable to sophisticated attacks. The experiments revealed that GPT-3.5 Turbo was the least safe model despite having 175 billion parameters, while smaller models like Llama 3.70B and Gemini Pro demonstrated higher safety scores. Notably, no model achieved complete safety, with all experiencing significant safety reductions after jailbreak attacks.

## Method Summary
The study employs a two-step methodology to assess LLM bias robustness. First, standard prompts are used to evaluate initial safety scores across seven bias categories (age, ethnicity, gender, sexual orientation, disability, religion, and socioeconomic status) using a sentence completion format. Safety scores combine robustness measures (refusal and debiasing rates) with fairness metrics (stereotype vs counterstereotype rates). Second, jailbreak prompts using five techniques (role-playing, machine translation, obfuscation, prompt injection, and reward incentive) are applied to categories that initially achieved safety scores ≥ 0.5. The effectiveness of each attack is measured by the reduction in safety scores, revealing hidden vulnerabilities that standard testing fails to uncover.

## Key Results
- GPT-3.5 Turbo was the least safe model with no refusal rate and minimal debiasing tendency
- Gemini Pro showed the highest resistance to attacks, maintaining safety in age, sexual orientation, and religion categories
- Safety scores dropped significantly after jailbreak attacks, with reductions up to 94.4% in some cases
- Larger models don't guarantee better safety - Llama 3.70B and Gemini Pro outperformed GPT-3.5 Turbo

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Jailbreak prompts can bypass safety filters even in models with high initial safety scores
- Mechanism: Adversarial prompts crafted using role-playing, machine translation, obfuscation, prompt injection, and reward incentive techniques trick the model into producing biased responses by disguising harmful intent or offering incentives
- Core assumption: The model's safety filters rely on pattern matching and semantic understanding that can be circumvented through linguistic manipulation
- Evidence anchors: [abstract] "investigate how known prompt engineering techniques can be exploited to effectively reveal hidden biases of LLMs, testing their adversarial robustness against jailbreak prompts specially crafted for bias elicitation"

### Mechanism 2
- Claim: Safety scores decrease significantly after adversarial testing, revealing hidden vulnerabilities
- Mechanism: The two-step methodology first establishes baseline safety through standard prompts, then reveals true robustness by applying jailbreak techniques to categories initially deemed safe, showing safety score reductions of up to 94.4%
- Core assumption: Initial safety assessments underestimate model vulnerability to sophisticated adversarial attacks
- Evidence anchors: [section 4.2] "a considerable reduction in bias-specific safety. The only bias categories that resist the performed attacks... are age, sexual orientation, and religion for Gemini Pro"

### Mechanism 3
- Claim: Model scale does not guarantee safety - GPT-3.5 Turbo (175B parameters) was the least safe model
- Mechanism: Despite having the largest parameter count, GPT-3.5 Turbo showed no refusal rate and minimal debiasing tendency, while smaller models like Llama 370B and Gemini Pro demonstrated higher safety scores
- Core assumption: Model size and parameter count do not directly correlate with safety alignment effectiveness
- Evidence anchors: [section 4.1] "surprisingly GPT-3.5 Turbo, despite having 175 billion parameters, falls below the safety threshold, resulting in the least safe model"

## Foundational Learning

- Concept: Adversarial prompt engineering techniques
  - Why needed here: The study relies on jailbreak techniques (role-playing, machine translation, obfuscation, prompt injection, reward incentive) to test model robustness
  - Quick check question: What are the five jailbreak techniques used to bypass LLM safety filters?

- Concept: Safety score calculation methodology
  - Why needed here: The study uses a two-component safety score combining robustness (refusal + debiasing rates) and fairness (stereotype vs counterstereotype rates)
  - Quick check question: How is the bias-specific safety score σpb calculated from robustness and fairness measures?

- Concept: Bias categories and stereotypes in LLMs
  - Why needed here: The study evaluates seven bias categories (age, ethnicity, gender, sexual orientation, disability, religion, socioeconomic status) using stereotype vs counterstereotype completion tasks
  - Quick check question: What are the seven bias categories examined in the study?

## Architecture Onboarding

- Component map: Data collection → Standard prompt testing → Safety score calculation → Jailbreak prompt generation → Adversarial testing → Results analysis → Model comparison
- Critical path: Standard prompt evaluation → Safety score computation → Jailbreak attack execution → Safety score re-evaluation → Vulnerability assessment
- Design tradeoffs: Comprehensive bias coverage vs. testing complexity; multiple attack types vs. execution time; parameter count vs. safety performance
- Failure signatures: Models showing high initial safety scores but dramatic reductions after adversarial testing; consistent vulnerability to specific attack types; size vs. safety misalignment
- First 3 experiments:
  1. Implement standard prompt testing for all seven bias categories on a target model
  2. Calculate initial safety scores using the robustness + fairness methodology
  3. Apply role-playing jailbreak attack to all categories with safety scores ≥ 0.5 and measure safety score reduction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the effectiveness and characteristics of jailbreak attacks differ across various model scales (small, medium, large)?
- Basis in paper: [explicit] The paper mentions evaluating the effectiveness of different jailbreak attacks (role-playing, machine translation, obfuscation, prompt injection, reward incentive) across models of different scales, but does not provide a detailed comparative analysis.
- Why unresolved: While the paper reports some effectiveness data, it lacks a comprehensive comparison of how these attacks perform specifically across small, medium, and large models.
- What evidence would resolve it: A detailed table or analysis showing the effectiveness of each attack type broken down by model scale, along with statistical significance testing.

### Open Question 2
- Question: What specific factors in model architecture or training contribute to varying robustness against different types of biases?
- Basis in paper: [inferred] The paper notes that some bias categories (e.g., sexual orientation, disability) are more effectively protected than others (e.g., gender, age), suggesting underlying factors influence robustness.
- Why unresolved: The paper does not investigate or explain the architectural or training-related factors that might contribute to these differences in bias robustness.
- What evidence would resolve it: Analysis of model architectures, training data composition, and alignment processes that correlates with bias robustness patterns.

### Open Question 3
- Question: How do safety measures against bias elicitation attacks evolve as models are fine-tuned or updated over time?
- Basis in paper: [inferred] The paper evaluates current state-of-the-art models but does not address how their robustness to bias elicitation changes over time with updates.
- Why unresolved: The study provides a snapshot of current model capabilities but does not track changes in safety measures over time or across model versions.
- What evidence would resolve it: Longitudinal studies comparing the same models across different versions or time periods, measuring changes in safety scores and robustness to attacks.

## Limitations
- Limited model sample: Only three LLM models were evaluated (GPT-3.5 Turbo, Gemini Pro, Llama 3.70B), which may not represent the full spectrum of available models
- Fixed bias categories: The study uses only seven bias categories, potentially missing other important dimensions of bias
- Time-sensitive results: The effectiveness of jailbreak techniques may vary depending on model versions and training updates

## Confidence

- **High Confidence**: The methodology for safety score calculation and the finding that no model achieved perfect safety are well-supported by the experimental design and results.
- **Medium Confidence**: The comparative analysis between models (e.g., GPT-3.5 Turbo being the least safe) is based on limited model samples and may not generalize to other model variants or sizes.
- **Medium Confidence**: The effectiveness of specific jailbreak techniques is demonstrated but may vary with prompt engineering sophistication and model-specific safety mechanisms.

## Next Checks

1. **Cross-Model Validation**: Test the same methodology on additional LLM models (including open-source and proprietary models) to verify if the observed safety score patterns and vulnerability rankings hold across a broader model spectrum.

2. **Temporal Robustness Check**: Re-run the jailbreak attacks on the same models after several months to assess whether safety improvements in model updates affect vulnerability to bias elicitation attacks.

3. **Bias Category Expansion**: Extend the evaluation to include additional bias categories beyond the seven examined, particularly those specific to different cultural contexts or emerging social issues, to determine if certain bias types are inherently more resistant to jailbreak attacks.