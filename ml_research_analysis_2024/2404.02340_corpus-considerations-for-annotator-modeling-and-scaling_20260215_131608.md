---
ver: rpa2
title: Corpus Considerations for Annotator Modeling and Scaling
arxiv_id: '2404.02340'
source_url: https://arxiv.org/abs/2404.02340
tags:
- annotator
- annotators
- number
- dataset
- modeling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the first systematic study of the relationship
  between annotator modeling methods and corpus statistics, including the number of
  annotations per annotator, the number of annotators, and the amount of overall data.
  The authors implement five different annotator modeling techniques and compare their
  performance across seven corpora and approximately 3k subsamples representing artificial
  datasets with controllable properties.
---

# Corpus Considerations for Annotator Modeling and Scaling

## Quick Facts
- arXiv ID: 2404.02340
- Source URL: https://arxiv.org/abs/2404.02340
- Authors: Olufunke O. Sarumi; Béla Neuendorf; Joan Plepi; Lucie Flek; Jörg Schlötterer; Charles Welch
- Reference count: 25
- Key outcome: The number of annotations per annotator is the most important factor for annotator modeling performance (R = 0.47, p < 0.0001), with simpler user token approaches often outperforming more complex methods.

## Executive Summary
This paper presents the first systematic study examining how corpus statistics affect the performance of different annotator modeling techniques in NLP. The authors compare five modeling approaches across seven datasets and approximately 3k subsamples representing artificial datasets with controllable properties. They find that when annotator agreement is high, composite embeddings perform best, while user token approaches excel when agreement is lower. The study reveals that the number of annotations per annotator is the most critical factor for model performance, with simpler approaches often outperforming more sophisticated methods developed in previous work.

## Method Summary
The study implements five annotator modeling techniques using SBERT with a DistilRoBERTa backbone across seven datasets. These techniques include a baseline SBERT model, user token embeddings (adding learnable identifier tokens for each annotator), composite embeddings (averaging embeddings of positive and negative instances per annotator), a combination of composite and user token approaches, and multi-task models with separate prediction layers per annotator. The researchers conduct extensive scaling experiments by subsampling annotations to create artificial datasets with varying numbers of annotators and annotations per annotator, evaluating performance using macro F1 scores across individual annotator labels.

## Key Results
- User token embeddings consistently outperform more complex annotator modeling methods across all datasets
- Composite embeddings perform best when annotator agreement is high (Krippendorff's α > 0.5)
- The number of annotations per annotator shows the strongest correlation with performance (R = 0.47, p < 0.0001)
- Simpler modeling approaches often outperform more sophisticated methods developed in previous work

## Why This Works (Mechanism)

### Mechanism 1
- Claim: User token embeddings consistently outperform more complex annotator modeling methods across datasets
- Mechanism: The user token approach adds a simple, learnable identifier token for each annotator that the model can adapt during training, providing sufficient capacity to capture annotator-specific patterns without overcomplicating the model
- Core assumption: Annotator identity contains sufficient signal for modeling individual perspectives, and simpler models generalize better than complex ones
- Evidence anchors:
  - [abstract] "we show that the commonly used user token model consistently outperforms more complex models"
  - [section] "we find that the user token approach often outperforms other more complex methods developed in previous work"
  - [corpus] Weak evidence - corpus statistics don't directly support why user tokens work better, but the finding holds across seven diverse datasets
- Break condition: When annotator identity alone lacks sufficient signal to distinguish perspectives, or when the number of annotators is extremely large, requiring more sophisticated modeling

### Mechanism 2
- Claim: Composite embeddings perform best when annotator agreement is high (Krippendorff's α > 0.5)
- Mechanism: When annotators agree more, aggregating their positive and negative embeddings creates a more stable, representative initialization that captures generalizable patterns across annotators
- Core assumption: High agreement means annotators are responding to similar features in the data, making their aggregated embeddings more informative
- Evidence anchors:
  - [abstract] "we find that when agreement is high, our composite embedding performs best"
  - [section] "This may be because when agreement is higher, the composite embedding is more informative as an initialization"
  - [corpus] Evidence from Table 1 shows datasets with K-α > 0.5 (ConvAbuse, ArMIS, Social Chemistry) show improved composite embedding performance
- Break condition: When agreement is low (K-α < 0.4), the aggregated embeddings become less representative of individual annotator patterns

### Mechanism 3
- Claim: Number of annotations per annotator is the most important factor for annotator modeling performance (R = 0.47, p < 0.0001)
- Mechanism: More annotations per annotator provide more training signal for the model to learn individual annotator patterns, regardless of the specific modeling approach used
- Core assumption: Each additional annotation provides independent signal about an annotator's judgment patterns
- Evidence anchors:
  - [abstract] "We find that the number of annotations per annotator is the most important factor for annotator modeling performance"
  - [section] "Our correlation coefficient was R = 0.47 (p < 0.0001)" when examining the relationship between performance and annotations per annotator
  - [corpus] The scaling experiments in Figure 2 show performance improvement plateaus after a couple hundred annotations per annotator
- Break condition: When annotators provide very few annotations (e.g., < 10), the model lacks sufficient signal to learn meaningful patterns

## Foundational Learning

- Concept: Annotator modeling and data perspectivism
  - Why needed here: Understanding why modeling individual annotators matters for subjective tasks and how it differs from traditional majority-vote approaches
  - Quick check question: Why might relying solely on majority labels be problematic for subjective NLP tasks?

- Concept: Dataset statistics and their impact on model performance
  - Why needed here: The paper shows how corpus properties (number of annotators, annotations per annotator, agreement level) affect which modeling approach works best
  - Quick check question: Based on the paper's findings, which corpus statistic shows the strongest correlation with annotator modeling performance?

- Concept: Embedding-based representation learning
  - Why needed here: All modeling approaches rely on embedding annotators (either through tokens or aggregated text embeddings) and understanding how these representations are learned and used is crucial
  - Quick check question: What is the key difference between how the user token and composite embedding approaches initialize annotator representations?

## Architecture Onboarding

- Component map: Text -> Base encoder (SBERT with DistilRoBERTa) -> Annotator representation (user token, composite embedding, or multi-task layers) -> Classification head -> Individual annotator predictions
- Critical path: Text → Base encoder → Annotator representation (concatenated or via separate layers) → Classification head → Individual annotator predictions
- Design tradeoffs: Simpler approaches (user token, composite) trade modeling capacity for efficiency and generalization, while multi-task models provide dedicated parameters per annotator but scale poorly
- Failure signatures: Poor performance on individual annotator labels despite good majority vote performance indicates the model is marginalizing minority perspectives; consistently poor performance across all methods suggests insufficient annotations per annotator
- First 3 experiments:
  1. Implement the user token baseline and verify it matches the reported performance on one dataset
  2. Add the composite embedding approach and test whether it improves performance on high-agreement datasets
  3. Run scaling experiments by subsampling annotations per annotator to confirm the correlation with performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the findings about annotator modeling performance generalize to tasks with more than two labels (multiclass classification)?
- Basis in paper: [inferred] The paper focuses on binary classification tasks and discusses the effectiveness of different annotator modeling techniques. It would be valuable to understand if these findings hold for multiclass classification tasks, which are common in NLP.
- Why unresolved: The paper explicitly states that all datasets used involve binary labels, and the analysis is based on these datasets. There is no discussion or experimentation with multiclass tasks.
- What evidence would resolve it: Conducting similar experiments on multiclass datasets and comparing the performance of annotator modeling techniques across different types of classification tasks would provide insights into the generalizability of the findings.

### Open Question 2
- Question: How do the underlying mechanisms of data collection and corpus construction impact annotator modeling performance, beyond surface-level corpus statistics?
- Basis in paper: [explicit] The paper mentions that their analysis is focused on correlations between performance and surface-level features, acknowledging that corpora have idiosyncrasies and future work should explore how to measure such qualities of dataset construction.
- Why unresolved: The paper does not delve into the specific mechanisms of data collection or corpus construction that might influence annotator modeling performance. It suggests that these factors could be significant but leaves them unexplored.
- What evidence would resolve it: Detailed studies comparing annotator modeling performance across datasets with different data collection and corpus construction methodologies would help identify the impact of these underlying mechanisms.

### Open Question 3
- Question: How robust are the findings about the importance of the number of annotations per annotator across different demographic groups of annotators?
- Basis in paper: [explicit] The paper notes that their datasets did not all include demographic information and that previous work has analyzed annotator identity groups' relation to performance. It suggests that future work should include more corpora with annotator information.
- Why unresolved: The lack of demographic information in the datasets used means the paper cannot assess whether the correlation between the number of annotations per annotator and performance is consistent across different demographic groups.
- What evidence would resolve it: Analyzing the performance of annotator modeling techniques on datasets that include detailed demographic information about annotators would reveal if the importance of the number of annotations per annotator varies across different groups.

## Limitations

- The study focuses exclusively on binary classification tasks, limiting generalizability to multi-class or regression tasks common in subjective annotation
- Computational constraints prevented testing more complex multi-task approaches at scale with 100+ annotations per annotator
- The paper identifies correlations between corpus statistics and model performance but does not establish causal relationships

## Confidence

**High Confidence:** The finding that the number of annotations per annotator shows the strongest correlation with performance (R = 0.47, p < 0.0001) is well-supported by extensive scaling experiments across multiple datasets. The user token approach consistently outperforming more complex methods across seven diverse datasets provides robust evidence for this claim.

**Medium Confidence:** The observation that composite embeddings perform best at high agreement levels (Krippendorff's α > 0.5) is supported by dataset analysis but could benefit from additional experimental validation through controlled manipulation of agreement levels within datasets.

**Low Confidence:** The suggestion that more complex multi-task models would outperform simpler approaches with 100+ annotations per annotator remains speculative due to computational constraints preventing testing at scale.

## Next Checks

1. **Agreement Level Manipulation Study:** Conduct controlled experiments where agreement levels are systematically varied within a single dataset (e.g., by filtering for high/low agreement instances) to directly test whether composite embeddings consistently outperform user tokens only when agreement exceeds the 0.5 threshold.

2. **Multi-Task Scaling Experiment:** Implement the multi-task model using distributed training or approximation techniques to test performance with 200+ annotations per annotator, validating whether the performance gains plateau at 50-100 annotations or continue improving with more data.

3. **Cross-Task Generalization Test:** Apply the five modeling approaches to a non-binary subjective task (e.g., fine-grained sentiment analysis or multi-class emotion classification) to evaluate whether the identified patterns hold across different task types and label spaces.