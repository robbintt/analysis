---
ver: rpa2
title: Zero-shot Safety Prediction for Autonomous Robots with Foundation World Models
arxiv_id: '2404.00462'
source_url: https://arxiv.org/abs/2404.00462
tags:
- world
- safety
- prediction
- state
- gemma
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes foundation world models that use a segmentation
  model to obtain object-level representations and a large language model to predict
  future object positions, enabling zero-shot safety prediction for autonomous robots.
  The approach outperforms standard world models in safety prediction tasks on cart
  pole and lunar lander benchmarks, achieving comparable performance to supervised
  learning despite not using any training data.
---

# Zero-shot Safety Prediction for Autonomous Robots with Foundation World Models

## Quick Facts
- arXiv ID: 2404.00462
- Source URL: https://arxiv.org/abs/2404.00462
- Reference count: 37
- One-line primary result: Foundation world models achieve zero-shot safety prediction for autonomous robots by using segmentation-derived object centroids and LLM prediction, outperforming standard world models and matching supervised learning performance on cart pole and lunar lander benchmarks.

## Executive Summary
This paper introduces foundation world models for zero-shot safety prediction in autonomous robotics. The approach replaces learned latent representations with object centroid positions derived from segmentation, then uses a large language model to predict future states without any training data. This eliminates the distribution shift problem that plagues standard world models while avoiding the need for safety classifiers. The method demonstrates competitive performance with supervised approaches on cart pole and lunar lander benchmarks, suggesting a promising path toward training-free safety prediction in robotic systems.

## Method Summary
The foundation world model operates by first using the Segment Anything Model (SAM) to extract object masks from observation images, then computing centroids of these objects as causally meaningful latent states. These states, along with actions, are assembled into prompts for a large language model (GPT-3.5 or Gemma) that predicts future object positions directly. Safety predictions are computed from these predicted positions without requiring additional classifiers. The approach reconstructs observations through pixel-level object movement rather than generative sampling, avoiding distribution shift issues. Performance is evaluated using a centroid distance metric that focuses on semantically important entities rather than pixel-wise aggregation.

## Key Results
- Foundation world models outperform standard world models in safety prediction tasks on cart pole and lunar lander benchmarks
- Safety prediction performance matches that of supervised learning approaches despite using no training data
- The centroid distance metric provides more system-relevant evaluation than image-wide metrics like MSE or SSIM
- LLM-based prediction achieves reasonable accuracy for prediction horizons up to k=10 without any model training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The foundation world model achieves zero-shot safety prediction by replacing learned latent representations with object centroid positions derived from segmentation, which have direct physical meaning.
- Mechanism: Segment Anything Model (SAM) extracts object masks from observations, computes centroids as causally meaningful latent states, and LLM predicts future centroids directly. This avoids the distribution shift problem inherent in standard VAE-based reconstruction.
- Core assumption: Object centroids are sufficient causal latent representations for the dynamics of the system, and future object positions are predictable from past positions and actions without training.
- Evidence anchors:
  - [abstract] "This enables the surrogate dynamics to directly predict causal future states by leveraging a training-free large language model."
  - [section II.B] "These objects in the observation are semantically important elements of the world that are assumed to have a causal relation to the future positions."
- Break condition: If object interactions involve complex physical phenomena (e.g., deformation, occlusion, or non-rigid bodies) that cannot be captured by simple centroid movement, prediction accuracy degrades.

### Mechanism 2
- Claim: Safety can be directly evaluated from predicted object positions without requiring an additional safety classifier.
- Mechanism: Since centroids represent physical object states, safety predicates (e.g., pole angle for cart pole, horizontal position bounds for lunar lander) can be computed directly from predicted centroid values rather than requiring image-based CNN classifiers.
- Core assumption: The safety condition is a simple function of object positions that can be computed without additional learned models.
- Evidence anchors:
  - [abstract] "Safety predictions are made directly from the predicted object positions, eliminating the need for additional safety classifiers."
  - [section II.C] "In standard world models, the CNN safety evaluator and controller suffer a distribution shift problem because the standard world model cannot perform perfectly and may reconstruct an out-of-distribution observation."
- Break condition: If safety depends on object shapes, velocities, or other properties not captured by centroids, direct safety evaluation fails.

### Mechanism 3
- Claim: The proposed metric using centroid distance (CD) provides a more system-relevant evaluation than image-wide metrics like MSE or SSIM.
- Mechanism: Instead of aggregating errors across all pixels, CD measures prediction accuracy for each object separately, focusing evaluation on semantically important entities while ignoring static background.
- Core assumption: Object centroids are the most important features for evaluating system dynamics, and background variations are irrelevant to safety and performance.
- Evidence anchors:
  - [section II.B] "The prediction of the objects' positions will be evaluated separately, unlike e.g. the image-wise MSE that takes all pixels into account and is affected by inconsequential entities like static objects/background that occupy most of the observation frame."
- Break condition: If background dynamics or pixel-level details are important for system evaluation, CD metric becomes insufficient.

## Foundational Learning

- Concept: Variational Autoencoder (VAE) and latent representations
  - Why needed here: Understanding standard world models helps appreciate why the foundation approach replaces learned latents with meaningful object representations
  - Quick check question: What are the two components that make up the VAE loss function during training?

- Concept: Large Language Models and prompting
  - Why needed here: The LLM is used to predict future object positions based on past states and actions, requiring understanding of prompt engineering and LLM inference
  - Quick check question: How does top-p sampling differ from temperature-based sampling in LLM inference?

- Concept: Segmentation and object detection
  - Why needed here: SAM provides the object masks that are converted to centroids, forming the causally meaningful latent states
  - Quick check question: What is the output format of SAM when segmenting an image?

## Architecture Onboarding

- Component map: Observation images → SAM Segmentation Model → Centroid Extraction → Prompt Assembly → LLM → Safety Evaluation → Output Disassembly
- Critical path: Observation → SAM → Centroids → LLM → Safety Prediction
- Design tradeoffs:
  - SAM provides zero-shot segmentation but may miss fine details
  - LLM prediction is training-free but may be less accurate than specialized models
  - Centroid representation simplifies prediction but loses shape information
  - No VAE decoder eliminates distribution shift but requires precise object tracking

- Failure signatures:
  - Safety predictions fail when objects are occluded or segmentation is inaccurate
  - LLM predictions degrade with long horizons or complex interactions
  - Centroid distance metric fails when object shapes or velocities matter for safety
  - Output reconstruction fails when object identities are ambiguous

- First 3 experiments:
  1. Test SAM segmentation on cart pole and lunar lander observations to verify object detection and centroid extraction
  2. Validate LLM prediction accuracy for short horizons (k=1-5) on both benchmarks
  3. Compare safety prediction F1 scores between foundation and standard world models for k=10

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of foundation model architecture (e.g., GPT-3.5 vs. Gemma) impact the accuracy and reliability of safety predictions in different robotic systems?
- Basis in paper: [explicit] The paper evaluates safety prediction performance using both GPT-3.5 and Gemma models, showing that different models exhibit varying performance across different benchmarks and input lengths.
- Why unresolved: While the paper compares two specific models, it doesn't systematically explore how different foundation model architectures affect performance across a wider range of robotic systems or safety-critical scenarios.
- What evidence would resolve it: A comprehensive study comparing multiple foundation model architectures (including newer models) across diverse robotic systems with varying complexity, safety requirements, and environmental conditions.

### Open Question 2
- Question: What is the optimal balance between prompt length and computational efficiency for foundation world models in real-time safety-critical applications?
- Basis in paper: [explicit] The paper mentions that input prompt length was chosen due to cost constraints and that longer inputs might improve performance for certain tasks like the lunar lander.
- Why unresolved: The paper demonstrates trade-offs between prompt length and performance but doesn't establish guidelines for balancing computational efficiency with prediction accuracy in real-time applications where latency is critical.
- What evidence would resolve it: Empirical studies quantifying the relationship between prompt length, prediction accuracy, computational cost, and real-time performance requirements across multiple robotic systems with varying safety criticality.

### Open Question 3
- Question: How can foundation world models be extended to handle multi-modal observations (e.g., combining visual, LiDAR, and inertial data) while maintaining zero-shot capabilities?
- Basis in paper: [inferred] The current model uses only visual segmentation data, and the discussion section mentions exploring multimodal foundation models as future work.
- Why unresolved: The paper focuses on single-modality visual observations, but real-world robotic systems typically rely on multiple sensor types. The challenge of integrating these without requiring additional training data remains unaddressed.
- What evidence would resolve it: Implementation and evaluation of foundation world models that process and integrate multiple sensor modalities without additional training, demonstrating maintained or improved safety prediction capabilities.

## Limitations

- The approach relies heavily on SAM segmentation accuracy, which may fail with complex scenes, occlusions, or non-rigid objects
- Centroid-based representation cannot capture object shape, velocity, or orientation information that may be critical for certain safety predicates
- LLM-based prediction performance depends on prompt engineering quality and may degrade with longer prediction horizons or novel trajectory patterns

## Confidence

| Claim | Confidence |
|-------|------------|
| Zero-shot safety prediction works without training data | Medium |
| Foundation world models match supervised learning performance | Medium |
| Centroid distance metric is more system-relevant than pixel-wise metrics | Medium |

## Next Checks

1. Test SAM segmentation and prediction accuracy on environments with dynamic backgrounds, occlusions, and non-rigid objects to assess robustness beyond the simple benchmarks presented.
2. Evaluate safety prediction performance when safety predicates depend on object velocity, orientation, or shape rather than just position, to test the limits of centroid-based safety evaluation.
3. Measure prediction accuracy and safety performance as a function of prediction horizon length (k=1 to k=50) to quantify error accumulation and identify the practical limits of the approach.