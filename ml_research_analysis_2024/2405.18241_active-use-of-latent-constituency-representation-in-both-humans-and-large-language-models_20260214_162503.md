---
ver: rpa2
title: Active Use of Latent Constituency Representation in both Humans and Large Language
  Models
arxiv_id: '2405.18241'
source_url: https://arxiv.org/abs/2405.18241
tags:
- sentences
- constituency
- language
- word
- sentence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study investigates how both human brains and large language
  models (LLMs) internally represent sentence structure, focusing on hierarchical
  linguistic constituents. A novel word deletion task was designed where participants
  inferred deletion rules from single demonstrations, without explicit linguistic
  knowledge.
---

# Active Use of Latent Constituency Representation in both Humans and Large Language Models

## Quick Facts
- arXiv ID: 2405.18241
- Source URL: https://arxiv.org/abs/2405.18241
- Authors: Wei Liu; Ming Xiang; Nai Ding
- Reference count: 0
- Primary result: Both humans and ChatGPT use latent constituency representations in a one-shot word deletion task, preferring to delete complete constituents over nonconstituents.

## Executive Summary
This study investigates whether humans and large language models (LLMs) actively use latent hierarchical linguistic structures during real-time language processing. Using a novel word deletion task where participants infer deletion rules from single demonstrations, the research demonstrates that both humans and ChatGPT consistently delete complete constituents more often than chance. This behavior cannot be explained by word properties or positions alone, and constituency trees reconstructed from deletion patterns show structural similarities to linguistically defined trees. The findings reveal that latent constituency representations emerge in both humans and LLMs and are actively employed during language comprehension.

## Method Summary
The study employed a novel word deletion task where participants and LLMs inferred deletion rules from single demonstrations. Sentences were sampled from Penn Chinese Treebank (CTB) and Penn Treebank (PTB) with 4-15 words and depth 3-8, excluding those with named entities/numbers/punctuation. Demonstration sentences embedded NPs in VPs, always deleting constituents. Participants/LLMs received 24 demonstration-test pairs and were asked to delete words to transform test sentences into similar meanings. Human participants (N=58) and ChatGPT (gpt-35-turbo-0301) were tested alongside an LSTM baseline trained with 1-700 demonstrations. Deletion behaviors were analyzed to calculate constituent rates, explained syntactic rules, and reconstruct constituency trees using CKY algorithm, which were then compared to linguistically defined trees using F1 scores and balance factors.

## Key Results
- Both humans and ChatGPT showed constituent deletion rates significantly above chance (80% for humans, 72% for ChatGPT vs. 60% chance)
- Deletion-based constituency trees explained approximately 80% of deleted word strings and showed structural similarity to linguistically defined trees
- Humans and LLMs flexibly adapted rule preferences based on input language, with English favoring parent-category rules and Chinese favoring node-category rules
- Semantic constraints had minimal influence on deletion choices compared to syntactic constituency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Humans and LLMs share an abstract capacity to construct hierarchical linguistic constituents even without explicit training.
- Mechanism: Both systems internally group words into latent syntactic units during real-time language processing, which manifests behaviorally in a novel word deletion task.
- Core assumption: The deletion behavior is guided by an implicit constituency structure rather than explicit memorization or shallow statistical cues.
- Evidence anchors:
  - [abstract]: "Both humans and LLMs tend to delete a constituent, instead of a nonconstituent word string."
  - [section]: "The results demonstrated that word deletion behaviors of human and ChatGPT could be well explained by a latent constituency tree."
- Break condition: If deletion behavior were driven solely by word frequency or surface patterns, LSTM baseline would match human/ChartGPT performance with single demonstration.

### Mechanism 2
- Claim: Rule inference in word deletion is language-dependent, reflecting different parsing strategies.
- Mechanism: English and Chinese activate distinct parsing strategies (top-down vs bottom-up) due to their structural differences, which is then used to infer deletion rules.
- Core assumption: Parsing strategies can be flexibly adapted based on input language rather than being fixed by native language background.
- Evidence anchors:
  - [abstract]: "Humans and LLMs flexibly adapted rule preferences based on input language."
  - [section]: "For both humans and ChatGPT, the constituent rate was significantly higher than chance... and the responses on English sentences were better explained by the parent-category rule while the responses on Chinese sentences were better explained by the node-category rule."
- Break condition: If native language background rigidly determined parsing strategies, L2 speakers would prefer rules aligned with their native language regardless of input.

### Mechanism 3
- Claim: Word deletion behavior can reconstruct the latent constituency tree, which structurally resembles the linguistically defined tree.
- Mechanism: By observing which constituents are deleted across multiple demonstrations, one can infer the underlying tree structure that best explains the deletion patterns.
- Core assumption: Deletion choices systematically reflect the internal constituency representation rather than random or surface-level preferences.
- Evidence anchors:
  - [abstract]: "Constituency trees could be reconstructed from deletion behaviors and showed structural similarities to linguistically defined trees."
  - [section]: "For both humans and ChatGPT, the deletion-based constituency tree explained about 80% of the deleted word strings... and the latent constituency tree was quantitatively similar to the linguistic constituency tree."
- Break condition: If deletion behavior were random or surface-based, reconstructed trees would show no systematic similarity to linguistic trees.

## Foundational Learning

- Concept: Hierarchical structure in language
  - Why needed here: Understanding why grouping words into constituents matters for parsing and comprehension
  - Quick check question: Can you identify the constituent boundaries in "The quick brown fox jumps over the lazy dog"?

- Concept: One-shot learning
  - Why needed here: The task requires inferring deletion rules from a single demonstration, not extensive training
  - Quick check question: If shown "She ate an apple" â†’ "She ate", what rule would you infer for "He drank some water"?

- Concept: Constituency vs dependency parsing
  - Why needed here: The study focuses on constituency trees (grouping words) rather than dependency relations (word-to-word links)
  - Quick check question: How would you represent "The cat sat on the mat" as a constituency tree versus a dependency tree?

## Architecture Onboarding

- Component map: Task interface -> Rule inference engine -> Constituency reconstruction module -> Behavioral analysis layer
- Critical path: Demonstration -> Deletion decision -> Rule inference -> Constituency tree reconstruction
- Design tradeoffs: Task simplicity vs ecological validity; single demonstration vs multiple demonstrations; behavioral inference vs direct neural access
- Failure signatures: 
  - Constituent rate near chance level
  - No systematic differences between languages
  - Reconstructed trees dissimilar to linguistic trees
- First 3 experiments:
  1. Replicate the word deletion task with a new LLM to test generalizability
  2. Test with children to probe developmental trajectory of constituency representation
  3. Use structurally ambiguous sentences to test interaction between syntax and semantics

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Do LLMs employ fundamentally different parsing strategies for different languages (Chinese vs English), or do they use the same strategy but adjust parameters based on linguistic properties?
- Basis in paper: [explicit] The paper shows that both humans and ChatGPT prefer different rules (node-category vs parent-category) when processing Chinese vs English, and this preference is linked to the right-branching nature of English.
- Why unresolved: While the paper demonstrates a language-dependent rule preference, it doesn't conclusively determine whether this reflects different parsing strategies or a single strategy with adjustable parameters.
- What evidence would resolve it: Analyzing the internal representations of LLMs during parsing of different languages, or designing experiments that manipulate specific linguistic parameters while keeping the overall strategy constant.

### Open Question 2
- Question: To what extent do LLMs rely on syntactic structure versus semantic plausibility when performing language tasks?
- Basis in paper: [explicit] The paper shows that humans primarily rely on syntax while ChatGPT shows a weak but significant influence of semantics, often deleting semantically implausible constituents.
- Why unresolved: The study focuses on a specific word deletion task, and it's unclear whether the observed behavior generalizes to other language tasks or if it's specific to the task design.
- What evidence would resolve it: Conducting similar experiments with different language tasks and comparing the performance of humans and LLMs on syntactically ambiguous sentences with varying degrees of semantic plausibility.

### Open Question 3
- Question: How do the number of demonstrations and the complexity of the constituency structure influence the ability of LLMs to learn and apply constituency-based rules?
- Basis in paper: [explicit] The paper shows that a naive LSTM model requires more demonstrations to achieve the same performance as humans and ChatGPT, and that the number of demonstrations needed varies between English and Chinese.
- Why unresolved: The study only explores a limited range of demonstration numbers and doesn't investigate the impact of constituency structure complexity on learning.
- What evidence would resolve it: Systematically varying the number of demonstrations and the complexity of the constituency structures in the word deletion task, and measuring the performance of both LLMs and humans across these conditions.

## Limitations
- The single-demonstration paradigm may not fully capture how constituency representations develop through exposure
- The LSTM baseline may not represent the full range of shallow models that could potentially succeed
- Behavioral inference cannot directly access internal representations, leaving open the possibility that deletion behavior is driven by different underlying mechanisms

## Confidence
- **High confidence**: Both humans and ChatGPT show constituent deletion rates significantly above chance, and deletion-based constituency trees resemble linguistically defined trees
- **Medium confidence**: The one-shot learning mechanism and language-dependent parsing strategy adaptation claims, as these rely on behavioral inference without direct neural evidence
- **Low confidence**: The claim that semantic constraints have minimal influence, given the limited scope of semantic manipulation in the study

## Next Checks
1. Test whether LLMs with explicit syntactic training (like CodeGen) show different deletion patterns than foundation models like ChatGPT
2. Conduct a pre-registered replication with a larger sample size and different treebank domains to assess generalizability
3. Implement neural probing techniques (e.g., diagnostic classifiers) on LLM hidden states during the deletion task to directly validate the behavioral inference of constituency use