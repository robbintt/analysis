---
ver: rpa2
title: Multi-Objective Evolutionary Neural Architecture Search for Recurrent Neural
  Networks
arxiv_id: '2403.11173'
source_url: https://arxiv.org/abs/2403.11173
tags:
- architecture
- algorithm
- search
- architectures
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a multi-objective evolutionary algorithm-based
  RNN architecture search method, called MOE/RNAS, to automatically construct RNN
  architectures for a provided dataset. The MOE/RNAS algorithm relies on a multi-objective
  EA that is based on the NSGA-II algorithm for the exploration of the cell-based
  RNN architecture search space.
---

# Multi-Objective Evolutionary Neural Architecture Search for Recurrent Neural Networks

## Quick Facts
- **arXiv ID:** 2403.11173
- **Source URL:** https://arxiv.org/abs/2403.11173
- **Reference count:** 40
- **Primary result:** MOE/RNAS algorithm automatically constructs RNN architectures optimizing accuracy and complexity trade-offs

## Executive Summary
This paper introduces MOE/RNAS, a multi-objective evolutionary algorithm-based approach for automatic RNN architecture search. The method employs NSGA-II to explore cell-based RNN architecture spaces while optimizing both accuracy and complexity objectives. Unlike existing approaches, MOE/RNAS leverages approximate network morphisms to optimize architecture complexity, enabling the evolution of computationally efficient RNNs that maintain good accuracy. The algorithm demonstrates capability to automatically construct novel RNN architectures that effectively learn from provided datasets while balancing model performance against computational resource demands.

## Method Summary
MOE/RNAS utilizes a multi-objective evolutionary algorithm based on NSGA-II to search through cell-based RNN architecture spaces. The algorithm maintains a population of RNN architectures and evolves them over generations by applying genetic operators such as crossover and mutation. A key distinguishing feature is the use of approximate network morphisms, which allow the algorithm to modify architectures while preserving learned functionality. This enables optimization of complexity-related objectives without sacrificing learned capabilities. The search process simultaneously optimizes multiple objectives including model accuracy and architectural complexity, ultimately producing a Pareto front of solutions representing different accuracy-efficiency trade-offs.

## Key Results
- MOE/RNAS successfully constructs novel RNN architectures that effectively learn from provided datasets
- The algorithm demonstrates capability to optimize RNN architecture complexity-related objectives
- When accepting reasonable trade-offs between accuracy and computational resources, MOE/RNAS evolves computationally efficient RNN architectures with good model accuracy

## Why This Works (Mechanism)
The algorithm works by maintaining diversity in the population through NSGA-II's selection mechanism, which preserves both high-performing and novel architectures. Approximate network morphisms enable complexity optimization by allowing architectural modifications that maintain learned functionality, preventing catastrophic forgetting during evolution. The multi-objective framework naturally balances competing objectives, producing architectures that achieve Pareto-optimal trade-offs between accuracy and efficiency.

## Foundational Learning
- **Evolutionary algorithms (NSGA-II):** Population-based optimization method needed for exploring large architectural search spaces; quick check: verify understanding of selection, crossover, and mutation operators
- **Neural architecture search (NAS):** Automated approach to discovering effective neural network architectures; quick check: confirm knowledge of cell-based search spaces
- **Approximate network morphisms:** Techniques for modifying neural networks while preserving learned functionality; quick check: understand how architectural changes can be made without retraining from scratch
- **Multi-objective optimization:** Optimization framework for balancing competing objectives; quick check: verify understanding of Pareto optimality and trade-off analysis
- **Recurrent neural networks (RNNs):** Neural networks designed for sequential data processing; quick check: understand RNN cell architectures and training procedures
- **Pareto optimality:** State where no objective can be improved without degrading others; quick check: verify ability to identify Pareto-optimal solutions from objective value sets

## Architecture Onboarding
**Component map:** MOE/RNAS algorithm -> Population initialization -> Evolutionary operators (crossover, mutation) -> Approximate network morphisms -> NSGA-II selection -> Pareto front extraction
**Critical path:** Population initialization → Evaluation on dataset → NSGA-II selection → Offspring generation via crossover/mutation → Network morphism application → Next generation formation
**Design tradeoffs:** Accuracy vs computational efficiency; exploration vs exploitation; mutation rates vs population diversity; morphism preservation strength vs architectural flexibility
**Failure signatures:** Population convergence to local optima; degradation of accuracy during complexity optimization; inability to escape suboptimal architectures; excessive computational resource consumption
**First experiments:** 1) Run algorithm on simple sequential dataset with known optimal architecture complexity; 2) Compare Pareto fronts with and without network morphism operators; 3) Measure convergence speed across different population sizes

## Open Questions the Paper Calls Out
None

## Limitations
- Limited experimental validation across diverse datasets beyond reported benchmarks
- Insufficient discussion of computational resource requirements for the evolutionary search process
- Claims about automatic construction of "novel" architectures lack comparative analysis against existing architectures

## Confidence
- Novel architecture discovery: Medium
- Multi-objective optimization capability: High
- Computational efficiency: Medium
- Automatic architecture construction: Medium

## Next Checks
1. Test MOE/RNAS across multiple diverse datasets beyond the reported benchmarks to assess generalizability
2. Conduct ablation studies comparing architectures with and without network morphism operators to quantify their contribution
3. Measure and report total computational resources required for the evolutionary search process versus final model inference time