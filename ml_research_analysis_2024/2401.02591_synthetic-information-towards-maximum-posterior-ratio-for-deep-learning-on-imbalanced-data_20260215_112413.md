---
ver: rpa2
title: Synthetic Information towards Maximum Posterior Ratio for deep learning on
  Imbalanced Data
arxiv_id: '2401.02591'
source_url: https://arxiv.org/abs/2401.02591
tags:
- data
- samples
- learning
- class
- minority
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of deep learning on imbalanced
  tabular data by proposing a minority oversampling technique called Synthetic Information
  towards Maximum Posterior Ratio (SIMPOR). The method generates synthetic minority
  samples by maximizing the posterior probability ratio using kernel density estimation,
  while focusing on balancing the informative region identified via entropy-based
  active learning.
---

# Synthetic Information towards Maximum Posterior Ratio for deep learning on Imbalanced Data

## Quick Facts
- arXiv ID: 2401.02591
- Source URL: https://arxiv.org/abs/2401.02591
- Authors: Hung Nguyen; Morris Chang
- Reference count: 40
- Primary result: SIMPOR achieves highest F1-score (23 wins) and AUC (25 wins) on 41 real-world datasets

## Executive Summary
This paper addresses deep learning on imbalanced tabular data through a novel minority oversampling technique called Synthetic Information towards Maximum Posterior Ratio (SIMPOR). The method generates synthetic minority samples by maximizing the posterior probability ratio using kernel density estimation, focusing on balancing the informative region identified via entropy-based active learning. Synthetic samples are generated within neighborhoods of minority instances to preserve data topology. Experiments demonstrate statistically significant improvements over six baseline methods across 41 datasets.

## Method Summary
SIMPOR generates synthetic minority samples through a three-stage process: (1) Entropy-based active learning identifies high-information samples comprising a specified informative portion of the training data, (2) For each minority sample in the informative region, synthetic neighbors are generated by maximizing the posterior probability ratio f1/f0 using kernel density estimation for likelihood approximation and gradient ascent optimization, (3) The remaining data is balanced using the same generation procedure without the informative region constraint. Synthetic samples are created on d-sphere surfaces centered at minority samples with radii sampled from Gaussian distributions, ensuring local neighborhood preservation while improving class balance.

## Key Results
- SIMPOR achieves the highest F1-score on 23 out of 41 real-world datasets tested
- SIMPOR achieves the highest AUC on 25 out of 41 real-world datasets tested
- Wilcoxon signed-rank tests confirm statistically significant improvements over six baseline methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generating synthetic minority samples within each minority sample's neighborhood preserves data topology while improving class balance.
- Mechanism: For each minority sample, synthetic samples are generated on a d-sphere surface centered at the minority sample, with the sphere radius sampled from a Gaussian distribution. This ensures synthetic samples surround the minority sample rather than creating artificial connections across the feature space.
- Core assumption: Data topology is best preserved by generating synthetic samples in local neighborhoods rather than through global interpolation methods.
- Evidence anchors:
  - [abstract] "synthetic data are closely generated within each minority sample neighborhood"
  - [section] "a synthetic neighbor x′ and its label y′ can be created surrounding a minority sample x by adding a small random vector v to the sample"
  - [corpus] Weak - corpus focuses on related oversampling techniques but doesn't directly validate the neighborhood preservation claim
- Break condition: If the neighborhood radius is too large or the Gaussian distribution has excessive variance, synthetic samples may cross decision boundaries, breaking topology preservation.

### Mechanism 2
- Claim: Maximizing the posterior probability ratio ensures synthetic samples fall into the minority class and stay far from the majority class.
- Mechanism: The technique maximizes f = f1/f0 where f1 is the posterior probability for minority class and f0 for majority class. By using kernel density estimation to approximate likelihoods and empirical Bayes for priors, the method avoids infeasible computations while directing synthetic samples toward the minority class.
- Core assumption: The posterior ratio maximization effectively balances proximity to minority class with distance from majority class.
- Evidence anchors:
  - [abstract] "maximizing the posterior probability ratio using kernel density estimation"
  - [section] "we propose a technique that maximizes the fractional posterior f = f1/f0"
  - [corpus] Weak - corpus mentions related techniques but doesn't validate the specific posterior ratio approach
- Break condition: If kernel density estimation poorly approximates the true likelihood due to insufficient data or inappropriate bandwidth selection, the posterior ratio maximization may fail to direct samples correctly.

### Mechanism 3
- Claim: Prioritizing the informative region (high entropy samples) reduces decision boundary distortion compared to balancing the entire dataset uniformly.
- Mechanism: Entropy-based active learning identifies samples that provide high information to the model. The technique first balances this informative region by generating synthetic minority samples, then balances the remaining data. This prevents the majority class from dominating decision boundaries in critical regions.
- Core assumption: The informative region contains the most critical information for classification and is most susceptible to imbalance effects.
- Evidence anchors:
  - [abstract] "prioritizes balancing the informative regions by identifying high entropy samples"
  - [section] "an entropy-based deep active learning technique is used to select samples yielding high entropy"
  - [corpus] Weak - corpus mentions entropy-based approaches but doesn't validate the specific prioritization strategy
- Break condition: If the entropy estimation is biased by the initial imbalanced training data, the informative region identification may be incorrect, leading to ineffective balancing.

## Foundational Learning

- **Kernel Density Estimation (KDE)**
  - Why needed here: KDE approximates the likelihood terms in the posterior ratio calculation without assuming specific data distributions, making it suitable for complex, high-dimensional tabular data.
  - Quick check question: What happens to KDE estimates when the bandwidth parameter is set too small versus too large?

- **Posterior Probability Ratio**
  - Why needed here: Maximizing the posterior ratio f1/f0 ensures synthetic samples are both likely to belong to the minority class and unlikely to belong to the majority class, addressing both class assignment and boundary separation.
  - Quick check question: How does the posterior ratio approach differ from simply maximizing the minority class posterior probability?

- **Entropy-based Active Learning**
  - Why needed here: Identifies the most informative samples where class imbalance has the greatest negative impact on model performance, allowing targeted balancing of critical regions first.
  - Quick check question: Why might entropy-based selection be more effective than random selection for identifying informative samples in imbalanced datasets?

## Architecture Onboarding

- **Component map**: Entropy-based active learning module -> Kernel density estimation module -> Gradient ascent optimization -> Data balancing controller
- **Critical path**: Informative region identification → Likelihood approximation via KDE → Posterior ratio maximization via gradient ascent → Synthetic sample generation → Data balancing completion
- **Design tradeoffs**: Higher neighborhood radius preserves more topology but risks boundary crossing; larger informative portion ensures critical regions are balanced but increases computational cost; more gradient ascent iterations improve sample placement but slow generation
- **Failure signatures**: Poor classification performance despite balancing (indicates incorrect informative region identification or posterior ratio maximization failure); excessive processing time (indicates inefficient KDE or gradient ascent); synthetic samples clustering in wrong regions (indicates bandwidth selection issues)
- **First 3 experiments**:
  1. Run SIMPOR on a simple 2D imbalanced dataset (like the Moon dataset) and visualize synthetic sample distribution compared to ground truth
  2. Test different bandwidth selection methods for KDE and measure impact on classification performance
  3. Vary the informative portion parameter and evaluate trade-offs between balancing effectiveness and computational cost

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of SIMPOR compare to other techniques when applied to image datasets, which are high-dimensional and topologically complex?
  - Basis in paper: [inferred] The paper mentions that most existing techniques for class imbalance in deep learning focus on image data, but SIMPOR is designed for tabular data. The authors suggest investigating the impact of class imbalance on image data types as future work.
  - Why unresolved: The paper only evaluates SIMPOR on tabular datasets and does not provide any results or analysis for image datasets.
  - What evidence would resolve it: Experimental results comparing the performance of SIMPOR and other techniques on a variety of image datasets with different levels of class imbalance.

- **Open Question 2**: What is the impact of the informative portion (IP) parameter on the performance of SIMPOR, and how can it be optimally tuned for different datasets?
  - Basis in paper: [explicit] The paper mentions that the IP parameter controls the proportion of informative samples used for balancing and suggests tuning it for each dataset between a range of (0.2, 0.6) to achieve higher performance.
  - Why unresolved: The paper does not provide a systematic approach or guidelines for tuning the IP parameter, and the impact of different IP values on the performance is not fully explored.
  - What evidence would resolve it: A comprehensive study investigating the relationship between IP values and the performance of SIMPOR on a wide range of datasets, along with guidelines for selecting the optimal IP value.

- **Open Question 3**: How does the choice of the radius factor r in the generation of synthetic samples affect the performance of SIMPOR, and what is the optimal range for this parameter?
  - Basis in paper: [explicit] The paper mentions that the radius factor r controls the distance of synthetic samples from the original minority samples and suggests that a Gaussian standard deviation ranging from 0.6R to R has minimal impact on the classification performance.
  - Why unresolved: The paper does not provide a detailed analysis of the impact of different radius factor values on the performance, and the optimal range for this parameter is not clearly defined.
  - What evidence would resolve it: A thorough investigation of the relationship between the radius factor r and the performance of SIMPOR on various datasets, along with recommendations for selecting the optimal range of this parameter.

## Limitations

- The methodology assumes KDE accurately captures likelihood distributions in high-dimensional spaces, which may not hold for all tabular datasets with sparse features or non-smooth decision boundaries.
- The entropy-based active learning component could be sensitive to initial class imbalance, potentially misidentifying the "informative region" if the minority class is extremely underrepresented.
- The paper doesn't address computational scalability for datasets with thousands of features or millions of samples, as the gradient ascent optimization for each synthetic sample could become prohibitively expensive.

## Confidence

- **High**: Empirical performance improvements over baseline methods (F1-score and AUC metrics)
- **Medium**: The theoretical framework connecting posterior ratio maximization to improved classification
- **Low**: Claims about topology preservation through neighborhood-based generation without rigorous validation

## Next Checks

1. **Theoretical validation**: Derive formal error bounds showing how posterior ratio maximization affects classification margin width and generalization error compared to uniform sampling approaches.

2. **Scalability assessment**: Benchmark SIMPOR on datasets with increasing dimensionality (100-1000 features) and sample sizes (10K-1M samples) to identify computational bottlenecks and determine practical limits.

3. **Topology preservation analysis**: Apply topological data analysis techniques (persistent homology) to compare the connectivity structure of original vs. synthetically augmented datasets, quantifying whether neighborhood-based generation truly preserves intrinsic data topology.