---
ver: rpa2
title: Probabilistic Emulation of a Global Climate Model with Spherical DYffusion
arxiv_id: '2406.14798'
source_url: https://arxiv.org/abs/2406.14798
tags:
- climate
- ensemble
- sfno
- time
- weather
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents the first conditional generative model for probabilistic
  emulation of a global climate model, addressing the challenge of long-term climate
  projections that require stable, physically consistent simulations over decades
  or centuries. The core method, Spherical DYffusion, integrates the dynamics-informed
  diffusion framework (DYffusion) with the Spherical Fourier Neural Operator (SFNO)
  architecture, enabling stable 100-year simulations at 6-hourly timesteps while maintaining
  low computational overhead compared to single-step deterministic baselines.
---

# Probabilistic Emulation of a Global Climate Model with Spherical DYffusion

## Quick Facts
- arXiv ID: 2406.14798
- Source URL: https://arxiv.org/abs/2406.14798
- Reference count: 40
- First conditional generative model for probabilistic emulation of a global climate model, achieving near gold-standard performance with stable 100-year simulations

## Executive Summary
This paper introduces Spherical DYffusion, the first conditional generative model for probabilistic emulation of a global climate model. The method addresses the challenge of long-term climate projections that require stable, physically consistent simulations over decades or centuries. By integrating the dynamics-informed diffusion framework (DYffusion) with the Spherical Fourier Neural Operator (SFNO) architecture, the model achieves stable 100-year simulations at 6-hourly timesteps while maintaining low computational overhead. The approach significantly outperforms existing baselines in reducing climate biases while providing reliable ensemble simulations.

## Method Summary
Spherical DYffusion integrates the DYffusion framework with SFNO architecture to create a probabilistic climate emulator. The method uses MC dropout and stochastic depth at inference time to generate ensemble simulations, with a two-stage training procedure: first training an interpolator (SFNOϕ) on L2 loss, then a forecaster (SFNOθ) on L1 loss. Time conditioning is implemented through Fourier feature embeddings mapped to scale and shift parameters for each SFNO block. The model takes 34 prognostic variables (8 vertical layers + 2 surface) and 7 forcing variables as inputs, producing 34 prognostic variables at each 6-hour step for 10 years. Inference uses cold sampling with correction to generate 25-member ensembles for probabilistic metrics.

## Key Results
- Reduces climate biases to within 50% of the reference model on average across all 34 predicted fields, more than 2× and 4× lower than the best baselines
- For critical fields such as total water path, achieves results within 20% of the reference model, representing a 5× improvement over the next best baseline
- Successfully reproduces climate variability consistent with the reference model and further reduces climate biases towards the theoretical minimum through ensemble-averaging
- Maintains stable 100-year simulations at 6-hourly timesteps while keeping computational overhead low compared to single-step deterministic baselines

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The spherical Fourier neural operator (SFNO) enables stable long-term climate emulation by capturing long-range interactions in the spherical domain without artifacts from Euclidean transforms.
- **Mechanism**: SFNO replaces the Euclidean Fast Fourier Transform with a spherical harmonic transform (SHT), which is the natural Fourier-like transform on a sphere. This preserves rotational and reflectional symmetries of Earth's atmospheric fields, reducing boundary-induced artifacts and ensuring stable autoregressive rollouts over decades.
- **Core assumption**: The spherical harmonic transform adequately represents atmospheric dynamics and their spatial correlations on a global grid without losing critical physics.
- **Evidence anchors**:
  - [section]: "SFNO efficiently model long-range interactions in the Fourier space, but because the underlying Fast Fourier Transform is defined on a Euclidean domain, this can lead to modeling artifacts. SFNOs overcome this issue by using the spherical harmonic transform (SHT) [12], a generalization of the Fourier transform, instead."
  - [abstract]: "Our model integrates the dynamics-informed diffusion framework (DYffusion) with the Spherical Fourier Neural Operator (SFNO) architecture, enabling stable 100-year simulations at 6-hourly timesteps while maintaining low computational overhead compared to single-step deterministic baselines."
- **Break condition**: If the SHT truncates important high-frequency components, or if atmospheric dynamics are not well captured by the spectral basis, long-term stability will fail.

### Mechanism 2
- **Claim**: DYffusion's dynamics-informed reverse process enables probabilistic climate emulation with computational overhead comparable to deterministic forecasting.
- **Mechanism**: Instead of corrupting data with noise and then denoising it autoregressively, DYffusion directly couples the forward and reverse processes to the physical time steps of the data. The reverse process is initialized with the initial state and evolves toward future time steps by alternating between direct forecasts and interpolations, requiring fewer neural network evaluations than standard diffusion.
- **Core assumption**: The interpolator and forecaster networks can approximate the true forward and reverse diffusion dynamics in the time domain, respectively, and dropout at inference time provides sufficient stochasticity for ensemble sampling.
- **Evidence anchors**:
  - [section]: "The key idea is to make the forward and reverse processes dynamics-informed by directly coupling them to the physical time steps of the data. That is, the reverse process is initialized with s(N ) = x0 and iteratively evolves jointly with the dynamics of the data x1, . . . ,xh−1 to reach the data at some target time step, s(0) = xh."
  - [section]: "DYffusion was shown to be faster at sampling time and more memory-efficient than standard diffusion models, while matching or outperforming their accuracy."
- **Break condition**: If the stochasticity from dropout is insufficient, the ensemble will be underdispersive. If the interpolator or forecaster architectures are mismatched, the dynamics coupling will fail and predictions will diverge.

### Mechanism 3
- **Claim**: Time-conditioning via Fourier feature embeddings allows the SFNO-based interpolator and forecaster to adapt their predictions to the temporal context of each step.
- **Mechanism**: For each SFNO block, the time index is transformed into 32-frequency sine/cosine features, passed through a 2-layer MLP to produce 128-dimensional embeddings, then mapped to learnable scale and shift parameters applied after normalization and before the spectral filter. This injects temporal context into each layer without breaking the spectral operation's structure.
- **Core assumption**: The linear mapping from Fourier features to scale/shift parameters can capture the necessary time-dependent modulation of the network's internal representations.
- **Evidence anchors**:
  - [section]: "We follow the same approach taken by standard diffusion models [11], which consists of transforming the time condition into a vector of sine/cosine Fourier features at 32 frequencies with base period 16, then pass them through a 2-layer MLP to obtain 128-dimensional time encodings that are mapped by a linear layer into the learnable scale and offset parameters."
  - [section]: "We scale and shift the neural representations of every SFNO block directly following the normalization layer and preceding the application of the SFNO spectral filter."
- **Break condition**: If the temporal dynamics are too complex for a simple scale/shift modulation, the model may fail to capture time-varying processes like diurnal cycles or seasonal shifts.

## Foundational Learning

- **Concept**: Spherical harmonic transform (SHT)
  - **Why needed here**: SHT generalizes Fourier analysis to the sphere, enabling efficient spectral modeling of atmospheric fields on a global grid without boundary artifacts.
  - **Quick check question**: What is the main difference between SHT and the Euclidean FFT in terms of handling global spatial data?

- **Concept**: Stochastic depth and dropout as inference stochasticity
  - **Why needed here**: Standard SFNO is deterministic; introducing dropout and stochastic depth at inference time provides the random variation necessary for ensemble climate simulations within the DYffusion framework.
  - **Quick check question**: How does MC dropout at inference time differ from training-time dropout in terms of variance introduced?

- **Concept**: Climate vs. weather forecasting separation
  - **Why needed here**: The paper shows that optimizing for short-term weather skill does not guarantee accurate long-term climate statistics; understanding this distinction is crucial for correct model evaluation.
  - **Quick check question**: Why might a model with low 5-day RMSE still produce large 10-year time-mean biases?

## Architecture Onboarding

- **Component map**: Input variables (34 prognostic + 7 forcing) -> SFNO interpolator/forecaster (time-conditioned) -> 34 prognostic outputs at 6-hour intervals -> 10-year simulation
- **Critical path**: Initial state -> forecaster (h-step) -> alternating interpolator/forecaster -> final state -> repeat autoregressively. Ensemble sampling comes from the stochastic interpolator.
- **Design tradeoffs**:
  - SFNO vs. UNet: SFNO is more efficient on spherical data and scales better, but lacks native time conditioning, requiring custom embedding modules.
  - Dropout vs. other stochasticity: Dropout is simple and integrates into SFNO, but may not capture all modes of uncertainty.
  - Horizon h=6: Balances training stability and computational cost, but may miss longer-term dependencies.
- **Failure signatures**:
  - Divergence or numerical instability over long rollouts
  - Persistent spatial biases (e.g., polar, high-altitude)
  - Underdispersive ensembles (spread/RMSE < 1)
  - Poor correlation between weather and climate metrics
- **First 3 experiments**:
  1. **Sanity check**: Run a single deterministic SFNO (ACE) forecast for 10 years; verify stability and compare time-mean RMSE to reference.
  2. **Stochastic ablation**: Train ACE-STO (SFNO with dropout at inference); compare ensemble spread and time-mean biases to deterministic ACE.
  3. **DYffusion baseline**: Train DYffusion with UNet interpolator/forecaster; compare time-mean biases and ensemble spread to Spherical DYffusion.

## Open Questions the Paper Calls Out
- **Question**: Can the spherical diffusion model be extended to directly handle output-only diagnostic variables like precipitation?
- **Basis in paper**: [explicit] "Additionally, we have ignored output-only variables such as precipitation. Training separate prediction heads for these variables or extending the DYffusion framework to support them is an interesting direction for future work."
- **Why unresolved**: The paper explicitly states this as a limitation and future work direction, noting that while diagnostic variables are not included in the current model, handling them is important for a complete Earth System Model.
- **What evidence would resolve it**: Demonstration of the spherical diffusion model successfully incorporating and predicting diagnostic variables like precipitation, showing comparable performance to physics-based models for these outputs.

## Limitations
- The 1° latitude-longitude grid limits representation of small-scale processes (e.g., convective systems, orographic effects) despite the spherical Fourier basis helping with global coverage
- The use of dropout and stochastic depth for ensemble generation may not capture all sources of uncertainty in climate systems, as it assumes learned features' stochasticity approximates true uncertainty distribution
- The model is trained on historical climate variability, raising questions about performance under unprecedented forcing scenarios (e.g., extreme greenhouse gas concentrations) where diffusion framework may struggle

## Confidence
- **Long-term stability claim** (100-year simulations): High confidence. The paper demonstrates stable rollouts with continuous evaluation and shows consistent performance across the full 10-year period.
- **Climate bias reduction claim** (50% of reference, 2-4× better than baselines): Medium confidence. While results are compelling, the comparison relies on a single reference model (FV3GFS), and the definition of "climate bias" could vary with different reference periods or metrics.
- **Ensemble skill claim** (spread-skill ratio ~1, consistent variability): Medium confidence. The ensemble metrics are promising, but the small ensemble size (25 members) may not fully characterize the true uncertainty distribution, and the results depend on the specific stochastic inference method.

## Next Checks
1. **Stress test with extreme forcing**: Run Spherical DYffusion under drastically altered boundary conditions (e.g., 4× CO2, extreme SST anomalies) and evaluate whether it maintains physical consistency and produces plausible climate states beyond the historical training distribution.
2. **Multimodel comparison**: Evaluate Spherical DYffusion against multiple GCM reference models (not just FV3GFS) to assess whether the reported bias reductions are robust across different climate system representations.
3. **Ensemble size sensitivity**: Systematically vary ensemble size (5, 25, 100 members) and measure how spread-skill ratio, CRPS, and climate bias evolve to determine whether the current 25-member ensemble is sufficient for robust uncertainty quantification.