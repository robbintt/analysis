---
ver: rpa2
title: Dynamic embedded topic models and change-point detection for exploring literary-historical
  hypotheses
arxiv_id: '2401.13905'
source_url: https://arxiv.org/abs/2401.13905
tags:
- topic
- word
- early
- words
- change-point
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper combines dynamic embedded topic models and change-point\
  \ detection to study semantic modality shifts in Latin texts from 250 BCE to 500\
  \ CE. The approach measures bimodality\u2014how words split probability mass between\
  \ two topics\u2014and tracks change-points where these splits shift most."
---

# Dynamic embedded topic models and change-point detection for exploring literary-historical hypotheses

## Quick Facts
- arXiv ID: 2401.13905
- Source URL: https://arxiv.org/abs/2401.13905
- Reference count: 4
- Primary result: Substantial decline in lexical modality shift from 200 CE, especially among Christian authors, suggesting linguistic standardization alongside religious consolidation.

## Executive Summary
This paper introduces a method combining dynamic embedded topic models (DETM) and change-point detection to study diachronic semantic modality shifts in Latin texts spanning 250 BCE to 500 CE. The approach identifies words that split probability mass between two topics and tracks when these splits change most dramatically. The main finding is a marked decline in modality shift starting around 200 CE, with Christian authors showing the least novelty, suggesting standardization of vocabulary alongside religious consolidation. The method surfaces both linguistic innovation (Apicius, Vitruvius) and conservative usage patterns (Christian writers), offering a scalable tool for literary-historical hypothesis generation.

## Method Summary
The method extracts Latin texts from the Perseus corpus, lemmatizes and groups them into 75-year windows with sub-documents of ≤500 tokens. Skip-gram word2vec embeddings (300-dim, 10 epochs) are trained on the corpus, then used to fit a 50-topic DETM with batch size 2000 and learning rate 0.016. For each word, bimodality scores are computed as the sum of top two topic probabilities and their gap, then change-point detection (L2 cost) identifies the window of maximal shift. Author novelty is measured via Jensen-Shannon divergence between topic distributions across time windows. Results are aggregated and visualized to reveal diachronic patterns.

## Key Results
- Decline in lexical modality shift beginning around 200 CE, especially pronounced among early Christian authors.
- Apicius and Vitruvius emerged as highly novel authors; Christian writers were least novel.
- Words like *spiritus* and *manus* showed dramatic modality shifts, interpreted as figurative-to-corporeal semantic change.
- Early Christian vocabulary adaptation tracked through modality deltas in theological and political terms.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dynamic embedded topic models (DETM) can capture temporal semantic modality shifts by modeling topic evolution across time windows.
- Mechanism: DETM combines topic modeling with word embeddings, allowing each word's probability distribution over topics to change smoothly across windows, which encodes semantic modality shifts.
- Core assumption: Topic assignments evolve gradually enough that piecewise-linear or smooth interpolation is valid across 75-year windows.
- Evidence anchors:
  - [abstract]: "We present a novel combination of dynamic embedded topic models and change-point detection to explore diachronic change of lexical semantic modality"
  - [section 2]: "The dynamic embedded topic model (DETM) extends topic models (Blei et al., 2003) to operate over word embeddings, and capture topic evolution over time."
- Break condition: If semantic shifts occur discontinuously within a window or if topic granularity is too coarse, modality detection will fail or be misattributed.

### Mechanism 2
- Claim: Bimodality scores derived from topic distributions identify words undergoing modality shifts.
- Mechanism: For each word in each window, bimodality is computed as the sum of (1 - gap between top two topic probabilities) and the sum of those top probabilities, peaking when a word is evenly split between two topics.
- Core assumption: Sharp topic splits reflect meaningful semantic shifts, not noise or overfitting.
- Evidence anchors:
  - [section 3.1]: "We define a word's bimodality, within a particular window, as the degree to which its probability mass is evenly and exhaustively between two topics."
  - [section 4]: "Figure 1 places snapshots of two topics side-by-side... The word manus (hand) moves from the former to the latter, which we interpret as a shift from a figurative to corporeal sense."
- Break condition: If topic distributions are noisy or overly sparse, bimodality scores will be unstable and change-points unreliable.

### Mechanism 3
- Claim: Change-point detection on bimodality time series isolates the window of maximal modality shift for each word.
- Mechanism: Dynamic programming with L2 cost finds the optimal piecewise-linear fit, identifying the change-point as the window where the mean difference across the split is maximal.
- Core assumption: Semantic modality shifts are localized in time and can be captured by a single change-point per word.
- Evidence anchors:
  - [section 3.1]: "Using the word's sequence of bimodality scores, we apply change-point detection with an L2 cost to find the window constituting the most-prominent shift in modality, which we refer to as the word's change-point."
  - [section 4]: "The five words with the highest and lowest deltas... Most of the top words are excellent examples of the early Christian church's adaptation of Classical vocabulary."
- Break condition: If shifts are gradual or multimodal, single change-points will misrepresent the dynamics.

## Foundational Learning

- Topic Modeling and Word Embeddings
  - Why needed here: The DETM merges these to allow semantic modality shifts to be modeled continuously over time.
  - Quick check question: What does it mean for a word to have a "sharp transition between having one or two senses" in topic space?

- Jensen-Shannon Divergence for Novelty
  - Why needed here: Author novelty is measured as divergence from the previous time window's topic distribution, enabling tracking of linguistic innovation.
  - Quick check question: How does JSD between topic distributions differ from simple frequency change?

- Change-Point Detection Theory
  - Why needed here: Identifies when and where bimodality shifts occur, isolating meaningful semantic events.
  - Quick check question: Why is L2 cost appropriate for detecting modality shifts versus other cost functions?

## Architecture Onboarding

- Component map:
  Corpus -> Lemmatize -> Group into windows -> Train embeddings -> Fit DETM -> Compute bimodality -> Detect change-points -> Aggregate deltas/novelty -> Interpret

- Critical path:
  Corpus → Lemmatize → Group into windows → Train embeddings → Fit DETM → Compute bimodality → Detect change-points → Aggregate deltas/novelty → Interpret

- Design tradeoffs:
  - Fixed 75-year windows vs. variable window size: balances granularity vs. data sparsity.
  - 50 topics: enough to capture nuance but avoid overfitting given corpus size.
  - L2 change-point cost: assumes smooth shifts; alternative costs might capture abrupt changes better.

- Failure signatures:
  - Bimodality scores flat across all words: likely topic collapse or insufficient embedding quality.
  - All change-points clustered in one window: may indicate corpus bias or modeling artifact.
  - Novelty scores all near zero: topic distributions too similar across time, possibly due to window size.

- First 3 experiments:
  1. Vary window size (50, 75, 100 years) and re-run DETM to see impact on bimodality stability.
  2. Replace L2 cost with binary segmentation change-point method to test robustness of identified shifts.
  3. Train embeddings with different parameters (window size 3 vs 5, 200 vs 300 dim) and compare downstream modality patterns.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust are change-points in low-data or highly variable periods?
- Basis in paper: [explicit] Authors note that *cathedra*’s 175 BCE change-point may be an artifact of high variance and low attestation.
- Why unresolved: The model currently lacks an explicit measure of change-point robustness; sparse data could yield spurious shifts.
- What evidence would resolve it: Integrate uncertainty estimates (e.g., bootstrapped variance in bimodality scores) and test sensitivity across subsampled corpora.

### Open Question 2
- Question: Does the observed decline in lexical modality shift reflect genuine linguistic standardization or simply stylistic homogeneity among Christian writers?
- Basis in paper: [explicit] Authors contrast Christian writers’ low novelty with non-Christian outliers but acknowledge this might reflect genre or register effects.
- Why unresolved: Novelty scores conflate semantic novelty with stylistic or doctrinal convergence; the pipeline cannot distinguish these.
- What evidence would resolve it: Compare modality shifts in parallel Christian vs. secular genres (e.g., sermons vs. technical treatises) or model genre as a covariate.

### Open Question 3
- Question: Can the method reliably detect higher-order modality (more than two senses) without conflating them into a single bimodal signal?
- Basis in paper: [inferred] The authors limit analysis to bimodality for simplicity, but note this leaves higher complexity to future work.
- Why unresolved: Current bimodality metric collapses multi-topic distributions into a pairwise score, obscuring richer semantic shifts.
- What evidence would resolve it: Extend the scoring to capture top-k topic splits and validate against curated multi-sense lexicons or semantic network annotations.

## Limitations
- Temporal granularity fixed at 75-year windows; sensitivity to window size not explored.
- Manual curation of Perseus corpus metadata limits generalizability to noisier corpora.
- Novelty scores conflate linguistic innovation with genre or register effects.

## Confidence

**High confidence** in the technical implementation of DETM and change-point detection methodology. The mathematical framework is sound, and the hyperparameters are explicitly specified.

**Medium confidence** in the interpretation of bimodality as semantic modality shift. While the mechanism is theoretically justified, the mapping between topic distribution splits and semantic change requires validation through linguistic expert review.

**Low confidence** in the generalizability to less-curated corpora. The paper acknowledges this as future work, and the current results are contingent on the quality of manual dating and author attribution in the Perseus corpus.

## Next Checks

1. **Window size sensitivity analysis**: Systematically vary the temporal window (50, 75, 100 years) and compare resulting bimodality scores, change-point locations, and novelty patterns. This would reveal whether the 75-year choice is optimal or arbitrary.

2. **Linguistic validation study**: Have classical scholars manually annotate a subset of high-delta words (like "spiritus" and "manus") to confirm whether the detected modality shifts correspond to actual semantic changes documented in historical linguistics literature.

3. **Noise injection experiment**: Deliberately corrupt a subset of the metadata (randomize author assignments or document dates) and rerun the analysis to quantify how sensitive the results are to curation quality. This would directly test the method's robustness to less-curated data.