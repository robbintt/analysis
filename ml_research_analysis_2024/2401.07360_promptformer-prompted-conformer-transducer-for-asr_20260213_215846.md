---
ver: rpa2
title: 'Promptformer: Prompted Conformer Transducer for ASR'
arxiv_id: '2401.07360'
source_url: https://arxiv.org/abs/2401.07360
tags:
- context
- bert
- encoder
- test
- rwerr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Promptformer, a method to integrate contextual
  information into conformer transducer ASR models using hyper-prompting-inspired
  techniques. The approach fuses textual context with acoustic representations within
  the attention mechanism, achieving 5.9% relative word error rate reduction (rWERR)
  over a strong baseline on multi-turn interaction test sets.
---

# Promptformer: Prompted Conformer Transducer for ASR

## Quick Facts
- arXiv ID: 2401.07360
- Source URL: https://arxiv.org/abs/2401.07360
- Reference count: 0
- Primary result: 5.9% relative WER reduction over strong baseline on multi-turn interactions

## Executive Summary
Promptformer introduces a hyper-prompting-inspired approach to integrate contextual information into conformer transducer ASR models. The method fuses textual context with acoustic representations within the attention mechanism using shared kernels, achieving significant performance improvements on multi-turn interaction test sets. By leveraging internal sentence-piece embeddings instead of external BERT models, the approach reduces computational overhead while maintaining effectiveness. The system demonstrates robustness by preserving performance even when context is absent, with consistent improvements across test sets.

## Method Summary
The Promptformer method fine-tunes a 91.7M parameter RNN-T conformer transducer model using Adam optimizer for 250K steps. Context is generated via sentence-piece embeddings and integrated into the multi-head self-attention mechanism by concatenating textual and acoustic representations in the keys and values, with shared kernels forcing multimodal fusion. The approach uses a context window of 40 tokens and maintains streaming capability through causal masking. Evaluation is performed on proprietary voice assistant datasets with average traffic (29.1K utterances, 71% with context) and multi-turn interaction (7K utterances, all with context) test sets.

## Key Results
- 5.9% relative WER reduction on multi-turn interaction test sets
- 1.6% relative WER reduction on average traffic test sets
- Maintains performance when context is absent
- Comparable performance with reduced parameter fine-tuning (only MHSA and projections)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Sharing acoustic and textual kernels in the MHA forces fused multimodal representations
- Mechanism: Concatenating context embeddings to acoustic features and projecting with shared kernels creates joint embeddings that balance acoustic and textual context in the softmax
- Core assumption: A single set of kernels can effectively align temporal acoustic features with textual context vectors
- Evidence anchors: Weak evidence from abstract and section 2.3; no direct comparisons with unshared kernels
- Break condition: If acoustic and textual modalities are too dissimilar, shared kernels may fail to produce meaningful fusion

### Mechanism 2
- Claim: Prompt-based context consumption improves over cross-attention methods
- Mechanism: Direct integration of context into attention computation without separate cross-attention modules reduces complexity and parameters
- Core assumption: Attention mechanism can effectively integrate context without dedicated cross-attention kernels
- Evidence anchors: Abstract comparison to cross-attention methods; section 2.3 draws inspiration from hyper-prompting
- Break condition: If context relevance varies significantly across time, separate cross-attention might better learn dynamic weighting

### Mechanism 3
- Claim: Internal sentence-piece embeddings outperform external BERT representations
- Mechanism: SPM embeddings trained on same vocabulary as ASR model avoid mismatch and reduce external inference overhead
- Core assumption: Vocabulary alignment is more important than general language understanding for context integration
- Evidence anchors: Abstract claim and section 4.2 finding BERT marginally outperforms SPM (+0.4% rWERR)
- Break condition: If ASR vocabulary is limited, external BERT might capture more relevant semantic information

## Foundational Learning

- Concept: Multi-Head Self-Attention (MHSA) mechanism
  - Why needed here: Conformer transducer uses MHSA blocks, and understanding context integration is crucial
  - Quick check question: How does temporal masking in streaming conformer affect attention computation?

- Concept: Conformer architecture
  - Why needed here: Understanding interplay between convolution layers, self-attention, and feed-forward layers is essential
  - Quick check question: What is the role of convolution module in conformer block and how does it complement self-attention?

- Concept: RNN-Transducer (RNN-T) loss and architecture
  - Why needed here: Model is conformer transducer, so understanding three components and their interaction is important
  - Quick check question: How does prediction network differ from encoder in attention-based encoder-decoder model?

## Architecture Onboarding

- Component map: Audio input → conformer encoder (with context integration) → prediction network → joint network → softmax output
- Critical path: Audio → conformer encoder → prediction network → joint network → output
- Design tradeoffs:
  - Shared vs separate kernels for acoustic and textual representations
  - Internal SPM vs external BERT for context encoding
  - Full fine-tuning vs partial fine-tuning (only MHSA and projections)
- Failure signatures:
  - Context integration degrades performance on utterances without context
  - Minimal improvement over baseline when context is present
  - Significant increase in computational overhead
- First 3 experiments:
  1. Implement prompting approach with internal SPM embeddings and compare against baseline without context
  2. Compare shared kernel prompting vs separate cross-attention modules for context integration
  3. Test partial fine-tuning (only MHSA and projections) vs full fine-tuning to assess parameter efficiency

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several areas remain unexplored based on the experimental scope and methodology.

## Limitations
- Performance improvements demonstrated only on proprietary English voice assistant dataset
- Limited ablation studies on critical design choices like kernel sharing and context window size
- No comparison with alternative context integration methods beyond brief mention of cross-attention
- Claims about mechanism effectiveness lack rigorous theoretical justification

## Confidence
- High Confidence: Baseline conformer transducer architecture and training procedure are well-specified and implementable
- Medium Confidence: Performance improvements are supported by experimental results but limited by proprietary dataset
- Low Confidence: Claims about why mechanisms work (shared kernels, internal vs external encoders) lack rigorous ablation studies or theoretical justification

## Next Checks
1. **Ablation Study on Kernel Sharing**: Implement prompting approach with separate kernels for acoustic and textual representations and compare performance against shared kernel baseline to validate kernel sharing's contribution.

2. **External Dataset Validation**: Evaluate prompting approach on publicly available multi-turn ASR dataset (MultiWOZ or similar) to assess generalization beyond proprietary voice assistant data.

3. **Context Quality Impact Analysis**: Systematically vary context quality (ground truth vs ASR-recognized vs corrupted) to determine robustness and identify failure modes when context is unreliable or absent.