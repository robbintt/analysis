---
ver: rpa2
title: The Privacy Power of Correlated Noise in Decentralized Learning
arxiv_id: '2405.01031'
source_url: https://arxiv.org/abs/2405.01031
tags:
- privacy
- noise
- learning
- users
- correlated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Decor, a decentralized SGD variant that matches
  the central differential privacy (DP) optimal privacy-utility trade-off under a
  new relaxation of local DP called secret-based local DP (SecLDP). Decor achieves
  this by securely exchanging randomness seeds to generate pairwise-canceling correlated
  Gaussian noises, which are injected to protect local models at every communication
  round.
---

# The Privacy Power of Correlated Noise in Decentralized Learning

## Quick Facts
- arXiv ID: 2405.01031
- Source URL: https://arxiv.org/abs/2405.01031
- Reference count: 40
- This paper presents Decor, a decentralized SGD variant that matches the central differential privacy (DP) optimal privacy-utility trade-off under a new relaxation of local DP called secret-based local DP (SecLDP).

## Executive Summary
This paper introduces Decor, a novel decentralized stochastic gradient descent (SGD) algorithm that achieves the same privacy-utility trade-off as central differential privacy while operating under the more stringent local privacy model. The key innovation is the use of correlated Gaussian noise generated through secure exchange of randomness seeds, which allows for better privacy protection while maintaining convergence. Decor theoretically and empirically demonstrates that for arbitrary connected graphs, it matches the central DP optimal privacy-utility trade-off under SecLDP, a new relaxation of local DP.

## Method Summary
Decor is a decentralized SGD variant that achieves strong privacy guarantees by injecting correlated Gaussian noise into the learning process. The algorithm works by having users securely exchange randomness seeds, which are then used to generate pairwise-canceling correlated noises. These noises are added to local models during each communication round to protect privacy. The method involves local gradient computation with clipping, addition of both correlated and uncorrelated Gaussian noise, and gossip averaging over a communication graph. The main theoretical challenge addressed is controlling the accumulation of non-canceling correlated noise due to network sparsity, which the authors tackle through careful noise calibration and privacy accounting.

## Key Results
- Decor achieves the same privacy-utility trade-off as central differential privacy under SecLDP
- The algorithm matches CDP baseline performance while surpassing LDP baseline on various tasks
- Theoretical analysis proves privacy guarantees hold for arbitrary connected graphs

## Why This Works (Mechanism)
Decor works by exploiting the structure of correlated noise to achieve better privacy-utility trade-offs than traditional LDP approaches. By securely exchanging randomness seeds between neighboring nodes, the algorithm generates noise patterns that partially cancel out during the gossip averaging process. This cancellation effect reduces the overall noise impact on the learning process while maintaining strong privacy guarantees. The key insight is that this correlated noise structure allows for tighter privacy accounting compared to independent noise addition, effectively bridging the gap between local and central DP models.

## Foundational Learning
- **Secret-based Local Differential Privacy (SecLDP)**: A relaxation of LDP that allows for correlated noise generation while maintaining privacy guarantees. Why needed: To bridge the gap between the strong guarantees of LDP and the better utility of CDP. Quick check: Verify that SecLDP provides stronger guarantees than vanilla LDP but weaker than CDP in the privacy-utility trade-off.
- **Correlated Gaussian Noise Injection**: The process of generating and injecting noise that exhibits specific correlation patterns across nodes. Why needed: To enable noise cancellation during gossip averaging, reducing the total noise impact. Quick check: Confirm that noise terms from neighboring nodes cancel out appropriately during the averaging step.
- **Gossip Averaging**: A decentralized communication protocol where nodes iteratively average their values with neighbors. Why needed: To enable information propagation across the network without a central coordinator. Quick check: Verify that the mixing matrix W is properly normalized and that values converge over iterations.
- **Privacy Accounting for Correlated Noise**: The mathematical framework for tracking cumulative privacy loss when using correlated noise mechanisms. Why needed: To provide rigorous (ε, δ)-SecLDP guarantees for the entire learning process. Quick check: Ensure the privacy accountant correctly accumulates privacy budgets across all iterations and noise injections.

## Architecture Onboarding

**Component Map**
Users -> Gossip Network -> Secure Seed Exchange -> Correlated Noise Generator -> Local Model Updates -> Privacy Accountant

**Critical Path**
1. Secure seed exchange between neighbors
2. Correlated noise generation using exchanged seeds
3. Local gradient computation with clipping
4. Noise injection (correlated + uncorrelated)
5. Gossip averaging with mixing matrix W
6. Privacy accounting for each iteration

**Design Tradeoffs**
- Security vs. Communication Overhead: Secure seed exchange requires additional communication but enables better noise correlation
- Noise Correlation Strength vs. Privacy: Stronger correlations improve utility but require more careful privacy accounting
- Graph Sparsity vs. Convergence: Sparse graphs reduce communication but may slow convergence and affect noise cancellation

**Failure Signatures**
- Poor privacy-utility trade-off: Indicates incorrect noise correlation implementation or miscalibrated privacy parameters
- Non-convergence or oscillations: Suggests issues with the gossip averaging process or inappropriate learning rates
- Excessive variance in results: Points to problems with noise generation or seed exchange mechanism

**3 First Experiments**
1. Implement Decor on a simple ring topology with n=4 users and verify noise cancellation during gossip averaging
2. Test privacy accounting by running Decor for varying numbers of iterations and checking accumulated (ε, δ) values
3. Compare Decor's performance against vanilla LDP on a simple convex task to demonstrate utility improvements

## Open Questions the Paper Calls Out
- How does Decor's performance compare to other state-of-the-art decentralized differentially private algorithms on non-convex tasks?
- How does Decor's performance scale with the number of users and the size of the network graph?
- How does the choice of mixing matrix W affect Decor's performance?

## Limitations
- Theoretical privacy analysis relies on simplifying assumptions about graph connectivity that may not hold in practice
- Scalability to large graphs (n > 100) remains an open question with potential performance degradation
- The correlated noise mechanism's robustness under adversarial conditions has not been thoroughly tested

## Confidence
- Theoretical privacy guarantees: High - The SecLDP framework and privacy accounting appear sound, with rigorous proofs provided
- Privacy-utility trade-off claims: Medium - While experiments demonstrate strong performance on small graphs, broader empirical validation is needed
- Implementation feasibility: Medium - The algorithm is clearly specified, but some implementation details are left to the reader

## Next Checks
1. Implement Decor on a larger graph (n > 100) and measure both privacy guarantees and utility degradation
2. Test the correlated noise mechanism under adversarial scenarios where users attempt to violate privacy guarantees
3. Compare Decor's performance against state-of-the-art decentralized learning methods that don't use privacy-preserving techniques