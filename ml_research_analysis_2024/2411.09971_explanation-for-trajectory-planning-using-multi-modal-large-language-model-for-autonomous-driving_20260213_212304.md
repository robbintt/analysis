---
ver: rpa2
title: Explanation for Trajectory Planning using Multi-modal Large Language Model
  for Autonomous Driving
arxiv_id: '2411.09971'
source_url: https://arxiv.org/abs/2411.09971
tags:
- image
- trajectory
- vehicle
- information
- planning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of limited interpretability in
  end-to-end autonomous driving systems, which can cause passenger anxiety due to
  the lack of understanding of the vehicle's decision-making process. The authors
  propose a novel method that combines visual information from a front camera with
  future trajectory planning information of the ego vehicle using a cross-attention
  mechanism.
---

# Explanation for Trajectory Planning using Multi-modal Large Language Model for Autonomous Driving

## Quick Facts
- arXiv ID: 2411.09971
- Source URL: https://arxiv.org/abs/2411.09971
- Reference count: 26
- Key outcome: Cross-attention with camera image features as queries and trajectory planning features as keys/values achieves BLEU-4 scores of 0.416 (whole), 0.483 (action), and 0.340 (justification) for generating captions describing ego vehicle future behaviors

## Executive Summary
This paper addresses the interpretability problem in end-to-end autonomous driving systems by generating captions that describe and justify future ego vehicle behaviors. The authors propose a multi-modal large language model approach that combines visual information from front cameras with trajectory planning data using a cross-attention mechanism. The method demonstrates improved performance over baseline models, achieving higher BLEU-4 and ROUGE-L scores for both action and justification captions. The cross-attention architecture, where camera image features query trajectory planning features, shows the best results, highlighting the importance of feature selection in the attention mechanism.

## Method Summary
The proposed method uses a BLIP-2 based vision-language model with three image-trajectory encoder architectures: concatenated, overlaid, and cross-attention. The system takes front camera images and trajectory planning information (future trajectory, road boundaries, lane lines) as inputs, transforms trajectory coordinates to image space using perspective projection, and generates English captions describing ego vehicle actions and justifications. The model freezes image encoders, Q-Formers, and LLM decoders during training, with the Q-Former and language projection modules also having frozen parameters. Training is performed for approximately 10 epochs on a newly collected dataset of ~120 minutes of urban driving in Japan.

## Key Results
- Cross-attention model with camera image features as queries and trajectory planning features as keys/values achieves best performance (BLEU-4: 0.416 for whole, 0.483 for action, 0.340 for justification)
- Adding trajectory planning information improves caption accuracy compared to camera-only baseline models across all proposed architectures
- Cross-attention architecture outperforms both concatenated and overlaid approaches for generating accurate future behavior captions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-attention with camera image features as queries and trajectory planning features as keys/values produces superior fused features for generating accurate future behavior captions.
- Mechanism: The cross-attention layers enable interaction between visual and trajectory features, where camera image features query the trajectory features to extract relevant future planning information, resulting in more accurate caption generation.
- Core assumption: Trajectory features contain sufficient information about future ego vehicle behavior that can be effectively extracted through cross-attention when queried by camera features.
- Evidence anchors:
  - [abstract]: "The cross-attention model with camera image features as queries and trajectory planning features as keys and values shows the best results, highlighting the importance of the feature selection in the cross-attention mechanism."
  - [section]: "The cross-attention model solves the above problem and achieves the best scores in both action and justification captions."
- Break condition: If trajectory planning information becomes noisy or irrelevant to the visual context, the cross-attention mechanism may amplify incorrect associations.

### Mechanism 2
- Claim: Adding future trajectory planning information to model inputs significantly improves caption accuracy compared to using only visual information.
- Mechanism: Future trajectory planning provides explicit information about intended ego vehicle behavior, allowing the model to generate more accurate predictions about future actions and their justifications.
- Core assumption: Future trajectory planning information is predictive of actual future behavior and contains relevant context for caption generation.
- Evidence anchors:
  - [abstract]: "The proposed method demonstrates improved performance compared to baseline models, achieving higher BLEU-4 and ROUGE-L scores for both action and justification captions."
  - [section]: "Both BLEU-4 and ROUGE-L are improved in all of the proposed models compared to the baseline, that implies a positive effect of adding trajectory planning information to the model inputs."
- Break condition: If trajectory planning information is inaccurate or does not match actual future behavior, the captions may become misleading.

### Mechanism 3
- Claim: Spatial fusion of visual information and trajectory planning information through coordinate transformation enables more accurate caption generation.
- Mechanism: Converting trajectory planning coordinates to image space allows direct spatial alignment with visual features, enabling the model to understand the relationship between planned trajectory and visual scene.
- Core assumption: The coordinate transformation preserves the spatial relationships necessary for accurate caption generation.
- Evidence anchors:
  - [section]: "In our proposed pipeline, it is transformed from Cartesian coordinates to image space coordinates by using perspective projection transformation after translation and rotation operations."
  - [section]: "The points on the image space coordinate system are connected and lines are drawn as a trajectory image, which has a same angle of view as a front camera image."
- Break condition: If the coordinate transformation introduces distortions or misalignment, the spatial fusion may produce incorrect associations.

## Foundational Learning

- Concept: Cross-attention mechanism
  - Why needed here: Enables interaction between visual features and trajectory planning features to produce fused representations that capture both current visual context and future intentions.
  - Quick check question: How does the cross-attention mechanism differ from simple concatenation of features?

- Concept: Trajectory planning representation
  - Why needed here: Provides explicit information about future ego vehicle behavior that can be used to generate accurate captions about future actions.
  - Quick check question: What are the key components of the trajectory planning information used in this system?

- Concept: Coordinate transformation for spatial alignment
  - Why needed here: Converts trajectory planning coordinates to image space to enable direct spatial fusion with visual features.
  - Quick check question: Why is it important to transform trajectory coordinates to the same coordinate system as the camera image?

## Architecture Onboarding

- Component map: Camera image → Image encoder → Cross-attention (with trajectory features) → Q-Former → Language projection → LLM decoder → Caption output

- Critical path: Camera image → Image encoder → Cross-attention (with trajectory features) → Q-Former → Language projection → LLM decoder → Caption output

- Design tradeoffs:
  - Cross-attention vs. concatenation: Cross-attention provides better feature interaction but adds computational complexity
  - Coordinate transformation precision vs. computational cost: More accurate transformations require more computation
  - Model freezing vs. fine-tuning: Freezing reduces training time but may limit adaptation to specific domain

- Failure signatures:
  - Inaccurate captions when trajectory planning information is noisy or missing
  - Poor spatial alignment between visual and trajectory features leading to incorrect associations
  - Overfitting to training data when trajectory patterns are too specific

- First 3 experiments:
  1. Compare baseline (camera only) vs. concatenated model to verify improvement from adding trajectory features
  2. Test cross-attention with different query/key/value assignments to optimize feature fusion
  3. Evaluate ablation study by removing coordinate transformation to assess impact on spatial alignment

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the cross-attention model vary with different types of trajectory planning information (e.g., only future trajectory, only road boundaries, only lane lines, or combinations thereof)?
- Basis in paper: [explicit] The paper mentions that trajectory planning information includes a future trajectory, road boundaries, and lane lines, and evaluates a model that uses all three. However, it does not explore the impact of using different combinations of this information.
- Why unresolved: The paper does not provide an ablation study that isolates the contribution of each type of trajectory planning information to the model's performance.
- What evidence would resolve it: Conducting experiments with models trained on different combinations of trajectory planning information (e.g., only future trajectory, only road boundaries, only lane lines, future trajectory + road boundaries, future trajectory + lane lines, road boundaries + lane lines) and comparing their performance using BLEU-4 and ROUGE-L scores.

### Open Question 2
- Question: How does the performance of the proposed method compare to other methods that generate explanations for autonomous driving actions, such as DriveGPT4, when using the same dataset?
- Basis in paper: [inferred] The paper compares the proposed method to a baseline model based on BLIP-2, but does not compare it to other state-of-the-art methods for generating explanations in autonomous driving, such as DriveGPT4.
- Why unresolved: The paper does not provide a direct comparison with other methods that generate explanations for autonomous driving actions.
- What evidence would resolve it: Conducting experiments that compare the performance of the proposed method to other state-of-the-art methods for generating explanations in autonomous driving, such as DriveGPT4, using the same dataset and evaluation metrics (BLEU-4 and ROUGE-L).

### Open Question 3
- Question: How does the performance of the proposed method vary with different types of front camera images (e.g., images with different lighting conditions, weather conditions, or road types)?
- Basis in paper: [inferred] The paper does not explicitly discuss the impact of different types of front camera images on the performance of the proposed method.
- Why unresolved: The paper does not provide an analysis of the model's performance under different environmental conditions or with different types of road scenes.
- What evidence would resolve it: Conducting experiments that evaluate the performance of the proposed method on datasets with front camera images captured under different lighting conditions (e.g., day, night, dawn, dusk), weather conditions (e.g., sunny, rainy, snowy, foggy), and road types (e.g., highways, urban roads, rural roads).

## Limitations

- Dataset scope: Evaluation based on single dataset from urban driving in Japan using specific hardware (comma 3X), limiting generalizability
- Lack of comparative analysis: No benchmarking against existing approaches for trajectory-based explanation generation in autonomous driving
- Model dependency: Use of frozen pretrained components may limit adaptation to specific driving scenarios

## Confidence

**High Confidence:**
- The cross-attention model with camera image features as queries and trajectory planning features as keys/values achieves the best performance among the three proposed architectures (BLEU-4: 0.416 for whole, 0.483 for action, 0.340 for justification).
- Adding trajectory planning information to model inputs improves caption accuracy compared to using only visual information, as evidenced by higher BLEU-4 and ROUGE-L scores across all proposed models.

**Medium Confidence:**
- The claim that the cross-attention mechanism specifically benefits from using camera image features as queries rather than trajectory features is supported by ablation studies but lacks direct comparison with alternative attention mechanisms.
- The assertion that the model generates more accurate future behavior captions due to better feature interaction through cross-attention is plausible but not explicitly validated through human evaluation or qualitative analysis.

**Low Confidence:**
- The claim that the model can accurately predict future behaviors based on current visual context and trajectory planning is not directly validated, as the evaluation focuses on caption generation quality rather than predictive accuracy of future behaviors.

## Next Checks

1. **Cross-Environment Validation**: Test the proposed model on datasets from different driving environments (e.g., highways, rural areas) and different geographical locations to assess generalizability and robustness to varying driving conditions.

2. **Human Evaluation Study**: Conduct a human evaluation study where participants assess the accuracy and usefulness of the generated captions in describing and justifying future ego vehicle behaviors, comparing the proposed method against baseline approaches.

3. **Alternative Attention Mechanism Comparison**: Implement and evaluate alternative attention mechanisms (e.g., self-attention, co-attention) to determine whether the cross-attention approach provides a significant advantage over other feature fusion methods for this specific task.