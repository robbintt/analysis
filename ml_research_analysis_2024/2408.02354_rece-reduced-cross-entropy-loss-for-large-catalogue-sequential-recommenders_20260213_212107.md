---
ver: rpa2
title: 'RECE: Reduced Cross-Entropy Loss for Large-Catalogue Sequential Recommenders'
arxiv_id: '2408.02354'
source_url: https://arxiv.org/abs/2408.02354
tags:
- memory
- loss
- rece
- ndcg
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RECE introduces a GPU-efficient approximation of full Cross-Entropy
  loss for large-scale sequential recommenders. It leverages locality-sensitive hashing
  to approximate softmax over hard negatives, reducing memory consumption while maintaining
  or exceeding SOTA performance.
---

# RECE: Reduced Cross-Entropy Loss for Large-Catalogue Sequential Recommenders

## Quick Facts
- **arXiv ID:** 2408.02354
- **Source URL:** https://arxiv.org/abs/2408.02354
- **Reference count:** 40
- **Primary result:** GPU-efficient approximation of full Cross-Entropy loss reducing memory usage by up to 12× while maintaining or exceeding SOTA performance

## Executive Summary
RECE introduces a memory-efficient approximation of full Cross-Entropy loss for large-scale sequential recommenders. The method uses locality-sensitive hashing (LSH) with angular distance to cluster similar items and transformer outputs, then computes logits only within and between nearby clusters. This selective computation strategy prioritizes the most informative elements while avoiding the full C x (s·l) logit matrix. On four datasets, RECE achieves up to 8.19% improvement in NDCG@10 with the same memory budget, or equivalent quality with 6.6× less memory compared to existing methods.

## Method Summary
RECE reduces memory consumption in large-catalogue sequential recommenders by selectively computing logits only for items likely to have high gradient magnitudes. The method leverages locality-sensitive hashing (LSH) with angular distance to cluster similar items and transformer outputs, then computes logits only within each cluster and with adjacent clusters. After sorting by LSH bucket index, items are divided into equal-sized chunks, and logits are computed within each chunk and with neighboring chunks. The procedure can be repeated over multiple rounds with fresh random vectors for bucketing, enriching the set of negative samples without blowing up memory. This approach maintains or exceeds SOTA performance while cutting training peak memory usage by up to 12× compared to existing methods.

## Key Results
- Achieves up to 8.19% improvement in NDCG@10 with the same memory budget
- Reduces peak memory usage by up to 12× compared to existing methods
- Maintains SOTA performance while using 6.6× less memory
- Demonstrated effectiveness across four real-world datasets (BeerAdvocate, Behance, Amazon Kindle Store, Gowalla)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: RECE reduces memory by selectively computing logits only for items likely to have high gradient magnitudes, which are the most informative for learning.
- Mechanism: Uses locality-sensitive hashing (LSH) with angular distance to cluster similar items and transformer outputs, then computes logits only within and between nearby clusters, avoiding the full C x (s·l) logit matrix.
- Core assumption: Items close in embedding space are more likely to cause misclassifications, so focusing on them approximates the full softmax well enough.
- Evidence anchors:
  - [abstract] "uses a selective computation strategy to prioritize the most informative elements from input sequences and item catalog"
  - [section] "We aim to compute only logits with the largest absolute gradient values to preserve the most information"
- Break condition: If item embeddings are poorly clustered by angular LSH, or if important negatives fall outside cluster neighborhoods, the approximation will miss critical gradient updates and hurt performance.

### Mechanism 2
- Claim: Chunking and adjacent-chunk lookup in RECE preserve locality while keeping GPU memory bounded, ensuring that most hard negatives are still captured.
- Mechanism: After sorting by LSH bucket index, items are divided into equal-sized chunks. Logits are computed within each chunk and with adjacent chunks, capturing nearby items likely to be competitive negatives.
- Core assumption: Sorting by LSH index groups similar items, so true negatives with large logits will appear in the same or adjacent chunks.
- Evidence anchors:
  - [section] "we sort elements based on new indices and divide them into equal-sized chunks... For logit calculation, we also select items from the neighboring chunks"
  - [corpus] No direct neighbor evidence; assumption from algorithmic design.
- Break condition: If chunk size is too small relative to batch size or catalog size, hard negatives may be split across non-adjacent chunks and missed.

### Mechanism 3
- Claim: Repeating the bucketing process over multiple rounds enriches the set of negative samples without blowing up memory.
- Mechanism: Each round uses a fresh set of random vectors for LSH bucketing, so different partitions of the catalog are explored; the union of negatives over rounds approximates the full softmax more closely.
- Core assumption: Different random LSH projections capture different aspects of similarity, so repeated sampling increases the chance of hitting informative negatives.
- Evidence anchors:
  - [section] "the described procedure (Lines 2-12) can be repeated in parallel over several rounds... the value of loss function is calculated over an enriched set of negative examples"
  - [corpus] No direct neighbor evidence; inferred from algorithmic description.
- Break condition: If r is too small, the enriched set remains sparse; if too large, memory savings diminish and computation time increases.

## Foundational Learning

- **Concept:** Locality-sensitive hashing for angular distance
  - Why needed here: LSH allows fast GPU-friendly approximate nearest neighbor search in high-dimensional embedding space, enabling the selective logit computation that underlies RECE.
  - Quick check question: If two item embeddings have a small angle between them, will they be likely to fall into the same LSH bucket?

- **Concept:** Softmax gradient magnitude and its relation to classification difficulty
  - Why needed here: RECE targets items with large |gradient| because those correspond to hard negatives that most influence the update direction.
  - Quick check question: What is the range of the gradient of CE loss with respect to a logit, and which logits have the largest magnitude?

- **Concept:** Chunk-based GPU parallelism
  - Why needed here: By chunking sorted items, RECE ensures that all logit calculations for a chunk fit into GPU memory and can be processed in parallel.
  - Quick check question: Why does sorting by LSH index before chunking help ensure locality in the computed logits?

## Architecture Onboarding

- **Component map:** Input transformer outputs (X) and catalog embeddings (Y) → LSH bucketing → sorting → chunking → per-chunk logit computation → loss aggregation. The critical GPU ops are matrix multiplies within chunks.
- **Critical path:** Bucketing → sorting → chunking → per-chunk inner products → loss aggregation. Memory peaks during per-chunk logit computation.
- **Design tradeoffs:** Larger chunks reduce the chance of missing hard negatives but increase peak memory; more rounds increase negative diversity but add computation overhead; chunk size must balance GPU memory and recall of informative negatives.
- **Failure signatures:** Degraded NDCG@10 with no memory savings suggests hard negatives are being missed; high memory usage with good NDCG suggests chunks are too large or rounds too many.
- **First 3 experiments:**
  1. Run RECE with a single round and chunk size equal to batch size; measure memory and NDCG@10 to confirm baseline behavior.
  2. Vary chunk size (smaller) and observe impact on NDCG@10 and memory to find sweet spot.
  3. Increase rounds to 2-3 and check if NDCG@10 improves without exceeding memory budget.

## Open Questions the Paper Calls Out

- **Question 1:** How does RECE's performance scale with even larger item catalogs (e.g., millions of items) compared to the datasets tested (up to ~173k items)?
  - Basis in paper: [explicit] The paper states RECE is designed for large catalogues but only tests up to Gowalla's 173,511 items.
  - Why unresolved: The memory reduction benefits of RECE are most pronounced with larger catalogs, but the paper doesn't evaluate this extreme case.
  - What evidence would resolve it: Experimental results showing RECE's performance and memory usage on datasets with millions of items, compared to baselines.

- **Question 2:** Can RECE be effectively integrated with other transformer-based recommendation models beyond SASRec?
  - Basis in paper: [explicit] The authors mention RECE could benefit "other domains" and "other loss functions and models in sequential recommender systems."
  - Why unresolved: The paper only demonstrates RECE with SASRec, leaving uncertainty about its generalizability to other architectures.
  - What evidence would resolve it: Experimental results applying RECE to models like BERT4Rec, Caser, or GRU4Rec, showing comparable or improved performance.

- **Question 3:** What is the impact of RECE on training time when processing extremely long user sequences?
  - Basis in paper: [inferred] The paper mentions RECE is GPU-efficient but doesn't discuss its performance with very long sequences.
  - Why unresolved: While RECE reduces memory usage, its computational complexity for sorting and chunking might become problematic with very long sequences.
  - What evidence would resolve it: Benchmark results comparing training time for varying sequence lengths (e.g., 50, 100, 200 items) between RECE and full CE loss.

## Limitations

- **Implementation details:** The paper lacks complete pseudocode for the bucketing, sorting, and chunking steps, making faithful reproduction challenging.
- **Dataset filtering:** Aggressive filtering of items with <5 interactions and users with <20 interactions may remove significant portions of the item space, particularly in datasets where this is a large fraction.
- **Generalizability beyond sequential recommendation:** While the paper claims applicability to other domains with large output spaces, no empirical validation is provided for non-sequential recommendation tasks.

## Confidence

- **High Confidence:** Memory reduction claims (up to 12×) and computational efficiency improvements are well-supported by the algorithmic design and ablation studies.
- **Medium Confidence:** Performance improvements (up to 8.19% NDCG@10) are demonstrated across four datasets, but the exact contribution of each RECE component is not fully isolated.
- **Medium Confidence:** The claim that RECE maintains SOTA performance while reducing memory is supported, but the comparison baselines may not represent the absolute state of the art.

## Next Checks

1. Implement a systematic ablation study isolating the contribution of each RECE component (LSH bucketing, chunking, multiple rounds) to verify which mechanisms drive the performance and memory improvements.

2. Test RECE on a non-sequential recommendation dataset with large output space (e.g., YouTube recommendation or large vocabulary language modeling) to validate the paper's claim about generalizability beyond sequential recommendation.

3. Conduct a hyperparameter sensitivity analysis varying nb, nc, nec, and r across a grid for each dataset to establish robust guidelines for parameter selection and identify potential failure modes when parameters are poorly chosen.