---
ver: rpa2
title: Using text embedding models as text classifiers with medical data
arxiv_id: '2402.16886'
source_url: https://arxiv.org/abs/2402.16886
tags:
- data
- vector
- text
- embedding
- database
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explored using text embedding models and vector databases
  to classify medical text data without training a new model. The authors generated
  medical data using LLMs (gpt-3.5-turbo, LLaMA 2 70b-chat, and flan-t5-xl), embedded
  it with text embedding models (text-embedding-ada-002 and textembedding-gecko@001),
  and stored it in a vector database.
---

# Using text embedding models as text classifiers with medical data

## Quick Facts
- arXiv ID: 2402.16886
- Source URL: https://arxiv.org/abs/2402.16886
- Reference count: 13
- Primary result: Text embedding models can classify medical data with 3.63% misclassification rate and 0.96 macro F1 score without training a new model

## Executive Summary
This study investigates using text embedding models and vector databases as an alternative to training dedicated text classifiers for medical data. The approach involves generating synthetic medical data with large language models (LLMs), embedding it with text embedding models, and storing it in a vector database for classification tasks. The research demonstrates that this method can achieve high classification accuracy without the need to train a new model, making it potentially useful for clinicians who need to classify medical text data without interacting with an LLM.

## Method Summary
The authors generated medical data using three LLMs (gpt-3.5-turbo, LLaMA 2 70b-chat, and flan-t5-xl), embedded it with text embedding models (text-embedding-ada-002 and textembedding-gecko@001), and stored it in a vector database. They then queried the database with medical notes and compared the results to ground truth data to evaluate classification performance.

## Key Results
- Higher embedding dimensions (1536 vs 768) led to better classification performance
- Sparse query data with a detailed knowledge base yielded optimal results
- Achieved a misclassification rate as low as 3.63% and a macro F1 score of 0.96

## Why This Works (Mechanism)
The approach leverages pre-trained text embedding models to transform medical text into high-dimensional vectors that capture semantic meaning. By storing these vectors in a vector database, the system can perform similarity searches to classify new medical text based on its semantic proximity to existing labeled examples. This eliminates the need for model training while maintaining high accuracy through the quality of the embeddings.

## Foundational Learning
- Text embeddings: Vector representations of text that capture semantic meaning - needed for transforming text into a format suitable for similarity search; quick check: verify embedding dimension and quality
- Vector databases: Specialized databases optimized for storing and querying high-dimensional vectors - needed for efficient similarity search operations; quick check: confirm indexing strategy and query performance
- Semantic similarity: Measuring the relatedness between text representations based on their vector embeddings - needed for classification based on nearest neighbors; quick check: validate similarity metrics and threshold selection

## Architecture Onboarding

**Component Map**
LLM -> Text Embedding Model -> Vector Database -> Query Engine

**Critical Path**
1. Generate synthetic medical data with LLM
2. Embed data using text embedding model
3. Store embeddings in vector database
4. Query database with new medical text
5. Retrieve and classify based on nearest neighbors

**Design Tradeoffs**
- Higher embedding dimensions improve accuracy but increase storage and computational costs
- Synthetic data generation is faster than collecting real medical data but may lack real-world complexity
- Vector database approach avoids model training but requires maintaining a comprehensive knowledge base

**Failure Signatures**
- Poor classification accuracy may indicate insufficient training data diversity or inappropriate embedding model selection
- High computational costs could suggest suboptimal embedding dimensions or inefficient vector database configuration
- Inconsistent results across different medical domains might reveal domain-specific vocabulary gaps in the knowledge base

**First 3 Experiments**
1. Compare classification performance using different embedding dimensions (768 vs 1536) on the same dataset
2. Test classification accuracy using real medical records instead of synthetic data
3. Evaluate performance across multiple embedding models to identify the most effective option

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on synthetic medical data rather than real patient records, raising questions about ecological validity
- Limited comparison to only two embedding models, potentially missing better alternatives
- Does not address potential biases introduced through LLM-generated data

## Confidence

**High Confidence**
- Higher embedding dimensions lead to better classification performance
- Basic methodology of using vector databases for classification without retraining is sound

**Medium Confidence**
- Sparse query data with detailed knowledge bases yields optimal results (may not generalize)
- Reported 3.63% misclassification rate and 0.96 macro F1 score (based on synthetic data)

**Low Confidence**
- Scalability claims for broader medical applications lack empirical support
- Clinician usability without LLM interaction is theoretical, not demonstrated

## Next Checks
1. Replicate classification accuracy findings using real patient medical records from diverse healthcare settings to validate generalizability
2. Compare performance across a broader range of embedding models (e.g., Cohere, OpenAI's newer models, open-source alternatives)
3. Conduct a bias analysis to identify and quantify potential systematic errors or disparities in classification performance across different medical specialties, patient demographics, or types of clinical documentation