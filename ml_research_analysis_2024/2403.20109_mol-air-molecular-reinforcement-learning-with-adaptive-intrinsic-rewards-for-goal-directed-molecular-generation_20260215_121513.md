---
ver: rpa2
title: 'Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic Rewards
  for Goal-directed Molecular Generation'
arxiv_id: '2403.20109'
source_url: https://arxiv.org/abs/2403.20109
tags:
- molecular
- intrinsic
- rewards
- reward
- exploration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces Mol-AIR, a reinforcement learning framework
  for molecular generation that combines two types of intrinsic rewards: history-based
  (HIR) and learning-based (LIR). HIR uses exponential decay over visit counts with
  Morgan fingerprints and locality-sensitive hashing to track molecular visits, while
  LIR employs random distillation networks to calculate rewards based on prediction
  errors between target and predictor networks.'
---

# Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic Rewards for Goal-directed Molecular Generation

## Quick Facts
- arXiv ID: 2403.20109
- Source URL: https://arxiv.org/abs/2403.20109
- Reference count: 40
- Primary result: Mol-AIR achieved penalized LogP score of 15.572, QED score of 0.948, and celecoxib similarity of 0.330

## Executive Summary
Mol-AIR introduces a novel reinforcement learning framework for molecular generation that combines history-based (HIR) and learning-based (LIR) intrinsic rewards. The method uses Morgan fingerprints with locality-sensitive hashing to track molecular visits and random distillation networks to calculate prediction error-based rewards. Mol-AIR demonstrates superior performance on benchmark molecular property optimization tasks, achieving state-of-the-art results including a QED score matching theoretical optimum and discovering novel molecular structures with higher penalized LogP than previously found.

## Method Summary
Mol-AIR employs PPO with combined episodic and non-episodic advantage estimation, using SELFIES for molecular representation. The framework generates two types of intrinsic rewards: HIR using exponential decay over visit counts tracked via Morgan fingerprints and LSH, and LIR using prediction errors between randomly initialized target and predictor networks. These are combined with property-based extrinsic rewards to guide molecular generation. The actor-critic architecture includes separate critic networks for episodic and non-episodic advantages, with adaptive weighting between intrinsic reward types controlled by parameter β.

## Key Results
- Penalized LogP score of 15.572 (vs best baseline 14.740)
- QED score of 0.948 (matching theoretical optimum)
- Celecoxib similarity of 0.330 (vs best baseline 0.173)
- GSK3B score of 0.730 and JNK3 score of 0.480
- Discovered novel molecular structure with sulfur-phosphorus-nitrogen chain having higher penalized LogP than previous structures

## Why This Works (Mechanism)

### Mechanism 1
Adaptive intrinsic rewards combine HIR and LIR to overcome exploration-exploitation tradeoff. HIR provides consistent long-term exploration through exponential decay over visit counts, while LIR provides strong early-phase exploration via prediction errors from random distillation networks. The weighted combination adapts exploration behavior based on training progress.

### Mechanism 2
RND-based intrinsic rewards work through prediction error minimization between target and predictor networks. Two randomly initialized neural networks process state information. The predictor network is trained to match the target network's output. Higher prediction errors for unvisited states generate higher intrinsic rewards, encouraging exploration.

### Mechanism 3
Exponential decay over visit counts prevents revisiting similar molecular structures. Morgan fingerprints represent molecular structures, LSH enables efficient frequency tracking, and exponential decay formula assigns higher rewards to less frequently visited structures. The decay prevents the agent from getting stuck in local optima by continuously encouraging exploration of novel structures.

## Foundational Learning

**Concept: Reinforcement learning with intrinsic rewards**
- Why needed: The vast chemical space requires intrinsic motivation beyond extrinsic property rewards to enable effective exploration
- Quick check: What is the difference between extrinsic and intrinsic rewards in reinforcement learning?

**Concept: Molecular representation with SELFIES**
- Why needed: Need robust string representation that always produces valid molecular structures during generation process
- Quick check: How does SELFIES differ from SMILES in handling molecular structure generation errors?

**Concept: Proximal Policy Optimization (PPO)**
- Why needed: Need stable policy gradient algorithm for training molecular generation policy with multiple reward signals
- Quick check: What problem does PPO's clipping mechanism solve compared to vanilla policy gradients?

## Architecture Onboarding

**Component map:** Policy network (SELFIES generation) → Environment (property evaluation) → Intrinsic reward modules (HIR + LIR) → PPO training loop

**Critical path:** Molecular generation → Property evaluation → Reward calculation (PER + HIR + LIR) → Policy update → Next generation

**Design tradeoffs:** HIR provides stability but requires careful frequency tracking setup; LIR provides strong early exploration but may generalize poorly; combination requires balancing weights

**Failure signatures:** If HIR dominates, agent explores but never exploits; if LIR dominates, agent may get stuck in local optima; if PER dominates, agent may generate invalid molecules

**First 3 experiments:**
1. Implement HIR-only version and verify exponential decay behavior on simple molecular property task
2. Implement LIR-only version and verify prediction error behavior on molecular generation task
3. Combine HIR and LIR with tunable weights and test on celecoxib similarity task to verify adaptive behavior

## Open Questions the Paper Calls Out

**Open Question 1**
- Question: What is the optimal balance between history-based and learning-based intrinsic rewards for different molecular property optimization tasks?
- Basis in paper: The paper demonstrates that Mol-AIR outperforms both history-based and learning-based approaches alone, and shows that the weight parameter β = 0.01 provides the best balance across multiple target properties
- Why unresolved: While the paper identifies that β = 0.01 is statistically optimal, the underlying reasons for why this specific ratio works best across different molecular properties remain unclear
- What evidence would resolve it: Systematic analysis of how the optimal β value varies across different molecular property types and chemical spaces, along with theoretical justification for the observed optimal ratio

**Open Question 2**
- Question: How can intrinsic reward functions be designed to consider target property information for more refined exploration in tasks requiring structural similarity to existing drugs?
- Basis in paper: The paper acknowledges that Mol-AIR still achieves relatively low similarity scores (0.330) for celecoxib-like molecule generation, and suggests that current intrinsic rewards are calculated independently of target property information
- Why unresolved: The paper identifies this as a limitation but does not propose specific solutions for integrating target property information into intrinsic reward calculation
- What evidence would resolve it: Development and testing of novel intrinsic reward functions that incorporate target property information, demonstrating improved performance in drug similarity tasks

**Open Question 3**
- Question: What is the theoretical foundation for why combining history-based and learning-based intrinsic rewards creates superior exploration dynamics in molecular reinforcement learning?
- Basis in paper: The paper shows through ablation studies that Mol-AIR outperforms both HIR and LIR alone, with LIR providing strong early exploration and HIR ensuring sustained exploration, but does not provide theoretical explanation for this synergy
- Why unresolved: The paper empirically demonstrates the superiority of the combined approach but lacks theoretical analysis of the underlying mechanisms
- What evidence would resolve it: Mathematical modeling of exploration dynamics showing how the combination of count-based and prediction-based approaches creates optimal exploration-exploitation trade-offs

## Limitations
- Adaptive combination mechanism relies on single balancing parameter β without clear theoretical justification for optimal weighting
- Morgan fingerprint + LSH approach assumes similar molecules have similar fingerprints, but doesn't address collision cases for chemically distinct molecules
- Random distillation network mechanism lacks detailed analysis of prediction error correlation with actual state novelty in chemical space

## Confidence

**High confidence:** The empirical results showing Mol-AIR outperforming baselines on all benchmark tasks, particularly the QED score achieving theoretical optimum (0.948) and the discovery of novel molecular structures with superior penalized LogP scores.

**Medium confidence:** The mechanism explaining why the combination of HIR and LIR works better than either alone, as the paper provides ablation results but limited theoretical analysis of the temporal dynamics.

**Low confidence:** The claim that the adaptive intrinsic reward scheme generalizes to entirely new molecular property optimization tasks beyond the benchmark set, as the paper only tests on a limited set of properties.

## Next Checks
1. **Ablation study on balancing parameter β**: Systematically vary the weight between HIR and LIR across multiple orders of magnitude to identify optimal ranges and verify that the reported performance isn't sensitive to specific parameter choices.

2. **Collision analysis for LSH implementation**: Quantify the false positive and false negative rates in the locality-sensitive hashing for molecular fingerprints, particularly for structurally similar but chemically distinct molecules.

3. **Cross-property generalization test**: Apply Mol-AIR to at least 5 new molecular property optimization tasks not included in the original benchmark to assess whether the adaptive intrinsic reward mechanism transfers to unseen property landscapes.