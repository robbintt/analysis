---
ver: rpa2
title: 'AnyCBMs: How to Turn Any Black Box into a Concept Bottleneck Model'
arxiv_id: '2405.16508'
source_url: https://arxiv.org/abs/2405.16508
tags:
- concept
- anycbms
- concepts
- black
- anycbm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: AnyCBMs introduce a method to convert any pre-trained black-box
  neural network into a Concept Bottleneck Model (CBM) without retraining the original
  model. The approach involves training a new module that maps black-box embeddings
  to interpretable concepts and back, preserving the original model's performance
  while adding interpretability.
---

# AnyCBMs: How to Turn Any Black Box into a Concept Bottleneck Model

## Quick Facts
- arXiv ID: 2405.16508
- Source URL: https://arxiv.org/abs/2405.16508
- Reference count: 15
- Can convert any pre-trained black-box model into a Concept Bottleneck Model without retraining

## Executive Summary
AnyCBMs present a novel approach to transform existing black-box neural networks into interpretable Concept Bottleneck Models (CBMs) without requiring retraining of the original model. The method introduces a separate module that learns to map between the black-box's internal embeddings and human-understandable concepts, while preserving the original model's performance. This approach bridges the gap between interpretability and performance by allowing users to leverage the power of pre-trained models while gaining insight into their decision-making process through concept-level reasoning.

## Method Summary
The AnyCBMs method works by training a new module that operates on the embeddings from a pre-trained black-box model. This module consists of two main components: a concept predictor that maps the black-box embeddings to interpretable concepts, and a task predictor that maps these concepts to the final classification. The approach uses a proxy training objective that encourages the concept predictor to approximate the black-box model's behavior while learning interpretable concepts. This allows the original model to remain untouched while gaining interpretability through the new concept-based interface.

## Key Results
- Achieves comparable classification accuracy to both standard CBMs and original black-box models (99.6% ROC-AUC on MNIST, 90.3% on CUB)
- Concept-based interventions in AnyCBMs are as effective as in standard CBMs for model steering
- Can be trained with a different dataset than the one used for the black-box model, offering flexibility in data requirements

## Why This Works (Mechanism)
The method works by leveraging the rich semantic information captured in black-box model embeddings while introducing an interpretable intermediate layer of concepts. By training the concept predictor to approximate the black-box model's behavior, AnyCBMs ensure that the original model's knowledge is preserved. The bidirectional mapping between embeddings and concepts allows for both forward inference and backward concept analysis, enabling interpretable reasoning without sacrificing performance.

## Foundational Learning
- Concept Bottleneck Models: Why needed - To create interpretable models with human-understandable decision paths; Quick check - Can concepts be validated by domain experts?
- Embedding spaces in neural networks: Why needed - Black-box models learn rich representations; Quick check - Are embeddings semantically meaningful?
- Concept regression vs classification: Why needed - To handle both categorical and continuous concepts; Quick check - Are concept predictions calibrated?

## Architecture Onboarding

Component Map:
Black-Box Model Embeddings -> Concept Predictor -> Concept Space -> Task Predictor -> Final Prediction

Critical Path:
Black-Box Embeddings → Concept Predictor → Task Predictor → Output

Design Tradeoffs:
- Flexibility vs. Fidelity: Training with different datasets provides flexibility but may reduce concept quality
- Interpretability vs. Performance: Concept granularity affects both interpretability and accuracy
- Computational overhead: Additional concept prediction step adds inference time

Failure Signatures:
- Poor concept prediction accuracy indicating misalignment with black-box reasoning
- Significant performance drop compared to original model
- Concept interventions not affecting predictions as expected

First Experiments:
1. Verify concept prediction accuracy on held-out concept labels
2. Compare task prediction accuracy with original black-box model
3. Test concept intervention effectiveness by modifying concept values and observing prediction changes

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Performance depends on quality and coverage of concept labels in probing dataset
- Requires access to black-box model's embeddings, which may not be feasible for all architectures
- Interpretability gains are bounded by choice and granularity of concepts used

## Confidence

**High Confidence:**
- Core technical approach and empirical validation on standard benchmarks (MNIST, CUB)

**Medium Confidence:**
- Method's generalizability to more complex, real-world datasets and tasks
- Practical utility of concept interventions for model steering in complex domains

## Next Checks
1. Evaluate AnyCBMs on more diverse and complex datasets (e.g., medical imaging, autonomous driving scenarios) to assess robustness and scalability
2. Conduct human studies to measure effectiveness of concept-based interventions for non-expert users in practical applications
3. Test method's performance when probing dataset has limited concept coverage or noisy labels to understand sensitivity to data quality