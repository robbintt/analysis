---
ver: rpa2
title: 'Premonition: Using Generative Models to Preempt Future Data Changes in Continual
  Learning'
arxiv_id: '2403.07356'
source_url: https://arxiv.org/abs/2403.07356
tags:
- data
- synthetic
- learning
- datasets
- pre-trained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Premonition uses a large language model to generate text prompts
  describing semantically related classes, which are then rendered as synthetic images
  using Stable Diffusion. These images are used to pre-train a network, which is then
  applied as an input to a continual learning method.
---

# Premonition: Using Generative Models to Preempt Future Data Changes in Continual Learning

## Quick Facts
- arXiv ID: 2403.07356
- Source URL: https://arxiv.org/abs/2403.07356
- Authors: Mark D. McDonnell; Dong Gong; Ehsan Abbasnejad; Anton van den Hengel
- Reference count: 40
- One-line primary result: Synthetic pre-training improves multiple Class Incremental Learning methods on fine-grained image classification benchmarks, with accuracy gains up to 8.1% for ResNet50 models.

## Executive Summary
Premonition addresses the challenge of catastrophic forgetting in continual learning by pre-training models on synthetic data generated to represent future tasks. The approach uses GPT-4 to generate text prompts describing semantically related classes, which are then rendered as synthetic images using Stable Diffusion. These images are used to pre-train a network, which is then applied as input to a continual learning method. Results show that Premonition consistently improves performance across multiple class-incremental learning methods and fine-grained datasets, with accuracy gains of up to 8.1% for ResNet50 models initialized with ImageNet-1K weights. The approach is especially effective when synthetic data is tailored to the target domain, and even outperforms methods that use real data replacement, highlighting the value of pre-training on synthetic data for continual learning.

## Method Summary
Premonition uses large language models to generate text prompts describing semantically related classes, which are rendered as synthetic images using diffusion models like Stable Diffusion. These synthetic images are used to pre-train a network (typically ResNet50 or ViT) initialized with ImageNet-1K weights. The pre-trained model is then used as input to class-incremental learning (CIL) methods such as Nearest Class Mean (NCM), Continual LDA, or RanPAC. The synthetic pre-training adjusts the feature space to be more generalizable to future real data, reducing the synthetic-to-real domain gap when the model encounters real data during continual learning tasks. The approach is particularly effective when synthetic data is tailored to the target domain and can even outperform methods that use real data replacement.

## Key Results
- Premonition consistently improves class-incremental learning performance across multiple methods (NCM, LDA, RanPAC) and fine-grained datasets
- Accuracy gains of up to 8.1% for ResNet50 models initialized with ImageNet-1K weights
- Realm-specific synthetic pre-training outperforms multi-realm synthetic pre-training for domain-specific datasets
- Even surpasses methods that use real data replacement, highlighting the value of pre-training on synthetic data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Premonition's synthetic pre-training creates representations that generalize better to future real data than models trained directly on synthetic data during continual learning.
- Mechanism: The pre-training phase on realm-specific synthetic data adjusts the feature space of the model before continual learning begins. This creates a more generalizable representation space that is less affected by the synthetic-to-real domain gap when the model encounters real data during continual learning tasks.
- Core assumption: The representations learned from synthetic data, while imperfect for zero-shot classification, provide a better initialization for continual learning than models trained directly on synthetic data or those initialized with generic ImageNet weights.
- Evidence anchors:
  - [abstract] "we show that pre-training models in this manner improves multiple Class Incremental Learning (CIL) methods on fine-grained image classification benchmarks"
  - [section] "training on well-designed synthetic datasets can result in a model equipped with useful feature representations"
- Break condition: If the synthetic data quality degrades significantly (as evidenced by CLIP zero-shot accuracy dropping below a threshold), the benefit disappears.

### Mechanism 2
- Claim: Realm-specific synthetic pre-training outperforms multi-realm synthetic pre-training for domain-specific datasets.
- Mechanism: When the synthetic data matches the realm of the target datasets, the pre-trained model develops features more relevant to that specific domain, leading to better transfer performance.
- Core assumption: The domain specificity of the synthetic data matters more than the sheer volume of diverse synthetic data.
- Evidence anchors:
  - [section] "we found that surpassing the performance of the IN1K pre-trained model clearly requires realm-specific synthetic imagery"
  - [section] "when models from a mis-matched realm are used, the performance mostly goes backwards"
- Break condition: If the realm specificity becomes too narrow (e.g., training only on one very specific subtype), the model may overfit to synthetic patterns and generalize poorly.

### Mechanism 3
- Claim: Premonition is particularly valuable when pre-trained models are not already well-specialized for a particular theme.
- Mechanism: For domains where existing pre-trained models (like CLIP or RedCaps) have limited coverage, Premonition's realm-specific synthetic pre-training fills this gap and provides better initialization for continual learning.
- Core assumption: The value of Premonition is inversely proportional to how well existing pre-trained models already capture the target domain.
- Evidence anchors:
  - [section] "we found that Premonition is more likely to have value when pre-trained models are not already very well specialized for a particular theme"
  - [section] "the inconsistency of the results... For the 'food' and 'plants' realms, although the use of Premonition causes accuracy to go backwards, the baseline LGSSL model is stronger than the other two"
- Break condition: If future pre-trained models become universally comprehensive across all realms, Premonition's advantage diminishes.

## Foundational Learning

- Concept: Catastrophic forgetting in continual learning
  - Why needed here: Understanding why continual learning is challenging is crucial to appreciating Premonition's solution. Without access to past data, models tend to degrade on previously learned tasks when new tasks are introduced.
  - Quick check question: What happens to a model's performance on Task 1 after it's trained on Task 2 without any mechanism to preserve past knowledge?

- Concept: Domain gap between synthetic and real data
  - Why needed here: The effectiveness of Premonition relies on understanding that synthetic data, while useful for pre-training, cannot directly replace real data during continual learning due to this gap.
  - Quick check question: Why does training a model entirely on synthetic data and then testing it on real data typically result in poor performance?

- Concept: Transfer learning and representation space adjustment
  - Why needed here: Premonition works by adjusting the representation space through synthetic pre-training, which then serves as a better starting point for continual learning methods.
  - Quick check question: How does pre-training on a task (even synthetic) improve performance on a different but related downstream task?

## Architecture Onboarding

- Component map:
  GPT-4 (LLM) -> Stable Diffusion -> ResNet50/ViT backbone -> CIL method (NCM/LDA/RanPAC) -> Realm-specific datasets

- Critical path:
  1. Generate realm-specific prompts using GPT-4
  2. Generate synthetic images using Stable Diffusion
  3. Pre-train model on synthetic dataset
  4. Apply pre-trained model as input to CIL method
  5. Evaluate on real datasets

- Design tradeoffs:
  - Quality vs. quantity of synthetic data: More samples improve pre-training but increase computational cost
  - Realm specificity vs. generality: More specific realms improve transfer but reduce reusability
  - Model architecture choice: ResNet50 easier to train from scratch vs. ViT better for pre-trained models

- Failure signatures:
  - CLIP zero-shot accuracy on synthetic data drops significantly → Synthetic data quality issue
  - Binary classifier easily distinguishes synthetic from real → Large domain gap
  - Premonition performs worse than baseline on certain realms → Realm mismatch or pre-trained model already specialized

- First 3 experiments:
  1. Generate synthetic 'birds' data and pre-train a ResNet50, then evaluate on CUB dataset using NCM
  2. Compare Premonition-enhanced RanPAC vs baseline RanPAC on Food-101
  3. Test multi-realm vs single-realm pre-training by creating a combined 'birds+food+plants' dataset and comparing performance across all three domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Premonition scale with the size and diversity of the synthetic dataset used for pre-training?
- Basis in paper: [inferred] The paper shows that Premonition improves CIL performance, but the specific relationship between synthetic dataset size/diversity and downstream CIL performance is not explored.
- Why unresolved: The experiments only use a fixed number of synthetic samples per class and do not investigate the impact of varying dataset size or diversity.
- What evidence would resolve it: Experiments systematically varying the number of synthetic samples per class and the diversity of classes in the synthetic dataset, and measuring the impact on CIL performance.

### Open Question 2
- Question: Can Premonition be extended to work with other types of generative models beyond Stable Diffusion, such as autoregressive models or GANs?
- Basis in paper: [explicit] The paper mentions that it would be interesting for future work to consider whether new generative text-to-image models can amplify Premonition's performance.
- Why unresolved: The experiments only use Stable Diffusion, and the potential benefits of other generative models are not explored.
- What evidence would resolve it: Experiments using different generative models (e.g., DALL-E, GANs) for synthetic data generation and comparing their impact on CIL performance.

### Open Question 3
- Question: How does Premonition perform on more complex and diverse datasets beyond fine-grained classification?
- Basis in paper: [inferred] The paper focuses on fine-grained datasets due to their long-tailed distributions and the difficulty they pose for CIL methods. The performance on more diverse datasets is not investigated.
- Why unresolved: The experiments only use fine-grained datasets, and the potential benefits of Premonition on other types of datasets are not explored.
- What evidence would resolve it: Experiments applying Premonition to a wider range of datasets with varying levels of complexity and diversity, such as object detection, semantic segmentation, or action recognition datasets.

## Limitations

- Effectiveness heavily dependent on synthetic data quality and realm specificity
- Requires significant computational resources for generating high-quality synthetic imagery
- Benefit may diminish as more comprehensive pre-trained models become available

## Confidence

**High Confidence**: Premonition improves class-incremental learning performance across multiple methods and datasets when synthetic data is realm-specific.

**Medium Confidence**: The synthetic pre-training mechanism creates more generalizable representations than models trained directly on synthetic data during continual learning.

**Low Confidence**: Premonition's value will decrease as pre-trained models become more universally comprehensive.

## Next Checks

1. Generate synthetic data for a new realm (e.g., "insects") and measure CLIP zero-shot accuracy to verify that the synthetic-to-real domain gap remains acceptable for effective pre-training.

2. Compare performance when using single-realm vs. multi-realm pre-training on a domain-agnostic dataset to quantify the importance of realm-specific synthetic data.

3. Measure the wall-clock time and GPU hours required to generate synthetic datasets versus the accuracy improvements gained, to validate the practical feasibility of the approach in resource-constrained settings.