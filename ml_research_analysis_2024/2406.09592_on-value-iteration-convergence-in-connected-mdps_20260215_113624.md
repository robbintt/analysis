---
ver: rpa2
title: On Value Iteration Convergence in Connected MDPs
arxiv_id: '2406.09592'
source_url: https://arxiv.org/abs/2406.09592
tags: []
core_contribution: "This paper proves that under a unique optimal policy and ergodic\
  \ transition matrix, the Value Iteration algorithm converges geometrically at a\
  \ rate exceeding the discount factor \u03B3. The authors show that after N iterations\
  \ (where N depends on mixing properties of the optimal policy), the span of the\
  \ error vector decreases by a factor \u03C4 \u2208 (0,1), where \u03B3N\u03C4 <\
  \ \u03B3."
---

# On Value Iteration Convergence in Connected MDPs

## Quick Facts
- **arXiv ID**: 2406.09592
- **Source URL**: https://arxiv.org/abs/2406.09592
- **Reference count**: 4
- **Key outcome**: Proves Value Iteration converges geometrically at rate exceeding discount factor γ under unique optimal policy and ergodic transition matrix

## Executive Summary
This paper establishes that Value Iteration converges faster than the discount factor γ when the optimal policy induces an irreducible and aperiodic Markov chain. The authors prove that after N iterations (bounded by n²−2n+2), the error span contracts by a factor τ ∈ (0,1) independent of γ. This leads to improved computational complexity for both discounted and average reward criteria, reducing the number of iterations needed for convergence.

## Method Summary
The paper analyzes three variants of Value Iteration: synchronous without learning rate, synchronous with learning rate α, and asynchronous with learning rate. The core mechanism relies on the optimal policy inducing an irreducible and aperiodic Markov chain, which guarantees mixing properties after N steps. The analysis tracks error span contraction rather than component-wise errors, showing that the span decreases by γ^N τ after N iterations, where τ ∈ (0,1). The learning rate variant trades off γ contraction for faster mixing, requiring only n−1 updates for positive matrices instead of n²−2n+2.

## Key Results
- Proves Value Iteration converges at rate τ < γ after N steps under unique optimal policy and ergodic transition
- Shows learning rate α allows mixing in n−1 steps instead of n²−2n+2 for positive matrices
- Demonstrates asynchronous updates achieve geometric convergence with proper state selection
- Improves computational complexity from O(log(1/ϵ) + log(1/(1−γ))/log(1/γ)) to O(log(1/ϵ) + log(1/(1−γ))/(log(1/γ) + log(1/τ)/N)) for discounted rewards

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Under unique optimal policy and ergodic transition matrix, value iteration converges at rate τ < γ after N steps.
- Mechanism: The optimal policy induces an irreducible and aperiodic Markov chain. After N steps (bounded by n²−2n+2), the error vector span contracts by factor τ ∈ (0,1), independent of γ contraction.
- Core assumption: The MRP implied by optimal policy π* is irreducible and aperiodic (Assumption 3.1).
- Evidence anchors:
  - [abstract] "ensures the convergence of various versions of the Value Iteration algorithm at a geometric rate that exceeds the discount factor γ"
  - [section] Theorem 3.4 proves sp(e_N) ≤ γ^N τ sp(e_0) where τ ∈ (0,1)
  - [corpus] Weak evidence: neighbor papers discuss geometric convergence but don't explicitly establish τ < γ factor
- Break condition: If the optimal policy doesn't yield an irreducible/aperiodic chain, or if multiple optimal policies exist breaking uniqueness.

### Mechanism 2
- Claim: Learning rate α trades off γ contraction for faster mixing, requiring only n−1 updates for positive matrix.
- Mechanism: Introducing α allows diagonal entries (1−α)/γ in P't,α to act as self-loops, ensuring positive matrices in n−1 steps instead of n²−2n+2.
- Core assumption: The optimal policy still induces an irreducible and aperiodic chain under learning rate updates.
- Evidence anchors:
  - [section] Theorem 3.8 shows sp(e_Nα) ≤ γ^Nα τα sp(e_0) with τα ∈ (0,1)
  - [section] "we actually need only Nα = n − 1 updates to guarantee that the matrix P′N t,α is positive"
  - [corpus] No direct evidence; requires assumption about α not breaking irreducibility
- Break condition: If α is too large or small, mixing may not occur or contraction may be insufficient.

### Mechanism 3
- Claim: Asynchronous updates with proper state selection achieve geometric convergence with rate τ' < 1 after B steps.
- Mechanism: Assumption 3.11 ensures every state updates at least n times in B steps, allowing mixing. Error span contracts by γ^N τ' plus a residual term.
- Core assumption: There exists B such that every state is updated at least n times within B steps (Assumption 3.11).
- Evidence anchors:
  - [section] Theorem 3.12: sp(e_B) ≤ γ^N τ' sp(e_0) + (1−γ^B) min(e_0)
  - [section] "if in the synchronous update case contraction factor γ being applied to all states simultaneously...in asynchronous case it might be increasing"
  - [corpus] No neighbor evidence; this is a novel asynchronous result
- Break condition: If state selection is unbalanced (some states rarely updated), convergence fails.

## Foundational Learning

- Concept: Irreducible and aperiodic Markov chains
  - Why needed here: The core convergence mechanism relies on mixing properties of the optimal policy's induced chain
  - Quick check question: Can you explain why irreducibility and aperiodicity together guarantee that some power of the transition matrix has all positive entries?

- Concept: Value iteration convergence bounds
  - Why needed here: Understanding standard O(log(1/ϵ) + log(1/(1−γ))/log(1/γ)) complexity is essential to appreciate the improvement
  - Quick check question: What is the standard convergence rate for value iteration without additional assumptions, and how does the τ factor improve it?

- Concept: Error span analysis
  - Why needed here: The paper analyzes convergence via sp(e) = max(e) − min(e) rather than component-wise error
  - Quick check question: How does analyzing error span differ from analyzing individual state value errors, and why is this approach useful here?

## Architecture Onboarding

- Component map:
  - Input: MDP tuple (S, A, P, r, γ), accuracy parameter ϵ
  - Core: Value iteration loop with modified stopping criteria based on span contraction
  - Output: ϵ-optimal policy
  - Key parameters: N (mixing steps), τ (extra convergence factor), α (learning rate for variants)

- Critical path:
  1. Verify Assumptions 3.1 and 3.2 (irreducibility, uniqueness)
  2. Run value iteration for N steps
  3. Check span contraction: sp(e_t) ≤ γ^t τ^⌊t/N⌋ sp(e_0)
  4. Stop when sp(e_{t+1} − e_t) ≤ stopping criteria

- Design tradeoffs:
  - Synchronous vs asynchronous: Synchronous has cleaner analysis but may be slower; asynchronous requires careful state selection strategy
  - Learning rate α: Introduces trade-off between γ contraction and mixing speed
  - Computational cost: N may be large (up to n²−2n+2) but improves overall convergence

- Failure signatures:
  - No improvement over standard rate: Likely Assumptions 3.1/3.2 violated
  - Oscillation or divergence: Learning rate too high, or state selection unbalanced in asynchronous case
  - Slow convergence despite meeting assumptions: τ may be close to 1, indicating poor mixing

- First 3 experiments:
  1. Verify on a simple 3-state MDP where optimal policy is known and induces an irreducible chain
  2. Test with different learning rates α to observe the trade-off between mixing and contraction
  3. Implement asynchronous version with round-robin state selection to confirm B-bound behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions does the asynchronous algorithm with learning rate exhibit convergence in the discounted reward case?
- Basis in paper: [explicit] The paper states that "the term (1 − γB) min(e0) does not allow to establish the convergence guarantees in discounted case using the same proof strategy."
- Why unresolved: The paper provides a proof for the average reward case but explicitly notes that the same approach doesn't work for the discounted case, suggesting there may be fundamental differences in the convergence behavior.
- What evidence would resolve it: A rigorous proof demonstrating convergence conditions and rates for the asynchronous algorithm in the discounted reward case, or a counterexample showing where it fails to converge.

### Open Question 2
- Question: How does the choice of learning rate α affect the convergence rate τ in the synchronous algorithm with learning rate?
- Basis in paper: [explicit] The paper derives that the convergence rate is τα where τα = [(1−α)/γ + α]N − nδ′Nα, but doesn't explore how different values of α impact this rate.
- Why unresolved: While the paper establishes the theoretical framework, it doesn't provide empirical analysis or theoretical bounds on how α should be chosen to optimize convergence.
- What evidence would resolve it: Empirical studies or theoretical analysis showing the relationship between α and τ, including optimal α selection strategies and trade-offs between learning rate and convergence speed.

### Open Question 3
- Question: What is the exact relationship between the number of states n and the required number of iterations N for mixing in the optimal policy's Markov chain?
- Basis in paper: [explicit] The paper mentions that N can be upper-bounded by n2 − 2n + 2, but this bound is quite loose and the actual N depends on the mixing properties of P∗.
- Why unresolved: The paper uses this upper bound in complexity calculations but acknowledges it's not tight, suggesting that better bounds could lead to improved complexity estimates.
- What evidence would resolve it: Analysis of the spectral gap or mixing time of the optimal policy's Markov chain that would provide tighter bounds on N, or empirical studies on random MDPs showing the actual relationship between n and N.

## Limitations

- The paper's theoretical improvements rely heavily on Assumptions 3.1 and 3.2 (unique optimal policy and ergodic transition matrix), which may not hold in many practical MDPs
- The asynchronous variant's state selection strategy (Assumption 3.11) is vaguely defined as "meaningful choice strategy" without concrete implementation guidelines
- The learning rate variant requires careful tuning of α parameter, but the paper provides limited guidance on optimal α selection

## Confidence

**High** - The core mathematical framework using irreducible and aperiodic chains to establish faster convergence is sound and well-proven in Markov chain theory.

**Medium** - The improvement from O(log(1/ϵ) + log(1/(1−γ))/log(1/γ)) to O(log(1/ϵ) + log(1/(1−γ))/(log(1/γ) + log(1/τ)/N)) is theoretically valid under stated assumptions, but practical gains depend on the mixing time N and contraction factor τ.

**Low-Medium** - The learning rate variant (Section 3.2) shows promise but requires careful tuning of α, and the paper provides limited guidance on selecting optimal α values.

## Next Checks

1. **Assumption Verification Pipeline**: Implement a systematic method to verify Assumptions 3.1 and 3.2 for arbitrary MDPs, including checking uniqueness of optimal policy and computing the mixing time N.

2. **Empirical Parameter Study**: Run comprehensive experiments across different MDP classes (grid worlds, random MDPs, real-world problems) to measure actual τ values and N mixing times, validating the theoretical bounds.

3. **Asynchronous Implementation Benchmark**: Develop multiple state selection strategies (round-robin, prioritized sweeping, random) for the asynchronous variant and benchmark their performance against synchronous updates to identify practical benefits.