---
ver: rpa2
title: An Embedding is Worth a Thousand Noisy Labels
arxiv_id: '2408.14358'
source_url: https://arxiv.org/abs/2408.14358
tags:
- noise
- noisy
- labels
- learning
- wann
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces WANN, a Weighted Adaptive Nearest Neighbor\
  \ approach for robust classification under label noise. The method uses self-supervised\
  \ embeddings from DINOv2 and introduces a reliability score \u03B7 to guide weighted\
  \ voting, where each training sample is weighted by its likelihood of having a correct\
  \ label."
---

# An Embedding is Worth a Thousand Noisy Labels

## Quick Facts
- arXiv ID: 2408.14358
- Source URL: https://arxiv.org/abs/2408.14358
- Reference count: 16
- One-line primary result: WANN achieves up to 97.21% accuracy on CIFAR-10N with noise by combining self-supervised embeddings with reliability-based adaptive weighting.

## Executive Summary
This paper introduces WANN, a Weighted Adaptive Nearest Neighbor approach that achieves robust classification under label noise by leveraging self-supervised embeddings from DINOv2. The method introduces a reliability score η to identify likely correct labels and uses this to guide weighted voting and adaptive neighborhood sizing. WANN consistently outperforms both traditional k-NN methods and deep learning approaches with robust loss functions across various noise patterns and severities, while maintaining efficiency and explainability.

## Method Summary
WANN computes a reliability score η for each training sample by finding the minimum k needed for correct k-NN classification. This score serves dual purposes: weighting training samples during voting (higher η = higher weight) and determining adaptive neighborhood size kT = 1/η for each test sample. The method optionally integrates with Filtered LDA (FLDA), which removes samples with η = 1/kmax before dimensionality reduction to produce more discriminative projections. Experiments use DINOv2 Large backbone embeddings (1024-dim from 14×14 patches) and evaluate performance across CIFAR-10/100, Animal-10N, and medical datasets under various noise patterns.

## Key Results
- WANN achieves 97.21% accuracy on CIFAR-10N with noise, outperforming robust loss functions and k-NN baselines
- Adaptive methods consistently approach best performances across noise rates and patterns, while fixed k-NN methods suffer accuracy drops
- FLDA improves classification performance with 10× and 100× smaller embeddings while maintaining accuracy
- WANN demonstrates strong generalization to medical datasets (BreastMNIST, DermaMNIST) and imbalanced data scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised embeddings from DINOv2 are sufficiently discriminative to support k-NN classification under label noise
- Mechanism: DINOv2's contrastive learning training causes semantically similar objects to cluster closely in embedding space, allowing small neighborhoods to capture correct class labels even when some training labels are noisy
- Core assumption: The embedding space preserves semantic similarity and class separability despite label corruption
- Evidence anchors: DINOv2 trained via contrastive learning represents similar objects closely within embedding space; proximity of similar objects suggests small k should be sufficient for accurate classification
- Break condition: If embedding space is not sufficiently class-separable (e.g., on very fine-grained or ambiguous classes), k-NN performance degrades regardless of weighting

### Mechanism 2
- Claim: The reliability score η effectively identifies likely correct labels and guides adaptive neighborhood sizing
- Mechanism: η is computed as the inverse of the minimum k needed for correct k-NN classification; high η indicates low k requirement and thus likely correct label. This score is used to weight training samples and set adaptive neighborhood size kT = 1/η
- Core assumption: Label correctness correlates with the minimum k needed for correct k-NN classification
- Evidence anchors: Proper weighting should assign higher weights to samples more likely to have correct labels; adaptive methods consistently approach best performances across noise rates and patterns
- Break condition: If η computation is unstable (e.g., due to small training sets or ambiguous class boundaries), adaptive weighting may mislead rather than help

### Mechanism 3
- Claim: Filtering noisy labels before LDA (FLDA) improves dimensionality reduction quality and downstream classification
- Mechanism: Standard LDA assumes correct labels; noisy labels distort the within-class and between-class scatter matrices. By removing samples with η = 1/kmax (likely noisy), FLDA produces more discriminative projections
- Core assumption: Label noise significantly degrades LDA projections and can be partially removed via η filtering
- Evidence anchors: LDA relies on correct labels as label noise can result in inaccurate linear mappings; FLDA is significantly better than all other techniques evaluated (Wilcoxon signed-rank test, p<0.05)
- Break condition: If noise is instance-dependent or subtle, η may fail to filter all noisy samples, limiting FLDA gains

## Foundational Learning

- **Concept**: Self-supervised contrastive learning
  - Why needed here: Explains why DINOv2 embeddings cluster semantically similar objects, enabling k-NN under noise
  - Quick check question: How does contrastive learning encourage embeddings of similar objects to be close?

- **Concept**: Label noise types (symmetric, asymmetric, instance-dependent)
  - Why needed here: Different noise patterns affect k-NN differently; understanding them clarifies why adaptive weighting helps
  - Quick check question: What is the key difference between symmetric and asymmetric label noise?

- **Concept**: Curse of dimensionality and dimensionality reduction
  - Why needed here: k-NN suffers in high dimensions; explains the need for FLDA and the benefit of projecting embeddings
  - Quick check question: Why does distance concentration in high dimensions hurt k-NN performance?

## Architecture Onboarding

- **Component map**: DINOv2 backbone → Pre-generated embedding database → η computation (offline) → k-NN with adaptive kT and weighted voting → Optional FLDA projection
- **Critical path**: Embedding generation → η score computation → Inference (adaptive k-NN with weights)
- **Design tradeoffs**: Larger kmax improves η reliability but increases computation; FLDA reduces latency but needs sufficient clean samples; DINOv2 offers strong embeddings but at high memory cost
- **Failure signatures**: If classification accuracy is close to random guessing, check: (1) embedding quality/diversity, (2) η computation stability, (3) label noise severity exceeding model's robustness
- **First 3 experiments**:
  1. Verify DINOv2 embeddings cluster by class on a small clean dataset using t-SNE or UMAP
  2. Compute η scores on a small noisy dataset and inspect whether high η correlates with visually correct labels
  3. Compare ANN vs WANN accuracy on a small subset with injected symmetric noise (10-20%) to confirm weighting helps

## Open Questions the Paper Calls Out

- **Open Question 1**: How does WANN's performance compare to deep learning methods when the feature extractor is trained on domain-specific data rather than general-purpose data?
  - Basis in paper: The paper notes WANN's effectiveness on medical datasets far from DINOv2's pretraining data, suggesting domain-specific features could further improve performance
  - Why unresolved: The experiments only test WANN with DINOv2's general-purpose features, not with domain-specific feature extractors
  - What evidence would resolve it: Experiments comparing WANN using general-purpose vs. domain-specific feature extractors on various domain-specific datasets

- **Open Question 2**: What is the optimal neighborhood size for fixed k-NN methods across different noise patterns and severities?
  - Basis in paper: The paper shows WANN outperforms fixed k-NN methods (11-NN and 51-NN) but doesn't systematically explore the optimal k values for fixed methods
  - Why unresolved: The paper only tests two fixed k values (11 and 51) while adaptive methods could theoretically find any k value
  - What evidence would resolve it: Comprehensive experiments testing fixed k-NN across a wide range of k values for different noise patterns and severities

- **Open Question 3**: How does WANN's explainability benefit scale with dataset size and complexity?
  - Basis in paper: The paper demonstrates explainability benefits through neighborhood visualization but doesn't quantify how these benefits change with dataset characteristics
  - Why unresolved: The qualitative explainability analysis is limited to a small subset of STL-10 without systematic scaling analysis
  - What evidence would resolve it: Quantitative metrics measuring explainability quality across datasets of varying sizes and complexity levels

- **Open Question 4**: What is the computational complexity trade-off between WANN and deep learning methods in real-world deployment scenarios?
  - Basis in paper: The paper highlights WANN's efficiency but doesn't provide comprehensive runtime comparisons with deep learning methods across different deployment scales
  - Why unresolved: The paper focuses on classification accuracy rather than comprehensive computational cost analysis
  - What evidence would resolve it: Detailed runtime benchmarks comparing WANN and deep learning methods across various dataset sizes, model architectures, and hardware configurations

## Limitations
- WANN's performance relies heavily on the quality of self-supervised embeddings, and domain shift from DINOv2 pretraining may limit effectiveness on specialized datasets
- The reliability score η may become unstable under severe noise rates or instance-dependent noise patterns, potentially reducing WANN's robustness
- Computational cost of pairwise distance computations for η may become prohibitive at scale, limiting practical deployment on very large datasets

## Confidence
- **High**: WANN's consistent improvement over robust loss functions and k-NN baselines under various noise types and severities
- **Medium**: The mechanism by which η identifies correct labels and guides adaptive neighborhood sizing
- **Medium**: The effectiveness of FLDA in improving classification performance after filtering noisy samples

## Next Checks
1. Test WANN's performance on a dataset with instance-dependent label noise to assess the robustness of the reliability score η under more challenging noise patterns
2. Evaluate the impact of varying kmin and kmax on η computation stability and classification accuracy to determine optimal hyperparameter ranges
3. Compare FLDA's performance against other label-noise-aware dimensionality reduction techniques, such as label-noise-robust LDA variants, to validate the filtering approach's effectiveness