---
ver: rpa2
title: Post-OCR Text Correction for Bulgarian Historical Documents
arxiv_id: '2409.00527'
source_url: https://arxiv.org/abs/2409.00527
tags:
- error
- which
- correction
- dataset
- errors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of post-OCR text correction for
  Bulgarian historical documents, focusing on the Drinov orthography from the 19th
  century. The authors developed a benchmark dataset for this task and created a method
  for generating synthetic training data by converting modern Bulgarian texts into
  historical orthographies.
---

# Post-OCR Text Correction for Bulgarian Historical Documents

## Quick Facts
- arXiv ID: 2409.00527
- Source URL: https://arxiv.org/abs/2409.00527
- Reference count: 40
- Primary result: 25% improvement in document quality for Bulgarian historical texts

## Executive Summary
This paper addresses post-OCR text correction for Bulgarian historical documents, specifically focusing on the Drinov orthography from the 19th century. The authors developed a comprehensive benchmark dataset and created a method for generating synthetic training data by converting modern Bulgarian texts into historical orthographies. Their approach combines a character-level sequence-to-sequence model with advanced mechanisms including diagonal attention loss, copy mechanism, and coverage mechanism. The method demonstrates significant improvements over existing approaches, achieving a 16% increase in performance on the ICDAR 2019 Bulgarian dataset while maintaining strong error detection capabilities with an F1-score of 0.90.

## Method Summary
The authors developed a multi-component system for post-OCR correction of Bulgarian historical documents. The core methodology involves a character-level sequence-to-sequence model augmented with diagonal attention loss to capture local dependencies, a copy mechanism for handling rare characters and words, and a coverage mechanism to prevent repetition. The system is trained on synthetic data generated by converting modern Bulgarian texts into historical Drinov orthography. For error detection, they employed mDeBERTaV3, achieving strong performance on both modern and historical datasets. The approach is specifically designed to handle the linguistic complexities of 19th-century Bulgarian orthography while maintaining robustness to OCR errors.

## Key Results
- 25% improvement in overall document quality for Bulgarian historical texts
- 16% improvement over state-of-the-art on ICDAR 2019 Bulgarian dataset
- Error detection F1-scores: 0.90 on ICDAR dataset, 0.65 on newly created Drinov dataset

## Why This Works (Mechanism)
The system works by combining multiple complementary mechanisms that address different aspects of the post-OCR correction problem. The character-level sequence-to-sequence architecture allows for precise character-by-character corrections, while the diagonal attention loss helps capture local orthographic patterns specific to historical Bulgarian. The copy mechanism enables the model to handle rare historical characters and words that may not appear frequently in training data, and the coverage mechanism prevents repetitive corrections that could degrade document quality.

## Foundational Learning
- **Sequence-to-sequence modeling**: Converts corrupted OCR output to corrected text through learned mappings
  - Why needed: OCR errors require complex transformations beyond simple pattern matching
  - Quick check: Verify model can handle both insertion and deletion errors effectively

- **Character-level processing**: Operates on individual characters rather than words
  - Why needed: Historical orthographies have many rare characters and spelling variations
  - Quick check: Test performance on documents with high character diversity

- **Attention mechanisms**: Focus on relevant parts of input during decoding
  - Why needed: OCR errors often cluster locally, requiring context-aware corrections
  - Quick check: Measure attention distribution patterns on error-prone regions

- **Synthetic data generation**: Converts modern texts to historical orthographies
  - Why needed: Limited availability of authentic historical document pairs
  - Quick check: Validate synthetic data preserves linguistic patterns of target orthography

## Architecture Onboarding

Component map: Modern text -> Synthetic data generator -> Training data -> Sequence-to-sequence model (with diagonal attention, copy, coverage) -> Post-OCR correction -> Error detection (mDeBERTaV3)

Critical path: OCR output → Sequence-to-sequence correction → Error detection validation → Final output

Design tradeoffs: Character-level processing provides fine-grained control but increases computational complexity compared to word-level approaches. The synthetic data generation trades authenticity for scalability in training data creation.

Failure signatures: Performance degradation on documents with heavy degradation, failure to correct context-dependent errors requiring deep linguistic knowledge, and potential overfitting to synthetic data patterns that don't fully capture authentic historical orthography variations.

First experiments to run:
1. Test sequence-to-sequence model on synthetic Drinov data to establish baseline correction capabilities
2. Evaluate error detection performance on a small subset of real historical documents
3. Conduct ablation study removing diagonal attention loss to measure its individual contribution

## Open Questions the Paper Calls Out
None

## Limitations
- Performance on authentic historical documents remains to be fully validated beyond synthetic data
- Error detection model shows significant performance drop (0.90 to 0.65 F1-score) when moving from modern to historical datasets
- Synthetic data generation may not capture all linguistic variations present in authentic 19th-century documents

## Confidence

High confidence: OCR error correction methodology, synthetic data generation pipeline, benchmark dataset creation

Medium confidence: Performance claims on historical Drinov orthography, error detection generalization across orthographies

Low confidence: Long-term maintenance of the approach as new historical document variants emerge

## Next Checks

1. Test the complete pipeline on a larger corpus of authentic Drinov orthography documents to validate performance claims beyond synthetic data

2. Conduct ablation studies to quantify the individual contributions of diagonal attention loss, copy mechanism, and coverage mechanism to the overall improvement

3. Evaluate the system's robustness to OCR errors from different recognition engines and document degradation levels not represented in the current benchmark datasets