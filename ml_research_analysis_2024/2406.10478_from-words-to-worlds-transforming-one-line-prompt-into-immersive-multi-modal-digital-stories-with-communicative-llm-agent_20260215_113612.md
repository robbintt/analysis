---
ver: rpa2
title: 'From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal
  Digital Stories with Communicative LLM Agent'
arxiv_id: '2406.10478'
source_url: https://arxiv.org/abs/2406.10478
tags:
- story
- setting
- character
- agent
- list
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: StoryAgent is a framework that automates the creation of immersive,
  multi-modal digital stories from a single text prompt. It uses a top-down story
  drafting process with communicative LLM agents to break down the narrative into
  a hierarchical text representation, then applies generative models and tools in
  a bottom-up fashion to produce corresponding assets for each modality.
---

# From Words to Worlds: Transforming One-line Prompt into Immersive Multi-modal Digital Stories with Communicative LLM Agent

## Quick Facts
- arXiv ID: 2406.10478
- Source URL: https://arxiv.org/abs/2406.10478
- Reference count: 40
- Key outcome: StoryAgent automates immersive, multi-modal digital story creation from a single text prompt using communicative LLM agents and hierarchical text representation

## Executive Summary
StoryAgent is a framework that transforms a one-line text prompt into immersive, multi-modal digital stories through a top-down story drafting process with communicative LLM agents. The system breaks down the narrative into a hierarchical text representation, then applies generative models and tools in a bottom-up fashion to produce corresponding assets for each modality. By using text as an intermediate representation, StoryAgent achieves scene interactivity, character consistency, and multi-modal alignment without requiring reference videos, while allowing easy integration of new generative models and supporting targeted modifications through natural language instructions.

## Method Summary
StoryAgent uses communicative LLM agents to decompose a text prompt into a hierarchical story structure through a top-down drafting approach. The framework then applies generative models and tools in a bottom-up fashion to create assets for each modality (text, image, audio, speech). Scene understanding is enabled through image segmentation and depth estimation, allowing character-object interactions in 2D art styles. The system maintains consistency across modalities by reusing assets identified through unique IDs and using text as an intermediate representation. Human intervention is supported by structuring the pipeline so downstream agents are strictly downstream of upstream changes, allowing targeted regeneration without affecting the overall narrative.

## Key Results
- Achieves long-duration consistency across modalities using text as intermediate representation with asset ID reuse
- Enables scene interactivity in 2D art styles through segmentation, depth estimation, and affordance reasoning
- Supports flexible human intervention by structuring agent dependencies so changes propagate only downstream
- Produces coherent digital stories while maintaining multi-modal alignment without requiring reference videos

## Why This Works (Mechanism)

### Mechanism 1
- Claim: StoryAgent achieves long-duration consistency across modalities by using text as an intermediate representation and reusing assets identified by unique IDs.
- Mechanism: The framework generates a hierarchical textual description where leaf nodes specify modality assets. These assets are referenced by unique IDs, ensuring consistent reuse across scenes (e.g., character costumes, background elements). The text also coordinates timing and content alignment between visual, audio, and narrative components.
- Core assumption: Text can serve as a stable, format-neutral bridge between diverse generative models without losing critical spatial or semantic information.

### Mechanism 2
- Claim: Scene interactivity in 2D art styles is enabled by combining image understanding (segmentation + depth) with affordance reasoning to script object-aware interactions.
- Mechanism: Generated images undergo semantic segmentation and depth estimation to extract object locations and spatial relationships. Setting affordance agents then reason about how characters can interact with these objects (e.g., pointing, gazing, referencing in dialogue), and screenplay agents use this data to direct camera focus and character gestures accordingly.
- Core assumption: Modern segmentation and depth models can reliably infer object boundaries and relative positions from 2D images without explicit 3D geometry.

### Mechanism 3
- Claim: Flexible human intervention is achieved by structuring the pipeline so that downstream agents are strictly downstream of upstream changes, allowing targeted regeneration.
- Mechanism: The story cluster follows a top-down decomposition where changes at any stage only affect agents downstream of that stage. For example, altering the screenplay style only changes downstream speech and gesture generation, preserving upstream plot and character consistency. This is enforced by agent prompts and JSON-based data flow.
- Core assumption: Agent roles and data dependencies can be strictly controlled via system prompts and JSON schemas, preventing unintended ripple effects.

## Foundational Learning

- Concept: Hierarchical story decomposition
  - Why needed here: Enables modular generation where changes to one component (e.g., plot) can be propagated to dependent components (e.g., scenes) without affecting unrelated parts (e.g., music).
  - Quick check question: What happens if a character's name is changed in the story arc? Which downstream teams need to update?

- Concept: Multi-modal alignment
  - Why needed here: Ensures that visual, audio, and narrative elements remain synchronized in time and content, critical for immersion.
  - Quick check question: How does the framework ensure that a character's voice matches their visual appearance across scenes?

- Concept: Image-based scene understanding
  - Why needed here: Provides spatial and semantic context for 2D scenes, enabling object-aware interactions and camera framing.
  - Quick check question: What information is extracted from images to enable characters to "look at" or "point to" objects?

## Architecture Onboarding

- Component map:
  - Text prompt → StoryCluster (StoryArc, Characters, Settings, StoryBeats, SettingAffordances, StoryScenes, Screenplay) → AssetGenerationTeams (Image, Sound, Speech) → Output: Multi-modal digital story

- Critical path:
  - Text prompt → StoryCluster → SceneUnderstanding (segmentation + depth) → Screenplay actions → AssetGenerationTeams → Rendering

- Design tradeoffs:
  - Text as intermediate: Increases flexibility and consistency but depends on model robustness to prompt variations.
  - Procedural pipeline: Allows fine-grained control and intervention but may be slower than end-to-end models.
  - Hybrid LLM + generative models: Leverages reasoning and world knowledge but requires careful prompt engineering.

- Failure signatures:
  - Inconsistency across modalities: Likely due to misaligned text descriptions or asset ID reuse errors.
  - Poor scene interactivity: Likely due to segmentation/depth estimation failures or missing affordances.
  - Intervention not working: Likely due to upstream changes not propagating or agent prompts not being followed.

- First 3 experiments:
  1. Run base story generation and verify character consistency across scenes by checking asset IDs.
  2. Modify screenplay style (e.g., rhyming couplets) and observe downstream changes while upstream remains fixed.
  3. Test scene understanding by generating an image, running segmentation + depth, and verifying affordance generation for character-object interactions.

## Open Questions the Paper Calls Out
- How does StoryAgent handle temporal consistency for long-duration narratives beyond the limitations of current diffusion-based animation generation methods?
- What are the specific performance metrics and benchmarks used to evaluate the quality and coherence of the generated digital stories compared to human-created content?
- How does StoryAgent ensure consistency between the narrative text and generated visual/audio elements when using multiple different generative models from various sources?

## Limitations
- Framework's reliance on text as intermediate representation depends heavily on generative model robustness to prompt variations
- Effectiveness depends on quality of image understanding tools for segmentation and depth estimation, which may struggle with stylized content
- Intervention mechanism assumes strict downstream dependencies can be enforced through agent prompts and JSON schemas, but real-world LLM behavior may deviate

## Confidence
- **High Confidence**: Framework's ability to produce coherent multi-modal digital stories through hierarchical text decomposition and bottom-up asset generation
- **Medium Confidence**: Framework's ability to enable flexible human intervention through structural design
- **Medium Confidence**: Framework's mechanism for achieving long-term consistency across modalities through text-based asset management
- **Medium Confidence**: Framework's approach to scene interactivity through image understanding and affordance reasoning

## Next Checks
1. **Character Consistency Test**: Generate a story with multiple scenes featuring the same character, then verify through asset ID tracking that visual elements (costume, appearance) remain consistent across all scenes, including checking that reused assets maintain identical properties.

2. **Intervention Propagation Test**: Modify a story beat at the screenplay level to change dialogue style (e.g., to rhyming couplets), then verify that only downstream components (speech generation, gestures) are affected while upstream components (plot, characters) remain unchanged.

3. **Scene Understanding Robustness Test**: Generate a stylized 2D image, run segmentation and depth estimation, then test whether affordance reasoning correctly identifies interactable objects and their spatial relationships across multiple different artistic styles.