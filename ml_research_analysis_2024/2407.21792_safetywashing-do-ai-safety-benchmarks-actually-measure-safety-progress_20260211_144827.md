---
ver: rpa2
title: 'Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?'
arxiv_id: '2407.21792'
source_url: https://arxiv.org/abs/2407.21792
tags:
- capabilities
- safety
- arxiv
- benchmarks
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether AI safety benchmarks genuinely
  measure safety progress or simply track general capabilities and training compute.
  Using a meta-analysis of dozens of models across various safety benchmarks, the
  authors empirically measure correlations between safety metrics and upstream model
  capabilities.
---

# Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?

## Quick Facts
- arXiv ID: 2407.21792
- Source URL: https://arxiv.org/abs/2407.21792
- Reference count: 40
- Primary result: Many AI safety benchmarks correlate strongly with general capabilities and compute, potentially enabling misleading claims about safety progress

## Executive Summary
This meta-analytic study investigates whether AI safety benchmarks genuinely measure safety progress or merely track general capabilities and training compute. The authors find that many safety benchmarks - including alignment with human preferences, misconception avoidance, scalable oversight, and static adversarial robustness - show high correlations with capabilities and compute, potentially enabling "safetywashing" where capability improvements are misrepresented as safety advancements. Conversely, benchmarks for bias, dynamic adversarial robustness, and some calibration metrics show lower correlations with capabilities. The study concludes that safety benchmarks often fail to isolate distinct safety properties and recommends that future benchmarks report their capabilities correlations to prevent misleading claims about AI safety progress.

## Method Summary
The authors conducted a comprehensive meta-analysis examining correlations between safety benchmark performance and upstream model capabilities across dozens of AI models. They collected publicly available benchmark data from various model families and training regimes, analyzing the statistical relationships between safety metrics and measures of model capability and training compute. The study examined multiple categories of safety benchmarks including alignment, misconception avoidance, scalable oversight, static adversarial robustness, bias detection, dynamic adversarial robustness, and calibration. By quantifying these correlations, the researchers identified which safety metrics appear to measure genuine safety properties versus those that primarily track general capability improvements.

## Key Results
- Safety benchmarks for alignment, misconception avoidance, scalable oversight, and static adversarial robustness show strong correlations with general capabilities and compute
- Benchmarks measuring bias, dynamic adversarial robustness, and some calibration metrics demonstrate lower correlations with capabilities
- The high correlations between many safety metrics and capabilities enable "safetywashing" where capability improvements are misrepresented as safety advancements
- Current safety benchmarks often fail to isolate distinct safety properties from general capability improvements

## Why This Works (Mechanism)
The study works by empirically demonstrating that many safety benchmarks are not measuring what they claim to measure. Instead of isolating safety-specific improvements, these benchmarks appear to track general capability increases that occur as models become larger and more powerful. This mechanism explains why models with higher compute budgets and stronger upstream capabilities tend to score better on safety benchmarks - not because they are safer in any meaningful sense, but because they are more capable overall. The correlation patterns reveal that safety metrics are often capturing general intelligence or capability improvements rather than safety-specific properties.

## Foundational Learning

**Statistical correlation analysis**: Why needed - to quantify relationships between safety metrics and capabilities; Quick check - understand Pearson correlation coefficient and p-values

**Benchmark design principles**: Why needed - to understand how safety metrics should ideally measure distinct properties; Quick check - familiarity with construct validity and measurement theory

**AI capability measurement**: Why needed - to contextualize what upstream capabilities are being correlated with safety metrics; Quick check - knowledge of standard AI evaluation metrics like MMLU, BIG-bench

**Meta-analysis methodology**: Why needed - to understand how the authors synthesized results across multiple studies; Quick check - basic understanding of systematic review and meta-analytic techniques

**Safetywashing concept**: Why needed - to grasp the phenomenon of conflating capability with safety improvements; Quick check - understanding of how research findings can be misrepresented

## Architecture Onboarding

**Component map**: Model capabilities and compute -> Safety benchmark performance -> Correlation analysis -> Safetywashing identification

**Critical path**: Data collection → Correlation computation → Statistical validation → Interpretation of safetywashing implications

**Design tradeoffs**: The study prioritizes breadth of analysis across many models and benchmarks over depth of analysis for individual benchmark designs, potentially missing nuanced differences in how specific benchmarks operate

**Failure signatures**: High correlation between safety metrics and capabilities indicates the benchmark may not be measuring safety-specific properties; Low correlation suggests the benchmark may be isolating genuine safety improvements

**First experiments**: 1) Replicate correlation analysis using a different statistical method (e.g., Spearman vs Pearson); 2) Test correlation patterns using only models from the same training regime; 3) Examine temporal trends by tracking individual models' safety benchmark performance over multiple training iterations

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but the findings raise several implicit questions: How can we design safety benchmarks that are truly independent of general capabilities? What would constitute a meaningful safety improvement that is not simply a byproduct of capability gains? How prevalent is safetywashing in published AI research, and what are its practical implications for AI governance and deployment decisions?

## Limitations

- The analysis relies on heterogeneous data from different model families and training regimes, introducing potential measurement inconsistencies
- Correlations do not establish causation, and confounding factors like architectural differences are not fully accounted for
- The study focuses on static correlations without examining how safety interventions impact benchmark performance over time
- The practical significance of correlation differences between benchmark categories requires further investigation

## Confidence

- **High confidence**: The empirical finding that safety benchmarks show strong correlations with general capabilities and compute for alignment, misconception avoidance, scalable oversight, and static adversarial robustness metrics
- **Medium confidence**: The conclusion that these correlations enable "safetywashing" practices, as this interpretation requires assumptions about how results are presented and interpreted in practice
- **Medium confidence**: The observation that certain safety aspects (bias, dynamic adversarial robustness, calibration) show lower correlations, though the practical significance of these differences requires further investigation

## Next Checks

1. Conduct longitudinal studies tracking individual models' performance on safety benchmarks over multiple training iterations to distinguish genuine safety improvements from capability-based gains

2. Design and validate new safety benchmarks with explicitly controlled capability components, then compare their correlation patterns against existing benchmarks using the same model sets

3. Perform qualitative analysis of published papers that claim safety improvements to empirically verify the prevalence and nature of safetywashing practices in the literature