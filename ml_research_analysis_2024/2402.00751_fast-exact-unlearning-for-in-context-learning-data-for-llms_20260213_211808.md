---
ver: rpa2
title: Fast Exact Unlearning for In-Context Learning Data for LLMs
arxiv_id: '2402.00751'
source_url: https://arxiv.org/abs/2402.00751
tags:
- unlearning
- learning
- in-context
- cost
- exact
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes an efficient exact unlearning method for fine-tuning
  large language models using in-context learning with quantized k-means clustering
  (ERASE). By avoiding SGD-based fine-tuning, the approach achieves dataset and model-size
  independent unlearning operations, drastically reducing costs compared to existing
  methods like SISA.
---

# Fast Exact Unlearning for In-Context Learning Data for LLMs

## Quick Facts
- arXiv ID: 2402.00751
- Source URL: https://arxiv.org/abs/2402.00751
- Reference count: 28
- The paper proposes an efficient exact unlearning method for fine-tuning large language models using in-context learning with quantized k-means clustering (ERASE).

## Executive Summary
This paper introduces ERASE, a method for exact unlearning in large language model fine-tuning that leverages quantized k-means clustering with in-context learning. By avoiding traditional SGD-based fine-tuning, ERASE achieves dataset and model-size independent unlearning operations, dramatically reducing costs compared to existing methods like SISA. Experiments on 15 Big-Bench tasks demonstrate that ERASE matches or exceeds the accuracy of SISA variants while offering significantly lower unlearning costs, making it a practical solution for exact unlearning in LLM fine-tuning scenarios.

## Method Summary
ERASE combines in-context learning with quantized k-means clustering to achieve exact unlearning for LLM fine-tuning. The method generates embeddings for training examples using Sentence-BERT, clusters them using quantized k-means, and selects the closest example to each centroid as in-context examples. For unlearning, ERASE checks if removed datapoints affect quantized centroids; if not, no action is needed. The approach uses a quantization parameter ϵ to ensure stability, enabling dataset-size independent unlearning operations. The method is evaluated against SISA variants on LLaMA 7B across 15 Big-Bench tasks, measuring both accuracy and holistic unlearning costs.

## Key Results
- ERASE achieves normalized aggregate scores matching or exceeding SISA variants on 15 Big-Bench tasks
- Unlearning operation costs are effectively constant and independent of dataset size
- ERASE provides significantly lower holistic unlearning costs (inferences per unlearning request) compared to SISA methods

## Why This Works (Mechanism)

### Mechanism 1
Quantized k-means clustering enables dataset-size independent unlearning for in-context learning. By quantizing cluster centroids to a lattice (ϵ∗Z^d + θ), they become stable to small changes. If a removed datapoint doesn't shift the unquantized centroid enough to cross the quantization boundary, the final clusters remain unchanged, allowing "do nothing" unlearning. Core assumption: The quantization parameter ϵ is chosen large enough that removing a single datapoint rarely changes the quantized centroids. Evidence anchors: [abstract] "we show that accurate in-context learning can be done with quantized k-means, which allows for effectively constant time unlearning operations"; [section] "Quantized k-means allows for dataset size independent unlearning of clusters... unlearning can be achieved by doing nothing". Break condition: If ϵ is too small, many datapoint removals will shift centroids enough to change quantized clusters, requiring expensive recomputation.

### Mechanism 2
In-context learning replaces weight fine-tuning, enabling exact unlearning without modifying model parameters. Instead of updating LLM weights through SGD, the method selects representative examples from the fine-tuning dataset to prepend to inputs. Unlearning just requires resampling examples from the dataset without the removed datapoints. Core assumption: The in-context learning algorithm's example selection depends only on the dataset, not on model parameters. Evidence anchors: [abstract] "we can use in-context learning to adapt the LLM to the fine-tuning dataset instead of SGD based algorithms"; [section] "in-context learning algorithms output a set of k examples from the fine-tuning dataset which are then prepended to the input given to an LLM". Break condition: If example selection depends on intermediate model states or learned representations that change with dataset modifications.

### Mechanism 3
Combining quantized k-means with in-context learning creates a method with both dataset-size and model-size independent unlearning costs. ERASE uses quantized k-means clustering on embeddings to select in-context examples. The unlearning cost is O(m^2d^5/2/ϵ) independent of dataset size and model parameters, while inference cost scales only with context length. Core assumption: The clustering-based example selection is accurate enough to match fine-tuning performance while being efficient to unlearn. Evidence anchors: [abstract] "accurate in-context learning can be done with quantized k-means, which allows for effectively constant time unlearning operations"; [section] "ERASE is an unlearning approach that combines in-context learning with quantized k-means clustering... has dataset and model-size independent unlearning operation costs". Break condition: If clustering accuracy degrades significantly compared to fine-tuning, or if quantization parameter tuning becomes task-dependent.

## Foundational Learning

- Concept: Quantized k-means clustering
  - Why needed here: Enables efficient exact unlearning by making cluster centroids stable to small dataset changes through quantization
  - Quick check question: What happens to quantized centroids when a datapoint is removed but the shift is smaller than the quantization parameter?

- Concept: In-context learning mechanism
  - Why needed here: Provides an alternative to parameter fine-tuning that can be exactly unlearned by resampling examples rather than retraining weights
  - Quick check question: How does the example selection process depend on the fine-tuning dataset but not on model parameters?

- Concept: Holistic unlearning cost metric
  - Why needed here: Captures the trade-off between unlearning operation cost and inference cost to determine when a method is more efficient than baseline
  - Quick check question: How does increasing the number of in-context examples affect both unlearning efficiency and inference cost?

## Architecture Onboarding

- Component map:
  Embedding generation (Sentence-BERT) → Quantized k-means clustering → Example selection → Inference with prepended examples
  Unlearning: Check centroid stability → Do nothing or resample examples

- Critical path:
  1. Generate embeddings for all training examples
  2. Run quantized k-means clustering to create k clusters
  3. Select closest example to each centroid as in-context examples
  4. For unlearning: Check if removed datapoint affects quantized centroids

- Design tradeoffs:
  - Larger quantization parameter ϵ: More stable clusters (cheaper unlearning) but potentially less accurate clustering
  - Number of in-context examples k: More examples improve accuracy but increase inference cost
  - Embedding quality: Better embeddings improve clustering but increase computational cost

- Failure signatures:
  - Poor accuracy: Clustering not representative of dataset structure
  - High unlearning cost: Quantization parameter too small, causing frequent centroid changes
  - Slow inference: Too many in-context examples or long example sequences

- First 3 experiments:
  1. Compare ERASE accuracy vs ACoT on 2-3 BigBench tasks with varying ϵ values
  2. Measure unlearning cost scaling with dataset size for different quantization parameters
  3. Benchmark holistic unlearning cost vs SISA for different numbers of in-context examples on representative tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can in-context learning be predicted to be effective for a given task without requiring fine-tuning for comparison?
- Basis in paper: [inferred] The paper notes significant variance in relative performance between tasks and states "we are not aware of work that predicts when in-context learning is accurate (relative to fine-tuning) and we do not have an explanation for our observed variance in competitiveness."
- Why unresolved: Current practice requires fine-tuning to compare performance, which defeats the purpose of using in-context learning for more efficient unlearning. The paper suggests this is "an important open problem for the trustworthy machine learning community."
- What evidence would resolve it: A predictive model or analysis framework that can accurately determine when in-context learning will perform comparably to fine-tuning for a given task and model, validated across multiple tasks and model architectures.

### Open Question 2
- Question: What are the fundamental limits of holistic unlearning cost efficiency in deep learning?
- Basis in paper: [explicit] The paper introduces the concept of holistic unlearning cost and suggests "Further study into this trade-off may be able to provide provable, or even empirically measured, lower-bounds on how holistically efficient unlearning can be in certain settings while still having an accurate learning algorithm."
- Why unresolved: The paper observes a trade-off between inference cost and unlearning operation cost but does not establish theoretical bounds. It suggests this "may be of fundamental interest to the core machine learning community."
- What evidence would resolve it: Theoretical proofs or empirical measurements demonstrating the minimum achievable holistic unlearning cost for specific classes of machine learning algorithms, establishing a trade-off curve between inference and unlearning costs.

### Open Question 3
- Question: How do different unlearning definitions (exact vs approximate) affect the trade-off between inference cost and unlearning operation cost?
- Basis in paper: [inferred] The paper notes that "Perhaps these trade-offs are also affected by the choice of approximate unlearning metric, giving an argument for or against certain metrics" and discusses the distinction between exact and approximate unlearning.
- Why unresolved: The paper focuses on exact unlearning but acknowledges that different unlearning definitions may have different cost structures. It states "We are not aware of work looking into the efficiency trade-off between unlearning for different approximate unlearning metrics."
- What evidence would resolve it: Comparative analysis showing how different unlearning definitions (exact, approximate with various metrics) affect the balance between inference and unlearning costs, potentially revealing that certain definitions enable more efficient trade-offs.

## Limitations
- Quantization parameter selection (ϵ=10^{-3}) is validated on limited tasks, raising questions about generalizability
- Experiments focus on synthetic BigBench tasks rather than real-world unlearning scenarios
- Only tested on LLaMA-7B, leaving scalability questions for larger frontier models

## Confidence

- **High Confidence**: The core mechanism of quantized k-means enabling dataset-size independent unlearning is well-supported by theoretical analysis and experimental evidence. The claim that ERASE achieves lower holistic unlearning costs than SISA variants is strongly validated across multiple metrics.
- **Medium Confidence**: The assertion that ERASE matches or exceeds SISA accuracy holds for the tested BigBench tasks, but may not generalize to all task types. The claim about model-size independent unlearning costs is supported but limited by the single model scale tested.
- **Low Confidence**: The practical viability of ERASE for real-world unlearning requests (e.g., selective removal of specific data points from training) is not demonstrated, as the experiments focus on synthetic dataset modifications.

## Next Checks
1. Evaluate ERASE on diverse real-world datasets (legal documents, medical records, proprietary code) to assess generalization beyond synthetic BigBench tasks, measuring both accuracy retention and unlearning efficiency.
2. Systematically vary the quantization parameter ϵ across multiple orders of magnitude on a subset of tasks to map the accuracy-unlearning cost tradeoff frontier and identify optimal parameter ranges.
3. Test ERASE on LLaMA-70B and LLaMA-13B to verify the claimed model-size independence of unlearning costs, measuring whether the quadratic scaling in embedding dimension remains manageable at larger scales.