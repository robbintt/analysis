---
ver: rpa2
title: 'Even-if Explanations: Formal Foundations, Priorities and Complexity'
arxiv_id: '2401.10938'
source_url: https://arxiv.org/abs/2401.10938
tags:
- semifactual
- explanations
- complexity
- preferences
- semifactuals
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper investigates local post-hoc explainability queries within\
  \ the semifactual \u2018even-if\u2019 thinking and their computational complexity\
  \ among different classes of models. It introduces a preference-based framework\
  \ enabling users to personalize explanations based on their preferences, both in\
  \ the case of semifactuals and counterfactuals."
---

# Even-if Explanations: Formal Foundations, Priorities and Complexity

## Quick Facts
- arXiv ID: 2401.10938
- Source URL: https://arxiv.org/abs/2401.10938
- Reference count: 40
- Linear and tree-based models are strictly more interpretable than neural networks for even-if explanations

## Executive Summary
This paper investigates local post-hoc explainability queries within the semifactual 'even-if' thinking framework, introducing a preference-based system that allows users to personalize explanations according to their preferences. The authors establish formal foundations for even-if explanations and compare their computational complexity against counterfactual explanations across different model classes. The primary finding is that both linear and tree-based models offer superior interpretability compared to neural networks when generating even-if explanations, with several polynomial-time algorithms provided for cases where efficient computation is possible.

## Method Summary
The authors develop a formal framework for even-if explanations by extending counterfactual reasoning to include "even if" scenarios where certain features are changed but others remain fixed. They introduce a preference-based system that allows users to specify priorities among features when generating explanations, applicable to both counterfactual and semifactual settings. The computational complexity of various explainability queries is analyzed across different model classes including FBDDs (Free Binary Decision Diagrams), perceptrons, and MLPs (Multi-Layer Perceptors). The analysis focuses on binary classification tasks with {0,1}^n → {0,1} input-output mappings, providing theoretical bounds and algorithms for polynomial cases.

## Key Results
- Linear and tree-based models are strictly more interpretable than neural networks for even-if explanations
- Several interpretability problems in the preference-based framework are explored with algorithms provided for polynomial cases
- FBDDs and perceptrons are more interpretable than MLPs under both counterfactual and semifactual settings
- The complexity of explainability queries is formally characterized across different model classes

## Why This Works (Mechanism)
The preference-based framework works by allowing users to specify feature priorities when generating explanations, which guides the search for minimal changes needed to achieve desired outcomes. By incorporating user preferences directly into the explanation generation process, the system can produce more relevant and actionable explanations that align with what users actually care about. The semifactual "even-if" approach differs from traditional counterfactuals by considering scenarios where some features change while others remain fixed, providing more nuanced explanations that reflect real-world decision-making processes.

## Foundational Learning
- **Even-if explanations**: Extensions of counterfactual reasoning that allow some features to change while others remain fixed; needed to capture more realistic decision scenarios where not all factors can be manipulated
- **Preference-based personalization**: Framework allowing users to specify feature priorities in explanations; needed to ensure explanations are relevant to user needs and values
- **Computational complexity analysis**: Theoretical characterization of algorithmic efficiency for explanation generation; needed to understand practical feasibility of different explanation approaches
- **Model interpretability classes**: Categorization of models by their explanation generation complexity; needed to guide model selection based on explainability requirements
- **Local post-hoc explanations**: Explanations generated for specific predictions rather than global model behavior; needed for actionable insights about individual decisions
- **Binary classification complexity**: Analysis of {0,1}^n → {0,1} decision problems; needed as foundational case for understanding more complex scenarios

## Architecture Onboarding
**Component Map**: User Preferences -> Explanation Generator -> Model Interface -> Decision Output -> Explanation Result
**Critical Path**: User specifies preferences → System identifies minimal feature changes → Explanation is generated and presented to user
**Design Tradeoffs**: Preference expressiveness vs. computational complexity; model interpretability vs. predictive performance; generality vs. specificity of explanations
**Failure Signatures**: Inability to find explanations when preferences conflict with model constraints; exponential runtime for complex preference structures; explanations that violate user-specified priorities
**3 First Experiments**:
1. Implement polynomial-time algorithm for FBDD explanations with simple preference structures
2. Compare explanation quality between linear models and MLPs under varying preference complexity
3. Test user comprehension of even-if vs. counterfactual explanations with different preference weights

## Open Questions the Paper Calls Out
None

## Limitations
- Complexity results rely heavily on binary classification setting and may not generalize to continuous or multi-class scenarios
- Preference-based framework complexity dimensions may not fully capture real-world user preferences
- Empirical validation of polynomial-time algorithms appears limited to theoretical analysis rather than practical implementation

## Confidence
- High confidence in formal definitions and complexity proofs for specific model classes studied
- Medium confidence in comparative interpretability claims between model types, dependent on restricted problem setting
- Low confidence in practical applicability of preference-based framework without user studies or real-world validation

## Next Checks
1. Implement the proposed polynomial-time algorithms and test on real-world datasets to verify practical runtime performance matches theoretical complexity bounds
2. Conduct user studies to validate whether the preference-based framework actually improves explanation quality and user understanding compared to standard counterfactual explanations
3. Extend the complexity analysis to continuous feature spaces and multi-class classification to assess generalizability beyond the binary setting used in the paper