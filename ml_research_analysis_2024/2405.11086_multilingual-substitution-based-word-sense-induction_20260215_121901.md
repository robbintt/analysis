---
ver: rpa2
title: Multilingual Substitution-based Word Sense Induction
arxiv_id: '2405.11086'
source_url: https://arxiv.org/abs/2405.11086
tags:
- word
- substitutes
- sense
- methods
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes multilingual substitution-based methods for
  Word Sense Induction (WSI) that work seamlessly across 100 languages with minimal
  adaptation. The methods generate multi-token lexical substitutes using multilingual
  masked language models (MLMs) like XLM-R, with target injection via symmetric dynamic
  patterns or similarity-based reranking (+embs).
---

# Multilingual Substitution-based Word Sense Induction

## Quick Facts
- arXiv ID: 2405.11086
- Source URL: https://arxiv.org/abs/2405.11086
- Reference count: 17
- Primary result: Proposes multilingual WSI methods working across 100 languages with monolingual fine-tuning enabling cross-lingual substitute generation

## Executive Summary
This paper introduces multilingual substitution-based methods for Word Sense Induction (WSI) that work seamlessly across 100 languages with minimal adaptation. The approach uses multilingual masked language models like XLM-R to generate multi-token lexical substitutes with target injection via symmetric dynamic patterns or similarity-based reranking. A key finding is that monolingual fine-tuning of WCM on English enables cross-lingual substitute generation that performs well on WSI tasks in other languages. The methods achieve competitive results on English benchmarks (SE10, SE13) compared to state-of-the-art monolingual approaches and show strong multilingual performance across 11 languages including Russian, German, and Chinese.

## Method Summary
The approach generates multi-token lexical substitutes using multilingual masked language models (MLMs) like XLM-R, with target injection via symmetric dynamic patterns (SDP) or similarity-based reranking (+embs). The method uses a concat generator that creates 1-3 consecutive masks to produce better substitutes than single masks for XLM-R. Monolingual fine-tuning of WCM on a single language (e.g., English) enables cross-lingual substitute generation that performs well on WSI tasks in other languages. The pipeline includes lemmatization of substitutes, TF-IDF vectorization, and agglomerative clustering with Calinski-Harabasz score optimization for cluster number selection.

## Key Results
- Monolingual fine-tuning of WCM on English enables cross-lingual lexical substitute generation for WSI across 100 languages
- WCM-en SDP-en achieves competitive performance on English benchmarks (SE10, SE13) compared to state-of-the-art monolingual approaches
- SDP-generated substitutes often include context-co-occurring words rather than traditional lexical substitutes, improving WSI clustering despite being weaker on direct lexical substitution tasks
- The best configuration (WCM-en SDP-en) requires no translation or lemmatizer adaptation for new languages, making it truly multilingual

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Monolingual fine-tuning of WCM on English enables cross-lingual lexical substitute generation for WSI.
- Mechanism: When WCM is fine-tuned on monolingual text (English or Russian), it learns to generate coherent substitutes in the finetuning language regardless of the input language, leveraging the multilingual coverage of XLM-R.
- Core assumption: The semantic relationships captured during monolingual finetuning generalize across languages when the input is processed through a multilingual model.
- Evidence anchors:
  - [section 3.1.3] "WCM-en and WCM-ru always yield substitutes in the finetuning language, not in the language of the input text... these substitutes often constitute a reasonable description of the meaning of the target word."
  - [section 4.2.1] "WCM-en is the second best-performing configuration on the Russian benchmark bts-rnc-ru-test-private while producing only English substitutes."

### Mechanism 2
- Claim: SDP-generated substitutes often include context-co-occurring words rather than traditional lexical substitutes, improving WSI clustering.
- Mechanism: Dynamic patterns inject target information while preserving context, leading to substitutes that capture broader semantic contexts and co-occurrences useful for clustering.
- Core assumption: Words that frequently co-occur with the target word in similar contexts help distinguish senses better than direct synonyms alone.
- Evidence anchors:
  - [section 5] "SDP substitutes still allow for better clustering according to WSI evaluation" despite having "unknown relation" more frequently than +embs.
  - [section 5] Examples like "blood" and "animal" help distinguish "bloodcell" from "prisoncell" but aren't traditional lexical substitutes.

### Mechanism 3
- Claim: Using 1-3 consecutive masks in Concat generator produces better substitutes than single masks for XLM-R.
- Mechanism: XLM-R's tokenization often splits words into multiple subwords, so generating multi-token substitutes captures more complete word forms and improves substitute quality.
- Core assumption: The quality of lexical substitutes depends on generating complete word forms rather than fragments when the underlying tokenizer produces multi-token words.
- Evidence anchors:
  - [section 3.1] "BERT LSDP method only produces single-subword substitutes, thus limiting the diversity of substitutes yielded by XLM-R."
  - [section C] "We see a notable benefit in using variable length substitutes (1,2,3 masks) in SE10, SE13, bts-rnc-ru and DWUG de Sense."

## Foundational Learning

- Concept: Masked Language Models (MLMs) and their limitations for lexical substitution
  - Why needed here: Understanding why vanilla MLMs produce general substitutes that don't relate to target words
  - Quick check question: Why does replacing a target word with a single mask token in BERT produce substitutes unrelated to the original word?

- Concept: Word Sense Induction vs Word Sense Disambiguation
  - Why needed here: WSI discovers senses without predefined inventories, while WSD classifies into existing senses
  - Quick check question: What's the key difference between WSI and WSD in terms of available resources?

- Concept: Cross-lingual representation learning and transfer
  - Why needed here: Explains how monolingual finetuning can produce cross-lingual substitutes through multilingual models
  - Quick check question: How can a model finetuned on English produce meaningful substitutes for Russian text?

## Architecture Onboarding

- Component map:
  Input processor → Substitute generator (Concat/WCM) → Target injection (SDP/+embs) → Lemmatizer → TF-IDF vectorizer → Clustering algorithm
  Multilingual masked language model (XLM-R) is the core engine
  External components: Stanza lemmatizer, FastText embeddings for +embs

- Critical path:
  1. Text preprocessing and target word identification
  2. Substitute generation with chosen method
  3. Target injection conditioning
  4. Lemmatization of substitutes
  5. TF-IDF vector construction
  6. Clustering with Calinski-Harabasz optimization

- Design tradeoffs:
  - Single vs multi-token substitutes: Multi-token captures complete words but increases sparsity
  - Target injection methods: SDP captures broader context but +embs produces more related substitutes
  - Number of substitutes (k): Higher k improves coverage but increases computational cost

- Failure signatures:
  - Poor clustering quality: Check if substitutes are semantically relevant and lemmatizer is working correctly
  - All instances clustered together: Likely insufficient discriminative substitutes or poor target injection
  - High variance across languages: May indicate multilingual model limitations for specific language pairs

- First 3 experiments:
  1. Test WCM-en SDP with a small multilingual dataset to verify cross-lingual substitute generation
  2. Compare ARI scores of different target injection methods on a single language benchmark
  3. Evaluate the impact of k (number of substitutes) on clustering quality for both SDP and +embs methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different target injection methods (SDP vs. +embs) compare in generating substitutes that are useful for clustering but not necessarily semantic substitutes?
- Basis in paper: The paper observes that SDP performs better than +embs for WSI despite +embs being superior on direct lexical substitution tasks, suggesting SDP generates substitutes useful for clustering but not necessarily semantic substitutes.
- Why unresolved: The paper does not provide a detailed analysis of the specific types of substitutes generated by each method or how these contribute differently to clustering performance.
- What evidence would resolve it: A detailed comparison of the substitutes generated by SDP and +embs, including their semantic relatedness to the target word and their impact on clustering quality, would clarify this distinction.

### Open Question 2
- Question: How do the multilingual capabilities of WCM-en and WCM-ru compare to monolingual WCM in terms of substitute generation quality and clustering performance across different languages?
- Basis in paper: The paper demonstrates that WCM-en and WCM-ru can generate cross-lingual substitutes, but it does not provide a direct comparison with monolingual WCM in terms of substitute quality or clustering performance.
- Why unresolved: The paper focuses on the cross-lingual capabilities of WCM-en and WCM-ru but does not compare them directly to monolingual WCM to assess any potential trade-offs in quality.
- What evidence would resolve it: A comparative study evaluating the quality of substitutes generated by WCM-en, WCM-ru, and monolingual WCM across multiple languages, along with their impact on clustering performance, would address this question.

### Open Question 3
- Question: How does the number of masks used in the concat approach affect the quality and diversity of generated substitutes, and what is the optimal number for different languages and datasets?
- Basis in paper: The paper mentions that the concat approach uses 1 to 3 masks and shows some benefit over fixed-length substitutes, but it does not explore the impact of varying the number of masks on substitute quality and diversity.
- Why unresolved: The paper does not provide a detailed analysis of how the number of masks influences the generated substitutes or how this varies across languages and datasets.
- What evidence would resolve it: An experimental study varying the number of masks in the concat approach and evaluating the resulting substitute quality, diversity, and clustering performance across different languages and datasets would provide insights into the optimal number of masks.

## Limitations

- Computational cost of multi-token substitute generation using concat approach, particularly for large k values
- Reliance on lemmatization tools that may not exist or be accurate for truly low-resource languages
- Evaluation focused on 11 languages, leaving performance on the full 100-language spectrum unexplored

## Confidence

**High Confidence Claims:**
- The general superiority of monolingual fine-tuning approach for cross-lingual substitute generation
- The effectiveness of multi-token substitute generation for XLM-R compared to single-token approaches
- The competitive performance of the proposed methods on English benchmarks (SE10, SE13)

**Medium Confidence Claims:**
- The strong multilingual performance across the 11 tested languages
- The relative performance of SDP vs +embs methods across different languages
- The claim that WCM-en SDP-en requires no adaptation for new languages

**Low Confidence Claims:**
- The scalability of the approach to the full 100-language set
- The performance consistency across languages with very different linguistic structures
- The computational feasibility for large-scale deployment without optimization

## Next Checks

1. **Computational Efficiency Validation**: Benchmark the time complexity of multi-token substitute generation (concat approach) across different k values and language pairs. Measure the trade-off between substitute quality and computational cost, and test whether approximation methods or beam search could maintain quality while reducing computation time.

2. **Cross-Lingual Transfer Robustness**: Systematically test the WCM-en fine-tuning approach on a diverse set of 20+ languages spanning different language families (e.g., including African, Indigenous American, and Pacific languages). Measure the correlation between source-target language similarity and WSI performance to identify the limits of cross-lingual transfer.

3. **Lemmatizer-Independent Performance**: Implement a version of the pipeline without lemmatization (using raw substitutes directly) and compare clustering performance across languages. This would validate whether the "no lemmatizer adaptation needed" claim holds when considering the full pipeline performance, not just substitute generation.