---
ver: rpa2
title: Large language model empowered participatory urban planning
arxiv_id: '2402.01698'
source_url: https://arxiv.org/abs/2402.01698
tags:
- planning
- area
- urban
- participatory
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a participatory urban planning framework
  leveraging Large Language Models (LLMs) to address challenges in traditional participatory
  planning methods. The approach employs LLM agents in role-play, collaborative generation,
  and feedback iteration to generate land-use plans while incorporating diverse stakeholder
  perspectives.
---

# Large language model empowered participatory urban planning

## Quick Facts
- arXiv ID: 2402.01698
- Source URL: https://arxiv.org/abs/2402.01698
- Authors: Zhilun Zhou; Yuming Lin; Yong Li
- Reference count: 40
- One-line primary result: LLM agents effectively generate participatory land-use plans matching RL performance while achieving high satisfaction and inclusion metrics.

## Executive Summary
This study introduces a participatory urban planning framework leveraging Large Language Models (LLMs) to address challenges in traditional participatory planning methods. The approach employs LLM agents in role-play, collaborative generation, and feedback iteration to generate land-use plans while incorporating diverse stakeholder perspectives. Tested on two Beijing communities with 1000+ virtual residents, the method achieved high satisfaction and inclusion metrics (0.78 and 0.77) while matching RL performance in service and ecology metrics. The framework demonstrates LLMs' potential to enhance participatory planning through natural language reasoning, scalability, and transparency, offering benefits for both planners and citizens through efficient, low-cost planning support.

## Method Summary
The framework uses LLM agents in role-play to simulate stakeholder behavior, with a Chief Planner (CP) generating initial proposals, Sub-Community Planners (SPs) moderating resident discussions, and resident agents providing input based on their profiles. The community is partitioned into sub-communities for efficient deliberation, and feedback iteration ensures plans meet minimum requirements while incorporating participatory input. The approach was tested on two Beijing communities with 1000+ virtual residents, comparing results against baselines using metrics for service accessibility, ecology, satisfaction, and inclusion.

## Key Results
- LLM-based framework achieved satisfaction metric of 0.78 and inclusion metric of 0.77 on two Beijing communities
- Performance matched reinforcement learning baselines on service (0.75) and ecology (0.76) metrics
- Framework demonstrated efficient participatory planning with natural language reasoning and transparent stakeholder representation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM agents can emulate stakeholder behavior accurately enough for participatory planning simulations.
- Mechanism: Role-play prompts enable LLMs to generate consistent profiles (age, gender, family size, background) and produce land-use needs aligned with those profiles, allowing scalable resident representation without manual interviews.
- Core assumption: The LLM's pretraining on web-scale text has captured sufficient implicit knowledge of human preferences and social roles to generate realistic demand patterns.
- Evidence anchors:
  - [abstract] "LLM agents can carry out complex reasoning and interactions [14], and even run a virtual town with events like birth party and mayor election [15]."
  - [section 5.1] "LLM agents can effectively comprehend human profiles and appropriately role-play based on their backgrounds, catering to individual-level demands and joining the participatory process."
  - [corpus] Found 25 related papers; average neighbor FMR=0.324; neighbor titles include "Large Language Model for Participatory Urban Planning" and "Intelli-Planner: Towards Customized Urban Planning via Large Language Model Empowered Reinforcement Learning."
- Break condition: Role-play outputs diverge from realistic demand patterns or fail to maintain consistent stakeholder profiles across iterations.

### Mechanism 2
- Claim: Hierarchical sub-community structure enables efficient, scalable deliberation among diverse agents.
- Mechanism: The community is partitioned into sub-communities; each sub-community planner (SP) moderates discussions among resident agents, aggregates preferences, and feeds back revisions to the chief planner (CP), enabling parallel processing of resident input.
- Core assumption: Partitioning based on road networks and residential addresses preserves locality of interests and reduces computational overhead while maintaining representativeness.
- Evidence anchors:
  - [section 5.2] "We partition the entire community into four distinct sub-communities based on the main road network. Resident agents participate in corresponding discussion groups according to the addresses assigned to them."
  - [section 3.3] "Resident agents participate in corresponding discussion groups according to the addresses assigned to them. These discussions encompass not only the rebuttals of existing proposals but also involve residents engaging in mutual discussions, disputes, and persuasion based on divergent interests."
  - [corpus] Average citations=0.0 (weak signal); however, multiple neighbor papers focus on multi-agent collaboration and participatory sensing.
- Break condition: Partitioning fails to capture cross-sub-community dependencies or creates echo chambers that skew final plans.

### Mechanism 3
- Claim: Feedback iteration loop corrects constraint violations without losing resident-driven improvements.
- Mechanism: After each round of SP-driven revisions, the plan is checked against minimum requirement constraints (e.g., minimum number of schools, hospitals); violations are translated into natural language prompts for the CP to adjust while preserving resident preferences.
- Core assumption: The CP agent can interpret constraint violation prompts and adjust the plan without undermining the participatory gains achieved in prior iterations.
- Evidence anchors:
  - [section 3.4] "Violations of requirements are translated into a natural language template, prompting CP to make corresponding adjustments."
  - [section 5.3] "Violations of requirements are translated into a natural language template, prompting CP to make corresponding adjustments."
  - [section 4.2] "To ensure that the communities are realistic and well-planned, we established some basic requirements. For both communities, a minimum number of plots are mandated to ensure adequate infrastructure coverage."
- Break condition: Constraint correction repeatedly overrides resident preferences, reducing satisfaction and inclusion metrics.

## Foundational Learning

- Concept: Role-based prompt engineering for LLM agents.
  - Why needed here: Enables LLMs to act as distinct stakeholders (CP, SP, residents) with consistent behavior aligned to their assigned profiles.
  - Quick check question: Can you design a prompt template that makes an LLM act as a "single elderly person living alone" and request healthcare facilities first?

- Concept: Multi-agent collaborative reasoning.
  - Why needed here: Allows resident agents to debate and negotiate within sub-communities, producing richer input than isolated preference lists.
  - Quick check question: How would you structure an SP's prompt to summarize conflicting resident opinions and propose a compromise plan?

- Concept: Constraint-aware iterative refinement.
  - Why needed here: Ensures generated plans meet minimum infrastructure requirements while incorporating participatory input.
  - Quick check question: What natural language feedback would you give a CP agent if the current plan lacks the required number of hospital plots?

## Architecture Onboarding

- Component map: Input (community map, resident profiles, requirements) -> Role-play module (CP, SP, resident agents) -> Collaborative generation module (SP discussions -> CP revision) -> Feedback iteration module (constraint check -> CP adjustment) -> Output (final plan + metrics)
- Critical path: 1. CP generates initial proposal. 2. SPs partition residents → collect preferences. 3. Residents discuss → SPs revise. 4. CP incorporates revisions. 5. Constraint check → CP adjusts if needed. 6. Repeat until constraints met and satisfaction maximized.
- Design tradeoffs:
  - Partitioning granularity vs. computation: finer partitions increase realism but cost more tokens.
  - Constraint strictness vs. flexibility: tighter requirements reduce plan space but improve feasibility.
  - Prompt verbosity vs. latency: detailed prompts improve output quality but slow iteration.
- Failure signatures:
  - Satisfaction metrics plateau despite more iterations → possible echo chamber.
  - Constraint violations persist → CP not updating correctly or constraints too strict.
  - Resident agents produce unrealistic demands → role-play prompt insufficient.
- First 3 experiments:
  1. Run CP alone (no residents) to measure baseline service/ecology metrics.
  2. Run CP + resident input without sub-community partitioning to assess impact of parallelization.
  3. Run full framework with 100 resident agents (vs. 1000) to test scalability and sensitivity to agent count.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the proposed LLM-based framework be extended to handle more complex urban planning scenarios, such as those involving ownership, development costs, and community visions?
- Basis in paper: [inferred] The paper acknowledges the need to address these factors in future work, but does not provide a clear methodology for doing so.
- Why unresolved: The current framework is limited to land-use planning and does not account for the complex interplay of various factors that influence real-world urban planning decisions.
- What evidence would resolve it: A demonstration of the framework's ability to handle more complex scenarios, including the incorporation of ownership, development costs, and community visions, would provide evidence of its extensibility.

### Open Question 2
- Question: To what extent can LLM agents authentically emulate human behavior in urban planning scenarios, and how can this be improved?
- Basis in paper: [explicit] The paper acknowledges the need for further advancements in computer technology to improve the authenticity of LLM agent behavior.
- Why unresolved: The current framework relies on LLM agents to emulate human behavior, but the extent to which these agents can accurately represent human decision-making is not well-established.
- What evidence would resolve it: A comparison of the framework's performance with that of human planners in real-world urban planning scenarios would provide evidence of the authenticity of LLM agent behavior.

### Open Question 3
- Question: How can the framework be adapted to ensure citizen engagement in the participatory planning process, beyond the role-playing and collaborative generation modules?
- Basis in paper: [explicit] The paper emphasizes the importance of citizen engagement in participatory planning and acknowledges the need for further research in this area.
- Why unresolved: The current framework focuses on the use of LLM agents to facilitate participatory planning, but does not provide a clear strategy for ensuring citizen engagement beyond the initial role-playing and collaborative generation stages.
- What evidence would resolve it: A demonstration of the framework's ability to incorporate citizen feedback and preferences throughout the planning process would provide evidence of its effectiveness in ensuring citizen engagement.

## Limitations
- Framework performance depends heavily on LLM prompt quality and model choice, which are not disclosed
- Study uses synthetic resident profiles rather than real community input, limiting ecological validity
- Lacks comparison with actual human participatory outcomes to validate real-world acceptability

## Confidence

- **High confidence** in the theoretical feasibility of using LLMs for participatory urban planning (supported by prior work on LLM agents in virtual environments)
- **Medium confidence** in the specific architectural approach (sub-community partitioning and feedback iteration) due to reasonable design but lack of comparative analysis against alternative LLM-based architectures
- **Low confidence** in the generalizability of results to different cultural contexts or larger urban areas, given the study's focus on two specific Beijing communities

## Next Checks

1. Conduct a controlled experiment comparing LLM-generated plans with those from actual community workshops in the same neighborhoods, measuring both quantitative metrics and qualitative resident feedback
2. Test the framework with different LLM models (e.g., GPT-3.5 vs. GPT-4) and prompt variations to establish sensitivity to model choice and prompt engineering quality
3. Validate the role-play mechanism by having domain experts review a sample of resident agent outputs to assess whether generated demands align with realistic human preferences in urban planning contexts