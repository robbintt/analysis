---
ver: rpa2
title: Can Large Language Models Write Parallel Code?
arxiv_id: '2401.12554'
source_url: https://arxiv.org/abs/2401.12554
tags:
- code
- parallel
- llms
- pass
- openmp
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents ParEval, a benchmark for evaluating large language
  models (LLMs) on parallel code generation tasks. ParEval consists of 420 prompts
  covering 12 problem types and 7 execution models (serial, OpenMP, MPI, Kokkos, CUDA,
  HIP).
---

# Can Large Language Models Write Parallel Code?

## Quick Facts
- **arXiv ID:** 2401.12554
- **Source URL:** https://arxiv.org/abs/2401.12554
- **Reference count:** 40
- **Key outcome:** ParEval benchmark shows LLMs perform significantly worse on parallel code generation (39.6% pass rate) compared to serial code (76.0% pass rate)

## Executive Summary
This paper introduces ParEval, a comprehensive benchmark for evaluating large language models on parallel code generation tasks. The benchmark covers 12 problem types across 7 execution models (serial, OpenMP, MPI, Kokkos, CUDA, HIP) with 420 total prompts. The authors develop novel metrics (speedup@k, efficiency@k) to assess both correctness and performance of generated parallel code. Through extensive experiments with multiple state-of-the-art LLMs, the study reveals that while LLMs can generate serial code with reasonable accuracy, their performance drops significantly when generating parallel code, particularly for MPI and sparse, unstructured problems.

## Method Summary
The authors created ParEval, a benchmark consisting of 420 prompts that cover 12 distinct problem types (including dense linear algebra, graph algorithms, and stencil computations) across 7 execution models. They developed two novel metrics: speedup@k measures performance relative to the serial version using the best of k attempts, while efficiency@k normalizes this by the number of parallel units. The benchmark was evaluated using several state-of-the-art LLMs including GPT-3.5, GPT-4, and open-source models. Generated code was compiled and tested for correctness and performance, with results compared against human programmers and existing solutions.

## Key Results
- GPT-3.5 achieved the best results with 76.0% pass@1 for serial code generation and 39.6% for parallel code generation
- All LLMs performed significantly worse on parallel code compared to serial code across all execution models
- MPI code generation proved most challenging for LLMs, with consistently lower pass rates than other execution models
- Generated parallel code often showed poor efficiency, indicating suboptimal use of parallel resources

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive coverage of parallel programming paradigms and its novel metrics that capture both correctness and performance aspects of generated code. By testing across multiple execution models and problem types, ParEval provides a holistic assessment of LLM capabilities in parallel programming.

## Foundational Learning
- **Parallel programming paradigms**: Understanding different execution models (OpenMP, MPI, CUDA, etc.) is essential for evaluating LLM performance across parallel computing domains
- **Performance metrics for parallel code**: Speedup and efficiency metrics are needed to assess not just correctness but also the quality of parallelization
- **Benchmark design principles**: Creating comprehensive test suites that cover diverse problem types and execution models ensures robust evaluation
- **LLM code generation evaluation**: Developing systematic methods to assess generated code for both correctness and performance

## Architecture Onboarding
**Component Map:** Problem types -> Execution models -> LLM code generation -> Compilation & testing -> Performance evaluation
**Critical Path:** Prompt generation → LLM response → Code compilation → Correctness testing → Performance benchmarking
**Design Tradeoffs:** Comprehensive coverage vs. evaluation complexity; automated testing vs. human oversight; metric sophistication vs. interpretability
**Failure Signatures:** Compilation errors indicate syntax issues; runtime failures suggest logic errors; poor performance reveals inefficient parallelization strategies
**3 First Experiments:** 1) Test LLM performance on simple vs. complex problem types within the same execution model, 2) Compare pass rates across different execution models for the same problem, 3) Evaluate the impact of providing reference implementations on code generation quality

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses exclusively on C++ code, leaving unclear whether results generalize to other programming languages
- The evaluation uses simplified versions of benchmark problems (e.g., excluding edge cases), which may overestimate LLM capabilities
- The metrics introduced only assess the best of k attempts, potentially masking systematic performance issues

## Confidence
- **High**: LLMs perform worse on parallel code than serial code (supported by consistent experimental results)
- **Medium**: MPI is the most challenging execution model (supported but could be influenced by problem selection)
- **Medium**: Providing implementations in one model helps improve code generation in another model (supported but needs further validation)

## Next Checks
1. Test the benchmark on additional programming languages (Python, Fortran) to assess language dependency of the results
2. Expand evaluation to include more comprehensive versions of benchmark problems with edge cases and error handling
3. Conduct a larger-scale comparison with experienced parallel programmers across multiple institutions to better assess the human baseline