---
ver: rpa2
title: 'EdgeFusion: On-Device Text-to-Image Generation'
arxiv_id: '2404.11925'
source_url: https://arxiv.org/abs/2404.11925
tags:
- data
- edgefusion
- dataset
- l-pps
- synt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the computational burden of Stable Diffusion
  for text-to-image generation on resource-constrained devices like NPUs. The core
  method involves starting with a compact SD variant, BK-SDM-Tiny, and applying advanced
  distillation techniques using high-quality synthetic image-text pairs and improved
  step distillation for LCM.
---

# EdgeFusion: On-Device Text-to-Image Generation

## Quick Facts
- arXiv ID: 2404.11925
- Source URL: https://arxiv.org/abs/2404.11925
- Reference count: 39
- Primary result: Enables two-step, sub-second text-to-image generation on NPUs with improved quality metrics

## Executive Summary
EdgeFusion addresses the computational challenges of deploying Stable Diffusion on resource-constrained devices like NPUs by optimizing a compact BK-SDM-Tiny variant. The approach leverages advanced distillation techniques using synthetic image-text pairs and improved step distillation for Latent Consistency Models (LCM). This enables rapid, high-quality image generation with significantly reduced latency while maintaining text-image alignment. The method demonstrates substantial improvements in image quality metrics compared to baseline approaches, validated through both quantitative measures and human evaluation.

## Method Summary
The paper starts with BK-SDM-Tiny, a compact variant of Stable Diffusion, and applies advanced distillation techniques to optimize performance on edge devices. The method uses high-quality synthetic image-text pairs for training and implements improved step distillation specifically for LCMs. This enables the model to generate photo-realistic, text-aligned images in just two steps with latency under one second on NPUs. The approach focuses on balancing computational efficiency with image quality through careful architectural choices and training methodology.

## Key Results
- Achieves photo-realistic image generation in two steps with sub-second latency on NPUs
- Significant improvements in image quality metrics (IS, FID, CLIP) compared to vanilla approach
- Human evaluation confirms superior text-image alignment and overall quality

## Why This Works (Mechanism)
The effectiveness stems from the combination of starting with a compact model architecture (BK-SDM-Tiny) and applying targeted distillation techniques. By using synthetic image-text pairs for training, the model learns to generate high-quality images with minimal computational steps. The improved step distillation for LCMs allows the model to maintain quality while reducing the number of inference steps from typical values to just two. This optimization is particularly effective on NPUs due to the model's reduced complexity and the hardware's parallel processing capabilities.

## Foundational Learning

**Latent Diffusion Models (LDM)**: Why needed - Core framework for text-to-image generation; Quick check - Understanding noise prediction in latent space
**Knowledge Distillation**: Why needed - Reduces model complexity while preserving quality; Quick check - Temperature scaling and loss functions
**CLIP Embeddings**: Why needed - Ensures text-image alignment; Quick check - Text encoder and image encoder architecture
**NPU Architecture**: Why needed - Understanding hardware constraints and optimizations; Quick check - Memory hierarchy and compute units
**Synthetic Data Generation**: Why needed - Provides high-quality training pairs at scale; Quick check - Data augmentation techniques

## Architecture Onboarding

**Component Map**: BK-SDM-Tiny -> Diffusion UNet -> CLIP Text Encoder -> Image Generator
**Critical Path**: Text encoding → Latent space manipulation → Image decoding → Post-processing
**Design Tradeoffs**: Reduced model size (faster inference) vs. potential quality loss (mitigated by distillation)
**Failure Signatures**: Text-image misalignment, visual artifacts in generated images, excessive computational latency
**First Experiments**:
1. Validate basic text-to-image generation with default parameters
2. Test inference latency on target NPU hardware
3. Compare quality metrics against baseline Stable Diffusion

## Open Questions the Paper Calls Out
None

## Limitations
- Limited quantitative comparisons with vanilla Stable Diffusion
- Reliance on synthetic data may create domain gaps
- Architectural details not fully disclosed, limiting reproducibility

## Confidence
High: Claims of sub-second latency and quality improvements
Medium: Lack of published quantitative metrics (FID, IS, CLIP scores)
Medium: Subjective human evaluation as primary validation method

## Next Checks
1. Obtain and compare quantitative image quality metrics (FID, IS, CLIP) between EdgeFusion and the vanilla Stable Diffusion approach
2. Conduct a reproducible, blind human evaluation study with a larger and more diverse set of prompts to validate subjective quality improvements
3. Test the model's performance and robustness using real-world, non-synthetic image-text pairs to assess potential domain gaps