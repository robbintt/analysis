---
ver: rpa2
title: Heavy-Ball Momentum Accelerated Actor-Critic With Function Approximation
arxiv_id: '2408.06945'
source_url: https://arxiv.org/abs/2408.06945
tags:
- policy
- gradient
- critic
- where
- convergence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a heavy-ball momentum accelerated actor-critic
  (HB-A2C) algorithm for reinforcement learning with function approximation. The key
  idea is to integrate heavy-ball momentum into the critic recursion to improve convergence
  rate.
---

# Heavy-Ball Momentum Accelerated Actor-Critic With Function Approximation

## Quick Facts
- arXiv ID: 2408.06945
- Source URL: https://arxiv.org/abs/2408.06945
- Authors: Yanjie Dong; Haijun Zhang; Gang Wang; Shisheng Cui; Xiping Hu
- Reference count: 38
- Primary result: O(1/√K) convergence rate without decaying variance assumptions

## Executive Summary
This paper introduces the Heavy-Ball Momentum Accelerated Actor-Critic (HB-A2C) algorithm that integrates heavy-ball momentum into the critic update to accelerate convergence in reinforcement learning with function approximation. The algorithm aims to find ε-approximate stationary points with O(ε⁻²) iterations under Markovian noise, improving upon previous actor-critic methods that required decaying variance assumptions. The theoretical analysis establishes finite-time convergence guarantees and reveals how momentum factors can balance initialization errors and stochastic approximation errors.

## Method Summary
HB-A2C modifies the standard actor-critic framework by incorporating heavy-ball momentum into the critic recursion. The algorithm uses two timescales for actor and critic updates, with the critic benefiting from momentum to accelerate convergence. The theoretical analysis assumes stationary policies and known state-transition dynamics, providing convergence guarantees without requiring decaying variance assumptions for Markovian noise. The learning rate schedule and momentum factor are shown to play crucial roles in balancing different error sources in the stochastic approximation process.

## Key Results
- Achieves O(1/√K) convergence rate without decaying variance assumptions
- Finds ε-approximate stationary points in O(ε⁻²) iterations under Markovian noise
- Reveals dependence of learning rates on trajectory length
- Shows momentum factor selection can balance initialization and stochastic approximation errors

## Why This Works (Mechanism)
The heavy-ball momentum accelerates convergence by accumulating past gradients to smooth the optimization trajectory and escape shallow local minima. This momentum term helps the critic update converge faster by providing inertia that reduces oscillations and dampens the effects of noisy gradients. The two-timescale update structure allows the actor to adapt while the critic benefits from accelerated convergence, creating a more efficient learning dynamics than standard actor-critic methods.

## Foundational Learning

**Markov Decision Process (MDP)** - The standard framework for sequential decision making where states transition according to stochastic dynamics. *Why needed:* HB-A2C assumes sample trajectories from MDPs, making this foundational. *Quick check:* Verify understanding of states, actions, rewards, and transition probabilities.

**Stochastic Approximation** - Iterative algorithms that converge despite noisy gradient estimates. *Why needed:* RL inherently involves stochastic gradients from sampled trajectories. *Quick check:* Can you explain Robbins-Monro conditions for convergence?

**Two-Timescale Stochastic Approximation** - Separate learning rates for different components to enable stable convergence. *Why needed:* Allows actor and critic to update at different speeds for better convergence properties. *Quick check:* Understand how slower critic vs faster actor helps convergence.

**Heavy-Ball Momentum Method** - Optimization technique using momentum to accelerate convergence. *Why needed:* Core innovation that accelerates critic updates. *Quick check:* Can you derive the heavy-ball update rule?

**Stationary Policy Assumption** - Policy that doesn't change during learning phase. *Why needed:* Simplifies theoretical analysis by decoupling policy improvement from policy evaluation. *Quick check:* Know why this is restrictive for practical RL.

## Architecture Onboarding

**Component Map:** MDP environment -> Critic with heavy-ball momentum -> Actor -> Policy update loop

**Critical Path:** State observation → Critic evaluation with momentum → Policy gradient computation → Actor update → New policy

**Design Tradeoffs:** Stationary policy assumption simplifies analysis but limits practical applicability; momentum helps convergence but requires careful tuning of momentum factor.

**Failure Signatures:** Slow convergence suggests poor momentum factor selection; instability indicates learning rates too high; poor performance may stem from critic-actor interaction issues.

**3 First Experiments:** 1) Compare HB-A2C vs standard actor-critic on simple MDPs with known dynamics. 2) Test different momentum factors to find optimal balance empirically. 3) Validate convergence on continuous control tasks with neural network function approximation.

## Open Questions the Paper Calls Out
None

## Limitations

- Assumes stationary policies and known transition dynamics, limiting real-world applicability
- Theoretical analysis focuses on critic update while treating actor separately, potentially missing important interactions
- Requires careful tuning of momentum factor and learning rates for optimal performance
- Does not account for function approximation errors from neural networks

## Confidence

**Convergence analysis:** Medium - strong theoretical framework but limited to specific assumptions
**O(1/√K) convergence rate claim:** Medium - theoretical but lacks empirical validation
**Momentum acceleration benefits:** Low - theoretical claims not yet experimentally verified

## Next Checks

1. Implement and test HB-A2C with neural network function approximators on standard continuous control benchmarks (e.g., MuJoCo tasks) to verify practical convergence benefits
2. Conduct ablation studies comparing different momentum factors to validate the theoretical recommendation about balancing initialization and stochastic approximation errors
3. Extend the analysis to non-stationary policies and unknown transition dynamics to assess real-world applicability