---
ver: rpa2
title: 'HILL: Hierarchy-aware Information Lossless Contrastive Learning for Hierarchical
  Text Classification'
arxiv_id: '2403.17307'
source_url: https://arxiv.org/abs/2403.17307
tags:
- learning
- information
- text
- encoder
- coding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces HILL, a contrastive learning approach for
  hierarchical text classification (HTC) that avoids data augmentation by directly
  generating positive samples using a structure encoder. The structure encoder extracts
  essential syntactic information from label hierarchies through structural entropy
  minimization and injects it into text representations via hierarchical representation
  learning.
---

# HILL: Hierarchy-aware Information Lossless Contrastive Learning for Hierarchical Text Classification

## Quick Facts
- arXiv ID: 2403.17307
- Source URL: https://arxiv.org/abs/2403.17307
- Authors: He Zhu; Junran Wu; Ruomei Liu; Yue Hou; Ze Yuan; Shangzhe Li; Yicheng Pan; Ke Xu
- Reference count: 33
- Key outcome: Achieves state-of-the-art performance on three HTC datasets with average improvements of 1.85% and 3.38% in Micro-F1 and Macro-F1 compared to vanilla BERT

## Executive Summary
HILL introduces an information lossless contrastive learning approach for hierarchical text classification that avoids data augmentation by directly generating positive samples through a structure encoder. The framework combines a text encoder (BERT) with a structure encoder that extracts essential syntactic information from label hierarchies using structural entropy minimization and injects it into text representations via hierarchical representation learning. Theoretical analysis proves that HILL retains the maximum possible information compared to any augmentation-based method.

## Method Summary
HILL operates by encoding documents with BERT and generating positive samples using a structure encoder that constructs coding trees from label hierarchies. The structure encoder initializes leaf nodes with document embeddings and propagates information upward through multiple layers of feedforward networks, creating a multi-granular representation that fuses semantic and syntactic information. The framework uses NT-Xent contrastive loss to maximize agreement between text and structure encoder representations while maintaining classification accuracy through BCE loss.

## Key Results
- HILL achieves state-of-the-art performance on three datasets (WOS, RCV1-v2, NYTimes) with average improvements of 1.85% and 3.38% in Micro-F1 and Macro-F1
- Theoretical analysis proves HILL's information retention is the upper bound of any augmentation-based method
- Ablation studies show that both the coding tree construction and hierarchical representation learning modules contribute significantly to performance gains

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HILL avoids information loss by directly generating positive samples via structure encoder instead of augmenting text
- Mechanism: The structure encoder extracts syntactic information from label hierarchies using structural entropy minimization and injects it into text representations via hierarchical representation learning, forming an information lossless positive pair
- Core assumption: Structural entropy minimization effectively captures essential hierarchy structure without losing semantic information
- Evidence anchors:
  - [abstract]: "we propose an information lossless contrastive learning strategy for HTC, namely HILL, which consists of a text encoder representing the input document, and a structure encoder directly generating the positive sample"
  - [section 3.3]: "To implement information lossless contrastive learning in the structure encoder, we propose an algorithm to extract structural information from the label hierarchy via structural entropy minimization"
  - [corpus]: Weak - corpus contains no direct discussion of information loss mechanisms

### Mechanism 2
- Claim: The hierarchical representation learning module effectively fuses semantic and syntactic information
- Mechanism: The structure encoder initializes leaf nodes with document embeddings, then propagates information upward through K layers of feedforward networks, finally integrating multi-granular information across hierarchy levels
- Core assumption: The layer-wise information propagation preserves both semantic and syntactic information effectively
- Evidence anchors:
  - [section 3.3]: "Based on the structure of coding trees, we design a hierarchical representation learning module. For xk_v ∈ X^TL in the k-th layer, xk_v = ϕ^k FFN(Σn∈C(v) xk-1_n)"
  - [section 3.3]: "Afterward, the structure encoder generates an overall representation of the coding tree, which serves as a contrastive sample for the text encoder"
  - [corpus]: Weak - corpus does not discuss hierarchical representation learning specifically

### Mechanism 3
- Claim: The information lossless property is theoretically bounded above all augmentation-based methods
- Mechanism: Theorem 1 proves that mutual information between (TL ◦ D) and Y is greater than or equal to that between any augmentation function θ(GL, D) and Y
- Core assumption: Structural entropy minimization produces optimal coding trees that preserve maximum mutual information
- Evidence anchors:
  - [section 3.4]: "Theorem 1 Given a document D and the coding tree TL of the label hierarchy GL. Denote their random variable as D, TL, and GL. For any augmentation function θ, we have, I((TL ◦ D); Y) ≥ I(θ(GL, D); Y)"
  - [section 3.4]: "Theoretical analysis is further given to reveal the information lossless property of HILL for HTC"
  - [corpus]: No direct evidence - corpus contains related contrastive learning papers but none discuss information bounds

## Foundational Learning

- Concept: Structural entropy and coding trees
  - Why needed here: Provides the theoretical foundation for extracting essential hierarchical structure without information loss
  - Quick check question: How does structural entropy measure the complexity of a graph's structure?

- Concept: Contrastive learning with positive pairs
  - Why needed here: The core learning paradigm that enables HILL to train without data augmentation
  - Quick check question: What distinguishes information lossless contrastive learning from standard contrastive learning?

- Concept: Hierarchical text classification taxonomy
  - Why needed here: Understanding how labels form directed acyclic graphs is crucial for implementing the structure encoder
  - Quick check question: What defines a valid label hierarchy in hierarchical text classification?

## Architecture Onboarding

- Component map: Text encoder (BERT) → Structure encoder (hierarchical representation learning + coding tree construction) → Projector → Contrastive loss + Classification loss → Classifier
- Critical path: Document embedding → Structure encoder hierarchical processing → Information fusion → Contrastive learning → Final prediction
- Design tradeoffs: HILL trades model complexity for information preservation - smaller structure encoder but more sophisticated hierarchy processing
- Failure signatures: Performance drops when label hierarchy depth increases, degradation in Macro-F1 vs Micro-F1, training instability with higher coding tree heights
- First 3 experiments:
  1. Verify structural entropy minimization produces consistent coding trees across different hierarchies
  2. Test information preservation by comparing representations before/after structure encoder
  3. Validate contrastive learning effectiveness by training with and without Lclr component

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of HILL vary when using different text encoders beyond BERT, such as TextRCNN or larger language models?
- Basis in paper: [inferred] The paper mentions that they focused on designing the structure encoder and did not report results on smaller models like TextRCNN or larger language models as the text encoder.
- Why unresolved: The paper explicitly states that they did not explore different text encoders beyond BERT due to their focus on the structure encoder design.
- What evidence would resolve it: Experiments comparing HILL's performance with various text encoders (e.g., TextRCNN, GPT, RoBERTa) would provide insights into how the choice of text encoder impacts HILL's effectiveness.

### Open Question 2
- Question: What is the impact of the height K of coding trees on HILL's performance, and how does it correlate with the volume of the label set Y?
- Basis in paper: [explicit] The paper discusses the impact of the height K of coding trees on HILL's performance and suggests that the optimal K is more likely to positively correlate with the volumes of the label set Y.
- Why unresolved: While the paper provides some insights into the relationship between K and Y, a more comprehensive analysis of this correlation across different datasets and scenarios is needed.
- What evidence would resolve it: Extensive experiments varying K across multiple datasets with different label set volumes would help establish a clearer understanding of the relationship between K and Y, and how it affects HILL's performance.

### Open Question 3
- Question: How does the performance of HILL compare to other contrastive learning methods for HTC that do not rely on data augmentation, such as those using alternative positive sample generation strategies?
- Basis in paper: [explicit] The paper introduces HILL as an information lossless contrastive learning approach for HTC that avoids data augmentation by directly generating positive samples using a structure encoder. It compares HILL to HGCLR, a contrastive learning model that relies on data augmentation.
- Why unresolved: While the paper compares HILL to HGCLR, it does not explore other contrastive learning methods for HTC that may have different positive sample generation strategies.
- What evidence would resolve it: Experiments comparing HILL to other contrastive learning methods for HTC, such as those using different positive sample generation techniques, would provide insights into the relative strengths and weaknesses of various approaches.

## Limitations
- The information lossless property relies heavily on the effectiveness of structural entropy minimization, which may not generalize well to highly complex or irregular label hierarchies
- Ablation studies are limited in scope and optimal hyperparameters appear to be dataset-specific, suggesting extensive tuning may be required for new datasets
- While the theoretical bound is proven, there is no empirical validation showing that the coding tree actually preserves maximum mutual information in practice

## Confidence

- **High Confidence**: The experimental results showing HILL outperforms baseline methods on all three datasets, particularly the consistent improvements in both Micro-F1 and Macro-F1 metrics. The theoretical proof of information lossless property is mathematically rigorous and well-supported.

- **Medium Confidence**: The claim that HILL avoids information loss through direct positive sample generation. While the mechanism is theoretically sound, the practical effectiveness depends heavily on implementation details of the structure encoder that are not fully specified in the paper.

- **Low Confidence**: The generalizability of HILL to different domain datasets and label hierarchy structures. The paper only tests on three datasets with relatively clean hierarchies, and the optimal hyperparameters vary significantly across them.

## Next Checks

1. **Empirical Information Preservation Test**: Implement the structure encoder and measure the mutual information between input documents and their coding tree representations. Compare this to the theoretical bound to validate that the information lossless property holds in practice, not just in theory.

2. **Hierarchy Complexity Stress Test**: Evaluate HILL on datasets with varying hierarchy depths and branching factors, particularly focusing on cases where label hierarchies have cycles or highly irregular structures. Measure performance degradation as a function of hierarchy complexity to identify failure modes.

3. **Ablation Study Extension**: Conduct a more comprehensive ablation study by systematically varying the structure encoder depth (K), contrastive loss weight (λclr), and coding tree construction parameters. Use these results to develop guidelines for hyperparameter selection on new datasets, addressing the current limitation where optimal settings appear dataset-specific.