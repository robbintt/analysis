---
ver: rpa2
title: Verifiably Robust Conformal Prediction
arxiv_id: '2405.18942'
source_url: https://arxiv.org/abs/2405.18942
tags:
- vrcp
- prediction
- robust
- coverage
- test
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes Verifiably Robust Conformal Prediction (VRCP),\
  \ a novel framework that leverages neural network verification to construct adversarially\
  \ robust prediction sets for both classification and regression tasks. Unlike prior\
  \ methods limited to \u21132-norm bounded perturbations and classification, VRCP\
  \ supports arbitrary \u2113p-norm perturbations and extends to regression settings."
---

# Verifiably Robust Conformal Prediction

## Quick Facts
- arXiv ID: 2405.18942
- Source URL: https://arxiv.org/abs/2405.18942
- Authors: Linus Jeary; Tom Kuipers; Mehran Hosseini; Nicola Paoletti
- Reference count: 13
- This paper proposes Verifiably Robust Conformal Prediction (VRCP), a novel framework that leverages neural network verification to construct adversarially robust prediction sets for both classification and regression tasks.

## Executive Summary
This paper introduces Verifiably Robust Conformal Prediction (VRCP), a framework that combines conformal prediction with neural network verification to construct prediction sets that maintain coverage under adversarial attacks. Unlike prior methods limited to ℓ2-norm bounded perturbations and classification tasks, VRCP supports arbitrary ℓp-norm perturbations and extends to regression settings. The framework introduces two variants: VRCP–I (verification at inference time) and VRCP–C (verification at calibration time). Experimental results on CIFAR10, CIFAR100, TinyImageNet, and deep reinforcement learning regression tasks show that VRCP maintains above-nominal coverage under adversarial attacks while producing significantly more efficient and informative prediction sets compared to state-of-the-art methods.

## Method Summary
VRCP leverages neural network verification algorithms to compute bounds on prediction scores under adversarial perturbations. The framework uses these bounds to inflate calibration scores (VRCP-C) or compute best-case inference scores (VRCP-I), ensuring that prediction sets remain valid even when inputs are adversarially perturbed. The method supports arbitrary ℓp-norm bounded perturbations and extends to both classification and regression tasks. VRCP–C computes worst-case calibration scores and uses them to construct a more conservative calibration distribution, while VRCP–I computes lower bounds on scores for test inputs and includes labels in prediction sets only if their best-case scores are below the critical value.

## Key Results
- VRCP maintains above-nominal coverage (≥ 0.9) under adversarial attacks while producing significantly more efficient prediction sets compared to state-of-the-art methods
- VRCP achieves this without requiring holdout sets, score function transformations, or probabilistic smoothing
- VRCP achieves up to 10x faster inference time compared to complete verification methods while maintaining comparable coverage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NN verification computes valid output bounds under adversarial perturbations
- Mechanism: Verification algorithms propagate input perturbation bounds through the network to produce conservative output bounds that hold for all possible inputs in the perturbation region
- Core assumption: The NN verifier provides sound bounds (incomplete but valid) or exact bounds (complete)
- Evidence anchors:
  - [section] "These approaches can be divided into complete and incomplete algorithms. Given a neural network f, a verifier is complete if it allows computing exact bounds f⊥ and f⊤ for the image f(Bϵ(x))"
  - [section] "A verifier is incomplete, but sound, if it computes bounds that are valid but not exact"
  - [corpus] No direct evidence about verifier performance characteristics found in related papers
- Break condition: Verifier bounds become too loose to be useful, or verifier runtime exceeds practical limits

### Mechanism 2
- Claim: Worst-case calibration scores inflate the critical value to maintain coverage under attacks
- Mechanism: VRCP-C computes upper bounds on calibration scores (s⊤i ≥ supx′∈Bϵ(xi) S(x′, yi)) and uses these to construct a more conservative calibration distribution F⊤, resulting in a higher critical value Q1−α(F⊤)
- Core assumption: Adversarial perturbations can only increase (not decrease) score values for the true label
- Evidence anchors:
  - [section] "In VRCP–C, for each calibration point (xi, yi) we compute s⊤(xi, yi) as s⊤(xi, yi) = 1 − f(xi)⊥yi" and "F⊤ = δ∞/(n+1) + Pn i=1 δs⊤ i/(n+1)"
  - [section] "Theorem 2. Let ˜xn+1 = xn+1 + δ for a clean test sample xn+1 and ∥δ∥p ≤ ϵ. The prediction set Cϵ(˜xn+1) defined in Eq. (8) satisfies P[yn+1 ∈ Cϵ(˜xn+1)] ≥ 1 − α"
  - [corpus] No direct evidence about worst-case calibration approach in related papers
- Break condition: Perturbations cause scores to decrease for true labels (violating core assumption)

### Mechanism 3
- Claim: Best-case inference scores maintain coverage by ensuring adversarial inputs cannot escape prediction sets
- Mechanism: VRCP-I computes lower bounds on scores for test inputs (s⊥(xn+1, y) ≤ infx′∈Bϵ(xn+1) S(x′, y)) and includes labels in prediction sets only if their best-case scores are below the critical value
- Core assumption: Adversarial perturbations can only increase (not decrease) score values for incorrect labels
- Evidence anchors:
  - [section] "In VRCP–I, for a test input xn+1 and for each y ∈ Y we derive s⊥(xn+1, y) as s⊥(xn+1, y) = 1 − f(xn+1)⊤y" and "Cϵ(xn+1) = {y : s⊥(xn+1, y) ≤ Q1−α(F)}"
  - [section] "Theorem 1. Let ˜xn+1 = xn+1 + δ for a clean test sample xn+1 and ∥δ∥p ≤ ϵ. The prediction set Cϵ(˜xn+1) defined in Eq. (6) satisfies P[yn+1 ∈ Cϵ(˜xn+1)] ≥ 1 − α"
  - [corpus] No direct evidence about best-case inference approach in related papers
- Break condition: Perturbations cause scores to decrease for incorrect labels (violating core assumption)

## Foundational Learning

- Concept: Conformal prediction fundamentals (split CP, score functions, coverage guarantees)
  - Why needed here: VRCP builds directly on CP framework, extending it to adversarial settings while maintaining theoretical guarantees
  - Quick check question: What is the difference between marginal coverage P[yn+1 ∈ C(xn+1)] ≥ 1 − α and conditional coverage P[yn+1 ∈ C(xn+1)|xn+1] ≥ 1 − α?

- Concept: Neural network verification techniques (sound vs complete, bound propagation, branch-and-bound)
  - Why needed here: VRCP's core innovation is using NN verification to compute score bounds under adversarial perturbations
  - Quick check question: What is the key difference between CROWN and α-CROWN verification methods mentioned in the paper?

- Concept: Adversarial attacks and perturbation norms (ℓp-bounded attacks, PGD, Fast Gradient Sign Method)
  - Why needed here: VRCP needs to handle arbitrary ℓp-norm bounded perturbations, not just ℓ2 as previous methods
  - Quick check question: Why is ℓ∞-norm particularly challenging for classification tasks compared to ℓ2-norm?

## Architecture Onboarding

- Component map:
  - Base neural network f (classification or regression)
  - NN verification module (CROWN/α-CROWN for classification, custom bounds for regression)
  - Score function S(x, y) (classification: 1 − f(x)y, regression: max{fαlow(x) − y, y − fαhigh(x)})
  - VRCP-C path: calibration-time verification → worst-case scores → critical value computation
  - VRCP-I path: inference-time verification → best-case scores → prediction set construction
  - Adversarial attack generator (PGD for classification, FGSM for regression)

- Critical path:
  1. Train base model on clean data
  2. Split data into train/calibration/test sets
  3. For VRCP-C: Compute worst-case calibration scores using NN verification
  4. Compute critical value from worst-case calibration distribution
  5. At inference: Generate adversarial test points and construct prediction sets using either:
     - VRCP-C: Regular scores with inflated critical value
     - VRCP-I: Best-case scores with regular critical value

- Design tradeoffs:
  - VRCP-C: Faster inference (no verification at test time) but less efficient sets, single calibration for fixed ϵ
  - VRCP-I: More efficient sets but slower inference (verification per test point), flexible ϵ per test point
  - Incomplete vs complete verification: Speed vs bound tightness
  - Verification completeness vs network size: Large networks may require incomplete methods

- Failure signatures:
  - VRCP-C: Overly large prediction sets (verification bounds too loose), coverage drops (bounds invalid)
  - VRCP-I: Very slow inference (complex verification), coverage drops (bounds invalid)
  - Both: Prediction sets become trivial (contain all labels) when attack budget ϵ is too large

- First 3 experiments:
  1. Implement VRCP-C on CIFAR10 with CROWN verification, compare to vanilla CP under ℓ∞ attacks
  2. Implement VRCP-I on CIFAR10 with α-CROWN verification, measure inference time vs VRCP-C
  3. Implement regression version on MPE environment, verify coverage under ℓ∞ attacks with FGSM

## Open Questions the Paper Calls Out
None

## Limitations
- The framework's effectiveness depends critically on the tightness of verification bounds - loose bounds could lead to overly conservative prediction sets that lose practical utility
- The regression extension relies on custom bound computation that may not generalize to all network architectures
- The computational overhead of verification, particularly for VRCP-I at inference time, could limit real-world deployment

## Confidence
- **High Confidence**: The theoretical framework connecting conformal prediction with neural network verification is sound, with formal coverage guarantees derived from established CP theory
- **Medium Confidence**: Empirical results showing coverage maintenance under attacks are convincing but limited to specific datasets and attack types
- **Low Confidence**: Claims about computational efficiency improvements are based on moderate-sized models and may not scale to larger architectures

## Next Checks
1. Test VRCP on larger-scale datasets (ImageNet) and more complex architectures (Vision Transformers) to evaluate scalability
2. Compare VRCP against certified defenses like randomized smoothing under identical threat models and computational budgets
3. Measure the impact of different verification completeness levels (sound vs complete) on prediction set efficiency and coverage trade-offs