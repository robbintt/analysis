---
ver: rpa2
title: 'FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural Architecture
  Search'
arxiv_id: '2404.15622'
source_url: https://arxiv.org/abs/2404.15622
tags:
- neural
- predictor
- graph
- search
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces FR-NAS, a novel graph neural network (GNN)
  predictor for neural architecture search (NAS) that leverages both forward and reverse
  graph representations of neural architectures. The key innovation is using two separate
  GIN encoders to process the forward and reverse DAG views of an architecture, along
  with a customized training loss that ensures embeddings from both encoders align
  by minimizing the difference in their instance relationship graphs.
---

# FR-NAS: Forward-and-Reverse Graph Predictor for Efficient Neural Architecture Search

## Quick Facts
- arXiv ID: 2404.15622
- Source URL: https://arxiv.org/abs/2404.15622
- Reference count: 40
- Key outcome: FR-NAS achieves 3%–16% improvements in Kendall-tau correlation over state-of-the-art GNN predictors, with largest gains when training data is limited

## Executive Summary
FR-NAS introduces a novel graph neural network predictor for neural architecture search that processes both forward and reverse graph representations of neural architectures. The predictor uses two separate GIN encoders and a customized training loss that aligns embeddings from both views by minimizing the difference in their instance relationship graphs. This dual-representation approach enables richer feature extraction, particularly when training data is scarce, leading to state-of-the-art performance on NAS-Bench-101, NAS-Bench-201, and DARTS search spaces.

## Method Summary
FR-NAS represents neural architectures as directed acyclic graphs and processes both the forward adjacency matrix and its transpose (reverse view) through separate GIN encoders. Each encoder generates embeddings that are passed through individual MLPs, with final predictions obtained by averaging the outputs. The training objective combines MSE loss for prediction accuracy with an Instance Relationship Graph (IRG) loss that minimizes the discrepancy between distance matrices of embeddings from both encoders, ensuring consistent feature extraction across views.

## Key Results
- Achieves 3%–16% improvements in Kendall-tau correlation over state-of-the-art GNN predictors
- Largest performance gains observed when training data is scarce
- Demonstrates superior performance on NAS-Bench-101, NAS-Bench-201, and DARTS search spaces

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual graph representations provide complementary feature views that enhance predictor performance
- Mechanism: Two GIN encoders process forward and reverse DAG views separately, learning distinct feature patterns from both propagation directions
- Core assumption: Forward and reverse graph encodings capture complementary information about architecture properties
- Evidence anchors:
  - [abstract] "This predictor renders neural architectures into vector representations by combining both the conventional and inverse graph views."
  - [section III-A] "Specifically, we refer to this type of encoding as forward graph encoding, which propagates features in alignment with edge directions, while its reverse counterpart is derived by transposing the adjacency matrix to obtain AT."
- Break condition: If the two graph views encode redundant or contradictory information, dual encoder setup may not improve performance

### Mechanism 2
- Claim: Customized training loss aligns embeddings from forward and reverse encoders for consistent feature extraction
- Mechanism: IRG loss minimizes discrepancy between distance matrices of embeddings from both encoders
- Core assumption: Architectures with similar performance should maintain similar relative distances in both embedding spaces
- Evidence anchors:
  - [section III-C] "we develop a loss function aimed at reducing the discrepancy between the two encoders... defined as Le = (1/M^2) * Σ_i Σ_j (||hfi - hfj||^2 - ||hri - hrj||^2)^2"
  - [section III-D] "Building on these insights and drawing inspiration from the IRG [32] framework, we develop a loss function aimed at reducing the discrepancy between the two encoders."
- Break condition: If relative distances in two embedding spaces are not meaningful for prediction task, minimizing their discrepancy could harm ranking accuracy

### Mechanism 3
- Claim: Dual-predictor averaging strategy reduces bias and variance in final prediction
- Mechanism: Two separate MLPs process embeddings from forward and reverse encoders, outputs are averaged
- Core assumption: The two predictors are sufficiently independent that their errors are not perfectly correlated
- Evidence anchors:
  - [section III-B] "we utilize two separate fully connected layers, each addressing the feature embeddings from a specific encoder... The ensuing prediction is ascertained by averaging the outcomes from the fully connected layers."
- Break condition: If two predictors are highly correlated in errors or one is significantly worse, simple averaging may not provide optimal results

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) and their message passing mechanism
  - Why needed here: Predictor relies on GINs to process graph-structured architecture data; understanding GNN aggregation is crucial for modification
  - Quick check question: What is the difference between GCN, GIN, and GAT in terms of how they aggregate information from neighboring nodes?

- Concept: Directed Acyclic Graphs (DAGs) and adjacency matrix representation
  - Why needed here: Neural architectures represented as DAGs using adjacency matrices; predictor operates on these representations
  - Quick check question: How do you convert a computation graph into an adjacency matrix and operation encoding?

- Concept: Kendall-tau correlation as a ranking metric
  - Why needed here: Predictor's performance evaluated using Kendall-tau, which measures correlation between predicted and true rankings
  - Quick check question: What does a Kendall-tau of 0.7 vs 0.8 indicate about predictor's ranking accuracy?

## Architecture Onboarding

- Component map:
  Input architecture → Forward DAG (A, O) and Reverse DAG (AT, O) → Forward GIN encoder → Embedding hf → Forward MLP predictor → Prediction pf → Average pf and pr → Final prediction p
  Input architecture → Reverse DAG (AT, O) → Reverse GIN encoder → Embedding hr → Reverse MLP predictor → Prediction pr

- Critical path:
  1. Convert architecture to forward and reverse adjacency matrices with operation encodings
  2. Pass both through respective GIN encoders
  3. Apply MLPs to each embedding to get predictions
  4. Compute MSE loss and IRG loss
  5. Backpropagate combined loss to update all parameters

- Design tradeoffs:
  - Two separate GIN encoders vs. single shared encoder with bidirectional message passing
  - IRG loss weighting (λ=0.8) vs. pure MSE loss
  - Separate MLPs vs. shared MLP for both predictions

- Failure signatures:
  - High IRG loss during training: embeddings from forward and reverse encoders not aligning
  - Low Kendall-tau on test set: predictor not learning to rank architectures correctly
  - IRG loss decreases but performance doesn't improve: alignment constraint may be too strong or not useful

- First 3 experiments:
  1. Train with only forward graph encoding (single GIN) to establish baseline performance
  2. Train with both forward and reverse encodings but without IRG loss to test dual representation benefit
  3. Train with both encodings and IRG loss, varying λ from 0.5 to 0.9 to find optimal alignment strength

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does performance of FR-NAS scale when using alternative graph neural network architectures (e.g., GAT, GCN) instead of GIN?
- Basis in paper: [explicit] Paper uses GIN as base encoder but does not explore other GNN architectures
- Why unresolved: Authors only tested GIN-based encoders, leaving open whether dual-representation framework generalizes to other GNN types
- What evidence would resolve it: Comparative experiments using GAT, GCN, or other GNN architectures within FR-NAS framework across multiple search spaces

### Open Question 2
- Question: What is impact of incorporating reverse graph data on predictor performance when using non-regression tasks (e.g., classification or ranking-only objectives)?
- Basis in paper: [inferred] Study focuses on regression tasks for performance prediction but does not address how reverse graph embeddings affect other objective types
- Why unresolved: Paper does not test alternative NAS objectives beyond performance regression
- What evidence would resolve it: Experiments evaluating FR-NAS on classification or ranking-based NAS tasks to measure effect of reverse graph embeddings

### Open Question 3
- Question: How sensitive is FR-NAS to choice of weight coefficient λ in loss function across different search space complexities?
- Basis in paper: [explicit] Authors fixed λ at 0.8 and performed sensitivity analysis, but only for limited conditions
- Why unresolved: Paper does not explore how λ should be tuned for search spaces with varying graph sizes or architectural diversity
- What evidence would resolve it: Systematic ablation studies varying λ across range of search space complexities to identify optimal settings

## Limitations
- Performance evaluation limited to three search spaces (NAS-Bench-101, NAS-Bench-201, DARTS) without testing on more modern or diverse benchmarks
- Specific architectural details of MLP predictors following GIN encoders are not fully specified
- IRG loss hyperparameter (λ=0.8) appears to be arbitrarily chosen without systematic optimization

## Confidence

- High confidence: Dual graph representation concept and IRG loss formulation are clearly described and mathematically specified
- Medium confidence: Claimed 3%–16% improvements in Kendall-tau correlation are reported, but independent verification needed
- Medium confidence: Mechanism that dual representations provide complementary information is theoretically sound but requires empirical validation

## Next Checks

1. Replicate the ablation study showing performance degradation when removing either the reverse graph encoding or the IRG loss to confirm both components are necessary
2. Test the predictor's robustness to different IRG loss weightings (λ values) to determine if λ=0.8 is optimal or could be improved
3. Evaluate performance on a larger, more diverse NAS benchmark (beyond NAS-Bench-101/201 and DARTS) to assess generalizability to modern search spaces