---
ver: rpa2
title: Breaking the Script Barrier in Multilingual Pre-Trained Language Models with
  Transliteration-Based Post-Training Alignment
arxiv_id: '2406.19759'
source_url: https://arxiv.org/abs/2406.19759
tags:
- languages
- language
- latn
- alignment
- arab
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work introduces a transliteration-based post-training alignment\
  \ method that operates at both sequence and token levels to improve cross-lingual\
  \ transfer in multilingual language models, especially for low-resource languages\
  \ using different scripts. By fine-tuning Glot500 on two language groups\u2014Mediterranean-Amharic-Farsi\
  \ and South+East Asian Languages\u2014using combined original and Latin-transliterated\
  \ data, the method boosts zero-shot transfer performance across sentence retrieval,\
  \ text classification, and sequence labeling tasks."
---

# Breaking the Script Barrier in Multilingual Pre-Trained Language Models with Transliteration-Based Post-Training Alignment

## Quick Facts
- **arXiv ID**: 2406.19759
- **Source URL**: https://arxiv.org/abs/2406.19759
- **Reference count**: 23
- **Primary result**: Transliteration-based post-training alignment improves cross-lingual transfer for low-resource languages with different scripts, achieving up to 50% gains in retrieval accuracy

## Executive Summary
This paper addresses the challenge of cross-lingual transfer in multilingual language models, particularly for low-resource languages that use different scripts. The authors introduce a transliteration-based post-training alignment method that operates at both sequence and token levels to improve transfer performance. By fine-tuning the Glot500 model on combined original and Latin-transliterated data, the method demonstrates significant improvements across sentence retrieval, text classification, and sequence labeling tasks. The approach is particularly effective for languages like Amharic, Hausa, and Tigrinya that lack substantial training data.

## Method Summary
The authors propose a two-stage approach that leverages transliteration to bridge script differences between languages. First, they transliterate target language text into Latin script to create a bridge between languages with different writing systems. Then, they perform post-training alignment on the Glot500 model using both original and transliterated data with sequence-level (contrastive learning) and token-level (masked language modeling) objectives. This approach is applied to two language groups: Mediterranean-Amharic-Farsi and South+East Asian Languages. The method includes a language selection component that identifies optimal source languages for each target language based on a relatedness score computed using pre-trained embeddings.

## Key Results
- The proposed method consistently outperforms the original Glot500 model across all tested tasks and language groups
- Up to 50% improvements in retrieval accuracy are observed when optimal source languages are selected
- Both sequence and token-level objectives are critical for maximizing performance, as shown by ablation studies
- The method shows particular effectiveness for low-resource languages using non-Latin scripts

## Why This Works (Mechanism)
The method works by creating a bridge between languages with different scripts through Latin transliteration, allowing the model to learn cross-lingual representations that are invariant to script differences. By training on both original and transliterated versions of the same content, the model learns to map semantically equivalent content across scripts into similar vector spaces. The sequence-level contrastive learning objective ensures that aligned sentence pairs across languages are mapped to similar representations, while the token-level masked language modeling objective helps the model learn to process and generate text in both scripts. The language selection component further optimizes transfer by identifying the most relevant source languages for each target language based on their semantic similarity.

## Foundational Learning
- **Contrastive learning**: Why needed - to align semantically equivalent sentences across languages; Quick check - measure cosine similarity between aligned sentence pairs
- **Masked language modeling**: Why needed - to maintain token-level understanding across scripts; Quick check - evaluate perplexity on both original and transliterated text
- **Transliteration**: Why needed - to create a common script representation across languages; Quick check - verify semantic preservation through back-translation
- **Cross-lingual embeddings**: Why needed - to measure language relatedness for source language selection; Quick check - compute relatedness scores between language pairs
- **Zero-shot transfer**: Why needed - to evaluate performance without target language training data; Quick check - measure performance drop when removing target language examples

## Architecture Onboarding
**Component map**: Input data -> Transliteration module -> Post-training module (sequence + token objectives) -> Fine-tuned model -> Evaluation on downstream tasks

**Critical path**: The most critical components are the transliteration module (which creates the bridge between scripts) and the sequence-level contrastive learning objective (which aligns representations across languages). The language selection component is also critical as it determines which source languages will provide the most effective transfer.

**Design tradeoffs**: The method trades computational overhead during post-training (due to processing both original and transliterated data) for improved cross-lingual transfer performance. The choice of transliteration system (Latin-based) is a design decision that may not be optimal for all language pairs.

**Failure signatures**: Performance degradation is likely when source languages are poorly selected (low relatedness to target), when transliteration introduces significant semantic drift, or when the model fails to align representations across scripts at the sequence level.

**3 first experiments**:
1. Evaluate the impact of using only sequence-level objectives vs. only token-level objectives vs. both
2. Test the method with different transliteration systems (e.g., IPA-based vs. Latin-based)
3. Compare performance when using random source language selection vs. relatedness-based selection

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the text provided.

## Limitations
- The method's effectiveness for language groups beyond the tested Mediterranean and South+East Asian languages remains unexplored
- The approach is evaluated only in zero-shot transfer scenarios, with no assessment of performance in few-shot or fully supervised settings
- Computational requirements and scalability for larger multilingual models are not discussed

## Confidence
- **High**: The method's ability to improve cross-lingual transfer when applied to Glot500 across tested tasks and language groups
- **High**: The necessity of both sequence and token-level objectives for optimal performance (supported by ablation studies)
- **Medium**: The claim of up to 50% improvements in retrieval accuracy (appears to be the maximum observed rather than consistent result)

## Next Checks
1. Test the method on additional language groups with different script families (e.g., Cyrillic, Arabic, or CJK scripts) to assess generalizability beyond the Mediterranean and South+East Asian languages evaluated
2. Evaluate the method's effectiveness in few-shot learning scenarios where limited target language data is available, comparing against standard fine-tuning approaches
3. Conduct a systematic study of the computational requirements and training dynamics when applying the method to larger multilingual models (e.g., beyond 500M parameters) to understand scalability constraints