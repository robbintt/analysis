---
ver: rpa2
title: 'Beyond Binary: Multiclass Paraphasia Detection with Generative Pretrained
  Transformers and End-to-End Models'
arxiv_id: '2407.11345'
source_url: https://arxiv.org/abs/2407.11345
tags:
- paraphasia
- speech
- detection
- paraphasias
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work explores multiclass paraphasia detection in aphasic
  speech using generative pretrained transformers (GPT) and end-to-end models. The
  authors propose three approaches: ASR+GPT (using ASR transcripts with GPT), single-seq
  (jointly modeling ASR and paraphasia classification), and multi-seq (separate ASR
  and paraphasia classification heads).'
---

# Beyond Binary: Multiclass Paraphasia Detection with Generative Pretrained Transformers and End-to-End Models

## Quick Facts
- arXiv ID: 2407.11345
- Source URL: https://arxiv.org/abs/2407.11345
- Authors: Matthew Perez; Aneesha Sampath; Minxue Niu; Emily Mower Provost
- Reference count: 0
- Primary result: Single-sequence model significantly outperforms GPT-4 for multiclass paraphasia detection in aphasic speech, especially for phonemic and neologistic paraphasias

## Executive Summary
This work addresses multiclass paraphasia detection in aphasic speech using generative pretrained transformers and end-to-end models. The authors propose three approaches: ASR+GPT (using ASR transcripts with GPT), single-seq (jointly modeling ASR and paraphasia classification), and multi-seq (separate ASR and paraphasia classification heads). They evaluate these methods on the AphasiaBank corpus using word-error-rate, augmented-WER, and temporal distance metrics. Results show that the single-seq model significantly outperforms GPT-4 for paraphasia detection and multiclass classification, especially for phonemic and neologistic paraphasias. The single-seq model achieves a TD-multiclass score of 1.51 compared to 1.88 for GPT-4. However, semantic paraphasia classification remains challenging across all approaches.

## Method Summary
The study explores three approaches for multiclass paraphasia detection in aphasic speech: (1) ASR+GPT pipeline using in-context learning on GPT-3.5-turbo and GPT-4 with ASR transcripts, (2) single-seq model jointly modeling ASR and paraphasia classification as a single sequence, and (3) multi-seq model with separate ASR and paraphasia classification heads optimized with multitask learning. Models use transformer architectures initialized with pretrained HuBERT and are evaluated on the AphasiaBank corpus using WER, AWER, temporal distance metrics, and utterance-level binary F1 scores. The single-seq model treats paraphasias as special tokens in the same sequence as ASR output, allowing contextual dependencies to inform paraphasia predictions.

## Key Results
- Single-seq model achieves TD-multiclass score of 1.51 versus 1.88 for GPT-4
- Single-seq significantly outperforms GPT-4 for phonemic and neologistic paraphasia detection
- Semantic paraphasia classification remains challenging across all approaches
- Single-seq shows 5.56-12.44 point AWER reductions compared to GPT-4

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The single-sequence model outperforms GPT-4 because it jointly learns ASR and paraphasia classification, allowing contextual dependencies between subword tokens and paraphasia labels to inform each other.
- Mechanism: By treating paraphasias as special tokens in the same sequence as ASR output, the model can use the full context of the utterance to make more accurate paraphasia predictions. This is particularly effective for phonemic and neologistic paraphasias where the model can learn patterns like "if I hear these sounds, it's likely a phonemic error" or "this word doesn't make sense in context, it's probably neologistic."
- Core assumption: The paraphasia type can be inferred from the surrounding context and the ASR output itself, rather than requiring separate classification.
- Evidence anchors:
  - [abstract] "We demonstrate that a single sequence model outperforms GPT baselines for multiclass paraphasia detection, specifically for phonemic and neologistic paraphasias."
  - [section] "By treating paraphasias as a separate token, the model can learn contextual dependencies across subword and paraphasia tokens."
  - [corpus] Weak evidence - the corpus analysis shows related work on paraphasia detection but doesn't directly support this specific mechanism.
- Break condition: If the paraphasia types are not contextually distinguishable or if the ASR errors are too severe to provide useful context for paraphasia classification.

### Mechanism 2
- Claim: The single-sequence model's joint optimization of ASR and paraphasia classification tasks improves performance over separate models because it creates a unified representation that captures both linguistic and error patterns.
- Mechanism: The shared encoder learns a representation that captures both the acoustic features needed for ASR and the linguistic features needed for paraphasia detection. This unified representation allows the model to learn that certain acoustic patterns often co-occur with certain types of paraphasias, and vice versa.
- Core assumption: The acoustic and linguistic features needed for ASR and paraphasia detection are sufficiently related that a shared representation can be learned.
- Evidence anchors:
  - [abstract] "two end-to-end approaches that focus on modeling both automatic speech recognition (ASR) and paraphasia classification as multiple sequences vs. a single sequence"
  - [section] "The single sequence model (Figure 1a) has the same architecture as the ASR model but the tokenizer has three added special tokens representing [p], [n], and [s] paraphasia labels."
  - [corpus] Weak evidence - the corpus shows that joint modeling has been explored in other domains but doesn't directly validate this specific claim.
- Break condition: If the acoustic and linguistic features are too dissimilar or if the model cannot effectively learn a unified representation that captures both.

### Mechanism 3
- Claim: The single-sequence model's performance advantage is particularly pronounced for phonemic and neologistic paraphasias because these error types have more distinctive acoustic and linguistic signatures that the joint model can learn to recognize.
- Mechanism: Phonemic paraphasias often involve substitutions, omissions, or rearrangements of phonemes, which create distinctive acoustic patterns. Neologistic paraphasias involve substituting nonsensical words, which have distinctive linguistic patterns (they don't fit the context and often have unusual sound patterns). The joint model can learn these patterns more effectively than separate models.
- Core assumption: Phonemic and neologistic paraphasias have more distinctive acoustic and linguistic signatures than semantic paraphasias, making them easier to learn for a joint model.
- Evidence anchors:
  - [abstract] "However, semantic paraphasia classification remains challenging across all approaches."
  - [section] "For phonemic paraphasias, we see that single-seq has the lowest TD-[p] and the highest F1 score... For neologistic paraphasias, we observe that single-seq has the lowest TD-[n]"
  - [corpus] Weak evidence - the corpus shows that phonemic and neologistic paraphasias are the focus of most paraphasia detection work, suggesting they may be easier to detect, but doesn't directly support this specific mechanism.
- Break condition: If the acoustic and linguistic signatures of phonemic and neologistic paraphasias are not distinctive enough or if the model cannot effectively learn these patterns.

## Foundational Learning

- Concept: Connectionist Temporal Classification (CTC) loss
  - Why needed here: CTC loss is used in the ASR model to handle unsegmented speech data, which is common in aphasic speech where the timing and segmentation of words may be irregular.
  - Quick check question: How does CTC loss differ from cross-entropy loss in handling variable-length sequences?

- Concept: Multi-task learning
  - Why needed here: The multi-sequence model uses multi-task learning to jointly optimize ASR and paraphasia classification, allowing the model to learn shared representations that benefit both tasks.
  - Quick check question: What are the potential benefits and drawbacks of using multi-task learning for ASR and paraphasia classification?

- Concept: Byte-pair encoding (BPE)
  - Why needed here: BPE is used for tokenization to handle the out-of-vocabulary problem, which is particularly important for neologistic paraphasias where the model may encounter completely novel words.
  - Quick check question: How does BPE tokenization help with handling neologistic paraphasias that contain novel words?

## Architecture Onboarding

- Component map: Audio input -> HuBERT-based transformer encoder -> Transformer decoder -> Output (ASR + paraphasia labels)
- Critical path: Audio input → Encoder → Decoder → Output (ASR + paraphasia labels)
  - The encoder processes the audio features and generates a sequence of hidden states
  - The decoder takes these hidden states and generates either subword tokens (for ASR) or paraphasia labels
  - The output is then post-processed to align the ASR output with the paraphasia labels at the word level
- Design tradeoffs:
  - Single-sequence vs. multi-sequence: Single-sequence allows for joint learning of contextual dependencies but may be more complex to train. Multi-sequence is simpler but may not capture the same level of contextual information.
  - Joint vs. separate optimization: Joint optimization allows for shared representations but may lead to interference between tasks. Separate optimization is simpler but may miss out on shared learning opportunities.
  - Token-level vs. word-level classification: Token-level classification allows for more fine-grained predictions but requires post-processing to get word-level labels. Word-level classification is simpler but may miss some nuances.
- Failure signatures:
  - High WER but low paraphasia detection accuracy: This suggests that the model is struggling with ASR but still able to detect paraphasias from the context. May need to improve the ASR component or add more context to the paraphasia detection.
  - Low WER but high paraphasia detection error: This suggests that the model is able to transcribe accurately but struggles to detect paraphasias. May need to add more paraphasia-specific features or use a different approach for paraphasia detection.
  - High error rates for semantic paraphasias: This suggests that the model is struggling with the more context-dependent paraphasia type. May need to add more contextual information or use a different approach for semantic paraphasia detection.
- First 3 experiments:
  1. Train the single-sequence model with different weightings of the ASR and paraphasia classification losses to find the optimal balance.
  2. Compare the performance of the single-sequence model with and without the use of oracle transcripts to quantify the impact of ASR errors on paraphasia detection.
  3. Evaluate the model's performance on each paraphasia type separately to identify which types are most challenging and why.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can semantic paraphasia detection be improved in continuous speech?
- Basis in paper: [explicit] The authors note that semantic paraphasia classification remains challenging across all approaches and suggest that future work should explore including conditioning models with the target script to provide contextual information relevant for identifying associative word substitutions.
- Why unresolved: The current models struggle with semantic paraphasias due to their low representation in the dataset and the inherent difficulty of distinguishing semantically related words from correct words without additional context.
- What evidence would resolve it: Evidence of improved semantic paraphasia detection accuracy when conditioning models with target scripts or other contextual information would resolve this question.

### Open Question 2
- Question: How effective are subword-level paraphasia predictions compared to word-level predictions in the multi-seq model?
- Basis in paper: [explicit] The authors mention that the paraphasia classification in the multi-seq model is performed at the subword level and that word-level paraphasia predictions are obtained by using the majority paraphasia class across all subwords in a given word. They note that the performance of multi-seq warrants further investigation.
- Why unresolved: The paper does not provide detailed analysis or comparison of subword-level vs. word-level paraphasia predictions in the multi-seq model, leaving the effectiveness of this approach unclear.
- What evidence would resolve it: Comparative analysis of subword-level and word-level paraphasia prediction performance in the multi-seq model would resolve this question.

### Open Question 3
- Question: How do the proposed methods perform on spontaneous speech compared to read speech?
- Basis in paper: [inferred] The paper focuses on continuous read speech from the AphasiaBank corpus and does not address spontaneous speech. The performance of the models on spontaneous speech remains unexplored.
- Why unresolved: The models have only been evaluated on read speech, and spontaneous speech presents additional challenges such as disfluencies, self-corrections, and lack of structure that may impact paraphasia detection performance.
- What evidence would resolve it: Evaluation of the proposed methods on spontaneous speech data with paraphasia annotations would provide evidence to resolve this question.

## Limitations

- The study relies on manually transcribed data from AphasiaBank, constraining evaluation to read speech rather than spontaneous conversation
- The semantic paraphasia detection performance remains poor across all approaches, suggesting fundamental limitations in current modeling approaches for this paraphasia type
- The evaluation metrics (particularly TD scores) may not fully capture clinical relevance, as they measure temporal alignment rather than functional impact on communication

## Confidence

**High Confidence**: The claim that single-seq models outperform GPT-4 for phonemic and neologistic paraphasia detection is well-supported by the empirical results showing consistent improvements across multiple metrics (TD-multiclass: 1.51 vs 1.88, AWER reductions of 5.56-12.44 points).

**Medium Confidence**: The assertion that joint optimization through single-sequence modeling is the primary driver of performance improvements. While the results support this, alternative explanations such as differences in model capacity or training procedures cannot be entirely ruled out.

**Low Confidence**: The generalizability of these findings to spontaneous speech contexts and other aphasic populations beyond the specific AphasiaBank subset studied.

## Next Checks

1. **Spontaneous Speech Validation**: Evaluate the single-seq model on a corpus of spontaneous aphasic speech to assess whether the performance gains observed in read speech generalize to more naturalistic communication contexts.

2. **Clinical Impact Assessment**: Conduct a user study with speech therapists to determine whether the improved paraphasia detection translates to better clinical outcomes, such as more accurate therapy planning or improved patient monitoring.

3. **Cross-aphasia Population Testing**: Test the model on multiple aphasic populations with varying severity levels and underlying etiologies to establish the robustness of the approach across the broader spectrum of aphasia.