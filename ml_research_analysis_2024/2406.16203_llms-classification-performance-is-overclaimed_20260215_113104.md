---
ver: rpa2
title: LLMs' Classification Performance is Overclaimed
arxiv_id: '2406.16203'
source_url: https://arxiv.org/abs/2406.16203
tags: []
core_contribution: This study investigates whether the high performance of Large Language
  Models (LLMs) in classification tasks is overstated. When gold labels are absent,
  LLMs often select incorrect options or generate new ones instead of recognizing
  no correct answer exists.
---

# LLMs' Classification Performance is Overclaimed

## Quick Facts
- arXiv ID: 2406.16203
- Source URL: https://arxiv.org/abs/2406.16203
- Reference count: 19
- Primary result: LLMs' classification performance drops significantly when gold labels are absent, revealing overclaimed capabilities

## Executive Summary
This study reveals that Large Language Models' (LLMs) classification performance is significantly overstated when gold labels are absent. While LLMs achieve high accuracy on classification tasks when gold labels are provided, their performance deteriorates markedly when these labels are removed. The authors introduce CLASSIFY-w/o-GOLD to evaluate this phenomenon and propose OMNI ACCURACY as a new metric that combines performance with and without gold labels. Experiments across multiple closed-source (GPT-4, Claude3) and open-source (Llama3, Gemma, Mistral) models demonstrate that LLMs struggle to recognize when no correct answer exists, often selecting incorrect options or generating new ones instead.

## Method Summary
The study evaluates LLMs' classification performance using the KNOW-NO benchmark, which includes three tasks: BANK-77 (intent classification), MC-TEST (multiple-choice QA), and EQUINFER (equation inference). Models are tested under two conditions: with gold labels present (w/ G) and without gold labels (w/o G). Three prompting strategies are employed for w/o G scenarios: "Hint as option," "Hint in instruction," and "No hint." The OMNI ACCURACY metric is calculated by averaging accuracy with gold labels present (Aw/) and the expected accuracy without gold labels (E[Aw/o]) across all prompt styles.

## Key Results
- LLMs achieve high accuracy with gold labels but show significant performance drops without them
- Open-source models tend to generate new answer options rather than selecting from provided options when gold labels are absent
- OMNI ACCURACY reveals that LLMs still lag considerably behind human performance in classification tasks
- Performance varies systematically between closed-source and open-source models, suggesting different training or prompting behaviors

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs fail to recognize absent gold labels because they default to selecting or generating labels from available options.
- Mechanism: When the gold label is not present, LLMs continue to treat classification as selecting the best match among provided options, rather than identifying that no correct answer exists.
- Core assumption: LLMs are primarily trained on tasks where gold labels are always present, leading them to default to selection behavior even in absence.
- Evidence anchors:
  - [abstract] "when the gold label is intentionally excluded from the label space, it becomes evident that LLMs still attempt to select from the available label candidates, even when none are correct"
  - [section] "However, when the gold label is intentionally excluded from the label space, it becomes evident that LLMs still attempt to select from the available label candidates, even when none are correct"
- Break condition: If LLMs were trained with significant exposure to "none-of-the-above" or open-set classification scenarios during pretraining.

### Mechanism 2
- Claim: OMNI ACCURACY provides a more comprehensive evaluation by combining performance with and without gold labels.
- Mechanism: By averaging accuracy across both scenarios (gold present and gold absent), OMNI ACCURACY captures limitations in task comprehension that standard accuracy metrics miss.
- Core assumption: Task comprehension requires both recognizing correct answers when present AND recognizing when no correct answer exists.
- Evidence anchors:
  - [abstract] "This work defines and advocates for a new evaluation metric, OMNI ACCURACY, which assesses LLMs' performance in classification tasks both when gold labels are present and absent"
  - [section] "We argue that OMNI ACCURACY offers a more comprehensive reflection of LLMs' classification performance"
- Break condition: If a different evaluation metric better captures the gap between LLM and human performance in classification tasks.

### Mechanism 3
- Claim: Open-source LLMs show different behavior patterns than closed-source models when gold labels are absent.
- Mechanism: Open-source models tend to generate new answer options rather than selecting from provided options, suggesting different training or prompting behaviors.
- Core assumption: Differences in pretraining data and instruction-tuning lead to systematic behavioral differences between model types.
- Evidence anchors:
  - [section] "We notice that these open-source LLMs achieve the highest performance under NO-HINT among the three w/o G prompts on both MC-TEST and BANK-77"
  - [section] "Therefore, even with HINT-AS-OPTION and HINT-IN-INSTRU, these models often ignore hints and propose self-generated answers without returning none-of-them"
- Break condition: If behavioral differences are primarily due to prompting rather than model architecture or training.

## Foundational Learning

- Concept: Classification task comprehension
  - Why needed here: Understanding whether LLMs truly comprehend classification tasks requires knowing what human-level comprehension looks like
  - Quick check question: Can you explain the difference between selecting from options and recognizing when no correct option exists?

- Concept: Prompt engineering
  - Why needed here: The study uses multiple prompting strategies to test model behavior, requiring understanding of how prompts influence responses
  - Quick check question: How would you design a prompt to explicitly ask a model to identify when no correct answer is provided?

- Concept: Evaluation metrics design
  - Why needed here: OMNI ACCURACY represents a novel approach to evaluation that combines multiple performance dimensions
  - Quick check question: What are the advantages and disadvantages of combining multiple evaluation metrics into a single score?

## Architecture Onboarding

- Component map: Benchmark datasets (BANK-77, MC-TEST, EQUINFER) -> Prompting strategies (with/without gold label, with/without hints) -> Evaluation metrics (standard accuracy, OMNI ACCURACY)
- Critical path: 1) Select dataset and model, 2) Generate prompts for both gold present and gold absent scenarios, 3) Collect and parse model responses, 4) Calculate accuracy and OMNI ACCURACY
- Design tradeoffs: Using OMNI ACCURACY provides comprehensive evaluation but requires running experiments twice (with and without gold labels), increasing computational cost
- Failure signatures: Models consistently selecting incorrect options when gold labels are absent, or generating new options without acknowledging absence of correct answers
- First 3 experiments:
  1. Run standard classification task with gold label present to establish baseline accuracy
  2. Run same task with gold label removed and "none-of-them" as an option to test hint-as-option behavior
  3. Run task with gold label removed and instruction to return "none-of-them" if no correct option exists to test hint-in-instru behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different prompt engineering techniques beyond the three tested in the study affect LLM performance in Classify-w/o-Gold tasks?
- Basis in paper: [inferred] The paper tests three prompt styles (Hint-as-option, Hint-in-instru, No-hint) and notes that "it is impossible to address them all in this paper" and encourages researchers to explore "the most appropriate form for their particular research."
- Why unresolved: The study only explores three specific prompt styles, leaving open the question of whether other prompt engineering approaches could yield significantly different results.
- What evidence would resolve it: Systematic testing of a wider variety of prompt engineering techniques (e.g., different phrasing, context lengths, few-shot examples) across multiple classification tasks and model types.

### Open Question 2
- Question: To what extent does pretraining data leakage explain the performance differences between closed-source and open-source LLMs in Classify-w/o-Gold tasks?
- Basis in paper: [explicit] The paper discusses potential data leakage, noting that "LLMs tend to generate a new option with an explanation if they believe no correct options are provided" and suggests this "may also indicate data leakage of our datasets in LLM pretraining."
- Why unresolved: While the paper raises the possibility of data leakage, it doesn't conduct a comprehensive investigation into how pretraining data exposure affects model behavior in this specific task.
- What evidence would resolve it: Detailed analysis of pretraining data sources, comparison of model performance on known training vs. unseen datasets, and investigation of model behavior when presented with subtly modified versions of training data.

### Open Question 3
- Question: How does the performance gap between humans and LLMs in Classify-w/o-Gold tasks change with model size and training duration?
- Basis in paper: [explicit] The paper compares human and LLM performance, noting that "LLMs still lag considerably behind human performance" and that performance varies significantly between different model types.
- Why unresolved: The study doesn't systematically explore how performance scales with model size or training duration, which could provide insights into the fundamental capabilities required for this task.
- What evidence would resolve it: Longitudinal studies tracking performance improvements as models increase in size and training duration, coupled with controlled experiments varying model parameters.

## Limitations

- The OMNI ACCURACY metric may conflate different types of errors (selection vs. generation of new options)
- Behavioral differences between closed-source and open-source models could be attributed to prompting strategies rather than fundamental architectural differences
- The benchmark tasks may not fully represent the diversity of real-world classification scenarios, potentially limiting generalizability

## Confidence

**High Confidence**: The core finding that LLMs' classification performance drops significantly when gold labels are absent is well-supported by experimental results across multiple models and datasets. The comparison between performance with and without gold labels is robust.

**Medium Confidence**: The interpretation that LLMs fundamentally lack task comprehension when gold labels are absent requires more evidence. While behavioral patterns are clear, the underlying cognitive mechanisms (whether this reflects true lack of comprehension versus learned behaviors) remain uncertain.

**Medium Confidence**: The claim that OMNI ACCURACY provides a more accurate representation of LLM classification capabilities is reasonable but may obscure important differences between models' abilities to recognize absent gold labels versus selecting incorrect options.

## Next Checks

1. **Behavioral Analysis**: Conduct detailed error analysis comparing the frequency of "selection of incorrect option" versus "generation of new option" across all models when gold labels are absent to validate the distinction between closed-source and open-source model behaviors.

2. **Prompt Sensitivity Test**: Systematically vary prompt phrasing and instruction clarity across all hint styles to determine whether observed behavioral differences are robust to prompting changes or primarily prompt-dependent.

3. **Cross-Dataset Generalization**: Test the same experimental protocol on additional classification datasets from different domains to verify whether the performance gap between w/ G and w/o G scenarios holds across task types and complexity levels.