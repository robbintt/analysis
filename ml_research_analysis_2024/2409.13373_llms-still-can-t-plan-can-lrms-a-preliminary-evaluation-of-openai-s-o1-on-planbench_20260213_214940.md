---
ver: rpa2
title: LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on
  PlanBench
arxiv_id: '2409.13373'
source_url: https://arxiv.org/abs/2409.13373
tags:
- object
- block
- action
- blocksworld
- following
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper evaluates OpenAI's o1 model on the PlanBench benchmark,
  which tests planning and reasoning abilities. While o1 shows significant improvement
  over previous LLMs, achieving 97.8% accuracy on basic blocksworld problems, it still
  struggles with more complex tasks and obfuscated versions of the domain.
---

# LLMs Still Can't Plan; Can LRMs? A Preliminary Evaluation of OpenAI's o1 on PlanBench

## Quick Facts
- arXiv ID: 2409.13373
- Source URL: https://arxiv.org/abs/2409.13373
- Authors: Karthik Valmeekam; Kaya Stechly; Subbarao Kambhampati
- Reference count: 40
- Primary result: o1 shows significant improvement over previous LLMs but still struggles with complex planning tasks and obfuscated domains

## Executive Summary
This paper evaluates OpenAI's o1 model on PlanBench, a benchmark for testing planning and reasoning abilities in large language models (LLMs). While o1 demonstrates substantial improvements over previous LLMs, achieving 97.8% accuracy on basic blocksworld problems, it still faces significant challenges with more complex tasks and obfuscated versions of the domain. The model's performance degrades quickly with increased problem size and it fails to reliably identify unsolvable instances. The authors argue that despite o1's advancements, it is not yet robust enough for real-world deployment in safety-critical domains.

## Method Summary
The paper tests OpenAI's o1-preview and o1-mini models on the PlanBench benchmark, which includes 600 three to five block Blocksworld problems, 600 obfuscated Mystery Blocksworld problems, and 110 larger Blocksworld problems (6 to 20 blocks). The evaluation compares o1's performance with previous LLMs and Fast Downward using zero-shot and one-shot prompts. The study focuses on accuracy, efficiency, cost, and the model's ability to identify unsolvable instances.

## Key Results
- o1 achieves 97.8% accuracy on basic Blocksworld problems, significantly outperforming previous LLMs
- Performance degrades to 68.9% on obfuscated Mystery Blocksworld problems and further to 40.9% on larger 6-20 block problems
- o1 struggles to identify unsolvable instances, correctly identifying only 16% of unsolvable Mystery Blocksworld problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: o1 uses reinforcement learning over private chain-of-thought traces to improve reasoning quality
- Mechanism: The model generates multiple reasoning paths internally, evaluates their correctness using a learned value function, and selects the highest-scoring trace before producing a final answer
- Core assumption: The RL training phase provides o1 with a learned "reward signal" that approximates the correctness of reasoning steps
- Evidence anchors: [abstract] "o1 seems to have been trained to be an approximate reasoner" and "RL-trained system in the same vein as AlphaGo, but where the ‘moves’ being generated and evaluated are Chains of Thought." [section] "o1 combines an underlying LLM, most likely a modified GPT-4o, into an RL-trained system that steers the creation, curation, and final selection of private Chain-of-Thought reasoning traces."

### Mechanism 2
- Claim: o1 adaptively scales inference-time compute based on problem difficulty
- Mechanism: The model dynamically allocates more reasoning tokens for harder problems, trading off cost and latency against accuracy
- Core assumption: The number of reasoning tokens correlates with problem complexity, and the model can self-assess when to stop reasoning
- Evidence anchors: [section] "measuring efficiency has become much more important" and "o1’s price-per-call includes a surcharge based on the number of 'reasoning tokens' it used." [section] "The early version of o1-preview that we have access to seems to be limited in the number of reasoning tokens it uses per problem."

### Mechanism 3
- Claim: o1 sometimes correctly identifies unsolvable problems by recognizing logical contradictions during reasoning
- Mechanism: The model's internal reasoning traces include consistency checks that can detect when no valid plan exists, leading to a "no solution" output
- Core assumption: The reasoning process can catch unsatisfiable constraints before producing a full (invalid) plan
- Evidence anchors: [section] "o1 was launched with the claim that it has started to overcome this issue, and can now accurately identify unsolvable problems." [section] "On Randomized Mystery Blocksworld, 16% of cases were correctly identified as unsolvable, 5% returned an empty plan."

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: Understanding how o1's internal reasoning traces differ from standard LLM outputs is key to interpreting its performance gains and limitations
  - Quick check question: What is the main difference between a standard LLM output and an LRM's CoT trace?

- Concept: Reinforcement learning from human feedback (RLHF) and its extensions
  - Why needed here: o1's training involves RL over synthetic data, which is central to its reasoning improvements
  - Quick check question: How does RLHF differ from supervised fine-tuning in the context of reasoning tasks?

- Concept: PDDL (Planning Domain Definition Language)
  - Why needed here: PlanBench uses PDDL to define planning problems, and understanding its syntax is essential for interpreting benchmark results
  - Quick check question: What are the core components of a PDDL domain file?

## Architecture Onboarding

- Component map: Base LLM (likely GPT-4o variant) -> RL training module (synthetic data generation + reward shaping) -> Inference-time reasoning engine (private CoT generation and selection) -> API layer (token billing, including reasoning tokens)
- Critical path: 1. Receive prompt 2. Generate multiple CoT traces internally 3. Evaluate traces using learned value function 4. Select best trace and produce final answer 5. Bill for input + output + reasoning tokens
- Design tradeoffs: Accuracy vs. cost: More reasoning tokens → higher accuracy but higher cost; Latency vs. robustness: Longer reasoning → slower responses but potentially better handling of complex problems; Transparency vs. performance: Hidden CoT traces improve performance but reduce interpretability
- Failure signatures: High variance in token usage across similar problems; Incorrect plans on obfuscated domains despite success on clean versions; Inconsistent handling of unsolvable instances
- First 3 experiments: 1. Compare o1's performance on clean vs. obfuscated Blocksworld to quantify robustness 2. Measure token usage vs. problem difficulty to understand cost scaling 3. Test o1 on a set of known unsolvable instances to evaluate its detection accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LRMs be made more cost-effective for planning tasks while maintaining or improving accuracy?
- Basis in paper: [explicit] The paper discusses that LRMs like o1 are significantly more expensive than LLMs and classical planners, with costs per 100 instances being $42.12 for o1-preview compared to $0.44 for Claude 3.5 Sonnet
- Why unresolved: While the paper highlights the cost issue, it does not explore potential optimizations or architectural changes that could make LRMs more efficient
- What evidence would resolve it: Experimental results showing improved cost-effectiveness of LRMs through techniques like more efficient inference scaling, model compression, or hybrid approaches combining LRMs with classical planners

### Open Question 2
- Question: How can we develop more robust evaluation methods for LRMs that go beyond accuracy to include guarantees and interpretability?
- Basis in paper: [explicit] The paper argues that current LRM evaluations lack guarantees of correctness and interpretability, making them unsuitable for safety-critical domains
- Why unresolved: While the paper identifies this limitation, it does not propose specific methods or benchmarks to address these gaps in LRM evaluation
- What evidence would resolve it: New evaluation frameworks that incorporate formal verification, explainability metrics, and safety guarantees, along with experimental results demonstrating their effectiveness

### Open Question 3
- Question: What is the impact of problem size and complexity on LRM performance, and can LRMs be scaled to handle larger, more complex planning problems?
- Basis in paper: [explicit] The paper shows that o1's performance degrades significantly when tested on larger blocksworld problems requiring 20-40 steps to solve
- Why unresolved: The paper does not explore whether this degradation is inherent to the LRM architecture or if there are ways to scale LRMs to handle more complex problems
- What evidence would resolve it: Experimental results comparing LRM performance on increasingly complex planning tasks, along with analyses of the factors contributing to performance degradation

## Limitations
- Evaluation focused primarily on Blocksworld domains, potentially missing failure modes in other planning domains
- Access was limited to early versions (o1-preview and o1-mini), which may not represent the final model's capabilities
- Exact architecture and training methodology of o1 remain opaque, limiting understanding of its reasoning mechanisms

## Confidence
- High confidence: o1's superior performance on basic Blocksworld problems compared to previous LLMs
- Medium confidence: o1's degradation pattern on larger problems and obfuscated domains
- Low confidence: Generalization of findings to other planning domains or real-world applications

## Next Checks
1. **Cross-domain generalization test**: Evaluate o1 on diverse planning benchmarks (e.g., logistics, scheduling) to assess whether performance patterns observed in Blocksworld generalize to other domains

2. **Cost-efficiency analysis**: Conduct a detailed study measuring the relationship between reasoning token usage, problem difficulty, and solution quality to establish practical cost bounds for deployment

3. **Unsolvable instance detection benchmark**: Create a comprehensive test suite of provably unsolvable planning problems across multiple domains to rigorously evaluate o1's ability to correctly identify and reject such instances