---
ver: rpa2
title: Augmenting In-Context-Learning in LLMs via Automatic Data Labeling and Refinement
arxiv_id: '2410.10348'
source_url: https://arxiv.org/abs/2410.10348
tags:
- examples
- table
- pool
- samples
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces ADLR, a method to automatically generate and
  refine in-context examples for LLM inference on tasks requiring intermediate reasoning
  steps, such as table QA and mathematical reasoning. Starting from a small set of
  manually crafted examples, ADLR uses a baseline algorithm to annotate a large training
  set with intermediate outputs (e.g., code or chain-of-thought), then filters these
  examples based on their difficulty and usefulness in solving hard queries.
---

# Augmenting In-Context-Learning in LLMs via Automatic Data Labeling and Refinement

## Quick Facts
- arXiv ID: 2410.10348
- Source URL: https://arxiv.org/abs/2410.10348
- Reference count: 24
- Primary result: ADLR improves LLM accuracy on table QA by up to 5.5% using automatically generated and filtered in-context examples

## Executive Summary
ADLR addresses the challenge of manual effort required to create effective in-context learning demonstrations for complex reasoning tasks. The method automatically generates intermediate-step examples (like code or chain-of-thought) from a small set of manually crafted examples, then filters these to retain only the most difficult and useful demonstrations. When used in diverse prompts for inference, these refined examples improve LLM performance on table QA and mathematical reasoning tasks compared to baseline approaches like Binder and ReAcTable.

## Method Summary
ADLR operates through a three-step process: First, a baseline algorithm generates intermediate outputs (e.g., executable code or CoT reasoning) for a large training dataset. Second, these examples are filtered based on two metrics - difficulty (examples that the LLM struggles to solve across multiple runs) and utility (examples that help solve other hard queries in one-shot regime). Third, the refined examples are used to create multiple diverse prompts through random sampling, with LLM inference results aggregated via majority voting to improve stability and accuracy.

## Key Results
- Achieved up to 5.5% accuracy gains on table QA tasks compared to baseline methods
- Improved performance on the TabMWP dataset for mathematical reasoning
- Demonstrated effectiveness of automatic example generation and refinement over manual demonstration creation

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** ADLR improves ICL by enriching prompts with hard, useful examples rather than relying solely on manually crafted demonstrations.
- **Mechanism:** The method first generates intermediate outputs (e.g., code or CoT reasoning) for a large training set using a baseline algorithm. It then filters these to retain examples that are both difficult for the LLM (low success rate across multiple runs) and useful for solving other hard samples (high one-shot success rate).
- **Core assumption:** Examples that are difficult for the LLM but can solve other hard examples contain valuable knowledge that, when included in the prompt, improves performance.
- **Evidence anchors:** [abstract] "providing demonstrations which include intermediate steps requires cumbersome manual work"; [section 3.2] "restrict the pool of examples to the hard ones, as those possibly contain more new information for the LLM"
- **Break condition:** If the baseline algorithm generates incorrect intermediate steps or if the difficulty metric poorly correlates with example utility, the filtering process may retain low-quality examples.

### Mechanism 2
- **Claim:** Running the LLM multiple times with non-zero temperature and aggregating via majority vote improves both accuracy and stability.
- **Mechanism:** Multiple inference runs with stochastic outputs allow identification of consistently solvable samples (via the difficulty metric D(s) = k/N). This enables selection of examples that the LLM finds challenging but solvable with sufficient attempts.
- **Core assumption:** LLM stochasticity under temperature > 0 reflects true sample difficulty and helps identify examples that benefit from additional context.
- **Evidence anchors:** [section 3.2] "by common practice the LLM is executed N times (with N ~ 10) with the sample prompt, while setting the LLM temperature at non-zero value"; [section 3.3] "we used the enlarged examples pool to increase the number of demonstrations in each prompt"
- **Break condition:** If temperature has minimal effect on output diversity or if the LLM is deterministic, the difficulty metric loses discriminative power.

### Mechanism 3
- **Claim:** Randomly sampling diverse examples from a large refined pool creates more effective prompts than fixed, hand-crafted contexts.
- **Mechanism:** Instead of using a single static context, ADLR generates multiple prompts by randomly sampling from the refined pool, increasing diversity and coverage of problem types.
- **Core assumption:** Diversity in prompt examples exposes the LLM to a broader range of reasoning patterns, improving generalization.
- **Evidence anchors:** [section 3.3] "we have experimented with similarity-based K-NN method... but eventually we chose to use random sampling"; [section 4.2] "One immediate usage for the pool-A data is to finetune the LLM"
- **Break condition:** If the refined pool lacks diversity or if the LLM overfits to certain example patterns, random sampling may not improve performance.

## Foundational Learning

- **Concept:** In-Context Learning (ICL)
  - **Why needed here:** ICL is the paradigm where LLMs learn from examples provided inline in the prompt, crucial for tasks requiring intermediate reasoning steps.
  - **Quick check question:** What distinguishes ICL from fine-tuning in terms of model adaptation?

- **Concept:** Chain-of-Thought (CoT) Reasoning
  - **Why needed here:** CoT demonstrates step-by-step reasoning to solve complex problems, providing intermediate outputs that guide the LLM.
  - **Quick check question:** How does CoT differ from direct answer generation in terms of intermediate steps?

- **Concept:** Few-Shot Learning
  - **Why needed here:** ADLR builds on few-shot learning by automatically generating and refining demonstrations to improve performance without additional training.
  - **Quick check question:** Why is few-shot learning particularly useful when labeled data is scarce?

## Architecture Onboarding

- **Component map:** Baseline Algorithm → Intermediate Output Generator → Example Pool (Pool A) → Difficulty Filter → Hard Example Pool (Pool B) → Utility Filter → Refined Example Pool (Pool C) → Multiple Diverse Prompts → LLM Inference → Majority Vote Aggregation

- **Critical path:** Generate intermediate outputs → Filter for difficulty → Filter for utility → Create diverse prompts → Run LLM inference → Aggregate results

- **Design tradeoffs:**
  - Using a larger example pool increases prompt diversity but risks exceeding context limits
  - Random sampling is simpler than similarity-based selection but may miss optimal examples
  - Majority voting improves stability but increases computational cost

- **Failure signatures:**
  - No improvement over baseline: Baseline algorithm may generate poor intermediate outputs or filters may be too strict/loose
  - Decreased accuracy: Refined pool may lack diversity or include misleading examples
  - High computational cost: Multiple LLM runs per sample may be prohibitive for large-scale deployment

- **First 3 experiments:**
  1. **Baseline replication:** Run the baseline algorithm (e.g., Binder) on a small dataset to verify accuracy and generate Pool A
  2. **Difficulty filtering:** Apply the difficulty filter to Pool A to create Pool B and verify the distribution of solved examples
  3. **Utility filtering:** Use one-shot inference to filter Pool B into Pool C and measure the success rate of examples in solving hard queries

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** What is the optimal threshold for filtering examples in Part II of ADLR based on difficulty and utility?
- **Basis in paper:** Inferred from the methodology section discussing the selection of pool B and pool C examples based on difficulty metrics (D(s) = k/N < 0.2) and utility thresholds (solving at least 10% of hard queries).
- **Why unresolved:** The paper uses specific thresholds (0.2 for difficulty and 10% for utility) but does not explore how varying these thresholds might affect the performance of ADLR. Different datasets or tasks might benefit from different thresholds.
- **What evidence would resolve it:** Conducting experiments with varying thresholds on multiple datasets to determine the impact on performance and identify optimal thresholds for different scenarios.

### Open Question 2
- **Question:** How does the performance of ADLR compare when using different types of intermediate steps (e.g., executable code vs. chain-of-thought reasoning) across various tasks?
- **Basis in paper:** Explicit from the discussion on the application of ADLR to table QA tasks using code generation and mathematical reasoning tasks using chain-of-thought reasoning, with demonstrated performance improvements.
- **Why unresolved:** While the paper shows improvements for specific tasks, it does not explore the effectiveness of ADLR across a broader range of tasks or compare the performance when using different types of intermediate steps.
- **What evidence would resolve it:** Applying ADLR to a diverse set of tasks requiring different intermediate steps and comparing the performance gains across these tasks to identify which types of intermediate steps yield the most significant improvements.

### Open Question 3
- **Question:** What are the computational trade-offs of using ADLR in terms of inference time and resource utilization compared to baseline methods?
- **Basis in paper:** Inferred from the description of ADLR's multi-step process and the use of multiple diverse contexts, which suggests increased computational demands.
- **Why unresolved:** The paper focuses on accuracy improvements but does not provide a detailed analysis of the computational costs associated with ADLR, such as increased inference time or resource usage.
- **What evidence would resolve it:** Measuring and comparing the inference time and resource utilization of ADLR against baseline methods across various datasets and tasks to quantify the trade-offs between accuracy gains and computational costs.

## Limitations
- The method's effectiveness depends heavily on the quality of the baseline algorithm's intermediate outputs
- Arbitrary thresholds (0.2 difficulty, 10% utility) lack sensitivity analysis or theoretical justification
- Computational overhead of multiple LLM runs and diverse prompt generation is not quantified

## Confidence

**High Confidence:** The core intuition that filtering for difficult-yet-useful examples should improve prompts is logically sound; majority voting for stability is a well-established technique with predictable effects

**Medium Confidence:** Random sampling for diversity is reasonable but lacks comparative validation against similarity-based methods; the claim that diverse prompts improve generalization needs more empirical support

**Low Confidence:** The 5.5% accuracy improvement is promising but may not generalize beyond the specific datasets and baselines tested; no statistical significance testing is reported for the performance differences

## Next Checks

1. **Ablation on Filtering Thresholds:** Systematically vary the difficulty (0.2) and usefulness (10%) thresholds to determine their impact on accuracy and identify optimal values for different task types

2. **Baseline Algorithm Sensitivity:** Replace the baseline algorithm with a simpler or noisier intermediate-output generator to quantify how much ADLR's performance depends on baseline quality

3. **Computational Cost-Benefit Analysis:** Measure the wall-clock time and API costs for ADLR versus baseline methods across different example pool sizes to determine practical scalability limits