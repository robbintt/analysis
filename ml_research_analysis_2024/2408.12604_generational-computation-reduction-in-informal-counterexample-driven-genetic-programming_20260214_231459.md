---
ver: rpa2
title: Generational Computation Reduction in Informal Counterexample-Driven Genetic
  Programming
arxiv_id: '2408.12604'
source_url: https://arxiv.org/abs/2408.12604
tags:
- cases
- training
- icdgp
- program
- case
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces informal counterexample-driven genetic programming
  (iCDGP), a method that applies the core idea of counterexample-driven GP to general
  program synthesis problems without requiring formal specifications. iCDGP uses a
  small active training set that grows by adding counterexamples whenever an individual
  passes all current cases.
---

# Generational Computation Reduction in Informal Counterexample-Driven Genetic Programming

## Quick Facts
- arXiv ID: 2408.12604
- Source URL: https://arxiv.org/abs/2408.12604
- Reference count: 40
- This paper introduces informal counterexample-driven genetic programming (iCDGP), a method that applies the core idea of counterexample-driven GP to general program synthesis problems without requiring formal specifications.

## Executive Summary
This paper introduces informal counterexample-driven genetic programming (iCDGP), a method that applies the core idea of counterexample-driven GP to general program synthesis problems without requiring formal specifications. iCDGP uses a small active training set that grows by adding counterexamples whenever an individual passes all current cases. The method is evaluated on 12 program synthesis benchmark problems and shows faster solution discovery compared to standard GP. Two variants are proposed: one that adds cases based on a fitness threshold and another that adds cases every fixed number of generations. The latter variant significantly outperforms standard GP on 5 of 12 problems. The study also confirms that adding counterexample cases to the training set is crucial for performance, as static subsampling alone performs significantly worse.

## Method Summary
iCDGP maintains a small active training set (TA) initialized with 10 random cases from the full training set (T). During evolution, programs are evaluated only on TA, reducing per-generation computation. When the best program passes all cases in TA, it's tested on T, and any failed cases are added to TA as counterexamples. The method uses lexicase selection with UMAD mutation. Two variants are tested: one that adds cases when the best program exceeds a fitness threshold (q=0.8), and another that adds cases every fixed number of generations regardless of performance. The method is implemented in PushGP and tested on 12 benchmark problems from PSB1.

## Key Results
- iCDGP finds solutions faster than standard GP, with fewer program executions required
- Generation-based case addition variant significantly outperforms standard GP on 5 of 12 problems
- Adding counterexample cases to the training set is crucial for performance, as static subsampling alone performs significantly worse
- iCDGP's success rate is significantly better than standard GP for 3 problems and nearly so for 2 more

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Reducing the number of fitness cases evaluated per generation lowers per-program execution cost, enabling more generations within the same computational budget.
- Mechanism: By evaluating each program on only a subset of the available training cases (the active training set TA), fewer fitness evaluations are needed per generation. This leaves more computational budget for additional generations, which increases the diversity of programs evaluated overall.
- Core assumption: The diversity gain from more generations outweighs any loss in selection pressure from incomplete fitness information.
- Evidence anchors:
  - [abstract] "finds solutions faster (i.e. with fewer program executions) than standard GP"
  - [section] "a smaller active training set allows iCDGP to perform fewer program executions per generation, making each generation computationally cheaper"
- Break condition: If selection pressure becomes too weak due to small TA, evolution may stall or converge prematurely.

### Mechanism 2
- Claim: Adding counterexamples to the active training set focuses evolutionary pressure on parts of the problem space that are currently unsolved by the population.
- Mechanism: When the best individual passes all cases in TA but fails some cases in T, those failed cases are added to TA as counterexamples. This increases the difficulty of the fitness landscape in a targeted way, guiding the population to solve cases that were previously missed.
- Core assumption: Counterexamples selected in this way are more informative for search than random additions.
- Evidence anchors:
  - [abstract] "These 'counterexamples' provide more focused guidance to the evolutionary process than do random test cases"
  - [section] "when training cases are added to the training set, they are not chosen randomly, but rather are chosen to be counterexamples for the best individuals in the current population"
- Break condition: If counterexample selection becomes too frequent or too difficult, TA may grow too large, eroding the computational benefit.

### Mechanism 3
- Claim: Generation-based case additions ensure continuous training set growth even when no individual fully passes TA, preventing evolutionary stagnation.
- Mechanism: Instead of waiting for an individual to pass all of TA, a new counterexample is added every d generations based on the best individual's performance on T. This guarantees that TA continues to grow and that search remains guided even in plateaus.
- Core assumption: Adding cases without requiring full TA mastery still improves search guidance.
- Evidence anchors:
  - [section] "many times iCDGP cannot find a program that passes all (or even a sufficiently high percentage to exceed a fitness threshold) of the active training set without passing all training cases"
  - [section] "This variant ensures that new cases are added to the training set throughout evolution, whether or not a program is found that passes the current training set"
- Break condition: If d is too small, TA may grow too quickly and lose the benefit of low per-generation cost.

## Foundational Learning

- Concept: Fitness case evaluation in genetic programming
  - Why needed here: Understanding how fitness is computed is critical to grasping the impact of reducing the number of cases per evaluation.
  - Quick check question: What is the trade-off between using the full training set vs. a subset when evaluating program fitness?

- Concept: Lexicase selection and diversity preservation
  - Why needed here: iCDGP uses lexicase selection, and its interaction with counterexample addition can cause hyperselection events that temporarily reduce diversity.
  - Quick check question: How does lexicase selection behave when one individual passes all cases in the active training set?

- Concept: Counterexample-driven learning in program synthesis
  - Why needed here: The core novelty of iCDGP is applying the counterexample concept without formal specifications, so understanding how counterexamples guide search is key.
  - Quick check question: In iCDGP, how are new counterexamples chosen and added to the training set?

## Architecture Onboarding

- Component map:
  - Training set T -> Active training set TA -> Evolution engine (PushGP + lexicase + UMAD) -> Counterexample module -> Performance monitor

- Critical path:
  1. Initialize TA with 10 random cases from T
  2. Evaluate each program in population on TA
  3. Select parents via lexicase selection
  4. Generate offspring via UMAD mutation
  5. If best program passes all TA, test on T; if not, add counterexample(s) to TA
  6. Repeat until solution found or execution budget exhausted

- Design tradeoffs:
  - Small TA reduces per-generation cost but may weaken selection pressure
  - Frequent counterexample addition improves guidance but risks TA growth
  - Fixed TA size variant caps growth but may remove useful cases
  - Generation-based additions ensure growth but may add less informative cases

- Failure signatures:
  - TA grows to full size without finding a solution: suggests counterexample selection is not effective
  - Population diversity collapses after hyperselection events: suggests lexicase selection is too brittle with iCDGP
  - No counterexamples are ever added: suggests TA is too easy or program space is too constrained

- First 3 experiments:
  1. Compare success rate of iCDGP vs. standard GP on a small benchmark with fixed TA size
  2. Test effect of generation-based vs. fitness-threshold counterexample addition on TA growth rate
  3. Measure impact of TA size cap on both computational efficiency and success rate

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does adding any random case from T \ TA provide the same benefits as adding a case not passed by the best individual in generation-based case additions?
- Basis in paper: [explicit] The paper suggests this as a potential area for future work, noting that it would be useful to learn whether selecting a new case that is not passed by the best individual helps, or if adding any random case from T \ TA would be sufficient.
- Why unresolved: The paper implemented generation-based case additions using the best individual to select cases, but did not test the alternative approach of random case selection.
- What evidence would resolve it: A controlled experiment comparing iCDGP with generation-based case additions using the best individual versus random case selection, measuring success rates and program execution efficiency.

### Open Question 2
- Question: How do other parent selection methods (e.g., tournament selection) interact with iCDGP's case addition mechanism compared to lexicase selection?
- Basis in paper: [inferred] The paper extensively discusses the interaction between iCDGP and lexicase selection, particularly regarding hyperselection events, but does not explore other selection methods.
- What evidence would resolve it: Comparative experiments implementing iCDGP with different parent selection methods (tournament, roulette wheel, etc.) while maintaining the same case addition mechanism, measuring effects on population diversity and problem-solving performance.

### Open Question 3
- Question: What is the optimal size for the active training set TA at the start of iCDGP runs?
- Basis in paper: [explicit] The paper uses an initial TA size of 10 cases but notes that "other sizes could be used" without investigating optimal sizes.
- What evidence would resolve it: Systematic experiments varying the initial TA size (e.g., 5, 10, 20, 50 cases) across the benchmark problems, measuring success rates and convergence speed to identify optimal initial sizes.

## Limitations

- The study focuses on relatively small benchmarks (100-250 training cases), so scalability to larger problems remains untested
- The impact of hyperselection events on long-term diversity and solution quality is not thoroughly explored
- Only one fitness threshold value (q=0.8) is tested for the threshold-based variant

## Confidence

- Confidence is **High** for the claim that iCDGP reduces per-generation computational cost and finds solutions faster than standard GP on the tested benchmarks
- Confidence is **Medium** for the claim that counterexample addition is crucial for performance, as the study only compares iCDGP to a static subsampling variant
- Confidence is **Low** for the claim that generation-based additions are superior to fitness-threshold additions, as the study only tests one threshold value and limited generation intervals

## Next Checks

1. Test iCDGP on larger program synthesis problems (e.g., 1000+ training cases) to evaluate scalability and the impact of TA growth on computational efficiency
2. Compare iCDGP to alternative adaptive case selection strategies, such as random case addition or adding cases based on population-wide performance rather than the best individual
3. Analyze the long-term effects of hyperselection events on population diversity and solution quality by tracking diversity metrics throughout evolution and comparing final solutions from runs with and without iCDGP