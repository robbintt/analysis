---
ver: rpa2
title: 'QNCD: Quantization Noise Correction for Diffusion Models'
arxiv_id: '2403.19140'
source_url: https://arxiv.org/abs/2403.19140
tags:
- quantization
- noise
- diffusion
- sampling
- qncd
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of deploying diffusion models\
  \ on resource-constrained devices through post-training quantization (PTQ), which\
  \ typically degrades image quality especially in low-bit settings. The authors identify\
  \ two sources of quantization noise\u2014intra noise from embedding-induced feature\
  \ distribution changes and inter noise from cumulative deviations across sampling\
  \ steps\u2014and propose QNCD to correct both."
---

# QNCD: Quantization Noise Correction for Diffusion Models

## Quick Facts
- **arXiv ID**: 2403.19140
- **Source URL**: https://arxiv.org/abs/2403.19140
- **Reference count**: 29
- **Primary result**: QNCD achieves near-lossless W4A8 and W8A8 quantization on ImageNet (LDM-4), reducing FID from 41.25 to 20.14 on W4A6

## Executive Summary
This paper addresses the challenge of deploying diffusion models on resource-constrained devices through post-training quantization (PTQ), which typically degrades image quality especially in low-bit settings. The authors identify two sources of quantization noise—intra noise from embedding-induced feature distribution changes and inter noise from cumulative deviations across sampling steps—and propose QNCD to correct both. Intra noise is mitigated by channel-wise smoothing factors derived from embeddings, making features more quantization-friendly. Inter noise is estimated and filtered using a runtime noise estimation module that leverages the diffusion model's training properties. Extensive experiments show QNCD achieves near-lossless results at W4A8 and W8A8 quantization on ImageNet (LDM-4), significantly outperforming prior methods.

## Method Summary
QNCD targets the quality degradation in quantized diffusion models by addressing two distinct sources of quantization noise. The first component, intra noise correction, uses channel-wise smoothing factors derived from embeddings to make feature distributions more quantization-friendly, reducing the embedding-induced feature distribution changes that occur during quantization. The second component, inter noise correction, estimates and filters quantization noise at runtime through a noise estimation module that exploits properties of the diffusion model's training process. The method is specifically designed for post-training quantization scenarios where retraining is not feasible, making it practical for deployment on existing models.

## Key Results
- QNCD achieves near-lossless quantization at W4A8 and W8A8 bit-widths on ImageNet using LDM-4
- On ImageNet W4A6, QNCD reduces FID from 41.25 to 20.14, demonstrating effectiveness in low-bit scenarios
- QNCD outperforms prior quantization methods significantly across multiple bit-width configurations

## Why This Works (Mechanism)
The method works by decomposing quantization noise into two components and addressing each systematically. Intra noise arises from changes in feature distributions when embeddings are quantized, which disrupts the delicate balance diffusion models rely on. By introducing channel-wise smoothing factors based on embedding similarity, QNCD makes these features more resilient to quantization errors. Inter noise accumulates across sampling steps as quantization errors compound, and the runtime noise estimation module corrects this by estimating the noise at each step and applying appropriate corrections based on the diffusion model's training properties.

## Foundational Learning

**Post-training quantization (PTQ)**: Method for reducing model size and computation without retraining. Needed to enable deployment on resource-constrained devices without access to training data or computational resources. Quick check: Verify quantization affects both weights and activations, and understand the difference from quantization-aware training.

**Diffusion models**: Generative models that iteratively denoise random noise through a learned reverse process. Needed as the target architecture where quantization noise has particularly severe effects. Quick check: Understand the U-Net architecture and how it's used in the denoising process.

**Feature distribution sensitivity**: How quantization changes the statistical properties of model activations. Needed to understand why embedding quantization causes quality degradation. Quick check: Examine how activation distributions shift when moving from FP32 to lower precision.

## Architecture Onboarding

**Component map**: Quantization-aware U-Net -> Channel-wise smoothing module -> Noise estimation module -> Corrected U-Net

**Critical path**: The forward pass through the U-Net is modified to include both smoothing factors at the embedding layers and noise correction at each sampling step. The critical path involves the noise estimation module which must run at inference time to correct inter noise.

**Design tradeoffs**: The method trades additional inference-time computation (for noise estimation) against improved image quality. Channel-wise smoothing adds minimal overhead but requires embedding statistics. The noise estimation module adds runtime overhead but is essential for correcting inter noise accumulation.

**Failure signatures**: Poor performance would manifest as visible artifacts in generated images, particularly structured noise patterns indicating uncorrected inter noise or feature distribution mismatches indicating inadequate intra noise correction.

**First experiments**:
1. Apply QNCD to a pre-trained LDM-4 model quantized to W4A6 and evaluate FID score
2. Perform ablation study removing the noise estimation module to quantify inter noise contribution
3. Test QNCD on a different dataset (e.g., LSUN) to verify generalizability

## Open Questions the Paper Calls Out

None

## Limitations

- Evaluation is limited to ImageNet at 256x256 resolution using LDM-4, restricting generalizability to other datasets and resolutions
- The linear relationship assumption between embedding similarity and quantization sensitivity may not hold across diverse model architectures
- Computational overhead of the noise estimation module for edge deployment is not thoroughly characterized

## Confidence

- **High Confidence**: The identification of quantization-induced quality degradation in diffusion models and the empirical demonstration of QNCD's effectiveness on the tested benchmarks
- **Medium Confidence**: The generalizability of QNCD across different diffusion model architectures, datasets, and bit-width settings beyond the reported configurations
- **Medium Confidence**: The scalability and computational efficiency claims for edge deployment, given limited evaluation of runtime overhead

## Next Checks

1. Evaluate QNCD on diverse datasets (e.g., LSUN, CelebA) and higher resolutions (1024x1024) to assess generalizability beyond ImageNet-256
2. Quantify the computational overhead of the noise estimation module in terms of inference latency and memory usage, particularly for mobile/edge deployment scenarios
3. Conduct ablation studies isolating the contributions of intra noise correction versus inter noise correction to validate the decomposition of quantization noise sources