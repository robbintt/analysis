---
ver: rpa2
title: 'ICM-Assistant: Instruction-tuning Multimodal Large Language Models for Rule-based
  Explainable Image Content Moderation'
arxiv_id: '2412.18216'
source_url: https://arxiv.org/abs/2412.18216
tags:
- moderation
- image
- explanation
- sexy
- rules
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a novel rule-based dataset generation pipeline
  for explainable image content moderation. The method decomposes concise human-defined
  rules into orthogonal attributes, uses multi-stage prompts to enrich short annotations,
  and generates detailed moderation explanations and Q-A pairs.
---

# ICM-Assistant: Instruction-tuning Multimodal Large Language Models for Rule-based Explainable Image Content Moderation

## Quick Facts
- arXiv ID: 2412.18216
- Source URL: https://arxiv.org/abs/2412.18216
- Reference count: 8
- Primary result: Rule-based dataset generation pipeline for explainable image content moderation using orthogonal attribute decomposition and multi-stage prompts

## Executive Summary
The paper addresses a critical gap in image content moderation: existing multimodal large language models (MLLMs) produce inconsistent classifications and explanations when applied to rule-based moderation tasks. The proposed ICM-Assistant model introduces a novel approach that decomposes human-defined moderation rules into orthogonal attribute products, uses multi-stage prompts to generate rich explanations and Q-A pairs, and instruction-tunes various MLLMs to achieve exceptional performance. The method demonstrates significant improvements in both moderation classification (36.8% on average) and explanation quality (26.6% on average) while maintaining flexibility to adapt to different cultural norms and age groups.

## Method Summary
The method employs a three-stage pipeline: first, human-defined moderation rules are decomposed into orthogonal attribute products (e.g., body position, exposure, action type combinations); second, multi-stage prompts progressively generate explicit content descriptions, implicit contextual information, and detailed moderation explanations using MLLMs, with LLMs generating corresponding Q-A pairs; third, the generated ICM-Instruct dataset is used to instruction-tune various MLLMs through either mixed SFT (combining with general datasets) or continuous SFT approaches. This framework enables the model to produce both accurate moderation classifications and human-aligned explanations while maintaining flexibility across different rule sets.

## Key Results
- ICM-Assistant achieves 94.8% average accuracy on test sets with 83.3% moderation explanation quality
- Outperforms existing MLLMs by 36.8% in classification and 26.6% in explanation quality on average
- Demonstrates strong zero-shot performance on "horrifying" and "gambling" moderation terms not in training data
- Maintains flexibility to adapt to different cultural norms and age groups through rule decomposition

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing moderation rules into orthogonal attribute products improves model flexibility across cultural norms and age groups.
- Mechanism: The rule decomposition pipeline splits complex moderation rules into distinct, non-overlapping attributes (e.g., body position, body exposure, action type). These attributes are combined into attribute products that represent all possible rule variations, enabling the model to handle different cultural and age-specific requirements by simply changing attribute labels.
- Core assumption: Moderation rules for different cultural norms and age groups can be expressed as combinations of a small set of orthogonal attributes.
- Evidence anchors:
  - [abstract] "The proposed ICM-Assistant model demonstrates exceptional performance and flexibility. Specifically, it significantly outperforms existing approaches on various sources, improving both the moderation classification (36.8% on average) and moderation explanation quality (26.6% on average) consistently over existing MLLMs."
  - [section] "Given pre-defined moderation rules, we decompose it into several sub-categories, termed attribute products. Therefore, we can easily adapt to different moderation rules by changing the classification labels of attribute products."
- Break condition: If real-world moderation rules cannot be decomposed into orthogonal attributes, or if attribute combinations don't capture rule nuances, the model's flexibility will break down.

### Mechanism 2
- Claim: Multi-stage prompt-guided data augmentation creates rich, explainable moderation explanations that align with human reasoning.
- Mechanism: The method uses a progressive prompt pipeline with three stages: (1) explicit content description, (2) implicit content description (atmosphere/overall context), and (3) explanation generation based on both explicit and implicit content. This chain-of-thought approach decomposes the reasoning process, making explanations more accurate and interpretable.
- Core assumption: Breaking down explanation generation into distinct reasoning steps produces more accurate and human-aligned explanations than single-shot generation.
- Evidence anchors:
  - [abstract] "The method decomposes concise human-defined rules into orthogonal attributes, uses multi-stage prompts to enrich short annotations, and generates detailed moderation explanations and Q-A pairs."
  - [section] "we adopt multi-stage prompts to progressively generate and enrich moderation explanations and Q-A pairs from the short annotations based on the 'Chain-of-Thought' (CoT) (Wei et al. 2022), which decomposes ICM to several steps and makes ICM more accurately and interpretably."
- Break condition: If the multi-stage reasoning process introduces inconsistency or if the prompts don't properly guide the model through the reasoning chain, explanation quality will degrade.

### Mechanism 3
- Claim: Instruction tuning with domain-specific datasets significantly improves MLLM performance on rule-based content moderation tasks.
- Mechanism: The ICM-Instruct dataset provides structured training data including explicit descriptions, enriched explanations, and Q-A pairs. This dataset is used to instruction-tune various MLLMs through either mixed SFT (combining with general datasets) or continuous SFT (sequential fine-tuning), injecting content moderation reasoning knowledge into the models.
- Core assumption: MLLMs can effectively learn rule-based moderation reasoning when trained on a well-structured, domain-specific dataset that captures the reasoning process.
- Evidence anchors:
  - [abstract] "Our ICM-Assistant model demonstrates exceptional performance and flexibility. Specifically, it significantly outperforms existing approaches on various sources, improving both the moderation classification (36.8% on average) and moderation explanation quality (26.6% on average) consistently over existing MLLMs."
  - [section] "With our ICM-Instruct dataset, we thereby perform instruction-tuning on various MLLMs by injecting content moderation reasoning knowledge. Results show the strong capabilities of our model and dataset in achieving flexible, explainable, and accurate moderation."
- Break condition: If the instruction-tuning process doesn't effectively transfer the moderation reasoning knowledge, or if the dataset size/quality is insufficient, the performance gains will not materialize.

## Foundational Learning

- Concept: Rule decomposition and attribute representation
  - Why needed here: Understanding how to break down complex moderation rules into orthogonal attributes is fundamental to creating a flexible system that can adapt to different cultural norms and age groups.
  - Quick check question: Can you explain why decomposing rules into orthogonal attributes is more effective than keeping them as monolithic rules?

- Concept: Chain-of-thought reasoning and prompt engineering
  - Why needed here: The multi-stage prompt pipeline relies on decomposing reasoning into sequential steps, which requires understanding how to structure prompts to guide MLLMs through complex reasoning tasks.
  - Quick check question: How does the three-stage prompt pipeline (explicit description → implicit description → explanation) improve upon single-shot explanation generation?

- Concept: Instruction tuning vs. fine-tuning vs. pre-training
  - Why needed here: Understanding the differences between these training approaches is crucial for implementing the model effectively and knowing when to use each strategy.
  - Quick check question: What are the key differences between mixed SFT and continuous SFT, and when would you choose one over the other?

## Architecture Onboarding

- Component map: Rule definition → Rule decomposition → Data generation → Instruction tuning → Model deployment → Inference
- Critical path: Human-defined rules → Orthogonal attribute products → Multi-stage prompt generation → ICM-Instruct dataset → MLLM instruction tuning → ICM-Assistant model
- Design tradeoffs:
  - Orthogonal vs. hierarchical attribute decomposition: Orthogonal attributes provide better flexibility but may miss some rule interactions
  - Single vs. multi-stage prompt generation: Multi-stage improves explanation quality but increases generation time
  - Mixed vs. continuous SFT: Mixed SFT provides better generalization but requires more computational resources
- Failure signatures:
  - Poor moderation accuracy on new rule sets: Likely indicates inadequate rule decomposition or insufficient training data
  - Inconsistent explanations across similar images: Suggests problems with the prompt pipeline or training process
  - Low flexibility across cultural norms: May indicate overly specific attribute definitions that don't generalize
- First 3 experiments:
  1. Test rule decomposition on a simple rule set (e.g., "no exposed shoulders") to verify attribute product generation works correctly
  2. Validate the prompt pipeline by generating explanations for a small set of images and checking consistency with human judgments
  3. Run a small-scale instruction tuning experiment (using 1000 samples) to verify the training pipeline works before full-scale training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ICM-Assistant model perform on zero-shot moderation tasks for content types not included in the ICM-Instruct dataset?
- Basis in paper: [explicit] The paper discusses the model's zero-shot ability on "horrifying" and "gambling" moderation terms, showing promising results, but suggests extending this to other content types as future work.
- Why unresolved: The paper only tests zero-shot performance on two specific moderation terms, leaving the generalizability to other content types unclear.
- What evidence would resolve it: Conducting experiments on a wider range of moderation terms and evaluating the model's performance on unseen content types would provide insights into its zero-shot generalization capabilities.

### Open Question 2
- Question: What is the impact of using different MLLMs and LLMs in the data generation pipeline on the final performance of the ICM-Assistant model?
- Basis in paper: [explicit] The paper mentions that the choice of MLLMs and LLMs in the pipeline has only a small impact on performance, but it does not provide a comprehensive comparison of different model combinations.
- Why unresolved: The paper does not explore the full range of possible model combinations or provide a detailed analysis of their impact on the final model's performance.
- What evidence would resolve it: Conducting experiments with various combinations of MLLMs and LLMs in the data generation pipeline and analyzing their impact on the ICM-Assistant model's performance would provide a clearer understanding of the optimal model choices.

### Open Question 3
- Question: How does the ICM-Assistant model handle ambiguous or borderline cases in content moderation, where the rules are not clearly defined or the content is subjective?
- Basis in paper: [inferred] The paper focuses on the model's ability to follow specific rules and provide explanations, but it does not explicitly address how the model handles ambiguous cases or subjective content.
- Why unresolved: The paper does not provide insights into the model's decision-making process for ambiguous or borderline cases, which are common in real-world content moderation scenarios.
- What evidence would resolve it: Analyzing the model's performance on ambiguous or borderline cases and comparing its decisions with human moderators' judgments would provide insights into its ability to handle such scenarios.

## Limitations

- The paper primarily validates performance on "sexy" content moderation with limited testing on other categories, raising questions about generalizability
- Rule decomposition assumes moderation rules can be expressed as orthogonal attribute combinations, which may not capture all real-world moderation complexities
- The instruction tuning process requires substantial computational resources (200K samples per category), potentially limiting practical adoption

## Confidence

**High Confidence**: The mechanism of rule decomposition into orthogonal attributes (Mechanism 1) is well-supported by the demonstrated 36.8% improvement in moderation classification accuracy and the flexibility to adapt to different rule sets through attribute label changes.

**Medium Confidence**: The multi-stage prompt-guided data augmentation approach (Mechanism 2) shows promise based on the 26.6% improvement in explanation quality, but the specific prompts and their optimal sequencing remain underspecified in the paper.

**Low Confidence**: The scalability claims for instruction tuning across diverse moderation categories are weakly supported, as the experiments primarily validate performance on "sexy" content moderation with only limited testing on other categories.

## Next Checks

1. **Cross-cultural validation**: Test the rule decomposition framework on moderation rules from at least three distinct cultural contexts to verify the orthogonal attribute approach maintains flexibility and accuracy across diverse norms.

2. **Prompt sensitivity analysis**: Systematically vary the multi-stage prompt templates and sequence to identify which components most significantly impact explanation quality and consistency across similar images.

3. **Resource efficiency benchmark**: Evaluate model performance using progressively smaller training subsets (10K, 50K, 100K samples) to determine the minimum dataset size required for maintaining the claimed accuracy improvements.