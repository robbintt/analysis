---
ver: rpa2
title: 'Mathematical Entities: Corpora and Benchmarks'
arxiv_id: '2406.11577'
source_url: https://arxiv.org/abs/2406.11577
tags:
- mathematical
- extraction
- language
- category
- corpora
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the lack of annotated corpora and benchmarks
  for natural language processing in mathematics, focusing on terminology extraction,
  definition extraction, and entity linking. The authors construct three mathematical
  corpora covering category theory, including TAC abstracts, nLab encyclopedic content,
  and a textbook, and annotate them with linguistic features.
---

# Mathematical Entities: Corpora and Benchmarks

## Quick Facts
- arXiv ID: 2406.11577
- Source URL: https://arxiv.org/abs/2406.11577
- Reference count: 0
- Key outcome: Terminology extraction models achieve precision/recall scores between 0.08-0.60; definition extraction models achieve F1 around 0.05-0.19; entity linking models achieve F1 of 0.55-0.68

## Executive Summary
This paper addresses the lack of annotated corpora and benchmarks for natural language processing in mathematics, focusing on terminology extraction, definition extraction, and entity linking. The authors construct three mathematical corpora covering category theory, including TAC abstracts, nLab encyclopedic content, and a textbook, and annotate them with linguistic features. They evaluate advanced NLP models (TextRank, DyGIE++, SpERT.PL, PL-Marker) on terminology extraction benchmarks, achieving precision/recall scores between 0.08-0.60 depending on model and corpus. Definition extraction models (Vanetik et al., Veyseh et al.) are tested on the textbook corpus, with F1 scores around 0.05-0.19. Entity linking experiments using a simple query-based model and spaCy achieve F1 scores of 0.55-0.68. A learning assistant interface is also provided for context-sensitive search of mathematical entities.

## Method Summary
The paper constructs three mathematical corpora covering category theory: TAC abstracts, nLab encyclopedic content, and a textbook. Each corpus is annotated with metadata including author keywords, nLab titles, and glossary terms. The authors evaluate pre-trained NLP models (TextRank, DyGIE++, SpERT.PL, PL-Marker) for terminology extraction using four benchmarks, test definition extraction models (Vanetik et al., Veyseh et al.) on textbook definitions, and evaluate entity linking using a query-based model and spaCy. All evaluations use precision, recall, and F1 scores without additional model training.

## Key Results
- Terminology extraction: TextRank, DyGIE++, SpERT.PL, and PL-Marker achieve precision/recall scores between 0.08-0.60 across different benchmarks
- Definition extraction: Vanetik et al. and Veyseh et al. models achieve F1 scores of 0.05-0.19 on textbook definitions
- Entity linking: Query-based and spaCy models achieve F1 scores of 0.55-0.68 for 126 mapped concepts
- Learning assistant interface enables context-sensitive search across all three corpora with Wikidata and nLab links

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mathematical language processing fails without domain adaptation because mathematical entities follow different linguistic patterns than general scientific text.
- Mechanism: General NLP models trained on scientific corpora like SciERC do not capture the formal, precise, and highly structured nature of mathematical terminology and definitions, leading to low precision and recall when applied to mathematical corpora.
- Core assumption: Mathematical texts have unique syntactic and semantic characteristics (e.g., inline formulas, rigorous definitions) that are not represented in general scientific datasets.
- Evidence anchors:
  - [abstract] "terminology extraction and definition extraction do not easily generalize to mathematics"
  - [section 4.1] "Additional work is necessary to achieve strong performance in mathematical language processing"
  - [corpus] Weak: corpus annotations lack formal mathematical markup beyond LaTeXML conversion.
- Break condition: If models are trained or fine-tuned on mathematical corpora with formal annotations, performance may improve.

### Mechanism 2
- Claim: Multi-source benchmarks improve evaluation robustness for terminology extraction in mathematics.
- Mechanism: Combining author keywords, nLab titles, glossary terms, and automatically extracted MWEs creates a comprehensive benchmark that captures both standard and novel mathematical concepts, reducing bias from any single source.
- Core assumption: Each benchmark source covers a different aspect of mathematical vocabulary (standard concepts, research-specific terms, etc.).
- Evidence anchors:
  - [abstract] "four benchmarks provided by our corpora: the set of author-selected keywords in TAC, the set of nLab titles, the set of glossary terms in BCT, and a set of automatically extracted multi-word expressions"
  - [section 4.1] "Each of these evaluations is imperfect...most of these benchmarks should exclusively contain valid mathematical concepts"
  - [corpus] Strong: corpora explicitly contain author keywords, nLab titles, and glossary terms.
- Break condition: If benchmark sources overlap too much or miss significant concept types, evaluation may not reflect true model capability.

### Mechanism 3
- Claim: Entity linking performance depends on phrase ambiguity and knowledge base coverage.
- Mechanism: Mathematical concepts with everyday language overlap (e.g., "composition") cause ambiguity in entity linking, while complex formal phrases (e.g., "Eilenberg-Moore object") map more reliably to specific knowledge base entries.
- Core assumption: The knowledge base contains sufficient and accurate entries for mathematical concepts, and the linking model can disambiguate based on context.
- Evidence anchors:
  - [abstract] "some specific terms are highly ambiguous, other mathematical concepts are complex phrases, which do not have everyday meanings"
  - [section 4.3] Query-based model filters out physical objects, locations, and activities to improve relevance
  - [corpus] Weak: Wikidata entries are manually matched but may not cover all corpus concepts.
- Break condition: If knowledge base entries are missing or incorrectly matched, linking performance degrades regardless of model sophistication.

## Foundational Learning

- Concept: Part-of-speech tagging and dependency parsing
  - Why needed here: Accurate syntactic analysis is required to identify multi-word expressions and definitional structures in mathematical texts.
  - Quick check question: What POS tag would you assign to "coequalizer" in "Reflexive coequalizers are sifted colimits"?

- Concept: Terminology extraction vs. Named Entity Recognition
  - Why needed here: Mathematical entities are not people, places, or organizations, requiring different extraction strategies than standard NER.
  - Quick check question: Why might "free double category" be a terminology extraction target but not a named entity?

- Concept: Knowledge base querying and filtering
  - Why needed here: Entity linking requires constructing queries that retrieve relevant mathematical concepts while excluding non-mathematical entries.
  - Quick check question: What filter criteria would you apply to a Wikidata query for mathematical entities?

## Architecture Onboarding

- Component map: Corpus preprocessing (LaTeXML → plain text + linguistic annotations) -> NLP model evaluation (TextRank, DyGIE++, SpERT.PL, PL-Marker) -> Benchmark comparison -> Entity linking (query-based and spaCy models) -> Learning assistant interface (search + Wikidata/nLab links)
- Critical path: Corpus → NLP model evaluation → Benchmark comparison → Learning assistant
- Design tradeoffs: General NLP models vs. domain-specific training; comprehensive benchmarks vs. evaluation complexity; simple vs. sophisticated entity linking
- Failure signatures: Low precision/recall in terminology extraction suggests domain mismatch; poor entity linking suggests knowledge base gaps or ambiguous queries
- First 3 experiments:
  1. Run TextRank on TAC corpus and compare extracted terms to author keywords benchmark
  2. Test query-based entity linking on a sample of mathematical phrases and evaluate precision
  3. Compare learning assistant search results for "double category" across all three corpora

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can mathematical terminology extraction models be adapted to achieve higher precision and recall in specialized domains like category theory?
- Basis in paper: [explicit] The paper shows that state-of-the-art models like TextRank, DyGIE++, SpERT.PL, and PL-Marker perform poorly on mathematical corpora, with precision/recall scores between 0.08-0.60.
- Why unresolved: The models were not trained on category theory data, and the paper suggests that additional training or adaptation is needed but does not provide specific methods.
- What evidence would resolve it: Comparative experiments showing improved performance after training these models on annotated category theory corpora versus their current out-of-domain performance.

### Open Question 2
- Question: What is the optimal balance between automated preprocessing and manual intervention for preparing mathematical corpora for NLP tasks?
- Basis in paper: [explicit] The authors used a combination of neural parsing models and manual intervention to provide annotations like POS tags and dependency trees.
- Why unresolved: The paper does not evaluate the impact of different levels of manual intervention versus fully automated preprocessing on model performance.
- What evidence would resolve it: Controlled experiments comparing model performance on corpora with varying degrees of manual annotation versus fully automated preprocessing.

### Open Question 3
- Question: How can entity linking models be improved to handle ambiguous mathematical terms that have multiple valid Wikidata entries?
- Basis in paper: [explicit] The query-based entity linking model achieved P@1 of 0.60 and F1 of 0.68, but the paper notes that some terms have multiple possible Wikidata entries.
- Why unresolved: The paper does not explore methods for disambiguation or ranking of multiple valid entity links.
- What evidence would resolve it: Experiments demonstrating improved linking accuracy using disambiguation techniques or ranking methods for cases with multiple valid Wikidata entries.

## Limitations
- Corpus annotations lack formal mathematical markup beyond LaTeXML conversion, limiting downstream utility
- Entity linking experiments depend on manual matching of corpus concepts to Wikidata entries
- Reported model performance scores (0.05-0.60 F1) are significantly below typical NLP benchmarks

## Confidence
- Performance results: Low-Medium confidence due to consistently poor scores suggesting domain mismatch
- Annotation quality: Medium confidence - linguistic features provided but lack formal mathematical markup
- Entity linking accuracy: Moderate confidence due to incomplete Wikidata coverage and manual matching

## Next Checks
1. Replicate terminology extraction evaluation using SciERC-trained models as a baseline to quantify domain adaptation benefits
2. Manually verify a sample of Wikidata matches to assess entity linking accuracy and coverage gaps
3. Test whether fine-tuning DyGIE++ or SpERT.PL on a subset of the annotated corpus improves terminology extraction performance on held-out test sets