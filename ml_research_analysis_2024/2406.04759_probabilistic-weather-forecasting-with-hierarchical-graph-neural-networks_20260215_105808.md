---
ver: rpa2
title: Probabilistic Weather Forecasting with Hierarchical Graph Neural Networks
arxiv_id: '2406.04759'
source_url: https://arxiv.org/abs/2406.04759
tags: []
core_contribution: This paper introduces Graph-EFM, a hierarchical graph neural network
  model for probabilistic weather forecasting. The method combines a latent-variable
  formulation with a hierarchical graph construction to efficiently sample spatially
  coherent ensemble forecasts.
---

# Probabilistic Weather Forecasting with Hierarchical Graph Neural Networks

## Quick Facts
- arXiv ID: 2406.04759
- Source URL: https://arxiv.org/abs/2406.04759
- Reference count: 40
- Key outcome: Graph-EFM achieves well-calibrated ensemble forecasts with single forward pass efficiency

## Executive Summary
This paper introduces Graph-EFM, a hierarchical graph neural network model for probabilistic weather forecasting. The method combines a latent-variable formulation with a hierarchical graph construction to efficiently sample spatially coherent ensemble forecasts. Graph-EFM requires only a single forward pass per time step, enabling fast generation of arbitrarily large ensembles. Experiments on both global and limited-area forecasting show that Graph-EFM achieves equivalent or lower errors than deterministic models, while accurately capturing forecast uncertainty.

## Method Summary
Graph-EFM is a hierarchical graph neural network that generates probabilistic weather forecasts through a latent-variable formulation. The model constructs a multi-level mesh graph where latent variables are sampled at the coarsest level and propagated downward through GNN sweeps to generate fine-grained ensemble members. During training, a variational autoencoder framework is used with ELBO optimization, followed by CRPS fine-tuning. The method supports both global (ERA5) and limited-area (MEPS) forecasting with single-pass ensemble generation.

## Key Results
- Graph-EFM achieves spread-skill-ratio close to 1 on global ERA5 data at 1.5° resolution, indicating well-calibrated ensembles
- The model generates ensemble forecasts with a single forward pass per time step, enabling efficient sampling of arbitrarily large ensembles
- Graph-EFM outperforms or matches deterministic baselines (GraphCast*, Graph-FM) on RMSE while providing calibrated uncertainty estimates

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The hierarchical graph construction enables efficient sampling of spatially coherent ensemble forecasts by imposing a low-dimensional latent space at the top level and propagating uncertainty through structured levels.
- **Mechanism**: By introducing a hierarchical graph with L levels, the model reduces the spatial dimensionality at the top level L (|VL| ≪ |VG|), samples from a Gaussian distribution there, and then propagates the sampled latent variable Z_t through downward GNN sweeps. This hierarchical propagation ensures that the spatial structure is preserved as fine details are added in lower levels, leading to coherent fields in the ensemble members.
- **Core assumption**: That spatial dependencies can be effectively introduced through the graph structure during downward propagation rather than requiring explicit modeling of spatial correlations in the latent distribution.
- **Evidence anchors**:
  - [abstract]: "The use of a hierarchical graph construction allows for efficient sampling of spatially coherent forecasts."
  - [section 4.3]: "We impose spatial dependencies by integrating the latent variable formulation with the hierarchical graph. We argue that as the independent components of Z_t are propagated down through the mesh graph, gradually increasing the spatial resolution, spatial dependencies are introduced by the model in the GNN layers."
- **Break condition**: If the hierarchical graph fails to propagate spatial structure properly (e.g., insufficient edge connectivity between levels or poor GNN parameterization), the ensemble members may become patchy and lose coherence.

### Mechanism 2
- **Claim**: The variational autoencoder (VAE) framework enables the model to learn both the mean and variance of the forecast distribution while avoiding collapse to deterministic predictions.
- **Mechanism**: The model defines a probabilistic decoder (predictor) and an encoder (variational approximation q), optimizing the ELBO. The KL term regularizes the latent distribution to stay close to the prior, while the NLL term ensures reconstruction accuracy. Fine-tuning with CRPS further calibrates the ensemble spread.
- **Core assumption**: That the autoregressive decomposition p(X_1:T|X_-1:0, F_1:T) = ∏_t p(X_t|X_t-2:t-1, F_t) holds and that the latent variable Z_t captures all necessary uncertainty for each step.
- **Evidence anchors**:
  - [section 4.3]: "To capture the uncertainty in the chaotic weather system we next aim to construct a probabilistic model from the ground up to capture the full distribution p(X_1:T|X_-1:0, F_1:T)."
  - [section 4.4]: "We introduce a variational approximation q(Z_t|X_t-2:t-1, X_t, F_t) at each time step, approximating the true posterior p(Z_t|X_t-2:t-1, X_t, F_t) over Z_t."
- **Break condition**: If the model ignores Z_t (collapses to deterministic) or the variational approximation poorly matches the true posterior, the ensemble will be either overconfident or uncalibrated.

### Mechanism 3
- **Claim**: Propagation Networks improve information flow from the grid to the latent variable Z_t, ensuring that the latent variable captures relevant spatial patterns rather than being ignored.
- **Mechanism**: Unlike Interaction Networks, which preserve old node representations, Propagation Networks average neighbor representations before applying the MLP. This encourages the model to pass information upward through the hierarchy, ensuring Z_t is informed by the grid state.
- **Core assumption**: That the bias in Interaction Networks toward retaining old representations prevents effective upward information flow in hierarchical graphs.
- **Evidence anchors**:
  - [section 4.5]: "In Graph-EFM there is a large amount of information that needs to be propagated between the grid and Z_t. However, the Interaction Network GNNs are biased towards keeping old representations of receiver nodes... To remedy this we propose an alternative GNN formulation that we call Propagation Network..."
  - [section K.1]: "We test this empirically, by training versions of Graph-FM using Interaction and Propagation networks... Here we see a greater advantage of the Propagation Networks."
- **Break condition**: If the GNN layers still fail to encode grid information into Z_t, the ensemble members will not reflect the true distribution of weather states.

## Foundational Learning

- **Concept**: Variational Autoencoders (VAEs)
  - Why needed here: Graph-EFM uses a VAE-like structure to model the latent distribution over weather states and train via ELBO maximization.
  - Quick check question: What is the role of the KL divergence term in the ELBO, and why is it necessary for avoiding mode collapse?

- **Concept**: Graph Neural Networks (GNNs) and message passing
  - Why needed here: The model relies on hierarchical GNNs to propagate information both within and between mesh levels, and to map between grid and mesh representations.
  - Quick check question: How do Interaction Networks differ from Propagation Networks in terms of information retention versus averaging?

- **Concept**: Ensemble forecasting and calibration metrics (CRPS, SpSkR)
  - Why needed here: The model is evaluated not just on RMSE but on probabilistic metrics that assess both accuracy and uncertainty calibration.
  - Quick check question: Why should a well-calibrated ensemble have SpSkR ≈ 1, and what does it mean if SpSkR > 1 or < 1?

## Architecture Onboarding

- **Component map**: Grid states X_t-2:t-1 and forcing F_t → Embedding → Upward GNN → Latent map (Z_t) → Downward GNN → Predictor → Output
- **Critical path**: Grid → Embedding → Upward GNN → Latent map (Z_t) → Downward GNN → Predictor → Output
- **Design tradeoffs**:
  - **Latent dimensionality vs. expressiveness**: Lower |VL| reduces computation but may limit representation power.
  - **Number of hierarchy levels**: More levels increase spatial granularity but add complexity.
  - **Propagation Networks vs. Interaction Networks**: Propagation Networks encourage information flow but may lose local detail.
- **Failure signatures**:
  - **Poor ensemble spread**: Model collapses to deterministic (check KL term weighting, latent map expressiveness).
  - **Patchy ensemble members**: Hierarchical propagation fails (check edge connectivity, GNN parameterization).
  - **Artifacts in forecasts**: Multi-scale vs. hierarchical graph issues (check λCRPS, graph construction).
- **First 3 experiments**:
  1. **Sanity check**: Train Graph-EFM with a static latent distribution (N(0,I)) to confirm the benefit of the learnable latent map.
  2. **GNN comparison**: Swap Propagation Networks for Interaction Networks in Graph-FM and measure RMSE changes.
  3. **Ensemble calibration**: Train with varying λKL and λCRPS to observe effects on SpSkR and CRPS.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the different ensemble sampling strategies (independent runs vs. correlated sampling) affect forecast quality and ensemble spread?
- Basis in paper: [explicit] The paper mentions that independent runs of algorithm 3 are used to generate ensemble members, but notes this results in K^T members at time T if not careful.
- Why unresolved: The paper does not provide a detailed comparison of different ensemble sampling strategies or their impact on forecast quality and ensemble spread.
- What evidence would resolve it: Experiments comparing independent runs, correlated sampling, and other ensemble sampling strategies on forecast quality metrics (RMSE, CRPS) and ensemble spread metrics (SpSkR) for various lead times and variables.

### Open Question 2
- Question: What is the optimal number of processing steps in Graph-FM and Graph-EFM for different forecasting tasks and resolutions?
- Basis in paper: [explicit] The paper mentions that the number of processing steps is a hyperparameter, but does not provide a detailed analysis of its impact on forecast quality.
- Why unresolved: The paper does not provide a systematic study of how the number of processing steps affects forecast quality and computational efficiency for different forecasting tasks and resolutions.
- What evidence would resolve it: Experiments varying the number of processing steps in Graph-FM and Graph-EFM for different forecasting tasks (global, LAM) and resolutions, evaluating forecast quality metrics (RMSE, CRPS) and computational efficiency.

### Open Question 3
- Question: How do the proposed models perform in extreme weather forecasting, particularly for rare and high-impact events?
- Basis in paper: [explicit] The paper mentions that extreme weather events are becoming more prevalent and severe, but does not provide a detailed analysis of the models' performance in extreme weather forecasting.
- Why unresolved: The paper does not provide a detailed evaluation of the models' performance in extreme weather forecasting, particularly for rare and high-impact events.
- What evidence would resolve it: Experiments evaluating the models' performance in extreme weather forecasting, including rare and high-impact events, using appropriate metrics (e.g., Brier score, reliability diagrams) and datasets (e.g., extreme weather events from ERA5 or MEPS).

## Limitations
- The hierarchical graph construction details for multi-scale refinement are only described in appendices, making exact reproduction challenging
- The trade-off between spatial resolution and computational efficiency at different hierarchy levels is not fully explored
- The model's performance on extreme weather events and rare phenomena is not explicitly evaluated

## Confidence
- **High**: Graph-EFM's ability to generate ensemble forecasts with a single forward pass (mechanism 2)
- **High**: The hierarchical GNN architecture's effectiveness for capturing spatial dependencies (mechanism 1)
- **Medium**: The claim that Propagation Networks significantly improve information flow compared to Interaction Networks (mechanism 3)
- **Medium**: The calibration metrics (SpSkR ≈ 1) indicating well-calibrated ensembles

## Next Checks
1. **Ablation study**: Train Graph-EFM without the hierarchical structure (flat graph) to quantify the specific contribution of hierarchy to ensemble quality
2. **Sensitivity analysis**: Systematically vary λKL and λCRPS to map their effects on SpSkR and CRPS, establishing more robust tuning guidelines
3. **Temporal stability test**: Evaluate Graph-EFM's ensemble calibration over longer forecast horizons (beyond 12 hours) to assess degradation in uncertainty quantification