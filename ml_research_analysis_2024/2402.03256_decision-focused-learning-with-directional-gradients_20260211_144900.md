---
ver: rpa2
title: Decision-Focused Learning with Directional Gradients
arxiv_id: '2402.03256'
source_url: https://arxiv.org/abs/2402.03256
tags:
- should
- losses
- loss
- error
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new family of decision-aware surrogate
  losses, called Perturbation Gradient (PG) losses, for the predict-then-optimize
  framework. The core idea is to connect the expected downstream decision loss with
  the directional derivative of a plug-in objective and approximate this derivative
  using zeroth-order gradient techniques.
---

# Decision-Focused Learning with Directional Gradients

## Quick Facts
- arXiv ID: 2402.03256
- Source URL: https://arxiv.org/abs/2402.03256
- Reference count: 40
- Introduces Perturbation Gradient (PG) losses for decision-focused learning with zeroth-order gradient estimation

## Executive Summary
This paper introduces a novel family of decision-aware surrogate losses called Perturbation Gradient (PG) losses for the predict-then-optimize framework. The core innovation connects the expected downstream decision loss with the directional derivative of a plug-in objective and approximates this derivative using zeroth-order gradient techniques. Unlike the original decision loss which is typically piecewise constant and discontinuous, PG losses are Lipschitz continuous and can be optimized using standard gradient-based methods. The authors prove that the approximation error of PG losses vanishes as the number of samples grows, yielding asymptotic optimality even in misspecified settings.

## Method Summary
The Perturbation Gradient approach works by estimating the directional derivative of the plug-in objective function through perturbations. Instead of directly optimizing the discontinuous decision loss, PG losses approximate the expected downstream loss by computing directional gradients using zeroth-order methods. This creates a smooth, differentiable surrogate that maintains theoretical connections to the original decision problem while enabling practical optimization with off-the-shelf gradient-based algorithms. The method provides a principled way to handle the inherent discontinuities in decision-focused learning while preserving convergence guarantees.

## Key Results
- PG losses are Lipschitz continuous and enable optimization with standard gradient-based methods
- Theoretical guarantees show approximation error vanishes asymptotically, yielding best-in-class policy
- Numerical experiments demonstrate superior performance over existing methods in misspecified settings

## Why This Works (Mechanism)
The success of PG losses stems from their ability to approximate the directional derivative of the plug-in objective, which captures how small changes in predictions affect the downstream decision. By using zeroth-order gradient estimation, the method avoids the discontinuities inherent in the original decision loss while maintaining a meaningful connection to the decision problem. The Lipschitz continuity ensures stable optimization, and the theoretical framework guarantees convergence to optimal policies as sample size increases.

## Foundational Learning
- Directional derivatives: Needed to capture how perturbations in predictions affect decision outcomes. Quick check: Verify that the directional derivative approximates the sensitivity of the objective to parameter changes.
- Zeroth-order gradient estimation: Required to approximate derivatives when explicit gradient computation is infeasible. Quick check: Confirm that perturbation magnitude balances bias and variance in gradient estimates.
- Lipschitz continuity: Essential for ensuring stable gradient-based optimization. Quick check: Verify that the surrogate loss satisfies Lipschitz conditions across the parameter space.

## Architecture Onboarding
- Component map: Prediction model -> Plug-in objective -> Directional derivative estimation -> PG loss computation -> Gradient-based optimization
- Critical path: The estimation of directional derivatives is the bottleneck, as it requires multiple function evaluations for each parameter update
- Design tradeoffs: The perturbation magnitude in zeroth-order estimation creates a bias-variance tradeoff - smaller perturbations reduce bias but increase variance, while larger perturbations have the opposite effect
- Failure signatures: Poor performance when the plug-in objective lacks sufficient smoothness, when perturbations are improperly scaled, or when sample sizes are too small for reliable gradient estimation
- First experiments:
  1. Compare PG loss performance against baseline surrogate losses on a simple linear optimization problem
  2. Evaluate the sensitivity of PG losses to perturbation magnitude across different problem scales
  3. Test convergence rates with varying sample sizes to validate theoretical guarantees

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Theoretical guarantees depend on specific regularity conditions that may not hold in all practical settings
- Computational overhead of estimating directional derivatives could be prohibitive for high-dimensional problems
- Finite-sample behavior and convergence rates are not thoroughly characterized

## Confidence
- High confidence in theoretical framework and convergence properties under stated assumptions
- Medium confidence in practical advantages over existing surrogate losses
- Low confidence in robustness across diverse problem domains and misspecification scenarios

## Next Checks
1. Conduct extensive ablation studies varying perturbation magnitude and sample sizes to characterize the trade-off between estimation accuracy and computational efficiency
2. Test the method on a broader range of optimization problems, including non-convex and large-scale instances, to evaluate scalability
3. Compare against decision-focused learning approaches in settings with varying degrees of model misspecification to quantify the robustness claims