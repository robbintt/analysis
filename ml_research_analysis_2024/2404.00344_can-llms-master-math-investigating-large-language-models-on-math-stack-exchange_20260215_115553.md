---
ver: rpa2
title: Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange
arxiv_id: '2404.00344'
source_url: https://arxiv.org/abs/2404.00344
tags:
- llms
- math
- questions
- answers
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates large language models (LLMs) on open-ended
  mathematical questions from Math Stack Exchange. The authors generate answers using
  six different LLMs and compare their performance against the ArqMATH3 dataset.
---

# Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange

## Quick Facts
- arXiv ID: 2404.00344
- Source URL: https://arxiv.org/abs/2404.00344
- Reference count: 40
- Key outcome: GPT-4 achieved nDCG of 0.48 and P@10 of 0.37 on Math Stack Exchange questions, outperforming fine-tuned mathematical models but struggling with complex problems requiring specialized knowledge

## Executive Summary
This paper evaluates large language models on open-ended mathematical questions from Math Stack Exchange, comparing their performance against the ArqMATH3 dataset. The authors generate answers using six different LLMs and find that GPT-4 significantly outperforms existing models fine-tuned on mathematical datasets. While GPT-4 shows strong contextual understanding of mathematical notation, a case study reveals it doesn't consistently provide accurate answers to all questions, particularly for complex mathematical problems requiring specialized knowledge. The findings highlight current limitations of LLMs in mathematical problem-solving and set the stage for future research in AI-driven mathematical reasoning.

## Method Summary
The study employs a two-step approach: first generating answers to 78 undergraduate-level mathematics questions from Math Stack Exchange using six different LLMs (GPT-4, Tora, LLaMa, MAmmoTH, Mistral, and a DPR baseline), then indexing these answers along with ArqMATH3 Task1 dataset answers using DPR vector embeddings. The most similar answers are retrieved using cosine similarity scoring, and manual evaluation is performed on the best-performing model (GPT-4) through expert assessment. The primary metrics used are nDCG and P@10 to evaluate retrieval performance.

## Key Results
- GPT-4 achieved nDCG of 0.48 and P@10 of 0.37, outperforming existing models fine-tuned on mathematical datasets
- Fine-tuned models like Tora-7b and LLaMa showed inferior performance compared to the DPR baseline
- Manual evaluation revealed GPT-4's accuracy degrades with complex questions requiring specialized mathematical knowledge
- Embedding-based retrieval alone proved insufficient for mathematical questions requiring semantic understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's performance advantage stems from superior contextual understanding of mathematical notation compared to DPR baselines.
- Mechanism: GPT-4 can interpret the semantic meaning of mathematical formulas without explicit context, while DPR requires surrounding text to disambiguate notation.
- Core assumption: Mathematical notation in isolation is semantically ambiguous to DPR but interpretable by GPT-4 through its training.
- Evidence anchors:
  - [abstract]: "Our Case analysis indicates that while the GPT-4 can generate relevant responses in certain instances, it does not consistently answer all questions accurately."
  - [section]: "We observe that, without context, DPR cannot infer any meaning from the formula. Hence, GPT-4 shows a good contextual understanding of the ground truth formula."
  - [corpus]: Weak - no direct corpus evidence comparing DPR vs GPT-4 on isolated formulas.
- Break condition: When questions require explicit multi-step reasoning beyond pattern matching of notation.

### Mechanism 2
- Claim: Models fine-tuned on MATH datasets (Tora, LLaMa, MAmmoTH) overfit to competition-style problems and underperform on open-ended MSE questions.
- Mechanism: Fine-tuning on MATH creates task-specific optimization that doesn't generalize to the broader, more varied question types found on MSE.
- Core assumption: MATH dataset structure and question format are sufficiently different from MSE to cause domain shift.
- Evidence anchors:
  - [abstract]: "GPT-4... outperformed existing LLMs fine-tuned for answering mathematics questions"
  - [section]: "Notably, increasing the model size of the top performer did not yield better results. The Mistral model... delivered performance comparable to that of Tora-7b."
  - [corpus]: Weak - no direct corpus evidence of overfitting, though related work suggests domain-specific training can create brittleness.
- Break condition: When MATH-style problems are directly tested, potentially reversing the performance gap.

### Mechanism 3
- Claim: Embedding-based retrieval using question-answer similarity is less effective than answer-generation followed by retrieval for mathematical questions.
- Mechanism: Mathematical questions often require understanding of both question intent and answer structure that embeddings alone cannot capture, while generated answers can bridge this gap.
- Core assumption: The semantic relationship between mathematical questions and answers is too complex for direct embedding similarity to capture reliably.
- Evidence anchors:
  - [section]: "This indicates that comparison question and answer embeddings might not solve the problem of retrieving relevant answers."
  - [section]: "The analysis revealed Tora-7b's Precision@10 to be inferior to all runs depicted in Table 1."
  - [corpus]: Weak - no direct corpus evidence comparing embedding-based vs generation-based approaches.
- Break condition: When question-answer pairs have clear semantic similarity that embeddings can capture.

## Foundational Learning

- Concept: Vector similarity measures (cosine similarity)
  - Why needed here: Used to compare embeddings from DPR and determine answer relevance
  - Quick check question: If two vectors have cosine similarity of 0.8, are they more or less similar than vectors with similarity 0.3?

- Concept: Precision@k and NDCG metrics
  - Why needed here: Primary evaluation metrics for assessing retrieval performance
  - Quick check question: If a system retrieves 2 relevant answers in top 10 out of 3 total relevant answers, what is P@10?

- Concept: Chain-of-thought reasoning
  - Why needed here: GPT-4's ability to generate step-by-step solutions vs. direct answer generation
  - Quick check question: Why might step-by-step reasoning be more effective for complex math problems than direct answer generation?

## Architecture Onboarding

- Component map: Question processing -> LLM generation -> Embedding indexing -> Similarity search -> Evaluation
- Critical path: GPT-4 answer generation -> DPR retrieval -> Cosine similarity scoring -> P@10/NDCG calculation
- Design tradeoffs: Generation-based vs embedding-based retrieval; model size vs performance; open-ended vs structured questions
- Failure signatures: Low P@10 with high NDCG (precision issues), high P@10 with low NDCG (ranking issues), generation failures (empty outputs)
- First 3 experiments:
  1. Compare GPT-4 vs DPR on a subset of questions with clear mathematical notation
  2. Test embedding-based retrieval on questions with well-defined semantic similarity
  3. Evaluate impact of answer truncation (512 tokens) on retrieval performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can specialized mathematical models like ToRA and LLeMA be effectively adapted for open-ended mathematical problem-solving beyond structured datasets like MATH?
- Basis in paper: [explicit] The paper notes that ToRA models, despite being top performers on MATH dataset, underperformed on open-ended questions from Math Stack Exchange compared to DPR baseline
- Why unresolved: The models were overfitted to the MATH dataset format and struggled with the unstructured nature of real-world mathematical questions
- What evidence would resolve it: Comparative studies testing these models on diverse mathematical question datasets beyond MATH, with systematic analysis of performance gaps

### Open Question 2
- Question: What architectural modifications could enable LLMs to better handle mathematical reasoning requiring specialized knowledge and precise logical steps?
- Basis in paper: [inferred] The case study revealed GPT-4's degradation in accuracy with complex questions demanding specialized knowledge, suggesting current architectures have limitations
- Why unresolved: Current LLMs struggle with the specialized structure and precision demands of mathematical language, as noted in the paper's discussion of mathematical reasoning challenges
- What evidence would resolve it: Empirical results from modified architectures incorporating mathematical reasoning modules or verification systems, demonstrating improved performance on complex mathematical problems

### Open Question 3
- Question: How can we develop evaluation frameworks that effectively measure LLMs' capabilities in solving open-ended mathematical problems where answers require human-like reasoning and explanation?
- Basis in paper: [explicit] The paper highlights the impracticality of manual verification for MSE questions due to interdisciplinary nature and expertise required, and notes student assessments have shown inaccuracies
- Why unresolved: Existing evaluation methods are inadequate for assessing open-ended mathematical reasoning, and the paper emphasizes the need for better benchmarks
- What evidence would resolve it: Development and validation of automated evaluation frameworks specifically designed for open-ended mathematical problem-solving, with demonstrated reliability compared to human expert assessment

## Limitations

- The study uses only 78 questions, which may not be representative of the full diversity of mathematical problems on Math Stack Exchange
- Manual evaluation was performed by only two experts, raising concerns about inter-rater reliability
- GPT-4's superior performance may be partly attributed to its significantly larger model size rather than inherent mathematical reasoning capabilities

## Confidence

- High Confidence: GPT-4's nDCG of 0.48 and P@10 of 0.37 outperform existing fine-tuned models on ArqMATH3
- Medium Confidence: Fine-tuned models (Tora, LLaMa, MAmmoTH) show inferior performance due to domain shift from MATH dataset
- Medium Confidence: Embedding-based retrieval alone is insufficient for mathematical questions requiring semantic understanding

## Next Checks

1. Replicate the study with a larger, more diverse sample of Math Stack Exchange questions to verify the robustness of findings
2. Conduct inter-rater reliability testing on the manual evaluation process with additional domain experts
3. Test GPT-4's performance against similarly-sized models on identical mathematical reasoning tasks to isolate the effect of model architecture vs scale