---
ver: rpa2
title: 'KS-LLM: Knowledge Selection of Large Language Models with Evidence Document
  for Question Answering'
arxiv_id: '2404.15660'
source_url: https://arxiv.org/abs/2404.15660
tags:
- evidence
- knowledge
- large
- language
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the hallucination problem in large language
  models (LLMs) when applied to knowledge-intensive tasks like question answering.
  The proposed KS-LLM method tackles this by effectively selecting relevant knowledge
  snippets from evidence documents to support LLM reasoning.
---

# KS-LLM: Knowledge Selection of Large Language Models with Evidence Document for Question Answering

## Quick Facts
- arXiv ID: 2404.15660
- Source URL: https://arxiv.org/abs/2404.15660
- Reference count: 13
- Achieves EM scores of 58.48, 24.70, and 21.69 on TriviaQA-verified, WebQ, and NQ datasets respectively using Vicuna-13B

## Executive Summary
KS-LLM addresses the hallucination problem in large language models by implementing a knowledge selection mechanism that extracts relevant information from evidence documents. The method generates triples from questions, selects semantically similar sentences from evidence documents, and combines both as supporting knowledge for answer generation. This approach significantly improves performance on knowledge-intensive question answering tasks while reducing hallucinations compared to baseline models.

## Method Summary
KS-LLM employs a two-stage knowledge selection process: first generating semantic triples from the input question, then selecting the most relevant sentences from evidence documents based on semantic similarity to these triples. The method uses Vicuna-13B as the base LLM and incorporates a knowledge-enhanced answer generation module. The system processes questions by extracting key information, matching it with relevant evidence snippets, and generating answers grounded in the selected knowledge rather than relying solely on the LLM's parametric memory.

## Key Results
- Achieved 58.48% exact match on TriviaQA-verified dataset
- Reached 24.70% exact match on WebQ dataset
- Obtained 21.69% exact match on NQ dataset
- Demonstrated significant improvements over baseline models across all evaluated datasets

## Why This Works (Mechanism)
KS-LLM reduces hallucinations by grounding LLM responses in verifiable evidence documents rather than relying solely on parametric knowledge. The triple generation step creates structured representations of question semantics, while semantic similarity matching ensures relevant evidence is selected. This combination provides the LLM with focused, contextually appropriate knowledge that guides answer generation toward factually accurate responses.

## Foundational Learning
- **Knowledge Graphs**: Structured representations of entities and relationships used to extract semantic triples from questions
  - Why needed: Provides structured semantic representation for effective knowledge matching
  - Quick check: Can generate valid triples from diverse question types
- **Semantic Similarity Metrics**: Distance measures (Euclidean, cosine) used to match question triples with evidence sentences
  - Why needed: Enables automated selection of relevant evidence snippets
  - Quick check: Higher similarity scores correlate with relevance to question intent
- **Evidence Document Processing**: Techniques for extracting and ranking relevant sentences from large text corpora
  - Why needed: Provides focused knowledge context for LLM reasoning
  - Quick check: Selected sentences directly address question requirements
- **Triple Generation**: Conversion of natural language questions into structured (subject, predicate, object) representations
  - Why needed: Creates semantic anchors for matching with evidence
  - Quick check: Generated triples capture essential question semantics
- **LLM Knowledge Integration**: Methods for combining external evidence with LLM reasoning capabilities
  - Why needed: Enables fact-based generation while maintaining fluency
  - Quick check: Generated answers reference specific evidence content
- **Hallucination Detection**: Evaluation methods for measuring factual accuracy versus generation fluency
  - Why needed: Quantifies reduction in hallucinated content
  - Quick check: Answers align with provided evidence documents

## Architecture Onboarding

**Component Map**: Question -> Triple Generator -> Semantic Matcher -> Evidence Selector -> Knowledge Combiner -> Vicuna-13B -> Answer

**Critical Path**: The triple generation and semantic similarity matching stages are critical for effective knowledge selection. Performance bottlenecks may occur during evidence document processing when dealing with large corpora or complex questions requiring multi-hop reasoning.

**Design Tradeoffs**: The system trades computational overhead (triple generation, similarity calculations) for improved factual accuracy. Using structured triples may miss nuanced relationships better captured by natural language. The semantic similarity approach may struggle with implicit relationships not explicitly stated in evidence documents.

**Failure Signatures**: Performance degradation occurs when triples poorly represent question semantics, when evidence documents lack relevant information, or when semantic similarity metrics fail to capture contextual relevance. Complex multi-hop reasoning questions may exceed the system's current capabilities.

**First Experiments**:
1. Test triple generation accuracy across diverse question types to identify representation weaknesses
2. Evaluate semantic similarity matching performance using different distance metrics on sample questions
3. Measure computational overhead introduced by knowledge selection stages versus baseline generation time

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the performance of KS-LLM vary with different knowledge graph sizes and complexities?
- Basis in paper: [inferred] The paper mentions using knowledge graphs and triples but doesn't explore the impact of varying knowledge graph sizes on performance.
- Why unresolved: The paper focuses on a fixed approach using triples and evidence documents without examining scalability or performance across different knowledge graph sizes.
- What evidence would resolve it: Comparative experiments testing KS-LLM with knowledge graphs of varying sizes and complexities, measuring performance changes.

### Open Question 2
- Question: What is the impact of different semantic similarity metrics on evidence sentence selection accuracy?
- Basis in paper: [explicit] The paper uses Euclidean distance for semantic similarity but acknowledges this is one possible approach.
- Why unresolved: The paper doesn't explore alternative similarity metrics or their comparative effectiveness.
- What evidence would resolve it: Experiments comparing KS-LLM performance using different semantic similarity metrics (cosine similarity, dot product, etc.).

### Open Question 3
- Question: How does KS-LLM perform in low-resource languages where knowledge graphs may be sparse or unavailable?
- Basis in paper: [inferred] The paper evaluates only on English datasets without considering multilingual or low-resource scenarios.
- Why unresolved: The paper focuses exclusively on English datasets and doesn't address performance in languages with limited knowledge graph resources.
- What evidence would resolve it: Experiments testing KS-LLM performance on multilingual datasets, particularly low-resource languages.

## Limitations
- Reliance on triple generation may introduce information loss for questions requiring nuanced or multi-faceted reasoning
- Semantic similarity matching may struggle with complex questions involving implicit relationships or abstract concepts
- Evaluation focuses primarily on exact match metrics, potentially missing other quality dimensions

## Confidence
- **High Confidence**: Experimental results showing improved EM scores over baselines are well-supported by reported metrics and dataset evaluations
- **Medium Confidence**: Claims regarding hallucination reduction would benefit from more direct evaluation methods such as human assessment of answer faithfulness
- **Medium Confidence**: Generalizability of results across diverse domains and question types remains to be fully established

## Next Checks
1. Conduct ablation studies to quantify individual contributions of triple generation versus semantic similarity matching to overall performance improvements
2. Perform human evaluation studies specifically measuring hallucination reduction by having annotators assess answer faithfulness to provided evidence documents
3. Test the method's performance on adversarial question-answer pairs designed to expose potential weaknesses in the knowledge selection process, particularly for questions requiring multi-hop reasoning or handling of implicit information