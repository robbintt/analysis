---
ver: rpa2
title: 'Hallucinations in Neural Automatic Speech Recognition: Identifying Errors
  and Hallucinatory Models'
arxiv_id: '2401.01572'
source_url: https://arxiv.org/abs/2401.01572
tags:
- hallucinations
- noise
- errors
- error
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores hallucinations in neural automatic speech recognition
  (ASR), defined as fluent but semantically unrelated transcriptions. The authors
  propose a perturbation-based method to assess hallucination susceptibility at test
  time without training data access.
---

# Hallucinations in Neural Automatic Speech Recognition: Identifying Errors and Hallucinatory Models

## Quick Facts
- **arXiv ID**: 2401.01572
- **Source URL**: https://arxiv.org/abs/2401.01572
- **Reference count**: 10
- **Key outcome**: The paper proposes a perturbation-based method to detect hallucinations in ASR models, identifying Unique-Unique (UU) models as most susceptible, with hallucination rates increasing from 1-2% to 7-10% after noise injection.

## Executive Summary
This paper addresses hallucinations in neural automatic speech recognition (ASR), where models generate fluent but semantically unrelated transcriptions. The authors develop a novel perturbation-based method to assess hallucination susceptibility at test time without requiring training data access. By injecting random noise at the beginning of utterances, they can distinguish hallucinatory outputs from phonetic errors using cosine similarity and perplexity metrics. The study reveals that traditional metrics like WER cannot differentiate between hallucinatory and non-hallucinatory models, while their proposed approach successfully identifies models prone to hallucinations and provides insights into error patterns.

## Method Summary
The method involves training ASR models on datasets with intentionally mismatched labels (8-16% noise) to create different error patterns, then applying random noise injection at the beginning of evaluation utterances to induce hallucinations. The hallucination detection algorithm compares outputs before and after perturbation, using cosine similarity to measure semantic disconnection (threshold 0.2) and perplexity to assess fluency. The approach leverages transformer models from fairseq and evaluates performance on LibriSpeech 360-hour dataset, creating four noisy variants (UU, RR, RU, UR) based on Raunak et al.'s label noise methodology.

## Key Results
- Random noise injection at the beginning of utterances significantly increases hallucination susceptibility, particularly in UU models (7-10% increase)
- Cosine similarity effectively distinguishes hallucinations (0.0-0.2) from phonetic errors (0.2-1.0)
- WER cannot differentiate between hallucinatory and non-hallucinatory models with similar baseline scores
- Noise injected throughout the entire utterance degrades overall performance rather than specifically inducing hallucinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random noise injection at the beginning of utterances increases hallucination susceptibility in ASR models with label noise.
- Mechanism: Noise disrupts the model's attention to the initial acoustic signal, causing it to rely more heavily on learned patterns from mismatched labels rather than the actual input.
- Core assumption: Models with mismatched training labels are more likely to generate hallucinations when their attention to the source signal is disrupted.
- Evidence anchors:
  - [abstract]: "We discover how to induce hallucinations with a random noise injection to the utterance."
  - [section]: "We discover that the noise injection at the beginning of the utterance has the influence on hallucination distribution, while the noise injected throughout the whole utterance does not."
  - [corpus]: Weak evidence - only mentions similar hallucination research without specific noise injection mechanisms.
- Break condition: If noise injection throughout the sentence shows similar hallucination increases, or if baseline models show hallucination increases with beginning noise injection.

### Mechanism 2
- Claim: Cosine similarity effectively distinguishes hallucinations from phonetic errors by measuring semantic disconnection.
- Mechanism: Hallucinations are semantically unrelated to the reference, resulting in low cosine similarity scores, while phonetic errors maintain some semantic connection despite transcription errors.
- Core assumption: Semantic similarity can be quantified through vector representations and cosine distance.
- Evidence anchors:
  - [abstract]: "We devise a framework for identifying hallucinations by analysing their semantic connection with the ground truth and their fluency."
  - [section]: "We discover that the cosine similarity is sensitive towards error distribution. In most of the analysed cases, we observe clear distinction between hallucinatory outputs and phonetic errors."
  - [corpus]: Weak evidence - mentions semantic disconnection but lacks specific cosine similarity methodology.
- Break condition: If cosine similarity fails to distinguish between error types in other ASR models or datasets, or if high cosine similarity hallucinatory outputs exist.

### Mechanism 3
- Claim: Unique-Unique (UU) models with mismatched labels show increased hallucination susceptibility without affecting baseline WER.
- Mechanism: Training with completely mismatched source-target pairs creates learned associations that produce hallucinatory outputs when the model encounters perturbed or ambiguous inputs.
- Core assumption: Mismatched label patterns create internal representations that favor hallucination generation over accurate transcription.
- Evidence anchors:
  - [abstract]: "We show that commonly used metrics, such as word error rates, cannot differentiate between hallucinatory and non-hallucinatory models."
  - [section]: "We discover that hallucinatory models return lower amounts of hallucinatory outputs before than after perturbation. Therefore, the hallucinatory model output returns negative results if perturbation hallucination scores are deducted from non-perturbed hallucination scores."
  - [corpus]: Moderate evidence - mentions similar label noise experiments but focuses on MT rather than ASR.
- Break condition: If other mismatched label types (RR, RU, UR) show similar hallucination increases, or if UU models maintain hallucination susceptibility with different perturbation methods.

## Foundational Learning

- Cosine similarity in NLP:
  - Why needed here: To quantify semantic disconnection between hallucinatory outputs and reference transcriptions.
  - Quick check question: If two sentences have cosine similarity of 0.95, are they semantically similar or dissimilar?

- Perplexity for fluency evaluation:
  - Why needed here: To distinguish fluent hallucinations from non-fluent errors like word salad or random repetitions.
  - Quick check question: Would a sentence with perplexity of 50 be considered more or less fluent than one with perplexity of 500?

- Error classification in ASR:
  - Why needed here: To differentiate hallucinations from phonetic errors, oscillations, and other transcription mistakes.
  - Quick check question: What distinguishes a phonetic error from a hallucination in terms of their relationship to the reference transcription?

## Architecture Onboarding

- Component map: ASR model (Fairseq transformer) -> Perturbation module (random noise injection) -> Evaluation pipeline (cosine similarity, perplexity, WER) -> Dataset preparation (mismatched labels) -> Language model (Flan T5 Small for perplexity)

- Critical path: 1. Train baseline and mismatched label models 2. Apply noise injection to evaluation set 3. Generate transcriptions 4. Calculate cosine similarity and perplexity 5. Classify errors and identify hallucinations

- Design tradeoffs:
  - Using cosine similarity vs. more complex semantic similarity measures
  - Random noise injection vs. structured perturbations
  - Mismatched label volume (8% vs 16%) affecting training time and error patterns

- Failure signatures:
  - High hallucination rates in baseline models indicate perturbation issues
  - Low distinction between error types suggests threshold problems
  - No hallucination increase in UU model indicates mismatched label insufficiency

- First 3 experiments:
  1. Train baseline model on clean LibriSpeech 360 and verify WER scores
  2. Create UU model with 8% mismatched labels and compare hallucination rates before/after noise injection
  3. Test cosine similarity thresholds using known error examples to validate distinction between hallucinations and phonetic errors

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the susceptibility to hallucinations vary across different languages and speech-oriented tasks such as spoken language understanding (SLU)?
- Basis in paper: [explicit] The paper acknowledges that their research is limited to English ASR and suggests that more work is needed regarding generalization across different languages and speech-oriented tasks such as SLU.
- Why unresolved: The study only focuses on English ASR, and the methods used may not be directly applicable to other languages or tasks like SLU. The paper does not provide any insights into how hallucinations might manifest or be detected in these other contexts.
- What evidence would resolve it: Conducting similar experiments on ASR models trained for different languages and speech-oriented tasks, and comparing the results to the findings in this paper. This would involve creating noisy datasets with mismatched labels for each language/task, inducing hallucinations through random noise injection, and applying the hallucination detection algorithm to assess susceptibility.

### Open Question 2
- Question: Can the hallucination detection algorithm be extended to work with unsupervised models, such as wav2vec2?
- Basis in paper: [explicit] The paper mentions that their methods could be re-examined in NLP tasks and suggests that the hallucination detection algorithm could be extended to explore the hallucinatory capability of unsupervised models like wav2vec2.
- Why unresolved: The current algorithm relies on having access to the reference transcriptions for calculating semantic similarity and perplexity, which are used to identify hallucinations. Unsupervised models do not have access to reference transcriptions during training, so it is unclear how the algorithm would need to be modified to work in this setting.
- What evidence would resolve it: Developing a modified version of the hallucination detection algorithm that can work with unsupervised models, and testing it on models like wav2vec2. This would involve finding alternative ways to assess semantic similarity and fluency without relying on reference transcriptions, and evaluating the effectiveness of the modified algorithm in detecting hallucinations.

### Open Question 3
- Question: How can the thresholds used for identifying hallucinations and phonetic errors be automated for better generalization capability of the methods?
- Basis in paper: [explicit] The paper states that the thresholds used for identifying hallucinations and phonetic errors are based on distributions of errors from the datasets and uses a heuristic approach. It suggests that automating these thresholds would improve the generalization capability of the methods.
- Why unresolved: The current approach relies on manually setting thresholds based on the specific datasets used in the experiments. This may not generalize well to other datasets or languages, as the distributions of errors could be different. An automated method for determining appropriate thresholds would make the approach more robust and widely applicable.
- What evidence would resolve it: Developing and testing an automated method for determining the thresholds used in the hallucination detection algorithm. This could involve using statistical techniques to analyze the distributions of errors in a given dataset and automatically setting the thresholds based on the characteristics of the data. The effectiveness of the automated method could then be evaluated by applying it to various datasets and comparing the results to the manual threshold approach used in this paper.

## Limitations
- The perturbation method relies on specific noise injection parameters that may not generalize across different ASR architectures or datasets.
- Cosine similarity thresholds (0.2 for hallucination detection) may not be universally applicable and assume consistent embedding space quality.
- The study focuses on mismatched label patterns from machine translation research, which may not represent all types of hallucinations in real-world ASR applications.

## Confidence
- **High confidence**: The perturbation method successfully induces hallucinations in UU models (7-10% increase observed), and cosine similarity effectively distinguishes hallucinations from phonetic errors (clear separation at 0.0-0.2 vs 0.2-1.0).
- **Medium confidence**: The claim that WER cannot differentiate hallucinatory from non-hallucinatory models is supported by the data, but this may be architecture-specific and requires validation across different ASR frameworks.
- **Medium confidence**: The mechanism linking mismatched training labels to hallucination susceptibility is plausible but not definitively proven - correlation is shown but causation requires further experimentation.

## Next Checks
1. Test perturbation effectiveness on a different ASR architecture (e.g., RNN-based models) to verify the noise injection mechanism generalizes beyond transformers.
2. Apply the cosine similarity threshold validation to CommonVoice dataset to confirm semantic similarity measures work across different speaker populations and recording conditions.
3. Compare hallucination detection performance using alternative semantic similarity metrics (e.g., BERTScore) against the cosine similarity approach to validate threshold robustness.