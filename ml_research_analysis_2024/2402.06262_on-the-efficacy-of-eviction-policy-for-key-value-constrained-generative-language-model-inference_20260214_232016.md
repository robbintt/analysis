---
ver: rpa2
title: On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language
  Model Inference
arxiv_id: '2402.06262'
source_url: https://arxiv.org/abs/2402.06262
tags:
- eviction
- cache
- attention
- arxiv
- key-value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes existing key-value cache eviction policies
  for memory-efficient large language model inference. The authors decompose eviction
  policies into importance score calculation and eviction scope construction, identifying
  limitations in current approaches.
---

# On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference

## Quick Facts
- arXiv ID: 2402.06262
- Source URL: https://arxiv.org/abs/2402.06262
- Authors: Siyu Ren; Kenny Q. Zhu
- Reference count: 19
- Key outcome: RoCo eviction policy achieves superior consistency and robustness across multiple tasks with reduced cache size, particularly excelling at low cache budgets

## Executive Summary
This paper analyzes existing key-value cache eviction policies for memory-efficient large language model inference. The authors decompose eviction policies into importance score calculation and eviction scope construction, identifying limitations in current approaches. They propose RoCo, which uses mean attention scores for importance and standard deviation for eviction scope construction, showing superior consistency and robustness. Experiments across multiple tasks demonstrate RoCo's effectiveness in maintaining generation quality with reduced cache size, particularly excelling at low cache budgets. The authors also release EasyKV, a software package for key-value constrained generative inference.

## Method Summary
The paper proposes RoCo (Robust and Consistent eviction policy), which decomposes eviction policies into two components: importance score calculation and eviction scope construction. For importance scoring, RoCo uses Mean Attention Score (MAS) that divides each token's accumulative attention score by how many times that token is attended by future tokens, normalizing for recency bias. For eviction scope construction, RoCo uses the standard deviation of attention scores across future tokens, prioritizing tokens with lowest variance for eviction. The method also introduces block-wise eviction for faster prefilling, where multiple tokens are evicted and encoded in parallel rather than one at a time.

## Key Results
- RoCo achieves over 0.9 Jaccard similarity with full KV cache even at 0.3 cache budget rate
- MAS consistently outperforms existing methods across all KV cache budget rates (0.3-0.6)
- RoCo shows superior performance particularly at low cache budgets, maintaining generation quality while reducing memory usage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mean Attention Score (MAS) better approximates global importance than accumulative attention scores
- Mechanism: MAS divides each token's accumulative attention score by the number of times that token is attended by future tokens, effectively normalizing for recency bias
- Core assumption: The relative importance of tokens can be better captured by their average influence rather than total accumulated influence
- Evidence anchors:
  - [abstract] "MAS has remarkably higher consistency among all methods, achieving over 0.9 Jaccard similarity even at 0.3 cache budget rate"
  - [section 4.3] "MAS divides each token's accumulative attention score by how many times that token is attended by future tokens"
  - [corpus] Weak evidence - no direct citations supporting this mechanism

### Mechanism 2
- Claim: Standard deviation of attention scores provides more stable eviction scope construction than local window
- Mechanism: Tokens with lowest variance in attention scores across future tokens are prioritized for eviction, as their influence is more predictable and less critical
- Core assumption: Tokens with stable (low variance) attention patterns have more predictable and therefore less critical influence on future generations
- Evidence anchors:
  - [abstract] "we propose another way to construct the eviction scope which exploits a phenomenon termed persistence of attention robustness"
  - [section 4.3] "the standard deviation of the attention probabilities a token receives from future tokens typically undergoes a brief ascending phase before settling into a stable decline"
  - [corpus] No direct evidence in corpus papers - this appears to be a novel contribution

### Mechanism 3
- Claim: Block-wise eviction significantly speeds up prefilling while maintaining quality
- Mechanism: Instead of evicting one token at a time, multiple tokens are evicted and encoded in parallel, turning I/O-bound prefilling into computation-bound processing
- Core assumption: Parallel eviction and encoding can be performed without significantly degrading the quality of selected tokens to retain
- Evidence anchors:
  - [section 5.3] "To accelerate key-value constrained prompt encoding, RoCo allows for performing evict-and-encode in a block-wise manner"
  - [section 5.3] "In Appendix B, we show that such block-wise eviction greatly speeds up prefilling while retaining similar output quality as token-wise eviction"
  - [corpus] No direct evidence in corpus papers - appears to be a practical optimization

## Foundational Learning

- Concept: Transformer attention mechanism
  - Why needed here: Understanding how key-value cache works in auto-regressive generation is fundamental to grasping why eviction policies matter
  - Quick check question: What is the shape of the key-value cache tensor and how does it grow during inference?

- Concept: Attention score interpretation
  - Why needed here: Different methods for calculating token importance rely on understanding what attention scores represent
  - Quick check question: How does last token attention score differ from accumulative attention score in measuring token importance?

- Concept: Cache eviction strategies
  - Why needed here: The paper builds on existing eviction methods and proposes improvements, requiring understanding of the baseline approaches
  - Quick check question: What is the difference between importance score calculation and eviction scope construction in eviction policies?

## Architecture Onboarding

- Component map:
  - Attention layer: produces Q, K, V matrices
  - Key-value cache: stores K, V vectors for auto-regressive generation
  - Eviction policy: decides which KV pairs to remove when cache is full
  - Cache budget manager: enforces maximum cache size constraints

- Critical path:
  1. Generate attention scores during forward pass
  2. Calculate importance scores (MAS + standard deviation)
  3. Construct eviction scope
  4. Remove lowest importance tokens when cache full
  5. Append new token's KV vectors

- Design tradeoffs:
  - MAS vs accumulative scores: better normalization vs. simpler implementation
  - Standard deviation vs. local window: more stable scope vs. simpler computation
  - Block-wise vs. token-wise eviction: faster prefilling vs. more precise eviction decisions

- Failure signatures:
  - Performance degradation with long sequences: may indicate ineffective importance scoring
  - Inconsistent results across different cache budgets: suggests instability in eviction scope construction
  - Memory errors during prefilling: could indicate block size too large for available memory

- First 3 experiments:
  1. Compare MAS against AAS and LTAS on Jaccard similarity with full KV cache at different budget rates
  2. Test standard deviation-based eviction scope against local window on summarization quality at various scope sizes
  3. Evaluate block-wise eviction speed-up versus token-wise eviction on long-context tasks with different block sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does RoCo's performance scale with sequence length and KV cache budget beyond the evaluated ranges?
- Basis in paper: [inferred] The paper evaluates RoCo at specific KV cache budget rates (0.3-0.6) and on tasks with typical sequence lengths, but does not explore extreme scenarios or longer sequences.
- Why unresolved: The paper focuses on practical use cases and common evaluation benchmarks, leaving the behavior at extreme values unexplored.
- What evidence would resolve it: Experiments showing RoCo's performance degradation curves at very low (<0.2) and very high (>0.8) KV cache budget rates, and with sequence lengths exceeding typical benchmarks (e.g., 8k+ tokens).

### Open Question 2
- Question: Does RoCo's attention-based importance scoring generalize to non-Transformer architectures like State Space Models or Mamba?
- Basis in paper: [inferred] The paper focuses exclusively on Transformer-based models and discusses extension to GQA/MQA as a special case, but does not address alternative architectures.
- Why unresolved: The authors do not explore how RoCo's core mechanism (attention score-based importance) would translate to architectures without traditional attention mechanisms.
- What evidence would resolve it: Empirical validation of RoCo or similar policies on non-Transformer models like Mamba, RWKV, or other State Space Model variants.

### Open Question 3
- Question: What is the theoretical relationship between RoCo's standard deviation-based eviction scope and the information bottleneck principle?
- Basis in paper: [explicit] The authors introduce the "persistence of attention robustness" phenomenon and use standard deviation for eviction scope construction, but do not provide theoretical justification.
- Why unresolved: While the empirical results show effectiveness, the paper does not explain why standard deviation of attention scores correlates with token importance from an information-theoretic perspective.
- What evidence would resolve it: A formal analysis connecting the variance of attention distributions to information retention capacity, or experiments showing correlation between standard deviation and downstream task performance across multiple model families.

### Open Question 4
- Question: How does RoCo's block-wise eviction affect long-range dependencies in tasks requiring extensive context?
- Basis in paper: [explicit] The authors introduce block-wise eviction for efficiency and show minimal performance degradation, but do not analyze its impact on preserving long-range dependencies.
- Why unresolved: The paper reports aggregate performance metrics but does not examine whether block-wise eviction disproportionately affects tokens that maintain long-range context.
- What evidence would resolve it: Ablation studies comparing RoCo's performance on tasks requiring distant context (e.g., multi-document QA, long-form reasoning) versus local context tasks, or analysis of which tokens are most frequently evicted in block-wise mode.

## Limitations
- Evaluation focuses primarily on 7B-parameter models, raising questions about scalability to larger models
- Standard deviation-based eviction scope construction lacks theoretical grounding and empirical validation
- Block-wise eviction optimization is only briefly mentioned with results relegated to the appendix

## Confidence
**High Confidence Claims:**
- RoCo's mean attention score (MAS) consistently outperforms existing importance scoring methods across all tested cache budget rates (0.3-0.6)
- RoCo demonstrates superior robustness and consistency compared to baseline methods, particularly at low cache budgets
- The decomposition of eviction policies into importance scoring and eviction scope construction provides a useful analytical framework

**Medium Confidence Claims:**
- Standard deviation-based eviction scope construction provides more stable eviction decisions than local window approaches
- Block-wise eviction significantly speeds up prefilling while maintaining quality (limited empirical evidence in appendix)

**Low Confidence Claims:**
- The theoretical justification for why standard deviation of attention scores predicts token importance
- Performance guarantees when scaling to larger models (13B, 70B+)
- Generalizability across diverse attention patterns and model architectures

## Next Checks
1. **Scalability Validation**: Test RoCo on larger model sizes (13B and 70B) to verify whether the performance gains observed with 7B models persist. This should include measuring both inference speed and generation quality across different cache budget rates.

2. **Ablation Studies on Eviction Scope Construction**: Conduct controlled experiments comparing standard deviation-based scope construction against alternative methods (e.g., attention entropy, temporal locality) to isolate the contribution of this component to RoCo's overall performance.

3. **Long-Context Behavior Analysis**: Evaluate RoCo on sequences exceeding 8K tokens to assess how attention patterns and eviction decisions evolve in long-context scenarios. This should include analysis of how mean attention scores and standard deviation metrics behave as sequence length increases.