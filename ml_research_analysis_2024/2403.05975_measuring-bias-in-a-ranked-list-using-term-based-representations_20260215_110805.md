---
ver: rpa2
title: Measuring Bias in a Ranked List using Term-based Representations
arxiv_id: '2403.05975'
source_url: https://arxiv.org/abs/2403.05975
tags:
- fairness
- ranked
- bias
- documents
- ranking
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the problem of measuring bias in ranked lists
  using term-based representations, specifically for gender bias in document ranking.
  The authors propose a novel metric called TExFAIR (term exposure-based fairness),
  which extends the attention-weighted ranking fairness (AWRF) framework with two
  key adaptations: (i) an explicit definition of associating documents to groups based
  on probabilistic term-level associations, and (ii) a rank-biased discounting factor
  (RBDF) for counting non-representative documents towards fairness measurement.'
---

# Measuring Bias in a Ranked List using Term-based Representations

## Quick Facts
- arXiv ID: 2403.05975
- Source URL: https://arxiv.org/abs/2403.05975
- Reference count: 0
- One-line primary result: Proposed TExFAIR metric correlates with NFaiRR at 0.34-0.55, measuring different dimension of fairness with less sensitivity to ranking cut-offs

## Executive Summary
This paper addresses the problem of measuring bias in ranked lists using term-based representations, specifically for gender bias in document ranking. The authors propose a novel metric called TExFAIR (term exposure-based fairness), which extends the attention-weighted ranking fairness (AWRF) framework with two key adaptations: (i) an explicit definition of associating documents to groups based on probabilistic term-level associations, and (ii) a rank-biased discounting factor (RBDF) for counting non-representative documents towards fairness measurement. The primary results show that TExFAIR measures a different dimension of fairness compared to the commonly used NFaiRR metric, with a correlation coefficient between 0.34 and 0.55 on two query sets from the MS MARCO Passage Ranking collection.

## Method Summary
The paper proposes TExFAIR, a metric for measuring bias in ranked lists using term-based representations. TExFAIR extends the AWRF framework by calculating group representation based on term exposures weighted by position bias, rather than document-level fairness scores. The metric incorporates a rank-biased discounting factor (RBDF) to account for non-representative documents that don't contain group-representative terms. The authors evaluate TExFAIR against NFaiRR on the MS MARCO Passage Ranking collection using two query sets (QS1 with 1756 non-gendered queries and QS2 with 215 bias-sensitive queries) across multiple ranking models including BM25, sparse/dense retrieval models, and re-rankers.

## Key Results
- TExFAIR measures a different dimension of fairness than NFaiRR, with correlation coefficient 0.34-0.55
- TExFAIR is less sensitive to ranking cut-off values than NFaiRR due to the RBDF component
- Counterfactual evaluation shows inherent bias of ranking models differs from bias in ranked results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TExFAIR measures a different dimension of fairness than NFaiRR by evaluating group representation at the term level rather than document level.
- Mechanism: TExFAIR calculates group representation based on the total exposure of group-representative terms in the ranked list, weighted by position bias. This captures how terms from different groups are distributed across the entire ranking, not just individual document unbiasedness scores.
- Core assumption: Group representation in a ranked list can be accurately estimated by aggregating term exposures rather than aggregating document-level fairness scores.
- Evidence anchors:
  - [abstract] "TExFAIR measures a different dimension of fairness than NFaiRR" with correlation coefficient 0.34-0.55
  - [section] "NFaiRR is document-centric... TExFAIR, on the other hand, is ranking-centric"
- Break condition: If term exposure calculations don't accurately reflect group representation, or if document-level fairness is more important than term-level distribution for the application context.

### Mechanism 2
- Claim: The rank-biased discounting factor (RBDF) addresses the limitation of ignoring non-representative documents in fairness evaluation.
- Mechanism: RBDF discounts the bias measurement by the proportion of documents that contain at least one group-representative term, weighted by their position. This ensures that documents without group terms still contribute to the fairness calculation.
- Core assumption: Non-representative documents significantly impact fairness evaluation and should be accounted for proportionally to their position and frequency.
- Evidence anchors:
  - [abstract] "(ii) a rank-biased discounting factor (RBDF) for counting non-representative documents towards the measurement of the fairness of a ranked list"
  - [section] "discounting seems to be necessary for the evaluation of gender fairness in document ranking with group-representative terms, due to the effect of non-representative documents"
- Break condition: If non-representative documents don't significantly impact fairness perception, or if the RBDF formula doesn't accurately capture their contribution.

### Mechanism 3
- Claim: Counterfactual data substitution (CDS) reveals inherent bias in ranking models that differs from bias in ranked results.
- Mechanism: By replacing gendered terms with their counterparts in the opposite gender and measuring rank divergence (CRBO), CDS estimates how much the ranking model's learned associations affect ranking outcomes regardless of input document content.
- Core assumption: Ranking models learn gendered correlations during pre-training/fine-tuning that persist even when input documents are counterfactual, and this learned bias can be measured through rank divergence.
- Evidence anchors:
  - [abstract] "perform a counterfactual evaluation to measure the inherent bias of ranking models towards different gender groups"
  - [section] "CRBO disentangles the bias of a model towards genders from the bias of the ranked results it provides"
- Break condition: If ranking models don't learn gendered correlations, or if rank divergence doesn't accurately reflect inherent bias.

## Foundational Learning

- Term frequency and position bias concepts
  - Why needed here: TExFAIR explicitly calculates term exposure as tf(t, drq)/|drq| weighted by position bias (log(r+1))^(-1)
  - Quick check question: How does position bias affect term exposure calculations in ranked lists?

- Fairness evaluation frameworks and distance metrics
  - Why needed here: TExFAIR builds on AWRF framework and uses absolute divergence |p(Gi|q,k) - p̂Gi| as the fairness criterion
  - Quick check question: What properties should a distance metric have for measuring group representation fairness?

- Counterfactual evaluation methodology
  - Why needed here: The paper uses CDS to measure inherent model bias by substituting gendered terms and measuring rank divergence
  - Quick check question: How does counterfactual data substitution help distinguish between input bias and model bias?

## Architecture Onboarding

- Component map: TExFAIR metric (term exposure calculation → group representation estimation → bias divergence → RBDF discounting → fairness score), NFaiRR baseline metric, CRBO counterfactual evaluation component, MS MARCO passage ranking dataset, PLM-based ranking models (BM25, UniCOIL, DeepImpact, ANCE, DistilBERTKD, DistilBERTTASB, TCT-ColBERTv1, SBERT, BERT, MiniLMKD, TinyBERTKD)

- Critical path: For TExFAIR calculation: term frequency extraction → position weighting → term exposure aggregation → group representation computation → divergence calculation → RBDF application → final fairness score

- Design tradeoffs: TExFAIR vs NFaiRR (term-level vs document-level fairness), with RBDF vs without RBDF (sensitivity to cut-off vs ignoring non-representative documents), term-based vs semantic fairness evaluation (practical vs user-centric)

- Failure signatures: High correlation with NFaiRR (>0.7) would indicate TExFAIR isn't measuring different dimension, strong cut-off sensitivity without RBDF would make metric unreliable, low CRBO values would suggest models aren't learning gendered correlations

- First 3 experiments:
  1. Replicate NFaiRR vs TExFAIR correlation calculation on QS1 and QS2 query sets to verify different dimension measurement
  2. Test RBDF impact by comparing TExFAIR with and without proportionality across different ranking cut-offs
  3. Implement counterfactual data substitution and measure CRBO for all ranking models to assess inherent bias vs output bias

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the relationship between inherent biases of ranking models and the fairness of the ranked lists they produce?
- Basis in paper: [explicit] The authors mention that their counterfactual evaluation shows a discrepancy between measured bias in ranked lists and inherent bias in ranking models themselves.
- Why unresolved: The paper acknowledges this discrepancy but states that further investigation is needed to understand the relationship between inherent model biases and the fairness of their output.
- What evidence would resolve it: Systematic studies comparing inherent model biases (measured through counterfactual evaluation) with the fairness of their ranked outputs across different ranking models and datasets.

### Open Question 2
- Question: How do more semantic approaches to societal fairness evaluation compare to term-based methods?
- Basis in paper: [explicit] The authors acknowledge that term-based evaluation is limited compared to real-world user evaluations and plan to work on more user-oriented methods.
- Why unresolved: The paper only uses term-based methods and does not compare them to semantic or user-oriented approaches.
- What evidence would resolve it: Empirical studies comparing term-based fairness metrics with semantic or user-based evaluations of fairness in document rankings.

### Open Question 3
- Question: How does the choice of term-based group representation affect fairness measurement?
- Basis in paper: [inferred] The paper uses gender-representative terms to define groups, but doesn't explore how different term sets might affect fairness measurements.
- Why unresolved: The paper assumes a fixed set of group-representative terms without investigating how different term selections might influence fairness results.
- What evidence would resolve it: Comparative studies using different sets of group-representative terms to measure how term selection affects fairness metrics and model evaluation.

## Limitations

- TExFAIR's correlation with NFaiRR (0.34-0.55) suggests the metrics measure related but not entirely distinct dimensions of fairness
- The choice of gender-representative terms may introduce bias in the fairness measurements
- The counterfactual evaluation methodology assumes that rank divergence (CRBO) accurately reflects inherent model bias

## Confidence

**Confidence assessment**: High confidence in the mechanism for term exposure calculation and RBDF formulation, as these are mathematically well-defined. Medium confidence in the claim that TExFAIR measures a different dimension of fairness than NFaiRR, given the moderate correlation values. Low confidence in the interpretation of CRBO results as measuring inherent model bias, as this depends on assumptions about how models learn gendered correlations during training.

## Next Checks

1. Perform ablation studies removing the RBDF component to quantify its impact on TExFAIR's sensitivity to ranking cut-offs across diverse query sets
2. Test TExFAIR on additional bias dimensions (e.g., racial, religious) using different term sets to assess generalizability beyond gender bias
3. Conduct user studies comparing TExFAIR scores with human judgments of fairness to validate whether term-level fairness aligns with user perception of fair rankings