---
ver: rpa2
title: 'SPORTU: A Comprehensive Sports Understanding Benchmark for Multimodal Large
  Language Models'
arxiv_id: '2410.08474'
source_url: https://arxiv.org/abs/2410.08474
tags:
- question
- video
- sports
- conference
- reasoning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SPORTU is a comprehensive benchmark for evaluating multimodal large
  language models (MLLMs) on sports understanding. It includes SPORTU-text, a 900-question
  dataset for text-based reasoning about sports rules, strategies, and scenarios,
  and SPORTU-video, a 12,048-question dataset with slow-motion video clips across
  seven sports, designed to assess multilevel reasoning from recognition to complex
  rule application.
---

# SPORTU: A Comprehensive Sports Understanding Benchmark for Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2410.08474
- Source URL: https://arxiv.org/abs/2410.08474
- Authors: Haotian Xia; Zhengbang Yang; Junbo Zou; Rhys Tracy; Yuqing Wang; Chi Lu; Christopher Lai; Yanjun He; Xun Shao; Zhuoqing Xie; Yuan-fang Wang; Weining Shen; Hanjie Chen
- Reference count: 40
- Primary result: SPORTU benchmark evaluates MLLMs on sports understanding, showing models struggle with rule-based reasoning and consistent visual perception across camera angles

## Executive Summary
SPORTU is a comprehensive benchmark designed to evaluate multimodal large language models on sports understanding across text and video domains. The benchmark includes SPORTU-text with 900 questions and SPORTU-video with 12,048 questions across seven sports, organized by difficulty levels from easy (common sense) to hard (deep rule-based reasoning). Evaluation of 14 MLLMs shows models like GPT-4o achieving 71% accuracy on text tasks but only 57.8% on hard video tasks, highlighting significant gaps in domain knowledge and reasoning capabilities. Error analysis reveals frequent issues with question understanding, hallucinations, and lack of consistent visual reasoning across different camera perspectives.

## Method Summary
SPORTU evaluates MLLMs using two datasets: SPORTU-text (900 questions) and SPORTU-video (1,701 clips, 12,048 QA pairs). The evaluation employs few-shot learning with chain-of-thought prompting for LLMs and multi-frame video input for MLLMs. Models are tested across difficulty levels (easy/medium/hard) and sports categories including basketball, soccer, and tennis. Evaluation metrics include accuracy, ROUGE-L, BERTScore, and human ratings. The benchmark also tests model consistency using multi-angle video inputs and analyzes error patterns across different reasoning types.

## Key Results
- GPT-4o achieves 71% accuracy on SPORTU-text and 57.8% on hard SPORTU-video tasks
- Model performance drops significantly on hard-level questions requiring deep rule-based reasoning
- Performance varies across camera angles, indicating inconsistent visual reasoning
- Chain-of-thought prompting sometimes decreases accuracy compared to direct answer prediction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The benchmark's tiered difficulty structure (easy/medium/hard) enables targeted evaluation of model reasoning depth by isolating tasks requiring commonsense from those requiring domain-specific knowledge.
- **Mechanism:** By stratifying tasks into levels where easy-level questions rely on general perception and hard-level questions require rule-based reasoning, the benchmark allows precise identification of where models fail—either due to insufficient domain knowledge or inability to integrate visual perception with rules.
- **Core assumption:** Models that perform well on easy-level questions but poorly on hard-level ones lack domain-specific reasoning capabilities, not general reasoning ability.
- **Evidence anchors:**
  - [abstract]: "Experiments show that models fall short on hard tasks that require deep reasoning and rule-based understanding."
  - [section]: "Easy-level questions... do not require sports knowledge. Medium-level questions... require sports knowledge beyond commonsense. Hard-level questions... involve deep rule-based reasoning."
- **Break condition:** If models perform uniformly poorly across all difficulty levels, the tiered structure fails to isolate the reasoning bottleneck.

### Mechanism 2
- **Claim:** Chain-of-thought prompting improves reasoning accuracy by forcing models to verbalize intermediate reasoning steps before producing final answers.
- **Mechanism:** By requiring models to generate a rationale before answering (X→RY strategy), the benchmark captures models' reasoning process, revealing whether errors stem from flawed reasoning chains or correct reasoning followed by incorrect predictions.
- **Core assumption:** The quality of intermediate reasoning steps correlates with final answer accuracy.
- **Evidence anchors:**
  - [abstract]: "We also systematically applied different prompt strategies... results reveal that models generally achieve the highest overall performance when directly predicting the answer without providing a rationale."
  - [section]: "However, performance declines when models are required to generate a rationale first and then predict the answer."
- **Break condition:** If models generate correct rationales but still produce incorrect final answers, the mechanism linking reasoning quality to prediction accuracy is broken.

### Mechanism 3
- **Claim:** Multi-angle video inputs test model consistency by evaluating whether models maintain stable understanding across different visual perspectives of the same event.
- **Mechanism:** By providing the same question across multiple camera angles, the benchmark measures whether models can integrate information consistently regardless of viewpoint, revealing robustness in visual reasoning.
- **Core assumption:** A model's ability to answer consistently across perspectives indicates genuine understanding rather than superficial pattern matching.
- **Evidence anchors:**
  - [section]: "We noticed that the performance of models on multi-angle videos shows variability depending on the camera perspectives for the same scene, indicating that models struggle with consistent understanding across different camera angles."
  - [section]: "The analysis reveals that camera angle variation can significantly affect the model's performance."
- **Break condition:** If models show no variation across angles, the multi-angle component fails to provide additional evaluative value.

## Foundational Learning

- **Concept:** Multimodal reasoning integration
  - Why needed here: The benchmark evaluates models' ability to combine visual perception with textual rule understanding, requiring comprehension of how different modalities interact.
  - Quick check question: Can you explain why a model might correctly identify a foul visually but fail to name it correctly without textual rule knowledge?

- **Concept:** Sports domain knowledge structure
  - Why needed here: Understanding the hierarchical nature of sports knowledge (rules → strategies → scenarios) is essential for interpreting the benchmark's tiered difficulty design.
  - Quick check question: What distinguishes a rule-related question from a strategy-related question in the benchmark's taxonomy?

- **Concept:** Prompt engineering for reasoning tasks
  - Why needed here: The benchmark's evaluation depends on understanding how different prompting strategies (direct answer vs. CoT) affect model performance on reasoning tasks.
  - Quick check question: Why might chain-of-thought prompting sometimes decrease accuracy despite improving transparency?

## Architecture Onboarding

- **Component map:** Video collection → Frame extraction → Question generation → Annotation → Storage → Model input → Prompt application → Answer generation → Metric calculation → Error analysis

- **Critical path:** Video preprocessing → Question template application → Expert annotation → Model inference → Performance aggregation → Error analysis

- **Design tradeoffs:**
  - Multi-angle videos provide consistency testing but increase annotation complexity and computational cost
  - Slow-motion clips improve perception accuracy but may not reflect real-time reasoning capabilities
  - Open-ended questions provide deeper insight but require more sophisticated evaluation metrics

- **Failure signatures:**
  - Uniform poor performance across difficulty levels suggests fundamental reasoning limitations
  - High error rates in question understanding indicate prompt comprehension issues
  - Inconsistent performance across camera angles reveals viewpoint-dependent reasoning

- **First 3 experiments:**
  1. Run baseline evaluation on single difficulty level to establish performance floor
  2. Compare prompting strategies (X→Y vs X→RY) on medium difficulty questions to identify reasoning bottlenecks
  3. Test multi-angle consistency on easy questions to verify basic visual understanding stability

## Open Questions the Paper Calls Out
None

## Limitations
- Performance varies significantly across different camera angles, indicating inconsistent visual reasoning
- Models frequently struggle with question understanding and generate hallucinations
- Chain-of-thought prompting sometimes decreases accuracy, suggesting complex reasoning strategies may not always help

## Confidence

**High Confidence**: The tiered difficulty structure's effectiveness in isolating reasoning bottlenecks (Mechanism 1) is well-supported by the observation that models perform poorly specifically on hard-level questions requiring deep rule-based reasoning, while maintaining better performance on easier tasks.

**Medium Confidence**: The finding that chain-of-thought prompting sometimes decreases accuracy (Mechanism 2) is supported by empirical results, though the exact conditions under which CoT helps versus hurts require further investigation.

**Low Confidence**: The claim about multi-angle video inputs testing model consistency (Mechanism 3) shows mixed evidence - while variability across angles is observed, the practical significance and implications for model evaluation remain unclear.

## Next Checks

1. **Cross-angle consistency test**: Conduct controlled experiments comparing model performance on identical questions across all camera angles to quantify the relationship between viewpoint variation and accuracy degradation.

2. **Rule comprehension validation**: Design ablation studies that isolate rule-based reasoning from visual perception by testing models on text-only rule descriptions versus video-only scenarios to determine which component causes the most errors.

3. **Prompt strategy optimization**: Systematically vary chain-of-thought prompt complexity and structure to identify whether simpler reasoning prompts or different prompting strategies could improve performance on hard-level questions.