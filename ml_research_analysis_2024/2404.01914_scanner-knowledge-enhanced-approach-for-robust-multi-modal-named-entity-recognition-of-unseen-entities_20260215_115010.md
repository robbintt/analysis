---
ver: rpa2
title: 'SCANNER: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity Recognition
  of Unseen Entities'
arxiv_id: '2404.01914'
source_url: https://arxiv.org/abs/2404.01914
tags:
- knowledge
- entity
- image
- recognition
- scanner
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SCANNER, a two-stage model for multimodal
  named entity recognition (MNER) and grounded MNER (GMNER) that leverages knowledge
  from multiple sources to improve performance on unseen entities. The approach extracts
  entity candidates in the first stage, then uses them as queries to retrieve relevant
  knowledge from Wikipedia, image captions, and object detectors.
---

# SCANNER: Knowledge-Enhanced Approach for Robust Multi-modal Named Entity Recognition of Unseen Entities

## Quick Facts
- arXiv ID: 2404.01914
- Source URL: https://arxiv.org/abs/2404.01914
- Reference count: 18
- Primary result: Introduces SCANNER, a two-stage model achieving 21% F1 improvement on grounded MNER benchmarks

## Executive Summary
This paper presents SCANNER, a two-stage knowledge-enhanced model for multi-modal named entity recognition (MNER) and grounded MNER (GMNER). The approach addresses the challenge of recognizing unseen entities by extracting knowledge from Wikipedia, image captions, and object detectors, then using this knowledge to enhance entity recognition. SCANNER introduces a novel self-distillation method called "Trust Your Teacher" to handle noisy annotations by blending teacher predictions with ground truth labels. The model establishes a new baseline for GMNER with a 21% improvement in F1 score over prior methods.

## Method Summary
SCANNER employs a two-stage architecture for efficient knowledge utilization. Stage 1 detects entity candidates using BIO tagging on text data. Stage 2 retrieves relevant knowledge for these candidates from Wikipedia, image captions, and object detectors, then performs recognition using a transformer encoder with the retrieved knowledge integrated into prompts. The model introduces a novel self-distillation method "Trust Your Teacher" that softly combines teacher model predictions with ground truth labels based on teacher confidence, improving robustness to noisy annotations. For GMNER, the model incorporates visual grounding using IoU loss to align entity predictions with image regions.

## Key Results
- Achieves 21% F1 improvement over prior methods on grounded MNER benchmarks
- Outperforms existing approaches on standard MNER benchmarks
- Ablation studies confirm effectiveness of knowledge utilization and Trust Your Teacher distillation method
- Establishes new baseline for GMNER task performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Two-stage architecture improves efficiency by extracting entity candidates first, then retrieving knowledge only for those candidates
- Mechanism: Stage 1 uses BIO tagging transformer encoder to detect named entity candidates. Stage 2 takes these candidates as queries to fetch relevant knowledge and perform recognition
- Core assumption: Knowledge retrieval cost is reduced when limited to entity candidates rather than full text
- Evidence anchors: [abstract] "SCANNER is a two-stage structure; we extract entity candidates in the first stage and use it as a query to get knowledge"
- Break condition: If candidate detection fails or produces too many false positives, efficiency gain disappears and retrieval becomes noisy

### Mechanism 2
- Claim: Trust Your Teacher self-distillation method improves robustness by softly blending teacher predictions with ground truth, especially for noisy annotations
- Mechanism: Computes balancing factor from teacher model's confidence on ground truth class. High confidence samples use cross-entropy with GT; low confidence samples use KL divergence with teacher prediction
- Core assumption: Teacher model's confidence score is a reliable proxy for sample noise
- Evidence anchors: [abstract] "we introduce a novel self-distillation method, enhancing the robustness and accuracy of our model in processing training data with inherent uncertainties"
- Break condition: If teacher model is poorly trained or overconfident on wrong labels, distillation becomes harmful

### Mechanism 3
- Claim: Knowledge from multiple modalities improves performance on unseen entities
- Mechanism: For each candidate, constructs prompt with Wikipedia text, image caption, and object-level captions ordered by CLIP similarity, then feeds to transformer encoder for prediction
- Core assumption: Entity-centric knowledge retrieval provides useful context that generalizes to unseen entities
- Evidence anchors: [abstract] "we can boost our performance by utilizing this entity-centric extracted knowledge to address unseen entities"
- Break condition: If knowledge sources are incomplete or noisy, or prompt construction fails to preserve relevant context, performance on unseen entities degrades

## Foundational Learning

- Concept: BIO (Beginning, Inside, Outside) tagging
  - Why needed here: Used in stage 1 to detect entity spans within text
  - Quick check question: Given "New York City", what BIO tags would the model assign to each token?

- Concept: Cross-modal attention alignment
  - Why needed here: Stage 2 integrates image features with text via attention to improve recognition
  - Quick check question: How does CLIP similarity influence object ordering in the prompt construction?

- Concept: Knowledge distillation and KL divergence
  - Why needed here: TYT uses KL divergence between teacher and student predictions to handle noisy labels
  - Quick check question: What does the KL divergence term in the loss function encourage the student model to do?

## Architecture Onboarding

- Component map: Input text -> Stage 1 span detector (RoBERTa-large, BIO tagging) -> Knowledge retrieval (Wikipedia, captions, objects) -> Prompt construction -> Stage 2 prediction (RoBERTa-large, cross-entropy + IoU loss, TYT optional)
- Critical path: Input text → Stage 1 span detector → For each candidate: knowledge retrieval → prompt construction → Stage 2 prediction
- Design tradeoffs:
  - Efficiency vs. recall: Two-stage reduces knowledge retrieval cost but may miss entities if stage 1 fails
  - Prompt length vs. context: More knowledge improves accuracy but increases token count and inference time
  - TYT vs. simplicity: Soft distillation improves robustness but adds training complexity
- Failure signatures:
  - Stage 1: Many false negatives → no candidates for stage 2 to process
  - Stage 2: Low accuracy despite good candidates → prompt construction or knowledge retrieval issue
  - TYT: Overfitting on noisy labels → high train but low dev/test performance
- First 3 experiments:
  1. Run stage 1 on a small dataset and inspect BIO predictions to validate span detection
  2. Test prompt construction with a known entity and manually verify knowledge inclusion
  3. Evaluate TYT by comparing performance on a noisy-labeled subset vs. clean subset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Trust Your Teacher (TYT) distillation method's performance vary with different noise levels in the training data?
- Basis in paper: [explicit] The paper mentions that TYT is designed to handle noisy annotations and compares it to conventional soft distillation methods, showing better performance on MNER and GMNER benchmarks
- Why unresolved: The paper does not provide detailed analysis of TYT's performance under varying noise levels in the training data
- What evidence would resolve it: Experiments comparing TYT's performance with different levels of synthetic noise added to the training data would provide insights into its robustness

### Open Question 2
- Question: What is the impact of using different knowledge sources (e.g., Wikipedia, image captions, object detectors) on the performance of SCANNER for unseen entities?
- Basis in paper: [explicit] The paper discusses the use of various knowledge sources and their contribution to performance improvement, particularly for unseen entities
- Why unresolved: The paper does not provide a detailed analysis of the individual impact of each knowledge source on the performance for unseen entities
- What evidence would resolve it: Ablation studies isolating the effect of each knowledge source on the performance for unseen entities would clarify their individual contributions

### Open Question 3
- Question: How does the choice of visual-language similarity model (e.g., CLIP) affect the performance of SCANNER in grounding entities within images?
- Basis in paper: [explicit] The paper uses CLIP for visual-language similarity to arrange object details in the text prompt, which helps the model focus on relevant objects
- Why unresolved: The paper does not explore the impact of using different visual-language similarity models on the grounding performance
- What evidence would resolve it: Comparative experiments using different visual-language similarity models (e.g., CLIP vs. others) would reveal the impact on grounding accuracy

### Open Question 4
- Question: What are the computational trade-offs of using SCANNER compared to methods that do not utilize extensive knowledge sources?
- Basis in paper: [explicit] The paper mentions that SCANNER takes longer inference time due to the use of vision experts and knowledge extraction
- Why unresolved: The paper does not provide a detailed analysis of the computational costs and trade-offs involved in using SCANNER
- What evidence would resolve it: Benchmarking SCANNER's computational efficiency against other methods in terms of inference time and resource usage would provide a clearer understanding of the trade-offs

### Open Question 5
- Question: How does SCANNER's performance scale with the size and diversity of the training data?
- Basis in paper: [inferred] The paper emphasizes SCANNER's ability to handle unseen entities, suggesting it might benefit from diverse training data
- Why unresolved: The paper does not explore how SCANNER's performance changes with varying sizes and diversity of training data
- What evidence would resolve it: Experiments training SCANNER on datasets of different sizes and diversity levels would reveal its scalability and robustness to data variations

## Limitations
- Claims about being first in grounded MNER lack systematic survey of prior work
- Effectiveness of Trust Your Teacher method relies on unverified assumption about teacher confidence correlation with annotation quality
- No significance testing reported for the claimed 21% F1 improvement
- Computational trade-offs of knowledge-intensive approach not thoroughly analyzed

## Confidence
- **High confidence**: Two-stage architecture design and knowledge integration approach are well-specified and technically sound
- **Medium confidence**: Performance improvements over baselines, particularly the 21% GMNER F1 gain
- **Low confidence**: Claims about being first in grounded MNER and the general effectiveness of the self-distillation method without ablation on noise levels

## Next Checks
1. **Significance testing**: Run SCANNER with 5 different random seeds on GMNER benchmarks and report mean F1 with confidence intervals to verify the claimed 21% improvement is statistically significant

2. **Teacher confidence validation**: Create synthetic noisy annotations at varying noise levels (10%, 30%, 50%) and measure whether TYT's performance degrades more gracefully than standard cross-entropy training

3. **Knowledge ablation study**: Evaluate SCANNER with different knowledge source combinations (Wikipedia only, image captions only, objects only, all three) to quantify the marginal contribution of each knowledge type to unseen entity performance