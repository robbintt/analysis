---
ver: rpa2
title: Preemptive Answer "Attacks" on Chain-of-Thought Reasoning
arxiv_id: '2405.20902'
source_url: https://arxiv.org/abs/2405.20902
tags:
- answer
- preemptive
- reasoning
- problem
- after
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Large language models exhibit strong reasoning when using Chain-of-Thought\
  \ prompting, but their robustness is undermined when they encounter an answer before\
  \ engaging in step-by-step reasoning. This scenario\u2014termed preemptive answer\u2014\
  can arise unintentionally from model outputs or be induced by malicious prompt injection."
---

# Preemptive Answer "Attacks" on Chain-of-Thought Reasoning

## Quick Facts
- **arXiv ID**: 2405.20902
- **Source URL**: https://arxiv.org/abs/2405.20902
- **Reference count**: 40
- **Primary result**: Preemptive answers degrade Chain-of-Thought reasoning accuracy by up to 62%

## Executive Summary
Large language models exhibit strong reasoning capabilities when using Chain-of-Thought prompting, but their robustness is significantly undermined when they encounter an answer before engaging in step-by-step reasoning. This vulnerability, termed "preemptive answer," can arise unintentionally from model outputs or be induced through malicious prompt injection. Experiments across six diverse datasets and multiple CoT methods demonstrate that preemptive answers substantially impair reasoning accuracy, with models sometimes aligning their subsequent reasoning to incorrect initial answers. While mitigation strategies like problem restatement and self-reflection partially reduce this impact, they cannot fully prevent performance decline, highlighting a critical vulnerability in LLM reasoning robustness.

## Method Summary
The study simulates both unintentional and malicious preemptive answers across six datasets (GSM8K, MathQA, MATH, HotpotQA, CommonsenseQA, StrategyQA) using 500 test samples each. Two models (ChatGPT and GPT-4) are evaluated using three CoT methods: Zero-Shot, Few-Shot, and Self-Consistency. The research measures Exact Match accuracy (EM), overall accuracy (ACC), and attack success rate (ASR) to quantify reasoning degradation. Mitigation strategies including problem restatement and self-reflection are tested to assess their effectiveness in reducing the impact of preemptive answers on reasoning performance.

## Key Results
- Preemptive answers significantly impair reasoning capability across various CoT methods and datasets, with accuracy degradation reaching up to 62%
- Problem restatement and self-reflection mitigation strategies only partially reduce the negative impact of preemptive answers on reasoning performance
- When models propose incorrect preemptive answers themselves, subsequent reasoning often aligns with these incorrect answers, demonstrating the vulnerability of the reasoning process

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preemptive answers interfere with Chain-of-Thought reasoning by providing an initial incorrect answer that the model may attempt to justify during reasoning.
- Mechanism: When a model generates or receives an incorrect preemptive answer, its reasoning process becomes influenced to align with that incorrect answer, leading to degraded reasoning accuracy.
- Core assumption: The model treats the preemptive answer as a starting point and attempts to justify it through subsequent reasoning steps, even when incorrect.
- Evidence anchors:
  - [abstract] "preemptive answers significantly impair the model's reasoning capability across various CoT methods and a broad spectrum of datasets"
  - [section] "if the model itself proposes an incorrect preemptive answer, subsequent reasoning results may align with it"
  - [corpus] "Early research into data poisoning attacks against Large Language Models (LLMs) demonstrated the ease with which backdoors could be injected" - weak but relevant to attack surface
- Break condition: When the model's reasoning steps are strong enough to detect and correct the discrepancy between the preemptive answer and the correct solution through logical deduction.

### Mechanism 2
- Claim: Problem restatement mitigates the impact of preemptive answers by refocusing the model's attention on the original question.
- Mechanism: Restating the problem before reasoning acts as a cognitive reset, shifting the model's token probability distribution back toward tokens relevant to the correct solution rather than those influenced by the preemptive answer.
- Core assumption: The model's attention mechanism can be redirected by restating the problem, overriding the influence of the preemptive answer.
- Evidence anchors:
  - [abstract] "Problem restatement... recalibrates the model's focus back to the original question"
  - [section] "Restating the problem can serve as a cognitive restructuring... clearing away biases or incorrect assumptions introduced by the preemptive answer"
  - [corpus] No direct corpus evidence available - weak connection
- Break condition: When the preemptive answer's influence is too strong or when the model's attention mechanism cannot be sufficiently redirected by simple restatement.

### Mechanism 3
- Claim: Self-reflection helps identify and potentially correct reasoning errors introduced by preemptive answers through metacognitive analysis.
- Mechanism: The model reviews its own reasoning steps, identifies inconsistencies between the preemptive answer and intermediate reasoning, and attempts to correct the path to reach a more accurate conclusion.
- Core assumption: The model has sufficient self-awareness to recognize when its reasoning contradicts the logical implications of the problem statement.
- Evidence anchors:
  - [abstract] "self-reflection... addresses misdirection in reasoning"
  - [section] "Reflection operates as a kind of metacognition... where the model effectively 'thinks about its own thinking process'"
  - [corpus] "To investigate the relationship between answer and reasoning, we design a novel evaluation framework" - weak but relevant
- Break condition: When the model's self-reflection capability is insufficient to detect the error, or when the reasoning steps are too influenced by the preemptive answer to allow correction.

## Foundational Learning

- Concept: Chain-of-Thought (CoT) prompting
  - Why needed here: Understanding CoT is essential to grasp how preemptive answers disrupt the reasoning process
  - Quick check question: What distinguishes Zero-Shot CoT from Few-Shot CoT in terms of demonstration usage?

- Concept: In-Context Learning (ICL)
  - Why needed here: ICL vulnerabilities are relevant to understanding how models respond to injected information like preemptive answers
  - Quick check question: How does in-context learning differ from traditional fine-tuning in terms of model parameter updates?

- Concept: Prompt injection attacks
  - Why needed here: Preemptive answers can be viewed as a form of prompt injection that manipulates the model's reasoning process
  - Quick check question: What distinguishes a malicious preemptive answer from an unintentional one in terms of the attack vector?

## Architecture Onboarding

- Component map: Question → Preemptive answer injection → Reasoning chain generation → Final answer selection → Mitigation application (if enabled)
- Critical path: 1. Question input, 2. Preemptive answer generation/injection, 3. Reasoning chain construction, 4. Answer determination, 5. Mitigation application (if enabled)
- Design tradeoffs:
  - Simplicity vs. robustness: Simple CoT methods are more vulnerable but easier to implement
  - Speed vs. accuracy: Self-consistency improves robustness but increases computation time
  - Transparency vs. protection: Mitigation strategies may make the reasoning process less interpretable
- Failure signatures:
  - ASR > 0 indicates attack effectiveness
  - ACC decrease after preemptive answer injection
  - Self-reflection failure to identify or correct errors
  - Problem restatement insufficient to redirect attention
- First 3 experiments:
  1. Measure ASR and ACC degradation across different CoT methods with and without preemptive answers
  2. Test effectiveness of problem restatement by comparing reasoning chains with and without restatement
  3. Evaluate self-reflection capability by measuring error detection and correction rates in reasoning chains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do preemptive answers impact reasoning performance differently across various types of mathematical reasoning problems (e.g., arithmetic vs. algebraic vs. geometric)?
- Basis in paper: [inferred] The paper notes that arithmetic problems show lower interference from incorrect preemptive answers compared to other reasoning tasks, suggesting different impacts across mathematical domains.
- Why unresolved: The paper only provides aggregate results across mathematical datasets without breaking down the differential impact by mathematical domain.
- What evidence would resolve it: Detailed experimental results showing accuracy degradation rates for different mathematical problem types (arithmetic, algebra, geometry) under preemptive answer conditions.

### Open Question 2
- Question: Can self-reflection mechanisms be improved to better detect and correct errors introduced by preemptive answers, particularly when the LLM has already derived a correct intermediate solution?
- Basis in paper: [explicit] The paper notes that even when errors are detected, the Self-Reflection mechanism struggles to deduce the correct answer, particularly when prior flawed reasoning steps have been induced by the malicious preemptive answer.
- Why unresolved: The paper's mitigation strategies only partially address the issue, and the specific challenge of correcting errors when a correct intermediate solution exists but is overridden by an incorrect preemptive answer is not resolved.
- What evidence would resolve it: Experimental results showing improved self-reflection mechanisms that can successfully identify and correct errors even when correct intermediate solutions are present but overridden by preemptive answers.

### Open Question 3
- Question: How do different formats of preemptive answers (e.g., numerical answers vs. step-by-step solutions) affect the LLM's reasoning robustness?
- Basis in paper: [inferred] The paper focuses on preemptive answers in the form of final answers, but does not explore how different formats of preemptive information might affect reasoning robustness.
- Why unresolved: The experimental setup only considers preemptive answers as final answers, not exploring the potential impact of providing partial solutions or step-by-step reasoning upfront.
- What evidence would resolve it: Comparative experiments testing LLM reasoning performance under different preemptive answer formats (numerical only, partial solutions, full step-by-step solutions) to determine which formats are most disruptive.

## Limitations
- Experimental scope limited to six specific datasets, potentially not capturing full diversity of reasoning tasks
- Evaluation focuses primarily on accuracy metrics without exploring other dimensions of reasoning quality
- Mitigation strategies tested are relatively simple interventions that may not represent full spectrum of defensive approaches
- Does not investigate impact of model scale, training data composition, or reasoning complexity on vulnerability

## Confidence
- **High Confidence**: Core finding that preemptive answers degrade reasoning accuracy across multiple CoT methods and datasets is well-supported by experimental results
- **Medium Confidence**: Proposed mechanisms explaining why preemptive answers disrupt reasoning are plausible but not definitively proven
- **Low Confidence**: Effectiveness of mitigation strategies is reported but not thoroughly validated, particularly for self-reflection approach

## Next Checks
1. Test the vulnerability across a broader range of reasoning tasks, including open-ended generation tasks and multi-modal reasoning scenarios, to determine whether observed degradation is consistent across different problem types and complexity levels.

2. Develop and test more sophisticated mitigation approaches beyond simple problem restatement and self-reflection, such as adversarial training with preemptive answers or attention mechanism regularization, to evaluate whether performance degradation can be meaningfully reduced.

3. Conduct human assessments of the reasoning chains generated with and without preemptive answers to determine whether observed accuracy drops correspond to fundamental reasoning errors or merely superficial answer mismatches, providing deeper insight into the nature of the vulnerability.