---
ver: rpa2
title: Coding historical causes of death data with Large Language Models
arxiv_id: '2405.07560'
source_url: https://arxiv.org/abs/2405.07560
tags:
- death
- https
- causesof
- arxiv
- page
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the feasibility of using pre-trained generative
  Large Language Models (LLMs) to automate the assignment of ICD-10 codes to historical
  causes of death. The HiCaD dataset, containing causes of death recorded in the civil
  death register entries of 19,361 individuals from the UK between 1861-1901, was
  used for evaluation.
---

# Coding historical causes of death data with Large Language Models

## Quick Facts
- arXiv ID: 2405.07560
- Source URL: https://arxiv.org/abs/2405.07560
- Reference count: 0
- Primary result: GPT-4 achieves 83% accuracy on historical ICD-10 code assignment from 19,361 UK death records (1861-1901)

## Executive Summary
This paper investigates the feasibility of using pre-trained generative Large Language Models (LLMs) to automate ICD-10 code assignment for historical causes of death. The HiCaD dataset, containing 19,361 causes of death from UK civil death registers (1861-1901), was used to evaluate GPT-3.5, GPT-4, and Llama 2. The findings show that while GPT-4 achieves 83% accuracy and GPT-3.5 achieves 69%, these results are still outperformed by standard machine learning techniques (89% accuracy). All LLMs performed better for causes containing modern terminology versus archaic terms, and for shorter causes (1-2 words) versus longer ones. The authors conclude that current LLMs are not yet adequate for this task and suggest fine-tuning or alternative frameworks like RAG for improvement.

## Method Summary
The study used three pre-trained LLMs (GPT-3.5, GPT-4, Llama 2) to assign ICD-10 codes to causes of death from the HiCaD dataset without any fine-tuning. Each cause of death was processed through the models using carefully engineered prompts that included clear instructions for ICD-10 format and error handling. The outputs were compared against expert-assigned codes to calculate full match, partial match (first 3 characters), and error rates. Performance was analyzed across different cause lengths (1-2, 3-4, 5+ words) and terminology types (archaic vs. current terms). Results were benchmarked against traditional machine learning methods (RandomForest, SVM) and string similarity approaches.

## Key Results
- GPT-4 achieves 83% full match accuracy, GPT-3.5 achieves 69%, Llama 2 achieves 40% on historical cause-of-death coding
- Standard machine learning techniques outperform all LLMs with 89% accuracy
- LLMs perform significantly better on modern terminology (86-96% accuracy) versus archaic terms (39-58% accuracy)
- Accuracy decreases substantially for longer causes: 89% for 1-2 words vs. 45-70% for 5+ words

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-trained LLMs can recognize and map medical terminology from historical death records to ICD-10 codes, but their accuracy is highly dependent on the presence of terms still in use today.
- Mechanism: The model leverages its pre-training on large corpora to identify patterns and semantic relationships between text descriptions and medical codes. When terms are modern, the model can match them to ICD-10 codes based on semantic similarity. For archaic terms, the model lacks contextual knowledge, leading to errors.
- Core assumption: The model's training data includes sufficient contemporary medical terminology to form reliable associations with ICD-10 codes.
- Evidence anchors:
  - [abstract] "All LLMs performed better for causes of death that contained terms still in use today, compared to archaic terms."
  - [section] "We expected that LLMs would perform worse on terms that only exist in historical registers or have a different meaning from the current understanding of the term."
- Break condition: If the model encounters terms that are no longer in use or have changed meaning significantly, accuracy drops sharply.

### Mechanism 2
- Claim: Prompt engineering can improve LLM performance by providing explicit instructions and examples, but the effect is limited by the model's inherent knowledge gaps.
- Mechanism: By structuring prompts with clear instructions and example formats, the model can be guided to produce outputs in the desired format. However, if the model lacks the underlying knowledge to map terms to codes, prompt engineering cannot fully compensate.
- Core assumption: The model can follow instructions but cannot generate knowledge it was not trained on.
- Evidence anchors:
  - [section] "We included the prompt as part of the input for each request to classify a cause of death, as we found that the model would start disregarding the specific instructions of the prompt over time if we did not."
  - [section] "If a user says that the answer is incorrect, the model changes its answer. Hence, we found that a set of very clear and concise instructions on how to behave, in a very neutral tone, works best."
- Break condition: If the prompt becomes too complex or the instructions are not specific enough, the model may ignore them or produce inconsistent outputs.

### Mechanism 3
- Claim: The hierarchical structure of ICD-10 codes allows for partial matches, where the first three characters (block) can be correctly identified even if the subcategory is wrong, reducing the complexity of the task.
- Mechanism: By focusing on the first three characters of the ICD-10 code, the model can achieve partial accuracy. This is because the block level is less specific than the subcategory, making it easier to map terms to the correct block.
- Core assumption: The first three characters of the ICD-10 code are sufficient to narrow down the category of the cause of death.
- Evidence anchors:
  - [section] "The second type of correct classification is called a partial match, and is defined as having the first 3 characters of the modelâ€™s output match the first 3 characters of the ICD-10 code assigned by the domain expert."
- Break condition: If the model cannot correctly identify the block level, the partial match metric becomes meaningless.

## Foundational Learning

- Concept: Understanding the ICD-10 hierarchical structure (chapters, blocks, categories, subcategories, codes).
  - Why needed here: To interpret the partial match metric and understand why some errors are less severe than others.
  - Quick check question: What is the difference between a block and a subcategory in ICD-10?

- Concept: Basics of prompt engineering and how LLM behavior can be influenced by input formatting.
  - Why needed here: To design effective prompts that guide the model to produce accurate and consistent outputs.
  - Quick check question: How does the tone and specificity of a prompt affect LLM output?

- Concept: The limitations of LLMs, including hallucinations and lack of transparency.
  - Why needed here: To interpret the results and understand why some errors are due to model hallucinations rather than knowledge gaps.
  - Quick check question: What is an LLM hallucination, and how can it be detected?

## Architecture Onboarding

- Component map: Data preprocessing -> Prompt generation -> LLM inference -> Post-processing -> Evaluation
- Critical path:
  1. Load and preprocess the HiCaD dataset
  2. Generate prompts for each cause of death
  3. Run the LLM to generate ICD-10 codes
  4. Compare the output to the ground truth (manual coding)
  5. Calculate accuracy metrics (full match, partial match, error rate)
- Design tradeoffs:
  - Using pre-trained LLMs vs. fine-tuning on historical data
  - Focusing on full matches vs. partial matches for evaluation
  - Including error codes in prompts vs. allowing the model to hallucinate
- Failure signatures:
  - High error rate for archaic terms
  - Inconsistent outputs for the same input over multiple runs
  - Ignoring prompt instructions and producing unformatted outputs
- First 3 experiments:
  1. Test the base performance of GPT-3.5, GPT-4, and Llama2 on the HiCaD dataset without any prompt engineering
  2. Implement prompt engineering with clear instructions and error codes, then re-run the models
  3. Compare the performance of the LLMs to traditional machine learning models (RandomForest, SVM) and string similarity methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Would fine-tuning the LLMs on domain-specific historical causes of death data significantly improve their classification accuracy compared to the out-of-the-box models tested?
- Basis in paper: [inferred] The authors suggest fine-tuning as a potential future work approach to improve results, noting that current models were trained as general-purpose chatbots rather than domain experts.
- Why unresolved: The paper only tested pre-trained, general-purpose LLMs without any domain-specific fine-tuning. The authors explicitly state this as a limitation and potential area for improvement.
- What evidence would resolve it: Comparative results showing accuracy rates of fine-tuned LLMs against the current baseline performance (69% for GPT-3.5, 83% for GPT-4, 40% for Llama 2).

### Open Question 2
- Question: How would Retrieval-Augmented Generation (RAG) framework perform compared to the current LLMs and traditional machine learning methods for historical ICD-10 code assignment?
- Basis in paper: [explicit] The authors explicitly suggest RAG as a promising solution, stating it would ground the model on external knowledge bases like the ICD-10h master list of terms and codes.
- Why unresolved: The paper only tested basic LLM prompting without any retrieval-augmented generation framework or external knowledge base integration.
- What evidence would resolve it: Direct comparison of RAG-enhanced LLM performance against the current LLM results and traditional machine learning methods (89% accuracy for SVM).

### Open Question 3
- Question: Would a two-stage classification approach (first identifying the subcategory, then selecting the specific code) improve accuracy for causes of death that currently achieve only partial matches?
- Basis in paper: [inferred] The authors note that many outputs achieved partial matches (correct first 3 characters) and suggest a second-round classification approach similar to Boyle et al.'s "meta-refinement of predicted codes."
- Why unresolved: The paper only performed single-pass classification without any iterative refinement or two-stage approach to improve subcategory selection.
- What evidence would resolve it: Comparative results showing whether the two-stage approach reduces partial match rates and increases full match accuracy compared to the current single-pass results.

## Limitations

- The HiCaD dataset is proprietary and not publicly available, limiting reproducibility and independent verification
- The study only tests three specific LLM models without exploring fine-tuning or alternative architectures
- Performance evaluation focuses on accuracy metrics without considering the clinical or research implications of coding errors

## Confidence

- Medium Confidence: GPT-4 achieves 83% accuracy on historical cause-of-death coding (supported by experimental results but limited by proprietary dataset)
- Low Confidence: LLMs "do not currently perform well enough" for this task (subjective threshold, dependent on application requirements)
- Medium Confidence: LLMs perform better on modern versus archaic terminology (well-supported by data, follows logically from training)

## Next Checks

1. **Dataset Replication Test:** Create a new, publicly available dataset of historical causes of death from a different geographic region or time period (e.g., US death records from 1900-1920) and test whether the observed performance patterns hold across different historical contexts.

2. **Fine-tuning Impact Study:** Implement a controlled experiment where GPT-4 is fine-tuned on a subset of the HiCaD data and compare the performance improvement against the base model, measuring whether the accuracy gap with standard ML techniques can be closed.

3. **Error Analysis Protocol:** Conduct a detailed error analysis categorizing mistakes by type (hallucinations, archaic terms, formatting issues, etc.) and evaluate whether these errors have different clinical or research implications, potentially informing more nuanced evaluation metrics beyond simple accuracy scores.