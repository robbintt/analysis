---
ver: rpa2
title: 'DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted
  Averaging'
arxiv_id: '2402.02622'
source_url: https://arxiv.org/abs/2402.02622
tags:
- denseformer
- transformer
- block
- weights
- self
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: DenseFormer improves transformer language models by adding depth-weighted-average
  (DWA) modules after each block, enabling direct access to outputs from all previous
  layers. This modification allows the model to achieve the same perplexity as much
  deeper standard transformers while being smaller, faster, and more memory-efficient.
---

# DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging

## Quick Facts
- arXiv ID: 2402.02622
- Source URL: https://arxiv.org/abs/2402.02622
- Authors: Matteo Pagliardini; Amirkeivan Mohtashami; Francois Fleuret; Martin Jaggi
- Reference count: 40
- Primary result: DenseFormer matches the perplexity of much deeper standard transformers while being smaller, faster, and more memory-efficient.

## Executive Summary
DenseFormer is a novel transformer architecture that improves information flow by adding depth-weighted-average (DWA) modules after each block. These modules allow each block to directly access outputs from all previous blocks, creating a multi-scale feature flow. The architecture enables models to achieve the same perplexity as much deeper standard transformers while reducing parameter count and improving inference speed. DenseFormer also demonstrates superior data efficiency and robustness to gradient noise.

## Method Summary
DenseFormer enhances standard transformers by inserting DWA modules after each block, which compute a weighted average over current and all past block outputs. The architecture is simple to implement, adding only i+1 learnable weights per layer (negligible compared to total parameters). The method includes sparsification techniques like dilation (connecting every k-th block) and DWA period (applying DWA every p blocks) to trade off performance for computational efficiency. Models are trained using AdamW optimizer with cosine learning rate schedule on OpenWebText2 dataset.

## Key Results
- DenseFormer achieves the same perplexity as much deeper standard transformers while being smaller and faster
- Inference speed improves by 2.7× to 3.4× compared to standard transformers
- Learned DWA weights exhibit structured patterns, showing strong reuse of activations from distant layers
- DenseFormer shows superior data efficiency and robustness to gradient noise

## Why This Works (Mechanism)

### Mechanism 1
Direct access to all previous layer outputs improves information reuse and alleviates diminishing returns from depth. DenseFormer inserts DWA modules that compute a weighted average over current and all past block outputs, bypassing the need to propagate early representations through many layers. Core assumption: Information from early layers is valuable for later predictions and would be lost or degraded if passed through many intermediate transformations. Evidence: DenseFormer achieves the same perplexity as much deeper standard transformers while being smaller and faster.

### Mechanism 2
Learned DWA weights exhibit structured patterns that facilitate targeted reuse of features from specific depths. The DWA module learns weights that assign high values to the diagonal (current block), immediate neighbors, the input embedding, and shows an aggregation pattern in later layers. Core assumption: The optimal information flow requires not just any access to past layers but a structured reuse pattern that the model learns during training. Evidence: High weights are on the diagonal and immediate previous blocks, high weights are given to initial embedding vectors, and an aggregation block is observed near final layers.

### Mechanism 3
Dilation and DWA periodicity allow trading off between performance and computational efficiency without major degradation. By sparsifying the DWA weight matrix (only connecting every k-th block and only adding DWA after every p blocks), the model reduces FLOPs and KV cache size while preserving most information flow benefits. Core assumption: Not all pairwise inter-block connections are necessary for performance; a structured subset suffices. Evidence: Small values of dilation factor (up to 4) have negligible effect on perplexity, and increasing DWA period provides similar trade-offs.

## Foundational Learning

- Concept: Skip connections and residual flows in deep networks
  - Why needed here: DenseFormer builds on the intuition that skip connections help information flow, but extends it by connecting every block to all previous ones
  - Quick check question: In a standard transformer with N layers, how many skip connections are there? (Answer: N-1; each layer connects to the next.)

- Concept: Attention mechanisms and KV cache
  - Why needed here: Understanding that storing past representations is necessary for both backpropagation and auto-regressive decoding explains why DWA has negligible memory overhead
  - Quick check question: During inference, what is stored in the KV cache for a transformer? (Answer: The key and value vectors for all previous tokens.)

- Concept: Sparsity patterns and their impact on model efficiency
  - Why needed here: Dilation and DWA periodicity are sparsity patterns that reduce computation while preserving performance; understanding this helps tune DenseFormer variants
  - Quick check question: What is the effect of increasing dilation k on the number of parameters in a DWA module at layer i? (Answer: It reduces the number of weights from i+1 to roughly (i+1)/k.)

## Architecture Onboarding

- Component map: Input embedding → Block 1 → DWA1 → Block 2 → DWA2 → ... → Block d → DWA_d → final LayerNorm and LM head
- Critical path: Token embedding → Block 1 → DWA1 → Block 2 → DWA2 → ... → Block d → DWA_d → final LayerNorm and LM head
- Design tradeoffs:
  - Performance vs. speed: Larger dilation or DWA period improves inference/training speed but may slightly hurt perplexity
  - Memory vs. accuracy: DenseFormer has negligible extra memory overhead (uses already stored activations), but very deep models may still hit KV cache limits
  - Complexity vs. benefits: Adding DWA is simple and modular; can be combined with other transformer optimizations (e.g., linear attention, RMSNorm)
- Failure signatures:
  - Perplexity does not improve over baseline: Check if DWA weights are initialized correctly (identity initialization) and if training is long enough for patterns to emerge
  - Training instability or divergence: Verify that the DWA modules are not introducing exploding gradients; monitor weight magnitudes
  - Speed does not improve with dilation: Ensure implementation is optimized; check if dilation factor is too small relative to model depth
- First 3 experiments:
  1. Train a 48-block DenseFormer (1x1) and a 48-block standard transformer on OpenWebText2; compare final perplexity and inference speed
  2. Train a 48-block 4x5-DenseFormer; verify that it matches the perplexity of a much deeper (e.g., 72-block) transformer while being faster at inference
  3. Ablation: Train a 48-block model with DWA weights dropped after training (sparsified); observe the impact on perplexity to confirm small weights matter

## Open Questions the Paper Calls Out

### Open Question 1
What are the optimal sparsity patterns for DWA weights beyond dilation and periodicity, and how do they impact performance? Basis: The paper explores dilation and periodicity as sparsity patterns, but mentions the possibility of finding more efficient patterns, especially given the visualization in Figure 5 suggesting such patterns might exist. Why unresolved: The paper only scratches the surface of alternative sparsity patterns and does not provide a comprehensive analysis of their impact on performance and efficiency. What evidence would resolve it: Experiments comparing various sparsity patterns (e.g., based on layer importance, information flow) against the current dilation and periodicity approach, measuring both perplexity and computational overhead.

### Open Question 2
How does DenseFormer's performance and robustness change with varying sequence lengths and batch sizes? Basis: The paper mentions experiments with longer sequences (512 tokens) and a smaller batch size (128), but does not provide a comprehensive analysis of how these factors affect DenseFormer's performance and robustness compared to standard Transformers. Why unresolved: The paper only provides limited evidence for specific sequence lengths and batch sizes, leaving the generalizability of DenseFormer's benefits unclear. What evidence would resolve it: Extensive experiments varying sequence lengths and batch sizes, comparing DenseFormer's perplexity, training efficiency, and robustness to gradient noise against standard Transformers across a wide range of configurations.

### Open Question 3
How does DenseFormer interact with other architectural modifications, such as those aimed at improving attention efficiency or memory usage? Basis: The paper mentions that DenseFormer can be readily used together with existing proposals to alleviate computational challenges of self-attention or reduce redundant operations, but does not provide experimental evidence for such combinations. Why unresolved: The paper focuses on DenseFormer as a standalone modification and does not explore its potential synergies or conflicts with other architectural improvements. What evidence would resolve it: Experiments combining DenseFormer with various attention efficiency techniques (e.g., linear approximations, kernel methods) and memory optimization strategies, evaluating their impact on performance, speed, and memory usage.

## Limitations

- The core mechanism relies on the assumption that early-layer information remains useful for later predictions, but limited ablation studies exist on pruning DWA connections to very early layers
- Learned weight patterns are only shown qualitatively; quantitative analysis of their stability across training runs or sensitivity to initialization is absent
- While memory overhead is claimed to be negligible, no direct measurements or analysis of KV cache scaling with depth are provided for DenseFormer versus standard transformers

## Confidence

- **High confidence**: DenseFormer reduces perplexity compared to standard transformers of the same depth, and the modular DWA architecture is simple to implement and integrate
- **Medium confidence**: DenseFormer matches the perplexity of much deeper standard transformers while being faster at inference; learned DWA weight patterns are structured and meaningful
- **Low confidence**: DenseFormer is robust to gradient noise and more data-efficient in all settings; the exact impact of dilation and DWA period on the Pareto frontier is fully characterized

## Next Checks

1. **Ablation on early-layer connections**: Train DenseFormer variants with DWA weights to early layers (e.g., first 2-4 blocks) zeroed out or gradually pruned; measure the impact on perplexity and inspect if learned patterns change
2. **KV cache and memory profiling**: Instrument training and inference to measure actual memory usage (especially KV cache) for DenseFormer versus standard transformers at varying depths; confirm the "negligible overhead" claim quantitatively
3. **Cross-dataset robustness**: Evaluate DenseFormer on multiple language modeling datasets (e.g., WikiText-103, C4) and a non-language task (e.g., CIFAR-10 with a vision transformer) to test if the benefits and learned patterns generalize beyond OpenWebText2