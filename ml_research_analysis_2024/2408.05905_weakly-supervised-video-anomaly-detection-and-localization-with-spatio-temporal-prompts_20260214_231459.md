---
ver: rpa2
title: Weakly Supervised Video Anomaly Detection and Localization with Spatio-Temporal
  Prompts
arxiv_id: '2408.05905'
source_url: https://arxiv.org/abs/2408.05905
tags:
- anomaly
- detection
- video
- spatial
- conference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes STPrompt, a novel method that leverages pre-trained
  vision-language models for weakly supervised video anomaly detection and localization
  (WSVADL). The core idea is to decompose the WSVADL task into two sub-tasks: temporal
  anomaly detection and spatial anomaly localization.'
---

# Weakly Supervised Video Anomaly Detection and Localization with Spatio-Temporal Prompts

## Quick Facts
- arXiv ID: 2408.05905
- Source URL: https://arxiv.org/abs/2408.05905
- Authors: Peng Wu; Xuerong Zhou; Guansong Pang; Zhiwei Yang; Qingsen Yan; Peng Wang; Yanning Zhang
- Reference count: 40
- Primary result: Achieves state-of-the-art performance on WSVADL with 88.08% AUC on UCF-Crime and 23.90% TIoU on spatial localization

## Executive Summary
This paper introduces STPrompt, a novel method for weakly supervised video anomaly detection and localization (WSVADL) that leverages pre-trained vision-language models. The approach decomposes the task into temporal anomaly detection and spatial anomaly localization sub-tasks, using spatial attention aggregation and temporal adapters for detection, and LLM-generated text prompts for localization. The method achieves state-of-the-art performance across three public benchmarks while requiring only video-level labels, avoiding the need for detailed spatio-temporal annotations.

## Method Summary
STPrompt uses a two-stream architecture: temporal anomaly detection and spatial anomaly localization. For temporal detection, it employs a spatial attention aggregation (SA2) mechanism that filters background noise by focusing on motion-based foreground patches, combined with a temporal adapter that adds contextual modeling to CLIP features. For spatial localization, it uses a training-free approach with LLM-generated text prompts (normal background descriptions and abnormal event augmentations) to retrieve anomalous patches via CLIP's image-to-text retrieval capability. The model is trained using multiple instance learning with video-level labels only.

## Key Results
- Achieves 88.08% AUC on UCF-Crime for temporal anomaly detection
- Achieves 23.90% TIoU on UCF-Crime for spatial anomaly localization
- Outperforms existing methods on all three benchmarks (UCF-Crime, ShanghaiTech, UBnormal)
- Demonstrates effective fine-grained spatial localization without requiring detailed annotations

## Why This Works (Mechanism)

### Mechanism 1
The spatial attention aggregation (SA2) module improves anomaly detection by focusing on motion-based foreground patches, filtering out background noise. SA2 computes frame differences to derive motion magnitude, selects patches with highest motion, and aggregates them via attention-weighted average. This highlights regions where anomalous events occur, assuming anomalous events are typically associated with foreground motion that differs from the background.

### Mechanism 2
The temporal adapter adds contextual temporal modeling to CLIP's static frame features, enabling frame-level anomaly detection that captures event evolution. A transformer-based adapter with relative distance-based attention computes temporal relationships between frames without positional encoding, producing enriched features for anomaly classification. This addresses CLIP's lack of temporal modeling, assuming temporal context is critical for anomaly detection.

### Mechanism 3
LLM-generated text prompts provide semantically rich, domain-specific descriptions for spatial anomaly localization without requiring detailed annotations or training. The LLM is queried to generate normal background descriptions and abnormal event augmentations, then used as queries to retrieve anomalous patches from CLIP embeddings. This assumes LLMs can generate descriptive prompts that accurately capture normal/abnormal semantics in video frames.

## Foundational Learning

- **Vision-Language Models (VLMs)**: Map images and text into shared embedding space for cross-modal retrieval. Why needed: Enables localization by retrieving patches based on text descriptions without training a detector. Quick check: How does CLIP's image-to-text retrieval differ from text-to-image retrieval in terms of use cases?

- **Multiple Instance Learning (MIL)**: Treats a video as a bag of frames and uses weak video-level labels to infer frame-level predictions. Why needed: The temporal detection branch uses MIL to infer frame-level anomaly scores from video-level labels. Quick check: What is the key difference between standard classification and MIL in terms of label granularity?

- **Attention mechanisms**: Compute weighted combinations of features to focus on relevant parts of input. Why needed: SA2 uses attention to aggregate motion-based patches, and the temporal adapter uses self-attention to model frame relationships. Quick check: How does relative distance-based attention differ from standard self-attention?

## Architecture Onboarding

- **Component map**: CLIP image encoder → frame-level features → SA2 aggregation → temporal adapter → dual-branch detection → frame-level scores; CLIP features → patch extraction → LLM prompts → patch retrieval → localization heatmap
- **Critical path**: CLIP features → SA2 aggregation → temporal adapter → dual-branch detection → frame-level scores; CLIP features → patch extraction → LLM prompts → patch retrieval → localization heatmap
- **Design tradeoffs**: Frozen CLIP vs. fine-tuning (faster, no training vs. limited adaptation); SA2 motion-based selection vs. full patch MIL (lower compute vs. may miss static anomalies); LLM prompts vs. learned prompts (no training overhead vs. depends on LLM quality)
- **Failure signatures**: Low temporal adapter impact (anomalies are instantaneous or context not useful); SA2 produces noisy patches (motion-based selection picks irrelevant motion); patch retrieval fails (LLM prompts misaligned with content or CLIP embeddings don't capture required semantics)
- **First 3 experiments**:
  1. Baseline: Run STPrompt without SA2 or temporal adapter; compare AUC to full model
  2. SA2 ablation: Replace motion-based selection with random patch selection or full patch MIL; measure compute vs. accuracy tradeoff
  3. Prompt quality test: Manually inspect LLM-generated normal/abnormal prompts on small video sample; verify semantic alignment

## Open Questions the Paper Calls Out
None explicitly identified in the paper.

## Limitations
- Performance depends heavily on LLM prompt quality and semantic alignment with actual video content
- Motion-based SA2 may miss static anomalies (hidden objects, subtle changes)
- Claims about relative distance-based attention superiority lack direct empirical validation

## Confidence
- High confidence: Temporal detection mechanism shows consistent performance improvements
- Medium confidence: Spatial localization results depend on LLM quality, not systematically evaluated
- Medium confidence: Relative distance attention claim lacks direct validation

## Next Checks
1. Conduct controlled experiment varying LLM prompts on subset of videos to measure localization sensitivity to prompt quality
2. Compare temporal adapter performance using standard self-attention vs. relative distance attention on UCF-Crime
3. Test SA2's motion-based patch selection on videos with static anomalies to verify it doesn't miss non-motion-based anomalies