---
ver: rpa2
title: 'Beyond Under-Alignment: Atomic Preference Enhanced Factuality Tuning for Large
  Language Models'
arxiv_id: '2406.12416'
source_url: https://arxiv.org/abs/2406.12416
tags:
- uni00000013
- uni00000011
- uni00000003
- uni00000048
- preference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of factual hallucination in large
  language models (LLMs) by exploring preference learning for factuality tuning. The
  authors find that existing methods fail to improve factuality on out-of-domain (OOD)
  queries due to under-alignment rather than over-alignment.
---

# Beyond Under-Alignment: Atomic Preference Enhanced Factuality Tuning for Large Language Models

## Quick Facts
- arXiv ID: 2406.12416
- Source URL: https://arxiv.org/abs/2406.12416
- Reference count: 32
- This paper addresses the problem of factual hallucination in large language models (LLMs) by exploring preference learning for factuality tuning, proposing APEFT to construct atomic preferences at the granularity of individual facts.

## Executive Summary
This paper addresses the problem of factual hallucination in large language models by exploring preference learning for factuality tuning. The authors find that existing methods fail to improve factuality on out-of-domain (OOD) queries due to under-alignment rather than over-alignment. They propose APEFT, a framework that constructs atomic preferences at the granularity of individual facts to enhance model awareness of factuality. Experiments show that APEFT improves model performance by an average of 3.45% on both in-domain and OOD datasets.

## Method Summary
The method constructs a preference dataset through biography generation tasks, then extracts atomic facts from preferred and dispreferred responses. Knowledge detection via stochastic sampling identifies "potentially-known" atomic facts where the model is uncertain. These atomic preferences are mixed with general preferences and used to fine-tune models using various preference learning algorithms (DPO, IPO, KTO, CPO, RSO). The approach is evaluated on in-domain (Bio) and out-of-domain (FAVA, FPQA, KUQA) datasets using FActScore and accuracy metrics.

## Key Results
- APEFT improves model performance by an average of 3.45% on both in-domain and out-of-domain datasets
- Models exhibit under-alignment on out-of-domain queries, with minimal token distribution shifts despite performance degradation
- Mixing atomic preferences with general preferences yields consistent improvements across all tested learning algorithms
- Scaling up preference pair quantity beyond optimal points can lead to decreased performance on some OOD tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preference learning using paragraph-level factuality judgments leads to under-alignment on out-of-domain queries because the model cannot localize which specific fact is wrong.
- Mechanism: General preferences contain too much information without specifying which fact is incorrect, so the model receives vague signals and changes token distributions minimally on OOD data.
- Core assumption: The preference signal must be at the granularity of individual facts to enable effective fine-tuning; paragraph-level feedback is too coarse.
- Evidence anchors:
  - [abstract]: "we posit that under-alignment may result from the original factual preference feedback, which is predominantly based on paragraphs and does not adequately inform the model about the factuality of individual facts."
  - [section 4.1]: "we posit that the original factual preference feedback, which is mainly based on paragraphs, is too coarse-grained and insufficient to convey the factuality of individual facts."

### Mechanism 2
- Claim: Knowledge detection through stochastic sampling can identify "potentially-known" atomic facts where the model is uncertain, making these the optimal targets for preference optimization.
- Mechanism: By sampling multiple responses and measuring the percentage of correct responses (r), the method selects atomic facts with 0 < r < 1 as those where the model has partial knowledge and can benefit most from targeted correction.
- Core assumption: The model's stochastic behavior reveals its internal knowledge state, and facts where it is uncertain (but not completely wrong) are the best candidates for fine-tuning.
- Evidence anchors:
  - [section 5.1]: "We assess the extent to which model knows the knowledge contained in each atomic fact... With the percentage of the correct responses after the sampling denoted as r, the unknown, potentially-known and known atomic facts are defined as when (r = 0), (0 < r < 1) and ( r = 1 ) respectively."

### Mechanism 3
- Claim: Mixing atomic preferences with general preferences during training preserves the broader context while adding fine-grained factuality signals, leading to better generalization.
- Mechanism: The training objective combines both preference types (Dg âˆª Da), allowing the model to learn both the overall structure of factual responses and the specific facts within them.
- Core assumption: Both coarse-grained and fine-grained preference signals are necessary; removing either would degrade performance.
- Evidence anchors:
  - [section 5.1]: "The atomic preferences are mixed together with the general preferences to enhance the factuality of LLMs."
  - [section 5.2]: "Add atomic preferences to the original general preferences gain performance increase among almost all the learning algorithms."

## Foundational Learning

- Concept: Preference learning and reward modeling
  - Why needed here: The entire approach relies on optimizing model behavior based on human preference judgments rather than explicit labels
  - Quick check question: What is the key difference between DPO and IPO in how they use preference pairs?

- Concept: Token distribution analysis for behavior change measurement
  - Why needed here: The paper uses this technique to distinguish under-alignment from over-alignment by measuring how much the model's predicted token distributions change
  - Quick check question: How is the "shifted token" defined in the token distribution analysis?

- Concept: Factuality evaluation metrics (FActScore)
  - Why needed here: The method needs to quantify how factual model responses are, both at the atomic fact level and in longer responses
  - Quick check question: What does FActScore measure in generated responses?

## Architecture Onboarding

- Component map:
  Preference dataset construction -> Atomic fact extraction -> Knowledge detection -> Preference mixing -> Training loop -> Evaluation framework

- Critical path:
  1. Generate preference pairs from biography task
  2. Extract atomic facts from preferred/dispreferred responses
  3. Detect knowledge state of each atomic fact via sampling
  4. Select potentially-known facts and create atomic preferences
  5. Mix atomic and general preferences
  6. Train using preference learning algorithm
  7. Evaluate on ID and OOD datasets

- Design tradeoffs:
  - Granularity vs. context: Atomic preferences provide fine signals but lose broader context; mixing with general preferences addresses this
  - Knowledge detection cost: Stochastic sampling adds computational overhead but enables targeted optimization
  - Preference quality vs. quantity: The paper shows that more preferences or higher quality differences don't necessarily improve performance

- Failure signatures:
  - Under-alignment: Minimal token distribution shift on OOD data, performance similar to baseline
  - Over-alignment: Significant token distribution shift on OOD data but performance degradation
  - Knowledge detection failure: r values don't correlate with actual improvement potential

- First 3 experiments:
  1. Replicate token distribution analysis to verify under-alignment finding on a new model/preference algorithm
  2. Test knowledge detection by comparing r values against actual improvement from atomic preference training
  3. Validate mixing strategy by training with only atomic preferences vs. only general preferences vs. mixed

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does APEFT's improvement in factuality on OOD datasets come at the cost of other capabilities like mathematical reasoning, code completion, or safety preservation?
- Basis in paper: [inferred] The paper mentions that APEFT improves factuality but acknowledges this naturally raises concerns about whether the same process could adversely affect other capabilities that LLMs have already developed.
- Why unresolved: The paper explicitly states it does not investigate changes in various capabilities during the fine-tuning process.
- What evidence would resolve it: Systematic evaluation of LLMs' performance across multiple capability domains (math, coding, safety, etc.) before and after APEFT fine-tuning.

### Open Question 2
- Question: Are there specific internal regions or mechanisms within LLMs that focus on factuality, and do the primary parameter changes during APEFT occur in these areas?
- Basis in paper: [inferred] The paper discusses under-alignment and over-alignment but does not delve into the internals of LLMs or whether there are specific regions focusing on factuality.
- Why unresolved: The paper explicitly states it does not explore the internals of LLMs and leaves this for future work.
- What evidence would resolve it: Neuroimaging-style analysis of LLM parameter changes during fine-tuning, or mechanistic interpretability studies identifying factuality-specific circuits.

### Open Question 3
- Question: Why does scaling up the number of training preference pairs beyond a certain threshold lead to decreased performance on some OOD tasks?
- Basis in paper: [explicit] The paper's data quantity experiments show that scaling is ineffective when training models for factuality, with performance declining on KUQA as preference pair count increases.
- Why unresolved: The paper observes this phenomenon but does not provide a theoretical explanation for why more data beyond a certain point becomes counterproductive.
- What evidence would resolve it: Theoretical analysis of model capacity saturation, or experiments varying model size to determine if the optimal preference pair count scales with model parameters.

## Limitations
- The knowledge detection mechanism relies on stochastic sampling without empirical validation of the correlation between r values and actual improvement potential
- The computational overhead of generating atomic preferences and performing knowledge detection is not quantified or discussed
- The paper does not explore whether different mixing ratios or alternative combination strategies might yield better results

## Confidence

**Confidence: Medium** The core finding that under-alignment (rather than over-alignment) is the primary obstacle to OOD generalization represents a significant theoretical contribution. However, the evidence is primarily based on token distribution analysis from a single experimental setup, which may not generalize to other preference learning scenarios or model architectures.

**Confidence: Low** The knowledge detection mechanism relies on stochastic sampling to identify "potentially-known" atomic facts. While the authors present this as a principled approach, the correlation between sampling results (r values) and actual model improvement potential is not empirically validated. The stochastic nature of sampling introduces variability that could affect the reproducibility and reliability of this component.

**Confidence: Medium** The mixing strategy for combining atomic and general preferences shows consistent improvements, but the ablation studies are limited. The paper does not explore whether different mixing ratios or alternative combination methods might yield better results. Additionally, the computational overhead of generating atomic preferences and performing knowledge detection is not quantified or discussed.

## Next Checks

1. **Knowledge Detection Validation**: Conduct controlled experiments to verify the correlation between r values from stochastic sampling and actual improvement potential. This could involve: (a) selecting atomic facts with varying r values, (b) creating separate preference datasets for each group, (c) measuring performance improvements after fine-tuning, and (d) comparing results against the predicted knowledge state.

2. **Token Distribution Analysis Replication**: Replicate the token distribution analysis on a different model architecture and preference learning algorithm to verify that the under-alignment phenomenon is not specific to LLaMA models or the particular algorithms tested. This would strengthen the generalizability of the theoretical contribution.

3. **Mixing Strategy Optimization**: Systematically explore the mixing ratio of atomic to general preferences (e.g., 1:3, 1:1, 3:1) and alternative combination strategies (e.g., sequential training, curriculum learning) to determine if the current equal mixing approach is optimal or if there are more effective ways to combine the two preference types.