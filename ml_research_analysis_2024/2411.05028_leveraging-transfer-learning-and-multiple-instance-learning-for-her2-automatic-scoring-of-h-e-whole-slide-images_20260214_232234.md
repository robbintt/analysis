---
ver: rpa2
title: Leveraging Transfer Learning and Multiple Instance Learning for HER2 Automatic
  Scoring of H\&E Whole Slide Images
arxiv_id: '2411.05028'
source_url: https://arxiv.org/abs/2411.05028
tags:
- her2
- learning
- images
- were
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work explores the use of transfer learning and multiple instance\
  \ learning for automatic HER2 scoring from H&E whole slide images. The key contribution\
  \ is a multiple instance learning framework that leverages pre-trained patch-embedding\
  \ models from different source domains\u2014non-medical images, H&E images of other\
  \ tissues, and IHC images of breast tissue\u2014to classify HER2 scores without\
  \ requiring pixel-level annotations."
---

# Leveraging Transfer Learning and Multiple Instance Learning for HER2 Automatic Scoring of H&E Whole Slide Images

## Quick Facts
- arXiv ID: 2411.05028
- Source URL: https://arxiv.org/abs/2411.05028
- Reference count: 1
- Pre-training on H&E images (PatchCamelyon dataset) yields best performance with average AUC-ROC of 0.622

## Executive Summary
This work presents a multiple instance learning framework for automatic HER2 scoring from H&E whole slide images without requiring pixel-level annotations. The approach leverages pre-trained patch-embedding models from different source domains—non-medical images, H&E images of other tissues, and IHC images of breast tissue—to classify HER2 scores. Experiments demonstrate that pre-training on H&E images (PatchCamelyon dataset) significantly outperforms models pre-trained on IHC and ImageNet, achieving an average AUC-ROC of 0.622 across four HER2 classes. The framework also generates attention-weighted heatmaps that visually highlight HER2-positive regions, providing interpretability alongside classification.

## Method Summary
The method uses a multiple instance learning framework with pre-trained AlexNet models as patch-embedders. H&E whole slide images are processed to extract patches at 0.5µm resolution, which are then embedded using frozen pre-trained models. An attention mechanism aggregates patch embeddings into bag-level representations, which are classified into HER2 scores (0-3). The study compares three pre-training strategies: ImageNet (non-medical), PatchCamelyon (H&E), and IHC HER2 datasets, using 5-fold cross-validation to evaluate performance.

## Key Results
- H&E pre-trained model achieves average AUC-ROC of 0.622 across four HER2 classes
- IHC pre-trained model performs worse with average AUC-ROC of 0.511
- ImageNet pre-trained model performs worst with average AUC-ROC of 0.454
- Attention mechanism generates interpretable heatmaps highlighting HER2-positive regions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Transfer learning from similar staining domains (H&E to H&E) is more effective than from dissimilar domains (ImageNet to H&E).
- Mechanism: Pre-trained models on similar histology data (PCAM) capture domain-specific features like tissue architecture and staining patterns that generalize better to the target HER2 classification task.
- Core assumption: The learned filters from PCAM capture biologically relevant texture and structural features present in HER2-positive and negative regions.
- Evidence anchors:
  - [abstract] "embedding models pre-trained on H&E images consistently outperformed the others, resulting in an average AUC-ROC value of 0.622"
  - [section] "Pre-trained models on similar histology data (PCAM) capture domain-specific features like tissue architecture and staining patterns that generalize better"
  - [corpus] Weak or missing - no corpus papers directly test H&E-to-H&E transfer learning specifically
- Break condition: If the source domain images are too different in staining protocols, resolution, or tissue types, the transferred features may become irrelevant or noisy.

### Mechanism 2
- Claim: MIL with attention pooling effectively handles weakly-labeled WSI data by aggregating patch-level evidence into slide-level predictions.
- Mechanism: Attention weights dynamically weight patch contributions based on their relevance to the HER2 score, allowing the model to focus on informative regions while ignoring background.
- Core assumption: Positive bags contain at least one positive patch, and attention can learn to identify these patches without pixel-level labels.
- Evidence anchors:
  - [abstract] "using multiple-instance learning with an attention layer not only allows for good classification results to be achieved, but it can also help with producing visual indication of HER2-positive areas"
  - [section] "attention weights were utilised to infer the contribution of each patch in the bag offered to the model for it to predict a specific HER2 score"
  - [corpus] Weak - corpus papers don't detail MIL+attention mechanisms specifically for HER2 scoring
- Break condition: If bags contain too few positive patches or patches are too small relative to positive regions, attention may fail to identify meaningful evidence.

### Mechanism 3
- Claim: Freezing pre-trained patch-embedding weights during MIL training preserves useful prior knowledge while allowing attention and classifier layers to adapt.
- Mechanism: The convolutional layers extract high-level features from patches, while the attention mechanism learns which patches matter most for slide-level classification.
- Core assumption: The pre-trained features remain useful across domains and don't need fine-tuning for this specific task.
- Evidence anchors:
  - [section] "The weights of the patch-embedding models remained frozen during the training of the MIL model"
  - [section] "The choice to freeze the patch-embedding model weights was made to avoid complexity of training and to explore the effect of the un-modified prior knowledge"
  - [corpus] Missing - corpus doesn't discuss freezing vs. fine-tuning strategies
- Break condition: If the source and target domains are too dissimilar, frozen features may limit model performance and fine-tuning would be necessary.

## Foundational Learning

- Concept: Multiple Instance Learning (MIL)
  - Why needed here: WSIs are too large to label at pixel level, so we only have slide-level HER2 scores
  - Quick check question: In MIL, what assumption is made about positive bags?

- Concept: Transfer Learning
  - Why needed here: Limited annotated HER2 data requires leveraging knowledge from other datasets
  - Quick check question: What's the difference between feature extraction and fine-tuning in transfer learning?

- Concept: Attention Mechanisms
  - Why needed here: Need to identify which patches contribute most to HER2 classification for interpretability
  - Quick check question: How does the attention weight for patch k get computed in this framework?

## Architecture Onboarding

- Component map: WSI -> HSV thresholding -> patches -> augmentation -> pre-trained AlexNet (frozen) -> embeddings -> attention layer -> bag pooling -> classification layer -> HER2 score
- Critical path: WSI → patches → embeddings → attention pooling → classification → slide score
- Design tradeoffs:
  - Frozen embeddings: simpler, faster training but less adaptation
  - Attention vs. mean/max pooling: attention provides interpretability but adds complexity
  - Patch size/number: larger patches capture more context but fewer per bag
- Failure signatures:
  - Low AUC-ROC with all models: poor patch extraction or inappropriate pre-training
  - High training accuracy but poor test: overfitting, try data augmentation or regularization
  - Attention weights all similar: model not learning which patches matter
- First 3 experiments:
  1. Train MIL model with no pre-training (random weights) to establish baseline
  2. Train with ImageNet-pretrained embeddings to test transfer from non-medical domain
  3. Train with PCAM-pretrained embeddings to test transfer from similar medical domain

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific aspects of H&E pre-training (versus IHC or ImageNet) contribute most to superior HER2 scoring performance?
- Basis in paper: [explicit] The paper states that "pre-training on H&E images (PatchCamelyon dataset) yields the best performance" but does not analyze which specific features or representations learned from H&E data drive this improvement.
- Why unresolved: The study compares overall performance but doesn't perform ablation studies or feature analysis to identify what aspects of H&E pre-training are most beneficial.
- What evidence would resolve it: Detailed feature visualization, ablation studies removing specific layers or features, or controlled experiments isolating particular characteristics of H&E images that transfer well.

### Open Question 2
- Question: How would fine-tuning the pre-trained patch-embedding models affect classification performance compared to keeping weights frozen?
- Basis in paper: [explicit] The authors state "The choice to freeze the patch-embedding model weights was made to avoid complexity of training and to explore the effect of the un-modified prior knowledge" but did not explore fine-tuning.
- Why unresolved: The paper deliberately chose not to fine-tune to simplify experiments, leaving open whether allowing adaptation of the embedding models would improve results.
- What evidence would resolve it: Experiments comparing frozen versus fine-tuned embedding models with identical architectures and training procedures.

### Open Question 3
- Question: What is the optimal number of patches per bag for the MIL framework, and how does this affect both classification accuracy and computational efficiency?
- Basis in paper: [explicit] The authors used "bags of 100 patches" but state this was a design choice without exploring the effect of different bag sizes.
- Why unresolved: The paper does not report experiments varying the number of patches per bag or analyze the trade-offs between performance and computational cost.
- What evidence would resolve it: Systematic experiments testing different bag sizes (e.g., 50, 100, 200, 500 patches) with corresponding performance metrics and computational benchmarks.

## Limitations
- Results based on single HER2 dataset without external validation, limiting generalizability
- Patch extraction method using HSV thresholding is described but not fully specified, affecting reproducibility
- Comparison with other pre-training domains limited to three options, leaving questions about other potential source domains

## Confidence
- **High Confidence**: The superiority of H&E pre-training over non-medical domains (ImageNet) is well-supported by experimental results (AUC-ROC 0.622 vs 0.454)
- **Medium Confidence**: The effectiveness of MIL with attention mechanisms for WSI classification is demonstrated, but the specific implementation details are not fully disclosed
- **Medium Confidence**: The claim that freezing pre-trained weights is optimal requires further validation, as fine-tuning strategies are not explored

## Next Checks
1. Test the trained model on an independent HER2 dataset from a different institution to assess generalizability
2. Compare frozen vs. fine-tuned pre-trained weights to determine if allowing adaptation improves performance
3. Evaluate how sensitive results are to different patch extraction parameters and thresholds to ensure robustness