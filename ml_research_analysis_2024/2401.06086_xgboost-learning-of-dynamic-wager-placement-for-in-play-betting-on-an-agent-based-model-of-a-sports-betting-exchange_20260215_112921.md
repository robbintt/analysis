---
ver: rpa2
title: XGBoost Learning of Dynamic Wager Placement for In-Play Betting on an Agent-Based
  Model of a Sports Betting Exchange
arxiv_id: '2401.06086'
source_url: https://arxiv.org/abs/2401.06086
tags: []
core_contribution: This paper presents initial results of applying the XGBoost machine
  learning method to learn profitable in-play betting strategies on a simulated sports
  betting exchange. The Bristol Betting Exchange (BBE) agent-based model simulates
  horse races and in-play betting markets.
---

# XGBoost Learning of Dynamic Wager Placement for In-Play Betting on an Agent-Based Model of a Sports Betting Exchange

## Quick Facts
- arXiv ID: 2401.06086
- Source URL: https://arxiv.org/abs/2401.06086
- Authors: Chawin Terawong; Dave Cliff
- Reference count: 11
- XGBoost agent learns profitable in-play betting strategies in simulated sports betting exchange

## Executive Summary
This paper presents initial results of applying the XGBoost machine learning method to learn profitable in-play betting strategies on a simulated sports betting exchange. The Bristol Betting Exchange (BBE) agent-based model simulates horse races and in-play betting markets. XGBoost is trained on data from profitable bets made by various simple betting agents in the BBE, then tested in the BBE with its learned strategy. Results show the XGBoost agent can learn strategies more profitable than those used to train it, demonstrating the potential of using ML to discover adaptive betting strategies. The complete BBE code with XGBoost integration is open-sourced.

## Method Summary
The method trains an XGBoost model to predict profitable betting actions (back/lay) using market state and race state features extracted from the BBE simulation. The BBE generates race data and bettor agent interactions, which are preprocessed to extract the top 20% most significant transactions. XGBoost is trained using the Scikit-learn API with GridSearchCV for hyperparameter optimization. The trained model is then integrated into BBE as a new betting agent and tested against other bettor types to validate its profitability using statistical hypothesis tests.

## Key Results
- XGBoost agent outperforms all individual bettor agents used for training in the BBE simulation
- Learned strategies generalize to new race scenarios and bettor populations
- XGBoost discovers non-linear interactions between race features that individual agents don't explicitly follow

## Why This Works (Mechanism)

### Mechanism 1
- Claim: XGBoost learns profitable betting strategies by extracting decision rules from profitable bettor behavior in the BBE simulation.
- Mechanism: XGBoost builds an ensemble of decision trees using gradient boosting to iteratively reduce prediction error on labeled betting actions (back/lay) from profitable agents. Each tree splits on features like distance, rank, time, and stake to form betting rules.
- Core assumption: Profitable betting actions in simulation are sufficiently complex to require tree-based learning rather than simpler rule extraction.
- Evidence anchors:
  - [abstract] "XGBoost trained in this way can indeed learn profitable betting strategies"
  - [section] "XGBoost is trained on the profitable betting activity of a population of minimally simple betting strategies"
- Break condition: If betting actions are too simple or highly correlated with a single feature, XGBoost's complexity offers no advantage over simpler models.

### Mechanism 2
- Claim: The BBE agent-based model generates realistic in-play betting market dynamics that serve as training data for XGBoost.
- Mechanism: BBE simulates horse races and bettor agents with diverse strategies (ZI, LW, BTF, LinEx, Underdog, Privileged) interacting in a matching engine. The resulting time series of market states and competitor positions provide labeled examples of profitable actions.
- Core assumption: The simulated race dynamics and bettor interactions closely enough resemble real-world in-play betting markets to transfer learned strategies.
- Evidence anchors:
  - [section] "BBE isn't designed to perfectly mimic a real track horse-race, but rather to generate convincing data resembling real race dynamics"
  - [section] "BBE's matching-engine is designed to implement exactly the same processes as are used in real-world betting exchanges"
- Break condition: If the simulation oversimplifies or omits key market features, the learned strategies may fail in real-world deployment.

### Mechanism 3
- Claim: XGBoost generalizes beyond the training data to discover strategies more profitable than any individual bettor agent used for training.
- Mechanism: XGBoost combines decision trees that capture non-linear interactions between race state features and betting outcomes. The ensemble learns composite rules that individual agents don't follow explicitly.
- Core assumption: The profitable betting actions in training data contain patterns that, when combined, produce strategies superior to any single source agent.
- Evidence anchors:
  - [abstract] "XGBoost generalises to learn strategies that outperform each of the set of strategies used for creation of the training data"
  - [section] "XGBoost generalises over the training data sufficiently well that the profitability of XGBoost-enabled bettor-agents can eventually be better than those of the non-learning bettors"
- Break condition: If training data is too homogeneous or limited in profitable patterns, generalization may not exceed individual agent performance.

## Foundational Learning

- Concept: Gradient boosting and decision trees
  - Why needed here: XGBoost uses gradient boosting to build an ensemble of decision trees that capture complex, non-linear relationships between race features and profitable betting actions
  - Quick check question: What is the primary difference between XGBoost and a single decision tree model?

- Concept: Feature engineering for betting data
  - Why needed here: The model uses engineered features like distance, rank, time, and stake to represent the state of the race and market for each betting decision
  - Quick check question: Which three features were identified as most important by the XGBoost F-score in this work?

- Concept: Cross-validation and hyperparameter tuning
  - Why needed here: GridSearchCV with k-fold cross-validation optimizes hyperparameters like learning rate, max depth, and subsample ratio to improve model generalization
  - Quick check question: What evaluation metric was used to optimize the XGBoost model during training?

## Architecture Onboarding

- Component map:
  - BBE simulator -> Data preprocessor -> XGBoost trainer -> XGBoost agent -> Experiment runner

- Critical path:
  1. Run 1000 races in BBE to collect raw data
  2. Preprocess data to extract profitable betting actions
  3. Train XGBoost model with hyperparameter optimization
  4. Integrate trained model into BBE as new betting agent
  5. Run test scenarios and validate profit performance

- Design tradeoffs:
  - Model complexity vs. interpretability: XGBoost provides better performance but less transparent decision rules than simple strategies
  - Simulation realism vs. computational cost: BBE balances realistic dynamics with manageable simulation time
  - Feature selection vs. overfitting: Using 4 key features reduces overfitting risk but may miss important signals

- Failure signatures:
  - XGBoost performance not exceeding random betting: suggests poor training data quality or insufficient model capacity
  - High variance in test results across scenarios: indicates overfitting to specific race conditions
  - Model consistently preferring one action type: suggests class imbalance or feature bias in training data

- First 3 experiments:
  1. Train XGBoost on 1000 races with 110 mixed-strategy bettors, validate on held-out races
  2. Integrate trained model into BBE and compare profit against all original bettor types in Scenario 1
  3. Test XGBoost agent in Scenario 2 with equal numbers of each bettor type to assess robustness to population changes

## Open Questions the Paper Calls Out

- How would the XGBoost agent's performance compare to other ML algorithms (e.g., Random Forest, Neural Networks) on the same betting data?
- How would the XGBoost agent perform when trained on real-world betting exchange data instead of synthetic BBE data?
- How would the XGBoost agent's performance change if it incorporated additional features beyond distance, time, rank, and stake?
- How would the XGBoost agent perform if it used online learning to continuously update its strategy during live betting sessions?

## Limitations
- BBE simulation may not fully capture real-world betting market complexity and noise
- Training data limited to 1,000 races, which may constrain model generalization
- Comparison against limited set of simple betting strategies doesn't establish superiority over more sophisticated approaches

## Confidence
- **High confidence**: The XGBoost model can learn profitable strategies from training data and outperform individual bettor agents in the BBE simulation
- **Medium confidence**: The learned strategies generalize to new race scenarios and bettor populations in the BBE
- **Low confidence**: The strategies would translate to profitable real-world in-play betting markets

## Next Checks
1. Integrate the XGBoost model into a live betting exchange API to test performance against actual market data and competition
2. Run the model across 10,000+ races with varied bettor populations and track distributions to assess robustness and identify failure modes
3. Compare XGBoost performance against other gradient boosting frameworks (LightGBM, CatBoost) and neural network approaches to isolate the contribution of the XGBoost-specific architecture