---
ver: rpa2
title: '[RE] Modeling Personalized Item Frequency Information for Next-basket Recommendation'
arxiv_id: '2402.17925'
source_url: https://arxiv.org/abs/2402.17925
tags:
- datasets
- basket
- user
- tifu-knn
- items
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reproduced and extended the TIFU-KNN model for next-basket
  recommendation, which leverages personalized item frequency (PIF) information. The
  reproduction confirmed that TIFU-KNN outperforms the baseline Personal Top Frequency
  method on multiple datasets.
---

# [RE] Modeling Personalized Item Frequency Information for Next-basket Recommendation

## Quick Facts
- arXiv ID: 2402.17925
- Source URL: https://arxiv.org/abs/2402.17925
- Authors: Sławomir Garcarz; Avik Pal; Pim Praat
- Reference count: 14
- Primary result: Reproduction confirmed TIFU-KNN outperforms Personal Top Frequency baseline; β-VAE architecture improves prediction of unseen items

## Executive Summary
This paper reproduces and extends the TIFU-KNN model for next-basket recommendation by leveraging personalized item frequency (PIF) information. The study confirms that TIFU-KNN outperforms the baseline Personal Top Frequency method across multiple datasets. Additionally, the authors introduce a β-VAE architecture that models NBR by learning dense latent representations of sparse user vectors and incorporating collaborative patterns via k-nearest neighbors. Fairness analysis reveals that model performance varies with user characteristics such as basket size, item popularity, and novelty, with β-VAE showing better capability in predicting unseen items.

## Method Summary
The paper implements two main approaches for next-basket recommendation. TIFU-KNN uses hierarchical time-decayed weights to prioritize recent purchases and combines a user's own PIF vector with the average vector of k nearest neighbors to form predictions. The β-VAE model learns dense latent representations of sparse user vectors through an MLP encoder/decoder architecture with 128-dimensional latent space. Both models use the same datasets filtered to include users with at least 3 baskets and items purchased by at least 5-40 users depending on dataset size. Hyperparameter tuning for TIFU-KNN was performed using Optuna, while the β-VAE used a fixed β=4 for KL-divergence weighting.

## Key Results
- TIFU-KNN consistently outperforms Personal Top Frequency baseline across Recall@10, NDCG@10, MRR, and PHR@10 metrics
- β-VAE model demonstrates superior performance in predicting unseen items compared to TIFU-KNN
- Fairness analysis shows performance variations based on user characteristics: larger basket sizes generally improve performance, while higher novelty (more unseen items) decreases it
- Tmall and Taobao datasets show particularly low performance scores (Recall 0.0013-0.0085), suggesting scalability limitations with very sparse data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: TIFU-KNN outperforms Personal Top Frequency by combining repeated and collaborative purchase patterns.
- Mechanism: The model uses hierarchical time-decayed weights to prioritize recent purchases and aggregates the user's own vector with the average vector of k nearest neighbors to form predictions.
- Core assumption: Recent purchase patterns are more relevant than older ones, and users with similar PIF vectors have similar future purchase behavior.
- Evidence anchors:
  - [abstract]: "The experimental results confirmed that the reproduced model, TIFU-KNN, outperforms the baseline model, Personal Top Frequency, on various datasets and metrics."
  - [section]: "TIFU-KNN introduces a novel approach to address the limitations of the Personalized Item Frequency (PIF) measure...The model achieves this by employing hierarchical time-decayed weights."
- Break condition: If user preferences change rapidly or if the k-nearest neighbors are not truly similar in purchasing behavior, the model's predictions could degrade.

### Mechanism 2
- Claim: Incorporating k-nearest neighbors in the latent space improves NBR performance compared to using only individual user representations.
- Mechanism: The β-VAE model learns dense latent representations of sparse user vectors, and the model's performance improves when aggregating the target user's dense vector with the mean of its k nearest neighbors.
- Core assumption: Collaborative patterns in the latent space capture meaningful relationships between users that can improve predictions.
- Evidence anchors:
  - [abstract]: "The β-VAE model demonstrated better capability in predicting unseen items."
  - [section]: "We hypothesize that collaborative purchase patterns are important for NBR and the second form of implementation would outperform the first."
- Break condition: If the latent space does not capture meaningful relationships, or if the k-nearest neighbors are not truly similar, the aggregation may not improve performance.

### Mechanism 3
- Claim: The performance of NBR models varies with user characteristics such as basket size, item popularity, and novelty.
- Mechanism: The fairness analysis shows that model performance is dependent on these user characteristics, with some datasets showing better performance for users with larger baskets or more popular items, and poorer performance for users with more unseen items in the test basket.
- Core assumption: These user characteristics are valid proxies for fairness and can reveal biases in the model's performance.
- Evidence anchors:
  - [section]: "We conducted a thorough examination of fairness by considering user characteristics such as average basket size, item popularity, and novelty."
  - [section]: "The results partially confirmed our hypotheses, indicating that performance varied based on these user characteristics."
- Break condition: If these user characteristics do not accurately reflect fairness concerns, or if the model's performance is not actually biased by these factors, the fairness analysis may not be meaningful.

## Foundational Learning

- Concept: Personalized Item Frequency (PIF)
  - Why needed here: PIF is the core feature used by both TIFU-KNN and β-VAE models to represent user purchase history.
  - Quick check question: What is the difference between PIF and traditional item frequency?

- Concept: Hierarchical time-decayed weights
  - Why needed here: This is the mechanism used by TIFU-KNN to prioritize recent purchases in the PIF vector.
  - Quick check question: How does the hierarchical structure of time-decayed weights work?

- Concept: Variational Autoencoder (VAE)
  - Why needed here: β-VAE is the deep learning architecture used to learn dense latent representations of user vectors.
  - Quick check question: What is the difference between a VAE and a traditional autoencoder?

## Architecture Onboarding

- Component map: Data preprocessing -> TIFU-KNN model (PIF computation, similarity calculation, neighbor aggregation) -> β-VAE model (encoding, neighbor aggregation, decoding) -> Evaluation (metrics calculation, fairness analysis)
- Critical path: 1. Load and preprocess datasets 2. Train TIFU-KNN model and evaluate performance 3. Train β-VAE model and evaluate performance 4. Conduct fairness analysis
- Design tradeoffs:
  - TIFU-KNN: Simpler model with interpretable features vs. potential limitations in capturing complex patterns
  - β-VAE: More complex model with potential for better representation learning vs. increased computational cost and risk of overfitting
- Failure signatures:
  - Poor performance on all datasets: Likely issues with data preprocessing or hyperparameter tuning
  - Inconsistent performance across datasets: Potential dataset-specific biases or limitations in the model
  - Fairness analysis revealing significant biases: Need for further investigation and potential model improvements
- First 3 experiments:
  1. Reproduce TIFU-KNN results on the original datasets to verify the implementation
  2. Evaluate TIFU-KNN performance on new datasets to assess generalizability
  3. Train and evaluate β-VAE model to compare with TIFU-KNN and explore potential improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the β-VAE model scale with the number of items in the dataset?
- Basis in paper: [inferred] The paper notes that the β-VAE model performs poorly on Tmall and Taobao datasets which have 24k and 32k items respectively, and reasons this is due to the inability of the deep model to generate a logit vector with large dimensions.
- Why unresolved: The paper only provides preliminary results and leaves further investigation of the β-VAE architecture for future work. It does not explore how the model's performance changes with varying numbers of items.
- What evidence would resolve it: Experimental results comparing the β-VAE model's performance on datasets with different numbers of items, while keeping other factors constant.

### Open Question 2
- Question: How does the choice of β in the β-VAE model affect the trade-off between reconstruction accuracy and disentanglement in the latent space for the NBR task?
- Basis in paper: [explicit] The paper mentions that the Lagrangian multiplier β acts as a disentanglement factor in the latent space, but does not explore how varying β affects the model's performance.
- Why unresolved: The paper only uses a fixed β of 4 and does not investigate the impact of different β values on the model's ability to capture purchase patterns and make accurate predictions.
- What evidence would resolve it: Experimental results showing the β-VAE model's performance on various NBR metrics with different β values, and an analysis of how β affects the quality of the latent representations.

### Open Question 3
- Question: How do the user characteristics (basket size, item popularity, and novelty) interact to influence the performance of the TIFU-KNN model?
- Basis in paper: [explicit] The paper conducts a fairness analysis considering these three user characteristics separately, but does not explore their interactions.
- Why unresolved: The paper only examines the effect of each user characteristic on the model's performance individually, without considering how they might jointly impact the results.
- What evidence would resolve it: A comprehensive analysis that considers the combined effect of user characteristics on the TIFU-KNN model's performance, potentially using techniques like clustering or dimensionality reduction to identify user segments with different performance profiles.

## Limitations

- The β-VAE model shows promise in predicting unseen items, but lacks direct comparison with other deep learning approaches for next-basket recommendation, making it difficult to assess whether the improvements are due to the VAE architecture or the k-NN aggregation strategy
- The fairness analysis reveals performance variations based on user characteristics, but the causality between these characteristics and model performance remains unclear, and the analysis does not address potential systemic biases in the data itself
- The reproduction of TIFU-KNN results shows consistent performance improvements over baseline methods, but the extent of improvement varies significantly across datasets, with particularly low Recall scores on Tmall (0.0013-0.0085) suggesting potential scalability limitations with very sparse datasets

## Confidence

- **High confidence**: The core finding that TIFU-KNN outperforms Personal Top Frequency is well-supported by experimental results across multiple datasets and metrics
- **Medium confidence**: The effectiveness of k-NN aggregation in latent space for β-VAE model is supported by comparisons but lacks ablation studies to isolate the contribution of different components
- **Low confidence**: The fairness implications of performance variations are suggestive but not conclusive, as the analysis does not account for potential confounding factors in the data

## Next Checks

1. **Ablation study for β-VAE**: Conduct experiments removing the k-NN aggregation to isolate whether improvements come from the VAE architecture itself or the collaborative component
2. **Scalability testing**: Evaluate TIFU-KNN performance on synthetic datasets with controlled sparsity levels to determine the model's limitations with sparse user-item interactions
3. **Bias investigation**: Perform additional analysis to determine whether performance variations in fairness metrics are due to model bias or inherent data imbalances in the original datasets