---
ver: rpa2
title: Understanding and Addressing the Under-Translation Problem from the Perspective
  of Decoding Objective
arxiv_id: '2405.18922'
source_url: https://arxiv.org/abs/2405.18922
tags:
- translation
- under-translation
- penalty
- words
- sentences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper conducts an in-depth analysis of the under-translation
  problem in NMT from the perspective of the decoding objective. The authors argue
  that to maximize the length-normalized log-probability, NMT models have a strong
  incentive to ignore high-entropy words that have low translation probabilities,
  leading to under-translation.
---

# Understanding and Addressing the Under-Translation Problem from the Perspective of Decoding Objective

## Quick Facts
- **arXiv ID**: 2405.18922
- **Source URL**: https://arxiv.org/abs/2405.18922
- **Reference count**: 40
- **Primary result**: Proposes EOS penalty enhancement to reduce under-translation by detecting candidates with high risk based on EOS probability

## Executive Summary
This paper presents a comprehensive analysis of under-translation in neural machine translation from the perspective of the decoding objective. The authors demonstrate that beam search optimization, when combined with length normalization, creates incentives for models to drop high-entropy words that have low translation probabilities. They show that while the EOS probability provides a mild penalty for under-translation, this penalty is often outweighed by the benefits of omitting uncertain words. The proposed solution enhances the EOS penalty specifically for candidates at high risk of under-translation, using EOS probability as a detector. Experiments on both synthetic and real-world data demonstrate the method's ability to reduce under-translation while maintaining overall translation quality.

## Method Summary
The authors propose a method to address under-translation by enhancing the EOS penalty during beam search decoding. The approach detects under-translated candidates using EOS probability as an indicator, then applies a scaled penalty based on the translation length. The penalty is calculated as α×(1+β)^max(0,L-l), where L is the maximum length limit, l is the actual translation length, and β is the scaling factor. The method is applied to a fine-tuned LLaMA2-7B model on both synthetic datasets (with controlled word entropy) and real-world Chinese-English translation tasks. The synthetic data construction involves creating sentence pairs with three words representing low, mid, and high entropy, while real-world evaluation uses the WMT22 Chinese-English test set.

## Key Results
- EOS probability decreases when under-translation occurs, serving as a mild penalty but often insufficient to prevent word omission
- The proposed EOS penalty enhancement can accurately detect under-translated outputs while minimizing impact on correct translations
- Experiments show BLEU improvement from 15.25 to 15.36 on WMT22 Zh-En when applied to Transformer-big architecture
- The method effectively reduces under-translation on synthetic data while maintaining translation quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Under-translation occurs because the decoding objective encourages dropping high-entropy words to maximize length-normalized log-probability.
- Mechanism: The beam search optimization prioritizes candidates with higher average log-probabilities per token. High-entropy words have low translation probabilities, making their inclusion detrimental to the normalized score. The model therefore has an incentive to omit these words, leading to under-translation.
- Core assumption: The length penalty α in the decoding objective is fixed and does not dynamically adjust for sentence complexity.
- Evidence anchors:
  - [abstract] "To optimize the beam search objective, the model tends to overlook words it is less confident about, leading to the under-translation phenomenon."
  - [section 3.1] "Maximizing the length-normalized log-probability implies that the output words should have the highest log-probabilities in the average sense. This inherently encourages the model to disregard words that fall below average, which contradicts the intention of producing a comprehensive translation."

### Mechanism 2
- Claim: The EOS probability acts as a penalty for under-translation, but this penalty is often outweighed by the benefits of dropping multiple high-entropy words.
- Mechanism: When under-translation occurs, the model's confidence in predicting EOS diminishes, indicating an incomplete translation. However, the decrease in EOS probability is not sufficient to outweigh the benefits of dropping high-entropy words in terms of the decoding objective.
- Core assumption: The relationship between EOS probability and the number of under-translated words is not linear.
- Evidence anchors:
  - [abstract] "Correspondingly, the model's confidence in predicting the End Of Sentence (EOS) diminishes when under-translation occurs, serving as a mild penalty for under-translated candidates."
  - [section 3.2] "Figure 2 shows that although the EOS log-probability and the number of under-translated words generally display a positive correlation, the rate of change does not reach linearity."

### Mechanism 3
- Claim: Enhancing the EOS penalty for candidates with a high risk of under-translation can effectively prevent under-translation.
- Mechanism: By detecting candidates with a high risk of under-translation based on the EOS probability and scaling the EOS penalty according to the translation length, the method can selectively penalize under-translated candidates without significantly impacting correct translations.
- Core assumption: The EOS probability is a reliable indicator of the risk of under-translation.
- Evidence anchors:
  - [abstract] "Building upon this analysis, we propose employing the confidence of predicting EOS as a detector for under-translation, and strengthening the confidence-based penalty to penalize candidates with a high risk of under-translation."
  - [section 3.3] "Since the model's confidence in predicting EOS diminishes when under-translation occurs, we employ the prediction probability of EOS as a detector for under-translation."

## Foundational Learning

- **Concept**: Beam search decoding with length normalization
  - Why needed here: Understanding the beam search decoding objective and the role of length normalization is crucial for grasping the mechanism behind under-translation and the proposed solution.
  - Quick check question: What is the primary objective of beam search decoding in NMT, and how does length normalization affect the selection of candidates?

- **Concept**: Translation entropy and its impact on translation quality
  - Why needed here: Recognizing the relationship between translation entropy and the likelihood of under-translation helps explain why high-entropy words are more prone to being dropped during decoding.
  - Quick check question: How does the translation entropy of a word influence its probability of being included in the final translation?

- **Concept**: End-of-sentence (EOS) probability and its role in translation completion
  - Why needed here: Understanding how the EOS probability reflects the model's confidence in the completeness of a translation is essential for the proposed method of detecting and penalizing under-translation.
  - Quick check question: What does a low EOS probability indicate about the quality of a translation, and how can it be used to identify under-translated candidates?

## Architecture Onboarding

- **Component map**: Synthetic data generator -> NMT model -> EOS penalty module -> Beam search decoder
- **Critical path**: Synthetic data generation → NMT model training → EOS penalty application during decoding → Evaluation of under-translation reduction
- **Design tradeoffs**: The EOS penalty method introduces a trade-off between reducing under-translation and potentially increasing over-translation. The hyperparameters τ and β control the balance between precision and recall in detecting and correcting under-translations.
- **Failure signatures**: If the EOS penalty is too aggressive (high β or low τ), it may lead to over-translation or hallucination. If it is too conservative, under-translation may persist.
- **First 3 experiments**:
  1. Evaluate the impact of different EOS penalty hyperparameters (τ and β) on the reduction of under-translation while monitoring the increase in over-translation.
  2. Compare the EOS penalty method with other existing penalties (length penalty and coverage penalty) in terms of their effectiveness in reducing under-translation.
  3. Assess the method's performance on real-world translation datasets (e.g., WMT22 Zh-En) and analyze the types of under-translation errors that are most effectively addressed.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed EOS penalty method perform on other NMT architectures beyond the fine-tuned LLaMA2 model?
- Basis in paper: [explicit] The authors mention that their method is also applicable to the traditional encoder-decoder Transformer architecture, showing an improvement in BLEU score from 15.25 to 15.36 on the WMT22 Zh-En dataset when applied to the Transformer-big model.
- Why unresolved: The paper only provides a brief comparison on one specific Transformer model and dataset. A more comprehensive evaluation across different architectures and language pairs would be needed to fully assess the method's generalizability.
- What evidence would resolve it: Experiments on a diverse set of NMT models (e.g., different Transformer variants, RNN-based models) and multiple language pairs, comparing the EOS penalty's effectiveness and impact on translation quality across these settings.

### Open Question 2
- Question: What is the optimal way to construct synthetic data that fully simulates real translation scenarios, including reordering and dependencies between target words?
- Basis in paper: [inferred] The authors acknowledge that their synthetic data construction only translates source words sequentially, neglecting potential reordering and dependencies between target words. They suggest that a more sophisticated construction of synthetic data may help further reveal the characteristics of under-translation and design better decoding objectives.
- Why unresolved: Creating realistic synthetic data that captures the complexities of real translation is a challenging task. The authors have identified a limitation in their current approach but have not provided a concrete solution or alternative method for generating more realistic synthetic data.
- What evidence would resolve it: A novel method for constructing synthetic translation data that incorporates reordering, dependencies, and other linguistic phenomena observed in real translation scenarios. Experiments demonstrating the effectiveness of this approach in exposing and addressing under-translation compared to the current method.

### Open Question 3
- Question: How can the proposed EOS penalty method be extended to handle under-translation occurring at the beginning and middle stages of translation?
- Basis in paper: [explicit] The authors acknowledge that their method has a weaker ability to handle under-translation at the beginning and middle stages. They explain that since their method modifies the final scores and rankings of beam search candidates but cannot influence the early decoding process, they are unable to resolve these under-translation errors by modifying scores of final candidates.
- Why unresolved: The paper does not provide a clear solution for extending the EOS penalty method to address under-translation at the beginning and middle stages. The authors only mention that overcoming this limitation may be possible by identifying additional signals strongly associated with under-translation during decoding and designing corresponding penalties.
- What evidence would resolve it: A novel approach that integrates the EOS penalty method with early decoding signals or alternative strategies to detect and address under-translation occurring at the beginning and middle stages of translation. Experiments demonstrating the effectiveness of this extended method in resolving under-translation across all stages compared to the original EOS penalty approach.

## Limitations

- The analysis relies heavily on synthetic data experiments that may not fully capture real-world translation complexities
- The method assumes EOS probability is a reliable indicator of under-translation risk, but this relationship may vary across language pairs and domains
- The approach introduces a new hyperparameter (τ) that requires careful tuning, and the fixed scaling factor (β) may not be optimal for all translation scenarios

## Confidence

### High Confidence Claims
- The mechanism by which beam search optimization encourages dropping high-entropy words to maximize length-normalized log-probability is well-supported by both theoretical analysis and empirical evidence.
- The observation that EOS probability serves as a mild penalty for under-translation is consistently demonstrated across experiments.
- The synthetic data experiments clearly illustrate the fundamental problem of under-translation in NMT decoding.

### Medium Confidence Claims
- The effectiveness of EOS penalty enhancement for preventing under-translation on real-world datasets is supported by experimental results but may not generalize to all language pairs and domains.
- The assumption that the relationship between EOS probability and under-translation risk is sufficiently reliable for detection purposes appears reasonable but needs more extensive validation.

### Low Confidence Claims
- The claim that the method "can accurately detect and rectify under-translated outputs with minimal impact on other correct translations" may be overstated, as the experiments show a trade-off between reducing under-translation and increasing over-translation.

## Next Checks

1. **Cross-linguistic validation**: Test the EOS penalty method on multiple language pairs beyond Chinese-English to verify its generalizability across different linguistic structures and translation challenges.

2. **Long sentence analysis**: Evaluate the method's performance on sentences longer than 20 words (the current L limit) to determine if the EOS penalty remains effective and whether new failure modes emerge.

3. **Ablation study on penalty parameters**: Conduct a systematic ablation study varying τ, β, and L across a wider range to map the complete trade-off space between under-translation reduction, over-translation increase, and overall translation quality (BLEU).