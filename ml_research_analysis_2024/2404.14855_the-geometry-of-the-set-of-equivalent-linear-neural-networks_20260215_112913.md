---
ver: rpa2
title: The Geometry of the Set of Equivalent Linear Neural Networks
arxiv_id: '2404.14855'
source_url: https://arxiv.org/abs/2404.14855
tags: []
core_contribution: This paper studies the geometry of linear neural networks, specifically
  characterizing the set of all weight vectors that compute the same linear transformation
  W. The authors introduce the concept of a "fiber" - the set of weight vectors that
  produce W - and show it is an algebraic variety, not necessarily a manifold.
---

# The Geometry of the Set of Equivalent Linear Neural Networks

## Quick Facts
- arXiv ID: 2404.14855
- Source URL: https://arxiv.org/abs/2404.14855
- Reference count: 26
- Key outcome: Characterizes the set of weight vectors that compute the same linear transformation W for linear neural networks using rank stratification and fiber geometry

## Executive Summary
This paper provides a comprehensive geometric analysis of linear neural networks, focusing on the set of weight vectors that compute the same linear transformation W. The authors introduce the concept of a "fiber" - the set of all weight vectors producing W - and show it is an algebraic variety that can be partitioned into smooth manifolds called strata using rank stratification. Each stratum represents a different pattern of information flow through the network. The paper develops theoretical tools including canonical weight vectors, linear transformations between strata, and "moves" that transform one weight vector to another on the same fiber. These contributions provide both theoretical insight into the geometry of linear networks and practical applications for improving neural network training.

## Method Summary
The authors characterize the fiber of a linear neural network by partitioning it into smooth manifolds (strata) based on rank lists that describe how information flows through each layer. They construct canonical weight vectors for each stratum and develop linear transformations to map between strata with the same rank list. The paper introduces moves - transformations in weight space that preserve the linear transformation - including one-matrix and two-matrix moves that provide insight into the fiber's geometry. Explicit formulas are derived for tangent spaces, normal spaces, and dimensions of each stratum, with the key theoretical result that each stratum's topology depends solely on its rank list.

## Key Results
- The fiber is an algebraic variety that can be partitioned into smooth strata using rank stratification
- Each stratum's topology depends solely on its rank list, and its geometry up to linear transformation
- Moves provide a way to transform weight vectors on the same fiber, offering practical applications for avoiding spurious critical points
- Explicit formulas are provided for tangent spaces, normal spaces, and dimension counts of strata

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The fiber of a linear neural network weight vector set that computes the same transformation W is an algebraic variety, not necessarily a manifold, and can be partitioned into smooth strata.
- Mechanism: The fiber is the set of all weight vectors that produce the same linear transformation under matrix multiplication. By using rank stratification based on the rank list of each subsequence matrix, the fiber can be partitioned into disjoint smooth manifolds called strata.
- Core assumption: The rank list of a weight vector determines its stratum uniquely and the frontier condition holds (if a stratum intersects the closure of another, it is a subset of the closure).
- Evidence anchors:
  - [abstract] "The fiber is an algebraic variety that is not necessarily a manifold. We describe a natural way to stratify the fiberâ€”that is, to partition the algebraic variety into a finite set of manifolds of varying dimensions called strata."
  - [section] "Each stratum represents a different pattern by which information flows (or fails to flow) through the neural network."
  - [corpus] FMR=0.37, related papers found; evidence suggests community interest in linear network geometry.
- Break condition: If the rank stratification does not satisfy the frontier condition, strata may not be well-behaved manifolds.

### Mechanism 2
- Claim: Each stratum's topology and geometry are determined solely by its rank list and a linear transformation in weight space.
- Mechanism: By constructing canonical weight vectors for each rank list and using linear transformations to map between strata with the same rank list, the topology and geometry of each stratum can be fully characterized.
- Core assumption: The canonical weight vector construction and linear transformation mapping are valid for all strata.
- Evidence anchors:
  - [abstract] "The topology of a stratum depends solely on its basis flow diagram. So does its geometry, up to a linear transformation in weight space."
  - [section] "There is a bijection from valid rank lists to valid multisets of intervals: if we are given a rank list, we can easily determine the interval multiplicities, and if we are given a list of interval multiplicities, we can easily determine the ranks."
  - [corpus] FMR=0.37, related papers found; evidence suggests community interest in linear network geometry.
- Break condition: If the canonical weight vector construction is invalid or the linear transformation mapping fails, the topology and geometry characterization breaks.

### Mechanism 3
- Claim: Moves (transformations) on the fiber can be used to understand the geometry and topology of the fiber and strata.
- Mechanism: One-matrix and two-matrix moves can transform one weight vector to another on the same fiber, providing insight into the fiber's geometry and practical applications for avoiding spurious critical points during training.
- Core assumption: Moves preserve the linear transformation computed by the network and can be used to characterize the fiber's geometry.
- Evidence anchors:
  - [abstract] "We define transformations in weight space called moves, which map one weight vector to another on the same fiber, thereby modifying the neural network's weights without changing the linear transformation that the network computes."
  - [section] "The practical motivation is that although two different neural networks might compute the same transformation, one might be much more amenable to training than the other."
  - [corpus] FMR=0.37, related papers found; evidence suggests community interest in linear network geometry.
- Break condition: If moves do not preserve the linear transformation or fail to characterize the fiber's geometry, the mechanism breaks.

## Foundational Learning

- Concept: Rank stratification of linear neural networks
  - Why needed here: Understanding how the fiber of a linear neural network can be partitioned into smooth strata based on rank lists is crucial for characterizing the geometry and topology of the fiber.
  - Quick check question: Can you explain how the rank list of a weight vector determines its stratum in the fiber?

- Concept: Canonical weight vectors and linear transformations
  - Why needed here: Constructing canonical weight vectors for each rank list and using linear transformations to map between strata with the same rank list is essential for characterizing the topology and geometry of each stratum.
  - Quick check question: Can you describe how to construct a canonical weight vector for a given rank list and how to use it to map between strata?

- Concept: Moves on the fiber
  - Why needed here: Understanding how one-matrix and two-matrix moves can transform weight vectors on the same fiber is crucial for gaining insight into the fiber's geometry and practical applications for avoiding spurious critical points during training.
  - Quick check question: Can you explain the difference between one-matrix and two-matrix moves and how they preserve the linear transformation computed by the network?

## Architecture Onboarding

- Component map: Rank stratification -> Canonical weight vectors -> Linear transformations -> Moves on the fiber
- Critical path:
  1. Understand rank stratification and its properties
  2. Learn how to construct canonical weight vectors and use linear transformations
  3. Study moves on the fiber and their applications
  4. Apply these concepts to characterize the geometry and topology of the fiber
- Design tradeoffs:
  - Rank stratification provides a natural way to partition the fiber, but may not always satisfy the frontier condition
  - Canonical weight vectors and linear transformations provide a way to characterize strata, but may be computationally expensive
  - Moves on the fiber provide insight into the fiber's geometry, but may not always preserve the linear transformation
- Failure signatures:
  - Rank stratification fails to satisfy the frontier condition, leading to ill-behaved strata
  - Canonical weight vector construction or linear transformation mapping fails, leading to incorrect topology and geometry characterization
  - Moves on the fiber fail to preserve the linear transformation or characterize the fiber's geometry
- First 3 experiments:
  1. Implement rank stratification for a simple linear neural network and verify that it satisfies the frontier condition
  2. Construct canonical weight vectors for different rank lists and use linear transformations to map between strata
  3. Study the effects of one-matrix and two-matrix moves on the fiber and characterize the fiber's geometry using these moves

## Open Questions the Paper Calls Out
None

## Limitations
- The algebraic variety characterization assumes perfect numerical precision, while practical implementations face floating-point errors
- The rank stratification approach requires that all weight vectors can be classified by their rank lists, which may not hold for certain pathological network architectures
- While the paper provides explicit formulas for tangent and normal spaces, the computational complexity of these calculations for large networks remains unexplored

## Confidence
- High confidence: The algebraic variety characterization of the fiber and the basic rank stratification framework
- Medium confidence: The explicit formulas for tangent spaces, normal spaces, and dimension counts, which require careful implementation
- Medium confidence: The characterization of stratum connectivity through moves, though practical implications need further validation

## Next Checks
1. Implement rank stratification for networks with L=3 and varying layer sizes, verifying the frontier condition holds and strata dimensions match theoretical predictions
2. Test the one-matrix and two-matrix moves on specific weight vectors to confirm they preserve the linear transformation and correctly navigate between strata
3. Create a small-scale experiment comparing gradient descent trajectories on different weight vectors within the same stratum to validate claims about spurious critical points