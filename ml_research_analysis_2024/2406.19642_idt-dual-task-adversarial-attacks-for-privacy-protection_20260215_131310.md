---
ver: rpa2
title: 'IDT: Dual-Task Adversarial Attacks for Privacy Protection'
arxiv_id: '2406.19642'
source_url: https://arxiv.org/abs/2406.19642
tags: []
core_contribution: This paper addresses the problem of protecting sensitive attributes
  (e.g., author gender, location) in text while maintaining utility (e.g., sentiment
  classification). It proposes IDT, a dual-task adversarial attack method that uses
  interpretable models to identify and modify words important for privacy while preserving
  those important for utility.
---

# IDT: Dual-Task Adversarial Attacks for Privacy Protection

## Quick Facts
- arXiv ID: 2406.19642
- Source URL: https://arxiv.org/abs/2406.19642
- Authors: Pedro Faustini; Shakila Mahjabin Tonni; Annabelle McIver; Qiongkai Xu; Mark Dras
- Reference count: 11
- Primary result: Outperforms existing methods in deceiving classifiers about sensitive attributes while retaining utility

## Executive Summary
This paper introduces IDT, a dual-task adversarial attack method for protecting sensitive attributes in text while maintaining utility for other classification tasks. The method uses interpretable models to identify which tokens are important for privacy versus utility, then modifies only the privacy-important tokens while preserving utility-important ones. Through extensive experiments on multiple datasets, IDT demonstrates superior performance in achieving privacy protection without sacrificing utility, validated through both automatic and human evaluations.

## Method Summary
IDT is a dual-task adversarial attack method that protects sensitive attributes in text while maintaining utility for other tasks. It works by training auxiliary models to mimic target models, using interpretable methods to score token importance for privacy and utility tasks, then modifying only tokens that are important for privacy but not utility. The method frames privacy-utility rewriting as an optimization problem rather than using generative approaches, avoiding issues like mode collapse. IDT transfers adversarial examples from auxiliary to target models, reducing query costs while maintaining effectiveness. The approach is evaluated on multiple datasets including TrustPilot, TOEFL11, and Shakespeare, with extensive automatic and human evaluations of attack success and text quality.

## Key Results
- IDT achieves significantly higher Attack Success rates than back-translation and TextAttack baselines across all tested datasets
- Utility Retention remains consistently high (>90%) even when achieving strong privacy protection
- Human evaluations confirm that adversarial examples maintain high text quality while successfully deceiving privacy classifiers
- The method reduces computational cost by transferring adversarial examples from auxiliary to target models rather than querying the target multiple times

## Why This Works (Mechanism)

### Mechanism 1
- Claim: IDT modifies tokens important for privacy while preserving those important for utility, thereby achieving dual-task adversarial success.
- Mechanism: IDT uses auxiliary interpretable models to score each token's influence on privacy and utility tasks. It selects tokens high in privacy importance but low in utility importance for modification, then replaces them with similar tokens from the vocabulary.
- Core assumption: The interpretable model's token importance scores accurately reflect each token's contribution to the respective classification tasks.
- Evidence anchors:
  - [abstract]: "IDT, a method that analyses predictions made by auxiliary and interpretable models to identify which tokens are important to change for the privacy task, and which ones should be kept for the utility task."
  - [section]: "IDT modifies important tokens for the privacy task, whilst keeping important ones for the utility task. We take inspiration from membership inference attacks... and train shadow (auxiliary) models, whose goal is to mimic the target models."
  - [corpus]: Weak - only 5/8 related papers have strong privacy-inference themes; others are unrelated or weak matches.
- Break condition: If interpretable model scores are inaccurate or do not generalize to target model, utility retention and privacy deception will fail.

### Mechanism 2
- Claim: Using adversarial attack optimization instead of generative methods avoids mode collapse and produces more reliable text rewrites.
- Mechanism: IDT frames privacy-utility rewriting as an optimization problem where the goal is to change privacy predictions without changing utility predictions, using interpretable model signals to guide token selection.
- Core assumption: Optimization-based adversarial methods are more stable than generative approaches for this dual-task scenario.
- Evidence anchors:
  - [abstract]: "However, these [generative approaches] often create extensively different texts from the original ones or face problems such as mode collapse."
  - [section]: "In contrast to these, we use an optimisation-based adversarial attack approach, that can be expected to produce rewritten texts in a more reliable fashion."
  - [corpus]: Weak - related papers focus on adversarial attacks but not dual-task privacy preservation specifically.
- Break condition: If optimization landscape is too complex or constraints too tight, method may fail to find valid adversarial examples.

### Mechanism 3
- Claim: Transferring adversarial examples from auxiliary to target models maintains attack effectiveness while reducing query costs.
- Mechanism: IDT generates adversarial examples using auxiliary models, then tests them against the target model, avoiding the need to query the target model multiple times during generation.
- Core assumption: Adversarial examples are transferable between models with similar architectures and training data distributions.
- Evidence anchors:
  - [section]: "IDT creates them offline with auxiliary models. Thus, it only queries the target once per sentence."
  - [section]: "We emphasise that any interpretable method which assigns scores to tokens can be adopted."
  - [corpus]: Weak - transferability is mentioned in one related paper but not in dual-task context.
- Break condition: If auxiliary and target models differ significantly in architecture or training data, transferability fails and attack success drops.

## Foundational Learning

- Concept: Adversarial attacks in NLP
  - Why needed here: Core technique for manipulating text to fool classifiers while preserving utility
  - Quick check question: What are the key components of an adversarial attack framework in NLP (goal function, constraints, transformations, search methods)?

- Concept: Interpretable machine learning for NLP
  - Why needed here: Provides token-level importance scores to guide which words to modify for privacy vs utility
  - Quick check question: How do methods like Layer Integrated Gradients assign importance scores to individual tokens?

- Concept: Privacy-utility tradeoff in machine learning
  - Why needed here: Framework for understanding how to protect sensitive attributes while maintaining task performance
  - Quick check question: What distinguishes empirical privacy protection from differential privacy guarantees in NLP applications?

## Architecture Onboarding

- Component map: Auxiliary models (privacy and utility) -> Interpretable model -> Token similarity module -> Adversarial generation loop -> Target model

- Critical path:
  1. Train auxiliary models on separate data
  2. Generate token importance scores using interpretable model
  3. Select tokens to modify based on privacy/utility importance
  4. Find similar replacement tokens
  5. Generate candidate adversarial texts
  6. Evaluate against target model

- Design tradeoffs:
  - More queries to target model → better adversarial examples but higher cost
  - Stricter POS tag matching → better text quality but fewer valid candidates
  - Larger k for similar words → more options but potentially lower quality replacements

- Failure signatures:
  - Attack success drops when auxiliary and target models differ significantly
  - Utility retention fails when too many important utility tokens are modified
  - Mode collapse occurs if generative components are added

- First 3 experiments:
  1. Train auxiliary models and verify they match target performance on validation set
  2. Generate token importance scores and visualize distribution across texts
  3. Create adversarial examples for single sentence and manually verify privacy/utility preservation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does IDT's performance compare to generative methods like GPT-based approaches for privacy-preserving text rewriting?
- Basis in paper: [explicit] The paper mentions generative methods face challenges like mode collapse and produce extensively different texts, but doesn't directly compare IDT to GPT-based approaches
- Why unresolved: The paper only compares IDT to back-translation and TextAttack methods, not modern generative approaches
- What evidence would resolve it: Direct comparison of IDT against GPT-based privacy protection methods on the same datasets and tasks

### Open Question 2
- Question: What is the relationship between privacy protection effectiveness and semantic similarity preservation in IDT?
- Basis in paper: [inferred] The paper measures both attack success and text quality metrics like cosine similarity and BERTScore, but doesn't analyze their correlation
- Why unresolved: While the paper shows IDT maintains good text quality while achieving privacy protection, it doesn't investigate if there's a trade-off between these objectives
- What evidence would resolve it: Correlation analysis between attack success rates and semantic similarity metrics across different datasets and parameter settings

### Open Question 3
- Question: How does IDT scale to more complex multi-class privacy tasks with larger class spaces?
- Basis in paper: [explicit] The paper tests IDT on datasets with up to 17 classes (Shakespeare plays) but notes utility retention becomes harder with larger class spaces
- Why unresolved: The experiments only go up to 17 classes, and the paper doesn't explore how IDT performs with much larger class spaces common in real-world applications
- What evidence would resolve it: Testing IDT on datasets with hundreds or thousands of classes while measuring attack success and utility retention rates

## Limitations

- The method's effectiveness critically depends on adversarial examples transferring from auxiliary to target models, which may fail when models have different architectures or training data
- The approach assumes interpretable models accurately identify which tokens are important for privacy versus utility, but doesn't validate this causal relationship
- Text quality constraints (POS tag preservation, similarity matching) may limit attack effectiveness and create a tradeoff between quality and privacy protection

## Confidence

**High Confidence (4/8 claims)**:
- IDT outperforms generative approaches in producing reliable text rewrites without mode collapse
- The method achieves significantly higher Attack Success rates than baselines across multiple datasets
- Human evaluations confirm that adversarial examples maintain high text quality while successfully deceiving privacy classifiers
- The approach is computationally efficient compared to query-intensive methods

**Medium Confidence (3/8 claims)**:
- Attack Success rates remain high even with stricter constraints (POS preservation, similarity matching)
- Utility Retention rates are consistently high across different dataset/task combinations
- The transferability-based approach effectively reduces the number of queries to target models

**Low Confidence (1/8 claim)**:
- The specific mechanism by which interpretable models perfectly balance privacy modification and utility preservation across all scenarios

## Next Checks

1. **Cross-Architecture Transferability Test**: Evaluate IDT when auxiliary and target models use different architectures (e.g., BERT vs RoBERTa vs DistilBERT) to quantify the impact of architectural differences on attack success and transferability.

2. **Interpretability Validation**: Conduct ablation studies where interpretable model importance scores are randomly shuffled or replaced with alternative attribution methods to verify that the specific choice of Layer Integrated Gradients is critical for performance.

3. **Constraint Relaxation Analysis**: Systematically relax the POS tag and similarity constraints to determine the true tradeoff curve between text quality metrics and attack success rates, identifying whether current constraints are overly conservative.