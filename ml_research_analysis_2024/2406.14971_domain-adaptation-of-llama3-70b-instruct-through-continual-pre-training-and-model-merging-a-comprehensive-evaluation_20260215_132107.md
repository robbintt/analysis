---
ver: rpa2
title: 'Domain Adaptation of Llama3-70B-Instruct through Continual Pre-Training and
  Model Merging: A Comprehensive Evaluation'
arxiv_id: '2406.14971'
source_url: https://arxiv.org/abs/2406.14971
tags:
- data
- merging
- domain-specific
- domain
- general
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses domain adaptation of large language models
  for specialized tasks, focusing on adapting Llama3-70B-Instruct for SEC financial
  data. The core method involves continual pre-training on SEC data combined with
  model merging using TIES to mitigate catastrophic forgetting.
---

# Domain Adaptation of Llama3-70B-Instruct through Continual Pre-Training and Model Merging: A Comprehensive Evaluation

## Quick Facts
- arXiv ID: 2406.14971
- Source URL: https://arxiv.org/abs/2406.14971
- Authors: Shamane Siriwardhana, Mark McQuade, Thomas Gauthier, Lucas Atkins, Fernando Fernandes Neto, Luke Meyers, Anneketh Vij, Tyler Odenthal, Charles Goddard, Mary MacCarthy, Jacob Solawetz
- Reference count: 5
- One-line primary result: Llama3-70B-Instruct adapted to SEC financial data through continual pre-training and model merging achieves high domain-specific accuracy while recovering general capabilities

## Executive Summary
This paper presents a comprehensive approach to domain adaptation of large language models for specialized tasks, specifically focusing on adapting Llama3-70B-Instruct for SEC financial data analysis. The authors employ continual pre-training (CPT) on a mixture of domain-specific SEC data and general RedPajama data, followed by model merging using TIES to mitigate catastrophic forgetting. The methodology successfully demonstrates that models can be specialized for domain-specific tasks while maintaining general capabilities, achieving near-perfect accuracy on certain financial classification tasks and showing significant perplexity improvements on domain-specific data.

## Method Summary
The approach combines continual pre-training with model merging to adapt Llama3-70B-Instruct for SEC financial data analysis. First, the model undergoes CPT on 70B tokens of SEC data mixed with 1B tokens of general RedPajama data using Megatron-Core for distributed training. Then, model merging is performed using MergeKit with TIES to combine the CPT checkpoint with the original Llama3-70B-Instruct, recovering general capabilities while preserving domain-specific knowledge. The training process requires significant computational resources, utilizing 128 H100 GPUs for approximately 31 days.

## Key Results
- CPT on SEC data significantly improves domain-specific performance, with accuracy nearing 100% on financial classification tasks
- Model merging with TIES effectively recovers general capabilities lost during CPT while maintaining domain-specific knowledge
- Domain-specific perplexity decreases substantially after CPT, demonstrating successful adaptation to SEC data
- General evaluation benchmarks show significant recovery post-merging, indicating successful mitigation of catastrophic forgetting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Continual Pre-Training (CPT) on domain-specific data improves domain-specific performance while maintaining general capabilities when properly mixed with general data
- Mechanism: CPT extends the training of a base model using a carefully curated mix of domain-specific data (70B SEC tokens) and general data (1B RedPajama tokens), allowing the model to learn domain-specific patterns while preserving general knowledge through the general data component
- Core assumption: The 1B general tokens are sufficient to maintain general capabilities while the 70B domain tokens provide adequate domain-specific knowledge
- Evidence anchors:
  - [abstract] "Through this study, we evaluated the impact of integrating financial regulatory data into a robust language model and examined the effectiveness of our model merging techniques in preserving and improving the model's instructive abilities."
  - [section] "This approach has allowed us to maintain a high level of generalization while benefiting from domain-specific data."
  - [corpus] Weak evidence - the corpus shows related work on CPT but doesn't specifically address the general data mixing ratio used here
- Break condition: If the ratio of general to domain-specific data is too low, the model may lose general capabilities; if too high, domain-specific knowledge may not be adequately acquired

### Mechanism 2
- Claim: Model merging with TIES technique effectively recovers general capabilities lost during CPT while preserving domain-specific knowledge
- Mechanism: TIES merging combines the weights of the CPT model (which has domain-specific knowledge but lost some general capabilities) with the original instruct model (which has strong general capabilities), using weighted parameter values that preserve both sets of knowledge
- Core assumption: The TIES merging technique can effectively balance the contributions of both models without significant interference between their learned parameters
- Evidence anchors:
  - [abstract] "Model merging effectively recovers general capabilities while maintaining domain-specific knowledge."
  - [section] "By employing TIES merging techniques, we observed that up to certain levels, we could bring back the general capabilities of the original instruct model while enhancing domain-specific knowledge, thereby addressing catastrophic forgetting."
  - [corpus] Moderate evidence - the corpus shows TIES is a recognized merging technique but doesn't specifically validate its effectiveness for SEC data adaptation
- Break condition: If the merging weights are not properly tuned, the technique may fail to recover general capabilities or may overly dilute domain-specific knowledge

### Mechanism 3
- Claim: Catastrophic forgetting is mitigated through the combination of CPT data mixing and TIES model merging
- Mechanism: The two-stage approach first uses mixed data during CPT to maintain some general capabilities, then uses TIES merging to explicitly recover any lost general capabilities, creating a synergistic effect that prevents catastrophic forgetting
- Core assumption: The combination of these two techniques provides complementary benefits that neither alone can achieve
- Evidence anchors:
  - [abstract] "Our focus included continual pre-training (CPT) and model merging, aiming to enhance the model's domain-specific capabilities while mitigating catastrophic forgetting."
  - [section] "A significant challenge arises with catastrophic forgetting (Kirkpatrick et al., 2017), wherein post-pretraining often results in a deterioration of the model's original general abilities hindering its fine-tuned performance across various tasks."
  - [corpus] Strong evidence - the corpus shows multiple papers addressing catastrophic forgetting through similar CPT and merging approaches
- Break condition: If either stage is poorly executed (e.g., inadequate general data mixing or improper merging weights), catastrophic forgetting may still occur

## Foundational Learning

- Concept: Catastrophic Forgetting
  - Why needed here: Understanding why models lose previously learned capabilities is crucial for appreciating why both CPT data mixing and TIES merging are necessary
  - Quick check question: What is catastrophic forgetting and why does it occur during continual pre-training?

- Concept: Domain Adaptation
  - Why needed here: The paper focuses on adapting a general-purpose model to a specific domain (SEC data), requiring understanding of how models can be specialized for particular tasks
  - Quick check question: How does domain adaptation differ from general pre-training, and what are the key challenges?

- Concept: Model Merging Techniques
  - Why needed here: TIES merging is the core technique for recovering general capabilities, requiring understanding of how different model weights can be combined
  - Quick check question: What is TIES merging and how does it differ from simple weight averaging?

## Architecture Onboarding

- Component map:
  Data Pipeline: SEC data acquisition -> text extraction -> cleaning -> storage
  Training: Megatron-Core for distributed training -> CPT with mixed data
  Merging: MergeKit with TIES configuration -> merged model checkpoint
  Evaluation: Domain-specific benchmarks + general evaluation harness

- Critical path:
  1. Data preparation and preprocessing
  2. CPT training with Megatron-Core on distributed cluster
  3. TIES model merging with MergeKit
  4. Comprehensive evaluation across multiple benchmarks

- Design tradeoffs:
  - General vs. domain-specific data ratio: More general data preserves capabilities but less domain-specific knowledge; more domain data improves specialization but risks forgetting
  - Merging method selection: TIES vs. other methods (SLERP, Linear, DARE) based on preserving both general and domain-specific capabilities
  - Training scale: 70B parameter model requires significant computational resources but provides better performance

- Failure signatures:
  - High perplexity on general datasets after CPT indicates loss of general capabilities
  - Poor performance on domain-specific tasks indicates insufficient domain adaptation
  - Merging artifacts or instability in model outputs suggest improper merging configuration

- First 3 experiments:
  1. Test CPT with varying general/domain data ratios (1B:70B, 5B:65B, 10B:60B) to find optimal balance
  2. Compare TIES merging with other methods (SLERP, Linear) using the same CPT checkpoint
  3. Evaluate perplexity on both general and domain-specific datasets after each stage to quantify forgetting and recovery

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can catastrophic forgetting be effectively mitigated during continual pre-training on domain-specific data while preserving general capabilities?
- Basis in paper: [explicit] The paper discusses catastrophic forgetting as a key challenge during domain adaptation, noting that post-pretraining often results in deterioration of the model's original general abilities
- Why unresolved: While the paper demonstrates that model merging techniques (like TIES) can recover some general capabilities, the exact balance between domain-specific training and general knowledge preservation remains unclear
- What evidence would resolve it: Systematic ablation studies comparing different pre-training data mixtures, training durations, and merging strategies to identify optimal configurations that minimize catastrophic forgetting while maximizing domain-specific performance

### Open Question 2
- Question: What is the optimal ratio of domain-specific data to general data during continual pre-training for maximizing both domain-specific performance and general capabilities?
- Basis in paper: [explicit] The paper mentions that Domain-specific Continual Pre-Training (Que et al., 2024) focuses on refining LLMs by optimizing the amalgamated ratio of general corpus data to domain-specific data
- Why unresolved: The paper uses a 70B token SEC data to 1B token general data ratio, but does not systematically explore whether this ratio is optimal or how performance varies with different ratios
- What evidence would resolve it: Comprehensive experiments testing multiple data ratio combinations and measuring their impact on both domain-specific metrics (perplexity, task accuracy) and general capabilities (performance on standard benchmarks)

### Open Question 3
- Question: How can the model merging process be improved to better preserve domain-specific knowledge while fully recovering general capabilities?
- Basis in paper: [explicit] The paper shows that model merging with TIES can recover general capabilities but notes that some domain-specific performance is lost during merging
- Why unresolved: The current merging approach uses fixed weight parameters that may not optimally balance the trade-off between domain-specific and general knowledge retention
- What evidence would resolve it: Development and testing of adaptive merging strategies that dynamically adjust weights based on performance metrics, or exploration of alternative merging techniques beyond TIES that might better preserve domain-specific knowledge

## Limitations
- The exact preprocessing pipeline for SEC data remains unspecified, including which filing types were included and how noise was filtered
- The computational cost (31 days on 128 H100s) makes the approach inaccessible to most research teams, limiting reproducibility
- The merging weights and configuration for TIES are not detailed, making it difficult to assess whether optimal performance was achieved

## Confidence
- High confidence in the general methodology of combining CPT with model merging to address catastrophic forgetting
- Medium confidence in the specific claim that a 70:1 domain-to-general data ratio is optimal
- Low confidence in the absolute performance numbers without access to exact evaluation protocols and datasets

## Next Checks
1. Implement an ablation study varying the general-to-domain data ratio (e.g., 1:1, 10:1, 70:1) to determine the optimal balance for maintaining general capabilities while achieving domain specialization
2. Compare TIES merging performance against alternative techniques (SLERP, Linear, DARE) using identical CPT checkpoints to validate that TIES is indeed the optimal choice for this application
3. Conduct a cost-benefit analysis of different training configurations (e.g., smaller model sizes, fewer GPUs, shorter training duration) to assess whether the 31-day, 128-GPU approach is necessary for achieving the reported performance gains