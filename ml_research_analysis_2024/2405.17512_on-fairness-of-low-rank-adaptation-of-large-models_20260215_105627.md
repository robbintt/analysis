---
ver: rpa2
title: On Fairness of Low-Rank Adaptation of Large Models
arxiv_id: '2405.17512'
source_url: https://arxiv.org/abs/2405.17512
tags:
- lora
- full
- gender
- fairness
- review
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the fairness implications of low-rank adaptation
  (LoRA) for large models, comparing it to full fine-tuning. Extensive experiments
  across vision and language domains (ViT-Base, Swin-v2-Large, Llama-2 7B, Mistral
  7B) on classification and generation tasks show that LoRA does not consistently
  worsen subgroup fairness compared to full fine-tuning.
---

# On Fairness of Low-Rank Adaptation of Large Models

## Quick Facts
- arXiv ID: 2405.17512
- Source URL: https://arxiv.org/abs/2405.17512
- Reference count: 40
- One-line primary result: LoRA does not consistently worsen subgroup fairness compared to full fine-tuning across vision and language models.

## Executive Summary
This paper investigates the fairness implications of low-rank adaptation (LoRA) for large models, comparing it to full fine-tuning. Extensive experiments across vision and language domains (ViT-Base, Swin-v2-Large, Llama-2 7B, Mistral 7B) on classification and generation tasks show that LoRA does not consistently worsen subgroup fairness compared to full fine-tuning. While isolated cases exist where LoRA exacerbates bias, the pattern is inconsistentâ€”LoRA often achieves equivalent or even improved fairness. The fairness implications depend on the quality of the pre-trained model, and LoRA rank has little impact on subgroup fairness. However, token biases in LLMs complicate fairness evaluations for generative tasks, highlighting the need for more careful evaluation methods.

## Method Summary
The study fine-tunes base models (ViT-Base, Swin-v2-Large, Llama-2 7B, Mistral 7B) using both LoRA and full fine-tuning across multiple vision and language tasks. Evaluation metrics include group-wise accuracy, demographic parity difference (DPD), equalized odds difference (EOD), expected calibration error (ECE), BLEU scores, and membership inference attack (MIA) success rates. The authors compare subgroup fairness across different models, tasks, and LoRA ranks, and analyze gender bias in language models through token likelihood comparisons. Experiments use datasets like Berkeley D-Lab hatespeech, UTK-Face, WinoMT, and Yelp reviews.

## Key Results
- LoRA does not consistently worsen subgroup fairness compared to full fine-tuning; in many cases, it achieves equivalent or improved fairness.
- The fairness implications of LoRA depend on the quality of the underlying pre-trained model.
- LoRA rank has little impact on subgroup fairness metrics.
- Token biases in LLMs complicate fairness evaluations for generative tasks.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA does not consistently worsen subgroup fairness compared to full fine-tuning.
- Mechanism: LoRA approximates weight changes using low-rank matrices, which may reduce the model's capacity to overfit to subgroup-specific patterns, thereby avoiding exacerbation of existing biases.
- Core assumption: The pre-trained model's fairness properties are preserved or improved during LoRA adaptation.
- Evidence anchors:
  - [abstract] "experiments suggest that while one can isolate cases where LoRA exacerbates model bias across subgroups, the pattern is inconsistent -- in many cases, LoRA has equivalent or even improved fairness compared to the base model or its full fine-tuning baseline."
  - [section] "No consistent pattern of LoRA worsening subgroup fairness compared to full fine-tuning. Overarchingly, LoRA and full fine-tuning exhibit similar performance across all subgroups..."
  - [corpus] Weak evidence; no corpus papers directly address LoRA fairness consistency.
- Break condition: If the pre-trained model itself is biased or if LoRA rank is too low to capture necessary subgroup distinctions.

### Mechanism 2
- Claim: The fairness implications of LoRA depend on the quality of the underlying pre-trained model.
- Mechanism: A stronger pre-trained model may have better inherent fairness, and LoRA fine-tuning on such a model preserves or enhances this property.
- Core assumption: The base model's fairness characteristics transfer to the fine-tuned model.
- Evidence anchors:
  - [section] "The fairness implications may depend on the quality of the underlying pre-trained model... when the base model is switched to the more powerful Swin-v2-Large (all else kept the same)... the tendency disappears."
  - [abstract] "The fairness implications may depend on the quality of the pre-trained model..."
  - [corpus] Weak evidence; no corpus papers explicitly link pre-trained model quality to LoRA fairness outcomes.
- Break condition: If the pre-trained model's fairness is poor, LoRA may not improve it.

### Mechanism 3
- Claim: LoRA rank has little impact on subgroup fairness.
- Mechanism: Increasing LoRA rank beyond a threshold necessary for task performance does not significantly alter fairness metrics, as the rank is not a primary driver of subgroup bias.
- Core assumption: Fairness is more influenced by the task and data than by the rank of LoRA matrices.
- Evidence anchors:
  - [section] "The LoRA rank has little impact on subgroup fairness... accuracy and fairness metrics (EOD) are not influenced by rank..."
  - [abstract] "LoRA rank has little impact on subgroup fairness."
  - [corpus] Weak evidence; no corpus papers directly test LoRA rank effects on fairness.
- Break condition: If rank is too low to learn necessary subgroup distinctions, fairness may degrade.

## Foundational Learning

- Concept: Fairness definitions (DPD, EOD)
  - Why needed here: To evaluate and compare fairness across subgroups in LoRA and full fine-tuning.
  - Quick check question: What is the difference between demographic parity difference (DPD) and equalized odds difference (EOD)?

- Concept: Parameter-efficient fine-tuning
  - Why needed here: To understand how LoRA modifies model weights differently from full fine-tuning.
  - Quick check question: How does LoRA approximate weight changes compared to full fine-tuning?

- Concept: Calibration and expected calibration error (ECE)
  - Why needed here: To assess if LoRA affects model confidence and fairness in probability estimates.
  - Quick check question: What does a low ECE indicate about a model's calibration?

## Architecture Onboarding

- Component map:
  Base model (frozen weights) -> LoRA adapters (low-rank matrices B and A) -> Classification head (for supervised tasks) -> Prompt templates (for generative tasks) -> Evaluation metrics (accuracy, fairness, calibration, MIA resistance)

- Critical path:
  1. Load pre-trained model
  2. Initialize LoRA adapters with chosen rank
  3. Fine-tune on downstream task data
  4. Evaluate fairness metrics across subgroups
  5. Compare against full fine-tuning baseline

- Design tradeoffs:
  - Rank vs. capacity: Higher rank increases capacity but also memory and computation.
  - Task complexity vs. rank: Simple tasks may need low rank; complex tasks may need higher.
  - Fairness vs. utility: Sometimes LoRA improves fairness but slightly reduces overall accuracy.

- Failure signatures:
  - Overconfidence in LoRA predictions (high ECE)
  - Token bias in generative tasks (strong preferences for specific tokens)
  - Sensitivity to subgroup size (accuracy varies with group size)

- First 3 experiments:
  1. Fine-tune ViT-Base on UTK-Face gender classification with LoRA rank 8 vs. full fine-tuning; compare subgroup accuracy and EOD.
  2. Fine-tune Llama-2 7B on D-Lab religion hatespeech detection with LoRA rank 64 vs. full fine-tuning; compare DPD and EOD across religious subgroups.
  3. Evaluate LoRA rank sweep (1, 8, 64, 256, 512) on UTK-Face gender classification; measure subgroup accuracy and fairness metrics.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the fairness of LoRA depend on the specific low-rank adaptation technique used (e.g., different parameterizations or optimization strategies)?
- Basis in paper: [inferred] The paper discusses LoRA as a general technique but does not explore variations in its implementation. It mentions that "the fairness properties of LoRA are not only a function of its parameter efficiency" but does not investigate different LoRA formulations.
- Why unresolved: The study focuses on a specific LoRA implementation and does not compare it with alternative low-rank adaptation methods or different optimization strategies within LoRA itself.
- What evidence would resolve it: Comparative experiments between different LoRA implementations or alternative low-rank adaptation techniques, measuring fairness metrics across various subgroup definitions and tasks.

### Open Question 2
- Question: How does LoRA's fairness performance compare to other parameter-efficient fine-tuning methods like prefix tuning or adapter-based approaches?
- Basis in paper: [inferred] The paper concludes by suggesting future work should "explore and compare other parameter-efficient methods (e.g., Li & Liang (2021); Liu et al. (2022a))" indicating this comparison has not been made.
- Why unresolved: The study is limited to comparing LoRA with full fine-tuning, without benchmarking against other parameter-efficient methods that might have different fairness implications.
- What evidence would resolve it: Direct comparisons of LoRA with other parameter-efficient fine-tuning methods across the same tasks and fairness metrics used in this study.

### Open Question 3
- Question: Can fairness gerrymandering be effectively mitigated in LoRA fine-tuning through adaptive subgroup definitions or fairness-aware training objectives?
- Basis in paper: [explicit] The paper explicitly discusses fairness gerrymandering as a limitation, noting that "fairness gerrymandering happens when change in subgroup definitions alters the fairness conclusions" and that addressing this is computationally demanding.
- Why unresolved: The paper acknowledges the problem but does not explore solutions, stating it "leave[s] exhaustive experimentation on varying subgroup definitions to future work."
- What evidence would resolve it: Experiments testing adaptive subgroup definitions, fairness-aware training objectives, or regularization techniques that explicitly address fairness gerrymandering in LoRA fine-tuning.

### Open Question 4
- Question: What are the long-term fairness implications of LoRA when fine-tuned models are deployed in real-world applications with dynamic data distributions?
- Basis in paper: [inferred] The paper focuses on static evaluation datasets and does not address deployment scenarios or concept drift in real-world applications.
- Why unresolved: The study evaluates fairness on fixed test sets without considering how model performance might change over time with evolving data distributions or user interactions.
- What evidence would resolve it: Longitudinal studies tracking fairness metrics of LoRA fine-tuned models deployed in production environments over extended periods, measuring performance across changing demographics and data distributions.

## Limitations
- Experimental scope covers only a subset of vision and language models and tasks, limiting generalizability.
- Fairness evaluations for generative tasks are complicated by inherent token biases in LLMs.
- The quality and composition of subgroups in datasets can significantly influence fairness metrics, but the study does not fully explore how subgroup definitions affect results.

## Confidence
- High Confidence: LoRA does not consistently worsen subgroup fairness compared to full fine-tuning; LoRA rank has little impact on subgroup fairness.
- Medium Confidence: The fairness implications of LoRA depend on the quality of the underlying pre-trained model.
- Low Confidence: Conclusions about fairness in generative tasks are tentative due to token biases in LLMs.

## Next Checks
1. Expand model and task diversity to validate fairness findings across more architectures and applications.
2. Refine generative fairness metrics to better disentangle model bias from task bias in LLMs.
3. Investigate subgroup sensitivity by systematically varying subgroup definitions and dataset compositions.