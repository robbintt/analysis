---
ver: rpa2
title: Can Similarity-Based Domain-Ordering Reduce Catastrophic Forgetting for Intent
  Recognition?
arxiv_id: '2402.14155'
source_url: https://arxiv.org/abs/2402.14155
tags:
- learning
- domain
- ordering
- path
- intent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether the order in which domains are presented
  during continual learning impacts catastrophic forgetting (CF) in intent recognition
  models. The authors use a generative intent recognition model and compare three
  domain-ordering strategies (min-sum path, max-sum path, random) based on inter-domain
  similarity graphs derived from SBERT embeddings.
---

# Can Similarity-Based Domain-Ordering Reduce Catastrophic Forgetting for Intent Recognition?

## Quick Facts
- arXiv ID: 2402.14155
- Source URL: https://arxiv.org/abs/2402.14155
- Reference count: 0
- Primary result: Min-sum path domain ordering significantly reduces catastrophic forgetting for smaller T5-Base models but not for larger T5-Large models.

## Executive Summary
This paper investigates whether the order in which domains are presented during continual learning impacts catastrophic forgetting (CF) in intent recognition models. The authors use a generative intent recognition model and compare three domain-ordering strategies (min-sum path, max-sum path, random) based on inter-domain similarity graphs derived from SBERT embeddings. Results show that for the smaller T5-Base model (220M parameters), the min-sum path strategy significantly reduces CF compared to the other strategies, aligning with curriculum learning principles. However, this effect disappears with the larger T5-Large model (770M parameters), indicating that model size influences the importance of domain ordering. The findings suggest that domain ordering can be an effective complementary strategy for mitigating CF, especially in resource-constrained scenarios where training smaller models is necessary.

## Method Summary
The study uses the Schema-Guided Dialog (SGD) dataset containing 20 domains with 1-10 intents each. The authors extract subsets of five domains and compute inter-domain similarities using SBERT embeddings to create fully-connected, weighted, undirected graphs. Three domain-ordering strategies are implemented: min-sum path (minimizing cumulative dissimilarity between consecutive domains), max-sum path (maximizing cumulative dissimilarity), and random ordering. T5-Base (220M parameters) and T5-Large (770M) models are incrementally fine-tuned on these ordered domains using 60% training data, 15% validation, and 25% testing with a 512-token context window. Performance is evaluated using Average Accuracy (mean accuracy across all domains) and Average Catastrophic Forgetting (average difference between best training accuracy and final accuracy for all tasks except the last).

## Key Results
- For T5-Base model, min-sum path strategy significantly reduces catastrophic forgetting compared to max-sum path (p = 0.0017) and random strategies
- The effectiveness of domain ordering disappears for T5-Large model, suggesting model size affects sensitivity to ordering
- Both models show sensitivity to intent label formation, particularly struggling with differentiating verbs in <verb>_<noun> label patterns

## Why This Works (Mechanism)

### Mechanism 1
Domain ordering affects catastrophic forgetting differently based on model size. The ordering strategy influences how well the model generalizes across domains during incremental training. For smaller models, a carefully chosen order (min-sum path) helps preserve knowledge from previously learned domains by building on similarities between consecutive domains. This breaks when model size is large enough that the model can inherently handle domain transitions without needing carefully ordered training sequences.

### Mechanism 2
The min-sum path strategy is most effective for reducing catastrophic forgetting in smaller models. This strategy orders domains to minimize the "jump" in dissimilarity between consecutive domains, allowing the model to build on previously learned patterns and reducing the likelihood of overwriting earlier knowledge. This progressive learning from similar to dissimilar domains helps maintain previously learned knowledge, but breaks when the domain similarities are not meaningful or when model capacity is sufficient to handle arbitrary domain ordering.

### Mechanism 3
Model sensitivity to intent label formation affects catastrophic forgetting patterns. The generative text-to-text approach used by T5 models makes them sensitive to how intent labels are structured. Models may struggle more with differentiating verbs in intent labels than nouns, leading to specific patterns of forgetting. This breaks when using a classification-based approach instead of a generative text-to-text approach, or when intent labels are structured differently.

## Foundational Learning

- Concept: Catastrophic forgetting
  - Why needed here: Understanding CF is essential to grasp why domain ordering matters and what the paper aims to mitigate
  - Quick check question: What happens to a model's performance on previously learned tasks when it encounters new tasks in continual learning?

- Concept: Curriculum learning
  - Why needed here: The paper's approach is rooted in curriculum learning principles, which suggest that learning is more effective when examples progress from simple to complex
  - Quick check question: How does the order in which examples are presented affect learning effectiveness in curriculum learning?

- Concept: Hamiltonian path in graph theory
  - Why needed here: The paper formulates domain ordering strategies as various path-finding algorithms on a domain-similarity graph, including Hamiltonian paths
  - Quick check question: What is a Hamiltonian path, and why is it relevant to the domain ordering problem?

## Architecture Onboarding

- Component map: Data preprocessing -> Domain-similarity graph construction -> Path-finding algorithms -> Continual learning training loop -> Evaluation metrics
- Critical path: Data preprocessing → Domain-similarity graph construction → Path-finding → Continual learning training → Evaluation
- Design tradeoffs:
  - Model size vs. ordering effectiveness: Larger models may not benefit from careful ordering
  - Computational cost of brute-force path-finding vs. approximate methods
  - Fixed context window vs. full dialogue history
- Failure signatures:
  - No significant difference in CF between ordering strategies (model too large)
  - High variance in results across runs (insufficient samples per domain)
  - Low accuracy overall (inappropriate tokenization or model choice)
- First 3 experiments:
  1. Run all three ordering strategies on a single domain subset with T5-Base model to verify implementation
  2. Compare min-sum path vs. random ordering on multiple domain subsets to confirm effect size
  3. Test T5-Large model with the same domain subsets to observe model size effect

## Open Questions the Paper Calls Out

### Open Question 1
How does the effectiveness of domain ordering strategies vary across different model architectures beyond T5 (e.g., BERT, GPT, or smaller Transformer variants)? The study focused exclusively on T5 models, leaving open whether similar patterns hold for other architectures with different parameter counts and pretraining objectives.

### Open Question 2
What alternative definitions of inter-domain distance (e.g., based on task difficulty, semantic diversity, or feature space clustering) might yield better domain orderings than cosine similarity of SBERT embeddings? The study used a single definition of inter-domain distance, which may not capture all relevant aspects of domain relationships.

### Open Question 3
How do domain ordering strategies interact with other continual learning techniques (e.g., experience replay, regularization methods) in mitigating catastrophic forgetting? The study isolated domain ordering as a single variable, without exploring how it might complement or conflict with other established CL methods.

## Limitations
- The study only tested T5-Base and T5-Large models, limiting generalizability to other architectures
- Computational resources required for brute-force min-sum path computation limit scalability to larger domain sets
- The use of five-domain subsets from SGD may not generalize to other domain distributions or task types

## Confidence
- High confidence: The core finding that min-sum path ordering significantly reduces catastrophic forgetting for T5-Base models (supported by p = 0.0017 statistical significance)
- Medium confidence: The generalization of domain ordering benefits to other similar intent recognition tasks or datasets
- Medium confidence: The specific threshold at which model size renders domain ordering ineffective (observed at 770M parameters but may vary with different architectures)

## Next Checks
1. Test the domain ordering strategies with additional model sizes (e.g., T5-3B or GPT-3 variants) to identify the precise model capacity threshold where ordering effects disappear
2. Evaluate the same ordering strategies on non-dialogue intent recognition tasks (e.g., text classification or semantic parsing) to assess generalizability
3. Compare the computational overhead of domain ordering against alternative CF mitigation strategies like rehearsal or regularization methods for different model scales