---
ver: rpa2
title: Enhancing LLM Safety via Constrained Direct Preference Optimization
arxiv_id: '2403.02475'
source_url: https://arxiv.org/abs/2403.02475
tags:
- reward
- preference
- c-dpo
- learning
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Constrained DPO (C-DPO), a lightweight and
  efficient extension of the recently proposed Direct Preference Optimization (DPO)
  approach for fine-tuning large language models (LLMs). C-DPO aims to improve LLM
  safety by optimizing for both helpfulness and harmlessness via a constrained optimization
  framework.
---

# Enhancing LLM Safety via Constrained Direct Preference Optimization

## Quick Facts
- arXiv ID: 2403.02475
- Source URL: https://arxiv.org/abs/2403.02475
- Authors: Zixuan Liu; Xiaolin Sun; Zizhan Zheng
- Reference count: 40
- Key outcome: C-DPO improves LLM safety by optimizing for both helpfulness and harmlessness via constrained optimization, achieving higher rewards than safe RLHF on human preference datasets

## Executive Summary
This paper proposes Constrained DPO (C-DPO), a lightweight extension of Direct Preference Optimization that improves LLM safety by optimizing for both helpfulness and harmlessness via a constrained optimization framework. C-DPO uses dual gradient descent to identify an optimal tradeoff between these objectives without requiring reinforcement learning. The method constructs synthetic preference datasets and iteratively adjusts a Lagrange multiplier to enforce safety constraints while maximizing helpfulness.

## Method Summary
C-DPO transforms the safe RLHF problem into an unconstrained Lagrangian form and uses dual gradient descent to optimize the policy. For each Lagrange multiplier λ, it constructs a synthetic preference dataset Drλ using a combined reward function rλ(x,y) = r(x,y) - λc(x,y), then applies DPO to optimize the policy. The Lagrange multiplier is updated via gradient descent on the expected constraint violation. The method requires pre-trained reward and cost models, a base LLM, and preference data.

## Key Results
- C-DPO provides strong safety guarantees while achieving significantly higher rewards than a recently proposed safe RLHF approach on a human preference dataset
- The method converges to a near-optimal tradeoff between helpfulness and harmlessness within 8-9 epochs
- C-DPO requires minimal computational resources (single 40GB A100 GPU) and is more efficient than traditional RLHF approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm enforces a safety constraint while optimizing for helpfulness by solving a constrained optimization problem using dual gradient descent over DPO.
- Mechanism: The original safe RLHF problem is transformed into an unconstrained Lagrangian form. For a given Lagrange multiplier λ, the optimal policy is derived as a Boltzmann distribution over a combined reward function rλ(x,y) = r(x,y) - λc(x,y), which is then optimized using DPO by constructing a synthetic preference dataset Drλ. The Lagrange multiplier λ is iteratively updated via gradient descent on the expected constraint violation.
- Core assumption: The Slater condition holds (there exists a strictly feasible policy) and the original RLHF objective is concave over policies, ensuring strong duality.
- Evidence anchors: [abstract]: "By integrating dual gradient descent and DPO, our method identifies a nearly optimal trade-off between helpfulness and harmlessness without using reinforcement learning." [section]: "Define the objective function Jr(πθ) = Ex∼D,y∼πθ(y|x)[r(x, y)] − βDKL[πθ(y|x)||πref (y|x)] and the constraint function Jc(πθ) = Ex∼D,y∼πθ(y|x)[c(x, y)] − Climit... Then the original problem can be solved using the dual gradient method..."
- Break condition: If the Slater condition fails (no strictly feasible policy exists), strong duality breaks and the dual gradient method may not find the correct solution.

### Mechanism 2
- Claim: The synthetic preference dataset Drλ is constructed so that optimizing the DPO objective on Drλ yields the same optimal policy as solving the original constrained RLHF problem.
- Mechanism: For each prompt-response pair in the original dataset, the reward and cost are computed using pretrained models. A new Bradley-Terry preference model pλ*(y1 ≻ y2|x) is defined using the combined preference function rλ. Response pairs in Drλ are chosen by comparing rλ values, creating a dataset consistent with this new preference model.
- Core assumption: The new dataset Drλ is sufficiently large and perfectly fits the Bradley-Terry preference model derived from rλ, so that minimizing the DPO loss on Drλ recovers the true preference function rλ.
- Evidence anchors: [section]: "After deriving the new preference dataset Drλ, we can formulate the following maximum likelihood objective analogous to DPO to optimize the policy πθ for a given λ... we can prove that if the new BT preference model pλ* is generated according to our preference function rλ... then the optimal policy that minimizes (13) coincides with the optimal policy that maximizes the original objective in (10)."
- Break condition: If the synthetic dataset Drλ is too small or poorly represents the true Bradley-Terry model, the equivalence breaks and DPO optimization will not recover the correct optimal policy.

### Mechanism 3
- Claim: The gradient of the dual function g(λ) can be estimated by sampling responses from the current policy πλ* and computing the expected constraint violation.
- Mechanism: The gradient dg(λ)/dλ = Ex∼D,y∼πλ*(y|x)[Climit - c(x,y)] measures how much the current policy violates the safety constraint. In practice, this is estimated by sampling a subset of prompts, generating multiple responses per prompt from πλ*, and averaging the cost differences.
- Core assumption: The sampled prompts and responses are representative of the true expectation over the entire dataset.
- Evidence anchors: [section]: "we update λ(t) by estimating the expected constraint violation Ex∼D,y∼πλ*(y|x)[Climit - c(x, y)], as delineated in Equation (14)."
- Break condition: If the sample size is too small or the prompts are not representative, the gradient estimate will be biased, leading to incorrect updates of λ and failure to converge to the optimal safety-helpfulness tradeoff.

## Foundational Learning

- Concept: Dual gradient descent for constrained optimization
  - Why needed here: The safe RLHF problem is a constrained optimization problem (maximize reward subject to cost constraint). Dual gradient descent transforms this into a sequence of unconstrained problems that can be solved iteratively.
  - Quick check question: What is the relationship between the primal and dual variables in a constrained optimization problem, and how does dual gradient descent update them?

- Concept: Bradley-Terry preference model
  - Why needed here: The DPO framework optimizes language models directly from pairwise preference data. The Bradley-Terry model provides a probabilistic interpretation of preferences based on reward differences, which is essential for constructing the synthetic dataset Drλ.
  - Quick check question: How does the Bradley-Terry model relate pairwise preferences to underlying reward functions, and why is this useful for preference optimization?

- Concept: Strong duality in convex optimization
  - Why needed here: The paper claims that the safe RLHF problem satisfies strong duality, meaning the optimal value of the dual problem equals the optimal value of the primal problem. This justifies using dual gradient descent to solve the constrained problem.
  - Quick check question: Under what conditions does strong duality hold in a constrained optimization problem, and how does this relate to the Slater condition?

## Architecture Onboarding

- Component map: Pre-trained SFT model (πref) -> Pre-trained reward/cost models (r, c) -> DPO trainer with synthetic dataset Drλ -> Policy updates -> Gradient estimator for λ updates -> Dual variable λ
- Critical path: For each iteration: (1) compute rλ(x,y) = r(x,y) - λc(x,y) for all prompt-response pairs, (2) construct Drλ by selecting response pairs with higher rλ, (3) run DPO to update the policy, (4) estimate the expected constraint violation by sampling from the updated policy, (5) update λ via gradient descent
- Design tradeoffs: Using a synthetic preference dataset Drλ avoids the need for explicit reward modeling during training (a key advantage of DPO), but requires accurate pretrained reward and cost models. The sampling-based gradient estimation is computationally efficient but introduces variance that can slow convergence.
- Failure signatures: If λ diverges or oscillates, the gradient estimates may be too noisy or the learning rate too high. If the policy fails to improve helpfulness while meeting the safety constraint, the Drλ dataset may not be representative or the pretrained models may be inaccurate. If the safety constraint is violated, the gradient estimates may be biased due to insufficient sampling.
- First 3 experiments:
  1. Verify that the synthetic preference dataset Drλ is constructed correctly by checking that response pairs are selected based on rλ values.
  2. Test the gradient estimation by comparing the sampled expected constraint violation to the true value on a small dataset.
  3. Run a single iteration of the algorithm and check that the policy updates improve the combined reward rλ while reducing the constraint violation.

## Open Questions the Paper Calls Out

- How does the performance of C-DPO compare to other state-of-the-art methods for aligning language models with dual objectives of helpfulness and harmlessness?
- How sensitive is the performance of C-DPO to the choice of the dual variable λ and the safety constraint threshold Climit?
- How does the computational efficiency of C-DPO compare to other methods for aligning language models, particularly in terms of training time and memory usage?

## Limitations

- Theoretical guarantees rely heavily on assumptions about reward/cost model accuracy and representativeness of synthetic preference datasets
- Experimental validation is limited to a single human preference dataset (BEaVERTAILS)
- Model-based evaluation methodology may not fully capture real-world safety performance
- Choice of safety threshold Climit=0 is somewhat arbitrary and may not generalize across domains

## Confidence

- **High Confidence**: The core algorithmic framework combining dual gradient descent with DPO is well-established in constrained optimization theory
- **Medium Confidence**: The empirical results showing improved safety and helpfulness trade-offs are promising but limited in scope
- **Low Confidence**: Claims about algorithm's efficiency and safety guarantees in real-world scenarios are not fully supported by current experimental evidence

## Next Checks

1. Test C-DPO on multiple human preference datasets with varying safety requirements and prompt distributions to assess generalizability beyond BEaVERTAILS

2. Implement automated safety evaluation using multiple adversarial prompts and safety classifiers to verify that constraint satisfaction holds under stress testing

3. Conduct experiments varying the learning rate for λ updates, number of samples for gradient estimation, and safety threshold Climit to understand algorithm stability across different configurations