---
ver: rpa2
title: Why is SAM Robust to Label Noise?
arxiv_id: '2405.03676'
source_url: https://arxiv.org/abs/2405.03676
tags:
- accuracy
- label
- noise
- clean
- gradient
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors investigate why Sharpness-Aware Minimization (SAM)
  is robust to label noise in training deep neural networks. They decompose SAM''s
  gradient update into two components: the logit scale term and the network Jacobian
  term.'
---

# Why is SAM Robust to Label Noise?

## Quick Facts
- arXiv ID: 2405.03676
- Source URL: https://arxiv.org/abs/2405.03676
- Reference count: 40
- Primary result: SAM's robustness to label noise is primarily due to its effect on the network Jacobian, which implicitly regularizes intermediate activations and final layer weights

## Executive Summary
This paper investigates why Sharpness-Aware Minimization (SAM) is robust to label noise in deep neural networks. Through theoretical analysis and experiments, the authors decompose SAM's gradient update into logit scale and network Jacobian components. They show that SAM's robustness is primarily driven by its Jacobian effect, which implicitly regularizes the norm of intermediate activations and final layer weights. This regularization keeps the loss of clean examples high while capping the loss of noisy examples, allowing SAM to learn clean examples faster. The authors propose a simpler regularization method that mimics SAM's Jacobian effect and demonstrate it can largely recover SAM's benefits in deep networks trained on real-world datasets.

## Method Summary
The authors analyze SAM's robustness to label noise through a combination of theoretical analysis and empirical experiments. They start by decomposing SAM's gradient update in linear models, showing how it implicitly reweights examples based on their loss. For deep networks, they propose modifying SAM to isolate its logit scale and Jacobian effects separately (L-SAM and J-SAM variants). They conduct experiments on linear models with synthetic data, 2-layer deep linear networks, and ResNet18 on CIFAR10 with various label noise rates. Additionally, they propose a simpler regularization method that applies L2 penalties to intermediate activations and final layer weights, comparing its performance to standard SAM.

## Key Results
- SAM's robustness to label noise is primarily due to its Jacobian effect, not its logit scale effect
- In linear models, SAM acts as a gradient reweighting scheme that up-weights low-loss examples
- For 2-layer DLNs, J-SAM recovers most of SAM's benefits, suggesting the Jacobian effect is the main driver of robustness
- A simpler regularization method mimicking SAM's Jacobian effect can largely recover SAM's benefits in deep networks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SAM preferentially up-weights the gradient contribution of low-loss examples, causing clean examples to be fit before noisy ones.
- Mechanism: In linear models, SAM's adversarial perturbation term increases the gradient norm of all examples, but the increase is larger for low-loss points. This is because the perturbation term is proportional to σ(-tf(w,x)+ρ||x||^2)/σ(-tf(w,x)), which is a monotonically increasing function of the loss. Thus, low-loss examples (which are more likely to be clean) get up-weighted more.
- Core assumption: The loss of clean examples decreases faster than the loss of noisy examples during early training.
- Evidence anchors:
  - [abstract] "SAM's effect in deeper networks is instead explained entirely by the effect SAM has on the network Jacobian."
  - [section 3.2] "We can interpret 1-SAM as a gradient reweighting scheme, where the weight for example xi is set to σ(-ti⟨w, xi⟩ + ρ∥xi∥^2)/σ(-ti⟨w, xi⟩)."
  - [corpus] Weak connection - no direct evidence of gradient reweighting in neighboring papers.
- Break condition: If the assumption about clean examples having lower loss than noisy examples early in training is violated, the mechanism breaks.

### Mechanism 2
- Claim: SAM's Jacobian perturbation regularizes the norm of intermediate activations and final layer weights, implicitly up-weighting clean examples.
- Mechanism: For 2-layer DLNs, J-SAM adds L2 regularization terms to the intermediate activations and last layer weights. This keeps the magnitude of the network output small, which keeps the loss of clean examples higher even as they are fit, and caps the loss of noisy examples.
- Core assumption: Regularizing the Jacobian term has a similar effect to explicit gradient reweighting in deep networks.
- Evidence anchors:
  - [abstract] "We theoretically derive the implicit regularization induced by this Jacobian effect in two layer linear networks."
  - [section 4.2] "J-SAM approximately reduces to SGD with L2 norm penalty on the intermediate activations and last layer weights."
  - [corpus] Weak connection - no direct evidence of Jacobian regularization in neighboring papers.
- Break condition: If the regularization effect of the Jacobian perturbation is not sufficient to balance the gradients of clean and noisy examples, the mechanism breaks.

### Mechanism 3
- Claim: SAM's robustness is primarily due to its Jacobian effect, not its logit scale effect, in deep networks.
- Mechanism: Experiments show that just applying the SAM perturbation to the Jacobian term (J-SAM) recovers most of SAM's label noise robustness, while just applying it to the logit scale term (L-SAM) does not. This suggests the Jacobian effect is the main driver of robustness.
- Core assumption: The logit scale effect is negligible compared to the Jacobian effect for typical perturbation magnitudes.
- Evidence anchors:
  - [abstract] "when we intervene and modify SAM to remove this effect, surprisingly, we see no visible degradation in performance."
  - [section 4.1] "J-SAM recovers almost all of the gains of SAM. This suggests that SAM's reweighting of the logit scale is not the main contributor to SAM's robustness in neural networks."
  - [corpus] Weak connection - no direct evidence of Jacobian vs logit scale effects in neighboring papers.
- Break condition: If the logit scale effect becomes significant for larger perturbation magnitudes, the mechanism breaks.

## Foundational Learning

- Concept: Chain rule decomposition of gradients
  - Why needed here: To understand how SAM's perturbation affects the logit scale and Jacobian terms separately.
  - Quick check question: Given a loss function ℓ(f(w,x),y) where f is the model output, write the chain rule decomposition of the gradient with respect to w.

- Concept: Sharpness-aware minimization
  - Why needed here: To understand the original motivation and mechanism of SAM.
  - Quick check question: What is the objective of sharpness-aware minimization, and how does it differ from standard gradient descent?

- Concept: Label noise robustness
  - Why needed here: To understand the problem setting and why SAM's robustness to label noise is significant.
  - Quick check question: Why is label noise a problem in training deep neural networks, and what are some common approaches to handle it?

## Architecture Onboarding

- Component map:
  SAM optimizer with 1-SAM variant -> Linear and deep neural network models -> Synthetic Gaussian data distribution with label noise -> Regularization terms for intermediate activations and final layer weights

- Critical path:
  1. Train model with SAM (1-SAM variant) on data with label noise
  2. Monitor test accuracy and clean/noisy training accuracy gap
  3. Analyze gradient norms and losses of clean vs noisy examples
  4. Compare performance to SGD and variants (L-SAM, J-SAM)

- Design tradeoffs:
  - SAM vs SGD: SAM is more robust to label noise but has higher computational cost due to per-example perturbations.
  - L-SAM vs J-SAM: L-SAM is simpler but less effective; J-SAM is more complex but recovers most of SAM's benefits.
  - Regularization strength: Too much regularization can hurt performance, too little may not be effective.

- Failure signatures:
  - If clean and noisy training accuracy gaps do not correlate with test accuracy, the mechanisms may not be working.
  - If J-SAM does not recover most of SAM's benefits, the Jacobian effect may not be the main driver of robustness.
  - If L-SAM performs as well as J-SAM, the logit scale effect may be more important than expected.

- First 3 experiments:
  1. Train a linear model on toy Gaussian data with 40% label noise using SGD and SAM (1-SAM). Plot test accuracy and clean/noisy training accuracy gap over time.
  2. Train a 2-layer DLN on CIFAR10 with 30% label noise using SGD, SAM, L-SAM, and J-SAM. Compare test accuracy and analyze gradient norms.
  3. Train a ResNet18 on CIFAR10 with 30% label noise using SGD with and without regularization on intermediate activations and final layer weights. Compare to SAM.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do SAM's robustness effects translate to more complex, real-world label noise patterns beyond synthetic uniform noise?
- Basis in paper: [explicit] The paper mentions that SAM's benefits extend beyond uniform synthetic noise to real-world datasets like CIFAR10, Tiny-ImageNet, and Flower102 with 30% label noise.
- Why unresolved: The paper primarily analyzes SAM's performance on synthetic uniform label noise and doesn't extensively explore how it behaves with more realistic, complex label noise patterns.
- What evidence would resolve it: Experiments comparing SAM's performance on datasets with various real-world label noise patterns (e.g., class-dependent noise, structured noise) against other robustness methods.

### Open Question 2
- Question: What is the theoretical relationship between SAM's sharpness minimization and its label noise robustness?
- Basis in paper: [explicit] The paper argues that understanding SAM's label noise robustness requires departing from characterizing it as a method for finding flatter minima, as peak performance occurs with early stopping far before convergence.
- Why unresolved: The paper provides empirical evidence that SAM's robustness isn't primarily due to sharpness minimization at convergence, but the theoretical relationship between these aspects remains unclear.
- What evidence would resolve it: A theoretical framework connecting SAM's optimization trajectory, sharpness properties, and its ability to learn clean examples faster than noisy examples.

### Open Question 3
- Question: How does SAM's performance scale with the dimensionality and complexity of the input data?
- Basis in paper: [inferred] The paper analyzes SAM's effects on linear models and simple deep linear networks, but the scaling behavior with higher-dimensional, complex data is not explored.
- Why unresolved: While the paper shows SAM's benefits on real-world image datasets, the impact of increasing input dimensionality and complexity on SAM's performance is not studied.
- What evidence would resolve it: Experiments systematically varying the input dimensionality and complexity (e.g., using different datasets, increasing input size) and measuring SAM's performance and robustness.

## Limitations

- The analysis primarily focuses on synthetic label noise and may not generalize to all real-world noise patterns
- The proposed regularization method's effectiveness beyond ResNet18 and the specific datasets tested remains unverified
- The paper doesn't extensively explore how SAM's performance scales with input dimensionality and complexity

## Confidence

| Claim | Confidence |
|-------|------------|
| SAM's robustness is primarily due to its Jacobian effect | Medium |
| The Jacobian regularization effect generalizes to complex deep networks | Medium |
| The proposed simpler regularization method can recover most of SAM's benefits | Medium |

## Next Checks

1. Test whether the clean/noisy loss gap persists across different label noise patterns (symmetric vs asymmetric) and noise rates beyond 40%
2. Verify that the Jacobian regularization effect remains dominant when varying the SAM perturbation magnitude ρ
3. Evaluate whether the proposed regularization method generalizes to architectures beyond ResNet18, such as Vision Transformers or architectures with different normalization schemes