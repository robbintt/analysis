---
ver: rpa2
title: 'Recent Trends of Multimodal Affective Computing: A Survey from NLP Perspective'
arxiv_id: '2409.07388'
source_url: https://arxiv.org/abs/2409.07388
tags:
- multimodal
- sentiment
- emotion
- learning
- recognition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This survey systematically reviews multimodal affective computing
  from a natural language processing perspective, focusing on four key tasks: multimodal
  sentiment analysis, multimodal emotion recognition in conversation, multimodal aspect-based
  sentiment analysis, and multimodal multi-label emotion recognition. It organizes
  existing research along four technical dimensions: multitask learning, pre-trained
  models, enhanced knowledge integration, and contextual information processing.'
---

# Recent Trends of Multimodal Affective Computing: A Survey from NLP Perspective

## Quick Facts
- **arXiv ID**: 2409.07388
- **Source URL**: https://arxiv.org/abs/2409.07388
- **Reference count**: 40
- **Primary result**: Systematic survey of multimodal affective computing from NLP perspective covering four key tasks with technical organization along four dimensions

## Executive Summary
This survey provides a comprehensive overview of multimodal affective computing from a natural language processing perspective, systematically reviewing four key tasks: multimodal sentiment analysis, multimodal emotion recognition in conversation, multimodal aspect-based sentiment analysis, and multimodal multi-label emotion recognition. The paper organizes existing research along four technical dimensions: multitask learning, pre-trained models, enhanced knowledge integration, and contextual information processing. It provides detailed coverage of formalization methods, relevant works, benchmark datasets, and evaluation metrics for each task while identifying consistency and differences across tasks. The survey also discusses related work involving facial expressions, acoustic signals, physiological signals, and emotion causes, highlighting technical approaches, challenges, and future directions.

## Method Summary
The survey systematically categorizes multimodal affective computing research along four technical dimensions: multitask learning approaches that jointly learn multiple tasks, pre-trained models used as backbones with parameter-efficient transfer learning methods (adapters, prompts), enhanced knowledge integration incorporating external knowledge sources, and contextual information processing capturing dependencies across utterances and speakers. For each of the four main tasks (multimodal sentiment analysis, multimodal emotion recognition in conversation, multimodal aspect-based sentiment analysis, and multimodal multi-label emotion recognition), the paper provides formalization, relevant works, benchmark datasets, and evaluation metrics. The survey identifies key mechanisms including modal consistency/difference modeling, pre-trained model transfer learning, and context-dependent emotion recognition.

## Key Results
- Organizes multimodal affective computing research into four key tasks with comprehensive technical coverage
- Identifies four technical dimensions: multitask learning, pre-trained models, knowledge integration, and contextual processing
- Highlights the importance of modeling both modal consistency (shared features) and modal difference (unique information)
- Provides detailed analysis of formalization methods, datasets, and evaluation metrics for each task
- Identifies future research directions including unification across granularity levels, external knowledge integration, and less-studied modalities

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Multimodal fusion effectiveness relies on modeling both consistency and difference between modalities
- **Mechanism**: Successful multimodal fusion separates representations into modal-invariant (consistency) and modal-specific (difference) components, allowing models to handle missing modalities while leveraging complementary information
- **Core assumption**: Different modalities contain both shared and unique information that can be explicitly modeled
- **Evidence anchors**:
  - [section] "Modal consistency refers to the shared feature space across different modalities for the same sample, while modal difference highlights the unique information each modality provides"
  - [section] "Most multimodal fusion approaches separate representations into modal-invariant (consistency) and modal-specific (difference) components"
  - [corpus] Weak evidence - corpus neighbors don't specifically address modal consistency/difference mechanisms
- **Break Condition**: If modalities don't have meaningful shared information or if noise dominates the differences

### Mechanism 2
- **Claim**: Pre-trained models provide transferable knowledge that improves multimodal affective computing performance
- **Mechanism**: Large pre-trained models (language models, multimodal models) contain massive transferred knowledge that can be fine-tuned for downstream affective tasks, serving as effective backbones
- **Core assumption**: General knowledge from pre-trained models transfers to affective computing tasks
- **Evidence anchors**:
  - [section] "pre-trained models contains massive transferred knowledge, which can be introduced into multimodal representation learning to probe the richer information"
  - [section] "From this perspective, multimodal affective computing tasks adopt pre-trained models as the backbone and then fine-tune them for downstream tasks"
  - [corpus] Weak evidence - corpus neighbors focus on general affective computing but don't specifically discuss pre-trained model transfer
- **Break Condition**: If affective computing requires highly specialized knowledge not captured in pre-training

### Mechanism 3
- **Claim**: Contextual information is crucial for accurate emotion recognition and sentiment analysis
- **Mechanism**: Context includes surrounding utterances, speaker information, and overall conversation history, enabling models to capture dependencies and improve prediction accuracy
- **Core assumption**: Emotion and sentiment are context-dependent and cannot be determined from isolated utterances
- **Evidence anchors**:
  - [section] "For MERC, contextual information encompasses the entire conversation, including both previous and subsequent utterances relative to the current utterance"
  - [section] "Researchers integrate contextual information using hierarchical approaches, self-attention mechanisms, and graph-based dependency modeling"
  - [corpus] Weak evidence - corpus neighbors mention emotion recognition but don't specifically discuss contextual modeling approaches
- **Break Condition**: If emotions are clearly expressed without context dependency

## Foundational Learning

- **Concept**: Multimodal learning fundamentals
  - **Why needed here**: The survey focuses on combining multiple modalities (text, audio, visual) for affective computing, requiring understanding of how different modalities interact
  - **Quick check question**: What are the three main strategies for multimodal fusion mentioned in the survey?

- **Concept**: Transfer learning principles
  - **Why needed here**: Pre-trained models are heavily used in the field, so understanding how knowledge transfers from general to specific tasks is essential
  - **Quick check question**: How does fine-tuning differ from using adapters for transfer learning?

- **Concept**: Emotion psychology basics
  - **Why needed here**: The survey deals with emotion recognition and sentiment analysis, requiring understanding of basic emotion categories and sentiment polarity concepts
  - **Quick check question**: What are the six universal emotions mentioned in the survey?

## Architecture Onboarding

- **Component map**: Text encoder (BERT, T5, etc.) → Audio encoder (AST, spectrogram processing) → Visual encoder (CLIP, Vision Transformer) → Fusion module (cross-attention, hierarchical fusion) → Classifier/regressor
- **Critical path**: Raw multimodal input → Feature extraction → Multimodal alignment → Fusion → Prediction
- **Design tradeoffs**: Early vs late vs intermediate fusion; separate vs unified encoders; explicit consistency/difference modeling vs implicit fusion
- **Failure signatures**: Poor performance on missing modalities; modality bias; inconsistent predictions across similar inputs
- **First 3 experiments**:
  1. Implement basic late fusion with pre-trained BERT, AST, and CLIP encoders
  2. Add cross-modal attention to test alignment improvements
  3. Implement consistency/difference separation to test modality handling

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How can multimodal affective computing models be effectively unified across different task granularities (e.g., aspect-level vs. utterance-level vs. document-level)?
- **Basis in paper**: [explicit] The paper discusses "unification across different granularity—from fine to coarse" as a future direction and notes that different tasks have different granularity levels (MERC and MECPE focus on utterances and speakers, while MSA and MMER focus on sentence-level information).
- **Why unresolved**: Current approaches tend to be task-specific, and there's no established framework for handling multiple granularities within a single unified model.
- **What evidence would resolve it**: Development and empirical validation of a unified architecture that can handle multiple granularities effectively, with performance comparisons against task-specific models.

### Open Question 2
- **Question**: How can external knowledge (such as sentiment lexicons and commonsense knowledge) be optimally integrated into multimodal affective computing models to improve cross-cultural understanding?
- **Basis in paper**: [explicit] The paper identifies "Transfer Learning with External Knowledge Distill" as a future direction, specifically noting that "the expression and perception of emotion also varies across cultures" and that incorporating external knowledge is crucial for understanding emotional expressions within cultural contexts.
- **Why unresolved**: While external knowledge has been used in affective computing, there's no established methodology for integrating diverse knowledge sources across different cultural contexts in multimodal settings.
- **What evidence would resolve it**: Development of a framework that effectively integrates multiple knowledge sources (cultural, linguistic, emotional) and demonstrates improved performance across different cultural datasets.

### Open Question 3
- **Question**: How can less-studied modalities like haptic signals be effectively incorporated into multimodal affective computing systems to enhance user experience and emotional understanding?
- **Basis in paper**: [explicit] The paper discusses "Affective Computing with Less-studied Modalities" as a future direction, specifically mentioning haptic signals as a promising but under-explored modality that "offer immediate feedback and can improve user engagement."
- **Why unresolved**: While haptic signals show promise, there's limited research on how to effectively integrate them with more established modalities (text, audio, visual) in affective computing systems.
- **What evidence would resolve it**: Development of benchmark datasets and evaluation protocols for haptic-based affective computing, along with demonstrated improvements in emotion recognition when haptic signals are incorporated.

## Limitations

- The survey's claims about multimodal fusion mechanisms lack experimental validation
- The effectiveness of pre-trained models for affective computing is presented without comparative empirical evidence
- Limited discussion of practical deployment challenges and real-world limitations of multimodal affective computing systems

## Confidence

- **High confidence**: The classification of tasks and technical dimensions (multitask learning, pre-trained models, knowledge integration, contextual processing) is well-supported by the literature survey
- **Medium confidence**: The mechanisms of multimodal fusion (consistency/difference modeling) are theoretically sound but require empirical validation
- **Low confidence**: The specific effectiveness of different pre-trained model strategies for affective computing lacks comparative evidence

## Next Checks

1. **Experimental validation**: Implement and compare different multimodal fusion approaches (early, late, cross-attention, consistency/difference separation) on the same dataset to verify the claimed effectiveness
2. **Pre-training comparison**: Conduct controlled experiments comparing different pre-trained models (BERT vs T5 vs specialized multimodal models) and transfer learning methods (fine-tuning vs adapters vs prompts) for affective computing tasks
3. **Context necessity test**: Design experiments removing contextual information from MERC tasks to quantify the actual impact of context on performance, identifying scenarios where context may not be beneficial