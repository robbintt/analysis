---
ver: rpa2
title: Insights from the Use of Previously Unseen Neural Architecture Search Datasets
arxiv_id: '2404.02189'
source_url: https://arxiv.org/abs/2404.02189
tags:
- datasets
- dataset
- search
- which
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the need for more diverse and challenging
  datasets in Neural Architecture Search (NAS) research. The authors argue that current
  NAS methods are often overfit to a small set of benchmark datasets and may not generalize
  well to real-world problems.
---

# Insights from the Use of Previously Unseen Neural Architecture Search Datasets

## Quick Facts
- arXiv ID: 2404.02189
- Source URL: https://arxiv.org/abs/2404.02189
- Reference count: 40
- Primary result: NAS methods outperform standard CNNs on most unseen datasets

## Executive Summary
This paper addresses the critical need for more diverse and challenging datasets in Neural Architecture Search (NAS) research. The authors argue that current NAS methods are overfit to a small set of benchmark datasets and may not generalize well to real-world problems. To tackle this issue, they introduce eight new datasets specifically designed for NAS challenges, covering various problem types and complexities. These datasets explore two main concepts: Type-1 problems that experts could solve with basic DL, and Type-2 problems that are nearly impossible for humans to solve without task-specific tools. The results show that NAS methods found the best models for seven out of eight datasets, supporting the usage of NAS methods in finding good models for given datasets.

## Method Summary
The paper introduces eight new datasets specifically designed to test NAS generalization on previously unseen data. The datasets are categorized into Type-1 (problems experts could solve with basic DL) and Type-2 (problems nearly impossible for humans without task-specific tools). CNN baselines (ResNet-18, AlexNet, VGG16, ConvNext, MNASNet, DenseNet, ResNeXt) are trained with fixed hyperparameters (SGD optimizer, learning rate 0.01, momentum 0.9, weight decay 3e-4, cosine annealing). NAS methods (PC-DARTS, DrNAS, Bonsai-Net) are run on each dataset to compare performance. The experimental setup uses consistent train/validation/test splits across all methods to ensure fair comparison.

## Key Results
- NAS methods found the best models for seven out of eight datasets
- Three architectures found by competition submissions were designed for unseen data
- Random search in DARTS search space often outperforms more sophisticated NAS methods like PC-DARTS and DrNAS
- CNN baselines serve as performance floors, with NAS methods consistently outperforming them on most datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NAS methods outperform standard CNNs on most unseen datasets because they can adapt architecture structure to dataset-specific characteristics.
- Mechanism: The NAS search process explores diverse architectural patterns that may exploit subtle, task-specific features which fixed CNN architectures cannot capture.
- Core assumption: The search space contains architectures capable of discovering these dataset-specific patterns, and the search strategy effectively navigates toward them.
- Evidence anchors:
  - [abstract] "NAS methods found the best models for seven out of eight datasets"
  - [section] "NAS found the best model for seven datasets, supporting the usage of NAS methods in finding good models for given datasets."
- Break condition: If the search space is too constrained or the search strategy is ineffective, NAS may fail to find better architectures than fixed CNNs.

### Mechanism 2
- Claim: Type-1 and Type-2 datasets are designed to test NAS generalization by presenting tasks that require inference beyond standard image classification.
- Mechanism: Type-1 datasets (e.g., AddNIST) require models to perform calculations or multi-step reasoning; Type-2 datasets (e.g., Language) encode non-obvious patterns that are difficult for humans but potentially learnable by DL models.
- Core assumption: NAS can discover architectures that implicitly learn these complex patterns without explicit human-designed rules.
- Evidence anchors:
  - [section] "Type-1: A problem an expert could solve themselves... Type-2: A problem that would be almost impossible for a human to solve..."
  - [section] "The research question for this dataset was whether specific spatial patterns of letters were sufficient information for a neural network to identify the authors..."
- Break condition: If the dataset encoding is too abstract or the patterns are not learnable through standard DL architectures, even NAS may fail.

### Mechanism 3
- Claim: Random search in the DARTS search space often outperforms more sophisticated NAS methods on these unseen datasets.
- Mechanism: The DARTS search space may be sufficiently diverse that random sampling frequently finds good architectures, while more complex search strategies may overfit to the search space topology rather than dataset characteristics.
- Core assumption: The performance of architectures in the DARTS space is relatively uncorrelated with the search strategy used, making random search competitive.
- Evidence anchors:
  - [section] "During our random search experiments using the DARTS search space, it is noted that random search often outperforms either PC-DARTS or DrNAS"
  - [section] "These results support findings by Yu et al. [39] that searches using these search spaces often do not significantly outperform random search."
- Break condition: If the search space is carefully designed or the datasets have clear architectural preferences, more sophisticated search strategies should outperform random search.

## Foundational Learning

- Concept: Neural Architecture Search (NAS) fundamentals
  - Why needed here: Understanding how NAS works is essential to grasp why it might outperform fixed architectures on diverse datasets
  - Quick check question: What are the key components of a NAS system (search space, search strategy, performance estimation)?

- Concept: Dataset design for NAS evaluation
  - Why needed here: The paper's contribution centers on creating datasets that test NAS generalization, so understanding dataset design principles is crucial
  - Quick check question: What makes a good NAS benchmark dataset, and how do Type-1 and Type-2 problems differ in their requirements?

- Concept: CNN architecture characteristics
  - Why needed here: The paper compares NAS results against standard CNNs, so understanding their strengths and limitations helps interpret the results
  - Quick check question: How do ResNet, AlexNet, VGG, and DenseNet differ in their architectural approaches and typical use cases?

## Architecture Onboarding

- Component map:
  - Dataset generation code (GitHub repository) -> CNN baseline implementations -> NAS method implementations -> Evaluation framework -> Metadata files

- Critical path:
  1. Clone and understand the dataset generation repository
  2. Run CNN baselines on each dataset to establish performance floors
  3. Implement or obtain NAS method code
  4. Run NAS methods with consistent hyperparameters
  5. Compare results against CNN baselines and competition submissions
  6. Analyze which datasets NAS performs well on and why

- Design tradeoffs:
  - Dataset complexity vs. human interpretability: Type-1 datasets should be solvable by experts but require DL for automation; Type-2 datasets should be difficult for humans but learnable by DL
  - Search space size vs. search efficiency: Larger search spaces may find better architectures but require more computational resources
  - CNN architecture diversity vs. experimental consistency: Using many different architectures provides comprehensive baselines but increases experimental complexity

- Failure signatures:
  - NAS methods performing similarly to random search (suggesting search strategy ineffectiveness)
  - NAS methods performing worse than simple CNN baselines (suggesting search space limitations or overfitting to CIFAR-10/ImageNet)
  - Large performance gaps between development and evaluation datasets (suggesting poor generalization)

- First 3 experiments:
  1. Run ResNet-18 baseline on all eight datasets to establish performance floors
  2. Run random search in DARTS space on AddNIST to verify it outperforms PC-DARTS/DrNAS
  3. Run NAS methods on Language dataset to test performance on encoded linguistic patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do NAS methods perform on datasets that are structurally different from the standard benchmarks (CIFAR-10, ImageNet)?
- Basis in paper: [explicit] The paper argues that current NAS methods are overfit to a small set of benchmark datasets and introduces eight new datasets to test NAS methods on more diverse and challenging problems.
- Why unresolved: While the paper presents some baseline results, it does not extensively test a wide range of NAS methods on these new datasets to draw definitive conclusions about their generalizability.
- What evidence would resolve it: Comprehensive testing of multiple NAS methods on the new datasets, comparing their performance to traditional CNN architectures and analyzing which types of datasets pose the greatest challenges for NAS methods.

### Open Question 2
- Question: Can NAS methods effectively solve Type-1 and Type-2 problems, where Type-1 involves problems that experts could solve with basic DL and Type-2 involves problems nearly impossible for humans without task-specific tools?
- Basis in paper: [explicit] The paper introduces Type-1 and Type-2 datasets specifically to explore these two concepts and evaluate NAS methods' ability to solve them.
- Why unresolved: The paper presents some initial results but does not thoroughly investigate NAS methods' performance on these specific problem types or analyze the underlying reasons for their success or failure.
- What evidence would resolve it: In-depth analysis of NAS methods' performance on Type-1 and Type-2 datasets, including a comparison of their strategies and architectures, and an examination of the specific challenges posed by each problem type.

### Open Question 3
- Question: How does the generalization capability of NAS methods compare to traditional CNN architectures when faced with unseen datasets?
- Basis in paper: [explicit] The paper argues that NAS methods need to be tested on unseen datasets to assess their generalization capability and presents some initial comparisons between NAS and CNN methods.
- Why unresolved: While the paper provides some baseline results, it does not conduct a comprehensive comparison of NAS and CNN methods' generalization performance across the new datasets or analyze the factors contributing to their differences.
- What evidence would resolve it: Extensive testing of multiple NAS and CNN methods on the new datasets, including a thorough analysis of their generalization performance and an investigation of the factors influencing their ability to adapt to unseen data.

## Limitations
- Limited dataset diversity: Eight new datasets may not fully represent the breadth of real-world problems NAS systems will encounter
- Search space constraints: All experiments use DARTS-like search spaces, which may not be optimal for all dataset types
- Computational resource constraints: NAS experiments are resource-intensive, limiting the exploration of full hyperparameter space

## Confidence
- High confidence: NAS methods outperform standard CNNs on most unseen datasets
- Medium confidence: Type-1 and Type-2 dataset design effectively tests NAS generalization
- Medium confidence: Random search often outperforms sophisticated NAS methods in DARTS space

## Next Checks
1. Conduct human expert trials on Type-1 datasets to verify they are solvable by experts but would benefit from automation through NAS-discovered architectures
2. Run NAS experiments using multiple different search spaces (not just DARTS variants) on the same datasets to determine if random search superiority is search space-specific
3. Train NAS-discovered architectures from one dataset on the new unseen datasets to test zero-shot transfer capability and measure overfitting to training datasets