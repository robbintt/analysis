---
ver: rpa2
title: Improving ASR Contextual Biasing with Guided Attention
arxiv_id: '2401.08835'
source_url: https://arxiv.org/abs/2401.08835
tags:
- contextual
- bias
- loss
- biasing
- adapter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a Guided Attention (GA) auxiliary training
  loss to improve the effectiveness and robustness of contextual biasing in automatic
  speech recognition (ASR). The proposed method addresses the challenge of diminishing
  WER reduction as the number of bias phrases increases.
---

# Improving ASR Contextual Biasing with Guided Attention

## Quick Facts
- **arXiv ID**: 2401.08835
- **Source URL**: https://arxiv.org/abs/2401.08835
- **Reference count**: 0
- **One-line primary result**: GA-CTC loss decreases rare vocabulary WER by up to 19.2% compared to baseline and 49.3% compared to vanilla Transducer.

## Executive Summary
This paper introduces Guided Attention (GA) auxiliary training losses to improve contextual biasing effectiveness and robustness in end-to-end ASR. The key challenge addressed is the diminishing WER reduction as the number of bias phrases increases. GA loss operates directly on cross attention weights, teaching the biasing adapter to better align bias phrases with audio frames or text tokens. Experiments on LibriSpeech demonstrate significant WER improvements while maintaining effectiveness as the number of bias phrases grows.

## Method Summary
The method introduces GA-CTC loss as an auxiliary training objective for contextual biasing adapters in Conformer Transducer models. The loss provides supervision on cross attention weights, using CTC-generated phrase index labels to teach the biasing adapter how to align bias phrases with the input sequence. This approach avoids the need for forced alignment while maintaining effectiveness. The method is evaluated on LibriSpeech using rare word lists to generate bias phrases and varying numbers of distractors to test robustness.

## Key Results
- GA-CTC loss reduces rare vocabulary WER by up to 19.2% compared to contextual biasing baseline
- GA-CTC loss achieves up to 49.3% WER reduction compared to vanilla Transducer
- The approach maintains effectiveness as the number of bias phrases increases, mitigating the diminishing returns problem

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GA-CTC loss teaches the biasing adapter to correctly align bias phrases with audio frames or text tokens by leveraging CTC's implicit alignment capabilities.
- Mechanism: GA-CTC operates directly on cross attention weights, using CTC to generate phrase index labels (C) without requiring forced alignment. This label is used as supervision to train the biasing adapter's attention mechanism to attend to the correct bias phrases at the right time steps.
- Core assumption: The cross attention weights in the biasing adapter can be effectively supervised using CTC-generated phrase index sequences to improve alignment between bias phrases and the input sequence.
- Evidence anchors:
  - [abstract] "The proposed GA loss aims to teach the cross attention how to align bias phrases with text tokens or audio frames."
  - [section] "One of the tricky parts of utilizing GA-CE loss is that C Enc is difficult to obtain for audio frames, as it usually involves performing forced alignment using an ASR model. Motivated by this, we propose a simpler alternative LGA-CTC using CTC [1]."
  - [corpus] Weak evidence; no direct mention of CTC alignment in related papers.
- Break condition: If the CTC-generated phrase index sequences are inaccurate or if the biasing adapter cannot learn effective attention patterns from this supervision signal.

### Mechanism 2
- Claim: GA-CTC loss is easier to implement than GA-CE loss because it avoids the need for forced alignment to generate phrase index labels for audio frames.
- Mechanism: Instead of using forced alignment to obtain C Enc (phrase index labels for audio frames), GA-CTC leverages CTC's inherent ability to handle blank symbols (equivalent to <no bias>) to generate these labels programmatically during data loading.
- Core assumption: CTC's handling of blank symbols is sufficient to generate accurate phrase index labels for audio frames without the need for forced alignment.
- Evidence anchors:
  - [section] "One of the tricky parts of utilizing GA-CE loss is that C Enc is difficult to obtain for audio frames, as it usually involves performing forced alignment using an ASR model. Motivated by this, we propose a simpler alternative LGA-CTC using CTC [1]."
  - [section] "It's crucial to understand that <no bias>is the equivalent of <blank> symbol for CTC. This allows the module to attend to <no bias>at any given time due to the rules of CTC [1]."
  - [corpus] Weak evidence; no direct mention of CTC's role in label generation in related papers.
- Break condition: If CTC's handling of blank symbols is insufficient to generate accurate phrase index labels, leading to poor supervision for the biasing adapter.

### Mechanism 3
- Claim: GA-CTC loss helps the biasing adapter maintain its effectiveness as the number of bias phrases (and distractors) increases, by improving its ability to distinguish relevant bias phrases from distractors.
- Mechanism: By providing explicit supervision on the cross attention weights, GA-CTC loss trains the biasing adapter to better align bias phrases with the input sequence, making it more robust to distractors and improving its ability to select the correct bias phrases even when the bias list is large.
- Core assumption: Improved alignment between bias phrases and the input sequence through GA-CTC loss leads to better discrimination between relevant bias phrases and distractors.
- Evidence anchors:
  - [abstract] "the proposed method not only leads to a lower WER but also retains its effectiveness as the number of bias phrases increases."
  - [section] "As we increase N, it is evident that the baseline adapter becomes much less effective... GA loss can mitigate this issue... both GA-CE and GA-CTC loss produce significantly lower B-WER when N > 0 compared to without them."
  - [corpus] Weak evidence; no direct mention of robustness to distractors in related papers.
- Break condition: If the improvement in alignment does not translate to better discrimination between relevant bias phrases and distractors, or if the model still struggles with large bias lists.

## Foundational Learning

- Concept: Cross Attention Mechanism
  - Why needed here: Understanding how cross attention works in the biasing adapter is crucial for grasping how GA-CTC loss provides supervision to improve alignment.
  - Quick check question: What is the role of the query, key, and value matrices in the cross attention mechanism used by the biasing adapter?

- Concept: Connectionist Temporal Classification (CTC)
  - Why needed here: CTC is used to generate phrase index labels without forced alignment, which is a key aspect of the GA-CTC loss.
  - Quick check question: How does CTC handle blank symbols, and why is this relevant to the GA-CTC loss?

- Concept: Transducer Architecture
  - Why needed here: The paper builds upon the Transducer architecture, adding the biasing adapter and GA-CTC loss. Understanding the Transducer is essential for understanding the overall system.
  - Quick check question: What are the main components of the Transducer architecture, and how does the biasing adapter integrate with it?

## Architecture Onboarding

- Component map:
  - Conformer Encoder -> Processes acoustic features into high-level representations
  - RNN Prediction Network -> Generates text sequence predictions in an auto-regressive manner
  - Joint Network -> Combines encoder and prediction network outputs to produce token posteriors
  - Catalog Encoder -> Encodes bias phrases into fixed-size vector representations
  - Audio Biasing Adapter -> Uses cross attention to align bias phrases with audio frames
  - Text Biasing Adapter -> Uses cross attention to align bias phrases with text tokens
  - GA-CTC Loss -> Provides auxiliary supervision on the cross attention weights of the biasing adapters

- Critical path:
  1. Acoustic features are processed by the Conformer Encoder
  2. Bias phrases are encoded by the Catalog Encoder
  3. The Audio Biasing Adapter aligns bias phrases with audio frames using cross attention
  4. The Text Biasing Adapter aligns bias phrases with text tokens using cross attention
  5. The Joint Network combines the context-aware encoder and prediction network outputs
  6. The GA-CTC loss provides supervision on the cross attention weights during training

- Design tradeoffs:
  - GA-CTC vs. GA-CE: GA-CTC is easier to implement (no forced alignment needed) but may be less effective for smaller bias lists
  - Single-head vs. Multi-head Attention: Single-head attention is simpler but may not capture all relevant patterns
  - Embedding Size: Larger embedding sizes may improve representation capacity but increase computational cost

- Failure signatures:
  - High B-WER even with GA-CTC loss: Indicates the model is not effectively learning to align bias phrases with the input sequence
  - Minimal improvement in B-WER as N increases: Suggests the model is not robust to distractors
  - Increased U-WER: Implies the model is over-biasing towards the bias phrases, negatively impacting general recognition

- First 3 experiments:
  1. Train a baseline Conformer Transducer with Contextual Adapter (CA) without GA-CTC loss
  2. Train a Conformer Transducer with Contextual Adapter and GA-CTC loss, varying the weight α in the loss function
  3. Compare the performance of the CA baseline and GA-CTC models on LibriSpeech with different numbers of distractors (N = 0, 100, 1000)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Guided Attention loss vary with different methods of generating the bias phrase list during training, beyond the current dynamic union approach?
- Basis in paper: [explicit] The paper mentions that future experiments can investigate the effect of different training bias list generation methods, such as one proposed in recent research [28].
- Why unresolved: The current experiments use a specific method of generating bias lists (union of rare words from utterances in the same batch), but the impact of alternative methods is not explored.
- What evidence would resolve it: Experiments comparing GA loss performance with different bias list generation strategies, showing variations in WER reduction and robustness against distractors.

### Open Question 2
- Question: What is the optimal balance between Guided Attention loss and Transducer loss for different sizes of bias phrase lists and varying levels of distractors?
- Basis in paper: [inferred] The paper uses a fixed weight (0.5) for the Guided Attention loss in the overall objective function, but does not explore how this weight should be tuned based on the characteristics of the bias phrase list.
- Why unresolved: The impact of different weight combinations on model performance, especially as the number of bias phrases and distractors changes, remains unexplored.
- What evidence would resolve it: A systematic study varying the weight of GA loss across different experimental conditions, demonstrating the optimal configuration for each scenario.

### Open Question 3
- Question: How does the Guided Attention approach perform when applied to other ASR architectures beyond Conformer Transducer with Contextual Adapter, such as RNN-T or attention-based encoder-decoder models?
- Basis in paper: [explicit] The paper demonstrates effectiveness on Conformer Transducer with Contextual Adapter but suggests potential for other methods and architectures.
- Why unresolved: The current experiments are limited to a specific ASR architecture, and it's unclear how well the GA approach generalizes to other model types.
- What evidence would resolve it: Comparative experiments applying GA loss to various ASR architectures, showing performance improvements and robustness across different model types.

## Limitations

- The approach's effectiveness with truly out-of-domain bias phrases and distractors remains unverified, as experiments are limited to LibriSpeech with synthetic distractors
- The computational overhead introduced by GA-CTC loss during both training and inference is not thoroughly analyzed
- The paper doesn't explore the sensitivity of GA-CTC performance to hyperparameters like loss weight α across different bias list sizes

## Confidence

**High Confidence** - The claim that GA-CTC loss reduces rare vocabulary WER by up to 19.2% compared to the baseline contextual biasing adapter is well-supported by the experimental results on LibriSpeech.

**Medium Confidence** - The claim that GA-CTC loss maintains effectiveness as the number of bias phrases increases is supported by the experiments, but the comparison is limited to up to 1000 distractors.

**Medium Confidence** - The claim that GA-CTC is easier to implement than GA-CE because it avoids forced alignment is plausible based on the described mechanism, but lacks concrete evidence comparing implementation complexity.

## Next Checks

1. **Real-World Out-of-Domain Testing**: Conduct experiments using bias phrases and distractors that are semantically and phonemically distinct from the LibriSpeech training data to validate whether the GA-CTC approach maintains its effectiveness when faced with truly novel vocabulary.

2. **Ablation Study on Hyperparameters**: Perform a comprehensive ablation study varying the GA-CTC loss weight α, embedding dimensions, and attention head configurations to understand the sensitivity of the approach to these parameters and identify optimal settings for different bias list sizes.

3. **Production-Ready Evaluation**: Measure and report the additional computational overhead (both memory and latency) introduced by the GA-CTC loss during inference, and evaluate whether the WER improvements justify this overhead in a real-time ASR deployment scenario.