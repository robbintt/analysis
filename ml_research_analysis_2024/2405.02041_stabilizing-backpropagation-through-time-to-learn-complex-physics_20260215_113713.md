---
ver: rpa2
title: Stabilizing Backpropagation Through Time to Learn Complex Physics
arxiv_id: '2405.02041'
source_url: https://arxiv.org/abs/2405.02041
tags:
- learning
- loss
- gradient
- field
- control
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to stabilize the backpropagation through
  time (BPTT) in recurrent learning setups, particularly in physics simulations. The
  authors argue that the gradient field, with its exploding and vanishing updates,
  is a poor choice for optimization, and seek to improve this suboptimal practice.
---

# Stabilizing Backpropagation Through Time to Learn Complex Physics

## Quick Facts
- arXiv ID: 2405.02041
- Source URL: https://arxiv.org/abs/2405.02041
- Reference count: 40
- Primary result: A method to stabilize BPTT in recurrent physics simulations by using gradient stopping through the physics simulator and a combined update vector that uses sign agreement with the original gradient

## Executive Summary
This paper addresses the exploding and vanishing gradient problems in backpropagation through time (BPTT) for recurrent learning in physics simulations. The authors propose a novel method that exploits the balanced gradient flows in physics simulators while addressing the rotational issues that arise from modified backpropagation. By stopping gradient feedback through the neural network portion but allowing it through the physics simulator, and then combining this with the original gradient based on sign agreement, the method achieves superior performance on complex control tasks while maintaining computational feasibility.

## Method Summary
The proposed method follows from two key principles: physics simulators have balanced gradient flows due to conservation laws, and certain modifications to backpropagation leave global minima unchanged while removing non-global critical points. The approach uses gradient stopping to propagate feedback only through the physics simulator while still considering the full time trajectory. This is combined with a sign-based update mechanism that sets components to zero when the modified and original gradients disagree, counteracting the rotational character of the modified field. The method is evaluated against regular gradient descent and the modified update alone on three control tasks of increasing complexity.

## Key Results
- The combined update method (C) outperforms both regular gradient (R) and modified update (M) on all three control tasks, especially as task complexity increases
- Contact-rich scenarios remain a limitation, with the stopped update (S) method failing to converge in such cases
- Runtime analysis shows the combined update (C) requires approximately double the computation time compared to regular gradient (R)
- The method shows minimal hyperparameter sensitivity with consistent performance across learning rates 0.0001 to 0.01

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Gradient stopping through physics simulators improves optimization by reducing exploding gradient magnitudes.
- **Mechanism:** By stopping backpropagation through the neural network part while allowing it through the physics simulator, the method exploits the fact that physical systems have well-behaved gradient flows due to conservation laws. This reduces the exponential growth of gradient magnitudes that occurs with repeated multiplication in recurrent systems.
- **Core assumption:** Physics simulators have balanced gradient flows compared to neural networks, and stopping feedback through the network doesn't lose temporal coherence since the physics path remains intact.
- **Evidence anchors:**
  - [abstract] "The alternative vector field we propose follows from two principles: physics simulators, unlike neural networks, have a balanced gradient flow"
  - [section] "This has a positive effect on the magnitude of the update because in physical systems, the rate of change over time is typically limited, for instance by conservation laws"
  - [corpus] Weak evidence - no direct corpus support found for this specific mechanism
- **Break condition:** This mechanism breaks if the physics simulator itself has unstable gradients or if the system lacks conservation properties that limit rate of change.

### Mechanism 2
- **Claim:** The modified vector field preserves positions of original minima while removing non-global critical points.
- **Mechanism:** By modifying backpropagation to stop feedback through the network, the method creates a new vector field where global minima positions remain unchanged but non-global critical points (saddle points, non-global local minima) are removed because their gradient becomes non-zero.
- **Core assumption:** Modifications that only change the gradient of the final state while preserving the loss function structure will maintain global minima while eliminating non-global critical points.
- **Evidence anchors:**
  - [section] "We distinguish these points depending on whether or not F(x0, θ) = y. If yes, such a critical point is a global minimum and the modified update will also vanish... If not, such a critical point is not a global minimum... For these, dθF is 0 while d|θF needs not to be and neither needs d|θL. Therefore, these points are removed through such a modification."
  - [corpus] Weak evidence - no direct corpus support found for this specific mechanism
- **Break condition:** This mechanism breaks if the modification inadvertently moves or eliminates global minima, or if the distinction between global and non-global critical points doesn't hold in complex landscapes.

### Mechanism 3
- **Claim:** The combined update vector counteracts rotation in the modified field by using sign agreement with the original gradient.
- **Mechanism:** The modified backpropagation creates a rotational vector field that can prevent convergence. The combined update sets components to zero when the modified and original gradients have opposite signs, effectively using the original gradient's direction near minima while maintaining the balanced updates of the modified field.
- **Core assumption:** Near global minima, the original gradient provides the correct direction, and combining this with the balanced updates from the modified field through sign-based selection improves convergence.
- **Evidence anchors:**
  - [section] "Since the regular gradient field gives the correct direction near minima, our final algorithm performs only updates in those components that coincide in sign with the original gradient to counteract rotation."
  - [section] "uj = dθj L if sign(dθj L) = sign(d|θj L) 0 otherwise"
  - [corpus] Weak evidence - no direct corpus support found for this specific mechanism
- **Break condition:** This mechanism breaks if the original gradient direction is unreliable near minima, or if sign agreement becomes too restrictive and prevents necessary updates.

## Foundational Learning

- **Concept:** Backpropagation Through Time (BPTT) and exploding/vanishing gradient problems
  - Why needed here: The paper directly addresses BPTT's gradient instability in recurrent physics simulations
  - Quick check question: What causes the exploding gradient problem in BPTT and how does it affect optimization?

- **Concept:** Vector field optimization and gradient field properties
  - Why needed here: The method fundamentally changes the optimization vector field from the standard gradient, requiring understanding of gradient field properties like rotation-freeness
  - Quick check question: Why is the rotation-free property of gradient fields important for optimization, and what happens when this property is lost?

- **Concept:** Physics simulators and conservation laws
  - Why needed here: The method relies on the well-behaved gradient flows in physics simulators due to conservation laws that limit rate of change
  - Quick check question: How do conservation laws in physical systems contribute to balanced gradient flows?

## Architecture Onboarding

- **Component map:**
  - Physics simulator (S) <- Neural network controller (N) <- State
  - Physics simulator (S) -> Next state
  - Loss function <- Final state, Target state

- **Critical path:**
  1. Forward pass through N and S unrolled over time steps
  2. Loss computation at final or accumulated time steps
  3. Backward pass with one of three gradient computation methods
  4. Parameter update via Adam optimizer

- **Design tradeoffs:**
  - Regular gradient (R): Computationally efficient but suffers from exploding/vanishing gradients
  - Modified update (M): Better gradient balance but introduces rotation that can prevent convergence
  - Combined update (C): Addresses rotation issues but requires two backpropagation passes
  - Runtime/memory: (S) fastest, (M) and (R) similar, (C) roughly double the time

- **Failure signatures:**
  - Diverging loss curves (especially for (S) in contact tasks)
  - Oscillating updates without convergence (rotation issues in (M))
  - Extremely large or small gradient magnitudes (exploding/vanishing problems in (R))
  - Inconsistent performance across runs (sensitivity to hyperparameters)

- **First 3 experiments:**
  1. Implement the toy example (polynomial controller with identity simulator) and visualize gradient fields for (R), (M), and (C)
  2. Test the cart pole swing-up task with 1-2 poles to observe performance differences between methods
  3. Run the quantum control task with different target states to evaluate scalability of the approach

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions beyond noting that contact-rich scenarios are a current limitation that requires further investigation.

## Limitations
- Contact-rich scenarios remain problematic, with the stopped update method failing to converge in such cases
- The combined update method requires approximately double the computation time compared to regular gradient descent
- The method's performance on high-dimensional physics simulations beyond the tested control tasks remains unverified

## Confidence

**High Confidence Claims**:
- The regular gradient method (R) suffers from exploding and vanishing gradient problems in recurrent learning setups (demonstrated across all three experiments)
- The modified update method (M) improves gradient balance but introduces rotational issues that can prevent convergence
- The combined update method (C) effectively addresses both exploding gradients and rotational problems, showing superior performance on complex tasks

**Medium Confidence Claims**:
- The theoretical justification for why physics simulators have balanced gradient flows (based on conservation laws) is sound but not extensively validated empirically
- The claim that the modified vector field preserves global minima while removing non-global critical points is theoretically supported but would benefit from more rigorous proof or empirical validation

**Low Confidence Claims**:
- The assertion that this method will be particularly useful for physics simulations in reinforcement learning contexts is speculative and not directly tested in the paper

## Next Checks
1. **Cross-domain validation**: Test the method on physics simulations from different domains (e.g., fluid dynamics, rigid body dynamics with contact) to assess generalization beyond the current control tasks.

2. **Gradient field visualization**: Create detailed visualizations of the gradient fields for all three methods across different stages of training to empirically validate the claims about exploding gradients, rotation, and the effectiveness of the combined update.

3. **Scalability analysis**: Evaluate the method on progressively more complex control tasks (e.g., cart pole with more poles, higher-dimensional quantum systems) and measure both performance and computational overhead to determine practical scalability limits.