---
ver: rpa2
title: 'VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with
  Large Language Models'
arxiv_id: '2402.18374'
source_url: https://arxiv.org/abs/2402.18374
tags:
- entity
- type
- knowledge
- cell
- verifi
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VerifiNER, a post-hoc verification framework
  that leverages external knowledge and large language models (LLMs) to identify and
  correct errors in named entity recognition (NER) predictions. The core idea is to
  use knowledge-grounded reasoning to verify entity spans and types, followed by contextual
  relevance verification to select the most appropriate entity.
---

# VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models

## Quick Facts
- arXiv ID: 2402.18374
- Source URL: https://arxiv.org/abs/2402.18374
- Authors: Seoyeon Kim, Kwangwook Seo, Hyungjoo Chae, Jinyoung Yeo, Dongha Lee
- Reference count: 21
- Primary result: Knowledge-grounded LLM reasoning improves NER precision by up to 20.03% across biomedical models

## Executive Summary
VerifiNER is a post-hoc verification framework that leverages external knowledge and large language models to identify and correct errors in named entity recognition predictions. The approach uses knowledge-grounded reasoning to verify both entity spans and types, followed by contextual relevance verification to select the most appropriate entity. By assuming gold entities are adjacent to predictions, VerifiNER can correct errors that internal model adjustments cannot fix. The framework is model-agnostic and shows substantial performance gains on biomedical datasets while maintaining robust performance in out-of-distribution and low-resource settings.

## Method Summary
VerifiNER takes NER model predictions and applies a three-stage verification process. First, it extracts candidate spans around predicted entities and retrieves knowledge from an external knowledge base (UMLS). Second, it uses LLM reasoning to generate knowledge-grounded evidence and reassign entity types to candidates. Third, it employs consistency voting across multiple reasoning paths to select the most contextually relevant entity. The framework adjusts initial predictions based on LLM output scores and is designed to improve precision without significant recall trade-offs. It works as a post-processing layer that can be applied to any NER model.

## Key Results
- Achieved up to 20.03% increase in precision across biomedical NER models
- Demonstrated model-agnostic improvements across different NER architectures
- Showed robustness in out-of-distribution and low-resource settings
- Maintained precision gains while minimizing recall trade-offs

## Why This Works (Mechanism)

### Mechanism 1: Knowledge-grounded reasoning for error correction
The framework uses external knowledge to provide factual evidence for verifying predicted entity spans and types, then employs LLM reasoning to bridge the gap between raw knowledge and model predictions. This works because the gold entity span is likely adjacent to the predicted span and can be found within a small token window.

### Mechanism 2: Multiple reasoning paths with consistency voting
The framework samples multiple reasoning paths and uses majority voting to select the most contextually appropriate entity, mimicking human consensus building. This improves accuracy by having diverse reasoning converge on the correct answer when knowledge-grounded evidence is provided.

### Mechanism 3: Post-hoc verification without retraining
The framework takes predictions from any NER model and verifies them using external knowledge and LLMs, achieving improvements across different model types. This works because errors in NER predictions are systematic enough to be corrected by knowledge-grounded verification.

## Foundational Learning

- **Named Entity Recognition (NER) task definition and error types**
  - Why needed here: Understanding the task and error types is essential for designing effective verification mechanisms
  - Quick check question: What are the main error types in NER and how do they differ between false positives and false negatives?

- **Knowledge base (KB) integration and retrieval**
  - Why needed here: The framework relies on external knowledge for factuality verification of entity predictions
  - Quick check question: How does the framework retrieve and use knowledge from UMLS to verify entity spans and types?

- **Large Language Model (LLM) reasoning and in-context learning**
  - Why needed here: LLMs are used for knowledge-grounded reasoning and contextual relevance verification
  - Quick check question: What role does the LLM play in generating knowledge-grounded evidence and selecting the most appropriate entity?

## Architecture Onboarding

- **Component map**: Input NER predictions → Span Factuality Verification → Type Factuality Verification → Contextual Relevance Verification → Output revised predictions
- **Critical path**: Input → Span Factuality → Type Factuality → Contextual Relevance → Output
- **Design tradeoffs**: KB vs. parametric knowledge (external KB provides more reliable domain knowledge but requires retrieval); Multiple reasoning paths (improves accuracy but increases computational cost); Span-first approach (ensures type assignment is based on correct span, but adds complexity)
- **Failure signatures**: No candidates found in KB (Span verification fails); LLM generates NONE for all candidates (Type verification fails); Inconsistent voting results (Contextual verification fails); High computational cost (may not be suitable for real-time applications)
- **First 3 experiments**: 1) Verify single entity with known gold answer to test KB retrieval and reasoning; 2) Compare performance with and without consistency voting on a small dataset; 3) Test model-agnostic capability by applying to different NER models on same dataset

## Open Questions the Paper Calls Out

- **Open Question 1**: How does VerifiNER's performance scale with different LLM model sizes and capabilities?
  - Basis in paper: The paper uses ChatGPT (gpt-3.5-turbo-1106) and acknowledges computational resources and API costs as limiting factors
  - Why unresolved: The paper does not explore performance differences across various LLM sizes or capabilities
  - What evidence would resolve it: Comparative experiments testing VerifiNER with different LLM models to measure performance trade-offs between accuracy and computational efficiency

- **Open Question 2**: Can VerifiNER be effectively adapted to domains outside of biomedical NER?
  - Basis in paper: The paper acknowledges its current limitation to biomedical domains and suggests potential applications in legal or scientific domains as future work
  - Why unresolved: The paper only demonstrates effectiveness on biomedical datasets without testing other domains
  - What evidence would resolve it: Experiments applying VerifiNER to different knowledge-intensive domains and measuring performance compared to domain-specific baselines

- **Open Question 3**: What is the impact of knowledge base quality and coverage on VerifiNER's performance?
  - Basis in paper: The paper augments UMLS knowledge base with dataset entities and mentions that over 90% of gold entities exist in the knowledge source
  - Why unresolved: The paper doesn't systematically investigate how different knowledge base qualities or coverage levels affect performance
  - What evidence would resolve it: Controlled experiments varying knowledge base completeness, relevance, and quality to measure their impact on VerifiNER's accuracy and error correction capabilities

## Limitations

- Heavy dependency on knowledge base coverage and quality, with fundamental limitations for entities not well-represented in UMLS
- Significant computational overhead from multi-step verification process limiting real-time application potential
- Unproven generalizability beyond biomedical domains where the span-first assumption may not hold

## Confidence

- **High Confidence**: Model-agnostic design principle and basic verification architecture are well-supported by experimental results showing consistent precision improvements across different NER models on biomedical datasets
- **Medium Confidence**: Effectiveness of knowledge-grounded reasoning for error correction is supported by experimental results, but specific mechanisms by which LLM reasoning improves over direct knowledge lookup are not fully characterized
- **Low Confidence**: Claims about out-of-distribution and low-resource robustness are based on limited experimental scenarios and require further investigation

## Next Checks

1. **Cross-Domain Evaluation**: Test VerifiNER on non-biomedical NER datasets (e.g., news, social media) to assess generalizability beyond the biomedical domain where knowledge base coverage may differ significantly

2. **Knowledge Base Coverage Analysis**: Quantify the percentage of gold entities that can be retrieved from UMLS within the assumed token window to measure the practical limitations of the span-first approach

3. **Real-Time Performance Assessment**: Measure end-to-end latency of the verification pipeline and compare against direct NER predictions to determine practical deployment thresholds for different use cases