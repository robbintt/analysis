---
ver: rpa2
title: Axioms for AI Alignment from Human Feedback
arxiv_id: '2405.14758'
source_url: https://arxiv.org/abs/2405.14758
tags:
- ranking
- linear
- will
- which
- rule
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning reward functions from
  human feedback in reinforcement learning from human feedback (RLHF). The authors
  frame this as a linear social choice problem, where the goal is to aggregate individual
  rankings over candidates into a collective ranking induced by a linear reward function.
---

# Axioms for AI Alignment from Human Feedback

## Quick Facts
- **arXiv ID**: 2405.14758
- **Source URL**: https://arxiv.org/abs/2405.14758
- **Reference count**: 40
- **Primary result**: Widely applied RLHF methods fail to meet basic social choice axioms, while Leximax Copeland subject to PO offers stronger theoretical guarantees

## Executive Summary
This paper establishes a theoretical framework for evaluating reinforcement learning from human feedback (RLHF) methods by connecting them to social choice theory. The authors identify that common RLHF approaches like the Bradley-Terry-Luce model violate fundamental axioms from social choice theory, specifically Pareto optimality and pairwise majority consistency. They propose a new aggregation rule called Leximax Copeland subject to PO that satisfies these desirable properties while maintaining practical applicability. This work provides a principled way to assess and compare RLHF methods beyond empirical performance metrics.

## Method Summary
The authors frame RLHF as a linear social choice problem where individual human rankings over candidate policies must be aggregated into a collective ranking induced by a linear reward function. They analyze existing methods like the Bradley-Terry-Luce model through the lens of social choice axioms and demonstrate their failure to satisfy Pareto optimality (PO) and pairwise majority consistency (PMC). The proposed solution, Leximax Copeland subject to PO, is designed to satisfy these axioms while also maintaining winner monotonicity and majority consistency. The method works by iteratively selecting candidates that maximize the minimum pairwise win margin while respecting Pareto-optimal constraints.

## Key Results
- Loss-based RLHF methods including Bradley-Terry-Luce violate Pareto optimality and pairwise majority consistency
- The proposed Leximax Copeland subject to PO satisfies PO, PMC, majority consistency, and winner monotonicity
- Theoretical proofs establish that no linear aggregation rule can simultaneously satisfy all desirable axioms without restrictions

## Why This Works (Mechanism)
The framework works by translating the RLHF problem into a social choice context where established axioms can be applied. By viewing human feedback as individual rankings that need aggregation, the method can leverage social choice theory to derive principled aggregation rules. The key insight is that desirable properties like Pareto optimality (if everyone prefers A to B, the aggregated ranking should too) and pairwise majority consistency (if a majority prefers A to B in every comparison, the aggregated ranking should reflect this) are fundamental to rational aggregation. The Leximax Copeland approach satisfies these by systematically considering pairwise comparisons while maintaining Pareto efficiency through constraint satisfaction.

## Foundational Learning
- **Social Choice Theory**: Framework for aggregating individual preferences into collective decisions - needed to establish theoretical foundations for evaluating RLHF methods
- **Pareto Optimality**: If all agents prefer A to B, then the aggregated ranking must place A above B - critical axiom that existing RLHF methods violate
- **Pairwise Majority Consistency**: If majority prefers A to B in all comparisons, aggregated ranking must reflect this - fundamental fairness property
- **Linear Reward Functions**: Assumption that rewards can be represented as weighted sums of features - simplifies aggregation but may limit expressiveness
- **Bradley-Terry-Luce Model**: Probabilistic model for pairwise comparisons - commonly used in RLHF but fails key axioms
- **Copeland Score**: Count of pairwise victories minus defeats - basis for the proposed aggregation method

## Architecture Onboarding

**Component Map**
Human Feedback -> Individual Rankings -> Linear Reward Function -> Aggregated Policy Ranking -> RL Policy Selection

**Critical Path**
1. Collect pairwise human feedback on candidate policies
2. Generate individual preference rankings
3. Apply aggregation rule (Leximax Copeland subject to PO)
4. Derive linear reward function from aggregated ranking
5. Use reward function for RL policy optimization

**Design Tradeoffs**
- Computational complexity vs. theoretical guarantees (Leximax Copeland is more expensive but satisfies more axioms)
- Linear vs. non-linear reward functions (linear is tractable but may be limiting)
- Strict axiom satisfaction vs. empirical performance (theoretical guarantees may not translate to better practical results)

**Failure Signatures**
- Aggregation produces intransitive preferences
- Pareto-optimal constraints cannot be satisfied
- Pairwise majority preferences are violated in final ranking
- Computational intractability for large candidate sets

**3 First Experiments**
1. Verify that Bradley-Terry-Luce violates PO on synthetic preference data
2. Test Leximax Copeland subject to PO on small-scale RLHF benchmark
3. Compare computational requirements of different aggregation methods

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes linear reward functions which may not capture complex reward landscapes
- Proposed method may be computationally intractable for large-scale applications
- Limited empirical validation beyond theoretical proofs
- Framework may not generalize well to non-linear or high-dimensional reward functions

## Confidence
- **High confidence**: Theoretical connections between RLHF and social choice theory, proof that common RLHF methods violate PO and PMC axioms
- **Medium confidence**: Practical relevance of proposed axioms for real-world RLHF applications, computational feasibility of Leximax Copeland subject to PO
- **Low confidence**: Impact of non-linear reward functions on proposed framework, scalability to complex RLHF scenarios

## Next Checks
1. Implement and benchmark Leximax Copeland subject to PO on standard RLHF datasets to assess practical feasibility
2. Test the framework with non-linear reward function approximations to evaluate robustness
3. Conduct user studies to validate whether proposed axioms align with human preferences in practical RLHF scenarios