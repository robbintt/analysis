---
ver: rpa2
title: What Have We Achieved on Non-autoregressive Translation?
arxiv_id: '2405.12788'
source_url: https://arxiv.org/abs/2405.12788
tags:
- translation
- machine
- evaluation
- sentence
- association
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Non-autoregressive translation (NAT) offers faster decoding than\
  \ autoregressive methods (AT) but suffers from lower translation quality due to\
  \ independence assumptions between target tokens. This study comprehensively compares\
  \ four representative NAT methods\u2014MgMO, CTC, DAT, and CMLM\u2014against AT\
  \ using rule-based, model-based, and GPT4-based metrics, along with human evaluation\
  \ under the MQM framework."
---

# What Have We Achieved on Non-autoregressive Translation?

## Quick Facts
- arXiv ID: 2405.12788
- Source URL: https://arxiv.org/abs/2405.12788
- Authors: Yafu Li; Huajian Zhang; Jianhao Yan; Yongjing Yin; Yue Zhang
- Reference count: 39
- Primary result: Comprehensive comparison of four NAT methods against AT reveals performance gaps and distinct error patterns

## Executive Summary
Non-autoregressive translation offers faster decoding through parallel token generation but suffers from lower translation quality due to independence assumptions between target tokens. This study comprehensively compares four representative NAT methods—MgMO, CTC, DAT, and CMLM—against AT using rule-based, model-based, and GPT4-based metrics, along with human evaluation under the MQM framework. Results show that while DAT achieves competitive BLEU scores, all NAT methods underperform AT on model-based and GPT4-based metrics, with significant differences in error patterns such as mistranslation, omission, and n-gram repetition. Human evaluation reveals that NAT models without explicit dependency modeling (MgMO, CTC) produce more accuracy errors, while those with weak dependency modeling (DAT) suffer from n-gram repetition. Experiments on cross-domain and compositional generalization, as well as robustness to input perturbations, indicate that explicit dependency modeling is crucial for generating human-like translations and generalizing to out-of-distribution data, but weak dependency provides stronger robustness to input noise.

## Method Summary
The study compares four NAT methods (MgMO, CTC, DAT, CMLM) against AT on WMT16 En→Ro and WMT21 De→En datasets using Transformer base architecture. All models use 6 encoder/decoder layers, 8 attention heads, hidden dim 512, and feed-forward dim 2048. Knowledge distillation is applied to most NAT models except DAT and CMLM. DAT uses directed acyclic graph modeling with one-layer transition attention, MgMO employs multi-granularity optimization, CTC uses connectionist temporal classification, and CMLM implements iterative refinement. Evaluation includes BLEU, chrF, COMET, BLEURT, GEMBA metrics, and MQM-based human evaluation covering accuracy, fluency, and terminology errors.

## Key Results
- DAT achieves competitive BLEU scores but all NAT methods underperform AT on model-based and GPT4-based metrics
- NAT models without explicit dependency modeling (MgMO, CTC) produce more accuracy errors and omissions
- DAT suffers from n-gram repetition despite weak dependency modeling providing stronger robustness to input perturbations
- Knowledge distillation significantly impacts NAT performance, with models without explicit dependency modeling most affected

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Explicit dependency modeling is crucial for generating human-like translations and generalizing to out-of-distribution sequences
- Mechanism: AT models build strong inter-token dependencies through multi-layer transformer blocks that encode the entire generation history, while NAT models either ignore or weakly model these dependencies
- Core assumption: Strong inter-token dependencies capture the natural flow and coherence of human language
- Evidence anchors:
  - [abstract]: "explicitly modeling dependencies is crucial for generating natural language and generalizing to out-of-distribution sequences"
  - [section]: "explicit dependency modeling is crucial for generating human-like languages and generalizing to out-of-distribution samples"
  - [corpus]: Weak - corpus neighbors discuss various NAT methods but don't directly address the dependency modeling claim
- Break condition: If dependency modeling becomes computationally prohibitive, forcing NAT to remain fully non-autoregressive

### Mechanism 2
- Claim: Weak dependency modeling provides stronger robustness to input perturbations
- Mechanism: Models without explicit dependency (MgMO, CTC) are less affected by input noise because they don't accumulate and propagate errors through strong dependencies
- Core assumption: Strong dependencies introduce exposure bias where early errors cascade through the generation process
- Evidence anchors:
  - [abstract]: "weak dependency provides stronger robustness to input perturbations, as it is less affected by exposure bias"
  - [section]: "explicit dependency generation may introduce exposure bias...making them susceptible to input perturbations"
  - [corpus]: Weak - corpus neighbors don't directly address input perturbation robustness
- Break condition: If input noise becomes severe enough to overwhelm the weak dependency modeling's ability to maintain coherence

### Mechanism 3
- Claim: Knowledge distillation is necessary for NAT models without explicit dependency modeling
- Mechanism: KD reduces data complexity by providing cleaner target sequences that are easier for NAT to model without strong dependencies
- Core assumption: Raw parallel data contains too much ambiguity for NAT models without strong dependency modeling to handle effectively
- Evidence anchors:
  - [section]: "The vanilla NAT models suffer a decrease of more than 7 BLEU points without KD. For strong NAT methods such as MgMO and CTC...the BLEU scores decrease by more than 2 and 3 points"
  - [abstract]: "models without explicit dependency modeling (MgMO and CTC) suffer the most mistranslation and omission errors"
  - [corpus]: Weak - corpus neighbors discuss KD but don't directly connect it to dependency modeling
- Break condition: If KD becomes unavailable or if models can learn to handle data complexity without it

## Foundational Learning

- Concept: Conditional independence assumption in NAT
  - Why needed here: Understanding why NAT models struggle with translation quality compared to AT
  - Quick check question: What is the key difference in how AT and NAT factor the conditional probability p(y|x)?

- Concept: Exposure bias in sequence generation
  - Why needed here: Explains why strong dependency models are more vulnerable to input perturbations
  - Quick check question: How does exposure bias manifest differently in AT versus NAT models during inference?

- Concept: Multi-modality in translation
  - Why needed here: Why NAT models need techniques like knowledge distillation or dependency modeling
  - Quick check question: Why is translation considered a multi-modal problem and how does this challenge NAT models?

## Architecture Onboarding

- Component map:
  Encoder -> Length Prediction (NAT only) -> Parallel Token Generation (fully NAT) or Sequential Generation (AT) -> Output Projection

- Critical path:
  1. Source encoding
  2. Target length prediction (NAT only)
  3. Parallel token generation (fully NAT) or sequential generation (AT)
  4. Output projection and vocabulary selection

- Design tradeoffs:
  - Decoding speed vs. translation quality: NAT offers parallel decoding but sacrifices quality
  - Model complexity vs. performance: More complex dependency modeling improves quality but may reduce efficiency
  - Knowledge distillation dependency: Some NAT methods heavily rely on KD, others less so

- Failure signatures:
  - N-gram repetition: Indicates weak dependency modeling (especially in DAT)
  - Omission errors: Suggests inadequate target-side information utilization
  - Spelling errors: Points to latent alignment issues (CTC)
  - Non-translation: Indicates fundamental generation failures (CMLM)

- First 3 experiments:
  1. Compare AT vs. NAT performance on a small dataset with controlled sequence lengths
  2. Test NAT models with and without knowledge distillation on the same dataset
  3. Evaluate model robustness by adding different types of input noise to test sets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different knowledge distillation methods compare in effectiveness for non-autoregressive translation?
- Basis in paper: [explicit] The paper notes that "all fully non-autoregressive methods except DAT and CMLM suffer more from training without distillation" and that "the performance of DAT and CMLM is as similarly affected as the AT counterpart."
- Why unresolved: The paper does not explore or compare different knowledge distillation techniques or their specific impacts on various NAT methods.
- What evidence would resolve it: A systematic comparison of multiple knowledge distillation methods (e.g., back-translation, tagged back-translation, noised back-translation) across different NAT architectures and datasets.

### Open Question 2
- Question: What is the optimal balance between explicit dependency modeling and decoding efficiency in non-autoregressive translation?
- Basis in paper: [explicit] The paper states "Future research on NAT should focus on how to consolidate explicit language dependency while maintaining decoding efficiency."
- Why unresolved: While the paper identifies this as a key challenge, it does not experimentally determine the optimal trade-off point or provide concrete architectural recommendations.
- What evidence would resolve it: A comprehensive study varying the degree of explicit dependency modeling (e.g., number of attention layers, dependency graph complexity) and measuring both translation quality and decoding speed across multiple NAT methods.

### Open Question 3
- Question: How does the performance gap between non-autoregressive and autoregressive translation change with increasing model scale?
- Basis in paper: [inferred] The paper compares NAT and AT methods but focuses on Transformer-Base configurations and does not explore scaling effects.
- Why unresolved: The study does not investigate how model size impacts the NAT-AT performance gap, particularly in the context of large language models.
- What evidence would resolve it: Scaling experiments comparing NAT and AT methods across different model sizes (e.g., Base, Large, XL) on multiple translation tasks, measuring both absolute performance and relative performance gaps.

### Open Question 4
- Question: What specific architectural modifications can reduce n-gram repetition in non-autoregressive translation without sacrificing decoding speed?
- Basis in paper: [explicit] The paper identifies n-gram repetition as a key weakness of DAT and proposes DAT* as a partial solution, but notes it "is limited in a one-step local transition foundation and cannot fundamentally resolve n-gram repetition."
- Why unresolved: While the paper identifies the problem and provides a partial solution, it does not explore the full space of potential architectural modifications or identify optimal solutions.
- What evidence would resolve it: Systematic exploration of architectural modifications (e.g., different attention mechanisms, memory structures, decoding strategies) and their effects on n-gram repetition, translation quality, and decoding speed.

## Limitations

- Data and Evaluation Limitations: The study relies on standard benchmarks which may not fully capture NAT performance across diverse language pairs and domains, and human evaluation sample size and domain coverage remain unclear.
- Implementation Variability: Several critical implementation details are underspecified, particularly for MgMO and DAT, which could lead to performance variations across implementations.
- Knowledge Distillation Dependency: Heavy reliance on KD for most NAT models raises questions about whether observed performance differences reflect inherent model capabilities or the quality of distillation data.

## Confidence

**High Confidence**: The fundamental tradeoff between decoding speed and translation quality in NAT vs AT models is well-established. The experimental methodology using multiple evaluation metrics (BLEU, chrF, COMET, BLEURT, GEMBA) provides robust comparative analysis.

**Medium Confidence**: The characterization of error patterns (mistranslation, omission, n-gram repetition) and their association with different NAT architectures is supported by comprehensive metrics and human evaluation. However, the exact mechanisms and error propagation paths could benefit from additional ablation studies.

**Low Confidence**: The generalization claims regarding cross-domain and compositional generalization require more extensive validation across diverse language pairs and domains. The robustness findings to input perturbations, while methodologically sound, may not capture all types of real-world noise.

## Next Checks

**Validation Check 1**: Conduct ablation studies on the knowledge distillation component by training NAT models with varying quality of distillation data (from different AT model strengths) to isolate the contribution of KD from inherent model capabilities.

**Validation Check 2**: Extend the error pattern analysis to additional language pairs beyond En→Ro and De→En to verify whether the observed error distributions generalize across different language families and translation directions.

**Validation Check 3**: Implement controlled experiments with different types of input noise (synthetic errors, real-world typos, grammatical errors) to systematically validate the robustness claims and understand the exposure bias mechanism across different NAT architectures.