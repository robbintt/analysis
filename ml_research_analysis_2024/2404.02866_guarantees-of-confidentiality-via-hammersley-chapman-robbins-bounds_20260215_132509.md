---
ver: rpa2
title: Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds
arxiv_id: '2404.02866'
source_url: https://arxiv.org/abs/2404.02866
tags:
- bounds
- features
- input
- vector
- perturbation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper examines privacy in deep neural network inference by
  adding noise to the features (activations) in the last layers before the classifier.
  The method quantifies confidentiality using Hammersley-Chapman-Robbins (HCR) bounds,
  which lower bound the variance of any unbiased estimator reconstructing the inputs
  from noisy features.
---

# Guarantees of confidentiality via Hammersley-Chapman-Robbins bounds

## Quick Facts
- arXiv ID: 2404.02866
- Source URL: https://arxiv.org/abs/2404.02866
- Authors: Kamalika Chaudhuri; Chuan Guo; Laurens van der Maaten; Saeed Mahloujifar; Mark Tygert
- Reference count: 3
- Primary result: HCR bounds can quantify privacy when adding noise to neural network features, but are only effective for small networks/datasets

## Executive Summary
This paper proposes using Hammersley-Chapman-Robbins (HCR) bounds to quantify confidentiality when adding noise to the last-layer activations (features) of neural networks before classification. The method provides a data-driven lower bound on the variance of any unbiased estimator trying to reconstruct inputs from noisy features. Experiments show the approach is reasonably effective for small networks on MNIST and CIFAR-10, but the bounds become too weak to guarantee privacy for large-scale networks like ResNet-18 on ImageNet-1000.

## Method Summary
The method adds Gaussian noise to the penultimate layer features of a neural network, then uses HCR bounds to quantify the resulting confidentiality. The HCR bound is computed using the pseudoinverse of the Jacobian of features with respect to inputs, approximated via LSQR iterations. This provides a lower bound on the variance of any unbiased estimator reconstructing the input from the noisy features.

## Key Results
- HCR bounds effectively quantify confidentiality for small networks on MNIST and CIFAR-10
- Bounds become too weak to guarantee privacy for large-scale ImageNet classification with standard architectures
- Classification accuracy remains relatively high when using HCR bounds for privacy, compared to differential privacy approaches
- The approach requires additional privacy methods (like limiting feature vector size) for large-scale applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding Gaussian noise to last-layer activations increases variance of unbiased input reconstruction estimators
- Mechanism: HCR bounds lower bound variance of any unbiased input reconstruction estimator, quantifying increased reconstruction difficulty
- Core assumption: Noise is i.i.d. and follows a known distribution
- Evidence anchors:
  - [abstract] "Lower bounding the variance of every possible unbiased estimator of the inputs quantifies the confidentiality arising from such added noise."
  - [section] "The Hammersley-Chapman-Robbins (HCR) bounds... provide easily interpretable, tight data-driven guarantees on confidentiality."
- Break condition: Unknown or adversarial noise distributions invalidate HCR bound applicability

### Mechanism 2
- Claim: HCR bounds are more effective for small networks where feature dimensionality is comparable to input dimensionality
- Mechanism: Tighter bounds occur when feature space dimensionality is not excessively high relative to input
- Core assumption: Network is not too deep and feature space is manageable relative to input
- Evidence anchors:
  - [abstract] "Numerical experiments indicate that the HCR bounds are on the precipice of being effectual for small neural nets with... 'MNIST' and 'CIFAR-10'"
  - [section] "The HCR bounds might be more useful a-priori for shallow neural-networks"
- Break condition: Deep networks with high-dimensional features (ResNet-18, Swin-T) produce weak bounds

### Mechanism 3
- Claim: HCR bounds can be computed efficiently using pseudoinverse of Jacobian via LSQR
- Mechanism: Iteratively applies pseudoinverse of Jacobian to find perturbation maximizing HCR bound
- Core assumption: Jacobian exists and can be computed via automatic differentiation
- Evidence anchors:
  - [section] "Algorithm 1: Calculation of a perturbation ε... LSQR should invoke the functions t and u to perform the matrix-vector multiplications"
  - [section] "Iterations of LSQR... with the Jacobian applied to vectors generated during the iterations... can approximate the action of the pseudoinverse"
- Break condition: Ill-conditioned Jacobian or high-dimensional features make computation unstable

## Foundational Learning

- Concept: Hammersley-Chapman-Robbins (HCR) bounds
  - Why needed here: Provide principled, data-driven quantification of privacy guarantees from feature noise
  - Quick check question: What is the key difference between HCR bounds and Cramér-Rao bounds in terms of applicability to deep neural networks?

- Concept: Automatic differentiation and Jacobian computation
  - Why needed here: Enables efficient computation of Jacobian-vector products without forming full Jacobian
  - Quick check question: How does automatic differentiation enable efficient computation of the Jacobian-vector products needed for the HCR bound algorithm?

- Concept: Differential privacy and its limitations
  - Why needed here: Helps contextualize HCR approach relative to established DP framework
  - Quick check question: In what key way does the HCR bound approach differ from DP in terms of the privacy guarantee it provides?

## Architecture Onboarding

- Component map: Input images -> Feature extraction layers -> Feature dithering layer -> Classifier layer -> HCR bound computation module
- Critical path: 1) Forward pass to extract features, 2) Add noise to features, 3) Compute HCR bound via Jacobian and LSQR, 4) Use dithered features for classification
- Design tradeoffs: More noise increases privacy but decreases accuracy; shallower networks improve bounds but hurt performance; exact bounds are expensive to compute
- Failure signatures: HCR bounds remain low despite noise (privacy not guaranteed); significant accuracy drop with noise (steep privacy-utility tradeoff); unstable or slow HCR bound computation
- First 3 experiments: 1) Train CNN on MNIST with feature noise, measure HCR bounds and accuracy across noise levels, 2) Repeat on CIFAR-10 with larger network, compare to MNIST results, 3) Apply to pre-trained ResNet-18 on ImageNet, note if bounds are weak compared to smaller datasets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions do HCR bounds become more effective for large-scale neural networks like ResNet-18 and Swin-T?
- Basis in paper: [inferred] The paper notes HCR bounds are weak for ImageNet-1000 with standard architectures
- Why unresolved: Paper doesn't specify modifications or conditions that could improve effectiveness
- What evidence would resolve it: Experimental results showing improved HCR bounds with specific modifications to architectures, training, or noise strategies on ImageNet-1000

### Open Question 2
- Question: How does choice of perturbation vector ε affect HCR bound tightness, and can we find optimal ε selection strategy?
- Basis in paper: [explicit] Paper discusses importance of selecting suitable ε and mentions maximizing ratio in HCR bound
- Why unresolved: Paper provides method for computing ε but doesn't explore impact of different selection strategies
- What evidence would resolve it: Comparative analysis of HCR bounds using different ε selection strategies (random, gradient-based, adaptive) across various architectures and datasets

### Open Question 3
- Question: Can combining HCR bounds with other privacy techniques provide stronger guarantees than HCR bounds alone?
- Basis in paper: [explicit] Paper suggests supplementing feature noise with other methods like limiting feature vector size
- Why unresolved: No experimental evidence or theoretical analysis of combined effectiveness
- What evidence would resolve it: Experimental results comparing combined HCR bounds with feature size limitation against either technique alone across various architectures and datasets

## Limitations

- HCR bounds become too weak to guarantee privacy for large-scale ImageNet classification
- Relationship between HCR bounds and actual reconstruction attack success rates is not empirically validated
- Computational complexity of computing HCR bounds at scale remains unclear

## Confidence

- **High confidence**: Mathematical derivation of HCR bounds and their relationship to unbiased estimator variance
- **Medium confidence**: Claim that HCR bounds are "reasonably effectual" for small networks like MNIST and CIFAR-10
- **Low confidence**: Assertion that HCR bounds alone can provide practical privacy guarantees for real-world applications

## Next Checks

1. **Empirical attack validation**: Implement and evaluate actual reconstruction attacks on models protected by feature dithering with various noise levels to measure real-world effectiveness of HCR bounds
2. **Scalability study**: Systematically evaluate HCR bounds across spectrum of model sizes and architectures (from simple MLPs to modern transformers) to characterize when bounds become too weak to be useful
3. **Alternative privacy metrics comparison**: Compare HCR bounds against established differential privacy metrics on same models to assess relative effectiveness and interpretability