---
ver: rpa2
title: Classification with neural networks with quadratic decision functions
arxiv_id: '2401.10710'
source_url: https://arxiv.org/abs/2401.10710
tags:
- neural
- classification
- functions
- networks
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates neural networks with quadratic decision
  functions (RQNNs) as an alternative to standard neural networks with affine linear
  decision functions (ALNNs) for classification tasks. The authors propose RQNNs as
  advantageous when objects or classes to be identified have compact geometries like
  circles or ellipses.
---

# Classification with neural networks with quadratic decision functions

## Quick Facts
- arXiv ID: 2401.10710
- Source URL: https://arxiv.org/abs/2401.10710
- Reference count: 17
- Shallow neural networks with quadratic decision functions perform comparably to deep neural networks for binary classification tasks with compact geometries

## Executive Summary
This paper proposes Quadratic Neural Networks (RQNNs) as an alternative to standard neural networks with affine linear decision functions (ALNNs) for classification tasks. The authors demonstrate that RQNNs are particularly advantageous when objects or classes to be identified have compact geometries like circles or ellipses. They show that RQNNs can be easily implemented using existing deep learning frameworks like TensorFlow and Keras, achieving performance comparable to deep neural networks (DNNs) on binary classification tasks while being simpler to train.

## Method Summary
The authors introduce RQNNs by replacing the standard affine linear decision function in neural networks with a quadratic decision function. They implement these models using standard deep learning frameworks, creating both shallow and deep versions (DRQNNs). The models are tested on two binary classification problems: subspecies identification in a population dataset and handwritten digit classification (distinguishing 8 from other digits in MNIST). The quadratic decision surfaces are designed to better capture compact class geometries, and the models are trained using standard backpropagation with cross-entropy loss.

## Key Results
- RQNNs achieved perfect accuracy (100%) for subspecies classification versus 98% for ALNNs
- For MNIST digit classification (8 vs others), RQNNs achieved higher accuracy than ALNNs
- Deep RQNNs (DRQNNs) outperformed standard deep neural networks (DNNs) on both datasets
- RQNNs successfully implemented using standard deep learning frameworks without requiring specialized architectures

## Why This Works (Mechanism)
RQNNs work better for classification tasks with compact class geometries because quadratic decision surfaces can naturally capture circular or elliptical boundaries that linear decision functions cannot. When classes have compact support or approximately circular distributions in feature space, quadratic functions provide the additional degrees of freedom needed to separate them effectively. The quadratic terms allow the decision boundary to curve and adapt to the intrinsic geometry of the data distribution, rather than being constrained to linear separations.

## Foundational Learning
1. **Decision Functions in Neural Networks** - Why needed: Forms the basis of classification in neural networks; quick check: understand how output layer transforms activations into class predictions
2. **Quadratic vs Linear Decision Boundaries** - Why needed: Core concept differentiating RQNNs from standard networks; quick check: visualize how parabolas/circles differ from lines in 2D feature space
3. **Backpropagation Algorithm** - Why needed: Required to train RQNNs with quadratic functions; quick check: trace gradient computation through quadratic terms
4. **Cross-Entropy Loss** - Why needed: Standard loss function for classification used in experiments; quick check: understand relationship between predicted probabilities and loss
5. **Keras/TensorFlow Implementation** - Why needed: Framework used for all experiments; quick check: familiarity with custom layer creation and model compilation

## Architecture Onboarding
Component map: Input -> Quadratic Layer -> Activation -> Output Layer -> Cross-Entropy Loss
Critical path: Data → Quadratic transformation → Non-linearity → Classification → Loss computation
Design tradeoffs: Quadratic terms add expressiveness but increase parameter count; shallow RQNNs vs deep DNNs in terms of parameter efficiency
Failure signatures: Underfitting when quadratic terms are insufficient; overfitting with too many quadratic parameters; convergence issues with poorly scaled features
First experiments:
1. Implement a single-layer RQNN on a synthetic 2D circular dataset
2. Compare training convergence between RQNN and ALNN on simple binary classification
3. Test RQNN on MNIST digit 8 vs others with varying quadratic term regularization

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical analysis of why quadratic decision surfaces outperform affine ones remains limited
- Experiments focus on binary classification with simple geometries, limiting generalizability
- Computational efficiency comparisons between models are not addressed
- Hyperparameter tuning process and its impact on results are not thoroughly discussed

## Confidence
- RQNNs performance on binary classification tasks: High
- RQNNs compatibility with existing deep learning frameworks: High
- Generalizability to complex, non-compact decision boundaries: Medium
- Computational efficiency advantages: Low

## Next Checks
1. Test RQNNs on multi-class classification problems with more complex decision boundaries
2. Conduct systematic hyperparameter optimization comparisons between RQNNs and DNNs
3. Evaluate computational efficiency (training time, memory usage) across different model architectures and dataset sizes