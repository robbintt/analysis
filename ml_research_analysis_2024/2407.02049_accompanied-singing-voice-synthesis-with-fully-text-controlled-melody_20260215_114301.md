---
ver: rpa2
title: Accompanied Singing Voice Synthesis with Fully Text-controlled Melody
arxiv_id: '2407.02049'
source_url: https://arxiv.org/abs/2407.02049
tags:
- midi
- generation
- vocal
- melodylm
- music
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MelodyLM is the first text-to-song model that synthesizes high-quality
  song pieces with fully text-controlled melodies. It eliminates the need for users
  to provide music scores or MIDI sequences, achieving minimal user requirements and
  maximum control flexibility.
---

# Accompanied Singing Voice Synthesis with Fully Text-controlled Melody

## Quick Facts
- **arXiv ID**: 2407.02049
- **Source URL**: https://arxiv.org/abs/2407.02049
- **Reference count**: 40
- **Primary result**: First text-to-song model achieving fully text-controlled melodies without requiring users to provide music scores or MIDI sequences

## Executive Summary
MelodyLM is the first text-to-song model that synthesizes high-quality song pieces with fully text-controlled melodies, eliminating the need for users to provide music scores or MIDI sequences. The model achieves minimal user requirements (lyrics and reference voice only) while maintaining maximum control flexibility (optional textual prompts or direct MIDI input). Using a three-stage framework with language models for MIDI and vocal generation plus a latent diffusion model for accompaniment, MelodyLM achieves subjective ratings of 75.6/100 compared to 79.8 for the best baseline on Mandarin pop song corpora.

## Method Summary
MelodyLM uses a three-stage framework: text-to-MIDI, text-to-vocal, and vocal-to-accompaniment. MIDI sequences are modeled as discrete tokens and generated using a language model conditioned on textual prompts containing musical attributes like key, tempo, and average pitch. Vocal tracks are generated using another language model conditioned on the generated MIDI, lyrics, and reference voice. Accompaniments are synthesized using a latent diffusion model with hybrid conditioning that combines channel-wise vocal injection and partial noising strategy for temporal alignment. The approach enables fully text-controlled melody synthesis while maintaining flexibility for different user requirements.

## Key Results
- MelodyLM achieves fully text-controlled melody synthesis without requiring users to provide music scores or MIDI sequences
- The model reaches subjective ratings of 75.6/100 compared to 79.8 for the best baseline on Mandarin pop song corpora
- Users can synthesize songs with minimal input (lyrics + reference voice) or full control (textual prompts + MIDI)
- The three-stage framework outperforms one-stage approaches in both objective and subjective metrics

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MelodyLM achieves fully text-controlled melody by modeling MIDI as discrete tokens in a language model.
- Mechanism: The model explicitly generates MIDI sequences as intermediate melody-related features using a multi-scale transformer. These MIDI sequences are conditioned on textual prompts containing musical attributes like key, tempo, and average pitch.
- Core assumption: MIDI sequences can be effectively modeled as discrete tokens and used as a bridge between textual prompts and vocal generation.
- Evidence anchors:
  - [abstract]: "MelodyLM explicitly models MIDI as the intermediate melody-related feature and sequentially generates vocal tracks in a language model manner, conditioned on textual and vocal prompts."
  - [section 3.1.1]: "MIDI satisfies all the requirements. MIDI can be maximally decoupled from the singer's identity information, and a range of 32 (G#1, 51.9Hz) to 80 (G#5, 830.6Hz) covers most vocal melodies."
  - [corpus]: Weak - no explicit corpus evidence for MIDI modeling effectiveness, but ROSVOT [28] is used for MIDI extraction.
- Break condition: If the MIDI sequences cannot be effectively decoupled from singer identity or if the range of MIDI pitches is insufficient to cover vocal melodies.

### Mechanism 2
- Claim: MelodyLM achieves controllable accompaniment generation using a latent diffusion model with hybrid conditioning.
- Mechanism: The latent diffusion model (LDM) uses a hybrid conditioning mechanism that combines channel-wise vocal injection and partial noising strategy. This allows the model to learn from both textual prompts and vocal conditioning for accurate temporal alignment.
- Core assumption: The hybrid conditioning mechanism can effectively balance textual prompting and vocal conditioning for accompaniment generation.
- Evidence anchors:
  - [abstract]: "The accompaniment music is subsequently synthesized by a latent diffusion model with hybrid conditioning for temporal alignment."
  - [section 3.3]: "To facilitate both in-context learning from textual prompts and vocal conditioning, we choose a multi-layer feed-forward transformer (FFT) [32] as the denoiser model. To achieve accurate temporal alignment, a hybrid conditioning mechanism is designed."
  - [corpus]: Weak - no explicit corpus evidence for hybrid conditioning effectiveness, but MusicLM [2] and MusicGen [10] are referenced as inspirations.
- Break condition: If the hybrid conditioning mechanism fails to provide accurate temporal alignment or if the model cannot effectively learn from both textual prompts and vocal conditioning.

### Mechanism 3
- Claim: MelodyLM achieves minimal user requirements and maximum control flexibility by allowing users to input lyrics, reference voice, and optional textual prompts or MIDI sequences.
- Mechanism: The model is designed to generate high-quality song pieces with fully text-controlled melodies, eliminating the need for users to provide music scores. Users can input lyrics and a reference voice for minimal requirements, or provide textual prompts or MIDI sequences for full control.
- Core assumption: The model can effectively generate high-quality song pieces with minimal user input or provide full control with additional inputs.
- Evidence anchors:
  - [abstract]: "With minimal requirements, users only need to input lyrics and a reference voice to synthesize a song sample. For full control, just input textual prompts or even directly input MIDI."
  - [section 1]: "Previous works [50] divide TTSong into two stages: 1) vocal synthesis given music score (or MIDI) and lyrics; and 2) accompaniment generation, conditioned on the vocal signals from stage 1. However, asking users to provide music scores is often unrealistic in practice, as it requires a certain level of musical expertise."
  - [corpus]: Weak - no explicit corpus evidence for user input effectiveness, but the model is evaluated with different degrees of control in experiments.
- Break condition: If the model cannot generate high-quality song pieces with minimal user input or if the additional inputs do not provide sufficient control over the generated output.

## Foundational Learning

- Concept: Language modeling
  - Why needed here: MelodyLM uses language models to generate MIDI sequences and vocal tracks conditioned on textual prompts.
  - Quick check question: How does a language model learn to generate sequences of discrete tokens?

- Concept: Diffusion models
  - Why needed here: MelodyLM uses a latent diffusion model for accompaniment generation with hybrid conditioning.
  - Quick check question: How does a diffusion model learn to denoise a corrupted input?

- Concept: Audio tokenization
  - Why needed here: MelodyLM uses SoundStream to tokenize vocal waveforms into discrete acoustic units for language modeling.
  - Quick check question: How does an audio tokenizer convert continuous audio signals into discrete tokens?

## Architecture Onboarding

- Component map: Textual prompts → MIDI-LM → Vocal-LM → LDM → Final song piece
- Critical path: Textual prompts → MIDI-LM → Vocal-LM → LDM → Final song piece
- Design tradeoffs:
  - Multi-stage vs. one-stage generation: MelodyLM uses a three-stage framework for better control and data efficiency, but this introduces potential error accumulation.
  - MIDI representation: MelodyLM uses unexpanded MIDI tokens for better positional information, but this may make numerical duration information more difficult to perceive.
- Failure signatures:
  - Poor MIDI generation: If the MIDI-LM cannot effectively generate MIDI sequences from textual prompts, the subsequent vocal and accompaniment generation may be affected.
  - Vocal generation issues: If the Vocal-LM cannot effectively align lyrics with MIDI tokens or model natural prosody, the generated vocals may sound unnatural.
  - Accompaniment generation problems: If the LDM cannot effectively learn from both textual prompts and vocal conditioning, the generated accompaniments may not align well with the vocals.
- First 3 experiments:
  1. Test MIDI generation with different textual prompts to evaluate controllability.
  2. Test vocal generation with GT MIDI input to evaluate prosody modeling and singer similarity.
  3. Test accompaniment generation with different degrees of control (e.g., with or without GT vocal input) to evaluate the effectiveness of the hybrid conditioning mechanism.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal intermediate melody representation that balances controllability and computational efficiency for text-to-song synthesis?
- Basis in paper: [explicit] The paper discusses using MIDI as an intermediate melody representation, noting it can be maximally decoupled from singer identity and covers most vocal ranges. However, it also mentions challenges with other representations like F0 contours (strongly correlated with prosody) and chromagrams (insufficient range).
- Why unresolved: The paper doesn't conduct a comprehensive comparison of different intermediate representations (e.g., MIDI vs. F0 vs. chromagram vs. others) in terms of controllability, computational cost, and final audio quality. It only mentions why MIDI was chosen over alternatives.
- What evidence would resolve it: A systematic ablation study comparing different intermediate representations using the same model architecture, measuring controllability metrics (key accuracy, pitch distribution similarity), computational efficiency, and subjective audio quality ratings.

### Open Question 2
- Question: How does the cascaded multi-stage approach compare to a unified one-stage model in terms of audio quality and controllability?
- Basis in paper: [explicit] The paper mentions ablation experiments comparing the cascaded architecture with one-stage singing voice synthesis, showing reduced performance in the one-stage approach. However, it suggests this might be due to limited training data and that larger datasets could enable one-stage generation.
- Why unresolved: The paper only provides one ablation experiment with limited data. It doesn't explore whether the performance gap persists with larger datasets, or whether architectural modifications could improve one-stage performance.
- What evidence would resolve it: Training both cascaded and one-stage models on progressively larger datasets (e.g., 10x, 100x current size), measuring objective metrics (FFE, FAD) and conducting subjective listening tests to determine if one-stage approaches can match or exceed cascaded performance with sufficient data.

### Open Question 3
- Question: What is the optimal conditioning mechanism for accompaniment generation that balances textual prompts, vocal conditioning, and temporal alignment?
- Basis in paper: [explicit] The paper describes a hybrid conditioning mechanism combining channel-wise vocal injection and partial noising, but doesn't explore alternative conditioning strategies or their relative contributions to final quality.
- Why unresolved: The paper only presents one hybrid approach without comparing it to alternatives like pure textual conditioning, pure vocal conditioning, different fusion strategies, or attention-based mechanisms.
- What evidence would resolve it: Ablation studies varying conditioning mechanisms (e.g., removing vocal conditioning, changing fusion strategies, using attention instead of concatenation), measuring FAD, KL divergence, CLAP scores, and subjective quality ratings for each approach.

## Limitations

- The three-stage cascaded approach introduces compounding error risks that are not fully characterized across different song characteristics
- The MIDI representation choice (unexpanded tokens) prioritizes positional information but may limit precise duration control
- The hybrid conditioning mechanism for accompaniment generation lacks sufficient architectural detail for exact replication
- Quality degradation when dropping optional inputs suggests limited robustness to minimal user requirements

## Confidence

- **High confidence**: The basic three-stage framework architecture is clearly specified and internally consistent. The use of language models for both MIDI and vocal generation is well-established in related work.
- **Medium confidence**: The claim of "fully text-controlled melody" is supported by objective metrics for MIDI controllability, but the subjective quality metrics (75.6/100 vs 79.8 for best baseline) show only modest improvement over existing approaches.
- **Medium confidence**: The minimal user requirement claim is demonstrated through the ability to generate with just lyrics and reference voice, though quality degradation when dropping optional inputs suggests limited robustness.

## Next Checks

1. **MIDI controllability validation**: Test MIDI generation with systematically varied textual prompts (varying key, tempo, pitch range) to verify the claimed controllability metrics hold across the full parameter space, not just average cases.
2. **Stage-wise error propagation analysis**: Measure quality degradation at each stage when dropping optional inputs (GT MIDI, textual prompts) to quantify the cumulative effect and identify which stage introduces the most error.
3. **Cross-genre generalization test**: Evaluate the model on genres outside the Mandarin pop corpus (e.g., rock, classical, folk) to assess whether the MIDI representation and language modeling approach generalizes beyond the training domain.