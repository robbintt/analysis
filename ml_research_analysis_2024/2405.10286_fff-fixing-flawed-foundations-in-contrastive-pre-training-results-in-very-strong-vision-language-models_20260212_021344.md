---
ver: rpa2
title: 'FFF: Fixing Flawed Foundations in contrastive pre-training results in very
  strong Vision-Language models'
arxiv_id: '2405.10286'
source_url: https://arxiv.org/abs/2405.10286
tags:
- label
- photo
- image
- training
- captions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses two key issues in vision-language contrastive
  pre-training: incorrect assignment of negative pairs due to semantically similar
  images/captions, and low caption quality and diversity. To fix these, the authors
  propose mining new positive pairs based on image-text, image-image, and text-text
  similarities, and using batch text augmentation to train with multiple pseudo-captions
  per image.'
---

# FFF: Fixing Flawed Foundations in contrastive pre-training results in very strong Vision-Language models

## Quick Facts
- arXiv ID: 2405.10286
- Source URL: https://arxiv.org/abs/2405.10286
- Reference count: 40
- Achieves state-of-the-art performance on both image recognition (+6% on average over 11 datasets) and image retrieval (+19% on Flickr30k and +15% on MSCOCO)

## Executive Summary
This paper addresses two key issues in vision-language contrastive pre-training: incorrect assignment of negative pairs due to semantically similar images/captions, and low caption quality and diversity. The authors propose mining new positive pairs based on image-text, image-image, and text-text similarities, and using batch text augmentation to train with multiple pseudo-captions per image. They also propose using sigmoid loss to accommodate the variable number of positive pairs. Their approach, FFF, achieves state-of-the-art performance on both image recognition and image retrieval tasks.

## Method Summary
The authors propose three main innovations: (1) mining new positive pairs using image-text, image-image, and text-text similarity thresholds to correct false negatives, (2) batch text augmentation with multiple pseudo-captions per image to increase caption diversity, and (3) sigmoid loss to handle variable positive counts per sample. The method uses similarity mining with thresholds (p1=0.27, p2=0.92, p3=0.99) to construct an assignment matrix, generates multiple pseudo-captions using a captioning model, and trains with sigmoid loss to accommodate the variable number of positives. The approach is evaluated on large-scale datasets (CC3M, CC12M, YFCC15M-v2, Open30M, Open70M) and achieves state-of-the-art results on both image recognition (+6% average over 11 datasets) and image retrieval (+19% on Flickr30k, +15% on MSCOCO).

## Key Results
- Achieves state-of-the-art performance on image recognition (+6% average over 11 datasets)
- Achieves state-of-the-art performance on image retrieval (+19% on Flickr30k, +15% on MSCOCO)
- Demonstrates effectiveness of fixing flawed foundations in contrastive pre-training

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The sigmoid loss enables training with variable positive counts per sample without sacrificing robustness to noise in the positive set.
- Mechanism: By mapping cosine similarities through a sigmoid, the model can handle multiple positives and avoid being dominated by hard negatives. The learnable β term further adapts the decision boundary per batch.
- Core assumption: The ground truth mask M is accurate enough that the sigmoid loss will still converge despite occasional errors from the similarity mining process.
- Evidence anchors: [abstract], [section 4.4]
- Break condition: If similarity mining produces many false positives, the mask M becomes noisy enough that sigmoid loss no longer outperforms contrastive loss.

### Mechanism 2
- Claim: Batch text augmentation with multiple pseudo-captions per image improves the quality of the positive set by averaging out caption-level noise.
- Mechanism: Generating k pseudo-captions per image increases caption diversity. Averaging image-text similarities over k captions yields features more aligned with the image, mitigating hallucination or redundancy in any single caption.
- Core assumption: The captioning model (e.g., BLIP2) generates pseudo-captions that are on average better aligned than raw captions, even if individual captions may still be noisy.
- Evidence anchors: [abstract], [section 2]
- Break condition: If the captioning model is biased or its training data is too similar to the pre-training set, pseudo-captions may repeat errors rather than diversify.

### Mechanism 3
- Claim: Correcting false negatives via similarity-based mining increases the number of true positive pairs, improving representation learning.
- Mechanism: By comparing image-text, image-image, and text-text similarities, the method flags pairs that are semantically similar but labeled as negatives, and reassigns them as positives. This reduces the gap between actual and assigned positives.
- Core assumption: High cosine similarity in CLIP space correlates well with semantic equivalence, so mining by thresholds p1, p2, p3 effectively captures true positives.
- Evidence anchors: [abstract], [section 4.1]
- Break condition: If similarity thresholds are too permissive, false positives get mined and degrade performance; if too strict, few false negatives are corrected.

## Foundational Learning

- Concept: Contrastive loss assumes exactly one positive per sample; it cannot natively handle variable positive counts.
  - Why needed here: The proposed methods generate a dynamic number of positives per image, violating the single-positive assumption.
  - Quick check question: What happens if you feed a contrastive loss multiple positives without modification?

- Concept: BCE (sigmoid) loss can handle multi-label classification by treating each class as an independent binary decision.
  - Why needed here: Each candidate caption-image pair is treated as a binary label (positive/negative), allowing arbitrary numbers of positives per sample.
  - Quick check question: How does BCE loss differ from InfoNCE in terms of handling multiple positives?

- Concept: Pseudo-caption generation via image captioning models can increase data diversity and quality.
  - Why needed here: Raw captions from web datasets are noisy and repetitive; synthetic captions can fill in detail and variation.
  - Quick check question: What metric would you use to evaluate whether a synthetic caption is "good enough" to be a positive?

## Architecture Onboarding

- Component map: Pre-trained CLIP model -> Captioning model (BLIP2) -> Similarity mining pipeline -> Sigmoid loss with learnable β -> Training loop
- Critical path: 1. Compute features for all images and captions (teacher model) 2. Build similarity matrices and assignment mask M 3. Generate pseudo-captions (offline) 4. Construct batch with multiple captions per image 5. Forward pass, compute sigmoid loss, backward pass
- Design tradeoffs:
  - Mining thresholds: higher thresholds → fewer false positives but more false negatives; lower → opposite
  - Number of pseudo-captions: more → higher diversity but higher compute and risk of noisy positives
  - Loss choice: sigmoid is more robust but less sharp than InfoNCE; may slow convergence
- Failure signatures:
  - Training loss plateaus early → likely too many false positives in M
  - Poor downstream accuracy → possible caption quality issue or over-mining
  - Memory errors → batch too large for available GPUs
- First 3 experiments:
  1. Run with only raw captions, no mining, using InfoNCE → establish baseline
  2. Run with raw captions, mining enabled, sigmoid loss → isolate effect of false-negative correction
  3. Run with pseudo-captions + batch augmentation, no mining → isolate effect of caption diversity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed sigmoid loss compare to other loss functions (e.g., supervised contrastive loss, InfoNCE) in terms of robustness to noise and scalability?
- Basis in paper: [explicit] The authors claim that sigmoid loss is more robust to noise and allows for a variable number of positive pairs, unlike contrastive loss and supervised contrastive loss.
- Why unresolved: The paper provides a comparison with supervised contrastive loss, but a more comprehensive evaluation against other loss functions is needed to fully understand the advantages of sigmoid loss.
- What evidence would resolve it: A systematic comparison of sigmoid loss with other loss functions (e.g., supervised contrastive loss, InfoNCE) on various datasets and tasks, considering both accuracy and computational efficiency.

### Open Question 2
- Question: What is the optimal number of pseudo-captions to generate per image for batch text augmentation?
- Basis in paper: [explicit] The authors propose using multiple pseudo-captions per image and show that increasing the number of captions improves accuracy. However, they do not explore the optimal number of captions.
- Why unresolved: The paper only evaluates the impact of using 1, 3, and 5 pseudo-captions, leaving the optimal number unexplored.
- What evidence would resolve it: An ablation study varying the number of pseudo-captions per image (e.g., 1, 3, 5, 7, 9) to determine the point of diminishing returns and identify the optimal number for maximizing performance.

### Open Question 3
- Question: How does the proposed approach handle cases where the ground-truth captions are already of high quality and diversity?
- Basis in paper: [inferred] The paper focuses on addressing the issues of noise and low caption quality in web-collected datasets. However, it does not explicitly discuss the scenario where ground-truth captions are already of high quality.
- Why unresolved: The paper does not provide insights into how the proposed approach performs when the ground-truth captions are already informative and diverse.
- What evidence would resolve it: An evaluation of the proposed approach on datasets with high-quality ground-truth captions to assess its effectiveness in such scenarios and identify potential limitations.

## Limitations
- The paper lacks direct validation of the sigmoid loss mechanism against alternatives like symmetric cross-entropy or debiased contrastive losses
- The batch text augmentation claims are based on intuition about caption diversity rather than rigorous ablations of captioning quality versus diversity trade-offs
- The false-negative mining mechanism relies heavily on similarity thresholds without exploring sensitivity or providing error analysis on mining quality

## Confidence

Empirical results: High
Sigmoid loss mechanism: Medium
Text augmentation mechanism: Medium
False-negative mining mechanism: Medium
Combined effect attribution: Low

## Next Checks

1. Ablate each mechanism individually (mining only, augmentation only, sigmoid loss only) to quantify their independent contributions
2. Vary similarity thresholds systematically to measure sensitivity of mining performance to parameter choices
3. Compare sigmoid loss against debiased contrastive loss variants to test if improvements are specific to the proposed approach