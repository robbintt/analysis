---
ver: rpa2
title: A Primer on Large Language Models and their Limitations
arxiv_id: '2412.04503'
source_url: https://arxiv.org/abs/2412.04503
tags:
- llms
- available
- page
- language
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper provides a comprehensive primer on Large Language Models
  (LLMs), explaining their architecture, development, and practical applications.
  It covers three main types of LLMs: decoder-only, encoder-only, and encoder-decoder
  models, detailing their pre-training and fine-tuning processes.'
---

# A Primer on Large Language Models and their Limitations

## Quick Facts
- arXiv ID: 2412.04503
- Source URL: https://arxiv.org/abs/2412.04503
- Authors: Sandra Johnson; David Hyland-Wood
- Reference count: 40
- Primary result: Comprehensive overview of LLM architectures, development, applications, and limitations

## Executive Summary
This paper provides a thorough primer on Large Language Models, explaining their fundamental architecture, development processes, and practical applications. The authors systematically cover three main LLM types (decoder-only, encoder-only, encoder-decoder) and detail their pre-training and fine-tuning methodologies. The work also addresses critical risks including catastrophic forgetting, model collapse, jailbreak attacks, and hallucinations, while proposing mitigation strategies for each. Given the rapid evolution of LLM technology, the paper emphasizes the importance of continual evaluation and adaptation as new models and approaches emerge.

## Method Summary
The paper synthesizes existing knowledge about LLM development and applications through comprehensive literature review and technical analysis. It examines self-supervised learning approaches including MLM and CLM, explores various fine-tuning techniques, and evaluates different interaction methods like in-context learning and prompt engineering. The methodology involves systematic categorization of LLM types, analysis of training methodologies, and evaluation of risk factors and mitigation strategies through technical examination rather than empirical experimentation.

## Key Results
- LLMs achieve optimal performance through balanced scaling of model size and training dataset size
- Self-supervised learning enables effective training on unlabeled data through techniques like MLM and CLM
- In-context learning allows zero/few-shot task performance without explicit fine-tuning
- Multiple critical risks exist including catastrophic forgetting, model collapse, jailbreak attacks, and hallucinations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs improve performance by increasing model size and dataset size together.
- Mechanism: Larger models trained on more data achieve better results than simply increasing model size or dataset size alone.
- Core assumption: There is an optimal relationship between model size and training data size.
- Evidence anchors:
  - [section]: "Hoffman et al. [24] found that training a smaller model on more data for a given compute budget is more performant than only increasing model size while keeping the size of the training data unchanged."
  - [corpus]: Weak - corpus neighbors focus on model architectures rather than size/data relationships.
- Break condition: If the optimal balance is exceeded, performance may plateau or degrade due to overfitting or computational inefficiency.

### Mechanism 2
- Claim: Self-supervised learning enables LLMs to learn from unlabeled data.
- Mechanism: Models generate their own labels from input data through techniques like masked language modeling and causal language modeling.
- Core assumption: Unlabeled data contains sufficient structure for meaningful learning.
- Evidence anchors:
  - [section]: "SSL is the most popular machine learning technique for LLMs. This approach is often referred to as the 'dark matter of intelligence' [4] and includes learning methods such as CLM, MLM, Span-Level Masking, and Contrastive Learning, where models learn without the need to explicitly apply external labels to the data."
  - [corpus]: Weak - corpus neighbors don't specifically address self-supervised learning techniques.
- Break condition: If the unlabeled data lacks sufficient diversity or structure, the model may fail to learn meaningful patterns.

### Mechanism 3
- Claim: In-context learning allows LLMs to perform new tasks without fine-tuning.
- Mechanism: Models adapt behavior based on examples, instructions, or demonstrations included in the prompt.
- Core assumption: LLMs can generalize from limited examples within their context window.
- Evidence anchors:
  - [section]: "In-context learning (ICL) refers to the capability of pre-trained LLMs to perform new tasks by leveraging information provided within the context window, without any explicit parameter updates or fine-tuning [9]."
  - [corpus]: Weak - corpus neighbors don't specifically address in-context learning capabilities.
- Break condition: If the context window is too small or the examples are insufficient, the model may fail to generalize correctly.

## Foundational Learning

- Concept: Transformer architecture
  - Why needed here: Understanding the core architecture is essential for grasping how LLMs process and generate text.
  - Quick check question: What are the main components of a Transformer and their roles?

- Concept: Self-supervised learning techniques
  - Why needed here: These techniques are fundamental to how LLMs are pre-trained on large amounts of unlabeled data.
  - Quick check question: How do MLM and CLM differ in their approach to self-supervised learning?

- Concept: Fine-tuning methods
  - Why needed here: Understanding different fine-tuning approaches is crucial for adapting LLMs to specific tasks.
  - Quick check question: What are the key differences between SFT, RLHF, and PEFT?

## Architecture Onboarding

- Component map:
  Encoder-only models (BERT, RoBERTa) for understanding tasks → Decoder-only models (GPT series) for generation tasks → Encoder-decoder models (T5, BART) for sequence-to-sequence tasks → Self-supervised learning components for pre-training → Fine-tuning modules for task-specific adaptation

- Critical path:
  1. Pre-training on large corpus using self-supervised learning
  2. Fine-tuning for specific tasks or use cases
  3. In-context learning for zero/few-shot performance
  4. Orchestration with other technologies for complex applications

- Design tradeoffs:
  - Model size vs. computational efficiency
  - Pre-training data diversity vs. task-specific relevance
  - Fine-tuning comprehensiveness vs. parameter efficiency
  - Generation quality vs. hallucination risk

- Failure signatures:
  - Catastrophic forgetting in sequential fine-tuning
  - Model collapse from recursive training on generated data
  - Jailbreak attacks exploiting prompt vulnerabilities
  - Hallucinations producing plausible but false information

- First 3 experiments:
  1. Test in-context learning capabilities with few-shot prompting on simple tasks
  2. Compare performance of different fine-tuning approaches on a specific task
  3. Evaluate hallucination rates using the Hallucination Index methodology

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can model collapse be effectively prevented in LLMs as they increasingly ingest machine-generated content?
- Basis in paper: [explicit] The paper discusses model collapse as a risk when LLMs ingest increasing amounts of machine-generated content, noting that current mitigations like data provenance and government regulation are insufficient.
- Why unresolved: Current strategies only mitigate, not solve, the problem, and many systems may ignore data provenance hints.
- What evidence would resolve it: Development of a robust method to differentiate human- and machine-generated content, or a regulatory framework that effectively reduces negative consequences of model collapse.

### Open Question 2
- Question: What are the most effective strategies to prevent catastrophic forgetting in LLMs during continual learning?
- Basis in paper: [explicit] The paper mentions catastrophic forgetting as a risk when LLMs are fine-tuned sequentially on different tasks, and discusses strategies like regularisation-based methods, replay-based methods, and architectural methods.
- Why unresolved: While several strategies are proposed, their effectiveness in practice is not fully explored, and combining multiple strategies may be necessary.
- What evidence would resolve it: Comparative studies of the proposed strategies in real-world applications, demonstrating their effectiveness in preventing catastrophic forgetting.

### Open Question 3
- Question: How can the performance of LLMs be improved for tasks requiring reasoning over individual characters or words, such as the "strawberry" example?
- Basis in paper: [explicit] The paper highlights that LLMs struggle with tasks requiring reasoning over individual characters or words, as they process text as tokens rather than individual characters.
- Why unresolved: Current LLMs lack the ability to reason over individual characters or words, and new approaches to reasoning in LLMs are still in development.
- What evidence would resolve it: Development of a new LLM architecture or training method that enables effective reasoning over individual characters or words, or a significant improvement in the performance of existing LLMs on such tasks.

## Limitations
- Lack of quantitative benchmarks or empirical validation for many claims
- Rapid evolution of the field may render some information outdated quickly
- Missing detailed implementation guidance for practical application of techniques

## Confidence

**High Confidence Claims:**
- The fundamental architecture of LLMs (Transformer-based models)
- The distinction between encoder-only, decoder-only, and encoder-decoder architectures
- Basic self-supervised learning approaches (MLM, CLM)
- The existence and general nature of identified risks (catastrophic forgetting, model collapse, jailbreak attacks, hallucinations)

**Medium Confidence Claims:**
- The effectiveness of various mitigation strategies
- The specific relationships between model size, dataset size, and performance
- The relative advantages of different fine-tuning approaches
- The practical applicability of in-context learning for various tasks

**Low Confidence Claims:**
- Specific quantitative performance improvements from various techniques
- The long-term effectiveness of current mitigation strategies
- Predictions about future LLM developments and their impacts
- The generalizability of findings across different domains and applications

## Next Checks

**Validation Check 1:** Conduct controlled experiments comparing the performance of different fine-tuning approaches (SFT, RLHF, PEFT) on a standardized task to validate the claimed advantages and limitations of each method. This should include quantitative metrics and qualitative analysis of outputs.

**Validation Check 2:** Systematically test the effectiveness of various mitigation strategies against known attack vectors, particularly jailbreak attacks, using established benchmarks and safety evaluation frameworks. Document success rates and false positive/negative rates.

**Validation Check 3:** Perform a longitudinal study tracking the evolution of model performance and behavior over multiple training iterations to validate claims about catastrophic forgetting and model collapse, particularly when models are trained on their own generated outputs.