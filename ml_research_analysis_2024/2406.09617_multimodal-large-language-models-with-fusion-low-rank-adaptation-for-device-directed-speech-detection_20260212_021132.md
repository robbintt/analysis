---
ver: rpa2
title: Multimodal Large Language Models with Fusion Low Rank Adaptation for Device
  Directed Speech Detection
arxiv_id: '2406.09617'
source_url: https://arxiv.org/abs/2406.09617
tags:
- flora
- data
- multimodal
- video
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of improving device-directed
  speech detection (DDSD) in voice assistants by incorporating multimodal information
  (audio, video, and text) into large language models (LLMs). The core method, Fusion
  Low Rank Adaptation (FLoRA), efficiently adapts pre-trained text-only LLMs to handle
  new modalities using low-rank adapters.
---

# Multimodal Large Language Models with Fusion Low Rank Adaptation for Device Directed Speech Detection

## Quick Facts
- arXiv ID: 2406.09617
- Source URL: https://arxiv.org/abs/2406.09617
- Reference count: 0
- Primary result: 22% relative reduction in EER for DDSD using multimodal fusion

## Executive Summary
This paper addresses the challenge of device-directed speech detection (DDSD) in voice assistants by incorporating multimodal information through an efficient adaptation method called Fusion Low Rank Adaptation (FLoRA). FLoRA enables pre-trained text-only large language models to handle new modalities (audio and video) using modality-specific low-rank adapters that can be dynamically dropped when corresponding data is missing. The approach achieves significant performance improvements while tuning only 1-5% of parameters compared to full fine-tuning.

## Method Summary
The core innovation is FLoRA, which extends pre-trained text-only LLMs to handle multimodal inputs through modality-specific low-rank adapters. These adapters can be dropped when their corresponding modality data is missing, enabling the model to handle incomplete inputs gracefully. The method uses a gating mechanism to learn when to drop each adapter during training, improving robustness to missing data. The approach scales efficiently across different model sizes (16M to 3B parameters) while maintaining performance comparable to full fine-tuning with significantly fewer trainable parameters.

## Key Results
- 22% relative reduction in equal error rate (EER) over text-only methods
- Achieves comparable performance to full fine-tuning while tuning only 1-5% of parameters
- With adapter dropout, shows 20% lower EER and 56% lower false accept rate compared to full fine-tuning

## Why This Works (Mechanism)
FLoRA works by adding modality-specific low-rank adapters to a pre-trained text LLM, allowing it to process new input types without modifying the core model. The adapters learn modality-specific transformations that can be dropped when data is missing, enabling graceful handling of incomplete inputs. The gating mechanism during training learns optimal dropping strategies, making the model robust to missing modalities. This approach preserves the knowledge in pre-trained LLMs while efficiently extending their capabilities to multimodal scenarios.

## Foundational Learning

**Low-Rank Adaptation (LoRA)**: Efficient parameter-efficient fine-tuning method using low-rank matrices to approximate weight updates. Why needed: Enables efficient adaptation of large models with minimal additional parameters. Quick check: Verify adapter rank matches computational constraints.

**Modality-Specific Adapters**: Separate adapter modules for each input modality (text, audio, video). Why needed: Allows independent learning of modality transformations and selective dropping. Quick check: Confirm adapters can be independently enabled/disabled.

**Adapter Dropout**: Training technique where adapters are randomly dropped to improve robustness. Why needed: Prevents overfitting to specific modality combinations and improves generalization. Quick check: Measure performance degradation when modalities are missing.

**Gating Mechanisms**: Learnable functions that determine when to drop adapters during inference. Why needed: Enables dynamic adaptation to available modalities at inference time. Quick check: Verify gating decisions align with actual modality availability.

## Architecture Onboarding

**Component Map**: Input Modalities -> Modality Adapters -> Gating Network -> Text LLM Backbone -> Output

**Critical Path**: Modality data flows through respective adapters, gating network decides which adapters to use, processed features are concatenated and fed to the LLM backbone for final prediction.

**Design Tradeoffs**: FLoRA trades some parameter efficiency for flexibility - modality adapters add parameters but enable selective dropping. The gating mechanism adds complexity but improves robustness to missing data compared to static approaches.

**Failure Signatures**: Performance degradation occurs when adapters are incorrectly dropped (false negatives) or kept (false positives). Missing modality handling may fail if gating network is poorly calibrated to actual data availability patterns.

**First Experiments**:
1. Test adapter dropping behavior with single missing modality (audio-only, video-only, text-only)
2. Measure parameter count and FLOPs for different adapter ranks and model sizes
3. Evaluate robustness to random adapter dropout during inference

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation focused on proprietary Alexa datasets, limiting generalizability to other multimodal applications
- Experiments assume only complete or missing modalities, not corrupted or noisy partial data
- Performance gains measured against text-only baselines rather than broader range of multimodal approaches

## Confidence

**High Confidence**: Core FLoRA architecture and efficiency benefits (1-5% parameter tuning, 20% lower EER with adapter dropout) are well-supported by experimental results

**Medium Confidence**: Scalability claims across model sizes (16M to 3B parameters) demonstrated but could benefit from testing on larger models

**Medium Confidence**: Robustness claims for missing modalities convincing but limited to specific experimental protocol

## Next Checks

1. Test FLoRA on corrupted or noisy multimodal inputs (not just complete/missing modalities) to assess real-world robustness
2. Evaluate the approach on additional multimodal tasks beyond DDSD to assess generalizability
3. Compare against state-of-the-art multimodal foundation models (not just text-only baselines) to better contextualize performance gains