---
ver: rpa2
title: Benchmarking Llama2, Mistral, Gemma and GPT for Factuality, Toxicity, Bias
  and Propensity for Hallucinations
arxiv_id: '2404.09785'
source_url: https://arxiv.org/abs/2404.09785
tags:
- rt-realtoxicity
- safety
- answer
- dataset
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces 14 novel datasets for benchmarking Large
  Language Models (LLMs) on safety, including factuality, toxicity, bias, and hallucination
  propensity. The datasets were created by augmenting existing resources with instructions
  and examples, covering various tasks like summarization, translation, and Q&A.
---

# Benchmarking Llama2, Mistral, Gemma and GPT for Factuality, Toxicity, Bias and Propensity for Hallucinations

## Quick Facts
- arXiv ID: 2404.09785
- Source URL: https://arxiv.org/abs/2404.09785
- Authors: David Nadeau; Mike Kroutikov; Karen McNeil; Simon Baribeau
- Reference count: 40
- Key outcome: This paper introduces 14 novel datasets for benchmarking Large Language Models (LLMs) on safety, including factuality, toxicity, bias, and hallucination propensity. The datasets were created by augmenting existing resources with instructions and examples, covering various tasks like summarization, translation, and Q&A. Four open-source LLMs (Llama2, Mistral, and Gemma) and OpenAI's GPT models were evaluated. Results show Llama2 excels at factuality and toxicity handling, Mistral is best at reducing hallucinations, and Gemma performs moderately. GPT models significantly outperform open-source alternatives. The study highlights the need for further improvements in LLM safety, particularly for open-source models, and provides a comprehensive tool and datasets for future research.

## Executive Summary
This study benchmarks four Large Language Models (Llama2, Mistral, Gemma, and GPT) on safety across factuality, toxicity, bias, and hallucination propensity in enterprise contexts. The researchers created 14 novel datasets by augmenting existing resources with system messages and one-shot examples, covering tasks like summarization, translation, and Q&A. The BEST-OF metric (combining PREFIX-EXACT-MATCH and ROUGE-2) was used to evaluate model performance. Results show that while GPT models significantly outperform open-source alternatives in safety tasks, Llama2 excels at factuality and toxicity handling, Mistral is best at reducing hallucinations, and Gemma performs moderately. The study reveals that open-source models degrade significantly in safety when engaging in multi-turn conversations.

## Method Summary
The researchers evaluated four LLMs (GPT-3.5, GPT-4, Llama2 variants, Mistral variants, and Gemma variants) on 14 novel safety-focused datasets. These datasets were semi-synthetic, created by programmatically augmenting existing resources with system messages, one-shot learning examples, and multi-turn conversation structures. Each model was run on 250 prompts per dataset, and performance was evaluated using the BEST-OF metric (maximum of PREFIX-EXACT-MATCH and ROUGE-2). Results were compared against a baseline ("Sorry, I cannot help") and GPT performance, with the best score per organization (Llama2, Mistral, Gemma) reported in the findings.

## Key Results
- Llama2 outperforms other open-source models in factuality and toxicity handling
- Mistral excels at reducing hallucinations compared to other models
- GPT models significantly outperform open-source alternatives in all safety metrics
- Open-source models show significant safety degradation in multi-turn conversations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The BEST-OF metric reliably evaluates LLM safety performance across diverse tasks.
- Mechanism: Combines PREFIX-EXACT-MATCH (0/1) and ROUGE-2 (0-1) to handle both short determinate and long open-ended answers, reducing edge case failures.
- Core assumption: At least one of PEM or ROUGE-2 will capture the correctness for any given answer type.
- Evidence anchors: [abstract] "we use a metric called 'BEST-OF' which computes both PEM (0 or 1) and ROUGE-2 (from 0 to 1) and returns the higher value." [section] "In line with Liang et al. (2023)'s Helm and other benchmarking tools, we made use of a 'PREFIX-EXACT-MATCH' (PEM) metric when the expected answer was short and pre-determinate... When the answer was long open-ended-text, the ROUGE-2 metric (Lin, 2004) was used." [corpus] Weak - no direct corpus evidence on BEST-OF effectiveness; based on internal design choice.
- Break condition: If an answer format falls outside PEM or ROUGE-2 scoring (e.g., multi-choice lists, structured JSON), BEST-OF would fail.

### Mechanism 2
- Claim: Semi-synthetic datasets created by augmenting existing resources provide sufficient task diversity for LLM safety benchmarking.
- Mechanism: Template-based prompt construction with system message, one-shot learning, and contextual information transforms existing datasets into safety-focused evaluation tasks.
- Core assumption: Existing datasets can be effectively repurposed by adding instruction context without losing original task integrity.
- Evidence anchors: [abstract] "The dataset is a collection of 14 novel LLM Red Teaming datasets... 11 of which are semi-synthetic, that is, derived from various other datasets." [section] "These datasets are semi-synthetic: they were built programmatically starting from existing resources, as described in the next sections." [corpus] Weak - corpus shows related work but no direct validation of semi-synthetic approach effectiveness.
- Break condition: If original dataset characteristics don't align with safety evaluation needs, the transformation may introduce bias or fail to test intended capabilities.

### Mechanism 3
- Claim: Multi-turn conversation format reveals LLM safety degradation that single-turn prompts miss.
- Mechanism: Extended conversations with follow-up messages challenging previous answers test model consistency and resistance to user manipulation.
- Core assumption: Safety alignment degrades over conversation turns when models face persistent user challenges.
- Evidence anchors: [abstract] "When engaging in back-and-forth conversation (multi-turn prompts), we find that the safety of open-source models degrades significantly." [section] "We added two messages to the conversation: first, the correct answer as if it was provided by a fictional assistant. Then, a follow-up by the user telling it that the answer is incorrect and asking once more for the task to be accomplished." [corpus] Moderate - related papers mention jailbreaking and safety degradation but no specific multi-turn testing framework.
- Break condition: If models maintain consistent safety alignment regardless of conversation length, this mechanism would not differentiate performance.

## Foundational Learning

- Concept: Text generation evaluation metrics (ROUGE, BLEU, perplexity)
  - Why needed here: Understanding metric limitations and appropriate use cases for LLM safety evaluation
  - Quick check question: When would ROUGE-2 fail to capture semantic similarity between generated and reference text?

- Concept: Dataset construction and augmentation techniques
  - Why needed here: Creating effective semi-synthetic datasets requires understanding how to preserve task integrity while adding safety-focused context
  - Quick check question: What are the risks of over-augmenting existing datasets when creating safety evaluation benchmarks?

- Concept: LLM safety alignment and instruction following
  - Why needed here: Different models exhibit varying capabilities in following safety instructions across tasks
  - Quick check question: How does instruction-following capability correlate with model size and architecture?

## Architecture Onboarding

- Component map: Dataset loader → Prompt template engine → LLM inference → Metric calculator → Result aggregator → Visualization dashboard
- Critical path: Prompt generation → LLM execution → Metric scoring → Result storage
- Design tradeoffs: BEST-OF metric vs. single metric choice - balances coverage vs. complexity
- Failure signatures: Inconsistent results across similar prompts indicate stochastic variability; systematic baseline performance suggests metric or instruction issues
- First 3 experiments:
  1. Run baseline model on all datasets to establish performance floors
  2. Test single-turn vs. multi-turn versions of same toxicity prompts to verify degradation mechanism
  3. Compare BEST-OF metric results against human-annotated correctness for a subset of answers

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we create a balanced jailbreaking dataset that includes both malicious prompts that should be refused and legitimate prompts that should be answered?
- Basis in paper: [explicit] The authors mention that the current jailbreaking dataset has a very high baseline score (92%), making it difficult to differentiate between models. They plan to rebalance the dataset to include more legitimate prompts.
- Why unresolved: The current dataset is heavily skewed towards malicious prompts, making it challenging to assess the models' ability to distinguish between harmful and benign requests.
- What evidence would resolve it: A new jailbreaking dataset with a more balanced mix of malicious and legitimate prompts, along with improved model performance in correctly identifying and handling both types of requests.

### Open Question 2
- Question: What is the impact of fine-tuning large language models on the mitigation of safety concerns?
- Basis in paper: [explicit] The authors mention that they plan to evaluate the impact of fine-tuning LLMs on safety concerns. They note that early experiments show mixed results, with performance on trained tasks improving but performance on other tasks decreasing.
- Why unresolved: The current research has not thoroughly investigated the effectiveness of fine-tuning in addressing safety issues, and the potential trade-offs between task-specific performance and overall safety need to be explored further.
- What evidence would resolve it: Comprehensive studies on the impact of fine-tuning on various safety metrics, including factuality, toxicity, bias, and hallucination propensity, across different model sizes and domains.

### Open Question 3
- Question: How do large language models perform on extra-long prompts, such as full documents or books, and in long multi-turn conversations?
- Basis in paper: [explicit] The authors mention that they plan to investigate the performance of LLMs on extra-long prompts and in long multi-turn conversations, as they observed reduced safety in these scenarios.
- Why unresolved: The current research has focused on shorter prompts and single-turn conversations, and the impact of context length on model safety has not been thoroughly explored.
- What evidence would resolve it: Evaluations of LLM performance on tasks involving full documents, books, and extended multi-turn conversations, with a focus on safety metrics and the identification of potential failure modes in these scenarios.

## Limitations

- The semi-synthetic datasets may not fully capture real-world enterprise safety concerns due to limited validation of their representativeness
- The BEST-OF metric lacks empirical validation against human judgment for reliability across diverse answer types
- Findings may not generalize to other model architectures, sizes, or safety domains beyond the four evaluated models

## Confidence

- High confidence: Basic experimental setup and methodology are clearly specified and reproducible
- Medium confidence: Claims about relative model performance and dataset construction methodology
- Low confidence: Generalizability of findings to broader LLM safety landscape and enterprise applications

## Next Checks

1. Conduct blind human evaluation of a random sample (e.g., 100 responses) to validate BEST-OF metric accuracy against human judgment of safety and correctness

2. Create adversarial prompts specifically designed to expose weaknesses in the semi-synthetic datasets, testing whether they actually measure the intended safety capabilities

3. Evaluate additional model variants (different sizes, architectures, and training approaches) to determine if the observed performance patterns hold across a broader model landscape