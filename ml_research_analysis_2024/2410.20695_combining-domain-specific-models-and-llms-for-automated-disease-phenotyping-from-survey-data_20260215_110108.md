---
ver: rpa2
title: Combining Domain-Specific Models and LLMs for Automated Disease Phenotyping
  from Survey Data
arxiv_id: '2410.20695'
source_url: https://arxiv.org/abs/2410.20695
tags:
- llms
- disease
- data
- bern2
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigated combining a domain-specific biomedical
  NER/NEN model (BERN2) with large language models (LLMs) to enhance automated disease
  phenotyping from survey data. BERN2 was adapted for batch processing of survey data
  and achieved high performance (F1: 87.9% for mention recognition, 83.1% for concept
  normalization) against manually curated ground truth.'
---

# Combining Domain-Specific Models and LLMs for Automated Disease Phenotyping from Survey Data

## Quick Facts
- **arXiv ID**: 2410.20695
- **Source URL**: https://arxiv.org/abs/2410.20695
- **Reference count**: 21
- **Primary result**: Combining domain-specific biomedical models with LLMs, particularly using few-shot inference with retrieval-augmented generation (FSI+RAG), substantially improves automated disease phenotyping from survey data (F1 up to 97%).

## Executive Summary
This study investigates the integration of domain-specific biomedical models with large language models (LLMs) to enhance automated disease phenotyping from survey data. The research adapts BERN2, a biomedical named entity recognition and normalization model, for batch processing of survey data, achieving high performance in mention recognition (F1: 87.9%) and concept normalization (F1: 83.1%). The study explores various LLM integration techniques, finding that while zero-shot prompting and fine-tuning show moderate improvements, the FSI+RAG approach delivers substantial performance gains. The findings suggest that combining domain-specific models with structured examples and external knowledge offers a promising approach for processing large-scale research datasets.

## Method Summary
The study employs a hybrid approach combining BERN2, a domain-specific biomedical NER/NEN model, with large language models (LLMs) for disease phenotyping from survey data. BERN2 is adapted for batch processing and achieves high performance metrics against manually curated ground truth data. Multiple LLM integration strategies are explored, including zero-shot prompting, fine-tuning on survey data, chain-of-thought prompting, and few-shot inference with retrieval-augmented generation (FSI+RAG). The FSI+RAG approach uses structured examples and external knowledge retrieval to enhance the LLM's understanding of biomedical concepts within survey responses. Performance is evaluated using standard metrics including precision, recall, and F1 scores across different components of the phenotyping pipeline.

## Key Results
- BERN2 adaptation achieved high performance: F1 scores of 87.9% for mention recognition and 83.1% for concept normalization
- Zero-shot LLM prompting showed moderate accuracy improvements but was inconsistent across different survey contexts
- Fine-tuning LLMs on survey data yielded minimal performance gains, suggesting limited benefit from domain adaptation alone
- FSI+RAG approach with structured examples and external knowledge retrieval delivered substantial improvements, achieving F1 scores up to 97%
- Chain-of-thought prompting did not enhance results, indicating that complex reasoning chains may not be necessary for this task
- Default embeddings outperformed alternative embedding strategies in the LLM integration pipeline

## Why This Works (Mechanism)
The FSI+RAG approach succeeds by combining the strengths of domain-specific models with the reasoning capabilities of LLMs. BERN2 provides accurate biomedical entity recognition and normalization, while the LLM leverages retrieval-augmented generation to access relevant external knowledge and structured examples. This combination addresses the limitations of both approaches: domain models struggle with contextual understanding and novel expressions, while LLMs may lack precision in specialized domains without proper grounding. The structured examples in few-shot learning help the LLM understand the specific requirements of disease phenotyping from survey data, while retrieval ensures access to up-to-date medical knowledge and terminology.

## Foundational Learning
- **Domain-specific NER/NEN models**: Specialized models trained on biomedical literature provide high accuracy for entity recognition and normalization in medical contexts, essential for handling technical terminology and standardized medical concepts
- **LLM prompting strategies**: Different prompting approaches (zero-shot, few-shot, chain-of-thought) significantly impact LLM performance, requiring careful selection based on task complexity and available training examples
- **Retrieval-augmented generation**: RAG enhances LLM responses by providing access to external knowledge bases, crucial for maintaining accuracy in rapidly evolving medical domains
- **Few-shot learning effectiveness**: Demonstrates that LLMs can achieve strong performance with limited examples when provided with well-structured prompts and relevant context
- **Embedding selection impact**: Default embeddings can outperform specialized alternatives, suggesting that simpler approaches may sometimes be more effective for specific tasks

## Architecture Onboarding

**Component Map**: BERN2 -> LLM Pipeline -> RAG System -> Output Layer

**Critical Path**: Survey data input → BERN2 entity recognition → Context enrichment → LLM inference with RAG → Normalized disease concepts → Output validation

**Design Tradeoffs**: Domain model accuracy vs. LLM flexibility; computational cost of RAG retrieval vs. response quality; prompt complexity vs. inference speed; fine-tuning investment vs. few-shot performance

**Failure Signatures**: 
- BERN2 errors manifest as missed entities or incorrect normalization codes
- LLM failures appear as hallucinated concepts or misinterpretation of survey context
- RAG failures result in irrelevant knowledge retrieval or incomplete concept coverage
- Integration issues cause cascading errors when one component's output doesn't match another's input requirements

**3 First Experiments**:
1. Baseline performance comparison: BERN2-only vs. combined approach on held-out survey data
2. RAG retrieval effectiveness: Measure knowledge base hit rates and their correlation with final performance
3. Prompt engineering ablation: Test different few-shot example structures to optimize LLM performance

## Open Questions the Paper Calls Out
The paper identifies several open questions for future research, including the generalizability of the FSI+RAG approach across different disease domains and survey types, the optimal balance between domain model specialization and LLM generalization, and the potential for automated prompt engineering to further improve performance. The authors also question whether the approach can scale to handle larger datasets with more complex disease relationships and whether similar techniques could be applied to other types of biomedical text beyond survey data.

## Limitations
- Evaluation focused on a single survey dataset with specific disease context, limiting generalizability
- Lack of statistical significance testing in performance comparisons between approaches
- No examination of potential biases in survey data or handling of ambiguous/incomplete responses
- Limited exploration of alternative prompting strategies beyond those tested

## Confidence

**High confidence**:
- BERN2's baseline performance metrics and adaptation methodology are well-established and validated

**Medium confidence**:
- LLM integration techniques' relative effectiveness requires more comparative analysis across diverse datasets
- FSI+RAG improvements are promising but need replication across different medical domains

**Low confidence**:
- Chain-of-thought prompting results due to negative findings without exploration of alternative reasoning approaches

## Next Checks

1. Replicate the FSI+RAG approach across multiple disease domains and survey types to assess generalizability of performance gains
2. Conduct ablation studies to isolate the specific contribution of retrieval-augmented generation versus other components in the pipeline
3. Perform head-to-head statistical comparison between BERN2-only and combined approaches using significance testing to validate performance differences