---
ver: rpa2
title: 'Few and Fewer: Learning Better from Few Examples Using Fewer Base Classes'
arxiv_id: '2401.15834'
source_url: https://arxiv.org/abs/2401.15834
tags:
- classes
- feature
- learning
- few-shot
- base
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether fine-tuning a pre-trained feature
  extractor on fewer, more relevant base classes can improve few-shot learning performance.
  The authors propose selecting subsets of base classes based on their activations
  on target domain examples (Domain-Informed) or few-shot support sets (Task-Informed),
  then fine-tuning the feature extractor on these subsets.
---

# Few and Fewer: Learning Better from Few Examples Using Fewer Base Classes
## Quick Facts
- arXiv ID: 2401.15834
- Source URL: https://arxiv.org/abs/2401.15834
- Reference count: 40
- Key outcome: Domain-Informed selection of base classes improves few-shot learning accuracy by 1.73% on average across eight domains.

## Executive Summary
This paper explores whether fine-tuning a pre-trained feature extractor on fewer, more relevant base classes can improve few-shot learning performance. The authors propose selecting subsets of base classes based on their activations on target domain examples (Domain-Informed) or few-shot support sets (Task-Informed). They evaluate this approach across eight domains from Meta-Dataset in both settings, plus an Uninformed setting where class subsets are chosen without task knowledge. Results show that Domain-Informed selection significantly improves accuracy in most cases, with an average boost of 1.73%. For the Uninformed setting, a static library of feature extractors fine-tuned on different class subsets can be effectively selected at runtime using simple heuristics like support set accuracy, yielding consistent accuracy improvements.

## Method Summary
The paper proposes selecting subsets of base classes for fine-tuning a feature extractor based on their relevance to target tasks. In the Domain-Informed setting, class subsets are chosen by analyzing activations on target domain examples. In the Task-Informed setting, selection is based on activations on few-shot support sets. The Uninformed setting uses predetermined class subsets without task-specific knowledge. The approach involves fine-tuning a pre-trained feature extractor on these selected subsets, then using the resulting model for few-shot learning tasks. A static library of feature extractors is maintained for the Uninformed setting, with runtime selection based on simple heuristics like support set accuracy.

## Key Results
- Domain-Informed selection improves few-shot learning accuracy by 1.73% on average across eight domains.
- The Uninformed static library approach with simple heuristics yields consistent accuracy improvements.
- Both Domain-Informed and Task-Informed settings show significant performance gains in most cases.

## Why This Works (Mechanism)
The proposed approach works by focusing the feature extractor's learning on base classes that are most relevant to the target domain or task. By fine-tuning on fewer, more representative classes, the model can develop more discriminative features for the few-shot learning task. The Domain-Informed selection leverages target domain examples to identify relevant base classes, while Task-Informed selection uses support set information. The Uninformed setting relies on pre-determined class subsets and runtime selection based on task-specific cues, demonstrating that even without direct task knowledge, a diverse set of fine-tuned models can be effectively leveraged.

## Foundational Learning
- **Few-shot learning**: Learning from very limited examples requires models to generalize from small support sets. Needed to understand the core problem being addressed.
- **Feature extraction**: The process of converting raw data into meaningful representations for downstream tasks. Critical for understanding how the model processes input.
- **Fine-tuning**: Adapting a pre-trained model to a specific task by further training on relevant data. Key to the proposed method's approach.
- **Class subset selection**: Choosing a subset of classes from a larger set based on specific criteria. Central to the paper's main contribution.
- **Activation analysis**: Examining model responses to understand feature representations. Used for selecting relevant base classes.
- **Meta-Dataset**: A benchmark dataset containing multiple domains for evaluating few-shot learning algorithms. The evaluation framework used in the paper.

## Architecture Onboarding
**Component Map**: Pre-trained feature extractor -> Class subset selector -> Fine-tuned feature extractor -> Few-shot learner

**Critical Path**: The critical path involves selecting relevant base classes, fine-tuning the feature extractor on these classes, and then using the fine-tuned model for few-shot learning tasks.

**Design Tradeoffs**: The approach trades increased computational cost (maintaining multiple fine-tuned models) for potential accuracy gains. It also requires careful selection of class subsets, which may not always be straightforward in real-world scenarios.

**Failure Signatures**: The approach may fail when the selected base classes are not truly representative of the target domain or when the support sets are too small to provide reliable selection signals. It may also underperform if the class subsets are poorly chosen or if the computational overhead outweighs the accuracy benefits.

**First 3 Experiments**:
1. Domain-Informed selection: Fine-tune on class subsets selected based on target domain activations.
2. Task-Informed selection: Fine-tune on class subsets selected based on support set activations.
3. Uninformed setting: Maintain a static library of fine-tuned models and select at runtime using support set accuracy.

## Open Questions the Paper Calls Out
None

## Limitations
- Domain-Informed method requires access to target domain examples, which may not be available in all deployment scenarios.
- Uninformed setting's effectiveness relies on the assumption that class subsets can be predetermined and reliably selected at runtime.
- The 1.73% average accuracy improvement may not justify the additional computational overhead in resource-constrained environments.

## Confidence
- Domain-Informed selection improves few-shot performance: High confidence
- Uninformed static library approach works effectively: Medium confidence
- Class subset selection based on activations is the key mechanism: Medium confidence

## Next Checks
1. Test the Uninformed setting's static library approach on domains with high class overlap or ambiguous class boundaries to evaluate robustness when support set-based selection heuristics may fail.
2. Conduct ablation studies comparing activation-based class selection with alternative methods such as random selection, class similarity metrics, or meta-learning-based approaches to isolate the contribution of the specific selection mechanism.
3. Evaluate the computational trade-offs between maintaining multiple fine-tuned feature extractors versus the accuracy gains, particularly focusing on scenarios with limited storage or inference time constraints.