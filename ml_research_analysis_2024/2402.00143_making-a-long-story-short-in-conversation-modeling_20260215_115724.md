---
ver: rpa2
title: Making a Long Story Short in Conversation Modeling
arxiv_id: '2402.00143'
source_url: https://arxiv.org/abs/2402.00143
tags:
- utterance
- responses
- language
- generated
- dialogue
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work studies how utterance length affects the quality of responses
  generated by conversational models. Using GPT-3 and five dialogue datasets, the
  authors compress long utterances to shorter versions while keeping meaning intact,
  then generate follow-up responses and evaluate them against reference utterances.
---

# Making a Long Story Short in Conversation Modeling

## Quick Facts
- arXiv ID: 2402.00143
- Source URL: https://arxiv.org/abs/2402.00143
- Authors: Yufei Tao; Tiernan Mines; Ameeta Agrawal
- Reference count: 10
- Primary result: Compressing utterances by up to 72% results in only 1-2% drop in response quality

## Executive Summary
This paper investigates how utterance length affects conversational model performance by compressing long dialogue utterances while preserving meaning. Using GPT-3 and five dialogue datasets, the authors compress answer utterances (U2) to shorter versions and generate follow-up responses (U3) for both original and compressed versions. Automatic metrics (ROUGE-L, METEOR, BERTScore) and human evaluation show that response quality remains largely unchanged despite significant input compression, suggesting computational efficiency gains are possible without sacrificing output quality.

## Method Summary
The study uses five dialogue datasets containing question-answer-followup triplets. GPT-3 compresses longer answer utterances (U2) into shorter versions while maintaining semantic meaning. The model then generates follow-up responses for both original (U2long) and compressed (U2short) versions. These generated responses are evaluated against reference follow-up utterances using ROUGE-L, METEOR, and BERTScore metrics, with human evaluators also assessing response quality and semantic similarity between compressed and original utterances.

## Key Results
- Utterance length reduction of up to 72% resulted in only 1-2% drop in response quality metrics
- Generated responses maintained similar length and quality regardless of input compression
- Human evaluation confirmed that compressed inputs produced responses nearly as good as those from original inputs
- Computational efficiency gains appear possible without significant quality trade-offs

## Why This Works (Mechanism)
None provided in source material.

## Foundational Learning
- **Dialogue datasets**: Why needed - provide realistic conversational context for testing utterance compression effects; Quick check - verify datasets contain question-answer-followup structure
- **Automatic evaluation metrics**: Why needed - enable quantitative comparison of generated responses; Quick check - ensure metrics align with human judgment of response quality
- **Human evaluation**: Why needed - validate automatic metrics and assess qualitative aspects of response quality; Quick check - confirm inter-rater reliability and clear evaluation criteria
- **GPT-3 compression**: Why needed - generate compressed versions of utterances while preserving meaning; Quick check - verify semantic preservation through manual validation
- **Semantic similarity**: Why needed - ensure compressed utterances retain meaning of original; Quick check - compare compressed vs original utterances using multiple similarity measures

## Architecture Onboarding

**Component Map:**
Dialogue datasets -> GPT-3 compression -> Response generation -> Evaluation (automatic metrics + human)

**Critical Path:**
1. Extract triplets from dialogue datasets
2. Compress U2 utterances using GPT-3
3. Generate U3 responses for both original and compressed U2
4. Evaluate responses against reference U3

**Design Tradeoffs:**
- GPT-3-based compression vs heuristic methods: GPT-3 provides better semantic preservation but at higher computational cost
- Automatic vs human evaluation: Automatic metrics enable large-scale analysis but may miss nuanced quality aspects
- Compression degree: Higher compression yields more efficiency but risks losing critical context

**Failure Signatures:**
- Overly verbose responses compared to reference U3
- Loss of critical context during compression
- Mismatch between automatic and human evaluation results

**First Experiments:**
1. Test compression on small subset with manual validation of semantic preservation
2. Compare response quality metrics for different compression ratios
3. Run human evaluation on sample responses to validate automatic metrics

## Open Questions the Paper Calls Out
**Open Question 1:** What specific aspects of utterance length (semantic density, syntactic complexity) most strongly correlate with follow-up response quality? The study measured overall length reduction without analyzing which linguistic features contribute most to response quality.

**Open Question 2:** How do different compression techniques (heuristic vs model-based) affect response quality compared to GPT-3-based compression? Authors suggest heuristic methods would be more efficient but were not tested.

**Open Question 3:** Does effectiveness of utterance length reduction vary across different conversational contexts beyond question-answer pairs? The study only examined question-answer-followup triplets, not other conversational structures.

**Open Question 4:** How do newer language models (GPT-4, open-source alternatives) compare to GPT-3 in maintaining response quality with compressed inputs? The study only used GPT-3, limiting comparison with newer models.

## Limitations
- Evaluation relies on GPT-3 for both compression and response generation without standardized compression parameters
- Manual validation criteria for semantic similarity lack standardization and reproducibility
- Computational efficiency claims lack empirical validation with actual compute cost measurements

## Confidence
- **High Confidence**: Core finding of minimal quality degradation (1-2%) with 72% compression is well-supported by multi-metric evaluation
- **Medium Confidence**: Computational efficiency claims are logical but lack empirical validation
- **Medium Confidence**: Generalization across five datasets strengthens findings, though dataset-specific effects may exist

## Next Checks
1. Conduct ablation studies varying GPT-3 compression parameters to quantify their impact on response quality
2. Implement standardized semantic similarity scoring to replace manual filtering for reproducible validation
3. Measure actual inference time and token costs for compressed vs uncompressed scenarios to validate efficiency claims