---
ver: rpa2
title: A Semantic Distance Metric Learning approach for Lexical Semantic Change Detection
arxiv_id: '2403.00226'
source_url: https://arxiv.org/abs/2403.00226
tags:
- sense-aware
- distance
- word
- semantic
- https
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a supervised two-stage approach for detecting
  semantic changes of words over time. The first stage learns a sense-aware encoder
  from WiC datasets to represent the meaning of a word in a sentence.
---

# A Semantic Distance Metric Learning approach for Lexical Semantic Change Detection

## Quick Facts
- arXiv ID: 2403.00226
- Source URL: https://arxiv.org/abs/2403.00226
- Reference count: 40
- Proposes a supervised two-stage approach for lexical semantic change detection with 2-5% improvements over prior methods

## Executive Summary
This paper introduces a novel two-stage approach for detecting semantic changes of words over time. The method combines sense-aware encoding with distance metric learning to improve semantic change detection accuracy. The first stage learns sense-aware representations using WiC datasets, while the second stage learns a distance metric to compare word meanings across different time periods. Experimental results demonstrate state-of-the-art performance across multiple benchmarks.

## Method Summary
The proposed approach consists of two main stages: sense-aware encoder training and distance metric learning. The sense-aware encoder is trained on Word-in-Context (WiC) datasets to capture contextual meaning of words in sentences. This encoder produces embeddings that are sensitive to different word senses. The distance metric learning stage then learns to compare these sense-aware embeddings across two corpora, identifying semantic shifts by measuring the distance between word representations in different time periods.

## Key Results
- Achieves state-of-the-art performance on multiple SCD benchmarks
- Demonstrates 2-5% improvements over prior methods
- Shows that sense-aware embeddings contain specialized dimensions related to semantic change
- Distance metric effectively exploits these specialized dimensions for improved detection

## Why This Works (Mechanism)
The approach works by first creating sense-aware embeddings that capture the contextual meaning of words, then learning an optimal distance metric to compare these embeddings across time periods. The sense-aware encoding allows the model to distinguish between different meanings of a word in various contexts, while the distance metric learning stage learns to effectively measure semantic distance between these contextualized representations. This combination enables more accurate detection of genuine semantic changes while filtering out contextual variations that don't represent true meaning shifts.

## Foundational Learning
- Word-in-Context (WiC) datasets: Essential for training sense-aware encoders by providing pairs of sentences where words have the same or different meanings
  - Why needed: Provides labeled data for distinguishing word senses in context
  - Quick check: Verify WiC dataset quality and coverage across target words

- Distance metric learning: Required for learning optimal comparison functions between word representations
  - Why needed: Enables effective measurement of semantic distance across time periods
  - Quick check: Validate learned metric generalizes beyond training data

- Sense-aware embeddings: Critical for capturing contextual meaning variations
  - Why needed: Distinguishes between different senses of words in various contexts
  - Quick check: Test embedding quality on word sense disambiguation tasks

## Architecture Onboarding

**Component Map:** WiC Dataset -> Sense-aware Encoder -> Distance Metric Learner -> Semantic Change Detector

**Critical Path:** The critical path involves training the sense-aware encoder on WiC datasets, then using these learned representations to train the distance metric learner, which ultimately drives the semantic change detection.

**Design Tradeoffs:** The method trades computational complexity for accuracy by using a two-stage approach rather than end-to-end learning. This allows for more specialized learning at each stage but requires careful coordination between components.

**Failure Signatures:** Performance degradation may occur when:
- WiC datasets don't adequately represent target word senses
- Distance between corpora is too large for effective metric learning
- Training data lacks sufficient examples of semantic change

**First Experiments:**
1. Validate sense-aware encoder on WiC test set
2. Test distance metric on synthetic semantic change examples
3. Evaluate complete system on controlled benchmark with known changes

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Evaluation methodology may not generalize across diverse language pairs and domains
- Sense-aware embeddings' specialized dimensions need more detailed validation
- Performance under varying corpus sizes and quality levels is not extensively discussed

## Confidence
- **High confidence**: Technical implementation of two-stage approach and reported benchmark results
- **Medium confidence**: General effectiveness of sense-aware embeddings for semantic change detection
- **Low confidence**: Claims about specific dimensions' role in semantic change and generalization to diverse languages/domains

## Next Checks
1. Test method's performance across additional language pairs and domains, particularly low-resource languages and specialized technical domains
2. Conduct ablation studies to isolate contribution of sense-aware encoding versus distance metric learning
3. Evaluate model's robustness using varying corpus sizes and quality levels, including historical text data with noise and limited availability