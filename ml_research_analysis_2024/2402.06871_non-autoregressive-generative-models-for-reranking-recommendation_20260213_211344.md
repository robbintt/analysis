---
ver: rpa2
title: Non-autoregressive Generative Models for Reranking Recommendation
arxiv_id: '2402.06871'
source_url: https://arxiv.org/abs/2402.06871
tags:
- sequence
- training
- items
- reranking
- recommendation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the efficiency and effectiveness challenges
  of autoregressive generative models in real-time recommendation reranking systems.
  The authors propose NAR4Rec, a non-autoregressive generative model that simultaneously
  generates all items in a target sequence.
---

# Non-autoregressive Generative Models for Reranking Recommendation

## Quick Facts
- arXiv ID: 2402.06871
- Source URL: https://arxiv.org/abs/2402.06871
- Authors: Yuxin Ren; Qiya Yang; Yichun Wu; Wei Xu; Yalong Wang; Zhiqiang Zhang
- Reference count: 39
- Key outcome: NAR4Rec achieves 74.86% Recall@6 vs 73.63% for Edge-Rerank with 1.161% increase in views

## Executive Summary
This paper addresses the efficiency and effectiveness challenges of autoregressive generative models in real-time recommendation reranking systems. The authors propose NAR4Rec, a non-autoregressive generative model that simultaneously generates all items in a target sequence. To tackle sparse training samples and dynamic candidates, they introduce a matching model with shared position embeddings. They employ sequence-level unlikelihood training to differentiate between feasible and unfeasible sequences, and contrastive decoding to capture item dependencies. Extensive offline experiments demonstrate superior performance over state-of-the-art methods, and online A/B tests show significant user experience improvements. The model has been fully deployed in Kuaishou with over 300 million daily active users.

## Method Summary
NAR4Rec is a non-autoregressive generative model that generates all items in a target sequence simultaneously rather than sequentially. The model uses two main components: a candidates encoder and a position encoder, both implemented as Transformer layers. The candidates encoder processes item representations through self-attention, while the position encoder uses cross-attention to capture position-specific information. The model employs shared position embeddings across training data to handle sparse sequences, sequence-level unlikelihood training to differentiate between high-utility and low-utility sequences, and contrastive decoding to capture item dependencies. During inference, the model computes item distributions independently for each position, enabling full parallelization.

## Key Results
- NAR4Rec achieves 74.86% Recall@6 compared to 73.63% for Edge-Rerank
- Online A/B tests show 1.161% increase in views after deployment
- The model is fully deployed in Kuaishou with over 300 million daily active users

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Non-autoregressive generation improves inference speed by generating all items simultaneously rather than sequentially.
- Mechanism: The model computes item distributions independently for each position in the target sequence, allowing full parallelization during inference.
- Core assumption: The sequential dependencies captured by autoregressive models can be approximated through other mechanisms (contrastive decoding) without significant performance loss.
- Evidence anchors:
  - [abstract] "Unlike autoregressive models, which generate sequences step by step, relying on their own previous outputs, NAR4Rec generates all items within the target sequence simultaneously."
  - [section] "Non-autoregressive sequence generation... eliminates autoregressive dependencies from existing models. Each element's distribution p(y_i) depends solely on the candidates X"
- Break condition: When item dependencies are too complex to capture through contrastive decoding alone, performance degrades compared to autoregressive approaches.

### Mechanism 2
- Claim: Shared position embeddings help address sparse training data by allowing the model to learn position-specific patterns across different sequences.
- Mechanism: The model randomly initializes embeddings for each position in target sequences and shares these across training data, enabling learning from limited data where the same position may contain different items.
- Core assumption: Position patterns are transferable across different sequences, so learning position embeddings once benefits all sequences.
- Evidence anchors:
  - [section] "Initially, we randomly initialize an embedding for each position in the target sequences. Notably, we share these position embeddings across training data to enhance learning on sparse data."
  - [section] "The sparse nature of training sequences presents learning difficulties... We introduce two key components to our models: a candidates encoder for effectively encoding representations of candidates and a position encoder to capture position-specific information"
- Break condition: When position patterns vary significantly across different contexts, shared embeddings become a bottleneck.

### Mechanism 3
- Claim: Sequence-level unlikelihood training helps the model distinguish between high-utility and low-utility sequences in recommendation contexts.
- Mechanism: The model applies a modified loss function that reduces probability for negative sequences (those with low utility) while maintaining probability for positive sequences, addressing the discrepancy between training objectives and reranking goals.
- Core assumption: User feedback patterns can be used to label sequences as positive or negative, providing supervision for the unlikelihood objective.
- Evidence anchors:
  - [section] "The essence of high-quality recommendations lies not just in sequence patterns from training data but, more crucially, in the user utility of the recommended list... we propose unlikelihood training, guiding the model to assign lower probabilities to undesired generations."
  - [section] "Unlikelihood training reduces the model's probability of generating a negative sequence... the unlikelihood loss is: L_ul = -ΣΣ p_ij log(1 - p̂_ij)"
- Break condition: When utility thresholds are not well-calibrated or user feedback is too sparse to provide reliable labels.

## Foundational Learning

- Concept: Transformer architecture with self-attention and cross-attention mechanisms
  - Why needed here: The model uses multiple Transformer layers for both candidates and position encoders, requiring understanding of how self-attention captures item-item relationships and cross-attention captures item-position relationships
  - Quick check question: How does the cross-attention mechanism differ from self-attention in the NAR4Rec architecture?

- Concept: Non-autoregressive sequence generation and its limitations
  - Why needed here: The core innovation relies on generating sequences without autoregressive dependencies, which introduces challenges in capturing item dependencies that must be addressed through contrastive decoding
  - Quick check question: What is the main limitation of non-autoregressive generation that contrastive decoding aims to address?

- Concept: Contrastive learning and similarity penalties
  - Why needed here: The model uses contrastive objectives (L_item and L_position) to create discriminative, isotropic item representations and contrastive decoding to encourage diversity in recommendations
  - Quick check question: How does the contrastive objective L_item help create more discriminative item representations?

## Architecture Onboarding

- Component map: Candidates Encoder (Transformer layers) → Position Encoder (Transformer layers with cross-attention) → Probability Matrix (softmax over dot products) → Unlikelihood Training (sequence-level loss) → Contrastive Decoding (diversity regularization)
- Critical path: Input candidates → Candidate embeddings → Self-attention (candidates) → Cross-attention (candidates to positions) → Position embeddings → Probability matrix generation → Loss computation → Parameter updates
- Design tradeoffs: Speed vs. accuracy (non-autoregressive faster but potentially less accurate), shared vs. separate position embeddings (parameter efficiency vs. specificity), unlikelihood vs. likelihood training (utility alignment vs. sequence likelihood)
- Failure signatures: Poor diversity in recommendations (contrastive decoding issue), slow training convergence (position embeddings or matching model issue), suboptimal sequence utility (unlikelihood training threshold issue)
- First 3 experiments:
  1. Test inference speed improvement by comparing NAR4Rec vs Seq2Slate on sequence generation time
  2. Evaluate the impact of shared position embeddings by training with and without sharing
  3. Measure the effect of unlikelihood training by comparing vanilla vs. unlikelihood training on sequence utility metrics

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation is limited to a single platform (Kuaishou) and may not generalize to different recommendation domains or user populations
- The mechanism of contrastive decoding for capturing item dependencies remains somewhat theoretical with limited ablation study validation
- The unlikelihood training approach depends heavily on the quality of negative sequence sampling and utility threshold selection

## Confidence

- **High confidence**: The non-autoregressive generation architecture and its speed benefits are well-established in the literature and clearly implemented in NAR4Rec
- **Medium confidence**: The shared position embeddings mechanism is reasonable and well-motivated, but the extent to which position patterns are truly transferable across sequences is not extensively validated
- **Medium confidence**: The unlikelihood training and contrastive decoding mechanisms show promise through end-to-end results, but the individual contributions of these components to overall performance are not clearly isolated

## Next Checks

1. **Ablation study for contrastive decoding**: Conduct controlled experiments removing contrastive decoding while keeping other components constant to isolate its specific contribution to capturing item dependencies and overall recommendation quality

2. **Cross-domain generalization test**: Deploy NAR4Rec on at least two different recommendation domains (e.g., e-commerce and news) to assess whether the model's performance gains generalize beyond the Kuaishou platform

3. **Sensitivity analysis for unlikelihood training**: Systematically vary the negative sampling strategy and utility threshold parameters to determine the robustness of the unlikelihood training approach and identify optimal hyperparameter ranges