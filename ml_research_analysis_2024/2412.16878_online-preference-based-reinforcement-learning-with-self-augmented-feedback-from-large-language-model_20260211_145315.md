---
ver: rpa2
title: Online Preference-based Reinforcement Learning with Self-augmented Feedback
  from Large Language Model
arxiv_id: '2412.16878'
source_url: https://arxiv.org/abs/2412.16878
tags:
- trajectory
- reward
- learning
- feedback
- rl-sallm-f
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of online preference-based reinforcement
  learning (PbRL) without relying on real-time human feedback or privileged rewards.
  The authors propose RL-SaLLM-F, which leverages a large language model (LLM) to
  generate self-augmented trajectories and provide preference labels for reward learning.
---

# Online Preference-based Reinforcement Learning with Self-augmented Feedback from Large Language Model

## Quick Facts
- arXiv ID: 2412.16878
- Source URL: https://arxiv.org/abs/2412.16878
- Reference count: 40
- Method achieves comparable performance to privileged rewards without real-time human feedback or predefined rewards

## Executive Summary
This paper presents RL-SaLLM-F, a novel approach for online preference-based reinforcement learning that eliminates the need for real-time human feedback or predefined rewards. The method leverages a large language model to generate self-augmented trajectories and provide preference labels for reward learning. By addressing the "query ambiguity" problem through a double-check mechanism and LLM-generated imagined trajectories, the approach achieves performance comparable to systems using privileged rewards. The experimental results on MetaWorld benchmarks demonstrate the practical viability of this new paradigm for PbRL.

## Method Summary
RL-SaLLM-F operates by having an LLM generate imagined trajectories that better achieve task goals, then using these trajectories to provide preference labels for reward learning. The system tackles the query ambiguity problem inherent in PbRL by implementing a double-check mechanism that verifies the quality and consistency of the generated preferences. During online training, the agent collects trajectories, which are then processed by the LLM to generate self-augmented versions with improved task completion. The LLM provides pairwise preferences between trajectories, which are used to train a reward model that guides the reinforcement learning agent. This creates a closed-loop system where the LLM continuously improves the quality of feedback without requiring human intervention.

## Key Results
- Achieves 72.3% average label accuracy on preference classification
- Performance on par with SAC using predefined rewards on MetaWorld benchmarks
- Matches or exceeds performance of feedback from a "scripted teacher" with privileged rewards
- Successfully operates without any predefined reward functions or real-time human interaction

## Why This Works (Mechanism)
The mechanism works by leveraging the LLM's ability to understand task goals and generate trajectories that better satisfy these goals compared to the agent's current policy. The double-check mechanism addresses the fundamental ambiguity in preference-based learning where the "better" trajectory may not be clearly defined. By having the LLM imagine trajectories from a task-completion perspective rather than just evaluating what the current agent does, the system provides more informative feedback signals. The self-augmentation process creates a curriculum of increasingly better trajectories that guide the agent toward optimal behavior without requiring expensive human annotation or pre-defined reward functions.

## Foundational Learning
- **Preference-based Reinforcement Learning**: Why needed - provides a way to learn without explicit reward functions; Quick check - verify the agent can learn from pairwise comparisons rather than scalar rewards
- **Large Language Model Integration**: Why needed - generates high-quality imagined trajectories and resolves ambiguity in preferences; Quick check - test LLM's ability to generate task-completing trajectories for simple environments
- **Double-check Mechanism**: Why needed - addresses query ambiguity where multiple trajectories may appear equally good; Quick check - verify the mechanism reduces inconsistent preference labels
- **Self-augmented Trajectory Generation**: Why needed - creates better training examples than the agent's current policy; Quick check - compare trajectory quality metrics between original and augmented versions
- **Online Learning Framework**: Why needed - enables continuous improvement without offline pretraining; Quick check - measure learning curves during active training sessions
- **Reward Modeling from Preferences**: Why needed - converts pairwise comparisons into learnable reward signals; Quick check - validate reward model accuracy on held-out preference pairs

## Architecture Onboarding

Component Map: Agent -> Trajectory Collector -> LLM -> Preference Generator -> Reward Model -> Agent

Critical Path: The agent collects trajectories → LLM generates imagined trajectories and provides preferences → Reward model is trained on these preferences → Agent policy is updated using the learned reward → Cycle repeats

Design Tradeoffs:
- LLM dependency vs. performance: More powerful LLMs provide better trajectories but increase computational cost
- Preference accuracy vs. query efficiency: More detailed preferences improve learning but require more LLM processing
- Self-augmentation frequency vs. stability: Frequent augmentation provides better guidance but may cause policy instability

Failure Signatures:
- Low preference accuracy indicates LLM misunderstanding of task goals
- Policy collapse suggests reward model overfitting to LLM preferences
- Slow learning indicates insufficient diversity in self-augmented trajectories
- High variance in performance suggests unstable preference generation

First Experiments:
1. Test preference accuracy on simple navigation tasks with clear optimal solutions
2. Compare learning curves with and without self-augmentation on basic manipulation tasks
3. Evaluate robustness by varying LLM temperature and sampling parameters

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Reliance on powerful LLMs may limit deployment in resource-constrained environments
- 72.3% label accuracy, while reasonable, indicates room for improvement in preference quality
- Limited evaluation to MetaWorld benchmarks may not reflect real-world complexity
- Computational overhead of LLM processing could be substantial for complex tasks

## Confidence
- RL-SaLLM-F achieving comparable performance to privileged rewards: High confidence
- Self-augmented trajectories reducing query ambiguity: Medium confidence
- Establishing a practical new paradigm for online PbRL: Low confidence

## Next Checks
1. **Generalization Testing**: Evaluate RL-SaLLM-F on continuous control tasks beyond MetaWorld, particularly those with higher dimensional state spaces and more complex dynamics to assess scalability.

2. **LLM Dependency Analysis**: Conduct ablation studies varying LLM model sizes and capabilities to quantify the minimum requirements for successful performance and identify bottlenecks.

3. **Real-world Deployment Simulation**: Create a simulated real-world deployment scenario with noisy sensors and partial observability to test the robustness of the self-augmented feedback mechanism under realistic conditions.