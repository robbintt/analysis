---
ver: rpa2
title: Chatbot-Based Ontology Interaction Using Large Language Models and Domain-Specific
  Standards
arxiv_id: '2408.00800'
source_url: https://arxiv.org/abs/2408.00800
tags:
- queries
- sparql
- ontology
- ontologies
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a concept to facilitate user interaction with
  ontologies by leveraging Large Language Models (LLMs) to generate SPARQL queries
  from natural language inputs. The approach integrates domain-specific standards
  into ontologies to improve query accuracy and prevent misinformation.
---

# Chatbot-Based Ontology Interaction Using Large Language Models and Domain-Specific Standards

## Quick Facts
- arXiv ID: 2408.00800
- Source URL: https://arxiv.org/abs/2408.00800
- Authors: Jonathan Reif; Tom Jeleniewski; Milapji Singh Gill; Felix Gehlhoff; Alexander Fay
- Reference count: 15
- Primary result: LLM-generated SPARQL queries achieve 100% accuracy for Boolean, Count, and Rank questions when using ontologies with rdfs:comment annotations

## Executive Summary
This paper proposes a novel approach for user interaction with ontologies using Large Language Models (LLMs) to generate SPARQL queries from natural language inputs. The system integrates domain-specific standards into ontologies to improve query accuracy and prevent misinformation. A chatbot interface allows intuitive querying while ensuring the LLM only generates queries without accessing factual content. Preliminary experiments using ChatGPT-4o on industrial ontologies showed high accuracy for simpler queries and improved results when ontologies included rdfs:comment annotations. The study demonstrates the potential of LLMs for ontology querying while emphasizing the need for further validation and error reduction in complex queries.

## Method Summary
The approach leverages LLMs to convert natural language questions into SPARQL queries by providing the LLM with TBox information and predefined prompts. The system uses Ontology Design Patterns (ODPs) aligned with domain standards (VDI 3682, DIN EN 61360, VDI 2206) to ensure consistent terminology. The LLM generates queries without accessing the ABox data directly, maintaining security and accuracy. Evaluation was conducted using ChatGPT-4o with questions categorized into seven types (Boolean, Count, Rank, Simple, String, Two Hop, Two Intent) across ontologies with and without rdfs:comment annotations.

## Key Results
- LLM-generated SPARQL queries achieved 100% accuracy for Boolean, Count, and Rank questions when using ontologies with rdfs:comment annotations
- ODPs augmented with rdfs:comment produced queries with greater precision compared to those without annotations
- The system effectively handles simple queries but shows performance degradation with complex Two Intent questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-based SPARQL query generation improves user interaction with ontologies by translating natural language into structured queries
- Mechanism: The LLM receives user questions plus the ontology's TBox in a prompt, enabling it to interpret domain-specific terms and generate syntactically correct SPARQL queries without accessing the ABox data directly
- Core assumption: The TBox alone provides sufficient context for the LLM to generate accurate queries; the ABox remains isolated for security and accuracy
- Evidence anchors:
  - [abstract] "Utilizing natural language inputs, the system converts user inquiries into accurate SPARQL queries that strictly query the factual content of the ontology"
  - [section III] "it uses the predefined prompt to transform the question into a SPARQL query"
- Break condition: If the TBox is incomplete or ambiguous, the LLM may generate incorrect or irrelevant SPARQL queries, leading to inaccurate results

### Mechanism 2
- Claim: rdfs:comment annotations enhance LLM query generation accuracy by providing contextual meaning for ontology terms
- Mechanism: When ODPs include rdfs:comment annotations, the LLM can disambiguate terms with multiple interpretations, resulting in more precise SPARQL queries aligned with the ontology's intended semantics
- Core assumption: LLMs can effectively leverage natural language comments in the ontology to improve semantic understanding and query formulation
- Evidence anchors:
  - [section III] "rdfs:comment annotations is critical, as these provide additional context about the classes, object properties, and data properties"
  - [section IV] "ODPs augmented with rdfs:comment produced queries with greater precision"
- Break condition: If rdfs:comments are absent or too vague, the LLM may misinterpret terms, reducing query accuracy

### Mechanism 3
- Claim: Using domain-specific standards within ODPs improves LLM query generation by providing consistent terminology
- Mechanism: ODPs aligned with industry standards (like VDI and DIN) provide the LLM with fixed, standardized terms that map directly to ontology concepts, reducing ambiguity in query generation
- Core assumption: Domain-specific standards offer unambiguous terminology that the LLM can reliably map to ontology concepts
- Evidence anchors:
  - [section III] "we employ Ontology Design Patterns (ODPs) that adhere to these established standards, functioning as templates for creating knowledge graphs"
  - [section IV] "the experimental study encompasses 28 questions for each ODP, resulting in a total of 84 questions with analyzed results"
- Break condition: If the user's natural language deviates significantly from standard terminology, the LLM may struggle to map questions correctly to ontology concepts

## Foundational Learning

- Concept: SPARQL query language
  - Why needed here: The system generates SPARQL queries from natural language, so understanding SPARQL syntax and semantics is essential for validating and debugging generated queries
  - Quick check question: What is the difference between a SPARQL SELECT query and a CONSTRUCT query?

- Concept: Ontology Design Patterns (ODPs)
  - Why needed here: ODPs provide the structural templates for building ontologies, and the system uses them to ensure consistent terminology aligned with domain standards
  - Quick check question: How do ODPs help maintain consistency across different ontologies in the same domain?

- Concept: RDF Schema (RDFS) and rdfs:comment
  - Why needed here: rdfs:comment annotations provide the contextual information that enhances LLM query generation accuracy by clarifying the intended meaning of ontology terms
  - Quick check question: What is the purpose of rdfs:comment in an ontology, and how does it differ from rdfs:label?

## Architecture Onboarding

- Component map: User Interface -> Backend Service -> LLM API (ChatGPT-4o) -> SPARQL Endpoint -> Ontology Repository

- Critical path: User question → Chat interface → Backend → LLM API → SPARQL query → SPARQL endpoint → Results → Backend processing → Chat interface display

- Design tradeoffs:
  - Security vs. Context: Keeping ABox separate from LLM ensures security but requires the TBox to contain sufficient information for accurate query generation
  - Prompt complexity vs. LLM performance: More detailed prompts improve accuracy but may exceed token limits or reduce response quality
  - Standard terminology vs. user flexibility: Strict adherence to standards improves LLM performance but may limit user expression

- Failure signatures:
  - Incorrect SPARQL syntax: LLM failed to properly format the query
  - No results returned: Query is syntactically correct but semantically wrong (wrong instance or property)
  - Ambiguous results: Query structure is correct but retrieves too many or irrelevant results
  - Security breach: ABox data was exposed to the LLM (should never happen in this design)

- First 3 experiments:
  1. Test LLM query generation with a simple Boolean question using an ODP with and without rdfs:comments to measure the impact of annotations
  2. Test query generation for a Count question with standard-compliant vs. non-standard-compliant phrasing to measure the impact of terminology alignment
  3. Test Two Intent question generation to identify the complexity threshold where LLM performance degrades significantly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of rdfs:comment annotations in ontologies impact the accuracy of SPARQL queries generated by LLMs?
- Basis in paper: [explicit] The paper discusses that ODPs augmented with rdfs:comment produced queries with greater precision
- Why unresolved: The paper provides preliminary results but suggests further evaluation is needed
- What evidence would resolve it: Additional experiments testing various levels of detail in rdfs:comment and their effects on query accuracy

### Open Question 2
- Question: Which LLMs are best suited for generating SPARQL queries in industrial ontology contexts?
- Basis in paper: [explicit] The paper mentions that future evaluation will focus on exploring which LLMs are best suited for the task
- Why unresolved: The paper only used ChatGPT-4o in its experiments and did not compare it with other LLMs
- What evidence would resolve it: Comparative studies of different LLMs in generating SPARQL queries across various ontologies

### Open Question 3
- Question: What strategies can be developed to reduce errors in SPARQL query generation for complex queries?
- Basis in paper: [inferred] The paper highlights the need for developing strategies to reduce errors, especially for complex queries, to enhance accuracy and reliability
- Why unresolved: The paper identifies the issue but does not provide solutions
- What evidence would resolve it: Research into error reduction techniques and their implementation in query generation systems

## Limitations
- Experimental validation is based on a relatively small dataset of 84 questions across three industrial ontologies, limiting generalizability
- The study focuses exclusively on ChatGPT-4o, raising questions about performance consistency across different LLMs
- Evaluation methodology relies on manual assessment of query correctness, which may introduce subjectivity

## Confidence
- High confidence: The separation of TBox from ABox for security and the effectiveness of rdfs:comment annotations in improving query accuracy are well-supported by experimental results
- Medium confidence: The impact of domain-specific standards on query generation accuracy requires further validation across diverse ontologies and domains
- Medium confidence: The system's scalability and performance with larger, more complex ontologies remain to be demonstrated

## Next Checks
1. **Cross-LLM Validation**: Test the approach with multiple LLM models (e.g., Claude, Gemini, Llama) to assess whether performance improvements from rdfs:comments and domain standards are consistent across different architectures

2. **Error Analysis**: Conduct detailed error analysis on incorrectly generated queries to identify specific failure patterns and determine whether errors stem from LLM limitations, prompt engineering issues, or ontology design problems

3. **Scalability Testing**: Evaluate the system's performance with larger ontologies containing thousands of classes and properties to identify performance bottlenecks and assess the approach's practical applicability in real-world scenarios