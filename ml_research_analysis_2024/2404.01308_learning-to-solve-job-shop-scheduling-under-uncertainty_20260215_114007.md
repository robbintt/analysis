---
ver: rpa2
title: Learning to Solve Job Shop Scheduling under Uncertainty
arxiv_id: '2404.01308'
source_url: https://arxiv.org/abs/2404.01308
tags:
- graph
- scheduling
- learning
- instances
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the Job Shop Scheduling Problem (JSSP) with
  uncertain task durations, aiming to minimize the average makespan. It proposes a
  novel approach that leverages Deep Reinforcement Learning (DRL) techniques, specifically
  the Wheatley method, which integrates Graph Neural Networks (GNNs) and DRL.
---

# Learning to Solve Job Shop Scheduling under Uncertainty

## Quick Facts
- arXiv ID: 2404.01308
- Source URL: https://arxiv.org/abs/2404.01308
- Reference count: 28
- The paper proposes a novel DRL approach using GNNs that outperforms classical methods on stochastic JSSPs, particularly for larger instances.

## Executive Summary
This paper addresses the Job Shop Scheduling Problem (JSSP) with uncertain task durations, aiming to minimize the average makespan. It introduces a novel approach that combines Deep Reinforcement Learning (DRL) and Graph Neural Networks (GNNs), specifically the Wheatley method. The approach represents the JSSP as a Markov Decision Process (MDP) where the agent learns to select tasks to schedule based on the current state of the graph and task attributes. The method shows improvements in generalization and scalability compared to existing DRL methods and outperforms classical techniques like CP-stoc and OR-Tools for stochastic JSSPs, particularly for larger problem instances.

## Method Summary
The Wheatley method formulates JSSP with uncertain durations as an MDP, where states are partial schedules represented as graphs, actions are task selections, and rewards are sampled makespans. A GNN encodes the relational structure of the disjunctive graph, including task precedences and machine conflicts. The model uses Proximal Policy Optimization (PPO) to learn a dispatch policy that minimizes expected makespan. The graph is rewired to include backward edges and machine cliques, enabling message passing between conflicting tasks and future tasks. Node attributes include task status, duration distribution parameters, and completion time estimates. The policy selects tasks to minimize future congestion and downstream conflicts.

## Key Results
- Wheatley outperforms CP-stoc and OR-Tools on large stochastic instances (30×20, 50×20), with makespan gaps of 1.5%-5.5%.
- The method generalizes from small (6×6, 10×10) to larger (15×15, 20×15, 50×20) instances without retraining.
- For deterministic instances, Wheatley is competitive with OR-Tools and MOPNR, setting a new benchmark for stochastic JSSPs.

## Why This Works (Mechanism)

### Mechanism 1
The GNN architecture is size-agnostic and learns to embed both local (task-level) and global (machine-level) dependencies. The rewired graph connects each task to its machine and to other tasks using the same machine, forming a clique per machine. This allows message passing between tasks that will conflict, while backward edges let the model reason about future constraints. Node embeddings include task attributes, duration distributions, and completion time estimates, which are propagated through the GNN layers. Because the number of nodes is preserved through to the action space, the learned policy can scale to unseen problem sizes.

### Mechanism 2
The method achieves robustness to uncertain durations by training on sampled scenarios and using a reward that reflects expected makespan. At each training step, the agent receives a partial schedule graph and selects the next task. Only at the end of a full schedule is a single sampled duration scenario evaluated to produce the makespan reward. Because PPO minimizes expected return over many sampled episodes, the policy learns to prefer schedules with low expected makespan under the given duration distributions. The node attributes include duration distribution parameters and propagated completion time estimates, allowing the agent to reason about risk.

### Mechanism 3
The method outperforms classical solvers on large stochastic instances because it learns a dispatch policy that anticipates downstream conflicts rather than reacting locally. The GNN-based policy can encode global information about which machines are heavily loaded and which jobs have long remaining chains. By selecting tasks that minimize future congestion, the policy implicitly builds robust schedules. Classical PDRs or even CP-stoc optimize for the sampled scenario or use heuristics without lookahead, so they are more vulnerable to worst-case realizations of uncertainty.

## Foundational Learning

- **Markov Decision Process formulation of scheduling**
  - Why needed here: The JSSP with uncertainty is modeled as an MDP where states are partial schedules, actions are task selections, and rewards are sampled makespans. This allows RL algorithms to learn a policy over the space of feasible schedules.
  - Quick check question: In the MDP, what defines the set of candidate actions at each step?
    - Answer: Actions are operations that have all predecessors scheduled and are not yet scheduled themselves.

- **Graph Neural Networks for relational reasoning**
  - Why needed here: The disjunctive graph representation of JSSP naturally maps to a graph where nodes are tasks and edges encode precedence and resource conflicts. GNNs can propagate information across these relationships to produce informed action values.
  - Quick check question: Why does the paper add "backward precedences" to the rewired graph?
    - Answer: To allow information flow from future tasks to the current decision point, enabling the agent to reason about downstream effects.

- **Reinforcement learning with stochastic rewards**
  - Why needed here: Because task durations are uncertain, the true makespan is only known after sampling. The policy must learn to optimize expected makespan, which RL handles naturally by minimizing the expectation of sampled returns.
  - Quick check question: How is the reward signal defined in this approach?
    - Answer: All intermediate rewards are zero; only at the terminal state is the sampled makespan returned as the cost.

## Architecture Onboarding

- **Component map:**
  - Input (disjunctive graph) -> Graph Rewirer -> Node/Edge Embedder -> GNN (EGATConv layers) -> Action Selector -> PPO Loop -> Simulator

- **Critical path:**
  1. Graph construction and embedding → 2. Message passing through GNN → 3. Action probability computation → 4. Action selection → 5. State transition in simulator → 6. Episode termination and reward sampling → 7. PPO update.

- **Design tradeoffs:**
  - Using a single sampled duration per rollout vs. averaging over multiple samples (faster but noisier).
  - Including completion time estimates in node features vs. computing them on the fly (more informative but requires careful updates).
  - Concatenating all layer outputs vs. using only the last layer (richer features vs. simpler model).

- **Failure signatures:**
  - If the agent consistently selects tasks that lead to high makespan on validation, check whether the GNN is learning useful embeddings (inspect attention weights or node representations).
  - If training is unstable, verify that action masking is correctly preventing invalid actions and that the critic is not diverging.
  - If generalization to larger problems fails, confirm that the rewired graph is correctly constructed and that the action space mapping is preserved.

- **First 3 experiments:**
  1. Train W-10x10 on deterministic Taillard instances, evaluate on 6×6, 10×10, 15×15, and compare average makespan to MOPNR and OR-Tools.
  2. Train W-10x10 on stochastic Taillard instances (using the same network), evaluate on 6×6, 10×10, 15×15, and compare to CP-stoc and OR-Tools mode/real.
  3. Test the trained W-10x10 policy on larger instances (20×15, 30×10, 50×20) without further training, report makespan and gap to CP-stoc.

## Open Questions the Paper Calls Out

- How would the Wheatley approach perform on real-world industrial scheduling problems compared to its performance on synthetic benchmarks?
- Would pretraining the policy before running PPO improve the performance and efficiency of Wheatley?
- How would Wheatley perform on other scheduling problems, such as the Resource-Constrained Project Scheduling Problem (RCPSP), especially in handling uncertainty?

## Limitations

- The single-sample reward estimation may be insufficient for highly skewed duration distributions, potentially leading to suboptimal policies.
- The scalability claims rely on the assumption that the GNN can learn to generalize from small to large instances, but the exact representational capacity required is not quantified.
- The graph rewiring strategy, while effective, is not thoroughly justified or compared against alternative relational encodings.

## Confidence

- Method's effectiveness on tested instances: High
- Generalization claims: Medium
- Robustness to arbitrary uncertainty structures: Low

## Next Checks

1. Test the trained policy on problem instances significantly larger than those seen during training (e.g., 100×20) to assess true generalization.
2. Evaluate the policy's performance under different uncertainty distributions (e.g., multimodal, heavy-tailed) to check robustness.
3. Compare the graph rewiring strategy against alternative relational encodings (e.g., without backward edges) to validate the design choices.