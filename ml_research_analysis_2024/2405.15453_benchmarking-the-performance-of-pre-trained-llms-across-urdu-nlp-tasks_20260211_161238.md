---
ver: rpa2
title: Benchmarking the Performance of Pre-trained LLMs across Urdu NLP Tasks
arxiv_id: '2405.15453'
source_url: https://arxiv.org/abs/2405.15453
tags:
- output
- llama
- urdu
- speech
- hate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the performance of seven pre-trained LLMs
  on 17 Urdu NLP tasks using 22 datasets and 13.8 hours of speech data in a zero-shot
  setting. The models tested include GPT-3.5-turbo, Llama 2-7B-Chat, Llama 3.1-8B,
  Bloomz 3B, Bloomz 7B1, Ministral-8B, and Whisper (in three variants).
---

# Benchmarking the Performance of Pre-trained LLMs across Urdu NLP Tasks

## Quick Facts
- arXiv ID: 2405.15453
- Source URL: https://arxiv.org/abs/2405.15453
- Reference count: 35
- Seven pre-trained LLMs evaluated on 17 Urdu NLP tasks using 22 datasets and 13.8 hours of speech data in zero-shot setting

## Executive Summary
This paper presents a comprehensive benchmark evaluating seven pre-trained large language models on 17 Urdu NLP tasks across 22 datasets and 13.8 hours of speech data. The models tested include GPT-3.5-turbo, Llama 2-7B-Chat, Llama 3.1-8B, Bloomz variants, Ministral-8B, and Whisper in three variants. The study reveals that state-of-the-art models currently outperform encoder-decoder models in most Urdu NLP tasks under zero-shot settings. Notably, Llama 3.1-8B with improved language coverage surpasses SOTA models in several tasks despite having fewer parameters than GPT-3.5-turbo, demonstrating that language-specific data richness can outweigh sheer parameter count.

## Method Summary
The study evaluates pre-trained LLMs on 17 Urdu NLP tasks using 22 datasets and 13.8 hours of speech data in a zero-shot setting. Seven models were tested: GPT-3.5-turbo, Llama 2-7B-Chat, Llama 3.1-8B, Bloomz 3B, Bloomz 7B1, Ministral-8B, and Whisper (three variants). Performance was measured using task-specific metrics, with prompts optimized after testing on few samples. Post-processing was required to align model outputs with desired outcomes. The evaluation framework focused on zero-shot performance without fine-tuning, comparing results against existing SOTA models.

## Key Results
- SOTA models outperform encoder-decoder models in most Urdu NLP tasks under zero-shot settings
- Llama 3.1-8B surpasses SOTA models in several tasks due to improved language coverage despite fewer parameters than GPT-3.5-turbo
- Models with fewer parameters but richer language-specific data (like Llama 3.1-8B) often outperform larger models with lower language diversity (like GPT-3.5)

## Why This Works (Mechanism)
The performance differences stem from language coverage and data richness rather than parameter count alone. Llama 3.1-8B's improved language coverage allows it to better handle Urdu NLP tasks despite having fewer parameters than GPT-3.5-turbo. The study demonstrates that models trained with more diverse language data can outperform larger models with less language diversity, particularly for low-resource languages like Urdu.

## Foundational Learning
- **Urdu NLP task diversity**: Understanding various NLP tasks (classification, translation, speech recognition) is crucial for evaluating model capabilities across different linguistic challenges
- **Zero-shot learning evaluation**: Essential for assessing model generalization without task-specific fine-tuning, providing baseline performance metrics
- **Language coverage importance**: Models with broader language coverage perform better on low-resource languages, highlighting the need for diverse training data
- **Prompt engineering**: Performance significantly depends on well-curated prompts and post-processing, requiring careful optimization for each task
- **Parameter vs. data quality tradeoff**: Fewer parameters with richer language-specific data can outperform larger models with less language diversity
- **Evaluation metric selection**: Task-specific metrics are necessary for accurate performance assessment across different NLP tasks

## Architecture Onboarding

**Component Map**: Input Text -> Prompt Engineering -> LLM Model (GPT-3.5, Llama, Bloomz, etc.) -> Post-processing -> Task-specific Output

**Critical Path**: Text Input → Prompt Generation → Model Inference → Output Post-processing → Performance Evaluation

**Design Tradeoffs**: The study prioritizes zero-shot evaluation over fine-tuning, sacrificing potential performance gains for generalizable baseline results. Hardware limitations prevent testing larger models, creating uncertainty about scaling effects.

**Failure Signatures**: Poor performance on tasks requiring deep linguistic understanding, inconsistent results across different prompt variations, and post-processing difficulties in aligning outputs with desired formats.

**First Experiments**:
1. Compare prompt engineering strategies (few-shot vs zero-shot) on a subset of tasks
2. Evaluate tokenization impact by testing different tokenization approaches on Urdu text
3. Assess model robustness by introducing adversarial examples in Urdu language

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: What is the optimal prompt engineering strategy for achieving maximum performance across diverse Urdu NLP tasks using LLMs?
- **Basis in paper**: Explicit - The paper emphasizes that performance of LLMs significantly depends on well-curated prompts and intelligent post-processing of outputs, and notes that despite careful prompting, model responses required post-processing to align with desired outcomes.
- **Why unresolved**: The paper only mentions that prompts were optimized after testing on few samples but does not provide a systematic methodology for prompt engineering or explore how different prompt strategies affect performance across tasks.
- **What evidence would resolve it**: A systematic study comparing different prompt engineering strategies (few-shot vs zero-shot, different instruction formats, prompt refinement techniques) across all tasks, with ablation studies showing performance impact of prompt variations.

### Open Question 2
- **Question**: How do larger parameter LLMs (e.g., Bloomz-170B or Llama 3.1 405B) perform on Urdu NLP tasks compared to the smaller models tested in this study?
- **Basis in paper**: Explicit - The paper explicitly states limitations regarding not including heavier versions of models such as Bloomz-170B or Llama 3.1 405B due to hardware and computational resource limitations.
- **Why unresolved**: The study's hardware constraints prevented evaluation of larger models, leaving uncertainty about whether the performance gap between smaller LLMs and SOTA models would diminish with increased model size.
- **What evidence would resolve it**: Benchmarking results of larger LLMs on the same Urdu NLP tasks and datasets, comparing their performance with the current SOTA models and the smaller LLMs tested in this study.

### Open Question 3
- **Question**: How would fine-tuning these LLMs on domain-specific Urdu data affect their performance compared to the zero-shot results reported?
- **Basis in paper**: Explicit - The paper states it "primarily concentrates on evaluating the models in a zero-shot setting" and notes that "it may not capture the full potential of fine-tuned models for specific tasks."
- **Why unresolved**: The study only evaluated zero-shot performance, leaving open questions about how much improvement could be achieved through fine-tuning, particularly for domain-specific tasks where the models showed weaker performance.
- **What evidence would resolve it**: Performance comparison of fine-tuned vs zero-shot models on the same tasks, with ablation studies showing improvement patterns across different domains and task types.

## Limitations
- Zero-shot setting may not reflect real-world performance where few-shot or fine-tuning approaches could yield different results
- Hardware constraints prevented evaluation of larger models like Bloomz-170B or Llama 3.1 405B
- Evaluation framework focuses on individual task performance without examining model robustness to adversarial examples or out-of-distribution inputs

## Confidence

**High confidence**: The comparative analysis between GPT-3.5-turbo and Llama 3.1-8B regarding language coverage versus parameter count

**Medium confidence**: Claims about encoder-decoder models being outperformed in zero-shot settings, as this may not generalize to other evaluation paradigms

**Low confidence**: The assertion that models with fewer parameters but richer language-specific data "often" outperform larger models with lower language diversity, given the limited scope of evaluation conditions

## Next Checks
1. Replicate the experiments using few-shot learning settings to assess whether the relative performance rankings remain consistent across different evaluation paradigms
2. Conduct ablation studies on prompt engineering and tokenization strategies to isolate their impact on Urdu language processing performance
3. Test model robustness by evaluating performance on adversarial examples and out-of-distribution Urdu text to assess generalization beyond benchmark datasets