---
ver: rpa2
title: 'AIGB: Generative Auto-bidding via Conditional Diffusion Modeling'
arxiv_id: '2405.16141'
source_url: https://arxiv.org/abs/2405.16141
tags:
- bidding
- auto-bidding
- diffbid
- diffusion
- advertising
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DiffBid, a diffusion modeling approach for
  auto-bidding in online advertising. Unlike traditional RL methods that rely on MDP
  assumptions, DiffBid directly models the correlation between returns and entire
  trajectories, avoiding error propagation in long horizons.
---

# AIGB: Generative Auto-bidding via Conditional Diffusion Modeling

## Quick Facts
- arXiv ID: 2405.16141
- Source URL: https://arxiv.org/abs/2405.16141
- Authors: Jiayan Guo; Yusen Huo; Zhilin Zhang; Tianyu Wang; Chuan Yu; Jian Xu; Yan Zhang; Bo Zheng
- Reference count: 40
- Key outcome: DiffBid achieves 2.81% increase in GMV and 3.36% increase in ROI compared to state-of-the-art methods in online A/B tests on Alibaba's platform

## Executive Summary
This paper introduces DiffBid, a diffusion modeling approach for auto-bidding in online advertising that directly models the correlation between returns and entire trajectories, avoiding error propagation across time steps in long horizons. Unlike traditional RL methods that rely on MDP assumptions, DiffBid uses conditional generative modeling to capture the joint distribution of bidding trajectories and their associated returns. Extensive experiments on real-world data and online A/B tests demonstrate that DiffBid outperforms state-of-the-art methods while offering flexibility to accommodate diverse advertiser constraints and needs.

## Method Summary
DiffBid uses conditional diffusion modeling to generate optimal bidding trajectories by gradually denoising latent samples conditioned on desired returns and constraints. The model consists of a forward process that corrupts trajectories with scheduled Gaussian noise, a reverse process with a U-Net architecture that reconstructs trajectories from corrupted ones, and an inverse dynamics model that generates bidding parameters. Training uses noise prediction objectives with classifier-free guidance, while inference employs a non-Markovian inverse dynamics model to convert target state trajectories into bidding parameters. The approach avoids MDP assumptions and handles non-Markovian characteristics inherent in auto-bidding problems.

## Key Results
- Achieves 2.81% increase in GMV and 3.36% increase in ROI compared to state-of-the-art methods in online A/B tests
- Outperforms traditional RL methods (USCB, BCQ, CQL, IQL, DT) in simulated environment across different budget scales
- Demonstrates resilience to noise and provides flexibility for diverse advertiser constraints (CPC, budget, timing preferences)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: DiffBid directly models the correlation between the return and the entire trajectory, avoiding error propagation across time steps in long horizons.
- **Mechanism**: Instead of using MDP assumptions where the next state depends only on the current state and action, DiffBid uses conditional diffusion modeling to capture the joint distribution of the entire bidding trajectory and its associated returns. This allows the model to generate trajectories conditioned on desired returns, effectively handling sparse returns and random advertising environments.
- **Core assumption**: The correlation between the return and the entire trajectory is stronger than the correlation between individual state-action pairs, making trajectory-level modeling more effective.
- **Evidence anchors**:
  - [abstract]: "DiffBid directly models the correlation between the return and the entire trajectory, effectively avoiding error propagation across time steps in long horizons."
  - [section]: "We model such sequential decision-making problem through conditional generative modeling...by maximum likelihood estimation...The goal is to estimate the conditional data distribution with pÎ¸ so that the future states of a trajectory x0(Ï„) from information y(Ï„) can be generated."
- **Break condition**: If the return is not strongly correlated with the entire trajectory, or if the trajectory length is too short for error propagation to be a significant issue, the benefits of this mechanism would diminish.

### Mechanism 2
- **Claim**: DiffBid offers flexibility to closely align with the specific needs of advertisers by accommodating diverse constraints like cost-per-click (CPC) and incorporating human feedback.
- **Mechanism**: DiffBid uses conditional generation with multiple types of conditions (returns, constraints, human feedback) that can be composed together. This allows the model to generate trajectories that maximize given targets while adhering to specific constraints, providing a unified solution for various bidding scenarios.
- **Core assumption**: Advertisers have diverse needs that can be effectively captured through different types of conditions, and the model can learn to balance these conditions during generation.
- **Evidence anchors**:
  - [abstract]: "Additionally, DiffBid offers a versatile approach for generating trajectories that maximize given targets while adhering to specific constraints."
  - [section]: "In this section, we present approaches transforming industrial metrics into conditions of DiffBid...We can then normalizeð‘¥ into [0, 1] through min-max normalization for simplification. E can be used to indicate whether trajectory Ï„ break the CPC constraint."
- **Break condition**: If the conditions are too numerous or conflicting, or if the model cannot effectively balance multiple constraints, the flexibility advantage may be compromised.

### Mechanism 3
- **Claim**: DiffBid achieves theoretical optimality equivalent to solving a non-Markovian decision problem, making it more powerful in handling randomness and sparse return.
- **Mechanism**: Through theoretical analysis, the paper proves that DiffBid using MLE as the objective has a corresponding non-Markovian decision problem with equivalent optimality. This means DiffBid does not require the MDP assumption and can handle the non-Markovian nature of the auto-bidding problem.
- **Core assumption**: The auto-bidding problem has non-Markovian characteristics that cannot be adequately captured by traditional MDP-based RL methods.
- **Evidence anchors**:
  - [abstract]: "Theoretical analysis proves DiffBid's optimality is equivalent to solving a non-Markovian decision problem."
  - [section]: "In this section, we theoretically analyze the property of DiffBid...Its objective yields the save optimal policy as the Maximum Likelihood Estimation Eð‘âˆ— (ð‘ 0:ð‘‡ ) [logð‘ðœƒ (ð‘ 0:ð‘‡ )]"
- **Break condition**: If the auto-bidding problem is actually Markovian, or if the non-Markovian characteristics are not significant enough to impact performance, the theoretical advantage may not translate to practical benefits.

## Foundational Learning

- **Concept**: Diffusion Models
  - **Why needed here**: DiffBid is built on diffusion modeling, which is essential for understanding how the model generates trajectories by gradually denoising latent samples. Understanding the forward process (adding noise) and reverse process (removing noise) is crucial for grasping the model's mechanism.
  - **Quick check question**: What is the difference between the forward process and reverse process in diffusion models, and how do they relate to DiffBid's trajectory generation?

- **Concept**: Reinforcement Learning (RL) and Markov Decision Processes (MDP)
  - **Why needed here**: The paper explicitly contrasts DiffBid with traditional RL methods that rely on MDP assumptions. Understanding why MDP assumptions may not hold in auto-bidding (non-Markovian state transitions) is key to appreciating DiffBid's advantages.
  - **Quick check question**: What are the limitations of MDP-based RL methods in auto-bidding, and how does DiffBid address these limitations?

- **Concept**: Conditional Generation and Maximum Likelihood Estimation (MLE)
  - **Why needed here**: DiffBid uses conditional generation to model the correlation between returns and trajectories, and MLE as the training objective. Understanding these concepts is essential for grasping how the model learns to generate optimal trajectories.
  - **Quick check question**: How does conditional generation with MLE enable DiffBid to capture the relationship between returns and trajectories, and why is this beneficial for auto-bidding?

## Architecture Onboarding

- **Component map**: Forward Process (adding noise) -> Reverse Process (denoising with U-Net) -> Noise Prediction Model (ÎµÎ¸) -> Inverse Dynamics Model (fÏ†) -> Bid Generation
- **Critical path**: Forward process â†’ Reverse process (with noise prediction) â†’ Inverse dynamics â†’ Bid generation. The model first corrupts the trajectory, then denoises it conditioned on the desired return and constraints, and finally generates the corresponding bidding parameters.
- **Design tradeoffs**:
  - **Diffusion steps (K)**: Higher K may improve generation quality but increase latency. The paper finds that relatively small K (e.g., 30) is sufficient for auto-bidding.
  - **Condition dropout ratio**: Random dropping of conditions during training enhances robustness but may slow convergence.
  - **Model capacity (hidden sizes)**: Larger hidden sizes may improve performance but increase computational cost.
- **Failure signatures**:
  - **Poor performance on long horizons**: If the model fails to capture long-term dependencies, it may struggle with the non-Markovian nature of auto-bidding.
  - **Inability to handle diverse constraints**: If the model cannot effectively balance multiple constraints, it may fail to meet advertisers' specific needs.
  - **Overfitting to training data**: If the model overfits, it may not generalize well to unseen scenarios or environments.
- **First 3 experiments**:
  1. **Validate diffusion step impact**: Test DiffBid with different numbers of diffusion steps (K) on a small dataset to find the optimal balance between performance and efficiency.
  2. **Compare with MDP-based RL**: Evaluate DiffBid against a strong MDP-based RL baseline (e.g., IQL) on a synthetic auto-bidding environment to demonstrate the advantage of non-Markovian modeling.
  3. **Test constraint handling**: Generate trajectories with different combinations of constraints (e.g., CPC, budget) to verify that DiffBid can effectively balance multiple objectives and meet advertisers' needs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific conditions under which DiffBid's performance degrades or fails to outperform traditional RL methods?
- Basis in paper: [inferred] The paper mentions DiffBid's resilience to noise but doesn't provide concrete failure scenarios or conditions where it might underperform.
- Why unresolved: The paper focuses on successful applications and comparisons but lacks a detailed analysis of limitations or edge cases where DiffBid might not be the optimal choice.
- What evidence would resolve it: Empirical studies testing DiffBid under various challenging conditions (e.g., extreme noise levels, non-stationary environments, or data scarcity) compared to traditional RL methods.

### Open Question 2
- Question: How does the performance of DiffBid scale with increasingly complex constraints or multiple simultaneous objectives?
- Basis in paper: [inferred] The paper demonstrates DiffBid's ability to handle multiple constraints but doesn't explore the limits of this capability or provide a quantitative analysis of performance degradation as complexity increases.
- Why unresolved: While the paper shows promising results with multiple constraints, it doesn't establish a clear relationship between constraint complexity and performance, nor does it identify a threshold beyond which DiffBid becomes less effective.
- What evidence would resolve it: Systematic experiments varying the number and complexity of constraints, measuring DiffBid's performance and comparing it to traditional RL methods under the same conditions.

### Open Question 3
- Question: What are the theoretical bounds on the approximation error when using DiffBid's non-Markovian inverse dynamics model compared to exact Markovian methods?
- Basis in paper: [explicit] The paper introduces a non-Markovian inverse dynamics model and claims it's more accurate, but doesn't provide theoretical bounds on the approximation error or compare it quantitatively to exact Markovian methods.
- Why unresolved: The paper demonstrates empirical superiority but lacks a rigorous theoretical analysis of the approximation error introduced by the non-Markovian approach, especially in comparison to exact Markovian methods.
- What evidence would resolve it: Formal proofs establishing bounds on the approximation error of DiffBid's non-Markovian approach, along with empirical validation comparing these bounds to the performance of exact Markovian methods in various scenarios.

## Limitations

- Limited External Validation: While online A/B tests show promising results, the exact methodology and statistical significance are not fully detailed.
- Simulation vs. Reality Gap: Heavy reliance on simulated environment raises questions about real-world applicability and accuracy of simulator.
- Computational Efficiency: Lacks concrete latency measurements and computational overhead comparisons with traditional methods.

## Confidence

- **High Confidence**: The core mechanism of using diffusion models for trajectory generation and the theoretical equivalence to non-Markovian decision problems are well-established concepts with clear derivations.
- **Medium Confidence**: The empirical results showing improvements over baselines in both simulation and online tests are compelling, but the lack of detailed statistical analysis and limited comparison with recent RL methods (beyond IQL) reduces confidence in the claimed superiority.
- **Low Confidence**: The scalability analysis and real-world deployment considerations (computational costs, latency requirements) are insufficiently addressed, making it difficult to assess practical deployment feasibility.

## Next Checks

1. **Statistical Significance Analysis**: Request detailed A/B test results including p-values, confidence intervals, and variance across multiple test periods to verify the claimed 2.81% GMV and 3.36% ROI improvements are statistically significant.

2. **Baseline Completeness**: Compare DiffBid against more recent RL methods (e.g., decision transformer variants, modern offline RL algorithms) in the simulated environment to ensure comprehensive baseline coverage.

3. **Real-world Deployment Audit**: Conduct a small-scale real-world deployment with detailed logging of computational latency, auction participation rates, and resource utilization to validate production readiness claims.