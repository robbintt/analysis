---
ver: rpa2
title: A Survey on Data Quality Dimensions and Tools for Machine Learning
arxiv_id: '2406.19614'
source_url: https://arxiv.org/abs/2406.19614
tags:
- data
- quality
- tools
- metrics
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey reviews 17 open-source data quality (DQ) tools for
  machine learning, addressing the growing importance of DQ in ML models' performance,
  fairness, and robustness. The authors summarize 4 DQ dimensions (intrinsic, contextual,
  representational, accessibility) and 12 relevant metrics, then analyze each tool's
  functions, usability, and effectiveness.
---

# A Survey on Data Quality Dimensions and Tools for Machine Learning

## Quick Facts
- arXiv ID: 2406.19614
- Source URL: https://arxiv.org/abs/2406.19614
- Reference count: 40
- Authors: Yuhan Zhou, Fengjiao Tu, Kewei Sha, Junhua Ding, Haihua Chen
- Primary result: Survey of 17 open-source DQ tools for ML, proposing roadmap for ML-tailored DQ tools

## Executive Summary
This survey comprehensively reviews 17 open-source data quality tools for machine learning, analyzing their functions, usability, and effectiveness in addressing critical data quality dimensions. The authors identify a gap between current general-purpose DQ tools and the specific needs of ML applications, noting that most tools lack ML-specific metrics like class imbalance and unbiasedness. They propose a five-step roadmap for developing new DQ tools and highlight the potential of AI and low-code solutions to improve usability and accessibility.

## Method Summary
The paper conducts a comprehensive survey of 17 open-source DQ tools, evaluating their functions, DQ metrics coverage, and usability for ML applications. The authors analyze tools across four DQ dimensions (intrinsic, contextual, representational, accessibility) and 12 relevant metrics, then propose a roadmap for developing new ML-tailored DQ tools. The methodology includes comparative analysis of existing tools and identification of gaps in current DQ tool capabilities.

## Key Results
- Most DQ tools focus on general metrics (completeness, correctness, consistency) rather than ML-specific metrics like class imbalance
- AI-embedded tools (Winpure, Ataccama ONE, Soda Core, Evidently) show potential for automated DQ evaluation
- Limited coverage of ML-specific DQ metrics across surveyed tools creates opportunity for new tool development
- Proposed roadmap outlines five steps for developing ML-tailored DQ tools: understanding background, defining scope, implementing technology, creating UI, and establishing documentation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data quality tools improve ML model performance by systematically detecting and correcting data issues before training.
- Mechanism: Tools profile data to identify missing values, duplicates, and inconsistencies; apply transformations to fix these issues; and monitor data over time to prevent model degradation.
- Core assumption: Poor data quality directly causes ML model performance degradation and bias.
- Evidence anchors:
  - [abstract] "data quality (DQ) is critical for the performance, fairness, robustness, safety, and scalability of ML models"
  - [section] "pervasive label errors exist and destabilize ML benchmarks [55]. For large multi-modality models, ensuring high-quality, contextually relevant, and well-reasoned training data is crucial"
  - [corpus] Weak - no direct corpus evidence provided for this specific mechanism
- Break condition: If tools fail to detect or correct the most impactful data quality issues, model performance gains will be minimal.

### Mechanism 2
- Claim: AI-powered DQ tools reduce the need for manual data cleaning and make DQ accessible to non-technical users.
- Mechanism: Tools integrate AI and low-code solutions to automate data profiling, transformation, and monitoring, enabling users without technical expertise to perform DQ tasks.
- Core assumption: Manual data cleaning is time-consuming and requires technical expertise that limits DQ adoption.
- Evidence anchors:
  - [abstract] "we further highlight the potential applications of large language models (LLMs) and generative AI in DQ evaluation and improvement for ML"
  - [section] "Some tools can automatically re-activate the workflow when new data is coming and generate up-to-date reports. Winpure, Ataccama ONE, Soda Core, and Evidently are the AI-embedded tools"
  - [corpus] Weak - corpus evidence is limited to the abstract claim about AI applications
- Break condition: If AI integration fails to accurately identify or fix data quality issues, manual intervention will still be required.

### Mechanism 3
- Claim: Standardized DQ metrics and dimensions provide a framework for consistent DQ evaluation across different ML applications.
- Mechanism: Tools adopt established DQ dimensions (intrinsic, contextual, representational, accessibility) and metrics (completeness, correctness, consistency) to ensure comprehensive DQ assessment.
- Core assumption: Without standardized metrics, DQ evaluation would be inconsistent and incomparable across different ML projects.
- Evidence anchors:
  - [abstract] "We present an overview of 4 DQ dimensions, and 12 metrics in ML, with the definitions, and examples"
  - [section] "Based on application scenarios, the definition of data quality may involve different detailed descriptions. Wang and Strong defined data quality as 'data that are fit for use by data consumers'"
  - [corpus] Weak - corpus evidence is limited to the abstract mention of metrics
- Break condition: If metrics don't align with specific ML task requirements, DQ evaluation may miss critical issues.

## Foundational Learning

- Concept: Data Quality Dimensions
  - Why needed here: Understanding DQ dimensions is essential for selecting appropriate metrics and tools for specific ML tasks.
  - Quick check question: What are the four DQ dimensions discussed in the paper and how do they differ?
- Concept: Data Quality Metrics
  - Why needed here: DQ metrics provide quantitative measures for evaluating data quality issues in ML datasets.
  - Quick check question: Which DQ metrics are most frequently adopted by current tools according to the comparative analysis?
- Concept: ML Model Performance Impact
  - Why needed here: Understanding how DQ affects ML models helps prioritize which DQ issues to address.
  - Quick check question: What evidence does the paper provide about the relationship between label errors and ML benchmark performance?

## Architecture Onboarding

- Component map: Data loading → Data profiling → Data integration → Data transformation → Automation and monitoring → Output and reports
- Critical path: Data profiling → Data transformation → Automation and monitoring
- Design tradeoffs: Comprehensive metrics vs. tool complexity; AI integration vs. explainability; batch vs. streaming processing
- Failure signatures: Inconsistent metrics across tools; tools missing ML-specific DQ metrics; poor user interface design
- First 3 experiments:
  1. Profile a small dataset using multiple tools to compare metric outputs and identify inconsistencies
  2. Apply transformations to fix detected DQ issues and measure impact on a simple ML model
  3. Set up automated monitoring on a streaming dataset and test alert functionality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop standardized metrics for data quality evaluation in ML that can be consistently applied across different domains and use cases?
- Basis in paper: [explicit] The paper mentions that "the lack of standardized metrics for DQ evaluation complicates the comparison of different approaches" and discusses the need for "metrics need to be checked and updated according to developments in ML research".
- Why unresolved: The paper highlights the complexity of creating universal metrics due to the overlapping nature of dimensions and the need for domain-specific adaptations, but doesn't propose a solution for standardization.
- What evidence would resolve it: Development and validation of a core set of standardized DQ metrics that can be applied across various ML domains, along with guidelines for domain-specific adaptations.

### Open Question 2
- Question: How can AI-powered tools, particularly those using large language models and generative AI, be effectively integrated into data quality evaluation and improvement workflows for ML?
- Basis in paper: [explicit] The paper emphasizes "the potential applications of large language models (LLMs) and generative AI in DQ evaluation and improvement for ML" and discusses using AI for tasks like data augmentation and anomaly detection.
- Why unresolved: While the paper highlights the potential of AI in DQ management, it doesn't provide specific implementation strategies or evaluate the effectiveness of these approaches in real-world scenarios.
- What evidence would resolve it: Case studies and empirical results demonstrating the effectiveness of AI-powered DQ tools in various ML applications, along with best practices for integration.

### Open Question 3
- Question: How can data quality tools be designed to handle the increasing volume and complexity of data in ML applications while maintaining real-time monitoring and evaluation capabilities?
- Basis in paper: [explicit] The paper discusses challenges in "profiling excessive data and monitoring frequently updated data" and mentions the need for tools to handle "larger datasets while monitoring data continuously".
- Why unresolved: The paper identifies the need for scalable solutions but doesn't provide specific architectural designs or performance benchmarks for handling large-scale, real-time DQ evaluation.
- What evidence would resolve it: Development and testing of scalable DQ tool architectures that can efficiently process large volumes of data and provide real-time insights, with performance metrics and scalability benchmarks.

## Limitations

- Limited empirical validation of claimed mechanisms, relying primarily on cited literature
- Focus on open-source tools may exclude commercial solutions that could offer different capabilities
- Limited evaluation of AI-powered tools' actual effectiveness in reducing manual intervention

## Confidence

- Metric consistency across tools: Medium
- AI integration effectiveness: Medium
- ML-specific metric coverage: Medium

## Next Checks

1. **Metric Consistency Test**: Profile the same dataset using 3-4 different tools from the survey to identify metric discrepancies and evaluate cross-tool reliability.

2. **AI Integration Validation**: Test one AI-embedded tool (e.g., Evidently or Soda Core) on a real ML dataset to measure actual reduction in manual data cleaning effort versus traditional tools.

3. **ML-Specific Metric Coverage**: Analyze the 12 metrics against a diverse set of ML tasks (classification, regression, NLP) to identify which metrics are most predictive of model performance degradation.