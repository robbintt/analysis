---
ver: rpa2
title: 'Better than Random: Reliable NLG Human Evaluation with Constrained Active
  Sampling'
arxiv_id: '2406.07967'
source_url: https://arxiv.org/abs/2406.07967
tags:
- sampling
- evaluation
- human
- samples
- overall
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the unreliability of human evaluation in NLG
  tasks caused by random sampling of evaluation data, which can lead to inconsistent
  system rankings. The authors propose a Constrained Active Sampling Framework (CASF)
  that uses a Learner to predict sample quality, a Systematic Sampler to divide samples
  into buckets, and a Constrained Controller to select representative, non-redundant
  samples across multiple phases.
---

# Better than Random: Reliable NLG Human Evaluation with Constrained Active Sampling

## Quick Facts
- arXiv ID: 2406.07967
- Source URL: https://arxiv.org/abs/2406.07967
- Authors: Jie Ruan; Xiao Pu; Mingqi Gao; Xiaojun Wan; Yuesheng Zhu
- Reference count: 20
- Key result: Achieves 93.18% top-ranked system accuracy and 0.83 Kendall correlation in NLG human evaluation

## Executive Summary
This paper addresses the critical problem of unreliable human evaluation in Natural Language Generation (NLG) caused by random sampling of evaluation data. The authors propose the Constrained Active Sampling Framework (CASF) to systematically select representative, non-redundant samples that preserve inter-system ranking accuracy while reducing evaluation costs. CASF operates through three components: a Learner that predicts sample quality, a Systematic Sampler that divides samples into buckets, and a Constrained Controller that selects optimal samples across multiple sampling phases.

## Method Summary
The Constrained Active Sampling Framework (CASF) improves human evaluation reliability by using a quality-predicting Learner (GBDT regressor) to score samples, a Systematic Sampler to create representative buckets based on quality scores, and a Constrained Controller to select samples across five active sampling phases. The framework starts with a preliminary 10% random sampling, then iteratively selects 10% samples per phase while avoiding redundancy. It was evaluated across 137 NLG systems, 16 datasets, and 5 NLG tasks using 44 human evaluation metrics, demonstrating superior performance compared to random and heuristic sampling methods.

## Key Results
- Achieves 93.18% accuracy in recognizing top-ranked NLG systems
- Maintains 0.83 Kendall correlation for inter-system ranking
- Significantly outperforms random sampling and heuristic methods across all tested datasets and tasks

## Why This Works (Mechanism)
The framework addresses the fundamental problem that random sampling can produce clustered selections that miss critical distinctions between system performances. By using a quality-predicting Learner to score samples and a Systematic Sampler to create quality-based buckets, CASF ensures diverse coverage of the quality spectrum. The Constrained Controller then actively selects samples from these buckets in multiple phases, avoiding redundancy while maintaining representativeness. This multi-phase approach allows the system to learn from previous selections and adaptively improve sample quality over time.

## Foundational Learning

**Active Sampling**: Iterative selection of data points to maximize information gain; needed because passive random sampling misses critical evaluation points. Quick check: Monitor Kendall's Tau variance across sampling runs.

**Quality Prediction**: Using automated metrics (MOVER-SCORE) to estimate human evaluation scores; needed because human evaluation is expensive and slow. Quick check: Compare predicted vs. actual quality score distributions.

**Redundancy Avoidance**: Selecting samples that provide new information rather than repeating similar examples; needed because clustered sampling reduces evaluation reliability. Quick check: Track sample diversity metrics across phases.

**Multi-phase Sampling**: Breaking sampling into sequential stages where each phase informs the next; needed because single-phase methods cannot adapt to learned patterns. Quick check: Monitor learning curve of sample quality predictions.

**Kendall's Tau Correlation**: Statistical measure of ranking agreement between sampled and full datasets; needed because it directly measures inter-system ranking preservation. Quick check: Calculate correlation between automated and human rankings.

## Architecture Onboarding

**Component Map**: Data Preprocessing -> Learner (GBDT) -> Systematic Sampler -> Constrained Controller -> Active Sampling Phases -> Evaluation

**Critical Path**: The sequence from quality prediction through constrained selection determines final sample quality and evaluation reliability.

**Design Tradeoffs**: CASF trades computational overhead (multiple sampling phases, Learner training) for improved evaluation accuracy and reduced human effort.

**Failure Signatures**: Poor Learner performance manifests as high Kendall's Tau variance; ineffective Systematic Sampler shows non-representative quality score distributions across buckets.

**First Experiments**: 1) Test CASF on a small dataset to verify basic functionality, 2) Compare different automated metrics (BERTScore vs. MOVER-SCORE) for quality prediction, 3) Run sensitivity analysis on the number of sampling phases.

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Assumes quality scores are available for all samples, which may not hold in real-world scenarios
- Computational cost of multiple sampling phases and Learner training is not discussed
- Performance on extremely small datasets (fewer than 1,000 samples) is not evaluated

## Confidence

**High Confidence**: The core methodology of using active sampling with quality prediction to improve evaluation reliability is well-supported by consistent results across multiple datasets.

**Medium Confidence**: Claims about outperforming heuristic methods are supported on tested datasets but may not generalize to all NLG scenarios.

**Low Confidence**: Framework's performance on very small datasets is unverified and claims about maintaining accuracy in such cases are unsupported.

## Next Checks

1. **Cross-task validation**: Test CASF on underrepresented NLG tasks (long-form generation, structured output) to verify generalizability.

2. **Metric sensitivity analysis**: Evaluate different preliminary sampling metrics (BERTScore, BLEURT) to identify optimal configurations and understand framework sensitivity.

3. **Real-world deployment study**: Implement CASF in live human evaluation settings to measure actual time savings and evaluate computational overhead justification.