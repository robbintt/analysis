---
ver: rpa2
title: Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning
  in Large Language Models
arxiv_id: '2402.08955'
source_url: https://arxiv.org/abs/2402.08955
tags:
- gpt-3
- problems
- humans
- webb
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether large language models (LLMs) exhibit
  genuine analogical reasoning or merely pattern-matching behaviors. The authors test
  GPT models and humans on letter-string analogy problems, including counterfactual
  variants with permuted or symbolic alphabets unlikely to appear in training data.
---

# Using Counterfactual Tasks to Evaluate the Generality of Analogical Reasoning in Large Language Models

## Quick Facts
- **arXiv ID**: 2402.08955
- **Source URL**: https://arxiv.org/abs/2402.08955
- **Reference count**: 6
- **Key outcome**: GPT models show sharp performance decline on counterfactual alphabet analogies compared to humans, suggesting pattern matching rather than abstract reasoning

## Executive Summary
This study investigates whether large language models exhibit genuine analogical reasoning or merely pattern-matching behaviors. The authors test GPT models and humans on letter-string analogy problems, including counterfactual variants with permuted or symbolic alphabets unlikely to appear in training data. While humans maintain high performance across all problem types and alphabets, GPT models show a sharp decline in accuracy on counterfactual versions, indicating a lack of robust abstract reasoning.

## Method Summary
The authors design controlled experiments using letter-string analogies, comparing human and LLM performance across standard and counterfactual tasks. Counterfactual variants use permuted or symbolic alphabets that are unlikely to appear in training data, allowing researchers to distinguish between pattern matching and abstract reasoning. Performance is measured across multiple GPT model versions (GPT-3, GPT-3.5, GPT-4) and compared against human baselines on the same tasks.

## Key Results
- Human accuracy remains consistently high (~75%) across all alphabet types and problem variants
- GPT-3, GPT-3.5, and GPT-4 show accuracy drops from ~50% on standard problems to ~20-30% on counterfactual versions
- The performance gap between humans and LLMs widens significantly on counterfactual tasks, suggesting LLMs rely on pattern matching rather than abstract reasoning

## Why This Works (Mechanism)
The study's counterfactual approach effectively isolates abstract reasoning capabilities by testing models on input patterns unlikely to appear in their training data. This methodology reveals whether models can generalize beyond memorized patterns to truly understand the underlying relationships in analogical problems.

## Foundational Learning
- **Analogical reasoning**: Understanding relationships between different domains or representations
  - *Why needed*: Core cognitive ability being tested
  - *Quick check*: Can identify same relationship in different contexts
- **Pattern matching**: Recognition of familiar input-output relationships
  - *Why needed*: Baseline comparison for LLM behavior
  - *Quick check*: Performance drops when patterns change
- **Counterfactual reasoning**: Testing responses to hypothetical scenarios
  - *Why needed*: Distinguishes memorization from understanding
  - *Quick check*: Performance on novel, unlikely scenarios

## Architecture Onboarding

**Component Map**
Letter-string input -> Pattern recognition layer -> Relationship mapping -> Output generation

**Critical Path**
Input encoding -> Relationship extraction -> Abstract reasoning application -> Response formulation

**Design Tradeoffs**
- Pattern matching: Fast, reliable on known patterns but fails on novel inputs
- Abstract reasoning: Slower, more flexible but requires deeper understanding

**Failure Signatures**
- Sharp performance decline on counterfactual variants
- Inability to transfer learned patterns to novel contexts
- Over-reliance on surface features rather than underlying relationships

**First Experiments**
1. Test additional symbolic alphabets beyond current permutations
2. Introduce noise or distortions to standard alphabets
3. Vary the complexity of relationships in analogy problems

## Open Questions the Paper Calls Out
None specified in the provided materials

## Limitations
- Study focuses on letter-string analogies which may not capture full breadth of analogical reasoning
- Human baseline sample size and demographic details not specified
- Does not explore potential improvements through fine-tuning or few-shot prompting

## Confidence

**High confidence**:
- Empirical finding that LLMs perform significantly worse on counterfactual alphabet tasks compared to humans

**Medium confidence**:
- Interpretation that LLMs rely on pattern matching rather than abstract reasoning
- Generalizability of conclusions to broader analogical reasoning capabilities

## Next Checks
1. Replicate with additional analogical reasoning task types (visual analogies, semantic analogies)
2. Conduct experiments with fine-tuned versions of LLMs on counterfactual tasks
3. Test LLMs with varying levels of alphabet complexity and permutation