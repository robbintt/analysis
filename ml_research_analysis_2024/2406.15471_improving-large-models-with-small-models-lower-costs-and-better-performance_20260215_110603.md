---
ver: rpa2
title: 'Improving Large Models with Small models: Lower Costs and Better Performance'
arxiv_id: '2406.15471'
source_url: https://arxiv.org/abs/2406.15471
tags: []
core_contribution: This paper introduces Data Shunt+ (DS+), a general paradigm for
  collaboration between small and large models to reduce costs and improve performance.
  The core idea is to use the confidence of small models to determine whether an input
  should be processed by small models alone or with the assistance of large models.
---

# Improving Large Models with Small models: Lower Costs and Better Performance

## Quick Facts
- arXiv ID: 2406.15471
- Source URL: https://arxiv.org/abs/2406.15471
- Reference count: 40
- Key outcome: DS+ framework reduces large model query costs to 31.18% while improving ChatGPT accuracy from 94.43% to 95.64% on Amazon Product sentiment analysis

## Executive Summary
This paper introduces Data Shunt+ (DS+), a paradigm for collaborative use of small and large models to reduce costs while improving performance. The framework uses small model confidence scores to determine whether inputs should be processed by small models alone or with large model assistance. DS+ includes Small Model for Large Model (S4L) techniques like Prompt Pruning and Prompt Transferring, as well as Large Model for Small Model (L4S) with 2-Stage Confidence Distillation. Extensive experiments across multiple modalities and tasks demonstrate significant cost reduction and performance improvement.

## Method Summary
DS+ is a framework that leverages the complementary strengths of small and large models through confidence-based data shunting and knowledge transfer. The method uses small model confidence scores to route inputs either to small models (high confidence cases) or to large models with refined prompts (low confidence cases). For S4L, Prompt Pruning incorporates small model confidence into large model prompts to reduce prediction space entropy, while Prompt Transferring decomposes complex tasks into simpler subtasks. For L4S, 2-Stage Confidence Distillation iteratively transfers knowledge from large to small models on high-confidence samples, enabling small models to learn from large models without forgetting their specialized knowledge.

## Key Results
- DS+ reduced large model query costs to 31.18% while improving ChatGPT accuracy from 94.43% to 95.64% on Amazon Product sentiment analysis
- On CIFAR-100-LT image classification, DS+ achieved 57.87% accuracy compared to 53.92% for CLIP alone
- For Microsoft COCO image captioning, DS+ achieved BLEU score of 36.57 compared to 32.95 for BLIP-2 alone

## Why This Works (Mechanism)

### Mechanism 1
Small models can handle easy samples with higher confidence than hard samples, enabling effective data shunting. Small models develop proficiency on specific data distributions during training, producing high confidence predictions on similar samples while showing low confidence on out-of-distribution or ambiguous samples.

### Mechanism 2
Large models can improve small models through knowledge distillation without catastrophic forgetting. Large models possess general knowledge that can be distilled into small models using high-confidence samples from both models, incrementally expanding small models' capabilities while preserving existing knowledge.

### Mechanism 3
Prompt Pruning improves large model performance by reducing prediction space entropy. Small model confidence scores incorporated into prompts as prior knowledge effectively narrow the candidate class space for large models, reducing entropy in the prediction space and making classification easier.

## Foundational Learning

- **Entropy and information theory**: Used to theoretically justify why prompt pruning works by reducing prediction space uncertainty. Quick check: If a small model has high confidence on class A, how does this affect the entropy of the large model's prediction space?
- **Knowledge distillation**: Core to the 2CD method for transferring knowledge from large to small models without losing specialized capabilities. Quick check: What happens if knowledge distillation is performed without confidence filtering?
- **Multimodal model architectures**: Required for applying the method across language, vision, and multimodal tasks. Quick check: How would you modify the confidence calculation for a generative task versus a classification task?

## Architecture Onboarding

- **Component map**: Input -> Small model ensemble -> Confidence calculation -> Shunting decision -> (High confidence: Small model output) or (Low confidence: Large model with refined prompt) -> Knowledge distillation module
- **Critical path**: 1) Input received by specific small models, 2) Confidence calculated for shunting decision, 3) If confidence > threshold, processed by small models only, 4) If confidence < threshold, sent to large models with refined prompt, 5) Learnable small models receive distilled knowledge from large models
- **Design tradeoffs**: Model selection (stronger small models reduce query proportion but increase baseline cost), threshold selection (higher thresholds reduce queries but risk missing complex cases), prompt type (soft prompts preserve more information but may be noisier)
- **Failure signatures**: Performance degradation (small models overfit and lose distinction ability), cost inefficiency (threshold too low causing unnecessary queries), accuracy drop (large models receive noisy confidence scores)
- **First 3 experiments**: 1) Implement basic data shunting with single small model and threshold tuning on sentiment analysis task, 2) Add prompt pruning with soft prompts and measure accuracy improvement on long-tail classification, 3) Implement 2CD and validate knowledge preservation through before/after comparison on specific distributions

## Open Questions the Paper Calls Out

1. **Leveraging overfitting**: How can overfitting in small models be intentionally used to improve performance on specific data distributions? The paper identifies this phenomenon but lacks concrete methods for leveraging it across various tasks.

2. **Alternative shunting methods**: What are optimal methods for shunting data between small and large models beyond confidence levels? The paper only tests a few shunting methods without exploring the full range of possible approaches.

3. **Complex task decomposition**: How can DS+ be extended to handle more complex task decompositions beyond the examples provided? The paper introduces Prompt Transferring but only provides a single case study on dispute liability determination.

## Limitations

- Confidence-based shunting relies heavily on the assumption that small models can reliably distinguish easy from hard samples, with limited analysis of failure cases for ambiguous or adversarial inputs.
- The entropy-based theoretical justification for prompt pruning operates under idealized conditions and may not hold when small model confidence scores are systematically biased.
- The 2CD method's claim of preserving small model knowledge lacks detailed ablation studies and could lead to catastrophic forgetting in long-term deployment.

## Confidence

- **High confidence**: Core empirical findings showing cost reduction and performance improvement across multiple tasks
- **Medium confidence**: Theoretical justifications, particularly entropy analysis for prompt pruning and confidence-based shunting mechanism
- **Low confidence**: Scalability claims beyond tested tasks and modalities

## Next Checks

1. **Confidence Calibration Analysis**: Evaluate how well small model confidence scores correlate with actual prediction accuracy across different data distributions to validate the shunting mechanism.

2. **Long-term Knowledge Retention Test**: Implement multi-stage training with repeated 2CD applications to measure whether small models retain original capabilities while acquiring new knowledge.

3. **Adversarial Sample Robustness**: Test DS+ performance on adversarial examples designed to fool small model confidence scores to identify potential exploitation of the shunting mechanism.