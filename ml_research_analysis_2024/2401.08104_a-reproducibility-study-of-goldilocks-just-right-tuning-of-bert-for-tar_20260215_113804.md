---
ver: rpa2
title: 'A Reproducibility Study of Goldilocks: Just-Right Tuning of BERT for TAR'
arxiv_id: '2401.08104'
source_url: https://arxiv.org/abs/2401.08104
tags:
- bert
- documents
- original
- pre-training
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper reproduces a BERT-based TAR pipeline, focusing on the
  effect of further pre-training epochs on classifier effectiveness. Using RCV1-v2
  and Jeb Bush datasets, the study confirms that further pre-training is essential
  for BERT's effectiveness, but the optimal "Goldilocks" epoch varies by dataset.
---

# A Reproducibility Study of Goldilocks: Just-Right Tuning of BERT for TAR

## Quick Facts
- arXiv ID: 2401.08104
- Source URL: https://arxiv.org/abs/2401.08104
- Authors: Xinyu Mao; Bevan Koopman; Guido Zuccon
- Reference count: 31
- One-line primary result: BERT-based TAR pipeline effectiveness improves with domain-specific pre-training, with optimal epochs varying by dataset.

## Executive Summary
This paper reproduces a BERT-based TAR pipeline focusing on the effect of further pre-training epochs on classifier effectiveness. The study confirms that further pre-training is critical for BERT's effectiveness, but the optimal "Goldilocks" epoch varies by dataset and category. On RCV1-v2, BERT with 10 further pre-training epochs outperforms baseline logistic regression across all metrics. For Jeb Bush, BERT underperforms logistic regression even at the Goldilocks epoch. The study also evaluates the pipeline on medical systematic review datasets (CLEF TAR), finding that domain-specific BERT models (e.g., BioLinkBERT) outperform both general BERT and logistic regression without requiring further pre-training.

## Method Summary
The study reproduces a BERT-based TAR pipeline using RCV1-v2 and Jeb Bush datasets for replication, and CLEF TAR collections for generalizability testing. The pipeline involves two key components: (1) Further Pre-training: BERT is further pre-trained using Masked Language Modeling on the target dataset for varying epochs (0, 1, 2, 5, 10), and (2) Fine-tuning: Active learning is used to iteratively fine-tune the BERT model, with relevance feedback and uncertainty sampling strategies. The process is repeated for 20 iterations, and the optimal epoch (Goldilocks epoch) is determined based on performance metrics including R-Precision and training costs.

## Key Results
- Further pre-training is critical for BERT effectiveness in TAR tasks, with optimal epochs varying by dataset and category
- BERT with 10 further pre-training epochs outperforms logistic regression on RCV1-v2 across all metrics
- Domain-specific BERT models (e.g., BioLinkBERT) outperform general BERT and logistic regression on CLEF TAR without requiring further pre-training
- Jeb Bush dataset shows BERT underperforms logistic regression even at optimal pre-training epochs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: BERT-based TAR pipeline effectiveness improves with additional pre-training epochs on the target corpus.
- Mechanism: Further pre-training aligns the language model's distribution with the domain-specific corpus, reducing the gap between pre-training and fine-tuning contexts. This improves the classifier's ability to distinguish relevant from irrelevant documents.
- Core assumption: Domain shift exists between general BERT pre-training corpus and target corpus; further pre-training can reduce this mismatch.
- Evidence anchors:
  - [abstract] "that further pre-training is critical to high effectiveness"
  - [section] "the additional pre-training of the BERT backbone to the document collection of the TAR task, which is critical to obtain effectiveness improvements"
- Break condition: If target corpus is too small or domain-specific pre-trained models exist, further pre-training may not help or could even degrade performance.

### Mechanism 2
- Claim: There exists a dataset-specific optimal number of pre-training epochs ("Goldilocks epoch").
- Mechanism: Too few epochs result in insufficient adaptation; too many epochs cause catastrophic forgetting of general language knowledge. The optimal epoch balances these effects.
- Core assumption: Pre-training effectiveness follows a non-linear curve with a peak at the Goldilocks epoch.
- Evidence anchors:
  - [abstract] "the optimal 'Goldilocks' epoch varies by dataset"
  - [section] "the existence of a Goldilocks (or 'just-right') epoch of further pre-training"
- Break condition: If the Goldilocks epoch is not identifiable through validation, the approach fails to outperform baselines.

### Mechanism 3
- Claim: Domain-specific pre-trained models (e.g., BioLinkBERT) can outperform general BERT with optimal pre-training.
- Mechanism: Domain-specific pre-training already aligns the model with the target domain, eliminating the need for further adaptation and avoiding the Goldilocks epoch optimization problem.
- Core assumption: Pre-training corpus domain alignment is more important than task-specific fine-tuning.
- Evidence anchors:
  - [abstract] "domain-specific BERT models (e.g., BioLinkBERT) outperform both general BERT and logistic regression without requiring further pre-training"
  - [section] "we show that there is no need for further pre-training if a domain-specific BERT backbone is used within the active learning pipeline"
- Break condition: If domain-specific models are unavailable or too computationally expensive to obtain, this advantage disappears.

## Foundational Learning

- Concept: Active Learning
  - Why needed here: The TAR pipeline uses active learning to iteratively select informative documents for human review, reducing annotation effort.
  - Quick check question: What are the two main active learning strategies mentioned, and how do they differ in document selection?

- Concept: Fine-tuning vs. Pre-training
  - Why needed here: Understanding the difference between further pre-training (domain adaptation) and fine-tuning (task adaptation) is crucial for interpreting the pipeline's components.
  - Quick check question: What is the key difference between further pre-training and fine-tuning in this pipeline?

- Concept: Domain Adaptation
  - Why needed here: The paper investigates whether domain mismatch affects BERT performance and how different approaches address this.
  - Quick check question: How does further pre-training address domain mismatch, and why might it still fail for out-of-domain tasks?

## Architecture Onboarding

- Component map: Document corpus and seed relevant document -> Further Pre-training (optional) -> Fine-tuning -> Active Learning iterations -> Ranked documents with relevance scores
- Critical path: Further Pre-training → Fine-tuning → Active Learning iterations → Final classifier
- Design tradeoffs:
  - Further pre-training vs. computational cost: More epochs improve effectiveness but increase training time significantly
  - Batch size selection: Larger batches (200) work for larger datasets but may be inefficient for smaller CLEF collections
  - Active learning strategy: Relevance feedback vs. uncertainty sampling affects both effectiveness and review costs differently
- Failure signatures:
  - BERT underperforms logistic regression even at Goldilocks epoch (indicates pre-training not helpful)
  - Inconsistent Goldilocks epoch across categories (indicates dataset-specific optimization challenges)
  - High review costs despite good R-Precision (indicates poor ranking quality)
- First 3 experiments:
  1. Replicate the original RCV1-v2 experiment with 0, 1, 2, 5, 10 pre-training epochs to observe Goldilocks effect
  2. Run the same experiment on CLEF TAR collections to test domain generalization
  3. Replace BERT-base with BioLinkBERT and compare effectiveness without further pre-training

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal Goldilocks epoch for BERT-based TAR models across different datasets and categories?
- Basis in paper: [explicit] The paper identifies that the optimal number of further pre-training epochs (Goldilocks epoch) varies by dataset and category, but this variation is not fully explained.
- Why unresolved: The study shows that the Goldilocks epoch differs between datasets (e.g., RCV1-v2 vs. Jeb Bush) and even within categories of the same dataset, but it does not provide a systematic method to determine this epoch a priori.
- What evidence would resolve it: A predictive model or set of heuristics that can accurately determine the Goldilocks epoch based on dataset characteristics (e.g., domain, size, category difficulty) without extensive experimentation.

### Open Question 2
- Question: How do domain-specific BERT models like BioLinkBERT compare to general BERT models in TAR tasks across different domains?
- Basis in paper: [explicit] The paper demonstrates that BioLinkBERT outperforms general BERT models in medical systematic review tasks without requiring further pre-training, but it does not explore this comparison across other domains.
- Why unresolved: The study focuses on biomedical literature, leaving open the question of whether domain-specific models would similarly outperform general BERT in other specialized domains (e.g., legal, patent).
- What evidence would resolve it: Comparative experiments using domain-specific BERT models (e.g., legalBERT, patentBERT) in their respective TAR tasks to assess generalizability of the findings.

### Open Question 3
- Question: What are the trade-offs between using relevance feedback and uncertainty sampling as active learning strategies in BERT-based TAR pipelines?
- Basis in paper: [explicit] The paper notes that the choice of active learning strategy affects the Goldilocks epoch and final performance, but it does not provide a clear framework for selecting between strategies.
- Why unresolved: While the study shows that relevance feedback and uncertainty sampling yield different results, it does not establish guidelines for when one strategy is preferable over the other.
- What evidence would resolve it: A systematic evaluation of active learning strategies across diverse TAR tasks, including metrics beyond R-Precision (e.g., computational efficiency, scalability) to inform strategy selection.

## Limitations
- Limited evaluation of domain adaptation effectiveness across diverse domains
- Computational cost analysis limited to two training strategies without considering real-world operational costs
- The generalizability of BioLinkBERT's effectiveness to non-biomedical domains remains untested

## Confidence

- **High Confidence**: The critical importance of further pre-training for BERT effectiveness in TAR tasks is well-supported by the experimental results across multiple datasets and metrics.
- **Medium Confidence**: The existence of dataset-specific Goldilocks epochs is supported by the experimental data, but the exact mechanism determining these optimal values remains unclear.
- **Low Confidence**: The claim that domain-specific pre-trained models eliminate the need for further pre-training is based on a single domain (biomedical) and requires validation across diverse domains.

## Next Checks
1. Cross-domain validation: Test the effectiveness of BioLinkBERT vs. general BERT with further pre-training on non-biomedical domains (e.g., legal or newswire) to validate the domain adaptation hypothesis.
2. Goldilocks epoch prediction: Investigate whether dataset characteristics (size, domain specificity, label distribution) can predict the optimal pre-training epoch, reducing the need for exhaustive epoch testing.
3. Cost-benefit analysis: Conduct a comprehensive analysis of the trade-off between computational costs of further pre-training and performance gains across different dataset sizes and domain specificities.