---
ver: rpa2
title: Extreme Learning Machines for Fast Training of Click-Through Rate Prediction
  Models
arxiv_id: '2406.17828'
source_url: https://arxiv.org/abs/2406.17828
tags:
- learning
- training
- dataset
- layer
- masknet
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the application of Extreme Learning Machines
  (ELMs) for Click-Through Rate (CTR) prediction, a high-dimensional problem where
  ELMs have been largely unexplored. The authors introduce an ELM-based model enhanced
  with embedding layers, which is a novel addition to the field.
---

# Extreme Learning Machines for Fast Training of Click-Through Rate Prediction Models

## Quick Facts
- arXiv ID: 2406.17828
- Source URL: https://arxiv.org/abs/2406.17828
- Authors: Ergun Biçici
- Reference count: 20
- Primary result: ELM with embeddings achieves competitive F1 results while significantly reducing training time compared to Masknet

## Executive Summary
This paper explores the application of Extreme Learning Machines (ELMs) for Click-Through Rate (CTR) prediction, a high-dimensional problem where ELMs have been largely unexplored. The authors introduce an ELM-based model enhanced with embedding layers, which is a novel addition to the field. Experimental results on benchmark datasets, including Avazu and Criteo, demonstrate that the proposed ELM with embeddings achieves competitive F1 results while significantly reducing training time compared to state-of-the-art models such as Masknet.

## Method Summary
The paper introduces an ELM-based model enhanced with embedding layers for CTR prediction. The method involves training DeepFM for one epoch to obtain embedding weights, which are then used with ELM and ML-ELM models. The ELMs use randomly initialized hidden layer weights and learn output layer weights via ridge regression. The approach is evaluated on Avazu and Criteo datasets, comparing performance metrics including F1 score, AUC, and logloss against the Masknet baseline.

## Key Results
- ELM with embeddings achieves competitive F1 results while significantly reducing training time compared to Masknet
- ELM model with embeddings outperforms Masknet on the Avazu dataset in terms of F1 score
- ML-ELM with embeddings performs better than Masknet on the Criteo dataset in terms of F1 score

## Why This Works (Mechanism)

### Mechanism 1
- ELM achieves fast training by avoiding gradient-based weight updates for the hidden layer, instead using randomly initialized weights and learning only output layer weights via closed-form least-squares solution
- Core assumption: Random projection of inputs to hidden layer features is sufficiently rich to approximate the target function
- Evidence anchors: [abstract] "ELM is a ridge regression model with a single hidden layer whose weights and biases are randomly initialized" and [section] "˜W are random and W is the OLS solution"

### Mechanism 2
- Adding embedding layers improves ELM's ability to handle high-cardinality categorical features by transforming sparse categorical indices into dense vectors that ELM can process efficiently
- Core assumption: Embedding weights learned on the same task are transferable and preserve meaningful relationships for ELM's random projection
- Evidence anchors: [section] "Embedding techniques can represent high-dimensional categorical data in a lower-dimensional space while preserving important relationships"

### Mechanism 3
- F1 metric is more sensitive to class imbalance and prediction confidence than AUC or logloss, making it a better fit for sparse CTR datasets
- Core assumption: The business or evaluation goal prioritizes detecting clicks over ranking all impressions
- Evidence anchors: [abstract] "Our proposed ELM with embeddings achieves competitive F1 results"

## Foundational Learning

- Concept: Linear algebra for least-squares solutions
  - Why needed here: ELM's output weights are computed as (HT H + λI)^−1 HT y, requiring matrix inversion and understanding of ridge regression
  - Quick check question: Can you derive the closed-form solution for W given H and y, and explain the role of λ?

- Concept: Random projection theory
  - Why needed here: ELM relies on randomly initialized hidden layer weights; understanding when random projections preserve distances or separability is key to predicting ELM's approximation power
  - Quick check question: Under what conditions does a random projection guarantee that the span of h(x, ˜w, b) is dense in L2 space?

- Concept: Embedding representations for categorical data
  - Why needed here: High-cardinality features are mapped to dense vectors via learned embeddings to reduce dimensionality before ELM processing
  - Quick check question: How do learned embeddings differ from one-hot encodings, and why are they preferable for large vocabularies?

## Architecture Onboarding

- Component map: Input layer → Embedding layer (pre-trained) → ELM hidden layer (random weights) → Output layer (ridge regression weights)
- Critical path:
  1. Load and preprocess raw CTR features
  2. Train DeepFM for one epoch to obtain embedding weights
  3. Save and load embeddings into ELM pipeline
  4. Randomly initialize ELM hidden layer weights
  5. Compute hidden layer matrix H
  6. Solve for output weights W using ridge regression
  7. Evaluate on validation/test sets using F1, AUC, logloss
- Design tradeoffs:
  - Random hidden weights give speed but no guarantee of optimal feature mapping
  - Pre-trained embeddings reduce dimensionality but require extra training step
  - Single hidden layer limits expressiveness compared to deep networks
- Failure signatures:
  - Extremely low F1 with decent AUC: class imbalance handling issue
  - High training time with small datasets: inefficient embedding loading or redundant computation
  - NaN or Inf in W: ill-conditioned H matrix (try adjusting λ)
- First 3 experiments:
  1. Train ELM without embeddings on a small subset; verify F1 and training time
  2. Add pre-trained embeddings; compare F1 and training time
  3. Vary hidden layer size (e.g., 500, 1000, 2000 neurons); observe F1/AUC tradeoffs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do ELM models with embeddings compare to other state-of-the-art CTR prediction models beyond Masknet in terms of AUC and logloss?
- Basis in paper: [explicit] The paper mentions that ELM models with embeddings outperform Masknet in F1 score but do not achieve the same performance in terms of AUC or logloss
- Why unresolved: The paper only compares ELM models with embeddings to Masknet and does not provide a broader comparison with other state-of-the-art models
- What evidence would resolve it: Conducting experiments comparing ELM models with embeddings to a wider range of state-of-the-art CTR prediction models and reporting their performance in terms of AUC and logloss

### Open Question 2
- Question: What is the impact of different embedding dimensions on the performance of ELM models for CTR prediction?
- Basis in paper: [explicit] The paper uses an embedding dimension of 8 for the Masknet model but does not explore the effect of varying embedding dimensions on ELM models
- Why unresolved: The paper does not investigate how different embedding dimensions affect the performance of ELM models
- What evidence would resolve it: Experimenting with ELM models using various embedding dimensions and analyzing their impact on performance metrics such as F1 score, AUC, and logloss

### Open Question 3
- Question: How does the training time of ELM models with embeddings scale with increasing dataset size and feature dimensionality?
- Basis in paper: [explicit] The paper mentions that ELM models significantly reduce training time compared to Masknet but does not provide detailed analysis on how training time scales with dataset size and feature dimensionality
- Why unresolved: The paper lacks a comprehensive analysis of the scalability of training time for ELM models with embeddings as dataset size and feature dimensionality increase
- What evidence would resolve it: Conducting experiments to measure the training time of ELM models with embeddings on datasets of varying sizes and feature dimensionalities, and analyzing the relationship between training time and these factors

## Limitations
- Exact implementation details of ELM and ML-ELM models are not fully specified
- Limited comparison with other state-of-the-art CTR prediction models beyond Masknet
- Lack of comprehensive analysis on how training time scales with dataset size and feature dimensionality

## Confidence
- Claim: ELM with embeddings achieves competitive F1 while significantly reducing training time compared to Masknet
  - Confidence: Medium (supported by experimental results but relies on unpublished implementation specifics)
- Claim: ELMs are useful for CTR prediction when fast training is needed
  - Confidence: Low (limited dataset scope and lack of comparison with other fast models)
- Claim: F1 is a better metric for CTR tasks with class imbalance
  - Confidence: Medium (justified by metric properties but not extensively validated)

## Next Checks
1. Replicate the ELM training pipeline on a small, public dataset to verify the reported training time reduction
2. Compare ELM+embeddings performance against other fast CTR models like logistic regression or linear models on the same metrics
3. Test ELM sensitivity to random seed and hidden layer size to establish robustness of the approach