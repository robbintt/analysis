---
ver: rpa2
title: 'Chain-of-Specificity: An Iteratively Refining Method for Eliciting Knowledge
  from Large Language Models'
arxiv_id: '2402.15526'
source_url: https://arxiv.org/abs/2402.15526
tags:
- specific
- prompt
- constraints
- general
- input
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of large language models (LLMs)
  struggling to adhere to specific constraints in input instructions, leading to generic
  responses. To tackle this, the authors propose Chain-of-Specificity (CoS), a method
  that iteratively emphasizes specific constraints to elicit underlying knowledge
  from LLMs and refine responses.
---

# Chain-of-Specificity: An Iteratively Refining Method for Eliciting Knowledge from Large Language Models

## Quick Facts
- arXiv ID: 2402.15526
- Source URL: https://arxiv.org/abs/2402.15526
- Reference count: 21
- One-line primary result: Chain-of-Specificity iteratively emphasizes specific constraints to improve LLM adherence to complex instructions, outperforming baselines especially with multiple constraints.

## Executive Summary
This paper addresses the problem of large language models struggling to adhere to specific constraints in input instructions, leading to generic responses. To tackle this, the authors propose Chain-of-Specificity (CoS), a method that iteratively emphasizes specific constraints to elicit underlying knowledge from LLMs and refine responses. Experiments on datasets with complex constraints demonstrate that CoS outperforms existing methods, especially as the number of specific constraints increases. Additionally, distilling CoS-generated responses effectively enhances smaller models' ability to follow constrained instructions, with a 90% beat rate over non-distilled methods. The study also introduces a new dataset, ConstrainSPEC, with more complex constraints to evaluate model performance in intricate scenarios.

## Method Summary
Chain-of-Specificity (CoS) is a method that iteratively emphasizes specific constraints in input instructions to elicit knowledge from LLMs and refine responses. It first identifies the general goal and all specific constraints in the input instruction. Then, it generates a general answer for the goal and iteratively refines this answer by emphasizing each specific constraint in sequence. This process allows the model to incorporate more relevant domain knowledge with each iteration. The method also includes a distillation component where responses generated by larger LLMs using CoS are used to fine-tune smaller models through supervised learning, transferring the ability to follow specific constraints.

## Key Results
- CoS outperforms existing methods, particularly as the number of specific constraints increases
- CoS-multi-step approach maintains stable performance across different constraint settings
- Distilling responses generated by CoS effectively enhances smaller models' ability to follow constrained instructions, with a 90% beat rate over non-distilled methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative refinement on specific constraints elicits more domain-specific knowledge from LLMs
- Mechanism: The CoS method first identifies the general goal and specific constraints from the input instruction, then generates a general answer for the goal. It then iteratively refines this answer by emphasizing each specific constraint in sequence, allowing the model to incorporate more relevant domain knowledge with each iteration.
- Core assumption: LLMs contain sufficient domain knowledge that can be elicited through targeted prompting on specific constraints
- Evidence anchors:
  - [abstract] "CoS iteratively emphasizes the specific constraints in the input instructions, unlocks knowledge within LLMs, and refines responses"
  - [section] "It first identify the general goal and all the specific constraints in the input instruction. After that, it takes the specific constraints as the reasoning chain and iteratively emphasises on the specific constraints to elicit the knowledge embedded in LLMs"
  - [corpus] Weak - no direct evidence in corpus neighbors, but related to "Towards eliciting latent knowledge from LLMs with mechanistic interpretability"
- Break condition: If the LLM lacks sufficient domain knowledge about the specific constraints, iterative refinement will not improve response quality

### Mechanism 2
- Claim: Multi-step prompting outperforms single-step approaches for complex constraint scenarios
- Mechanism: CoS uses a multi-step approach where specific constraints are added iteratively, allowing the model to build upon previous context and reasoning. This contrasts with single-step approaches that must address all constraints simultaneously.
- Core assumption: LLMs can better reason about complex problems when constraints are introduced sequentially rather than all at once
- Evidence anchors:
  - [abstract] "as the number of specific constraints increase, other baselines falter, while CoS still performs well"
  - [section] "CoS-multi-step approach maintained a relatively stable performance across different specific constraint settings"
  - [corpus] Weak - no direct evidence, but related to "Adaptive Multi-Agent Response Refinement in Conversational Systems"
- Break condition: If constraints are simple or few in number, the overhead of multi-step prompting may not provide additional benefit

### Mechanism 3
- Claim: Distilling refined responses improves smaller model performance on constrained tasks
- Mechanism: Responses generated by larger LLMs using CoS are used to fine-tune smaller models through supervised learning, transferring the ability to follow specific constraints
- Core assumption: High-quality responses from larger models contain patterns and knowledge that can be effectively transferred to smaller models
- Evidence anchors:
  - [abstract] "distilling responses generated by CoS effectively enhances the ability of smaller models to follow the constrained instructions. With a 90% beat rate over non-distilled methods"
  - [section] "distilling responses from larger LLMs to the smaller LLMs... the beat rate of 55.8% favoring the CoS-multi-step over direct prompting"
  - [corpus] Weak - no direct evidence, but related to knowledge distillation approaches
- Break condition: If the gap between large and small models is too significant, distilled knowledge may not transfer effectively

## Foundational Learning

- Concept: Chain-of-thought reasoning
  - Why needed here: CoS builds on the principle that LLMs can generate better responses when they reason through intermediate steps
  - Quick check question: How does CoS differ from standard chain-of-thought prompting?

- Concept: Prompt engineering and template design
  - Why needed here: CoS relies on carefully crafted prompt templates to identify constraints and iteratively refine responses
  - Quick check question: What are the key components of the CoS prompt template structure?

- Concept: Knowledge distillation
  - Why needed here: The CoS method leverages distillation to transfer refined response capabilities to smaller models
  - Quick check question: What type of data is used to train smaller models in the CoS distillation approach?

## Architecture Onboarding

- Component map:
  Constraint identification module -> General response generator -> Iterative refinement engine -> Distillation pipeline

- Critical path:
  1. Parse input instruction for constraints
  2. Generate general response
  3. Iteratively refine with each specific constraint
  4. (Optional) Distill refined responses to smaller models

- Design tradeoffs:
  - Single-step vs multi-step refinement: Single-step is faster but less effective for complex constraints
  - Automatic vs manual constraint identification: Automatic is scalable but may miss nuanced constraints
  - Distillation vs direct prompting: Distillation improves smaller models but requires additional training resources

- Failure signatures:
  - Poor constraint identification leading to irrelevant refinements
  - Iterative refinements diverging from original intent
  - Distillation failing to transfer knowledge effectively

- First 3 experiments:
  1. Compare CoS-multi-step vs CoS-one-step on ConstrainSPEC with varying constraint numbers
  2. Test CoS performance on public datasets with limited constraints (CoScript, EXPLORE-INSTRUCT)
  3. Evaluate distillation effectiveness by comparing trained smaller models vs direct prompting on constrained tasks

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of Chain-of-Specificity (CoS) scale with increasingly complex or numerous specific constraints beyond those tested in the ConstrainSPEC dataset?
- **Basis in paper**: [explicit] The paper demonstrates CoS's effectiveness on datasets with varying numbers of constraints and introduces ConstrainSPEC to test more complex scenarios.
- **Why unresolved**: The paper's experiments focus on a limited range of constraint complexities. It's unclear how CoS would perform on instructions with an order of magnitude more constraints or constraints that are highly interdependent.
- **What evidence would resolve it**: Systematic testing of CoS on datasets with progressively more complex and numerous constraints, measuring performance degradation points and identifying breaking complexity thresholds.

### Open Question 2
- **Question**: What are the limitations of CoS when applied to domains outside brainstorming and script generation, particularly in technical or highly specialized fields?
- **Basis in paper**: [explicit] The paper validates CoS on brainstorming and script domains but does not explore its generalizability to other domains.
- **Why unresolved**: The method's reliance on identifying "general goals" and "specific constraints" may not translate well to domains where these concepts are less clear-cut or where domain-specific knowledge is crucial.
- **What evidence would resolve it**: Applying CoS to diverse domains (e.g., legal document generation, scientific hypothesis formation) and measuring performance compared to domain-specific baselines.

### Open Question 3
- **Question**: How does the iterative refinement process in CoS affect the coherence and consistency of generated responses, especially when constraints conflict or when refinement introduces contradictions?
- **Basis in paper**: [explicit] CoS iteratively refines responses by adding specific constraints, but the paper doesn't address how this process handles conflicting constraints or maintains overall response coherence.
- **Why unresolved**: Iterative refinement could potentially lead to responses that are internally inconsistent or that prioritize certain constraints over others in unintended ways.
- **What evidence would resolve it**: Analysis of CoS-generated responses for internal consistency, especially in cases where constraints might conflict, and comparison with non-iterative approaches.

### Open Question 4
- **Question**: What is the computational overhead of CoS compared to direct prompting, and how does this impact its practical deployment in real-time applications?
- **Basis in paper**: [explicit] The paper mentions that CoS performs multiple iterations of refinement, implying additional computational cost, but does not quantify this overhead.
- **Why unresolved**: While CoS shows improved performance, the trade-off between performance gains and increased computational resources is not explored, which is crucial for practical applications.
- **What evidence would resolve it**: Benchmarking the time and resource usage of CoS against direct prompting across various instruction complexities, and analyzing the performance-to-resource ratio.

### Open Question 5
- **Question**: How sensitive is CoS to the quality and specificity of the initial identification of general goals and specific constraints?
- **Basis in paper**: [explicit] CoS relies on accurately identifying general goals and specific constraints in the first stage, but the paper doesn't explore how errors in this initial step propagate through the refinement process.
- **Why unresolved**: Misidentification of constraints or goals could lead to suboptimal refinements, potentially degrading performance rather than improving it.
- **What evidence would resolve it**: Controlled experiments where the initial identification step is intentionally perturbed or made erroneous, measuring the impact on final response quality and comparing with ground truth constraint identification.

## Limitations
- CoS relies heavily on accurate constraint identification, which may fail with nuanced or implicit constraints
- The computational overhead of multi-step refinement presents practical constraints for latency-sensitive applications
- Knowledge distillation effectiveness depends on the quality of generated responses and may not generalize across all model architectures

## Confidence

High confidence: Core iterative refinement mechanism consistently improves performance across multiple datasets and evaluation methods

Medium confidence: Distillation approach effectiveness, demonstrated only on specific model pairs, may not generalize to all LLM architectures

Low confidence: Method's performance on highly complex constraint scenarios beyond those tested in ConstrainSPEC

## Next Checks

1. Test CoS performance on open-ended generation tasks with implicit constraints rather than explicit instruction-following
2. Evaluate the method's robustness across different model families (e.g., decoder-only vs encoder-decoder architectures)
3. Conduct ablation studies isolating the contribution of each refinement step to quantify diminishing returns in constraint complexity scenarios