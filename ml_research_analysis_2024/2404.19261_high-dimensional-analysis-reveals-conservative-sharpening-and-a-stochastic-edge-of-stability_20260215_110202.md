---
ver: rpa2
title: High dimensional analysis reveals conservative sharpening and a stochastic
  edge of stability
arxiv_id: '2404.19261'
source_url: https://arxiv.org/abs/2404.19261
tags:
- learning
- batch
- dynamics
- have
- stability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the dynamics of stochastic gradient descent
  (SGD) in deep learning, focusing on the behavior of Hessian eigenvalues during training.
  The authors introduce the concept of a stochastic edge of stability (S-EOS), which
  differs from the deterministic EOS in the full batch setting.
---

# High dimensional analysis reveals conservative sharpening and a stochastic edge of stability

## Quick Facts
- arXiv ID: 2404.19261
- Source URL: https://arxiv.org/abs/2404.19261
- Authors: Atish Agarwala; Jeffrey Pennington
- Reference count: 40
- This paper investigates the dynamics of stochastic gradient descent (SGD) in deep learning, focusing on the behavior of Hessian eigenvalues during training and introducing the concept of a stochastic edge of stability (S-EOS).

## Executive Summary
This paper investigates the dynamics of stochastic gradient descent (SGD) in deep learning, focusing on the behavior of Hessian eigenvalues during training. The authors introduce the concept of a stochastic edge of stability (S-EOS), which differs from the deterministic EOS in the full batch setting. They analyze a quadratic regression model to derive theoretical insights, showing that conservative sharpening - the suppression of Hessian eigenvalue increase with decreasing batch size - depends on both the Jacobian and its gradient statistics. The key result is the noise kernel norm K, which characterizes stability in the stochastic setting and self-stabilizes near the critical value of 1. Experimental results on MNIST and CIFAR10 datasets with fully connected networks and ResNet18 architectures demonstrate that K is a more informative quantity than λmax for understanding SGD dynamics, particularly in small batch regimes.

## Method Summary
The authors analyze a quadratic regression model to derive theoretical insights into the behavior of Hessian eigenvalues during SGD training. They introduce the noise kernel norm K as a measure of stability in the stochastic setting and show that it self-stabilizes near the critical value of 1. The theoretical analysis is complemented by experiments on MNIST and CIFAR10 datasets using fully connected networks and ResNet18 architectures, comparing the behavior of K and λmax in understanding SGD dynamics across different batch sizes.

## Key Results
- Introduction of the stochastic edge of stability (S-EOS) concept, which differs from the deterministic EOS in the full batch setting
- Theoretical analysis of conservative sharpening, showing suppression of Hessian eigenvalue increase with decreasing batch size
- Noise kernel norm K characterizes stability in the stochastic setting and self-stabilizes near the critical value of 1
- Experimental validation on MNIST and CIFAR10 datasets demonstrates K's superiority over λmax in understanding SGD dynamics, especially in small batch regimes

## Why This Works (Mechanism)
The mechanism behind the stochastic edge of stability (S-EOS) lies in the interplay between the deterministic gradient descent (GD) dynamics and the stochastic fluctuations introduced by SGD. The noise kernel norm K captures the stability of the system in the presence of these fluctuations. When K is below the critical threshold of 1, the system is stable, and when it exceeds 1, the system becomes unstable. The self-stabilizing property of K near the critical value of 1 emerges from the conservative sharpening effect, where the suppression of Hessian eigenvalue increase with decreasing batch size balances the increasing noise in the gradient estimates.

## Foundational Learning

1. **Stochastic Gradient Descent (SGD)**
   - Why needed: Understanding the optimization algorithm used in deep learning
   - Quick check: Familiarize with the update rule: w_{t+1} = w_t - η ∇L(w_t) + noise, where η is the learning rate and noise represents the stochastic fluctuations

2. **Edge of Stability (EOS)**
   - Why needed: Concept from deterministic gradient descent (GD) that characterizes the transition between stable and unstable dynamics
   - Quick check: Understand that EOS occurs when the maximum Hessian eigenvalue λmax ≈ 2/η, where η is the learning rate

3. **Conservative Sharpening**
   - Why needed: Phenomenon where the suppression of Hessian eigenvalue increase with decreasing batch size depends on both the Jacobian and its gradient statistics
   - Quick check: Recognize that conservative sharpening is a key mechanism behind the self-stabilizing property of the noise kernel norm K

4. **Noise Kernel Norm K**
   - Why needed: Measure of stability in the stochastic setting that characterizes the S-EOS
   - Quick check: Understand that K self-stabilizes near the critical value of 1 and is more informative than λmax for understanding SGD dynamics

5. **Quadratic Regression Model**
   - Why needed: Analytically tractable model used to derive theoretical insights into the behavior of Hessian eigenvalues during SGD training
   - Quick check: Familiarize with the quadratic loss function L(w) = (1/2)(w^T A w - b^T w)^2 and its properties

## Architecture Onboarding

- **Component Map**: SGD optimization -> Hessian eigenvalue analysis -> Conservative sharpening -> Noise kernel norm K -> S-EOS characterization
- **Critical Path**: The critical path involves analyzing the behavior of Hessian eigenvalues during SGD training, deriving the noise kernel norm K, and establishing its relationship to the S-EOS
- **Design Tradeoffs**: The quadratic regression model provides analytical tractability but may not fully capture the complexity of deep learning landscapes. The experiments validate the theoretical insights but are limited to specific architectures and datasets.
- **Failure Signatures**: If K does not self-stabilize near the critical value of 1 or if the conservative sharpening effect is not observed, the theoretical framework may need refinement.
- **First Experiments**:
  1. Replicate the theoretical analysis of conservative sharpening for different loss landscapes and network architectures
  2. Conduct experiments on a wider range of datasets and network architectures to assess the universality of the S-EOS framework
  3. Investigate the practical implications of S-EOS control in real-world training scenarios, including its impact on generalization performance and training efficiency across different learning tasks

## Open Questions the Paper Calls Out
None

## Limitations
- The quadratic regression model, while analytically tractable, may not fully capture the complexity of deep learning landscapes, potentially limiting the generalizability of the results
- The experimental validation focuses on specific network architectures (fully connected networks and ResNet18) and datasets (MNIST and CIFAR10), raising questions about broader applicability
- The claim about controlling the S-EOS for improved optimization outcomes is supported by limited exploration of the optimization space

## Confidence
- Theoretical claims regarding conservative sharpening: Medium
- Experimental results supporting the S-EOS framework and the superiority of K over λmax: High
- Claim about controlling the S-EOS for improved optimization outcomes: Medium

## Next Checks
1. Extend the theoretical analysis to more complex loss landscapes, such as those encountered in modern deep learning architectures
2. Conduct experiments on a wider range of datasets and network architectures, including vision transformers and language models, to assess the universality of the S-EOS framework
3. Investigate the practical implications of S-EOS control in real-world training scenarios, including its impact on generalization performance and training efficiency across different learning tasks