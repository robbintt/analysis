---
ver: rpa2
title: 'SERPENT-VLM : Self-Refining Radiology Report Generation Using Vision Language
  Models'
arxiv_id: '2404.17912'
source_url: https://arxiv.org/abs/2404.17912
tags:
- report
- image
- generation
- serpent-vlm
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SERPENT-VLM, a self-refining method for generating
  accurate and coherent radiology reports from chest X-ray images. The method employs
  a visual encoder to process the image, followed by a Large Language Model (LLM)
  to generate an initial report.
---

# SERPENT-VLM : Self-Refining Radiology Report Generation Using Vision Language Models

## Quick Facts
- arXiv ID: 2404.17912
- Source URL: https://arxiv.org/abs/2404.17912
- Authors: Manav Nitin Kapadnis; Sohan Patnaik; Abhilash Nandy; Sourjyadip Ray; Pawan Goyal; Debdoot Sheet
- Reference count: 14
- Key outcome: Achieves state-of-the-art performance on IU X-ray and ROCO datasets with Bleu4 score of 0.190, surpassing previous state-of-the-art by 4%

## Executive Summary
SERPENT-VLM introduces a self-refining method for generating accurate radiology reports from chest X-ray images using a visual encoder, LLM, and a novel self-supervised loss that aligns image and text representations. The method combines standard causal language modeling with similarity-based self-refining to reduce hallucination and improve report quality. It outperforms existing baselines including LLaVA-Med and BiomedGPT, and demonstrates robustness against noisy images.

## Method Summary
SERPENT-VLM processes chest X-ray images through a frozen Swin Transformer visual encoder, projects the features via a linear visual mapper into the LLM embedding space, and uses LLaMA2-7B (with LoRA) to generate reports. A self-refining loss minimizes the negative exponential of the similarity between pooled image representations and attention-weighted aggregated representations of the generated text, alongside standard causal language modeling. The model is trained on IU X-ray and ROCO datasets and achieves state-of-the-art performance.

## Key Results
- Achieves Bleu4 score of 0.190 on IU X-ray dataset, surpassing previous state-of-the-art by 4%
- Outperforms LLaVA-Med and BiomedGPT baselines on both IU X-ray and ROCO datasets
- Demonstrates robustness against noisy images, maintaining performance under degraded image quality

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-refining loss improves alignment between generated reports and input images by encouraging high similarity between pooled image representation and contextual report representation.
- Mechanism: The model computes an aggregated representation of the generated report using attention weights from the LLM's last layer, then minimizes the negative exponential of the similarity between this text representation and the pooled image representation. This dynamic alignment reduces hallucination.
- Core assumption: Image and text representations can be meaningfully compared in a shared semantic space, and increasing their similarity leads to more grounded reports.
- Evidence anchors:
  - [abstract] "a unique self-supervised loss that leverages similarity between pooled image representations and the contextual representations of the generated radiological text"
  - [section 3.3] "we enforce a self-refining loss between ht and ev... Minimizing the negative exponential of the similarity between the image and generated text representation pushes the representation closer"
  - [corpus] Weak evidence; no related work explicitly discusses this specific self-refining loss design.
- Break condition: If the visual encoder and LLM produce incompatible feature spaces, the similarity maximization may not lead to grounded reports or could even harm performance.

### Mechanism 2
- Claim: The combination of causal language modeling loss and self-refining loss achieves better performance than either alone.
- Mechanism: Causal language modeling ensures the report is coherent and fluent given the image, while self-refining loss grounds the report to the image content. Together, they balance generation quality and factual alignment.
- Core assumption: Both objectives are complementary and can be optimized jointly without interfering destructively.
- Evidence anchors:
  - [abstract] "alongside the standard Causal Language Modeling objective, to refine image-text representations"
  - [section 3.3] "We optimize our network with a weighted combination of both the causal language modeling objective and the self-refining objective"
  - [section 4.4] "combining the two losses yields much better performance... highlights that self-refining loss complements the report generation loss"
- Break condition: If the weighting between the two losses is poorly tuned, one objective may dominate and degrade overall performance.

### Mechanism 3
- Claim: Attention-based aggregation of generated report tokens outperforms other pooling strategies in creating the contextual representation used for self-refining.
- Mechanism: Attention weights from the LLM's last layer are used to weight token embeddings, creating a weighted sum that emphasizes contextually important tokens for alignment with the image.
- Core assumption: Attention weights effectively capture the relative importance of tokens for grounding the report in the image content.
- Evidence anchors:
  - [section 3.3] "we construct an aggregated representation ht âˆˆ Rdt of the predicted token embeddings by leveraging the attention weights from the last layer of T D"
  - [section 4.4] "attention-based aggregation outperforms other aggregation strategies by a significant margin"
  - [corpus] No direct evidence; the claim is based on the paper's own ablation study.
- Break condition: If attention weights are not reliable indicators of token importance for image alignment, the aggregation may not capture the most relevant content.

## Foundational Learning

- Concept: Multi-modal representation alignment
  - Why needed here: The model must map visual features from the X-ray into the same semantic space as the LLM's text embeddings to enable comparison and alignment.
  - Quick check question: How does the visual mapper project image features into the LLM's embedding space, and why is this projection necessary?

- Concept: Attention mechanisms in transformers
  - Why needed here: Attention weights are used to aggregate the generated report tokens into a single contextual representation for comparison with the image.
  - Quick check question: What role do the attention weights from the LLM's last layer play in forming the aggregated text representation?

- Concept: Gumbel-Softmax for differentiable sampling
  - Why needed here: Used to convert the LLM's logit distribution over vocabulary into a differentiable embedding for each generated token, enabling gradient flow for the self-refining loss.
  - Quick check question: Why is Gumbel-Softmax used instead of directly taking the argmax of the logits when constructing the token embeddings?

## Architecture Onboarding

- Component map: Visual encoder (Swin Transformer) -> Visual mapper (linear layer) -> LLM (LLaMA2-7B with LoRA) -> Cross-entropy loss + Self-refining loss
- Critical path: Image -> Visual encoder -> Visual mapper -> LLM input -> LLM generation -> Token embeddings -> Attention aggregation -> Self-refining loss computation -> Total loss -> Backpropagation
- Design tradeoffs: Freezing the visual encoder reduces training cost but may limit adaptation; using a frozen LLM with LoRA balances performance and efficiency but may constrain fine-tuning capacity.
- Failure signatures: Hallucinations in generated reports indicate poor alignment; low similarity between image and text representations despite self-refining loss suggests feature space mismatch; degraded performance on noisy images may indicate overfitting to clean data.
- First 3 experiments:
  1. Validate that the visual mapper successfully projects image features into the LLM embedding space by checking similarity distributions.
  2. Test the self-refining loss in isolation (no causal loss) to confirm it can improve alignment without harming fluency.
  3. Compare different aggregation strategies (average, max, attention-based) for the contextual text representation to confirm attention-based is superior.

## Open Questions the Paper Calls Out
None

## Limitations
- The method relies on similarity maximization between pooled image and text representations, which may not guarantee clinical accuracy despite improving BLEU scores.
- Performance is evaluated primarily using BLEU and CIDEr metrics, which have known weaknesses in capturing clinical accuracy and relevance.
- Freezing the visual encoder and using LoRA for LLM fine-tuning may limit the model's capacity to learn complex radiological reporting patterns and adapt to domain-specific features.

## Confidence

**High Confidence** (Strong evidence from ablation studies and multiple datasets):
- The combination of causal language modeling and self-refining loss yields better performance than either loss alone
- Attention-based aggregation of generated tokens outperforms other pooling strategies
- The overall approach achieves state-of-the-art performance on IU X-ray and ROCO datasets

**Medium Confidence** (Evidence primarily from the paper's own experiments):
- Self-refining loss effectively reduces hallucination in generated reports
- The model demonstrates robustness against noisy images
- The visual mapper successfully projects image features into the LLM embedding space

**Low Confidence** (Limited external validation or theoretical guarantees):
- Maximizing similarity between pooled representations ensures clinical accuracy
- The specific weighting of the two losses (0.1 for self-refining) is optimal
- Performance improvements generalize to diverse radiological findings beyond the test datasets

## Next Checks
1. **Clinical Expert Validation**: Have board-certified radiologists evaluate a sample of generated reports for clinical accuracy, completeness, and appropriateness of findings - comparing SERPENT-VLM outputs against both human-written reports and baseline model outputs.

2. **Cross-Institutional Testing**: Evaluate the model on radiology reports from different hospitals and imaging systems to assess generalizability beyond the IU X-ray and ROCO datasets, particularly for different X-ray equipment and protocols.

3. **Failure Mode Analysis**: Systematically identify and analyze cases where the model generates hallucinations or misses critical findings, categorizing failure types and determining whether the self-refining loss actually prevents specific types of errors.