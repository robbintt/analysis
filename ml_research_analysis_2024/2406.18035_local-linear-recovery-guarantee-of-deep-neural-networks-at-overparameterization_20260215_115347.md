---
ver: rpa2
title: Local Linear Recovery Guarantee of Deep Neural Networks at Overparameterization
arxiv_id: '2406.18035'
source_url: https://arxiv.org/abs/2406.18035
tags:
- function
- rank
- sample
- target
- recovery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a theoretical framework for analyzing the
  recovery capabilities of deep neural networks (DNNs) in overparameterized settings.
  The authors propose the concept of "local linear recovery" (LLR), a weaker form
  of target function recovery that is more amenable to theoretical analysis.
---

# Local Linear Recovery Guarantee of Deep Neural Networks at Overparameterization

## Quick Facts
- arXiv ID: 2406.18035
- Source URL: https://arxiv.org/abs/2406.18035
- Authors: Yaoyu Zhang; Leyang Zhang; Zhongwang Zhang; Zhiwei Bai
- Reference count: 6
- One-line primary result: Functions expressible by narrower DNNs can be recovered from fewer samples than parameters at overparameterization

## Executive Summary
This paper introduces a theoretical framework for analyzing the recovery capabilities of deep neural networks (DNNs) in overparameterized settings. The authors propose the concept of "local linear recovery" (LLR), a weaker form of target function recovery that is more amenable to theoretical analysis. They prove that functions expressible by narrower DNNs are guaranteed to be recoverable from fewer samples than model parameters. Specifically, they establish upper bounds on the optimistic sample sizes for functions in the space of a given DNN and show that these bounds are achieved for two-layer tanh neural networks. The main contributions include introducing the LLR-guarantee and its theoretical framework, deriving upper bounds for optimistic sample sizes using the Embedding Principle, and pinpointing exact optimistic sample sizes for two-layer tanh neural networks. The results provide a solid groundwork for future investigations into the recovery capabilities of DNNs in overparameterized scenarios.

## Method Summary
This paper presents a theoretical framework for analyzing the recovery capabilities of DNNs in overparameterized settings. The framework introduces the concept of "local linear recovery" (LLR) guarantee, which is a weaker form of target function recovery. The authors establish upper bounds on the optimistic sample sizes for functions in the space of a given DNN using the Embedding Principle. They also pinpoint the exact optimistic sample sizes for two-layer tanh neural networks by analyzing the linear independence of neurons and their tangent functions. The theoretical results are validated through experiments with synthetic datasets generated from target functions.

## Key Results
- Functions expressible by narrower DNNs are guaranteed to be recoverable from fewer samples than model parameters at overparameterization.
- Upper bounds on optimistic sample sizes for functions in the space of a given DNN are derived using the Embedding Principle.
- Exact optimistic sample sizes for two-layer tanh neural networks are pinpointed, which meet their respective upper bounds.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local linear recovery (LLR) guarantee ensures that functions expressible by narrower DNNs can be recovered from fewer samples than parameters at overparameterization.
- Mechanism: LLR relaxes global recovery to local recovery in the vicinity of an optimal point, leveraging the tangent function hyperplane. By analyzing model rank through embedding principles, narrower DNNs maintain recoverable expressiveness even when the sample size is less than the number of parameters.
- Core assumption: The target function lies within the model space of a narrower DNN, and the tangent function space is preserved under critical embeddings.
- Evidence anchors:
  - [abstract] "we prove that functions expressible by narrower DNNs are guaranteed to be recoverable from fewer samples than model parameters."
  - [section] "We prove that functions expressible by narrower DNNs are guaranteed to be recoverable from fewer samples than model parameters."
  - [corpus] Weak corpus match; no direct citations to embedding-based recovery guarantees.
- Break condition: If the embedding does not preserve the tangent function space, or if the target function lies outside the narrower DNN's function space, LLR guarantee fails.

### Mechanism 2
- Claim: Embedding Principle preserves model rank when mapping a narrower DNN to a wider one.
- Mechanism: Critical embeddings map parameters from narrower to wider DNNs while preserving both the output function and the criticality of the network. This allows bounding the optimistic sample size of a wide DNN by the model rank of the narrowest DNN expressing the target function.
- Core assumption: Critical embeddings exist between networks of different widths and maintain the model rank.
- Evidence anchors:
  - [section] "the critical embedding operators introduced in Refs. (Zhang et al., 2021b, 2022) can preserve the tangent function space and the model rank when transitioning from a parameter point in a narrower DNN to one in a wider DNN."
  - [corpus] No direct corpus citations; relies on referenced works.
- Break condition: If critical embeddings do not exist or do not preserve model rank, the optimistic sample size bounds become invalid.

### Mechanism 3
- Claim: For two-layer tanh neural networks, the optimistic sample size equals the model rank, which can be computed exactly.
- Mechanism: By analyzing the linear independence of neurons and their tangent functions, the model rank for two-layer tanh NNs can be calculated. This rank equals the optimistic sample size, providing a tight bound on recoverable sample complexity.
- Core assumption: The activation function (tanh) satisfies analyticity and derivative non-vanishing conditions, enabling precise rank computation.
- Evidence anchors:
  - [section] "We pinpoint the exact optimistic sample sizes for two-layer tanh neural networks, which meet their respective upper bounds."
  - [corpus] No corpus citations; relies on theoretical derivation.
- Break condition: If the activation function does not satisfy analyticity or derivative conditions, the exact rank computation fails.

## Foundational Learning

- Concept: Embedding Principle in DNNs
  - Why needed here: Provides the theoretical foundation for mapping narrower to wider networks while preserving recovery guarantees.
  - Quick check question: Can you explain how critical embeddings preserve both output function and model rank when transitioning between network widths?

- Concept: Model Rank and Tangent Function Space
  - Why needed here: Central to understanding the expressive capacity and recovery guarantees of DNNs.
  - Quick check question: How does the dimension of the span of tangent functions relate to the model rank and optimistic sample size?

- Concept: Local Linear Recovery (LLR) Guarantee
  - Why needed here: Offers a tractable form of recovery analysis for overparameterized DNNs.
  - Quick check question: What distinguishes LLR guarantee from global recovery, and why is it theoretically more accessible?

## Architecture Onboarding

- Component map: Two-layer tanh neural networks with hidden layer width m, input dimension d, and output dimension 1. Includes weight matrices W[1] ∈ R^(m×d), W[2] ∈ R^(1×m), and bias terms.
- Critical path: Compute model rank via neuron linear independence → Establish optimistic sample size bound → Validate via experimental fitting.
- Design tradeoffs: Narrower networks offer tighter recovery guarantees but may lack expressiveness; wider networks provide more capacity but increase optimistic sample size.
- Failure signatures: Recovery failure when sample size < model rank; poor generalization despite zero training error.
- First 3 experiments:
  1. Fit a simple linear target function with a width-1 two-layer tanh NN and measure recovery sample size.
  2. Compare recovery performance of CNNs with and without weight sharing for a target expressible by both.
  3. Vary hidden layer width and observe how optimistic sample size scales with network capacity.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can we identify an optimal training strategy and hyperparameters that enable recovery of a target function with a sample size approaching its optimistic estimate?
- Basis in paper: [explicit] The authors note that identifying an optimal training strategy and hyperparameters that enable recovery of a target function as close to its model rank as possible remains a significant challenge in the field.
- Why unresolved: The current theoretical framework provides upper bounds on optimistic sample sizes, but practical recovery often requires larger sample sizes due to suboptimal hyperparameter tuning and training dynamics.
- What evidence would resolve it: Empirical studies demonstrating recovery of target functions with sample sizes close to the theoretical optimistic estimates under various training strategies and hyperparameter settings.

### Open Question 2
- Question: Do DNNs offer stronger forms of recovery guarantees at overparameterization beyond the local linear recovery (LLR) guarantee?
- Basis in paper: [explicit] The authors suggest that having LLR-guarantee at overparameterization is a necessary condition for having any stronger type of recovery guarantee, but the existence of such stronger guarantees is still an open question.
- Why unresolved: The LLR guarantee is a weaker form of recovery that scales back expectations from global recovery to local recovery in the vicinity of an optimal point. Establishing stronger guarantees would require deeper understanding of DNN training dynamics and loss landscape properties.
- What evidence would resolve it: Theoretical proofs or empirical evidence demonstrating the existence of stronger recovery guarantees (e.g., local or global recovery guarantees) for DNNs at overparameterization.

### Open Question 3
- Question: How can we estimate the model rank for deeper neural networks beyond two-layer networks?
- Basis in paper: [inferred] The authors note that estimating the model rank for DNNs with three or more layers is challenging due to the complexity of disentangling linear dependencies among compositional tangent functions.
- Why unresolved: The current theoretical framework provides exact estimates of optimistic sample sizes for two-layer tanh neural networks by leveraging the linear independence of neurons. Extending this analysis to deeper networks requires new techniques to handle the increased complexity of the loss landscape.
- What evidence would resolve it: Development of new theoretical tools or empirical methods to estimate the model rank for deeper neural networks, potentially building on the concept of critical embeddings or other structural properties of the loss landscape.

## Limitations

- The theoretical framework relies heavily on the existence and properties of critical embedding operators, which are referenced from external works but not fully detailed in this paper.
- The exact computation of model rank for two-layer tanh networks assumes specific conditions on activation functions (analyticity, non-vanishing derivatives) that may not hold for all practical scenarios.
- While the framework provides upper bounds on optimistic sample sizes, these bounds are derived under idealized conditions that may not capture all real-world complexities.

## Confidence

- **High Confidence:** The mathematical definitions and framework for local linear recovery guarantee are rigorously established and internally consistent.
- **Medium Confidence:** The upper bounds on optimistic sample sizes for general DNNs, based on the Embedding Principle, are theoretically sound but rely on assumptions about critical embeddings.
- **Medium Confidence:** The exact computation of optimistic sample sizes for two-layer tanh networks is mathematically precise under the stated conditions, but practical applicability depends on meeting these conditions.

## Next Checks

1. **Empirical Validation of Embedding Principle:** Design experiments to empirically verify whether critical embeddings preserve model rank when mapping between networks of different widths. This could involve training target functions with both narrow and wide networks and comparing recovery performance.

2. **Robustness to Activation Functions:** Test the framework's applicability to activation functions beyond tanh, such as ReLU or sigmoid, to assess whether the assumptions about analyticity and derivative conditions are too restrictive.

3. **Sample Size Scaling in Practice:** Conduct experiments with varying sample sizes to observe the transition point where recovery fails, and compare this with the theoretical optimistic sample size bounds. This would validate the practical relevance of the theoretical guarantees.