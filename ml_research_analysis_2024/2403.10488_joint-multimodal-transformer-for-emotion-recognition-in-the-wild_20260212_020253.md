---
ver: rpa2
title: Joint Multimodal Transformer for Emotion Recognition in the Wild
arxiv_id: '2403.10488'
source_url: https://arxiv.org/abs/2403.10488
tags:
- fusion
- recognition
- proposed
- joint
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a joint multimodal transformer (JMT) for
  emotion recognition in the wild, focusing on integrating visual, audio, and physiological
  modalities. The proposed method uses separate backbones to capture intra-modal dependencies
  and a joint representation branch to enhance inter-modal relationships.
---

# Joint Multimodal Transformer for Emotion Recognition in the Wild

## Quick Facts
- arXiv ID: 2403.10488
- Source URL: https://arxiv.org/abs/2403.10488
- Reference count: 40
- Primary result: JMT achieves 0.458 average CCC on Affwild2 and 89.1% accuracy on Biovid

## Executive Summary
This paper introduces a Joint Multimodal Transformer (JMT) for emotion recognition in the wild, integrating visual, audio, and physiological modalities. The proposed method uses separate backbones to capture intra-modal dependencies and a joint representation branch to enhance inter-modal relationships. Extensive experiments on Affwild2 (valence-arousal estimation) and Biovid (pain estimation) demonstrate that JMT outperforms relevant baselines and state-of-the-art methods, achieving 0.458 average CCC on Affwild2 and 89.1% accuracy on Biovid.

## Method Summary
The JMT architecture processes visual, audio, and physiological modalities through separate backbone networks (R(2+1)D or I3D for visual, ResNet18+GRU for audio, 1D CNN for physiological). Each modality undergoes self-attention processing to capture intra-modal spatiotemporal dependencies. The model then applies cross-attention (6 layers with shared Q/K/V matrices) to fuse modality embeddings and capture inter-modal relationships. A novel joint representation branch concatenates visual and audio features, processes them through a fully connected layer to create 512-dimensional joint features, and feeds them back into the transformer to enhance robustness and performance. The system is trained with CCC loss for continuous emotion dimensions or standard classification loss for discrete pain intensity.

## Key Results
- Achieves 0.458 average CCC on Affwild2 dataset for valence-arousal estimation
- Reaches 89.1% accuracy on Biovid dataset for pain estimation
- Outperforms state-of-the-art methods on both benchmark datasets
- Joint representation branch significantly improves robustness to noisy inputs

## Why This Works (Mechanism)

### Mechanism 1: Joint Representation Branch
The joint representation branch captures both inter- and intra-modal dependencies more effectively than cross-attention alone by concatenating visual and audio features into a joint representation. This provides access to redundant and complementary information that cross-attention might miss, allowing the model to dynamically focus on combined information through shared Q/K/V matrices.

### Mechanism 2: Robustness to Noisy Inputs
The JMT fusion architecture improves robustness to noisy or missing modality inputs by providing additional context through the joint representation branch. This helps the model remain stable when individual modalities contain noise or are missing, particularly important in "in the wild" scenarios where data quality is variable.

### Mechanism 3: Hierarchical Fusion Approach
The hierarchical fusion approach (intra-modal → inter-modal → joint) creates a more complete feature representation by processing information through multiple stages. Separate backbones extract intra-modal spatiotemporal dependencies, the JMT layer captures inter-modal relationships through cross-attention, and the joint representation branch provides a third perspective that combines information from both modalities.

## Foundational Learning

- Concept: Cross-attention mechanisms in transformers
  - Why needed here: The JMT relies on cross-attention to capture inter-modal relationships between audio and visual features.
  - Quick check question: What is the difference between self-attention and cross-attention, and when would you use each?

- Concept: Concordance Correlation Coefficient (CCC) as an evaluation metric
  - Why needed here: The model is optimized to maximize CCC, which measures agreement between predictions and ground truth for continuous emotion dimensions.
  - Quick check question: How does CCC differ from simple correlation metrics, and why is it more appropriate for dimensional emotion recognition?

- Concept: Multimodal fusion strategies
  - Why needed here: Understanding different fusion approaches (early, late, hybrid) is crucial for appreciating why the JMT's joint representation approach is novel.
  - Quick check question: What are the tradeoffs between early fusion, late fusion, and hybrid fusion approaches in multimodal systems?

## Architecture Onboarding

- Component map: Backbone extraction -> Self-attention -> Cross-attention (6 layers) -> Joint representation processing -> Final self-attention -> Prediction
- Critical path: Backbone extraction → Self-attention → Cross-attention (6 layers) → Joint representation processing → Final self-attention → Prediction head
- Design tradeoffs:
  - Separate backbones preserve modality-specific features but increase model complexity
  - 6 cross-attention layers provide rich interactions but add computational cost
  - Joint representation adds redundancy but could introduce noise
  - Shared Q/K/V matrices promote consistency but may limit flexibility
- Failure signatures:
  - Poor performance on individual modalities suggests backbone issues
  - Inconsistent results across folds suggest overfitting or data quality problems
  - Large gap between valence and arousal performance suggests task-specific challenges
  - Degraded performance with added noise suggests insufficient robustness
- First 3 experiments:
  1. Ablation study: Remove the joint representation branch and compare performance to full JMT
  2. Robustness test: Add synthetic noise to one modality and measure performance degradation
  3. Backbone comparison: Replace R(2+1)D with I3D for visual features and measure impact on fusion performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the JMT architecture perform when extended to more than two modalities, such as adding physiological signals to the Affwild2 dataset?
- Basis in paper: The paper focuses on two-modality fusion but mentions "our future work includes introducing more modalities."
- Why unresolved: The paper only tests the JMT with two modalities and does not explore multi-modality extensions.
- What evidence would resolve it: Experiments comparing JMT performance on Affwild2 with three modalities (face, voice, and physiological signals) against the current two-modality setup.

### Open Question 2
- Question: What is the impact of the joint representation branch on the model's robustness to missing or corrupted data in individual modalities?
- Basis in paper: The paper states that "the proposed method becomes more robust to noise or irrelevant information present in individual sequences."
- Why unresolved: While the paper claims improved robustness, it does not provide quantitative comparisons of performance degradation when individual modalities are missing or corrupted.
- What evidence would resolve it: Controlled experiments showing JMT performance with varying levels of noise or missing data in individual modalities, compared to baseline methods.

### Open Question 3
- Question: How does the JMT's performance scale with the size of the training dataset, especially for smaller datasets?
- Basis in paper: The paper uses two specific datasets but does not analyze how performance changes with dataset size or discuss the model's data efficiency.
- Why unresolved: The paper does not explore how the JMT's performance varies with different dataset sizes or its effectiveness on smaller datasets.
- What evidence would resolve it: Experiments training JMT on subsets of varying sizes from the original datasets and measuring performance changes.

## Limitations

- The robustness claims regarding noisy inputs lack comprehensive ablation studies under controlled noise conditions
- The architectural complexity may limit real-time deployment due to computational overhead
- The joint representation mechanism's benefits are primarily demonstrated through end-to-end performance rather than clear attribution of gains to specific components

## Confidence

- **High confidence**: The hierarchical fusion approach and overall JMT architecture are well-grounded in established multimodal learning principles. Experimental results on Affwild2 and Biovid are reproducible given dataset availability.
- **Medium confidence**: The specific claim that joint representation captures unique information not accessible through cross-attention alone needs further validation. Robustness improvements are supported by qualitative reasoning but lack quantitative ablation studies.
- **Low confidence**: Claims about joint representation's ability to mitigate cross-attention sensitivity to noise are based on indirect evidence and require more rigorous testing.

## Next Checks

1. Conduct systematic ablation studies removing the joint representation branch under various noise conditions to quantify its contribution to robustness
2. Perform cross-dataset generalization tests to evaluate whether performance gains transfer to unseen emotion recognition scenarios beyond Affwild2 and Biovid
3. Implement computational efficiency analysis comparing JMT inference time and memory usage against simpler fusion baselines to assess real-world deployment feasibility