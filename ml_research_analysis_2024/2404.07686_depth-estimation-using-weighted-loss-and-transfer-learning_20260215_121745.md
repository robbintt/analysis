---
ver: rpa2
title: Depth Estimation using Weighted-loss and Transfer Learning
arxiv_id: '2404.07686'
source_url: https://arxiv.org/abs/2404.07686
tags:
- depth
- loss
- estimation
- used
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses depth estimation from single 2D images, a
  key task in computer vision with applications in autonomous vehicles, scene understanding,
  and robotics. The authors propose a simplified approach combining transfer learning
  and an optimized weighted loss function to improve depth estimation accuracy.
---

# Depth Estimation using Weighted-loss and Transfer Learning

## Quick Facts
- arXiv ID: 2404.07686
- Source URL: https://arxiv.org/abs/2404.07686
- Authors: Muhammad Adeel Hafeez; Michael G. Madden; Ganesh Sistu; Ihsan Ullah
- Reference count: 4
- One-line primary result: EfficientNet with optimized weighted loss achieves best depth estimation performance (RMSE 0.386, REL 0.113, log10 0.049) on NYU Depth Dataset v2

## Executive Summary
This paper presents a simplified approach to depth estimation from single 2D images using transfer learning and an optimized weighted loss function. The authors propose combining Mean Absolute Error (MAE), Edge Loss, and Structural Similarity Index (SSIM) with weights determined through grid and random search. Multiple encoder-decoder models including DenseNet121, DenseNet169, DenseNet201, and EfficientNet are evaluated using the NYU Depth Dataset v2. The results show that EfficientNet, pre-trained on ImageNet and combined with the optimized loss function, achieves superior performance with RMSE of 0.386, REL of 0.113, and log10 error of 0.049. Qualitative analysis demonstrates that the model can even correct some errors in the ground truth depth maps.

## Method Summary
The method combines transfer learning with an optimized weighted loss function for depth estimation. The encoder uses pre-trained DenseNet or EfficientNet models from ImageNet classification, while a simple decoder with skip connections reconstructs depth maps through upsampling layers. The loss function combines MAE, Edge Loss, and SSIM with weights (0.6, 0.2, 1.0) optimized via grid and random search. The model is trained on NYU Depth Dataset v2 with RGB images resized to 320×240, using Adam optimizer with learning rate 0.0001 and early stopping around 23 epochs.

## Key Results
- EfficientNet encoder with optimized weighted loss (0.6·MAE + 0.2·EdgeLoss + 1·SSIM) achieves best performance: RMSE 0.386, REL 0.113, log10 0.049
- DenseNet169 with optimized loss achieves RMSE 0.408, REL 0.119, log10 0.051
- The model successfully corrects some ground truth errors in NYU Depth Dataset v2 through qualitative analysis
- Combined weighted loss consistently outperforms single-loss alternatives across all tested architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The optimized loss function improves depth estimation accuracy by balancing multiple loss components (MAE, Edge Loss, SSIM) with learned weights.
- Mechanism: Each loss component targets a different aspect of depth estimation quality: MAE ensures pixel-wise accuracy, Edge Loss preserves depth discontinuities and object boundaries, and SSIM maintains structural similarity. By optimizing their relative weights through grid and random search, the combined loss better captures both global and local depth features.
- Core assumption: The three loss components are sufficiently independent and complementary that their weighted combination provides better performance than any single loss alone.
- Evidence anchors:
  - [abstract] The optimized loss function is a combination of weighted losses to enhance robustness and generalization: Mean Absolute Error (MAE), Edge Loss and Structural Similarity Index (SSIM).
  - [section 3.2] We explored that fine-tuning of the weights of the loss function is crucial and it directly affects the model's behaviour for the task of depth estimation.

### Mechanism 2
- Claim: Transfer learning from ImageNet-pretrained models provides a strong feature representation that accelerates convergence and improves final accuracy for depth estimation.
- Mechanism: Models pre-trained on ImageNet have learned rich hierarchical visual features that are transferable to depth estimation tasks. These features capture edge, texture, and shape information that is useful for predicting depth from RGB images. The pre-training acts as a regularizer and provides better initialization than training from scratch.
- Core assumption: Visual features learned for classification tasks are relevant and useful for depth estimation from single images.
- Evidence anchors:
  - [abstract] The EfficientNet model, pre-trained on ImageNet for classification when used as an encoder, with a simple upsampling decoder, gives the best results.
  - [section 3.3] For the Encoder part, we used four different models: DenseNet121, DenseNet169, DenseNet201 and EfficientNet. All these models were pre-trained on ImageNet for classification tasks.

### Mechanism 3
- Claim: The decoder architecture with skip connections preserves spatial information while maintaining global context for accurate depth map reconstruction.
- Mechanism: Skip connections from the encoder to the decoder allow high-resolution features from early layers to be combined with deeper semantic features. This preserves fine-grained spatial details (edges, small objects) while the deeper layers provide global context and understanding of the scene structure. The combination enables both accurate local depth estimation and coherent global depth predictions.
- Core assumption: The encoder-decoder architecture with skip connections is well-suited for mapping from high-dimensional feature space back to dense depth maps while preserving spatial information.
- Evidence anchors:
  - [section 3.3] We implemented a simple encoder-decoder-based network with skip connections.
  - [figure 1] The figure shows the encoder-decoder architecture with skip connections.

## Foundational Learning

- Concept: Transfer learning and pre-trained models
  - Why needed here: The paper relies heavily on using pre-trained models (DenseNet, EfficientNet) fine-tuned for depth estimation rather than training from scratch.
  - Quick check question: What is the main benefit of using pre-trained models for depth estimation compared to training from scratch?

- Concept: Loss function optimization and weighted combinations
  - Why needed here: The paper proposes an optimized loss function combining MAE, Edge Loss, and SSIM with specific weights determined through grid and random search.
  - Quick check question: Why might a weighted combination of different loss functions perform better than using a single loss function for depth estimation?

- Concept: Encoder-decoder architectures with skip connections
  - Why needed here: The proposed method uses an encoder-decoder architecture where the encoder extracts features and the decoder reconstructs depth maps, with skip connections preserving spatial information.
  - Quick check question: What is the purpose of skip connections in encoder-decoder architectures for dense prediction tasks like depth estimation?

## Architecture Onboarding

- Component map: RGB image (320×240) → Encoder (DenseNet/EfficientNet) → Skip connections → Decoder (upsampling) → Depth map (1/2× resolution) → Combined loss (0.6·MAE + 0.2·EdgeLoss + 1·SSIM) → Backpropagation

- Critical path: RGB image → Encoder feature extraction → Skip connections → Decoder upsampling → Depth map output → Combined loss calculation → Backpropagation for weight updates

- Design tradeoffs:
  - Simple decoder vs. complex decoder: Simple decoder reduces parameters and training time but may limit representational capacity
  - Pre-trained vs. from-scratch: Pre-trained provides better initialization and faster convergence but may introduce domain mismatch
  - Combined loss vs. single loss: Combined loss captures multiple aspects of depth quality but requires careful weight tuning

- Failure signatures:
  - Poor edge preservation: Indicates Edge Loss weight too low or Edge Loss not effective
  - Blurry depth maps: Suggests decoder is too simple or skip connections not properly implemented
  - Slow convergence: May indicate inappropriate learning rate or initialization issues
  - Overfitting: Could result from insufficient regularization or too complex decoder

- First 3 experiments:
  1. Train DenseNet121 with equal weights (1.0 for all losses) to establish baseline performance
  2. Implement grid search for loss weights using a small subset of the dataset to find optimal combination
  3. Compare DenseNet169 vs. EfficientNet encoders with optimized loss to determine best architecture

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed weighted loss function parameters (0.6 for MAE, 0.2 for Edge Loss, and 1 for SSIM) generalize to other depth estimation datasets beyond NYU Depth Dataset v2?
- Basis in paper: [explicit] The paper states these weights "consistently outperform other combinations" but only tests on NYU2 dataset
- Why unresolved: The authors only tested on one dataset and didn't explore cross-dataset validation
- What evidence would resolve it: Testing the same weight configuration on multiple depth estimation datasets (KITTI, Make3D, etc.) and comparing performance consistency

### Open Question 2
- Question: What is the computational overhead of using EfficientNet compared to other encoder architectures when running in real-time applications like autonomous vehicles?
- Basis in paper: [inferred] The paper mentions limitations for safety-critical applications but doesn't provide detailed computational analysis
- Why unresolved: The paper focuses on accuracy metrics but doesn't discuss inference time or resource requirements
- What evidence would resolve it: Benchmarking inference speed (FPS), memory usage, and power consumption across different architectures under identical hardware conditions

### Open Question 3
- Question: How does the model's performance change when trained on lower resolution images or when deployed on devices with limited computational resources?
- Basis in paper: [explicit] The authors reduced image resolution from 640×480 to 320×240 for training, but don't explore further downsampling
- Why unresolved: The study only examines one downsampling level and doesn't address mobile/edge deployment scenarios
- What evidence would resolve it: Systematic evaluation of model performance across multiple resolution levels and analysis of trade-offs between accuracy and computational efficiency

## Limitations
- Lack of detailed decoder architecture specifications makes complete reproduction difficult
- Single dataset evaluation (NYU Depth Dataset v2) limits generalizability to other scenarios
- No computational efficiency analysis for real-time or resource-constrained deployment
- Limited exploration of alternative loss weight configurations beyond the optimized combination

## Confidence

- **High Confidence**: The overall methodology of combining transfer learning with an optimized weighted loss function is well-supported by both the experimental results and related literature. The superior performance of EfficientNet is convincingly demonstrated.
- **Medium Confidence**: The specific mechanism by which the weighted loss components interact to improve depth estimation requires further validation. While the optimization process is described, the theoretical justification for the particular weight combination could be stronger.
- **Medium Confidence**: The architectural claims regarding skip connections and their role in preserving spatial information are reasonable but lack detailed implementation specifics that would allow for complete reproduction.

## Next Checks

1. **Decoder Architecture Validation**: Implement multiple decoder variations (different numbers of upsampling layers, attention mechanisms, or residual connections) to determine whether the simple decoder is indeed optimal or if more complex architectures could yield further improvements.

2. **Cross-Dataset Generalization**: Evaluate the trained models on alternative depth estimation datasets (such as KITTI or Make3D) to assess whether the transfer learning benefits generalize beyond NYU Depth Dataset v2.

3. **Loss Component Analysis**: Conduct ablation studies systematically removing each loss component (MAE, Edge Loss, SSIM) to quantify their individual contributions and validate that the weighted combination provides statistically significant improvements over single-loss alternatives.