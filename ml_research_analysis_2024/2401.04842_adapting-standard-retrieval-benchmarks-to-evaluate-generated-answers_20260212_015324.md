---
ver: rpa2
title: Adapting Standard Retrieval Benchmarks to Evaluate Generated Answers
arxiv_id: '2401.04842'
source_url: https://arxiv.org/abs/2401.04842
tags:
- generated
- answers
- retrieval
- arxiv
- similarity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evaluating the quality of
  answers generated by large language models (LLMs) for question-answering tasks,
  particularly in the absence of established evaluation methods. The authors propose
  and empirically validate an evaluation framework that leverages existing retrieval
  benchmarks to assess generated answers, allowing for comparison of different LLMs
  and prompts under a common framework.
---

# Adapting Standard Retrieval Benchmarks to Evaluate Generated Answers

## Quick Facts
- arXiv ID: 2401.04842
- Source URL: https://arxiv.org/abs/2401.04842
- Reference count: 40
- One-line primary result: The framework demonstrates that information retrieval benchmarks serve as reliable anchors for assessing generated answer quality without explicit relevance judgments.

## Executive Summary
This paper addresses the challenge of evaluating generated answers from large language models (LLMs) for question-answering tasks. The authors propose a framework that leverages existing retrieval benchmarks to assess answer quality by measuring similarity between generated answers and either relevance-judged passages or top-retrieved passages. The method allows for standardized evaluation of different LLMs and prompts under a common framework, filling a critical gap in the current evaluation landscape.

The framework demonstrates strong empirical validity, showing that generative models like GPT-4 and LLaMA2 perform comparably to the best retrieval-based runs when measured by similarity to relevance-judged documents. The approach achieves high correlation with traditional evaluation metrics like nDCG, establishing that retrieval benchmarks can serve as reliable anchors for assessing generated answer quality even without explicit relevance judgments.

## Method Summary
The authors propose an evaluation framework that measures the similarity between generated answers and passages from existing retrieval benchmarks using various embedding methods. The framework includes two main approaches: comparing generated answers to judged relevant passages from retrieval benchmarks, and comparing generated answers to top-retrieved passages from diverse retrieval models. The similarity scores serve as indicators of answer quality, allowing for standardized evaluation of different LLMs and prompts. The method leverages the extensive work in retrieval evaluation to provide a practical solution for assessing generated answers in the absence of established evaluation methods.

## Key Results
- Generative models (gpt-4, gpt-3.5-turbo, LLaMA2-7b-chat, LLaMA2-13b-chat) perform comparably to best retrieval-based runs on TREC Deep Learning 2019/2020 datasets
- High correlation between embedding similarity and traditional evaluation metrics like nDCG
- Retrieval benchmarks serve as reliable anchors for assessing generated answer quality without explicit relevance judgments
- Consistent results across different retrieval methods and embedding approaches

## Why This Works (Mechanism)
The framework works by leveraging the established relationship between retrieved documents and answer quality in traditional information retrieval. By measuring the similarity between generated answers and either judged relevant passages or top-retrieved passages from diverse retrieval models, the framework captures the essential connection between retrieval effectiveness and answer quality. The use of embedding similarity as a proxy for semantic alignment allows for scalable and automated evaluation that correlates well with human judgments encoded in traditional metrics.

## Foundational Learning

1. **Retrieval Benchmark Structure** - Understanding how retrieval benchmarks organize queries, documents, and relevance judgments
   - Why needed: Essential for mapping generated answers to benchmark components
   - Quick check: Can identify query-document pairs and relevance levels in a benchmark

2. **Embedding Similarity Metrics** - Knowledge of cosine similarity and other vector-based comparison methods
   - Why needed: Core mechanism for comparing generated answers to benchmark passages
   - Quick check: Can compute similarity between two text passages using embeddings

3. **nDCG and Traditional IR Metrics** - Understanding normalized discounted cumulative gain and related metrics
   - Why needed: Provides baseline for validating the proposed evaluation approach
   - Quick check: Can explain how nDCG measures retrieval effectiveness

## Architecture Onboarding

Component map: Benchmark Collection -> Query Generation -> Answer Generation -> Similarity Computation -> Quality Assessment

Critical path: The framework requires access to established retrieval benchmarks, generation of queries or use of existing ones, production of answers by LLMs, computation of similarity scores using embeddings, and aggregation of results for quality assessment.

Design tradeoffs: The method balances between using explicit relevance judgments (more accurate but limited availability) versus top-retrieved passages from diverse models (broader coverage but potentially noisier signals). The choice of embedding method affects both computational efficiency and accuracy of similarity measurements.

Failure signatures: Poor performance may indicate mismatch between benchmark domain and generated answers, inadequate embedding methods for the domain, or queries that are too ambiguous for reliable assessment. Low correlation with traditional metrics suggests the framework may not capture aspects of answer quality that retrieval benchmarks don't measure well.

First experiments:
1. Run the framework on a small subset of TREC Deep Learning queries with multiple LLMs to verify basic functionality
2. Compare results using different embedding methods (e.g., SBERT vs. OpenAI embeddings) on the same queries
3. Validate the correlation between similarity scores and nDCG on a held-out test set

## Open Questions the Paper Calls Out
None

## Limitations
- Embedding similarity may not fully capture semantic nuances or factual accuracy for complex queries
- Focus on specific datasets (TREC Deep Learning 2019/2020) raises questions about generalizability
- Correlation with traditional metrics doesn't guarantee perfect alignment in all cases

## Confidence
- Core methodology: High confidence given consistent performance across multiple models and strong correlation with established metrics
- Framework applicability to diverse scenarios: Medium confidence due to limited scope of datasets and models tested
- Assumption about retrieval benchmarks as reliable anchors: Medium confidence, as this relationship may vary with benchmark characteristics

## Next Checks
1. Test the framework across diverse benchmark collections beyond TREC Deep Learning to assess domain generalizability and identify potential limitations with different query types.
2. Conduct ablation studies comparing embedding-based similarity with human judgments on generated answers to quantify the correlation and identify edge cases where the method may fail.
3. Evaluate the framework's sensitivity to different embedding methods and model configurations to establish robustness and identify optimal parameter settings for various use cases.