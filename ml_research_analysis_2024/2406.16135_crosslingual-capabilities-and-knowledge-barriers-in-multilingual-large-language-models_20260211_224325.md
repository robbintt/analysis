---
ver: rpa2
title: Crosslingual Capabilities and Knowledge Barriers in Multilingual Large Language
  Models
arxiv_id: '2406.16135'
source_url: https://arxiv.org/abs/2406.16135
tags:
- language
- knowledge
- crosslingual
- llms
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Multilingual LLMs like Llama and Mistral show strong surface-level
  crosslingual abilities, performing nearly as well as dedicated translation models
  on machine translation tasks. However, they exhibit a significant crosslingual knowledge
  barrier: while they can translate or embed mixed-language text well, they struggle
  to transfer learned knowledge across languages in question-answering tasks.'
---

# Crosslingual Capabilities and Knowledge Barriers in Multilingual Large Language Models

## Quick Facts
- arXiv ID: 2406.16135
- Source URL: https://arxiv.org/abs/2406.16135
- Reference count: 40
- Key outcome: Multilingual LLMs like Llama and Mistral show strong surface-level crosslingual abilities, performing nearly as well as dedicated translation models on machine translation tasks. However, they exhibit a significant crosslingual knowledge barrier: while they can translate or embed mixed-language text well, they struggle to transfer learned knowledge across languages in question-answering tasks.

## Executive Summary
This paper investigates the crosslingual capabilities of multilingual large language models (LLMs) like Llama and Mistral. While these models demonstrate strong surface-level abilities in translation and embedding mixed-language text, they face a significant knowledge barrier when transferring learned information across languages. This barrier persists across various tasks and benchmarks, including general knowledge (MMLU) and domain-specific knowledge (Harry Potter quiz, TOFU benchmark).

The study reveals that simple inference-time mitigation methods like prompt engineering or few-shot demonstrations provide limited improvement. However, fine-tuning these models on mixed-language data effectively reduces this gap, improving crosslingual task performance and even boosting English performance. These findings highlight the need for explicit crosslingual optimization to unlock the full potential of multilingual LLMs.

## Method Summary
The researchers evaluated 15 multilingual LLM models across 16 languages using various benchmarks, including MMLU for general knowledge and domain-specific tests like the Harry Potter quiz and TOFU benchmark. They assessed the models' performance in both monolingual and crosslingual settings, comparing translation quality, embedding capabilities, and knowledge transfer abilities. The study also explored the effectiveness of different mitigation strategies, such as prompt engineering, few-shot demonstrations, and fine-tuning on mixed-language data.

## Key Results
- Multilingual LLMs exhibit strong surface-level crosslingual abilities, performing nearly as well as dedicated translation models on machine translation tasks.
- A significant crosslingual knowledge barrier exists, with models struggling to transfer learned knowledge across languages in question-answering tasks.
- Simple inference-time mitigation methods like prompt engineering or few-shot demonstrations provide limited improvement in addressing the crosslingual knowledge gap.
- Fine-tuning on mixed-language data effectively reduces the crosslingual knowledge gap, improving performance on crosslingual tasks and boosting English performance as well.

## Why This Works (Mechanism)
The paper does not explicitly discuss the underlying mechanisms that explain why multilingual LLMs exhibit strong surface-level crosslingual abilities but struggle with deeper knowledge transfer. This could be due to the models' architecture, training data composition, or optimization objectives that prioritize surface-level alignment across languages without explicitly encouraging knowledge transfer.

## Foundational Learning
- Crosslingual embeddings: why needed - to represent meaning across languages in a shared space; quick check - similarity between embeddings of parallel sentences
- Knowledge transfer: why needed - to leverage learned information across languages; quick check - performance on crosslingual QA tasks
- Multilingual training data: why needed - to expose models to diverse linguistic patterns; quick check - language coverage and quality of training corpora

## Architecture Onboarding

### Component Map
- Multilingual LLM (e.g., Llama, Mistral) -> Translation tasks -> Crosslingual QA tasks -> Knowledge transfer evaluation -> Mitigation strategies (prompt engineering, fine-tuning)

### Critical Path
1. Model training on multilingual data
2. Evaluation on translation and crosslingual QA tasks
3. Identification of crosslingual knowledge barrier
4. Testing of mitigation strategies (prompt engineering, fine-tuning)
5. Analysis of fine-tuning effectiveness on crosslingual performance

### Design Tradeoffs
- Balancing model size and multilingual coverage
- Prioritizing surface-level alignment vs. deep knowledge transfer
- Computational resources for fine-tuning vs. inference-time methods

### Failure Signatures
- Strong performance on translation but poor crosslingual QA results
- Limited improvement from simple inference-time mitigation methods
- Persistent knowledge gaps across diverse benchmarks and languages

### 3 First Experiments
1. Evaluate crosslingual QA performance across diverse knowledge domains and task types
2. Test fine-tuning effectiveness on models with different architectures and training objectives
3. Investigate the impact of fine-tuning on crosslingual performance for low-resource languages

## Open Questions the Paper Calls Out
None

## Limitations
- The study focuses primarily on translation quality and QA performance, potentially missing other crosslingual capabilities or task types where the barrier may manifest differently.
- The sample of 15 models, while diverse, may not represent the full spectrum of multilingual LLM architectures and training approaches.
- The out-of-domain fine-tuning approach, though effective, may not generalize to all knowledge domains or model types.

## Confidence
- Strong surface-level crosslingual abilities (translation, embeddings) while struggling with deeper knowledge transfer: High
- Simple inference-time mitigation methods are ineffective: High
- Fine-tuning on mixed-language data universally reduces crosslingual gaps: Medium

## Next Checks
1. Evaluate the crosslingual knowledge barrier across additional task types (e.g., summarization, reasoning) and knowledge domains to determine if the phenomenon generalizes beyond translation and QA.
2. Test the effectiveness of fine-tuning on mixed-language data for models with different architectures (e.g., encoder-decoder, decoder-only) and training objectives to assess the approach's broad applicability.
3. Investigate the impact of fine-tuning on crosslingual performance for low-resource languages, which may exhibit different knowledge transfer patterns due to limited training data.