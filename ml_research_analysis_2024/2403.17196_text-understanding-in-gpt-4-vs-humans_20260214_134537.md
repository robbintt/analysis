---
ver: rpa2
title: Text Understanding in GPT-4 vs Humans
arxiv_id: '2403.17196'
source_url: https://arxiv.org/abs/2403.17196
tags:
- gpt-4
- test
- questions
- price
- story
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study compares the text understanding capabilities of GPT-4
  and humans using standardized tests of discourse comprehension. GPT-4 performed
  slightly better than humans on simple passages from the Discourse Comprehension
  Test, matching human performance overall and excelling in three of four experimental
  conditions.
---

# Text Understanding in GPT-4 vs Humans

## Quick Facts
- arXiv ID: 2403.17196
- Source URL: https://arxiv.org/abs/2403.17196
- Reference count: 0
- GPT-4 outperforms humans on complex text comprehension tests, achieving 96.3% percentile scores

## Executive Summary
This study compares GPT-4's text understanding capabilities against human performance using standardized discourse comprehension tests. The research finds that GPT-4 matches human-level comprehension on simple passages while significantly outperforming humans on more difficult academic reading comprehension tests. The results suggest GPT-4 demonstrates genuine understanding through its ability to make correct inferences about unstated information and provide concise, accurate justifications for its answers.

## Method Summary
The study employs a direct comparison approach using standardized tests of discourse comprehension. GPT-4 was tested on the Discourse Comprehension Test (12 stories with 8 yes/no questions each) and more difficult passages from SAT, GRE, and LSAT tests. Performance was measured by proportion of correct answers, with GPT-4 using zero-shot learning (no training on test materials). Human performance data was obtained from published test statistics for comparison.

## Key Results
- GPT-4 matches human performance on simple 5th-6th grade reading level passages
- On difficult academic passages, GPT-4 achieves 96.3% percentile scores versus human 50th percentile average
- GPT-4 provides more concise justifications than test-makers while maintaining accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 matches human-level comprehension on simple text because both use similar inference processes.
- Mechanism: GPT-4 constructs multi-level representations of text like humans do, drawing correct inferences about unstated information.
- Core assumption: Text comprehension fundamentally relies on building representations and making inferences, not just pattern matching.
- Evidence anchors: Both GPT-4 and humans make correct inferences about information not explicitly stated in the text, a critical test of understanding.

### Mechanism 2
- Claim: GPT-4 outperforms humans on difficult text because it can handle more complex inference patterns.
- Mechanism: When text complexity increases, GPT-4's pattern recognition and generalization capabilities exceed human working memory limits.
- Core assumption: Human comprehension bottlenecks occur at working memory capacity when processing complex passages.
- Evidence anchors: GPT-4 significantly outperformed human test-takers on difficult passages, achieving 96.3% percentile scores.

### Mechanism 3
- Claim: GPT-4's text understanding appears human-like because it provides appropriate justifications for answers.
- Mechanism: GPT-4 generates concise, accurate explanations that demonstrate reasoning rather than simple retrieval.
- Core assumption: The quality and conciseness of justifications reflects genuine understanding rather than pattern matching.
- Evidence anchors: GPT-4's explanations were far more concise than test-makers' while being just as informative and easier to understand.

## Foundational Learning

- Concept: Discourse comprehension theory
  - Why needed here: The study uses established psychological frameworks for measuring text understanding
  - Quick check question: What are the two key dimensions manipulated in the Discourse Comprehension Test?

- Concept: Inference vs. stated information
  - Why needed here: The experiments deliberately separate direct from implied knowledge to test true understanding
  - Quick check question: How do implied questions differ from stated questions in measuring comprehension?

- Concept: Generalization in text understanding
  - Why needed here: The study identifies generalization as a signature of genuine understanding
  - Quick check question: Why is the ability to generalize considered evidence of understanding rather than memorization?

## Architecture Onboarding

- Component map: Text input -> Transformer layers -> Attention mechanisms -> Output generation -> Inference extraction
- Critical path: Token processing -> context window management -> attention pattern recognition -> answer generation
- Design tradeoffs: Model size vs. inference speed vs. accuracy on complex passages
- Failure signatures: Hedging responses, incorrect implied information answers, verbose justifications
- First 3 experiments:
  1. Replicate simple passage comprehension test with modified prompts to test prompt sensitivity
  2. Test GPT-4 on increasingly complex passages to map the human-machine performance crossover point
  3. Compare GPT-4's inference generation to human-written explanations on the same passages

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would GPT-4's performance compare to humans on discourse comprehension tests if the test passages were designed at different grade reading levels?
- Basis in paper: The paper notes that both GPT-4 and humans performed at very high levels on 5th-6th grade passages, leaving little room for GPT-4 to statistically exceed human performance.
- Why unresolved: The study only tested GPT-4 on passages at a single difficulty level (5th-6th grade) and found no statistically significant difference from human performance.
- What evidence would resolve it: Testing GPT-4 on multiple versions of the Discourse Comprehension Test with passages at various grade reading levels and comparing its performance to humans on each version.

### Open Question 2
- Question: Does GPT-4's tendency to provide more concise justifications for correct answers compared to test-makers' explanations contribute to its superior performance on difficult reading comprehension tests?
- Basis in paper: The paper notes that GPT-4's explanations were "far more concise" than test-makers' explanations while being "just as informative and easier to understand."
- Why unresolved: While the paper observes GPT-4's more concise explanations, it doesn't experimentally test whether this conciseness directly contributes to better comprehension or test performance.
- What evidence would resolve it: Conducting an experiment where human test-takers are given either GPT-4's concise explanations or the original test-maker explanations for LSAT questions, then comparing their subsequent performance.

### Open Question 3
- Question: Would prompting GPT-4 to provide more inferences when summarizing passages lead to better performance on comprehension tests?
- Basis in paper: The paper found that when GPT-4 was prompted to summarize stories "mentioning main ideas that are not stated and must be inferred," it produced many additional inferences.
- Why unresolved: The paper observed GPT-4's inference generation capabilities but didn't test whether actively prompting for inferences during test-taking would improve performance on the comprehension questions themselves.
- What evidence would resolve it: Testing GPT-4 on comprehension questions with and without explicit prompts to generate inferences during the reading process.

## Limitations
- Narrow focus on text comprehension while excluding other aspects of understanding such as causal reasoning, planning, and physical world interaction
- Reliance on standardized test formats may not capture full complexity of real-world text understanding
- Difficulty distinguishing genuine understanding from sophisticated pattern matching in LLM behavior

## Confidence
- High Confidence: GPT-4's superior performance on difficult academic passages (SAT, GRE, LSAT) compared to human test-takers
- Medium Confidence: GPT-4's equivalence to human performance on simple passages
- Low Confidence: The claim that GPT-4 demonstrates "genuine understanding" through generalization and inference

## Next Checks
1. Systematically vary the prompts provided to GPT-4 across all test conditions to determine how sensitive its performance is to prompt engineering versus actual comprehension capabilities
2. Test multiple LLM variants (including smaller models) on the same passages to establish whether GPT-4's performance represents a qualitative leap or quantitative scaling
3. Evaluate GPT-4's text understanding in applied contexts such as technical documentation comprehension, legal document analysis, or medical literature interpretation to assess whether standardized test performance translates to practical understanding