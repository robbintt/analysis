---
ver: rpa2
title: 'BUSTER: a "BUSiness Transaction Entity Recognition" dataset'
arxiv_id: '2402.09916'
source_url: https://arxiv.org/abs/2402.09916
tags:
- documents
- company
- buster
- dataset
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces BUSTER, a dataset for business transaction
  entity recognition. The dataset contains 3779 manually annotated documents and an
  additional 6196 automatically annotated documents.
---

# BUSTER: a "BUSiness Transaction Entity Recognition" dataset

## Quick Facts
- arXiv ID: 2402.09916
- Source URL: https://arxiv.org/abs/2402.09916
- Reference count: 6
- Primary result: RoBERTa achieves 72.34% micro F1 score on business transaction entity recognition

## Executive Summary
BUSTER is a novel dataset for business transaction entity recognition containing 3,779 manually annotated documents and 6,196 automatically annotated documents from SEC Form 8K reports. The dataset focuses on identifying entities involved in financial transactions, including companies, advisors, and economic entities. Several transformer-based models were evaluated, with RoBERTa achieving the best performance at 72.34% micro F1 score. The dataset aims to support industry-oriented research in financial NLP and is publicly available for the research community.

## Method Summary
The BUSTER dataset was created by collecting SEC Form 8K documents with Exhibit 99.1 disclosures, which are financial reports submitted to the SEC. Documents were split into contiguous chunks to handle long sequences, and expert annotators labeled entities related to business transactions using the expert.ai platform. The dataset includes both a manually annotated "Gold" corpus (3,779 documents) and an automatically annotated "Silver" corpus (6,196 documents). Five transformer-based models (BERT, RoBERTa, SEC-BERT, and Longformer) were evaluated using 5-fold cross-validation to establish baseline performance metrics.

## Key Results
- RoBERTa achieved the highest micro F1 score of 72.34% on the BUSTER dataset
- SEC-BERT, a domain-specific model pre-trained on financial documents, showed improved performance over generic BERT
- The dataset demonstrates the challenges of entity recognition in long financial documents, with most documents exceeding 500 words
- Cohen's kappa score of 0.7402 indicates substantial inter-annotator agreement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Domain-specific pretraining improves entity recognition in financial texts.
- Mechanism: The SEC-BERT model, pretrained on EDGAR-CORPUS (a large collection of financial documents), achieves higher F1 scores compared to generic BERT, demonstrating the benefit of domain adaptation.
- Core assumption: The EDGAR-CORPUS sufficiently captures the domain-specific vocabulary and structure of financial documents.
- Evidence anchors:
  - [abstract]: "SEC-BERT, a model pre-trained from scratch on EDGAR-CORPUS, a large collection of financial documents"
  - [section]: "SEC-BERT... turned out to be the best performing model... with Longformer achieving similar levels of accuracy."
  - [corpus]: Weak - no explicit coverage statistics for EDGAR-CORPUS provided in the paper.
- Break condition: If the domain-specific corpus lacks sufficient diversity or contains biases, the improvement may not materialize.

### Mechanism 2
- Claim: Long documents can be effectively processed by chunking strategies.
- Mechanism: BUSTER documents are split into contiguous chunks, allowing transformer models to handle documents exceeding their maximum sequence length.
- Core assumption: Chunking does not significantly disrupt entity boundaries or context needed for recognition.
- Evidence anchors:
  - [abstract]: "The vast majority of documents in BUSTER has more than 500 words, which typically exceeds the maximum sequence length that LLMs... can take in input."
  - [section]: "Therefore, we split documents into contiguous chunks of text. Chunking is done such that no token is truncated at all and we fill each chunk sequence as much as possible."
  - [corpus]: Weak - no quantitative evaluation of chunking impact on F1 scores provided.
- Break condition: If entities span across chunk boundaries, the model may fail to recognize them.

### Mechanism 3
- Claim: High inter-annotator agreement indicates dataset quality.
- Mechanism: The Cohen's kappa score of 0.7402 suggests substantial agreement between annotators, indicating reliable annotations.
- Core assumption: The annotation guidelines are clear and unambiguous.
- Evidence anchors:
  - [abstract]: "The quality assessment results of the output of the annotation process."
  - [section]: "The values of Cohen's kappa (Îº) show a substantial agreement between the two evaluators."
  - [corpus]: Strong - explicit kappa score reported in Table 2.
- Break condition: If the guidelines are unclear, the high agreement might be superficial and not reflect true entity boundaries.

## Foundational Learning

- Concept: Entity recognition vs. named entity recognition
  - Why needed here: BUSTER focuses on entity-role recognition, not just named entity recognition, requiring understanding of context.
  - Quick check question: What is the difference between recognizing a company name and recognizing a company involved in a specific transaction role?

- Concept: Inter-rater reliability
  - Why needed here: Evaluating annotation quality using Cohen's kappa is crucial for dataset credibility.
  - Quick check question: What does a Cohen's kappa score of 0.7402 indicate about annotator agreement?

- Concept: Document-level vs. sentence-level processing
  - Why needed here: BUSTER is a document-level dataset, requiring models to handle long-range dependencies.
  - Quick check question: Why might entity recognition in financial documents require context beyond a single sentence?

## Architecture Onboarding

- Component map: Data collection (EDGAR) -> Annotation (expert.ai platform) -> Model training (BERT, RoBERTa, SEC-BERT, Longformer) -> Evaluation (5-fold CV)
- Critical path: Document collection -> Annotation -> Model training -> Evaluation -> Silver corpus creation
- Design tradeoffs: Manual annotation ensures quality but is time-consuming; automatic annotation is faster but may introduce noise.
- Failure signatures: Low F1 scores could indicate issues with annotation quality, model architecture, or chunking strategy.
- First 3 experiments:
  1. Evaluate the impact of different chunking strategies on F1 scores.
  2. Compare the performance of SEC-BERT with other domain-specific models.
  3. Analyze the error patterns for each entity type to identify areas for improvement.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the best model (RoBERTa) change when trained on an increased amount of manually labeled data?
- Basis in paper: [explicit] The authors mention plans to increase the amount of manually labeled data in future work.
- Why unresolved: The current dataset has a limited number of manually annotated documents (3779), and the impact of scaling up the manual annotations is unknown.
- What evidence would resolve it: Training and evaluating RoBERTa on a significantly larger manually annotated dataset and comparing its performance to the current baseline.

### Open Question 2
- Question: What is the impact of introducing specific types of relations between entities on the dataset's performance for Relation Extraction tasks?
- Basis in paper: [explicit] The authors plan to extend the dataset to include Relational Extraction in future work.
- Why unresolved: The current dataset focuses solely on Entity Recognition without considering relationships between entities.
- What evidence would resolve it: Annotating and incorporating entity relations into the dataset, then evaluating the performance of relation extraction models on this extended dataset.

### Open Question 3
- Question: How does the performance of domain-specific models like SEC-BERT compare to general-purpose models like RoBERTa when fine-tuned on the BUSTER dataset?
- Basis in paper: [explicit] The authors evaluate SEC-BERT and RoBERTa on the BUSTER dataset, but do not directly compare their performance after fine-tuning on BUSTER.
- Why unresolved: The paper shows that SEC-BERT improves when fine-tuned on financial domain data, but does not compare its performance to RoBERTa when both are fine-tuned on BUSTER.
- What evidence would resolve it: Fine-tuning both SEC-BERT and RoBERTa on the BUSTER dataset and comparing their performance metrics.

## Limitations

- Annotation Scope Limitations: The dataset focuses on business transaction entities but excludes certain entity types due to lack of explicit definitions in the annotation guidelines.
- Chunking Strategy Uncertainty: The specific chunking strategy and its impact on entity recognition performance across chunk boundaries is not thoroughly evaluated.
- Silver Corpus Quality Concerns: The automatically annotated silver corpus (6,196 documents) is not quality-assessed, potentially introducing noise into downstream applications.

## Confidence

**High Confidence**: The dataset construction methodology (using SEC Form 8K documents), the 5-fold cross-validation evaluation setup, and the basic performance trends (RoBERTa outperforming BERT, SEC-BERT showing best results) are well-documented and reproducible.

**Medium Confidence**: The reported F1 scores and inter-annotator agreement (kappa = 0.7402) are reliable, but the exact impact of chunking on entity recognition and the quality of the silver corpus remain uncertain.

**Low Confidence**: Claims about the relative importance of domain pretraining versus architecture, and the generalizability of results to other financial document types beyond SEC Form 8K, lack sufficient supporting evidence.

## Next Checks

1. **Chunk Boundary Impact Analysis**: Conduct experiments to measure entity recognition performance specifically for entities that span chunk boundaries versus those fully contained within chunks. This would quantify the effectiveness of the current chunking strategy and identify potential improvements.

2. **Silver Corpus Validation**: Sample and manually validate a subset of the automatically annotated silver corpus to estimate its quality and noise level. This would determine whether the silver corpus is suitable for training or if it requires filtering or additional annotation.

3. **Cross-Document Type Generalization**: Evaluate the trained models on a different corpus of financial documents (e.g., annual reports or press releases) to assess how well the BUSTER-trained models generalize beyond SEC Form 8K documents. This would validate the domain adaptation claims and identify potential overfitting to the specific document type.