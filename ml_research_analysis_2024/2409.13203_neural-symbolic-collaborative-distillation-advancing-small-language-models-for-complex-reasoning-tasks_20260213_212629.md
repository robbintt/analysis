---
ver: rpa2
title: 'Neural-Symbolic Collaborative Distillation: Advancing Small Language Models
  for Complex Reasoning Tasks'
arxiv_id: '2409.13203'
source_url: https://arxiv.org/abs/2409.13203
tags:
- knowledge
- reasoning
- learning
- nesycd
- specialized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes NesyCD, a novel neural-symbolic knowledge distillation
  method that enhances small language models (SLMs) for complex reasoning tasks by
  combining general knowledge modeling with specialized symbolic knowledge bases.
  The approach addresses the limitations of traditional CoT distillation by decoupling
  general reasoning capabilities from specialized knowledge, using a symbolic KB to
  store sparse, task-specific knowledge derived from LLM error analysis.
---

# Neural-Symbolic Collaborative Distillation: Advancing Small Language Models for Complex Reasoning Tasks

## Quick Facts
- arXiv ID: 2409.13203
- Source URL: https://arxiv.org/abs/2409.13203
- Authors: Huanxuan Liao; Shizhu He; Yao Xu; Yuanzhe Zhang; Kang Liu; Jun Zhao
- Reference count: 33
- One-line primary result: LLaMA3-8B and Qwen2-7B surpass GPT-3.5-turbo and approach LLaMA3-70B performance with 9× fewer parameters

## Executive Summary
This paper introduces NesyCD, a neural-symbolic knowledge distillation method that enhances small language models for complex reasoning tasks by combining general knowledge modeling with specialized symbolic knowledge bases. The approach addresses the limitations of traditional CoT distillation by decoupling general reasoning capabilities from specialized knowledge, using a symbolic KB to store sparse, task-specific knowledge derived from LLM error analysis. Experiments show that NesyCD significantly improves SLM performance across multiple benchmarks while maintaining human-comprehensible specialized knowledge.

## Method Summary
NesyCD distills general reasoning capabilities into neural networks while storing specialized knowledge in a symbolic knowledge base. The process involves first training students with general CoT distillation, then analyzing their errors to generate specialized knowledge using LLMs. This knowledge is stored symbolically and retrieved during inference based on model confidence thresholds. The approach combines the efficiency of neural networks for general patterns with the precision of symbolic representations for sparse, specialized knowledge.

## Key Results
- LLaMA3-8B and Qwen2-7B surpass GPT-3.5-turbo performance despite having 9× fewer parameters
- NesyCD demonstrates superior sample efficiency compared to traditional CoT distillation methods
- The approach maintains human-comprehensible specialized knowledge while achieving strong performance across multiple benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decoupling general reasoning from specialized knowledge enables more efficient knowledge capture in SLMs
- Mechanism: General reasoning capabilities are distilled into neural networks while specialized knowledge is stored in a symbolic KB, allowing each to be optimized for its respective complexity
- Core assumption: Neural networks struggle to model sparse, specialized knowledge while symbolic representations handle such knowledge efficiently
- Evidence anchors:
  - [abstract] "NesyCD distills the general capabilities and specialized knowledge in LLMs using different manners"
  - [section] "Neural knowledge distillation faces challenges in representing sparse and specialized knowledge due to the limited parameter space"
  - [corpus] Weak evidence - corpus contains related work on knowledge distillation but lacks direct evidence of symbolic KB benefits

### Mechanism 2
- Claim: Error analysis from student SLMs enables targeted generation of specialized knowledge
- Mechanism: SLMs are first trained with general CoT distillation, then their errors are collected and analyzed by LLMs to generate specialized knowledge addressing specific failure modes
- Core assumption: LLMs can accurately identify and articulate the specialized knowledge needed to correct student errors
- Evidence anchors:
  - [section] "LLMs analyze error cases, extracting specialized knowledge through elaborate prompts"
  - [section] "We gather correct and error cases made by SLMs fine-tuned with CoT distillation"
  - [corpus] Weak evidence - corpus contains error analysis work but not specifically for knowledge generation

### Mechanism 3
- Claim: Confidence-based retrieval of specialized knowledge optimizes the trade-off between performance and computational overhead
- Mechanism: During inference, specialized knowledge is retrieved only when model confidence falls below a threshold, preventing unnecessary retrieval for easy questions
- Core assumption: Model confidence can reliably indicate when specialized knowledge is needed
- Evidence anchors:
  - [section] "we determine the necessity of retrieval based on the model confidence"
  - [section] "A higher confidence threshold means the model requires greater certainty to trust its output"
  - [corpus] Weak evidence - corpus contains retrieval work but not confidence-based retrieval

## Foundational Learning

- Concept: Chain-of-Thought (CoT) reasoning
  - Why needed here: Understanding CoT is essential for grasping how the student model initially learns and how errors are generated for analysis
  - Quick check question: What distinguishes CoT reasoning from direct answer generation in LLMs?

- Concept: Knowledge distillation
  - Why needed here: The entire approach relies on transferring knowledge from teacher LLMs to student SLMs, requiring understanding of distillation mechanisms
  - Quick check question: How does standard CoT distillation differ from the neural-symbolic approach proposed here?

- Concept: Symbolic knowledge representation
  - Why needed here: The specialized knowledge base uses symbolic representations, requiring understanding of their advantages and limitations
  - Quick check question: What are the key differences between neural and symbolic approaches to knowledge representation?

## Architecture Onboarding

- Component map:
  Teacher LLM (GPT-3.5-turbo) -> Student SLM (LLaMA2-7B/TinyLLaMA-1.1B) -> Error collection module -> Teacher LLM