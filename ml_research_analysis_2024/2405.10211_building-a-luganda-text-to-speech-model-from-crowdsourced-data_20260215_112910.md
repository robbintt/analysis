---
ver: rpa2
title: Building a Luganda Text-to-Speech Model From Crowdsourced Data
arxiv_id: '2405.10211'
source_url: https://arxiv.org/abs/2405.10211
tags:
- speech
- speakers
- data
- quality
- voice
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper improves Luganda text-to-speech (TTS) synthesis by training
  on Common Voice recordings from six female speakers with closely matched intonation,
  rather than using data from speakers of varying ages. The authors apply speech enhancement
  to reduce background noise and filter for high-quality audio using MOS scoring,
  and also clean text transcripts.
---

# Building a Luganda Text-to-Speech Model From Crowdsourced Data

## Quick Facts
- arXiv ID: 2405.10211
- Source URL: https://arxiv.org/abs/2405.10211
- Reference count: 0
- Six female speakers with closely matched intonation used for training

## Executive Summary
This paper presents a significant improvement in Luganda text-to-speech synthesis by carefully selecting training data from six female speakers with closely matched intonation profiles. The authors enhanced speech quality through noise reduction and filtered recordings using Mean Opinion Score (MOS) evaluation, while also cleaning text transcripts. Their VITS-based system achieved a MOS score of 3.55, substantially outperforming previous baselines of 2.50 (best prior), 3.13 (one speaker), and 3.22 (two speakers).

## Method Summary
The authors leveraged Common Voice dataset recordings from six female Luganda speakers with closely matched intonation profiles. They applied speech enhancement techniques to reduce background noise and used MOS scoring to filter for high-quality audio samples. Text transcripts were also cleaned as part of the preprocessing pipeline. The core model architecture was based on VITS (Variational Inference with adversarial learning for End-to-end Text-to-Speech), a state-of-the-art TTS framework. Training data was carefully curated to ensure consistency across speakers in terms of intonation and pronunciation patterns.

## Key Results
- Achieved MOS score of 3.55, significantly outperforming previous best of 2.50
- Outperformed single-speaker model (3.13 MOS) and two-speaker model (3.22 MOS)
- Demonstrated that speaker selection with matched intonation profiles substantially improves TTS quality in low-resource languages

## Why This Works (Mechanism)
The key insight is that selecting speakers with closely matched intonation profiles reduces acoustic variability in the training data, making it easier for the model to learn consistent pronunciation patterns. By applying speech enhancement and MOS-based filtering, the authors ensured high-quality audio input, while transcript cleaning eliminated textual inconsistencies. This comprehensive preprocessing approach, combined with the VITS architecture's ability to learn from high-quality paired text-audio data, enabled superior synthesis performance.

## Foundational Learning

**VITS Architecture** - End-to-end TTS model combining variational inference with adversarial training
- Why needed: Provides state-of-the-art speech synthesis without requiring separate duration and acoustic models
- Quick check: Verify model can generate speech from arbitrary text input

**Mean Opinion Score (MOS)** - Human evaluation metric for speech quality assessment
- Why needed: Objective measure to compare model performance against baselines
- Quick check: Ensure MOS scores are collected from native speakers familiar with Luganda

**Speech Enhancement** - Techniques to reduce background noise and improve audio quality
- Why needed: Cleaner audio enables better feature learning during model training
- Quick check: Compare spectrograms before and after enhancement to verify noise reduction

## Architecture Onboarding

Component map: Text Input -> VITS Encoder -> Duration Predictor -> Mel-Spectrogram Generator -> Vocoder -> Speech Output

Critical path: The VITS encoder-decoder pipeline is the core processing chain, with the duration predictor ensuring proper timing alignment between text and speech. The adversarial training component (via the discriminator) helps produce more natural-sounding speech by matching the distribution of generated audio to real speech.

Design tradeoffs: The authors prioritized data quality over quantity by selecting only six carefully matched speakers rather than including more speakers with varying characteristics. This reduced acoustic variability but limited speaker diversity. The VITS architecture trades computational complexity for superior end-to-end performance compared to cascaded approaches.

Failure signatures: Poor MOS scores would indicate issues with speaker matching, speech enhancement effectiveness, or model training instability. Inconsistent pronunciation or unnatural prosody would suggest problems with the duration predictor or adversarial training components.

First experiments:
1. Test model on held-out speakers to verify generalization beyond training speakers
2. Compare MOS scores with and without speech enhancement preprocessing
3. Evaluate impact of removing MOS-based filtering on final output quality

## Open Questions the Paper Calls Out

The paper does not explicitly call out specific open questions, but the limitations section implicitly raises questions about generalizability to other languages and speaker groups.

## Limitations

- Results are specific to Luganda and six female speakers with matched intonation, limiting generalizability
- No ablation study to quantify individual contributions of speaker selection, speech enhancement, and filtering
- Lack of validation across mixed-gender speaker groups or speakers of varying ages

## Confidence

| Claim | Confidence |
|-------|------------|
| 3.55 MOS improvement over baselines | High |
| Speaker selection with matched intonation improves TTS quality | Medium |
| Methodology generalizes to other low-resource languages | Low |

## Next Checks

1. Conduct an ablation study to quantify the individual impact of speaker selection, speech enhancement, and MOS filtering on final MOS scores
2. Test the methodology with mixed-gender speaker groups and speakers of varying ages to assess generalizability
3. Apply the same speaker selection and preprocessing pipeline to at least two other low-resource languages to verify cross-linguistic effectiveness