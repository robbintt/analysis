---
ver: rpa2
title: 'Fine-Grained Modeling of Narrative Context: A Coherence Perspective via Retrospective
  Questions'
arxiv_id: '2402.13551'
source_url: https://arxiv.org/abs/2402.13551
tags:
- context
- questions
- narco
- question
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes NARCO, a novel graph-based paradigm for narrative
  comprehension. NARCO represents narratives as graphs where nodes are text passages
  and edges are retrospective questions reflecting coherence relations, inspired by
  human cognitive perception.
---

# Fine-Grained Modeling of Narrative Context: A Coherence Perspective via Retrospective Questions

## Quick Facts
- **arXiv ID**: 2402.13551
- **Source URL**: https://arxiv.org/abs/2402.13551
- **Reference count**: 16
- **Primary result**: Graph-based NARCO model improves narrative comprehension tasks including recap identification, plot retrieval, and long document QA through retrospective question-based coherence modeling

## Executive Summary
This paper introduces NARCO, a novel graph-based paradigm for narrative comprehension that represents stories as networks of text passages connected by retrospective questions. The approach captures coherence relations by generating questions that bridge temporal and causal dependencies between passages, inspired by human cognitive processing of narratives. NARCO operates without human annotations, using a two-stage LLM prompting scheme to generate and filter questions. Three comprehensive studies demonstrate its effectiveness: boosting recap identification F1 scores by up to 4.7 points, improving plot retrieval performance by 3.4% zero-shot and 2.4% supervised, and enhancing long document QA accuracy by 2-5% through better context retrieval.

## Method Summary
NARCO constructs narrative graphs where nodes represent text passages and edges are retrospective questions that capture coherence relations between passages. The system uses a two-stage LLM prompting approach: first generating candidate retrospective questions that connect passages through temporal and causal links, then filtering these questions to ensure relevance and coherence. The resulting graph structure enriches passage representations by incorporating task-agnostic coherence information, allowing downstream models to leverage better context embeddings. The approach is fully automated without requiring human annotations, making it scalable for various narrative understanding tasks.

## Key Results
- Improves recap identification F1 scores by up to 4.7 points
- Boosts plot retrieval nDCG@10 by 3.4% zero-shot and 2.4% supervised
- Enhances long document QA accuracy by 2-5% through improved context retrieval

## Why This Works (Mechanism)
NARCO works by modeling narrative coherence through retrospective question generation, which captures the cognitive process humans use to understand stories. By representing narratives as graphs with passages as nodes and coherence-reflecting questions as edges, the system creates rich structural representations that encode temporal and causal dependencies. The two-stage LLM prompting ensures that generated questions are both relevant and meaningful for bridging passage relationships. This graph-based representation provides task-agnostic coherence information that enriches local context embeddings, making it particularly effective for downstream tasks requiring deep narrative understanding.

## Foundational Learning
- **Narrative coherence modeling**: Understanding how stories maintain logical flow through temporal and causal relationships - needed to capture the structural dependencies that make narratives comprehensible; quick check: can the model identify logical gaps in story progression
- **Graph-based representation learning**: Converting text into graph structures where nodes and edges capture semantic relationships - needed to represent narrative passages and their coherence connections; quick check: does the graph preserve important narrative dependencies
- **Large language model prompting strategies**: Designing effective prompts for generating and filtering retrospective questions - needed to automate the creation of meaningful coherence links without human annotation; quick check: are generated questions relevant and useful for narrative understanding
- **Task-agnostic feature extraction**: Creating representations that improve multiple downstream tasks - needed to demonstrate NARCO's general utility beyond specific applications; quick check: does NARCO improve performance across diverse narrative tasks
- **Context embedding enrichment**: Enhancing passage representations with additional structural information - needed to provide better input features for downstream models; quick check: do enriched embeddings lead to measurable performance gains

## Architecture Onboarding

**Component map**: Text passages -> Passage encoder -> LLM question generator -> Question filter -> Graph constructor -> Enriched embeddings -> Downstream task models

**Critical path**: The essential flow is passage encoding → question generation → question filtering → graph construction → embedding enrichment. The LLM-based question generation and filtering stages are most critical, as they determine the quality of coherence relations captured in the graph.

**Design tradeoffs**: The system trades computational overhead of graph construction for improved narrative understanding. Using LLMs without human annotation enables scalability but introduces model bias risks. The graph-based approach assumes retrospective questions adequately capture all relevant coherence relations, potentially missing nuanced narrative dependencies.

**Failure signatures**: Poor performance may manifest as irrelevant or hallucinated retrospective questions, leading to noisy graph edges that confuse rather than clarify passage relationships. Overly dense graphs with excessive questions may dilute important coherence signals. Insufficient question diversity may fail to capture all relevant narrative dependencies.

**First experiments to run**:
1. Generate retrospective questions for a simple two-passage narrative and manually verify their coherence-reflecting quality
2. Create the graph structure for a short story and visualize the passage-question connections to assess coverage
3. Apply NARCO to a basic recap identification task and compare performance with baseline passage representations

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Heavy reliance on LLMs for question generation and filtering without human annotation validation, introducing potential model bias and hallucination risks
- Evaluation primarily focuses on narrative understanding tasks, leaving uncertainty about effectiveness on non-narrative domains
- Graph construction assumes retrospective questions adequately capture all relevant coherence relations, though some narrative dependencies may require more nuanced representation

## Confidence
- Medium-High for NARCO's effectiveness in narrative-specific tasks (recap identification, plot retrieval, QA)
- Medium for claims about task-agnostic coherence capture
- Medium-Low for scalability claims to very long documents or domains outside narrative comprehension

## Next Checks
1. Conduct human evaluation studies to validate that NARCO-generated retrospective questions accurately reflect human-perceived coherence relations and narrative understanding
2. Test NARCO's performance on non-narrative domains (scientific papers, news articles, technical documentation) to assess generalization beyond story structures
3. Perform ablation studies varying the number and types of retrospective questions per passage pair to determine optimal graph density and identify which question categories contribute most to downstream task performance