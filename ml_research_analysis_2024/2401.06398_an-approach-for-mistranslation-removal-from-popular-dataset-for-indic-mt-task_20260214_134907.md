---
ver: rpa2
title: An approach for mistranslation removal from popular dataset for Indic MT Task
arxiv_id: '2401.06398'
source_url: https://arxiv.org/abs/2401.06398
tags:
- dataset
- translation
- machine
- quality
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of incorrect translations in the
  Samanantar dataset for Indian languages (ILs), which negatively impacts the performance
  of neural machine translation (NMT) systems. The authors propose an algorithm to
  remove mistranslations from the training corpus by training a model on the original
  dataset, generating translations for a validation set, and using the BLEU score
  to create a threshold.
---

# An approach for mistranslation removal from popular dataset for Indic MT Task

## Quick Facts
- arXiv ID: 2401.06398
- Source URL: https://arxiv.org/abs/2401.06398
- Reference count: 40
- Primary result: Dataset quality significantly impacts MT performance; removing mistranslations improves BLEU, METEOR, and RIBES scores

## Executive Summary
This paper addresses the critical issue of mistranslations in the Samanantar dataset for Indian languages, which negatively impacts neural machine translation performance. The authors propose an algorithm that identifies and removes mistranslated sentence pairs from the training corpus using BLEU score thresholds. By filtering out low-quality data, the method improves translation quality across both Hindi and Odia language pairs, demonstrating that dataset curation is essential for effective MT systems.

## Method Summary
The authors develop a mistranslation removal algorithm that trains an NMT model on the original dataset, generates translations for a validation set, and uses BLEU score thresholds to identify and discard mistranslated sentence pairs. The threshold retains the top 75% of sentences based on performance, effectively removing noisy data from the training corpus. The method is evaluated on Hindi and Odia languages using transformer-based NMT models with BPE tokenization, comparing unfiltered datasets against filtered datasets to measure quality improvements.

## Key Results
- Odia-English translation achieved BLEU score of 16.22 after mistranslation removal
- Hindi-English translation achieved BLEU score of 35.32 after mistranslation removal
- ILs-to-English consistently outperforms English-to-ILs despite using identical training data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Removing mistranslated sentence pairs from the training corpus improves translation quality by eliminating incorrect learning signals.
- Mechanism: The method trains an NMT model on the full dataset, translates the validation set, computes BLEU scores, and sets a threshold (BLEU/4) to discard low-scoring sentence pairs. This removes pairs that likely contain mistranslations.
- Core assumption: BLEU score differences between source and target in a pair reliably indicate translation quality; pairs below the threshold are mistranslations.
- Evidence anchors:
  - [abstract] The paper proposes an algorithm to remove mistranslations by training a model on the original dataset, generating translations for a validation set, and using the BLEU score to create a threshold. Sentences with BLEU scores below the threshold are discarded as mistranslations.
  - [section] "For our case, the threshold score is calculated using the validation dataset to remove incorrect translations from the training dataset. The threshold allows only the top 75% performant sentences to be retained in the dataset."
  - [corpus] Weak evidence; the corpus provides related work on data filtering but no direct evidence on BLEU-based mistranslation detection.
- Break condition: If the validation set contains mistranslations, the threshold may incorrectly classify good translations as bad or vice versa, leading to loss of quality data or retention of noise.

### Mechanism 2
- Claim: Using the same dataset for both ILs-to-English and English-to-ILs directions, but ILs-to-English performs better due to higher information density in ILs.
- Mechanism: ILs have richer morphological and structural information (e.g., 8 grammatical cases, gendered verbs) compared to English, which has only 3 cases and no verb genders. This extra information loss during English-to-ILs translation degrades performance.
- Core assumption: The structural and morphological richness of ILs cannot be fully recovered from English translations, causing information loss in the English-to-ILs direction.
- Evidence anchors:
  - [section] "ILs being able to contain more information than ENG results in a loss of context and information present in the ILs with the process of translation into English for its limit to represent all the informatic inputs. This effect is fairly pronounced in the case of translations from English to ILs, where the model does not have sufficient information to infer the additional context from English itself."
  - [abstract] "It is also noticed that, despite the fact that the ILs -English and English -ILs systems are trained using the same corpus, ILs -English works more effectively across all the evaluation metrics."
  - [corpus] Weak evidence; related papers focus on other filtering techniques but do not directly address this directional performance asymmetry.
- Break condition: If the dataset quality improves significantly or if a model architecture better handles information loss (e.g., attention mechanisms that capture missing context), the directional performance gap may narrow.

### Mechanism 3
- Claim: Increasing dataset size proportionally improves translation quality for low-resource ILs like Odia.
- Mechanism: Larger datasets provide more diverse linguistic patterns and structures, allowing the NMT model to learn better representations and generalize more effectively.
- Core assumption: For low-resource languages, the scarcity of training data is the primary bottleneck, and more data directly translates to better model performance.
- Evidence anchors:
  - [section] "Results also reveal that Indic to the English language performs better than English to the English language." (Note: likely meant English-to-ILs vs ILs-to-English)
  - [abstract] "It is also investigated how different dataset divisions (into multiple subsets) influence the translation quality."
  - [corpus] Weak evidence; the corpus contains related work on data selection and filtering but not direct evidence on dataset size scaling for ILs.
- Break condition: If the dataset contains persistent noise or mistranslations, simply increasing size may not improve quality and could even degrade it if the noise scales proportionally.

## Foundational Learning

- Concept: Neural Machine Translation (NMT) and Transformer architecture
  - Why needed here: The paper uses a transformer-based NMT model for translation tasks. Understanding the architecture and training process is essential to grasp how the mistranslation removal algorithm integrates.
  - Quick check question: What are the key components of the transformer architecture, and how do encoder-decoder attention mechanisms contribute to translation quality?

- Concept: Evaluation metrics (BLEU, METEOR, RIBES)
  - Why needed here: The paper evaluates translation quality using these metrics. Understanding their calculation and interpretation is crucial for assessing the effectiveness of the mistranslation removal method.
  - Quick check question: How does BLEU differ from METEOR and RIBES in terms of precision, recall, and penalization of translation errors?

- Concept: Byte Pair Encoding (BPE) tokenization
  - Why needed here: The paper uses BPE for segmenting the dataset into subwords. Understanding BPE helps in comprehending how the model handles vocabulary and rare words.
  - Quick check question: How does BPE tokenization improve the handling of out-of-vocabulary words in NMT systems?

## Architecture Onboarding

- Component map: Dataset preprocessing (BPE tokenization, filtering) -> Transformer NMT model (6 encoder-decoder layers, 512 hidden units) -> Fairseq training -> Evaluation (BLEU, METEOR, RIBES)

- Critical path:
  1. Load and preprocess dataset (tokenize, split into train/val/test)
  2. Train baseline NMT model on full dataset
  3. Generate translations for validation set using baseline model
  4. Compute BLEU scores and set threshold
  5. Filter training dataset by removing low-BLEU pairs
  6. Retrain NMT model on filtered dataset
  7. Evaluate and compare performance with baseline

- Design tradeoffs:
  - Dataset size vs. quality: Removing mistranslations reduces dataset size but improves quality. The threshold setting (BLEU/4) balances this tradeoff.
  - Computational cost: Retraining the model on the filtered dataset adds overhead but is necessary to realize quality gains.
  - Generalization: Aggressive filtering might remove useful but low-scoring pairs, potentially harming model robustness.

- Failure signatures:
  - No improvement in BLEU scores after filtering: Indicates the threshold is too strict or the validation set contains mistranslations.
  - Significant drop in dataset size with minimal BLEU gain: Suggests the filtering is too aggressive, removing useful data.
  - Inconsistent performance across language pairs: May indicate language-specific issues in the filtering algorithm or dataset characteristics.

- First 3 experiments:
  1. Run the baseline NMT model on the full dataset and record BLEU, METEOR, RIBES scores.
  2. Apply the mistranslation removal algorithm with a threshold of BLEU/4 and retrain the model. Compare scores.
  3. Vary the threshold (e.g., BLEU/3, BLEU/5) and observe the impact on dataset size and translation quality.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the effectiveness of the mistranslation removal algorithm vary with different threshold settings beyond the 75% top-performer cutoff?
- Basis in paper: [explicit] The authors mention using a threshold based on the top 75% performant sentences and suggest investigating the impact of different threshold variations in future work.
- Why unresolved: The paper only tested one threshold value, limiting understanding of how the algorithm performs across different levels of stringency.
- What evidence would resolve it: Experiments varying the threshold (e.g., 50%, 60%, 80%, 90%) and measuring resulting BLEU, METEOR, and RIBES scores for each setting.

### Open Question 2
- Question: Why do Indic-to-English translations consistently outperform English-to-Indic translations despite using the same training corpus?
- Basis in paper: [explicit] The authors note that "Indic to the English language performs better than English to the English language" and that this occurs "despite the fact that the ILs-English and English-ILs systems are trained using the same corpus."
- Why unresolved: The paper observes this phenomenon but does not investigate its underlying causes.
- What evidence would resolve it: Analysis of translation errors, linguistic feature alignment, and model behavior patterns between the two translation directions.

### Open Question 3
- Question: How does the mistranslation removal approach scale to other low-resource Indian languages beyond Hindi and Odia?
- Basis in paper: [explicit] The authors state that "More language pairs with different dataset sizes for MT tasks need to be tested for future work" and suggest investigating other languages.
- Why unresolved: The study only examined two languages, leaving uncertainty about generalizability to other Indian languages with different resource levels and linguistic properties.
- What evidence would resolve it: Applying the same methodology to other Indian languages (e.g., Tamil, Telugu, Bengali) and comparing performance improvements.

## Limitations

- The BLEU-based threshold approach may incorrectly filter useful linguistic diversity by conflating complex translations with mistranslations
- The study focuses only on two Indian languages (Hindi and Odia), limiting generalizability to other languages with different linguistic properties
- Claims about dataset size scaling benefits for low-resource languages are not directly tested in the paper

## Confidence

- **High confidence**: The experimental setup is well-defined, with clear baseline comparisons and consistent evaluation across languages and directions. The observation that ILs-to-English outperforms English-to-ILs is supported by multiple evaluation metrics.
- **Medium confidence**: The mistranslation removal mechanism is plausible and demonstrates improvement, but the BLEU-based threshold approach lacks validation against human-annotated mistranslation datasets. The directional performance asymmetry is explained through linguistic theory but could benefit from more rigorous analysis.
- **Low confidence**: Claims about dataset size scaling benefits for low-resource languages are not directly tested in the paper. The assertion that ILs contain "more information" than English is presented without quantitative measures of information density or cross-linguistic comparison.

## Next Checks

1. **Human validation of threshold selection**: Manually annotate a subset of discarded sentences to verify whether the BLEU-based threshold accurately identifies mistranslations versus complex but correct translations. This would validate whether the filtering algorithm is removing true noise or useful linguistic diversity.

2. **Cross-linguistic threshold analysis**: Apply the same mistranslation removal algorithm to other language pairs (e.g., Tamil-English, Bengali-English) with varying linguistic distances to determine if the BLEU threshold approach generalizes beyond Hindi and Odia, and whether language-specific adjustments are needed.

3. **Ablation study on threshold values**: Systematically vary the threshold from retaining 50% to 90% of sentences and measure the tradeoff between dataset size and translation quality. This would identify the optimal balance point and test the robustness of the mistranslation detection method across different filtering strictness levels.