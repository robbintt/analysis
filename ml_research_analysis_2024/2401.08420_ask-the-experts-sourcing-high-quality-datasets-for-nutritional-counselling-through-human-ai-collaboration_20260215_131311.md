---
ver: rpa2
title: 'Ask the experts: sourcing high-quality datasets for nutritional counselling
  through Human-AI collaboration'
arxiv_id: '2401.08420'
source_url: https://arxiv.org/abs/2401.08420
tags:
- experts
- text
- struggle
- struggles
- food
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study explores sourcing high-quality nutrition counseling
  datasets by combining LLMs, crowd-workers, and nutrition experts. The approach involves
  crowdsourcing diet-related struggles, clustering them into topics, and using ChatGPT
  to generate supportive text under expert guidance.
---

# Ask the experts: sourcing high-quality datasets for nutritional counselling through Human-AI collaboration

## Quick Facts
- arXiv ID: 2401.08420
- Source URL: https://arxiv.org/abs/2401.08420
- Reference count: 40
- Primary result: Human-AI collaboration combining LLMs, crowd-workers, and nutrition experts successfully sources high-quality nutrition counseling datasets, with ChatGPT generating ~85% safe supportive text but struggling with mental health topics

## Executive Summary
This study demonstrates that high-quality nutrition counseling datasets can be created through human-AI collaboration, where crowd-workers provide authentic dietary struggles, ChatGPT generates supportive text under expert guidance, and nutrition professionals evaluate safety. The approach addresses limitations of using LLMs alone by leveraging expert knowledge to filter unsafe or low-quality outputs. The resulting HAI-Coaching dataset contains ~2.4K crowdsourced struggles and ~97K expert-annotated supportive texts, with approximately 85% of ChatGPT-generated candidates deemed safe for deployment.

## Method Summary
The method combines crowdsourcing, clustering, and human-AI collaboration to create nutrition counseling datasets. First, crowd-workers write diet-related struggles, which are clustered into topics by experts. ChatGPT then generates supportive text (reflections, comfort, reframing, suggestions) for each struggle using expert-crafted prompts. Experts evaluate the safety of generated text through iterative safety assessments and prompt refinements. The process ensures that the final dataset contains only high-quality, safe supportive text while capturing diverse real-world dietary challenges.

## Key Results
- ChatGPT generates fluent, human-like supportive text for nutrition counseling
- ~85% of generated candidates were deemed safe by expert evaluation
- ChatGPT struggles with mental health topics, occasionally generating harmful or useless text
- The model tends to reinforce stereotypes in its generated content
- Experts conclude ChatGPT is not ready for unsupervised deployment in counseling contexts

## Why This Works (Mechanism)

### Mechanism 1
Human-AI collaboration leverages domain experts to mitigate LLM hallucinations and biases. Nutrition experts guide ChatGPT prompts and evaluate generated supportive text, filtering unsafe or low-quality outputs. Core assumption: Human expertise can reliably identify and correct AI-generated errors in sensitive domains. Evidence anchors: [abstract] problems like hallucinations and biases limit such applications... we pick nutrition counseling... and show that high-quality datasets can be gathered by combining LLMs, crowd-workers and nutrition experts; [section 5.2] Since previous experts could not help us because of work commitments, we recruit 13 new ones... to contribute to both prompt engineering and safety assessment. Break condition: If experts disagree consistently or cannot identify harmful patterns, the collaboration breaks down.

### Mechanism 2
Iterative prompt engineering improves ChatGPT's ability to generate safe, domain-appropriate supportive text. Experts refine prompts based on batch outputs, aligning model behavior with counseling best practices. Core assumption: LLMs can be effectively steered through prompt iteration to meet domain-specific quality standards. Evidence anchors: [section 5.2.1] Experts also helped with prompts wording and structure tuning and provided multiple slots that we used to instruct ChatGPT on how to start the sentence, to further reduce chances of harmful output; [section 5.2] We follow an iterative process... experts mark it on safety... we then improve the prompts after experts’ feedback and start the cycle again. Break condition: If prompt changes do not reduce unsafe outputs, iteration fails.

### Mechanism 3
Crowdsourcing struggles from real users provides representative, varied data for model training. Mechanical Turk and Prolific workers write personal dietary struggles, capturing real-world issues. Core assumption: Crowdworkers can articulate genuine struggles that reflect diverse user experiences. Evidence anchors: [section 5.1] We choose to source struggles from people as 1) human-generated text is still considered the gold standard in terms of quality and representativeness...; [section 5.1.1] After sanity check we accept the work of 816 workers... for a total of 2448 individual struggles, highly varying in length. Break condition: If struggles are dominated by a narrow demographic or fail to cover key counseling topics, representativeness breaks.

## Foundational Learning

- Concept: Nutrition counseling process
  - Why needed here: Understanding the domain is essential for evaluating and generating supportive text
  - Quick check question: What are the four main elements of supportive text in nutrition counseling?

- Concept: Prompt engineering
  - Why needed here: LLMs are sensitive to prompt wording; effective engineering is crucial for quality outputs
  - Quick check question: How did experts improve ChatGPT prompts during iteration?

- Concept: Inter-annotator agreement (IAA)
  - Why needed here: IAA measures consistency among expert annotators, indicating reliability of safety labels
  - Quick check question: What IAA values indicate substantial agreement among annotators?

## Architecture Onboarding

- Component map: Crowdsourcing interface → Worker submissions → Sanity checks → Clustering → Prompt engineering cycle → Expert annotation → Final prompts → Mass generation → Expert safety annotation → Dataset release

- Critical path: Crowdworker struggle collection → Expert-driven clustering → Prompt engineering → Safety annotation → Dataset

- Design tradeoffs: Expert involvement ensures quality but increases cost and time; open models reduce dependency but may lack performance

- Failure signatures: High unsafe output rates; low IAA; biased or unrepresentative struggle topics

- First 3 experiments:
  1. Compare safety annotation rates with and without expert guidance
  2. Test different clustering algorithms on struggle diversity
  3. Evaluate prompt variations on generated text quality

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ChatGPT be fine-tuned to improve its performance on sensitive topics like mental health while maintaining its general fluency and human-likeness?
- Basis in paper: Explicit - The paper states that ChatGPT struggles with mental health topics and occasionally generates harmful text in such cases
- Why unresolved: The paper only discusses the challenges faced by ChatGPT in handling sensitive topics, but does not propose specific solutions for fine-tuning the model to address these issues
- What evidence would resolve it: A study comparing the performance of a fine-tuned ChatGPT model on mental health topics with the original model, demonstrating improved safety and reduced harmful outputs

### Open Question 2
- Question: Can the HAI-Coaching dataset be used to train smaller, more energy-efficient language models for nutrition counseling applications without significant loss in performance?
- Basis in paper: Inferred - The paper mentions the potential benefits of using smaller models, such as reduced energy/environmental costs, but does not provide a direct comparison of performance between small and large models
- Why unresolved: The paper only provides a preliminary evaluation of perplexity for different model sizes, but does not assess their actual performance in generating supportive text or classifying struggles
- What evidence would resolve it: A study comparing the performance of small and large language models fine-tuned on HAI-Coaching for tasks such as supportive text generation and struggle classification, demonstrating whether smaller models can achieve comparable results

### Open Question 3
- Question: How can the issue of "safe but useless" candidates in the HAI-Coaching dataset be addressed to improve the overall quality and usefulness of the generated supportive text?
- Basis in paper: Explicit - The paper mentions that experts reported a consistent amount of "safe but useless" candidates, which are text that is safe but does not provide meaningful help to the user
- Why unresolved: The paper acknowledges the issue but does not propose specific strategies for filtering out or improving these candidates
- What evidence would resolve it: A study investigating methods for identifying and filtering out "safe but useless" candidates from the dataset, such as using additional expert annotations or developing automated quality assessment metrics, and demonstrating the impact on the performance of models trained on the improved dataset

## Limitations

- Safety assessment relies entirely on expert judgment without clear, reproducible guidelines
- No quantitative analysis of prompt engineering effectiveness or iterative improvement
- Dataset representativeness assumed from crowdworker submissions but not validated against real counseling data

## Confidence

- **High confidence**: The core finding that ChatGPT struggles with mental health topics and can generate harmful content when prompted with such struggles
- **Medium confidence**: The ~85% safety rate figure, as it depends on subjective expert assessment without inter-rater reliability metrics reported
- **Low confidence**: The generalizability of results to other domains, as nutrition counseling may have unique characteristics that don't transfer to other expert-guided LLM applications

## Next Checks

1. Replicate the safety assessment with a different pool of nutrition experts using blind annotations to measure inter-rater reliability and check for systematic bias in the original expert group
2. Conduct an ablation study comparing ChatGPT outputs generated with expert-guided prompts versus standard prompts to quantify the improvement from iterative prompt engineering
3. Validate the crowdsourced struggles dataset against real nutrition counseling session transcripts to assess whether crowdworkers accurately captured authentic client struggles or introduced artifacts