---
ver: rpa2
title: 'Salute the Classic: Revisiting Challenges of Machine Translation in the Age
  of Large Language Models'
arxiv_id: '2401.08350'
source_url: https://arxiv.org/abs/2401.08350
tags:
- translation
- data
- llms
- language
- words
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper revisits six classical challenges in machine translation
  through the lens of large language models (LLMs). It finds that LLMs reduce the
  need for extensive parallel data for high-resource languages and improve long sentence
  translation, even at the document level.
---

# Salute the Classic: Revisiting Challenges of Machine Translation in the Age of Large Language Models

## Quick Facts
- arXiv ID: 2401.08350
- Source URL: https://arxiv.org/abs/2401.08350
- Authors: Jianhui Pang; Fanghua Ye; Longyue Wang; Dian Yu; Derek F. Wong; Shuming Shi; Zhaopeng Tu
- Reference count: 40
- Primary result: LLM-based machine translation shows improvements in long sentence translation and reduced parallel data requirements, but still struggles with domain mismatch and rare word prediction

## Executive Summary
This paper revisits six classical challenges in machine translation through the lens of large language models (LLMs), providing a comprehensive assessment of how these foundational issues have evolved in the LLM era. The authors systematically examine the effectiveness of LLMs across key translation challenges including data requirements, long sentence translation, domain adaptation, and rare word handling. While LLMs demonstrate notable improvements in certain areas, particularly for high-resource language pairs, they still face significant hurdles that require innovative solutions. The study also identifies new challenges specific to LLMs, including inference efficiency, low-resource language translation during pretraining, and human-aligned evaluation metrics.

## Method Summary
The paper conducts a comprehensive reassessment of classical machine translation challenges by examining LLM performance across multiple dimensions. The authors analyze existing literature and empirical evidence to evaluate how LLMs have impacted traditional translation problems. The study covers six classical challenges: parallel data requirements, long sentence translation, document-level translation, domain adaptation, low-resource translation, and rare word prediction. Additionally, the authors identify three new challenges specific to LLMs: inference efficiency, low-resource language translation during pretraining, and human-aligned evaluation. The analysis draws from a wide range of recent research papers and experimental results to provide a holistic view of the current state of LLM-based translation systems.

## Key Results
- LLMs significantly reduce the need for extensive parallel data for high-resource languages while maintaining translation quality
- Long sentence translation shows marked improvement with LLMs, even at document level, compared to traditional NMT approaches
- Domain mismatch and rare word prediction remain significant challenges for LLM-based translation systems

## Why This Works (Mechanism)
LLMs leverage their pretraining on vast amounts of multilingual text to develop robust language understanding and generation capabilities that transfer well to translation tasks. The attention mechanisms and transformer architecture enable better handling of long-range dependencies and contextual information compared to traditional NMT models. The few-shot and zero-shot learning capabilities of LLMs allow them to perform translation with limited parallel data by utilizing their pretraining knowledge. The autoregressive nature of LLMs provides better coherence in document-level translation by maintaining contextual consistency across sentences.

## Foundational Learning
- **Attention Mechanisms**: Critical for capturing long-range dependencies and contextual relationships in translation; quick check involves measuring attention distribution across source and target sentences
- **Transformer Architecture**: Forms the backbone of modern LLMs; quick check involves analyzing layer-wise representations and their impact on translation quality
- **Pretraining Objectives**: Language modeling objectives enable transfer learning for translation; quick check involves evaluating zero-shot translation performance
- **Cross-lingual Representations**: Enable translation between languages without explicit parallel data; quick check involves probing for language-specific vs. language-agnostic features
- **Context Window**: Determines the maximum sequence length for translation; quick check involves measuring performance degradation with increasing sentence length
- **Prompt Engineering**: Critical for guiding LLM translation behavior; quick check involves comparing different prompting strategies and their impact on quality

## Architecture Onboarding

**Component Map:**
Multilingual Pretraining -> Translation Fine-tuning -> Inference Pipeline -> Evaluation Framework

**Critical Path:**
Multilingual Pretraining -> Translation Fine-tuning -> Inference Pipeline

**Design Tradeoffs:**
- Model size vs. inference efficiency: Larger models provide better quality but slower inference
- Context window size vs. computational cost: Longer contexts improve coherence but increase memory requirements
- Fine-tuning data quantity vs. quality: More data improves performance but increases cost and time

**Failure Signatures:**
- Hallucinations in low-resource languages due to insufficient pretraining data
- Domain-specific terminology errors when fine-tuning data is limited
- Performance degradation on very long sentences due to context window limitations
- Inconsistent translations across similar contexts due to lack of explicit alignment during pretraining

**First 3 Experiments to Run:**
1. Compare zero-shot vs. few-shot translation performance across different language pairs and domains
2. Evaluate the impact of context window size on document-level translation quality
3. Measure inference efficiency (tokens/second) for different model sizes and batch configurations

## Open Questions the Paper Calls Out
The paper identifies several open questions that require further investigation in the field of LLM-based machine translation. These include: how to effectively translate truly low-resource languages that are underrepresented in pretraining data; what are the optimal strategies for human-aligned evaluation that capture nuanced quality aspects beyond traditional metrics; how to improve inference efficiency for practical deployment of large translation models; and what are the best approaches for domain adaptation when fine-tuning data is limited or unavailable.

## Limitations
- The study focuses primarily on English-centric translation tasks, limiting generalizability to truly low-resource and non-English-centric scenarios
- Quantitative benchmarks comparing LLM-based systems against contemporary NMT models under controlled conditions are lacking
- The analysis of rare word prediction challenges is primarily qualitative without sufficient empirical evidence
- Newly identified challenges regarding inference efficiency and human-aligned evaluation are discussed conceptually without systematic measurement

## Confidence

**High confidence:**
- LLMs demonstrate improved performance on long sentences and reduced parallel data requirements for high-resource languages

**Medium confidence:**
- Domain mismatch remains a significant challenge for LLMs, though the magnitude relative to traditional NMT systems is unclear

**Low confidence:**
- The severity of rare word prediction issues and the specific nature of human-aligned evaluation challenges require further empirical validation

## Next Checks
1. Conduct controlled experiments comparing LLM-based translation systems with state-of-the-art NMT models on standardized benchmarks, measuring performance across sentence length, domain adaptation, and rare word handling
2. Design and implement a systematic evaluation framework for human-aligned metrics, including both automatic and human evaluation protocols that capture nuanced quality aspects
3. Test the scalability and efficiency of LLM-based translation systems across diverse language pairs, including genuinely low-resource languages, to validate the claimed improvements and identify remaining bottlenecks