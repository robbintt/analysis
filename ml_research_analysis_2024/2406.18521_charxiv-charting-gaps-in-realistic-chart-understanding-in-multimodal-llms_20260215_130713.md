---
ver: rpa2
title: 'CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs'
arxiv_id: '2406.18521'
source_url: https://arxiv.org/abs/2406.18521
tags:
- answer
- question
- example
- page
- back
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CharXiv introduces a realistic benchmark for chart understanding
  in multimodal large language models by curating 2,323 diverse, real-world charts
  from arXiv papers paired with human-annotated questions. Existing chart benchmarks
  are limited by synthetic data and template-based questions, leading to overestimated
  model performance.
---

# CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs

## Quick Facts
- arXiv ID: 2406.18521
- Source URL: https://arxiv.org/abs/2406.18521
- Reference count: 40
- Primary result: Benchmark reveals significant performance gaps between GPT-4o (47.1%) and open-source models (29.2%) on reasoning tasks

## Executive Summary
CharXiv introduces a realistic benchmark for evaluating chart understanding in multimodal large language models by curating 2,323 real-world charts from arXiv papers paired with human-annotated questions. Unlike existing benchmarks that rely on synthetic data and template-based questions, CharXiv captures the complexity and diversity of actual scientific charts. The evaluation reveals substantial performance gaps, with GPT-4o achieving 47.1% on reasoning tasks while the best open-source model reaches only 29.2%, both falling far below human performance of 80.5%.

## Method Summary
The benchmark was constructed by collecting charts from arXiv papers across diverse scientific domains and having human annotators generate questions that reflect realistic usage scenarios. Questions were categorized into descriptive tasks (basic chart element extraction) and reasoning tasks (complex visual analysis). The dataset includes various chart types found in scientific literature, with questions designed to test both fundamental understanding and higher-level reasoning capabilities. Models were evaluated on their ability to accurately answer these questions across different chart complexities.

## Key Results
- GPT-4o achieves 47.1% accuracy on reasoning questions versus 29.2% for the best open-source model
- Human performance benchmark stands at 80.5%, highlighting significant room for improvement
- Open-source models struggle with basic descriptive tasks, not just complex reasoning
- Both models perform significantly below human capabilities across all chart types

## Why This Works (Mechanism)
The benchmark addresses the fundamental limitation of existing chart understanding evaluations that overestimate model capabilities through synthetic, template-based data. By using real-world charts from scientific literature with naturally occurring question patterns, CharXiv captures the true complexity and diversity of chart understanding tasks that models encounter in practice. The human-annotated questions reflect realistic user queries rather than artificially constrained templates, providing a more accurate assessment of model generalization and reasoning abilities.

## Foundational Learning

**Multimodal Large Language Models** - Why needed: These models process both visual and textual information simultaneously, essential for chart understanding. Quick check: Verify model can process both image inputs and generate coherent textual responses.

**Chart Element Recognition** - Why needed: Basic understanding of axes, labels, data points, and chart types forms foundation for all chart tasks. Quick check: Ensure model can accurately identify chart components before attempting reasoning tasks.

**Visual Reasoning** - Why needed: Chart understanding requires inferring relationships, trends, and conclusions from visual data representations. Quick check: Test model on simple trend identification and comparison tasks.

**Domain Knowledge Integration** - Why needed: Scientific charts often require understanding of specific terminology and conventions from different fields. Quick check: Verify model performance across diverse scientific domains represented in the benchmark.

## Architecture Onboarding

Component map: Image input -> Visual feature extraction -> Multimodal fusion -> Text generation -> Answer output

Critical path: Visual encoder processes chart images, multimodal transformer integrates visual and textual representations, decoder generates answers to questions about chart content.

Design tradeoffs: The benchmark reveals tradeoffs between model size (GPT-4o vs open-source) and reasoning performance, with larger models showing better generalization but still failing on basic tasks. The choice between synthetic and real data significantly impacts evaluation validity.

Failure signatures: Models consistently fail on questions requiring cross-chart comparisons, trend extrapolation, and understanding of complex multi-series visualizations. Basic descriptive tasks show surprisingly high error rates, indicating fundamental visual processing limitations.

First experiments: 1) Test model on single-chart descriptive questions to establish baseline capabilities, 2) Evaluate cross-chart comparison performance to assess reasoning abilities, 3) Measure performance degradation across chart complexity levels to identify failure thresholds.

## Open Questions the Paper Calls Out
None

## Limitations
- Potential sampling bias in the 2,323 chart sample from arXiv corpus may not fully represent all chart usage scenarios
- Human performance benchmark of 80.5% lacks detailed inter-annotator agreement statistics
- Question generation process, while more realistic than templates, lacks systematic coverage metrics

## Confidence
- High confidence in GPT-4o vs open-source performance gap (47.1% vs 29.2%) on reasoning tasks
- Medium confidence in 80.5% human performance benchmark due to potential annotator pool bias
- High confidence that open-source models struggle with basic descriptive tasks based on reported error rates
- Medium confidence in benchmark's ecological validity due to uncertain representativeness of arXiv corpus

## Next Checks
1. Conduct inter-annotator agreement studies on a subset of charts to establish reliability of the 80.5% human performance baseline
2. Test model generalization by evaluating performance on charts from domains not represented in the arXiv training corpus
3. Perform ablation studies comparing performance on template-generated questions versus the manually created questions to quantify the impact of question realism on model evaluation