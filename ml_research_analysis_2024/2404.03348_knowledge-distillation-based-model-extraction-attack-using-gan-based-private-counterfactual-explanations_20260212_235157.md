---
ver: rpa2
title: Knowledge Distillation-Based Model Extraction Attack using GAN-based Private
  Counterfactual Explanations
arxiv_id: '2404.03348'
source_url: https://arxiv.org/abs/2404.03348
tags:
- data
- private
- explanations
- training
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the vulnerability of MLaaS platforms to
  model extraction attacks (MEA) facilitated by counterfactual explanations (CFs).
  The authors propose a novel knowledge distillation-based MEA approach that exploits
  CFs to train high-fidelity substitute models with fewer queries than baseline methods.
---

# Knowledge Distillation-Based Model Extraction Attack using GAN-based Private Counterfactual Explanations

## Quick Facts
- **arXiv ID**: 2404.03348
- **Source URL**: https://arxiv.org/abs/2404.03348
- **Reference count**: 40
- **Primary result**: KD-based MEA exploits CFs to train high-fidelity substitute models with fewer queries than baseline methods

## Executive Summary
This work investigates the vulnerability of MLaaS platforms to model extraction attacks (MEA) facilitated by counterfactual explanations (CFs). The authors propose a novel knowledge distillation-based MEA approach that exploits CFs to train high-fidelity substitute models with fewer queries than baseline methods. To mitigate this attack, they integrate differential privacy (DP) into the CF generator training pipeline, producing private CFs that preserve privacy at the cost of reduced CF quality. Experiments on real-world datasets demonstrate that the KD-based MEA outperforms direct training approaches in terms of agreement with the target model, while DP effectively mitigates MEA by reducing the effectiveness of CFs in exposing training data distribution.

## Method Summary
The proposed method combines knowledge distillation (KD) with counterfactual explanations (CFs) to extract target models deployed on MLaaS platforms. The attacker generates random queries, obtains predictions and CFs from the target model, and trains a substitute model using KD with both classification loss and Jensen-Shannon divergence. The CounterGAN architecture generates CFs that are boundary-proximal and informative for extraction. To mitigate the attack, DP is integrated into the CF generator training using DP Adam optimizer with gradient clipping and noise addition, producing private CFs that preserve privacy while maintaining reasonable explanation quality.

## Key Results
- KD-based MEA achieves higher agreement with target models compared to direct training approaches
- Including DP in CF generation effectively mitigates MEA but reduces CF quality and utility
- CFs provide more informative queries for MEA than random data points due to their proximity to decision boundaries
- The privacy-utility tradeoff shows that higher DP guarantees lead to less effective CFs for model extraction

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Knowledge Distillation (KD) improves MEA effectiveness by transferring the teacher model's probability distribution to the student model, not just class predictions.
- **Mechanism**: The student model is trained to minimize both classification loss and a distillation loss (Jensen-Shannon divergence) that encourages matching the target model's probability distribution. This allows the student to capture the nuanced decision boundaries rather than just hard labels.
- **Core assumption**: The target model's probability distribution contains more information about its decision boundaries than class labels alone.
- **Evidence anchors**: [abstract] "Our proposed KD-based MEA can yield a high-fidelity substitute model with a reduced number of queries"; [section] "The rationale for selecting KD as the extraction method lies in its unique capability to address the problem not only as a standard supervised classification task but also in its proficiency in precisely imitating the probability distribution"

### Mechanism 2
- **Claim**: Counterfactual explanations (CFs) provide more informative queries for MEA than random data points because they lie near the decision boundary and reveal model behavior patterns.
- **Mechanism**: CFs are generated by perturbing original data points to achieve different model predictions while minimizing changes. These points cluster near decision boundaries, exposing the model's behavior in critical regions that are most informative for extraction.
- **Core assumption**: Decision boundary proximity makes CFs more informative than random samples for understanding model behavior.
- **Evidence anchors**: [abstract] "Our findings reveal that including a privacy layer can allow mitigating the MEA. However, on the account of the quality of CFs, impacts the performance of the explanations"; [section] "CFs possess a dual nature, serving not only as interpretative tools but also as potential assets for enhancing attacks as they provide additional insights and knowledge for potentially exploiting the vulnerabilities of the model under attack"

### Mechanism 3
- **Claim**: Differential Privacy (DP) integration into CF generation mitigates MEA by adding calibrated noise that obscures training data patterns while preserving CF utility.
- **Mechanism**: DP is incorporated into the CF generator's optimization process using DP Adam optimizer with gradient clipping and noise addition. This ensures generated CFs don't closely resemble training data while maintaining reasonable explanation quality.
- **Core assumption**: The noise introduced by DP sufficiently obscures training data patterns without completely destroying CF utility.
- **Evidence anchors**: [abstract] "Our findings reveal that including a privacy layer can allow mitigating the MEA. However, on the account of the quality of CFs, impacts the performance of the explanations"; [section] "We propose an approach to integrating DP into the CF generation process and generating private CFs. Our methodology does not memorize or expose statistical information about the training set"

## Foundational Learning

- **Knowledge Distillation**
  - Why needed here: KD is the core extraction technique that allows the attacker to learn the target model's probability distribution, not just its predictions
  - Quick check question: What is the difference between student loss and distillation loss in KD?

- **Counterfactual Explanations**
  - Why needed here: CFs are the attack vector that provides boundary-proximal queries revealing model behavior
  - Quick check question: How do CFs differ from adversarial examples in their generation objective?

- **Differential Privacy**
  - Why needed here: DP is the mitigation technique that protects training data while maintaining CF utility
  - Quick check question: What is the relationship between ε, δ parameters and the strength of DP guarantees?

## Architecture Onboarding

- **Component map**: Target Model -> Counterfactual Generator -> Attacker System -> Knowledge Distillation Module -> Substitute Model
- **Critical path**: 1. Attacker generates random queries and sends to MLaaS API; 2. MLaaS returns predictions and CFs; 3. Attacker collects (query, CF, prediction) triples; 4. KD module trains substitute model using both classification and distillation losses; 5. DP layer (if enabled) modifies CF generator to produce private CFs
- **Design tradeoffs**: Query efficiency vs. model fidelity (more queries generally improve extraction quality); Privacy vs. utility (higher DP guarantees reduce CF quality and attack effectiveness); KD hyperparameters (α, temperature) affect balance between task accuracy and knowledge transfer
- **Failure signatures**: Low agreement despite many queries (CF generator may not be producing informative samples); Private CFs with unrealistic distributions (DP noise level may be too high); Substitute model overfitting to CFs (training data may not be diverse enough)
- **First 3 experiments**: 1. Baseline comparison: Run Direct training vs. KD-based training with identical query sets; 2. CF utility test: Compare MEA performance with CFs vs. random data points; 3. Privacy impact analysis: Measure MEA effectiveness with and without DP in CF generation

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the effectiveness of KD-based MEA with CFs compare to other model extraction attacks that do not rely on counterfactual explanations?
- **Basis in paper**: [explicit] The paper compares KD-based MEA with CFs to a baseline approach (Direct Train) that trains a threat model directly on extracted CFs.
- **Why unresolved**: The paper does not provide a comprehensive comparison with other model extraction attacks, such as those based on gradient-based explanations or transfer learning.
- **What evidence would resolve it**: A comparative study evaluating the effectiveness of KD-based MEA with CFs against other model extraction attacks on various datasets and model architectures.

### Open Question 2
- **Question**: What is the impact of different DP mechanisms (e.g., Gaussian noise, Laplace noise) on the quality of counterfactual explanations and the effectiveness of model extraction attacks?
- **Basis in paper**: [inferred] The paper uses DP Adam optimizer to incorporate DP into the CF generator training pipeline.
- **Why unresolved**: The paper does not explore the impact of different DP mechanisms on the quality of CFs and the effectiveness of MEA.
- **What evidence would resolve it**: An experimental study comparing the performance of MEA and the quality of CFs generated using different DP mechanisms on various datasets.

### Open Question 3
- **Question**: How does the effectiveness of KD-based MEA with CFs scale with the complexity of the target model (e.g., number of layers, number of parameters)?
- **Basis in paper**: [inferred] The paper uses a target model with 16 hidden layers and a threat model with 3 hidden layers.
- **Why unresolved**: The paper does not investigate how the effectiveness of KD-based MEA with CFs varies with the complexity of the target model.
- **What evidence would resolve it**: An experimental study evaluating the performance of MEA and the quality of CFs generated using KD-based MEA with CFs on target models of varying complexity.

## Limitations
- The paper lacks ablation studies showing how much each component (KD vs Direct training, CFs vs random queries, DP strength) contributes to the overall performance
- The specific CounterGAN architecture details are sparse, making exact reproduction challenging
- The DP implementation's impact on CF realism and actionability is not thoroughly quantified across different privacy budgets

## Confidence
- **High Confidence**: The theoretical foundation linking KD to improved MEA effectiveness and DP to CF privacy is well-established in the literature
- **Medium Confidence**: The empirical results showing KD superiority over Direct training are convincing, but absolute performance metrics lack comparison to state-of-the-art MEA methods
- **Low Confidence**: The DP mitigation effectiveness is demonstrated only at one parameter setting, limiting generalizability to other privacy budgets or threat models

## Next Checks
1. **Ablation Study**: Run experiments isolating the impact of KD by comparing KD-based MEA with and without CFs, and Direct training with and without CFs, to quantify each component's contribution
2. **DP Parameter Sweep**: Systematically vary ε and δ parameters in the DP implementation to map the privacy-utility tradeoff curve and identify optimal settings for different use cases
3. **Cross-Dataset Generalization**: Test the approach on additional datasets (beyond the three used) to validate whether the observed patterns hold across different data distributions and model complexities