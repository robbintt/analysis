---
ver: rpa2
title: 'Gemini Goes to Med School: Exploring the Capabilities of Multimodal Large
  Language Models on Medical Challenge Problems & Hallucinations'
arxiv_id: '2402.07023'
source_url: https://arxiv.org/abs/2402.07023
tags: []
core_contribution: Google's Gemini multimodal LLM was evaluated on medical reasoning,
  hallucination detection, and visual question answering benchmarks. While demonstrating
  broad medical knowledge, Gemini lagged behind state-of-the-art models like MedPaLM
  2 and GPT-4, achieving 61.45% accuracy on the VQA dataset compared to GPT-4V's 88%.
---

# Gemini Goes to Med School: Exploring the Capabilities of Multimodal Large Language Models on Medical Challenge Problems & Hallucinations

## Quick Facts
- **arXiv ID**: 2402.07023
- **Source URL**: https://arxiv.org/abs/2402.07023
- **Reference count**: 24
- **Primary result**: Google's Gemini multimodal LLM lags behind state-of-the-art models like MedPaLM 2 and GPT-4 on medical reasoning benchmarks, with high susceptibility to hallucinations and overconfidence

## Executive Summary
This study evaluates Google's Gemini multimodal large language model on medical reasoning, hallucination detection, and visual question answering tasks. Despite its broad medical knowledge, Gemini underperforms compared to specialized models like MedPaLM 2 and GPT-4, achieving only 61.45% accuracy on the VQA dataset versus GPT-4V's 88%. The model shows significant weaknesses in specialized domains like cardiology (26.67%) and dermatology (58.82%), while demonstrating strengths in biostatistics, cell biology, and epidemiology (100% accuracy). Advanced prompting techniques like chain-of-thought and ensemble refinement improved performance in some areas. A Python module and Hugging Face leaderboard were released to facilitate future research on this topic.

## Method Summary
The study evaluated Gemini Pro using the Google AI Studio API with temperature 0.0, max tokens 32,000 (text) or 12,000 (visual), top-p of 1.0, and high safety thresholds for medical content. Various prompting techniques were tested including zero-shot, few-shot, chain-of-thought, self-consistency, and ensemble refinement across multiple medical benchmarks: MultiMedQA (MedQA, MedMCQA, PubMedQA), Med-HALT hallucination tests, and a 100-question NEJM Image Challenge subset. Performance was measured using accuracy (correct predictions/total predictions) and pointwise score (combining positive scoring with penalties). Results were compared against baseline models including open-source (Llama, Mistral, Yi, Zephyr, Qwen, Meditron) and commercial models (MedPaLM, MedPaLM 2, GPT-4).

## Key Results
- Gemini achieved 61.45% accuracy on the VQA dataset compared to GPT-4V's 88%, significantly underperforming state-of-the-art models
- The model showed high susceptibility to hallucinations, overconfidence, and knowledge gaps, with subject-wise accuracy ranging from 26.67% (cardiology) to 100% (biostatistics, cell biology, epidemiology)
- Advanced prompting techniques like chain-of-thought and ensemble refinement improved performance in some areas but could not fully address fundamental knowledge gaps
- A Python module (RosettaEval) and Hugging Face leaderboard were released to enable community evaluation of LLMs on medical benchmarks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gemini's architecture, built on Transformer decoders and optimized for multimodal inputs, enables it to process medical data across text, images, and audio, providing a foundation for medical reasoning tasks.
- Mechanism: The model leverages attention mechanisms and context lengths up to 32,000 tokens to integrate complex medical information from diverse modalities, allowing it to handle multi-step diagnostic reasoning and visual question answering.
- Core assumption: The Transformer decoder architecture and attention mechanisms are sufficiently powerful to capture the nuanced relationships required for medical reasoning when trained on appropriate datasets.
- Evidence anchors:
  - [abstract] "Gemini (et al., 2023a) uses cutting-edge multimodal architecture. It is built on Transformer decoders and optimized for efficient and reliable performance at scale."
  - [section A.2.1] "Gemini combines text, graphics, and sounds seamlessly by utilizing distinct visual symbols and direct voice analysis."

### Mechanism 2
- Claim: Advanced prompting techniques like chain-of-thought and ensemble refinement improve Gemini's performance on complex medical tasks by guiding the model through structured reasoning processes.
- Mechanism: Chain-of-thought prompting augments few-shot examples with detailed reasoning paths, helping the model break down complex diagnostic problems into manageable steps. Ensemble refinement generates multiple responses and refines them iteratively, mimicking expert collaboration for robust analysis.
- Core assumption: LLMs can effectively leverage structured prompting to improve their reasoning capabilities, particularly when the prompts align with the underlying knowledge structure of the task.
- Evidence anchors:
  - [section A.2.9.3] "CoT (Wei et al., 2023) augments few-shot examples with detailed reasoning paths. This method is especially relevant for medical questions involving complex reasoning or multi-step problem-solving."
  - [section A.2.9.5] "As shown in the Figure A.3, Ensemble Refinement (ER) (et al., 2023d) first generates multiple responses and then refines them in a second stage, similar to experts brainstorming different perspectives before converging on an optimal solution."

### Mechanism 3
- Claim: Gemini's susceptibility to hallucinations and overconfidence in medical contexts stems from its tendency to generate plausible-sounding but factually incorrect information when faced with challenging questions or knowledge gaps.
- Mechanism: When the model encounters questions beyond its competence limits, it may fabricate connections or provide definitive answers without sufficient medical support, leading to potentially unsafe errors in diagnostic reasoning.
- Core assumption: LLMs trained on large datasets will naturally develop tendencies to fill knowledge gaps with plausible-sounding but potentially incorrect information, especially in specialized domains like medicine where precise knowledge is critical.
- Evidence anchors:
  - [abstract] "Our analysis revealed that Gemini is highly susceptible to hallucinations, overconfidence, and knowledge gaps, which indicate risks if deployed uncritically."
  - [section A.4.3.1] "In a clinical setting, overconfidence in diagnostics, as evidenced in the FCT, could lead to premature closure – a cognitive bias where a diagnosis is made without sufficient consideration of all possibilities."

## Foundational Learning

- Concept: Medical domain knowledge integration
  - Why needed here: Understanding how Gemini integrates medical knowledge from various sources is crucial for evaluating its performance on medical reasoning tasks and identifying potential knowledge gaps.
  - Quick check question: Can you explain how Gemini's architecture enables it to process and integrate information from medical texts, images, and clinical guidelines?

- Concept: Multimodal reasoning capabilities
  - Why needed here: Assessing Gemini's ability to combine visual and textual information is essential for evaluating its performance on medical visual question answering tasks and understanding its limitations in image-based diagnostics.
  - Quick check question: How does Gemini's multimodal architecture support the integration of visual information (like medical images) with textual medical knowledge for diagnostic reasoning?

- Concept: Hallucination detection and mitigation
  - Why needed here: Understanding Gemini's susceptibility to hallucinations and the techniques used to detect and mitigate them is crucial for evaluating its reliability in medical applications where factual accuracy is paramount.
  - Quick check question: What are the key differences between reasoning-based and memory-based hallucination tests, and how do they help identify different types of potential errors in medical LLMs?

## Architecture Onboarding

- Component map: Input → Multimodal Transformer decoder → Attention mechanisms → Context integration (up to 32,000 tokens) → Output generation → Multimodal response synthesis

- Critical path: Text/image/audio input → Multimodal processing → Attention-based information integration → Context window processing → Decoder layers → Output generation → Response refinement (if using advanced prompting)

- Design tradeoffs: Larger context lengths enable more complex reasoning but increase computational costs; multimodal design allows integrated processing but introduces complexity in aligning different input modalities; model size vs. efficiency trade-off for medical domain specialization

- Failure signatures: Hallucination in knowledge gaps, overconfidence in diagnostic reasoning, struggles with integrating visual and textual information for medical imaging tasks, difficulties with specialized medical domains like cardiology and dermatology

- First 3 experiments:
  1. Test Gemini's performance on a subset of MedQA questions using zero-shot prompting to establish baseline capabilities without any example guidance
  2. Evaluate the impact of chain-of-thought prompting on a set of complex diagnostic reasoning problems to measure improvements in multi-step reasoning
  3. Assess hallucination susceptibility using the Reasoning FCT test to quantify overconfidence tendencies in challenging diagnostic scenarios

## Open Questions the Paper Calls Out
None

## Limitations
- Gemini significantly underperforms state-of-the-art models in specialized medical domains, with accuracy below 60% in cardiology and dermatology
- The model shows high susceptibility to hallucinations and overconfidence, representing critical safety concerns for clinical deployment
- Performance may not generalize to real-world medical scenarios beyond the tested benchmarks due to limited domain coverage

## Confidence
- **High confidence**: Gemini's overall performance lags behind state-of-the-art models like MedPaLM 2 and GPT-4V across all evaluated benchmarks
- **Medium confidence**: The effectiveness of advanced prompting techniques (chain-of-thought, ensemble refinement) for medical reasoning tasks
- **Medium confidence**: The characterization of Gemini's hallucination susceptibility and overconfidence tendencies

## Next Checks
1. Replicate the evaluation using the open-sourced RosettaEval module with the exact few-shot examples to verify the reported performance metrics
2. Conduct a follow-up study testing Gemini's performance on clinical case studies and patient interactions to assess real-world applicability
3. Evaluate the effectiveness of additional hallucination detection and mitigation strategies, particularly for the most problematic domains (cardiology, dermatology)