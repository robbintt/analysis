---
ver: rpa2
title: Code Hallucination
arxiv_id: '2407.04831'
source_url: https://arxiv.org/abs/2407.04831
tags:
- code
- hallucination
- generation
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper identifies pervasive code hallucination in large language\
  \ models used for software generation, where models frequently produce incorrect,\
  \ irrelevant, or nonsensical outputs despite appearing syntactically valid. The\
  \ authors introduce HallTrigger, a prompt-based framework leveraging three dynamic\
  \ attributes of LLMs\u2014sequential prompting, meta-prompting with role-playing,\
  \ and reinforcement learning feedback\u2014to deliberately induce hallucinations\
  \ without accessing model internals."
---

# Code Hallucination

## Quick Facts
- arXiv ID: 2407.04831
- Source URL: https://arxiv.org/abs/2407.04831
- Reference count: 10
- Key outcome: Code hallucination is pervasive in LLMs for software generation, producing incorrect, irrelevant, or nonsensical outputs despite appearing syntactically valid

## Executive Summary
This paper identifies pervasive code hallucination in large language models used for software generation, where models frequently produce incorrect, irrelevant, or nonsensical outputs despite appearing syntactically valid. The authors introduce HallTrigger, a prompt-based framework leveraging three dynamic attributes of LLMs—sequential prompting, meta-prompting with role-playing, and reinforcement learning feedback—to deliberately induce hallucinations without accessing model internals. Experiments on ChatGPT (GPT-3.5, GPT-4), Google Gemini Advanced, and Microsoft Copilot show models consistently hallucinate across multiple scenarios, demonstrating that code hallucination is model-agnostic and widespread, posing significant risks to software development reliability.

## Method Summary
The study employs HallTrigger, a prompt-based framework that exploits three dynamic attributes of LLMs: sequential prompting (providing feedback within the same context to influence generation), meta-prompting with role-playing (having the model generate within dual personas to increase creative output), and reinforcement learning feedback (manipulating reward signals to influence model behavior). The framework tests three black-box LLMs—ChatGPT (GPT-3.5, GPT-4), Google Gemini Advanced, and Microsoft Copilot—across multiple hallucination scenarios including algorithm generation with impractical bounds, code bloating, imaginary methods, runtime errors, variable type mismatches, repetitive hallucinations, and code analysis flaws.

## Key Results
- Models consistently hallucinate across multiple scenarios: generating algorithms with impractical complexity bounds, introducing unused code dependencies, creating non-existent functions, and failing to detect subtle logical flaws
- Gemini exhibited persistent repetitive hallucination errors, while GPT-4 and Copilot showed vulnerabilities to variable type mismatches and inflated complexity prompts
- Code hallucination is model-agnostic and widespread, posing significant risks to software development reliability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HallTrigger leverages sequential prompting to manipulate LLM output by providing feedback in the same context, which impacts the model's generation path.
- Mechanism: Initial prompt generates code, then user provides positive or negative feedback on that generation. This influences the model to modify significant portions of the code or regenerate entirely to align with user sentiment.
- Core assumption: LLMs treat ongoing conversations as continuous context and adjust outputs based on feedback within the same session.
- Evidence anchors: [section] "To utilize the first factor we design sequential prompts where we initially ask the models to generate a code and later provide positive/negative feedback on its generation."
- Break condition: If the model resets context between prompts or if feedback is too vague to influence generation direction.

### Mechanism 2
- Claim: Meta-prompting with role-playing causes models to generate within dual personas, increasing creative output that may deviate from factual accuracy.
- Mechanism: User and model interact initially to set rules for the conversation, such as "I want you to work both as a user and an intelligent AI agent." The model then generates conversations between these roles, essentially replicating its generative process within both entities.
- Core assumption: The creative generation required for role-playing increases the likelihood of hallucination as the model prioritizes narrative coherence over factual accuracy.
- Evidence anchors: [section] "We conjecture that this largely instigates the creativity of the models and motivates newer and newer token generation ignoring factuality."
- Break condition: If the model refuses to engage in role-playing or if the meta-prompting becomes too restrictive to allow creative generation.

### Mechanism 3
- Claim: Reinforcement Learning from Human Feedback (RLHF) mechanisms can be manipulated by introducing scoring/reward systems that influence model behavior.
- Mechanism: User appends reward process within user-agent meta-prompts, such as "based on the code you generate you will be rewarded a score out of 10." The model then adapts its generation to maximize this reward score.
- Core assumption: LLMs trained with RLHF are sensitive to reward signals and will adjust their outputs to optimize for the highest score.
- Evidence anchors: [section] "Now depending on the direction of the generation (creative or more correct), the user can adapt next prompts and reward value in the same context so that the model is drifted towards a certain direction."
- Break condition: If the model ignores reward signals or if the scoring system becomes too complex for the model to optimize effectively.

## Foundational Learning

- Concept: Code generation fundamentals and common programming patterns
  - Why needed here: Understanding typical code structures helps identify when generated code deviates from expected patterns or contains nonsensical elements
  - Quick check question: What are the key differences between syntactically valid code and functionally correct code?

- Concept: Large Language Model architecture and autoregressive generation
  - Why needed here: Understanding how LLMs generate text token-by-token helps explain why they might produce outputs that seem plausible but are factually incorrect
  - Quick check question: How does the transformer architecture's attention mechanism influence the likelihood of hallucination?

- Concept: Software development lifecycle and testing methodologies
  - Why needed here: Evaluating code hallucination requires understanding how code is typically tested and validated in real-world scenarios
  - Quick check question: What are the limitations of unit testing when evaluating LLM-generated code?

## Architecture Onboarding

- Component map: Prompt generation system (sequential, meta-prompting, RLHF-based) -> LLM interface (ChatGPT, Gemini, Copilot) -> Code analysis tools (static and dynamic analysis) -> Hallucination detection system -> Result aggregation and reporting

- Critical path: Prompt generation → LLM response → Code analysis → Hallucination classification → Result reporting

- Design tradeoffs:
  - Manual vs automated prompt generation (manual allows expert insight but is labor-intensive)
  - Black box vs white box approach (black box is more realistic for deployed models but limits insight)
  - Comprehensive vs focused testing (comprehensive testing is more thorough but resource-intensive)

- Failure signatures:
  - Model refuses to generate code or stops mid-generation
  - Generated code consistently passes basic syntax checks but fails functional testing
  - Hallucination detection system produces high false positive rates

- First 3 experiments:
  1. Test sequential prompting by generating code for a simple function, then providing feedback that the code is inefficient and requesting a more optimal solution
  2. Implement meta-prompting by asking the model to role-play as both a developer and code reviewer, then evaluate the generated code for consistency
  3. Create an RLHF-style prompt that rewards code with specific characteristics (e.g., brevity, use of certain libraries) and measure how the model's output changes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can code hallucination be completely eliminated through architectural modifications to large language models?
- Basis in paper: [explicit] The paper states that "finding a complete, preemptive measure is also difficult as the process can be dynamic and remediation requires knowledge of a long context" and "it is proved that hallucination can not be completely removed (Xu et al., 2024)."
- Why unresolved: The paper acknowledges the inherent limitations of training data and the dynamic nature of hallucination generation, suggesting that complete elimination may be impossible.
- What evidence would resolve it: Comparative studies testing modified LLM architectures against baseline models on standardized hallucination benchmarks, demonstrating consistent improvement in hallucination detection and prevention.

### Open Question 2
- Question: How effective are automated detection systems compared to manual expert review in identifying code hallucinations across different programming languages?
- Basis in paper: [inferred] The paper mentions that "code hallucination can be partially remediated by analyzing the codes based on the ruleset of specific programming language and through the combination of static and dynamic analysis tools," suggesting potential for automated solutions.
- Why unresolved: The paper discusses both manual and automated approaches but doesn't provide direct comparison of their effectiveness across different programming languages.
- What evidence would resolve it: Empirical studies comparing automated hallucination detection tools with human expert reviews across multiple programming languages, measuring precision, recall, and false positive rates.

### Open Question 3
- Question: What is the relationship between model size and hallucination frequency in code generation tasks?
- Basis in paper: [explicit] The paper tests multiple models including GPT-3.5, GPT-4, Gemini Advanced, and Microsoft Copilot, observing different hallucination patterns across these models.
- Why unresolved: While the paper demonstrates model-agnostic hallucination patterns, it doesn't systematically analyze how hallucination frequency scales with model size or parameters.
- What evidence would resolve it: Controlled experiments varying model size while keeping other factors constant, measuring hallucination frequency across different task complexities and prompt types.

### Open Question 4
- Question: How does reinforcement learning feedback mechanism contribute to code hallucination generation, and can it be modified to reduce hallucination?
- Basis in paper: [explicit] The paper's HallTrigger framework specifically leverages "Reinforcement Learning from Human Feedback (RLHF) mechanism while training" and demonstrates how reward-based mechanisms can impact model responses.
- Why unresolved: While the paper shows that RLHF can be manipulated to trigger hallucinations, it doesn't explore whether the same mechanism can be adapted to prevent hallucinations.
- What evidence would resolve it: Experiments modifying RLHF training objectives and reward structures, measuring their impact on hallucination frequency and severity in code generation tasks.

## Limitations

- The study relies on black-box experimentation without accessing model internals, limiting understanding of why specific hallucinations occur
- The prompt engineering approach may not represent natural usage patterns where developers interact with these models
- The study focuses primarily on code generation scenarios and doesn't extensively explore hallucination patterns in code completion or debugging contexts

## Confidence

**High Confidence**: The observation that LLMs produce incorrect, irrelevant, or nonsensical code outputs is well-supported by the experimental results across multiple models and scenarios.

**Medium Confidence**: The claim that code hallucination poses significant risks to software development reliability is reasonable but requires further validation in real-world development contexts.

**Low Confidence**: The assertion that HallTrigger can reliably induce hallucinations across all tested models may be overstated, as different models showed varying vulnerability to different hallucination triggers.

## Next Checks

1. **Cross-domain validation**: Test HallTrigger's effectiveness on code generation tasks from different programming paradigms (functional, object-oriented, procedural) and domains (web development, data science, systems programming) to assess whether hallucination patterns are consistent across contexts.

2. **Long-term interaction analysis**: Conduct extended interaction sessions where developers engage with LLMs over multiple days or weeks, tracking how hallucination patterns evolve with continued use and whether models learn to avoid certain error types through feedback.

3. **Impact assessment in production**: Implement a controlled study where development teams use LLM-assisted coding with and without hallucination detection mechanisms, measuring actual code quality metrics, bug rates, and development velocity to quantify the real-world impact of code hallucination.