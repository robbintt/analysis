---
ver: rpa2
title: Achieving Fairness Across Local and Global Models in Federated Learning
arxiv_id: '2406.17102'
source_url: https://arxiv.org/abs/2406.17102
tags:
- fairness
- local
- learning
- clients
- client
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of achieving fairness in federated
  learning (FL) when client data contains sensitive attributes. The proposed method,
  EquiFL, incorporates a fairness term into the local optimization objective at each
  client to balance performance and fairness, while selectively updating model parameters
  during aggregation to prevent bias propagation.
---

# Achieving Fairness Across Local and Global Models in Federated Learning

## Quick Facts
- arXiv ID: 2406.17102
- Source URL: https://arxiv.org/abs/2406.17102
- Authors: Disha Makhija; Xing Han; Joydeep Ghosh; Yejin Kim
- Reference count: 40
- Primary result: Proposed EquiFL method achieves superior accuracy-fairness trade-offs in federated learning with test accuracies 82.9-83.8% and fairness metrics (Δ-DP) as low as 0.02-0.03

## Executive Summary
This paper addresses the challenge of achieving fairness in federated learning when client data contains sensitive attributes. The proposed method, EquiFL, incorporates a fairness term into the local optimization objective at each client to balance performance and fairness, while selectively updating model parameters during aggregation to prevent bias propagation. Extensive experiments on multiple datasets including Adult, COMPAS, Heritage Health, and a real-world healthcare application demonstrate that EquiFL achieves superior accuracy-fairness trade-offs compared to state-of-the-art methods. The approach maintains balanced fairness and performance across clients and shows consistent performance even with increasing numbers of clients, making it robust for real-world FL deployments where equitable outcomes are critical.

## Method Summary
EquiFL introduces a two-pronged approach to fairness in federated learning. First, it incorporates a fairness term into each client's local optimization objective, allowing clients to optimize for both accuracy and fairness simultaneously during local training. Second, it implements a selective parameter updating mechanism during global aggregation that prevents biased parameters from propagating across the federation. This selective updating ensures that the global model doesn't inherit biases present in any individual client's data. The method operates within the standard FL framework but modifies both the local objective function and the aggregation process to explicitly account for fairness metrics, particularly demographic parity.

## Key Results
- Achieved test accuracies of 82.9-83.8% across different datasets while maintaining fairness metrics (Δ-DP) as low as 0.02-0.03
- Demonstrated superior accuracy-fairness trade-offs compared to state-of-the-art methods on Adult, COMPAS, and Heritage Health datasets
- Showed consistent performance with increasing numbers of clients, proving scalability for real-world FL deployments
- Successfully applied to a real-world healthcare application, demonstrating practical utility

## Why This Works (Mechanism)
The method works by simultaneously addressing fairness at both the local and global levels. At the local level, incorporating fairness into the objective function ensures each client's model considers equity during training, preventing the development of client-specific biases. At the global level, selective parameter updating acts as a filter that prevents biased parameters from dominating the aggregated model. This dual approach ensures that fairness is maintained throughout the entire federated learning pipeline, from individual client updates to the final global model. The combination of these mechanisms creates a system where fairness is explicitly optimized rather than being an afterthought, leading to more equitable outcomes across all clients.

## Foundational Learning

- **Federated Learning**: Decentralized machine learning where multiple clients train models collaboratively without sharing raw data. Needed to understand the distributed nature of the problem and why traditional centralized fairness approaches don't work. Quick check: Can you explain the difference between horizontal and vertical FL?

- **Demographic Parity**: A fairness metric ensuring equal positive prediction rates across sensitive groups. Essential for measuring and optimizing fairness in classification tasks. Quick check: Can you calculate demographic parity difference between two groups given their prediction rates?

- **Local Objective Functions**: The optimization problems solved independently by each client. Critical for understanding how fairness can be incorporated into the learning process at the client level. Quick check: Can you write a local objective function that includes both accuracy and fairness terms?

## Architecture Onboarding

**Component Map**: Local clients -> Local training with fairness term -> Selective parameter filtering -> Global aggregation -> Global model

**Critical Path**: The most important sequence is local training incorporating fairness terms, followed by selective parameter filtering during aggregation. This path ensures that fairness is maintained throughout the learning process and prevents bias propagation.

**Design Tradeoffs**: The method trades some potential accuracy gains for fairness improvements. The selective parameter updating mechanism adds computational overhead but prevents bias propagation. The fairness term in local objectives may slow convergence but ensures equitable outcomes.

**Failure Signatures**: If the fairness term weight is too low, the model may converge to biased solutions. If selective updating is too aggressive, it may prevent useful parameter updates and harm overall performance. If the method is applied to highly non-IID data without proper tuning, fairness improvements may be uneven across clients.

**3 First Experiments**:
1. Reproduce baseline results on Adult dataset to establish comparison point
2. Test EquiFL on a synthetic dataset with known bias to verify fairness improvements
3. Evaluate performance degradation when fairness term weight is varied to understand sensitivity

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental results are primarily evaluated on four datasets, raising questions about generalization to other domains
- Computational overhead of selective parameter updating mechanism is not extensively discussed
- Limited validation in production-like environments or with complex sensitive attributes beyond those tested
- Paper lacks detailed analysis of method's effectiveness in highly non-IID data distributions

## Confidence

- **High Confidence**: Empirical results showing improved accuracy-fairness trade-offs on tested datasets; consistent performance across increasing numbers of clients
- **Medium Confidence**: Prevention of bias propagation through selective parameter updates; effectiveness in diverse data distributions
- **Low Confidence**: Claims of "equitable outcomes" in real-world FL deployments; long-term fairness maintenance

## Next Checks

1. Conduct scalability and efficiency analysis to measure computational overhead and communication costs in large-scale FL settings with hundreds or thousands of clients

2. Evaluate performance on datasets with highly non-IID distributions and diverse sensitive attributes to ensure effectiveness across varied real-world scenarios

3. Perform longitudinal studies to assess whether fairness improvements are maintained over extended periods of training and under dynamic data distributions