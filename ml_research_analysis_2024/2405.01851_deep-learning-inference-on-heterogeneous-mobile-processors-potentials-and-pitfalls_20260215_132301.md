---
ver: rpa2
title: 'Deep Learning Inference on Heterogeneous Mobile Processors: Potentials and
  Pitfalls'
arxiv_id: '2405.01851'
source_url: https://arxiv.org/abs/2405.01851
tags:
- inference
- parallel
- mobile
- processors
- uni00000003
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper investigates parallel deep learning inference on heterogeneous\
  \ mobile processors (CPU, GPU, DSP, NPU) across 8 commercial mobile devices. The\
  \ study evaluates representative parallel inference strategies (Mace, \u03BCLayer,\
  \ CoDL, NN-Stretch) using various models (YOLOv2, VGG-16, PoseNet, etc.) under different\
  \ workload patterns and resource availability scenarios."
---

# Deep Learning Inference on Heterogeneous Mobile Processors: Potentials and Pitfalls

## Quick Facts
- arXiv ID: 2405.01851
- Source URL: https://arxiv.org/abs/2405.01851
- Reference count: 34
- Primary result: Parallel inference strategies face challenges with unsupported operators, scheduling granularity, and mobile resource dynamics

## Executive Summary
This paper investigates parallel deep learning inference on heterogeneous mobile processors (CPU, GPU, DSP, NPU) across 8 commercial mobile devices. The study evaluates representative parallel inference strategies using various models under different workload patterns and resource availability scenarios. Key findings reveal that existing parallel inference strategies face significant challenges in real-world mobile contexts, including unsupported operators causing processor fallbacks, the need for cross-level optimization to achieve latency reduction, and the importance of runtime profiling over offline estimation due to mobile resource dynamics.

## Method Summary
The study evaluates parallel deep learning inference strategies across 8 mobile devices with different SoCs, using 8 DL models and 6 parallel inference strategies. Experiments measure inference latency, accuracy, memory usage, speedup, unsupported operator ratio, frame drop rate, and cache hit rate. The research compares single-processor vs. multi-processor execution, analyzes operator support across processors, tests cross-level optimization opportunities, and evaluates scheduling granularity impacts. Mobile resource dynamics are simulated across five conditions to assess runtime profiling effectiveness.

## Key Results
- Unsupported operators cause significant processor fallbacks, with ResNet-50 on Xiaomi 9 showing approximately 48% fallback ratio during CPU+GPU parallel inference
- Cross-level optimization combining frontend compilation techniques with backend compilation methods achieved up to 48.4% latency reduction
- Overly fine-grained scheduling can cause process suspensions, while coarse-grained scheduling may leave cores idle
- Runtime latency profiling is necessary due to mobile resource dynamics, with offline estimation errors ranging from 25% to 126% under dynamic conditions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-level optimization combining frontend and backend compilation improves latency more than backend-only approaches.
- Mechanism: Frontend optimizations (pruning, low-rank decomposition) reduce model size and computational complexity before backend compilation. Backend optimizations (operator fusion, memory allocation) then operate on a smaller, more efficient graph, achieving greater latency reduction than backend alone.
- Core assumption: Frontend optimizations preserve sufficient accuracy to enable backend optimizations to operate effectively on the reduced model.
- Evidence anchors:
  - [abstract] "Cross-level optimization combining frontend compilation techniques (pruning, low-rank decomposition) with backend compilation methods (operator fusion, memory allocation) achieved up to 48.4% latency reduction."
  - [section] "Incorporating frontend compilation-level optimizations, such as pruning, can improve resource utilization and accelerate DL inference at the backend."
  - [corpus] Weak evidence: No direct citations about cross-level optimization effectiveness, but related papers discuss heterogeneous scheduling and processor utilization.
- Break condition: If frontend optimizations degrade accuracy below acceptable thresholds, the backend optimizations cannot compensate, negating cross-level benefits.

### Mechanism 2
- Claim: Runtime latency profiling is necessary due to dynamic mobile resource conditions.
- Mechanism: Mobile devices experience varying temperatures, competing processes, and cache availability that offline profiling cannot capture. Runtime profiling adapts scheduling decisions to current conditions, reducing latency prediction errors.
- Core assumption: The overhead of runtime profiling is less than the latency gains from adaptive scheduling.
- Evidence anchors:
  - [abstract] "Mobile resource dynamics necessitates runtime latency profiling rather than offline estimation, with latency prediction errors ranging from 25% to 126% under dynamic conditions."
  - [section] "We simulate five resource conditions σ0 ~ σ4 commonly seen in mobile systems... Actual and estimated latency diverge significantly under dynamic conditions."
  - [corpus] No direct evidence in corpus papers about runtime profiling, but related work discusses energy-latency attacks and predictive models.
- Break condition: If runtime profiling overhead exceeds the benefits of dynamic adaptation, or if mobile conditions are sufficiently stable that offline estimates remain accurate.

### Mechanism 3
- Claim: Fine-grained parallel scheduling causes process suspensions and blocks due to increased data transmission delays.
- Mechanism: When scheduling granularity is too fine, the overhead of managing many small tasks and transmitting intermediate results between processors exceeds the benefits of parallelism, causing processes to suspend and block.
- Core assumption: The communication overhead between processors scales with the number of parallel tasks, and fine granularity increases this overhead disproportionately.
- Evidence anchors:
  - [abstract] "overly fine-grained scheduling can cause process suspensions, while coarse-grained scheduling may leave cores idle."
  - [section] "An overly fine granularity may cause process suspensions and blocks due to increased data transmission delays."
  - [corpus] No direct evidence in corpus papers about scheduling granularity effects, but related work discusses scheduling multiple DNNs on heterogeneous processors.
- Break condition: If communication overhead is reduced through better interconnect or if processors have sufficient memory to hold intermediate results locally, fine-grained scheduling may become beneficial.

## Foundational Learning

- Concept: Heterogeneous processor architecture (CPU, GPU, DSP, NPU)
  - Why needed here: Understanding the capabilities and limitations of each processor type is crucial for effective parallel inference scheduling and identifying unsupported operators.
  - Quick check question: What are the primary differences between CPU, GPU, DSP, and NPU in terms of computational strengths and supported operations?

- Concept: Backend compilation optimization (operator fusion, memory allocation)
  - Why needed here: These techniques directly impact inference latency and are key components of the cross-level optimization strategy.
  - Quick check question: How does operator fusion reduce latency, and what are the conditions that enable successful fusion?

- Concept: Mobile system dynamics (temperature, competing processes, cache availability)
  - Why needed here: These factors cause the significant latency prediction errors that motivate runtime profiling over offline estimation.
  - Quick check question: How do temperature changes affect GPU performance through dynamic voltage and frequency scaling?

## Architecture Onboarding

- Component map: Frontend compilation (model optimization) -> Backend compilation (graph optimization, operator distribution, memory allocation) -> Runtime profiling. Mobile devices provide heterogeneous processors (CPU, GPU, DSP, NPU) with varying capabilities.
- Critical path: Model compilation → Graph optimization → Operator distribution → Memory allocation → Runtime scheduling. Latency bottlenecks occur at operator distribution (unsupported operators) and runtime scheduling (granularity issues).
- Design tradeoffs: Fine-grained vs. coarse-grained scheduling balances core utilization against communication overhead. Frontend vs. backend optimization balances accuracy preservation against latency reduction.
- Failure signatures: High frame drop rates indicate excessive GPU utilization. Process suspensions indicate overly fine-grained scheduling. Significant latency prediction errors indicate inadequate runtime profiling.
- First 3 experiments:
  1. Test parallel inference with varying scheduling granularities (sub-graph, inter-operator, intra-operator, task) on ResNet-50 with competing processes.
  2. Compare latency prediction errors between offline and runtime profiling under different temperature and cache conditions.
  3. Evaluate cross-level optimization by combining pruning with operator fusion on ResNet-18 and measure accuracy-latency tradeoffs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we systematically design cross-level optimization frameworks that integrate frontend compilation techniques (pruning, low-rank decomposition) with backend compilation methods (operator fusion, memory allocation) to maximize parallel inference efficiency across heterogeneous mobile processors?
- Basis in paper: [explicit] The paper explicitly identifies "Cross-Level Optimization Opportunities" and demonstrates that combining frontend compilation-level optimizations with backend compilation-level optimizations can achieve up to 48.4% latency reduction.
- Why unresolved: While the paper demonstrates the potential of cross-level optimization through specific combinations, it does not provide a systematic framework or methodology for determining which frontend and backend optimizations should be combined, in what order, and under what conditions.
- What evidence would resolve it: A comprehensive study testing all possible combinations of frontend and backend optimizations across diverse model-device pairs, along with a machine learning-based recommendation system that predicts optimal cross-level optimization strategies based on model characteristics and device specifications.

### Open Question 2
- Question: What are the optimal scheduling granularity strategies for parallel inference across heterogeneous mobile processors under varying competing process workloads?
- Basis in paper: [explicit] The paper's "Performance of parallel strategies with diverse scheduling granularity" section shows that "overly fine-grained scheduling can cause process suspensions, while coarse-grained scheduling may leave cores idle."
- Why unresolved: The study only tests a limited set of granularity levels and a small number of competing process scenarios. The paper doesn't provide a methodology for determining the optimal granularity dynamically based on real-time system conditions, nor does it explore intermediate granularity levels or hybrid approaches.
- What evidence would resolve it: Empirical data showing latency and resource utilization across a comprehensive range of granularity levels and competing process scenarios, along with a dynamic scheduling algorithm that can adaptively adjust granularity based on real-time system metrics.

### Open Question 3
- Question: How can runtime latency profiling be implemented to accurately account for non-stationary dynamics in mobile environments, and what are the most effective predictive models for latency estimation under varying resource conditions?
- Basis in paper: [explicit] The paper demonstrates that "offline estimation" methods have significant errors ranging from 25% to 126% under dynamic conditions, and concludes that "Mobile resource dynamics necessitates runtime latency profiling rather than offline estimation."
- Why unresolved: The paper only tests five static resource conditions and uses simple comparison metrics. It doesn't explore continuous real-time profiling mechanisms, nor does it evaluate advanced machine learning models for dynamic latency prediction.
- What evidence would resolve it: Implementation and evaluation of real-time latency profiling systems that continuously monitor mobile device metrics and use advanced predictive models to estimate inference latency with quantified accuracy under diverse dynamic conditions.

## Limitations

- Study relies on simulation of mobile resource conditions rather than real-world dynamic measurements
- Does not provide detailed information about specific parallel inference strategy implementations
- Cross-level optimization results depend on specific combinations that may not generalize across all model architectures

## Confidence

- High confidence: Unsupported operators causing processor fallbacks
- Medium confidence: Cross-level optimization effectiveness
- Medium confidence: Runtime profiling necessity claims

## Next Checks

1. Implement and measure actual runtime profiling overhead vs. gains on real mobile devices with varying temperature conditions, comparing against offline profiling predictions.
2. Test the impact of scheduling granularity by systematically varying task sizes and measuring communication overhead and core utilization across different model architectures.
3. Reproduce cross-level optimization experiments by implementing the same pruning and operator fusion combinations on ResNet-18/50, measuring both accuracy retention and latency improvements.