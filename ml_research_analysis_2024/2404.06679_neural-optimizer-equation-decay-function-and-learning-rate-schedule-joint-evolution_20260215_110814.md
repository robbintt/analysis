---
ver: rpa2
title: Neural Optimizer Equation, Decay Function, and Learning Rate Schedule Joint
  Evolution
arxiv_id: '2404.06679'
source_url: https://arxiv.org/abs/2404.06679
tags:
- learning
- optimizers
- decay
- rate
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a new dual-joint search space for Neural Optimizer
  Search (NOS) that simultaneously optimizes the weight update equation, internal
  decay functions, and learning rate schedules. The authors propose a mutation-only,
  particle-based genetic algorithm to search the space, along with an integrity check
  to eliminate degenerate optimizers.
---

# Neural Optimizer Equation, Decay Function, and Learning Rate Schedule Joint Evolution

## Quick Facts
- arXiv ID: 2404.06679
- Source URL: https://arxiv.org/abs/2404.06679
- Reference count: 40
- One-line primary result: Discovered optimizers consistently outperformed Adam and other standard optimizers across image classification and fine-tuning tasks.

## Executive Summary
This paper presents a novel dual-joint search space for Neural Optimizer Search (NOS) that simultaneously optimizes the weight update equation, internal decay functions, and learning rate schedules. The authors propose a mutation-only, particle-based genetic algorithm to search the space, along with an integrity check to eliminate degenerate optimizers. The final optimizers were evaluated on CIFAR-10, CIFAR-100, TinyImageNet, and fine-tuned on Flowers102, Cars196, and Caltech101 using EfficientNetV2Small. The results show that the discovered optimizers consistently outperformed Adam and other standard deep learning optimizers across image recognition tasks.

## Method Summary
The method employs a particle-based genetic algorithm with mutation-only operations to search a dual-joint space of optimizer equations, decay functions, and learning rate schedules. An integrity check filters out degenerate optimizers using a shifted sphere problem before evaluation on a small ConvNet surrogate. The search uses 50 independent particles, each performing 6 mutations per timestep over 6 timesteps. Top performers are progressively evaluated on larger models and fine-tuning tasks.

## Key Results
- Opt6 and Opt10 achieved best performance on training from scratch tasks
- A1 and A5 excelled in fine-tuning scenarios
- All discovered optimizers outperformed Adam, RMSProp, SGD, Nesterov momentum, PowerSign, AddSign, and QHM across image classification tasks
- Supplementary experiments on PTB language modeling showed Nesterov's momentum and Opt101 achieved the best results

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dual-joint search space enables simultaneous optimization of weight updates, decay functions, and learning rate schedules, leading to optimizers that adapt better to complex training dynamics.
- Mechanism: By representing optimizers as computational graphs with interchangeable operations, the search can discover non-trivial combinations of momentum, gradient normalization, and decay schedules that standard hand-designed optimizers miss.
- Core assumption: The space of possible optimizer behaviors is large enough that meaningful improvements exist over current best practices, and the genetic algorithm can effectively explore this space without getting stuck.
- Evidence anchors:
  - [abstract] "Our dual-joint search space simultaneously allows for the optimization of not only the update equation, but also internal decay functions and learning rate schedules for optimizers."
  - [section] "Unlike Bello et al. (2017), we do not limit our decay schedules to decay only, but also include their increase as well."
- Break condition: If the search space becomes too sparse (few viable optimizers), the algorithm will converge prematurely to suboptimal solutions.

### Mechanism 2
- Claim: Integrity checking during evolution prevents degenerate optimizers from consuming computational resources, enabling efficient exploration of the search space.
- Mechanism: Each candidate optimizer is tested on a shifted sphere optimization problem; if it fails to achieve moderate convergence, it is rejected before full training. This ensures only optimizers with basic optimization capability proceed.
- Core assumption: Performance on the shifted sphere problem correlates well enough with performance on real deep learning tasks to serve as a useful filter.
- Evidence anchors:
  - [section] "To prevent degenerate optimizers from wasting valuable evaluation time, an integrity check was implemented from the motivation of Liu et al. (2021) in their evolution of loss functions."
- Break condition: If the integrity check is too strict or too lenient, it may either eliminate promising optimizers or allow poor ones to proceed, degrading final results.

### Mechanism 3
- Claim: The particle-based genetic algorithm with mutation-only and aging allows for efficient exploration of a sparse search space.
- Mechanism: Independent particles perform multiple mutations per timestep, selecting the best child as the next position. This structure allows parallel exploration around each optimizer, avoiding premature convergence seen in regularized evolution.
- Core assumption: The search space is so sparse that local minima are sharp and exploration must be broad to find good optimizers; independent particles prevent competition-induced convergence.
- Evidence anchors:
  - [section] "We believe that this occurrence was due to our extremely sparse search space, paired with our integrity check, creating an extremely sharp local minima for each optimizer."
- Break condition: If the mutation rate is too high, the search may become random; if too low, it may get stuck in local minima.

## Foundational Learning

- Concept: Genetic algorithms and evolutionary search
  - Why needed here: The optimizer search space is too large and non-differentiable for gradient-based methods; GA provides a robust alternative for combinatorial optimization.
  - Quick check question: What is the key difference between a mutation-only GA and one that also uses crossover?

- Concept: Computational graph representation of optimizers
  - Why needed here: Allows flexible composition of operations (gradient, momentum, decay) without hard-coding specific update rules, enabling discovery of novel optimizer forms.
  - Quick check question: How does a computational graph differ from a traditional hand-designed optimizer update equation?

- Concept: Learning rate schedules and decay functions
  - Why needed here: Static learning rates are often suboptimal; adaptive schedules can improve convergence and generalization across tasks.
  - Quick check question: Why might a cosine decay schedule be preferable to a linear decay in some training scenarios?

## Architecture Onboarding

- Component map: Initial optimizers -> Integrity check -> Surrogate evaluation -> Early stopping -> Particle-based GA evolution -> Progressive elimination -> Final evaluation on EfficientNetV2Small and fine-tuning tasks
- Critical path:
  1. Initialize random optimizers and decay functions
  2. Apply integrity check to filter out degenerate candidates
  3. Evaluate survivors on surrogate function with early stopping
  4. Evolve population via mutation-only GA with parallel particles
  5. After evolution, perform progressive elimination on larger models
  6. Final evaluation on EfficientNetV2Small and fine-tuning tasks
- Design tradeoffs:
  - Mutation-only vs. crossover: Simpler implementation, avoids disrupting good sub-structures, but may explore less efficiently.
  - Particle-based vs. regularized evolution: Better for sparse search spaces, allows parallel exploration, but requires more particles.
  - Integrity check strictness: Balances filtering out bad optimizers vs. not rejecting promising ones.
- Failure signatures:
  - If integrity check is too strict: Very few optimizers pass, evolution stalls.
  - If early stopping thresholds are too low: Poor optimizers proceed, wasting resources.
  - If mutation rate is too high: Evolution becomes random, no convergence.
  - If surrogate function is poorly correlated: Final optimizers don't transfer well to large-scale tasks.
- First 3 experiments:
  1. Run the integrity check on a small set of randomly generated optimizers to verify it filters as expected.
  2. Train a known optimizer (e.g., Adam) on the surrogate function to establish a baseline fitness value.
  3. Run a short evolutionary trial (few generations) to confirm mutation operations work and particles evolve.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the discovered optimizers perform on datasets with significantly different characteristics, such as medical imaging or satellite imagery?
- Basis in paper: [inferred] The paper evaluates the optimizers on image classification tasks like CIFAR, TinyImageNet, and fine-tuning on Flowers102, Cars196, and Caltech101. It does not explore performance on datasets with significantly different characteristics.
- Why unresolved: The paper's experiments are limited to specific image classification tasks, and there is no analysis of how the optimizers generalize to other types of image data with different characteristics.
- What evidence would resolve it: Evaluating the discovered optimizers on datasets with different characteristics, such as medical imaging or satellite imagery, and comparing their performance to standard optimizers.

### Open Question 2
- Question: How do the discovered optimizers compare to other state-of-the-art optimizers that were developed after the search space was defined?
- Basis in paper: [inferred] The paper compares the discovered optimizers to standard optimizers like Adam, RMSProp, SGD, and Nesterov momentum, but does not mention any optimizers developed after the search space was defined.
- Why unresolved: The paper does not include a comparison to more recent optimizers that may have been developed after the search space was defined, which could potentially outperform the discovered optimizers.
- What evidence would resolve it: Conducting experiments to compare the performance of the discovered optimizers against state-of-the-art optimizers developed after the search space was defined.

### Open Question 3
- Question: How does the choice of the integrity check impact the quality and diversity of the discovered optimizers?
- Basis in paper: [explicit] The paper mentions an integrity check to eliminate degenerate optimizers, but does not provide a detailed analysis of how the choice of the integrity check impacts the final results.
- Why unresolved: The paper does not explore the impact of different integrity checks on the quality and diversity of the discovered optimizers, leaving uncertainty about the robustness of the results to the choice of the integrity check.
- What evidence would resolve it: Conducting experiments with different integrity checks and analyzing their impact on the quality and diversity of the discovered optimizers.

## Limitations
- The correlation between surrogate fitness and final task performance remains unquantified, creating uncertainty about search efficiency
- Integrity check mechanism may inadvertently filter out optimizers that perform well only on complex, high-dimensional loss surfaces
- Comparison against only a limited set of hand-designed optimizers leaves room for stronger baselines

## Confidence
- Main claim (discovered optimizers outperform Adam and other standard optimizers): Medium
- Integrity check effectiveness: Medium
- Surrogate function correlation: Low

## Next Checks
1. **Surrogate correlation validation**: Systematically measure how well optimizer fitness on the small ConvNet surrogate predicts performance on EfficientNetV2Small and fine-tuning tasks. This would quantify the reliability of the evolutionary search process.
2. **Ablation on integrity checking**: Run evolutionary searches with and without the integrity check to measure its impact on final optimizer quality and search efficiency. This would validate whether the check is necessary or potentially too restrictive.
3. **Extended baseline comparison**: Include recent learned optimizers from literature (e.g., from Liu et al. 2021, or other NAS-based approaches) as additional baselines to strengthen the claim of state-of-the-art performance.