---
ver: rpa2
title: Lifelong Knowledge Editing for Vision Language Models with Low-Rank Mixture-of-Experts
arxiv_id: '2411.15432'
source_url: https://arxiv.org/abs/2411.15432
tags:
- editing
- edit
- language
- liveedit
- visual
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of lifelong knowledge editing
  for Vision-Language Models (VLLMs), which existing methods designed for pure Large
  Language Models (LLMs) cannot directly adapt to due to the additional visual modality.
  The proposed LiveEdit framework introduces a lifelong vision language model editing
  approach that bridges this gap.
---

# Lifelong Knowledge Editing for Vision Language Models with Low-Rank Mixture-of-Experts

## Quick Facts
- arXiv ID: 2411.15432
- Source URL: https://arxiv.org/abs/2411.15432
- Reference count: 40
- Achieved 95.36% (±0.57) average reliability, generality, and locality on E-VQA with LLaVA (7B) after 1000 edits

## Executive Summary
This paper introduces LiveEdit, a framework for lifelong knowledge editing of Vision-Language Models (VLLMs) that addresses the gap between existing LLM editing methods and VLLM requirements. The core innovation is a mixture-of-experts approach that generates low-rank experts for each editing instance and routes them using both visual and textual semantic relevance. Experiments demonstrate significant improvements over baseline methods across multiple benchmarks, achieving state-of-the-art performance in reliability, generality, and locality metrics for VLLM editing.

## Method Summary
LiveEdit trains an editing expert generator to create low-rank experts from concatenated representations of edit samples (image, prompt, target output) using cross-attention operations. The framework employs a two-stage routing mechanism: hard filtering eliminates visually irrelevant experts based on semantic similarity, followed by soft routing that fuses multiple experts using combined absolute and relative similarity weights. The system stores generated experts in a repository and updates them incrementally as new edits arrive. Training involves optimizing both edit accuracy and routing feature consistency through contrastive learning, with experiments conducted across three VLLM backbones (LLaVA-V1.5, BLIP2-OPT, MiniGPT-4) on multiple benchmarks.

## Key Results
- Achieved 95.36% (±0.57) average reliability, generality, and locality on E-VQA with LLaVA (7B) after 1000 edits
- Demonstrated significant improvements over baseline editing methods across all three tested VLLM architectures
- Showed consistent performance gains in both text-based and multimodal editing scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The expert generator creates low-rank experts that correct specific VLLM responses without affecting unrelated knowledge
- Mechanism: A trainable module feg extracts editing signals from the concatenated representations of the edit sample (image, prompt, and target output) and generates a low-rank expert pair (Uet, Vet) using cross-attention operations
- Core assumption: Each edit sample can be independently modeled as a sub-task that requires a specialized expert
- Evidence anchors:
  - [abstract]: "We begin by training an editing expert generator to independently produce low-rank experts for each editing instance"
  - [section 4.1]: "we propose an expert generator that creates low-rank experts for each new edit sample"
  - [corpus]: Weak - no direct corpus evidence found for this specific expert generation mechanism
- Break condition: If the expert generator fails to isolate edit-specific knowledge from general knowledge, or if the cross-attention mechanism cannot capture sufficient edit information

### Mechanism 2
- Claim: Hard routing filters out visually irrelevant experts while preserving those relevant to the input sample's visual content
- Mechanism: Extracts key visual features from both input and edit samples using prompt semantics, then filters experts based on similarity between these features using a dynamic threshold from a visual sentinel
- Core assumption: Visual semantic similarity between input and edit samples is a reliable indicator of expert relevance
- Evidence anchors:
  - [abstract]: "A hard filtering mechanism is developed to utilize visual semantic knowledge, thereby coarsely eliminating visually irrelevant experts"
  - [section 4.2]: "We filter for experts that are highly relevant to the input sample's visual content by calculating the similarity between the key visual features"
  - [corpus]: Weak - no direct corpus evidence found for this specific hard routing mechanism
- Break condition: If visual similarity does not correlate with semantic relevance, or if the sentinel threshold becomes unreliable across diverse visual inputs

### Mechanism 3
- Claim: Soft routing achieves multi-expert fusion by weighting experts based on both absolute similarity and relative ranking among candidates
- Mechanism: Combines sigmoid-based absolute weights (controlling individual expert output strength) with softmax-based relative weights (balancing among selected experts) to produce final adapted representations
- Core assumption: Combining absolute and relative similarity measures prevents both under- and over-weighting of individual experts
- Evidence anchors:
  - [abstract]: "we introduce a soft routing mechanism based on textual semantic relevance to achieve multi-expert fusion"
  - [section 4.2]: "The Soft Routing function fsr(·) multiplies absolute weights from the sigmoid and relative weights from the softmax"
  - [corpus]: Weak - no direct corpus evidence found for this specific soft routing mechanism
- Break condition: If the combination of absolute and relative weights fails to properly balance expert contributions, leading to degraded performance

## Foundational Learning

- Concept: Low-rank adaptation (LoRA)
  - Why needed here: Enables efficient parameter updates by decomposing weight changes into low-rank matrices, reducing computational cost while maintaining model capacity
  - Quick check question: What is the computational advantage of using low-rank matrices versus full-rank matrices for parameter updates?

- Concept: Cross-attention mechanisms
  - Why needed here: Allows the expert generator to extract relevant information from edit samples by attending to specific tokens in the concatenated representation
  - Quick check question: How does cross-attention differ from self-attention in terms of information flow between query and key/value pairs?

- Concept: Mixture-of-Experts (MoE) routing strategies
  - Why needed here: Provides the framework for selecting and combining multiple specialized experts based on input characteristics
  - Quick check question: What are the key differences between hard routing, soft routing, and top-k routing in MoE architectures?

## Architecture Onboarding

- Component map: Expert Generator (feg) -> Feature Extractors (ˆff e, ¯ff e) -> Hard Router -> Soft Router -> Expert Repository
- Critical path: Edit sample → Expert generation → Repository update → Input sample → Feature extraction → Hard routing → Soft routing → Adapted output
- Design tradeoffs:
  - Memory vs Performance: Higher expert rank r increases repository size but may improve edit quality
  - Routing complexity vs Efficiency: Hard routing reduces computation but may miss relevant experts; soft routing is more comprehensive but computationally heavier
  - Feature extraction dimension k affects noise level vs information capture
- Failure signatures:
  - Poor reliability: Experts not properly isolating edit knowledge
  - Low generality: Experts overfitting to specific edit samples
  - Weak locality: Experts affecting unrelated knowledge
  - Slow inference: Inefficient routing or excessive expert combinations
- First 3 experiments:
  1. Verify expert generation: Feed an edit sample through feg and confirm the generated expert modifies the VLLM's response to match the target output
  2. Test hard routing: Provide an input sample and verify that only visually similar experts are selected from the repository
  3. Validate soft routing: Confirm that selected experts receive appropriate fusion weights based on both visual and textual similarity measures

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed hard and soft routing mechanism in LiveEdit compare to other routing strategies in terms of computational efficiency during inference?
- Basis in paper: [explicit] The paper mentions that the soft routing function multiplies absolute and relative weights to control the output strength of each expert, but does not provide a detailed analysis of the computational overhead compared to other routing strategies.
- Why unresolved: The paper focuses on the effectiveness of the routing mechanisms in terms of edit performance but does not delve into the computational efficiency aspect.
- What evidence would resolve it: A comparative analysis of the computational overhead of LiveEdit's routing mechanism versus other routing strategies during inference would provide clarity.

### Open Question 2
- Question: What is the impact of the module dimension (dm) and expert rank (r) on the scalability of LiveEdit when applied to larger VLLMs?
- Basis in paper: [explicit] The paper discusses the impact of dm and r on edit performance and model scale, suggesting that dm predominantly influences variation while r has minimal impact. However, it does not explore the scalability implications for larger VLLMs.
- Why unresolved: The paper provides insights into the effects of dm and r on edit performance but does not extend the analysis to larger VLLMs, leaving scalability questions unanswered.
- What evidence would resolve it: Experiments evaluating LiveEdit's performance and scalability with larger VLLMs and varying dm and r values would address this question.

### Open Question 3
- Question: How does LiveEdit handle the integration of new edit samples in real-time applications where the model needs to be continuously updated?
- Basis in paper: [inferred] The paper introduces LiveEdit as a framework for lifelong VLLM editing, suggesting its capability to handle continuous updates. However, it does not provide specific details on real-time integration processes.
- Why unresolved: While the framework is designed for lifelong editing, the paper does not explicitly discuss the mechanisms for real-time integration of new edit samples.
- What evidence would resolve it: A detailed explanation or experimental results demonstrating LiveEdit's real-time integration capabilities in dynamic environments would clarify this aspect.

## Limitations

- Limited comparison with VLLM-adapted baseline methods, only comparing against non-VLLM-specific editing approaches
- No analysis of long-term stability beyond 1000 edits, which is insufficient for true lifelong learning scenarios
- Routing mechanisms rely on semantic similarity metrics that may not generalize well across diverse visual domains

## Confidence

**High Confidence**
- The core architecture of LiveEdit (expert generation, routing mechanisms, and low-rank adaptation) is sound and well-implemented
- The experimental methodology for evaluating reliability, generality, and locality is rigorous and follows established practices
- The performance improvements over baseline methods on the tested benchmarks are real and measurable

**Medium Confidence**
- The claim that LiveEdit significantly outperforms existing LLM editing methods on VLLM tasks
- The assertion that the hard routing mechanism effectively filters visually irrelevant experts
- The claim that the soft routing mechanism achieves optimal multi-expert fusion

**Low Confidence**
- Long-term performance and stability of LiveEdit after thousands of edits
- Generalization to VLLMs with architectures significantly different from those tested
- Robustness to noisy or adversarial edit samples

## Next Checks

1. **Scalability Test**: Evaluate LiveEdit's performance after 10,000+ edits to assess long-term stability and potential catastrophic forgetting, using the same benchmarks (E-VQA, E-IC, VLKEB) but extending the edit sequence.

2. **Architecture Generalization**: Test LiveEdit on a broader range of VLLM architectures (e.g., Flamingo, LLaVA-Next) to verify that the method generalizes beyond the three backbones used in the paper.

3. **Baselines with VLLM Adaptation**: Implement and compare against existing LLM editing methods that have been explicitly adapted for VLLMs (such as editing methods that incorporate visual tokens or cross-modal attention) to provide a fairer comparison.