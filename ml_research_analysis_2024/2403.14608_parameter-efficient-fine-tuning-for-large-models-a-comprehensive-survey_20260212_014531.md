---
ver: rpa2
title: 'Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey'
arxiv_id: '2403.14608'
source_url: https://arxiv.org/abs/2403.14608
tags:
- arxiv
- peft
- preprint
- fine-tuning
- tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a comprehensive survey of Parameter-Efficient
  Fine-Tuning (PEFT) methods for large models, addressing the challenge of adapting
  pre-trained models to downstream tasks with limited computational resources. PEFT
  techniques aim to modify only a small subset of model parameters, significantly
  reducing training costs while maintaining performance.
---

# Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey

## Quick Facts
- **arXiv ID**: 2403.14608
- **Source URL**: https://arxiv.org/abs/2403.14608
- **Reference count**: 40
- **Primary result**: Comprehensive survey of PEFT methods categorizing algorithms into additive, selective, reparameterized, and hybrid approaches while addressing system design challenges and applications across multiple domains

## Executive Summary
This survey provides a systematic overview of Parameter-Efficient Fine-Tuning (PEFT) methods for adapting large pre-trained models to downstream tasks with limited computational resources. PEFT techniques modify only a small subset of parameters, significantly reducing training costs while maintaining performance. The paper categorizes PEFT algorithms into four types - additive (adapters, soft prompts), selective (structured/unstructured masking), reparameterized (LoRA), and hybrid approaches - while exploring system design challenges including centralized serving, distributed training, and multi-tenant workloads. Applications extend beyond NLP to vision transformers, vision-language models, and diffusion models, demonstrating PEFT's versatility.

## Method Summary
The survey systematically reviews PEFT literature through comprehensive categorization and analysis of existing methods. It examines the fundamental principles underlying different PEFT approaches, including how they modify model parameters, the computational tradeoffs involved, and their effectiveness across various domains. The analysis covers both theoretical foundations and practical implementation considerations, providing insights into when and how different PEFT methods should be applied based on task requirements and resource constraints.

## Key Results
- PEFT methods can reduce parameter updates by orders of magnitude while maintaining competitive performance compared to full fine-tuning
- The four-category classification scheme (additive, selective, reparameterized, hybrid) provides a comprehensive framework for understanding PEFT approaches
- PEFT applications extend beyond NLP to vision transformers, vision-language models, and diffusion models, demonstrating broad applicability
- System design challenges for PEFT include optimization for centralized serving, distributed training efficiency, and multi-tenant workload management

## Why This Works (Mechanism)
PEFT works by modifying only a small subset of parameters while leveraging the knowledge encoded in pre-trained models. Additive methods insert new trainable modules (adapters, prompts) that interact with frozen base models. Selective methods identify and update only the most relevant parameters through masking strategies. Reparameterized methods use efficient parameter updates that can be merged with base weights for inference. Hybrid approaches combine multiple strategies to balance efficiency and performance. These techniques exploit the observation that large models have sufficient capacity to solve multiple tasks through small parameter modifications rather than complete weight updates.

## Foundational Learning

**Parameter-Efficient Fine-Tuning (PEFT)**: Techniques that update only a small fraction of model parameters during adaptation, reducing computational costs while maintaining performance. *Why needed*: Full fine-tuning of large models is computationally expensive and may lead to overfitting on small datasets. *Quick check*: Compare parameter count of PEFT vs full fine-tuning methods.

**Adapter-based methods**: Insert small trainable modules between layers of frozen base models. *Why needed*: Enables task-specific adaptation without modifying original model weights. *Quick check*: Verify adapter dimensions and placement within model architecture.

**Low-Rank Adaptation (LoRA)**: Reparameterizes weight updates using low-rank matrices to reduce trainable parameters. *Why needed*: Exploits the observation that weight updates often lie in low-dimensional subspaces. *Quick check*: Confirm rank selection and its impact on performance.

**Prompt engineering**: Modifies input representations or adds trainable embeddings to guide model behavior. *Why needed*: Provides flexible task adaptation without changing model architecture. *Quick check*: Evaluate prompt effectiveness across different task types.

**System optimization**: Techniques for efficient deployment including quantization, pruning, and memory management. *Why needed*: Addresses practical constraints in real-world applications. *Quick check*: Measure inference latency and memory usage improvements.

## Architecture Onboarding

**Component map**: Base pre-trained model <-(frozen) PEFT modules -> Downstream task data -> Fine-tuned adapter outputs

**Critical path**: Input data → Base model forward pass → PEFT module computation → Task-specific head → Output prediction

**Design tradeoffs**: Parameter efficiency vs performance, training speed vs inference speed, modularity vs integration complexity, task-specific vs multi-task capability

**Failure signatures**: Performance degradation when PEFT modules are too small, training instability with improper initialization, catastrophic forgetting in multi-task settings, increased inference latency from complex PEFT architectures

**First experiments**: 1) Compare PEFT performance vs full fine-tuning on benchmark tasks, 2) Evaluate parameter efficiency across different PEFT categories, 3) Measure training/inference speed improvements with various PEFT methods

## Open Questions the Paper Calls Out

The survey identifies several open research directions: developing more effective hybrid PEFT approaches that combine strengths of different methods, addressing challenges in multi-task and continual learning scenarios, improving theoretical understanding of why PEFT works so effectively, optimizing PEFT for specific hardware architectures and deployment constraints, and extending PEFT to emerging model architectures beyond transformers. The paper also highlights the need for standardized evaluation protocols and benchmarks to fairly compare different PEFT methods across diverse tasks and domains.

## Limitations

- Classification scheme may not fully capture emerging approaches or edge cases where methods fit multiple categories
- System design evaluation focuses on theoretical considerations rather than empirical performance comparisons across deployment scenarios
- Limited discussion of long-term stability and generalization of PEFT-tuned models across varying data distributions
- Potential ambiguity in categorizing methods that exhibit characteristics of multiple PEFT types

## Confidence

- Claims about PEFT's effectiveness in reducing computational costs while maintaining performance: **High**
- Categorization of PEFT methods and their core principles: **Medium**
- Claims about future research directions and emerging applications: **Medium**

## Next Checks

1. Conduct empirical comparisons of PEFT methods across diverse downstream tasks to validate performance claims and identify scenarios where specific approaches excel
2. Evaluate practical deployment challenges through case studies in real-world multi-tenant and distributed training environments
3. Analyze long-term stability and generalization of PEFT-tuned models across varying data distributions and task complexities