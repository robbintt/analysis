---
ver: rpa2
title: 'On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey'
arxiv_id: '2406.15126'
source_url: https://arxiv.org/abs/2406.15126
tags: []
core_contribution: This survey paper provides a comprehensive overview of the rapidly
  evolving field of LLM-driven synthetic data generation for deep learning. It organizes
  recent research into a unified workflow encompassing data generation, curation,
  and evaluation.
---

# On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey

## Quick Facts
- **arXiv ID**: 2406.15126
- **Source URL**: https://arxiv.org/abs/2406.15126
- **Reference count**: 40
- **Primary result**: Comprehensive survey organizing LLM-driven synthetic data generation research into unified workflow with identification of key challenges and techniques

## Executive Summary
This survey paper provides a comprehensive overview of the rapidly evolving field of LLM-driven synthetic data generation for deep learning. It organizes recent research into a unified workflow encompassing data generation, curation, and evaluation. The paper highlights the key challenges of ensuring faithfulness and diversity in generated data, and summarizes various techniques to address these issues, including prompt engineering, multi-step generation, sample filtering, label enhancement, and quality evaluation. By synthesizing the current state of the field and identifying key areas for future research, this work aims to guide both academic and industrial communities in leveraging LLMs for more effective and scalable data generation, ultimately advancing the capabilities of deep learning models across diverse applications.

## Method Summary
The paper systematically categorizes and analyzes existing literature on LLM-driven synthetic data generation, curation, and evaluation. It identifies and organizes techniques into three main stages: data generation (including prompt engineering, multi-step generation, and self-evaluation), data curation (including filtering, denoising, and label enhancement), and evaluation (including faithfulness and diversity metrics). The survey synthesizes findings from 40 references to create a unified workflow that addresses the key challenges in the field.

## Key Results
- Comprehensive organization of LLM-driven synthetic data generation research into unified three-stage workflow
- Identification of faithfulness and diversity as key challenges requiring specialized techniques
- Summary of current state-of-the-art methods including prompt engineering, multi-step generation, filtering, and evaluation metrics

## Why This Works (Mechanism)
This survey works by systematically categorizing and synthesizing a large body of rapidly evolving research into a coherent framework. By organizing techniques into generation, curation, and evaluation stages, it provides a structured approach to understanding the complex interplay between different components of LLM-driven synthetic data pipelines. The framework helps identify critical bottlenecks and challenges while highlighting effective solutions that have emerged across different research efforts.

## Foundational Learning
- **LLM prompting techniques** (why needed: to control synthetic data quality; quick check: test different prompt formats on same task)
- **Data faithfulness metrics** (why needed: to measure alignment between generated and real data; quick check: compare synthetic vs real data distributions)
- **Diversity measurement methods** (why needed: to ensure broad coverage of data space; quick check: calculate diversity scores across different generation methods)
- **Multi-step generation approaches** (why needed: to improve quality through iterative refinement; quick check: compare single-step vs multi-step outputs)
- **Sample filtering strategies** (why needed: to remove low-quality or noisy data; quick check: measure impact of filtering on downstream model performance)
- **Label enhancement techniques** (why needed: to improve annotation quality in synthetic data; quick check: compare original vs enhanced labels)

## Architecture Onboarding
- **Component map**: User Query -> Prompt Engineering -> Data Generation -> Multi-step Refinement -> Self-evaluation -> Filtering -> Label Enhancement -> Quality Evaluation -> Output Dataset
- **Critical path**: Prompt Engineering → Data Generation → Filtering → Quality Evaluation
- **Design tradeoffs**: Quality vs. quantity (higher quality requires more computation), faithfulness vs. diversity (hard to optimize both simultaneously), human effort vs. automation (manual curation improves quality but reduces scalability)
- **Failure signatures**: Poor faithfulness manifests as hallucinated content or domain shift; low diversity appears as repetitive patterns or mode collapse; filtering failures result in inclusion of low-quality samples
- **First experiments**: 1) Generate synthetic data using different prompting strategies and measure faithfulness scores, 2) Apply multi-step refinement and compare quality improvement, 3) Test filtering algorithms on synthetic datasets and measure downstream model performance

## Open Questions the Paper Calls Out
None

## Limitations
- The field of LLM-driven synthetic data generation is rapidly evolving, with new methods and techniques emerging frequently, which means some recent developments may not be fully captured
- The paper relies heavily on the categorization of existing techniques, which may not fully represent the diversity of approaches or their relative effectiveness in real-world applications
- The major claims about the unified workflow and the effectiveness of various techniques are supported by existing literature but lack extensive empirical validation across diverse domains and use cases

## Confidence
- **High confidence**: The identification of key challenges (faithfulness and diversity) in synthetic data generation
- **Medium confidence**: The categorization of techniques into generation, curation, and evaluation stages
- **Medium confidence**: The summary of current state-of-the-art methods, pending empirical validation

## Next Checks
1. Conduct a systematic literature review to identify and incorporate the most recent papers published after the survey's cutoff date, particularly those addressing emerging challenges or novel approaches
2. Perform empirical studies comparing the effectiveness of different LLM-driven synthetic data generation techniques across multiple domains and tasks, using standardized benchmarks and metrics
3. Develop and validate a practical implementation guide that addresses real-world challenges such as computational costs, scalability, and integration with existing ML pipelines, potentially through case studies or industry collaborations