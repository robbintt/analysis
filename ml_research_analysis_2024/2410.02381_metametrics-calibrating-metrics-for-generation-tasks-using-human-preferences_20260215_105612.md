---
ver: rpa2
title: 'MetaMetrics: Calibrating Metrics For Generation Tasks Using Human Preferences'
arxiv_id: '2410.02381'
source_url: https://arxiv.org/abs/2410.02381
tags:
- metrics
- human
- metric
- metametrics
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces METAMETRICS, a supervised meta-metric designed
  to calibrate and combine existing evaluation metrics to better align with human
  preferences across different modalities and tasks. METAMETRICS addresses the limitations
  of individual metrics, which often excel in specific aspects but fail to capture
  the full spectrum of human judgment.
---

# MetaMetrics: Calibrating Metrics For Generation Tasks Using Human Preferences

## Quick Facts
- arXiv ID: 2410.02381
- Source URL: https://arxiv.org/abs/2410.02381
- Authors: Genta Indra Winata; David Anugraha; Lucky Susanto; Garry Kuwanto; Derry Tanti Wijaya
- Reference count: 40
- Primary result: Introduces a supervised meta-metric that calibrates and combines existing evaluation metrics to better align with human preferences across multiple modalities and tasks

## Executive Summary
MetaMetrics addresses the fundamental limitation of existing evaluation metrics that often excel in specific aspects but fail to capture the full spectrum of human judgment. The method optimizes combinations of multiple existing metrics using human assessment scores, enabling more accurate and comprehensive evaluations across language and vision generation tasks. By learning which metrics are most predictive of human preferences for each task, MetaMetrics achieves state-of-the-art performance in machine translation, abstractive summarization, question answering, image captioning, and reward model scoring, outperforming existing metrics in correlation with human preferences.

## Method Summary
MetaMetrics calibrates evaluation metrics by combining multiple existing metrics using weights optimized to maximize correlation with human judgment scores. The method employs two optimization approaches: Bayesian Optimization with Gaussian Processes and XGBoost boosting. Metric scores are first normalized to [0,1] and standardized for fair combination. The optimization learns which metrics are most predictive of human preferences for each task. For efficiency, an iterative pruning method removes less important metrics while maintaining performance. The approach is demonstrated across multiple tasks and datasets, showing significant improvements in alignment with human preferences.

## Key Results
- Achieves state-of-the-art performance across machine translation, abstractive summarization, question answering, image captioning, and reward model scoring
- Shows significant improvements in correlation with human preferences compared to individual baseline metrics
- Demonstrates effectiveness across multilingual and multi-domain scenarios with flexible integration into various applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MetaMetrics improves alignment with human preferences by calibrating existing metrics using human assessment scores rather than relying on a single metric.
- Mechanism: The method combines multiple existing evaluation metrics using weights optimized to maximize correlation with human judgment scores. It employs two optimization approaches: Bayesian Optimization with Gaussian Processes and XGBoost boosting. The optimization learns which metrics are most predictive of human preferences for each task.
- Core assumption: Individual metrics capture different aspects of quality, and their combination can better represent the full spectrum of human judgment than any single metric.
- Evidence anchors:
  - [abstract] "MetaMetrics optimizes the combination of existing metrics to enhance their alignment with human preferences"
  - [section 2] "Each metric may be good for a specific human preference aspect, and it is necessary to identify which metric is suitable for each aspect to ensure comprehensive evaluation"
  - [corpus] Weak - no direct evidence in corpus neighbors about this specific mechanism
- Break condition: If human assessment scores are inconsistent, noisy, or capture irrelevant aspects of quality, the optimization will learn incorrect weightings that degrade performance.

### Mechanism 2
- Claim: The normalization and preprocessing of metric scores enables fair combination across metrics with different scales and ranges.
- Mechanism: Each metric score is clipped to its valid range, normalized to [0,1], and inverted if higher scores indicate worse performance. This standardization allows the optimization to compare and combine metrics on equal footing.
- Core assumption: The pre-processing transforms metric scores into a common interpretable scale where 0 represents poor performance and 1 represents perfect performance.
- Evidence anchors:
  - [section 3.2] "Some metrics, particularly neural-based ones, can fall outside this defined range. To standardize these metrics, we need to normalize them to a common scale that ranges from 0 to 1"
  - [corpus] Missing - no evidence in corpus neighbors about normalization mechanisms
- Break condition: If the assumed mapping between raw metric scores and the normalized scale doesn't hold for certain metrics, the combination will be biased toward or against those metrics.

### Mechanism 3
- Claim: Iterative pruning in XGBoost improves efficiency without sacrificing performance by identifying the most important metrics.
- Mechanism: The method starts with all metrics, trains an XGBoost model, identifies the least important metric by feature importance, removes it, and repeats for k iterations. The best-performing model across iterations is selected.
- Core assumption: A small subset of metrics captures most of the predictive power for human preferences, and removing less important metrics doesn't significantly degrade performance.
- Evidence anchors:
  - [section 3.3.2] "To address the scalability and efficiency issues, we propose an iterative pruning method"
  - [abstract] "Our metric demonstrates flexibility and effectiveness in both language and vision downstream tasks"
  - [corpus] Weak - corpus neighbors mention efficiency concerns but not this specific pruning mechanism
- Break condition: If the feature importance scores are unstable or if metrics are highly correlated, the pruning might remove important metrics or create fragile models sensitive to training data variations.

## Foundational Learning

- Concept: Bayesian Optimization with Gaussian Processes
  - Why needed here: Provides interpretable optimization by showing which metrics contribute most to the objective, and handles expensive-to-evaluate objective functions efficiently
  - Quick check question: What is the role of the kernel function in Gaussian Process Bayesian Optimization?

- Concept: Feature importance in tree-based models
  - Why needed here: XGBoost uses feature importance to identify which metrics are most predictive of human preferences and to enable iterative pruning
  - Quick check question: How does XGBoost calculate feature importance, and what are the different importance measures available?

- Concept: Correlation metrics (Kendall τ)
  - Why needed here: The optimization objective function measures alignment between metric scores and human preferences using Kendall correlation
  - Quick check question: Why might Kendall τ correlation be preferred over Pearson correlation when comparing metric scores to human rankings?

## Architecture Onboarding

- Component map: Raw model output -> Multiple metric evaluations -> Preprocessing (clipping, normalization, inversion) -> Combination Layer (GP or XGBoost) -> Post-processor -> Final Score
- Critical path: Raw model output → Multiple metric evaluations → Preprocessing → Weighted combination → Human preference alignment
- Design tradeoffs: 
  - GP offers interpretability but may be slower; XGBoost is faster but less interpretable
  - Using more metrics increases coverage but also computational cost
  - Cross-dataset training improves generalization but may reduce task-specific performance
- Failure signatures:
  - Performance degrades when training and test distributions differ significantly
  - Some metrics dominate the combination regardless of task
  - The metric becomes unstable across different random seeds
- First 3 experiments:
  1. Compare GP vs XGBoost performance on a single task to understand the tradeoff between interpretability and performance
  2. Test cross-dataset generalization by training on one dataset and evaluating on another
  3. Perform ablation study by removing individual metrics to identify which ones contribute most to performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does METAMETRICS perform when calibrated on datasets with significantly different distributions than the test sets, particularly in cross-modal tasks?
- Basis in paper: [explicit] The paper mentions cross-dataset experiments in image captioning where METAMETRICS is tuned on one dataset and tested on another, showing robustness.
- Why unresolved: The paper only demonstrates cross-dataset performance for image captioning. It's unclear how well METAMETRICS generalizes across different modalities or when trained on datasets from entirely different domains.
- What evidence would resolve it: Conducting cross-dataset experiments for other tasks like machine translation, summarization, and question answering, comparing performance when training and testing on datasets with different characteristics.

### Open Question 2
- Question: What is the impact of including or excluding large language models (LLMs) exceeding 10.7B parameters on METAMETRICS' performance and efficiency?
- Basis in paper: [explicit] The paper mentions computational resource limitations led to excluding LLMs exceeding 10.7B parameters from their experiments.
- Why unresolved: The paper doesn't explore how including larger LLMs might affect METAMETRICS' performance, potentially missing out on the benefits of more powerful models or insights into scalability.
- What evidence would resolve it: Experimenting with METAMETRICS using larger LLMs and comparing performance, efficiency, and resource requirements against the current setup.

### Open Question 3
- Question: How does METAMETRICS' performance compare to task-specific metrics that are specifically designed and fine-tuned for individual tasks?
- Basis in paper: [inferred] The paper shows METAMETRICS outperforms or matches existing metrics across various tasks, but it's unclear how it compares to highly specialized, task-specific metrics.
- Why unresolved: While METAMETRICS demonstrates strong general performance, the paper doesn't benchmark against metrics that are optimized for specific tasks, leaving uncertainty about its relative effectiveness in specialized scenarios.
- What evidence would resolve it: Comparing METAMETRICS' performance against state-of-the-art, task-specific metrics in each domain to determine if its generalization comes at the cost of task-specific accuracy.

## Limitations

- Metric Selection Bias: The effectiveness depends heavily on which base metrics are included, with no systematic criteria provided for metric selection across different tasks and domains.
- Human Preference Dataset Quality: The method's performance is fundamentally limited by the quality, consistency, and coverage of human preference datasets used for training.
- Cross-Dataset Generalization: While effectiveness is demonstrated across multiple tasks, validation is primarily within-dataset, with uncertainty about generalization to new datasets, languages, or domains.

## Confidence

**High Confidence**
- The basic approach of combining multiple metrics using learned weights to improve correlation with human judgments is well-supported by experimental results and follows established practices.

**Medium Confidence**
- The claim of state-of-the-art performance across all tested tasks is supported by correlation improvements, but direct comparisons with other meta-metric approaches are limited.
- The efficiency gains from iterative pruning are demonstrated, but sensitivity to different pruning criteria and stability across runs is not fully characterized.

**Low Confidence**
- The claim of seamless integration across language and vision tasks assumes the same underlying human preference structures, which may not hold across such different modalities.
- The long-term stability and maintenance requirements for metric combinations are not addressed.

## Next Checks

1. **Cross-Dataset Transferability Test**: Train MetaMetrics on one human preference dataset (e.g., SummEval for summarization) and evaluate its performance on completely independent datasets from the same task (e.g., XSum human judgments) to assess generalization beyond the training distribution.

2. **Metric Sensitivity Analysis**: Systematically remove or add individual metrics to the combination and measure the impact on performance to determine which metrics are truly essential versus those that can be substituted or omitted without significant degradation.

3. **Annotator Agreement Stability**: Split human preference datasets by annotator and train separate MetaMetrics models for each annotator group, then measure the variance in learned weights and performance to assess sensitivity to individual annotator biases and the robustness of the optimization approach.