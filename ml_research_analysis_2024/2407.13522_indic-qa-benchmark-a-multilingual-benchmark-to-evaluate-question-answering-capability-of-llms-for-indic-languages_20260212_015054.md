---
ver: rpa2
title: 'INDIC QA BENCHMARK: A Multilingual Benchmark to Evaluate Question Answering
  capability of LLMs for Indic Languages'
arxiv_id: '2407.13522'
source_url: https://arxiv.org/abs/2407.13522
tags:
- languages
- dataset
- hindi
- language
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Indic QA Benchmark, a large multilingual
  dataset for evaluating context-grounded question answering in 11 major Indian languages.
  It addresses the scarcity of high-quality benchmarks for low-resource languages
  by compiling existing datasets, translating English datasets into Indian languages,
  and generating synthetic data verified by language experts.
---

# INDIC QA BENCHMARK: A Multilingual Benchmark to Evaluate Question Answering capability of LLMs for Indic Languages

## Quick Facts
- **arXiv ID**: 2407.13522
- **Source URL**: https://arxiv.org/abs/2407.13522
- **Reference count**: 9
- **Primary result**: Translate-Test paradigm significantly outperforms direct multilingual inference for low-resource Indic languages in question answering tasks.

## Executive Summary
This paper introduces the Indic QA Benchmark, a large multilingual dataset for evaluating context-grounded question answering in 11 major Indian languages. The benchmark addresses the scarcity of high-quality benchmarks for low-resource languages by compiling existing datasets, translating English datasets into Indian languages, and generating synthetic data verified by language experts. Experiments on various multilingual LLMs reveal poor performance in low-resource languages due to English-language bias in training data, with the Translate-Test paradigm significantly outperforming direct multilingual inference in these settings.

## Method Summary
The study involves creating the Indic QA Benchmark by combining existing datasets, translating English datasets into 11 Indian languages using IndicTrans2, and generating synthetic data using Gemini models verified by language experts. The benchmark covers both extractive and abstractive QA tasks across domain-diverse content. Various multilingual LLMs including base and instruction-fine-tuned variants are evaluated using F1 score for extractive tasks and ROUGE-L for abstractive tasks. The Translate-Test paradigm is implemented by translating inputs to English, processing with LLMs, and translating outputs back to source languages, then compared against direct multilingual inference.

## Key Results
- Base models like Gemma2 performed best on extractive QA tasks, while instruction tuning showed mixed results
- Translate-Test paradigm significantly outperformed direct multilingual inference in low-resource language settings
- Instruction fine-tuning generally improved abstractive QA tasks but sometimes harmed extractive performance
- Few-shot prompting notably improved answer quality in extractive tasks by guiding models to extract more precise information

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Translate-Test outperforms direct multilingual inference for low-resource languages.
- Mechanism: Translation-based processing leverages high-resource language capabilities for reasoning, then maps back to the source language.
- Core assumption: Translation quality is high enough to preserve semantic content, and back-translation accurately reconstructs the answer in the source language.
- Evidence anchors:
  - [abstract] "Translate-Test paradigm, where inputs are translated to English for processing and the results are translated back into the source language for output. This approach outperformed multilingual LLMs, particularly in low-resource settings."
  - [section] "The Translate-Test paradigm significantly outperforms them in low-resource languages."
  - [corpus] Weak - corpus shows related work on translation but no direct evidence on this specific translation accuracy claim.
- Break condition: Translation errors compound during inference, causing cascading failures that degrade final answer quality.

### Mechanism 2
- Claim: Base models with few-shot prompting improve answer quality in extractive tasks.
- Mechanism: Few-shot examples provide task-specific context that guides the model to extract precise information rather than hallucinate or generalize incorrectly.
- Core assumption: The model can leverage the few-shot examples to understand the expected output format and extract relevant spans.
- Evidence anchors:
  - [abstract] "base pre-trained models frequently produce incorrect or illogical answers. However, few-shot prompting notably improves answer quality by guiding the models to extract more precise information from the text."
  - [section] "Using few-shot (1-shot and 3-shot) almost always improves over the zero-shot base model."
  - [corpus] Weak - corpus mentions related work on few-shot learning but not specific to this model behavior pattern.
- Break condition: Few-shot examples are not representative of the task distribution, leading the model to overfit to specific patterns.

### Mechanism 3
- Claim: Instruction fine-tuning improves abstractive QA but may harm extractive QA.
- Mechanism: Instruction tuning adapts models to follow explicit instructions, which benefits generative tasks but may reduce the model's ability to perform precise span extraction.
- Core assumption: The instruction-tuning process prioritizes instruction-following over task-specific capabilities, creating a trade-off.
- Evidence anchors:
  - [abstract] "instruction fine tuned versions, revealed weak performance in low-resource languages due to a strong English-language bias in their training data."
  - [section] "Instruction finetuning generally improves abstractive QA tasks across all models, but its impact on extractive QA varies."
  - [corpus] Weak - corpus mentions instruction tuning but lacks direct evidence on this specific trade-off pattern.
- Break condition: The instruction-tuning data contains insufficient target language content, causing catastrophic forgetting of multilingual capabilities.

## Foundational Learning

- Concept: Multilingual model training dynamics
  - Why needed here: Understanding how multilingual models handle low-resource languages versus high-resource ones is crucial for interpreting performance differences.
  - Quick check question: What architectural choices enable a model to handle multiple languages simultaneously, and how do these choices affect performance on under-represented languages?

- Concept: Translation system reliability
  - Why needed here: The Translate-Test approach depends entirely on translation quality for both directions.
  - Quick check question: What metrics (like CHRF) are used to evaluate translation quality, and what threshold indicates acceptable performance for QA tasks?

- Concept: Evaluation metrics for QA tasks
  - Why needed here: Different metrics (F1, ROUGE-L) measure different aspects of QA performance, and understanding their differences is essential for proper evaluation.
  - Quick check question: How does F1 score differ from ROUGE-L in measuring answer quality, and why would one be preferred over the other for extractive versus abstractive tasks?

## Architecture Onboarding

- Component map: Data curation → Translation → Model Inference → Back-translation (if Translate-Test) → Evaluation → Results
- Critical path: Data → Translation → Model Inference → Back-translation (if Translate-Test) → Evaluation → Results
- Design tradeoffs: Direct inference preserves original language context but suffers from low-resource limitations; Translate-Test leverages English capabilities but introduces translation overhead and potential error propagation.
- Failure signatures: Poor performance on low-resource languages indicates translation issues or model bias; inconsistent results across languages suggest data quality problems; degraded extractive performance after instruction tuning suggests catastrophic forgetting.
- First 3 experiments:
  1. Run direct inference on a representative sample from each language family to establish baseline performance patterns.
  2. Implement Translate-Test pipeline on the same sample to compare performance gains/losses across resource levels.
  3. Test few-shot prompting on extractive tasks to measure quality improvement and identify optimal shot count per language.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the Translate-Test approach perform on low-resource Indic languages when using advanced multilingual translation models like NLLB?
- Basis in paper: [explicit] The paper mentions that IndicTrans2 was chosen over NLLB for translation tasks, but it doesn't explore the performance of Translate-Test with NLLB.
- Why unresolved: The paper only tested IndicTrans2 for translation tasks and didn't compare its performance with other models like NLLB in the Translate-Test paradigm.
- What evidence would resolve it: Conducting experiments using the Translate-Test approach with NLLB for low-resource Indic languages and comparing the results with those obtained using IndicTrans2.

### Open Question 2
- Question: What is the impact of using different amounts of in-context examples (few-shot learning) on the performance of LLMs for abstractive QA tasks in Indic languages?
- Basis in paper: [inferred] The paper mentions that using few-shot examples improves performance for extractive QA tasks, but it doesn't explore this for abstractive QA tasks.
- Why unresolved: The paper only reports the effect of few-shot learning on extractive QA tasks and doesn't investigate its impact on abstractive QA tasks.
- What evidence would resolve it: Conducting experiments with varying numbers of in-context examples for abstractive QA tasks and analyzing the performance changes across different Indic languages.

### Open Question 3
- Question: How does the performance of instruction-tuned LLMs for Indic languages change when they are fine-tuned with a more balanced mix of high-resource and low-resource language data?
- Basis in paper: [explicit] The paper discusses that instruction-tuned models like Aya-101 and Narvasa 2.0 perform better because they are fine-tuned on Indic data, but it doesn't explore the impact of a balanced data mix.
- Why unresolved: The paper doesn't investigate how the performance of instruction-tuned models changes when they are fine-tuned with a more balanced mix of high-resource and low-resource language data.
- What evidence would resolve it: Fine-tuning instruction-tuned models with a balanced mix of high-resource and low-resource language data and evaluating their performance on the Indic QA benchmark.

## Limitations
- The study doesn't fully characterize translation quality or error rates, making it difficult to assess scalability to more diverse domains.
- Synthetic data generation using Gemini models introduces potential bias that wasn't extensively validated beyond language expert verification.
- The paper doesn't explore why instruction fine-tuning creates a trade-off between abstractive and extractive QA performance.

## Confidence
- **High confidence**: The overall finding that low-resource languages perform poorly due to English bias is well-supported by consistent experimental results across multiple models and tasks.
- **Medium confidence**: The Translate-Test superiority claim is supported by results but lacks detailed analysis of translation quality impact and error propagation.
- **Medium confidence**: The instruction fine-tuning trade-off finding is observed but not deeply analyzed - the paper doesn't explain why this trade-off occurs or how to mitigate it.

## Next Checks
1. **Translation Quality Analysis**: Measure CHRF scores and human evaluation of translation quality for each language pair used in Translate-Test, then correlate these metrics with QA performance to establish translation quality thresholds.

2. **Error Propagation Study**: Track error rates at each stage of the Translate-Test pipeline (source→English, English inference, English→target) to quantify how translation errors compound and identify optimal error thresholds.

3. **Synthetic Data Validation**: Conduct blind human evaluation comparing synthetic data quality against real human-curated data across all 11 languages, measuring factual accuracy and linguistic naturalness to assess the synthetic data contribution to overall benchmark quality.