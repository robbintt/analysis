---
ver: rpa2
title: Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding
arxiv_id: '2401.05054'
source_url: https://arxiv.org/abs/2401.05054
tags:
- diverse
- samples
- beam
- epsilon
- search
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces two novel decoding algorithms, Diverse MBR
  (DMBR) and k-Medoids MBR (KMBR), that generate diverse and high-quality texts by
  extending Minimum Bayes Risk (MBR) decoding. DMBR promotes diversity by introducing
  a diversity penalty to the MBR objective, while KMBR selects a set of sentences
  by solving the k-medoids problem.
---

# Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding

## Quick Facts
- arXiv ID: 2401.05054
- Source URL: https://arxiv.org/abs/2401.05054
- Reference count: 40
- One-line primary result: DMBR and KMBR achieve better quality-diversity trade-offs than diverse beam search and sampling algorithms

## Executive Summary
This paper introduces two novel decoding algorithms, Diverse MBR (DMBR) and k-Medoids MBR (KMBR), that generate diverse and high-quality texts by extending Minimum Bayes Risk (MBR) decoding. DMBR promotes diversity by introducing a diversity penalty to the MBR objective, while KMBR selects a set of sentences by solving the k-medoids problem. The experimental results on machine translation, image captioning, question generation, generative common sense reasoning, and text summarization tasks show that DMBR and KMBR achieve better quality-diversity trade-offs than diverse beam search and sampling algorithms, with DMBR and KMBR achieving higher Oracle quality scores in all tasks.

## Method Summary
The paper proposes two decoding algorithms that extend MBR decoding to generate diverse and high-quality texts. DMBR modifies the MBR objective by adding a diversity penalty that discourages selecting similar sentences, using a greedy approximation algorithm when the penalty weight is small. KMBR uses k-medoids clustering to group similar sentences and selects one representative sentence from each cluster. Both methods require sampling multiple candidate sentences, computing utilities with respect to reference sentences, and then selecting diverse sets based on their respective objectives. The algorithms are evaluated against diverse beam search and sampling baselines across multiple text generation tasks.

## Key Results
- DMBR and KMBR achieve better quality-diversity trade-offs than diverse beam search and sampling algorithms
- DMBR achieves higher diversity than underlying sampling algorithms for all evaluated sampling methods
- Both DMBR and KMBR achieve higher Oracle quality scores than baselines in all evaluated tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DMBR achieves better diversity than sampling algorithms by maximizing the expected utility while minimizing pairwise similarity between generated sentences.
- Mechanism: The diversity objective in DMBR penalizes outputs with high pairwise similarity (e.g., using BLEU or BERTScore), encouraging the selection of more diverse sets of sentences. This is implemented by modifying the MBR objective to include a diversity penalty term.
- Core assumption: The utility function used for diversity (e.g., pairwise-BLEU) effectively captures semantic diversity.
- Evidence anchors:
  - [abstract]: "DMBR achieves better trade-offs than diverse beam search and sampling algorithms"
  - [section]: "DMBR achieves higher diversity than the underlying sampling algorithm for all four sampling algorithms we evaluate"
  - [corpus]: Weak evidence. Related papers focus on MBR for document-level tasks, not diversity.

### Mechanism 2
- Claim: KMBR promotes diversity by clustering sampled sentences and selecting one representative sentence from each cluster.
- Mechanism: KMBR uses k-medoids clustering to group similar sentences together and then selects the sentence with the highest utility from each cluster. This ensures diversity by choosing sentences from different clusters.
- Core assumption: The clustering algorithm effectively groups similar sentences.
- Evidence anchors:
  - [abstract]: "KMBR selects a set of sentences by solving the k-medoids problem"
  - [section]: "KMBR can be understood as a generalization of the vanilla MBR decoding which is solving the 1-Medoid problem"
  - [corpus]: Weak evidence. No direct mention of k-medoids for diversity in corpus.

### Mechanism 3
- Claim: MBR inherently generates higher quality sentences than MAP decoding or beam search.
- Mechanism: MBR selects the sentence that maximizes the expected utility over a set of sampled hypotheses, rather than just the most probable sentence. This leads to higher quality outputs by considering the overall distribution of likely sentences.
- Core assumption: The sampled hypotheses are representative of the true distribution of likely sentences.
- Evidence anchors:
  - [abstract]: "MBR decoding is shown to generate higher quality sentences than random sampling and beam search"
  - [section]: "MBR decoding is shown to generate higher quality sentences than random sampling and beam search in directed text generation tasks"
  - [corpus]: Moderate evidence. Several papers discuss MBR for text generation quality.

## Foundational Learning

- Concept: Minimum Bayes Risk (MBR) decoding
  - Why needed here: MBR is the foundation of both DMBR and KMBR. Understanding MBR is crucial for grasping how these algorithms promote diversity while maintaining quality.
  - Quick check question: What is the key difference between MBR and MAP decoding?

- Concept: k-medoids clustering
  - Why needed here: KMBR uses k-medoids clustering to group similar sentences. Understanding k-medoids is essential for understanding how KMBR ensures diversity.
  - Quick check question: How does k-medoids differ from k-means clustering?

- Concept: Submodular function maximization
  - Why needed here: DMBR's diversity objective is a submodular function, and understanding submodularity is crucial for understanding why the greedy algorithm provides a good approximation.
  - Quick check question: What is the key property of submodular functions that allows for efficient approximation algorithms?

## Architecture Onboarding

- Component map: Text generation model -> Sampling algorithm -> Utility function -> (DMBR: Diversity objective -> Greedy selection) OR (KMBR: Clustering algorithm -> Cluster selection)

- Critical path:
  1. Sample a set of candidate sentences from the text generation model.
  2. Compute the utility of each candidate sentence with respect to a set of reference sentences.
  3. For DMBR: Compute the pairwise similarity between all candidate sentences and select a diverse set using the greedy algorithm.
  4. For KMBR: Cluster the candidate sentences and select one representative sentence from each cluster.
  5. Return the selected set of diverse and high-quality sentences.

- Design tradeoffs:
  - Sample size vs. computational cost: Larger sample sizes generally lead to better results but increase computation time.
  - Diversity penalty weight (for DMBR): Higher weights promote more diversity but may sacrifice quality.
  - Number of clusters (for KMBR): More clusters promote more diversity but may also increase computation time.

- Failure signatures:
  - DMBR/KMBR generates low-quality sentences: The sampling algorithm or utility function may be inadequate.
  - DMBR/KMBR generates similar sentences: The diversity objective or clustering algorithm may be ineffective.
  - DMBR/KMBR is too slow: The sample size or number of clusters may be too large.

- First 3 experiments:
  1. Implement DMBR with a simple utility function (e.g., BLEU) and evaluate its diversity on a small dataset.
  2. Implement KMBR with a small number of clusters and evaluate its diversity on the same dataset.
  3. Compare the performance of DMBR and KMBR with different sampling algorithms and utility functions.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the computational complexity of DMBR and KMBR be reduced to make them feasible for large-scale text generation tasks?
- Basis in paper: [explicit] The paper states that DMBR and KMBR are much slower than DBS as they require the computation of the MBR objective which needs a computation of the utility function for N^2 times.
- Why unresolved: The paper mentions that recent work has shown that the computation of the MBR objective can be significantly reduced, but it is not directly applicable to DMBR and KMBR.
- What evidence would resolve it: Developing approximation algorithms or optimizations specifically for DMBR and KMBR that can significantly reduce their computational complexity while maintaining or improving their quality-diversity trade-off.

### Open Question 2
- Question: How do DMBR and KMBR perform on open-ended text generation tasks like story generation or dialogue systems?
- Basis in paper: [explicit] The paper's experiments are focused on directed text generation tasks, and the authors acknowledge that evaluating the methods in open-ended text generation tasks is future work.
- Why unresolved: The performance of DMBR and KMBR on open-ended tasks has not been evaluated, and it is unclear whether they would maintain their advantages in such settings.
- What evidence would resolve it: Conducting experiments on open-ended text generation tasks and comparing the performance of DMBR and KMBR against other diversity-promoting methods in terms of quality, diversity, and coherence.

### Open Question 3
- Question: How do different utility functions affect the quality-diversity trade-off of DMBR and KMBR?
- Basis in paper: [explicit] The paper uses BERTScore as the utility function for all experiments, but acknowledges that using other quality objective functions such as model-based estimate is future work.
- Why unresolved: The choice of utility function can significantly impact the performance of DMBR and KMBR, and it is unclear how different utility functions would affect their quality-diversity trade-off.
- What evidence would resolve it: Conducting experiments with different utility functions (e.g., model-based estimates, human judgments) and comparing the performance of DMBR and KMBR in terms of quality, diversity, and task-specific metrics.

## Limitations

- Computational complexity is significantly higher than diverse beam search due to N^2 utility computations required for MBR objective.
- Experimental evaluation is primarily focused on English-centric translation tasks, with limited analysis of low-resource languages or non-English source languages.
- Human evaluation is limited to translation tasks, leaving questions about performance on other generation tasks from a human perspective.

## Confidence

**High confidence**: MBR decoding generates higher quality sentences than random sampling and beam search. This claim is well-supported by both the experimental results and the extensive literature on MBR decoding.

**Medium confidence**: DMBR and KMBR achieve better quality-diversity trade-offs than diverse beam search and sampling algorithms. While the experimental results support this claim across multiple tasks, the evaluation is limited to specific metrics and may not capture all aspects of quality and diversity.

**Medium confidence**: DMBR's greedy approximation algorithm provides good solutions when λ is small. The paper demonstrates this empirically but doesn't provide theoretical guarantees for larger λ values.

## Next Checks

1. **Diversity metric validation**: Conduct human evaluations specifically focused on measuring semantic diversity across different text generation tasks, comparing DMBR and KMBR against diverse beam search and sampling baselines using the same evaluation pool of annotators.

2. **Computational complexity analysis**: Systematically vary sample size, diversity penalty λ, and number of clusters k across different hardware configurations (GPU vs CPU) to establish clear scaling relationships and identify optimal parameter ranges for practical deployment.

3. **Cross-linguistic generalization**: Evaluate DMBR and KMBR on low-resource language pairs and non-English source languages to determine whether the quality-diversity improvements observed in English-centric tasks generalize to other linguistic contexts.