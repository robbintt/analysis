---
ver: rpa2
title: Reducing Human-Robot Goal State Divergence with Environment Design
arxiv_id: '2404.15184'
source_url: https://arxiv.org/abs/2404.15184
tags:
- robot
- human
- design
- state
- goal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces Goal State Divergence (GSD), a novel metric\
  \ to quantify discrepancies between a robot's final goal state and the one expected\
  \ by a human user. GSD is approximated using upper and lower bounds (GD\u2191 and\
  \ GD\u2193), which can be efficiently calculated using classical planning compilations."
---

# Reducing Human-Robot Goal State Divergence with Environment Design

## Quick Facts
- arXiv ID: 2404.15184
- Source URL: https://arxiv.org/abs/2404.15184
- Reference count: 27
- Key outcome: Introduces Goal State Divergence (GSD) metric and HRGA design problem to minimize robot-human goal misalignment through environment modifications, achieving zero lower bound and controlled upper bound on GSD in standard benchmarks.

## Executive Summary
This paper addresses the critical issue of human-robot goal misalignment by introducing Goal State Divergence (GSD) as a novel metric to quantify discrepancies between robot and human expected goal states. The authors propose a Human-Robot Goal Alignment (HRGA) design problem that identifies minimal environment modifications to reduce this divergence. By leveraging classical planning compilations, they efficiently compute upper and lower bounds on GSD, enabling practical optimization of environment design to ensure better alignment between robot behavior and human expectations.

## Method Summary
The method introduces Goal State Divergence (GSD) as a metric for quantifying the difference between robot and human expected goal states. GSD is computed using upper and lower bounds (GD↑ and GD↓) through classical planning compilations that encode the divergence problem as an extended planning task. The HRGA design problem then identifies minimal environment modifications—such as altering initial states or action preconditions—that reduce GSD to acceptable levels. The approach iteratively increases a design budget to find the minimal set of modifications that achieves the desired bound constraints.

## Key Results
- GSD metric successfully quantifies robot-human goal misalignment across multiple IPC domains
- Compilation-based approach efficiently computes GD↑ and GD↓ bounds for divergence quantification
- HRGA design problem identifies minimal environment modifications that reduce GSD while controlling design cost
- Method outperforms naive baselines in finding minimal designs while ensuring zero lower bound and controlled upper bound on GSD

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GSD quantifies the difference between human-expected and robot-achievable goal states.
- Mechanism: The metric measures symmetric difference in fluents between human and robot goal states, capturing misalignment in expectations.
- Core assumption: Both models share the same fluent set and the human's goal specification matches the robot's.
- Evidence anchors:
  - [abstract] "Goal State Divergence (GSD), which represents the difference between a robot's final goal state and the one a human user expected."
  - [section] "For a pair of models that are not necessarily distinct, M1 and M2, let π1 be a valid plan in M1, and π2 be a valid plan in M2. Given this, goal state divergence (GSD ) of the plan-model pairs is defined as the state divergence between the final state of these two plans."
- Break condition: The mechanism fails if the human and robot models have different fluent sets or if the human's goal specification does not match the robot's.

### Mechanism 2
- Claim: GD↑ and GD↓ provide upper and lower bounds on GSD when exact calculation is infeasible.
- Mechanism: GD↑ finds maximum divergence across all plan pairs, while GD↓ finds minimum divergence, bracketing the true GSD value.
- Core assumption: Access to both human and robot models is available for planning compilation.
- Evidence anchors:
  - [abstract] "we show how it can be approximated using maximal and minimal bounds."
  - [section] "Definition 3: For two given models, M1, M2, the worst-case or maximal goal state divergence ( GD ↑) is given by the cardinality of the maximum goal state divergence possible between all executable plans in M1, ΠM1, and M2, i.e.: GD ↑(M1, M2) = maxπ1∈ΠM1,π2∈ΠM2(|GSD (π1, M1, π2, M2)|)"
- Break condition: The bounds become meaningless if the human's expected plan is not representative of their actual behavior.

### Mechanism 3
- Claim: Environment design modifications can reduce GSD by altering the planning space.
- Mechanism: By modifying initial states or action preconditions, the design space is constrained to eliminate divergences between human and robot goal states.
- Core assumption: Designers have access to a set of modifiable environment parameters and can predict their effects on both models.
- Evidence anchors:
  - [abstract] "We then input the GSD value into our novel human-robot goal alignment (HRGA) design problem, which identifies a minimal set of environment modifications that can prevent mismatches like this."
  - [section] "Definition 5: A human-robot goal-state alignment (HRGA) design problem is characterized by the tuple, DP = ⟨MR, MH, U, Λ, C⟩, where U is a set of available environment modifications or model updates."
- Break condition: The mechanism fails if the set of available modifications is insufficient to eliminate the divergence or if the designer cannot accurately predict modification effects.

## Foundational Learning

- Concept: Classical planning compilation
  - Why needed here: The paper uses planning compilations to compute GD↑ and GD↓ bounds efficiently
  - Quick check question: What is the primary benefit of using planning compilations for computing divergence bounds instead of enumerating all plan pairs?

- Concept: Symmetric difference in sets
  - Why needed here: GSD is defined using symmetric difference between goal states
  - Quick check question: How does symmetric difference capture the difference between two states in this context?

- Concept: Environment design as model modification
  - Why needed here: The HRGA problem identifies minimal environment changes to reduce GSD
  - Quick check question: What types of environment modifications are considered in the HRGA problem definition?

## Architecture Onboarding

- Component map:
  - Model pair (MR, MH) -> Compilation module -> Design optimizer -> Evaluator

- Critical path:
  1. Input models and goal specification
  2. Compile planning problem for GD↑/GD↓ computation
  3. Calculate bounds using cost-optimal planner
  4. If bounds exceed thresholds, identify minimal modifications
  5. Output modified environment configuration

- Design tradeoffs:
  - Accuracy vs. computation: Exact GSD is intractable, so bounds are used
  - Generality vs. specificity: The approach works for various domains but requires domain-specific model definitions
  - Design scope: Limited to initial state and action modifications, which may not capture all possible divergences

- Failure signatures:
  - Compilation timeouts: Indicates problem complexity exceeds planner capabilities
  - No solution found: Suggests divergence cannot be eliminated with available modifications
  - Bound gaps remain large: Indicates the approximation may not be tight enough

- First 3 experiments:
  1. Verify compilation correctly computes bounds on a simple domain (e.g., blocksworld)
  2. Test design optimizer on a domain with known minimal modifications
  3. Evaluate bound tightness by comparing approximate and exact GSD on small problems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the HRGA method perform in environments with high state-space complexity compared to simpler benchmarks?
- Basis in paper: [inferred] The evaluation uses standard IPC benchmarks, but the paper does not explore how the method scales to more complex domains or high-dimensional state spaces.
- Why unresolved: The experiments are limited to smaller IPC domains, and the scalability of the method to larger, more complex environments is not addressed.
- What evidence would resolve it: Testing the HRGA method on domains with significantly larger state spaces or more complex action dynamics would provide insights into its scalability and computational feasibility.

### Open Question 2
- Question: Can the HRGA method be extended to handle dynamic environments where the state changes over time?
- Basis in paper: [explicit] The paper focuses on static environment design, but does not address scenarios where the environment evolves dynamically.
- Why unresolved: The current formulation assumes a fixed environment, and the adaptation of the method to dynamic settings is not explored.
- What evidence would resolve it: Evaluating the HRGA method in dynamic environments where state transitions occur over time would demonstrate its applicability and limitations in such settings.

### Open Question 3
- Question: How does the HRGA method perform when human models are uncertain or partially incorrect?
- Basis in paper: [inferred] The paper assumes access to a known human model, but does not address the impact of model uncertainty or errors in the human model.
- Why unresolved: The robustness of the HRGA method to inaccuracies in the human model is not tested or discussed.
- What evidence would resolve it: Experiments where the human model is intentionally perturbed or incomplete would reveal the method’s sensitivity to model uncertainty and its ability to handle such cases.

## Limitations
- Assumes both human and robot models share the same fluent set, which may not hold in real-world scenarios
- Compilation-based bounds provide only approximations that could be loose in complex domains
- HRGA design problem focuses on initial state and action modifications, potentially missing other divergence sources

## Confidence

- **High confidence**: The GSD metric definition and its interpretation as symmetric difference between goal states is mathematically sound and well-founded.
- **Medium confidence**: The compilation-based approximation using GD↑ and GD↓ bounds is theoretically valid but requires empirical validation across diverse domains.
- **Low confidence**: The claim that environment modifications can fully eliminate GSD is strong, as real-world human behavior may introduce divergences not captured by the model modifications considered.

## Next Checks

1. **Bound tightness evaluation**: Compare exact GSD calculations (on small problems) against the GD↑/GD↓ bounds to quantify approximation error and identify domains where bounds may be loose.
2. **Cross-domain generalization**: Test the approach on domains where human and robot models have different fluent sets to assess robustness when the shared-fluents assumption is violated.
3. **Real-world applicability assessment**: Implement a prototype system where human users interact with the robot in a physical environment to evaluate whether theoretical GSD reduction translates to improved human-robot collaboration in practice.