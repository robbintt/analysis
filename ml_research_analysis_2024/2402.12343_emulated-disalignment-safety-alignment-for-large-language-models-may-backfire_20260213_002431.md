---
ver: rpa2
title: 'Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!'
arxiv_id: '2402.12343'
source_url: https://arxiv.org/abs/2402.12343
tags:
- harmful
- language
- safety
- emulated
- disalignment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents emulated disalignment (ED), a training-free
  attack method that reverses safety alignment in large language models (LLMs) by
  contrasting the output token distributions of a safety-aligned model against its
  pre-trained version. The method works by sampling from a contrastive distribution
  that provably emulates the result of fine-tuning to minimize a safety reward, effectively
  generating harmful content without additional training.
---

# Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!

## Quick Facts
- arXiv ID: 2402.12343
- Source URL: https://arxiv.org/abs/2402.12343
- Reference count: 16
- Primary result: Emulated disalignment doubles harmfulness of pre-trained models and outperforms strong baselines, achieving highest harmful rates in 43 out of 48 evaluation subsets.

## Executive Summary
This paper presents emulated disalignment (ED), a training-free attack method that reverses safety alignment in large language models by contrasting output token distributions of safety-aligned models against their pre-trained versions. The method exploits the duality between language models and reward functions, sampling from a contrastive distribution that emulates the result of fine-tuning to minimize safety reward. Empirical evaluations across three datasets and four model families demonstrate that ED significantly increases harmful content generation without requiring additional training, highlighting vulnerabilities in current safety alignment approaches and raising concerns about open accessibility of language models even after safety alignment.

## Method Summary
Emulated disalignment works by sampling from a contrastive distribution defined by the ratio of pre-trained model outputs to safety-aligned model outputs raised to a power α. Specifically, it uses the sampling distribution πbase(y|x)^(α+1) / πalign(y|x)^α, where πbase is the pre-trained model and πalign is the safety-aligned model. This approach exploits the log-likelihood difference between aligned and pre-trained models as a safety reward function, effectively reversing alignment by minimizing this reward through sampling. The method requires access to both pre-trained and safety-aligned model outputs but doesn't need additional training, making it a practical attack vector against open-source models.

## Key Results
- ED doubles the harmfulness of pre-trained models across multiple model families
- ED outperforms resource-intensive direct disalignment methods in 43 out of 48 evaluation subsets
- Stronger safety alignment creates greater potential for harm under ED attacks
- ED is particularly effective against open-source models due to their accessible output distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The method exploits the duality between language models and reward functions to reverse safety alignment.
- Mechanism: By taking the log-likelihood difference between a safety-aligned model and its pre-trained version, ED creates a safety reward function that penalizes harmful responses. Minimizing this reward through sampling effectively emulates disalignment.
- Core assumption: The log-likelihood difference log πalign(y|x) - log πbase(y|x) accurately represents the safety reward function used during alignment.
- Evidence anchors:
  - [abstract]: "Specifically, our method achieves this reversal by contrasting the output token distribution of a safety-aligned language model...against its pre-trained version"
  - [section 4]: "Empirically, we verify that ralign is a safety reward by analyzing its distributions on different types of responses"
  - [corpus]: Weak evidence - the corpus contains papers on safety disalignment but doesn't directly confirm the log-likelihood difference as a reward function.
- Break condition: If the log-likelihood difference doesn't accurately represent the safety reward function, or if the safety alignment process doesn't use a reward-based approach.

### Mechanism 2
- Claim: Sampling from the contrastive distribution defined by pre-trained and safety-aligned models can emulate the result of fine-tuning to minimize safety reward.
- Mechanism: The sampling distribution πemulated-disalign(yt|x, y<t) ∝ πbase(yt|x, y<t)^(α+1) / πalign(yt|x, y<t)^α approximates the result of adversarial fine-tuning without requiring actual training.
- Core assumption: The per-token approximation to the sequence-level distribution is effective enough to produce harmful outputs.
- Evidence anchors:
  - [section 4]: "Crucially, such adversarial fine-tuning...can be emulated by sampling from a contrastive distribution defined by the pre-trained and safety-aligned models"
  - [section 6]: "Emulated disalignment surprisingly outperforms resource-intensive direct disalignment"
  - [corpus]: Moderate evidence - the corpus shows research on disalignment methods but doesn't specifically validate the sampling approximation.
- Break condition: If the per-token approximation fails to capture sequence-level dependencies, or if the sampling distribution doesn't converge to the desired harmful distribution.

### Mechanism 3
- Claim: Safety alignment creates vulnerabilities that can be exploited through output token distribution manipulation.
- Mechanism: The safety alignment process inherently creates a measurable difference in token distributions between aligned and pre-trained models, which can be manipulated at inference time to reverse alignment.
- Core assumption: Safety alignment fundamentally changes the token distribution in ways that can be measured and exploited.
- Evidence anchors:
  - [abstract]: "This attack builds on the following intuition: The more effort invested in aligning a language model to be safe, the greater the potential for harm if adversaries can reverse the alignment direction"
  - [section 4.2]: "ED as contrastive decoding...amplify the harmfulness exhibited by a pre-trained model by contrasting it with a safety-aligned model"
  - [corpus]: Strong evidence - multiple papers in the corpus discuss vulnerabilities in safety alignment and disalignment methods.
- Break condition: If safety alignment doesn't create measurable differences in token distributions, or if output distribution access is restricted.

## Foundational Learning

- Concept: Reward maximization and minimization in reinforcement learning
  - Why needed here: Understanding how safety alignment can be reversed by minimizing a safety reward function is central to grasping ED's mechanism.
  - Quick check question: How does minimizing a reward function differ from maximizing it in the context of language model fine-tuning?

- Concept: Contrastive learning and decoding
  - Why needed here: ED uses contrastive decoding to shift token predictions towards harmful content by contrasting aligned and pre-trained model outputs.
  - Quick check question: What is the difference between standard decoding and contrastive decoding in language models?

- Concept: Per-token vs. sequence-level sampling distributions
  - Why needed here: ED uses per-token approximation to sequence-level distributions to avoid cumbersome fine-tuning while still achieving harmful outputs.
  - Quick check question: Why might per-token sampling be preferred over sequence-level sampling in this context?

## Architecture Onboarding

- Component map: Pre-trained model -> Safety-aligned model -> Sampling mechanism with hyperparameter α -> Evaluation framework
- Critical path: Access pre-trained and safety-aligned model outputs → Compute contrastive sampling distribution → Sample from distribution → Evaluate harmfulness
- Design tradeoffs: Training-free attack vs. effectiveness (ED vs. direct disalignment), open-source model requirement vs. practical applicability, hyperparameter tuning complexity vs. attack success
- Failure signatures: Harmfulness rates not significantly higher than baselines, performance degradation at high α values, cross-family model attacks less effective than within-family attacks
- First 3 experiments:
  1. Implement ED with Llama-2 and Llama-2-chat models on a small subset of Anthropic-HH dataset to verify basic functionality
  2. Vary α parameter to observe its effect on harmfulness rates and identify optimal values
  3. Compare ED against BaseMP and EDBase baselines on multiple datasets to establish effectiveness improvements

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can emulated disalignment be used to attack proprietary black-box language models that only reveal limited token distribution information?
- Basis in paper: [explicit] The paper notes that "Some black-box LLMs (e.g., GPT-4) do allow limited visibility into their output token distributions, such as showing the log-likelihoods of the top-5 tokens" and wonders "if this limited transparency is enough to undo efforts to make these models safe."
- Why unresolved: The paper only mentions this as a future work question and doesn't test whether ED can work with such limited information from black-box models.
- What evidence would resolve it: Experiments showing whether ED can successfully attack black-box models when given only top-k token distributions, or theoretical analysis proving the minimum token distribution information needed for ED to work.

### Open Question 2
- Question: Does emulated disalignment generalize to language models with different vocabularies?
- Basis in paper: [explicit] The paper states "Since the Llama-1, Llama-2, and Alpaca families share the same Llama-1 vocabulary, we can test ED on model pairs from different families" and notes that "techniques like cross-vocabulary test-time search might help" but doesn't test this.
- Why unresolved: The paper only demonstrates ED within vocabulary-sharing families and mentions cross-vocabulary attacks as future work without testing them.
- What evidence would resolve it: Successful application of ED to models with different vocabularies using cross-vocabulary techniques, or proof that ED fundamentally requires shared vocabularies.

### Open Question 3
- Question: What defense mechanisms can effectively protect against emulated disalignment attacks?
- Basis in paper: [explicit] The paper identifies this as an open question, stating "For open-source models, this may involve designing more robust alignment algorithms during training. For closed-source models, the challenge is balancing output token distribution transparency with the risk of misuse."
- Why unresolved: The paper only identifies the need for defenses but doesn't propose or test any specific defense mechanisms against ED.
- What evidence would resolve it: Development and validation of training-time alignment techniques that are resistant to ED, or evaluation of different levels of token distribution transparency that balance usability with security.

## Limitations
- Requires access to both pre-trained and safety-aligned model outputs, limiting applicability to closed-source models
- Per-token approximation to sequence-level distributions introduces approximation error that may reduce effectiveness
- Hyperparameter α requires careful tuning, with high values potentially causing nonsensical outputs
- Relies on automated safety classifiers for evaluation, which may have their own limitations and biases

## Confidence

**High confidence**: The core mechanism of using contrastive sampling between pre-trained and safety-aligned models is technically sound and well-supported by the mathematical formulation. The empirical observation that stronger alignment creates greater potential for reversal through ED is consistently demonstrated across multiple datasets and model families.

**Medium confidence**: The claim that ED outperforms resource-intensive direct disalignment methods is supported by experimental results, but the comparison may be influenced by implementation details and hyperparameter choices. The assertion that ED doubles the harmfulness of pre-trained models is statistically significant but may vary with different evaluation settings.

**Low confidence**: The generalizability of ED across all safety alignment approaches is uncertain, as the method assumes specific alignment mechanisms (likely reward-based fine-tuning). The long-term effectiveness of ED against evolving safety alignment techniques remains unknown.

## Next Checks

1. **Cross-alignment method validation**: Test ED against safety-aligned models trained using different alignment approaches (Constitutional AI, supervised fine-tuning, reinforcement learning from human feedback) to verify the method's robustness across alignment techniques.

2. **Adversarial robustness evaluation**: Implement defensive mechanisms such as output filtering, temperature adjustment, or contrastive decoding with safety-aligned models to measure their effectiveness in mitigating ED attacks.

3. **Real-world deployment simulation**: Conduct red-teaming exercises where human evaluators assess the practical harmfulness of ED-generated outputs in realistic scenarios, comparing results with automated classifier evaluations to validate the true safety implications.