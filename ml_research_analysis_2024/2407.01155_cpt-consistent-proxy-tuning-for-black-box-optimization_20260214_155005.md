---
ver: rpa2
title: 'CPT: Consistent Proxy Tuning for Black-box Optimization'
arxiv_id: '2407.01155'
source_url: https://arxiv.org/abs/2407.01155
tags:
- black-box
- tuning
- white-box
- proxy-tuning
- fine-tuning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of tuning black-box large language
  models (LLMs) and vision-language models (VLMs) without access to their internal
  parameters. The proposed method, Consistent Proxy Tuning (CPT), improves upon existing
  proxy-tuning techniques by ensuring consistency between the training objective and
  test-time inference.
---

# CPT: Consistent Proxy Tuning for Black-box Optimization

## Quick Facts
- arXiv ID: 2407.01155
- Source URL: https://arxiv.org/abs/2407.01155
- Reference count: 40
- CPT improves proxy-tuning performance by 2.41% (LLMs) and 1.24% (VLMs) in mean accuracy

## Executive Summary
This paper introduces Consistent Proxy Tuning (CPT), a novel approach for optimizing black-box large language models (LLMs) and vision-language models (VLMs) without access to their internal parameters. CPT addresses a fundamental limitation in proxy-tuning methods by ensuring consistency between training objectives and test-time inference. The method achieves this by incorporating both frozen black-box and white-box models into the training process of a tunable small white-box model. Experimental results demonstrate significant performance improvements over existing proxy-tuning techniques across multiple datasets and task types.

## Method Summary
CPT introduces a consistency-aware training framework that resolves the mismatch between training objectives and test-time inference in black-box optimization. The core innovation lies in training a tunable white-box proxy model while simultaneously incorporating both frozen black-box and white-box models during the training process. This dual incorporation ensures that the proxy model's predictions remain consistent with both models during training and inference. The method employs a consistency loss that aligns the proxy model's outputs with the frozen models, creating a more robust optimization process that better generalizes to test-time scenarios where only the black-box model is available.

## Key Results
- CPT outperforms proxy-tuning by 2.41% mean accuracy on LLM tasks
- CPT achieves 1.24% mean accuracy improvement on VLM tasks
- Method demonstrates model-agnostic applicability across various datasets

## Why This Works (Mechanism)
CPT works by addressing the fundamental inconsistency between training objectives and test-time inference in black-box optimization. During training, the proxy model has access to both the frozen black-box model and a frozen white-box model, allowing it to learn from multiple perspectives. However, at test time, only the black-box model is available. CPT bridges this gap by incorporating both models during training through a consistency loss, ensuring that the proxy model's behavior remains aligned with what will be available at inference. This consistency-aware training prevents the proxy model from developing dependencies on information that won't be available during actual use, resulting in better generalization and performance.

## Foundational Learning

**Black-box optimization**: Understanding how to optimize models without access to their parameters is crucial for practical applications where model access is restricted. Quick check: Can you explain the difference between white-box and black-box optimization approaches?

**Proxy-tuning**: This technique involves training a smaller, accessible model to approximate the behavior of a larger, inaccessible model. Quick check: What are the key challenges in traditional proxy-tuning methods?

**Consistency loss**: The mathematical formulation that ensures alignment between training-time and inference-time behavior. Quick check: How does consistency loss differ from standard cross-entropy loss in training objectives?

**Logit-level classification**: Working with raw model outputs before softmax activation, which provides richer information for optimization. Quick check: Why might working at the logit level be advantageous compared to working with probabilities?

**Model-agnostic approaches**: Techniques that can be applied across different model architectures and sizes without modification. Quick check: What are the benefits and challenges of developing model-agnostic optimization methods?

## Architecture Onboarding

**Component map**: Tunable small white-box proxy model <- Frozen black-box model + Frozen white-box model -> Consistency loss -> Optimized proxy model

**Critical path**: The essential training pipeline involves passing inputs through both frozen models, computing consistency loss between their outputs and the proxy model's predictions, and updating the proxy model parameters accordingly.

**Design tradeoffs**: The method requires maintaining two frozen models during training, which increases computational overhead but provides the consistency benefits. The choice of white-box proxy model architecture involves balancing expressiveness against computational efficiency.

**Failure signatures**: Poor performance may occur when the frozen white-box model is poorly tuned for the target task, when there's a large domain gap between the black-box and white-box models, or when the consistency loss is not properly weighted.

**Three first experiments**:
1. Validate CPT on a simple classification task with known black-box and white-box model pairs
2. Test the impact of consistency loss weight on overall performance
3. Compare CPT performance against proxy-tuning with varying proxy model sizes

## Open Questions the Paper Calls Out

None

## Limitations

- Requires access to a well-tuned white-box proxy model, which may not always be available
- Computational overhead of maintaining and training with multiple frozen models
- Limited evaluation to classification tasks at the logit level, with unclear performance on generation or structured prediction tasks

## Confidence

High Confidence: The core mathematical formulation of CPT and its consistency objective is well-defined and internally coherent. The improvement metrics over proxy-tuning for the tested tasks are clearly demonstrated.

Medium Confidence: The claim that CPT is model-agnostic and broadly applicable across different tasks. While the method is theoretically general, the empirical evidence is limited to specific scenarios.

Low Confidence: The assertion that CPT significantly reduces computational overhead compared to direct fine-tuning of black-box models. The paper does not provide detailed computational complexity analysis or runtime comparisons.

## Next Checks

1. Test CPT's performance on non-classification tasks such as text generation, summarization, or structured output prediction to validate its broader applicability beyond logit-level tasks.

2. Conduct experiments with a wider range of black-box model sizes and architectures, including smaller and larger models than those tested, to assess scalability and robustness across different scenarios.

3. Perform an ablation study to quantify the contribution of each component in the CPT framework (e.g., the frozen black-box model, white-box proxy, and consistency loss) to identify potential simplifications or optimizations.