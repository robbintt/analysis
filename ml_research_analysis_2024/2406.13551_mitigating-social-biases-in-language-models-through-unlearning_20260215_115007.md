---
ver: rpa2
title: Mitigating Social Biases in Language Models through Unlearning
arxiv_id: '2406.13551'
source_url: https://arxiv.org/abs/2406.13551
tags:
- pcgu
- bias
- task
- base
- llama-2
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares two machine unlearning methods for mitigating
  social biases in language models. The first, Partitioned Contrastive Gradient Unlearning
  (PCGU), selectively updates model weights based on contrastive gradient similarity.
---

# Mitigating Social Biases in Language Models through Unlearning

## Quick Facts
- arXiv ID: 2406.13551
- Source URL: https://arxiv.org/abs/2406.13551
- Reference count: 24
- One-line primary result: Task Vector method reduces bias more effectively than PCGU while maintaining better model quality

## Executive Summary
This paper introduces two machine unlearning methods for mitigating social biases in language models: Partitioned Contrastive Gradient Unlearning (PCGU) and Negation via Task Vector. The methods are evaluated on OPT and LLaMA-2 models across multiple sizes. The Task Vector method outperforms PCGU in debiasing effectiveness while preserving model quality and perplexity. Specifically, on LLaMA-27B, Task Vector achieved 11.8% bias reduction with minimal performance impact, whereas PCGU caused significant degradation in both perplexity and task accuracy despite similar bias reduction levels.

## Method Summary
The paper proposes two unlearning approaches for bias mitigation. PCGU selectively updates model weights based on contrastive gradient similarity between advantaged and disadvantaged terms, partitioning weights and updating only those with low gradient similarity scores. The Task Vector method fine-tunes the model on biased data to create a task vector, then negates this vector to remove bias. Both methods aim to reduce bias as measured by CrowS-Pairs while maintaining perplexity and task performance. The methods are tested on OPT (1.3B, 2.7B, 6.7B) and LLaMA-2 (7B) models using datasets including BBQ, StereoSet, and Civil Comments for training, with evaluation on WikiText-2 for perplexity and various benchmarks for task performance.

## Key Results
- Task Vector method reduces bias by 11.8% on LLaMA-27B with minimal impact on model quality
- PCGU achieves similar bias reduction but causes significant degradation in perplexity and task accuracy
- Task Vector maintains superior performance across all model sizes tested
- Fine-tuning on biased data combines bias reduction with language modeling constraints, providing better regularization than PCGU's isolated weight updates

## Why This Works (Mechanism)

### Mechanism 1: Task Vector Direction Negation
The Task Vector method works by isolating bias as a consistent direction in weight space that can be negated. Fine-tuning on biased data creates a task vector representing the bias direction, which is then scaled and negated to reduce bias while preserving other capabilities.

### Mechanism 2: Selective Weight Updates via Gradient Similarity
PCGU works by identifying and updating only the weight vectors that contribute most to bias, based on contrastive gradient similarity between advantaged and disadvantaged terms. This targeted approach aims to modify bias-contributing weights while preserving the rest.

### Mechanism 3: Joint Training Regularization
The Task Vector method preserves model quality because fine-tuning on biased data combines bias reduction with the language modeling task, providing natural constraints that prevent catastrophic forgetting of general capabilities.

## Foundational Learning

- **Concept**: Contrastive gradient similarity
  - **Why needed**: PCGU relies on comparing gradients of advantaged and disadvantaged terms to identify bias-contributing weights
  - **Quick check**: How would you compute the cosine similarity between two gradient vectors to identify which weights contribute most to bias?

- **Concept**: Task arithmetic and parameter-efficient fine-tuning
  - **Why needed**: The Task Vector method uses task vectors and LoRA to efficiently apply bias reduction without full fine-tuning
  - **Quick check**: How does LoRA reduce the number of trainable parameters while still allowing task-specific adaptation?

- **Concept**: Perplexity as a proxy for generation quality
  - **Why needed**: Both methods are evaluated on perplexity to ensure bias reduction does not harm the model's ability to generate coherent text
  - **Quick check**: What does a significant increase in perplexity after debiasing indicate about the model's generation ability?

## Architecture Onboarding

- **Component map**: Dataset preprocessing (BBQ, StereoSet, Civil Comments) → Bias measurement (CrowS-Pairs) → PCGU with gradient partitioning OR TV with task vector negation → Evaluation (perplexity, task performance)
- **Critical path**: For TV: preprocess biased data → fine-tune to get biased model → compute task vector → negate and scale → apply to base model. For PCGU: preprocess BBQ data → partition weights → compute gradients → update selected weights → measure bias and perplexity
- **Design tradeoffs**: PCGU is more targeted but brittle and computationally heavy; TV is smoother and more efficient but requires careful scaling. PCGU may harm generation quality more than TV
- **Failure signatures**: PCGU: high perplexity, random token repetition, poor TriviaQA scores. TV: bias reduction plateaus, task performance drops with high λ, incoherent generations at extreme λ
- **First 3 experiments**:
  1. Run TV debiasing with λ = 0.2, 0.6, 1.0 on LLaMA-2 7B and compare CrowS bias and perplexity
  2. Run PCGU with k = 20%, 25%, 30% on OPT 2.7B and compare bias reduction vs perplexity increase
  3. Compare generation quality qualitatively on BOLD prompts for both methods at their best-performing hyperparameters

## Open Questions the Paper Calls Out

### Open Question 1
How can the ranking procedure for selecting weight vectors in PCGU be improved to incorporate the flexibility of weights rather than just contrastive gradient similarity? The paper observes that some weight intervals are more "rigid" than others, suggesting that flexibility is an important factor that's currently not accounted for in the ranking process.

### Open Question 2
What would be the effect of combining task vectors for bias reduction with task vectors for preserving specific capabilities on the overall model performance? The paper suggests this as a future direction: "the performance drop can be improved by fine-tuning the model for a particular task and obtaining the corresponding task vector."

### Open Question 3
How does the effectiveness of machine unlearning methods scale with model size beyond 7B parameters? The experiments were conducted on models up to 7B parameters due to computational limitations, leaving uncertainty about performance on larger models like LLaMA-2 13B or 70B.

## Limitations
- PCGU implementation details, particularly weight partitioning strategy and gradient similarity calculation, are not fully specified
- Task Vector method's LoRA configurations and fine-tuning procedures lack complete documentation
- Results may not generalize to model sizes beyond 7B parameters due to computational constraints
- Qualitative generation analysis is missing, relying primarily on perplexity metrics for quality assessment

## Confidence

- **High confidence**: The comparative results showing Task Vector's superior bias reduction with minimal quality degradation are well-supported by experimental data
- **Medium confidence**: The mechanisms explaining why Task Vector preserves model quality better than PCGU are plausible but not definitively proven
- **Low confidence**: The generalizability of results across different model sizes and architectures is not fully established, particularly regarding optimal hyperparameter ranges

## Next Checks

1. Conduct ablation studies on PCGU's weight partitioning granularity and Task Vector's scaling coefficient λ to identify optimal ranges for different model sizes
2. Perform qualitative analysis of generated text before and after debiasing to assess generation quality beyond perplexity metrics
3. Test both methods on additional bias benchmarks (e.g., Winogender, StereoSet) to verify robustness of bias reduction across different bias types and evaluation frameworks