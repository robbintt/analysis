---
ver: rpa2
title: 'Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations
  in Large Language Models'
arxiv_id: '2402.19103'
source_url: https://arxiv.org/abs/2402.19103
tags:
- 'false'
- uni00000013
- premise
- attention
- heads
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes and mitigates false premise hallucinations
  in large language models (LLMs), where models generate hallucinated responses when
  confronted with false premise questions. The authors propose an automatic dataset
  construction pipeline and create two representative datasets to facilitate analysis.
---

# Whispers that Shake Foundations: Analyzing and Mitigating False Premise Hallucinations in Large Language Models

## Quick Facts
- arXiv ID: 2402.19103
- Source URL: https://arxiv.org/abs/2402.19103
- Authors: Hongbang Yuan; Pengfei Cao; Zhuoran Jin; Yubo Chen; Daojian Zeng; Kang Liu; Jun Zhao
- Reference count: 13
- One-line primary result: Constraining ~1% of attention heads reduces false premise hallucinations by ~20%

## Executive Summary
This paper investigates false premise hallucinations in LLMs, where models generate incorrect responses when presented with questions containing false information. The authors develop an automatic dataset construction pipeline and create two representative datasets (Prize and Movie) to systematically study this phenomenon. Through detailed analysis of attention mechanisms, they identify specific "false premise attention heads" in shallow layers that disrupt knowledge extraction and lead to hallucinations. Building on this insight, they propose FAITH, a method that constrains these problematic attention heads during inference, achieving significant performance improvements with minimal impact on overall model capability.

## Method Summary
The authors propose FAITH (False premise Attention head constraIning for miTigating Hallucinations), a novel approach that identifies and constrains false premise attention heads during inference. The method involves three key steps: first, constructing datasets with false premise questions using an automatic pipeline; second, analyzing attention head influence on factual knowledge extraction to identify false premise heads; and third, applying constraints to these heads during inference to prevent hallucination. The approach focuses on the observation that a small subset of attention heads in shallow layers are primarily responsible for false premise hallucinations, allowing for targeted mitigation without degrading overall model performance.

## Key Results
- Constraining only ~1% of attention heads yields nearly 20% improvement in model performance
- False premise attention heads are concentrated in shallow layers and focus on false object tokens
- Hallucinated answers show significantly higher uncertainty (correlation >0.9 for 13B model) than non-hallucinated answers

## Why This Works (Mechanism)

### Mechanism 1
- Claim: False premise attention heads in shallow layers disturb knowledge extraction by focusing on the false object token, causing hallucinated outputs.
- Mechanism: The identified false premise attention heads concentrate their attention on the region around the false object in the input sequence, disregarding the semantic connection between the false object and the subject. This localized focus disrupts the model's ability to correctly extract factual knowledge about the subject, leading to hallucination.
- Core assumption: Attention heads in shallow layers are responsible for initial information routing and disturbance can propagate to later layers.
- Evidence anchors:
  - [abstract] "a small subset of attention heads (which we designate as false premise heads) disturb the knowledge extraction process, leading to the occurrence of false premise hallucination."
  - [section 4.3] "We observe that a few attention heads exert significantly greater influence than others. We conclude that these attention heads will have to take the blame for the false premise hallucination and we designate them as False Premise Attention Heads."
  - [corpus] "Seeing Clearly by Layer Two: Enhancing Attention Heads to Alleviate Hallucination in LVLMs" - this related work also targets attention heads for hallucination mitigation.

### Mechanism 2
- Claim: Constraining the false premise attention heads around the false object tokens during inference significantly reduces hallucination without degrading overall performance.
- Mechanism: By zeroing out the attention weights of the identified false premise heads for the false object token region, the model is prevented from giving undue importance to the misleading information in that region. This allows the model to better extract the correct factual knowledge about the subject from other attention heads and layers.
- Core assumption: The false premise attention heads are not essential for correct knowledge extraction in the presence of false premises, and their influence can be safely removed.
- Evidence anchors:
  - [abstract] "constraining only approximately 1% of the attention heads in the model yields a notable increase of nearly 20% of model performance."
  - [section 5.1] "To eliminate the impact of these false premise heads and mitigate hallucinations, we constrain these attention heads around the false object tokens during the model inference process."
  - [corpus] "Mitigating Hallucinations in Vision-Language Models through Image-Guided Head Suppression" - this related work also uses head suppression for hallucination mitigation.

### Mechanism 3
- Claim: The model exhibits higher uncertainty when generating hallucinated answers compared to non-hallucinated answers, and this uncertainty can be measured and used for detection.
- Mechanism: When the model is led astray by the false premise and generates a hallucinated answer, the internal probability distributions are more spread out or uncertain. This manifests as higher perplexity and lower log-likelihood scores. By measuring this uncertainty, we can detect when the model is likely to hallucinate.
- Core assumption: The model's internal uncertainty is correlated with the correctness of its outputs, and this correlation can be quantified.
- Evidence anchors:
  - [section 4.1] "We observe a strong correlation (over 0.9 for the 13B model) between the occurrence of hallucinations and model uncertainty."
  - [section 4.1] "Our semantic-based uncertainty metric score is far more effective than the other two methods. It can be further employed in the prediction of the occurrence of false premise hallucinations without relying on an external knowledge base."

## Foundational Learning

- Concept: Attention mechanisms in transformers
  - Why needed here: The paper heavily relies on analyzing the behavior of individual attention heads and their contribution to hallucination. Understanding how attention works is crucial to grasp the proposed mechanisms.
  - Quick check question: What is the role of the attention pattern matrix A in a transformer layer, and how does it determine which tokens a head focuses on?

- Concept: Residual connections and layer stacking
  - Why needed here: The paper discusses how information flows through the layers and how disturbance in shallow layers can affect deeper layers. Understanding the residual architecture is key to following this analysis.
  - Quick check question: How does the residual connection in a transformer layer (xl = xl-1 + al + ml) allow information from earlier layers to bypass later transformations?

- Concept: Knowledge representation in LLMs
  - Why needed here: The paper is fundamentally about how factual knowledge is stored and recalled in LLMs, and how false premises can disrupt this process. A basic understanding of how LLMs store and retrieve knowledge is essential.
  - Quick check question: In the context of the paper, what is meant by "factual knowledge about the subject", and how is it typically extracted by the model?

## Architecture Onboarding

- Component map:
  - Input question tokens
  - False premise attention heads (shallow layers)
  - Other attention heads (all layers)
  - MLP layers
  - Output logits
  - Knowledge assessment task (for analysis)
  - Beam search decoding (for experiments)

- Critical path:
  1. Question is encoded into token embeddings
  2. False premise attention heads in shallow layers focus on false object region
  3. This disturbs the knowledge extraction process
  4. Other heads and layers try to counteract the disturbance
  5. Model generates answer based on the final hidden state
  6. If false premise heads were not constrained, answer is likely hallucinated

- Design tradeoffs:
  - Constraining false premise heads improves hallucination mitigation but may slightly reduce model flexibility
  - Using beam search decoding reduces the impact of individual decoding errors but increases computation
  - Focusing on shallow layers for analysis is efficient but may miss some deeper layer effects

- Failure signatures:
  - If false premise heads are not correctly identified, the method will not be effective
  - If the model has a different architecture where shallow layers don't perform the same routing function, the analysis may not apply
  - If the knowledge assessment task does not accurately reflect the model's ability to recall facts, the influence calculation may be misleading

- First 3 experiments:
  1. Implement the head influence calculation (Section 4.3) on a small toy model to verify it correctly identifies influential heads.
  2. Apply the FAITH head constraining method (Section 5.1) to a simple question-answering task and measure the impact on hallucination rate.
  3. Measure the uncertainty of the model's outputs (Section 4.1) on a set of questions with known answers, and verify that hallucinated answers have higher uncertainty.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do false premise attention heads interact with other attention heads in multi-head attention mechanisms?
- Basis in paper: [inferred] The paper identifies false premise attention heads and their influence on factual knowledge extraction, but does not explore their interaction with other attention heads.
- Why unresolved: The paper focuses on individual attention head influence and does not investigate the collective behavior of multiple attention heads, especially false premise heads in combination with others.
- What evidence would resolve it: Experiments examining the combined effects of multiple attention heads, particularly false premise heads with other types, on model performance and hallucination mitigation.

### Open Question 2
- Question: Can the proposed FAITH method be extended to other types of hallucinations beyond false premise hallucinations?
- Basis in paper: [explicit] The paper specifically addresses false premise hallucinations but does not explore the applicability of FAITH to other hallucination types.
- Why unresolved: The paper's analysis and mitigation method are tailored to false premise hallucinations, leaving open the question of generalizability to other hallucination types.
- What evidence would resolve it: Experiments applying FAITH or a modified version to different types of hallucinations, such as repetitive or contradictory hallucinations, to assess its effectiveness and potential adaptations.

### Open Question 3
- Question: What are the long-term effects of constraining false premise attention heads on model performance and knowledge retention?
- Basis in paper: [inferred] The paper demonstrates short-term effectiveness of FAITH in mitigating false premise hallucinations but does not address long-term consequences.
- Why unresolved: The paper focuses on immediate performance improvements without considering potential impacts on the model's ability to learn and retain knowledge over time.
- What evidence would resolve it: Longitudinal studies tracking model performance and knowledge retention with and without constrained false premise attention heads across multiple training epochs and tasks.

## Limitations

- Analysis is limited to false premise hallucinations and two specific dataset domains (prizes and movies)
- FAITH method requires careful tuning of which heads to constrain and may have long-term impacts on model performance
- Uncertainty-based hallucination detection relies on specific metrics that may not capture all forms of hallucination

## Confidence

**High Confidence Claims:**
- Identification of false premise attention heads in shallow layers and their role in disturbing knowledge extraction is well-supported by presented evidence
- FAITH method's effectiveness in reducing hallucinations by constraining ~1% of attention heads is convincingly demonstrated
- Hallucinated answers exhibit higher uncertainty than non-hallucinated answers with robust correlation scores

**Medium Confidence Claims:**
- Generalizability of false premise attention head concept to other hallucination types or model architectures is plausible but not directly tested
- Assumption that constraining false premise heads won't significantly impact non-hallucination tasks is reasonable but long-term effects unexplored
- Specific threshold values for selecting false premise attention heads are not thoroughly justified

**Low Confidence Claims:**
- Paper doesn't provide evidence that false premise attention heads aren't essential for correct knowledge extraction in some cases
- Impact of FAITH method on handling other types of misinformation or ambiguous queries is unexplored

## Next Checks

1. **Cross-Hypothesis Validation**: Test FAITH method on diverse hallucination types (confabulation, repetition) and model architectures to verify generalizability of false premise attention head concept.

2. **Ablation Study**: Conduct ablation study where different subsets of identified false premise attention heads are constrained to determine minimum set required for effective hallucination mitigation and assess performance trade-offs.

3. **Long-Term Impact Analysis**: Evaluate long-term impact of FAITH method on model's overall performance by testing on wide range of tasks and queries unrelated to hallucination to ensure constraining false premise heads doesn't degrade general language understanding.