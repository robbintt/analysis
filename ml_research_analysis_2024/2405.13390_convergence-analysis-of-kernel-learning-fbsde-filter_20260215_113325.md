---
ver: rpa2
title: Convergence analysis of kernel learning FBSDE filter
arxiv_id: '2405.13390'
source_url: https://arxiv.org/abs/2405.13390
tags:
- convergence
- density
- rate
- kernel
- state
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper provides a rigorous convergence analysis of the kernel
  learning forward backward SDE (FBSDE) filter, an adaptive mesh-free method for solving
  nonlinear filtering problems. The method estimates evolving posterior density by
  combining FBSDE equations for state variable density with kernel density estimation
  (KDE) to learn the density from samples.
---

# Convergence analysis of kernel learning FBSDE filter

## Quick Facts
- **arXiv ID**: 2405.13390
- **Source URL**: https://arxiv.org/abs/2405.13390
- **Authors**: Yunzheng Lyu; Feng Bao
- **Reference count**: 24
- **Primary result**: Provides rigorous convergence analysis of kernel learning FBSDE filter, proving local and global convergence with rates of L^{-2/(4+d_x)} for KDE, 1/\sqrt{M} for fixed-point iteration, and \sqrt{\Delta t} or \Delta t for numerical discretization.

## Executive Summary
This paper presents a rigorous convergence analysis of the kernel learning forward backward stochastic differential equation (FBSDE) filter, an adaptive mesh-free method for solving nonlinear filtering problems. The method combines FBSDE equations for state variable density evolution with kernel density estimation (KDE) to learn the density function from samples. The theoretical analysis proves both local and global convergence under suitable assumptions, with convergence rates depending on the sample size, kernel bandwidth, and numerical discretization parameters. The global convergence is maintained only for partial samples with sufficiently small likelihood densities across all time steps.

## Method Summary
The kernel learning FBSDE filter estimates the evolving posterior density by combining FBSDE equations for state variable density with kernel density estimation to learn the density from samples. The algorithm propagates prior density using FBSDE, updates posterior via Bayesian inference, and learns the density function from samples using KDE. The convergence analysis proves local and global convergence under assumptions including Lipschitz continuity of drift and diffusion coefficients, bounded derivatives, and appropriate bandwidth selection in KDE. The theoretical framework provides convergence rates for each component: O(L^(-2/(4+dx))) for KDE, O(1/√M) for fixed-point iteration, and O(√Δt) or O(Δt) for numerical discretization.

## Key Results
- The algorithm achieves pointwise convergence to true posterior density through combination of fixed-point iteration and kernel density estimation
- Uniform convergence rate of 1/√M is established for the fixed-point iteration scheme solving the FBSDE system
- Global convergence is maintained only for partial samples with sufficiently small likelihood densities across all time steps
- The method demonstrates superior convergence speed and efficiency compared to mainstream particle filter methods in high-dimensional problems

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The kernel learning FBSDE filter achieves pointwise convergence to the true posterior density through a combination of fixed-point iteration and kernel density estimation.
- **Mechanism**: The algorithm propagates prior density using FBSDE equations, updates posterior via Bayesian inference, and learns the density function from samples using KDE. The KDE convergence rate of O(L^(-2/(4+dx))) combined with fixed-point iteration error of O(1/√M) and numerical discretization error of O(√Δt) ensures overall convergence.
- **Core assumption**: The underlying state variable density can be accurately represented by a kernel density estimator with sufficiently large sample size and bandwidth.
- **Evidence anchors**:
  - [abstract]: "The method estimates evolving posterior density by combining FBSDE equations for state variable density with kernel density estimation (KDE) to learn the density from samples."
  - [section]: "From theorem 4.1.2, we can conclude that for multi-dimensional state variable x∈Rdx, Gaussian KDE converges with a polynomial order of 4/(4+dx) to true underlying density."
  - [corpus]: Weak evidence - no direct references to kernel learning FBSDE convergence in corpus papers.
- **Break condition**: If the bandwidth selection in KDE is inappropriate (too large or too small), the density estimation will not converge to the true posterior, breaking the overall algorithm convergence.

### Mechanism 2
- **Claim**: The fixed-point iteration scheme for solving the FBSDE system converges uniformly with rate 1/√M.
- **Mechanism**: Instead of traditional fixed-point iteration, the algorithm uses left-point approximation for the drift integral, which avoids contraction mapping requirements and provides uniform convergence guarantees.
- **Core assumption**: The drift function g and its derivatives are bounded, and the state transition can be accurately approximated by the discrete-time update.
- **Evidence anchors**:
  - [section]: "Lemma 4.2.1... For∀ x∈Rdx: limM→∞Y_Otk−1,N,Mk(x)=p(Sk=x|Ot−1) where Sk is a discrete time update of Stk..."
  - [section]: "The uniform convergence rate of mean squared error is 1/M."
  - [corpus]: No direct evidence found in corpus papers regarding fixed-point iteration convergence for FBSDE filters.
- **Break condition**: If the state transition function has discontinuities or unbounded derivatives, the left-point approximation error will not converge uniformly, breaking the fixed-point iteration scheme.

### Mechanism 3
- **Claim**: The Bayesian update step converges uniformly to the true posterior density with rate 1/√N and 1/√M.
- **Mechanism**: The algorithm computes weights directly through Bayesian inference without resampling, which prevents sample degeneracy and maintains efficiency. The convergence rate combines the prediction step error and sampling error.
- **Core assumption**: The likelihood function relating state variables to observation data is well-behaved (Lipschitz continuous and bounded).
- **Evidence anchors**:
  - [section]: "Lemma 4.3.1... The uniform convergence rate of RMSE is 1/√M and 1/√N."
  - [abstract]: "This algorithm has shown more superior performance than mainstream particle filter method, in both convergence speed and efficiency of solving high dimension problems."
  - [corpus]: Limited evidence - corpus papers focus on diffusion models and sampling rather than Bayesian update convergence analysis.
- **Break condition**: If the likelihood function becomes extremely peaked or multimodal, the uniform convergence guarantee may break down, requiring more sophisticated sampling strategies.

## Foundational Learning

- **Concept**: Forward-Backward Stochastic Differential Equations (FBSDE)
  - Why needed here: FBSDE forms the mathematical foundation for propagating the prior density of state variables in nonlinear filtering problems.
  - Quick check question: What is the relationship between FBSDE and parabolic SPDE, and why is this equivalence important for filtering problems?

- **Concept**: Kernel Density Estimation (KDE)
  - Why needed here: KDE is used to learn the density function from samples, providing a smooth approximation to the true posterior density.
  - Quick check question: How does the bandwidth selection in KDE affect the convergence rate, and what is the optimal bandwidth scaling for multi-dimensional problems?

- **Concept**: Nonlinear Filtering Theory
  - Why needed here: Understanding the mathematical framework for estimating hidden state variables from noisy observations is essential for implementing and analyzing the FBSDE filter.
  - Quick check question: What are the key differences between linear filtering (Kalman filter) and nonlinear filtering approaches, and why do nonlinear methods require more sophisticated algorithms?

## Architecture Onboarding

- **Component map**: State propagation module -> Bayesian update module -> Kernel learning module -> Resampling module -> Next time step
- **Critical path**: State propagation → Bayesian update → Kernel learning → Resampling → Next time step
- **Design tradeoffs**:
  - Fixed-point iteration vs. left-point approximation: Left-point approximation avoids contraction mapping requirements but may have different error characteristics
  - Kernel size L vs. sample size N: Larger kernel size improves density estimation but increases computational cost
  - Time step Δt vs. convergence rate: Smaller time steps improve accuracy but increase computational burden
- **Failure signatures**:
  - Divergence in fixed-point iteration: Indicates inappropriate step size or ill-conditioned FBSDE system
  - Poor density estimation: Suggests bandwidth selection issues in KDE or insufficient sample size
  - Sample degeneracy: May occur if resampling is not performed appropriately
- **First 3 experiments**:
  1. Implement the FBSDE filter for a simple linear Gaussian state-space model and compare convergence rates with analytical Kalman filter solution
  2. Test the kernel learning module on synthetic density estimation problems with known ground truth
  3. Evaluate the impact of different discretization schemes (Euler vs. Milstein) on overall filter performance for nonlinear systems

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the convergence rate change when using non-uniform kernel bandwidths instead of uniform ones in the KDE component?
- Basis in paper: [inferred] The paper mentions that "the bandwidth may not be uniform, and the algorithm does propose different λ for different samples, in which case the convergence rate can be faster than O(L−4/(4+dx))" but does not provide specific analysis.
- Why unresolved: The authors acknowledge this possibility but do not explore the theoretical implications or provide quantitative comparisons between uniform and non-uniform bandwidth approaches.
- What evidence would resolve it: Mathematical analysis showing the convergence rate bounds for various non-uniform bandwidth strategies, along with empirical comparisons demonstrating actual performance differences.

### Open Question 2
- Question: What is the optimal balance between fixed-point iteration steps (M) and gradient descent steps (S) in the KDE learning phase to minimize overall computational cost while maintaining convergence guarantees?
- Basis in paper: [explicit] The paper states "We need to assume that gradient descent steps are sufficiently large, so that empirical model YN,Mk will converge to the underlying KDE kernel model ˆYLtk(x" but does not provide guidance on the relationship between these parameters.
- Why unresolved: The authors focus on proving convergence under the assumption of "sufficiently large" steps without characterizing what constitutes sufficient size or the trade-offs involved.
- What evidence would resolve it: Theoretical bounds on the relationship between M and S, or empirical studies showing the impact of different M/S combinations on convergence speed and accuracy.

### Open Question 3
- Question: Under what conditions can global convergence be achieved for all samples rather than just those with sufficiently small likelihood densities?
- Basis in paper: [explicit] The paper states that "Only for partial sample points x whose likelihood density is sufficiently small across all time...can recurrence coefficient be kept below 1 and global convergence rate be obtained."
- Why unresolved: The authors identify this as a limitation of the current analysis but do not propose modifications to the algorithm or additional assumptions that would enable global convergence.
- What evidence would resolve it: Modified theoretical framework showing how to adjust the algorithm parameters or impose additional conditions to achieve global convergence, or empirical demonstrations of the current limitations in practice.

## Limitations

- The global convergence is maintained only for partial samples with sufficiently small likelihood densities across all time steps
- The theoretical analysis assumes bounded derivatives and Lipschitz continuity for drift and diffusion coefficients, which may be violated in practical applications
- Limited experimental validation across diverse nonlinear filtering problems, with confidence primarily based on theoretical proofs

## Confidence

- **High confidence**: The local convergence analysis for fixed-point iteration and numerical discretization, as these follow standard mathematical proofs with clear error bounds.
- **Medium confidence**: The global convergence results for KDE, as they rely on asymptotic theory that may not fully capture finite-sample behavior in high dimensions.
- **Low confidence**: The empirical performance claims, as the paper provides limited experimental validation across diverse nonlinear filtering problems.

## Next Checks

1. **Theoretical validation**: Verify the regularity conditions (Lipschitz continuity, linear growth bounds) for specific nonlinear filtering problems where the FBSDE filter is applied, checking whether the assumptions hold in practice.

2. **Numerical verification**: Implement a comprehensive suite of numerical experiments comparing the kernel learning FBSDE filter against particle filter baselines across different state-space dimensions, noise levels, and observation frequencies.

3. **Robustness assessment**: Test the algorithm's performance under violation of key assumptions, such as using non-smooth likelihood functions or discontinuous state transition dynamics, to identify failure modes and practical limitations.