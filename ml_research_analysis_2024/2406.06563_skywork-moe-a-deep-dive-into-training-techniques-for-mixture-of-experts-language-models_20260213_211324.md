---
ver: rpa2
title: 'Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language
  Models'
arxiv_id: '2406.06563'
source_url: https://arxiv.org/abs/2406.06563
tags:
- training
- experts
- loss
- expert
- rate
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents Skywork-MoE, a 146-billion-parameter MoE language
  model with 16 experts, trained by upcycling from a dense Skywork-13B checkpoint.
  The authors explore the effectiveness of upcycling versus training from scratch
  and find that upcycling is preferred when the MoE training budget is much smaller
  than the dense model training budget, while training from scratch is better when
  the MoE budget is at least twice the dense budget.
---

# Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models

## Quick Facts
- arXiv ID: 2406.06563
- Source URL: https://arxiv.org/abs/2406.06563
- Reference count: 21
- Skywork-MoE achieves 82.2 on CEVAL, 79.5 on CMMLU, and 77.4 on MMLU benchmarks

## Executive Summary
Skywork-MoE presents a 146-billion-parameter MoE language model upcycled from a dense Skywork-13B checkpoint, demonstrating that upcycling can be more effective than training from scratch when MoE budgets are constrained. The paper introduces two novel techniques: gating logit normalization to improve expert diversification and adaptive auxiliary loss coefficients for layer-specific optimization. These innovations enable Skywork-MoE to achieve state-of-the-art performance on multiple benchmarks while using 192 NVIDIA-HGX-A800 nodes for training.

## Method Summary
The paper explores training a 146B-parameter MoE model using upcycling from a dense 13B checkpoint versus training from scratch, finding that upcycling is preferred when the MoE training budget is much smaller than the dense model training budget. The authors introduce gating logit normalization to improve expert diversification and adaptive auxiliary loss coefficients for layer-specific adjustment. The model is trained using the Megatron-LM framework with Expert Data Parallelism (EDP) and custom pipeline parallelism on 192 NVIDIA-HGX-A800 nodes with 1536 A800-80G SXM GPUs. Training involves monitoring loss curves, expert similarity, and token drop rates to optimize performance.

## Key Results
- Skywork-MoE achieves 82.2 on CEVAL, 79.5 on CMMLU, and 77.4 on MMLU benchmarks
- Upcycling from dense checkpoint is preferred when MoE budget is much smaller than dense budget
- Novel gating logit normalization reduces expert similarity and improves model performance

## Why This Works (Mechanism)
The upcycling approach leverages pre-trained dense model knowledge, requiring fewer resources to achieve competitive performance compared to training from scratch. Gating logit normalization ensures balanced expert utilization by preventing gating collapse, while adaptive auxiliary loss coefficients allow for dynamic adjustment of regularization across different layers. The combination of these techniques with Expert Data Parallelism enables efficient training of large-scale MoE models while maintaining performance and stability.

## Foundational Learning

**Expert Data Parallelism (EDP)**
- Why needed: Enables efficient training of large MoE models by distributing expert parameters across devices
- Quick check: Verify that expert load is balanced across all available GPUs during training

**Gating Logit Normalization**
- Why needed: Prevents gating collapse by normalizing gating logits to ensure balanced expert selection
- Quick check: Monitor expert similarity metrics to confirm diversification improvement

**Adaptive Auxiliary Loss Coefficients**
- Why needed: Allows dynamic adjustment of regularization strength across different layers for optimal training
- Quick check: Track training loss curves to ensure smooth convergence without overfitting

## Architecture Onboarding

**Component Map**
Skywork-13B checkpoint -> Upcycling process -> Gating logit normalization -> Adaptive auxiliary loss coefficients -> Expert Data Parallelism -> Trained Skywork-MoE

**Critical Path**
Dense model initialization → MoE conversion → Training with gating normalization → Layer-specific loss adjustment → Expert load balancing → Performance optimization

**Design Tradeoffs**
- Resource efficiency vs. model performance in upcycling approach
- Expert diversity vs. computational overhead in gating normalization
- Layer-specific optimization vs. training complexity in adaptive loss coefficients

**Failure Signatures**
- High expert similarity indicates gating collapse
- Imbalanced expert load suggests normalization issues
- Training instability points to incorrect loss coefficient values

**First Experiments**
1. Test gating logit normalization on a small MoE model to verify expert diversification
2. Implement adaptive auxiliary loss coefficients on a single layer to validate effectiveness
3. Compare upcycling performance against training from scratch on a reduced-scale model

## Open Questions the Paper Calls Out
None specified in the paper.

## Limitations
- Significant computational requirements (192 NVIDIA-HGX-A800 nodes) limit reproducibility
- Proprietary nature of Skywork-13B checkpoint restricts independent validation
- Focus on specific hardware configuration may limit generalizability to other GPU setups

## Confidence

| Claim | Confidence |
|-------|------------|
| Upcycling vs. training from scratch effectiveness | Medium |
| Novel techniques (gating logit normalization and adaptive auxiliary loss coefficients) | Medium |
| Benchmark performance metrics | Medium |

## Next Checks
1. Reproduce the upcycling process on a different dense model checkpoint to verify generalizability of the approach
2. Implement and test the gating logit normalization and adaptive auxiliary loss coefficients on a smaller-scale MoE model
3. Conduct ablation studies to isolate the impact of each novel technique on model performance