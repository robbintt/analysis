---
ver: rpa2
title: 'SFB-net for cardiac segmentation: Bridging the semantic gap with attention'
arxiv_id: '2410.18503'
source_url: https://arxiv.org/abs/2410.18503
tags:
- segmentation
- image
- encoder
- used
- decoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of cardiac image segmentation,
  focusing on the semantic gap between encoder and decoder feature maps in U-Net architectures.
  The proposed Swin Filtering Block network (SFB-net) integrates conventional and
  Swin transformer layers to introduce spatial attention and focus on semantically
  rich features.
---

# SFB-net for cardiac segmentation: Bridging the semantic gap with attention

## Quick Facts
- arXiv ID: 2410.18503
- Source URL: https://arxiv.org/abs/2410.18503
- Authors: Nicolas Portal; Nadjia Kachenoura; Thomas Dietenbeck; Catherine Achard
- Reference count: 0
- One-line primary result: SFB-net achieves 92.4% Dice score on ACDC and 87.99% on M&M's datasets, outperforming existing methods.

## Executive Summary
This paper introduces SFB-net, a novel U-Net architecture designed to address the semantic gap between encoder and decoder feature maps in cardiac MRI segmentation. By integrating Swin Filtering Blocks with cross-attention and a bottleneck transformer layer, the method enhances feature fusion and global context modeling. The approach achieves state-of-the-art Dice scores on both ACDC and M&M's datasets while maintaining computational efficiency.

## Method Summary
SFB-net is a U-Net-like architecture that incorporates Swin Filtering Blocks (SFBs) for cross-attention gating and a conventional transformer layer at the bottleneck. The SFBs use window-based cross-attention to filter noise from encoder feature maps before concatenation with decoder outputs. Deep supervision is applied at each decoder stage to improve gradient flow and training stability. The network is trained using AdamW optimizer with a combination of cross-entropy and Dice loss, and extensive data augmentation is applied to enhance generalization.

## Key Results
- SFB-net achieves an average Dice score of 92.4% on the ACDC dataset, outperforming existing methods.
- The network demonstrates strong generalization with an 87.99% Dice score on the M&M's dataset.
- Ablation studies confirm the effectiveness of both the Swin Filtering Blocks and the transformer layer at the bottleneck.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Swin Filtering Blocks (SFBs) reduce semantic gap by selectively amplifying encoder features that are semantically consistent with decoder outputs.
- Mechanism: SFBs perform cross-attention between encoder and decoder feature maps using shifted-window multi-head self-attention (SW-MHSA). The attention weights, after sigmoid activation, act as a mask that scales encoder features before concatenation, filtering out noise and emphasizing relevant details.
- Core assumption: Encoder feature maps contain high-resolution local details but also significant noise; decoder features carry stronger semantic context; cross-attention can effectively align these maps.
- Evidence anchors:
  - [abstract] "SFB mechanism uses window-based cross-attention to filter noise from encoder feature maps before concatenation with decoder outputs."
  - [section 2.2] "The goal is to enable the decoder, to filter out irrelevant information originating from the encoder... values are chosen to come from the encoder while both query and key should come from the decoder."
  - [corpus] Weak: Corpus neighbors focus on transformer-based segmentation but do not explicitly describe Swin Filtering Blocks or cross-attention gating mechanisms.
- Break condition: If cross-attention fails to align encoder/decoder semantics, the gating mask may suppress useful detail or amplify noise, reducing segmentation accuracy.

### Mechanism 2
- Claim: Adding a conventional transformer layer at the bottleneck increases receptive field and captures global context without sacrificing computational efficiency.
- Mechanism: The transformer layer replaces one convolutional block at the bottleneck, enabling window-based self-attention across the full spatial extent of the deepest feature map. This compensates for the limited depth of the network while preserving local feature extraction in earlier layers.
- Core assumption: Global context is critical for cardiac structure delineation; a single transformer layer at the bottleneck provides this benefit without the cost of full-depth transformer integration.
- Evidence anchors:
  - [abstract] "The former are used to introduce spatial attention at the bottom of the network, while the latter are applied to focus on high level semantically rich features between the encoder and decoder."
  - [section 2.1] "To compensate for the resulting shallowness of the network... a conventional transformer layer is introduced at the bottleneck... This enables the network to take advantage of global contextual information."
  - [corpus] Weak: Corpus neighbors mention transformer architectures but do not detail bottleneck placement or receptive field trade-offs.
- Break condition: If the transformer layer oversmooths features or misaligns with encoder/decoder semantics, segmentation quality may degrade, especially for small anatomical structures.

### Mechanism 3
- Claim: Deep supervision at multiple decoder resolutions improves gradient flow and stabilizes training, leading to better final segmentation.
- Mechanism: Loss is computed at each decoder stage with scaled weights; this provides intermediate supervision signals that guide feature refinement throughout the decoder path, not just at the output.
- Core assumption: Intermediate supervision helps the network learn more robust features and mitigates vanishing gradients in deep architectures.
- Evidence anchors:
  - [section 2.1] "Deep supervision is applied at each stage of the decoder... The final loss is the sum of successive stages loss..."
  - [section 3.2] Implementation uses "a combination of cross-entropy and Dice loss" with per-stage weighting.
  - [corpus] Weak: Corpus neighbors discuss segmentation accuracy but do not mention deep supervision as a training strategy.
- Break condition: If supervision weights are poorly tuned or the loss formulation is unstable, training may diverge or converge to suboptimal local minima.

## Foundational Learning

- Concept: Cross-attention mechanisms
  - Why needed here: To align encoder and decoder features by highlighting semantically consistent regions and suppressing noise before feature fusion.
  - Quick check question: In Swin Filtering Blocks, which feature map provides the query and key, and which provides the value?
- Concept: Window-based self-attention (SW-MHSA)
  - Why needed here: To efficiently capture long-range dependencies in feature maps while limiting computational cost compared to full self-attention.
  - Quick check question: How does the shifted window strategy in SW-MHSA differ from standard window-based attention?
- Concept: Deep supervision in encoder-decoder networks
  - Why needed here: To provide gradient signals at multiple decoder resolutions, improving convergence and segmentation accuracy.
  - Quick check question: How are the loss weights αi adjusted across decoder stages in this architecture?

## Architecture Onboarding

- Component map: Encoder (conv blocks + down-sampling) → Swin Filtering Blocks (cross-attention gating) → Decoder (up-sampling + conv blocks) → Output; bottleneck contains a conventional transformer layer; deep supervision branches from each decoder stage.
- Critical path: Input → Encoder → Bottleneck transformer → SFB-gated skip connections → Decoder → Output segmentation.
- Design tradeoffs: Using SFBs and a bottleneck transformer improves semantic alignment and context modeling but increases parameter count and inference time versus a pure U-Net; deep supervision adds training stability at the cost of extra loss computation.
- Failure signatures: Poor Dice scores on small structures (e.g., RV) suggest SFBs are not effectively filtering encoder noise; high inference latency indicates inefficient window or attention configuration.
- First 3 experiments:
  1. Train baseline U-Net without SFBs or bottleneck transformer; measure Dice scores on ACDC to establish reference.
  2. Add bottleneck transformer only; compare Dice and throughput to baseline.
  3. Add SFBs only; compare Dice and throughput to both baseline and bottleneck-only variants.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different attention mechanisms (e.g., window vs. full spatial attention) affect the trade-off between segmentation accuracy and computational efficiency in cardiac MRI segmentation?
- Basis in paper: [explicit] The paper compares Swin Filtering Blocks (SFB) using window-based cross-attention with other methods using full spatial attention or self-attention, noting computational complexity differences.
- Why unresolved: While the paper demonstrates that window-based cross-attention reduces computational load, it does not provide a detailed analysis of how this affects segmentation accuracy compared to other attention mechanisms.
- What evidence would resolve it: Comparative experiments directly measuring segmentation accuracy and computational efficiency (e.g., inference time, memory usage) for different attention mechanisms on the same dataset.

### Open Question 2
- Question: Can the proposed SFB-net architecture be effectively extended to other medical imaging modalities or anatomical structures beyond cardiac MRI?
- Basis in paper: [inferred] The paper focuses on cardiac MRI segmentation but does not explore the generalizability of the SFB-net architecture to other medical imaging tasks or anatomical structures.
- Why unresolved: The effectiveness of the SFB-net architecture is demonstrated only on cardiac MRI data, leaving its potential applicability to other medical imaging domains unexplored.
- What evidence would resolve it: Experiments applying the SFB-net architecture to other medical imaging modalities (e.g., CT, ultrasound) or anatomical structures (e.g., liver, lungs) and comparing its performance to existing methods.

### Open Question 3
- Question: How does the performance of SFB-net change when trained on datasets with varying levels of class imbalance or annotation quality?
- Basis in paper: [inferred] The paper does not address the impact of class imbalance or annotation quality on the performance of SFB-net, which are common challenges in medical image segmentation.
- Why unresolved: Real-world medical imaging datasets often suffer from class imbalance or inconsistent annotation quality, but the paper does not investigate how these factors affect the proposed method's performance.
- What evidence would resolve it: Experiments training SFB-net on datasets with controlled levels of class imbalance or artificially degraded annotation quality to assess its robustness and identify potential improvements.

## Limitations
- Limited external validation: The paper relies heavily on internal evidence for its claims, with weak support from the corpus, which does not provide detailed comparisons or independent validation of the proposed mechanisms.
- Uncertain generalizability: The effectiveness of SFB-net is demonstrated only on cardiac MRI data, leaving its potential applicability to other medical imaging domains or anatomical structures unexplored.
- Implementation complexity: The Swin Filtering Blocks and bottleneck transformer introduce additional complexity compared to standard U-Net architectures, which may hinder reproducibility and widespread adoption.

## Confidence
- Mechanism 1 (SFBs with cross-attention): Medium
- Mechanism 2 (Bottleneck transformer): Medium
- Mechanism 3 (Deep supervision): Medium

## Next Checks
1. Implement and compare a baseline U-Net without SFBs or transformer; measure Dice scores on ACDC to establish reference.
2. Add only the bottleneck transformer and evaluate performance; compare Dice and throughput to baseline.
3. Add only SFBs and compare results; compare Dice and throughput to both baseline and bottleneck-only variants.