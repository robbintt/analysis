---
ver: rpa2
title: Data-Driven Estimation of Conditional Expectations, Application to Optimal
  Stopping and Reinforcement Learning
arxiv_id: '2407.13189'
source_url: https://arxiv.org/abs/2407.13189
tags:
- sqrt
- conditional
- zeros
- cost
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a data-driven method for estimating conditional
  expectations without requiring knowledge of underlying probability densities. The
  core idea involves solving optimization problems involving regular expectations
  to estimate conditional expectations.
---

# Data-Driven Estimation of Conditional Expectations, Application to Optimal Stopping and Reinforcement Learning

## Quick Facts
- arXiv ID: 2407.13189
- Source URL: https://arxiv.org/abs/2407.13189
- Authors: George V. Moustakides
- Reference count: 17
- The paper proposes a data-driven method for estimating conditional expectations without requiring knowledge of underlying probability densities.

## Executive Summary
This paper introduces a novel data-driven approach for estimating conditional expectations using only observational data, without requiring knowledge of the underlying probability densities. The method transforms the problem of estimating conditional expectations into an optimization problem involving only regular expectations, which can be solved using neural networks trained on data. The approach is then extended to solve systems of equations involving conditional expectations, enabling applications to Optimal Stopping and Reinforcement Learning problems where the underlying distributions are unknown.

## Method Summary
The method involves parameterizing unknown functions as neural networks and minimizing a cost function involving only regular expectations. For conditional expectation estimation, the approach solves an optimization problem where the conditional expectation appears in the optimal solution of a scalar function. For systems of equations, the method iteratively updates neural network parameters using gradient descent on a data-driven cost function. The approach handles both Markov optimal stopping and reinforcement learning problems by appropriately defining the functions and using corresponding conditional densities.

## Key Results
- Successfully estimates conditional expectations without knowing underlying probability densities
- Solves Optimal Stopping problems through single equation involving conditional expectations
- Solves Reinforcement Learning problems through systems of equations with discrete action spaces
- Demonstrates effectiveness through numerical examples with comparisons to exact solutions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conditional expectations can be estimated without knowing the underlying probability densities by transforming the problem into a standard expectation minimization problem.
- Mechanism: The method uses a specific optimization framework where the conditional expectation appears in the optimal solution of a scalar function ω(z) = b(X)/a(X), where a(X) and b(X) are expectations of functions c(Y) and d(Y) respectively. By parameterizing the unknown function u(X) with a neural network and minimizing a cost function involving only regular expectations, the conditional expectation is recovered through the optimal network output.
- Core assumption: The range of the ratio b(X)/a(X) is contained within the range of a strictly increasing function ω(z).
- Evidence anchors:
  - [abstract] "proposes a data-driven method for estimating conditional expectations without requiring knowledge of underlying probability densities"
  - [section II.A.3] "the advantage of the method we are going to introduce is that we can estimate conditional expectations by solving optimization problems involving regular expectations"
- Break condition: If the range of b(X)/a(X) is not contained within the range of ω(z), the optimization will not converge to the correct conditional expectation.

### Mechanism 2
- Claim: The data-driven estimation method can solve systems of equations involving conditional expectations by iteratively updating neural network parameters.
- Mechanism: For systems of equations of the form Uj(X) = Ej[hj(Y, U1(Y), ..., UK(Y))|X = X], the method replaces each function Uj(X) with a neural network u(X, θj) and updates the parameters θj using gradient descent on a data-driven cost function. The key insight is that the conditional expectation of hj can be estimated using the previously described method, allowing the entire system to be solved without knowing the underlying densities.
- Core assumption: The system of equations has a unique solution and the neural networks have sufficient capacity to approximate the solution functions.
- Evidence anchors:
  - [section IV.B] "Key observation for solving the system in (18) is that all functions of interest are defined in terms of conditional expectations"
  - [section IV.D] "We are looking for the best action policy that will result in maximal average reward... If at time t = 0 we observe state S0 = S and we decide in favor of action j... then let us call the resulting reward Uj(S). It can then be proved... that these functions satisfy the following system of equations"
- Break condition: If the system of equations does not have a unique solution or the neural networks cannot approximate the solution functions adequately, the method will fail to converge to the correct solution.

### Mechanism 3
- Claim: The method can handle both Markov optimal stopping and reinforcement learning problems by appropriately defining the functions hj and using the corresponding conditional densities.
- Mechanism: For Markov optimal stopping, the method estimates the optimal stopping time by solving a single equation involving the conditional expectation of the minimum between the stopping cost and the continuation cost plus the expected future value. For reinforcement learning, the method solves a system of equations where each equation corresponds to the expected reward for taking a specific action plus the discounted maximum expected future reward.
- Core assumption: The transition densities or conditional densities are known or can be approximated from data, and the discount factor is appropriately chosen.
- Evidence anchors:
  - [section IV.C] "Consider a homogeneous Markov process {Xt}... We are interested in the minimization of the following exponentially discounted average cost over a stopping time T"
  - [section IV.D] "A second perhaps more popular nowadays problem is the optimal action policy in Reinforcement Learning... Suppose {St} is a Markov controlled process with the action taking discrete values in the set {1, 2, ..., K}"
- Break condition: If the transition densities are not known and cannot be approximated from data, or if the discount factor is not appropriately chosen, the method will not produce accurate results.

## Foundational Learning

- Concept: Law of Large Numbers and its application to expectation estimation
  - Why needed here: The method relies on replacing expectations with sample averages to create a data-driven cost function that can be minimized using gradient descent.
  - Quick check question: How does the Law of Large Numbers justify replacing the expectation EX[h(X, θ)] with the sample average 1/n Σ h(Xi, θ)?

- Concept: Universal approximation theorem for neural networks
  - Why needed here: The method assumes that the neural networks can approximate arbitrarily well any sufficiently smooth function, which is necessary for the method to recover the true conditional expectations.
  - Quick check question: What conditions must a function class satisfy to be a universal approximator, and do neural networks with ReLU activations meet these conditions?

- Concept: Stochastic approximation and convergence of stochastic gradient descent
  - Why needed here: The method uses stochastic gradient descent to minimize the data-driven cost function, and the convergence of this algorithm to the optimal solution is crucial for the method to work.
  - Quick check question: What are the key conditions for the convergence of stochastic gradient descent, and how do they apply to the updates in equations (15) and (16)?

## Architecture Onboarding

- Component map:
  - Input data: Training data {(Yi, Xi)} sampled from the joint density f(Y, X)
  - Neural networks: K neural networks u(X, θj) for j = 1, ..., K, each with its own parameter vector θj
  - Optimization algorithm: Gradient descent or stochastic gradient descent to minimize the data-driven cost function
  - Functions ω(z) and ρ(z): Strictly increasing and strictly negative functions that define the optimization problem and determine the output activation
  - Datasets: K datasets {(Y j i, X j i)} for j = 1, ..., K, each corresponding to a different conditional density f j(Y |X)

- Critical path:
  1. Initialize the neural network parameters θj randomly
  2. For each iteration t:
     a. Sample a mini-batch of data from each dataset
     b. Compute the network outputs u(X j i, θj t-1) for each j and each data point in the mini-batch
     c. Apply the function ω(z) to the network outputs to get the current estimates of Uj(Y j i)
     d. Compute the gradients of the data-driven cost function with respect to each θj using backpropagation
     e. Update each θj using the gradients and a learning rate
  3. After convergence, use the final network outputs u(X, θj o) to estimate the solution functions Uj(X)

- Design tradeoffs:
  - Network architecture: Deeper networks may have more capacity to approximate complex functions but are harder to train and more prone to overfitting. Shallower networks are easier to train but may not have enough capacity for complex problems.
  - Choice of ω(z) and ρ(z): Different choices of these functions can lead to different optimization problems and different properties of the solution. The choice should be guided by the range of the conditional expectations to be estimated.
  - Learning rate: A larger learning rate can lead to faster convergence but may cause the algorithm to overshoot the optimal solution. A smaller learning rate is more stable but may require more iterations to converge.

- Failure signatures:
  - High variance in the cost function across iterations: This may indicate that the learning rate is too large or that the mini-batch size is too small.
  - Cost function not decreasing: This may indicate that the learning rate is too small, that the gradients are being computed incorrectly, or that the neural networks do not have enough capacity to approximate the solution functions.
  - Oscillations in the cost function: This may indicate that the learning rate is too large or that the optimization problem is ill-conditioned.

- First 3 experiments:
  1. Implement the method for a simple 1D example where the conditional expectation can be computed analytically, such as Y = sign(X) X2 + W with X ~ N(0,1) and W ~ N(0,0.1). Compare the estimated conditional expectation with the true value and visualize the results.
  2. Implement the method for a 2D example where the conditional expectation cannot be computed analytically but can be approximated numerically, such as Y = 1[-1,1](X + W) with X and W both ~ N(0,1). Compare the estimated conditional expectation with the numerical approximation and visualize the results.
  3. Implement the method for a reinforcement learning example with a discrete action space, such as the two AR(1) processes example with K=2 actions. Compare the estimated optimal action values with the numerical solution and visualize the results.

## Open Questions the Paper Calls Out

- How sensitive is the proposed data-driven estimation method to the choice of neural network architecture and size relative to the training dataset size?
  - Basis in paper: [explicit] The paper mentions that selecting the appropriate network size is a fundamental problem and there is no analytically trustworthy answer for how it should relate to the training dataset size.
  - Why unresolved: The paper only uses one example with a specific network configuration (shallow network with 50 nodes) and does not systematically explore how different architectures or sizes affect estimation quality.
  - What evidence would resolve it: Systematic experiments comparing estimation accuracy across different network architectures (shallow vs deep), sizes, and training set sizes would help establish guidelines for appropriate network selection.

## Limitations

- The method relies on the assumption that the range of b(X)/a(X) is contained within the range of a strictly increasing function ω(z), which may not hold in all practical scenarios.
- The theoretical convergence guarantees for the stochastic gradient descent updates in the system case are not provided, limiting the understanding of when and how the method converges.
- The computational efficiency for high-dimensional problems is not thoroughly investigated, and the method may face scalability challenges in such cases.

## Confidence

- Core mathematical formulation (Section II): High confidence
- Empirical validation across numerical examples: Medium confidence
- Sensitivity to hyperparameters like network architecture and learning rate: Low confidence

## Next Checks

1. Verify the strict increasingness assumption for ω(z) holds in practical scenarios by testing the method on datasets where the range of b(X)/a(X) is known or can be estimated.

2. Conduct systematic hyperparameter sensitivity analysis for the neural network training by varying network architectures (depth, width), learning rates, and mini-batch sizes across multiple examples.

3. Benchmark against alternative conditional expectation estimation methods on standard datasets to evaluate the relative performance and identify scenarios where the proposed method excels or struggles.