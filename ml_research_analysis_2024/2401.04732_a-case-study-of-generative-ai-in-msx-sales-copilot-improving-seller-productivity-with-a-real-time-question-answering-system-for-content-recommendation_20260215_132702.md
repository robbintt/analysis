---
ver: rpa2
title: 'A case study of Generative AI in MSX Sales Copilot: Improving seller productivity
  with a real-time question-answering system for content recommendation'
arxiv_id: '2401.04732'
source_url: https://arxiv.org/abs/2401.04732
tags:
- documents
- query
- relevant
- copilot
- content
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper describes a real-time question-answering system integrated
  into Microsoft''s MSX Sales Copilot that helps sellers find relevant documentation
  from the Seismic content repository. The system uses a two-stage LLM-based architecture:
  a bi-encoder for fast candidate retrieval and a cross-encoder for re-ranking, operating
  on document metadata rather than content.'
---

# A case study of Generative AI in MSX Sales Copilot: Improving seller productivity with a real-time question-answering system for content recommendation

## Quick Facts
- arXiv ID: 2401.04732
- Source URL: https://arxiv.org/abs/2401.04732
- Authors: Manpreet Singh; Ravdeep Pasricha; Nitish Singh; Ravi Prasad Kondapalli; Manoj R; Kiran R; Laurent Boué
- Reference count: 14
- Primary result: Real-time QA system using metadata-based prompt engineering and two-stage LLM architecture achieves median latency of 0.54-2.34s with 15/31 queries rated relevant by human annotators

## Executive Summary
This paper presents a real-time question-answering system integrated into Microsoft's MSX Sales Copilot that helps sellers find relevant documentation from the Seismic content repository. The system uses a two-stage LLM-based architecture: a bi-encoder for fast candidate retrieval and a cross-encoder for re-ranking, operating on document metadata rather than content. Human annotators rated query results as relevant in 15 out of 31 cases, with median latency ranging from 0.54-2.34 seconds depending on configuration. The system replaces previous filter-based search methods and has received positive feedback for improving seller productivity.

## Method Summary
The system uses a two-stage LLM architecture with metadata prompt engineering to enable real-time search without accessing document content. First, document metadata features are converted into textual prompts through concatenation and percentile-based categorization of numerical features. A DistilBERT bi-encoder retrieves top-100 candidates using cached embeddings and cosine similarity. Then a MiniLM cross-encoder re-ranks these candidates in batches (b=2 or b=4) for optimal latency-relevance balance. The system is deployed as an Azure ML endpoint integrated into MSX Copilot via Semantic Kernel planner, with the entire pipeline completing in 0.54-2.34 seconds per query.

## Key Results
- Human annotators rated query results as relevant in 15 out of 31 evaluation queries
- Median system latency ranges from 0.54 seconds (bi-encoder only) to 2.34 seconds (full two-stage with b=4)
- System receives positive user feedback for improving productivity compared to previous filter-based search
- Optimal batch size of b=2 or b=4 provides balance between GPU utilization and response time

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Metadata prompt engineering converts document metadata into textual descriptions that can be processed by LLM embeddings, enabling semantic search without document content access.
- Mechanism: The system concatenates document features (categorical and numerical) into a single prompt string, then encodes these prompts with DistilBERT embeddings. Numerical features are mapped to categories using percentiles (e.g., "high" if above 85th percentile) to create discrete text representations.
- Core assumption: Metadata features contain sufficient semantic information to distinguish document relevance when converted to text form.
- Evidence anchors:
  - [abstract] "We achieve this by engineering prompts in an elaborate fashion that makes use of the rich set of meta-features available for documents and sellers."
  - [section 3] "Given a document ck, we concatenate each one its features fi with and their string-valued representations g ◦ fi(ck) so that ck is now translated into a 'prompt'"
  - [corpus] Weak - corpus neighbors discuss recommendation systems but not metadata-to-text encoding specifically
- Break condition: If metadata features lack discriminative power or are sparse, the text prompts won't capture document semantics effectively.

### Mechanism 2
- Claim: Two-stage architecture (bi-encoder + cross-encoder) balances latency and relevance accuracy for real-time search.
- Mechanism: Bi-encoder retrieves top-K candidates quickly using cached document embeddings and cosine similarity, then cross-encoder re-ranks these candidates using query-document pair attention for better relevance.
- Core assumption: Fast initial retrieval with approximate similarity is sufficient to narrow candidates for accurate re-ranking.
- Evidence anchors:
  - [abstract] "Using a bi-encoder with cross-encoder re-ranker architecture, we show how the solution returns the most relevant content recommendations in just a few seconds"
  - [section 4] "we employ a 2-stage LLM-based architecture to solve the search problem" and "The cross-encoder model we have tuned for performance is the batch size b"
  - [corpus] Weak - corpus neighbors discuss recommendation systems but not two-stage architectures specifically
- Break condition: If initial retrieval misses relevant documents (low recall), cross-encoder cannot recover them.

### Mechanism 3
- Claim: Batch size optimization in cross-encoder inference directly impacts real-time performance without sacrificing relevance.
- Mechanism: Larger batch sizes improve GPU utilization but increase latency; experiments show b=2 or b=4 provides optimal balance on tested hardware.
- Core assumption: GPU-based inference benefits from batching while maintaining acceptable response times.
- Evidence anchors:
  - [section 5.1] "we experimented with different number of top-K candidates shortlisted during the retrieval phase" and "we found that K = 100 was the best option for re-ranking in a reasonable time"
  - [section 5.1] "Table 1 shows average latency with standard deviation over the evaluation queries with different values of the batch size b"
  - [corpus] Weak - corpus neighbors discuss recommendation systems but not batch size optimization specifically
- Break condition: If hardware changes or query patterns differ significantly, optimal batch size may shift.

## Foundational Learning

- Concept: Semantic search vs keyword search
  - Why needed here: System replaces traditional filter-based search with context-aware matching using LLM embeddings
  - Quick check question: What's the key difference between matching based on keyword presence versus semantic meaning?

- Concept: Asymmetric search problem
  - Why needed here: Short queries must match against longer metadata descriptions, requiring specialized architecture
  - Quick check question: Why can't we simply use the same embedding model for both queries and documents?

- Concept: Feature engineering for numerical data
  - Why needed here: Numerical metadata (views, dates) must be converted to categorical text for prompt engineering
  - Quick check question: How does percentile-based categorization of numerical features affect the resulting embeddings?

## Architecture Onboarding

- Component map: Query → bi-encoder → top-100 candidates → cross-encoder (batch size b) → top-5 → UI response
- Critical path: Query → bi-encoder → top-100 candidates → cross-encoder (batch size b) → top-5 → UI response
- Design tradeoffs:
  - Latency vs accuracy: Larger K in retrieval improves recall but increases cross-encoder latency
  - Batch size vs responsiveness: Larger batches improve GPU utilization but delay individual responses
  - Feature complexity vs token limits: More features improve representation but may exceed 512-token context window
- Failure signatures:
  - High latency (>3 seconds): Check batch size, VM type, or retrieval K value
  - Poor relevance scores: Verify metadata quality, feature engineering, or K value adequacy
  - System crashes: Monitor GPU memory usage with different batch sizes
- First 3 experiments:
  1. Test latency impact of different batch sizes (b=1,2,4,8) on Standard DS4 v2 VM with fixed K=100
  2. Compare relevance scores using bi-encoder only vs full two-stage system on sample queries
  3. Measure impact of including vs excluding numerical features in prompt engineering using human evaluation

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between metadata features and content features for maximizing recommendation relevance while maintaining real-time performance?
- Basis in paper: [inferred] The authors note that their current system uses only metadata features due to programmatic access limitations to actual document content, and mention trade-offs between feature quantity and latency/performance.
- Why unresolved: The paper explicitly states this is reserved for future work once programmatic access to document content is available, and the authors acknowledge the need to balance feature richness against real-time requirements.
- What evidence would resolve it: Comparative experiments measuring relevance metrics (like NDCG) and latency across models using different combinations of metadata-only vs. content+metadata features on the same dataset.

### Open Question 2
- Question: How does the relevance of recommendations vary across different types of seller queries (e.g., specific product requests vs. general topic inquiries)?
- Basis in paper: [explicit] The human annotator analysis showed that overly verbose or generic queries performed poorly, and the authors noted this as an area for improvement.
- Why unresolved: While the paper mentions this issue qualitatively, it doesn't provide systematic analysis of query type distributions or detailed performance breakdown by query characteristics.
- What evidence would resolve it: Comprehensive categorization of evaluation queries by type and a detailed analysis showing recommendation relevance scores broken down by query category.

### Open Question 3
- Question: What is the long-term impact of the recommender system on seller productivity and customer engagement compared to the previous filter-based search system?
- Basis in paper: [explicit] The authors report initial positive feedback and survey scores but note they are continuously gathering feedback for further improvements.
- Why unresolved: The paper presents only initial satisfaction survey results without longitudinal data on actual productivity metrics or customer engagement outcomes.
- What evidence would resolve it: Longitudinal studies tracking seller metrics (time-to-meeting-preparation, number of relevant documents shared, etc.) and customer engagement data (meeting success rates, conversion rates) before and after system implementation.

## Limitations

- Evaluation scope is limited with only 31 queries used for human relevance assessment, providing weak statistical power for generalization
- System performance heavily depends on metadata quality, but paper doesn't discuss handling of sparse or missing metadata features
- Claims about productivity improvements are based on qualitative feedback rather than systematic measurement of actual productivity gains

## Confidence

- **High Confidence**: The two-stage LLM architecture with bi-encoder + cross-encoder is technically sound and well-established in the literature. The latency measurements and batch size optimization are reproducible given the described infrastructure.
- **Medium Confidence**: The prompt engineering approach for metadata conversion should work as described, though effectiveness depends heavily on metadata quality and feature selection which aren't fully specified.
- **Low Confidence**: Claims about productivity improvements for sellers are based on qualitative feedback rather than systematic measurement of actual productivity gains.

## Next Checks

1. **Statistical Significance Test**: Conduct a proper statistical analysis of the 31-query human evaluation results using significance testing (e.g., McNemar's test) to determine if the relevance improvements are statistically significant beyond small sample noise.

2. **Metadata Quality Impact**: Systematically test the system's performance on documents with varying levels of metadata completeness - create controlled experiments where you gradually remove metadata features to identify the minimum viable feature set for acceptable performance.

3. **Scalability Benchmarking**: Measure how latency and relevance scores degrade as the document corpus scales from 1K to 100K+ documents, testing whether the current K=100 retrieval threshold remains optimal at larger scales or if recall becomes a bottleneck.