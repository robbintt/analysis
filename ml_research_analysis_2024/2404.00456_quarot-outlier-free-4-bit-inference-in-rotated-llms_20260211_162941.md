---
ver: rpa2
title: 'QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs'
arxiv_id: '2404.00456'
source_url: https://arxiv.org/abs/2404.00456
tags:
- quantization
- quarot
- hadamard
- llama
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QuaRot introduces a new quantization method for large language
  models (LLMs) that enables end-to-end 4-bit inference by eliminating outlier features
  in activations and KV caches using Hadamard transformations. The method rotates
  weights, activations, and KV caches in a way that removes outliers without changing
  the model output, making quantization easier.
---

# QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs

## Quick Facts
- arXiv ID: 2404.00456
- Source URL: https://arxiv.org/abs/2404.00456
- Reference count: 40
- Enables 4-bit inference in LLMs while preserving 99% zero-shot accuracy

## Executive Summary
QuaRot introduces a novel approach to 4-bit quantization of large language models by using Hadamard transformations to eliminate outlier features that typically prevent effective low-bit quantization. The method rotates weight matrices, activations, and KV caches using randomized orthogonal transformations that preserve model outputs while spreading values evenly. This allows all matrix multiplications to be performed in 4 bits, achieving up to 3.33× speedup in prefill and 3.89× memory savings during decoding on LLaMA-2-70B models, while also supporting lossless 6/8-bit quantization.

## Method Summary
QuaRot applies randomized Hadamard transformations to weight matrices and fuses them into the model structure, allowing all matrix multiplications to be performed in 4 bits. The method eliminates outlier features in activations and KV caches by rotating the activation space, spreading large-magnitude values evenly. This rotation is performed using orthogonal matrices (Hadamard matrices) that preserve the model output due to computational invariance. The transformations are absorbed into the model weights through pre-multiplication and post-multiplication, eliminating runtime overhead. KV cache quantization uses asymmetric schemes with group sizes of 128, and the method supports both online and fused transformation approaches.

## Key Results
- Preserves 99% of zero-shot performance on LLaMA-2-70B models
- Achieves 3.33× speedup in prefill stage compared to FP16 baseline
- Provides 3.89× memory savings during decoding through 4-bit quantization
- Supports lossless 6 and 8-bit quantization using round-to-nearest quantization

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Randomized Hadamard transformations eliminate outlier features in activations and KV cache.
- Mechanism: Multiplying weight matrices by orthogonal matrices (Hadamard matrices) rotates the activation space, spreading large-magnitude values evenly and removing extreme outliers that hinder low-bit quantization.
- Core assumption: Orthogonal rotations preserve the model output due to computational invariance (Theorem 1 from Ashkboos et al., 2024).
- Evidence anchors:
  - [abstract] "QuaRot rotates LLMs in a way that removes outliers from the hidden state without changing the output"
  - [section 4] "We first make use of computational invariance to multiply each weight matrix by an orthogonal matrix"
- Break condition: If the orthogonal rotation is not perfectly invertible or if numerical precision loss occurs during rotation, outliers may reappear.

### Mechanism 2
- Claim: Fusing Hadamard transformations into weight matrices eliminates the need for online transformations.
- Mechanism: By pre-multiplying and post-multiplying weight matrices with Hadamard matrices, the transformations are absorbed into the model weights, so activations pass through rotated implicitly without runtime overhead.
- Core assumption: The computational invariance theorem ensures that rotating both sides of a matrix multiplication preserves the model output.
- Evidence anchors:
  - [section 4] "This weight modification does not affect the output of the model (assuming sufficient precision) as per the computational invariance theorem"
- Break condition: If the Hadamard matrix size does not match the model dimension, Kronecker construction introduces approximation error.

### Mechanism 3
- Claim: Quantizing KV cache in 4 bits with Hadamard rotations preserves accuracy.
- Mechanism: Rotating both keys and values using Hadamard matrices removes outliers from the attention mechanism, enabling low-bit quantization without significant accuracy loss.
- Core assumption: Attention scores computed from rotated keys and queries remain unchanged due to symmetry of the rotation.
- Evidence anchors:
  - [section 4] "Using the method above, we can successfully quantize the value vectors"
  - [section 5] "QuaRot preserves 99% of the accuracy of zero-shot tasks"
- Break condition: If positional embeddings interact poorly with Hadamard rotations, attention scores may change.

## Foundational Learning

- Concept: Orthogonal matrices and computational invariance
  - Why needed here: Understanding why rotating weight matrices doesn't change model output is fundamental to QuaRot's approach
  - Quick check question: What property of orthogonal matrices allows them to be fused into weight matrices without changing the model output?

- Concept: Hadamard transformations and their computational properties
  - Why needed here: Hadamard matrices enable fast O(d log d) transformations that spread values evenly
  - Quick check question: How does the Walsh-Hadamard transform compute Hx in O(d log2(d)) operations?

- Concept: Quantization and outlier sensitivity
  - Why needed here: Understanding why outliers prevent effective low-bit quantization is crucial for grasping QuaRot's motivation
  - Quick check question: Why do outlier values in activations make uniform quantization particularly difficult for 4-bit schemes?

## Architecture Onboarding

- Component map: Input → FFN blocks with integrated Hadamard → Attention blocks with KV cache quantization → Output
- Critical path: Input → FFN blocks with integrated Hadamard → Attention blocks with KV cache quantization → Output
- Design tradeoffs:
  - Precision vs. speed: FP32 Hadamard transforms add overhead but improve quantization quality
  - Group size in KV cache: Smaller groups improve accuracy but increase memory overhead
  - Online vs. fused transformations: Fused transformations eliminate runtime overhead but require careful weight modification
- Failure signatures:
  - Accuracy degradation: Indicates outliers reappeared or rotations were not perfectly invertible
  - Performance regression: Suggests Hadamard transform overhead is too high or quantization is not effective
  - Memory issues: Points to problems with KV cache quantization or group size selection
- First 3 experiments:
  1. Apply QuaRot transformations to a small LLM and verify computational invariance by comparing outputs with and without rotations
  2. Quantize activations using different clipping ratios (0.9, 0.95, 1.0) to find optimal trade-off between accuracy and quantization range
  3. Test KV cache quantization with different bit-width combinations (e.g., 4-bit keys/3-bit values vs. 3-bit keys/4-bit values) to evaluate sensitivity

## Open Questions the Paper Calls Out
None

## Limitations

- Universal applicability across different LLM architectures remains unverified
- Speedup claims may be affected by Hadamard transform overhead and precision requirements
- Lossless 6/8-bit quantization needs more comprehensive empirical validation

## Confidence

**Confidence: Low** on universal applicability across all LLM architectures
**Confidence: Medium** on claimed speedups due to hardware-specific measurements
**Confidence: Medium** on lossless 6/8-bit quantization without comprehensive validation

## Next Checks

1. Cross-architecture validation: Apply QuaRot to at least three different LLM architectures and evaluate both accuracy preservation and speed/memory benefits across different hardware platforms.

2. Numerical stability analysis: Systematically test the precision requirements for Hadamard rotations by gradually reducing precision from FP32 to FP16 and INT8, measuring when computational invariance breaks down and outliers reappear.

3. Long-context evaluation: Test QuaRot on tasks requiring extended KV cache usage (e.g., 32K+ context length) to validate that the asymmetric 4-bit quantization scheme maintains accuracy over long sequences without accumulation of quantization errors.