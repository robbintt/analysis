---
ver: rpa2
title: Evaluating the Adversarial Robustness of Detection Transformers
arxiv_id: '2412.18718'
source_url: https://arxiv.org/abs/2412.18718
tags:
- attacks
- detr
- adversarial
- object
- attack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the adversarial robustness of detection transformers
  (DETRs) under both white-box and black-box adversarial attacks. The authors extend
  classic attack methods (FGSM, PGD, and C&W) to assess DETR vulnerability, demonstrating
  that DETR models are significantly susceptible to adversarial attacks, similar to
  traditional CNN-based detectors.
---

# Evaluating the Adversarial Robustness of Detection Transformers

## Quick Facts
- **arXiv ID**: 2412.18718
- **Source URL**: https://arxiv.org/abs/2412.18718
- **Reference count**: 30
- **Primary result**: DETR models are highly vulnerable to adversarial attacks, with AP drops from 0.420 to 0.047-0.063 under various attacks

## Executive Summary
This paper comprehensively evaluates the adversarial robustness of Detection Transformers (DETRs) under both white-box and black-box attack scenarios. The authors extend classic attack methods including FGSM, PGD, and C&W to assess DETR vulnerability, demonstrating that these models are significantly susceptible to adversarial attacks similar to traditional CNN-based detectors. Through extensive experimentation on MS-COCO and KITTI datasets, they reveal critical vulnerabilities in DETR's architecture and propose a novel untargeted attack specifically designed to exploit DETR's intermediate loss functions.

The study's transferability analysis reveals high intra-network transferability among DETR variants but limited cross-network transferability to CNN-based models. Visualizations of self-attention feature maps demonstrate that adversarial attacks significantly alter DETR's internal representations. These findings emphasize the urgent need for enhanced robustness mechanisms in detection transformers, particularly for safety-critical applications where reliable object detection is essential.

## Method Summary
The authors evaluate DETR robustness through multiple attack scenarios including white-box and black-box adversarial attacks. They extend classic attack methods (FGSM, PGD, and C&W) specifically for DETR architecture and propose a novel untargeted attack that exploits DETR's intermediate loss functions. The evaluation is conducted on two major datasets: MS-COCO and KITTI, with comprehensive analysis of attack transferability across different DETR variants and comparison with CNN-based detectors. The methodology includes extensive visualization of self-attention feature maps to understand how adversarial perturbations affect DETR's internal representations.

## Key Results
- DETR models show significant vulnerability to adversarial attacks with AP drops from 0.420 to as low as 0.047-0.063
- High intra-DETR network transferability observed, but limited cross-network transferability to CNN-based models
- Novel untargeted attack exploits DETR's intermediate loss functions for effective misclassification with minimal perturbations
- Visualizations reveal adversarial attacks significantly alter DETR's self-attention feature maps and internal representations

## Why This Works (Mechanism)
The vulnerability of DETR models to adversarial attacks stems from their transformer-based architecture that relies heavily on self-attention mechanisms and intermediate loss functions. Unlike traditional CNN-based detectors, DETR processes input features through multiple transformer layers that create complex dependencies between spatial regions. Adversarial perturbations can exploit these dependencies by introducing small changes that propagate through the self-attention mechanism, causing significant downstream effects on the final detection outputs. The proposed untargeted attack specifically targets DETR's intermediate loss functions, which are used during training to guide the object detection process, making it particularly effective at inducing misclassification.

## Foundational Learning
**Adversarial Attacks**: Techniques that introduce small perturbations to input data to fool machine learning models - needed to understand how DETR can be compromised, quick check: can be verified by examining attack success rates on clean vs. perturbed images.

**Transferability**: The ability of adversarial examples generated for one model to successfully attack another model - crucial for understanding attack effectiveness across different architectures, quick check: measured by success rate of attacks when transferred between models.

**Self-Attention Mechanism**: Transformer component that captures relationships between different spatial regions in input data - fundamental to DETR's operation, quick check: visualized through attention maps to understand feature dependencies.

**Intermediate Loss Functions**: Auxiliary loss components used during DETR training to guide the detection process - key target for the proposed untargeted attack, quick check: can be verified by examining their influence on final predictions.

**White-box vs Black-box Attacks**: Different attack scenarios based on attacker's knowledge of the target model - important for understanding real-world attack scenarios, quick check: distinguished by access to model parameters and gradients.

## Architecture Onboarding

**Component Map**: Input Image -> Backbone Feature Extractor -> Transformer Encoder -> Transformer Decoder -> Detection Head -> Output Predictions

**Critical Path**: The most critical components are the transformer encoder-decoder layers and the detection head, as these directly process the features and generate final predictions. The backbone feature extractor provides essential spatial information that is then processed through the transformer architecture.

**Design Tradeoffs**: DETR's transformer-based design offers global context awareness through self-attention but introduces vulnerability to adversarial perturbations that can exploit these long-range dependencies. The trade-off between detection accuracy and robustness becomes apparent when comparing DETR to CNN-based detectors.

**Failure Signatures**: Adversarial attacks cause significant AP drops (0.420 to 0.047-0.063) and alter self-attention feature maps, indicating that small perturbations can disrupt the model's ability to correctly identify and localize objects. The proposed untargeted attack specifically exploits intermediate loss functions, causing misclassification with minimal perturbations.

**First Experiments**:
1. Apply standard FGSM attack to a pre-trained DETR model and measure AP drop on MS-COCO validation set
2. Generate adversarial examples using the proposed untargeted attack and evaluate their transferability to other DETR variants
3. Visualize self-attention feature maps before and after adversarial perturbations to observe internal representation changes

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation focuses primarily on MS-COCO and KITTI datasets, potentially limiting generalizability to other domains
- Proposed untargeted attack may not generalize to all DETR variants or real-world deployment scenarios
- Transferability findings show limited effectiveness across different architectural paradigms (DETR vs CNN-based models)

## Confidence
- **High confidence**: DETR vulnerability to adversarial attacks is well-established through multiple attack methods and datasets
- **Medium confidence**: The proposed untargeted attack's effectiveness is demonstrated but may require further validation on diverse DETR architectures
- **Medium confidence**: Transferability findings are robust within DETR variants but limited generalizability to CNN-based models

## Next Checks
1. Evaluate the proposed untargeted attack on additional DETR variants and emerging detection transformer architectures to assess broader applicability
2. Test attack transferability across diverse real-world scenarios beyond MS-COCO and KITTI, including domain-specific datasets
3. Implement and evaluate defensive mechanisms against the proposed attacks to establish baseline robustness metrics for safety-critical applications