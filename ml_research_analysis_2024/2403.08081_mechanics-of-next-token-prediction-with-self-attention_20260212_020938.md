---
ver: rpa2
title: Mechanics of Next Token Prediction with Self-Attention
arxiv_id: '2403.08081'
source_url: https://arxiv.org/abs/2403.08081
tags:
- wsvm
- then
- token
- have
- tokens
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper studies how self-attention learns to predict the next
  token in a sequence. The authors formulate next-token prediction as a multi-class
  classification problem and show that under suitable conditions, training a single-layer
  self-attention model with gradient descent learns to implement a two-step process:
  (1) hard retrieval - selecting high-priority input tokens associated with the last
  input token, and (2) soft composition - creating a convex combination of these high-priority
  tokens from which the next token can be sampled.'
---

# Mechanics of Next Token Prediction with Self-Attention

## Quick Facts
- arXiv ID: 2403.08081
- Source URL: https://arxiv.org/abs/2403.08081
- Reference count: 40
- Primary result: Single-layer self-attention trained with gradient descent learns to implement hard retrieval and soft composition for next-token prediction through an implicit Graph-SVM mechanism

## Executive Summary
This paper provides a theoretical framework for understanding how self-attention learns to predict the next token in a sequence. The authors show that training a single-layer self-attention model with gradient descent results in a two-step process: (1) hard retrieval of high-priority input tokens associated with the last input token, and (2) soft composition of these tokens to sample the next token. The key insight is that gradient descent implicitly discovers the strongly-connected components (SCCs) of a token-priority graph extracted from the training data, and the attention mechanism learns to suppress lower priority tokens through an SVM-based mechanism.

## Method Summary
The paper formulates next-token prediction as a multi-class classification problem using a single-layer self-attention model with combined key-query weights. The model is trained using gradient descent with log-loss on a dataset of input sequences and their next tokens. The authors construct token-priority graphs (TPGs) from the training data and prove that the learned attention weights decompose into directional (hard retrieval) and finite (soft composition) components. The key theoretical result is that gradient descent converges to a solution that implements an implicit Graph-SVM problem based on the TPG structure.

## Key Results
- Single-layer self-attention learns a two-step process: hard retrieval of high-priority tokens followed by soft composition within strongly-connected components
- Gradient descent implicitly discovers the strongly-connected components (SCCs) of the token-priority graph during training
- The self-attention mechanism learns to suppress lower priority tokens in favor of sampling higher priority tokens through an SVM mechanism
- The model weights decompose into directional (C·W_hard) and finite (W_soft) components, with the directional component dominating for hard retrieval

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Single-layer self-attention trained with gradient descent learns a two-step process for next-token prediction: (1) hard retrieval - selecting high-priority input tokens associated with the last input token, and (2) soft composition - creating a convex combination of these high-priority tokens from which the next token can be sampled.
- Mechanism: The combined attention weights evolve as W = C · W_hard + W_soft where C → ∞ makes W_hard the dominant component for hard retrieval, while W_soft handles soft composition within strongly-connected components.
- Core assumption: The directed graph over tokens extracted from training data has strongly-connected components (SCCs) that capture priority relationships.
- Evidence anchors:
  - [abstract]: "We show that training self-attention with gradient descent learns an automaton which generates the next token in two distinct steps: (1) Hard retrieval: Given input sequence, self-attention precisely selects the high-priority input tokens associated with the last input token. (2) Soft composition: It then creates a convex combination of the high-priority tokens from which the next token can be sampled."
  - [section 2.1]: "We associate the dataset DSET = {(Xi, yi)}n i=1 with multiple TPGs {G(k)}K k=1, with each TPG focusing on a subset of the dataset comprising of those input sequences that agree on the last token ¯x."
- Break condition: If the token-priority graph has no strict priority order (all SCCs are non-singleton), W_hard may not exist or be meaningful.

### Mechanism 2
- Claim: Gradient descent implicitly discovers the strongly-connected components (SCCs) of the token-priority graph during training.
- Mechanism: The optimization landscape is convex under log-loss, and gradient descent converges to the solution of an SVM problem (Graph-SVM) that enforces topological order between SCCs.
- Core assumption: The log-loss function combined with the weight tying strategy (Assumption 1) makes the optimization problem convex.
- Evidence anchors:
  - [section 3]: "We show that the answers to both of these questions are intertwined which we achieve by significantly expanding the recently proposed framework that connects learning with Transformers to the celebrated support vector machines (SVMs)."
- Break condition: If the loss function is not log-loss (e.g., squared loss), the optimization landscape may not be convex, leading to local convergence instead of global.

### Mechanism 3
- Claim: The self-attention model learns to suppress lower priority tokens in favor of sampling higher priority tokens through an SVM mechanism.
- Mechanism: The Graph-SVM formulation enforces that for any two tokens i, j where i has higher priority than j, the attention weights must satisfy (ei - ej)⊤Wek ≥ 1, creating a margin that separates higher from lower priority tokens.
- Core assumption: The token-priority graph captures the true priority relationships in the data, and the SVM formulation is feasible.
- Evidence anchors:
  - [section 2.2]: "Given (G(k)k )K k=1, we introduce the following SVM formulation: Wsvm = arg minW ∥W ∥F (Graph-SVM) s.t. (ei - ej)⊤Wek = 0 ∀(i ≍ j) ∈ G(k) ≥ 1 ∀(i ⇒ j) ∈ G(k) for all k ∈ [K]."
- Break condition: If the Graph-SVM is not feasible (e.g., due to conflicting priority constraints), the mechanism breaks down.

## Foundational Learning

- Concept: Support Vector Machines (SVMs) and their implicit bias
  - Why needed here: The paper builds on the framework connecting Transformer learning to SVMs, using Graph-SVM formulations to characterize the implicit bias of self-attention training.
  - Quick check question: What is the key difference between the Graph-SVM formulation and standard SVMs in terms of constraints?

- Concept: Strongly-connected components (SCCs) in directed graphs
  - Why needed here: SCCs are used to partition the token-priority graph and determine which tokens should be treated as having equal priority versus strict priority ordering.
  - Quick check question: How does the presence of cycles in the token-priority graph affect the attention mechanism's behavior?

- Concept: Convex optimization and gradient descent convergence
  - Why needed here: The paper relies on the convexity of the log-loss objective to prove global convergence of gradient descent to the Graph-SVM solution.
  - Quick check question: Why does the log-loss function make the optimization landscape convex while other loss functions like squared loss do not?

## Architecture Onboarding

- Component map:
  - Token embeddings (E ∈ RK×d) -> Attention weights (W ∈ Rd×d) -> Softmax operation (S) -> Classification head (c1,...,cK)

- Critical path:
  1. Input sequence X with last token ¯x
  2. Compute attention scores: XW¯x
  3. Apply softmax: S(XW¯x)
  4. Weighted combination: X⊤S(XW¯x)
  5. Classification: c⊤yX⊤S(XW¯x)

- Design tradeoffs:
  - Single-layer vs. multi-layer: Single-layer allows for theoretical analysis but may limit representational capacity
  - Log-loss vs. other losses: Log-loss ensures convexity but may not be optimal for all applications
  - Graph construction: The choice of token-priority graph construction affects the learned priorities

- Failure signatures:
  - Non-convergence: If the loss function is not log-loss, gradient descent may converge to local minima instead of the global Graph-SVM solution
  - Incorrect priorities: If the token-priority graph construction doesn't capture true relationships, the attention mechanism may learn incorrect priorities
  - Numerical instability: When K >> d, the Graph-SVM may become infeasible or the optimization may become unstable

- First 3 experiments:
  1. Train on acyclic dataset with log-loss and verify global convergence to Graph-SVM solution
  2. Train on cyclic dataset and observe soft composition within SCCs
  3. Switch to squared loss and compare convergence behavior to identify local vs. global convergence patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the implicit bias of self-attention change when using more complex architectures like multi-layer or multi-head attention?
- Basis in paper: [explicit] The paper discusses extending analysis to multi-layer multi-head self-attention models as a natural future direction.
- Why unresolved: The paper focuses on single-layer self-attention and does not explore how additional layers or heads affect the implicit bias.
- What evidence would resolve it: Experiments comparing the implicit bias of single-layer vs. multi-layer or multi-head attention on various datasets and tasks.

### Open Question 2
- Question: What is the role of the feed-forward layers (MLP) in Transformers and how do they affect the optimization dynamics and token selection/composition mechanisms?
- Basis in paper: [explicit] The paper mentions exploring how feed-forward layers affect optimization dynamics and token selection/composition mechanisms as an interesting direction.
- Why unresolved: The paper does not investigate the impact of MLP layers on the learned representations or optimization process.
- What evidence would resolve it: Analyzing the learned weights and attention patterns with and without MLP layers, and comparing the performance and implicit bias.

### Open Question 3
- Question: How does the choice of loss function affect the implicit bias and convergence of self-attention?
- Basis in paper: [explicit] The paper analyzes the convergence and implicit bias under log-loss and mentions investigating other loss functions like squared loss as future work.
- Why unresolved: The paper primarily focuses on log-loss and does not extensively study the impact of different loss functions on the learned representations and optimization dynamics.
- What evidence would resolve it: Experiments comparing the convergence, implicit bias, and performance of self-attention under various loss functions (e.g., cross-entropy, squared loss) on different tasks and datasets.

## Limitations

- The theoretical framework assumes log-loss ensures convexity, but this may not hold for complex real-world datasets
- The Graph-SVM formulation relies on token-priority graph construction that may not accurately capture true priority relationships in all sequential data
- The analysis is limited to single-layer self-attention models, with unclear extension to multi-layer architectures commonly used in practice

## Confidence

**High Confidence** (Supporting evidence is direct and comprehensive):
- The two-step process (hard retrieval followed by soft composition) is formally derived from the theoretical framework
- The connection between self-attention training and Graph-SVM problems is rigorously established
- The existence of the directional component W_hard is proven under stated assumptions

**Medium Confidence** (Supporting evidence is indirect or has some gaps):
- The claim that gradient descent implicitly discovers SCCs lacks extensive empirical validation
- The assertion that the self-attention layer learns to suppress lower priority tokens would benefit from more direct experimental verification
- The mechanism by which W_hard becomes dominant through the C → ∞ limit has unclear practical implications

**Low Confidence** (Supporting evidence is weak or indirect):
- The claim that this mechanism generalizes to more complex architectures is speculative
- The assertion that learned priorities reflect meaningful semantic relationships lacks empirical validation
- The practical implications of the finite component W_soft for downstream tasks are not thoroughly investigated

## Next Checks

1. **Empirical Validation of Convergence Patterns**: Conduct controlled experiments training the single-layer self-attention model on synthetic datasets with known priority structures (e.g., strictly ordered tokens vs. cyclic dependencies). Track the evolution of attention weights and verify that they converge to the theoretical decomposition (W ≈ C·Wsvm + Wfin) as predicted. This would provide direct empirical support for the hard/soft decomposition mechanism.

2. **Robustness Testing Across Loss Functions**: Systematically compare the training dynamics and convergence behavior when using log-loss versus other loss functions (e.g., squared loss, cross-entropy with temperature scaling). This would test the critical assumption that log-loss ensures convexity and global convergence, and help identify conditions under which the theoretical guarantees break down.

3. **Extension to Multi-Layer Architectures**: Design experiments that gradually increase model complexity from single-layer to two-layer self-attention models. Analyze how the attention mechanisms in each layer interact and whether the hard retrieval/soft composition paradigm extends to deeper architectures. This would help validate the paper's claim that these insights pave the way for understanding more complex models.