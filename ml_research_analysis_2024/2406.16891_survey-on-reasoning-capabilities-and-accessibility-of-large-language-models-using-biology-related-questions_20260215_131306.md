---
ver: rpa2
title: Survey on Reasoning Capabilities and Accessibility of Large Language Models
  Using Biology-related Questions
arxiv_id: '2406.16891'
source_url: https://arxiv.org/abs/2406.16891
tags:
- gpt3
- additionally
- page
- yinetal
- montaneretal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This survey compared reasoning capabilities between GPT-3.5 and
  GPT-4 using biology-related open-ended questions and a scoring rubric. GPT-4 outperformed
  GPT-3.5 overall, scoring 162 vs 140 points, but the difference was smaller than
  expected.
---

# Survey on Reasoning Capabilities and Accessibility of Large Language Models Using Biology-related Questions

## Quick Facts
- arXiv ID: 2406.16891
- Source URL: https://arxiv.org/abs/2406.16891
- Reference count: 0
- GPT-4 scored 162 vs GPT-3.5's 140 points on biology-related questions

## Executive Summary
This survey compared reasoning capabilities between GPT-3.5 and GPT-4 using biology-related open-ended questions and a scoring rubric. GPT-4 outperformed GPT-3.5 overall, scoring 162 vs 140 points, but the difference was smaller than expected. Prompt engineering had a larger impact on both models than anticipated, with engineered prompts boosting GPT-3.5 from 33 to 107 points and GPT-4 from 42 to 120 points. The study concludes that GPT-4's reasoning improvements are modest but its accessibility to average users is enhanced through more consistent performance without specialized prompts.

## Method Summary
The study used a controlled comparison approach with biology-related open-ended questions and a standardized scoring rubric to evaluate reasoning capabilities. Both GPT-3.5 and GPT-4 were tested under two conditions: with and without engineered prompts. Performance was quantified through total scores, with accuracy percentages calculated for each prompt condition. The methodology focused on measuring the relative impact of prompt engineering versus model improvements in reasoning tasks.

## Key Results
- GPT-4 scored 162 points versus GPT-3.5's 140 points overall
- Engineered prompts improved GPT-3.5 from 33 to 107 points and GPT-4 from 42 to 120 points
- GPT-4 showed higher consistency (98% accuracy with engineered prompts vs 68% without) compared to GPT-3.5 (90% vs 58%)

## Why This Works (Mechanism)
Assumption: GPT-4's enhanced architecture likely incorporates more sophisticated reasoning patterns and better context understanding, though the paper does not explicitly detail the technical mechanisms behind performance differences. The consistency improvements suggest better internal reasoning chains rather than surface-level pattern matching.

## Foundational Learning
- Prompt engineering fundamentals: Why needed - significantly impacts LLM performance; Quick check - measure score differences with/without engineered prompts
- Scoring rubric design: Why needed - ensures consistent evaluation of reasoning capabilities; Quick check - validate rubric reliability across multiple evaluators
- Comparative model evaluation: Why needed - isolates improvements between model versions; Quick check - control for prompt conditions across both models

## Architecture Onboarding
The evaluation system consists of Question Bank -> Scoring Rubric -> Model Responses -> Score Aggregation. The critical path flows from question selection through response generation to final scoring. Design tradeoffs include question domain specificity versus generalizability, and scoring granularity versus practicality. Failure signatures include inconsistent scoring across evaluators and performance drops with specific question types. Three first experiments: 1) Test rubric consistency with multiple evaluators, 2) Evaluate performance on non-biology questions, 3) Compare results with additional contemporary LLM models.

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out specific open questions for future research. Potential areas include: 1) How performance scales with domain complexity beyond biology, 2) Whether observed improvements transfer to real-world applications, 3) The relationship between reasoning improvements and underlying model architecture changes.

## Limitations
- Narrow domain focus on biology questions may not generalize to other knowledge domains
- Only tested two models from a single provider, limiting broader applicability
- Scoring rubric subjectivity and potential evaluation bias remain uncertain

## Confidence
High confidence in prompt engineering impact and GPT-4 consistency findings
Medium confidence in comparative performance analysis between models

## Next Checks
1. Replicate the study using questions from multiple domains (physics, chemistry, mathematics) to assess domain generalizability
2. Test additional contemporary LLMs (Claude, Gemini, Llama) to compare performance patterns beyond OpenAI models
3. Conduct inter-rater reliability analysis with multiple evaluators scoring the same responses to quantify rubric subjectivity and scoring consistency