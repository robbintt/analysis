---
ver: rpa2
title: Exploring the Robustness of In-Context Learning with Noisy Labels
arxiv_id: '2404.18191'
source_url: https://arxiv.org/abs/2404.18191
tags:
- uni00000048
- uni00000013
- uni00000055
- uni00000003
- uni00000056
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the robustness of transformer models during
  in-context learning (ICL) when presented with noisy labels in demonstrations. The
  authors conduct extensive experiments on synthetic datasets, focusing on noisy linear
  regression tasks.
---

# Exploring the Robustness of In-Context Learning with Noisy Labels

## Quick Facts
- **arXiv ID**: 2404.18191
- **Source URL**: https://arxiv.org/abs/2404.18191
- **Authors**: Chen Cheng; Xinzhi Yu; Haodong Wen; Jingsong Sun; Guanzhang Yue; Yihao Zhang; Zeming Wei
- **Reference count**: 15
- **Primary result**: Transformers exhibit notable resilience against diverse types of label noise during inference in in-context learning, outperforming simple baselines like least squares estimators

## Executive Summary
This paper investigates the robustness of transformer models during in-context learning when presented with noisy labels in demonstrations. The authors conduct extensive experiments on synthetic datasets, focusing on noisy linear regression tasks. They find that transformers demonstrate significant resilience against various types of label noise during inference, outperforming simple baselines like least squares estimators. Additionally, they explore whether introducing noise into the training set can enhance robustness during inference, discovering that adding noise to training labels can indeed improve ICL robustness, especially for larger model sizes.

## Method Summary
The authors conduct systematic experiments on synthetic linear regression tasks to evaluate transformer robustness during in-context learning with noisy labels. They test multiple noise types (uniform, Gaussian, and adversarial) and noise levels, comparing transformer performance against baseline methods like least squares estimators. The study also investigates whether training transformers on noisy data can improve their robustness during inference, examining different model sizes to understand how scale affects noise resilience.

## Key Results
- Transformers show notable resilience against diverse types of label noise during inference, outperforming simple baselines like least squares estimators
- Adding noise to training labels can improve the robustness of in-context learning, particularly for larger model sizes
- The robustness improvement through noisy training represents a potential data augmentation technique for enhancing model resilience

## Why This Works (Mechanism)
Transformers leverage their attention mechanisms and representation learning capabilities to identify and downweight the influence of noisy demonstrations during in-context learning. The self-attention mechanism allows the model to selectively focus on relevant examples while filtering out corrupted information, enabling it to maintain performance even when some demonstrations contain incorrect labels.

## Foundational Learning
- **In-context learning**: Understanding how models learn from demonstrations without parameter updates
  - Why needed: Core mechanism being studied for robustness evaluation
  - Quick check: Verify understanding of how ICL differs from traditional fine-tuning

- **Label noise types**: Familiarity with uniform, Gaussian, and adversarial noise characteristics
  - Why needed: Different noise types affect model robustness differently
  - Quick check: Can identify characteristics of each noise type and their expected impact

- **Linear regression fundamentals**: Basic understanding of regression tasks and error metrics
  - Why needed: Synthetic experiments focus on linear regression tasks
  - Quick check: Can explain mean squared error and its relevance to regression evaluation

## Architecture Onboarding
- **Component map**: Input embeddings -> Self-attention layers -> Feed-forward networks -> Output layer
- **Critical path**: Input -> Token embeddings -> Multi-head attention -> Feed-forward -> Output prediction
- **Design tradeoffs**: Fixed attention patterns vs. learned attention weights; depth vs. width optimization
- **Failure signatures**: Complete collapse with extreme noise levels; performance degradation with mixed noise types
- **First experiments**:
  1. Replicate baseline least squares performance on clean data
  2. Test transformer on uniform noise regression task
  3. Compare model performance across different noise types at identical levels

## Open Questions the Paper Calls Out
None

## Limitations
- Experiments are primarily conducted on synthetic datasets, potentially limiting real-world applicability
- Focus exclusively on linear regression tasks, leaving generalization to non-linear problems unclear
- Limited range of model sizes tested, constraining conclusions about scale-dependent effects

## Confidence
- **High Confidence**: Transformers show robustness to noisy labels during ICL compared to baselines
- **Medium Confidence**: Noisy training improves inference-time robustness, but needs broader validation
- **Medium Confidence**: Larger models benefit more from noisy training, but evidence is limited to tested sizes

## Next Checks
1. Replicate key experiments on established benchmarks with naturally occurring label noise (e.g., WebVision, Clothing1M) to verify if synthetic findings transfer to practical scenarios

2. Test the noisy training robustness mechanism on non-linear regression tasks and classification problems to assess the breadth of applicability

3. Compare robustness patterns across different transformer variants (BERT, RoBERTa, T5) and attention mechanisms to determine if findings are architecture-specific or general principles