---
ver: rpa2
title: 'MultiLS: A Multi-task Lexical Simplification Framework'
arxiv_id: '2402.14972'
source_url: https://arxiv.org/abs/2402.14972
tags:
- simpli
- lexical
- complexity
- cation
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MultiLS is a multi-task framework for lexical simplification (LS)
  that provides a unified dataset containing lexical complexity values and gold candidate
  simplifications for all LS sub-tasks. This addresses the lack of datasets that cover
  the full LS pipeline, from lexical complexity prediction (LCP) to substitute generation
  and ranking.
---

# MultiLS: A Multi-task Lexical Simplification Framework

## Quick Facts
- **arXiv ID**: 2402.14972
- **Source URL**: https://arxiv.org/abs/2402.14972
- **Reference count**: 40
- **Primary result**: MultiLS provides unified dataset covering full lexical simplification pipeline for Portuguese with strong model performance

## Executive Summary
MultiLS is a multi-task framework for lexical simplification (LS) that addresses the critical gap in datasets covering the complete LS pipeline from lexical complexity prediction to substitute generation and ranking. The framework introduces MultiLS-PT, the first Portuguese dataset containing 5,165 target words annotated with complexity values and candidate simplifications across three genres: Bible, news, and biomedical texts. The study demonstrates that transformer models excel at lexical complexity prediction (R up to 0.8423), while large language models achieve better results for substitute generation (A@1@Top1 up to 0.1708) when provided with context-oriented prompts.

## Method Summary
The MultiLS framework creates a unified dataset that spans the complete lexical simplification pipeline by collecting annotations for lexical complexity values and gold candidate simplifications for each sub-task. The methodology involves gathering target words from multiple genres (Bible, news, biomedical texts) and annotating them with complexity scores while also collecting context-appropriate substitute candidates. For Portuguese (MultiLS-PT), the dataset contains 5,165 target words with comprehensive annotations. The framework evaluates different model types including transformers for complexity prediction and large language models for substitute generation, using both traditional metrics and human evaluation where applicable.

## Key Results
- Transformer models achieved strong performance on lexical complexity prediction (R up to 0.8423)
- Large language models outperformed other approaches for substitute generation (A@1@Top1 up to 0.1708) when using context-oriented prompts
- Genre and context significantly impact LS task performance, with genre-specific fine-tuning recommended
- MultiLS-PT successfully demonstrates the framework's applicability to resource-scarce languages

## Why This Works (Mechanism)
The MultiLS framework succeeds by unifying the fragmented landscape of lexical simplification datasets and tasks into a coherent pipeline. By providing a single dataset that covers all LS sub-tasks (complexity prediction, generation, and ranking) with consistent annotations, it enables end-to-end evaluation of LS systems. The inclusion of context-oriented prompts for substitute generation allows models to better understand usage patterns, while the multi-genre approach captures the variation in simplification needs across different text types. This comprehensive coverage enables more realistic evaluation of LS systems in practical applications.

## Foundational Learning

**Lexical Complexity Prediction (LCP)**: Why needed - Core task of determining word difficulty for target audiences; Quick check - Verify Pearson correlation metrics are correctly calculated between predicted and actual complexity scores.

**Substitute Generation**: Why needed - Essential for identifying simpler alternatives to complex words; Quick check - Confirm that generated candidates maintain semantic meaning while reducing complexity.

**Substitute Ranking**: Why needed - Determines optimal simplification from multiple candidates; Quick check - Validate ranking accuracy using appropriate evaluation metrics like BLEU or chrF.

**Genre Adaptation**: Why needed - Different text types require different simplification strategies; Quick check - Test model performance across multiple genres to identify domain-specific patterns.

## Architecture Onboarding

**Component Map**: Data Collection -> Complexity Annotation -> Substitute Collection -> Model Training (LCP/Generation/Ranking) -> Evaluation

**Critical Path**: Target word extraction → Complexity annotation → Candidate collection → Model training → Pipeline evaluation

**Design Tradeoffs**: The framework prioritizes comprehensive coverage over dataset size, choosing 5,165 carefully annotated words across multiple genres rather than a larger monolingual dataset. This enables genre-aware simplification but may limit statistical power for some analyses.

**Failure Signatures**: Poor substitute generation indicates inadequate context modeling; low complexity prediction scores suggest insufficient training data for certain word types; genre-specific performance drops reveal need for domain adaptation.

**3 First Experiments**:
1. Evaluate transformer performance on complexity prediction using standard regression metrics
2. Test LLM substitute generation with and without context-oriented prompts
3. Compare genre-specific vs. general model performance across all three text types

## Open Questions the Paper Calls Out

None provided in the source material.

## Limitations

- Dataset size of 5,165 target words may limit generalizability across broader linguistic phenomena
- Focus on only three genres (Bible, news, biomedical texts) may not capture full diversity of real-world simplification needs
- Modest results for substitute generation (A@1@Top1 up to 0.1708) indicate room for improvement in the complete LS pipeline
- Limited cross-linguistic and cross-domain testing beyond Portuguese evaluation

## Confidence

- **High**: Framework novelty in providing unified LS sub-task coverage; creation of first Portuguese dataset
- **Medium**: Claims about genre and context importance in LS tasks
- **Low**: Generalization claims about model performance across languages and domains

## Next Checks

1. Evaluate MultiLS-PT performance across additional Portuguese text genres beyond the three currently covered
2. Conduct cross-lingual experiments to assess framework applicability to other languages with varying resource availability
3. Implement long-term user studies to measure whether generated simplifications actually improve comprehension for target populations compared to existing approaches