---
ver: rpa2
title: When Will Gradient Regularization Be Harmful?
arxiv_id: '2406.09723'
source_url: https://arxiv.org/abs/2406.09723
tags:
- warmup
- training
- gradient
- regularization
- norm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Gradient regularization (GR) improves generalization but can degrade
  performance when combined with adaptive optimizers and learning rate warmup. The
  issue arises because GR induces gradient norm instability at early training stages,
  leading to divergence in adaptive optimizer statistics.
---

# When Will Gradient Regularization Be Harmful?

## Quick Facts
- arXiv ID: 2406.09723
- Source URL: https://arxiv.org/abs/2406.09723
- Reference count: 32
- Gradient regularization improves generalization but can degrade performance when combined with adaptive optimizers and learning rate warmup

## Executive Summary
Gradient regularization (GR) is a technique used to improve generalization in deep learning models, but this paper identifies scenarios where it can be harmful. The authors discover that GR causes gradient norm instability during early training stages, particularly when combined with adaptive optimizers like AdamW and learning rate warmup schedules. This instability leads to divergence in the adaptive optimizer's statistics, ultimately degrading model performance. To address this issue, the paper proposes three warmup strategies (r-warmup, λ-warmup, and zero-warmup) that gradually relax GR's regularization effect during the warmup phase.

## Method Summary
The paper investigates why gradient regularization sometimes degrades performance when used with adaptive optimizers and learning rate warmup. Through theoretical analysis and empirical validation, the authors identify that GR induces gradient norm instability at early training stages, causing divergence in adaptive optimizer statistics. To address this, they propose three GR warmup strategies that gradually reduce the regularization effect during warmup: r-warmup (reducing regularization strength), λ-warmup (increasing regularization weight), and zero-warmup (temporarily disabling GR). These strategies are evaluated across multiple Vision Transformer architectures and datasets, showing consistent performance improvements over standard GR implementations.

## Key Results
- Gradient regularization degrades performance when combined with adaptive optimizers and learning rate warmup due to gradient norm instability
- The proposed warmup strategies improve performance across CIFAR-10, CIFAR-100, TinyImageNet, and ImageNet datasets
- Zero-warmup strategy achieves the best results, with up to 3% improvement on CIFAR-10 for large models compared to baseline GR

## Why This Works (Mechanism)
Gradient regularization works by penalizing large gradients during training, which theoretically improves generalization by smoothing the loss landscape. However, when combined with adaptive optimizers like AdamW and learning rate warmup, GR creates instability in gradient norms during early training stages. This instability causes the adaptive optimizer's moving statistics (momentum and variance estimates) to diverge, leading to incorrect adaptive learning rate calculations. The proposed warmup strategies mitigate this by gradually introducing or reducing the regularization effect, allowing the optimizer's statistics to stabilize before full regularization is applied.

## Foundational Learning

**Adaptive optimization algorithms (e.g., Adam, AdamW)**
*Why needed:* Understanding how AdamW adapts learning rates based on gradient statistics is crucial for grasping why GR-induced instability causes problems
*Quick check:* Know that AdamW maintains moving averages of gradients and squared gradients to scale learning rates per parameter

**Gradient regularization techniques**
*Why needed:* GR adds a penalty term proportional to gradient norm to the loss function, affecting both optimization dynamics and generalization
*Quick check:* Understand that GR modifies the effective learning rate and can smooth the loss landscape

**Learning rate warmup schedules**
*Why needed:* Warmup gradually increases learning rate from a small value, which is standard practice for training deep networks
*Quick check:* Recognize that warmup is used to stabilize early training when using large initial learning rates

## Architecture Onboarding

**Component map:** Vision Transformer -> Gradient Regularization -> Adaptive Optimizer (AdamW) -> Learning Rate Warmup

**Critical path:** Data input → Vision Transformer forward pass → Loss computation → Gradient calculation → GR penalty application → Optimizer update with adaptive learning rates

**Design tradeoffs:** Standard GR provides generalization benefits but causes instability with adaptive optimizers during warmup; warmup strategies trade immediate regularization benefits for stable training dynamics

**Failure signatures:** Performance degradation (lower accuracy/validation loss) when using GR with AdamW and warmup; unstable training curves with spikes in gradient norms; optimizer divergence indicated by exploding parameter updates

**3 first experiments:**
1. Train Vision Transformer with GR + AdamW + warmup vs. GR + AdamW without warmup to observe performance differences
2. Compare gradient norm stability across different warmup strategies during early training epochs
3. Evaluate the impact of warmup strategy choice on final model accuracy across different dataset scales (CIFAR-10 vs. ImageNet)

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Findings are based on Vision Transformer architectures and may not generalize to other model families like convolutional or recurrent networks
- Theoretical analysis focuses on gradient norm instability but may not capture all failure modes of gradient regularization
- Proposed warmup strategies introduce additional hyperparameters that require tuning, potentially offsetting practical benefits

## Confidence

**High confidence:** Gradient regularization degrades performance when combined with adaptive optimizers and learning rate warmup (supported by empirical evidence and theoretical analysis)

**Medium confidence:** The proposed warmup strategies consistently improve performance across all tested scenarios (empirical results show improvements but depend on hyperparameter choices)

**Medium confidence:** Gradient norm instability is the primary mechanism causing performance degradation (supported by theoretical analysis but alternative explanations not fully ruled out)

## Next Checks

1. Test the proposed warmup strategies on non-Transformer architectures (e.g., convolutional networks, recurrent networks) to assess generalizability across model families

2. Evaluate performance on additional datasets beyond vision tasks, including NLP and tabular data, to verify domain independence

3. Conduct ablation studies on the warmup strategies to determine whether similar benefits could be achieved through alternative mechanisms (e.g., modified optimizer initialization or learning rate schedules)