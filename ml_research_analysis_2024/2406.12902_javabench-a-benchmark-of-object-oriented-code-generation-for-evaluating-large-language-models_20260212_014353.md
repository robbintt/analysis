---
ver: rpa2
title: 'JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating Large
  Language Models'
arxiv_id: '2406.12902'
source_url: https://arxiv.org/abs/2406.12902
tags:
- code
- context
- llms
- java
- class
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the scarcity of Java benchmarks for evaluating
  large language models (LLMs) in object-oriented programming. The authors introduce
  JavaBench, a project-level Java benchmark that exercises OOP features (encapsulation,
  inheritance, polymorphism) across four Java projects containing 389 methods in 106
  classes.
---

# JavaBench: A Benchmark of Object-Oriented Code Generation for Evaluating Large Language Models

## Quick Facts
- arXiv ID: 2406.12902
- Source URL: https://arxiv.org/abs/2406.12902
- Reference count: 40
- This paper introduces JavaBench, a project-level Java benchmark for evaluating LLMs on object-oriented programming features, revealing that while LLMs achieve high completion rates (91.73%), they struggle significantly with compilation (72.33%) and passing test cases (70.92%) compared to undergraduate students.

## Executive Summary
This paper addresses the scarcity of Java benchmarks for evaluating large language models (LLMs) in object-oriented programming. The authors introduce JavaBench, a project-level Java benchmark that exercises OOP features (encapsulation, inheritance, polymorphism) across four Java projects containing 389 methods in 106 classes. The benchmark is validated by 282 undergraduate students, achieving a 90.93/100 average score, and includes comprehensive test suites with 92% code coverage.

To evaluate LLMs, the authors propose a systematic evaluation design using three context settings (maximum, minimum, selected), five synthesis strategies (holistic, independent, incremental variants), and three metrics (completion, compilation, pass rates) at two granularities (class-wise, test-wise). Extensive experiments with five LLMs reveal that while LLMs perform reasonably at completion (91.73% average), they struggle with compilation (72.33%) and passing test cases (70.92%), significantly lagging behind undergraduate students. The optimal context setting was found to be providing only method signatures, highlighting that too much or too little context negatively impacts performance. This benchmark provides a valuable tool for assessing LLMs' capabilities in handling complex Java OOP features.

## Method Summary
The authors constructed JavaBench by selecting four Java projects with diverse OOP features and comprehensive test suites. They validated the benchmark through 282 undergraduate students who achieved an average score of 90.93/100. For LLM evaluation, they systematically varied context settings (maximum, minimum, selected), synthesis strategies (holistic, independent, incremental), and evaluation granularities (class-wise, test-wise). The selected context setting included only method signatures from related classes, while the holistic synthesis strategy generated all methods simultaneously to maintain consistency.

## Key Results
- LLMs achieved high completion rates (91.73%) but struggled with compilation (72.33%) and passing test cases (70.92%)
- The selected context setting (method signatures only) outperformed both maximum and minimum context settings
- Holistic synthesis strategy consistently outperformed independent and incremental strategies across all evaluated LLMs
- Finer-grained evaluation granularity (class-wise vs test-wise) captured more subtle success cases in model performance

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The selected context setting achieves optimal performance because it provides sufficient contextual dependencies without overwhelming the model's input window.
- **Mechanism**: By including only method signatures from related classes rather than full method bodies or entire project skeletons, the model receives necessary dependency information while avoiding token truncation and context dilution effects.
- **Core assumption**: The model can infer implementation details from signatures alone when given appropriate dependency relationships.
- **Evidence anchors**:
  - [abstract] "using method signature as prompt context may strike an ideal balance for project-level code generation"
  - [section] "we only include the method signatures in the related class, excluding the method body"
  - [corpus] Weak - no direct evidence from corpus papers about signature-only context effectiveness
- **Break condition**: If models require more contextual information than signatures can provide for complex dependency resolution, or if signature patterns become too ambiguous.

### Mechanism 2
- **Claim**: Holistic synthesis strategy outperforms other strategies because it allows the model to maintain consistency across method implementations within a class.
- **Mechanism**: Generating all methods in one pass enables the model to understand cross-method dependencies and maintain internal consistency, avoiding the coordination problems that arise when methods are generated independently or in sequence.
- **Core assumption**: The model's context window is sufficient to hold all method implementations simultaneously during generation.
- **Evidence anchors**:
  - [abstract] "the best scores were achieved using the holistic strategy"
  - [section] "holistic synthesis was generally better than independent and incremental among all LLMs"
  - [corpus] Weak - corpus papers don't directly compare holistic vs. other synthesis strategies
- **Break condition**: If context windows become too small relative to class complexity, or if models struggle with maintaining coherence across multiple simultaneous generation tasks.

### Mechanism 3
- **Claim**: Finer-grained evaluation granularity captures subtle success cases that would be missed by coarser evaluation, revealing true model capabilities.
- **Mechanism**: By evaluating at class-wise and test-wise levels rather than project-wide, the benchmark can identify successful partial completions that contribute to overall understanding, even when the full project fails.
- **Core assumption**: Partial success in code generation provides meaningful signal about model capabilities.
- **Evidence anchors**:
  - [abstract] "finer granularities can capture subtle success in performance, enabling more distinguishable results"
  - [section] "class-wise scores were always better than test-wise scores, with a gap up to 49.92%"
  - [corpus] Weak - corpus papers don't discuss evaluation granularity tradeoffs in detail
- **Break condition**: If partial successes don't generalize to complete solutions, or if granularity differences create misleading performance comparisons.

## Foundational Learning

- **Concept**: Object-Oriented Programming principles (encapsulation, inheritance, polymorphism)
  - Why needed here: JavaBench specifically exercises these advanced OOP features, so understanding them is crucial for both benchmark creation and evaluation
  - Quick check question: What is the key difference between encapsulation and inheritance in Java, and how would each affect code generation?

- **Concept**: Large Language Model context window limitations and tokenization
  - Why needed here: The benchmark design must work within LLM constraints, and context selection strategies depend on understanding how different tokenizers affect input length
  - Quick check question: Why might character count be a more reliable metric than token count when comparing context sizes across different LLM architectures?

- **Concept**: Test-driven development and code coverage metrics
  - Why needed here: JavaBench uses comprehensive test suites with 92% coverage, so understanding how to design effective tests and interpret coverage metrics is essential
  - Quick check question: How does mock-based testing help isolate components in object-oriented code, and why is this particularly important for benchmark evaluation?

## Architecture Onboarding

- **Component map**: Project Skeleton → Context Setting Selection → Synthesis Strategy Application → Code Generation → Evaluation Framework (Class-wise → Test-wise)
- **Critical path**: Skeleton → Context Setting → Synthesis Strategy → Code Generation → Evaluation → Results
- **Design tradeoffs**: Context richness vs. token limits (selected context wins), generation consistency vs. computational efficiency (holistic wins), granularity vs. evaluation clarity (finer granularity wins)
- **Failure signatures**: Completion errors (missing method bodies), compilation errors (syntax/semantic issues), test failures (logic/implementation mismatches)
- **First 3 experiments**:
  1. Run baseline with maximum context and independent synthesis to establish worst-case performance
  2. Test selected context with holistic synthesis to verify optimal setting claims
  3. Compare class-wise vs test-wise evaluation results to validate granularity effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal context size for project-level Java code generation that balances information richness and input token limits?
- Basis in paper: [explicit] The paper explores three context settings (maximum, minimum, selected) and finds selected context performs best, but doesn't systematically test intermediate context sizes or determine the optimal point
- Why unresolved: The study only tests extreme cases (maximum vs minimum) and one intermediate setting (selected context with method signatures only), without exploring the continuum between these points or finding the sweet spot
- What evidence would resolve it: Systematic experiments varying context size (e.g., 25%, 50%, 75% of available context) across multiple projects and measuring compilation/pass rates to identify the point of diminishing returns

### Open Question 2
- Question: How do different project complexity metrics correlate with LLM code generation performance?
- Basis in paper: [explicit] The paper mentions cyclomatic and cognitive complexity but doesn't analyze how these complexity metrics relate to LLM performance on different projects
- Why unresolved: The paper establishes that projects have varying complexity but doesn't investigate whether certain complexity types (e.g., high cyclomatic vs high cognitive) are more challenging for LLMs
- What evidence would resolve it: Regression analysis correlating complexity metrics with completion/compilation/pass rates across projects to identify which complexity factors most impact LLM performance

### Open Question 3
- Question: Can retrieval-augmented generation techniques improve project-level Java code generation compared to simple context selection?
- Basis in paper: [explicit] The paper mentions RAG as relevant related work but doesn't test whether more sophisticated retrieval methods would outperform their simple "selected context" approach
- Why unresolved: The paper implements a basic context selection method (method signatures only) but doesn't compare it against more advanced retrieval techniques that could potentially provide better context
- What evidence would resolve it: Head-to-head comparison of simple context selection versus RAG-based approaches using the same evaluation framework to measure improvements in compilation and pass rates

## Limitations
- Benchmark validation relies on undergraduate students rather than expert developers, potentially missing production-quality code expectations
- Minimal related work (average neighbor citations: 0.0) limits comparative validation of findings
- Java-specific focus may not generalize to other programming paradigms or languages

## Confidence
- **High confidence**: Benchmark construction methodology and evaluation framework are well-documented and reproducible
- **Medium confidence**: Student validation provides reasonable verification, though expert review would strengthen claims
- **Low confidence**: Claims about optimal context settings and synthesis strategies lack external validation

## Next Checks
1. Have experienced Java developers evaluate the same benchmark projects to compare against undergraduate assessments and validate the 90.93/100 score
2. Apply the same benchmark methodology to other object-oriented languages (e.g., C++, Python) to test generalizability of findings about context settings and synthesis strategies
3. Track LLM performance across multiple versions of the same benchmark to assess whether improvements in models translate to better OOP feature handling over time