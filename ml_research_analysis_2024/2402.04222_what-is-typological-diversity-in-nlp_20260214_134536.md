---
ver: rpa2
title: What is "Typological Diversity" in NLP?
arxiv_id: '2402.04222'
source_url: https://arxiv.org/abs/2402.04222
tags:
- language
- languages
- computational
- linguistics
- typological
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically investigates the use of the term 'typological
  diversity' in multilingual NLP research. It finds that there is no consistent definition
  or methodology for making such claims, leading to considerable variation in estimates
  of typological diversity across papers.
---

# What is "Typological Diversity" in NLP?

## Quick Facts
- arXiv ID: 2402.04222
- Source URL: https://arxiv.org/abs/2402.04222
- Reference count: 33
- Primary result: Systematic investigation of 'typological diversity' in multilingual NLP research reveals inconsistent definitions and methodologies, leading to overestimated multilingual performance due to skewed language selection.

## Executive Summary
This study systematically investigates the use of the term 'typological diversity' in multilingual NLP research, finding that there is no consistent definition or methodology for making such claims. The analysis shows that language selections in these papers vary in terms of mean pairwise syntactic distance and absolute typological feature value inclusion. Crucially, the study demonstrates that skewed language selection can lead to overestimated multilingual performance, as exemplified by the XTREME-R benchmark. The authors recommend that future work include an operationalization of 'typological diversity' and add an empirical justification for such claims, such as mean pairwise language distance or typological feature inclusion. They provide software to facilitate this process.

## Method Summary
The study conducted a systematic review of papers from ACL Anthology and top-tier AI venues containing claims about 'typological diversity'. Papers were annotated for typological diversity claims, dataset introductions, and language identification. The authors calculated mean pairwise syntactic distance (MPSD) and Grambank feature value inclusion for each language set. They analyzed the relationship between diversity metrics and multilingual performance using the XTREME-R benchmark, comparing overall performance averages with feature-specific averages to demonstrate the impact of skewed language selection.

## Key Results
- No consistent definition or methodology for 'typological diversity' exists across multilingual NLP papers
- Language selection varies significantly in terms of mean pairwise syntactic distance and feature value inclusion
- Skewed language selection can lead to overestimated multilingual performance, as demonstrated by XTREME-R benchmark analysis
- The increase in typological feature value coverage flattens at approximately 40 languages
- Geographic and genealogical sampling does not guarantee typological diversity

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Including more languages does not necessarily increase typological diversity in a multilingual NLP dataset.
- Mechanism: Language addition is often driven by dataset availability rather than systematic typological coverage. As a result, new languages added to a dataset are frequently typologically similar to already included ones, leading to a plateau in diversity gains.
- Core assumption: Resource availability biases language selection toward languages with existing data, which tend to be geographically or genealogically proximate.
- Evidence anchors:
  - [abstract] "in NLP, adding more languages typically means adding more similar languages, since it is easier to incorporate existing datasets than it is to create new ones."
  - [section 5.3] "we observe that up to a certain point, including more languages implies that more typological feature values are covered. However, this increase flattens at approximately 40 languages."
- Break condition: If language selection becomes explicitly driven by maximizing typological distance rather than dataset availability, the mechanism no longer holds.

### Mechanism 2
- Claim: Skewed language selection can lead to overestimated multilingual model performance.
- Mechanism: When evaluation languages are clustered by typological features (e.g., all suffixing languages), the average performance metric becomes biased toward that cluster. This creates an inflated sense of generalizability because the model is tested primarily on languages with similar properties.
- Core assumption: Aggregated evaluation metrics (e.g., average F1 across languages) are sensitive to the distribution of typological properties in the evaluation set.
- Evidence anchors:
  - [abstract] "skewed language selection can lead to overestimated multilingual performance, as exemplified by the XTREME-R benchmark."
  - [section 6] "the delta column shows the difference between the macro average over all languages and the macro average per feature value... estimations of multilingual model performance vary considerably when accounting for typological imbalances."
- Break condition: If evaluation is stratified by typological features or if balanced sampling is enforced, the overestimation effect diminishes.

### Mechanism 3
- Claim: Geographic or genealogical sampling does not guarantee typological diversity.
- Mechanism: Languages grouped by geography or genealogy often share structural features due to contact or inheritance, but these groupings do not necessarily capture the full range of typological variation. Direct measurement of typological features is required to ensure diversity.
- Core assumption: Genealogical and geographical proximity are imperfect proxies for typological similarity.
- Evidence anchors:
  - [section 2] "Nichols (1996) argue that 'the kind of variables that define genealogical groups and tree shapes have a very different nature from the kind of variables that define typological diversity'."
  - [section 5.1] "Interestingly, the genetic distance is much higher and less spread out than the syntactic distance. This emphasizes that genealogical sampling does not by default ensure typological diversity."
- Break condition: If genealogical or geographical proxies are validated as reliable indicators of typological diversity for a specific research question, the mechanism weakens.

## Foundational Learning

- Concept: Typological diversity
  - Why needed here: The paper's core argument is that "typological diversity" is used inconsistently in NLP research, and its definition matters for evaluating multilingual models.
  - Quick check question: What is the difference between typological diversity and genealogical diversity?

- Concept: Mean Pairwise Syntactic Distance (MPSD)
  - Why needed here: MPSD is one of the proposed metrics for quantifying typological diversity in language samples.
  - Quick check question: How is MPSD calculated from language vectors in lang2vec?

- Concept: Feature value inclusion
  - Why needed here: Feature value inclusion measures the coverage of typological feature values in a language sample, providing another dimension of diversity assessment.
  - Quick check question: What does it mean if a language sample has high feature value inclusion but low MPSD?

## Architecture Onboarding

- Component map:
  - Data collection -> Paper annotation -> Diversity metric calculation -> Performance analysis -> Software release

- Critical path:
  1. Retrieve papers with typological diversity claims
  2. Annotate claims, datasets, and languages
  3. Calculate diversity metrics (MPSD, feature inclusion)
  4. Analyze relationship between diversity and performance
  5. Recommend best practices and release code

- Design tradeoffs:
  - Search scope vs. annotation effort: Broad search captures more papers but increases annotation load
  - Metric choice: MPSD captures pairwise distances but may miss feature value coverage; feature inclusion captures coverage but ignores feature combinations
  - Resource bias: Bibliographic bias in typological databases affects diversity measurements

- Failure signatures:
  - Inconsistent annotations due to unclear guidelines or ambiguous language mentions
  - Skewed diversity metrics due to missing language coverage in typological databases
  - Overstated performance improvements due to unbalanced language selection

- First 3 experiments:
  1. Reproduce the MPSD calculation for a sample of papers to verify implementation
  2. Calculate feature value inclusion for the XTREME-R languages and compare to the paper's results
  3. Simulate performance overestimation by creating balanced and unbalanced language sets and evaluating a multilingual model on each

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the geographic and genealogical proximity of languages in a typologically diverse sample affect the performance of multilingual NLP models?
- Basis in paper: [inferred] The paper discusses how languages from certain areas are overrepresented and suggests that geographic and genealogical sampling does not ensure typological diversity.
- Why unresolved: The paper highlights the potential issues with using geography and genealogy as proxies for typological diversity but does not empirically investigate how these factors affect model performance.
- What evidence would resolve it: Empirical studies comparing model performance on geographically and genealogically diverse samples versus typologically diverse samples, controlling for other variables.

### Open Question 2
- Question: What is the optimal number of languages to include in a multilingual NLP evaluation to ensure robust typological coverage without redundancy?
- Basis in paper: [explicit] The paper suggests that simply adding more languages does not necessarily raise the average syntactic distance and that the increase in typological feature value coverage flattens at approximately 40 languages.
- Why unresolved: The paper provides insights into the relationship between the number of languages and typological diversity but does not determine an optimal number that balances coverage and redundancy.
- What evidence would resolve it: Studies analyzing the trade-off between the number of languages and typological coverage in terms of model performance and evaluation robustness.

### Open Question 3
- Question: How do different language sampling methods (e.g., random, probability, variety) impact the generalizability of multilingual NLP models?
- Basis in paper: [explicit] The paper mentions that language sampling methods in linguistic typology include random, probability, and variety sampling, but it does not explore how these methods impact NLP model performance.
- Why unresolved: The paper discusses the importance of diverse language sampling for generalizable conclusions but does not empirically compare the effects of different sampling methods on model performance.
- What evidence would resolve it: Comparative studies evaluating multilingual NLP models trained on datasets sampled using different methods, assessing their performance across a wide range of languages.

## Limitations
- The study's conclusions are based on a sample of papers that may not fully represent the broader field of multilingual NLP research
- The search string used to retrieve papers may have missed relevant work
- The annotation process may have introduced bias
- The study focuses on syntactic distance and feature value inclusion as metrics for typological diversity, which may not capture all relevant dimensions of linguistic variation

## Confidence
- High: The claim that there is no consistent definition or methodology for 'typological diversity' in multilingual NLP research is well-supported by the analysis of 54 papers.
- Medium: The claim that skewed language selection can lead to overestimated multilingual performance is supported by the XTREME-R benchmark analysis, but the generalizability to other benchmarks and models is uncertain.
- Low: The claim that including more languages does not necessarily increase typological diversity is based on a specific observation in the study's dataset and may not hold across all language samples.

## Next Checks
1. Replicate the analysis using a broader search string and additional NLP venues to assess the generalizability of the findings
2. Compare the performance overestimation effect in XTREME-R to other multilingual benchmarks, such as XGLUE or MultiCoNER, to determine if the effect is specific to XTREME-R or a more general phenomenon
3. Evaluate the impact of different typological metrics (e.g., phonological distance, semantic similarity) on the relationship between language selection and multilingual performance