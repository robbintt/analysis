---
ver: rpa2
title: Breeze-7B Technical Report
arxiv_id: '2403.02712'
source_url: https://arxiv.org/abs/2403.02712
tags:
- language
- chinese
- benchmark
- breeze-7b
- traditional
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Breeze-7B is an open-source language model based on Mistral-7B,
  designed to improve Traditional Chinese language comprehension and chatbot capabilities.
  The model uses a customized tokenizer with extended vocabulary to increase compression
  efficiency for Chinese text.
---

# Breeze-7B Technical Report

## Quick Facts
- arXiv ID: 2403.02712
- Source URL: https://arxiv.org/abs/2403.02712
- Authors: Chan-Jan Hsu; Chang-Le Liu; Feng-Ting Liao; Po-Chun Hsu; Yi-Chang Chen; Da-Shan Shiu
- Reference count: 6
- Breeze-7B-Instruct achieves 36.46 accuracy on TMMLU+ and 80.61 exact match on DRCD for Traditional Chinese language comprehension

## Executive Summary
Breeze-7B is an open-source language model based on Mistral-7B, specifically designed to improve Traditional Chinese language comprehension and chatbot capabilities. The model features a customized tokenizer with extended vocabulary that increases compression efficiency for Chinese text, and was trained on 650GB of data using a flat learning rate of 1e-6. The model demonstrates competitive performance on Traditional Chinese benchmarks while maintaining strong English capabilities inherited from the base model.

## Method Summary
Breeze-7B extends the Mistral-7B tokenizer by adding 29,873 Traditional Chinese tokens using Byte-Pair Encoding, resulting in a total vocabulary of 61,872 tokens. The model undergoes additional pretraining on 650GB of Chinese data using a flat learning rate of 1e-6 and batch size of 4 million tokens, implemented with the Megatron-LLM library using tensor and data parallel training. Instruction finetuning is performed with cosine annealing learning rate to enable conversational abilities while preserving base model knowledge.

## Key Results
- Breeze-7B-Instruct achieves 36.46 accuracy on TMMLU+ Traditional Chinese comprehension benchmark
- Model scores 80.61 exact match on DRCD reading comprehension test
- Performs competitively on chatbot benchmarks (6.0 on MT-Bench-tw, 7.4 on MT-Bench) while trailing only Qwen1.5-7B

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Token vocabulary extension directly improves Chinese text compression efficiency.
- Mechanism: By using Byte-Pair Encoding (BPE) on a Chinese corpus to generate additional tokens, the model reduces the average token count per Chinese character from 2-3 tokens down to 1 token, effectively doubling the compression ratio.
- Core assumption: The BPE algorithm can identify frequent Chinese character sequences that benefit from being treated as single tokens.
- Evidence anchors:
  - [abstract] "The model uses a customized tokenizer with extended vocabulary to increase compression efficiency for Chinese text."
  - [section 2.1] "We use the BPE algorithm [Sennrich et al., 2016] on a Chinese corpus to generate tokens to appended to the original tokenizer. After de-duplicating identical tokens already present in Mistral tokenizer, we extended 29873 tokens to it to arrive at 61872 total tokens. With the extended vocabulary, the compression rate on the Chinese corpus is around 2x compared to that of the Mistral model."

### Mechanism 2
- Claim: Large-scale pretraining on Chinese corpus improves Traditional Chinese language comprehension benchmarks.
- Mechanism: The model undergoes additional pretraining on 650GB of Chinese data, allowing it to learn domain-specific patterns and knowledge that are not present in the original Mistral-7B training data.
- Core assumption: The 650GB Chinese corpus contains sufficient diversity and quality to meaningfully improve Chinese language understanding.
- Evidence anchors:
  - [abstract] "The model uses a customized tokenizer with extended vocabulary to increase compression efficiency for Chinese text. It was trained on 650GB of data with a flat learning rate of 1e-6 and a batch size of 4 million tokens."
  - [section 2.2] "We trained the model on 650 gigabytes of data using a total of 7,000 H100 hours to arrive at Breeze-7B-Base-v1 0."
  - [section 4.2.1] "From the results, we can draw three conclusions. First, the Breeze-7B-Instruct model performs well on TMMLU+ compared to other models of the same size, while outperforming other models in the Penguins-in-a-table-TC dataset."

### Mechanism 3
- Claim: Instruction finetuning preserves base model knowledge while adding conversational abilities.
- Mechanism: The model is finetuned using publicly available instruction datasets with filtering and transformation methods, allowing it to maintain its knowledge base while learning to follow instructions and engage in conversations.
- Core assumption: The finetuning process with cosine annealing learning rate prevents catastrophic forgetting of the base model's knowledge.
- Evidence anchors:
  - [abstract] "It also maintains all the remarkable capabilities inherited from Mistral-7B."
  - [section 2.3] "To enable the Breeze-7B to complete question answering in a human-preferred instructional or chatting format, we fine-tuned the model with additional paired data. We fine-tuned the model for multiple epochs with a cosine annealing learning rate [Loshchilov and Hutter, 2016]."
  - [section 4.2.1] "Second, Breeze-7B-Instruct demonstrates superior or comparable performance to the Breeze-7B-Base model, indicating that our finetuning procedure effectively retains the knowledge in the base model."

## Foundational Learning

- Concept: Tokenization and compression efficiency
  - Why needed here: Understanding how tokenization affects model performance and inference speed is crucial for optimizing language models for specific languages.
  - Quick check question: How does extending the tokenizer vocabulary from 32,000 to 61,872 tokens improve Chinese text compression ratio from 1:2 to 1:1?

- Concept: Large-scale pretraining methodology
  - Why needed here: The pretraining process on 650GB of Chinese data is the foundation for the model's improved Chinese language capabilities.
  - Quick check question: What are the key differences between pretraining a model from scratch versus continuing pretraining on an existing model like Mistral-7B?

- Concept: Instruction finetuning and catastrophic forgetting
  - Why needed here: The finetuning process must balance learning new conversational abilities while preserving the knowledge gained during pretraining.
  - Quick check question: How does cosine annealing learning rate help prevent catastrophic forgetting during instruction finetuning?

## Architecture Onboarding

- Component map: Tokenizer extension → Additional pretraining → Instruction finetuning → Evaluation
- Critical path: Tokenizer extension → Additional pretraining → Instruction finetuning → Evaluation
- Design tradeoffs:
  - Tokenizer extension increases vocabulary size and model parameters but improves Chinese compression and inference speed
  - Large-scale pretraining requires significant computational resources but improves domain-specific capabilities
  - Instruction finetuning must balance learning new abilities with preserving base model knowledge
- Failure signatures:
  - Poor Chinese compression ratio despite tokenizer extension indicates ineffective token selection
  - Decreased performance on English benchmarks suggests catastrophic forgetting during pretraining
  - Inability to follow instructions despite finetuning indicates insufficient or poor-quality instruction data
- First 3 experiments:
  1. Measure Chinese text compression ratio with and without extended tokenizer on a representative Chinese corpus
  2. Compare perplexity on Chinese validation set during pretraining to identify optimal checkpoint
  3. Evaluate instruction-following ability on a small set of Chinese instructions before and after finetuning

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to small model sizes, preventing assessment of scaling effects
- Lack of ablation studies to isolate individual contribution of each improvement
- Safety and bias characteristics not evaluated, which is concerning for Chinese language model
- Limited detail on data curation and quality control procedures

## Confidence

**High Confidence:** Technical specifications of model architecture and training setup are clearly documented and verifiable.

**Medium Confidence:** Claim of competitive performance on Traditional Chinese benchmarks is supported by reported scores but relies on limited evaluation set.

**Low Confidence:** Assertion that model "maintains all remarkable capabilities inherited from Mistral-7B" lacks comprehensive empirical verification.

## Next Checks
1. Measure actual token counts and compression ratios on standardized Traditional Chinese corpus to verify claimed 2x improvement.
2. Track English benchmark performance throughout pretraining process to quantify knowledge retention and identify potential degradation.
3. Evaluate performance of Breeze-7B on Chinese benchmarks compared with scaled-up versions to determine if improvements follow expected scaling trends.