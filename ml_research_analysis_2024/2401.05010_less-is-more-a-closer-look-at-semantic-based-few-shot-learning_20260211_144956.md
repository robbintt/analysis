---
ver: rpa2
title: 'Less is More: A Closer Look at Semantic-based Few-Shot Learning'
arxiv_id: '2401.05010'
source_url: https://arxiv.org/abs/2401.05010
tags:
- learning
- visual
- few-shot
- shot
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a simple yet effective semantic-based few-shot
  learning framework that directly adds visual and textual features for classification,
  avoiding complex fusion modules. The method uses learnable prompts to exploit the
  zero-shot capability of pre-trained language models and applies self-ensemble and
  distillation for further performance improvement.
---

# Less is More: A Closer Look at Semantic-based Few-Shot Learning

## Quick Facts
- arXiv ID: 2401.05010
- Source URL: https://arxiv.org/abs/2401.05010
- Reference count: 40
- Simple semantic-based few-shot learning framework achieves SOTA results, outperforming previous methods by 3.3% in 1-shot learning accuracy

## Executive Summary
This paper introduces a streamlined approach to semantic-based few-shot learning that challenges the prevailing trend of complex fusion architectures. The proposed SimpleFSL framework achieves state-of-the-art performance by directly adding visual and textual features for classification, avoiding intricate fusion modules. By leveraging learnable prompts to tap into pre-trained language models' zero-shot capabilities and incorporating self-ensemble and distillation techniques, the method demonstrates significant accuracy improvements across multiple few-shot learning benchmarks.

## Method Summary
The proposed framework takes a refreshingly simple approach to semantic-based few-shot learning by directly adding visual and textual features for classification, bypassing complex fusion modules that dominate the field. The method employs learnable prompts to exploit the zero-shot capabilities of pre-trained language models, creating a semantic space where visual features can be directly compared to textual descriptions. To further enhance performance, the framework incorporates self-ensemble techniques to improve model robustness and applies knowledge distillation to refine predictions. The resulting SimpleFSL++ variant demonstrates that sophisticated architecture is not always necessary for achieving state-of-the-art results in few-shot learning tasks.

## Key Results
- Achieves state-of-the-art performance on four few-shot learning datasets
- Outperforms previous methods by an average of 3.3% in 1-shot learning accuracy
- Demonstrates effectiveness across multiple shot settings and benchmark datasets

## Why This Works (Mechanism)
The success of this approach stems from its ability to create a unified semantic space where visual and textual features can be directly compared without complex transformations. By leveraging pre-trained language models' inherent understanding of semantic relationships, the framework can effectively bridge the gap between visual and textual modalities. The direct addition of features preserves the semantic information from both modalities while maintaining computational simplicity. The learnable prompts allow the model to fine-tune the semantic space for specific few-shot tasks, while self-ensemble and distillation techniques help stabilize training and improve generalization.

## Foundational Learning
- **Few-shot learning**: Learning from very limited examples (typically 1-5 samples per class) - needed to understand the extreme data scarcity scenario
- **Semantic-based approaches**: Methods that use textual descriptions to augment visual learning - quick check: verify the textual descriptions are available for all classes
- **Feature fusion**: Combining information from different modalities - needed to understand why direct addition is novel
- **Prompt learning**: Using learnable prompts to guide pre-trained models - quick check: ensure prompts are properly initialized
- **Knowledge distillation**: Transferring knowledge from larger to smaller models - needed to understand the distillation component
- **Self-ensemble**: Using multiple model variants for improved predictions - quick check: verify ensemble diversity

## Architecture Onboarding

**Component Map**: Visual Features -> Prompt Learning -> Semantic Space -> Direct Addition -> Classification

**Critical Path**: Input images → Visual feature extraction → Prompt-guided semantic transformation → Direct feature addition → Classification head

**Design Tradeoffs**: Simplicity vs. potential expressiveness of complex fusion mechanisms; computational efficiency vs. potential accuracy gains from more sophisticated architectures

**Failure Signatures**: Poor performance when textual descriptions are vague or when visual features don't align well with semantic space; potential issues with class imbalance in few-shot scenarios

**First Experiments**:
1. Verify direct feature addition works on a simple dataset with clear visual-textual correspondences
2. Test learnable prompt effectiveness with varying prompt sizes and initialization strategies
3. Evaluate self-ensemble impact by comparing single-model vs. ensemble performance

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Computational overhead of using large pre-trained language models is not fully characterized
- Limited investigation of dataset-specific biases affecting performance
- Potential adaptability limitations to domain-specific tasks without fine-tuning

## Confidence

**Performance Claims**: High confidence - well-supported by extensive experimental results across multiple datasets

**Simplicity Claims**: Medium confidence - while architecturally simpler, computational costs of PLMs are not fully addressed

**Generalizability Claims**: Low confidence - limited testing beyond standard benchmark datasets

## Next Checks
1. Conduct comprehensive resource efficiency analysis comparing computational costs (GPU memory, training time, inference latency) against traditional and recent SOTA methods

2. Perform detailed ablation studies to quantify individual contributions of each component (direct addition, prompts, self-ensemble, distillation) to final performance

3. Evaluate method's performance across diverse domain shifts and real-world variations in data quality and distribution