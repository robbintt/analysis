---
ver: rpa2
title: 'Adaptive Sampling Policies Imply Biased Beliefs: A Generalization of the Hot
  Stove Effect'
arxiv_id: '2404.02591'
source_url: https://arxiv.org/abs/2404.02591
tags:
- period
- average
- belief
- after
- learner
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The Hot Stove Effect is a negativity bias resulting from the adaptive
  character of learning. The mechanism is that learning algorithms that pursue alternatives
  with positive estimated values, but avoid alternatives with negative estimated values,
  will correct errors of overestimation but fail to correct errors of underestimation.
---

# Adaptive Sampling Policies Imply Biased Beliefs: A Generalization of the Hot Stove Effect

## Quick Facts
- arXiv ID: 2404.02591
- Source URL: https://arxiv.org/abs/2404.02591
- Authors: Jerker Denrell
- Reference count: 2
- One-line primary result: Negativity bias persists in adaptive sampling when sample size depends on initial observed average, generalizing the Hot Stove Effect.

## Executive Summary
This paper generalizes the Hot Stove Effect by showing that negativity bias arises not only when negative estimates lead to avoidance, but also when they lead to smaller sample sizes. The key insight is that adaptive sampling policies create a systematic underestimation of alternatives believed to be inferior. The paper demonstrates both theoretically and formally that this bias affects not only frequentist estimators but also Bayesian learners, where most individuals will underestimate the expected value of what they are learning about.

## Method Summary
The paper employs theoretical analysis using probability theory and Bayesian updating to examine how adaptive sampling policies create negativity bias. The core approach involves deriving expected values and probabilities under different sampling regimes where the number of samples in a second period depends on the initial sample average. The analysis covers both frequentist estimators (sample averages) and Bayesian estimators (posterior means), proving conditions under which negative bias occurs.

## Key Results
- Negativity bias remains when negative estimates lead to smaller sample sizes rather than complete avoidance
- Sample averages in adaptive sampling settings are biased downward when true mean is zero
- Most Bayesian learners underestimate the expected value when sample size increases with initial belief

## Why This Works (Mechanism)

### Mechanism 1
Adaptive sampling creates bias when sample size is a function of the initial observed average. When the initial average is low, fewer additional samples are taken, making negative initial averages more persistent in the final estimate. The first component (initial sum) is weighted more heavily when the initial average is low because fewer additional samples are taken.

Core assumption: Sample size in the second period is a strictly increasing function of the first period's average.

### Mechanism 2
Bayesian learners systematically underestimate the expected value when sample size is an increasing function of the initial belief. Positive initial beliefs lead to larger sample sizes, making those estimates more precise but also more likely to regress toward the prior mean, while negative beliefs are based on fewer samples and remain more extreme.

Core assumption: The prior distribution is symmetric around zero and sample size is strictly increasing in the initial belief.

### Mechanism 3
The bias is eliminated in the long run as the number of samples increases. As total sample size grows, the weight of the initial average diminishes, and the sample average converges to the true expected value.

Core assumption: The law of large numbers applies and the distribution has finite variance.

## Foundational Learning

- **Concept**: Conditional expectation and martingales
  - Why needed here: The proof that E[b2] = m relies on the property that conditional expectations are martingales
  - Quick check question: If E[Y|X] is the conditional expectation of Y given X, what is E[E[Y|X]]?

- **Concept**: Bias in statistical estimators
  - Why needed here: Understanding how adaptive sampling affects the bias of sample averages compared to fixed-sample estimators
  - Quick check question: If an estimator is biased, is E[estimator] greater than, less than, or equal to the true parameter value?

- **Concept**: Bayesian updating with conjugate priors
  - Why needed here: The paper analyzes how Bayesian learners update beliefs when sample size depends on initial beliefs
  - Quick check question: In Bayesian updating with normal distributions, how does the posterior mean weight the prior mean versus the sample mean?

## Architecture Onboarding

- **Component map**: Data generation -> Initial sampling -> Adaptive sampling policy -> Aggregation -> Analysis
- **Critical path**: 
  1. Generate initial sample of size k
  2. Compute initial average x̄1
  3. Determine second period sample size n(x̄1)
  4. Generate additional n(x̄1) observations
  5. Compute final aggregate (average or Bayesian belief)
  6. Analyze bias properties
- **Design tradeoffs**:
  - Fixed vs adaptive sampling: Fixed sampling eliminates bias but may be inefficient
  - Sample size function: Steeper functions create stronger bias but may improve efficiency
  - Prior specification: Informative priors can reduce variance but may introduce bias
- **Failure signatures**:
  - E[x̄2] ≈ 0 when true mean is 0 indicates negative bias from adaptive sampling
  - P(b2 < m) > 0.5 indicates Bayesian underestimation
  - Convergence to true mean only in long run suggests short-term bias
- **First 3 experiments**:
  1. Simulate adaptive sampling with normal distribution, k=2, n(x̄1) = 10 if x̄1>0 else 1, verify E[x̄2] < 0
  2. Implement Bayesian updating with normal-normal conjugate, verify E[b2] = m but P(b2 < m) > 0.5
  3. Test fixed sampling policy (n constant) to confirm bias elimination

## Open Questions the Paper Calls Out

### Open Question 1
How does the Hot Stove Effect generalize to multi-armed bandit settings with more than two alternatives? The paper only provides theoretical analysis for two-period settings and does not explore multi-armed bandit scenarios with multiple alternatives.

### Open Question 2
What are the practical implications of the negativity bias for recommendation systems and online review platforms? The paper mentions that the Hot Stove Effect can explain biased averages in online reviews but does not provide empirical evidence or specific recommendations for mitigation.

### Open Question 3
How does the negativity bias interact with other cognitive biases, such as confirmation bias or the availability heuristic? The paper focuses on the Hot Stove Effect in isolation and does not consider its interaction with other well-established cognitive biases.

## Limitations
- Relies on idealized conditions including infinite populations and perfect sampling
- Abstracted from cognitive or computational constraints that typically drive adaptive behavior in practice
- Specific distributional assumptions may not hold in real-world learning scenarios

## Confidence
- **High confidence**: Mathematical derivations and proofs of negativity bias in adaptive sampling
- **Medium confidence**: Extension to Bayesian learners depending on specific prior assumptions
- **Medium confidence**: Claim that most Bayesian learners underestimate the true value (probabilistic claim)

## Next Checks
1. Implement adaptive sampling policy with varying sample size functions (linear, threshold, exponential) to verify negativity bias across different functional forms
2. Test how Bayesian underestimation claim holds under different prior distributions (non-symmetric, non-normal) to assess robustness
3. Examine how quickly the bias diminishes as total sample size increases, and identify practical sample size thresholds where bias becomes negligible in typical use cases