---
ver: rpa2
title: 'YuLan: An Open-source Large Language Model'
arxiv_id: '2406.19853'
source_url: https://arxiv.org/abs/2406.19853
tags:
- data
- training
- chinese
- dataset
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces YuLan, a series of open-source large language
  models with 12 billion parameters, trained from scratch using a three-stage pre-training
  approach and enhanced with instruction-tuning and human alignment. The model leverages
  a diverse corpus of 1.7T tokens in English, Chinese, and multilingual texts, and
  employs curriculum learning to improve performance on complex and long-tail knowledge.
---

# YuLan: An Open-source Large Language Model

## Quick Facts
- arXiv ID: 2406.19853
- Source URL: https://arxiv.org/abs/2406.19853
- Reference count: 40
- 12B parameter model achieving state-of-the-art performance on English and Chinese benchmarks

## Executive Summary
YuLan is a series of open-source large language models trained from scratch using a three-stage pre-training approach enhanced with instruction-tuning and human alignment. The model leverages 1.7T tokens from diverse sources in English, Chinese, and multilingual texts, employing curriculum learning to systematically improve handling of complex and long-tail knowledge. Through this approach, YuLan achieves state-of-the-art performance across various benchmarks including commonsense reasoning, factual knowledge, reading comprehension, mathematical reasoning, and comprehensive exams.

## Method Summary
YuLan employs a three-stage pre-training process: standard pre-training, capability-enhanced pre-training with educational assessments, and long-tail knowledge-aware pre-training. This is followed by curriculum-based instruction tuning using 42M synthesized instructions, and curriculum human alignment learning using a DPO-based reward function. The model uses a Transformer decoder with 40 layers, rotary position embeddings, SwiGLU activation, and RMSNorm normalization, trained with AdamW optimizer and mixed precision (BFloat16) on 96 A800 GPUs.

## Key Results
- Achieves state-of-the-art performance on various English and Chinese benchmarks
- Superior performance on commonsense reasoning, factual knowledge, reading comprehension, and mathematical reasoning
- Win rates exceeding 55% on AlpacaEval alignment benchmark
- Outperforms baseline models on AlignBench alignment evaluation

## Why This Works (Mechanism)

### Mechanism 1
Three-stage curriculum pre-training systematically improves handling of long-tail knowledge by starting with standard pre-training, then enhancing capabilities with educational assessments, and finally focusing on identified long-tail knowledge gaps through iterative data augmentation. Core assumption: Educational assessments provide task-specific formats that general web data lacks. Evidence: [abstract] mentions three-stage pre-training to enhance overall capabilities; [section 3.2.2] discusses incorporating educational assessments into pre-training. Break condition: If educational assessments don't improve MMLU performance, the second stage loses justification.

### Mechanism 2
Curriculum-based instruction tuning enables the model to learn from simple to complex instructions by starting with simple NLP tasks and progressively introducing more complex multi-turn instructions. Core assumption: Learning in easy-to-hard manner prevents overwhelming the model and improves generalization. Evidence: [abstract] mentions curriculum-learning framework; [section 4.1] describes training progression from simple to complex instruction sets. Break condition: If performance on complex instructions doesn't improve after curriculum training, the approach may be ineffective.

### Mechanism 3
Difficulty-based human alignment learning improves alignment with human preferences by using a DPO-based reward function that measures instance difficulty, with an easy-to-hard curriculum including progressively challenging instances. Core assumption: The model can effectively differentiate between positive and negative examples when reward values are high. Evidence: [abstract] mentions human alignment learning; [section 4.2] discusses DPO strategy optimization. Break condition: If reward values don't correlate with alignment quality, difficulty estimation loses effectiveness.

## Foundational Learning

- Concept: Curriculum learning
  - Why needed here: LLMs need structured progression from simple to complex tasks to build foundational understanding before tackling difficult problems
  - Quick check question: Can the model solve basic single-turn instructions before attempting multi-turn complex instructions?

- Concept: Knowledge gap identification
  - Why needed here: Pre-trained models often have blind spots in long-tail knowledge that degrade performance on specific benchmarks
  - Quick check question: Can the model correctly answer questions about entities it previously struggled with after targeted training?

- Concept: Reward-based difficulty estimation
  - Why needed here: Human alignment requires distinguishing subtle differences between positive and negative examples, which improves with progressively harder examples
  - Quick check question: Does the model's reward value increase as it learns to better differentiate between aligned and misaligned responses?

## Architecture Onboarding

- Component map: Pre-training (1.7T tokens) -> Instruction tuning (42M instructions) -> Human alignment (0.2M prompts)
- Critical path: Pre-training establishes foundational knowledge → Instruction tuning adapts to human tasks → Human alignment ensures value alignment
- Design tradeoffs: 12B parameters vs. computational efficiency (96 A800 GPUs); Mixed precision (BFloat16) vs. numerical stability; Three-stage pre-training vs. single-stage training time
- Failure signatures: Training loss plateaus indicate potential data quality issues; Benchmark performance drops suggest knowledge gaps; Reward values stagnating indicate alignment learning saturation
- First 3 experiments: 1) Test pre-training with standard vs. educational assessment data on MMLU performance; 2) Compare simple vs. complex instruction tuning on multi-turn task completion; 3) Validate DPO-based reward correlation with human preference alignment

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of YuLan-Chat vary when trained with different distributions of Chinese data categories (HQ, web, news, law)? Basis: [explicit] Section 6.2 discusses preliminary experiments with 1.3B model investigating impact of Chinese data categories. Why unresolved: Paper mentions mixing data sources enhances performance but lacks specific results for different distributions. Evidence needed: Experiments comparing different Chinese data category combinations on MMLU and C-Eval benchmarks.

### Open Question 2
What is the impact of continual training with newly added data sources on YuLan's performance compared to training with all data sources from the beginning? Basis: [explicit] Section 6.2 discusses continual training effects. Why unresolved: Paper suggests continual training yields better performance but lacks detailed analysis. Evidence needed: Comparative experiments between training with all data sources initially vs. adding new sources continually.

### Open Question 3
How does YuLan-Chat's performance on alignment benchmarks compare to other state-of-the-art LLMs, and what factors contribute to its superior alignment capabilities? Basis: [explicit] Section 5.6 presents alignment benchmark results. Why unresolved: Paper mentions superior alignment due to curriculum learning and human alignment strategies but lacks detailed factor analysis. Evidence needed: Ablation studies evaluating individual alignment training components' impact on benchmark performance.

## Limitations
- Insufficient detail on curriculum learning framework implementation and educational assessment selection criteria
- Lack of comprehensive comparison with other models for Chinese language tasks
- Sparse methodology descriptions preventing full reproducibility of the three-stage pre-training process

## Confidence
- **High confidence**: Standard architectural choices and overall training pipeline structure
- **Medium confidence**: Benchmark performance claims but could use more comprehensive comparisons
- **Low confidence**: Specific curriculum learning implementations effectiveness cannot be fully assessed

## Next Checks
1. Run controlled experiments comparing standard vs. three-stage pre-training on MMLU to isolate educational assessment and long-tail knowledge stage contributions
2. Evaluate DPO-based reward function correlation with human preferences through human preference studies on model outputs before and after alignment training
3. Test cross-lingual transfer by evaluating English benchmark performance after Chinese-focused training and vice versa