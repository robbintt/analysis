---
ver: rpa2
title: 'Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives'
arxiv_id: '2401.02009'
source_url: https://arxiv.org/abs/2401.02009
tags:
- reflection
- arxiv
- reasoning
- prompt
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Self-Contrast addresses the problem of poor reflection performance
  in LLMs by introducing a contrastive strategy. Instead of directly evaluating responses,
  which can be challenging and inconsistent, it explores diverse solving perspectives,
  contrasts differences, and summarizes discrepancies into a checklist for reflection.
---

# Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives

## Quick Facts
- arXiv ID: 2401.02009
- Source URL: https://arxiv.org/abs/2401.02009
- Reference count: 40
- One-line primary result: Self-Contrast improves reflection accuracy by +7.2% on math tasks and +0.95 BLEURT on translation by contrasting diverse solving perspectives

## Executive Summary
Self-Contrast addresses the fundamental problem of poor reflection performance in large language models (LLMs) by introducing a contrastive strategy that explores diverse solving perspectives rather than attempting direct self-evaluation. The method recognizes that LLMs struggle with direct self-assessment due to overconfidence and inconsistency, making it difficult to generate reliable feedback for reflection. Instead of evaluating a single solution directly, Self-Contrast generates multiple diverse perspectives through self-curated prompts, contrasts the differences between solutions, and summarizes these discrepancies into actionable checklists for reflection.

The approach significantly improves reflection accuracy compared to vanilla reflection methods, achieving an average improvement of +7.2% on mathematical reasoning tasks and +0.95 BLEURT score on creative translation tasks. Self-Contrast demonstrates better generality across different LLMs and tasks, showing robust performance on widely used commercial (GPT-3.5, GPT-4) and open-source (Llama2) models. The method effectively addresses the overconfidence and inconsistency issues in LLM self-evaluation by leveraging the relative ease of comparing differences versus assessing absolute correctness.

## Method Summary
Self-Contrast is a reflection strategy that improves LLM performance by contrasting diverse solving perspectives rather than direct self-evaluation. The method generates multiple solutions using self-curated prompts that offer unique perspectives (different thinking styles, identities, or preferences), then contrasts pairs of solutions to identify discrepancies. These differences are abstracted into detailed checklists that guide targeted reflection and revision of the original request and solutions. The approach addresses the fundamental problem that LLMs often exhibit overconfidence or high randomness when self-evaluating, offering stubborn or inconsistent feedback that causes poor reflection.

## Key Results
- Self-Contrast achieves +7.2% average improvement on mathematical reasoning tasks (GSM8K, SV AMP) compared to vanilla reflection
- Translation tasks show +0.95 BLEURT score improvement on CommonMT dataset
- The method demonstrates better generality across different LLMs, with robust performance on GPT-3.5, GPT-4, and Llama2 models
- Self-Contrast effectively reduces overconfident and inconsistent feedback, which accounts for 53.5% and 45.3% of invalid reflections respectively

## Why This Works (Mechanism)

### Mechanism 1
LLMs struggle with direct self-evaluation due to overconfidence and inconsistency, leading to poor reflection. Instead of evaluating a single solution directly, contrasting multiple diverse perspectives helps identify discrepancies that are easier to spot than evaluating correctness. Core assumption: LLMs are better at comparing differences than assessing absolute correctness. Evidence: LLMs exhibit overconfidence (53.5%) or high inconsistency (45.3%) in self-evaluation.

### Mechanism 2
Diverse solving perspectives reduce bias from any single prompt and maximize contrast effectiveness. Self-curated prompts adaptively generate multiple unique perspectives (e.g., literal vs liberal translation), ensuring varied solutions for meaningful comparison. Core assumption: Different prompts lead to meaningfully different solutions, not just superficial variations. Evidence: LLM adaptively designs numerous unique solving perspectives for mathematical reasoning, generating a variety of results.

### Mechanism 3
Summarizing discrepancies into a checklist creates actionable, specific feedback for reflection. After contrasting pairs of solutions, LLMs abstract differences into detailed checking instructions that guide re-examination of the original request and solutions. Core assumption: LLMs can accurately identify and abstract the root causes of discrepancies into useful checklists. Evidence: Self-Contrast brings significant and stable improvement on mathematical reasoning tasks.

## Foundational Learning

- Concept: Contrastive reasoning
  - Why needed here: Self-Contrast relies on comparing multiple solutions to identify discrepancies rather than direct evaluation.
  - Quick check question: Can you explain why contrasting two different incorrect solutions might be more effective than evaluating one incorrect solution directly?

- Concept: Prompt engineering and perspective generation
  - Why needed here: Self-Contrast uses self-curated prompts to generate diverse solving perspectives adaptively.
  - Quick check question: What are the key elements that should be varied when designing prompts to generate different solving perspectives?

- Concept: Checklist-based reflection
  - Why needed here: The method converts identified discrepancies into actionable checklists for targeted revision.
  - Quick check question: How does a checklist-based approach differ from general "please check your work" instructions in terms of specificity and effectiveness?

## Architecture Onboarding

- Component map: User request → Self-curated prompt generation → Multiple solution generation → Clustering + selection → Pairwise contrast → Checklist generation → Reflection & revision → Final consistent solution
- Critical path: The sequence from self-curated prompt generation through to checklist-based reflection is the core workflow that drives improvements.
- Design tradeoffs: Self-curated prompts offer flexibility but require stronger instruction-following; direct sampling is simpler but produces less diverse solutions.
- Failure signatures: If the final solutions are not consistent, check whether: (1) prompts generated too similar perspectives, (2) checklist instructions were too vague, or (3) the LLM couldn't effectively use the checklist.
- First 3 experiments:
  1. Test self-curated prompt generation on a simple translation task to verify diverse perspectives are created.
  2. Compare performance of self-contrast vs. direct self-evaluation on 10 GSM8K problems.
  3. Evaluate the impact of checklist generation by running self-contrast with and without the checklist phase.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of perspectives generated by Self-Curated Prompting affect the performance of Self-Contrast?
- Basis in paper: [explicit] The paper mentions that LLM generates at least two different prompts and a maximum of nine prompts for each request, and the distribution of the perspectives generated is analyzed.
- Why unresolved: The paper does not provide a detailed analysis of how varying the number of perspectives impacts the effectiveness of the contrastive strategy.
- What evidence would resolve it: Empirical studies varying the number of perspectives and measuring the performance of Self-Contrast.

### Open Question 2
- Question: Can Self-Contrast be effectively combined with external feedback to further improve reflection accuracy?
- Basis in paper: [inferred] The paper focuses on self-correction without external feedback, but mentions that external feedback is not always available in most scenarios.
- Why unresolved: The paper does not explore the potential benefits of integrating external feedback with the Self-Contrast strategy.
- What evidence would resolve it: Experiments comparing the performance of Self-Contrast with and without external feedback.

### Open Question 3
- Question: How does the quality of the initial responses affect the performance of Self-Contrast?
- Basis in paper: [explicit] The paper mentions that the quality of the initial responses can impact the reflection results, and that LLMs tend to trust previous responses over diligently examining and correcting errors.
- Why unresolved: The paper does not provide a detailed analysis of how the quality of the initial responses affects the performance of Self-Contrast.
- What evidence would resolve it: Experiments varying the quality of the initial responses and measuring the performance of Self-Contrast.

## Limitations

- Heavy dependence on LLM's ability to generate sufficiently diverse and high-quality perspectives through self-curated prompts
- Computational cost of generating multiple perspectives versus potential gains is not thoroughly quantified
- Generality claims across different LLMs based on testing only three models (GPT-3.5, GPT-4, Llama2)

## Confidence

**High Confidence**: The core mechanism that contrasting differences is easier than direct evaluation is well-supported by empirical results showing +7.2% improvement on math tasks and +0.95 BLEURT on translation.

**Medium Confidence**: The claim that self-curated prompts adaptively generate unique perspectives appears supported by mathematical reasoning results, but evidence is less clear for translation tasks.

**Low Confidence**: Generality claims across different LLMs are based on testing only three models, leaving uncertainty about robustness on other model architectures and problem types.

## Next Checks

1. **Model Capability Threshold Test**: Systematically evaluate Self-Contrast performance across a gradient of model sizes (7B, 13B, 34B, 70B parameters) to determine the minimum capability threshold required for effective self-curated prompt generation.

2. **Perspective Diversity Analysis**: Measure the actual diversity of solutions generated by different prompts using embedding similarity metrics to quantify whether the generated perspectives are meaningfully different versus superficially distinct.

3. **Cross-Domain Generalization**: Apply Self-Contrast to three additional problem domains (commonsense reasoning, code generation, scientific question answering) to test whether the contrastive strategy generalizes beyond mathematical and translation tasks.