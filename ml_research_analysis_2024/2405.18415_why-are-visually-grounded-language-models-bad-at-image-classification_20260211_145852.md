---
ver: rpa2
title: Why are Visually-Grounded Language Models Bad at Image Classification?
arxiv_id: '2405.18415'
source_url: https://arxiv.org/abs/2405.18415
tags:
- classification
- vlms
- data
- image
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Visually-grounded language models (VLMs) underperform CLIP in image
  classification due to insufficient classification data during training. The paper
  shows that critical classification information is encoded in VLMs' latent space
  but cannot be effectively decoded without enough training data.
---

# Why are Visually-Grounded Language Models Bad at Image Classification?

## Quick Facts
- arXiv ID: 2405.18415
- Source URL: https://arxiv.org/abs/2405.18415
- Reference count: 40
- Primary result: VLMs underperform CLIP in image classification due to insufficient classification data during training

## Executive Summary
Visually-grounded language models (VLMs) are pretrained on large amounts of image-text pairs, enabling them to perform well on tasks requiring advanced reasoning. However, these models struggle with image classification compared to models like CLIP that are explicitly trained for this task. This paper investigates why VLMs underperform in classification tasks and demonstrates that the issue stems from insufficient classification-focused data during training, rather than fundamental architectural limitations.

The researchers show that critical classification information exists in VLMs' latent space but cannot be effectively decoded without adequate training data. By integrating classification-focused datasets into VLM training, they achieve performance parity with state-of-the-art classification models. Moreover, this enhancement improves general VLM performance, yielding an 11.8% improvement on ImageWikiQA, a benchmark testing both classification and advanced reasoning abilities.

## Method Summary
The researchers conducted experiments by training VLMs with varying amounts of classification-focused data while keeping other pretraining conditions constant. They compared the performance of these models against CLIP and standard VLMs on image classification benchmarks. Additionally, they evaluated the models on ImageWikiQA to assess whether improvements in classification capabilities transferred to more complex reasoning tasks. The team also analyzed the latent representations of VLMs to identify whether classification-relevant information was present but inaccessible.

## Key Results
- VLMs encode classification-relevant information in their latent space but cannot decode it effectively without sufficient training data
- Adding classification-focused datasets to VLM training enables performance matching with state-of-the-art classification models
- Improved classification capabilities in VLMs lead to an 11.8% improvement on ImageWikiQA, demonstrating transfer to advanced reasoning tasks

## Why This Works (Mechanism)
The mechanism behind VLMs' classification underperformance and subsequent improvement lies in the relationship between pretraining objectives and the information encoded in latent representations. VLMs are trained primarily to align visual and textual representations for retrieval and reasoning tasks, not for explicit classification. During this process, the models do learn visual features relevant to classification, but these features are not organized or emphasized in a way that makes them easily accessible for classification tasks. The classification information exists in a latent form that requires specific training signals to be properly decoded and utilized.

## Foundational Learning
- **Latent space representations**: Understanding how VLMs encode visual information in their internal representations is crucial for diagnosing classification performance issues. Quick check: Visualize t-SNE or UMAP projections of VLM embeddings to see if classification-relevant clusters exist.
- **Pretraining objectives and their effects**: Different pretraining objectives (contrastive learning vs. generative modeling) shape what information models prioritize encoding. Quick check: Compare activation patterns when models process classification vs. reasoning tasks.
- **Transfer learning dynamics**: How improvements in one capability (classification) transfer to others (reasoning) reveals the interconnected nature of learned representations. Quick check: Track performance changes across multiple task types when modifying training data composition.

## Architecture Onboarding

**Component map**: Image Encoder -> Text Encoder -> Fusion Module -> Multimodal Representation -> Classification Head (optional)

**Critical path**: Image Input → Visual Feature Extraction → Cross-modal Alignment → Semantic Integration → Task-specific Decoding

**Design tradeoffs**: VLMs prioritize cross-modal alignment and reasoning capabilities over task-specific optimizations like classification. This generalist approach enables versatility but sacrifices peak performance on specialized tasks.

**Failure signatures**: VLMs produce plausible but incorrect classifications, often choosing categories that are semantically related but not visually accurate. The models may correctly identify high-level concepts but fail at fine-grained distinctions.

**First experiments**: 
1. Probe VLMs' latent representations with linear classifiers to test if classification information is present but inaccessible
2. Fine-tune VLMs on classification datasets of varying sizes to measure performance scaling
3. Compare VLMs' attention patterns on classification vs. reasoning tasks to identify architectural differences in information utilization

## Open Questions the Paper Calls Out
None

## Limitations
- Uncertain whether VLMs can match the precision of specialized classification models across all tasks and domains
- Unclear mechanism of how classification improvements transfer to general task performance
- Other factors beyond training data (architectural limitations, pretraining objectives) may contribute to classification underperformance

## Confidence

| Claim | Confidence |
|-------|------------|
| VLMs encode classification-relevant information in latent space | High |
| Adding classification-focused training data improves performance | Medium |
| Insufficient training data is primary reason for underperformance | Low |

## Next Checks
1. Conduct ablation studies varying classification-focused training data amounts to identify minimum thresholds and potential saturation points
2. Perform cross-domain evaluations on diverse image classification benchmarks not seen during training to test generalization
3. Investigate potential trade-offs by evaluating VLMs on comprehensive task suites before and after classification-focused training to measure changes across different capabilities