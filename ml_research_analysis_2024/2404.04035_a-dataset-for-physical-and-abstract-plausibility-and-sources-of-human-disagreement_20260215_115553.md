---
ver: rpa2
title: A Dataset for Physical and Abstract Plausibility and Sources of Human Disagreement
arxiv_id: '2404.04035'
source_url: https://arxiv.org/abs/2404.04035
tags:
- triples
- plausible
- ratings
- implausible
- plausibility
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a dataset for assessing the plausibility
  of events in English, considering both physical and abstract levels. The dataset
  is constructed by extracting event triples from Wikipedia and assigning abstractness
  ratings to the participants.
---

# A Dataset for Physical and Abstract Plausibility and Sources of Human Disagreement

## Quick Facts
- **arXiv ID**: 2404.04035
- **Source URL**: https://arxiv.org/abs/2404.04035
- **Reference count**: 23
- **Primary result**: Dataset of 1,733 event triples with abstractness ratings showing annotators favor plausible over implausible events and exhibit stronger disagreements for implausible ones

## Executive Summary
This paper introduces a novel dataset for assessing event plausibility that explicitly distinguishes between physical and abstract plausibility levels. The dataset is constructed by extracting subject-verb-object triples from Wikipedia and assigning abstractness ratings to each constituent, creating a systematic exploration of how abstractness affects plausibility judgments. The authors generate perturbed pseudo-implausible events and use crowd-sourcing to annotate a subset, revealing that implausible events trigger higher annotator disagreement and that concrete participants are more likely to be judged as implausible.

## Method Summary
The dataset construction process involves extracting event triples from Wikipedia, assigning abstractness scores to nouns and verbs using concreteness ratings from Brysbaert et al. (2014), partitioning triples into 27 abstractness combinations across five bins, and generating perturbed pseudo-implausible events. The authors then collect crowd-sourced plausibility annotations using Amazon Mechanical Turk, applying extensive quality controls including check instances, filtering based on disagreement rates, and pairwise Jaccard coefficients to ensure annotation quality.

## Key Results
- Annotators consistently favor plausible events over implausible ones in their ratings
- Implausible events show significantly higher annotator disagreement compared to plausible events
- Event abstractness impacts plausibility ratings, with concrete participants more likely to trigger implausibility judgments
- The dataset contains 1,733 event triples with an average of 8.9 ratings per triple

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Integrating abstractness ratings into event triples systematically increases the discriminative power of plausibility annotation by creating a more balanced contrast between plausible and implausible instances.
- **Mechanism**: By assigning abstractness ratings to each constituent and partitioning into 27 abstractness combinations, the dataset construction ensures that implausible events are not trivially generated. This systematic variation allows the dataset to capture both physical and abstract plausibility.
- **Core assumption**: Abstractness is a continuous property that meaningfully interacts with plausibility judgments.
- **Evidence anchors**: [abstract] "We propose to systematically examine plausibility across levels of abstractness" and [section] "To discern triples containing highly concrete words from triples which encompass more abstract words, we assign abstractness scores to all nouns and verbs in a triple."
- **Break condition**: If abstractness ratings do not correlate with human plausibility judgments or if the 27 combinations fail to capture meaningful variation.

### Mechanism 2
- **Claim**: Crowd disagreement in plausibility annotation reflects inherent uncertainty in the task rather than annotation error, making it a valuable signal for modeling.
- **Mechanism**: The study explicitly preserves and examines disagreement by collecting multiple ratings per triple and providing various aggregation methods. The finding that implausible events show higher disagreement supports the view that some events are genuinely ambiguous.
- **Core assumption**: Human intuition regarding plausibility is inherently multi-faceted and highly individual.
- **Evidence anchors**: [abstract] "Human intuition regarding the assessment of plausibility is, however, incredibly multi-faceted, highly individual" and [section] "We argue for the necessity to preserve and examine disagreement when annotating and modelling plausibility."
- **Break condition**: If disagreement patterns can be fully explained by poor task design or insufficient annotator training.

### Mechanism 3
- **Claim**: Concrete event participants trigger a stronger perception of implausibility compared to abstract participants, due to more stable mental imagery grounded in the real world.
- **Mechanism**: The analysis shows that triples with concrete subjects and objects are more likely to receive implausible ratings, while abstract constituents correlate with plausible ratings. This suggests that concrete concepts evoke more rigid mental models, making violations more noticeable.
- **Core assumption**: Concrete words evoke more stable mental images grounded in the real world, while abstract words allow for more interpretive flexibility.
- **Evidence anchors**: [abstract] "Our results reveal a positive relation between plausibility and events consisting of more abstract words" and [section] "We observe more abstract verbs, while shares of concrete and mid-range verbs decrease."
- **Break condition**: If the observed pattern is specific to the dataset construction method rather than a general cognitive phenomenon.

## Foundational Learning

- **Concept**: Abstractness as a continuous scale rather than binary property
  - Why needed here: The dataset construction explicitly partitions event triples into 5 bins of abstractness, requiring understanding that abstractness is not just "abstract" vs. "concrete."
  - Quick check question: If a word has a concreteness rating of 3.5 on a 1-7 scale (where 1 is most concrete), what abstractness bin would it fall into?
- **Concept**: Selectional preference vs. plausibility distinction
  - Why needed here: The paper distinguishes plausibility from selectional preference by noting that plausibility includes "unusual but nevertheless plausible events."
  - Quick check question: Is "cat-eat-sardine" more plausible or more selectionally preferred? Explain the difference.
- **Concept**: Crowdsourcing quality control methods
  - Why needed here: The dataset uses multiple post-processing steps including check instances and filtering based on disagreement rates to ensure annotation quality.
  - Quick check question: Why would an annotator who disagrees with the original "plausible" label in more than 75% of their submissions be excluded from the final dataset?

## Architecture Onboarding

- **Component map**: Wikipedia extraction → abstractness assignment → binning → sampling → annotation collection (AMT interface) → quality checks → post-processing → analysis
- **Critical path**: The most critical path is from abstractness assignment through to the final plausibility ratings, as this determines the dataset's core value proposition of capturing both physical and abstract plausibility.
- **Design tradeoffs**: Using Wikipedia as source limits diversity but ensures plausibility of original triples; automatic perturbation for implausible events is efficient but may not capture all forms of implausibility; strict quality controls reduce noise but may exclude some valid annotator perspectives.
- **Failure signatures**: Low inter-annotator agreement across all triples suggests task ambiguity or poor instructions; implausible triples consistently receiving high plausibility ratings indicates generation method issues; no variation in abstractness distributions suggests binning parameters need adjustment.
- **First 3 experiments**:
  1. Analyze the distribution of abstractness ratings for a random sample of 100 triples to verify the binning process is working as intended
  2. Compute inter-annotator agreement for the top 50 most concrete vs. top 50 most abstract triples to test the hypothesis about concrete vs. abstract disagreement patterns
  3. Train a simple classifier using only abstractness features to predict plausibility ratings and measure baseline performance before adding other features

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the perceived plausibility of abstract concepts change when they are presented in different syntactic roles (subject, verb, object) within event triples?
- **Basis in paper**: [explicit] The paper examines the impact of abstractness on plausibility ratings but does not specifically address how syntactic roles affect this relationship.
- **Why unresolved**: The study focuses on the abstractness of event constituents but does not delve into the nuanced effects of their syntactic roles on plausibility perception.
- **What evidence would resolve it**: Experimental data showing how plausibility ratings vary for the same abstract concepts when used in different syntactic roles within event triples.

### Open Question 2
- **Question**: To what extent does the cultural background of annotators influence their assessment of plausibility in event triples?
- **Basis in paper**: [inferred] The paper acknowledges the potential for bias in datasets derived from Wikipedia and notes that plausibility assessments can be highly individual.
- **Why unresolved**: The study uses annotators from the United States and the United Kingdom, but does not investigate how cultural differences might affect plausibility judgments.
- **What evidence would resolve it**: Comparative analysis of plausibility ratings from annotators with diverse cultural backgrounds, highlighting any significant differences in their assessments.

### Open Question 3
- **Question**: How do plausibility ratings for event triples change when contextual information beyond simple subject-verb-object structures is provided?
- **Basis in paper**: [explicit] The discussion suggests that events based on simple s-v-o triples might benefit from additional context to resolve ambiguity and potentially reduce disagreement among annotators.
- **Why unresolved**: The study focuses on plausibility ratings for simple event triples without exploring the impact of additional contextual information.
- **What evidence would resolve it**: Comparative study of plausibility ratings for event triples presented with and without additional contextual information, measuring changes in agreement and plausibility assessments.

## Limitations

- The dataset's reliance on Wikipedia-derived triples may overrepresent formal or encyclopedic events while underrepresenting colloquial or domain-specific event structures.
- The perturbation method for generating implausible events, while systematic, may not capture all types of implausibility, particularly those arising from complex causal relationships or cultural context.
- The abstractness ratings derived from Brysbaert et al.'s concreteness norms may not perfectly align with abstractness judgments for event constituents.

## Confidence

**High Confidence**: The finding that annotators exhibit higher disagreement on implausible events is well-supported by statistical evidence (Fig. 4) and consistent with the paper's theoretical framework about inherent task ambiguity.

**Medium Confidence**: The claim that concrete event participants trigger stronger perceptions of implausibility is supported by observed correlations but lacks experimental validation of causation.

**Low Confidence**: The assertion that the dataset captures both physical and abstract plausibility at meaningful levels is theoretically sound but would require extensive downstream modeling studies to validate empirically.

## Next Checks

1. **Abstractness Rating Validation**: Conduct a small-scale human validation study where annotators rate the abstractness of 100 randomly selected event constituents from the dataset, then compute correlation with the Brysbaert-derived ratings to quantify alignment.

2. **Perturbation Method Analysis**: Systematically sample 50 implausible events and analyze whether human judges agree they are implausible for reasons captured by the perturbation method, or whether alternative forms of implausibility are missing.

3. **Downstream Modeling Test**: Train a simple transformer-based plausibility classifier on the dataset and evaluate its performance on a held-out set of Wikipedia-derived triples not used in training, measuring whether abstractness features provide statistically significant improvement over baseline models.