---
ver: rpa2
title: One-shot Active Learning Based on Lewis Weight Sampling for Multiple Deep Models
arxiv_id: '2405.14121'
source_url: https://arxiv.org/abs/2405.14121
tags:
- instances
- learning
- lewis
- sampling
- weights
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a one-shot active learning method for training
  multiple deep models concurrently. The key idea is to sample and reweight unlabeled
  instances based on their maximum Lewis weights across different network representations.
---

# One-shot Active Learning Based on Lewis Weight Sampling for Multiple Deep Models

## Quick Facts
- **arXiv ID**: 2405.14121
- **Source URL**: https://arxiv.org/abs/2405.14121
- **Reference count**: 40
- **Primary result**: One-shot active learning method for training multiple deep models concurrently using Lewis weight sampling

## Executive Summary
This paper introduces a novel one-shot active learning approach for training multiple deep models simultaneously. The method leverages Lewis weight sampling to identify and reweight informative unlabeled instances based on their maximum Lewis weights across different network representations. Unlike traditional iterative active learning methods that require multiple rounds of model training and querying, this approach enables efficient instance selection in a single pass. The proposed method demonstrates competitive performance with state-of-the-art iterative approaches while significantly reducing computational overhead, making it particularly valuable for scenarios where computational resources are limited.

## Method Summary
The core innovation lies in using Lewis weight sampling to perform one-shot active learning across multiple deep models. The method computes Lewis weights for each unlabeled instance across all model representations, then selects instances with the highest maximum Lewis weights for labeling. This sampling strategy effectively captures the informativeness of instances across the ensemble of models without requiring repeated retraining. The theoretical framework establishes that this approach achieves constant-factor approximations for multiple models, with sample complexity bounded by the sum of maximum Lewis weights. The implementation involves parallel computation of Lewis weights for each model, aggregation to find maximum weights per instance, and selection of the top-k instances for labeling in a single query step.

## Key Results
- Achieves competitive performance with iterative active learning methods on 11 datasets using 50 deep models
- Demonstrates significant computational efficiency gains by eliminating repeated model training cycles
- Shows that sum of maximum Lewis weights grows slowly as model count increases, indicating strong correlation among deep representations

## Why This Works (Mechanism)
The method exploits the geometric properties of Lewis weights to identify instances that are most informative across multiple model representations simultaneously. By sampling based on maximum Lewis weights rather than averaging or other aggregation methods, the approach captures instances that are particularly challenging or informative for at least one model in the ensemble. This one-shot selection mechanism avoids the computational burden of iterative methods while maintaining theoretical guarantees on approximation quality. The slow growth of maximum Lewis weight sums suggests that deep models tend to agree on the relative informativeness of instances, allowing for efficient joint sampling.

## Foundational Learning
- **Lewis weights**: Statistical leverage scores that measure the importance of data points in linear regression; needed for quantifying instance informativeness across models; quick check: verify that computed weights satisfy Lewis weight properties
- **Active learning sampling theory**: Framework for selecting informative instances to label; needed to establish theoretical guarantees; quick check: confirm approximation bounds hold under proposed sampling distribution
- **Multiple model correlation**: Analysis of how different deep representations agree/disagree on instance importance; needed to justify one-shot approach; quick check: measure correlation coefficients between model-specific Lewis weight rankings
- **One-shot vs iterative tradeoffs**: Understanding computational vs performance tradeoffs; needed to position contribution relative to existing methods; quick check: compare query efficiency and final accuracy metrics
- **Deep representation spaces**: Properties of learned feature spaces that affect sampling strategies; needed to understand why Lewis weights work well; quick check: analyze distribution of Lewis weights across different layers

## Architecture Onboarding

**Component Map**: Data -> Lewis Weight Computation (per model) -> Max Weight Aggregation -> Instance Selection -> Labeled Dataset

**Critical Path**: The most time-critical components are parallel Lewis weight computation across all models and the max aggregation step. These must be optimized for large-scale deployment, as they directly impact the one-shot efficiency advantage.

**Design Tradeoffs**: The method trades some potential accuracy gains from iterative refinement for substantial computational savings. The choice of Lewis weight sampling versus other importance measures (entropy, margin, etc.) represents a fundamental design decision that affects both theoretical guarantees and empirical performance.

**Failure Signatures**: Performance degradation may occur when model representations are highly uncorrelated (sum of max weights grows quickly), when Lewis weights fail to capture true instance informativeness for specific tasks, or when the assumption of similar data distributions across models is violated.

**First Experiments**:
1. Verify Lewis weight computation correctness on synthetic data with known leverage scores
2. Compare one-shot selection performance against random sampling baseline
3. Test sensitivity to the number of selected instances (k) on a validation set

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Theoretical guarantees rely on assumptions about Lewis weight distributions that may not hold uniformly across all deep learning architectures
- Empirical validation focuses primarily on image classification tasks with specific deep architectures, limiting generalizability
- The assumption that sum of maximum Lewis weights grows slowly with model count needs more rigorous verification across diverse model families

## Confidence

**Major claim clusters confidence:**
- Theoretical approximation guarantees: Medium - depends heavily on idealized assumptions about weight distributions
- Computational efficiency improvements: High - well-supported by experimental comparisons
- Performance competitiveness with iterative methods: Medium - strong results but limited to specific task types
- Correlation among deep representations: Medium - empirical observation needs more theoretical grounding

## Next Checks
1. Test the method on non-image datasets (text, audio, tabular) with different model architectures to verify cross-domain effectiveness
2. Conduct ablation studies to isolate the impact of Lewis weight sampling versus other components of the active learning pipeline
3. Perform long-term stability analysis to assess whether the observed slow growth of maximum Lewis weight sums persists as model diversity increases