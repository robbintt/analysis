---
ver: rpa2
title: 'SAP: Corrective Machine Unlearning with Scaled Activation Projection for Label
  Noise Robustness'
arxiv_id: '2403.08618'
source_url: https://arxiv.org/abs/2403.08618
tags:
- samples
- label
- training
- data
- noise
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Verifix, a novel SVD-based algorithm for
  post-training correction of label noise in deep learning models. The key idea is
  to leverage a small subset of trusted samples, identified using curvature scores,
  to estimate a clean activation space via SVD.
---

# SAP: Corrective Machine Unlearning with Scaled Activation Projection for Label Noise Robustness

## Quick Facts
- arXiv ID: 2403.08618
- Source URL: https://arxiv.org/abs/2403.08618
- Authors: Sangamesh Kodge; Deepak Ravikumar; Gobinda Saha; Kaushik Roy
- Reference count: 40
- Key outcome: Verifix achieves up to 11.19% accuracy gains on CIFAR-10 and 7.78% on CIFAR-100, outperforming baselines like SAM while requiring significantly less computational resources

## Executive Summary
This paper introduces Verifix, a novel SVD-based algorithm for post-training correction of label noise in deep learning models. The key innovation is leveraging a small subset of trusted samples, identified using curvature scores, to estimate a clean activation space via SVD. The model weights are then projected onto this space to suppress noisy activations. Experiments demonstrate significant improvements in generalization on both synthetic and real-world noisy datasets.

## Method Summary
Verifix operates as a post-training correction method that identifies trusted samples using curvature scores, collects their activations, performs SVD decomposition to obtain clean activation basis vectors, scales these vectors using importance scoring, and projects the model weights onto this cleaned activation space. The method requires only a small fraction (2.5%) of trusted data and operates as a one-time correction step after standard training.

## Key Results
- Achieves up to 11.19% accuracy gains on CIFAR-10 with 25% synthetic label noise
- Demonstrates 7.78% improvement on CIFAR-100 under similar noise conditions
- Shows average 1.13% improvement on real-world noisy datasets (WebVision1.0, Clothing1M)
- Outperforms baseline methods like SAM while requiring significantly less computational resources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Low-curvature samples correspond to correctly labeled training data
- Mechanism: Curvature measures how sensitive the model's loss is to small input perturbations. Samples with low curvature are less sensitive, indicating they are correctly labeled and not memorized by the model
- Core assumption: The curvature score is a reliable proxy for label correctness and not simply a measure of sample difficulty
- Evidence anchors: The paper proposes automatic trusted data selection using curvature scores, citing that low curvature input samples are a good way to obtain trusted data as shown in prior work

### Mechanism 2
- Claim: SVD decomposition of trusted activation space provides a clean basis for projection
- Mechanism: SVD decomposes the trusted activation matrix into orthogonal basis vectors (U) and singular values (Σ). These basis vectors represent the activation patterns of clean data. Projecting noisy activations onto this space suppresses noise-induced activations
- Core assumption: The activation space of trusted (clean) samples is sufficiently orthogonal to the activation space of noisy samples
- Evidence anchors: The paper shows that Verifix performs SVD on trusted dataset to form basis directions of activation patterns associated with clean data

### Mechanism 3
- Claim: Importance scaling of SVD basis vectors controls the strength of noise suppression
- Mechanism: Each basis vector is scaled by a factor proportional to the explained variance (singular value) and a hyperparameter α. Higher α increases the scaling, allowing more aggressive noise suppression
- Core assumption: The relative importance of basis vectors correlates with their ability to represent clean activations
- Evidence anchors: The parameter α serves as a hyperparameter controlling the scaling of basis vectors, with higher values approaching stronger noise suppression

## Foundational Learning

- Concept: Singular Value Decomposition (SVD)
  - Why needed here: SVD is used to decompose the trusted activation matrix into orthogonal basis vectors, which are then used to project and clean noisy activations
  - Quick check question: What do the left singular vectors (U) represent in the SVD decomposition A = UΣV^T?

- Concept: Curvature score
  - Why needed here: Curvature scores are used to identify low-curvature (likely correctly labeled) samples from the noisy training data
  - Quick check question: How does the curvature score measure the sensitivity of the model's loss to input perturbations?

- Concept: Activation space alignment
  - Why needed here: Aligning the activation space of the noisy model with the clean activation space obtained from trusted samples helps suppress noise-induced activations
  - Quick check question: How does projecting noisy activations onto the clean activation basis space help remove noise?

## Architecture Onboarding

- Component map: Trusted Data Estimation -> Representation Sampling -> SVD Decomposition -> Importance Scaling -> Weight Update
- Critical path: Trusted Data Estimation → Representation Sampling → SVD Decomposition → Importance Scaling → Weight Update
- Design tradeoffs:
  - Computational cost vs. accuracy: Using more trusted samples improves accuracy but increases computation
  - Noise suppression strength vs. over-smoothing: Higher α leads to more aggressive noise suppression but may also smooth out clean activations
  - Trusted data selection: Curvature-based selection vs. random sampling or expert verification
- Failure signatures:
  - Insufficient noise suppression: Model performance does not improve after applying Verifix
  - Over-smoothing: Model performance degrades due to excessive smoothing of clean activations
  - Computational issues: SVD decomposition or weight updates fail due to numerical instability or memory constraints
- First 3 experiments:
  1. Synthetic noise experiment: Apply Verifix to a model trained on CIFAR-10 with 25% synthetic label noise and evaluate the improvement in test accuracy
  2. Real-world noise experiment: Apply Verifix to a model trained on Clothing1M and evaluate the improvement in test accuracy
  3. Ablation study: Compare the performance of Verifix with different trusted data selection strategies (curvature-based, random, expert-verified) and scaling hyperparameters (α)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of sampling strategy (e.g., curvature-based vs. random) impact the computational efficiency of Verifix?
- Basis in paper: The paper discusses the effect of different sampling strategies on generalization performance but does not delve into their impact on computational efficiency
- Why unresolved: While the paper shows that curvature-based sampling improves generalization, it does not quantify the trade-off between improved performance and increased computational cost
- What evidence would resolve it: A detailed analysis comparing the computational cost of different sampling strategies, including their impact on runtime and memory usage

### Open Question 2
- Question: Can Verifix be effectively applied to datasets with label noise levels exceeding 25%?
- Basis in paper: The paper primarily focuses on datasets with up to 25% label corruption. It is unclear whether the method's effectiveness diminishes or plateaus beyond this level
- Why unresolved: The paper does not provide empirical results or theoretical analysis for higher noise levels, leaving the method's scalability in question
- What evidence would resolve it: Experiments testing Verifix on datasets with varying levels of label corruption (e.g., 30%, 40%, 50%) and comparing its performance to other methods

### Open Question 3
- Question: How does the choice of network architecture affect the performance of Verifix?
- Basis in paper: The paper evaluates Verifix on ResNet18 and VGG11 architectures but does not explore its performance on other architectures (e.g., Vision Transformers, MobileNets)
- Why unresolved: The effectiveness of Verifix may vary depending on the network's depth, width, and architecture-specific properties, which are not addressed in the paper
- What evidence would resolve it: Experiments applying Verifix to a diverse range of network architectures and analyzing its performance relative to the architecture's complexity and design

## Limitations

- The curvature score-based trusted data selection lacks strong empirical validation in the literature, with the cited works not found in corpus search
- The method's effectiveness for label noise levels exceeding 25% is not established
- The reliance on a small trusted subset (2.5% of training data) may limit applicability when clean data is scarce

## Confidence

- Mechanism 1 (Curvature-based trusted data selection): Low confidence
- Mechanism 2 (SVD-based activation space cleaning): Medium confidence
- Mechanism 3 (Importance scaling): Medium confidence
- Overall performance claims: Medium confidence

## Next Checks

1. **Curvature score validation**: Conduct an ablation study comparing curvature-based trusted data selection with random sampling and expert-verified clean samples. Measure the correlation between curvature scores and label correctness across different datasets and noise levels.

2. **SVD decomposition analysis**: Visualize and analyze the learned basis vectors from SVD decomposition. Examine how well the basis vectors capture the activation patterns of clean vs. noisy samples and how the projection affects the overall activation space geometry.

3. **Scaling hyperparameter sensitivity**: Perform a comprehensive sensitivity analysis of the scaling hyperparameter α. Evaluate the model's performance across a wide range of α values and identify the optimal range for different datasets and noise levels.