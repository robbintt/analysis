---
ver: rpa2
title: Variational DAG Estimation via State Augmentation With Stochastic Permutations
arxiv_id: '2402.02644'
source_url: https://arxiv.org/abs/2402.02644
tags: []
core_contribution: This paper introduces VDESP, a Bayesian method for learning the
  structure of directed acyclic graphs (DAGs) from observational data. The core idea
  is to represent a joint distribution over DAGs and permutations, where a permutation
  constrains the possible parental relationships in the graph.
---

# Variational DAG Estimation via State Augmentation With Stochastic Permutations

## Quick Facts
- arXiv ID: 2402.02644
- Source URL: https://arxiv.org/abs/2402.02644
- Reference count: 40
- Key outcome: VDESP achieves SHD of 3.5 on synthetic linear data vs 5.2 for next best method (BCDNET)

## Executive Summary
This paper introduces VDESP, a Bayesian method for learning directed acyclic graph (DAG) structures from observational data. The key innovation is representing a joint distribution over DAGs and permutations, where permutations inherently enforce acyclicity constraints. By using variational inference with continuous relaxations of discrete distributions, the method scales to moderate-sized problems while maintaining theoretical guarantees. The approach is evaluated across synthetic, pseudo-real, and real datasets, showing superior performance compared to competitive baselines in structural recovery metrics.

## Method Summary
VDESP addresses the fundamental challenge of DAG structure learning by jointly modeling DAGs and permutations. The method represents the posterior over DAGs conditioned on data by introducing auxiliary permutation variables that naturally enforce acyclicity. Variational inference is performed using continuous relaxations of discrete distributions, specifically Gumbel-Softmax for categorical variables and concrete distributions for relaxed permutations. The optimization employs the Adam optimizer with a learning rate of 0.001, running for 75,000 iterations on synthetic linear data and 30,000 on nonlinear cases. A temperature parameter of 0.5 is used for the relaxed permutation distributions. The method is designed to handle both linear and nonlinear structural equation models, with the latter requiring additional components like graph conditioner networks.

## Key Results
- On synthetic linear data with 16 variables and 16 edges, VDESP achieved an average SHD of 3.5 compared to 5.2 for the next best method (BCDNET)
- Superior performance on pseudo-real and real datasets (DREAM4, SACHS, SYNTREN) in both SHD and F1 score metrics
- Successful application to Alzheimer's disease biomarker analysis, inferring causal relationships between cognitive measures and biomarkers

## Why This Works (Mechanism)
VDESP works by exploiting the fundamental relationship between DAGs and permutations. By jointly modeling the posterior distribution over DAG structures and node orderings, the method ensures that all sampled graphs are guaranteed to be acyclic. The variational inference framework allows efficient approximation of this joint distribution through continuous relaxations, enabling gradient-based optimization that scales to moderate-sized problems. The Gumbel-Softmax relaxation of discrete edge variables and concrete relaxation of permutations together create a differentiable path from observed data to the posterior over valid DAGs.

## Foundational Learning
- Variational inference with continuous relaxations: Why needed - to approximate intractable posterior distributions over discrete graph structures; Quick check - verify the convergence of ELBO during training
- Gumbel-Softmax relaxation: Why needed - to enable gradient-based optimization over categorical variables representing edge existence; Quick check - ensure the relaxed samples approximate discrete samples as temperature approaches zero
- Permutation-based acyclicity enforcement: Why needed - to guarantee that learned graphs satisfy DAG constraints without explicit acyclicity checking; Quick check - verify that all sampled permutations correspond to valid DAGs

## Architecture Onboarding
- Component map: Data -> SEM Model -> Joint Distribution (DAG + Permutations) -> Variational Inference -> Posterior Approximation
- Critical path: The variational inference loop is critical, where the relaxed permutation samples are generated and used to construct valid DAGs for computing the ELBO
- Design tradeoffs: The relaxation of discrete distributions enables gradient-based optimization but introduces approximation error; the choice of temperature parameter balances exploration and exploitation
- Failure signatures: Poor performance on nonlinear SEMs may indicate issues with the graph conditioner network implementation or insufficient training; overfitting can be diagnosed by monitoring validation performance
- First experiments: 1) Verify the acyclicity enforcement by sampling from relaxed permutation distributions, 2) Test the linear SEM case with known ground truth to validate basic functionality, 3) Compare the impact of different temperature values on the quality of relaxed samples

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the performance of VDESP scale with the number of variables (D) and the expected number of edges (E) in large-scale DAG structure learning problems?
- Basis in paper: The paper only reports results for D = 16 variables and E âˆˆ {16, 64}. It is unclear how the method would perform on larger, more complex DAGs.
- Why unresolved: The experiments were limited to small-scale synthetic and real datasets. Scaling up to hundreds or thousands of variables, as is common in many real-world applications, remains an open question.
- What evidence would resolve it: Experiments evaluating VDESP on large-scale DAGs with varying D and E, comparing its performance and computational efficiency to other methods.

### Open Question 2
- Question: How robust is VDESP to violations of the underlying structural equation model (SEM) assumptions, such as non-additive noise or non-linear relationships?
- Basis in paper: The paper mentions that VDESP can handle nonlinear SEMs but only demonstrates this on synthetic data with a simple MLP architecture. It is unclear how well it would perform with more complex or realistic SEMs.
- Why unresolved: The experiments only tested VDESP on simple nonlinear SEMs. Real-world data often exhibits more complex relationships and noise structures that may violate the SEM assumptions.
- What evidence would resolve it: Experiments evaluating VDESP on real-world datasets with known ground truth DAGs and complex SEMs, such as those from genetics or neuroscience applications.

### Open Question 3
- Question: How sensitive is VDESP to the choice of hyperparameters, such as the temperature parameter of the relaxed permutation distributions and the prior scales of the DAG distributions?
- Basis in paper: The paper provides some details on the hyperparameter settings used in the experiments but does not explore their impact on the performance of VDESP. It is unclear how robust the method is to different hyperparameter choices.
- Why unresolved: The experiments used a fixed set of hyperparameters without exploring their sensitivity. Hyperparameter tuning can be crucial for the performance of machine learning methods, especially those with many parameters like VDESP.
- What evidence would resolve it: Sensitivity analysis of VDESP's performance to different hyperparameter settings, such as grid search or Bayesian optimization over the hyperparameter space.

## Limitations
- Unknown implementation details of the graph conditioner network for nonlinear SEMs limit exact reproducibility
- Limited exploration of hyperparameter sensitivity across different dataset types and scales
- Small-scale demonstration of real-world application (Alzheimer's disease) without broader validation

## Confidence
- Performance metrics (SHD, F1): Medium - detailed experimental setup but incomplete baseline specifications
- Nonlinear SEM results: Medium - promising but missing full implementation details
- Real-world application validity: Low-Medium - interesting but small-scale demonstration

## Next Checks
1. Implement and test the graph conditioner network proposed by Wehenkel & Louppe (2021) for the nonlinear SEM case to verify the reported performance improvements
2. Conduct ablation studies to quantify the impact of different temperature parameters in the relaxed permutation distributions on model performance
3. Validate the Alzheimer's disease application with additional biomarker datasets to assess the generalizability of the inferred causal relationships