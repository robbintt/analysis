---
ver: rpa2
title: Text-Driven Weakly Supervised OCT Lesion Segmentation with Structural Guidance
arxiv_id: '2411.12615'
source_url: https://arxiv.org/abs/2411.12615
tags:
- segmentation
- images
- lesion
- labels
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles weakly supervised semantic segmentation (WSSS)
  for OCT retinal lesion detection, addressing the high cost of pixel-level annotation.
  The proposed method integrates visual and textual guidance using a multimodal framework
  that combines structural features (retinal layers and anomaly signals) with text-driven
  cues from large vision-language models.
---

# Text-Driven Weakly Supervised OCT Lesion Segmentation with Structural Guidance

## Quick Facts
- arXiv ID: 2411.12615
- Source URL: https://arxiv.org/abs/2411.12615
- Authors: Jiaqi Yang; Nitish Mehta; Xiaoling Hu; Chao Chen; Chia-Ling Tsai
- Reference count: 14
- Primary result: Achieves mIoU up to 61.15% on RESC and 68.11% on Duke datasets using only image-level labels

## Executive Summary
This paper addresses the challenge of weakly supervised semantic segmentation (WSSS) for OCT retinal lesion detection, where pixel-level annotations are costly and lesion boundaries are often ambiguous. The proposed framework integrates visual and textual guidance through a multimodal architecture that combines structural features (retinal layers and anomaly signals) with text-driven cues from large vision-language models. By leveraging label-derived descriptions for category-specific semantics and domain-agnostic synthetic descriptions for global context, the method achieves state-of-the-art performance on three OCT datasets without requiring expensive pixel-level annotations.

## Method Summary
The approach employs a multimodal framework with two visual modules (primary and structural) and two textual modules (label-derived and synthetic descriptions). The primary visual module processes original OCT images, while the structural module incorporates cross-attention between retinal layer features and anomaly signals. Textual guidance comes from fixed lesion descriptions encoded via CLIP and domain-agnostic synthetic captions generated by BLIP. The model is trained with multi-label classification losses, generating pseudo labels through combined Class Activation Maps (CAMs) and similarity maps. These pseudo labels are then used to train downstream segmentation models, achieving superior mIoU scores compared to existing WSSS methods.

## Key Results
- Achieves mIoU of 61.15% on RESC dataset and 68.11% on Duke dataset
- Outperforms existing WSSS methods on three OCT datasets with only image-level labels
- Ablation studies confirm effectiveness of both visual and textual guidance components
- Demonstrates significant improvement in pseudo-label quality and segmentation accuracy

## Why This Works (Mechanism)
The method succeeds by leveraging complementary information sources: visual structural features capture anatomical context and lesion anomalies, while textual descriptions provide semantic grounding for lesion categories. The cross-attention mechanism allows the model to focus on relevant structural regions when processing visual features, while the combination of label-derived and synthetic textual cues ensures both specific and general semantic understanding. This multimodal approach addresses the ambiguity inherent in OCT lesion boundaries by providing multiple perspectives on the same data.

## Foundational Learning
- **OCT imaging fundamentals**: Understanding retinal layer structures and common lesion types (SRF, PED, Fluid, IRF, EZ, HRD) is essential for interpreting the structural guidance component
  - Why needed: To understand how structural features relate to lesion detection
  - Quick check: Can identify lesion types and retinal layers in sample OCT images

- **Weakly supervised learning concepts**: Familiarity with pseudo-label generation, Class Activation Maps, and multi-label classification losses
  - Why needed: Core methodology relies on WSSS techniques
  - Quick check: Can explain difference between fully supervised and weakly supervised segmentation

- **Multimodal learning integration**: Understanding how to combine visual and textual features through cross-attention mechanisms
  - Why needed: The paper's key innovation is multimodal integration
  - Quick check: Can describe how cross-attention works between different modality features

## Architecture Onboarding

Component Map:
MiT encoder (frozen stages 1-2) -> Primary Visual Module + Structural Visual Module (with cross-attention) -> Combined Visual Features
CLIP encoder -> Label-Derived Textual Module + Synthetic Textual Module -> Combined Textual Features
Cross-attention between visual and textual features -> Multi-label Classification Head

Critical Path:
Image input -> MiT encoder -> Primary visual features + Structural visual features (with retinal layers + anomaly signals) -> Cross-attention with textual features -> Classification

Design Tradeoffs:
- Uses frozen MiT encoder for efficiency vs. full fine-tuning for potentially better adaptation
- Combines two textual sources (specific labels vs. generic descriptions) for broader semantic coverage
- Employs cross-attention rather than simple concatenation for more targeted feature integration

Failure Signatures:
- Poor pseudo-label quality indicates insufficient text-image alignment or inappropriate threshold selection
- Training instability suggests incorrect implementation of frozen layers or loss weight imbalance
- Suboptimal segmentation performance may indicate inadequate structural feature extraction or textual-semantic misalignment

First Experiments:
1. Verify pseudo-label generation quality by visualizing CAMs against ground truth annotations
2. Test individual textual modules (label-derived vs synthetic) to assess their separate contributions
3. Evaluate cross-attention impact through ablation by comparing with simple feature concatenation

## Open Questions the Paper Calls Out
None

## Limitations
- Exact architectural details of cross-attention mechanism between visual and textual features are not specified
- Computational requirements and inference speed are not disclosed, limiting practical deployment assessment
- Reliance on multiple pretrained models (MiT, CLIP, BLIP, GANomaly) may limit accessibility and reproducibility
- Performance on private dataset cannot be independently verified due to undisclosed characteristics

## Confidence
- High confidence: The multimodal framework combining visual and textual guidance for OCT lesion segmentation is clearly articulated
- Medium confidence: Reported mIoU scores (61.15% on RESC, 68.11% on Duke) appear promising but lack detailed comparison methodology
- Low confidence: Specific contributions of individual components to final performance are not quantified in the abstract

## Next Checks
1. Implement the cross-attention mechanism between visual and textual features with ablation studies to verify its contribution to segmentation performance
2. Test the method on additional OCT datasets with different lesion types to assess generalizability beyond the three mentioned datasets
3. Conduct a thorough computational complexity analysis comparing inference times with existing weakly supervised segmentation methods