---
ver: rpa2
title: A Study in Dataset Pruning for Image Super-Resolution
arxiv_id: '2403.17083'
source_url: https://arxiv.org/abs/2403.17083
tags:
- dataset
- training
- samples
- image
- sampling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores dataset pruning to address the computational
  and storage challenges in training image Super-Resolution (SR) models. The authors
  propose a novel loss-value-based sampling method that reduces the training dataset
  size by selecting samples with high reconstruction difficulty, as determined by
  a pre-trained SRCNN model.
---

# A Study in Dataset Pruning for Image Super-Resolution

## Quick Facts
- arXiv ID: 2403.17083
- Source URL: https://arxiv.org/abs/2403.17083
- Reference count: 40
- Primary result: Dataset pruning using loss-value-based sampling achieves SR performance comparable to or better than full dataset training with reduced computational resources.

## Executive Summary
This paper addresses the computational and storage challenges in training image Super-Resolution (SR) models by proposing a novel dataset pruning approach. The authors introduce a loss-value-based sampling method that selects the most informative samples for training by focusing on those with the highest reconstruction difficulty, as determined by a pre-trained SRCNN model. Their approach reduces the training dataset size by 50% while achieving comparable or superior performance. Notably, excluding the top 5% of hardest samples further improves results, demonstrating that the hardest samples may be detrimental to training.

## Method Summary
The method involves pre-training a simple SRCNN model on the DIV2K dataset, then using it to compute MSE loss values for all training samples. Samples are ranked by their loss values, and descending sampling is applied to select the top 50% with highest losses, focusing training on the most difficult samples. The authors further refine this by excluding the top 5% of hardest samples, which they find negatively impacts training. The pruned dataset is then used to train various SR models (SwinIR, FSRCNN, DRRN, IDN, RDN) with matched total iterations to full dataset training. Performance is evaluated using PSNR and SSIM on benchmark datasets including Set5, Set14, BSDS100, and Urban100.

## Key Results
- Loss-value-based sampling reduces training dataset size by 50% while maintaining or improving SR performance
- Excluding the top 5% hardest samples further enhances results by removing potentially detrimental outliers
- The approach achieves state-of-the-art performance across multiple SR models with significantly reduced computational resources
- Maintaining equal training iterations (epochs × steps) is crucial when using pruned datasets

## Why This Works (Mechanism)

### Mechanism 1
High-loss samples dominate gradient signals, so removing low-loss samples does not significantly degrade learning. The pre-trained SRCNN model computes reconstruction loss for each sample; selecting the top 50% by loss focuses training on samples the model currently struggles with, which are likely to provide the most informative gradient updates. Core assumption: Loss values correlate with sample informativeness for improving the model.

### Mechanism 2
Removing the top 5% hardest samples improves performance by eliminating outliers or noisy samples that mislead optimization. After initial descending sampling, excluding the extreme tail of loss values shifts the core-set toward moderately hard but still informative samples, avoiding the destabilizing effect of very high-loss outliers. Core assumption: The top 5% hardest samples are either noisy or otherwise detrimental to training stability.

### Mechanism 3
Maintaining the same number of training iterations (epochs × steps per epoch) is more important than reducing total training time. When reducing dataset size, fewer steps per epoch occur; compensating by increasing epochs preserves the total number of gradient updates, which is the primary driver of learning progress. Core assumption: Model convergence depends on total gradient steps, not on dataset size per se.

## Foundational Learning

- Concept: Loss function as a proxy for sample difficulty
  - Why needed here: The method relies on loss values computed by a pre-trained model to rank samples; understanding how loss reflects reconstruction difficulty is essential to justify the sampling strategy.
  - Quick check question: If a sample has high MSE loss under SRCNN, does that necessarily mean it will also be hard for the target SR model?

- Concept: Core-set selection in dataset pruning
  - Why needed here: The paper's main contribution is a specific loss-value-based core-set selection; knowing the general idea of core-set methods (selecting a representative subset) helps frame the novelty.
  - Quick check question: How does loss-value-based sampling differ from random sampling or importance sampling based on model uncertainty?

- Concept: Dataset distillation vs. pruning
  - Why needed here: The related work section contrasts their approach with data distillation; understanding that distillation creates synthetic compact data while pruning selects real samples clarifies the method's position.
  - Quick check question: Why might pruning be preferable to distillation for SR tasks, given the differences in data requirements between classification and SR?

## Architecture Onboarding

- Component map: SRCNN pre-trained model -> Loss computation module (MSE) -> Core-set selection logic (sorting + thresholding) -> Training pipeline for target SR model -> Evaluation pipeline (PSNR/SSIM on benchmark datasets)
- Critical path: 1) Load DIV2K training set and pre-compute LR/HR pairs. 2) Run SRCNN inference to get loss values per sample. 3) Sort samples by loss, select top 50% (or refined subset). 4) Train target SR model on the pruned set with matching total iterations. 5) Evaluate on benchmark datasets.
- Design tradeoffs: Using SRCNN vs. a more complex proxy model: simpler, faster, but potentially less accurate difficulty estimation. Fixed 50% vs. adaptive ratio: simpler to implement, but may not be optimal for all datasets/models. Excluding top 5% hardest: empirically beneficial but heuristic; could remove genuinely hard but important samples.
- Failure signatures: Performance drops sharply if the pre-trained proxy model poorly estimates difficulty. Overfitting if too few samples remain after pruning and epochs are not adjusted. No improvement or degradation if the hardest samples are actually the most informative.
- First 3 experiments: 1) Run SRCNN on full DIV2K subset, plot loss-value distribution, verify that descending sampling selects texture-rich samples. 2) Train SwinIR on full vs. 50% descending core-set with matched iterations, compare PSNR/SSIM. 3) Test refined core-set (excluding top 5% hardest) vs. original 50% set, measure impact on performance.

## Open Questions the Paper Calls Out

- What is the optimal balance between excluding the top 5% of hardest samples and retaining the remaining 95% to maximize SR model performance?
- How does the performance of loss-value-based sampling compare to other core-set selection methods, such as data distillation or proxy datasets, in the context of image SR?
- Can the loss-value-based sampling approach be extended to other image processing tasks beyond SR, such as image denoising or image inpainting?

## Limitations
- Reliance on a pre-trained SRCNN model to estimate sample difficulty may not generalize to all SR architectures or datasets
- The heuristic of excluding the top 5% hardest samples lacks theoretical justification and could remove genuinely informative samples
- The method assumes that maintaining total gradient steps is sufficient for convergence, which may not hold for all models or when overfitting risk is high

## Confidence
- High Confidence: The core finding that descending loss-value sampling (top 50%) maintains or improves SR performance with fewer samples is well-supported by multiple experiments across different SR models.
- Medium Confidence: The improvement from excluding the top 5% hardest samples is empirically demonstrated but relies on a heuristic that may not generalize.
- Low Confidence: The theoretical justification for why loss values from a pre-trained SRCNN correlate with sample informativeness for arbitrary SR models is not established.

## Next Checks
1. Validate the method using a proxy model other than SRCNN (e.g., FSRCNN or a deeper CNN) to ensure the loss-value correlation is not specific to SRCNN's architecture.
2. Experiment with early stopping and regularization on the pruned datasets to assess whether the fixed iteration strategy risks overfitting, especially for very small core-sets.
3. Systematically test whether retaining (rather than excluding) the top 5% hardest samples improves performance on specific SR tasks, such as preserving rare textures or fine details.