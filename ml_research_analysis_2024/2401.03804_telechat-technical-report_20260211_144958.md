---
ver: rpa2
title: TeleChat Technical Report
arxiv_id: '2401.03804'
source_url: https://arxiv.org/abs/2401.03804
tags:
- data
- arxiv
- training
- language
- telechat
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This technical report introduces TeleChat, a collection of large
  language models with 3B, 7B, and 12B parameters, trained on extensive Chinese and
  English text data. TeleChat undergoes supervised fine-tuning and reinforcement learning
  to align with human preferences.
---

# TeleChat Technical Report

## Quick Facts
- arXiv ID: 2401.03804
- Source URL: https://arxiv.org/abs/2401.03804
- Reference count: 40
- Three models with 3B, 7B, and 12B parameters achieve competitive performance on benchmarks

## Executive Summary
This technical report introduces TeleChat, a family of large language models with 3B, 7B, and 12B parameters trained on extensive Chinese and English text data. The models undergo supervised fine-tuning and reinforcement learning to align with human preferences, achieving competitive performance on benchmarks for language understanding, mathematics, reasoning, code generation, and knowledge-based question answering. The authors propose using knowledge graphs to mitigate hallucinations by augmenting prompts with relevant factual information. The 7B and 12B fine-tuned models, along with code and partial pretraining data, are publicly released.

## Method Summary
The TeleChat models are trained on a mix of Chinese and English text data, followed by supervised fine-tuning and reinforcement learning to align with human preferences. The approach includes knowledge graph integration to reduce hallucinations by augmenting prompts with factual information. The 7B and 12B models are released publicly along with training code and partial pretraining data.

## Key Results
- Competitive performance on language understanding, mathematics, reasoning, code generation, and knowledge-based question answering benchmarks
- Outperforms similar-sized open-source models on key metrics
- Achieves state-of-the-art results among comparable parameter models
- Public release of 7B and 12B models with code and partial training data

## Why This Works (Mechanism)
The effectiveness of TeleChat stems from its comprehensive training approach combining large-scale pretraining on multilingual data with supervised fine-tuning and reinforcement learning. The knowledge graph integration addresses hallucination issues by providing factual context during inference, while the model architecture and parameter scaling enable strong performance across diverse tasks.

## Foundational Learning
- Large-scale pretraining: Why needed - Builds general language understanding; Quick check - Verify training data diversity and scale
- Supervised fine-tuning: Why needed - Aligns model behavior with human preferences; Quick check - Confirm quality of fine-tuning datasets
- Reinforcement learning: Why needed - Further refines model responses; Quick check - Validate reward model effectiveness
- Knowledge graph integration: Why needed - Reduces hallucinations through factual grounding; Quick check - Test knowledge graph coverage and relevance
- Multilingual training: Why needed - Enables cross-lingual capabilities; Quick check - Evaluate performance across language pairs
- Parameter scaling: Why needed - Increases model capacity and capability; Quick check - Analyze performance scaling with model size

## Architecture Onboarding
**Component Map:** Data preprocessing -> Pretraining -> Supervised fine-tuning -> RL fine-tuning -> Knowledge graph integration -> Inference
**Critical Path:** The training pipeline follows a sequential progression from pretraining through fine-tuning stages, with knowledge graph integration applied during inference
**Design Tradeoffs:** Model size versus performance, multilingual capability versus specialization, hallucination mitigation versus response flexibility
**Failure Signatures:** Poor generalization on out-of-distribution data, hallucination persistence despite knowledge graph integration, suboptimal performance on low-resource languages
**First Experiments:**
1. Benchmark evaluation across all three model sizes on standard tasks
2. Ablation study comparing performance with and without knowledge graph integration
3. Cross-lingual capability assessment between Chinese and English tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Insufficient detail about training methodology and hyperparameter choices
- Limited experimental validation of knowledge graph hallucination mitigation approach
- Lack of clarity regarding training data composition and quality assurance
- Missing comparison with established hallucination detection methods

## Confidence
- Model architecture and parameter details: High confidence
- Benchmark performance claims: Medium confidence (limited evaluation details)
- Hallucination mitigation approach: Low confidence (minimal validation provided)
- Data composition and training process: Low confidence (insufficient detail)

## Next Checks
1. Conduct independent reproduction of key benchmark results using the publicly released models and provided code
2. Perform ablation studies to quantify the effectiveness of the knowledge graph hallucination mitigation approach
3. Execute thorough analysis of model outputs to verify claims about language understanding and reasoning capabilities across diverse domains