---
ver: rpa2
title: Learning Interpretable Classifiers for PDDL Planning
arxiv_id: '2410.10011'
source_url: https://arxiv.org/abs/2410.10011
tags:
- formula
- formulas
- problem
- learning
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of learning interpretable, human-readable
  temporal logic formulas that can recognize and distinguish the behavior of planning
  agents on a set of similar PDDL planning tasks. The proposed approach uses first-order
  temporal logic (FTL), which generalizes well to unseen instances and captures agent
  policies as concise, formal descriptions.
---

# Learning Interpretable Classifiers for PDDL Planning

## Quick Facts
- arXiv ID: 2410.10011
- Source URL: https://arxiv.org/abs/2410.10011
- Authors: Arnaud Lequen
- Reference count: 38
- Key outcome: Learns interpretable temporal logic formulas for PDDL planning with perfect test accuracy on small domains

## Executive Summary
This paper addresses the challenge of learning human-readable temporal logic formulas that can recognize and distinguish agent behaviors in PDDL planning tasks. The approach uses first-order temporal logic to create formulas that generalize across planning instances while remaining interpretable. The method employs a topology-guided MaxSAT optimization to efficiently search for optimal formulas, achieving perfect classification accuracy on test traces from domains like Childsnack and Spanner.

## Method Summary
The method learns first-order temporal logic (FTL) formulas through a MaxSAT-based optimization framework. The core innovation is fixing the formula topology before MaxSAT solving, which reduces the search space from variable combinations to filling in predicate symbols and quantifiers. The algorithm encodes the learning problem as a weighted MaxSAT instance, where the objective is to maximize the sum of satisfied training traces. By enumerating different formula topologies separately and using a parallelizable search strategy, the approach generates diverse, interpretable formulas that generalize to unseen planning instances.

## Key Results
- Achieved perfect classification accuracy on test traces for Childsnack and Spanner domains
- Successfully learned accurate formulas from very few training examples (often just 1-2 instances)
- Demonstrated formula interpretability while maintaining generalization across PDDL instances
- Showed improved efficiency through topology-guided MaxSAT encoding compared to exhaustive search

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MaxSAT-based encoding enables efficient search over combinatorial FTL formula space while handling noise
- Mechanism: Fixing formula topology before MaxSAT solving reduces search space, allowing the solver to optimize weighted trace satisfaction
- Core assumption: Topology-guided approach preserves expressive power while reducing computational burden
- Evidence anchors: [abstract] topology-guided compilation to MaxSAT; [section] encoding learning problem into MaxSAT instance
- Break condition: Fixed topology may be too restrictive or MaxSAT solver cannot handle constraints efficiently

### Mechanism 2
- Claim: FTL language generalizes across PDDL instances while remaining interpretable
- Mechanism: Quantification over object types in type hierarchy enables learned properties to apply to new instances
- Core assumption: Type hierarchy is rich enough to capture generalization patterns
- Evidence anchors: [abstract] formulas generalize to unseen instances; [section] FTL expresses properties that generalize between instances
- Break condition: Shallow type hierarchy or complex generalization patterns may prevent accurate formula learning

### Mechanism 3
- Claim: Topology-guided approach increases formula diversity and efficiency
- Mechanism: Separate enumeration of TL chains and quantifier/type combinations enables parallel exploration of diverse structures
- Core assumption: Different topologies capture different behavior aspects and enumeration strategy explores space effectively
- Evidence anchors: [abstract] fixing topology before encoding increases diversity; [section] algorithm is more parallelizable
- Break condition: Missing important topologies or parallel exploration fails to yield diverse formulas

## Foundational Learning

- Concept: PDDL planning domain and instance structure
  - Why needed here: Algorithm operates on PDDL domains and traces, requiring understanding of type hierarchy and state transitions
  - Quick check question: What are the components of a PDDL domain and how do they relate to planning instances?

- Concept: First-order temporal logic semantics and model checking
  - Why needed here: Learned formulas evaluated against traces using FTL semantics, requiring understanding of quantification and temporal operators
  - Quick check question: How does an FTL formula get evaluated against a trace in a specific environment?

- Concept: MaxSAT optimization and constraint encoding
  - Why needed here: Algorithm reduces formula learning to MaxSAT, requiring understanding of SAT variable encoding and weight optimization
  - Quick check question: How are formula constraints encoded as MaxSAT clauses and how does solver find optimal solutions?

## Architecture Onboarding

- Component map: PDDL domain parser -> Type hierarchy extractor -> Trace generator -> TL chain enumerator -> Quantifier enumerator -> MaxSAT encoder -> Z3 solver -> Formula evaluator -> Result aggregator

- Critical path: 1) Parse PDDL domain and extract type hierarchy 2) Generate training and test traces 3) Enumerate TL chains for given logical operator count 4) For each topology, enumerate quantifier and type combinations 5) Encode constraints as MaxSAT clauses with weights 6) Solve MaxSAT to find optimal formulas 7) Evaluate formulas against test set and select best 8) Repeat for different topologies to ensure diversity

- Design tradeoffs: Fixing topology reduces search space but may miss optimal formulas; FTL enables generalization but increases complexity; Parallel enumeration increases diversity but requires more resources; Splitting predicates to arity 2 reduces flents but may lose domain structure

- Failure signatures: Algorithm fails to find formulas (restrictive topology or encoding errors); Low accuracy (insufficient training data or improper weights); Runtime exceeds limits (expensive enumeration or solving); Formulas not interpretable (too complex structures)

- First 3 experiments: 1) Run on Childsnack with 1 training instance, q=1, |ϕ|=2, verify accuracy 2) Run on Spanner with 2 training instances, q=2, |ϕ|=3, measure runtime and diversity 3) Vary logical operator count from 2-4, observe impact on accuracy and runtime

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the algorithm be adapted to generate domain-specific control knowledge for real-world planning problems?
- Basis in paper: [explicit] Future work includes tailoring algorithm and datasets for domain-specific control knowledge integration
- Why unresolved: Paper focuses on theoretical aspects without practical integration exploration
- What evidence would resolve it: Demonstrating effectiveness in real-world planning domains with measurable improvements

### Open Question 2
- Question: Is the NP-hardness of FTL learning problem also PSPACE-hard?
- Basis in paper: [explicit] Problem is NP-hard and in PSPACE, but PSPACE-hardness remains open
- Why unresolved: Paper provides NP-hardness proof but does not explore PSPACE-hardness
- What evidence would resolve it: Formal proof showing FTL learning is PSPACE-hard through reduction from PSPACE-complete problem

### Open Question 3
- Question: How does performance scale with larger and more complex planning domains?
- Basis in paper: [inferred] Experiments limited to small IPC domains without scalability testing
- Why unresolved: Paper focuses on small-scale problems without addressing demanding scenarios
- What evidence would resolve it: Benchmarking on larger domains with hundreds of objects/actions and analyzing runtime/accuracy

## Limitations

- Computational complexity of MaxSAT encoding is NP-hard, with unclear scalability to larger domains
- Empirical evaluation limited to specific domains without extensive testing across diverse planning benchmarks
- Algorithm's robustness to noisy or incomplete training data not thoroughly validated in practice

## Confidence

- **High Confidence:** Theoretical foundation of using FTL for generalizable behavior recognition is well-established
- **Medium Confidence:** Empirical results showing accurate formula learning from few examples are promising but limited in scope
- **Low Confidence:** Algorithm's robustness to noise and scalability to complex domains mentioned but not thoroughly tested

## Next Checks

1. **Scalability Test:** Apply algorithm to larger, more complex PDDL domains and measure runtime and formula accuracy as domain complexity increases
2. **Noise Robustness:** Systematically introduce noise into training traces and evaluate how well learned formulas maintain accuracy and interpretability
3. **Cross-Domain Generalization:** Learn formulas on one PDDL domain and test ability to recognize behaviors in structurally similar but unseen domains, quantifying generalization performance