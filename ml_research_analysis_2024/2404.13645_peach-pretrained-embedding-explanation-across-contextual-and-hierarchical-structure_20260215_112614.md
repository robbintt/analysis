---
ver: rpa2
title: 'PEACH: Pretrained-embedding Explanation Across Contextual and Hierarchical
  Structure'
arxiv_id: '2404.13645'
source_url: https://arxiv.org/abs/2404.13645
tags:
- decision
- peach
- tree
- figure
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces PEACH, a tree-based explanation method that
  interprets text classification decisions using fine-tuned contextual embeddings.
  PEACH extracts feature matrices from embeddings using correlation clustering, K-means,
  or CNN-based grouping, then trains interpretable decision trees (ID3, C4.5, CART,
  or Random Forest) on these features.
---

# PEACH: Pretrained-embedding Explanation Across Contextual and Hierarchical Structure

## Quick Facts
- **arXiv ID**: 2404.13645
- **Source URL**: https://arxiv.org/abs/2404.13645
- **Reference count**: 17
- **Primary result**: PEACH achieves comparable or better accuracy than fine-tuned PLMs and outperforms LIME and Anchor in human interpretability studies (>84% preferred, κ > 0.7)

## Executive Summary
PEACH introduces a tree-based explanation method for interpreting text classification decisions made by fine-tuned contextual embeddings. The approach extracts feature matrices from embeddings using correlation clustering, K-means, or CNN-based grouping, then trains interpretable decision trees (ID3, C4.5, CART, or Random Forest) on these features. Visualizations employ word clouds filtered by POS and NER tags to highlight salient terms in decision paths. Across nine NLP benchmarks, PEACH demonstrates competitive accuracy with fine-tuned PLMs while achieving superior human interpretability compared to LIME and Anchor methods.

## Method Summary
PEACH processes text through fine-tuned contextual embeddings, extracts feature matrices using clustering or CNN-based grouping techniques, and trains interpretable decision trees on these reduced representations. The method employs word cloud visualizations filtered by POS and NER tags to highlight important features along decision paths. Four tree algorithms are evaluated: ID3, C4.5, CART, and Random Forest. The approach balances model performance with interpretability by operating on compressed feature representations rather than full embedding spaces, enabling human-understandable explanations while maintaining competitive accuracy on classification tasks.

## Key Results
- PEACH achieves comparable or better accuracy than fine-tuned PLMs across nine NLP benchmarks
- Outperforms LIME and Anchor in human interpretability studies (over 84% preferred, κ > 0.7)
- Validated across diverse tasks including sentiment analysis, topic classification, question type classification, semantic similarity, and medical text analysis

## Why This Works (Mechanism)
PEACH works by reducing the high-dimensional embedding space to interpretable feature matrices through clustering or CNN-based grouping, then applying decision tree algorithms that create human-understandable decision paths. The word cloud visualizations filtered by linguistic tags provide intuitive representations of feature importance. This hierarchical approach preserves essential decision-making information while eliminating noise and redundancy in full embeddings. The combination of feature extraction, tree-based learning, and linguistic visualization creates explanations that are both faithful to model decisions and comprehensible to humans.

## Foundational Learning
**Contextual Embeddings**: Pre-trained language models that capture word meaning based on surrounding context, essential for modern NLP tasks. *Why needed*: Provide rich semantic representations for downstream classification. *Quick check*: Verify embeddings capture polysemy and context-dependent meanings.

**Correlation Clustering**: Groups similar feature vectors based on pairwise correlations. *Why needed*: Identifies semantically related embedding dimensions for feature reduction. *Quick check*: Ensure clusters capture meaningful semantic relationships.

**Decision Tree Algorithms (ID3, C4.5, CART, Random Forest)**: Supervised learning methods that create interpretable decision rules. *Why needed*: Generate human-understandable explanations from feature matrices. *Quick check*: Verify trees capture important decision boundaries without overfitting.

**POS/NER Tag Filtering**: Linguistic annotation that categorizes words by grammatical role or named entity type. *Why needed*: Enhances interpretability by focusing on semantically relevant words. *Quick check*: Confirm taggers maintain high accuracy on target domain text.

## Architecture Onboarding

**Component Map**: Text -> Embedding Layer -> Feature Matrix Extraction (Clustering/CNN) -> Decision Tree Training -> Word Cloud Visualization (POS/NER Filtered)

**Critical Path**: Embedding extraction → Feature matrix grouping → Tree training → Visualization generation

**Design Tradeoffs**: Feature matrix reduction improves interpretability but may lose some nuanced information from full embeddings. Word cloud visualizations sacrifice precision for accessibility.

**Failure Signatures**: Poor clustering results in noisy feature matrices; shallow trees may oversimplify decisions; inaccurate POS/NER tagging produces misleading visualizations.

**First Experiments**:
1. Test feature extraction methods (correlation clustering, K-means, CNN) on sample embeddings to verify semantic grouping
2. Train simple decision trees on extracted features to assess classification performance
3. Generate sample word clouds with POS/NER filtering to evaluate visualization quality

## Open Questions the Paper Calls Out
None

## Limitations
- Human evaluation relies on small sample sizes (20-50 participants) that may not generalize
- Tree-based explanations may oversimplify complex decision boundaries in fine-tuned PLMs
- Performance claims comparing PEACH to fine-tuned PLMs should be interpreted cautiously since PEACH uses reduced feature representations

## Confidence
- **High confidence** in methodological framework and implementation details
- **Medium confidence** in experimental results on standard benchmarks
- **Low confidence** in generalizability of human evaluation results and superiority claims

## Next Checks
1. Conduct ablation studies to quantify information loss from feature matrix extraction compared to full embeddings
2. Test PEACH explanations on out-of-domain datasets to evaluate robustness and generalizability
3. Implement automated metrics for explanation faithfulness (sensitivity analysis, input perturbation studies) to complement human evaluations