---
ver: rpa2
title: 'Detecting Car Speed using Object Detection and Depth Estimation: A Deep Learning
  Framework'
arxiv_id: '2408.04360'
source_url: https://arxiv.org/abs/2408.04360
tags:
- speed
- vehicle
- were
- estimation
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a deep learning-based framework for estimating
  vehicle speed using handheld cameras, addressing the need for cost-effective and
  portable alternatives to traditional speed detection devices like LIDAR and radar
  guns. The method combines object detection (using YOLOv8, YOLOv5, and SSD) with
  depth estimation (using MiDAS) to estimate speed.
---

# Detecting Car Speed using Object Detection and Depth Estimation: A Deep Learning Framework

## Quick Facts
- arXiv ID: 2408.04360
- Source URL: https://arxiv.org/abs/2408.04360
- Authors: Subhasis Dasgupta; Arshi Naaz; Jayeeta Choudhury; Nancy Lahiri
- Reference count: 9
- Best performance: YOLOv8 + polynomial regression (R² 0.81, Adjusted R² 0.74, RMSE 2.24)

## Executive Summary
This paper presents a deep learning framework for estimating vehicle speed using handheld cameras, offering a cost-effective alternative to traditional speed detection devices like LIDAR and radar guns. The method combines object detection (YOLOv8, YOLOv5, SSD) with depth estimation (MiDAS) to extract features from vehicle videos. A polynomial regression model trained on bounding box area and average distance features predicts vehicle speed. YOLOv8 with polynomial regression achieved the best performance, with R² of 0.81, Adjusted R² of 0.74, and RMSE of 2.24. The framework shows that depth estimation significantly improves accuracy, particularly for vehicles closer to the camera.

## Method Summary
The framework processes vehicle videos frame-by-frame to estimate speed. First, object detection models (YOLOv8, YOLOv5, or SSD) identify vehicles and extract bounding box areas. Simultaneously, the MiDAS model estimates depth information, generating average distance values. Mask-RCNN is used for masking to improve depth estimation accuracy. These features (bounding box area and average distance) from the first and last frames are then used to train a polynomial regression model (maximum degree 3) to predict vehicle speed. The framework was tested on 100 videos (3-7 seconds each, 640x480 resolution) of single vehicles with known speeds.

## Key Results
- YOLOv8 with polynomial regression achieved R² of 0.81, Adjusted R² of 0.74, and RMSE of 2.24
- Depth estimation significantly improved accuracy, with change in average distance being the most important feature
- The model performs better when vehicles are closer to the camera
- Linear regression served as a baseline, outperformed by polynomial regression

## Why This Works (Mechanism)
The framework leverages the relationship between apparent object size (bounding box area) and distance to estimate vehicle speed. As a vehicle approaches or moves away from the camera, both its apparent size and distance change in predictable ways. By combining these visual cues through depth estimation and object detection, the model can infer speed from the rate of change in these features. The polynomial regression captures non-linear relationships between these visual features and actual speed.

## Foundational Learning
- Object Detection (YOLO): Why needed - to identify vehicles and extract bounding box areas; Quick check - verify detection accuracy on sample frames
- Depth Estimation (MiDAS): Why needed - to estimate distance of vehicle from camera; Quick check - compare estimated depth with ground truth distances
- Polynomial Regression: Why needed - to model non-linear relationship between visual features and speed; Quick check - test different polynomial degrees on validation set

## Architecture Onboarding

**Component Map:** Video -> Frame Extraction -> Object Detection -> Depth Estimation -> Feature Extraction -> Polynomial Regression -> Speed Prediction

**Critical Path:** Frame extraction → object detection → depth estimation → feature extraction → polynomial regression

**Design Tradeoffs:** YOLO variants offer speed vs. accuracy tradeoffs; depth estimation adds computational overhead but improves accuracy; polynomial regression captures non-linear relationships but may overfit

**Failure Signatures:** Poor bounding box accuracy leads to incorrect area measurements; inaccurate depth estimation results in wrong distance features; insufficient feature correlation causes low R²

**First Experiments:**
1. Test YOLOv8 detection accuracy on sample vehicle frames
2. Validate MiDAS depth estimation against ground truth distances
3. Train linear regression baseline and compare with polynomial regression

## Open Questions the Paper Calls Out

**Open Question 1:** How does the proposed framework perform for vehicles traveling at very high speeds (above 100 kmph)?
- Basis in paper: The paper mentions that high shutter speed of the camera becomes crucial for sharp images at high speeds, suggesting potential limitations
- Why unresolved: The study only collected data for vehicles within a certain speed range, and did not specifically test or analyze the framework's performance at very high speeds
- What evidence would resolve it: Collecting and analyzing data for vehicles traveling at speeds above 100 kmph, and evaluating the framework's accuracy and limitations in such scenarios

**Open Question 2:** How does the proposed framework perform in real-world scenarios with varying lighting conditions, weather, and road types?
- Basis in paper: The paper mentions using pre-trained models and collecting data under controlled conditions, but does not discuss the framework's robustness to real-world variations
- Why unresolved: The study was conducted in a controlled environment, and the impact of external factors on the framework's performance is unknown
- What evidence would resolve it: Testing the framework in diverse real-world scenarios with varying lighting, weather, and road conditions, and evaluating its accuracy and reliability

**Open Question 3:** Can the proposed framework be extended to handle multi-vehicle scenarios and detect individual vehicle speeds simultaneously?
- Basis in paper: The paper mentions that the study was not generalized for multi-vehicle scenarios, as vehicle tracking would have become important
- Why unresolved: The study focused on single-vehicle scenarios, and the framework's capability to handle multiple vehicles and track individual speeds is unexplored
- What evidence would resolve it: Modifying the framework to incorporate vehicle tracking and testing its performance in multi-vehicle scenarios, evaluating its accuracy in detecting individual vehicle speeds

## Limitations
- Limited dataset details and unknown collection protocols affect reproducibility
- Performance in multi-vehicle scenarios and varying environmental conditions remains unverified
- Computational requirements for real-time deployment on handheld devices are not discussed

## Confidence
- Framework architecture and model selection: High
- Performance metrics on reported dataset: Medium (due to limited dataset details)
- Real-world applicability and robustness: Low

## Next Checks
1. Conduct controlled experiments varying vehicle distance, lighting conditions, and camera motion to assess framework robustness
2. Perform feature ablation study to statistically validate the importance of change in average distance versus bounding box area
3. Test the framework on diverse video datasets (different camera types, environmental conditions, and vehicle types) to evaluate generalizability