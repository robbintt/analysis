---
ver: rpa2
title: 'Towards AI-$45^{\circ}$ Law: A Roadmap to Trustworthy AGI'
arxiv_id: '2412.14186'
source_url: https://arxiv.org/abs/2412.14186
tags:
- arxiv
- safety
- trustworthiness
- preprint
- systems
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes the AI-45\xB0 Law as a guiding principle for\
  \ balanced development of AI capabilities and safety, addressing the current imbalance\
  \ where AI safety lags behind rapid capability advancements. The authors introduce\
  \ the Causal Ladder of Trustworthy AGI framework, consisting of three layers (Approximate\
  \ Alignment, Intervenable, and Reflectable) inspired by Judea Pearl's Ladder of\
  \ Causation."
---

# Towards AI-$45^{\circ}$ Law: A Roadmap to Trustworthy AGI

## Quick Facts
- arXiv ID: 2412.14186
- Source URL: https://arxiv.org/abs/2412.14186
- Authors: Chao Yang; Chaochao Lu; Yingchun Wang; Bowen Zhou
- Reference count: 40
- This paper proposes the AI-45° Law as a guiding principle for balanced development of AI capabilities and safety, addressing the current imbalance where AI safety lags behind rapid capability advancements

## Executive Summary
This paper introduces the AI-45° Law as a guiding principle for balanced development of AI capabilities and safety, addressing the current imbalance where AI safety lags behind rapid capability advancements. The authors propose the Causal Ladder of Trustworthy AGI framework, consisting of three layers (Approximate Alignment, Intervenable, and Reflectable) inspired by Judea Pearl's Ladder of Causation. The framework defines five levels of trustworthy AGI: perception, reasoning, decision-making, autonomy, and collaboration trustworthiness. The approach aims to create AI systems that are both highly capable and aligned with human values, with governance measures to support development as a global public good.

## Method Summary
The paper presents a conceptual framework for AGI development that combines safety considerations with capability advancement through the AI-45° Law. The Causal Ladder of Trustworthy AGI framework draws heavily on existing causal inference literature but does not provide concrete metrics or benchmarks for measuring progress through its proposed layers and levels. The proposed five-level trustworthiness hierarchy (perception, reasoning, decision-making, autonomy, and collaboration) is presented without clear operational definitions or validation criteria. The framework aims to create AI systems that are both highly capable and aligned with human values, with governance measures to support development as a global public good.

## Key Results
- Introduces AI-45° Law as a guiding principle for balanced development of AI capabilities and safety
- Proposes Causal Ladder of Trustworthy AGI framework with three layers (Approximate Alignment, Intervenable, and Reflectable)
- Defines five levels of trustworthy AGI: perception, reasoning, decision-making, autonomy, and collaboration trustworthiness

## Why This Works (Mechanism)
The framework works by establishing a structured progression through increasingly sophisticated causal reasoning capabilities, ensuring that as AI systems become more capable, they simultaneously develop the ability to understand and respect human values and intentions. The three-layer structure (Approximate Alignment, Intervenable, and Reflectable) creates a scaffold where each layer builds upon the previous one, starting with basic value alignment and progressing toward systems that can reason about their own decision-making processes and their impact on the world.

## Foundational Learning
- Causal Inference Theory - why needed: Provides mathematical foundation for understanding cause-effect relationships in AI decision-making
- Value Alignment Principles - why needed: Ensures AI systems develop goals and behaviors aligned with human values
- Multi-agent Systems - why needed: Enables collaboration and coordination between AI systems and humans
- Safety Engineering - why needed: Incorporates safety considerations into the core development process
- Governance Frameworks - why needed: Establishes mechanisms for responsible AGI development as a global public good

Quick check: Can you trace how each layer of the Causal Ladder builds from basic alignment to reflective reasoning?

## Architecture Onboarding

Component map: Data Inputs -> Causal Reasoning Engine -> Value Alignment Module -> Decision Layer -> Safety Guardrails -> Output

Critical path: The progression through the three layers of the Causal Ladder (Approximate Alignment → Intervenable → Reflectable) represents the critical path for developing trustworthy AGI. Each layer must be sufficiently developed before advancing to the next, with the Reflectable layer being essential for achieving full autonomy and collaboration capabilities.

Design tradeoffs: The framework prioritizes safety and alignment over pure capability advancement, potentially slowing down certain aspects of AI development but ensuring more robust and trustworthy systems. The governance recommendations for treating AGI as a global public good may conflict with commercial interests but aim to prevent monopolization of critical technology.

Failure signatures: Potential failures include getting stuck at lower layers without progressing to higher levels of trustworthiness, misalignment between the AI's understanding of human values and actual human preferences, and governance challenges in coordinating global efforts for AGI development.

First experiments:
1. Implement the Approximate Alignment layer in a narrow AI system and measure its ability to understand and follow simple human instructions
2. Test the Intervenable layer by introducing controlled perturbations and measuring the system's ability to reason about causal relationships
3. Evaluate the Reflectable layer by assessing an AI system's ability to analyze its own decision-making process and explain its reasoning

## Open Questions the Paper Calls Out
None

## Limitations
- The framework remains largely theoretical, lacking empirical validation or implementation examples
- Does not provide concrete metrics or benchmarks for measuring progress through proposed layers and levels
- Governance recommendations lack specific implementation mechanisms or policy frameworks

## Confidence
- Medium: The conceptual framework connecting AI capabilities with safety through the 45° development angle is internally consistent and builds on established causal inference theory, but lacks empirical demonstration
- Medium: The three-layer structure provides a logical progression, though the specific properties assigned to each layer are somewhat abstract
- Low: The governance recommendations for treating AGI as a global public good are valuable but lack specific implementation mechanisms or policy frameworks

## Next Checks
1. Develop concrete metrics and benchmarks to assess progress through each layer and level of the Causal Ladder framework, including specific tests for causal reasoning and reflection capabilities
2. Design pilot implementations of AI-45° Law principles in existing AI systems to evaluate whether the proposed 45° development trajectory is practically achievable and beneficial
3. Create interdisciplinary validation studies involving AI safety researchers, capability developers, and ethicists to assess the framework's applicability across different AI development contexts and value systems