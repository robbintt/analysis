---
ver: rpa2
title: 'Position Paper: Why the Shooting in the Dark Method Dominates Recommender
  Systems Practice; A Call to Abandon Anti-Utopian Thinking'
arxiv_id: '2402.02152'
source_url: https://arxiv.org/abs/2402.02152
tags:
- systems
- learning
- information
- recommender
- reward
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper critiques the dominant "shooting in the dark" approach
  in recommender systems, where A/B testing is used to measure performance but new
  systems are proposed by optimizing proxies rather than direct rewards. This leads
  to uncertainty about whether proposed proxies correlate with actual performance.
---

# Position Paper: Why the Shooting in the Dark Method Dominates Recommender Systems Practice; A Call to Abandon Anti-Utopian Thinking

## Quick Facts
- arXiv ID: 2402.02152
- Source URL: https://arxiv.org/abs/2402.02152
- Reference count: 16
- The paper argues for replacing proxy-based optimization in recommender systems with direct reward optimization using deep learning and Bayesian decision theory

## Executive Summary
This paper critiques the dominant "shooting in the dark" approach in recommender systems where A/B testing measures performance but new systems are proposed by optimizing proxies rather than direct rewards. The author argues that modern deep learning tools now make it feasible to build truly reward-optimizing recommender systems by combining contextual bandit formulations with Bayesian inference. The paper calls for abandoning proxy-based methodology in favor of direct reward optimization, claiming this approach can better capture the true objectives of recommendation systems.

## Method Summary
The paper proposes formulating recommendation as a contextual bandit problem with Bayesian decision theory. The method uses deep learning to handle high-dimensional context-action interactions and small effect sizes, while employing variational Bayesian inference to approximate the posterior and incorporate prior information. The approach aims to build bespoke models that learn directly from reward signals rather than optimizing proxies like clicks or time spent. The framework combines matrix normal priors for incorporating prior information about similarities between contexts, actions, and context-action pairs.

## Key Results
- The current proxy-based methodology in recommender systems leads to uncertainty about whether proposed proxies correlate with actual performance
- Deep learning enables Bayesian inference over previously intractable high-dimensional context-action interactions
- Contextual bandit formulation reduces recommendation to a regression problem while maintaining causal inference properties

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Deep learning enables Bayesian inference over high-dimensional context-action interactions that were previously intractable
- Mechanism: Variational Bayesian methods approximate the posterior distribution of parameters in contextual bandit models, allowing incorporation of prior information when likelihood is flat over most of parameter space
- Core assumption: The likelihood function becomes flat when dealing with high-dimensional context-action interactions and small effect sizes
- Evidence anchors:
  - [section] "Unfortunately, this is false, even in toy recommendation situations where the set of recommendable items is a single item... In order for reduce the standard error of the difference between the best action and another action down to ±0.1%, this requires 100000 observations on each context-action pair"
  - [section] "Deep learning however allows us to handle these limitations cleanly. In short we will use deep learning, but not to go deep"
  - [corpus] Weak corpus support for this specific mechanism - neighbors focus on different topics
- Break condition: When effect sizes become large enough that maximum likelihood estimation becomes viable

### Mechanism 2
- Claim: Contextual bandit formulation reduces recommendation to regression problem while maintaining causal inference properties
- Mechanism: By restricting to T=1 (single decision point), the recommendation problem becomes a non-standard regression task where the optimal action is simply the argmax of expected reward
- Core assumption: The full sequential recommendation problem can be approximated by treating each decision point as independent contextual bandit
- Evidence anchors:
  - [section] "Under a contextual bandit assumption the model becomes: P(S1 = su1, S0 = su0|f1(·), θ1, θ0) = P(S1 = su1|S0 = su0, A1 = f1(su0), θ1)P(S0 = su0|θ0)"
  - [section] "Moreover, depending on the paramterization it may be immediately construct an optimal f1(·) for all values of S1 = s1 from an estimate of θ1"
  - [corpus] Weak corpus support - neighbors don't discuss this specific formulation
- Break condition: When user trajectories show significant temporal dependencies that cannot be captured by single decision points

### Mechanism 3
- Claim: Bayesian decision theory provides correct causal inference without requiring specialized causal inference techniques
- Mechanism: Regression-based approaches to contextual bandits automatically provide correct causal estimates because they condition on all observed confounders
- Core assumption: In production systems, treatments are allocated based only on observed variables, preventing unobserved confounding
- Evidence anchors:
  - [section] "Causal inference is trivial. Indeed, in general from a Bayesian decision theoretic position, causal inference is inference and there is no need to augment it with exotic extensions"
  - [section] "Unobserved confounding occurs only when an un-observed variable determines both the allocation of the treatment, and the response to the treatment. This cannot occur in a production system"
  - [corpus] Weak corpus support - neighbors don't discuss causal inference methodology
- Break condition: When feature engineering drops relevant variables or creates nested models with different information access

## Foundational Learning

- Concept: Bayesian decision theory and posterior inference
  - Why needed here: The paper relies on Bayesian methods to handle the flat likelihood problem and incorporate prior information
  - Quick check question: If you observe a dataset where certain context-action pairs have no examples, what would a maximum likelihood estimate do versus a Bayesian estimate with informative priors?

- Concept: Contextual bandits and reinforcement learning
  - Why needed here: The paper reframes recommendation as a contextual bandit problem to enable direct reward optimization
  - Quick check question: What is the key difference between a contextual bandit (T=1) and a full reinforcement learning problem (T>1)?

- Concept: Variational inference and reparameterization trick
  - Why needed here: These techniques enable practical Bayesian inference with deep learning models
  - Quick check question: How does the local reparameterization trick make variational inference more efficient in deep learning contexts?

## Architecture Onboarding

- Component map: User context processing → Action space generation → Slate recommendation generation → Click/engagement tracking → Reward model training → Variational inference → Policy optimization
- Critical path: Context → Reward prediction → Slate selection → User interaction → Reward signal → Model update
- Design tradeoffs: Model complexity vs. inference speed, prior strength vs. data reliance, slate size vs. action space explosion
- Failure signatures: Poor cold-start performance (insufficient prior), high variance in recommendations (poor variational approximation), bias toward popular items (prior dominance)
- First 3 experiments:
  1. Implement simple contextual bandit with maximum likelihood estimation on synthetic data to reproduce the flat likelihood problem
  2. Add variational Bayesian inference with matrix normal priors to see if it handles the flat likelihood better
  3. Test the impact of different prior strengths on recommendation diversity and accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we quantitatively measure the improvement in reward optimization when using deep learning-based Bayesian contextual bandit approaches compared to current proxy-based methods?
- Basis in paper: [explicit] The paper proposes using deep learning tooling with variational Bayesian inference to build reward-optimizing recommender systems, but does not provide quantitative comparisons or benchmarks against existing proxy-based methods.
- Why unresolved: The paper presents a theoretical framework and discusses potential benefits but lacks empirical validation or quantitative evidence showing the actual performance gains of the proposed approach over current industry practices.
- What evidence would resolve it: Controlled A/B testing results comparing deep learning-based Bayesian contextual bandit approaches against proxy-based methods on real-world recommender systems, measuring improvements in key performance metrics like click-through rates, conversion rates, or user satisfaction scores.

### Open Question 2
- Question: What are the computational and infrastructure requirements for implementing deep learning-based Bayesian contextual bandit approaches at scale in production recommender systems?
- Basis in paper: [inferred] The paper suggests that deep learning tooling can now handle the computational challenges of Bayesian inference and high-dimensional interactions, but does not address practical implementation concerns or scalability issues.
- Why unresolved: The paper focuses on theoretical advantages of the proposed approach but does not discuss the practical challenges of implementing such systems at the scale required by major tech companies with millions of users and items.
- What evidence would resolve it: Case studies or pilot implementations of deep learning-based Bayesian contextual bandit approaches in production recommender systems, detailing computational resources, latency considerations, and infrastructure requirements.

### Open Question 3
- Question: How can we effectively incorporate prior information and embeddings to improve inference in high-dimensional context-action spaces while avoiding potential biases?
- Basis in paper: [explicit] The paper proposes using matrix normal priors and embeddings to incorporate prior information about similarities between contexts, actions, and context-action pairs, but does not discuss potential biases or limitations of this approach.
- Why unresolved: While the paper suggests this as a promising direction, it does not address how to select or validate these priors and embeddings, or how to avoid introducing biases that could negatively impact recommendation quality.
- What evidence would resolve it: Empirical studies comparing the performance of different prior and embedding strategies in various recommendation scenarios, along with methods for validating and mitigating potential biases in the learned models.

## Limitations
- The paper presents a theoretical framework without empirical validation on real-world datasets
- Claims about the superiority of direct reward optimization lack practical demonstration and may underestimate the value of proxy metrics
- The transition from proxy-based to direct reward optimization faces significant practical challenges in implementation and evaluation

## Confidence

- **High Confidence**: The critique of current "shooting in the dark" methodology and the identification of the proxy optimization problem is well-founded and widely recognized in the field.
- **Medium Confidence**: The theoretical formulation of recommendation as contextual bandit with Bayesian decision theory is sound, though practical implementation challenges are substantial.
- **Low Confidence**: Claims about the superiority of direct reward optimization over proxy-based methods lack empirical support and may underestimate the value of proxy metrics in practice.

## Next Checks

1. Implement the proposed Bayesian contextual bandit framework on a standard recommendation dataset (e.g., MovieLens) and compare performance against established proxy-based methods across multiple reward metrics.
2. Conduct a systematic ablation study varying the strength of Bayesian priors to understand their impact on cold-start performance versus data-driven learning.
3. Design and execute a small-scale online experiment to validate whether direct reward optimization translates to improved user outcomes compared to proxy-optimized systems in production settings.