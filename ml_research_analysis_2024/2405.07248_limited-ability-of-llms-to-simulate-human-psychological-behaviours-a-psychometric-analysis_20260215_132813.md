---
ver: rpa2
title: 'Limited Ability of LLMs to Simulate Human Psychological Behaviours: a Psychometric
  Analysis'
arxiv_id: '2405.07248'
source_url: https://arxiv.org/abs/2405.07248
tags: []
core_contribution: "This study psychometrically evaluated whether GPT-3.5 and GPT-4\
  \ can simulate human psychological responses using generic versus specific persona\
  \ descriptions. Using standardized personality measures (Big Five, PANAS, SSCS,\
  \ BPAQ), researchers found that GPT-4 with generic personas showed acceptable reliability\
  \ (\u03B1 \u2265 .70) and better internal consistency than GPT-3.5, but both models\
  \ exhibited higher intercorrelations between traits than human data."
---

# Limited Ability of LLMs to Simulate Human Psychological Behaviours: a Psychometric Analysis

## Quick Facts
- arXiv ID: 2405.07248
- Source URL: https://arxiv.org/abs/2405.07248
- Reference count: 40
- Key outcome: GPT-3.5 and GPT-4 show poor structural validity in simulating human psychological traits, with CFA failing to recover expected factor structures and GFI=.52, IFI=.64, RMSEA=.11 for Big Five with generic personas

## Executive Summary
This study psychometrically evaluated whether GPT-3.5 and GPT-4 can simulate human psychological responses using standardized personality measures. Researchers found that while GPT-4 with generic personas showed acceptable reliability (α ≥ .70) and better internal consistency than GPT-3.5, both models exhibited higher intercorrelations between traits than human data. Structural validity was poor: confirmatory factor analysis failed to recover expected factor structures, particularly with silicon personas. The study concludes that current LLMs poorly simulate individual-level human psychological traits.

## Method Summary
The study employed a comprehensive psychometric approach, administering four standardized personality measures (Big Five, PANAS, SSCS, BPAQ) to both human participants and LLM simulations. Researchers tested GPT-3.5 and GPT-4 using both generic personas (descriptions of a random person) and specific silicon personas (named individuals with detailed backgrounds). Each model completed the assessments twice, with prompts varying between personal experience and observed behavior contexts. The study analyzed reliability, factor structure, trait bias, and demographic correlations across all conditions.

## Key Results
- GPT-4 with generic personas achieved GFI=.52, IFI=.64, RMSEA=.11 in confirmatory factor analysis for the Big Five
- Both models showed higher intercorrelations between personality traits than human data, with silicon personas showing the highest correlations
- Trait bias was low (.63 GPT-3.5, .62 GPT-4) with no demographic correlations but negative associations with lower agreeableness, conscientiousness, extraversion, and higher neuroticism

## Why This Works (Mechanism)
Unknown: The paper does not provide a clear mechanism for why LLMs show poor structural validity in simulating human psychological traits. Possible factors could include the training data composition, the way models process personality-related prompts, or limitations in how LLMs represent psychological constructs.

## Foundational Learning
Assumption: The study builds on foundational psychometric research about personality structure and measurement, but does not explicitly discuss how these foundations apply to LLM simulations. The paper appears to assume that human-derived personality measures should be applicable to LLM outputs, without addressing potential differences in how humans and models might conceptualize personality.

## Architecture Onboarding
Unknown: The paper does not provide information about how the LLM architectures (GPT-3.5 and GPT-4) relate to the observed limitations in simulating psychological behaviors. No discussion of model architecture, training approaches, or how these might impact the ability to simulate human-like personality responses is provided.

## Open Questions the Paper Calls Out
Unknown: The paper does not explicitly call out specific open questions for future research. While it identifies limitations and suggests areas for further investigation, it does not pose direct questions that remain unanswered by the current study.

## Limitations
- Fixed system prompts may have constrained model responses, potentially underestimating true simulation capabilities
- Study did not systematically test impact of different prompting strategies or iterative refinement
- Analysis focused on self-report measures without validating against actual human behavioral outcomes

## Confidence
**High**: Poor structural validity and factor recovery (GFI=.52, IFI=.64, RMSEA=.11)
**Medium**: Acceptable reliability (α ≥ .70) for GPT-4 with generic personas
**Low**: Claims about limited simulation ability at individual level due to lack of sophisticated prompting approaches tested

## Next Checks
1. Test alternative prompting strategies (e.g., iterative refinement, chain-of-thought prompting) to determine if improved structural validity can be achieved
2. Validate model responses against actual human behavioral data rather than self-report measures alone
3. Conduct cross-cultural testing with diverse persona descriptions to assess generalizability of the findings