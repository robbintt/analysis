---
ver: rpa2
title: 'SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership
  Inference Attacks'
arxiv_id: '2403.08481'
source_url: https://arxiv.org/abs/2403.08481
tags:
- training
- privacy
- attacks
- inference
- membership
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper systematically evaluates the vulnerability of fine-tuned
  large language models to membership inference attacks and explores various mitigation
  strategies. The authors examine factors like model size, training iterations, batch
  size, and the application of differential privacy.
---

# SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks

## Quick Facts
- arXiv ID: 2403.08481
- Source URL: https://arxiv.org/abs/2403.08481
- Reference count: 40
- This paper systematically evaluates the vulnerability of fine-tuned large language models to membership inference attacks and explores various mitigation strategies.

## Executive Summary
This systematic study evaluates the vulnerability of fine-tuned large language models to membership inference attacks (MIA) and tests multiple mitigation strategies. The authors find that larger batch sizes, LoRA fine-tuning, and differential privacy methods (DP-SGD and DP-LoRA) significantly reduce MIA vulnerability while maintaining reasonable model accuracy. The research demonstrates that the choice of fine-tuning procedure has substantial impact on privacy leakage, with LoRA combined with smaller model sizes offering particularly strong protection. The study provides empirical evidence that common defense mechanisms can effectively reduce MIA success rates without completely sacrificing model performance.

## Method Summary
The paper evaluates MIA vulnerability across multiple LLMs (Roberta-base, Flan-t5-base) fine-tuned on binary classification datasets (Tweet Eval hate subset, Rotten Tomatoes reviews). The attack framework uses black-box access with multiple input features including loss, entropy, and logits. Various defenses are tested: pruning (unstructured, GAP), LoRA (rank 8), DP-SGD, DP-LoRA, prompt-tuning, and distillation. Models are fine-tuned using AdamW optimizer with linear learning rate scheduling, batch sizes of 64/128, and 10 epochs. MIA evaluation runs 20 times with 2000 samples each (1000 members, 1000 non-members), reporting average AUC-ROC, TPR@FPR1%, and accuracy metrics.

## Key Results
- Larger batch sizes during fine-tuning significantly reduce MIA vulnerability by averaging gradients across more samples
- LoRA fine-tuning mitigates attacks while maintaining accuracy by reducing trainable parameters
- DP-based methods (DP-SGD and DP-LoRA) offer the best privacy/accuracy tradeoff
- Model size reduction through pruning or using smaller architectures also provides MIA protection

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Larger batch sizes during fine-tuning reduce membership inference attack (MIA) success.
- Mechanism: When more samples' gradients are averaged in each update, the model receives less specific information about any single sample, reducing memorization and thus MIA vulnerability.
- Core assumption: The model's ability to memorize training samples is directly tied to the granularity of information it receives during gradient updates.
- Evidence anchors:
  - [abstract] "larger batch sizes reduce MIA vulnerability"
  - [section] "When more samples' gradients are averaged, the knowledge infused into the weights is more general, and contributes less accurate information about specific samples"
- Break condition: If batch size is so large that gradient updates become noisy or ineffective, or if the dataset size is very small relative to batch size, leading to insufficient gradient diversity.

### Mechanism 2
- Claim: LoRA fine-tuning significantly mitigates MIA while maintaining model accuracy.
- Mechanism: LoRA only trains a small fraction of parameters (adapter layers), reducing the model's capacity to memorize training samples compared to full fine-tuning.
- Core assumption: Model memorization of training data is proportional to the number of trainable parameters.
- Evidence anchors:
  - [abstract] "LoRA fine-tuning significantly mitigates attacks while maintaining accuracy"
  - [section] "LoRA diminishes the effective capacity of the model to memorize the training set"
- Break condition: If the adapter layers become too large or the original model weights are also updated, reducing the privacy benefit.

### Mechanism 3
- Claim: Differential privacy (DP) methods like DP-SGD and DP-LoRA offer the best privacy/accuracy tradeoff.
- Mechanism: DP methods add calibrated noise to gradients and limit training iterations, ensuring that individual training samples have minimal impact on the model, thus protecting against MIA.
- Core assumption: Theoretical DP guarantees translate to empirical resistance against MIA in practice.
- Evidence anchors:
  - [abstract] "DP-based methods like DP-SGD and DP-LoRA offer the best privacy/accuracy tradeoff"
  - [section] "DP guarantees that the participation of any individual data point does not have a significant effect on the algorithm outputs"
- Break condition: If the noise level is too high, degrading model accuracy significantly, or if the privacy budget (ε) is too large, weakening the privacy guarantee.

## Foundational Learning

- Concept: Membership Inference Attacks (MIAs)
  - Why needed here: Understanding how MIAs work is crucial to evaluating the effectiveness of defense strategies and interpreting experimental results.
  - Quick check question: What is the primary assumption behind most MIAs, and how do they typically operate?

- Concept: Differential Privacy (DP)
  - Why needed here: DP is a key defense mechanism evaluated in the paper, and understanding its theoretical foundations and practical implementation is essential for assessing its effectiveness.
  - Quick check question: How does DP-SGD ensure differential privacy during model training, and what are its key hyperparameters?

- Concept: Fine-tuning vs. Pre-training
  - Why needed here: The paper focuses on fine-tuning, which is distinct from pre-training and has different privacy implications due to the use of potentially sensitive proprietary data.
  - Quick check question: What is the difference between pre-training and fine-tuning, and why is fine-tuning a more appropriate target for MIA?

## Architecture Onboarding

- Component map:
  - Datasets (Tweet Eval, Rotten Tomatoes) -> Models (Roberta-base, Flan-t5-base) -> Defenses (Pruning, LoRA, DP-SGD, DP-LoRA) -> Attack Framework -> Evaluation Metrics

- Critical path:
  1. Fine-tune base models on datasets
  2. Apply defense strategies (if any)
  3. Perform MIA using the attack framework
  4. Evaluate model accuracy and MIA vulnerability
  5. Analyze results and draw conclusions

- Design tradeoffs:
  - Privacy vs. accuracy: Stronger defenses (e.g., DP) may reduce model accuracy
  - Computational cost: Some defenses (e.g., DP-SGD) are more computationally expensive
  - Implementation complexity: DP methods require careful hyperparameter tuning

- Failure signatures:
  - High MIA AUC-ROC and TPR@FPR1%: Indicates vulnerability to MIA
  - Significant drop in model accuracy: Indicates over-regularization or excessive noise
  - Inconsistent results across runs: May indicate instability in the attack framework or defense strategy

- First 3 experiments:
  1. Fine-tune Roberta-base on Tweet Eval without any defense and evaluate MIA vulnerability
  2. Apply LoRA fine-tuning to Roberta-base on Tweet Eval and compare MIA vulnerability to baseline
  3. Apply DP-SGD to Roberta-base on Tweet Eval with ε=2 and compare MIA vulnerability and accuracy to baseline and LoRA

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different combinations of DP hyper-parameters (gradient norm, epochs, noise multiplier) specifically affect MIA vulnerability at the same theoretical privacy budget?
- Basis in paper: [explicit] The paper explores different DP hyper-parameter combinations and finds that they lead to different MIA AUC-ROC scores even when the theoretical privacy budget (epsilon) is the same.
- Why unresolved: While the paper identifies that different combinations affect vulnerability, it doesn't provide a comprehensive analysis of which specific combinations offer the best protection against MIA.
- What evidence would resolve it: A systematic evaluation of all possible DP hyper-parameter combinations at the same epsilon value, measuring their impact on both MIA vulnerability and model accuracy.

### Open Question 2
- Question: What is the optimal batch size for balancing model accuracy and MIA protection across different model architectures and datasets?
- Basis in paper: [explicit] The paper finds that larger batch sizes provide good protection against MIA but doesn't explore the optimal trade-off point between batch size, accuracy, and privacy.
- Why unresolved: The relationship between batch size, accuracy, and privacy is complex and may vary depending on the specific model and dataset. The paper doesn't explore the full range of possible batch sizes or their effects on different architectures.
- What evidence would resolve it: A comprehensive study varying batch sizes across multiple model architectures and datasets, measuring both accuracy and MIA vulnerability at each point.

### Open Question 3
- Question: How does the vulnerability to MIA change as the size of the fine-tuning dataset increases, and is there a point of diminishing returns?
- Basis in paper: [explicit] The paper briefly examines the effect of dataset size on MIA vulnerability but doesn't explore this relationship in depth or identify any potential threshold effects.
- Why unresolved: The paper only tests a limited range of dataset sizes and doesn't analyze whether there's a point where increasing dataset size no longer significantly reduces MIA vulnerability.
- What evidence would resolve it: An extensive analysis testing a wide range of dataset sizes, identifying any non-linear relationships or threshold effects in the relationship between dataset size and MIA vulnerability.

## Limitations
- Experiments limited to binary classification tasks on specific datasets (Tweet Eval, Rotten Tomatoes)
- Only black-box MIA framework evaluated, not testing white-box attack scenarios
- No exploration of multi-defense combinations or adaptive adversary strategies
- Limited model architecture diversity (only Roberta and Flan-t5 tested)

## Confidence
**High Confidence Claims:**
- LoRA fine-tuning consistently reduces MIA vulnerability across datasets and models
- Larger batch sizes correlate with reduced MIA success
- DP-SGD and DP-LoRA provide the strongest privacy/accuracy tradeoff

**Medium Confidence Claims:**
- The relative effectiveness of pruning and distillation methods (based on limited ablation studies)
- Model size reduction as a privacy mechanism (based on only two model scales tested)
- The robustness of results across different attack hyperparameters (fixed attack parameters used)

**Low Confidence Claims:**
- Generalizability to non-classification tasks (no experiments beyond binary classification)
- Effectiveness against white-box attacks (only black-box framework evaluated)
- Long-term stability of defenses (no temporal analysis of attack effectiveness)

## Next Checks
1. **Cross-task validation:** Apply the evaluated defense strategies to sequence labeling or generative tasks (e.g., NER, summarization) to verify if the observed privacy/accuracy tradeoffs hold beyond binary classification.

2. **Adaptive attack testing:** Implement white-box MIA variants that leverage model parameter information and evaluate whether the top defenses maintain their effectiveness when adversaries can access model gradients or intermediate representations.

3. **Multi-defense interaction analysis:** Systematically test combinations of defenses (e.g., LoRA + DP-SGD, pruning + distillation) to identify potential synergies or degradations in privacy/accuracy tradeoffs that may not be apparent when defenses are evaluated in isolation.