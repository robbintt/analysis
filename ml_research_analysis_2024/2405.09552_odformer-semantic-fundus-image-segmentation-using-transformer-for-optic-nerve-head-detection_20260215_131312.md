---
ver: rpa2
title: 'ODFormer: Semantic Fundus Image Segmentation Using Transformer for Optic Nerve
  Head Detection'
arxiv_id: '2405.09552'
source_url: https://arxiv.org/abs/2405.09552
tags: []
core_contribution: This paper introduces ODFormer, a transformer-based semantic segmentation
  network designed for optic nerve head (ONH) detection in fundus images. The authors
  address the challenge of limited generalizability of ONH detection models due to
  the lack of diverse, multi-camera datasets.
---

# ODFormer: Semantic Fundus Image Segmentation Using Transformer for Optic Nerve Head Detection

## Quick Facts
- arXiv ID: 2405.09552
- Source URL: https://arxiv.org/abs/2405.09552
- Reference count: 35
- Key outcome: ODFormer achieves state-of-the-art performance on three fundus image datasets for optic nerve head detection, outperforming 13 baseline methods in IoU and F-score metrics.

## Executive Summary
This paper introduces ODFormer, a transformer-based semantic segmentation network designed specifically for optic nerve head (ONH) detection in fundus images. The authors address a critical challenge in medical imaging: limited generalizability of ONH detection models due to the lack of diverse, multi-camera datasets. To overcome this, they propose ODFormer with novel components including Multi-Scale Context Aggregator (MSCA) and Lightweight Bidirectional Feature Recalibrator (LBFR), and introduce TongjiU-DROD, a large-scale dataset captured using two different fundus cameras. Experimental results demonstrate that ODFormer outperforms existing state-of-the-art methods on three datasets, achieving superior performance and generalizability for ONH detection.

## Method Summary
ODFormer is a transformer-based semantic segmentation architecture that leverages a Swin Transformer backbone with several key innovations. The Multi-Scale Context Aggregator (MSCA) uses atrous convolutions with increasing dilation rates to extract multi-scale features without significantly increasing computational complexity. The Lightweight Bidirectional Feature Recalibrator (LBFR) recalibrates features before upsampling by extracting attention maps through spatial separable convolutions. The architecture also incorporates relative position bias maps to improve the self-attention mechanism by capturing local relationships within patches. The model is trained on three datasets including the newly introduced TongjiU-DROD, which contains 400 fundus images captured using two different cameras to improve generalizability.

## Key Results
- ODFormer achieves state-of-the-art performance on DRIONS-DB, DRISHTI-GS1, and TongjiU-DROD datasets
- Superior performance in both Intersection over Union (IoU) and F-score metrics compared to 13 baseline semantic segmentation methods
- Demonstrates improved generalizability across different fundus camera types through evaluation on multi-camera dataset

## Why This Works (Mechanism)

### Mechanism 1: Multi-Scale Context Aggregator (MSCA)
The MSCA improves feature extraction by incorporating multi-scale context through atrous convolutions with increasing dilation rates. This hierarchical structure expands the receptive field progressively, allowing the model to capture context at different scales simultaneously without significantly increasing computational complexity.

### Mechanism 2: Lightweight Bidirectional Feature Recalibrator (LBFR)
The LBFR improves segmentation accuracy by recalibrating features before upsampling. It extracts an attention map from encoder feature maps using spatial separable convolutions, which is then used to recalibrate and fuse the original encoder feature maps, adding relevant local information that might be lost during downsampling.

### Mechanism 3: Relative Position Bias Map
The relative position bias map improves the self-attention mechanism by incorporating local relationships and structural information within patches. An additional convolutional layer extracts this bias map, which is integrated into self-attention calculations to provide more comprehensive position and order information compared to absolute positional encoding.

## Foundational Learning

- **Multi-scale feature extraction**: Needed to capture ONH regions that vary in size and shape across different fundus images. Quick check: How does MSCA extract multi-scale features, and why is this important for ONH detection?
- **Self-attention mechanisms**: Crucial for capturing long-range dependencies and global context in fundus images. Quick check: How does the relative position bias map improve self-attention, and what is the benefit for ONH detection?
- **Semantic segmentation**: The ONH detection task requires classifying each pixel as ONH or background. Quick check: What role does LBFR play in ODFormer's decoder, and how does it contribute to segmentation performance?

## Architecture Onboarding

- **Component map**: Input: Fundus image (H×W×3) → MSCA → ODFormer Encoder → ODFormer Decoder → Output: Segmentation map (H×W×2)
- **Critical path**: MSCA → ODFormer Encoder → ODFormer Decoder → Output
- **Design tradeoffs**: Atrous convolutions enable multi-scale feature extraction without excessive computational cost but may cause checkerboard artifacts. Relative position bias improves local relationship capture but increases parameters. LBFR enhances local detail representation but adds complexity and potential overfitting risk.
- **Failure signatures**: Poor segmentation accuracy may indicate MSCA issues; inaccurate boundary delineation suggests LBFR problems; limited generalizability points to self-attention mechanism limitations.
- **First 3 experiments**: 1) Evaluate ODFormer with/without MSCA on TongjiU-DROD to assess multi-scale impact. 2) Compare ODFormer with different relative position bias configurations to find optimal design. 3) Assess LBFR effectiveness by comparing ODFormer with/without LBFR on DRIONS-DB and DRISHTI-GS1.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the text. However, based on the limitations identified in the paper, several implicit open questions emerge:

1. How does ODFormer's performance generalize to fundus images captured with camera types beyond the two used in TongjiU-DROD?
2. How does ODFormer scale to datasets with significantly larger numbers of fundus images than those evaluated?
3. Can ODFormer's techniques be effectively applied to other medical imaging tasks beyond ONH detection in fundus images?

## Limitations

- Key hyperparameters such as learning rate schedules, batch sizes, and training iterations are not specified, making exact reproduction difficult
- Validation is limited to only three datasets, with no explicit cross-camera testing protocols defined
- The TongjiU-DROD dataset, while larger than previous datasets, is still relatively modest in size for a "large-scale" dataset claim

## Confidence

- **High confidence**: Architectural innovations are clearly described and technically sound based on established transformer principles
- **Medium confidence**: Performance improvements over 13 SoTA methods are impressive but lack complete experimental details for independent verification
- **Low confidence**: Claims about superior generalizability across different fundus cameras are not fully substantiated by experimental evidence

## Next Checks

1. Conduct ablation studies on TongjiU-DROD by systematically removing MSCA, LBFR, and relative position bias components to quantify their individual contributions
2. Perform cross-camera validation by training on Zeiss CLARUS 500 images and testing exclusively on NES-1000P images (and vice versa)
3. Compare ODFormer's parameter efficiency and inference speed against the 13 SoTA methods, as only accuracy metrics are currently reported