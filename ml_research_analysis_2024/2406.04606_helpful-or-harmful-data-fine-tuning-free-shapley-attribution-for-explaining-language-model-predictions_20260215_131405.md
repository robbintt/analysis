---
ver: rpa2
title: Helpful or Harmful Data? Fine-tuning-free Shapley Attribution for Explaining
  Language Model Predictions
arxiv_id: '2406.04606'
source_url: https://arxiv.org/abs/2406.04606
tags: []
core_contribution: This paper addresses the challenge of robust instance attribution
  in fine-tuned language models, focusing on the sign consistency of instance scores
  across different resampled datasets. The authors propose a new notion of robustness
  based on the sign of instance scores and theoretically demonstrate that the Shapley
  value is more robust than leave-one-out (LOO) methods.
---

# Helpful or Harmful Data? Fine-tuning-free Shapley Attribution for Explaining Language Model Predictions

## Quick Facts
- arXiv ID: 2406.04606
- Source URL: https://arxiv.org/abs/2406.04606
- Reference count: 40
- Proposes FreeShap, an efficient fine-tuning-free Shapley value approximation for language model instance attribution

## Executive Summary
This paper addresses the challenge of robust instance attribution in fine-tuned language models by introducing a new notion of robustness based on sign consistency of instance scores across different resampled datasets. The authors demonstrate that Shapley values provide better robustness than leave-one-out methods and propose FreeShap, a fine-tuning-free approximation using neural tangent kernel regression to reduce computational costs. FreeShap is validated on both pre-trained LMs and LLMs, showing effectiveness in explaining model predictions and improving data quality across various data-centric applications.

## Method Summary
The authors introduce FreeShap, an efficient fine-tuning-free approximation of Shapley values for instance attribution in language models. Traditional Shapley value computation is computationally expensive, so they leverage neural tangent kernel regression to approximate the Shapley values without requiring fine-tuning. The approach focuses on sign consistency of instance scores across different resampled datasets as a measure of robustness, theoretically demonstrating that Shapley values are more robust than leave-one-out methods. FreeShap is applied to various data-centric tasks including data removal, data selection, and wrong label detection, showing superior performance compared to existing methods.

## Key Results
- FreeShap outperforms other methods in instance attribution accuracy and stability
- The method shows effectiveness across both pre-trained LMs and LLMs
- Superior performance in data-centric applications: data removal, data selection, and wrong label detection
- Demonstrates computational efficiency compared to exact Shapley value computation

## Why This Works (Mechanism)
FreeShap works by approximating Shapley values through neural tangent kernel regression, avoiding the need for expensive fine-tuning while maintaining attribution quality. The theoretical foundation rests on demonstrating that Shapley values provide better sign consistency across resampled datasets compared to leave-one-out methods, making them more robust for instance attribution. The neural tangent kernel captures the relationship between training instances and model predictions, enabling efficient computation of attribution scores that reflect each instance's contribution to model behavior.

## Foundational Learning
- Shapley value theory: Measures fair contribution of each instance to model predictions; needed for robust attribution, quick check: verify additivity property
- Neural tangent kernel: Captures model behavior during training; needed for efficient approximation, quick check: confirm NTK matrix invertibility
- Sign consistency: Robustness metric across resampled datasets; needed for reliable attribution, quick check: test sign preservation across 10 random samples
- Leave-one-out methods: Baseline attribution approach; needed for comparison, quick check: verify computational complexity scaling
- Fine-tuning-free attribution: Avoids expensive retraining; needed for practical deployment, quick check: measure runtime vs exact Shapley
- Instance attribution: Identifying influential training examples; needed for data quality improvement, quick check: validate attribution scores correlate with removal impact

## Architecture Onboarding

Component Map:
Data -> FreeShap computation -> Attribution scores -> Data-centric applications

Critical Path:
Input data → NTK regression → Shapley approximation → Attribution analysis → Application (removal/selection/detection)

Design Tradeoffs:
- Exact vs approximate Shapley: Computational efficiency vs precision
- Sign consistency vs magnitude: Robustness metric choice affects attribution quality
- Fine-tuning-free vs fine-tuned: Deployment practicality vs attribution accuracy
- Pre-trained vs fine-tuned models: Different attribution patterns and needs

Failure Signatures:
- Attribution scores that don't correlate with removal impact
- High variance in scores across different random seeds
- Computational bottlenecks in NTK matrix operations
- Poor performance on out-of-distribution data

First Experiments:
1. Validate sign consistency across 10 random data resamples
2. Compare FreeShap runtime vs exact Shapley computation
3. Test attribution quality on a simple synthetic dataset with known influential instances

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Experimental validation primarily focuses on classification tasks, leaving performance on other language modeling objectives uncertain
- Limited testing on extremely large language models (>100B parameters) raises questions about scalability claims
- The assumption that neural tangent kernel adequately approximates Shapley values across diverse model architectures needs further validation

## Confidence
- Theoretical robustness claims (High): Mathematical proofs and theoretical foundations appear sound
- FreeShap approximation quality (Medium): Empirically grounded but limited validation across diverse scenarios
- Data-centric application results (Medium): Promising results but potentially sensitive to experimental conditions
- Scalability claims (Low): Limited testing on very large models makes broad scalability claims uncertain

## Next Checks
1. Test FreeShap's attribution quality on non-classification tasks (e.g., text generation, summarization) to verify generalizability
2. Compare FreeShap's attribution stability across different model architectures (CNNs, transformers, etc.) to assess architecture dependence
3. Evaluate the computational overhead of FreeShap at extreme scales (>100B parameters) to verify scalability claims under real-world constraints