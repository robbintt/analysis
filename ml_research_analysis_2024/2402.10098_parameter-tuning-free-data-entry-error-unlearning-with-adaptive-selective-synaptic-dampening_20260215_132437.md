---
ver: rpa2
title: Parameter-tuning-free data entry error unlearning with adaptive selective synaptic
  dampening
arxiv_id: '2402.10098'
source_url: https://arxiv.org/abs/2402.10098
tags:
- unlearning
- data
- performance
- assd
- error
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces adaptive selective synaptic dampening (ASSD),
  a parameter-tuning-free extension to selective synaptic dampening for machine unlearning.
  The method automatically selects optimal dampening parameters based on dataset characteristics
  and the proportion of data to be forgotten, removing the need for manual parameter
  tuning.
---

# Parameter-tuning-free data entry error unlearning with adaptive selective synaptic dampening

## Quick Facts
- arXiv ID: 2402.10098
- Source URL: https://arxiv.org/abs/2402.10098
- Reference count: 4
- Parameter-tuning-free extension to selective synaptic dampening that automatically selects optimal dampening parameters based on dataset characteristics and proportion of data to be forgotten

## Executive Summary
This paper introduces Adaptive Selective Synaptic Dampening (ASSD), a parameter-tuning-free extension to Selective Synaptic Dampening (SSD) for machine unlearning. The method eliminates the need for manual hyperparameter search by automatically selecting optimal dampening parameters based on dataset characteristics and the proportion of data to be forgotten. ASSD is evaluated on CIFAR image classification benchmarks and a real-world supply chain delay prediction problem with synthetically introduced labeling errors, demonstrating performance comparable to SSD while removing the burden of parameter tuning.

## Method Summary
ASSD builds on the SSD framework by computing Fisher Information Matrices (FIM) for both the forget set and full dataset, then using a percentile-based approach to automatically determine the dampening parameter α. The method fixes λ = 1 and calculates α as the p-th percentile of relative FIM importances, where p is derived from the fraction of data to be forgotten. This adaptive selection eliminates manual hyperparameter tuning while maintaining the core unlearning mechanism of selectively dampening parameters most important for the data to be forgotten.

## Key Results
- ASSD achieves performance comparable to SSD on CIFAR benchmarks while eliminating hyperparameter search
- On supply chain delay prediction with synthetic labeling errors, ASSD outperforms fine-tuning across multiple neural network architectures
- ASSD improves model accuracy on both training and test data for error rates up to 5%
- The method demonstrates effectiveness across ResNet18 and Vision Transformer architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ASSD eliminates parameter tuning by setting λ = 1 and using an adaptive percentile selection for α.
- Mechanism: By fixing λ and dynamically choosing α based on the proportion of data to forget, the method avoids costly hyperparameter searches while maintaining performance.
- Core assumption: The importance of α far outweighs λ for SSD performance, and the Fisher Information Matrix (FIM) parameter importances follow a long-tail distribution.
- Evidence anchors:
  - [abstract] "automatically selects optimal dampening parameters based on dataset characteristics and the proportion of data to be forgotten"
  - [section] "Our assumption that the importance of α far outweighs λ... supported by the results shown in Fig. 2"
  - [corpus] No direct corpus evidence found; relies on internal empirical validation.
- Break condition: If the FIM importance distribution is not long-tailed, the percentile selection may fail, leading to either insufficient forgetting or excessive performance degradation.

### Mechanism 2
- Claim: Shared memorization parameters between similar images enable sub-linear scaling of parameter changes as the amount of data to forget increases.
- Mechanism: When multiple similar data points need to be forgotten, they likely share critical parameters in the FIM, so removing one reduces the need to adjust others, lowering the total number of parameter changes.
- Core assumption: Overlapping FIM importances exist for similar data points, allowing synergistic forgetting.
- Evidence anchors:
  - [section] "with an increasing number of samples to be forgotten, there will be some kind of overlap that can be used for more efficient forgetting"
  - [corpus] No direct corpus evidence found; this is an inference from the described behavior of the method.
- Break condition: If data points to be forgotten are highly dissimilar, no overlap in FIM importances occurs, and parameter changes scale linearly, undermining efficiency.

### Mechanism 3
- Claim: Adaptive percentile selection ensures the method works across diverse tasks and model sizes without manual tuning.
- Mechanism: By calculating α as the p-th percentile of relative FIM importances, where p is derived from the fraction of data to forget, the method adapts to each task's unique distribution.
- Core assumption: The FIM relative importance distribution for forget and retain data has a consistent relationship that can be captured by a percentile.
- Evidence anchors:
  - [section] "Our approach determines a viable α parameter using assumptions 2 and 3" and "Equation 1 then uses the calculated percentile p to determine α"
  - [corpus] No direct corpus evidence found; based on internal assumptions and experimental validation.
- Break condition: If the relationship between FIM importances and forgetting requirements varies drastically between tasks, the percentile method may not generalize well.

## Foundational Learning

- Concept: Fisher Information Matrix (FIM)
  - Why needed here: FIM is used to identify which model parameters are most important for retaining or forgetting specific data.
  - Quick check question: What does a high FIM value for a parameter indicate about its relationship to the data?

- Concept: Machine unlearning vs. model editing
  - Why needed here: Understanding the difference helps clarify why parameter tuning is avoided and why forgetting without knowing correct labels is important.
  - Quick check question: In what scenario would model editing be preferable to unlearning?

- Concept: Long-tail distributions in data
  - Why needed here: The method assumes data and parameter importance distributions follow a long-tail pattern, which is key to the adaptive selection logic.
  - Quick check question: Why is a long-tail distribution relevant to choosing α adaptively?

## Architecture Onboarding

- Component map:
  Input -> FIM computation -> Percentile calculation -> Parameter selection -> Selective dampening -> Unlearned model

- Critical path: Compute FIM for Df → Compute FIM for D → Sort relative importances → Calculate percentile p → Determine α → Apply selective dampening to parameters where importance exceeds threshold

- Design tradeoffs:
  - Fixing λ = 1 simplifies usage but may limit performance in edge cases.
  - Percentile-based α selection adapts to data but assumes consistent FIM distribution patterns.
  - No retraining step keeps it fast but risks accuracy loss with very high error rates.

- Failure signatures:
  - Accuracy drops sharply on retain data → α set too low (over-aggressive forgetting)
  - MIA remains high → α set too high (insufficient forgetting)
  - Performance varies wildly across tasks → percentile assumption invalid for that task

- First 3 experiments:
  1. Apply ASSD to a small ResNet18 trained on CIFAR10 with 1% random label errors; compare accuracy to baseline and fine-tuning.
  2. Test ASSD on CIFAR100 full-class forgetting for a rare class (e.g., rockets) and verify accuracy/MIA balance.
  3. Run ASSD on the supply chain dataset with 2.5% label errors; measure train/test accuracy improvement over fine-tuning.

## Open Questions the Paper Calls Out
None

## Limitations
- Performance may degrade with error rates ≥5% due to lack of repair steps after unlearning
- Relies heavily on long-tail FIM importance distributions which may not hold for all datasets
- Limited external validation beyond CIFAR and one real-world dataset raises generalizability concerns

## Confidence
- Core mechanism of parameter-free operation through adaptive percentile selection: High
- Performance comparable to SSD: Medium
- Scalability claim regarding overlapping FIM importances: Low

## Next Checks
1. Test ASSD on datasets with known non-long-tail FIM distributions (e.g., tabular data) to evaluate robustness when the core assumption fails.
2. Implement a grid search for λ on CIFAR100 to quantify the performance cost of fixing λ = 1.
3. Evaluate MIA performance across multiple attack methods to verify that unlearning effectiveness generalizes beyond the reported baseline.