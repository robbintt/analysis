---
ver: rpa2
title: A Multi-Graph Convolutional Neural Network Model for Short-Term Prediction
  of Turning Movements at Signalized Intersections
arxiv_id: '2406.00619'
source_url: https://arxiv.org/abs/2406.00619
tags:
- traffic
- data
- prediction
- graph
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces a multigraph convolutional neural network
  (MGCNN) for short-term prediction of turning movements at signalized intersections.
  The proposed architecture combines a multigraph structure to model temporal variations
  in traffic data with spectral convolution to capture spatial variations over the
  graphs.
---

# A Multi-Graph Convolutional Neural Network Model for Short-Term Prediction of Turning Movements at Signalized Intersections

## Quick Facts
- arXiv ID: 2406.00619
- Source URL: https://arxiv.org/abs/2406.00619
- Reference count: 38
- Primary result: MGCNN achieves MSE of 0.9 for 1-5 minute ahead turning movement prediction

## Executive Summary
This study introduces a multigraph convolutional neural network (MGCNN) for short-term prediction of turning movements at signalized intersections. The proposed architecture combines a multigraph structure to model temporal variations in traffic data with spectral convolution to capture spatial variations over the graphs. The model was tested on 20 days of flow and traffic control data from an arterial in Chattanooga, TN, with 10 signalized intersections. Results show that MGCNN outperforms four state-of-the-art baseline models in predicting turning movements while maintaining a simpler architecture that enables faster training times.

## Method Summary
The MGCNN model constructs multigraph sequences where each graph represents the transportation network state at a specific time step. A lookback window of 10 minutes is used to stack graphs over time, capturing temporal patterns. Spectral graph convolution with Chebyshev polynomial approximation is applied to extract spatial correlations between intersections. The model uses two graph convolution layers followed by a dropout layer and fully connected output layer. Training employs the Adam optimizer with a learning rate of 0.0007 and 35% dropout for 20 epochs.

## Key Results
- MGCNN achieves MSE of 0.9 across 1-5 minute prediction horizons
- Outperforms baseline models (T-GCN, DCRNN, GRU, GCN+RNN) in turning movement prediction accuracy
- Simpler architecture compared to hybrid models results in faster training times
- Model captures both spatial correlations and temporal patterns effectively

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The multigraph construction captures temporal variations by explicitly modeling the network state at each time step as a separate graph.
- **Mechanism**: By creating a graph ùí¢‚Çú for each time step t, the model encodes the state of the entire transportation network (intersections and links) at that moment. Stacking these graphs over a lookback window M forms a multigraph sequence that preserves temporal patterns.
- **Core assumption**: Traffic flow patterns are sufficiently captured by discrete snapshots of the network state at regular time intervals.
- **Evidence anchors**:
  - [abstract]: "The proposed architecture combines a multigraph structure, built to model temporal variations in traffic data..."
  - [section]: "At any time step t, the graph is denoted by ùí¢‚Çú = (ùí±,‚Ñ∞,ùëä‚Çú)... a set of graphs {ùí¢‚Çú‚Çã‚Çò,‚Ä¶..ùí¢‚Çú} is constructed to represent the transportation network/corridor state over time window t:t-M"
  - [corpus]: Weak. No corpus evidence directly discusses multigraph temporal modeling.
- **Break condition**: If traffic dynamics change faster than the time step resolution, the discrete snapshots will miss critical transient behaviors.

### Mechanism 2
- **Claim**: Spectral graph convolution with Chebyshev polynomial approximation efficiently captures spatial correlations in non-Euclidean traffic networks.
- **Mechanism**: The graph Laplacian ùêø‚Çú is decomposed and approximated using Chebyshev polynomials, allowing localized filtering that captures how traffic states at one intersection affect others over the network.
- **Core assumption**: The spatial dependencies in traffic flow can be represented as smooth functions over the graph topology.
- **Evidence anchors**:
  - [abstract]: "...with a spectral convolution operation to support modeling the spatial variations in traffic data over the graphs."
  - [section]: "The graph convolution then can be expressed as ùúô ‚àó ùëîùëß‚Çú ‚âà ‚àë ùúÉ‚Çñ ùêæ‚Çã‚ÇÅ‚Çñ‚Çå‚ÇÄ ùëá‚Çñ(ùêøÃÉ‚Çú)ùëß‚Çú..."
  - [corpus]: Weak. No corpus papers explicitly validate spectral convolution on Chebyshev polynomials for traffic flow.
- **Break condition**: If the underlying graph structure changes rapidly (e.g., due to incidents), the fixed Laplacian basis becomes less representative.

### Mechanism 3
- **Claim**: Eliminating RNN components avoids gradient issues and reduces computational overhead while maintaining prediction accuracy.
- **Mechanism**: By relying purely on convolution operations (both graph and temporal stacking) instead of iterative RNN steps, the model sidesteps vanishing/exploding gradient problems and reduces training time.
- **Core assumption**: Temporal dependencies in traffic data can be captured through convolution over stacked graphs without iterative recurrence.
- **Evidence anchors**:
  - [abstract]: "While previous GNN and multigraph approaches relied on RNN element to model temporal components, we get rid of this need of RNN element saving us from heavy computation..."
  - [section]: "Recurrent networks are well known to need iterative training... the gradient explosion or gradient vanish issue, affects the RNN..."
  - [corpus]: Weak. No corpus evidence directly compares RNN-free multigraph models to hybrid RNN-based models.
- **Break condition**: If traffic dynamics require very long-range temporal dependencies, convolution over finite lookback windows may be insufficient.

## Foundational Learning

- **Concept**: Graph Neural Networks (GNN) basics
  - **Why needed here**: Understanding how GNNs aggregate node information through message passing is essential to grasp how spatial correlations are modeled.
  - **Quick check question**: What is the role of the adjacency matrix in a GNN layer?

- **Concept**: Spectral graph theory and graph Laplacian
  - **Why needed here**: Spectral convolution relies on graph Laplacian decomposition; without this, the mechanism of spatial filtering cannot be understood.
  - **Quick check question**: How does the graph Laplacian encode the structure of a network?

- **Concept**: Chebyshev polynomial approximation for large-scale graphs
  - **Why needed here**: This approximation makes spectral convolution computationally feasible for large traffic networks.
  - **Quick check question**: Why is direct Laplacian decomposition expensive for large graphs?

## Architecture Onboarding

- **Component map**: Input layer ‚Üí Graph construction layer ‚Üí Multigraph stacking/fusion layer ‚Üí Spectral convolution layers ‚Üí Dropout layer ‚Üí Fully connected output layer

- **Critical path**: Data ‚Üí Multigraph construction ‚Üí Stacking ‚Üí Spectral convolution ‚Üí Fully connected ‚Üí Prediction

- **Design tradeoffs**:
  - Lookback window size vs. model complexity: Longer windows capture more temporal context but increase computation.
  - Chebyshev polynomial order (kernel size K) vs. localization: Higher order captures broader spatial dependencies but may overfit.
  - Dropout rate vs. overfitting: Higher dropout reduces overfitting but may underfit if too aggressive.

- **Failure signatures**:
  - High training loss but low validation loss: Model is underfitting; increase model capacity or reduce dropout.
  - Low training loss but high validation loss: Model is overfitting; increase dropout or add regularization.
  - Prediction lag behind ground truth: Lookback window too short or temporal resolution too coarse.

- **First 3 experiments**:
  1. **Lookback window sensitivity**: Train with lookback windows of 10, 20, 30, 40, 50, and 60 minutes; evaluate MSE on validation set.
  2. **Chebyshev polynomial order sensitivity**: Vary K (kernel size) from 2 to 5; compare spatial correlation capture and training time.
  3. **Comparison with RNN baseline**: Replace spectral convolution with GRU layers; measure training time and prediction accuracy to validate RNN elimination benefit.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the MGCNN model's performance scale with larger and more complex transportation networks containing hundreds of intersections rather than just ten?
- Basis in paper: [inferred] The study tested on a corridor with only 10 signalized intersections. The authors note that their multigraph approach could model "large-scale graphs" but do not empirically validate this claim with larger networks.
- Why unresolved: The current study uses a relatively small network, and scaling to real-world metropolitan networks with hundreds of intersections would require additional validation of computational efficiency and prediction accuracy.
- What evidence would resolve it: Testing the MGCNN on multiple corridors of increasing size (e.g., 20, 50, 100 intersections) while measuring MSE, training time, and memory usage would demonstrate scalability.

### Open Question 2
- Question: Can the MGCNN model effectively incorporate additional external factors such as weather conditions, special events, or incidents to improve prediction accuracy?
- Basis in paper: [inferred] The authors acknowledge that traffic flow is affected by "weather conditions" and other parameters, but the current model only uses traffic counts, speeds, and signal timing data without external variables.
- Why unresolved: The study focuses on baseline performance without external features, leaving uncertainty about whether incorporating such factors would significantly improve predictions in real-world conditions.
- What evidence would resolve it: Training and testing the MGCNN with additional weather, event, and incident data streams and comparing performance metrics (MSE, RMSE, MAE) against the baseline model would quantify the impact.

### Open Question 3
- Question: How robust is the MGCNN model to missing or corrupted data, which is common in real-world traffic sensor networks?
- Basis in paper: [explicit] The authors performed outlier detection and handling by replacing outliers with median values, but do not address scenarios with substantial missing data or sensor failures.
- Why unresolved: Real-world deployment would inevitably face data quality issues, and the current preprocessing steps may not be sufficient to maintain prediction accuracy under such conditions.
- What evidence would resolve it: Systematically removing varying percentages of data (e.g., 10%, 25%, 50%) from the training and testing sets and measuring prediction performance degradation would reveal the model's robustness to data quality issues.

## Limitations
- Tested only on a single arterial corridor with 10 intersections, limiting generalizability
- Chebyshev polynomial approximation parameters and baseline model hyperparameters not fully specified
- Does not address model performance under missing or corrupted data conditions

## Confidence

- **High confidence**: MGCNN achieves superior prediction accuracy compared to baseline models on the Chattanooga dataset
- **Medium confidence**: Spectral convolution effectively captures spatial correlations in traffic networks
- **Low confidence**: RNN elimination provides computational benefits without accuracy trade-offs

## Next Checks

1. Test MGCNN on multiple arterial corridors and network topologies to assess generalizability
2. Conduct ablation study comparing MGCNN with RNN-based multigraph variants under identical conditions
3. Validate model performance during incident conditions where graph structure changes rapidly