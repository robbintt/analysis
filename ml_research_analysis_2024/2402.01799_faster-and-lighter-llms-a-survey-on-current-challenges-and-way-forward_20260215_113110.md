---
ver: rpa2
title: 'Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward'
arxiv_id: '2402.01799'
source_url: https://arxiv.org/abs/2402.01799
tags: []
core_contribution: The paper presents a comprehensive survey of model compression
  and system-level optimization techniques for improving the inference efficiency
  of large language models (LLMs). It addresses the challenges posed by the substantial
  computational and memory requirements of LLMs during inference.
---

# Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward

## Quick Facts
- arXiv ID: 2402.01799
- Source URL: https://arxiv.org/abs/2402.01799
- Reference count: 10
- One-line primary result: Comprehensive survey and empirical evaluation of model compression and system-level optimization techniques for improving inference efficiency of large language models (LLMs), with focus on LLaMA(/2)-7B.

## Executive Summary
This survey provides a comprehensive overview of model compression and system-level optimization techniques aimed at improving the inference efficiency of large language models (LLMs). It addresses the substantial computational and memory requirements of LLMs during inference by examining various compression methods, including structured pruning, quantization, knowledge distillation, and low-rank approximations. The paper also explores system-level optimizations such as paged attention, tensor parallelism, and speculative decoding, which complement model compression by reducing runtime overhead and improving memory efficiency. Through empirical evaluation on LLaMA(/2)-7B, the authors highlight the trade-offs between model size, inference speed, and performance, offering practical insights for efficient LLM deployment. The survey identifies current limitations and discusses potential future directions to further enhance LLM inference efficiency.

## Method Summary
The paper presents a comprehensive survey of model compression and system-level optimization techniques for improving the inference efficiency of large language models (LLMs). It addresses the challenges posed by the substantial computational and memory requirements of LLMs during inference. The survey covers various compression methods, including structured pruning, quantization, knowledge distillation, and low-rank approximations, as well as system-level optimizations such as paged attention, tensor parallelism, and speculative decoding. Through experiments on LLaMA(/2)-7B, the authors evaluate the effectiveness of different compression techniques under a unified setting. The empirical analysis highlights the trade-offs between model size, inference speed, and performance, providing practical insights for efficient LLM deployment. The survey identifies current limitations and discusses potential future directions to further enhance LLM inference efficiency.

## Key Results
- Model compression techniques (pruning, quantization, distillation, low-rank approximations) effectively reduce model size and improve inference speed with minimal performance loss.
- System-level optimizations (paged attention, tensor parallelism, speculative decoding) complement model compression by improving runtime efficiency and memory utilization.
- Empirical evaluation on LLaMA(/2)-7B demonstrates trade-offs between model size, inference speed, and performance, highlighting the need for balanced optimization strategies.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: System-level optimizations complement model compression by reducing runtime overhead and improving memory efficiency.
- Mechanism: Techniques like paged attention, tensor parallelism, and speculative decoding restructure how inference is executed at the system level, enabling better hardware utilization and faster token generation without altering the model itself.
- Core assumption: The underlying model architecture remains unchanged, but the execution pipeline is optimized.
- Evidence anchors:
  - [abstract] The survey covers "system-level optimizations such as paged attention, tensor parallelism, and speculative decoding."
  - [section] "System level approaches... improve the complementary infrastructure and runtime architecture of LLMs."
  - [corpus] Weak evidence - no direct neighbor papers discuss these specific techniques.
- Break condition: If hardware or memory constraints prevent parallelization or caching strategies.

### Mechanism 2
- Claim: Low-rank approximations reduce model size and computational load by decomposing weight matrices into smaller, efficient components.
- Mechanism: Matrix factorization and tensor decomposition reduce parameter count and accelerate matrix multiplications, achieving compression with minimal performance loss.
- Core assumption: The original high-rank matrix can be accurately approximated by a low-rank representation.
- Evidence anchors:
  - [section] "Low-rank decomposition reduces the computational complexity of models by decomposing weight matrices into smaller ones with fewer dimensions."
  - [section] "TensorGPT [Xu et al., 2023] compressed the embedding layer of LLMs through Tensor-Train Decomposition."
  - [corpus] No direct evidence from neighbors on low-rank methods.
- Break condition: When the rank reduction is too aggressive, leading to significant accuracy degradation.

### Mechanism 3
- Claim: Knowledge distillation transfers capabilities from large teacher models to smaller student models, maintaining performance while reducing size.
- Mechanism: Student models mimic the teacher's outputs, intermediate representations, or learned relationships, allowing deployment in resource-constrained environments.
- Core assumption: The student model can effectively learn from the teacher's knowledge without requiring full-scale fine-tuning.
- Evidence anchors:
  - [section] "Knowledge distillation aims at training a computationally efficient model... to mimic the predictions of a larger and more complex model known as the teacher model."
  - [section] "Generalized KD [Agarwal et al., 2023] trains the student on its self-generated output sequences by leveraging feedback from the teacher."
  - [corpus] No direct evidence from neighbors on knowledge distillation techniques.
- Break condition: When the student model's capacity is insufficient to capture the teacher's knowledge, resulting in performance loss.

## Foundational Learning

- Concept: Matrix decomposition and low-rank approximations
  - Why needed here: Essential for understanding how low-rank methods reduce model complexity and accelerate inference.
  - Quick check question: How does Singular Value Decomposition (SVD) help in compressing neural network layers?

- Concept: Knowledge transfer and distillation techniques
  - Why needed here: Critical for grasping how smaller models can inherit the capabilities of larger ones without full fine-tuning.
  - Quick check question: What is the difference between response-based and feature-based knowledge distillation?

- Concept: Parallel computing and memory management
  - Why needed here: Important for understanding system-level optimizations like tensor parallelism and paged attention.
  - Quick check question: How does tensor parallelism differ from pipeline parallelism in distributed training/inference?

## Architecture Onboarding

- Component map:
  - Model compression layer: Pruning, quantization, distillation, low-rank approximations
  - System optimization layer: Paged attention, tensor parallelism, speculative decoding, fused operations
  - Runtime layer: Inference engines (TensorRT-LLM, vLLM, MLC-LLM, Llama.cpp, ExLlama)

- Critical path: Model compression → System-level optimization → Inference engine deployment

- Design tradeoffs:
  - Compression level vs. model performance
  - Memory efficiency vs. inference speed
  - Hardware compatibility vs. optimization effectiveness

- Failure signatures:
  - Significant performance drop after aggressive compression
  - Incompatibility with target hardware for system optimizations
  - Increased latency due to inefficient quant-dequant operations

- First 3 experiments:
  1. Apply structured pruning to LLaMA-7B and measure perplexity and token generation rate.
  2. Implement quantization-aware training and compare memory usage and inference speed.
  3. Deploy LLaMA-7B with TensorRT-LLM and measure performance improvements on NVIDIA GPUs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal balance between memory efficiency and computational speed when using lower-precision floating-point formats for LLM inference?
- Basis in paper: [explicit] The paper discusses the challenges of using lower-precision formats like FP4, which offer memory efficiency but can slow down inference due to Quantization-Dequantization overhead.
- Why unresolved: The paper highlights the need to strike a balance between memory efficiency and computational speed but does not provide a definitive solution or optimal configuration.
- What evidence would resolve it: Empirical studies comparing different precision formats (e.g., FP4, FP8, INT8) across various hardware platforms, measuring both memory usage and inference speed, would help determine the optimal balance.

### Open Question 2
- Question: How can localized distillation methods be effectively developed to compress LLMs without the computational burden of full-scale distillation?
- Basis in paper: [explicit] The paper suggests that localized distillation, which involves learning localized parts of the teacher network in smaller-scale student sub-networks, could be a potential solution to the computational challenges of LLM distillation.
- Why unresolved: The paper does not provide a concrete method or framework for implementing localized distillation in LLMs.
- What evidence would resolve it: Development and experimental validation of localized distillation methods for LLMs, demonstrating their effectiveness in reducing computational cost while maintaining model performance.

### Open Question 3
- Question: What strategies can be employed to determine the optimal rank for layerwise low-rank approximation in LLM compression?
- Basis in paper: [explicit] The paper discusses the challenges of determining the optimal rank for low-rank approximation in LLMs, noting that it is not easily addressed through hyperparameter search due to computational expense.
- Why unresolved: The paper identifies the difficulty of rank selection but does not propose a specific strategy or method for determining the optimal rank.
- What evidence would resolve it: Development and evaluation of strategies for rank selection in low-rank approximation, such as adaptive methods or techniques that leverage the characteristics of different layers, would provide insights into effective approaches.

## Limitations
- The empirical validation is limited to a single model (LLaMA-7B) and a specific hardware setup (Nvidia A100 40GB), which may not generalize to other architectures or resource constraints.
- The survey does not address the practical deployment challenges of combining multiple optimization techniques or the long-term stability of compressed models.
- The paper references multiple compression techniques but provides limited quantitative comparisons across different methods under identical conditions.

## Confidence
- High Confidence: The survey's comprehensive coverage of existing literature on LLM compression and system optimizations is well-supported by citations and technical descriptions.
- Medium Confidence: The empirical results demonstrating the effectiveness of individual compression techniques are plausible but limited in scope and generalizability.
- Low Confidence: The paper's discussion of future directions and potential limitations of current approaches is speculative and lacks quantitative backing.

## Next Checks
1. Apply the same compression and optimization pipeline to different LLM architectures (e.g., OPT, BLOOM) to assess generalizability of reported performance gains.
2. Systematically evaluate combinations of compression methods (e.g., pruning + quantization) to identify potential synergies or conflicts in their effects on inference efficiency.
3. Monitor the performance and accuracy of compressed models over extended inference sessions to detect any degradation or numerical instability that may emerge during prolonged use.