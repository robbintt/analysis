---
ver: rpa2
title: Multi-Review Fusion-in-Context
arxiv_id: '2403.15351'
source_url: https://arxiv.org/abs/2403.15351
tags:
- summary
- highlights
- review
- faithfulness
- coverage
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends the Controlled Text Reduction (CTR) task from
  (Slobodkin et al., 2022) to a multi-document setting, formalizing Fusion-in-Context
  (FiC) as a standalone task. The task takes as input a set of documents with pre-selected
  highlights, and requires generating a coherent text that includes all and only the
  highlighted content.
---

# Multi-Review Fusion-in-Context

## Quick Facts
- arXiv ID: 2403.15351
- Source URL: https://arxiv.org/abs/2403.15351
- Authors: Aviv Slobodkin; Ori Shapira; Ran Levy; Ido Dagan
- Reference count: 36
- Primary result: This work extends the Controlled Text Reduction (CTR) task from (Slobodkin et al., 2022) to a multi-document setting, formalizing Fusion-in-Context (FiC) as a standalone task.

## Executive Summary
This paper formalizes the Fusion-in-Context (FiC) task as a multi-document text generation problem where models must generate coherent text containing all and only the highlighted content from a set of documents. The authors construct a dataset of 1000 instances in the reviews domain using controlled crowdsourcing and develop an evaluation framework that assesses faithfulness and coverage of highlights. Several baseline models are explored, with a finetuned Flan-T5-only-H model achieving the best results. The study demonstrates that context is crucial for coherence while highlights significantly improve model performance, laying the groundwork for modular text generation in multi-document settings.

## Method Summary
The paper constructs the FUSE REVIEWS dataset of 1000 instances through controlled crowdsourcing, leveraging existing multi-document summarization datasets. Each instance contains 8 reviews on average with pre-selected highlights and corresponding fused summaries. The evaluation framework uses an NLI-based metric for faithfulness (measuring whether generated text is entailed by highlights) and a trained metric for coverage (measuring whether all highlights are represented). Several baseline models are explored including fine-tuned Flan-T5 variants with different input configurations (with/without highlights, with/without context) and a GPT-4 model. The best performing model is a fine-tuned Flan-T5-only-H model that achieves strong faithfulness and coverage scores.

## Key Results
- A finetuned Flan-T5-only-H model achieved the best results (faithfulness: 84.6, coverage: 87.8, F-1: 86.2)
- Excluding surrounding context from the input yields the strongest faithfulness and coverage scores
- Highlights significantly improve model performance compared to models without highlight access
- The NLI-based faithfulness metric and trained coverage metric strongly correlate to human judgment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The NLI-based faithfulness metric correlates strongly with human judgment because it evaluates entailment at the sentence level, which aligns with how humans judge faithfulness.
- Mechanism: By breaking down the generated passage into sentences and comparing each against the concatenated highlights, the metric captures whether each sentence is fully supported by the highlighted content. This granular evaluation matches human assessment patterns.
- Core assumption: Human faithfulness judgment operates primarily at the sentence level, evaluating whether each statement can be inferred from the source highlights.
- Evidence anchors: [abstract] "novel evaluation framework for assessing the faithfulness and coverage of highlights, which strongly correlate to human judgment"; [section] "We employ the flan-t5-xxl model (Chung et al., 2022), shown to exhibit high performance on NLI tasks, for evaluating faithfulness to highlights in a zero-shot setting with a standard NLI prompt"
- Break condition: If the highlights contain information that requires aggregation across multiple sentences to evaluate faithfulness, the sentence-level approach may miss the holistic entailment relationship.

### Mechanism 2
- Claim: Training a dedicated coverage model using synthesized data from the FiC dataset provides better coverage evaluation than lexical matching methods.
- Mechanism: The trained model learns to recognize when individual highlights are represented in the output, whether explicitly or through generalization/aggregation. This captures semantic coverage beyond surface-level n-gram matching.
- Core assumption: Coverage evaluation requires understanding semantic equivalence and aggregation, not just lexical overlap.
- Evidence anchors: [abstract] "evaluation framework for assessing the faithfulness and coverage of highlights"; [section] "we finetune an LLM that is tasked to assess whether the generated passage fully covers the highlights"
- Break condition: If the training data doesn't capture the full range of how highlights can be represented (through explicit mention, generalization, or aggregation), the model may miss valid coverage instances.

### Mechanism 3
- Claim: Excluding surrounding context from the input (Flan-T5only-H) yields the strongest faithfulness and coverage scores because the model focuses solely on the highlighted content without being distracted by irrelevant information.
- Mechanism: When the model only sees the highlights, it has no choice but to generate content strictly from those highlights, resulting in higher faithfulness. The coverage score is also higher because the model attempts to cover all highlighted content.
- Core assumption: Context information in the source documents often contains information that can lead the model astray from the highlights, reducing faithfulness and coverage scores.
- Evidence anchors: [section] "The exclusion of context from the input (Flan-T5only-H) yields the strongest faithfulness and coverage scores"; [section] "Meanwhile, the removal of highlights (Flan-T5no-H) leads to a substantial degradation in faithfulness and coverage"
- Break condition: If the context provides necessary information for coherent generation that isn't captured in the highlights, excluding it may lead to incoherent outputs despite high faithfulness scores.

## Foundational Learning

- Concept: Natural Language Inference (NLI) and entailment relationships
  - Why needed here: The evaluation framework relies on NLI models to determine if generated text is entailed by the highlights, which is fundamental to measuring faithfulness
  - Quick check question: Can you explain the difference between entailment, contradiction, and neutral relationships in NLI?

- Concept: Data synthesis for model training
  - Why needed here: The coverage evaluation model is trained on synthesized data derived from the FiC dataset, requiring understanding of how to create effective training examples
  - Quick check question: How would you create positive and negative examples for a coverage evaluation model from existing data?

- Concept: Reinforcement Learning with custom reward functions
  - Why needed here: One of the baseline models uses RL with faithfulness and coverage metrics as rewards, requiring understanding of how to implement and tune such approaches
  - Quick check question: What are the potential pitfalls of using the same metrics for both training and evaluation in RL?

## Architecture Onboarding

- Component map: Dataset collection -> Baseline model training -> Generation -> NLI-based faithfulness evaluation -> Trained coverage evaluation -> Manual coherence/redundancy evaluation
- Critical path: Data collection → Model training → Generation → Evaluation → Analysis. The most critical sequence is ensuring the evaluation metrics are reliable before drawing conclusions about model performance.
- Design tradeoffs: The choice between lexical matching (ROUGE, BERTScore) and learned metrics (NLI, trained coverage) involves a tradeoff between interpretability and semantic understanding. Similarly, including context in the input trades faithfulness for coherence.
- Failure signatures: Low faithfulness scores despite high coverage might indicate the model is generating plausible content not in the highlights. High coherence but low faithfulness suggests the model is relying too heavily on context rather than the provided highlights.
- First 3 experiments:
  1. Run Flan-T5H with different input formats (highlights only, full context, both) to verify the context tradeoff hypothesis
  2. Test the NLI-based faithfulness metric on a small sample with human evaluation to confirm correlation
  3. Compare the trained coverage model against simple ROUGE-based coverage to quantify the benefit of semantic understanding

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the Fusion-in-Context (FiC) task vary across different domains, such as news articles versus business reviews?
- Basis in paper: [inferred] The paper mentions that the dataset is focused on the business reviews domain and notes that this may constrain generalizability to other contexts with distinct textual structures, like news articles.
- Why unresolved: The paper only evaluates the FiC task on business reviews, and does not explore its applicability or performance in other domains.
- What evidence would resolve it: Testing the FiC models on datasets from different domains, such as news articles, and comparing the performance metrics (faithfulness, coverage, coherence, redundancy) across these domains.

### Open Question 2
- Question: How effective are the proposed evaluation metrics (NLI-based and trained metrics) for faithfulness and coverage in assessing the quality of generated text in multi-document settings?
- Basis in paper: [explicit] The paper proposes NLI-based and trained metrics for evaluating faithfulness and coverage, and correlates these metrics to human judgment.
- Why unresolved: While the paper shows that these metrics correlate well with human judgment, it does not explore their effectiveness in other multi-document settings or compare them to other evaluation methods.
- What evidence would resolve it: Conducting experiments to compare the proposed metrics with other evaluation methods in various multi-document settings and analyzing their performance and reliability.

### Open Question 3
- Question: What are the potential benefits and limitations of using the FiC task for attributed generation, where the pre-selected segments serve as supporting cited content for the fused text?
- Basis in paper: [explicit] The paper discusses the potential for using FiC in attributed generation, where the pre-selected segments (highlights) also serve as supporting cited content for the generated text.
- Why unresolved: The paper does not explore the practical implementation or effectiveness of using FiC for attributed generation, nor does it discuss potential challenges or limitations.
- What evidence would resolve it: Implementing and testing the FiC task in scenarios that require attributed generation, and analyzing the accuracy and reliability of the citations and the overall quality of the generated text.

## Limitations
- The evaluation framework relies heavily on learned metrics without extensive human validation across the full dataset
- The dataset construction process lacks details on inter-annotator agreement and potential biases in highlight selection
- The absolute performance numbers of baseline models lack comparison to strong contemporary baselines

## Confidence

**High Confidence:**
- The task formalization as a standalone problem (FiC) with clear evaluation metrics
- The finding that context exclusion (Flan-T5-only-H) improves faithfulness and coverage scores
- The overall finding that highlights improve model performance compared to models without highlight access

**Medium Confidence:**
- The correlation between learned evaluation metrics and human judgment, based on limited manual evaluation of 100 instances
- The effectiveness of the trained coverage model compared to lexical matching baselines
- The generalization potential of the evaluation framework to other domains beyond reviews

**Low Confidence:**
- The absolute performance numbers of the baseline models without comparison to strong contemporary baselines
- The long-term reliability of the learned evaluation metrics without periodic human recalibration
- The dataset's representativeness for all multi-document fusion scenarios, given its restriction to the reviews domain

## Next Checks

1. **Human Validation of Evaluation Metrics**: Conduct comprehensive human evaluation (minimum 200 instances) comparing the NLI-based faithfulness and trained coverage metrics against human judgments to validate the claimed correlation and establish confidence intervals for the metrics.

2. **Cross-Domain Generalization Test**: Apply the FiC task formulation and evaluation framework to a different domain (such as scientific papers or news articles) to assess whether the same patterns in model performance and metric reliability hold across domains.

3. **Robustness Analysis of Synthetic Training Data**: Perform ablation studies on the coverage evaluation model by varying the synthetic training data generation process (different summarization models, different negative sampling strategies) to determine the sensitivity of the learned coverage metric to training data quality.