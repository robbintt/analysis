---
ver: rpa2
title: 'Think out Loud: Emotion Deducing Explanation in Dialogues'
arxiv_id: '2406.04758'
source_url: https://arxiv.org/abs/2406.04758
tags: []
core_contribution: This paper introduces a new task, "Emotion Deducing Explanation
  in Dialogues" (EDEN), which requires models to recognize emotion and its causes
  in an explicitly thinking way by generating an explanation text. The task involves
  summarizing causes, analyzing speakers' inner activities using common sense, and
  deducing the emotion.
---

# Think out Loud: Emotion Deducing Explanation in Dialogues

## Quick Facts
- **arXiv ID:** 2406.04758
- **Source URL:** https://arxiv.org/abs/2406.04758
- **Reference count:** 40
- **Primary result:** EDEN-LLaMA achieves best performance in reasonableness and outperforms SOTA in emotion/cause recognition

## Executive Summary
This paper introduces the Emotion Deducing Explanation in Dialogues (EDEN) task, which requires models to generate explanations that deduce emotion and its causes through reasoning. The task goes beyond classification by requiring models to summarize causes, analyze speakers' inner activities using common sense, and deduce the emotion. The authors construct two datasets (EDEN-DD and EDEN-FR) through human effort assisted by ChatGPT, and evaluate various models including PLMs and LLMs. Results show that LLMs, particularly fine-tuned LLaMA models, outperform conventional PLMs and demonstrate stronger reasoning capabilities.

## Method Summary
The EDEN task requires models to generate an explanation text that first summarizes the causes of emotion in a dialogue, analyzes the inner activities of speakers triggered by these causes using common sense, and then deduces the emotion. The datasets (EDEN-DD with 5,331 samples and EDEN-FR with 6,741 samples) are constructed from existing ECED resources with human annotation assisted by ChatGPT. Models are evaluated using automatic metrics (BLEU, ROUGE-L, METEOR, CIDEr), emotion and cause recognition F1 scores, and a GPT-4-based reasonableness scoring system.

## Key Results
- LLMs significantly outperform PLMs on the EDEN task, demonstrating superior reasoning capabilities
- EDEN-LLaMA (fine-tuned LLaMA models) achieves the best performance in terms of reasonableness scores
- EDEN can serve as a substitute for emotion and cause recognition tasks while providing additional reasoning insights
- PLMs like BART and GPT2 struggle with EDEN due to their reliance on co-occurrence features rather than genuine reasoning

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** EDEN task structure explicitly models emotion deduction process, activating reasoning ability of LLMs beyond classification.
- **Mechanism:** Instead of only predicting emotion, models must generate explanations that summarize causes, analyze speaker's inner activities, and deduce emotion. This forces compositional and multi-hop reasoning.
- **Core assumption:** LLMs can be activated to perform explicit reasoning when task requires explanation generation rather than classification.
- **Evidence anchors:** [abstract] "models need to generate an explanation text, which first summarizes the causes; analyzes the inner activities of the speakers triggered by the causes using common sense; then guesses the emotion accordingly."

### Mechanism 2
- **Claim:** Human-curated data with ChatGPT-assisted annotation provides high-quality explanations that serve as reasoning exemplars.
- **Mechanism:** ChatGPT generates initial analysis which humans refine, creating high-quality training data with diverse reasoning patterns. This bootstrapping enables better reasoning.
- **Core assumption:** ChatGPT's initial analysis is useful as starting point for human annotators, reducing annotation burden while maintaining quality.
- **Evidence anchors:** [section 3.2] "To assist human annotators, we also utilize ChatGPT to generate the topic of the dialogue, which can help annotators quickly understand the dialogue."

### Mechanism 3
- **Claim:** EDEN framework improves both emotion and cause recognition by forcing models to consider mutual indication between them.
- **Mechanism:** The explanation generation requires identifying causes and explaining how they lead to emotion, creating stronger coupling between cause and emotion representations.
- **Core assumption:** Understanding causes improves emotion recognition because emotions are better understood in context of their triggers.
- **Evidence anchors:** [abstract] "EDEN recognizes emotion and causes in an explicitly thinking way... This thinking process especially reflected in the reasoning ability of Large Language Models (LLMs) is under-explored."

## Foundational Learning

- **Concept: Chain-of-Thought reasoning**
  - Why needed here: EDEN requires models to generate explanations following a logical chain from causes to emotion, similar to CoT prompting.
  - Quick check question: How does requiring explicit reasoning steps in explanations differ from direct emotion classification?

- **Concept: Commonsense reasoning in dialogues**
  - Why needed here: EDEN requires analyzing speakers' inner activities and mental states based on social commonsense, which is crucial for understanding emotion causes.
  - Quick check question: Why is social commonsense particularly important for emotion cause extraction compared to other NLP tasks?

- **Concept: Emotion-valence relationships**
  - Why needed here: The paper notes confusion between negative emotions and between surprise and other emotions, suggesting understanding valence relationships is important.
  - Quick check question: How might emotion valence affect the difficulty of distinguishing between similar emotions?

## Architecture Onboarding

- **Component map:** Dialogue context → Identify causal utterances → Generate explanation with emotion deduction → Extract emotion and causes from explanation → Evaluate reasonableness

- **Critical path:** Input dialogue context → Identify causal utterances → Generate explanation with emotion deduction → Extract emotion and causes from explanation → Evaluate reasonableness

- **Design tradeoffs:**
  - Human annotation vs. automated generation: Human annotation ensures quality but is expensive; ChatGPT assistance reduces burden but may introduce noise
  - Task complexity: EDEN is more complex than classification but potentially more informative
  - Model choice: Fine-tuning LLMs vs. in-context learning - trade-off between performance and flexibility

- **Failure signatures:**
  - Low reasonableness scores despite good automatic metrics indicate models are not genuinely reasoning
  - Confusion between similar emotions suggests reasoning limitations
  - Performance drop when directly generating explanations (without emotion list) indicates reliance on task structure

- **First 3 experiments:**
  1. Compare EDEN-LLaMA performance with and without commonsense pretraining to assess benefit of external knowledge
  2. Test different prompt structures for EDEN-LLaMA to optimize reasoning activation
  3. Evaluate zero-shot vs. few-shot performance of ChatGPT to understand demonstration sensitivity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal window size for considering causal utterances in dialogues for emotion understanding?
- Basis in paper: [explicit] The paper mentions using a history context window of 15 utterances, but notes that most causal utterances are located near the target utterance and less than 15 turns away.
- Why unresolved: The paper does not explore different window sizes or their impact on model performance.
- What evidence would resolve it: Experiments comparing different window sizes (e.g., 5, 10, 15, 20) and their effects on emotion and cause recognition accuracy.

### Open Question 2
- Question: How does the performance of EDEN compare to other emotion understanding tasks (ERD, ECED, ECTEC) when evaluated on the same datasets?
- Basis in paper: [explicit] The paper states that EDEN can be a substitution for these classification tasks, but does not provide direct comparisons.
- Why unresolved: The paper focuses on EDEN's performance and its potential as a replacement, but does not directly compare it to the existing tasks.
- What evidence would resolve it: A comprehensive evaluation of EDEN alongside ERD, ECED, and ECTEC models on the same datasets, measuring performance in terms of emotion and cause recognition accuracy.

### Open Question 3
- Question: What is the impact of different prompt formats and instructions on the performance of EDEN-LLaMA?
- Basis in paper: [explicit] The paper explores different prompt formats (full, speaker-utterance, speaker-turn, speaker, emotion, none) and their effects on EDEN-LLaMA's performance.
- Why unresolved: The paper only provides results for one variant of EDEN-LLaMA and does not explore the impact of different prompt formats on its performance.
- What evidence would resolve it: Experiments comparing the performance of EDEN-LLaMA using different prompt formats and instructions on the same datasets, measuring performance in terms of emotion and cause recognition accuracy.

## Limitations
- **Dataset scale:** The EDEN datasets are relatively small (5,331 and 6,741 samples), which may limit generalizability
- **Annotation bias:** Reliance on ChatGPT for initial annotation assistance may introduce systematic bias in training data
- **Evaluation constraints:** GPT-4-based reasonableness scoring may not fully capture human judgment of reasoning quality

## Confidence

- **High Confidence:** The finding that LLMs outperform PLMs on the EDEN task is well-supported by the empirical results and aligns with established knowledge about LLM capabilities in reasoning tasks.
- **Medium Confidence:** The claim that EDEN can serve as a substitute for emotion and cause recognition tasks is supported by performance comparisons, but the practical equivalence needs further validation on independent benchmarks.
- **Medium Confidence:** The assertion that EDEN activates reasoning abilities in LLMs is plausible given the task structure, but the specific mechanisms and conditions for this activation require further investigation.

## Next Checks

1. **External Validation:** Test EDEN-LLaMA on established emotion and cause recognition benchmarks (e.g., ERC, ECED) to verify whether the reasoning capabilities generalize beyond the EDEN datasets.

2. **Scale Sensitivity Analysis:** Evaluate model performance as a function of dataset size to determine the minimum effective training set and assess whether the current scale is sufficient for robust reasoning.

3. **Human Evaluation Validation:** Conduct comprehensive human evaluation of model-generated explanations to validate the GPT-4 reasonableness scores and assess whether they align with human judgments of reasoning quality.