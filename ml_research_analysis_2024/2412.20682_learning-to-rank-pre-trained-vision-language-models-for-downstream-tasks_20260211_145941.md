---
ver: rpa2
title: Learning to Rank Pre-trained Vision-Language Models for Downstream Tasks
arxiv_id: '2412.20682'
source_url: https://arxiv.org/abs/2412.20682
tags:
- performance
- vega
- downstream
- class
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of selecting the best-performing
  pre-trained vision-language model (VLM) for a given unlabeled downstream task without
  relying on additional supervised datasets or large language models. The proposed
  method, Visual-tExtual Graph Alignment (VEGA), constructs fully connected graphs
  for visual and textual modalities based on the feature distributions of class names
  and unlabeled images.
---

# Learning to Rank Pre-trained Vision-Language Models for Downstream Tasks

## Quick Facts
- arXiv ID: 2412.20682
- Source URL: https://arxiv.org/abs/2412.20682
- Authors: Yuhe Ding; Bo Jiang; Aihua Zheng; Qin Xu; Jian Liang
- Reference count: 40
- Key outcome: VEGA achieves state-of-the-art VLM selection with Top-5 recall 0.64, Kendall correlation 0.62, and Top-1 accuracy 0.66

## Executive Summary
This paper introduces Visual-tExtual Graph Alignment (VEGA), a method for ranking pre-trained vision-language models (VLMs) for downstream tasks without requiring additional supervised data or large language models. The core innovation lies in constructing fully connected graphs for visual and textual modalities based on feature distributions of class names and unlabeled images, then computing alignment between these graphs at both node and edge levels to predict zero-shot classification performance. The approach demonstrates strong empirical results across three benchmarks, achieving state-of-the-art performance in unsupervised VLM selection.

## Method Summary
VEGA operates by first constructing fully connected graphs for both visual and textual modalities using feature distributions from class names and unlabeled images. The method then computes graph alignment at two levels: node-level alignment that measures semantic similarity between corresponding nodes, and edge-level alignment that captures the consistency of relationships between nodes. This dual-level alignment score is used to rank pre-trained VLMs for downstream tasks. The key insight is that better-aligned visual-textual graphs correlate with superior zero-shot classification performance, enabling effective model selection without task-specific supervision.

## Key Results
- Achieves state-of-the-art results with average Top-5 recall of 0.64
- Demonstrates Kendall correlation of 0.62 for ranking accuracy
- Achieves Top-1 accuracy of 0.66 in VLM selection
- Outperforms existing unsupervised VLM selection methods across three benchmark datasets

## Why This Works (Mechanism)
The method leverages the fundamental principle that effective vision-language models must establish strong semantic connections between visual features and textual concepts. By constructing graphs that represent the feature distributions of both modalities and measuring their alignment, VEGA captures the model's ability to map visual information to linguistic concepts. The node-level alignment ensures that individual visual and textual elements are properly matched, while edge-level alignment verifies that the relationships between elements are consistent across modalities. This dual-level approach provides a comprehensive measure of cross-modal understanding that correlates strongly with downstream task performance.

## Foundational Learning
- **Graph construction for visual and textual features**: Creating fully connected graphs from feature distributions is essential for capturing the complete relationship structure between modalities. Quick check: Verify that graph construction maintains computational efficiency as feature dimensionality increases.
- **Cross-modal alignment metrics**: Node and edge-level alignment measures are needed to quantify the semantic consistency between vision and language representations. Quick check: Test alignment sensitivity to different distance metrics and normalization schemes.
- **Zero-shot classification evaluation**: Understanding how graph alignment correlates with downstream performance without task-specific supervision is crucial for the method's validity. Quick check: Validate correlation strength across diverse downstream tasks and datasets.
- **Feature distribution analysis**: Analyzing how class name features relate to image features through graph alignment is fundamental to the approach. Quick check: Examine feature space overlap and separability across different VLMs.
- **Computational graph theory applications**: Applying graph-based methods to model selection represents an innovative use of network analysis techniques. Quick check: Benchmark computational complexity against traditional selection methods.

## Architecture Onboarding

Component Map: Class Names + Unlabeled Images -> Feature Extraction -> Graph Construction (Visual + Textual) -> Node Alignment + Edge Alignment -> VLM Ranking Score

Critical Path: Feature Extraction → Graph Construction → Graph Alignment → VLM Ranking Score

Design Tradeoffs: The method trades computational complexity (fully connected graphs scale quadratically with nodes) for comprehensive relationship capture. The assumption that graph alignment predicts downstream performance may not generalize to all vision-language tasks.

Failure Signatures: Poor alignment scores may result from domain shift between training and downstream data, insufficient feature quality, or fundamental limitations in the VLM's cross-modal capabilities. High computational cost for large-scale applications.

First Experiments:
1. Test VEGA on a small benchmark with 3-5 VLMs to verify basic functionality and correlation with known performance
2. Validate graph alignment scores against ground-truth downstream performance on a held-out validation set
3. Compare VEGA's rankings against random selection and simple heuristic-based approaches

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the provided content.

## Limitations
- Computational scalability concerns with fully connected graphs as node count increases significantly
- Assumption that graph alignment correlates with downstream performance may not hold for specialized or domain-specific tasks
- Limited evaluation to zero-shot classification tasks; performance on few-shot or fine-tuned scenarios remains unexplored

## Confidence
High: State-of-the-art benchmark performance, strong empirical validation across three datasets
Medium: Generalizability to diverse vision-language tasks beyond standard classification, theoretical guarantees of graph alignment correlation

## Next Checks
1. Evaluate VEGA on few-shot and fine-tuned learning scenarios to assess robustness beyond zero-shot settings
2. Test computational scalability with large-scale graphs (thousands of nodes) and analyze runtime complexity
3. Validate performance across diverse vision-language tasks including detection, segmentation, and multimodal reasoning tasks to establish broader applicability