---
ver: rpa2
title: 'MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data'
arxiv_id: '2402.08957'
source_url: https://arxiv.org/abs/2402.08957
tags:
- data
- proof
- formal
- informal
- mustard
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MUSTARD is a framework that generates high-quality mathematical
  theorem and proof data by combining large language models with theorem provers.
  It samples mathematical concepts, generates problems and solutions, and validates
  them using Lean Prover.
---

# MUSTARD: Mastering Uniform Synthesis of Theorem and Proof Data

## Quick Facts
- **arXiv ID**: 2402.08957
- **Source URL**: https://arxiv.org/abs/2402.08957
- **Reference count**: 40
- **Primary result**: MUSTARD SAUCE dataset of 5,866 validated theorem-proof pairs, yielding 15.41% average relative performance gains in automated theorem proving and 8.18% in math word problems when fine-tuning Llama 2-7B.

## Executive Summary
MUSTARD is a framework that generates high-quality mathematical theorem and proof data by integrating large language models with theorem provers. It systematically samples mathematical concepts, generates problems and solutions, and validates them using Lean Prover to produce a dataset of formally verified theorem-proof pairs. The resulting MUSTARD SAUCE dataset contains 5,866 data points with both informal and formal proofs. Fine-tuning Llama 2-7B on this dataset demonstrates significant performance improvements in automated theorem proving and math word problem solving, highlighting the value of curated, validated mathematical data for training mathematical reasoning models.

## Method Summary
MUSTARD operates through a three-stage pipeline: (1) concept sampling, where relevant mathematical concepts are selected from existing corpora; (2) problem and solution generation, using large language models to produce theorems and proofs; and (3) validation, where generated proofs are checked for correctness using Lean Prover. This process ensures that only mathematically sound and formally verifiable theorem-proof pairs are included in the final dataset. The framework is designed to produce diverse, high-quality data that bridges informal mathematical reasoning and formal proof verification.

## Key Results
- MUSTARD SAUCE dataset contains 5,866 validated theorem-proof pairs with both informal and formal proofs.
- Fine-tuning Llama 2-7B on MUSTARD SAUCE yields 15.41% average relative performance gains in automated theorem proving.
- Fine-tuning yields 8.18% average relative performance gains in math word problem solving.

## Why This Works (Mechanism)
The framework leverages the strengths of large language models for creative problem generation and theorem formulation, while relying on Lean Prover for rigorous correctness validation. This hybrid approach ensures that the dataset contains only mathematically sound content, which is critical for training robust mathematical reasoning models. By combining informal and formal proofs, MUSTARD SAUCE supports both intuitive understanding and formal verification, bridging the gap between human-readable mathematics and machine-checkable proofs.

## Foundational Learning
- **Concept sampling**: Selecting relevant mathematical concepts from existing corpora to guide problem generation. Why needed: Ensures diversity and relevance of generated theorems. Quick check: Verify sampled concepts cover a broad range of mathematical domains.
- **Informal proof generation**: Using LLMs to produce human-readable proofs. Why needed: Supports intuitive understanding and educational use. Quick check: Assess readability and logical flow of generated proofs.
- **Formal proof validation**: Checking proofs using Lean Prover for correctness. Why needed: Guarantees mathematical rigor and prevents propagation of errors. Quick check: Confirm Lean Prover successfully verifies a high percentage of generated proofs.
- **Dataset curation**: Filtering and organizing validated theorem-proof pairs. Why needed: Produces a clean, high-quality dataset for training. Quick check: Measure dataset diversity and balance across mathematical domains.

## Architecture Onboarding
- **Component map**: Concept Sampling -> Problem Generation -> Solution Generation -> Lean Validation -> Dataset Curation
- **Critical path**: The validation step (Lean Prover) is the bottleneck, as it ensures only correct proofs are included, directly impacting dataset quality and downstream performance.
- **Design tradeoffs**: Balancing generation diversity with validation strictness; using a smaller dataset (5,866 examples) for high quality versus larger, less curated datasets.
- **Failure signatures**: Low validation rate indicates issues with generation quality or concept sampling; poor downstream performance suggests dataset may not generalize well to broader mathematical domains.
- **First experiments**: (1) Measure Lean Prover validation rate for generated proofs; (2) Evaluate downstream performance on held-out theorem-proof pairs; (3) Test fine-tuning on a larger LLM (e.g., Llama 2-13B) to assess scalability.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions.

## Limitations
- The evaluation focuses on automated theorem proving and math word problems, not on generating novel theorem-proof pairs or on diverse mathematical domains.
- The dataset size (5,866 examples) is relatively small, raising questions about scalability and generalizability.
- Lean Prover validation may introduce bias toward formal proof styles and may not capture all forms of valid mathematical reasoning.

## Confidence
- **High**: The dataset generation pipeline (concept sampling, problem generation, Lean validation) is technically sound and well-described.
- **Medium**: The reported performance gains from fine-tuning are likely real but may not transfer robustly to other models or domains.
- **Medium**: The claim that MUSTARD SAUCE improves both automated theorem proving and math word problem solving is supported, but the breadth of this improvement is uncertain.

## Next Checks
1. Test the generalizability of MUSTARD SAUCE by fine-tuning larger or different LLMs (e.g., Llama 2-13B, Code Llama) and evaluating on out-of-domain mathematical problems.
2. Conduct ablation studies to isolate the impact of each component in the MUSTARD pipeline (concept sampling, generation, validation) on final dataset quality.
3. Evaluate whether models trained on MUSTARD SAUCE can generate novel, valid theorem-proof pairs not present in the original dataset.