---
ver: rpa2
title: Learning Expressive Random Feature Models via Parametrized Activations
arxiv_id: '2411.19468'
source_url: https://arxiv.org/abs/2411.19468
tags:
- random
- have
- functions
- activation
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Random Feature model with Learnable Activation
  Functions (RFLAF), a novel approach that parametrizes activation functions as weighted
  sums of basis functions within the random feature framework. The authors provide
  theoretical analysis for models with single and multiple radial basis function (RBF)
  activations, demonstrating that RFLAF expands the represented function space and
  requires only about twice the parameter number compared to traditional RF models
  for enhanced expressivity.
---

# Learning Expressive Random Feature Models via Parametrized Activations

## Quick Facts
- arXiv ID: 2411.19468
- Source URL: https://arxiv.org/abs/2411.19468
- Authors: Zailin Ma; Jiansheng Yang; Yaodong Yang
- Reference count: 40
- Primary result: Introduces RFLAF, a random feature model with learnable activation functions that achieves 3x faster computation than splines while requiring only ~2x the parameters of traditional RF models

## Executive Summary
This paper introduces the Random Feature model with Learnable Activation Functions (RFLAF), which parametrizes activation functions as weighted sums of basis functions within the random feature framework. The authors provide theoretical analysis showing that RFLAF expands the represented function space while requiring only about twice the parameter number compared to traditional RF models. Experimental results validate these findings across various tasks, demonstrating consistent outperformance over other RF models. The model achieves 3 times faster computational efficiency with RBFs compared to splines and provides interpretability by revealing learned activation functions through optimized weights.

## Method Summary
RFLAF uses random features with learnable activation functions parametrized as weighted sums of basis functions (specifically RBFs). The model takes input x ∈ ℝᵈ, generates M random features from W ~ N(0, I_d), applies learnable activation functions (weighted sums of N RBFs with coefficients a_i), and produces output through a weighted sum with coefficients v_m. Total parameters are M + N (v coefficients + a coefficients). The model is trained using Adam optimizer with MSE loss and regularization terms λ₁(∥a∥²₂ - ∥v∥²₂)² + λ₂∥a∥₁. The number of random features scales with the square root of the sample size, matching state-of-the-art RF results while requiring fewer than double the parameters.

## Key Results
- RFLAF expands representational capacity with only slight parameter increase (~2x traditional RF models)
- Model achieves 3x faster computational efficiency with RBFs compared to splines
- Learned activation functions provide interpretability through optimized weights
- Number of random features needed scales with square root of sample size, matching state-of-the-art results

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The RBF learnable activation function expands the representational capacity of the RF model with only a slight increase in parameter count.
- **Mechanism:** By approximating the target activation function with a weighted sum of RBFs, the model gains universal approximation power. The number of RBFs needed scales with the square root of the sample size, matching state-of-the-art RF results while requiring fewer than double the parameters.
- **Core assumption:** The target activation function belongs to a compact support class Cc(K) and can be well-approximated by RBFs.
- **Evidence anchors:** Abstract statement about parameter efficiency; theoretical section on scaling; lack of comparable papers in neighbor corpus.
- **Break condition:** If the target function is not well-approximated by RBFs or if the sample size is extremely small relative to the function complexity.

### Mechanism 2
- **Claim:** The learnable activation function provides interpretability by revealing the underlying activation structure through optimized weights.
- **Mechanism:** The optimized weights of the RBF basis functions directly encode the shape of the learned activation function, allowing direct inference of the activation function from the weights.
- **Core assumption:** The learned activation function can be meaningfully interpreted from its basis function decomposition.
- **Evidence anchors:** Abstract statement about interpretability; experimental section showing activation function visualization; lack of interpretability comparison in neighbor corpus.
- **Break condition:** If the basis function decomposition is too complex or the learned weights are degenerate.

### Mechanism 3
- **Claim:** The RBF formulation enables efficient computation and convergence compared to spline-based approaches.
- **Mechanism:** RBFs and their derivatives are easily computed, facilitating gradient-based optimization. This contrasts with spline-based methods that require more complex optimization techniques like LBFGS with line search.
- **Core assumption:** RBFs are computationally simpler than splines for the same approximation task.
- **Evidence anchors:** Abstract statement about 3x faster computation; discussion of RBF computational advantages; lack of computational efficiency comparison in neighbor corpus.
- **Break condition:** If the computational advantage disappears for high-dimensional problems or if the optimization landscape becomes pathological.

## Foundational Learning

- **Concept: Universal Approximation Theorem**
  - Why needed here: The theoretical foundation for why RBFs can approximate any continuous function on a compact set, which justifies the approach of using RBFs as basis functions for learnable activation functions.
  - Quick check question: Can any continuous function on a compact interval be approximated arbitrarily well by a weighted sum of RBFs?

- **Concept: Random Feature Approximation**
  - Why needed here: Understanding how random features approximate kernel methods and how this approximation quality scales with the number of features is essential for the theoretical analysis.
  - Quick check question: How does the approximation error of random features scale with the number of features M?

- **Concept: Reproducing Kernel Hilbert Space (RKHS)**
  - Why needed here: The theoretical framework for analyzing the represented function class and generalization bounds relies on RKHS theory, particularly for deriving the kernel associated with the RBF activation.
  - Quick check question: What is the relationship between the function class induced by the random feature model and its corresponding RKHS?

## Architecture Onboarding

- **Component map:** Input x ∈ ℝᵈ -> Random features W ~ N(0, I_d) with M samples -> Learnable activation (weighted sum of N RBFs) -> Feature mapping σ(w⊤x) -> Output weighted sum with v coefficients

- **Critical path:**
  1. Generate random features W
  2. Compute RBF activations for each feature
  3. Apply learned weights a to combine RBFs
  4. Compute weighted sum with v coefficients
  5. Forward pass through network
  6. Backpropagation to update a and v

- **Design tradeoffs:**
  - More RBFs (larger N) increases expressivity but also parameter count and computational cost
  - Wider random features (larger M) improves approximation but increases computation
  - Choice of RBF width h affects approximation quality and optimization stability
  - Grid spacing of RBF centers affects coverage of activation function space

- **Failure signatures:**
  - Training loss plateaus early: likely insufficient expressivity (increase N or M)
  - Training loss oscillates: learning rate too high or RBF widths poorly chosen
  - Poor generalization: overfitting (increase regularization or reduce N/M)
  - Extremely slow convergence: consider switching from Adam to LBFGS if using splines instead of RBFs

- **First 3 experiments:**
  1. Train on simple target function f(x) = sin(πx)1[−1,1] with varying N and M to observe expressivity-accuracy tradeoff
  2. Compare convergence speed with fixed activation RF models on the same task
  3. Visualize learned activation functions by plotting the weighted RBF sum to verify interpretability claims

## Open Questions the Paper Calls Out
- The paper does not explicitly call out open questions, but based on the limitations and scope of the work, several questions emerge:
  - How does the choice of basis functions beyond RBFs (such as wavelets or learned dictionaries) affect the approximation and generalization properties of RFLAF models?
  - What is the optimal trade-off between the number of random features (M) and the number of basis functions (N) for minimizing the excess risk in RFLAF models?
  - How does the introduction of learnable activation functions in RFLAF models impact the interpretability of the model in comparison to traditional RF models with fixed activations?

## Limitations
- Experimental scope is limited to synthetic datasets, leaving unclear how RFLAF scales to noisy, high-dimensional real-world data
- The "3× faster" computational efficiency claim rests on a single RBF-vs-spline comparison without ablation of other hyperparameters
- Interpretability assertion is supported only by visualization of learned activations, lacking quantitative metrics or user studies

## Confidence
- Theoretical claims on approximation: Medium (backed by RBF universality theory)
- Computational-efficiency claim: Low (single comparison without ablation)
- Interpretability claims: Low (visualization only, no quantitative metrics)

## Next Checks
1. Run ablation studies varying N and h on a standard UCI regression dataset to confirm computational and expressivity claims in a non-synthetic setting.
2. Quantify the smoothness/regularity of learned activations (e.g., Lipschitz constant, curvature) and test whether these metrics correlate with downstream task performance.
3. Benchmark against recent learnable-activation RF variants (e.g., adaptive splines or Fourier features) on both synthetic and real data to validate the stated efficiency and accuracy advantages.