---
ver: rpa2
title: Can Stability be Detrimental? Better Generalization through Gradient Descent
  Instabilities
arxiv_id: '2412.17613'
source_url: https://arxiv.org/abs/2412.17613
tags:
- learning
- generalization
- rates
- training
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that training instabilities in deep learning,
  caused by large learning rates beyond the stability threshold, actually improve
  generalization by driving parameters toward flatter regions of the loss landscape.
  The authors prove that network depth causes rotations in the principal eigenvectors
  of the loss Hessian during unstable training, which allows exploration of flatter
  regions.
---

# Can Stability be Detrimental? Better Generalization through Gradient Descent Instabilities

## Quick Facts
- arXiv ID: 2412.17613
- Source URL: https://arxiv.org/abs/2412.17613
- Reference count: 40
- This paper shows that training instabilities in deep learning, caused by large learning rates beyond the stability threshold, actually improve generalization by driving parameters toward flatter regions of the loss landscape.

## Executive Summary
This paper challenges the conventional wisdom that stability is necessary for good generalization in deep learning. Through theoretical analysis and empirical experiments, the authors demonstrate that training instabilities - caused by large learning rates beyond the stability threshold - can actually improve generalization by driving parameters toward flatter regions of the loss landscape. The key insight is that network depth causes rotations in the principal eigenvectors of the loss Hessian during unstable training, allowing exploration of flatter regions that wouldn't be accessible during stable training.

The work has practical implications for training deep networks, suggesting that practitioners should consider using learning rates much higher than what is traditionally recommended by the descent lemma. The authors show that starting with large learning rates and reducing them later in training can lead to improved generalization performance, even on standard benchmarks like CIFAR10 and FashionMNIST.

## Method Summary
The paper combines theoretical analysis of Diagonal Linear Networks (DLNs) with empirical validation on small VGG and ResNet architectures. The authors compute Hessian eigenvalues and eigenvectors using Pearlmutter's trick with Modified Parlett-Kahan re-orthogonalization to track sharpness and eigenvector rotations during training. Experiments are conducted using full-batch gradient descent with exponentially sampled learning rates above and below the stability threshold. The study monitors sharpness (maximum eigenvalue of Hessian), eigenvector rotation similarity, and progressive flattening (peak sharpness reduction over time) to analyze the relationship between training instabilities and generalization.

## Key Results
- Large learning rates beyond the stability threshold cause rotations in the principal eigenvectors of the loss Hessian, driving parameters toward flatter regions of the loss landscape
- Starting with large learning rates and reducing them later in training leads to improved generalization performance on CIFAR10 and FashionMNIST
- Sharpness (maximum eigenvalue of Hessian) is not always a reliable predictor of generalization; eigenvector rotation similarity can be a better indicator in some cases
- The rotational behavior of Hessian eigenvectors serves as a better predictor of generalization than sharpness alone in some cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large learning rates beyond the stability threshold cause rotations in the principal eigenvectors of the loss Hessian, driving parameters toward flatter regions of the loss landscape.
- Mechanism: When learning rates exceed the stability threshold, parameter growth becomes unstable, causing the sharpest eigenvector to rotate away from the direction of the sharpest parameter. This rotation allows exploration of flatter regions of the loss landscape.
- Core assumption: The loss landscape can be approximated as a sum of multiplicative terms (DLN model), and that rotations in Hessian eigenvectors correlate with exploration of flatter regions.
- Evidence anchors:
  - [abstract]: "Our crucial insight lies in noting that, during these instabilities, the orientation of the Hessian eigenvectors rotate."
  - [section 3.1]: Derivation showing that for a 2-parameter DLN, unstable growth causes rotations in the sharpest eigenvector away from the sharpest parameter.
  - [corpus]: Limited direct evidence - the paper itself is the primary source for this mechanism.
- Break condition: If the DLN model assumptions don't hold (e.g., parameters aren't ill-conditioned, or the loss landscape isn't well-approximated by sums of multiplicative terms), the rotational mechanism may not apply.

### Mechanism 2
- Claim: Repeated cycles of instability followed by resolution lead to progressive flattening of the loss landscape.
- Mechanism: Each instability cycle causes parameter growth that forces exploration of the loss landscape periphery. Resolution of instability occurs when flatter regions are found, leading to an overall reduction in landscape curvature over time.
- Core assumption: Resolution of instability requires finding flatter regions, and that this process can be repeated across training cycles.
- Evidence anchors:
  - [abstract]: "These rotations are a consequence of network depth, and we prove that for any network with depth > 1, unstable growth in parameters causes rotations in the principal components of the Hessian, which promotes exploration of the parameter space away from unstable directions."
  - [section 3.3]: Empirical evidence showing that repeated instabilities lead to reduced maximum sharpness (S(θ)max) over time.
  - [corpus]: Weak - the paper provides the primary evidence for this mechanism.
- Break condition: If instabilities don't resolve (e.g., learning rates are too large), or if resolution doesn't require finding flatter regions, progressive flattening won't occur.

### Mechanism 3
- Claim: Sharpness (maximum eigenvalue of Hessian) is not always a reliable predictor of generalization; eigenvector rotation similarity can be a better indicator in some cases.
- Mechanism: The orientation of Hessian eigenvectors changes during training, and the degree of rotation can correlate better with generalization performance than sharpness alone.
- Core assumption: Eigenvector orientation changes are meaningful and correlate with generalization, independent of sharpness changes.
- Evidence anchors:
  - [abstract]: "Additionally, we show that starting with large learning rates have long-term benefits in generalization performance, even when learning rates are reduced later in training. However, our findings also challenge the reliability of sharpness as a metric for generalization, as the degree of eigenvector rotation can, in some cases, be a more effective predictor."
  - [section 4.3]: Comparison of rank correlation between validation accuracy and both sharpness and eigenvector rotation similarity.
  - [corpus]: Weak - the paper provides the primary evidence for this claim.
- Break condition: If eigenvector rotations don't correlate with generalization, or if sharpness remains the dominant factor, this mechanism fails.

## Foundational Learning

- Concept: Edge of Stability
  - Why needed here: Understanding the stability threshold (S(θ) ≤ 2/η) is crucial for identifying when instabilities occur and how they affect training.
  - Quick check question: What happens to training loss and sharpness when learning rate exceeds the stability threshold?

- Concept: Hessian Eigenvectors and Eigenvalues
  - Why needed here: The paper's core mechanism relies on understanding how the orientation and magnitude of Hessian eigenvectors change during training.
  - Quick check question: How does the orientation of the sharpest eigenvector relate to the direction of steepest descent?

- Concept: Progressive Sharpening
  - Why needed here: This concept explains how sharpness increases during stable training phases, which is important for understanding the full training dynamics.
  - Quick check question: What causes progressive sharpening, and how does it differ from the effects of instabilities?

## Architecture Onboarding

- Component map:
  Loss function (L(θ)) -> Hessian (H(θ)) -> Learning rate (η) -> Parameter updates via gradient descent -> Sharpness metric (S(θ) = λmax(H(θ))) -> Eigenvector rotation similarity metric -> Progressive flattening metric (S(θ)max reduction over time)

- Critical path:
  1. Initialize model with large learning rate η0
  2. Monitor sharpness S(θ) and eigenvalue rotations
  3. When S(θ) exceeds 2/η0, instabilities begin
  4. Track eigenvector rotations and parameter growth
  5. Monitor for resolution of instabilities (return to stability)
  6. Reduce learning rate if desired for final training phase
  7. Evaluate generalization performance

- Design tradeoffs:
  - Larger initial learning rates provide more regularization but risk instability
  - Longer duration of large learning rates increases progressive flattening but may slow convergence
  - Monitoring eigenvector rotations requires additional computation

- Failure signatures:
  - Model divergence (parameters grow unbounded)
  - No improvement in generalization despite instabilities
  - Sharpness continues to increase without resolution
  - Eigenvector rotations don't correlate with performance

- First 3 experiments:
  1. Train a simple model (e.g., MLP on MNIST) with varying learning rates above and below stability threshold, comparing generalization performance.
  2. Monitor sharpness and eigenvector rotations during training to observe the Edge of Stability phenomenon.
  3. Implement learning rate reduction at different points in training to study progressive flattening effects.

## Open Questions the Paper Calls Out
The paper doesn't explicitly call out open questions, but several implications arise from the work:

### Open Question 1
- Question: How do rotations of Hessian eigenvectors specifically contribute to finding flatter minima, beyond just exploring different regions of the loss landscape?
- Basis in paper: [explicit] The paper conjectures that eigenvector rotations allow exploration of flatter regions but doesn't fully explain the mechanism.
- Why unresolved: The theoretical analysis focuses on proving rotations occur and empirically showing they correlate with better generalization, but doesn't establish a direct causal link between rotations and finding flatter minima.
- What evidence would resolve it: Experiments comparing the flatness of minima reached with and without eigenvector rotations (e.g., by artificially preventing rotations through constrained optimization), or theoretical analysis connecting eigenvector orientations to local curvature properties.

### Open Question 2
- Question: Can the benefits of large learning rates and resulting instabilities be achieved through other optimization methods or regularization techniques that don't involve unstable parameter growth?
- Basis in paper: [inferred] The paper focuses on gradient descent instabilities but doesn't explore alternative ways to achieve similar generalization benefits.
- Why unresolved: The study is limited to gradient descent and its instabilities, leaving open whether other approaches could replicate the observed effects.
- What evidence would resolve it: Experiments comparing generalization performance of large learning rate gradient descent with other optimizers (e.g., Adam with appropriate tuning) or regularization methods (e.g., sharpness-aware minimization) on the same tasks.

### Open Question 3
- Question: How does the relationship between sharpness, eigenvector rotations, and generalization change in stochastic gradient descent (SGD) compared to full-batch gradient descent?
- Basis in paper: [explicit] The authors note their experiments are conducted in a non-stochastic setting and encourage further exploration in SGD.
- Why unresolved: The study only uses full-batch gradient descent, which has different dynamics than SGD, particularly regarding noise and convergence behavior.
- What evidence would resolve it: Replicating the experiments with SGD on the same tasks, measuring sharpness, eigenvector rotations, and generalization performance, and comparing the results to full-batch gradient descent.

## Limitations
- The theoretical framework relies heavily on the Diagonal Linear Network (DLN) model, which may not fully capture the complexity of deep networks with non-linear activations.
- Empirical validation is primarily conducted on small-scale datasets (CIFAR10, FashionMNIST) with limited model architectures.
- The study uses full-batch gradient descent without stochasticity, which may not reflect typical deep learning training scenarios.
- The computational cost of computing Hessian eigenvectors throughout training is significant and may limit practical applicability.

## Confidence
**High Confidence:** The Edge of Stability phenomenon and the relationship between sharpness and learning rates are well-established in the literature and consistently demonstrated in this work.

**Medium Confidence:** The rotational mechanism of Hessian eigenvectors during instabilities is theoretically sound for DLNs but requires more empirical validation across diverse architectures and activation functions.

**Medium Confidence:** The progressive flattening hypothesis is supported by the presented experiments but needs validation on larger-scale problems and with stochastic optimization methods.

**Low Confidence:** The claim that eigenvector rotation similarity is a better predictor of generalization than sharpness alone is based on limited experiments and may not generalize across different problem domains.

## Next Checks
1. **Architecture Generalization Test:** Validate the rotational mechanism and progressive flattening across diverse architectures (CNNs, Transformers, MLPs with non-linear activations) on larger datasets (ImageNet, COCO) to assess the universality of the findings.

2. **Stochastic Training Validation:** Replicate the experiments using stochastic gradient descent with momentum and adaptive optimizers to determine if the benefits of large learning rates persist in more realistic training scenarios.

3. **Long-Training Horizon Study:** Conduct extended training runs (10x longer) to investigate whether the progressive flattening effect continues over longer timescales and whether the optimal learning rate schedule changes with training duration.