---
ver: rpa2
title: 'Real2Code: Reconstruct Articulated Objects via Code Generation'
arxiv_id: '2406.08474'
source_url: https://arxiv.org/abs/2406.08474
tags:
- object
- objects
- joint
- part
- parts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Real2Code, a novel method for reconstructing
  articulated objects from visual observations using code generation. The core idea
  is to represent object parts with oriented bounding boxes and use a fine-tuned large
  language model to predict joint articulation as code.
---

# Real2Code: Reconstruct Articulated Objects via Code Generation

## Quick Facts
- arXiv ID: 2406.08474
- Source URL: https://arxiv.org/abs/2406.08474
- Reference count: 40
- This paper proposes Real2Code, a novel method for reconstructing articulated objects from visual observations using code generation.

## Executive Summary
Real2Code is a novel method for reconstructing articulated objects from visual observations using code generation. The core idea is to represent object parts with oriented bounding boxes and use a fine-tuned large language model to predict joint articulation as code. Real2Code leverages pre-trained vision and language models, allowing it to scale elegantly with the number of articulated parts and generalize from synthetic training data to real-world objects. The method achieves state-of-the-art performance in reconstruction accuracy, successfully reconstructing objects with up to 10 articulated parts, which is beyond the complexity seen in training data. When combined with a stereo reconstruction model, Real2Code can also reconstruct real-world objects from a handful of multi-view RGB images without requiring depth or camera information.

## Method Summary
Real2Code reconstructs articulated objects from visual observations by first segmenting object parts using a fine-tuned SAM model, then completing their geometry using a learned shape completion model. The parts are represented as oriented bounding boxes (OBBs), which are input to a fine-tuned large language model (LLM) to predict joint articulation as code. The method leverages pre-trained vision and language models, allowing it to scale elegantly with the number of articulated parts and generalize from synthetic training data to real-world objects. The LLM-based approach avoids the need to change output dimensions or run multiple inferences for objects with varying numbers of joints, unlike prior methods.

## Key Results
- Real2Code achieves state-of-the-art performance in reconstruction accuracy on PartNet-Mobility benchmark.
- The method successfully reconstructs objects with up to 10 articulated parts, beyond the complexity seen in training data.
- When combined with a stereo reconstruction model, Real2Code can reconstruct real-world objects from a handful of multi-view RGB images without requiring depth or camera information.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Real2Code scales elegantly with object complexity by using language modeling's next-token prediction formulation.
- Mechanism: The LLM-based approach avoids the need to change output dimensions or run multiple inferences for objects with varying numbers of joints, unlike prior methods.
- Core assumption: Language models can handle arbitrary-length outputs, making them suitable for predicting joint articulation as code.
- Evidence anchors:
  - [abstract]: "our approach scales elegantly with the number of articulated parts, and generalizes from synthetic training data to real world objects"
  - [section]: "the next-token prediction formulation in language modeling allows generating arbitrary-length outputs, i.e., the model architecture needs no adjustment to handle varying number of object joints"
- Break condition: If the LLM cannot generalize to objects with more parts than seen during training, the scaling advantage disappears.

### Mechanism 2
- Claim: Oriented Bounding Boxes (OBBs) serve as an effective abstraction layer between raw sensory input and the LLM.
- Mechanism: OBBs provide a concise yet precise representation of object parts, allowing the LLM to reason about 3D spatial information and predict joint parameters without needing to regress to continuous values.
- Core assumption: OBBs can effectively summarize raw sensory observation to the LLM in a way that preserves necessary spatial information for joint prediction.
- Evidence anchors:
  - [abstract]: "We then represent the object parts with oriented bounding boxes, which are input to a fine-tuned large language model (LLM) to predict joint articulation as code"
  - [section]: "we propose to use oriented bounding boxes (OBBs) as an abstraction layer that summarizes the raw sensory observation to the LLM in a concise yet precise manner"
- Break condition: If the OBB representation loses critical spatial information needed for accurate joint parameter prediction, the LLM's performance will degrade.

### Mechanism 3
- Claim: Fine-tuning a pre-trained LLM on code generation tasks allows Real2Code to leverage strong priors for both common-sense objects and syntactically correct code.
- Mechanism: By adapting a pre-trained LLM to specialize in articulation prediction through fine-tuning, Real2Code benefits from the model's existing knowledge while focusing on the specific task of generating simulation-ready code.
- Core assumption: Pre-trained LLMs have sufficient general knowledge and code generation capabilities that can be effectively adapted to the specific task of articulated object reconstruction.
- Evidence anchors:
  - [abstract]: "By leveraging pre-trained vision and language models, our approach scales elegantly with the number of articulated parts"
  - [section]: "pre-trained LLMs are equipped with strong priors for both common-sense objects and for generating syntactically correct code, making them easily adaptable to our task"
- Break condition: If the pre-trained LLM lacks sufficient relevant knowledge or if the fine-tuning process fails to adequately specialize the model, the performance will suffer.

## Foundational Learning

- Concept: Large Language Models (LLMs) and their code generation capabilities
  - Why needed here: Real2Code relies on fine-tuning an LLM to generate code representing articulated object structures, leveraging the model's ability to predict joint articulation as code.
  - Quick check question: What is the primary advantage of using an LLM for code generation in the context of articulated object reconstruction?

- Concept: Oriented Bounding Boxes (OBBs) and their role in 3D spatial representation
  - Why needed here: OBBs serve as an abstraction layer between raw sensory input and the LLM, allowing for precise representation of object parts while being concise enough for language modeling.
  - Quick check question: How do OBBs help in converting a regression task to a classification task for joint parameter prediction?

- Concept: 3D shape completion and its importance in object reconstruction
  - Why needed here: Real2Code uses a learned shape completion model to extract watertight meshes from partially observed point clouds, addressing the challenge of frequent self-occlusion in articulated objects.
  - Quick check question: Why is shape completion necessary for reconstructing articulated objects, and how does it improve the quality of the final reconstruction?

## Architecture Onboarding

- Component map: Visual observations -> Segmentation (SAM) -> Shape Completion -> OBB Extraction -> Code Generation (LLM) -> Executable Simulation Code

- Critical path: Visual observations → Segmentation → Shape Completion → OBB Extraction → Code Generation → Executable Simulation Code

- Design tradeoffs:
  - Using OBBs as input abstraction simplifies the problem for the LLM but may lose some fine-grained spatial details
  - Relying on pre-trained models (SAM, LLM) reduces training data requirements but may limit flexibility in handling novel object types
  - Separating the problem into segmentation, shape completion, and code generation allows for modular improvements but introduces potential error accumulation between stages

- Failure signatures:
  - Inaccurate segmentation leading to wrong OBBs and subsequent incorrect joint predictions
  - Shape completion model failing to recover complete part geometries, especially for occluded areas
  - LLM generating syntactically correct but semantically incorrect code for joint parameters
  - Inability to handle objects with more parts than seen during training

- First 3 experiments:
  1. Evaluate the accuracy of the fine-tuned SAM model on unseen object images by comparing predicted segmentations with ground-truth masks
  2. Test the shape completion model on partial point clouds from the test set and measure the Chamfer Distance between predicted and ground-truth part meshes
  3. Assess the articulation prediction accuracy by measuring the error of joint type, joint axis, and joint position predictions on objects with varying numbers of parts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well would Real2Code generalize to objects with more than 10 articulated parts, beyond the complexity seen in the training data?
- Basis in paper: [inferred] The paper mentions that Real2Code is the first approach to reconstruct objects with up to 10 articulated parts, but does not explicitly test or report results on objects with more than 10 parts.
- Why unresolved: The paper does not provide any data or analysis on objects with more than 10 articulated parts, leaving the generalization capability beyond this complexity untested.
- What evidence would resolve it: Testing Real2Code on objects with more than 10 articulated parts and reporting the reconstruction accuracy and joint prediction performance would provide evidence of its generalization capability.

### Open Question 2
- Question: How would Real2Code perform on objects with non-cuboid-like parts, such as objects with curved or irregular geometries?
- Basis in paper: [explicit] The paper mentions that many common articulated objects consist of cuboid-like parts, but does not explicitly test or report results on objects with non-cuboid-like parts.
- Why unresolved: The paper does not provide any data or analysis on objects with non-cuboid-like parts, leaving the performance on such objects untested.
- What evidence would resolve it: Testing Real2Code on objects with non-cuboid-like parts and reporting the reconstruction accuracy and joint prediction performance would provide evidence of its performance on such objects.

### Open Question 3
- Question: How sensitive is Real2Code to the quality of the initial 2D image segmentation, and can human corrective feedback improve its performance?
- Basis in paper: [explicit] The paper mentions that the articulation prediction accuracy is sensitive to failures in the first 2D image segmentation module, and suggests that human corrective feedback as proposed in [52] could improve performance.
- Why unresolved: The paper does not provide any quantitative data or analysis on the impact of segmentation quality on Real2Code's performance, nor does it report any results on the use of human corrective feedback.
- What evidence would resolve it: Conducting experiments with varying segmentation quality and reporting the corresponding reconstruction accuracy and joint prediction performance would provide evidence of Real2Code's sensitivity to segmentation quality. Additionally, testing the use of human corrective feedback and reporting the resulting performance improvements would provide evidence of its effectiveness.

## Limitations
- The approach relies heavily on synthetic training data and may struggle with objects exhibiting significantly different kinematic structures than those seen during training.
- The OBB abstraction, while effective, may lose fine-grained geometric details critical for precise joint parameter estimation in complex articulated systems.
- The method's performance on real-world objects, though demonstrated, remains primarily qualitative without extensive quantitative validation across diverse object categories.

## Confidence
- High: The method's architecture and training pipeline are well-specified, and the reported results show strong performance on PartNet-Mobility benchmarks.
- Medium: The generalization to real-world objects is demonstrated on a limited set of examples, and the quantitative metrics for real-world reconstruction are not as thoroughly established as for the synthetic benchmarks.
- Medium: The claim of handling objects with up to 10 articulated parts is impressive but based on a single example (desk lamp) rather than systematic evaluation across multiple complex objects.

## Next Checks
1. **Systematic Complexity Scaling**: Evaluate Real2Code on a curated set of objects with increasing numbers of articulated parts (e.g., 2, 4, 6, 8, 10 parts) to quantitatively measure performance degradation and identify the practical limits of the approach.

2. **Cross-Category Generalization**: Test the method on articulated objects from categories not represented in PartNet-Mobility (e.g., mechanical assemblies, toys, or kitchen appliances) to assess true generalization capabilities beyond the training distribution.

3. **Real-World Quantitative Benchmark**: Establish a standardized benchmark for real-world articulated object reconstruction using objects with known ground-truth articulation parameters, measuring both geometric accuracy (e.g., CD) and articulation prediction accuracy across multiple views and object types.