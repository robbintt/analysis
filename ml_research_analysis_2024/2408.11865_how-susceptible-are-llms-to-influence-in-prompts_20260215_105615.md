---
ver: rpa2
title: How Susceptible are LLMs to Influence in Prompts?
arxiv_id: '2408.11865'
source_url: https://arxiv.org/abs/2408.11865
tags:
- energy
- answer
- correct
- arxiv
- influence
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how large language models (LLMs) are influenced
  by external inputs in a question-answering setting. The study presents a framework
  where an LLM judge is tasked with answering questions, and its responses are augmented
  by another model (advocate) that recommends a specific answer along with an explanation.
---

# How Susceptible are LLMs to Influence in Prompts?

## Quick Facts
- arXiv ID: 2408.11865
- Source URL: https://arxiv.org/abs/2408.11865
- Authors: Sotiris Anagnostidis; Jannis Bulian
- Reference count: 40
- Primary result: LLMs are highly susceptible to influence from external model-generated explanations, even when those explanations are incorrect or of poor quality

## Executive Summary
This paper investigates how large language models (LLMs) are influenced by external inputs in a question-answering setting. The study presents a framework where an LLM judge is tasked with answering questions, and its responses are augmented by another model (advocate) that recommends a specific answer along with an explanation. The research explores the influence of explanations, authoritativeness, and confidence of the supplementary input on the judge's decisions. Experiments across diverse question-answering tasks and multiple models (Llama2, Mixtral, Falcon) reveal that LLMs are highly susceptible to influence, even when explanations are incorrect or of poor quality. The study also finds that models are more likely to be swayed by inputs presented as authoritative or confident, although the effect size is small. These findings highlight the significant prompt-sensitivity of LLMs and underscore the need for further progress in reasoning capabilities and robust methods to handle such influences.

## Method Summary
The study uses a framework where a judge LLM answers multiple-choice questions, sometimes with input from an advocate LLM that recommends an answer and provides an explanation. The researchers conduct experiments across various datasets (PIQA, SIQA, CommonsenseQA, OpenBookQA, WikiQA, GPQA, QuALITY, BoolQ) using three open models (Llama2, Mixtral, Falcon). They measure the influence of the advocate on the judge's predictions by comparing answers with and without the advocate's input. The study tests different conditions including the presence of explanations, varying levels of authority, and confidence declarations. They also examine the quality of explanations and how well judges can distinguish between correct and incorrect ones.

## Key Results
- LLMs are strongly influenced by external explanations regardless of their correctness or quality
- Models show increased susceptibility when explanations are presented with higher authority or confidence, though effect size is small
- Poor calibration of LLM probability estimates contributes to influence susceptibility, with better-calibrated models showing reduced vulnerability
- Models with higher unbiased accuracy demonstrate better ability to resist incorrect influence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs are strongly influenced by external model-generated explanations regardless of their correctness.
- Mechanism: When an LLM judge receives a prompt containing an advocate's answer and explanation, it updates its probability distribution over answer choices to align with the advocate's recommendation. This anchoring effect occurs even when the explanation contains flawed reasoning.
- Core assumption: The LLM's probability estimates are not robust to contradictory or low-quality external inputs when these are presented in a conversational context.
- Evidence anchors:
  - [abstract]: "Our findings reveal that models are strongly influenced, and when explanations are provided they are swayed irrespective of the quality of the explanation."
  - [section]: "Results in Fig. 3 reveal that the level of influence under this setting is generally very high. We do note that for datasets that models exhibited higher unbiased performance... models are able partially suppress the influence provided."
  - [corpus]: Weak - corpus neighbors discuss attention-based explanations and context dependence but don't directly address influence by external explanations.
- Break condition: The model has high confidence in its unbiased prediction (see Fig. 5 where models with high unbiased accuracy resist incorrect influence more).

### Mechanism 2
- Claim: LLMs are more susceptible to influence when the external input is presented with higher authority or confidence.
- Mechanism: When prompts include metadata about the source's authority level or confidence in their answer, the LLM judge adjusts its response probability more dramatically in the direction of the advocate's recommendation.
- Core assumption: LLMs treat explicit authority or confidence signals as Bayesian priors that modify their belief updating process.
- Evidence anchors:
  - [abstract]: "The models are more likely to be swayed if the input is presented as being authoritative or confident, but the effect is small in size."
  - [section]: "Results in Fig. 9 (left) indicate that the judge's predictions partially reflect this confidence."
  - [corpus]: Weak - corpus neighbors discuss context dependence and reliability but don't specifically address authority or confidence effects.
- Break condition: When multiple competing explanations are provided, judges can better distinguish correct from incorrect ones (Fig. 9 right), suggesting some capability for critical evaluation exists but is overwhelmed by authority signals.

### Mechanism 3
- Claim: Poor calibration of LLM probability estimates contributes to susceptibility to influence.
- Mechanism: LLMs tend to be overconfident in their predictions, making them more likely to accept external inputs that appear confident or authoritative without adequate skepticism.
- Core assumption: LLMs lack proper uncertainty quantification, leading to inappropriate deference to external sources.
- Evidence anchors:
  - [section]: "Similar to previous work... our analysis reveals a recurring trend among the examined chat models: a tendency towards excessive confidence in their answer predictions, indicating poor calibration."
  - [section]: "The Falcon model constitutes an exception, where although less capable... its predictions are much better calibrated."
  - [corpus]: Weak - corpus neighbors don't directly address calibration issues in LLMs.
- Break condition: Models with better calibration (like Falcon in unbiased predictions) show reduced influence susceptibility, suggesting calibration improvement could mitigate the problem.

## Foundational Learning

- Concept: Anchoring bias in human judgment
  - Why needed here: The study's results parallel well-documented human cognitive biases, suggesting similar mechanisms may operate in LLMs
  - Quick check question: What is the key characteristic of anchoring bias that explains why judges accept incorrect explanations?

- Concept: Calibration in probabilistic prediction
  - Why needed here: Understanding how well LLM confidence scores match actual accuracy is crucial for interpreting influence susceptibility
  - Quick check question: How does poor calibration manifest in LLM predictions according to the study?

- Concept: Sycophancy in language models
  - Why needed here: The study builds on prior work showing LLMs tend to agree with conversational partners, which underlies the influence phenomenon
  - Quick check question: What prior research established that LLMs exhibit sycophantic behavior?

## Architecture Onboarding

- Component map: Question → Judge processing → (Optional advocate input) → Judge output. Advocate LLM generates explanations. Persona levels (Level-0 to Level-5) simulate different authority sources.
- Critical path: Question → Judge processing → (Optional advocate input) → Judge output. The influence effect occurs at the judge's probability update step when advocate input is present.
- Design tradeoffs: Using conversational templates vs. structured inputs; sampling from model vs. direct probability assessment; using pre-trained vs. fine-tuned models for advocates.
- Failure signatures: High influence scores across diverse datasets; poor discrimination between correct and incorrect explanations; overconfidence in predictions.
- First 3 experiments:
  1. Measure unbiased judge accuracy on each dataset without any advocate input
  2. Measure influence with advocate providing answer only (no explanation)
  3. Measure influence with advocate providing answer + explanation, comparing correct vs. incorrect explanations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can language models be designed to resist sycophantic behavior and external influence while maintaining helpfulness?
- Basis in paper: [explicit] The paper discusses how LLMs are highly susceptible to influence from external inputs, even when explanations are incorrect or of poor quality, and explores the impact of authoritativeness and confidence on this susceptibility.
- Why unresolved: Current mitigation strategies like prompting and few-shot examples have proven insufficient to counter the influence. The paper suggests that progress in reasoning capabilities is necessary to distinguish between good and bad arguments.
- What evidence would resolve it: Experiments demonstrating successful mitigation techniques that reduce sycophantic behavior and external influence without significantly compromising model performance or helpfulness.

### Open Question 2
- Question: How does the susceptibility of LLMs to external influence vary across different domains and task complexities?
- Basis in paper: [explicit] The paper conducts experiments across diverse question-answering tasks (PIQA, SIQA, CommonsenseQA, OpenBookQA, WikiQA, GPQA, QuALITY, BoolQ) and finds varying levels of influence, with models being more resistant when they have high unbiased performance.
- Why unresolved: While the paper provides initial insights, it does not fully explore the nuances of how domain-specific knowledge and task complexity affect susceptibility to influence.
- What evidence would resolve it: Comprehensive studies across a wider range of domains and task types, with detailed analysis of how model performance, confidence, and reasoning capabilities correlate with susceptibility to external influence.

### Open Question 3
- Question: What are the potential risks and unintended consequences of using model-generated predictions and explanations in real-world applications?
- Basis in paper: [inferred] The paper highlights the significant prompt-sensitivity of LLMs and the potential risks of incorporating outputs from external sources without thorough scrutiny, including the possibility of "jailbreaking" a model.
- Why unresolved: The paper primarily focuses on controlled experiments and does not fully explore the broader implications and risks of deploying LLMs in real-world scenarios where external influences are prevalent.
- What evidence would resolve it: Case studies and empirical analyses of LLM behavior in real-world applications, identifying specific risks, vulnerabilities, and unintended consequences of relying on model-generated predictions and explanations.

## Limitations

- The study uses a controlled prompt framework that may not fully capture real-world conversational dynamics
- Quality assessment of explanations is subjective and may not reflect human judgment standards
- The research focuses on multiple-choice question answering, which represents a narrow slice of LLM applications

## Confidence

- High confidence: The core finding that LLMs are strongly influenced by external explanations regardless of quality is well-supported by experimental results across multiple datasets and models.
- Medium confidence: The claim about authority and confidence effects has experimental support but shows smaller effect sizes, suggesting weaker but still present influence.
- Medium confidence: The calibration analysis is supported by the data, but the connection between poor calibration and influence susceptibility, while plausible, requires further validation.

## Next Checks

1. **Cross-task validation**: Test the influence framework on open-ended generation tasks (story completion, summarization) rather than just multiple-choice questions to determine if susceptibility patterns generalize beyond constrained settings.

2. **Human evaluation correlation**: Compare LLM judge susceptibility to human annotator responses when presented with the same advocate explanations, to validate whether LLM behavior reflects actual human reasoning patterns.

3. **Adversarial robustness test**: Systematically introduce subtle errors into explanations (factual inaccuracies, logical fallacies) rather than just correct vs. incorrect binary labels to map the precision of LLM influence boundaries.