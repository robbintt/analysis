---
ver: rpa2
title: 'Shapley Value Based Multi-Agent Reinforcement Learning: Theory, Method and
  Its Application to Energy Network'
arxiv_id: '2402.15324'
source_url: https://arxiv.org/abs/2402.15324
tags:
- power
- game
- markov
- learning
- value
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis addresses the challenge of credit assignment in multi-agent
  reinforcement learning (MARL), where agents in a cooperative setting only receive
  a global reward without knowing their individual contributions. The author extends
  the convex game and Shapley value from cooperative game theory to Markov decision
  processes, creating Markov convex game and Markov Shapley value.
---

# Shapley Value Based Multi-Agent Reinforcement Learning: Theory, Method and Its Application to Energy Network

## Quick Facts
- arXiv ID: 2402.15324
- Source URL: https://arxiv.org/abs/2402.15324
- Reference count: 0
- This thesis extends cooperative game theory concepts to Markov decision processes for credit assignment in multi-agent reinforcement learning.

## Executive Summary
This thesis addresses the fundamental challenge of credit assignment in cooperative multi-agent reinforcement learning (MARL), where agents receive only a global reward without knowing their individual contributions. The author extends the convex game and Shapley value concepts from cooperative game theory to the Markov decision process framework, creating Markov convex game and Markov Shapley value. This theoretical foundation enables the use of Shapley value as a principled credit assignment scheme in global reward games. The work proposes three MARL algorithms (SHAQ, SQDDPG, and SMFPPO) based on this framework and evaluates them on benchmark tasks like Predator-Prey and StarCraft Multi-Agent Challenge, demonstrating superior performance and interpretability compared to state-of-the-art baselines.

## Method Summary
The thesis establishes a theoretical framework by extending convex games and Shapley value to Markov decision processes, creating Markov convex game and Markov Shapley value. This framework provides the theoretical foundation for using Shapley value as a credit assignment scheme in cooperative MARL with global rewards. Based on this framework, three algorithms are proposed: SHAQ (Shapley value-based Q-learning), SQDDPG (Shapley value-based Deep Deterministic Policy Gradient), and SMFPPO (Shapley value-based Multi-Agent Proximal Policy Optimization). These algorithms incorporate Shapley value calculations to attribute credit to individual agents based on their marginal contributions to the team's performance. The methods are evaluated on benchmark tasks and applied to active voltage control in power distribution networks, demonstrating both superior performance and interpretability.

## Key Results
- Markov Shapley value provides a principled credit assignment scheme for global reward games in cooperative MARL
- SHAQ and SQDDPG algorithms outperform state-of-the-art baselines on Predator-Prey and StarCraft Multi-Agent Challenge environments
- Application to active voltage control in power distribution networks shows potential for real-world deployment
- Markov Shapley values correlate with active and reactive power at bus locations, providing physical interpretability

## Why This Works (Mechanism)
The approach works by leveraging cooperative game theory's Shapley value concept, which fairly distributes credit among agents based on their marginal contributions to the collective outcome. By extending this to Markov decision processes, the framework can handle sequential decision-making in multi-agent settings. The Shapley value calculation considers all possible orderings of agent participation, providing a principled way to attribute credit that accounts for interactions and dependencies between agents. This addresses the credit assignment problem where global rewards alone are insufficient for determining individual agent contributions.

## Foundational Learning
1. **Shapley Value**: A solution concept in cooperative game theory that fairly distributes gains among players based on their marginal contributions. Needed to provide a principled credit assignment mechanism in cooperative settings. Quick check: Verify that the sum of Shapley values equals the total payoff.

2. **Markov Decision Process (MDP)**: A mathematical framework for modeling sequential decision-making where outcomes are partly random and partly under the control of a decision-maker. Needed to extend game-theoretic concepts to sequential decision problems. Quick check: Ensure state transitions and rewards follow the Markov property.

3. **Convex Game**: A class of cooperative games where the characteristic function satisfies certain convexity conditions, ensuring the existence of a unique Shapley value. Needed to establish theoretical guarantees for the credit assignment scheme. Quick check: Verify convexity conditions for the characteristic function.

4. **Credit Assignment Problem**: The challenge of determining the contribution of individual agents to the overall team performance in multi-agent systems. Needed to justify the research problem and approach. Quick check: Compare global reward-based learning with credit-assigned learning.

## Architecture Onboarding

**Component Map**: MDP Environment -> Global Reward -> Shapley Value Calculator -> Individual Agent Critics/Actors -> Action Selection

**Critical Path**: State observation → Agent policy → Joint action → Environment transition → Global reward → Shapley value calculation → Individual credit assignment → Policy update

**Design Tradeoffs**: The primary tradeoff is between the accuracy of Shapley value calculations (which scale exponentially with the number of agents) and computational efficiency. The thesis introduces approximations to address this, trading some theoretical precision for practical feasibility.

**Failure Signatures**: Poor performance may indicate incorrect Shapley value attribution, particularly in highly non-linear or non-convex environments where the Shapley value assumptions break down. Excessive variance in training could suggest unstable credit assignment due to approximation errors.

**First Experiments**: 
1. Compare credit assignment methods (global reward vs. Shapley-based) on a simple 2-agent coordination task
2. Test scalability by varying the number of agents in the Predator-Prey environment
3. Validate the interpretability of Shapley values by correlating them with known agent contributions in a controlled setting

## Open Questions the Paper Calls Out
None

## Limitations
- Computational complexity of Shapley value calculations scales exponentially with the number of agents, requiring approximations
- Physical interpretation of Shapley values in power distribution networks needs validation across diverse grid configurations
- Application to active voltage control is demonstrated on a specific network topology, limiting generalizability

## Confidence
- **Theoretical Framework**: High - The extension of cooperative game theory to MDPs is mathematically sound
- **Algorithmic Performance**: High - Demonstrated superiority on benchmark tasks against state-of-the-art baselines
- **Physical Interpretability**: Medium - Correlation with power metrics observed but requires broader validation
- **Real-world Applicability**: Medium - Promising results on power systems but limited to specific topology

## Next Checks
1. Evaluate the algorithms on power systems with varying network topologies and load conditions to test robustness and generalizability
2. Conduct ablation studies to quantify the impact of different Shapley value approximation methods on both performance and interpretability
3. Test the credit assignment interpretability on human experts in the power systems domain to validate the practical utility of the physical insights provided by the Markov Shapley values