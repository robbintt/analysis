---
ver: rpa2
title: Promoting Target Data in Context-aware Neural Machine Translation
arxiv_id: '2402.06342'
source_url: https://arxiv.org/abs/2402.06342
tags:
- context
- target
- source
- translation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper investigates whether target language data should be promoted
  in context-aware neural machine translation (NMT). It proposes simple concatenation-based
  approaches where target context sentences are prepended to the source sentence,
  either in isolation or combined with source context.
---

# Promoting Target Data in Context-aware Neural Machine Translation

## Quick Facts
- arXiv ID: 2402.06342
- Source URL: https://arxiv.org/abs/2402.06342
- Authors: Harritxu Gete; Thierry Etchegoyhen
- Reference count: 16
- Key outcome: Including target context in source leads to significant improvements on target language phenomena while maintaining or slightly degrading source-dependent phenomena.

## Executive Summary
This paper investigates whether target language data should be promoted in context-aware neural machine translation by comparing standard concatenation-based approaches with variants that prioritize target context. The authors propose simple concatenation-based approaches where target context sentences are prepended to the source sentence, either in isolation or combined with source context. Experimental results on English-Russian and Basque-Spanish show that including target context on the source side significantly improves translation of target language phenomena (lexical cohesion, deixis, register consistency) while maintaining performance on source-dependent phenomena.

## Method Summary
The method uses concatenation-based context-aware NMT with Transformer-base models trained with Marian. The key variants are: nton (standard concatenation), tgt-nton (target context prepended to source), and src+tgt-nton (both source and target context prepended to source). Models are initialized from sentence-level weights and trained on document-level data. Evaluation uses both parallel test sets (BLEU) and contrastive test sets for discourse phenomena, with analysis of performance across different distances to disambiguating context.

## Key Results
- tgt-nton models achieve significant improvements on target language phenomena (deixis, lexical cohesion, gender selection) compared to standard nton
- Combining source and target context (src+tgt-nton) leads to significant gains across all phenomenon types, including source-dependent phenomena
- tgt-nton variants prove more robust than nton when using back-translated data for training and inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Target language context on the source side acts as a direct reference for target-side discourse phenomena.
- Mechanism: When target context is prepended to the source sentence, the encoder processes it as foreign tokens but the decoder can directly copy the correct target-side information for phenomena like lexical cohesion, deixis, and register consistency.
- Core assumption: The NMT model can effectively ignore or minimally process foreign language context while still benefiting from target-side reference information during decoding.
- Evidence anchors:
  - [abstract] "including target context in the source leads to large improvements on target language phenomena"
  - [section 3] "most discourse-level phenomena feature information that is either present mainly in the target language"
  - [corpus] Weak - corpus neighbors show related work but don't directly confirm this copying mechanism
- Break condition: If the encoder fails to treat foreign tokens as copyable units, or if the model overfits to source-side information and ignores target context.

### Mechanism 2
- Claim: Combining source and target context provides complementary information for phenomena requiring both sides.
- Mechanism: Source context provides disambiguation for source-side phenomena while target context ensures consistency for target-side phenomena, allowing the model to leverage both sources of information.
- Core assumption: The model can effectively integrate and prioritize different types of context information based on the translation task.
- Evidence anchors:
  - [abstract] "combining source and target context on the source side leads to significant gains across the board"
  - [section 3] "phenomena for which the relevant context information is in both the source and the target context"
  - [corpus] Weak - neighbors discuss multi-encoder approaches but don't specifically validate this integration mechanism
- Break condition: If the model cannot effectively balance source and target context, leading to confusion or incorrect prioritization.

### Mechanism 3
- Claim: Back-translated target context maintains target-side information quality while providing scalability.
- Mechanism: Even with back-translation errors, the target context retains target-language information that helps with target-side phenomena, while source context remains available for source-side disambiguation.
- Core assumption: Back-translated target context retains sufficient target-language information to be useful despite potential translation errors.
- Evidence anchors:
  - [section 7] "the tgt-nton variants proved more robust than the nton model" when using back-translated data
  - [section 7] "the X-tgt-nton models overall" are particularly exploitable with back-translated data
  - [corpus] Weak - corpus neighbors don't specifically address back-translation quality for context
- Break condition: If back-translation quality degrades too much, losing target-side information integrity.

## Foundational Learning

- Concept: Concatenation-based context-aware NMT
  - Why needed here: This work builds on and extends concatenation-based approaches by promoting target language data
  - Quick check question: What are the main variants of concatenation-based approaches in context-aware NMT?

- Concept: Document-level phenomena in machine translation
  - Why needed here: The paper specifically targets discourse-level phenomena like lexical cohesion, deixis, and gender selection
  - Quick check question: What are the four broad categories of context-dependent phenomena described in the paper?

- Concept: Contrastive evaluation for discourse phenomena
  - Why needed here: Standard BLEU scores are insufficient for evaluating document-level translation quality
  - Quick check question: Why do challenge test sets use reference translations for context instead of machine-translated context?

## Architecture Onboarding

- Component map:
  - Encoder: Processes concatenated source sentence with prepended target context
  - Decoder: Generates translation using both source and target context information
  - Special token: Separates context sentences in the input sequence
  - BPE model: Jointly learned on source and target data for better foreign token processing

- Critical path:
  1. Prepare training data with context concatenation
  2. Train model with target context prepended to source
  3. At inference, use previously translated sentences as target context
  4. Evaluate using both parallel test sets (BLEU) and challenge sets (contrastive accuracy)

- Design tradeoffs:
  - More context increases computational cost but improves discourse handling
  - Using target context on source side may introduce noise but enables target-side reference
  - Back-translated context provides scalability but may reduce quality

- Failure signatures:
  - BLEU scores drop significantly with target context
  - Contrastive accuracy degrades on source-dependent phenomena
  - Model fails to maintain consistency in target language

- First 3 experiments:
  1. Compare tgt-nton vs nton on target-language phenomena (Deixis, Lexical Cohesion)
  2. Test src+tgt-nton vs tgt-nton on source-dependent phenomena (Ellipsis tests)
  3. Evaluate impact of machine-translated vs reference context on parallel test sets

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How widespread are contextual phenomena that require only source context information compared to those requiring target or both source/target context?
- Basis in paper: [explicit] The paper identifies four categories of contextual phenomena but notes uncertainty about the prevalence of source-only cases.
- Why unresolved: The paper acknowledges this gap but doesn't provide empirical data on the relative frequency of these phenomena.
- What evidence would resolve it: A comprehensive corpus analysis quantifying the distribution of different contextual phenomenon types across various language pairs and domains.

### Open Question 2
- Question: What is the optimal balance between source and target context when combining them for context-aware NMT?
- Basis in paper: [explicit] The paper shows that combining source and target context performs best, but doesn't explore different ratios or positioning strategies.
- Why unresolved: The experiments only tested fixed ratios (e.g., 1:1) and specific concatenation orders without exploring the full parameter space.
- What evidence would resolve it: Systematic experiments varying the ratio and positioning of source vs. target context sentences to find optimal configurations.

### Open Question 3
- Question: How does context distance affect the performance of target-promoted models compared to traditional approaches?
- Basis in paper: [explicit] The paper measures accuracy at different distances but focuses on a limited range (up to 5 sentences).
- Why unresolved: The analysis is limited to short distances, and the paper doesn't explore the degradation patterns over longer distances or across different phenomenon types.
- What evidence would resolve it: Extended experiments measuring accuracy across much longer context distances (10+ sentences) and analyzing degradation patterns for different contextual phenomenon categories.

## Limitations
- The exact mechanism by which the encoder processes foreign language tokens remains underspecified
- The quality threshold at which back-translated context becomes detrimental is not quantified
- The degradation pattern for distant contexts and specific distance thresholds are not fully characterized

## Confidence

**High Confidence**: The core experimental results showing that tgt-nton variants improve contrastive accuracy on target language phenomena (deixis, lexical cohesion, gender selection) while maintaining BLEU scores.

**Medium Confidence**: The mechanism explanation for why target context on the source side works - the direct copying hypothesis is plausible but not directly validated.

**Low Confidence**: The scalability claims regarding back-translated data and the generalization of results across different language pairs and document domains.

## Next Checks

1. **Neural Attention Analysis**: Conduct attention weight analysis to verify whether the decoder actually copies from target context tokens when translating target language phenomena.

2. **Back-translation Quality Threshold**: Systematically evaluate model performance across different levels of back-translation quality to determine the minimum quality threshold where back-translated target context remains beneficial.

3. **Context Distance Decay Study**: Extend the distance analysis to include more granular distance bins and characterize the exact decay patterns for different phenomena.