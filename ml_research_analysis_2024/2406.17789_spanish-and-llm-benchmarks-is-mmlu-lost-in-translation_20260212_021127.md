---
ver: rpa2
title: 'Spanish and LLM Benchmarks: is MMLU Lost in Translation?'
arxiv_id: '2406.17789'
source_url: https://arxiv.org/abs/2406.17789
tags:
- translation
- questions
- spanish
- english
- benchmarks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study analyzes the impact of automatic translation on the
  evaluation of Large Language Models (LLMs) in languages other than English. Specifically,
  it investigates the use of automated translation tools to translate the MMLU benchmark
  into Spanish and then run it on ChatGPT4.
---

# Spanish and LLM Benchmarks: is MMLU Lost in Translation?

## Quick Facts
- **arXiv ID**: 2406.17789
- **Source URL**: https://arxiv.org/abs/2406.17789
- **Reference count**: 2
- **Primary result**: Automatic translation of MMLU into Spanish introduces significant bias, with translation errors causing incorrect LLM answers

## Executive Summary
This study investigates the impact of automatic translation on Large Language Model evaluation using the MMLU benchmark in Spanish. The researchers found that a significant portion of incorrect answers when ChatGPT4 processes Spanish-translated MMLU questions can be attributed to translation errors rather than genuine model limitations. This reveals a critical bias in multilingual LLM evaluation: results depend not only on the model's language capabilities but also on the quality of automatic translation tools. The findings suggest that current non-English LLM benchmarks may systematically underestimate model performance and call for improved translation practices or language-specific benchmark adaptation with expert input.

## Method Summary
The study analyzed the use of automated translation tools to translate the MMLU benchmark into Spanish and evaluated ChatGPT4's performance on these translated questions. The methodology involved comparing ChatGPT4's responses to the correct answers in the translated Spanish version, identifying instances where the model provided incorrect answers. The researchers then examined whether these incorrect answers could be traced back to errors or ambiguities introduced during the automatic translation process from English to Spanish.

## Key Results
- Automatic translation of MMLU into Spanish introduces significant bias in LLM evaluation
- A substantial fraction of incorrect Spanish answers stem from translation errors rather than model limitations
- Benchmark results depend on both LLM performance and translation quality, creating systematic evaluation bias

## Why This Works (Mechanism)
The study demonstrates that automatic translation tools, while improving, still introduce errors and ambiguities when translating complex technical and academic content. These translation errors propagate through to the LLM's understanding of the question, leading to incorrect answers even when the model has the requisite knowledge. This mechanism shows that multilingual evaluation using translated benchmarks conflates two separate issues: the model's actual language capability and the translation tool's accuracy.

## Foundational Learning
- **Benchmark translation methodology**: Understanding how automated tools handle complex academic language is crucial for evaluating their impact on LLM performance assessment.
- **Error classification in LLM evaluation**: The ability to distinguish between model knowledge gaps and translation-induced errors is essential for accurate benchmarking.
- **Multilingual model training data composition**: Knowing whether an LLM was trained on translated content versus natively multilingual data affects how translation errors impact performance.
- **Automatic translation quality assessment**: Evaluating translation accuracy requires understanding both linguistic nuances and domain-specific terminology handling.

## Architecture Onboarding
- **Component map**: Translation Tool -> Benchmark Questions -> LLM Input -> Model Processing -> Answer Output
- **Critical path**: Translation accuracy directly impacts the information available to the LLM, making it the most critical factor in this evaluation chain.
- **Design tradeoffs**: Using automatic translation offers scalability and cost-effectiveness but sacrifices accuracy compared to professional human translation.
- **Failure signatures**: Incorrect answers that align with known translation errors or ambiguities indicate bias rather than model limitations.
- **3 first experiments**: 1) Compare automatic vs human translation performance on same LLM, 2) Test multiple LLMs with same translated benchmark, 3) Analyze specific error patterns in translated questions that lead to incorrect answers

## Open Questions the Paper Calls Out
None

## Limitations
- The study only tested ChatGPT4 and the MMLU benchmark, limiting generalizability to other models and tests
- The methodology lacks detailed error classification and quantitative attribution of errors to translation vs model limitations
- The analysis focuses exclusively on multiple-choice format questions, not capturing open-ended task challenges

## Confidence
- **Core claim (translation bias exists)**: Medium confidence - logical argument supported but limited scope and lack of quantitative error analysis
- **Recommendation (improve non-English benchmarks)**: High confidence - logical solution following from observed problems
- **Quantification of translation error impact**: Low confidence - exact proportion not measured and methodology unclear

## Next Checks
1. Conduct detailed quantitative error classification with inter-rater reliability to establish exact proportion of errors attributable to translation issues
2. Test the same translated benchmark across multiple LLMs with different training approaches to assess consistency of translation bias
3. Create a small subset of professionally translated MMLU questions and compare performance against automatic translation to quantify performance gaps