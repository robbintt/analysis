---
ver: rpa2
title: Cross-Dataset Generalization For Retinal Lesions Segmentation
arxiv_id: '2405.08329'
source_url: https://arxiv.org/abs/2405.08329
tags:
- datasets
- dataset
- lesions
- segmentation
- different
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study addresses the challenge of generalizing retinal lesion
  segmentation models across multiple datasets with varying annotation styles and
  qualities. The authors characterize five publicly available fundus image datasets
  and analyze the impact of combining coarsely and finely annotated data on segmentation
  performance.
---

# Cross-Dataset Generalization For Retinal Lesions Segmentation

## Quick Facts
- arXiv ID: 2405.08329
- Source URL: https://arxiv.org/abs/2405.08329
- Reference count: 0
- Primary result: Combining fine-grained datasets improves retinal lesion segmentation performance more than using coarse datasets alone, with ensembles providing consistent gains but SWA and model soups showing limited benefits.

## Executive Summary
This study investigates cross-dataset generalization for retinal lesion segmentation across five publicly available fundus image datasets with varying annotation styles. The authors characterize datasets into coarse and fine-grained clusters based on annotation consistency, then evaluate three generalization techniques: model ensembles, Stochastic Weight Averaging (SWA), and model soups applied to a U-Net with ResNet-101 encoder. They find that combining fine-grained datasets yields better performance than using coarse datasets alone, with ensembles providing reliable improvements despite higher computational cost. Notably, SWA and model soups showed limited effectiveness for segmentation compared to their success in classification tasks, suggesting fundamental differences in loss landscape properties between these domains.

## Method Summary
The authors trained U-Net architectures with ResNet-101 encoders on five retinal fundus image datasets (IDRID, DDR, FGADR, RET, MESSIDOR-MAPLES-DR) using Optuna for hyperparameter optimization. Data preprocessing included cropping black backgrounds, resizing to 1536×1536 pixels, and extracting random 512×512 patches during training with geometric and color augmentations. Three generalization techniques were evaluated: model ensembles (averaging predictions from multiple independently trained models), SWA (averaging weights from models with different hyperparameters or checkpoints), and model soups (combining weights from models trained with different random seeds). Performance was measured using Dice scores and area under precision-recall curves for various lesion types across datasets.

## Key Results
- Combining fine-grained datasets (IDRID, DDR, MESSIDOR, FGADR) consistently outperformed using coarse datasets alone
- Model ensembles provided reliable performance improvements across all dataset combinations
- SWA and model soups showed limited benefits for segmentation, contrasting with their effectiveness in classification tasks
- Performance varied significantly based on annotation style compatibility between training and test datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Combining fine-grained datasets improves segmentation performance more than using coarse datasets alone.
- Mechanism: Fine-grained datasets contain more detailed annotations, providing richer information about lesion boundaries and sizes. When combined with other fine datasets, the model learns more precise features across similar annotation styles.
- Core assumption: Lesions are annotated consistently within each "fine-grained" cluster, so combining them reinforces shared features.
- Evidence anchors:
  - [abstract] "combining fine-grained datasets (IDRID, DDR, MESSIDOR, FGADR) yields better performance than using coarse datasets alone"
  - [section] "We thereby identified three clusters in terms of labelling styles: coarse for RET, fine-grained for IDRID, MESSIDOR and DDR"
  - [corpus] Weak - no direct citation on annotation consistency.
- Break condition: If annotation styles within the "fine-grained" cluster are actually inconsistent, combining them could confuse the model rather than help.

### Mechanism 2
- Claim: Model ensembles consistently improve generalization performance despite higher computational cost.
- Mechanism: Ensembles average predictions from multiple independently trained models, reducing variance and overfitting to dataset-specific artifacts.
- Core assumption: Individual models capture different aspects of the data distribution, and averaging preserves robust features while canceling noise.
- Evidence anchors:
  - [abstract] "ensembles providing consistent improvements at the cost of increased computation"
  - [section] "Model ensemble: as an approach to improve generalization, model ensembles can be built at inference time. Multiple trained models are used and their predictions are then simply averaged"
  - [corpus] Weak - no direct citation on ensemble effectiveness for segmentation.
- Break condition: If all models overfit to the same dataset-specific biases, ensembling won't improve generalization.

### Mechanism 3
- Claim: SWA and model soups showed limited benefits in segmentation compared to classification tasks.
- Mechanism: These methods rely on averaging weights from models trained with slightly different hyperparameters or checkpoints, assuming smoother loss landscapes and better minima.
- Core assumption: The loss landscape for segmentation is less smooth than for classification, making weight averaging less effective.
- Evidence anchors:
  - [abstract] "SWA and model soups showed limited benefits in this segmentation context, contrasting with their effectiveness in classification tasks"
  - [section] "Interestingly, for the model soup, combining the weights of the decoder or of the full-model simply did not work"
  - [corpus] Weak - no direct citation explaining why segmentation differs from classification.
- Break condition: If the segmentation loss landscape is actually smooth, these methods should work better.

## Foundational Learning

- Concept: Understanding dataset annotation styles and their impact on model generalization
  - Why needed here: The study shows that combining datasets with similar annotation styles (fine-grained vs coarse) affects performance differently than mixing styles
  - Quick check question: Why did combining RET (coarse) with fine-grained datasets sometimes hurt performance?

- Concept: Weight averaging techniques (SWA and model soups) and their typical use cases
  - Why needed here: The study found these techniques worked well for classification but not segmentation, requiring understanding of their assumptions
  - Quick check question: What's the key difference between how SWA and model soups average model weights?

- Concept: Ensemble methods and their computational tradeoffs
  - Why needed here: Ensembles improved performance consistently but at higher computational cost, requiring understanding of when to use them
  - Quick check question: How does model ensemble averaging reduce overfitting compared to single model training?

## Architecture Onboarding

- Component map:
  - U-Net architecture with ResNet-101 encoder (backbone) -> Decoder network for segmentation mask generation -> Training pipeline with Optuna hyperparameter optimization -> Data preprocessing (cropping, resizing to 1536×1536, patch extraction) -> Evaluation metrics (Dice score, precision-recall curves)

- Critical path:
  1. Data preprocessing and augmentation
  2. Model training with hyperparameter optimization
  3. Performance evaluation on test sets
  4. Generalization technique application (ensembles, SWA, model soups)

- Design tradeoffs:
  - Full image resolution (1536×1536) vs. computational efficiency
  - Fine-grained vs coarse annotations - richer information vs annotation cost
  - Ensemble size vs inference speed
  - Model complexity vs overfitting risk

- Failure signatures:
  - Poor performance on datasets with different annotation styles indicates domain shift
  - SWA/model soups not improving performance suggests irregular loss landscape
  - Large performance variance across hyperparameter seeds indicates unstable training

- First 3 experiments:
  1. Train baseline U-Net on single fine-grained dataset, measure Dice score
  2. Combine two fine-grained datasets, train and compare performance
  3. Apply model ensemble to best-performing fine-grained combination

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions do model soups and stochastic weight averaging (SWA) techniques provide performance benefits in retinal lesion segmentation tasks?
- Basis in paper: [explicit] The authors state that these techniques did not provide the performance boosts observed in their original papers (aimed at classification) and suggest this may be due to the smaller amount of data available for segmentation and higher variability in ground truth annotations.
- Why unresolved: The study only tested these techniques on the specific combination of five retinal datasets with their particular characteristics. The effectiveness of these methods likely depends on dataset size, annotation variability, and other factors not fully explored.
- What evidence would resolve it: Systematic experiments varying dataset size, annotation quality, and lesion diversity would clarify when these techniques are effective for segmentation tasks.

### Open Question 2
- Question: How does the style of lesion annotations (coarse vs. fine-grained) impact the performance of disease classification models that use segmentation outputs as inputs?
- Basis in paper: [explicit] The authors note that an important question remains regarding the effect of lesion labelling style on disease classification and whether it influences the performance of segmentation-guided classifiers.
- Why unresolved: The study focused exclusively on segmentation performance and did not investigate how different annotation styles might affect downstream classification tasks that rely on segmentation outputs.
- What evidence would resolve it: Comparative studies training classification models on segmentations produced from coarsely vs. finely annotated training data would reveal the impact on classification accuracy.

### Open Question 3
- Question: Can source-domain adaptation techniques effectively compensate for differences between retinal lesion datasets, including variations in both image appearance and label space?
- Basis in paper: [explicit] The authors mention that they left unexplored the body of work on source-domain adaptation, which could compensate for differences between datasets, including in the label space.
- Why unresolved: The study did not test any domain adaptation approaches to address dataset differences, focusing instead on combining datasets directly and using ensemble methods.
- What evidence would resolve it: Experiments comparing segmentation performance when applying domain adaptation techniques (such as adversarial training, domain-invariant feature learning, or label space alignment) versus direct dataset combination would demonstrate their effectiveness.

## Limitations
- The study lacks quantitative measures of annotation consistency within identified clusters, making it difficult to verify the assumption that fine-grained datasets share similar annotation styles.
- The ineffectiveness of SWA and model soups for segmentation is reported but not mechanistically explained - whether this stems from segmentation loss landscapes being inherently less smooth or from implementation-specific factors remains unclear.
- Computational costs of ensemble methods are noted but not precisely quantified against performance gains, limiting understanding of the efficiency tradeoffs.

## Confidence
- **High Confidence**: Fine-grained dataset combinations improve performance over coarse datasets - directly measured and statistically supported.
- **Medium Confidence**: Model ensembles provide consistent improvements - observed across experiments but computational costs not fully characterized.
- **Low Confidence**: SWA and model soups limited effectiveness in segmentation - result is reported but mechanistic explanation is absent.

## Next Checks
1. **Annotation Consistency Analysis**: Quantify inter-annotator agreement and annotation variability within each dataset cluster to verify the assumption that "fine-grained" datasets share consistent annotation styles.
2. **Loss Landscape Analysis**: Compare segmentation versus classification loss landscapes through curvature measurements or sharpness analysis to test whether SWA effectiveness differences stem from landscape properties.
3. **Ensemble Efficiency Study**: Systematically measure the performance-to-computation tradeoff across different ensemble sizes and combinations to identify optimal ensemble configurations.