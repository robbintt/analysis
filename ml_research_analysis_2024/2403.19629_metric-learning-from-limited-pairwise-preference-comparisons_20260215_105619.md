---
ver: rpa2
title: Metric Learning from Limited Pairwise Preference Comparisons
arxiv_id: '2403.19629'
source_url: https://arxiv.org/abs/2403.19629
tags:
- metric
- user
- subspace
- learning
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies metric learning from limited pairwise preference
  comparisons under the ideal point model. It shows that learning an unknown Mahalanobis
  distance from o(d) comparisons per user is generally impossible unless items exhibit
  subspace-clusterable structure.
---

# Metric Learning from Limited Pairwise Preference Comparisons

## Quick Facts
- arXiv ID: 2403.19629
- Source URL: https://arxiv.org/abs/2403.19629
- Reference count: 40
- One-line primary result: Learning an unknown Mahalanobis distance from o(d) comparisons per user is impossible unless items exhibit subspace-clusterable structure

## Executive Summary
This paper studies the problem of metric learning from limited pairwise preference comparisons under the ideal point model. The authors show that learning an unknown Mahalanobis distance from o(d) comparisons per user is generally impossible unless items exhibit subspace-clusterable structure. When items lie in a union of low-dimensional subspaces, they propose a divide-and-conquer approach that recovers the metric by learning subspace metrics and reconstructing the full metric. Theoretical recovery guarantees are provided for both noiseless and noisy binary responses.

## Method Summary
The method employs a divide-and-conquer approach to metric learning. First, it learns Mahalanobis metrics within individual subspaces using maximum likelihood estimation under a probabilistic noise model. Then, it reconstructs the full metric by combining subspace metrics through robust regression and projecting onto the positive semidefinite cone. This approach exploits the structure of items lying in a union of low-dimensional subspaces, where each user's comparisons contribute information about the metric restricted to specific subspaces.

## Key Results
- It is generally impossible to learn anything about the metric from o(d) comparisons per user unless items have subspace-clusterable structure
- The divide-and-conquer approach can recover the metric from noiseless comparisons when items lie in a union of low-dimensional subspaces
- The algorithm provides recovery guarantees for noisy, quantized binary responses on subspace-clusterable items

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Learning the metric from very few comparisons per user is possible when items exhibit subspace-clusterable structure.
- **Mechanism:** Each user provides preference comparisons within low-dimensional subspaces. The subspace metrics are learned separately and then stitched together via linear reconstruction to recover the full metric.
- **Core assumption:** Items lie in a union of low-dimensional subspaces, and these subspaces collectively span enough degrees of freedom of the Mahalanobis metric.
- **Evidence anchors:**
  - [abstract] "However, when comparisons are made over items that exhibit low-dimensional structure, each user can contribute to learning the metric restricted to a low-dimensional subspace so that the metric can be jointly identified."
  - [section] "We present a divide-and-conquer approach that achieves this, and provide theoretical recovery guarantees and empirical validation."
- **Break condition:** If items do not lie in a union of sufficiently rich subspaces, the metric cannot be reconstructed from subspace metrics alone.

### Mechanism 2
- **Claim:** With subspace-clusterable items, even noisy and quantized binary responses suffice for approximate metric recovery.
- **Mechanism:** Maximum likelihood estimation is used to recover each subspace metric under a probabilistic noise model. Subspace metrics are then combined via robust regression and projection onto the positive semidefinite cone.
- **Core assumption:** User responses follow a known probabilistic model (e.g., logistic sigmoid link) and the noise level is bounded.
- **Evidence anchors:**
  - [abstract] "Given noisy, quantized comparisons in the form of binary responses over subspace-clusterable items, we present recovery guarantees in terms of identification errors for our approach."
  - [section] "Approximate recovery in each subspace is known to be possible... we present a version of Theorem 4.1 of Canal et al. (2022) adapted to subspaces."
- **Break condition:** If the noise model is misspecified or noise levels are too high, recovery guarantees no longer hold.

### Mechanism 3
- **Claim:** It is generally impossible to learn anything about the metric from o(d) comparisons per user unless items have subspace-clusterable structure.
- **Mechanism:** The negative result shows that without sufficient structure, each user introduces new degrees of freedom (ideal points) that prevent any metric information from being gleaned.
- **Core assumption:** Items have generic pairwise relations, meaning no low-dimensional structure exists.
- **Evidence anchors:**
  - [abstract] "We show that in general, o(d) comparisons reveal no information about the metric, even with infinitely many users."
  - [section] "We provide an impossibility result: nothing can be learned if the items are in general position."
- **Break condition:** If items have even approximate subspace structure, the negative result no longer applies.

## Foundational Learning

- **Concept:** Mahalanobis distance and its matrix representation.
  - Why needed here: The paper's goal is to recover an unknown Mahalanobis metric from preference comparisons.
  - Quick check question: What does the matrix representation of a Mahalanobis distance look like, and how is it related to the metric's unit sphere?

- **Concept:** Subspace-clusterable structure and quadratic spanning.
  - Why needed here: The paper's divide-and-conquer approach relies on items lying in a union of subspaces that collectively span the metric's degrees of freedom.
  - Quick check question: What does it mean for a set of items to quadratically span a subspace, and why is this condition necessary and sufficient for learning the subspace metric?

- **Concept:** Maximum likelihood estimation under a probabilistic noise model.
  - Why needed here: The paper's algorithm for learning from binary responses assumes a known probabilistic model for user responses.
  - Quick check question: How does the maximum likelihood estimator work in this context, and what are the implications of a misspecified noise model?

## Architecture Onboarding

- **Component map:** Generate synthetic data -> Collect preference comparisons -> Learn subspace metrics -> Reconstruct full metric
- **Critical path:** The critical path is learning each subspace metric accurately, as errors in subspace metric estimation propagate to the full metric reconstruction.
- **Design tradeoffs:** The algorithm trades off the number of subspaces, users per subspace, and comparisons per user. More subspaces or users can compensate for fewer comparisons per user.
- **Failure signatures:** If the items do not exhibit sufficient subspace-clusterable structure, the algorithm will fail to recover the metric. If the noise model is misspecified, the subspace metric estimates will be biased.
- **First 3 experiments:**
  1. Verify that the algorithm can recover the metric from noiseless, unquantized comparisons on synthetic subspace-clusterable data.
  2. Test the algorithm's robustness to varying noise levels in the binary response model.
  3. Evaluate the algorithm's performance on data that approximately lies in a union of subspaces, rather than exactly.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the divide-and-conquer approach for metric learning extend to more general low-dimensional structures beyond unions of subspaces, such as manifolds or sparse representations?
- Basis in paper: [inferred] The paper focuses on subspace-clusterable items but suggests the approach may apply more broadly.
- Why unresolved: The paper does not explore alternative low-dimensional structures or provide theoretical guarantees for them.
- What evidence would resolve it: Experimental results or theoretical analysis showing successful metric recovery from preference comparisons on data exhibiting manifold or sparse structures.

### Open Question 2
- Question: How does the performance of the divide-and-conquer algorithm scale with the ambient dimension d when the number of subspaces and their dimensions are fixed?
- Basis in paper: [inferred] The paper provides some empirical results but does not systematically study the scaling behavior.
- Why unresolved: The paper does not present a comprehensive analysis of how the algorithm's performance varies with d.
- What evidence would resolve it: Empirical results or theoretical bounds demonstrating the algorithm's performance as a function of d for fixed subspace structures.

### Open Question 3
- Question: Can the divide-and-conquer approach be adapted to handle preference comparisons with non-uniform noise levels across users or items?
- Basis in paper: [explicit] The paper mentions that noise can depend on the user and items but does not explore this scenario.
- Why unresolved: The paper assumes uniform noise levels and does not investigate the impact of non-uniform noise.
- What evidence would resolve it: Experimental results or theoretical analysis showing the algorithm's performance under non-uniform noise conditions.

### Open Question 4
- Question: What is the impact of the choice of link function in the probabilistic model on the recovery guarantees for the divide-and-conquer algorithm?
- Basis in paper: [explicit] The paper uses a logistic sigmoid link function but does not systematically study the impact of different link functions.
- Why unresolved: The paper does not provide a comprehensive analysis of how the choice of link function affects the algorithm's performance.
- What evidence would resolve it: Experimental results or theoretical analysis comparing the algorithm's performance under different link functions.

## Limitations
- The algorithm's performance on real-world datasets with approximate subspace structure remains untested
- The sensitivity to response model misspecification is not fully characterized
- The theoretical guarantees assume exact subspace clustering, but real data may only exhibit approximate structure

## Confidence
**High confidence**: The impossibility result showing o(d) comparisons are insufficient without subspace structure.
**Medium confidence**: The divide-and-conquer algorithm's recovery guarantees for noiseless comparisons.
**Low confidence**: The algorithm's performance with noisy, quantized binary responses on approximately subspace-clusterable data.

## Next Checks
1. Test the algorithm on real-world recommendation datasets to evaluate performance on approximately subspace-clusterable data
2. Conduct sensitivity analysis by varying the noise model and assessing recovery quality under different link functions
3. Compare the divide-and-conquer approach against alternative metric learning methods that don't require subspace structure assumptions