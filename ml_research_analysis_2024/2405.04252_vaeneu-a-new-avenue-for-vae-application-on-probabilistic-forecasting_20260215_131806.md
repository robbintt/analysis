---
ver: rpa2
title: 'VAEneu: A New Avenue for VAE Application on Probabilistic Forecasting'
arxiv_id: '2405.04252'
source_url: https://arxiv.org/abs/2405.04252
tags: []
core_contribution: This paper proposes VAEneu, a novel autoregressive method for univariate
  probabilistic time series forecasting. The approach leverages a conditional VAE
  framework and optimizes the lower bound of the predictive distribution likelihood
  using the Continuous Ranked Probability Score (CRPS) as the loss function.
---

# VAEneu: A New Avenue for VAE Application on Probabilistic Forecasting

## Quick Facts
- arXiv ID: 2405.04252
- Source URL: https://arxiv.org/abs/2405.04252
- Reference count: 40
- Key outcome: Novel autoregressive method for univariate probabilistic time series forecasting using conditional VAE with CRPS loss, achieving best performance on 5 of 12 datasets

## Executive Summary
This paper introduces VAEneu, a novel autoregressive method for univariate probabilistic time series forecasting that leverages a conditional VAE framework optimized with the Continuous Ranked Probability Score (CRPS). By replacing traditional likelihood-based losses with CRPS, the method learns sharp and well-calibrated predictive distributions without restrictive assumptions. VAEneu is rigorously benchmarked against 12 baseline models across 12 diverse datasets, demonstrating superior forecasting performance on multiple datasets while maintaining competitive results on others.

## Method Summary
VAEneu is a conditional variational autoencoder (CVAE) framework that optimizes the lower bound of predictive distribution likelihood using CRPS as the loss function. The model consists of an encoder that maps historical data and target values to latent distribution parameters, a decoder that generates forecast samples conditioned on historical data, and an auto-regressive extension for multi-step forecasting. During training, CRPS is approximated using multiple forecast samples to encourage both sharpness (via MAE minimization) and calibration (via diversity promotion). The method is implemented in two variants: VAEneu-RNN using LSTM layers and VAEneu-TCN using temporal convolutional networks.

## Key Results
- VAEneu achieves the best performance on five out of twelve benchmark datasets
- The method closely follows the best-performing model on the remaining datasets
- VAEneu provides effective quantification of future uncertainties through well-calibrated predictive distributions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: VAEneu achieves superior calibration and sharpness by optimizing CRPS instead of likelihood.
- Mechanism: CRPS balances minimizing mean absolute error (sharpness) and maximizing forecast sample diversity (calibration), avoiding overly sharp distributions that occur with MSE reconstruction loss.
- Core assumption: CRPS is a strictly proper scoring rule that aligns the predictive distribution with the true data distribution when minimized.
- Evidence anchors: Abstract mentions optimizing CRPS as a strictly proper scoring rule; section 3.3 explains CRPS balances contrasting objectives of sharpness and calibration.

### Mechanism 2
- Claim: Conditional VAE framework enables learning of complex predictive distributions without restrictive assumptions.
- Mechanism: By conditioning the VAE on historical data and using CRPS as reconstruction loss, the model learns the true predictive distribution shape rather than assuming Gaussianity.
- Core assumption: The true predictive distribution can be well-approximated by the VAE decoder's implicit distribution.
- Evidence anchors: Abstract states the method learns a sharp and well-calibrated predictive distribution without enforcing restrictive assumptions; section 3.4 describes optimizing CRPS approximation to train a CVAE.

### Mechanism 3
- Claim: Auto-regressive extension enables multi-step forecasting while maintaining calibration.
- Mechanism: During inference, forecasts are fed back as conditioning inputs, allowing the model to generate coherent multi-step predictions that maintain the calibrated distribution learned during training.
- Core assumption: The learned one-step predictive distribution generalizes well when used autoregressively for longer horizons.
- Evidence anchors: Section 3.1 describes extending the model to multi-step forecasting using auto-regression; section 4.3 mentions using 1,000 samples per time step to approximate CRPS during evaluation.

## Foundational Learning

- Concept: Conditional Variational Autoencoder (CVAE)
  - Why needed here: CVAE provides a framework to learn conditional distributions, essential for time series forecasting where we want p(xt+1 | x0:t).
  - Quick check question: How does a CVAE differ from a standard VAE in terms of input and output structure?

- Concept: Continuous Ranked Probability Score (CRPS)
  - Why needed here: CRPS is a proper scoring rule that evaluates both calibration and sharpness of predictive distributions, making it ideal for training probabilistic forecasters.
  - Quick check question: What is the mathematical relationship between CRPS and quantile loss?

- Concept: Auto-regressive forecasting
  - Why needed here: Enables generation of multi-step forecasts from a one-step model by iteratively feeding predictions back as inputs.
  - Quick check question: What are the potential pitfalls of auto-regressive forecasting in terms of error accumulation?

## Architecture Onboarding

- Component map:
  Historical window + target -> Encoder -> Latent distribution -> Sample z -> Decoder (with historical window) -> Forecast samples -> CRPS estimator -> Loss computation -> Backpropagation

- Critical path:
  Input historical window and target → Encoder → Latent distribution → Sample z → Decoder (with historical window) → Forecast samples → CRPS estimation → Loss computation → Backpropagation

- Design tradeoffs:
  - Sample size vs. training time: Larger sample sizes improve CRPS approximation but increase computation
  - Model capacity vs. overfitting: Deeper/larger models can capture more complex patterns but risk overfitting on smaller datasets
  - Conditioning window size vs. computational cost: Longer windows provide more context but increase memory requirements

- Failure signatures:
  - Poor calibration: Predictive distribution consistently under/over-dispersed relative to observations
  - Mode collapse: Forecast samples concentrate around a single value, ignoring data diversity
  - Error accumulation: Multi-step forecasts drift significantly from ground truth over longer horizons

- First 3 experiments:
  1. Train VAEneu with sample size=1 (reduces to MAE loss) and compare CRPS to sample size=8 baseline
  2. Test different conditioning window sizes (e.g., 24 vs 48 timesteps) on a dataset with clear seasonality
  3. Implement and test the auto-regressive wrapper on a dataset with known multi-step patterns (e.g., Mackey-Glass)

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of VAEneu vary across datasets with different characteristics (e.g., stationary vs non-stationary, short vs long history lengths)?
- Basis in paper: The paper benchmarks VAEneu on 12 diverse datasets but does not provide an in-depth analysis of performance variation based on dataset characteristics.
- Why unresolved: The paper focuses on overall performance comparison with baselines rather than analyzing the impact of specific dataset properties on VAEneu's performance.
- What evidence would resolve it: Detailed analysis of VAEneu's performance on subsets of datasets categorized by properties like stationarity, history length, or frequency.

### Open Question 2
- Question: What is the impact of different sample sizes during training on the calibration of the predictive distribution?
- Basis in paper: The paper discusses the effect of sample size on CRPS and training time but does not explore its impact on calibration.
- Why unresolved: The paper focuses on the trade-off between sample size, CRPS, and training time, leaving the impact on calibration unexplored.
- What evidence would resolve it: Experiments varying sample size and evaluating the resulting predictive distributions for calibration using metrics like reliability diagrams or expected calibration error.

### Open Question 3
- Question: How does VAEneu compare to other methods in terms of computational efficiency during inference?
- Basis in paper: The paper mentions that VAEneu is trained using RMSProp optimizer and provides details on training setup, but does not discuss inference efficiency.
- Why unresolved: The paper focuses on training and overall performance, not addressing the computational cost of generating forecasts during inference.
- What evidence would resolve it: Comparative analysis of inference time and memory usage of VAEneu against baseline models on representative datasets.

## Limitations

- The paper lacks detailed architectural specifications, making exact reproduction challenging
- No ablation studies are provided to isolate the contribution of individual design choices
- The method's performance on datasets with complex patterns (regime shifts, non-stationarity) is not thoroughly explored

## Confidence

- Methodological novelty: High - The approach of using CRPS optimization for VAE-based forecasting is clearly novel
- Experimental validation: Medium - Extensive benchmarking across 12 datasets and 12 baselines, but limited architectural transparency
- Reproducibility: Medium - Key implementation details are missing, particularly regarding model architecture and hyperparameters

## Next Checks

1. Conduct ablation studies varying sample size (1, 4, 8, 32) to quantify the impact of CRPS approximation quality on forecast performance
2. Implement and test explicit calibration metrics (e.g., reliability diagrams, calibration error) alongside CRPS to verify the claimed calibration improvements
3. Apply the method to datasets with known challenging characteristics (e.g., regime shifts, non-stationary patterns) to assess robustness beyond the current benchmark suite