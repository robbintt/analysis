---
ver: rpa2
title: 'Toucan: Many-to-Many Translation for 150 African Language Pairs'
arxiv_id: '2407.04796'
source_url: https://arxiv.org/abs/2407.04796
tags:
- languages
- african
- language
- translation
- machine
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Toucan, a many-to-many translation model
  supporting 156 African language pairs, addressing the significant gap in NLP for
  low-resource African languages. The authors developed Cheetah-1.2B and Cheetah-3.7B,
  foundational language models with 1.2 billion and 3.7 billion parameters respectively,
  which were fine-tuned to create Toucan.
---

# Toucan: Many-to-Many Translation for 150 African Language Pairs

## Quick Facts
- arXiv ID: 2407.04796
- Source URL: https://arxiv.org/abs/2407.04796
- Authors: AbdelRahim Elmadany; Ife Adebara; Muhammad Abdul-Mageed
- Reference count: 27
- Primary result: Toucan significantly outperformed other models, achieving 22.11 spBLEU1K score on the test set

## Executive Summary
This paper introduces Toucan, a many-to-many translation model supporting 156 African language pairs, addressing the significant gap in NLP for low-resource African languages. The authors developed Cheetah-1.2B and Cheetah-3.7B, foundational language models with 1.2 billion and 3.7 billion parameters respectively, which were fine-tuned to create Toucan. They also introduced AfroLingu-MT, the largest African MT benchmark to date, and spBLEU1K, an evaluation metric covering 1,003 languages including 614 African languages. Toucan significantly outperformed other models, achieving 22.11 spBLEU1K score on the test set, demonstrating remarkable performance on MT for African languages.

## Method Summary
The authors developed Toucan through a two-stage process: pretraining Cheetah models on 517 African languages and 10 global languages, followed by fine-tuning on the AfroLingu-MT benchmark containing 156 language pairs. They introduced two model variants (Cheetah-1.2B and Cheetah-3.7B) and evaluated them using a newly created spBLEU1K metric that covers 1,003 languages. The fine-tuning process involved 5 epochs with specific learning rates and batch sizes, saving the best checkpoint based on development set performance. The models were compared against established baselines including NLLB-200-1.3B and Aya 13B.

## Key Results
- Toucan models achieved 22.11 spBLEU1K score on the test set, outperforming NLLB-200-1.3B, Aya 13B, and various mT5 variants
- Larger model size (3.7B vs 1.2B vs 580M) consistently provided performance gains across all evaluation metrics
- The many-to-many architecture successfully handled translation between 156 African language pairs without requiring separate models for each direction

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Many-to-many translation architecture enables translation between any supported language pair without requiring separate models for each direction
- Mechanism: By finetuning sequence-to-sequence models on bidirectional parallel data (e.g., English→Yoruba and Yoruba→English pairs from the same source), the model learns to translate in both directions using shared parameters
- Core assumption: Bidirectional parallel data provides sufficient signal for the model to learn translation patterns in both directions
- Evidence anchors:
  - [abstract]: "Toucan models...capable of translating between 46 different languages (43 African languages and the three non-Indigenous major languages in Africa: Arabic, English, and French)"
  - [section 3.3]: "we adopt a multifaceted approach essentially enabling many-to-many translation...allowing for versatile translation possibilities including translation from any language into English"
  - [corpus]: Weak - the corpus shows related papers but doesn't directly validate this specific mechanism
- Break condition: Insufficient parallel data for rare language pairs, or if the model architecture cannot handle bidirectional mapping effectively

### Mechanism 2
- Claim: Larger model size (3.7B vs 1.2B vs 580M) provides significant performance gains for African language translation
- Mechanism: Increased parameter count allows the model to capture more complex linguistic patterns and handle the morphological richness of African languages better
- Core assumption: African languages benefit more from increased model capacity than high-resource languages due to their linguistic diversity
- Evidence anchors:
  - [abstract]: "We refer to these models as 'Cheetah-1.2B' and 'Cheetah-3.7B'"
  - [section 5.3.1]: "Larger models perform better. Again, unsurprisingly, we observe that larger models outperform smaller ones"
  - [corpus]: Weak - corpus evidence is limited and doesn't directly address model size impact on African languages
- Break condition: When the increase in parameters leads to overfitting on limited data or when computational constraints make the larger model impractical

### Mechanism 3
- Claim: Extensive pretraining on 517 African languages creates a foundation that significantly improves downstream MT performance
- Mechanism: The pretraining phase exposes the model to diverse linguistic structures and patterns across many African languages, creating transferable representations that enhance MT fine-tuning
- Core assumption: Linguistic features learned during pretraining on diverse African languages transfer effectively to specific MT tasks
- Evidence anchors:
  - [abstract]: "We present a new sequence-to-sequence large language model that covers 517 African languages and 10 foreign languages"
  - [section 4.1]: "we utilize the same pretraining dataset employed in training the original Cheetah-base model...ensuring consistency in the foundational data across models"
  - [corpus]: Weak - corpus evidence shows related work but doesn't validate the specific pretraining approach
- Break condition: If pretraining data quality is poor or if the languages are too diverse for effective transfer learning

## Foundational Learning

- Concept: Sequence-to-sequence modeling with transformer architecture
  - Why needed here: The translation task requires mapping from source language sequences to target language sequences, which is exactly what seq2seq transformers are designed for
  - Quick check question: What is the difference between encoder-only, decoder-only, and encoder-decoder transformer architectures, and why is encoder-decoder appropriate for translation?

- Concept: Tokenization and subword units (SentencePiece)
  - Why needed here: African languages have rich morphology and many rare words; subword tokenization helps handle this while maintaining vocabulary size
  - Quick check question: How does SentencePiece tokenization handle out-of-vocabulary words compared to word-level tokenization, and why is this important for low-resource languages?

- Concept: Bidirectional parallel corpora and data balancing
  - Why needed here: Many-to-many translation requires data in both directions for each language pair; proper balancing ensures fair performance across all pairs
  - Quick check question: Why is it important to have both English→Yoruba and Yoruba→English data when training a many-to-many translation model?

## Architecture Onboarding

- Component map: Pretraining (Cheetah models) → Finetuning (Toucan models) → Evaluation (AfroLingu-MT benchmark + spBLEU1K metric)
- Critical path: Data collection → Pretraining on 517 languages → Finetuning on 156 language pairs → Evaluation on benchmark
- Design tradeoffs: Model size vs. computational cost vs. performance; coverage of 517 languages vs. depth of representation for each language
- Failure signatures: Poor performance on specific language pairs might indicate insufficient bidirectional data; high variance across languages might indicate imbalanced training
- First 3 experiments:
  1. Finetune Cheetah-base on a small subset of AfroLingu-MT (5 language pairs) and evaluate spBLEU1K to verify the pipeline works
  2. Compare Cheetah-1.2B vs Cheetah-3.7B finetuned on the same subset to measure performance impact of model size
  3. Test bidirectional translation on a single language pair (e.g., English↔Yoruba) to verify the many-to-many capability works as expected

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the spBLEU1K metric compare to AfriCOMET in terms of correlation with human judgments for African languages?
- Basis in paper: [explicit] The authors introduce spBLEU1K as an alternative evaluation metric with wider coverage (1,003 languages including 614 African languages) and mention that AfriCOMET can only cover 17 African languages, though they developed spBLEU1K to allow dependable evaluation of their dataset.
- Why unresolved: The paper does not provide a direct comparison between spBLEU1K and AfriCOMET's correlation with human judgments, only noting that spBLEU1K improves translation scores and covers more languages.
- What evidence would resolve it: A study comparing human evaluation results with both spBLEU1K and AfriCOMET scores across the same African language pairs, measuring correlation coefficients.

### Open Question 2
- Question: What is the performance of Toucan models on language pairs with extremely limited training data (less than 1,000 examples)?
- Basis in paper: [explicit] The authors mention that for pairs with limited data, they swap data points between directions to augment the dataset, but do not provide specific performance metrics for low-resource pairs.
- Why unresolved: The paper focuses on overall benchmark performance and does not break down results by data availability or specifically analyze the lowest-resource language pairs.
- What evidence would resolve it: Detailed performance metrics (spBLEU1K, ChrF++, AfriCOMET) for individual language pairs grouped by training data volume, particularly those with fewer than 1,000 examples.

### Open Question 3
- Question: How do the Cheetah backbone models (1.2B and 3.7B parameters) perform on monolingual African language tasks beyond machine translation, such as text classification or question answering?
- Basis in paper: [inferred] The authors introduce Cheetah models as sequence-to-sequence LMs with extensive African language coverage (517 languages) and evaluate them specifically for machine translation, but do not explore other potential applications of these models.
- Why unresolved: The paper's evaluation is limited to machine translation tasks, leaving the broader capabilities of the Cheetah models unexplored.
- What evidence would resolve it: Performance metrics on various African language NLP tasks (classification, QA, summarization) using the Cheetah models, comparing them to other multilingual models on the same tasks.

## Limitations

- Data Quality and Representativeness: The AfroLingu-MT benchmark claims to be "the largest African MT benchmark to date," but the paper doesn't provide detailed analysis of data quality, domain coverage, or potential biases in the 156 language pairs.
- Evaluation Metric Validity: While spBLEU1K covers 1,003 languages including 614 African languages, the paper doesn't provide validation that this metric correlates well with human judgments specifically for African languages.
- Reproducibility Constraints: Key implementation details are missing, including exact pretraining data composition, specific tokenization strategies for each language, and hyperparameter tuning procedures.

## Confidence

**High Confidence**: Claims about model architecture (many-to-many seq2seq), basic training procedures (learning rates, batch sizes, number of epochs), and comparative performance against established baselines (NLLB-200, Aya 13B) are well-supported by the experimental setup and results presented.

**Medium Confidence**: Claims about Cheetah models' pretraining on 517 African languages and the effectiveness of the finetuning approach are supported by results but lack detailed validation of the pretraining methodology and intermediate checkpoint analysis.

**Low Confidence**: Claims about Toucan being "the first African-focused LLM-based MT model" and specific assertions about language coverage (517 languages) require verification against existing literature and may need clarification about what constitutes "African-focused."

## Next Checks

1. **Human Evaluation Study**: Conduct a small-scale human evaluation comparing Toucan translations against baseline models (NLLB-200, Aya) for 5-10 representative African language pairs to validate that spBLEU1K scores correlate with actual translation quality.

2. **Cross-Domain Performance Analysis**: Test Toucan on out-of-domain data (e.g., social media, news, technical documentation) for 3-4 language pairs to assess whether the model generalizes beyond the AfroLingu-MT benchmark domain.

3. **Error Analysis for Low-Resource Pairs**: Perform detailed error analysis on the bottom 10% performing language pairs to identify systematic weaknesses and determine whether they stem from data quality issues, architectural limitations, or the finetuning approach.