---
ver: rpa2
title: 'Beyond Agreement: Diagnosing the Rationale Alignment of Automated Essay Scoring
  Methods based on Linguistically-informed Counterfactuals'
arxiv_id: '2405.19433'
source_url: https://arxiv.org/abs/2405.19433
tags:
- essay
- feedback
- scoring
- language
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study introduces a counterfactual intervention approach to
  diagnose rationale alignment in Automated Essay Scoring (AES) methods. By manipulating
  linguistic features such as conventions, language complexity, and organization through
  both rule-based and LLM-assisted methods, the research reveals that BERT-like models
  primarily focus on sentence-level features while LLMs demonstrate sensitivity to
  higher-level linguistic concepts and provide feedback that reflects intervention
  effects.
---

# Beyond Agreement: Diagnosing the Rationale Alignment of Automated Essay Scoring Methods based on Linguistically-informed Counterfactuals

## Quick Facts
- arXiv ID: 2405.19433
- Source URL: https://arxiv.org/abs/2405.19433
- Authors: Yupei Wang; Renfen Hu; Zhe Zhao
- Reference count: 37
- The study introduces a counterfactual intervention approach to diagnose rationale alignment in Automated Essay Scoring (AES) methods

## Executive Summary
This study introduces a novel counterfactual intervention approach to diagnose rationale alignment in Automated Essay Scoring (AES) methods. By manipulating linguistic features such as conventions, language complexity, and organization through both rule-based and LLM-assisted methods, the research reveals that BERT-like models primarily focus on sentence-level features while LLMs demonstrate sensitivity to higher-level linguistic concepts and provide feedback that reflects intervention effects. Experimental results show that although BERT-like models achieve higher scoring agreement with human raters, LLMs exhibit better rationale alignment with scoring rubrics, particularly when fine-tuned with limited training data.

## Method Summary
The research employs a counterfactual intervention methodology where linguistic features of essays are systematically perturbed through two approaches: rule-based manipulations (lexical, syntactic, discourse level changes) and LLM-assisted interventions. The study compares two model families - BERT-like models (RoBERTa-base) and LLMs (GPT-3.5-turbo) - across their ability to score essays and align with human rationales. The framework manipulates conventions, language complexity, and organization features, then evaluates model responses through scoring changes and feedback generation. The experiments are conducted on the ASAP dataset (1,578 essays across 8 prompts) using Multi-Trait Essay Scoring.

## Key Results
- BERT-like models achieve higher scoring agreement with human raters compared to LLMs
- LLMs demonstrate better rationale alignment with scoring rubrics than BERT-like models
- Fine-tuning LLMs with limited training data (1,000 samples) significantly improves their rationale alignment
- LLMs show greater sensitivity to higher-level linguistic concepts compared to BERT-like models' sentence-level focus

## Why This Works (Mechanism)
The counterfactual intervention approach works by systematically perturbing linguistic features and observing how different AES models respond to these changes. This reveals the underlying decision-making mechanisms of each model family - BERT-like models rely heavily on local, sentence-level patterns, while LLMs can capture broader contextual and structural elements. The feedback generation capability of LLMs provides additional interpretability by explicitly acknowledging intervention effects, whereas BERT-like models lack this explainability feature.

## Foundational Learning
- **Counterfactual intervention methodology** - needed to systematically test model behavior by creating controlled perturbations; quick check: verify perturbations maintain essay coherence while changing targeted features
- **Multi-Trait Essay Scoring framework** - needed to evaluate essays across multiple dimensions (conventions, language complexity, organization); quick check: ensure trait definitions align with scoring rubrics
- **Linguistic feature manipulation** - needed to create meaningful variations in essay characteristics; quick check: validate that rule-based and LLM-assisted interventions produce distinguishable effects
- **Rationale alignment evaluation** - needed to assess whether model decisions reflect intended scoring criteria; quick check: compare model feedback with human rater rationales
- **Limited data fine-tuning** - needed to evaluate model performance with realistic training constraints; quick check: measure performance across different training set sizes
- **Agreement vs. alignment distinction** - needed to differentiate between score matching and decision rationale understanding; quick check: verify that high agreement doesn't necessarily indicate good alignment

## Architecture Onboarding

Component map: Counterfactual Intervention -> Model Scoring -> Feedback Generation -> Alignment Evaluation

Critical path: Essay perturbation → Model prediction → Feedback analysis → Rationale alignment assessment

Design tradeoffs: The study balances between model performance (agreement with human scores) and interpretability (rationale alignment), accepting that high agreement doesn't guarantee transparent decision-making.

Failure signatures: BERT-like models may over-rely on surface-level features, LLMs may show inconsistent responses to perturbations, and both may struggle with maintaining coherence during feature manipulation.

First experiments to run:
1. Baseline scoring comparison between RoBERTa-base and GPT-3.5-turbo on unperturbed essays
2. Rule-based perturbation of conventions features followed by scoring analysis
3. LLM-assisted organization feature manipulation with feedback generation evaluation

## Open Questions the Paper Calls Out
None

## Limitations
- The study uses a relatively small corpus (1,578 essays) which may limit generalizability
- Comparison is based on single-model representatives rather than exploring the full spectrum of each architecture family
- The counterfactual interventions may not comprehensively represent all linguistic variations affecting essay scores
- The distinction between sentence-level and higher-level concepts may oversimplify complex model behaviors

## Confidence
- High confidence: BERT-like models achieve higher scoring agreement with human raters
- Medium confidence: LLMs demonstrate better rationale alignment with scoring rubrics
- Medium confidence: Fine-tuning LLMs with limited data improves rationale alignment
- Low confidence: Generalization of findings to other AES tasks and larger datasets

## Next Checks
1. Replicate experiments across multiple BERT variants (BERT, RoBERTa, DeBERTa) and LLM architectures (GPT-4, Claude, LLaMA) to assess result robustness
2. Test the counterfactual intervention framework on alternative AES tasks (e.g., short-answer scoring, scientific writing assessment) to evaluate generalizability
3. Conduct human evaluation studies to validate whether LLM-generated feedback more effectively reflects intervention effects and aligns with human scoring rationales compared to BERT-based feedback