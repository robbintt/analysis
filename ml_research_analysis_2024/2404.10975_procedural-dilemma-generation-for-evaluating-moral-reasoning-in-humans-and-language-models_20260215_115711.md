---
ver: rpa2
title: Procedural Dilemma Generation for Evaluating Moral Reasoning in Humans and
  Language Models
arxiv_id: '2404.10975'
source_url: https://arxiv.org/abs/2404.10975
tags:
- moral
- harm
- language
- causal
- means
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a method for generating moral reasoning dilemmas
  to evaluate AI systems, specifically language models. The approach uses causal graphs
  as a scaffold to guide item generation with language models, allowing for systematic
  manipulation of variables like causal structure, evitability, and action (commission
  vs.
---

# Procedural Dilemma Generation for Evaluating Moral Reasoning in Humans and Language Models

## Quick Facts
- arXiv ID: 2404.10975
- Source URL: https://arxiv.org/abs/2404.10975
- Reference count: 5
- Primary result: Generated moral dilemmas using causal graph templates show effects of means vs. side effect and evitability on permissibility judgments for both humans and language models

## Executive Summary
This paper introduces a procedural generation framework called OffTheRails for creating controlled moral reasoning evaluations. The method uses causal graph templates as scaffolds to guide language models in generating diverse moral dilemmas with systematically manipulated variables. The approach enables scalable creation of test items while maintaining experimental control over key moral reasoning factors. The framework was validated by comparing human participant judgments with those from GPT-4 and Claude-2 across scenarios that varied in causal structure (means vs. side effect), evitability (evitable vs. inevitable), and action type (commission vs. omission).

## Method Summary
The methodology employs a two-stage procedural generation process using GPT-4. First, causal graph templates encoding moral factors (means/side effect, evitability, commission/omission) are used to generate scenario components. Second, these components are combined into complete test items. The OffTheRails benchmark consists of 50 scenarios and 400 unique test items across 8 conditions per scenario. Human participants (80 for Experiment 1, 100 for Experiment 2) rated moral permissibility and intention on Prolific. GPT-4 and Claude-2 were evaluated using similar prompts. Results were analyzed using Bayesian mixed-effects regression to compare human and model judgments across conditions.

## Key Results
- Scenarios where harm is a necessary means (vs. side effect) resulted in lower permissibility and higher intention ratings for both humans and language models
- Evitable scenarios (vs. inevitable) showed lower permissibility and higher intention ratings across human and model judgments
- No clear effect was found for commission versus omission, potentially due to ceiling effects in model responses

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Causal graph templates provide a structured scaffold that guides language models to generate morally relevant scenarios with controlled variables
- Mechanism: The causal graph encodes key moral factors (means vs. side effect, evitability, commission vs. omission) as nodes and edges. When language models fill these templates, they preserve the intended causal relationships while generating natural language descriptions
- Core assumption: Language models can reliably follow causal templates to generate coherent, morally coherent scenarios when given clear instructions
- Evidence anchors:
  - [abstract] "We provide a framework that uses a language model to translate causal graphs that capture key aspects of moral dilemmas into prompt templates"
  - [section] "Our goal is to create diverse evaluations that include control conditions to systematically assess language models' response tendencies and failure modes"
- Break condition: Language models fail to reliably distinguish between means and side effects, requiring separate API calls for each structure

### Mechanism 2
- Claim: Procedural generation enables systematic manipulation of moral variables while maintaining scenario diversity and scalability
- Mechanism: By encoding moral factors as variables in causal templates, researchers can generate multiple scenarios across different conditions (e.g., 8 conditions per scenario) without manual writing, allowing for controlled experiments at scale
- Core assumption: Generated scenarios maintain psychological validity and moral complexity despite being procedurally created
- Evidence anchors:
  - [abstract] "With this framework, we procedurally generated a large and diverse set of moral dilemmas—the OffTheRails benchmark—consisting of 50 scenarios and 400 unique test items"
  - [section] "Overall, we make the following contributions: (1) We propose a procedural generation methodology leveraging abstract causal graphs to create controlled and scalable sets of moral dilemmas"
- Break condition: Generated scenarios lack psychological realism or fail to capture nuanced moral reasoning, reducing experimental validity

### Mechanism 3
- Claim: Comparing human and language model judgments across controlled conditions reveals alignment and divergence in moral reasoning
- Mechanism: By collecting permissibility and intention ratings from both humans and models across identical scenarios, researchers can quantify correlations and identify systematic differences in how moral judgments are made
- Evidence anchors:
  - [abstract] "We collected moral permissibility and intention judgments from human participants for a subset of our items and compared these judgments to those from two language models (GPT-4 and Claude-2) across eight conditions"
  - [section] "Both GPT-4's and Claude-2's permissibility judgment correlated with participants. Correlations for intention ratings were lower, which we attribute to lower variance in model responses"
- Break condition: Low correlation between human and model judgments suggests fundamental differences in moral reasoning processes that procedural generation cannot bridge

## Foundational Learning

- Concept: Causal reasoning and causal graphs
  - Why needed here: The framework relies on representing moral dilemmas as causal graphs to systematically manipulate variables like means vs. side effect and evitability
  - Quick check question: Can you explain how a causal graph might represent the difference between a harm being a means versus a side effect in a moral dilemma?

- Concept: Moral psychology and factors influencing moral judgments
  - Why needed here: Understanding established factors (intention, causation, evitability, etc.) is essential for designing the causal templates and interpreting results
  - Quick check question: What are the key differences between deontological and utilitarian approaches to moral permissibility, and how might these be reflected in causal structures?

- Concept: Language model prompting and template filling
  - Why needed here: The methodology depends on effectively prompting language models to fill causal templates with coherent, morally relevant content
  - Quick check question: How might few-shot examples improve a language model's ability to distinguish between different causal structures when generating scenarios?

## Architecture Onboarding

- Component map:
  Causal template designer -> Language model interface -> Template stitcher -> Human evaluation system -> Model evaluation system -> Analysis pipeline

- Critical path:
  1. Design causal template (expert knowledge)
  2. Generate scenarios using language model (automation)
  3. Stitch components into test items (automation)
  4. Collect human ratings (external service)
  5. Collect model ratings (API calls)
  6. Analyze results (statistical analysis)

- Design tradeoffs:
  - Flexibility vs. control: More variables in templates increase scenario diversity but may reduce experimental control
  - Scale vs. quality: Procedural generation enables large datasets but may sacrifice scenario quality
  - Simplicity vs. realism: Simple causal structures are easier to manipulate but may miss complex moral factors

- Failure signatures:
  - Low correlations between human and model judgments suggest models aren't capturing intended moral factors
  - Inconsistent scenario quality indicates language models aren't following templates reliably
  - Weak experimental effects suggest scenarios aren't sufficiently distinct across conditions

- First 3 experiments:
  1. Test language model's ability to distinguish means vs. side effects in isolated prompts
  2. Generate a small set of scenarios (10) across all 8 conditions and check for consistency
  3. Pilot human evaluation with 5 scenarios to validate scenario quality before full-scale data collection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the OffTheRails benchmark be extended to include additional moral reasoning factors beyond causal structure, evitability, and action (commission vs. omission)?
- Basis in paper: [explicit] The authors acknowledge that moral reasoning is more complex than what can be captured by the three factors they explored, and they view their work as a proof-of-concept that can be extended to study a variety of moral reasoning phenomena
- Why unresolved: The paper only focuses on three specific factors, leaving open the question of how to incorporate other known influences on moral judgments, such as personal force, severity of outcomes, or agent's knowledge
- What evidence would resolve it: A systematic exploration of how other moral reasoning factors can be represented as causal graphs and incorporated into the procedural generation framework, along with empirical validation of the effects of these additional factors on human and language model judgments

### Open Question 2
- Question: How can the distinction between means and side effects be reliably generated in the OffTheRails pipeline without requiring separate API calls?
- Basis in paper: [explicit] The authors note that GPT-4 struggled to reliably distinguish between means and side effects when generating both causal structures within one completion, necessitating a two-stage approach with independent API calls
- Why unresolved: The current approach of using separate API calls for generating means and side effects scenarios is less efficient and scalable compared to a single-stage generation process
- What evidence would resolve it: An improved prompt design or generation strategy that enables GPT-4 to reliably generate scenarios with the correct causal structure (means vs. side effect) in a single API call, validated through empirical testing

### Open Question 3
- Question: How do different prompting or fine-tuning techniques impact language model responses on the OffTheRails benchmark?
- Basis in paper: [explicit] The authors acknowledge that the current evaluation is done in a simplified single-shot setting and suggest that different prompts, in-context demonstrations, or "personas" could change how language models respond to the moral dilemmas
- Why unresolved: The paper does not explore how variations in prompting or fine-tuning techniques affect language model judgments on the moral reasoning benchmark
- What evidence would resolve it: Empirical comparisons of language model responses across different prompting strategies (e.g., few-shot learning, in-context demonstrations, persona-based prompting) or fine-tuning approaches, evaluating the impact on judgment patterns and alignment with human judgments

## Limitations

- The procedural generation approach struggled to reliably distinguish between means and side effects within single API calls, requiring a less efficient two-stage process
- The absence of a commission vs. omission effect may be due to ceiling effects in model responses rather than a true absence of this moral factor's influence
- The psychological validity of procedurally generated scenarios compared to traditional human-written moral dilemmas was not directly validated

## Confidence

- High confidence: The general finding that scenarios where harm is a necessary means (vs. side effect) and evitable (vs. inevitable) result in lower permissibility ratings for both humans and language models
- Medium confidence: The procedural generation methodology can successfully create controlled moral dilemmas at scale
- Low confidence: The absence of a commission vs. omission effect, and that procedurally generated scenarios have equivalent psychological validity to traditional dilemmas

## Next Checks

1. Conduct a validation study comparing human ratings of procedural generation scenarios against established moral dilemma datasets to verify psychological equivalence
2. Test the reliability of language model generation by having multiple models (including open-source alternatives) generate scenarios from the same templates and comparing structural consistency
3. Investigate the commission/omission finding further by using alternative measurement approaches (e.g., different rating scales, forced-choice paradigms) to rule out ceiling effects