---
ver: rpa2
title: 'Fully Decentralized Cooperative Multi-Agent Reinforcement Learning: A Survey'
arxiv_id: '2401.04934'
source_url: https://arxiv.org/abs/2401.04934
tags:
- policy
- learning
- decentralized
- agents
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper surveys the problem of fully decentralized cooperative
  multi-agent reinforcement learning (MARL), where agents must learn optimal joint
  policies without access to information from other agents. The main challenge is
  the non-stationary environment from each agent's perspective due to other agents'
  continuously updating policies.
---

# Fully Decentralized Cooperative Multi-Agent Reinforcement Learning: A Survey

## Quick Facts
- arXiv ID: 2401.04934
- Source URL: https://arxiv.org/abs/2401.04934
- Authors: Jiechuan Jiang; Kefan Su; Zongqing Lu
- Reference count: 9
- Primary result: Comprehensive survey of fully decentralized cooperative MARL methods

## Executive Summary
This paper provides a comprehensive survey of fully decentralized cooperative multi-agent reinforcement learning (MARL), where agents must learn optimal joint policies without access to information from other agents. The key challenge addressed is the non-stationary environment from each agent's perspective due to other agents' continuously updating policies. The survey systematically categorizes existing methods into value-based and policy-based approaches for two distinct settings: shared reward (all agents maximize a common reward) and reward sum (agents maximize the sum of individual rewards).

The authors present theoretical foundations and practical algorithms, including value-based approaches like BQL (Best Possible Q-Learning) which converges to optimal joint policies in both deterministic and stochastic environments, and policy-based approaches like DPO (Decentralized Policy Optimization) and TVPO (Total Variation Policy Optimization) which provide monotonic policy improvement guarantees in fully decentralized settings. The survey identifies critical open questions in the field including optimal joint policy convergence, sample complexity, coordination challenges, and offline decentralized MARL applications.

## Method Summary
The survey methodology involves systematic categorization of fully decentralized MARL approaches into two main frameworks: value-based and policy-based methods. For each framework, the authors analyze algorithms under two reward settings - shared reward and reward sum. The analysis includes theoretical convergence properties, algorithmic implementations, and comparative performance characteristics. The survey examines BQL's ability to converge to optimal joint policies through iterative value function updates, and analyzes DPO and TVPO's monotonic improvement guarantees through decentralized policy optimization techniques. The methodology also includes identification of key challenges such as non-stationarity, credit assignment, and coordination in fully decentralized settings.

## Key Results
- BQL algorithm converges to optimal joint policies in both deterministic and stochastic environments
- DPO and TVPO provide monotonic policy improvement guarantees in fully decentralized settings
- Survey identifies critical open questions in optimal joint policy convergence and sample complexity
- Comprehensive categorization of methods into value-based and policy-based approaches for shared and sum reward settings

## Why This Works (Mechanism)
The effectiveness of fully decentralized cooperative MARL approaches stems from their ability to handle non-stationarity through sophisticated learning mechanisms. Value-based methods like BQL work by maintaining individual Q-value estimates while implicitly accounting for other agents' policies through best-response updates. Policy-based methods like DPO and TVPO employ trust region techniques that ensure monotonic improvement despite the lack of global information. These approaches succeed by either learning to predict others' behaviors (value-based) or constraining policy updates to maintain stability (policy-based), effectively transforming the non-stationary multi-agent problem into a series of stationary single-agent problems.

## Foundational Learning
- Markov Decision Processes (MDPs): The fundamental framework for modeling sequential decision-making under uncertainty. Why needed: Provides the theoretical foundation for reinforcement learning algorithms. Quick check: Verify state transition dynamics and reward structure are properly defined.
- Multi-Agent Systems: Frameworks for modeling multiple interacting decision-makers. Why needed: Essential for understanding the non-stationary environment challenge in decentralized settings. Quick check: Confirm agent interaction models and information constraints.
- Policy Gradient Methods: Techniques for optimizing policies through gradient ascent. Why needed: Forms the basis for policy-based decentralized approaches like DPO and TVPO. Quick check: Validate gradient estimation accuracy and convergence properties.
- Value Function Approximation: Methods for representing value functions in high-dimensional spaces. Why needed: Critical for scaling value-based methods to complex environments. Quick check: Verify approximation accuracy and generalization capabilities.
- Game Theory: Mathematical framework for analyzing strategic interactions. Why needed: Provides theoretical tools for understanding equilibrium concepts in multi-agent settings. Quick check: Confirm equilibrium conditions and stability analysis.

## Architecture Onboarding

Component Map:
Agent 1 -> Local Policy Network -> Local Q-Value Network -> Action Selection
Agent 2 -> Local Policy Network -> Local Q-Value Network -> Action Selection
Agent N -> Local Policy Network -> Local Q-Value Network -> Action Selection
Shared Environment -> State Observations -> Reward Signals

Critical Path:
State Observation -> Policy Network Processing -> Q-Value Estimation -> Action Selection -> Environment Interaction -> Reward Collection -> Value Update -> Policy Improvement

Design Tradeoffs:
- Decentralization vs. Performance: Fully decentralized methods sacrifice some performance for robustness and scalability
- Communication Constraints: Complete lack of agent-to-agent communication requires sophisticated learning mechanisms
- Computational Efficiency: Individual agent processing vs. centralized joint optimization
- Sample Efficiency: Decentralized learning often requires more samples due to lack of coordination information

Failure Signatures:
- Policy oscillation due to non-stationarity
- Suboptimal convergence to local equilibria
- High variance in policy updates
- Slow learning progress in complex coordination tasks

First Experiments:
1. Simple grid-world coordination task with 2-3 agents
2. Stochastic environment with varying reward structures
3. Scalability test with increasing number of agents in partially observable environments

## Open Questions the Paper Calls Out
The paper identifies several critical open questions in fully decentralized cooperative MARL:
- Optimal joint policy convergence guarantees in general stochastic environments
- Sample complexity bounds for decentralized learning algorithms
- Coordination mechanisms without any communication between agents
- Extension to offline decentralized MARL settings
- Scalability to large-scale multi-agent systems with high-dimensional state spaces

## Limitations
- Extremely low average neighbor citations (0.0) suggesting limited integration with existing literature
- High topic divergence from neighboring research areas (FMR 0.811)
- Limited empirical validation of theoretical convergence guarantees
- Unclear scalability performance in large-scale multi-agent systems
- Potential gaps in coverage of emerging decentralized MARL techniques

## Confidence
Medium
- Survey methodology: Medium
- Theoretical analysis: Medium
- Practical applicability: Low-Medium
- Literature integration: Low
- Empirical validation: Low

## Next Checks
1. Verify convergence proofs for BQL in both deterministic and stochastic environments by examining formal mathematical derivations
2. Assess the empirical performance gap between fully decentralized methods and partially decentralized baselines through controlled experiments
3. Investigate the scalability limitations of proposed methods when applied to large-scale multi-agent systems with high-dimensional state spaces