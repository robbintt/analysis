---
ver: rpa2
title: Human-artificial intelligence teaming for scientific information extraction
  from data-driven additive manufacturing research using large language models
arxiv_id: '2407.18827'
source_url: https://arxiv.org/abs/2407.18827
tags:
- information
- relevant
- paragraphs
- data
- system
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study addresses the challenge of extracting scientific information
  from data-driven additive manufacturing (AM) literature, which has grown rapidly
  with the integration of artificial intelligence (AI). Existing reviews are limited
  in scope and require manual effort for information retrieval.
---

# Human-artificial intelligence teaming for scientific information extraction from data-driven additive manufacturing research using large language models

## Quick Facts
- arXiv ID: 2407.18827
- Source URL: https://arxiv.org/abs/2407.18827
- Reference count: 29
- Custom LLM framework achieves 0.79-0.90 F1 scores for classifying AM research paragraphs

## Executive Summary
This paper presents a human-AI teaming framework for extracting scientific information from rapidly growing data-driven additive manufacturing literature. The framework combines large language models with iterative human feedback to improve information retrieval accuracy. A prototype tool enables domain experts to upload PDFs, parse text, and refine retrieval queries through iterative labeling. The approach demonstrates significant improvements in paragraph ranking relevance and achieves strong classification performance across four key information categories.

## Method Summary
The framework consists of three components: a base information extraction system that parses PDFs and generates paragraph embeddings, a paragraph classification tier that categorizes content into data, modeling, sensing, and AM systems, and a query tier that uses LLMs for detailed information extraction. Custom retrieval embeddings are computed through iterative human labeling of positive and negative paragraphs, which updates the embedding space to prioritize relevant content. A Random Forest multi-label classifier is trained on OpenAI embeddings to categorize paragraphs, with evaluation using precision, recall, and F1-score metrics.

## Key Results
- Customized retrieval embeddings significantly improve paragraph ranking similarity scores
- Global classifier achieves precision, recall, and F1-scores ranging from 0.79 to 0.90 across four information categories
- Prototype tool successfully processes 100 research articles from Scopus database
- Iterative human-in-the-loop refinement demonstrates measurable improvements in retrieval relevance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Iterative human-in-the-loop customization of retrieval embeddings improves paragraph ranking relevance
- Mechanism: Domain experts label paragraphs as positive/negative for each information category, which updates retrieval embeddings through weighted averaging of query and paragraph embeddings
- Core assumption: Expert-labeled relevance signals can effectively guide embedding similarity to prioritize relevant content
- Evidence anchors:
  - [section] "The labeling process led to a multi-label text dataset for ML-driven AM literature... The improvement in the similarity ensures that top matches to a selected retrieval contain relevant paragraphs"
  - [section] "We show the ability of LLMs to expedite the extraction of relevant information from data-driven AM literature"
  - [corpus] Weak - no direct corpus evidence comparing baseline vs customized ranking
- Break condition: When expert labeling introduces bias or the embedding space cannot capture nuanced relevance distinctions

### Mechanism 2
- Claim: Paragraph-level retrieval before LLM processing reduces computational cost and improves factual consistency
- Mechanism: By retrieving only the top similar paragraphs based on cosine similarity, the system limits LLM context to manageable chunks while maintaining semantic relevance
- Core assumption: Most relevant information for any query is contained within a small subset of paragraphs rather than requiring full-text analysis
- Evidence anchors:
  - [section] "Because our use case involves extracting information from a given paper, we retrieve relevant paragraphs instead... feeding full-texts into our LLM-based IE model is infeasible or too costly"
  - [section] "we use the OpenAI embeddings API instead of our own models" due to computational constraints
  - [corpus] Missing - no empirical comparison of full-text vs paragraph-only processing
- Break condition: When relevant information is distributed across many paragraphs requiring holistic context

### Mechanism 3
- Claim: Multi-label classification of paragraphs enables rapid filtering before detailed extraction
- Mechanism: Trained classifiers categorize paragraphs into data, modeling, sensing, and system relevance, allowing downstream queries to focus on likely-relevant passages
- Core assumption: Paragraph content patterns are sufficiently distinct across categories to enable reliable classification
- Evidence anchors:
  - [section] "The classifier can categorize the paragraph into four categories of relevant information... precision, recall, and F1 score" ranging from 0.79 to 0.90
  - [section] "We expect to find the relevant information in specific paragraphs and hence there is value in training paragraph classifiers"
  - [corpus] Weak - only single dataset performance reported without external validation
- Break condition: When paragraph categories overlap significantly or require cross-category context for accurate classification

## Foundational Learning

- Concept: Cosine similarity in high-dimensional embedding spaces
  - Why needed here: The system relies on cosine similarity to rank paragraph relevance against category-specific queries
  - Quick check question: If two paragraphs have embeddings [0.5, 0.3, 0.2] and [0.6, 0.4, 0.1], which is more similar to query [0.5, 0.3, 0.2]?

- Concept: Multi-label vs multi-class classification
  - Why needed here: Paragraphs can belong to multiple categories (e.g., a paragraph about ML modeling using specific AM system data), requiring multi-label approach
  - Quick check question: Can a single paragraph be labeled as both "data relevant" and "model relevant" in this system?

- Concept: Human-in-the-loop iterative refinement
  - Why needed here: The system depends on expert feedback to improve retrieval through positive/negative paragraph labeling
  - Quick check question: What happens to the retrieval embedding when a paragraph is labeled as negative for a given category?

## Architecture Onboarding

- Component map: Base IE System (PDF parsing → paragraph embeddings → search/retrieval) → Paragraph Classification Tier (global classifier) → Query Tier (LLM extraction)
- Critical path: PDF upload → GROBID parsing → paragraph embedding generation → semantic search/retrieval → paragraph labeling → classifier training → LLM query processing
- Design tradeoffs: Computational efficiency vs. comprehensive context (paragraph-level vs. full-text processing), model accuracy vs. training data requirements, flexibility vs. system complexity
- Failure signatures: Poor ranking despite labeling (embedding space inadequacy), classifier confusion between categories (insufficient training data), LLM confabulation despite retrieval (retrieval quality issues)
- First 3 experiments:
  1. Upload a sample PDF and verify GROBID parsing accuracy by comparing extracted paragraphs to original text
  2. Test semantic search with a known query and verify retrieved paragraphs contain expected information
  3. Create a simple retrieval with one positive and one negative query, then test paragraph ranking changes

## Open Questions the Paper Calls Out
- What is the optimal threshold for similarity scores to determine relevant paragraphs in scientific information extraction?
- How can the framework be validated in other design and manufacturing subdomains?
- What are the long-term effects of human-in-the-loop feedback on the performance of the information extraction system?

## Limitations
- Framework performance limited to single domain (additive manufacturing) without external validation
- Paragraph-level retrieval may miss information distributed across multiple paragraphs requiring holistic context
- LLM-based extraction quality heavily dependent on prompt engineering quality, which remains underspecified

## Confidence
**High Confidence**: Base IE system architecture and classification tier performance (0.79-0.90 F1 scores) are well-established and validated
**Medium Confidence**: Computational efficiency claims from paragraph-level processing lack direct empirical comparison to full-text approaches
**Low Confidence**: Framework generalizability beyond AM literature remains unproven

## Next Checks
1. Apply the trained classifier and retrieval system to a different engineering domain (e.g., aerospace materials research) and measure performance degradation
2. Systematically compare LLM extraction quality and computational cost between full-text processing and the paragraph-level approach across 10 diverse papers
3. Test the retrieval system with alternative embedding models (e.g., sentence-transformers vs. OpenAI embeddings) while keeping the same positive/negative query sets