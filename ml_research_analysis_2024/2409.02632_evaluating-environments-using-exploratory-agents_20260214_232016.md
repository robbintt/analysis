---
ver: rpa2
title: Evaluating Environments Using Exploratory Agents
arxiv_id: '2409.02632'
source_url: https://arxiv.org/abs/2409.02632
tags:
- levels
- agent
- engaging
- level
- object
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates using exploratory agents to evaluate procedurally
  generated game levels, distinguishing between engaging and unengaging environments.
  The researchers developed an exploratory agent framework with six metrics (coverage,
  inspection, novelty, entropy, motivation, and a combined metric) to assess level
  quality.
---

# Evaluating Environments Using Exploratory Agents

## Quick Facts
- arXiv ID: 2409.02632
- Source URL: https://arxiv.org/abs/2409.02632
- Reference count: 11
- This study investigates using exploratory agents to evaluate procedurally generated game levels, distinguishing between engaging and unengaging environments.

## Executive Summary
This paper presents an exploratory agent framework for evaluating procedurally generated game levels. The researchers developed an agent with six metrics (coverage, inspection, novelty, entropy, motivation, and a combined metric) to assess level quality. Two procedural generators were used - one producing engaging levels and another producing unengaging levels, both using identical tile sets but different object placement probabilities. The agent successfully distinguished between level types, with engaging levels achieving higher fitness scores (average 0.808) compared to unengaging levels (average 0.494).

## Method Summary
The study used Unity with Tessera's Wave Function Collapse (WFC) implementation to generate 5 engaging and 5 unengaging levels (350x350 units each with 35 tiles). An exploratory agent framework was implemented with 36 directional samples, camera-based object detection, and A* pathfinding for navigation. The agent explored each level from 3 spawn points for 3 minutes, calculating six metrics (coverage, inspection, novelty, entropy, motivation, combined). A fitness function with weighted metrics (0.1 each for individual metrics, 0.5 for combined) evaluated level quality, comparing engaging vs. unengaging levels.

## Key Results
- The exploratory agent successfully distinguished between engaging (average fitness 0.808) and unengaging (average fitness 0.494) levels using the combined metric approach
- Engaging levels showed higher motivation and inspection rates, with more consistent high motivation in engaging environments
- The combined metric weighted at 0.5 proved most effective at evaluating exploratory potential, demonstrating exploratory agents can reliably assess procedurally generated content

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The exploratory agent successfully distinguishes between engaging and unengaging levels by using a weighted combination of multiple metrics that capture different aspects of exploration quality.
- Mechanism: The fitness function combines coverage, inspection, entropy, novelty, and motivation metrics, with the combined metric receiving a higher weight (0.5) than individual metrics (0.1 each). This weighting prioritizes holistic assessment over isolated measurements.
- Core assumption: Different metrics capture complementary aspects of exploration quality, and their combination provides a more reliable assessment than any single metric alone.
- Evidence anchors:
  - [abstract] "The combined metric weighted at 0.5 proved most effective at evaluating exploratory potential"
  - [section] "For our fitness function, every metric was given a weight of 0.1, except for our agent which had all the metrics loaded in, this combination of metrics was given a weight of 0.5"
  - [corpus] Weak evidence - no directly comparable studies found in corpus
- Break condition: If the weights are poorly calibrated, the combined metric may overweight certain aspects of exploration while underweighting others, leading to misclassification of level quality.

### Mechanism 2
- Claim: The agent's motivation metric effectively captures the agent's engagement with the environment by measuring the highest-scoring direction at each time step.
- Mechanism: Motivation is calculated as the highest interest score from the agent's direction map at each time step, reflecting how compelling the environment is for exploration. Higher motivation indicates more engaging environments with objects and features that attract the agent's attention.
- Core assumption: The agent's interest in directions correlates with environmental quality and exploration potential.
- Evidence anchors:
  - [abstract] "Key findings include higher motivation and inspection rates in engaging levels"
  - [section] "Motivation is a measure of the highest scoring direction, according to the agent's attached metrics, at a given time step (1 second)"
  - [corpus] No direct evidence in corpus about motivation metrics for exploration assessment
- Break condition: If the agent's metric calculations are flawed or the environment contains misleading features that artificially inflate motivation scores without providing meaningful exploration, the metric may not accurately reflect exploration quality.

### Mechanism 3
- Claim: The novelty metric effectively distinguishes engaging from unengaging levels by measuring the diversity of stimuli experienced by the agent over time.
- Mechanism: Novelty accumulates when new objects are seen and recovers when objects are not visible, creating a dynamic measure of environmental freshness. Engaging levels with diverse, spread-out objects maintain higher novelty scores, while unengaging levels with repetitive or clustered objects show lower novelty.
- Core assumption: Diverse environmental stimuli maintain agent interest and promote continued exploration.
- Evidence anchors:
  - [abstract] "The novelty histograms for the engaging levels compared to the unengaging levels shows significant differences for each metric"
  - [section] "Novelty is a measure of the stimuli experienced by the agent in it's path" with detailed calculation method
  - [corpus] No corpus evidence found for novelty metrics in procedural content evaluation
- Break condition: If the environment contains many similar objects that the novelty metric treats as distinct, or if the recovery rate is poorly calibrated, the novelty measure may not accurately reflect true environmental diversity.

## Foundational Learning

- Concept: Shannon Entropy
  - Why needed here: Entropy measures the unpredictability of the agent's exploration path, helping distinguish between too predictable (low entropy) and too chaotic (high entropy) exploration patterns.
  - Quick check question: What range of entropy values would indicate an ideal balance between predictable and random exploration patterns?

- Concept: Procedural Content Generation (PCG) with Wave Function Collapse
  - Why needed here: Understanding how WFC generates levels with different object placement probabilities is crucial for interpreting why the agent can distinguish between engaging and unengaging levels.
  - Quick check question: How does changing object placement probabilities in WFC affect the spatial distribution and diversity of objects in generated levels?

- Concept: Fitness Function Design
  - Why needed here: The fitness function combines multiple metrics with different weights to evaluate level quality, requiring understanding of how to balance competing objectives in evaluation systems.
  - Quick check question: Why might the combined metric receive a higher weight (0.5) compared to individual metrics (0.1) in the fitness function?

## Architecture Onboarding

- Component map: Procedural Level Generators (A and B) -> Unity Environment -> Exploratory Agent Framework -> Metric Calculation -> Fitness Evaluation -> Level Classification
- Critical path:
  1. Generate levels using WFC with different object placement probabilities
  2. Agent explores each level from 3 spawn points for 3 minutes
  3. Calculate metrics (coverage, inspection, entropy, novelty, motivation) at each time step
  4. Apply fitness function with weighted metrics
  5. Compare fitness scores between engaging and unengaging levels
- Design tradeoffs:
  - View distance (115 units) vs. computational cost: Longer view distance allows better object detection but increases computation
  - Number of directions (36) vs. granularity: More directions provide finer-grained exploration but increase complexity
  - Individual metric weights vs. combined metric: Balancing specific vs. holistic assessment approaches
- Failure signatures:
  - Low fitness score variance between engaging and unengaging levels suggests metric insensitivity
  - High coverage but low motivation indicates levels that are navigable but not engaging
  - Consistently high entropy suggests overly chaotic exploration patterns
- First 3 experiments:
  1. Test agent with individual metrics only (coverage, inspection, entropy, novelty, motivation) to establish baseline performance and identify which metrics best distinguish level types
  2. Test agent with all metrics combined to verify if holistic assessment outperforms individual metrics
  3. Test random agent control to establish baseline exploration patterns without guided behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How well would the exploratory agent framework generalize to completely different game genres and environments beyond the tested tile-based levels?
- Basis in paper: [explicit] The paper mentions future work could explore "a broader range of PCG techniques" and "a wider variety of levels" with "a wider range of assets"
- Why unresolved: The study only tested the framework on 10 procedurally generated tile-based levels using WFC. No testing was done on different game genres, 3D environments, or non-tile-based level designs.
- What evidence would resolve it: Testing the framework on levels from different genres (e.g., platformers, RPGs, first-person shooters), different generation methods (L-systems, grammar-based generation), and different spatial configurations (3D spaces, continuous environments).

### Open Question 2
- Question: What is the optimal balance between exploration and exploitation for the agent to provide the most useful feedback on level design?
- Basis in paper: [inferred] The paper notes that "an ideal exploration path strikes a balance, showing neither excessive randomness nor predictability" and discusses how different metrics affect exploration patterns, but doesn't investigate the optimal balance.
- Why unresolved: While the paper uses multiple metrics and discusses their effects, it doesn't systematically investigate how to balance exploration breadth (coverage) with depth (inspection) to provide the most useful design feedback.
- What evidence would resolve it: Controlled experiments varying the relative weights of coverage vs inspection metrics and measuring which balance provides the most actionable feedback for improving level design.

### Open Question 3
- Question: How closely do the agent's exploration patterns align with actual human player behavior and preferences?
- Basis in paper: [explicit] The paper explicitly suggests "incorporating human/player feedback could enhance the understanding of how these environments support real player experiences" and proposes comparing agent patterns with human players.
- Why unresolved: The study only used the exploratory agent without any validation against human players. While the agent successfully distinguished between engaging and unengaging levels, we don't know if these distinctions match what humans would perceive.
- What evidence would resolve it: User studies where human players explore the same levels and their behavior and subjective experiences are compared with the agent's metrics to establish correlation or divergence.

## Limitations

- The study used only two procedural generators with identical tile sets, limiting generalizability to different generation methods and content types
- The sample size was relatively small (5 levels per generator), which may not capture the full variability in procedural generation
- No human validation studies were conducted to confirm that the agent's classifications of engaging vs. unengaging levels align with human perceptions

## Confidence

The study demonstrates Medium confidence in its primary claim that exploratory agents can reliably distinguish between engaging and unengaging procedurally generated levels. The agent successfully classified engaging levels (average fitness 0.808) from unengaging levels (average fitness 0.494) using the combined metric approach. However, limitations include the use of only two procedural generators with identical tile sets, a relatively small sample size (5 levels per generator), and the absence of human validation studies to confirm that the agent's classifications align with human perceptions of level quality.

The individual metric analyses show High confidence in the observation that engaging levels exhibit higher motivation and inspection rates, supported by clear quantitative differences in histogram distributions. The mechanism linking motivation scores to environmental engagement appears sound, though the exact relationship between novelty recovery rates and environmental diversity remains Low confidence due to limited validation of the novelty calculation method.

The fitness function weighting approach shows Medium confidence in effectiveness, as the combined metric (weight 0.5) outperformed individual metrics, but the optimal weight configuration was not systematically explored across different level types or generator parameters.

## Next Checks

1. **Human Validation Study**: Conduct user testing where human players rate the same levels used in the agent evaluation to verify whether the agent's classifications of engaging vs. unengaging levels align with human preferences and experiences.

2. **Metric Sensitivity Analysis**: Systematically vary the fitness function weights and test the agent across levels with different object placement probabilities to determine optimal weight configurations and identify which metrics are most critical for accurate classification.

3. **Cross-Generator Generalization**: Test the exploratory agent framework with multiple pairs of generators using different tile sets and generation algorithms to evaluate whether the approach generalizes beyond the specific WFC-based generators used in this study.