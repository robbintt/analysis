---
ver: rpa2
title: 'MMREC: LLM Based Multi-Modal Recommender System'
arxiv_id: '2408.04211'
source_url: https://arxiv.org/abs/2408.04211
tags:
- information
- systems
- recommender
- data
- user
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MMREC, a novel LLM-based multi-modal recommender
  system that addresses the challenge of effectively leveraging natural language data
  and images in recommendation contexts. The proposed framework uses large language
  models (LLMs) to extract and integrate text and image information, unifying diverse
  modalities in a shared latent space to simplify the learning process for ranking
  models.
---

# MMREC: LLM Based Multi-Modal Recommender System

## Quick Facts
- arXiv ID: 2408.04211
- Source URL: https://arxiv.org/abs/2408.04211
- Authors: Jiahao Tian; Jinman Zhao; Zhenkai Wang; Zhicheng Ding
- Reference count: 40
- Primary result: 19.4% FPR improvement at 10% accuracy cost

## Executive Summary
This paper introduces MMREC, a novel LLM-based multi-modal recommender system that addresses the challenge of effectively leveraging natural language data and images in recommendation contexts. The proposed framework uses large language models (LLMs) to extract and integrate text and image information, unifying diverse modalities in a shared latent space to simplify the learning process for ranking models. By employing LLM summarization and multimodal processing, the system achieves significant improvements in recommendation quality, particularly for imbalanced datasets.

## Method Summary
The MMREC framework preprocesses restaurant review data containing text and images, then applies LLM-based summarization to extract key information from reviews and convert images to textual descriptions using BLIP-2. Both text and image-derived text are processed through the same sentence transformer embedding technique to create unified representations in a shared latent space. An MLP-based upstream model reduces dimensionality before feeding features into a DLRM ranking model. The system uses weighted loss functions (basic and square root) with dropout rates of 0.1, 0.3, and 0.5, trained with early stopping after 300 epochs without improvement.

## Key Results
- 19.4% improvement in false positive rate at the expense of 10% decrease in accuracy
- Best performance: 18.16% false positive rate and 83.48% accuracy with dropout rate 0.5
- Both text and image modalities provide critical complementary information for predicting user preferences

## Why This Works (Mechanism)

### Mechanism 1
The LLM summarization approach improves discriminative power by filtering out noise from raw user reviews and focusing on the most salient features. Instead of averaging embeddings across all reviews, the LLM extracts and condenses the most important information, reducing irrelevant signals that dilute the model's learning.

### Mechanism 2
Converting images to text descriptions enables unified processing in a shared latent space, improving model performance. Images are transformed into textual descriptions using a multimodal model (BLIP-2), then processed by the same text encoder as user reviews, ensuring both modalities share the same embedding space.

### Mechanism 3
The combination of text and image modalities provides complementary information that enhances the model's ability to predict user preferences and reduce false positives. Both text reviews and image descriptions contribute distinct signals about user preferences and item characteristics, and their combination provides more complete information than either modality alone.

## Foundational Learning

- **Large Language Model capabilities for summarization and reasoning**: Needed for extracting meaningful information from raw reviews and converting images to text descriptions. Quick check: What are the key differences between how an LLM processes information versus traditional embedding methods?

- **Multi-modal information fusion and latent space alignment**: Required for effectively combining text and image information in a shared representation space. Quick check: How does processing different modalities in the same latent space improve recommendation quality compared to separate processing?

- **Imbalanced dataset handling and evaluation metrics**: Critical because the paper emphasizes false positive rate improvement, which is particularly important for imbalanced datasets in recommendation systems. Quick check: Why is false positive rate often more important than accuracy in recommender systems with imbalanced data?

## Architecture Onboarding

- **Component map**: Raw data preprocessing -> LLM processing (summarization, price extraction, categorization) -> BLIP-2 image captioning -> Text embedding generation -> Image feature extraction -> Dimensionality reduction -> DLRM ranking model

- **Critical path**: 1) Data collection and preprocessing, 2) LLM summarization of reviews, 3) Image captioning via BLIP-2, 4) Embedding generation and dimensionality reduction, 5) Feature concatenation and DLRM training, 6) Evaluation using accuracy and false positive rate metrics

- **Design tradeoffs**: Using LLMs adds inference cost but improves feature quality; image-to-text conversion may lose some visual information but enables unified processing; dimensionality reduction prevents overfitting but may lose some information; the system trades some accuracy for significantly improved false positive rate

- **Failure signatures**: High false positive rates indicate poor feature quality or misalignment; low accuracy with good false positive rate suggests the model is too conservative; training instability may indicate issues with dimensionality reduction or feature concatenation

- **First 3 experiments**: 1) Baseline comparison: Run model with only text reviews (no LLM summarization, no images), 2) Ablation study: Remove LLM summarization to measure impact on false positive rate, 3) Modality isolation: Test with only text reviews or only image-derived features to quantify individual contributions

## Open Questions the Paper Calls Out

- **Optimal balance between text and image modalities**: How does the optimal balance vary across different recommendation scenarios (restaurants vs. products vs. movies)? The paper shows both modalities contribute but doesn't investigate whether their relative importance differs by domain.

- **Scalability to larger datasets**: How does the LLM-based approach scale to industrial-sized recommendation systems with millions of users and items? The paper demonstrates effectiveness on a restaurant dataset but doesn't address computational efficiency or performance at scale.

- **Impact of different LLM variants**: How do different types of LLMs (e.g., GPT-4 vs. smaller models) affect performance in terms of accuracy and false positive rate? The study uses specific LLM models without comparing their relative effectiveness.

## Limitations

- The LLM summarization quality and consistency across different review types is not validated, raising concerns about potential bias amplification
- The paper does not provide ablation studies on the specific contributions of different LLM components (summarization vs. price extraction vs. categorization)
- The image-to-text conversion process via BLIP-2 may lose critical visual information that could affect recommendation quality

## Confidence

- **High confidence**: The general framework of using LLM for summarization and unified latent space processing is technically sound and well-established
- **Medium confidence**: The claimed 19.4% FPR improvement and 10% accuracy trade-off is based on a single dataset and may not generalize to other domains
- **Low confidence**: The assertion that text and image modalities provide "complementary" information is not empirically validated through proper feature attribution analysis

## Next Checks

1. Conduct a comprehensive ablation study to isolate the impact of LLM summarization from other components (price extraction, categorization) on FPR and accuracy
2. Test the model on multiple datasets with different imbalance ratios to verify generalizability of the FPR improvement claims
3. Implement feature importance analysis to quantify the actual complementary value added by combining text and image modalities versus using the stronger modality alone