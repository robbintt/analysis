---
ver: rpa2
title: 'Mask and Compress: Efficient Skeleton-based Action Recognition in Continual
  Learning'
arxiv_id: '2407.01397'
source_url: https://arxiv.org/abs/2407.01397
tags:
- recognition
- learning
- action
- masking
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses continual learning for skeleton-based action
  recognition, where the goal is to learn new classes over time without forgetting
  previously learned ones. The authors propose CHARON, which improves memory efficiency
  by compressing skeleton sequences via uniform sampling and linear interpolation.
---

# Mask and Compress: Efficient Skeleton-based Action Recognition in Continual Learning

## Quick Facts
- arXiv ID: 2407.01397
- Source URL: https://arxiv.org/abs/2407.01397
- Reference count: 40
- Primary result: CHARON achieves state-of-the-art accuracy in continual skeleton-based action recognition with reduced computational cost through temporal compression and masked autoencoder training

## Executive Summary
This paper addresses continual learning for skeleton-based action recognition, where the goal is to learn new classes over time without forgetting previously learned ones. The authors propose CHARON, which improves memory efficiency by compressing skeleton sequences via uniform sampling and linear interpolation. They also introduce a masked autoencoder-inspired training approach that reconstructs partial inputs and uses linear probing to adapt to full sequences during inference. Experiments on Split NTU-60 and a newly introduced Split NTU-120 show that CHARON outperforms existing methods, achieving state-of-the-art accuracy with reduced computational cost.

## Method Summary
CHARON employs a transformer-based backbone (modified STTFormer) with a memory buffer for replay, temporal compression via uniform sampling with linear interpolation for decompression, random masking of input frames during training, reconstruction-based auxiliary loss, and linear probing at the end of each task to realign the classifier. The method processes 3D skeleton sequences from NTU RGB+D datasets, converting them into incremental tasks. Training involves 30 epochs per task with masking ratio 30%, followed by 3 epochs of linear probing per task to realign the classifier. The approach is evaluated using Final Average Accuracy (FAA) across different buffer sizes and masking ratios.

## Key Results
- CHARON outperforms existing methods on both Split NTU-60 and Split NTU-120 benchmarks
- Achieves up to 25x compression ratios while maintaining recognition accuracy
- Reduces training computational requirements through masked autoencoder training
- Linear probing phase improves performance by realigning classifier to full-sequence inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Temporal redundancy in skeleton sequences allows effective compression via uniform sampling and linear interpolation without significant loss of recognition accuracy.
- Mechanism: Skeletal data inherently resides in Euclidean space where joints move smoothly over time. By uniformly sampling one frame every s frames and later reconstructing via linear interpolation, the method retains essential motion patterns while drastically reducing memory footprint in the episodic buffer.
- Core assumption: Human joint trajectories are sufficiently smooth and temporally redundant that missing intermediate frames can be accurately interpolated.
- Evidence anchors:
  - [abstract]: "enhance the memory efficiency of each sample, thus expanding the effective capacity of the buffer within the same memory allocation"
  - [section 3.2]: "skeleton sequences often exhibit temporal redundancy [16]. In light of these peculiarities, skeletal data can be easily compressed upon need: for instance, we do so before storing a sequence into the memory buffer. Notably, the compressed sequences can also be reconstructed with minimal loss through a simple linear interpolation"
  - [corpus]: Weak - No corpus papers directly address temporal redundancy compression for skeleton sequences.
- Break condition: If skeleton sequences contain highly erratic or non-smooth motion, linear interpolation would fail to reconstruct missing frames accurately, leading to degraded recognition performance.

### Mechanism 2
- Claim: Masked autoencoder training with random frame masking reduces computational cost during training while maintaining recognition accuracy.
- Mechanism: By randomly masking a subset of frames (η ratio) and training the encoder to reconstruct the full sequence via a lightweight decoder, the method reduces the number of tokens processed per forward pass. This cuts training time and memory usage. The reconstruction task also acts as a regularizer, encouraging the encoder to learn more robust spatiotemporal features.
- Core assumption: Randomly masking frames during training forces the encoder to learn robust representations that generalize to full sequences during inference.
- Evidence anchors:
  - [abstract]: "Through techniques like uniform sampling, interpolation, and a memory-efficient training stage based on masking, we achieve improved recognition accuracy while minimizing computational overhead"
  - [section 3.2]: "we feed the encoder with a temporally masked sample obtained by dropping a random subset of frames from the input sequence... Such a choice brings the benefit of reducing the training requirements of the whole pipeline, avoiding the pre-training phase"
  - [corpus]: Weak - While masked autoencoders exist in computer vision, specific application to skeleton-based action recognition in continual learning is not well-documented in corpus.
- Break condition: If masking ratio is too high (>50-60%), the encoder may not learn sufficient temporal context, leading to poor recognition performance despite computational savings.

### Mechanism 3
- Claim: Linear probing phase after each task realigns the classifier to prevent covariate shift between masked training and full-sequence inference.
- Mechanism: During training, the encoder sees only masked sequences, but during inference it must process full sequences. This mismatch can cause classifier misalignment. By freezing the encoder and fine-tuning only the classifier on full sequences for a few epochs (10% of main training), the method corrects this shift without significant computational overhead.
- Core assumption: The encoder's learned features remain valid when applied to full sequences, but the classifier needs adjustment to the new feature distribution.
- Evidence anchors:
  - [abstract]: "we introduce a linear probing phase to better conciliate the self-reconstruction approach with online scenarios"
  - [section 3.2]: "we argue that the classification heads could be subject to possible misalignment due to the different conditions we have at training (masking on) and test time (masking off)... we freeze the encoder parameters and realign the classifier in the presence of unmasked input sequences"
  - [corpus]: Weak - Linear probing is common in self-supervised learning but its specific application to mitigate covariate shift in masked skeleton training is not well-documented.
- Break condition: If the encoder's feature space changes significantly between tasks, simply realigning the classifier may be insufficient, requiring more extensive adaptation.

## Foundational Learning

- Concept: Temporal redundancy in time-series data
  - Why needed here: Understanding why skeleton sequences can be compressed without significant information loss is crucial for implementing the buffer strategy effectively.
  - Quick check question: Why can we sample every 5th frame in a skeleton sequence and still reconstruct it accurately via linear interpolation?

- Concept: Masked autoencoders and self-supervised learning
  - Why needed here: The masking strategy is central to reducing computational cost during training while maintaining performance.
  - Quick check question: How does masking frames during training force the encoder to learn more robust spatiotemporal features?

- Concept: Catastrophic forgetting and rehearsal strategies in continual learning
  - Why needed here: The method uses an episodic memory buffer to replay past samples and prevent forgetting, which is fundamental to continual learning.
  - Quick check question: Why is it important to store compressed samples in the memory buffer rather than full sequences?

## Architecture Onboarding

- Component map:
  STTFormer backbone (encoder-only variant) -> Masked input preprocessing -> Lightweight decoder for reconstruction -> Classification head -> Memory buffer with compression/reconstruction pipeline -> Linear probing module

- Critical path:
  1. Input skeleton sequence → Uniform sampling + compression
  2. Store compressed sample in memory buffer
  3. During training: Random masking → Encoder → Classification + Reconstruction
  4. After each task: Linear probing with full sequences
  5. During inference: Full sequence → Encoder → Classification

- Design tradeoffs:
  - Higher masking ratios reduce training cost but risk performance degradation
  - Higher sampling intervals increase buffer capacity but may reduce reconstruction fidelity
  - Linear probing adds post-task overhead but improves final accuracy

- Failure signatures:
  - Poor performance on tasks learned early (forgetting)
  - Degradation when masking ratio exceeds 50-60%
  - Memory buffer overflow if compression is insufficient
  - Classifier misalignment between training and inference

- First 3 experiments:
  1. Verify temporal redundancy by comparing recognition accuracy using full sequences vs. uniformly sampled + interpolated sequences
  2. Test masking strategy by training with varying masking ratios (30%, 50%, 60%) and measuring accuracy vs. computational cost
  3. Validate linear probing by comparing performance with and without the post-task classifier realignment phase

## Open Questions the Paper Calls Out
None

## Limitations
- Effectiveness of linear interpolation for reconstructing skeleton sequences from sparse samples may not hold for complex or non-smooth motion patterns
- Optimal masking ratio appears task-dependent, with limited exploration beyond 75% masking
- Newly proposed Split NTU-120 benchmark has limited validation with only one competing method tested

## Confidence

### Major Uncertainties and Limitations
The primary uncertainty lies in the effectiveness of linear interpolation for reconstructing skeleton sequences from sparse samples, particularly for complex or non-smooth motion patterns. The paper assumes sufficient temporal redundancy in skeletal data, but this assumption may not hold for all action categories. Additionally, the masking strategy's optimal ratio appears task-dependent, with the paper only exploring up to 75% masking, leaving questions about robustness at higher ratios. The newly proposed Split NTU-120 benchmark, while addressing a gap in continual learning literature, has limited validation with only one competing method tested.

### Confidence Labels
- **High confidence**: The memory efficiency improvements from compression are well-supported by the mathematical formulation and ablation studies showing performance retention at 20-25x compression ratios.
- **Medium confidence**: The masking strategy's computational benefits are demonstrated, but the optimal masking ratio appears sensitive to task complexity, suggesting the need for adaptive masking in practice.
- **Medium confidence**: The linear probing phase shows consistent improvements, but the mechanism (classifier realignment vs. additional fine-tuning) could benefit from more rigorous ablation studies.

## Next Checks
1. Test reconstruction fidelity across diverse action categories with varying motion complexity to quantify the limits of temporal redundancy assumptions.
2. Implement an adaptive masking strategy that adjusts the masking ratio based on task difficulty or buffer utilization to optimize the computational-accuracy tradeoff.
3. Conduct extensive ablation studies isolating the contributions of compression, masking, and linear probing to quantify their individual and combined effects on FAA across multiple buffer sizes.