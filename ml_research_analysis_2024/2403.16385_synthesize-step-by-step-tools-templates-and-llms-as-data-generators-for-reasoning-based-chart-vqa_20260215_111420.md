---
ver: rpa2
title: 'Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for
  Reasoning-Based Chart VQA'
arxiv_id: '2403.16385'
source_url: https://arxiv.org/abs/2403.16385
tags: []
core_contribution: This paper addresses the challenge of complex reasoning in chart
  visual question answering (chart VQA) models. It proposes using Large Language Models
  (LLMs) as automatic data annotators to generate question-answer annotations for
  chart images, enhancing the reasoning ability of chart VQA models.
---

# Synthesize Step-by-Step: Tools, Templates and LLMs as Data Generators for Reasoning-Based Chart VQA

## Quick Facts
- arXiv ID: 2403.16385
- Source URL: https://arxiv.org/abs/2403.16385
- Authors: Zhuowan Li; Bhavan Jasani; Peng Tang; Shabnam Ghadar
- Reference count: 40
- Key outcome: 16-point accuracy improvement (38% to 54%) on human-written questions in ChartQA dataset using LLM-augmented synthetic data

## Executive Summary
This paper addresses the challenge of complex reasoning in chart visual question answering (chart VQA) models. It proposes using Large Language Models (LLMs) as automatic data annotators to generate question-answer annotations for chart images, enhancing the reasoning ability of chart VQA models. The key innovation is the Synthesize Step-by-Step strategy, which decomposes complex questions into step-by-step sub-questions and derives the final answer using external tools like Python. The method is trained on synthetic data generated using a template-based QA generation pipeline. Experimental results show that training with the LLM-augmented data (LAMENDA) significantly improves the accuracy of chart VQA models, achieving state-of-the-art performance on the ChartQA and PlotQA datasets.

## Method Summary
The proposed approach leverages LLMs as automatic data annotators to generate question-answer pairs for chart images, addressing the challenge of complex reasoning in chart VQA. The method introduces a Synthesize Step-by-Step strategy that breaks down complex questions into simpler sub-questions, using external tools like Python to derive intermediate answers and ultimately compute the final response. A template-based QA generation pipeline creates synthetic training data, which is then used to train the LAMENDA model. This approach significantly improves reasoning capabilities compared to traditional chart VQA methods, achieving state-of-the-art performance on established benchmarks.

## Key Results
- 16-point accuracy improvement on human-written questions in ChartQA dataset (38% to 54%)
- State-of-the-art performance achieved on both ChartQA and PlotQA datasets
- Demonstrates effectiveness of LLM-augmented synthetic data for complex reasoning tasks

## Why This Works (Mechanism)
The method works by leveraging LLMs' reasoning capabilities to generate diverse question-answer pairs, while the Synthesize Step-by-Step strategy enables handling of complex multi-step reasoning tasks through decomposition. By using external tools like Python for computation, the approach can handle numerical and analytical aspects that pure visual models struggle with. The template-based generation pipeline ensures comprehensive coverage of different question types and reasoning patterns, creating a rich training dataset that improves model generalization.

## Foundational Learning
**LLM as Data Annotator**: Large Language Models can generate high-quality question-answer pairs when given appropriate prompts and context. This is needed because manual annotation of chart VQA data is time-consuming and expensive. Quick check: Evaluate generated annotations against human-written questions for quality and diversity.

**Template-Based QA Generation**: Structured templates ensure systematic coverage of different question types and reasoning patterns. This is needed to create comprehensive training data that addresses various aspects of chart understanding. Quick check: Analyze template coverage across different chart types and question categories.

**Synthesize Step-by-Step Strategy**: Decomposing complex questions into simpler sub-questions enables systematic reasoning. This is needed because complex reasoning tasks often require multiple intermediate steps. Quick check: Verify that all sub-questions contribute meaningfully to the final answer.

## Architecture Onboarding

**Component Map**: Chart Images -> Visual Encoder -> LLM-based Question Generator -> Python Computation Engine -> Answer Derivation -> LAMENDA Training

**Critical Path**: Visual features → Question generation → Step-by-step decomposition → External tool computation → Answer synthesis

**Design Tradeoffs**: The approach trades computational overhead (using external tools) for improved reasoning accuracy. This enables handling complex numerical reasoning but may limit real-time deployment.

**Failure Signatures**: 
- Incorrect visual encoding leading to wrong question generation
- Sub-question decomposition failures causing reasoning breakdown
- External tool computation errors propagating to final answer
- Template coverage gaps resulting in poor generalization

**First Experiments**:
1. Evaluate LLM-generated question quality on a small subset of charts
2. Test the Synthesize Step-by-Step strategy on simple numerical questions
3. Validate template coverage across different chart types

## Open Questions the Paper Calls Out
None

## Limitations
- Reliance on synthetic data without extensive real human annotation validation
- Template-based generation may not capture full diversity of real-world queries
- External tool dependency (Python) may limit deployment flexibility
- No assessment of model performance on adversarial or edge-case chart examples

## Confidence
- 16-point accuracy improvement on human-written questions: **High confidence**
- LLM as data annotator methodology: **Medium confidence**
- Generalizability of Synthesize Step-by-Step strategy: **Low confidence**

## Next Checks
1. Conduct ablation studies comparing LAMENDA performance against models trained with real human-annotated data
2. Test the generalizability of the Synthesize Step-by-Step strategy on non-chart VQA tasks
3. Perform robustness testing using adversarial chart examples and edge-case questions