---
ver: rpa2
title: Are Large Language Models More Empathetic than Humans?
arxiv_id: '2406.05063'
source_url: https://arxiv.org/abs/2406.05063
tags: []
core_contribution: The study compared empathetic response generation between humans
  and four large language models (GPT-4, LLaMA-2, Gemini-Pro, and Mixtral-8x7B) using
  a between-subjects design with 1,000 participants. Participants rated responses
  to 2,000 emotional dialogue prompts spanning 32 distinct emotions.
---

# Are Large Language Models More Empathetic than Humans?

## Quick Facts
- arXiv ID: 2406.05063
- Source URL: https://arxiv.org/abs/2406.05063
- Reference count: 38
- LLMs (GPT-4, LLaMA-2, Gemini-Pro, Mixtral-8x7B) outperformed humans in empathetic response generation with 21-31% improvement in "Good" ratings

## Executive Summary
This study conducted a large-scale comparative analysis of empathetic response generation between humans and four leading large language models (LLMs). Using a between-subjects design with 1,000 participants rating 2,000 responses to 32 distinct emotional prompts, the research found that all four LLMs significantly outperformed human participants in empathetic responding. GPT-4 achieved the highest performance with approximately 31% more "Good" ratings than human responses, while other models showed improvements of 24-10%.

The study's novel evaluation framework demonstrated superior statistical power and scalability compared to traditional within-subjects designs, enabling efficient future LLM comparisons without requiring full replication studies. The findings suggest that LLMs can match or exceed human performance in controlled empathetic text generation tasks, though the results are limited to textual responses without non-verbal cues.

## Method Summary
The study employed a between-subjects experimental design where 1,000 participants rated responses to 2,000 emotional dialogue prompts. Four LLMs (GPT-4, LLaMA-2, Gemini-Pro, and Mixtral-8x7B) and human responses were evaluated using a 5-point Likert scale for empathetic responding. The study covered 32 distinct emotions and used statistical analysis to compare performance across models and humans, with the between-subjects approach enabling more efficient future evaluations.

## Key Results
- All four LLMs outperformed human participants in empathetic response generation
- GPT-4 achieved highest performance with ~31% increase in "Good" ratings over human baseline
- LLaMA-2, Mixtral-8x7B, and Gemini-Pro showed improvements of approximately 24%, 21%, and 10% respectively
- Between-subjects design demonstrated superior statistical power for future LLM comparisons

## Why This Works (Mechanism)
None

## Foundational Learning

Empathy Assessment Frameworks
- Why needed: To evaluate and compare empathetic responses objectively across humans and AI
- Quick check: Use validated psychological scales for empathy measurement in human-AI comparisons

Large Language Model Architectures
- Why needed: Understanding different model capabilities and training approaches
- Quick check: Review transformer architecture fundamentals and fine-tuning methods

Statistical Power Analysis
- Why needed: To determine appropriate sample sizes and experimental design validity
- Quick check: Calculate effect sizes and required participant numbers for between-subjects designs

## Architecture Onboarding

Component Map:
Emotional Prompts -> Response Generation (Human/LLMs) -> Rating Collection (1000 participants) -> Statistical Analysis -> Performance Comparison

Critical Path:
Prompt Generation → Response Generation → Rating Collection → Statistical Analysis

Design Tradeoffs:
- Between-subjects vs within-subjects design: Between-subjects offered better statistical power but lost direct comparison capability
- 5-point Likert scale: Simple but may oversimplify complex empathy dimensions
- Single language focus: Enables cultural consistency but limits generalizability

Failure Signatures:
- Low inter-rater reliability indicating ambiguous response evaluation criteria
- Statistical insignificance between human and LLM responses suggesting evaluation methodology issues
- Cultural bias in responses or ratings pointing to limited cultural context

First Experiments:
1. Conduct inter-rater reliability analysis on a subset of responses
2. Test model performance with different prompt types and emotional intensities
3. Evaluate cross-cultural consistency by translating prompts and responses

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation methodology may introduce systematic biases between human and machine-generated response assessment
- 5-point Likert scale may oversimplify complex emotional interactions and empathy
- Findings limited to textual responses without non-verbal cues or cultural context considerations

## Confidence

High confidence in:
- Statistical methodology and comparative performance metrics between models
- Relative ranking of LLMs versus humans appears robust given large sample size

Medium confidence in:
- Absolute empathy ratings and percentage improvements claimed
- These are dependent on subjective nature of empathy assessment

Low confidence in:
- Generalizability to real-world empathetic interactions and cross-cultural contexts
- Controlled environment may not reflect natural empathetic communication scenarios

## Next Checks

1. Conduct a within-subjects validation study where same participants rate both human and LLM responses to identical prompts, allowing for direct comparison and control of individual rating biases.

2. Expand evaluation to include multi-modal inputs (text, voice, video) to assess whether findings hold when non-verbal cues are present.

3. Test models across diverse cultural contexts and languages to validate whether observed performance differences persist across different cultural frameworks of empathy expression and interpretation.