---
ver: rpa2
title: 'Position: Explain to Question not to Justify'
arxiv_id: '2402.13914'
source_url: https://arxiv.org/abs/2402.13914
tags:
- explanations
- such
- different
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies a crisis in the field of Explainable Artificial
  Intelligence (XAI) stemming from divergent and incompatible goals. It proposes separating
  XAI into two complementary cultures: BLUE XAI (human/value-oriented explanations)
  and RED XAI (model/validation-oriented explanations).'
---

# Position: Explain to Explain not to Justify

## Quick Facts
- arXiv ID: 2402.13914
- Source URL: https://arxiv.org/abs/2402.13914
- Reference count: 20
- The paper proposes separating XAI into BLUE XAI (human/value-oriented) and RED XAI (model/validation-oriented) cultures to address a crisis in the field and open new research directions for model debugging and exploration.

## Executive Summary
The paper identifies a crisis in the field of Explainable Artificial Intelligence (XAI) stemming from divergent and incompatible goals. It proposes separating XAI into two complementary cultures: BLUE XAI (human/value-oriented explanations) and RED XAI (model/validation-oriented explanations). The authors argue that RED XAI is under-explored and presents significant research opportunities for questioning models, extracting knowledge from well-performing models, and spotting and fixing bugs in faulty models. They present promising challenges for RED XAI including the construction of complementary explanations, need for benchmarks and tools, explorer mindset for data and models, multiple models and Rashomon explanations, and XAI for Science. The paper concludes that focusing on RED XAI can lead to important research necessary to ensure the safety of AI systems.

## Method Summary
This is a conceptual position paper based on literature review and analysis of XAI methods, survey papers, and applications. The authors critically analyze the current state of XAI research, identify divergent goals causing a "crisis," and propose a new framework separating BLUE XAI (human/values-oriented) from RED XAI (model/validation-oriented). They identify five fallacies in current XAI thinking and outline specific research challenges for RED XAI development.

## Key Results
- RED XAI is under-explored and presents significant research opportunities for questioning models, extracting knowledge from well-performing models, and spotting and fixing bugs in faulty models
- The separation of BLUE XAI (user-focused) from RED XAI (developer-focused) can reduce confusion and enable more effective research in both areas
- Promising RED XAI challenges include complementary explanations, benchmarks/tools, explorer mindset, Rashomon set exploration, and XAI for Science applications

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The paper introduces a practical framework by distinguishing two complementary XAI cultures, enabling focused research and reducing confusion.
- Mechanism: By separating BLUE XAI (human/values-oriented) from RED XAI (model/validation-oriented), the paper clarifies that these serve different audiences and purposes, preventing conflation of trust/ethics goals with model debugging and exploration goals.
- Core assumption: The two cultures are largely independent and addressing both together creates inefficiency and misdirection in the XAI field.
- Evidence anchors:
  - [abstract] The authors argue that RED XAI is under-explored and presents significant research opportunities for questioning models, extracting knowledge from well-performing models, and spotting and fixing bugs in faulty models.
  - [section 2] We identify two cultures of thinking about explainability – one centered on human values such as fairness, ethics, trust and the other centered on model validation, exploration, and research on model and data.
- Break condition: If RED XAI and BLUE XAI goals become inseparable in practice, or if RED XAI research becomes dominated by user trust considerations, the framework loses utility.

### Mechanism 2
- Claim: RED XAI is positioned as essential for AI safety and debugging, which motivates new research directions and tools.
- Mechanism: By highlighting that RED XAI enables model developers to understand, debug, and improve models (rather than justify to end users), the paper reframes XAI as a critical engineering practice for robust AI systems.
- Core assumption: AI safety and robustness fundamentally depend on developers having deep insight into model behavior, not just user-facing justifications.
- Evidence anchors:
  - [abstract] We conclude this paper by presenting promising challenges in this area... necessary to ensure the safety of AI systems.
  - [section 3] Challenge: need for benchmarks, tools, standards. RED XAI is more engineering-focused, which also means it can benefit more from automating and streamlining the way models are explored.
- Break condition: If AI safety can be achieved without deep model introspection, or if RED XAI tools do not translate into measurable model improvements.

### Mechanism 3
- Claim: Introducing a multi-faceted, interactive approach to model explanation increases effectiveness for RED XAI practitioners.
- Mechanism: The paper advocates for complementary explanations (e.g., SHAP, LIME, PDP) used together rather than seeking a single "silver bullet," enabling richer exploration of model behavior and detection of issues.
- Core assumption: No single explanation method can fully capture the behavior of complex models; different methods reveal different aspects.
- Evidence anchors:
  - [abstract] They present promising challenges for RED XAI including the construction of complementary explanations...
  - [section 3] Challenge: construction of complementary explanations. There is no single best technique for explanations and we need a multi-faced approach to model analysis.
- Break condition: If practitioners find that combining multiple methods adds complexity without practical benefit, or if a single method emerges that captures all needed insights.

## Foundational Learning

- Concept: Model interpretability vs. explainability distinction
  - Why needed here: The paper hinges on clarifying what explanations are for; understanding this difference helps grasp why BLUE and RED XAI serve different purposes.
  - Quick check question: Is a linear regression model automatically "interpretable," or does it still require explanation methods for complex, high-dimensional data?

- Concept: Rashomon set and multiple good models
  - Why needed here: The paper discusses exploring sets of models with similar performance but different behaviors, which is key to understanding RED XAI's focus on model exploration.
  - Quick check question: If two models achieve the same accuracy, why might their explanations differ, and how could that be useful?

- Concept: User study limitations in XAI evaluation
  - Why needed here: The paper critiques the over-reliance on user studies for validating XAI methods, especially for RED XAI, where the audience is technical.
  - Quick check question: When would a user study be unnecessary or even misleading for evaluating an XAI technique?

## Architecture Onboarding

- Component map:
  - RED XAI tools: Model debugging interfaces, interactive exploration dashboards, explanation method suites (SHAP, LIME, PDP, etc.), benchmarks for model analysis
  - RED XAI workflow: Data/model loading → Exploration via multiple explanations → Detection of model issues → Debugging/refinement → Validation
  - Supporting infrastructure: Data visualization libraries, model-agnostic explanation frameworks (e.g., DALEX), benchmark datasets, automated testing pipelines

- Critical path:
  1. Load a trained model and dataset
  2. Apply multiple complementary explanation methods
  3. Visualize and compare explanations to identify inconsistencies or model weaknesses
  4. Use insights to debug or improve the model
  5. Validate improvements with new explanations and, if needed, user studies (for BLUE XAI components)

- Design tradeoffs:
  - Depth vs. simplicity: RED XAI explanations can be complex and technical; balance between actionable insights and cognitive load for developers
  - Automation vs. manual exploration: Automated "checks" can streamline debugging, but manual creativity remains important
  - Breadth vs. specificity: Using many explanation methods provides richer insights but may overwhelm; focus on key methods relevant to the task

- Failure signatures:
  - Explanations from different methods contradict each other without clear reason
  - RED XAI tools do not lead to actionable model improvements
  - RED XAI is used to justify rather than debug, blurring lines with BLUE XAI

- First 3 experiments:
  1. Apply SHAP, LIME, and PDP to a trained model and compare explanations for a subset of predictions to identify discrepancies
  2. Use a benchmark dataset (e.g., FICO challenge) to test RED XAI methods' ability to detect model issues
  3. Implement a simple interactive dashboard (e.g., with ExplAIner or similar) to allow model developers to explore explanations and debug models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific technical metrics and benchmarks needed to evaluate RED XAI methods for model debugging and knowledge extraction?
- Basis in paper: [explicit] The paper explicitly calls for the development of benchmarks, tools, and standards for RED XAI, particularly for model debugging applications.
- Why unresolved: Current XAI evaluation relies heavily on user studies and subjective measures, which are inadequate for assessing RED XAI's engineering-focused goals of model exploration and debugging.
- What evidence would resolve it: Creation of standardized benchmark datasets with known model failures and performance metrics that quantify the effectiveness of RED XAI methods in detecting and explaining model errors.

### Open Question 2
- Question: How can complementary explanations be effectively constructed and integrated to provide a comprehensive understanding of complex model behaviors?
- Basis in paper: [explicit] The paper identifies the challenge of constructing complementary explanations and references the "Blind men and an elephant" story to illustrate the need for multiple perspectives.
- Why unresolved: Current XAI methods typically provide single-perspective explanations, and there's no established framework for combining different explanation types to create a holistic understanding.
- What evidence would resolve it: Development and validation of methods that can automatically combine multiple explanation techniques (e.g., SHAP, LIME, partial dependence plots) to create integrated, multi-faceted explanations that reveal different aspects of model behavior.

### Open Question 3
- Question: What are the most effective ways to explore and analyze the Rashomon set of models to identify simpler, more robust models that perform equally well?
- Basis in paper: [explicit] The paper discusses the concept of Rashomon explanations and the potential to find simpler models within the Rashomon set, but notes this is still under-researched.
- Why unresolved: While the concept of the Rashomon set is established, there are no systematic methods for exploring it to find simpler, more interpretable models that maintain performance.
- What evidence would resolve it: Development of algorithms that can efficiently characterize the Rashomon set and identify models that are simpler, more robust, or better aligned with domain knowledge while maintaining performance.

## Limitations
- The paper is conceptual and does not provide specific benchmarks, datasets, or experimental results to validate the RED XAI framework
- The separation of BLUE and RED XAI goals may be difficult to maintain in practice as user trust and model debugging needs often overlap
- There is uncertainty about whether RED XAI tools will actually lead to measurable improvements in model safety and robustness

## Confidence
- High: The paper accurately identifies current issues in XAI research, particularly the confusion caused by conflating user-facing trust and model debugging goals
- Medium: The RED XAI framework is both necessary and sufficient to address these issues, given the lack of empirical validation
- Low: The practical utility of the proposed RED XAI research directions without further concrete examples or case studies

## Next Checks
1. Conduct a small-scale study where a team of data scientists uses RED XAI tools (e.g., multiple complementary explanations) to debug a model, measuring the impact on model quality and developer understanding
2. Survey current XAI practitioners to assess whether the BLUE/RED XAI distinction is recognized and useful in their daily work
3. Attempt to implement and evaluate a simple RED XAI pipeline (e.g., SHAP + LIME + PDP on a benchmark dataset) to identify practical challenges and benefits