---
ver: rpa2
title: Quantifying the Risks of Tool-assisted Rephrasing to Linguistic Diversity
arxiv_id: '2410.17670'
source_url: https://arxiv.org/abs/2410.17670
tags:
- rephrasing
- text
- texts
- llms
- writing
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study quantifies how writing assistants and large language
  models (LLMs) alter text when used for rephrasing. The authors analyzed 819 paragraphs
  from eight domains using four writing assistants (Grammarly, Quillbot, Wordtune,
  Rephrase) and four LLMs (ChatGPT, GPT-4, Vicuna-13b, Flan-T5).
---

# Quantifying the Risks of Tool-assisted Rephrasing to Linguistic Diversity

## Quick Facts
- arXiv ID: 2410.17670
- Source URL: https://arxiv.org/abs/2410.17670
- Reference count: 39
- Primary result: LLMs significantly alter text more than writing assistants, reducing vocabulary overlap and semantic diversity

## Executive Summary
This study quantifies how writing assistants and large language models (LLMs) alter text when used for rephrasing. The authors analyzed 819 paragraphs from eight domains using four writing assistants and four LLMs, measuring changes in length, vocabulary overlap, semantic similarity, and semantic dispersion. They found that LLMs consistently shorten text more than writing assistants and cause significantly greater changes to vocabulary and semantics, with Jaccard similarity scores averaging 0.88 compared to writing assistants above 0.94. The study concludes that LLMs should not be considered equivalent replacements for writing assistants due to their more dramatic alterations to text.

## Method Summary
The study used 819 paragraphs from eight domains (literature, academic papers, encyclopedic texts, instruction manuals, news, social media posts, interview transcripts, and speeches) collected from pre-2010 sources. Four writing assistants (Grammarly, Quillbot, Wordtune, Rephrase) were used with manual rephrasing following strict acceptance rules, while four LLMs (ChatGPT, GPT-4, Vicuna-13b, Flan-T5) were used via zero-shot prompting with optimized prompts. The analysis measured four word-based metrics (paragraph length change, Jaccard similarity) and two vector-based metrics (cosine similarity, conicity) comparing original and rephrased texts.

## Key Results
- LLMs consistently shorten text more than writing assistants across all domains
- LLMs show significantly lower vocabulary overlap (Jaccard scores average 0.88) compared to writing assistants (above 0.94)
- LLMs cause greater semantic alterations and reduce semantic dispersion (conicity) while writing assistants have minimal impact

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs consistently shorten text more than writing assistants, reducing overall text length.
- Mechanism: LLMs are trained on summarization tasks during instruction tuning, leading them to default to shorter outputs even when asked for rephrasing.
- Core assumption: The rephrasing prompts used were neutral and did not explicitly request maintaining original length.
- Evidence anchors:
  - [abstract] "LLMs consistently shorten text more than writing assistants"
  - [section] "we find that the latter are more strongly inclined to reduce the length of the text, despite the token generation constraint being set well above the length of the input texts"
  - [corpus] "text length reduction is observed across domains, with LLMs causing significantly greater shortening than writing assistants"
- Break condition: If prompts explicitly instruct LLMs to maintain or extend length, the shortening behavior may diminish or reverse.

### Mechanism 2
- Claim: LLMs cause significantly greater changes to vocabulary and semantics compared to writing assistants.
- Mechanism: LLMs use their own default vocabulary during rephrasing, shifting text away from the original domain-specific vocabulary.
- Core assumption: The rephrasing process with LLMs introduces vocabulary shifts that are not present with writing assistants.
- Evidence anchors:
  - [abstract] "LLMs cause significantly greater changes to vocabulary and semantics"
  - [section] "LLMs also show stronger intra-domain vocabulary shifts, suggesting adoption of their own default vocabularies"
  - [corpus] "Jaccard similarity scores are lower for LLMs (average 0.88) compared to writing assistants (above 0.94), indicating less vocabulary overlap"
- Break condition: If domain-specific vocabulary is explicitly preserved in prompts, the vocabulary shift may be reduced.

### Mechanism 3
- Claim: LLMs narrow semantic spread while writing assistants have minimal impact.
- Mechanism: LLMs' rephrasing process reduces semantic diversity by converging text toward their internal semantic representations.
- Core assumption: The reduction in conicity indicates a narrowing of semantic spread, which is more pronounced with LLMs.
- Evidence anchors:
  - [abstract] "Conicity measurements reveal LLMs narrow semantic spread while writing assistants have minimal impact"
  - [section] "LLMs cause a narrowing of the semantic spread in the texts, while WATs cause close to no change"
  - [corpus] "Conicity consistently decreases for LLMs for both embeddings, indicating reduced semantic dispersion"
- Break condition: If prompts encourage semantic diversity or preserve original semantic spread, the narrowing effect may be mitigated.

## Foundational Learning

- Concept: Jaccard similarity
  - Why needed here: To measure vocabulary overlap between original and rephrased texts, quantifying how much the vocabulary changes.
  - Quick check question: If two texts have identical vocabularies, what would their Jaccard similarity score be?

- Concept: Cosine similarity
  - Why needed here: To assess semantic similarity between original and rephrased texts using vector embeddings, indicating how much meaning is preserved.
  - Quick check question: If two paragraphs have identical meanings but different wordings, what would their cosine similarity be?

- Concept: Conicity
  - Why needed here: To measure semantic dispersion in embedding space, indicating whether rephrased text narrows or broadens semantic spread.
  - Quick check question: If rephrased text converges toward a single semantic representation, would conicity increase or decrease?

## Architecture Onboarding

- Component map: Input paragraphs -> Writing assistants (Grammarly, Quillbot, Wordtune, Rephrase) and LLMs (ChatGPT, GPT-4, Vicuna-13b, Flan-T5) -> Metric computation (Jaccard, cosine similarity, conicity) -> Analysis
- Critical path: 1) Select paragraphs from corpus, 2) Rephrase using each tool, 3) Compute metrics comparing original and rephrased text, 4) Analyze results across domains
- Design tradeoffs: Manual rephrasing with writing assistants ensures consistency but limits scale; automated LLM rephrasing enables larger scale but introduces variability in outputs
- Failure signatures: Unexpected length changes, low vocabulary overlap, reduced semantic similarity, or abnormal conicity shifts may indicate tool malfunction or prompt issues
- First 3 experiments:
  1. Test rephrasing with a single tool on a small sample to verify metric computation works correctly
  2. Compare results between two tools on the same text to validate consistency in measurements
  3. Run full pipeline on one domain to check domain-specific patterns before scaling to all domains

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific prompting strategies affect the linguistic diversity changes caused by LLMs?
- Basis in paper: [explicit] The authors note that future work should investigate whether style-sensitive prompting strategies can mitigate the impact of LLMs on semantic and vocabulary changes
- Why unresolved: The study only used neutral rephrasing prompts without style requests or domain-specific adaptations, leaving the question of how different prompts might influence the extent of vocabulary shrinkage and semantic alteration
- What evidence would resolve it: Comparative experiments testing various prompt types (style-specific, domain-specific, length-constrained) with the same corpus and metrics to measure differences in Jaccard similarity, semantic similarity, and conicity scores

### Open Question 2
- Question: What is the long-term impact of widespread writing assistant adoption on language evolution?
- Basis in paper: [inferred] The authors express concern about vocabulary shrinkage and loss of linguistic diversity when LLMs shift vocabulary toward their internal defaults, but their corpus size was limited due to manual processing requirements
- Why unresolved: The study's corpus of 819 paragraphs, while multi-domain, is insufficient to fully quantify vocabulary shift patterns across larger populations and longer time periods
- What evidence would resolve it: Large-scale longitudinal studies tracking vocabulary changes in texts produced with writing assistants over extended periods, comparing user groups with and without assistant usage

### Open Question 3
- Question: How do writing assistants affect non-English languages and language varieties?
- Basis in paper: [explicit] The authors explicitly state their results are restricted to English and that further languages should be considered
- Why unresolved: The study only examined English texts, leaving unknown whether the observed patterns of vocabulary reduction and semantic alteration generalize to other languages with different grammatical structures
- What evidence would resolve it: Replication of the study's methodology across multiple languages with varying morphological complexity, using native speakers to assess semantic preservation and vocabulary changes

## Limitations

- The corpus consists of pre-2010 texts, which may not represent contemporary writing styles or domain-specific vocabulary accurately
- Manual rephrasing process for writing assistants introduces potential human bias in tool selection and parameter tuning
- Study focuses only on English text, limiting applicability to other languages with different grammatical structures

## Confidence

**High Confidence**: The finding that LLMs consistently shorten text more than writing assistants is well-supported by direct measurements across all domains. The vocabulary overlap differences (Jaccard scores) between LLMs and writing assistants are robust and consistently measured. The reduction in semantic dispersion (conicity) for LLMs is demonstrated through multiple embedding models.

**Medium Confidence**: The interpretation that LLMs adopt their own default vocabulary during rephrasing assumes this behavior is intrinsic to the models rather than an artifact of prompt formulation. The conclusion that LLMs should not be considered equivalent to writing assistants is reasonable but may depend on specific use cases and user preferences not fully explored in this study.

**Low Confidence**: The specific mechanisms by which LLMs cause vocabulary shifts are not fully characterized. The study does not explore whether different prompting strategies could mitigate the observed effects, particularly for maintaining semantic diversity or vocabulary preservation.

## Next Checks

1. **Prompt Sensitivity Analysis**: Systematically test different rephrasing prompts with LLMs to determine how prompt formulation affects vocabulary overlap, semantic similarity, and text length. This would clarify whether the observed LLM behaviors are intrinsic or prompt-dependent.

2. **Contemporary Corpus Validation**: Repeat the analysis using a corpus of contemporary texts (post-2010) to verify whether the patterns hold for modern writing styles and domain-specific vocabulary that may already be influenced by AI-generated content.

3. **Domain-Specific Prompting**: Test whether domain-specific prompts (e.g., "maintain technical terminology" for academic texts) can reduce the vocabulary shift and semantic narrowing observed with LLMs, potentially making them more comparable to writing assistants for certain use cases.