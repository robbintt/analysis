---
ver: rpa2
title: 'LEGAL-UQA: A Low-Resource Urdu-English Dataset for Legal Question Answering'
arxiv_id: '2410.13013'
source_url: https://arxiv.org/abs/2410.13013
tags:
- urdu
- dataset
- legal
- english
- context
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LEGAL-UQA introduces the first Urdu-English legal question-answering
  dataset derived from Pakistan's constitution, containing 619 parallel QA pairs with
  legal article contexts. The dataset was created through OCR extraction of Urdu text,
  manual refinement, and GPT-4-assisted translation and QA pair generation.
---

# LEGAL-UQA: A Low-Resource Urdu-English Dataset for Legal Question Answering

## Quick Facts
- arXiv ID: 2410.13013
- Source URL: https://arxiv.org/abs/2410.13013
- Reference count: 23
- LEGAL-UQA introduces the first Urdu-English legal question-answering dataset derived from Pakistan's constitution, containing 619 parallel QA pairs with legal article contexts.

## Executive Summary
LEGAL-UQA presents the first Urdu-English legal question-answering dataset derived from Pakistan's constitution, containing 619 parallel QA pairs with legal article contexts. The dataset was created through OCR extraction of Urdu text, manual refinement, and GPT-4-assisted translation and QA pair generation. Experiments show Claude-3.5-Sonnet achieving 99.19% human-evaluated accuracy on the dataset, while OpenAI's text-embedding-3-large outperforms other models in retrieval tasks with 53.23% top-1 accuracy. Fine-tuning mt5-large-UQA-1.0 on the dataset demonstrates challenges in adapting multilingual models to specialized legal domains, achieving only 74.19% accuracy.

## Method Summary
The dataset creation involved OCR extraction of Urdu constitution text using UTRNet and YoloV8, manual refinement with GPT-4 assistance, article-wise chunking with delimiters, and GPT-4 QA pair generation. English QA pairs were then translated to Urdu using GPT-4o while maintaining legal style and tone. The dataset was evaluated through fine-tuning mt5-large-UQA-1.0 on 495 training pairs and testing on 124 validation pairs, alongside retrieval experiments using various embedding models with cosine similarity.

## Key Results
- Claude-3.5-Sonnet achieves 99.19% human-evaluated accuracy on LEGAL-UQA
- OpenAI's text-embedding-3-large outperforms other models in retrieval tasks with 53.23% top-1 accuracy
- Fine-tuning mt5-large-UQA-1.0 demonstrates challenges in domain adaptation, achieving only 74.19% accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4-assisted translation produces high-quality Urdu-English legal QA pairs
- Mechanism: GPT-4 uses provided Urdu context as style guide to maintain legal tone and terminology consistency when translating English QA pairs
- Core assumption: GPT-4 can accurately transfer stylistic and terminological patterns between languages when given appropriate grounding context
- Evidence anchors: [abstract] "We used OpenAI's GPT-4o model, providing it with an English context... The Urdu context was provided to ground the model in the Urdu language style used in the official Urdu Constitution document."
- Break condition: GPT-4 fails to maintain legal terminology consistency or introduces cultural/contextual errors when translating specialized legal concepts

### Mechanism 2
- Claim: Article-wise chunking enables effective RAG-based legal chatbots
- Mechanism: Each constitution article becomes a separate context unit that can be retrieved based on query similarity, allowing precise information access
- Core assumption: Legal questions can be effectively mapped to specific constitutional articles through semantic similarity
- Evidence anchors: [abstract] "Article-wise chunking in the dataset also opens doors for Retrieval-Augmented Generation (RAG)-based legal chatbots"
- Break condition: Legal questions often span multiple articles or require synthesis beyond single article context, making retrieval-based approach insufficient

### Mechanism 3
- Claim: Fine-tuning mT5 models on specialized legal data improves performance on legal QA tasks
- Mechanism: Domain adaptation through continued pre-training on legal corpus enables better understanding of legal terminology and question patterns
- Core assumption: mT5's multilingual capabilities can be effectively adapted to specialized legal domain despite being trained on general text
- Evidence anchors: [abstract] "We fine-tune mt5-large-UQA-1.0, highlighting the challenges of adapting multilingual models to specialized domains, achieving only 74.19% accuracy"
- Break condition: mT5 architecture proves fundamentally insufficient for legal domain understanding regardless of fine-tuning data quantity or quality

## Foundational Learning

- Concept: Legal document structure and terminology
  - Why needed here: Understanding Pakistan's constitutional structure is essential for creating valid QA pairs and evaluating model outputs
  - Quick check question: What are the key components of Pakistan's constitution and how are legal articles typically structured?

- Concept: Cross-lingual question-answering dataset creation
  - Why needed here: The dataset requires parallel QA pairs in Urdu and English with consistent meaning and tone
  - Quick check question: What are the key challenges in maintaining semantic equivalence when creating parallel QA datasets across languages?

- Concept: Retrieval-augmented generation systems
  - Why needed here: The dataset's article-wise chunking is designed for RAG-based legal information retrieval
  - Quick check question: How does document chunking strategy affect retrieval performance in RAG systems?

## Architecture Onboarding

- Component map: OCR pipeline (UTRNet + YoloV8) → Manual refinement → GPT-4 QA generation → GPT-4o translation → Dataset assembly → Model evaluation
- Critical path: OCR extraction → Article parsing → QA pair generation → Translation → Dataset creation → Model fine-tuning/evaluation
- Design tradeoffs: Parallel Urdu-English dataset vs single-language dataset; article-wise chunking vs document-level context; human evaluation vs automated metrics
- Failure signatures: OCR errors leading to incorrect article content; translation drift between Urdu and English pairs; model performance degradation on specialized legal terminology
- First 3 experiments:
  1. Evaluate mT5 fine-tuning with different learning rates and epochs to optimize legal domain adaptation
  2. Test various chunking strategies (sentence-level, paragraph-level, article-level) for RAG retrieval performance
  3. Compare different embedding models (text-embedding-3-large, mistral-embed, text-multilingual-embedding-002) for retrieval task accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LEGAL-UQA scale with larger datasets, and what is the impact on retrieval accuracy and QA pair generation?
- Basis in paper: [inferred] The paper mentions the dataset's small size and plans to expand it, suggesting potential performance changes with larger datasets.
- Why unresolved: The paper does not provide experiments or results on how the model performs with a larger dataset or the impact on retrieval accuracy and QA generation.
- What evidence would resolve it: Conducting experiments with an expanded dataset and analyzing the changes in retrieval accuracy and QA generation performance.

### Open Question 2
- Question: How do models specifically fine-tuned for legal text in Urdu compare to generalist models on LEGAL-UQA?
- Basis in paper: [explicit] The paper notes that pre-trained models and embeddings are not optimized for legal text, especially in low-resource languages like Urdu.
- Why unresolved: The paper does not explore the performance of models specifically fine-tuned for legal text in Urdu, only using generalist models.
- What evidence would resolve it: Training and evaluating models specifically fine-tuned for legal text in Urdu on the LEGAL-UQA dataset and comparing their performance to generalist models.

### Open Question 3
- Question: What are the specific challenges in adapting multilingual models like mT5 to specialized legal domains in low-resource languages?
- Basis in paper: [explicit] The paper highlights challenges in adapting multilingual models to specialized domains and notes the mT5 model's poor performance on the dataset.
- Why unresolved: The paper does not delve into the specific reasons or challenges faced when adapting multilingual models like mT5 to legal domains in low-resource languages.
- What evidence would resolve it: Conducting detailed analysis and experiments to identify specific adaptation challenges, such as vocabulary limitations, context understanding, or domain-specific terminology handling.

### Open Question 4
- Question: How does the inclusion of additional legal domains (e.g., criminal, civil, administrative law) affect the dataset's applicability and model performance?
- Basis in paper: [explicit] The paper mentions plans to expand the dataset to include criminal, civil, and administrative law, indicating potential impacts on applicability and performance.
- Why unresolved: The paper does not provide insights into how expanding the dataset to include additional legal domains would affect its applicability and model performance.
- What evidence would resolve it: Expanding the dataset to include additional legal domains and evaluating the changes in model performance and dataset applicability across these domains.

### Open Question 5
- Question: What are the implications of using GPT-4 for generating QA pairs on the dataset's quality and potential biases?
- Basis in paper: [explicit] The paper acknowledges the use of GPT-4 to generate QA pairs and notes potential subtle errors in style or tone.
- Why unresolved: The paper does not explore the broader implications of using GPT-4 for QA pair generation, such as quality control, bias introduction, or consistency issues.
- What evidence would resolve it: Analyzing the dataset for quality control, bias, and consistency issues introduced by GPT-4 and comparing it with manually generated QA pairs.

## Limitations
- Dataset Size and Coverage: Only 619 parallel QA pairs may lack sufficient diversity for robust legal QA models, with unclear distribution across 312 constitutional articles
- Evaluation Methodology: Human evaluation of only 30 QA pairs for translation quality and limited detail on human accuracy metric methodology
- Model Comparison Fairness: Comparing mT5 fine-tuned on 619 examples against extensively pre-trained generalist models may not fairly assess domain adaptation effectiveness

## Confidence

**High Confidence**: The dataset creation methodology (OCR extraction, manual refinement, GPT-4 QA generation, GPT-4o translation) is clearly specified and reproducible. The experimental setup for retrieval tasks using text-embedding-3-large is well-documented with clear performance metrics.

**Medium Confidence**: The claim that Claude-3.5-Sonnet achieves 99.19% accuracy on the dataset is supported by experimental results, though the evaluation methodology lacks detail. The assertion that mT5 fine-tuning demonstrates challenges in domain adaptation is supported by quantitative results showing 74.19% accuracy.

**Low Confidence**: The effectiveness of GPT-4-assisted translation in maintaining legal terminology consistency across Urdu-English pairs is supported only by qualitative observations from random sampling rather than systematic evaluation. The assertion that article-wise chunking enables effective RAG-based legal chatbots lacks experimental validation demonstrating actual RAG system performance.

## Next Checks
1. **Systematic Translation Quality Assessment**: Conduct comprehensive automated and human evaluation of all 619 QA pairs to measure cross-lingual consistency, including metrics like BLEU, TER, and human judgment of semantic equivalence across all pairs rather than random sampling.

2. **RAG System Performance Validation**: Implement and evaluate an end-to-end RAG system using the dataset to demonstrate whether article-wise chunking actually enables effective legal information retrieval, measuring retrieval accuracy, response relevance, and answer correctness.

3. **Domain Adaptation Experiment Series**: Conduct controlled experiments comparing mT5 fine-tuning with different dataset sizes, training strategies, and regularization techniques to determine whether the 74.19% accuracy reflects fundamental model limitations or sub-optimal fine-tuning parameters.