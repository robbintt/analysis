---
ver: rpa2
title: Can We Talk Models Into Seeing the World Differently?
arxiv_id: '2403.09193'
source_url: https://arxiv.org/abs/2403.09193
tags:
- shape
- bias
- texture
- accuracy
- vision
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how visual cue biases, specifically the
  texture vs. shape bias, behave in vision-language models (VLMs) compared to vision-only
  models.
---

# Can We Talk Models Into Seeing the World Differently?

## Quick Facts
- **arXiv ID:** 2403.09193
- **Source URL:** https://arxiv.org/abs/2403.09193
- **Reference count:** 40
- **Primary result:** Vision-language models can be steered toward shape or texture-based decisions using natural language prompts without significant accuracy loss

## Executive Summary
This study investigates visual cue biases in vision-language models (VLMs) compared to vision-only models, focusing on the texture versus shape bias. Traditional vision models strongly prefer texture cues over shape information, but VLMs show reduced texture bias and greater reliance on shape, though still not reaching human-level shape bias. The researchers demonstrate that simple natural language prompts can actively steer VLMs toward either texture or shape-based decisions, with some models achieving up to a 23.3% shift in bias without significant accuracy loss. This steerability extends beyond texture/shape bias to other visual biases like frequency preferences.

## Method Summary
The researchers conducted experiments across multiple VLMs and vision-only models, testing their responses to images that contain conflicting texture and shape information. They used established datasets designed to measure texture-shape bias and systematically applied various natural language prompts to investigate whether model behavior could be steered. The study compared model performance across different prompting conditions while measuring both accuracy and bias metrics. Additional experiments tested the generalizability of prompt-based steering to other visual biases, such as frequency preferences.

## Key Results
- VLMs exhibit significantly less texture bias compared to vision-only models, showing greater reliance on shape information
- Natural language prompts can shift model bias by up to 23.3% without significant accuracy loss
- The LLM component in VLMs plays a crucial role in modulating visual cue preferences
- Prompt-based steering extends beyond texture/shape bias to frequency preference biases

## Why This Works (Mechanism)
The steerability of visual cue biases in VLMs appears to work through the interaction between the vision encoder and the language model component. When prompts are provided, they likely influence the attention mechanisms and feature weighting within the multimodal transformer architecture, effectively changing how visual features are interpreted and prioritized. The language model component appears to act as a mediator that can reinterpret visual inputs based on linguistic context, allowing for dynamic adjustment of bias preferences without architectural modifications.

## Foundational Learning
- **Texture vs. Shape Bias:** Understanding how models prioritize surface patterns versus object contours when making visual decisions - needed to establish baseline model behavior, quick check through established diagnostic datasets
- **Vision-Language Model Architecture:** Knowledge of multimodal transformers that process both visual and textual inputs - needed to understand how language can influence visual processing, quick check through component analysis
- **Visual Cue Calibration:** The concept that models can be adjusted to emphasize different types of visual information - needed to frame the steering capability, quick check through bias measurement metrics
- **Prompt Engineering:** How natural language instructions can modify model behavior - needed to implement the steering approach, quick check through controlled prompting experiments
- **Multimodal Fusion:** How vision and language representations are combined in VLMs - needed to understand bias modulation mechanisms, quick check through ablation studies
- **Frequency Bias in Vision:** How models respond to different spatial frequency components in images - needed for generalization experiments, quick check through frequency-specific image datasets

## Architecture Onboarding

### Component Map
Vision Encoder -> Multimodal Transformer -> LLM Decoder -> Output

### Critical Path
The critical path for bias steering involves: input image → vision encoder feature extraction → multimodal fusion with text prompts → attention-based feature weighting → LLM interpretation → output decision. The multimodal transformer layer is where prompt-based bias modulation occurs.

### Design Tradeoffs
The architecture trades pure visual processing accuracy for multimodal flexibility. While vision-only models achieve higher raw accuracy on texture-based tasks, VLMs sacrifice some performance for the ability to be steered via language. This represents a fundamental shift from fixed visual processing to adaptable multimodal reasoning.

### Failure Signatures
Models may show inconsistent bias shifts across different prompt formulations, exhibit accuracy degradation when pushed too far toward one bias extreme, or fail to generalize steering effects beyond the specific dataset or task type used in training. Some models may also show context-dependent responses where the same prompt produces different effects based on input image characteristics.

### 3 First Experiments
1. Test baseline texture-shape bias across multiple diagnostic datasets to establish model-specific bias profiles
2. Apply systematic variations of steering prompts to identify the most effective phrasing and structure
3. Measure accuracy trade-offs across different bias steering intensities to find optimal balance points

## Open Questions the Paper Calls Out
None

## Limitations
- Results primarily based on a limited set of pretrained models and may not generalize to all VLM architectures
- 23.3% bias shift achieved through controlled prompts that may not transfer to natural user prompts
- Binary texture-shape distinction may oversimplify complex human visual processing
- Frequency preference analysis was less extensively validated than texture-shape experiments

## Confidence
- **High** confidence that VLMs exhibit reduced texture bias compared to vision-only models
- **Medium** confidence in prompt-based steerability claims due to model and prompt variability
- **Medium** confidence that LLM component is crucial for bias modulation without ablation studies

## Next Checks
1. Test prompt-based steering approach across broader range of VLM architectures including diverse visual domains
2. Evaluate persistence of bias shifts across multiple inference sessions to assess stability
3. Conduct human studies comparing VLM outputs under different prompts to human perceptual judgments