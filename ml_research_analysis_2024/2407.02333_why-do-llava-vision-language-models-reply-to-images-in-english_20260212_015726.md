---
ver: rpa2
title: Why do LLaVA Vision-Language Models Reply to Images in English?
arxiv_id: '2407.02333'
source_url: https://arxiv.org/abs/2407.02333
tags:
- language
- image
- training
- llav
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LLaVA-style vision-language models suffer from image-induced fidelity
  loss (IFL): adding images to multilingual prompts significantly increases the likelihood
  of incorrect language responses, regardless of the query language. The problem stems
  from the language model component, not the vision encoder.'
---

# Why do LLaVA Vision-Language Models Reply to Images in English?

## Quick Facts
- arXiv ID: 2407.02333
- Source URL: https://arxiv.org/abs/2407.02333
- Reference count: 24
- Key outcome: Image-induced fidelity loss (IFL) causes LLaVA models to default to English responses when images are present, regardless of query language

## Executive Summary
LLaVA-style vision-language models exhibit a phenomenon called image-induced fidelity loss (IFL), where the presence of images significantly increases the likelihood of incorrect language responses, typically defaulting to English even when prompts are in other languages. This behavior is localized within the language model component rather than the vision encoder. The study demonstrates that switching to bilingual language backbones (Yi-6b-chat, Leo-7b-chat) reduces IFL by 7-17 percentage points, while a simple training-free mechanistic intervention steering intermediate LLM representations reduces IFL by 25-233% across pretrained models.

## Method Summary
The researchers investigated IFL through systematic experimentation with LLaVA-style models, testing different language backbones and implementing a mechanistic intervention that steers intermediate LLM representations. They evaluated performance across various language pairs, primarily focusing on English-Chinese interactions, and analyzed the spatial relationship between vision and text embeddings. The intervention approach was training-free and could be applied to pretrained models without additional fine-tuning.

## Key Results
- Bilingual language backbones (Yi-6b-chat, Leo-7b-chat) reduce IFL by 7-17 percentage points
- Vision embeddings consistently occupy a distinct space separate from text embeddings across all configurations
- Training-free mechanistic intervention reduces IFL by 25-233% across pretrained models
- IFL is localized within the LLM component, not the vision encoder

## Why This Works (Mechanism)
The mechanistic intervention works by steering intermediate LLM representations to better handle multilingual contexts when images are present. This approach appears to correct the misalignment between visual and textual processing that causes the language switching behavior, suggesting that the LLM's internal representation space needs adjustment to properly integrate visual information across different languages.

## Foundational Learning
- **Multimodal Fusion**: Understanding how vision and language models combine different modalities is crucial for diagnosing IFL
- **Language Model Architecture**: Knowledge of LLM internal representations helps explain why steering intermediate states is effective
- **Cross-Lingual Processing**: Understanding how models handle multiple languages is essential for contextualizing IFL
- **Vision Encoder Integration**: Knowing how visual features are incorporated into language models is key to understanding the separation between vision and text embeddings
- **Mechanistic Interpretability**: Techniques for analyzing and modifying internal model states enable the development of steering interventions

## Architecture Onboarding
**Component Map**: Vision Encoder -> Fusion Layer -> LLM Backbone -> Output
**Critical Path**: Image input → Vision Encoder → Visual Embeddings → Fusion with Text → LLM Processing → Language Output
**Design Tradeoffs**: The separation of vision and text embeddings may provide modularity but creates challenges for multilingual consistency
**Failure Signatures**: English default responses despite non-English prompts when images are present
**First Experiments**:
1. Test bilingual backbone performance with monolingual prompts
2. Measure embedding space distances across language conditions
3. Apply steering intervention to single-language scenarios

## Open Questions the Paper Calls Out
The paper does not explicitly call out specific open questions in the provided content.

## Limitations
- The exact mechanisms causing IFL within the LLM remain unclear
- Evaluation focuses primarily on English-Chinese language pairs, limiting generalizability
- The functional significance of vision-text embedding separation for IFL requires further theoretical grounding

## Confidence
- High Confidence: Bilingual backbone switching reduces IFL by 7-17 percentage points
- Medium Confidence: IFL stems from the language model component
- Medium Confidence: Vision embeddings occupy distinct space from text embeddings

## Next Checks
1. Test IFL phenomenon and intervention effectiveness across broader language pairs (English-Japanese, English-Arabic, English-Hindi)
2. Conduct ablation studies on steering intervention to identify critical components
3. Perform controlled experiments varying vision encoder architecture while keeping LLM constant