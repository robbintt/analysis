---
ver: rpa2
title: 'MAFT: Efficient Model-Agnostic Fairness Testing for Deep Neural Networks via
  Zero-Order Gradient Search'
arxiv_id: '2412.20086'
source_url: https://arxiv.org/abs/2412.20086
tags:
- gradient
- fairness
- maft
- testing
- eidig
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MAFT introduces a black-box individual fairness testing method
  for deep neural networks that addresses the limitations of existing approaches which
  either require white-box access or show poor performance. The core innovation is
  using zero-order gradient estimation to approximate gradients without requiring
  model access, combined with a two-phase generation approach that clusters seeds
  globally and then performs local refinement.
---

# MAFT: Efficient Model-Agnostic Fairness Testing for Deep Neural Networks via Zero-Order Gradient Search

## Quick Facts
- arXiv ID: 2412.20086
- Source URL: https://arxiv.org/abs/2412.20086
- Reference count: 40
- MAFT achieves approximately 14.69× better effectiveness and 32.58× better efficiency compared to state-of-the-art black-box fairness testing methods

## Executive Summary
MAFT introduces a black-box individual fairness testing method for deep neural networks that addresses the limitations of existing approaches which either require white-box access or show poor performance. The core innovation is using zero-order gradient estimation to approximate gradients without requiring model access, combined with a two-phase generation approach that clusters seeds globally and then performs local refinement. The method demonstrates substantial improvements over existing black-box approaches, achieving approximately 14.69× better effectiveness and 32.58× better efficiency compared to state-of-the-art methods. Experimental results on seven real-world datasets show MAFT achieves performance comparable to state-of-the-art white-box methods while maintaining model-agnostic capabilities. The approach successfully generates individual discriminatory instances that can be used to identify and mitigate bias in deep learning models.

## Method Summary
MAFT addresses black-box individual fairness testing through a novel combination of zero-order gradient estimation and two-phase instance generation. The method uses asymmetric difference quotient to estimate gradients without requiring model access, then employs a vectored approach to compute gradients for all attributes simultaneously. The two-phase generation process first clusters input data globally to discover diverse seeds, then refines these seeds locally to generate discriminatory instances. This approach balances exploration of the input space with exploitation of promising regions, enabling efficient discovery of individual discriminatory instances that reveal fairness violations in deep neural networks.

## Key Results
- MAFT generates approximately 14.69× more individual discriminatory instances than state-of-the-art black-box methods
- The approach achieves 32.58× better efficiency (generation speed) compared to existing black-box approaches
- MAFT's effectiveness approaches that of white-box methods while maintaining model-agnostic capabilities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: MAFT achieves comparable effectiveness to state-of-the-art white-box methods by using zero-order gradient estimation to approximate the gradients without requiring model access.
- Mechanism: The asymmetric difference quotient is used to estimate the gradient of a black-box model at a given input instance. By perturbing each attribute of the input with a small perturbation step size and observing the model's output, the finite difference is used to calculate the gradient in each dimension. This allows MAFT to guide attribute selection and perturbation without needing to access the model's internal structure.
- Core assumption: The estimation error is of order O(h) and is proportional to the step size h, but an accurate gradient estimate is not necessary for successful searching. Even if the initial approximations may lack accuracy, they can achieve high success rates.
- Evidence anchors:
  - [abstract]: "Our approach adopts lightweight procedures such as gradient estimation and attribute perturbation rather than non-trivial procedures like symbol execution"
  - [section 3.1.1]: "The estimation error of the asymmetric zero-order gradient on dimension i is proportional to h. This means that the error decreases linearly with the step size h."
- Break condition: If the perturbation size h is chosen too small, the perturbation may be rounded down too much by finite-precision numerical computations, leading to inaccurate gradient estimates. If h is too large, the estimated gradient may deviate significantly from the true gradient.

### Mechanism 2
- Claim: MAFT improves efficiency by using a vectored zero-order gradient algorithm that estimates the entire gradient with just one or a fixed number of forward propagation steps, regardless of the number of attributes.
- Mechanism: Instead of computing the gradient for each attribute separately, the vectored algorithm constructs an input square matrix where each row is a variant of the input vector perturbed on a different feature. By performing forward propagation once on this matrix, the gradients for all attributes can be computed simultaneously, significantly reducing the computational overhead.
- Core assumption: The gradient of the model output confidence on the correct class with respect to the input can be estimated by perturbing the entire input vector simultaneously.
- Evidence anchors:
  - [section 3.1.3]: "It enables us to estimate the entire gradient with just one or a fixed number of forward propagation, regardless of the number of attributes"
  - [section 3.1.3]: "The efficiency achieved by the vectored approach brings it close to the computational efficiency of directly utilizing the computational graph for backpropagation"
- Break condition: If the model's output confidence is not available or if the model is not differentiable, the vectored gradient estimation may not work.

### Mechanism 3
- Claim: MAFT uses a two-phase generation approach that clusters seeds globally and then performs local refinement to generate diverse and effective individual discriminatory instances.
- Mechanism: In the global generation phase, the original input data is clustered to discover diverse instances. Seeds are sampled from the clusters in a round-robin fashion, and for each seed, iterations are performed to find global discriminatory instances by perturbing the seed until a new discriminatory instance is generated or the maximum iterations are reached. In the local generation phase, the global seeds are used to quickly generate as many discriminatory instances as possible in their neighborhoods by performing slight perturbations to maintain the original predictions.
- Core assumption: Similar inputs lead to similar or the same outputs in DNNs, so by starting from seeds that are already discriminatory and performing slight perturbations, more similar discriminatory instances can be generated.
- Evidence anchors:
  - [section 3.2.2]: "Unlike the global phase, which maximizes output variation to discover potential individual discriminatory instance pairs at each iteration, the local phase focuses on changing model variation to maintain the original predictions from the model."
  - [section 3.2.2]: "In doing so, we could get more similar discriminatory instances, which is motivated by DNN robustness that similar inputs lead to similar or the same outputs"
- Break condition: If the model's decision boundary is highly irregular or if the local perturbations cause the instances to cross the decision boundary into a different class, the local generation phase may not be effective.

## Foundational Learning

- Concept: Individual fairness in machine learning
  - Why needed here: MAFT is designed to test and mitigate individual discrimination in deep neural networks, so understanding the concept of individual fairness is crucial for understanding the problem MAFT addresses.
  - Quick check question: What is the difference between individual fairness and group fairness in machine learning?

- Concept: Adversarial attacks and gradient-based optimization
  - Why needed here: MAFT uses gradient estimation and attribute perturbation techniques inspired by adversarial attacks to generate discriminatory instances. Understanding these concepts is important for understanding how MAFT works.
  - Quick check question: How do adversarial attacks leverage gradients to find inputs that cause misclassification in deep neural networks?

- Concept: Zero-order optimization and finite difference methods
  - Why needed here: MAFT uses zero-order gradient estimation, which is a type of optimization that does not require access to the gradients of the objective function. Understanding this concept is key to understanding how MAFT can work with black-box models.
  - Quick check question: What is the difference between zero-order and first-order optimization methods?

## Architecture Onboarding

- Component map:
  Vectored zero-order gradient estimation module -> Global generation phase with clustering and seed sampling -> Local generation phase with neighborhood exploration -> Individual discriminatory instance collection and output

- Critical path:
  1. Estimate gradient using vectored zero-order gradient estimation
  2. Cluster input data and sample seeds for global generation
  3. Perform global generation to find initial discriminatory instances
  4. Perform local generation to refine and expand the set of discriminatory instances
  5. Output the final set of individual discriminatory instances

- Design tradeoffs:
  - Using zero-order gradient estimation allows MAFT to work with black-box models but may be less accurate than white-box methods that can directly access gradients.
  - The two-phase generation approach balances exploration (global phase) and exploitation (local phase) but may require careful tuning of parameters like cluster count and step sizes.
  - The vectored gradient estimation improves efficiency but may be less interpretable than non-vectored methods that compute gradients for each attribute separately.

- Failure signatures:
  - If the estimated gradients are inaccurate, the generated instances may not be truly discriminatory or may not effectively test the model's fairness.
  - If the clustering in the global phase is poor, the seeds may not be diverse enough, leading to a less comprehensive set of discriminatory instances.
  - If the local perturbations are too large, the generated instances may cross the decision boundary into a different class, reducing their effectiveness as fairness tests.

- First 3 experiments:
  1. Test the accuracy of the vectored zero-order gradient estimation by comparing it to gradients computed using backpropagation on a white-box model.
  2. Evaluate the effectiveness of the two-phase generation approach by comparing the number and diversity of discriminatory instances generated by MAFT to those generated by a baseline method that does not use clustering or local refinement.
  3. Measure the efficiency of MAFT by comparing its runtime and memory usage to that of white-box methods that can directly access model gradients.

## Open Questions the Paper Calls Out
None

## Limitations
- The paper does not address how the method performs on more complex models beyond 6-layer fully connected networks, limiting generalizability to modern architectures like transformers or convolutional networks
- While MAFT shows improved efficiency over black-box methods, the comparison with white-box methods on efficiency metrics is less clear, as the paper focuses primarily on effectiveness improvements
- The method requires careful tuning of multiple hyperparameters (perturbation size ℎ, clustering parameters, momentum factors) which could affect both performance and reproducibility

## Confidence
- **High Confidence**: The zero-order gradient estimation mechanism and its theoretical foundation are well-established and clearly explained
- **Medium Confidence**: The empirical improvements over baseline methods are demonstrated but limited to specific datasets and model architectures
- **Medium Confidence**: The efficiency claims are supported by experimental results but lack comparison to white-box efficiency benchmarks

## Next Checks
1. **Cross-architecture validation**: Test MAFT on diverse model architectures including CNNs and transformers to assess generalizability beyond fully connected networks
2. **White-box efficiency comparison**: Conduct direct runtime and memory usage comparisons between MAFT and white-box methods on identical tasks
3. **Hyperparameter sensitivity analysis**: Systematically evaluate how variations in ℎ, clustering parameters, and momentum factors affect both effectiveness and efficiency across different datasets