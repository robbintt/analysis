---
ver: rpa2
title: Trained quantum neural networks are Gaussian processes
arxiv_id: '2402.08726'
source_url: https://arxiv.org/abs/2402.08726
tags:
- theorem
- page
- which
- will
- function
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies quantum neural networks in the infinite-width
  limit, where the function generated is the expectation value of the sum of single-qubit
  observables over all qubits. The authors prove that the probability distribution
  of the function generated by the untrained network with randomly initialized parameters
  converges to a Gaussian process, provided each measured qubit is correlated only
  with a few other measured qubits.
---

# Trained quantum neural networks are Gaussian processes

## Quick Facts
- arXiv ID: 2402.08726
- Source URL: https://arxiv.org/abs/2402.08726
- Reference count: 40
- This paper proves that trained quantum neural networks in the infinite-width limit converge to Gaussian processes and can be trained in polynomial time

## Executive Summary
This paper establishes a theoretical framework for understanding quantum neural networks in the infinite-width limit, demonstrating that both untrained and trained networks converge to Gaussian processes. The authors prove that randomly initialized quantum neural networks generate functions whose probability distributions converge to Gaussian processes under certain correlation constraints between measured qubits. They extend this analysis to trained networks, showing that gradient descent with square loss can perfectly fit training data while maintaining the Gaussian process structure, provided the network avoids barren plateaus. The work also addresses measurement noise, proving that polynomially many measurements suffice for these results to hold.

## Method Summary
The authors employ a rigorous mathematical analysis of quantum neural networks operating in the infinite-width limit, where each measured qubit's expectation value contributes to the overall function. They use techniques from statistical learning theory and random matrix theory to characterize the distribution of functions generated by these networks. The analysis tracks how this distribution evolves during training via gradient descent with square loss, proving convergence properties and training feasibility. The work also incorporates measurement noise considerations, deriving bounds on the number of measurements needed for accurate function estimation.

## Key Results
- Untrained quantum neural networks with randomly initialized parameters converge to Gaussian processes when each measured qubit correlates with only a few other qubits
- Trained networks can perfectly fit training data while maintaining Gaussian process structure, assuming no barren plateaus
- Polynomial measurement requirements suffice for all theoretical results to hold, enabling polynomial-time training

## Why This Works (Mechanism)
The infinite-width limit causes quantum neural networks to behave like Gaussian processes due to the central limit theorem applied to the sum of many independent qubit measurements. When each measured qubit correlates with only a few others, the dependencies between measurements remain weak enough that the overall function distribution approaches a Gaussian. During training, gradient descent with square loss modifies the network parameters in a way that preserves this Gaussian structure while adapting to fit the training data. The absence of barren plateaus ensures that gradients remain informative, enabling efficient parameter updates. Measurement noise introduces uncertainty that can be quantified and bounded, with polynomial measurement requirements ensuring statistical convergence.

## Foundational Learning
- **Infinite-width limit**: When quantum circuits have infinitely many qubits, their behavior converges to a Gaussian process due to central limit theorem effects
  - Why needed: Enables tractable mathematical analysis of quantum neural networks
  - Quick check: Verify that increasing circuit width leads to Gaussian-distributed outputs

- **Barren plateaus**: Regions in parameter space where gradients vanish exponentially, preventing efficient training
  - Why needed: Critical condition for proving polynomial training time
  - Quick check: Monitor gradient magnitudes during training to detect barren plateau emergence

- **Gaussian processes**: Probability distributions over functions where any finite collection of function values has a multivariate Gaussian distribution
  - Why needed: Provides the theoretical foundation for characterizing network behavior
  - Quick check: Test if finite-width networks' outputs follow Gaussian statistics

## Architecture Onboarding

**Component map**: Input qubits -> Parameterized gates -> Measurement operators -> Expectation values -> Loss function -> Gradient descent

**Critical path**: Parameter initialization → Circuit execution → Measurement → Expectation value calculation → Loss computation → Gradient calculation → Parameter update

**Design tradeoffs**: Width vs. trainability (wider networks approach Gaussian processes but may require more resources); measurement noise vs. accuracy (more measurements improve estimates but increase resource requirements)

**Failure signatures**: Exponentially vanishing gradients indicate barren plateaus; non-Gaussian output distributions suggest finite-width effects; poor training performance may indicate barren plateaus or insufficient measurements

**Three first experiments**:
1. Verify Gaussian distribution of outputs for randomly initialized circuits of increasing width
2. Test training convergence on simple supervised learning tasks with varying circuit widths
3. Measure the relationship between number of measurements and training accuracy

## Open Questions the Paper Calls Out
None

## Limitations
- Assumes each measured qubit correlates with only a few others, which may not hold for many practical architectures
- Relies on the absence of barren plateaus, a condition that may be violated in certain configurations
- Assumes idealized gradient descent and specific loss landscapes that may not generalize to all quantum learning tasks

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| Convergence to Gaussian processes | High |
| Polynomial measurement requirements | Medium |
| Polynomial training time | Medium |

## Next Checks
1. Test the Gaussian process approximation on finite-width quantum circuits with varying depths and architectures to determine when the infinite-width limit breaks down
2. Empirically verify the polynomial measurement requirement by comparing theoretical predictions with actual quantum hardware performance
3. Evaluate the training time predictions on quantum hardware, particularly for circuits where barren plateau conditions may be violated