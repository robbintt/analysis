---
ver: rpa2
title: Data Poisoning Attacks on Off-Policy Policy Evaluation Methods
arxiv_id: '2404.04714'
source_url: https://arxiv.org/abs/2404.04714
tags:
- attack
- value
- data
- policy
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies data poisoning attacks on off-policy evaluation
  (OPE) methods. The authors propose a novel framework called DOPE to construct adversarial
  perturbations that maximize error in policy value estimates.
---

# Data Poisoning Attacks on Off-Policy Policy Evaluation Methods

## Quick Facts
- **arXiv ID**: 2404.04714
- **Source URL**: https://arxiv.org/abs/2404.04714
- **Reference count**: 12
- **Key outcome**: The DOPE framework can corrupt 3-5% of data to cause over 340% error in HIV and 100% error in MountainCar domains.

## Executive Summary
This paper introduces DOPE (Data POisoning Attacks on Policy Evaluation), a novel framework for constructing adversarial perturbations that maximize error in off-policy evaluation (OPE) methods. The framework leverages influence functions from robust statistics to identify the most influential data points for perturbation. Through extensive experiments on five datasets and five popular OPE methods, the authors demonstrate that DOPE significantly outperforms random and FGSM-based attack baselines, with particularly high effectiveness against BRM, PDIS, and WDR methods. The findings raise serious concerns about the reliability of OPE methods in real-world applications.

## Method Summary
The DOPE framework operates by first computing influence scores for each data point using influence functions from robust statistics. These scores identify which data points most strongly affect the OPE estimate. The framework then optimizes perturbations within a bounded budget to maximize the error in the policy value estimate. The approach is evaluated across five datasets (Cancer, HIV, Mountain Car, Cartpole, Continuous Gridworld) and five OPE methods (BRM, WIS, PDIS, CPDIS, WDR). Evaluation policies are trained using Deep Q-learning, while behavior policies are estimated via multinomial logistic regression. The framework's effectiveness is measured by percentage error in policy value estimates and compared against baseline attacks using random perturbations and FGSM-based methods.

## Key Results
- Corrupting only 3-5% of data causes over 340% error in HIV domain and 100% error in MountainCar domain
- DOPE significantly outperforms random and FGSM-based attack baselines across all tested OPE methods
- Attack effectiveness is particularly high against BRM, PDIS, and WDR methods
- Influence function-based targeting is crucial for attack success, as random perturbations show much lower effectiveness

## Why This Works (Mechanism)
The DOPE framework works by exploiting the sensitivity of OPE methods to specific data points. Influence functions provide a principled way to identify which data points have the strongest impact on the policy value estimate. By targeting these influential points with carefully optimized perturbations within a bounded budget, the attack can maximize the error in the OPE estimate while minimizing the amount of data that needs to be corrupted. The framework's effectiveness stems from its ability to efficiently navigate the non-convex optimization landscape of the OPE error with respect to the perturbations.

## Foundational Learning
1. **Off-Policy Evaluation (OPE)**: Methods for estimating policy values using data from a different policy than the one being evaluated. Needed because evaluating policies in real environments can be costly or dangerous.
2. **Influence Functions**: Statistical tools for measuring the sensitivity of model parameters to individual data points. Quick check: Verify that influence scores correctly identify data points whose removal significantly changes the OPE estimate.
3. **Data Poisoning Attacks**: Adversarial attacks where an attacker corrupts training data to manipulate model behavior. Quick check: Confirm that perturbations remain within the specified budget constraint.
4. **Policy Gradient Methods**: Reinforcement learning techniques for optimizing policies. Quick check: Ensure evaluation policies are properly trained and stable.
5. **Bootstrapping**: Statistical technique for estimating uncertainty by resampling with replacement. Quick check: Verify that confidence intervals capture the true variability in percentage error estimates.
6. **Function Approximation**: Using parameterized functions (linear or neural networks) to approximate value functions in RL. Quick check: Confirm that approximation error is reasonable and doesn't dominate the analysis.

## Architecture Onboarding

**Component Map**: Deep Q-learning -> Evaluation Policy -> Dataset Generation -> OPE Methods -> DOPE Attack Framework -> Error Metrics

**Critical Path**: The attack proceeds by first computing influence scores for all data points, then optimizing perturbations within the budget constraint to maximize the percentage error in the OPE estimate. The influence scores guide the perturbation optimization, while the budget constraint ensures the attack remains stealthy.

**Design Tradeoffs**: The linear function approximation assumption simplifies the influence computation but may limit generalizability to more complex settings. The perturbation budget controls attack stealthiness versus effectiveness. The corruption percentage balances attack success against detection risk.

**Failure Signatures**: High false positive rates in anomaly detection methods when identifying poisoned data points. Non-monotonic trends in percentage error with respect to perturbation budget due to non-convex loss functions. Degradation of attack effectiveness when influence function approximations are noisy or biased.

**First 3 Experiments**:
1. Verify that influence scores correctly identify the most influential data points by comparing OPE estimates with and without these points.
2. Test the sensitivity of attack effectiveness to the perturbation budget by varying Îµ and measuring percentage error.
3. Evaluate the effectiveness of anomaly detection methods (Isolation Forests, Local Outlier Factor) in identifying poisoned data points.

## Open Questions the Paper Calls Out
None

## Limitations
- Attack effectiveness heavily depends on access to accurate influence function estimates, which may degrade when approximations are noisy or biased
- Linear function approximation assumption may not hold in complex real-world scenarios, potentially limiting generalizability
- Study focuses primarily on reward manipulation attacks, with state-space perturbations only briefly explored

## Confidence

**High confidence** in the core theoretical framework and methodology
**Medium confidence** in the quantitative results due to potential sensitivity to hyperparameter choices and implementation details not fully specified
**Low confidence** in the generalizability of attack effectiveness to more complex function approximators and state-space manipulations

## Next Checks

1. Test DOPE attack effectiveness with non-linear function approximators (e.g., neural networks) to assess robustness beyond linear settings
2. Conduct ablation studies varying the corruption percentage threshold to identify critical tipping points in attack success rates
3. Implement and evaluate the proposed anomaly detection methods on the poisoned datasets to verify their effectiveness in identifying adversarial manipulations