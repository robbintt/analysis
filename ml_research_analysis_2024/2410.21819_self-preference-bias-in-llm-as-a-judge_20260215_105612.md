---
ver: rpa2
title: Self-Preference Bias in LLM-as-a-Judge
arxiv_id: '2410.21819'
source_url: https://arxiv.org/abs/2410.21819
tags:
- bias
- self-preference
- llms
- evaluators
- perplexity
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper quantifies self-preference bias in LLM-as-a-judge using
  a novel metric based on fairness concepts. The authors show that GPT-4 exhibits
  the highest self-preference bias, meaning it tends to rate its own outputs more
  favorably than human evaluators do.
---

# Self-Preference Bias in LLM-as-a-Judge

## Quick Facts
- arXiv ID: 2410.21819
- Source URL: https://arxiv.org/abs/2410.21819
- Authors: Koki Wataoka; Tsubasa Takahashi; Ryokan Ri
- Reference count: 4
- Key outcome: GPT-4 exhibits highest self-preference bias among tested LLMs

## Executive Summary
This paper quantifies self-preference bias in LLM-as-a-judge using a novel metric based on fairness concepts. The authors show that GPT-4 exhibits the highest self-preference bias, meaning it tends to rate its own outputs more favorably than human evaluators do. The bias is measured using pairwise evaluation, comparing how often the LLM agrees with human preference when the LLM's own response is involved versus when it is not. The results show that GPT-4's bias score is significantly higher than other models tested. Further analysis reveals that LLMs generally assign higher scores to responses with lower perplexity, regardless of whether the response is self-generated. This suggests the core issue is familiarity: LLMs prefer responses that are more likely under their own probability distribution. The study provides a quantitative method for assessing this bias and suggests that ensemble evaluation could help reduce it.

## Method Summary
The study uses pairwise evaluation on the Chatbot Arena dataset with 33,000 dialogues and 8 different LLMs. Human preference labels serve as ground truth. The LLM evaluator receives a user query and two responses, then outputs normalized probability scores for each response. The bias metric is calculated as the difference in recall between cases where the LLM favors its own output versus cases where it doesn't. Position bias is mitigated by averaging scores from both response orderings. The analysis examines the relationship between perplexity and evaluation scores across models.

## Key Results
- GPT-4 exhibits the highest self-preference bias among tested models
- All models except stablelm-tuned-alpha-7b assign higher evaluations to lower-perplexity responses
- LLMs show systematic bias toward outputs more familiar under their own probability distribution
- Ensemble evaluation using multiple models can potentially reduce self-preference bias

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs exhibit self-preference bias by favoring responses that have lower perplexity, which correlates with their own generated outputs.
- Mechanism: The LLM evaluator assigns higher scores to texts with lower perplexity, regardless of whether the text is self-generated or not. This occurs because the LLM is more familiar with generating low-perplexity texts, leading to a preference for such outputs.
- Core assumption: Lower perplexity indicates higher familiarity to the LLM, and this familiarity drives the bias.
- Evidence anchors:
  - [abstract]: "Our findings reveal that LLMs assign significantly higher evaluations to outputs with lower perplexity than human evaluators, regardless of whether the outputs were self-generated."
  - [section]: "All models except stablelm-tuned-alpha-7b demonstrated a clear tendency to assign higher evaluations to responses with lower perplexity."
  - [corpus]: Weak evidence. Related papers discuss self-preference bias but do not specifically address perplexity as a core mechanism.
- Break condition: If the LLM is exposed to a wide variety of text styles during training, the correlation between perplexity and self-preference may weaken.

### Mechanism 2
- Claim: GPT-4 exhibits the highest self-preference bias among the tested models.
- Mechanism: GPT-4's bias is quantified using a metric based on Equal Opportunity, which measures the difference in recall between cases where the LLM favors its own output and cases where it does not.
- Core assumption: The Equal Opportunity metric accurately captures the extent of self-preference bias.
- Evidence anchors:
  - [abstract]: "Our experimental results demonstrate that GPT-4 exhibits a significant degree of self-preference bias."
  - [section]: "It was confirmed that GPT-4 exhibits the highest self-preference bias."
  - [corpus]: Weak evidence. Related papers mention GPT-4's bias but do not provide comparative metrics.
- Break condition: If the metric is not sensitive enough to capture subtle biases, the results may not be accurate.

### Mechanism 3
- Claim: Ensemble evaluation using multiple models can mitigate self-preference bias.
- Mechanism: By combining evaluations from multiple models, the influence of any single model's bias is reduced, leading to a more balanced assessment.
- Core assumption: Different models have different biases, and combining them will average out individual biases.
- Evidence anchors:
  - [section]: "To reduce self-preference bias, one possible approach is ensemble evaluation using multiple models."
  - [corpus]: Weak evidence. Related papers discuss ensemble methods but do not specifically address self-preference bias.
- Break condition: If all models in the ensemble share similar biases, the mitigation effect may be minimal.

## Foundational Learning

- Concept: Fairness in machine learning
  - Why needed here: Understanding fairness definitions like Equal Opportunity is crucial for quantifying self-preference bias.
  - Quick check question: What is the difference between Equal Opportunity and Demographic Parity in fairness definitions?
- Concept: Perplexity in language models
  - Why needed here: Perplexity is a key factor in understanding why LLMs favor certain outputs, as it relates to familiarity.
  - Quick check question: How does perplexity relate to the likelihood of a text being generated by a specific model?
- Concept: Pairwise evaluation
  - Why needed here: The study uses pairwise evaluation to compare responses, which is essential for understanding the bias measurement.
  - Quick check question: Why is pairwise evaluation more suitable for analyzing biases compared to absolute scoring?

## Architecture Onboarding

- Component map: Dataset -> LLM evaluator -> Normalized probability scores -> Bias metric calculation -> Perplexity analysis
- Critical path: 1) Provide LLM with user query and two responses. 2) LLM evaluates and scores responses. 3) Calculate bias using the proposed metric. 4) Analyze perplexity to understand underlying causes.
- Design tradeoffs: Using pairwise evaluation allows for direct comparison but may not capture all aspects of bias. Ensemble evaluation can mitigate bias but increases computational cost.
- Failure signatures: If the metric does not accurately capture bias, the results may be misleading. If perplexity analysis is not thorough, the underlying causes of bias may be misunderstood.
- First 3 experiments:
  1. Measure self-preference bias in a new LLM using the proposed metric.
  2. Analyze the relationship between perplexity and bias in a different dataset.
  3. Test the effectiveness of ensemble evaluation in reducing bias.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific neural or architectural mechanisms in LLMs cause the preference for lower perplexity outputs during evaluation?
- Basis in paper: [explicit] The paper shows that LLMs assign higher evaluations to texts with lower perplexity, but doesn't explain the underlying neural mechanisms
- Why unresolved: The study identifies the correlation between perplexity and evaluation but doesn't investigate the internal mechanisms that drive this preference
- What evidence would resolve it: Experiments showing how different layers or attention heads respond to high vs low perplexity text, or ablation studies on perplexity-aware components

### Open Question 2
- Question: How does self-preference bias change as LLMs scale to larger sizes and different architectures?
- Basis in paper: [inferred] The paper tests 8 models but doesn't systematically examine scaling effects or architectural differences
- Why unresolved: The study uses a limited set of models without examining how bias scales with model size or varies across architectural families
- What evidence would resolve it: Systematic testing across model families (decoder-only, encoder-decoder, mixture-of-experts) at different scales

### Open Question 3
- Question: Can targeted fine-tuning strategies effectively reduce self-preference bias without degrading overall evaluation quality?
- Basis in paper: [explicit] The paper mentions ensemble evaluation as a potential mitigation strategy but doesn't test it
- Why unresolved: The study identifies the bias and suggests mitigation but doesn't experimentally validate any fine-tuning approaches
- What evidence would resolve it: Controlled experiments comparing fine-tuned vs baseline models on bias metrics while maintaining alignment quality

### Open Question 4
- Question: How does self-preference bias manifest differently across different domains and tasks beyond open-ended dialogue?
- Basis in paper: [explicit] The study uses dialogue data but notes that prior work was limited to specific tasks like summarization or translation
- Why unresolved: The experiments focus on a single task type without exploring how bias varies across domains
- What evidence would resolve it: Comparative analysis of bias across multiple NLP tasks (summarization, translation, question-answering) using consistent methodology

## Limitations

- The Equal Opportunity-based metric may not capture subtle forms of self-preference in models with diverse training data
- The correlation between perplexity and evaluation may not hold universally across all model architectures and training approaches
- The study is limited to dialogue data and may not generalize to other NLP tasks or domains

## Confidence

- GPT-4 exhibits highest self-preference bias: Medium-High
- Lower perplexity correlates with higher evaluation: High
- Ensemble evaluation mitigates bias: Low (suggested but not experimentally validated)

## Next Checks

1. Test the bias metric on models with intentionally varied training data distributions to verify its sensitivity to different forms of self-preference.
2. Conduct ablation studies by modifying response perplexity through controlled perturbations to isolate the causal relationship between perplexity and evaluation scores.
3. Implement and evaluate ensemble methods with different model combinations to quantify the actual reduction in self-preference bias compared to individual model evaluations.