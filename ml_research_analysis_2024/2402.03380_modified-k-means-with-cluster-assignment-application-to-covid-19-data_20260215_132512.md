---
ver: rpa2
title: Modified K-means with Cluster Assignment -- Application to COVID-19 Data
arxiv_id: '2402.03380'
source_url: https://arxiv.org/abs/2402.03380
tags:
- cluster
- information
- clustering
- clusters
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a modified K-means clustering approach for
  extracting text information from large COVID-19 research corpora. The method incorporates
  keyword weighting, two-cluster assignment, and vector averaging damping to improve
  semantic relevance and cluster separation.
---

# Modified K-means with Cluster Assignment -- Application to COVID-19 Data

## Quick Facts
- arXiv ID: 2402.03380
- Source URL: https://arxiv.org/abs/2402.03380
- Reference count: 17
- One-line primary result: Modified K-means with keyword weighting and two-cluster assignment yields more targeted, topic-specific COVID-19 text clusters than standard K-means.

## Executive Summary
This paper introduces a modified K-means clustering approach for extracting semantically relevant text from large COVID-19 research corpora. The method enhances standard K-means by incorporating keyword weighting, two-cluster assignment for overlapping topics, and vector averaging damping to improve cluster separation and relevance. Preprocessing includes chunking, POS-based filtering, and TF-IDF vectorization, followed by dimensionality reduction for computational efficiency. Experiments on three COVID-19 datasets show the approach retrieves more targeted, topic-specific paragraphs compared to baseline search or standard K-means.

## Method Summary
The method preprocesses COVID-19 research papers by chunking into 2-3 sentence groups, applying POS filtering, and generating TF-IDF vectors. Dimensionality reduction (PCA) is applied for computational efficiency. Modified K-means clustering uses keyword-weighted vectors, two-cluster assignment (if distances are within a threshold), and vector averaging damping during centroid updates. The approach is evaluated by querying specific keywords and measuring the number of topically relevant paragraphs retrieved compared to baseline methods.

## Key Results
- Modified K-means retrieves more topic-specific paragraphs than standard search or baseline K-means for queries like "Vaccine", "Transmission", "Incubation", and "Surveillance".
- Two-cluster assignment captures cross-topic sentences, enhancing context and semantic coverage.
- Vector averaging damping stabilizes centroid updates, improving convergence and cluster separation.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The modified K-means with keyword weighting improves semantic clustering by giving explicit importance to user-specified terms during centroid updates.
- Mechanism: Each document chunk is assigned a weight vector based on the presence and significance of a search keyword. During cluster centroid calculation, weighted chunks pull the centroid more strongly than unweighted ones, biasing clusters toward semantically relevant documents.
- Core assumption: Document chunks can be reliably weighted by keyword presence without over-distorting the global semantic structure.
- Evidence anchors:
  - [abstract] "By utilizing new technique called two cluster assignment technique with K-means model, we improved the ontology of the retrieved text."
  - [section] "The major one being giving explicit weights to each tf-idf vector [15]."
  - [corpus] Weak evidence; corpus neighbors discuss clustering and assignment but do not directly support the weighting claim.
- Break condition: If keyword weighting is too extreme, clusters may collapse around a single document, losing diversity.

### Mechanism 2
- Claim: Two-cluster assignment allows a single chunk to be assigned to two clusters if distances are below a small threshold, improving cross-topic coverage.
- Mechanism: During assignment, if the difference between the closest and second-closest cluster distances is less than a small threshold (e.g., 0.01), the chunk is added to both clusters. This captures documents relevant to multiple themes.
- Core assumption: Documents relevant to multiple topics will be close in feature space to multiple centroids.
- Evidence anchors:
  - [abstract] "By utilizing new technique called two cluster assignment technique with K-means model..."
  - [section] "The motivation behind this technique was the inclusion of chunks under more than one topic or cluster."
  - [corpus] No direct evidence in neighbors about multi-assignment clustering.
- Break condition: If the threshold is too high, all documents may be assigned to all clusters, collapsing distinctions.

### Mechanism 3
- Claim: Vector average damping with a small damping factor preserves stability during centroid updates and prevents drastic centroid movement.
- Mechanism: When updating cluster centroids, the new centroid is calculated as a weighted average of current centroid and weighted chunk sum, with a small damping weight (e.g., 0.01) given to the current centroid to stabilize movement.
- Core assumption: Damping reduces the risk of centroid oscillation and improves convergence.
- Evidence anchors:
  - [abstract] "We further apply the vector average damping technique for flexible movement of clusters."
  - [section] "Another change that we have incorporated is the usage of centroid damping while calculating the mean of cluster centroids."
  - [corpus] No direct evidence in neighbors about damping in clustering.
- Break condition: If damping is too large, centroids may not move enough to capture new structure, slowing convergence.

## Foundational Learning

- Concept: Term Frequency-Inverse Document Frequency (TF-IDF)
  - Why needed here: TF-IDF vectorization provides a normalized representation of document chunks, reducing the influence of frequent but non-informative terms and emphasizing rare but significant words.
  - Quick check question: What is the primary difference between TF-IDF and simple term frequency in the context of document clustering?

- Concept: Dimensionality Reduction (e.g., PCA)
  - Why needed here: High-dimensional TF-IDF vectors can cause computational inefficiency and redundancy; PCA reduces dimensions while preserving variance, enabling faster clustering and visualization.
  - Quick check question: Why is PCA preferred over LDA for clustering in this context?

- Concept: K-means Clustering Algorithm
  - Why needed here: Standard K-means is the foundation for the modified algorithm; understanding its mechanics (centroid initialization, assignment, update) is crucial for grasping the enhancements.
  - Quick check question: How does random centroid initialization affect the stability of K-means results?

## Architecture Onboarding

- Component map: Data ingestion → Preprocessing (chunking, tokenization, stop-word removal, POS filtering) → TF-IDF vectorization → Dimensionality reduction (PCA) → Modified K-means (weighted assignment, two-cluster assignment, damping) → Cluster evaluation and output.
- Critical path: The most sensitive sequence is preprocessing → TF-IDF → modified K-means, as errors here directly affect cluster quality.
- Design tradeoffs: Keyword weighting increases semantic relevance but may bias clusters; two-cluster assignment improves coverage but can reduce cluster purity; damping stabilizes convergence but may slow adaptation.
- Failure signatures: Poor cluster separation (overlapping clusters), too few or too many relevant documents per cluster, long convergence times, or missing cross-topic documents.
- First 3 experiments:
  1. Run standard K-means on a small COVID-19 subset with keyword "Vaccine" and compare cluster purity to modified K-means.
  2. Test different damping factors (e.g., 0.001, 0.01, 0.1) and measure convergence speed and cluster stability.
  3. Vary the two-cluster assignment threshold (e.g., 0.001, 0.01, 0.05) and evaluate how many documents are assigned to multiple clusters and whether this improves semantic coverage.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the optimal values for the weight assigned to non-relevant chunks (e.g., 0.01 in the paper) and how do these values vary across different datasets and contexts?
- Basis in paper: [explicit] The paper mentions that the weight for non-related chunks was set to 0.01 and states that this may vary depending upon the nonzero weights of the other chunks.
- Why unresolved: The paper does not provide a systematic method for determining the optimal weight values and suggests that the choice may be empirical.
- What evidence would resolve it: Experiments comparing clustering results with different weight values across multiple datasets and contexts to identify optimal weight settings.

### Open Question 2
- Question: How does the performance of the modified K-means algorithm compare to other advanced text clustering methods, such as Latent Dirichlet Allocation (LDA) or hierarchical clustering, in terms of both accuracy and computational efficiency?
- Basis in paper: [inferred] The paper claims that the modified K-means approach yields more targeted, topic-specific results compared to standard search or baseline K-means, but does not compare its performance to other advanced clustering methods.
- Why unresolved: The paper only compares the modified K-means to standard search and baseline K-means, leaving a gap in understanding how it performs relative to other advanced methods.
- What evidence would resolve it: Comparative studies evaluating the modified K-means against LDA, hierarchical clustering, and other advanced methods on the same datasets and metrics.

### Open Question 3
- Question: What is the impact of different dimensionality reduction techniques (e.g., PCA, LDA, autoencoders) on the clustering results, and how does the choice of technique affect the semantic relevance and computational efficiency of the clusters?
- Basis in paper: [explicit] The paper mentions the use of PCA for dimensionality reduction but does not explore the impact of other techniques like LDA or autoencoders.
- Why unresolved: The paper only uses PCA for dimensionality reduction and does not investigate how other techniques might affect clustering outcomes.
- What evidence would resolve it: Experiments applying different dimensionality reduction techniques to the same datasets and comparing clustering results in terms of semantic relevance and computational efficiency.

## Limitations
- The effectiveness of keyword weighting relies on the assumption that keyword presence directly correlates with semantic relevance, but the paper does not validate this assumption or provide sensitivity analysis for weight values.
- The two-cluster assignment mechanism lacks theoretical grounding and may blur cluster boundaries, reducing interpretability without rigorous justification.
- Evaluation is limited to counting retrieved paragraphs without qualitative assessment of cluster coherence or comparison to state-of-the-art topic modeling approaches like LDA.

## Confidence
- High confidence: The core workflow (chunking → TF-IDF → PCA → K-means) is standard and well-justified.
- Medium confidence: The claim that modified K-means yields "more targeted, topic-specific results" is supported by keyword query counts but lacks rigorous statistical validation or qualitative analysis.
- Low confidence: Claims about "enhanced context" and "semantically richer" clusters are not substantiated with concrete examples or user studies.

## Next Checks
1. Conduct a qualitative review of top-10 paragraphs per cluster for "Vaccine" and "Transmission" to assess semantic coherence and topical relevance.
2. Compare cluster purity and F1 scores against standard K-means and LDA on the same COVID-19 datasets using a held-out labeled subset.
3. Perform ablation studies removing keyword weighting, two-cluster assignment, and damping to quantify each component's contribution to final cluster quality.