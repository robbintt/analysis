---
ver: rpa2
title: 'RECIPE4U: Student-ChatGPT Interaction Dataset in EFL Writing Education'
arxiv_id: '2403.08272'
source_url: https://arxiv.org/abs/2403.08272
tags:
- students
- chatgpt
- essay
- request
- recipe4u
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: RECIPE4U is a student-ChatGPT interaction dataset for EFL writing
  education, containing 4,330 utterances from 212 students. The dataset includes conversation
  logs, student intent (annotated with 13 labels), satisfaction ratings, and essay
  edit histories.
---

# RECIPE4U: Student-ChatGPT Interaction Dataset in EFL Writing Education

## Quick Facts
- arXiv ID: 2403.08272
- Source URL: https://arxiv.org/abs/2403.08272
- Reference count: 0
- 4,330 utterances from 212 students in EFL writing education context

## Executive Summary
RECIPE4U is a student-ChatGPT interaction dataset designed for English as a Foreign Language (EFL) writing education. The dataset contains conversation logs, student intent annotations, satisfaction ratings, and essay edit histories collected over a semester. It enables research on student-AI interactions in educational contexts and provides baseline models for intent detection and satisfaction estimation tasks. The dataset reveals students perceive ChatGPT as human-like and intelligent, offering insights into how students engage with AI writing assistants.

## Method Summary
The dataset was collected through a dedicated platform (RECIPE) where 212 students interacted with ChatGPT to revise their essays over a semester. Interactions were logged along with essay edit histories and student satisfaction ratings. Student utterances were annotated with 13 intent categories. Baseline models were established using fine-tuned multilingual BERT and XLM-R for intent detection and satisfaction estimation, along with GPT-3.5-turbo-16k and GPT-4 for comparison.

## Key Results
- Intent detection F1 scores: 0.83-0.43 across different models and settings
- Satisfaction estimation F1 scores: 0.91-0.58 across different models and settings
- Students frequently anthropomorphize ChatGPT, treating it as a human-like peer with personality
- Code-switching between English and Korean is common in student prompts

## Why This Works (Mechanism)

### Mechanism 1
The dataset captures authentic student-AI interaction patterns in EFL writing contexts by collecting semester-long interaction data from real students using a dedicated platform, preserving natural dialogue flows, essay edits, and satisfaction ratings that reflect genuine usage rather than artificial lab scenarios. Students interact with ChatGPT as if it were a human-like peer, not just a tool, leading to more natural and varied utterances. Break condition: If students become overly cautious or self-censoring due to awareness of data collection, the naturalness of interactions would degrade.

### Mechanism 2
Intent detection and satisfaction estimation tasks are feasible and useful for understanding student needs through labeled student utterances (13 intent categories) and self-rated satisfaction scores, enabling supervised learning baselines that can guide future AI tutoring system improvements. Students' satisfaction ratings and intent labels are consistent and meaningful indicators of interaction quality and learning needs. Break condition: If student satisfaction ratings are inconsistent or biased, the satisfaction estimation model performance would degrade significantly.

### Mechanism 3
Essay edit histories provide insights into student learning and AI feedback effectiveness by linking each utterance to specific essay edits, enabling analysis of how students incorporate AI suggestions, revealing patterns of acceptance, rejection, and learning progress. Students' essay edits directly reflect their reception and understanding of AI feedback. Break condition: If students edit essays for reasons unrelated to AI feedback (e.g., instructor comments), the correlation between edits and AI suggestions would weaken.

## Foundational Learning

- Task-oriented dialogue systems: Needed to understand how RECIPE4U is framed as a task-oriented dialogue dataset for educational contexts, requiring understanding of how such systems classify intents and estimate user satisfaction. Quick check: What are the key differences between task-oriented and open-domain dialogue datasets?

- Code-switching in EFL contexts: Needed because the dataset includes code-switched utterances between English and Korean, which affects model design and annotation schemes. Quick check: How does code-switching impact multilingual NLP model performance and annotation consistency?

- Essay scoring rubrics in EFL education: Needed because the dataset evaluation includes standard EFL essay scoring dimensions (content, organization, language), which contextualize the interaction data. Quick check: What are the typical criteria used in EFL essay scoring rubrics?

## Architecture Onboarding

- Component map: Data collection platform (RECIPE) -> Dialogue logs + Intent annotations + Satisfaction ratings + Essay edit histories -> NLP models (BERT/XLM-R/gpt-3.5-turbo-16k/gpt-4) -> Intent detection and Satisfaction estimation -> Analysis pipeline (Dialogue pattern analysis + Essay statistics + Edit pattern analysis)

- Critical path: Data collection -> Annotation -> Model training -> Baseline establishment -> Analysis

- Design tradeoffs: Multilingual support vs. model complexity (using M-BERT and XLM-R); Raw vs. masked essay content in input (masking improves performance by focusing on core utterances); Temperature settings in few-shot prompting (0.2 temperature with zero-shot gives best results)

- Failure signatures: Low intent detection F1 (<0.5) indicates poor annotation quality or overly fine-grained categories; High variance in satisfaction estimation suggests inconsistent student self-ratings; Code-switching not handled well by models indicates need for better multilingual preprocessing

- First 3 experiments: 1) Fine-tune M-BERT on intent detection with essay masking to see if performance improves over raw input; 2) Test few-shot prompting with gpt-4 at different temperatures for satisfaction estimation; 3) Analyze correlation between intent categories and satisfaction scores to identify which student needs are best met by ChatGPT

## Open Questions the Paper Calls Out

### Open Question 1
How does the anthropomorphism of AI affect student learning outcomes in EFL contexts? The paper mentions students' perception of ChatGPT as human-like but does not measure the impact on learning outcomes. A study comparing learning outcomes between students who anthropomorphize AI versus those who do not would resolve this.

### Open Question 2
What are the long-term effects of using LLMs for essay revision on students' writing skills and academic integrity? The study is semester-long, but the paper does not explore long-term effects beyond this period. A longitudinal study tracking students' writing skills and academic integrity over multiple semesters or years would resolve this.

### Open Question 3
How does code-switching between English and Korean in student prompts affect the quality of AI responses in EFL education? The paper notes code-switching but does not assess its effect on AI response quality or learning outcomes. An analysis comparing AI response quality and student satisfaction for prompts in English versus code-switched prompts would resolve this.

## Limitations

- Single-course context may limit generalizability to other educational settings or student populations
- Reliance on self-reported satisfaction ratings could introduce social desirability bias
- Code-switching between languages creates challenges for model generalization to other multilingual contexts
- Absence of long-term follow-up data prevents assessment of sustained learning improvements

## Confidence

**High Confidence**: Dataset specifications (4,330 utterances, 13 intent categories, 212 students) and baseline model performances (F1: 0.83-0.43 for intent detection, 0.91-0.58 for satisfaction estimation) are well-documented and reproducible.

**Medium Confidence**: Claims about students perceiving ChatGPT as "human-like" and "intelligent peer" are supported by qualitative analysis but could benefit from additional validation through student interviews or surveys.

**Low Confidence**: The assumption that satisfaction ratings directly correlate with learning outcomes remains untested without control groups or comparison to traditional writing instruction.

## Next Checks

1. **Generalizability Test**: Collect and analyze RECIPE4U-style interaction data from at least two additional EFL courses with different student demographics and institutional contexts to assess whether observed interaction patterns and model performances hold across settings.

2. **Learning Outcome Validation**: Design a controlled study comparing writing quality improvements between students who received ChatGPT assistance and those who used traditional writing instruction, using standardized EFL essay scoring rubrics as the evaluation metric.

3. **Satisfaction-Rating Consistency**: Conduct a small-scale validation where 30-50 randomly selected students from the original dataset re-rate their satisfaction with a subset of interactions after 4-6 weeks, measuring inter-rater reliability and temporal consistency of satisfaction scores.