---
ver: rpa2
title: 'ReXTrust: A Model for Fine-Grained Hallucination Detection in AI-Generated
  Radiology Reports'
arxiv_id: '2412.15264'
source_url: https://arxiv.org/abs/2412.15264
tags:
- rextrust
- arxiv
- findings
- hallucination
- detection
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces ReXTrust, a white-box model for detecting
  hallucinations in AI-generated radiology reports. The method leverages hidden states
  from large vision-language models through a self-attention mechanism to produce
  finding-level hallucination risk scores.
---

# ReXTrust: A Model for Fine-Grained Hallucination Detection in AI-Generated Radiology Reports

## Quick Facts
- arXiv ID: 2412.15264
- Source URL: https://arxiv.org/abs/2412.15264
- Reference count: 6
- Primary result: ReXTrust achieves AUROC of 0.8751 for hallucination detection in radiology reports

## Executive Summary
ReXTrust is a white-box model that detects hallucinations in AI-generated radiology reports by analyzing hidden states from large vision-language models. The approach uses a self-attention mechanism to produce finding-level hallucination risk scores, enabling fine-grained detection of potentially harmful errors that could impact patient care. Evaluated on a subset of the MIMIC-CXR dataset, ReXTrust demonstrates superior performance compared to existing approaches, particularly for clinically significant findings.

The model addresses a critical challenge in AI-assisted radiology reporting by providing interpretable, finding-level assessments of report reliability. This granular approach allows radiologists to identify specific problematic findings rather than simply flagging entire reports as unreliable, potentially improving the clinical utility of AI-generated content in medical workflows.

## Method Summary
ReXTrust operates by leveraging hidden states from Llama 3.1 8B, a large vision-language model, through a self-attention mechanism to detect hallucinations at the finding level in AI-generated radiology reports. The model was trained and evaluated on a subset of the MIMIC-CXR dataset using a synthetically generated hallucination dataset called ReXErr. The approach analyzes internal model representations rather than relying solely on final output probabilities, enabling more nuanced detection of reporting errors. The finding-level granularity allows the system to identify specific problematic findings within otherwise accurate reports.

## Key Results
- AUROC of 0.8751 across all findings in the MIMIC-CXR dataset
- AUROC of 0.8963 on clinically significant findings
- Superior performance compared to existing hallucination detection approaches
- Effective finding-level granularity for targeted error identification

## Why This Works (Mechanism)
ReXTrust works by exploiting the internal representations of vision-language models, which contain rich information about the relationship between image features and generated text. By applying self-attention to hidden states rather than just final output probabilities, the model can capture subtle inconsistencies between what the model "sees" in the image and what it generates in the report. This white-box approach allows for more precise localization of hallucinations at the finding level, rather than simply flagging entire reports as unreliable.

The method's effectiveness stems from the observation that hallucinations often manifest as internal inconsistencies in the model's reasoning process, which can be detected through careful analysis of intermediate representations. The self-attention mechanism helps identify which parts of the hidden states are most relevant for determining the reliability of each finding, allowing for fine-grained risk scoring.

## Foundational Learning
- **Vision-Language Models**: Large models that process both images and text; needed for generating radiology reports from medical images; quick check: can process chest X-rays and produce structured text descriptions
- **Hidden States Analysis**: Examination of intermediate model representations; needed to detect internal inconsistencies before they manifest as hallucinations; quick check: can extract layer activations from Llama 3.1 8B
- **Self-Attention Mechanisms**: Neural network layers that weigh the importance of different input elements; needed to identify relevant features for hallucination detection; quick check: can compute attention weights between image regions and report findings
- **Finding-Level Granularity**: Analysis at the individual finding level rather than whole-report; needed for precise error localization in clinical contexts; quick check: can score reliability of individual findings like "cardiomegaly" or "pleural effusion"
- **MIMIC-CXR Dataset**: Large collection of chest X-ray images and associated reports; needed for training and evaluating radiology-focused models; quick check: contains labeled findings with clinical significance annotations
- **Synthetic Hallucination Generation**: Creation of controlled error patterns for training; needed to create labeled data for hallucination detection; quick check: can generate realistic-looking but incorrect findings in reports

## Architecture Onboarding

**Component Map**: Image Encoder -> Llama 3.1 8B -> Hidden States -> Self-Attention Module -> Finding-Level Risk Scores

**Critical Path**: The model processes chest X-ray images through a vision encoder, passes features to Llama 3.1 8B, extracts hidden states from specific layers, applies self-attention to identify relevant patterns, and produces finding-level hallucination risk scores. The self-attention module is critical as it determines which internal representations are most indicative of hallucinations.

**Design Tradeoffs**: The model trades computational efficiency for interpretability and granularity. Analyzing hidden states through self-attention is more computationally intensive than simple confidence thresholding, but provides finding-level insights and explains why certain findings are flagged as potentially hallucinated. The choice of Llama 3.1 8B balances model capacity with practical deployment considerations.

**Failure Signatures**: The model may struggle with findings that have subtle visual presentations or require extensive clinical context. Performance could degrade on imaging modalities different from chest X-rays, and the finding-level approach may miss multi-finding contextual errors. Overreliance on specific error patterns from the synthetic dataset could lead to blind spots for real-world hallucination types.

**First 3 Experiments**:
1. Run ReXTrust on a held-out test set from MIMIC-CXR to verify AUROC scores of 0.8751 overall and 0.8963 for clinically significant findings
2. Compare ReXTrust's finding-level risk scores against radiologist annotations on a small validation set to assess clinical alignment
3. Perform ablation studies removing the self-attention mechanism to quantify its contribution to performance gains

## Open Questions the Paper Calls Out
None

## Limitations
- Limited generalizability beyond MIMIC-CXR dataset and chest X-ray modality
- Potential overfitting to synthetic hallucination patterns in ReXErr dataset
- Computational requirements may limit clinical workflow integration
- Finding-level granularity may miss multi-finding contextual errors

## Confidence
- Technical implementation and performance metrics: High
- Clinical applicability and real-world effectiveness: Medium
- Generalizability across institutions and imaging modalities: Medium

## Next Checks
1. Test ReXTrust on radiology reports from multiple institutions and imaging modalities to assess generalizability across different clinical contexts and report styles.

2. Conduct a pilot study measuring the time and cognitive burden required for radiologists to use ReXTrust outputs in their interpretation workflow, including any observed impact on diagnostic accuracy or efficiency.

3. Perform a systematic analysis comparing the hallucination types detected by ReXTrust with actual clinical errors in retrospective radiology report audits to validate the model's clinical relevance.