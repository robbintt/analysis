---
ver: rpa2
title: 'HLAT: High-quality Large Language Model Pre-trained on AWS Trainium'
arxiv_id: '2404.10630'
source_url: https://arxiv.org/abs/2404.10630
tags:
- training
- arxiv
- hlat
- tokens
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HLAT presents the first successful end-to-end pre-training of large
  language models (7B and 70B parameters) using AWS Trainium accelerators, covering
  1.8 trillion tokens across up to 4096 accelerators. The approach leverages NxDT
  for efficient distributed training with optimizations like layer coalescing, selective
  activation checkpointing, and mixed-precision strategies.
---

# HLAT: High-quality Large Language Model Pre-trained on AWS Trainium

## Quick Facts
- arXiv ID: 2404.10630
- Source URL: https://arxiv.org/abs/2404.10630
- Reference count: 40
- HLAT successfully pre-trains 7B and 70B parameter LLMs on AWS Trainium, achieving performance comparable to GPU/TPU baselines at 60% lower cost.

## Executive Summary
HLAT presents the first successful end-to-end pre-training of large language models using AWS Trainium accelerators. The work trains models up to 70B parameters across 4096 accelerators using 1.8 trillion tokens, achieving performance comparable to similarly sized models trained on GPUs and TPUs. The approach introduces several optimizations including layer coalescing, selective activation checkpointing, and an online dataloader, while demonstrating AWS Trainium's viability for large-scale LLM pre-training at reduced cost.

## Method Summary
HLAT leverages AWS Trainium's NeuronX Distributed Training (NxDT) library with 3D parallelism (Tensor, Pipeline, Data) to distribute training across up to 4096 accelerators. The approach introduces layer coalescing to merge matrix operations, selective activation checkpointing for memory optimization, and an online dataloader that tokenizes data during training. Mixed precision strategies vary by model size: HLAT-7B uses BF16 with stochastic rounding while HLAT-70B employs standard mixed precision training. The models are trained on 1.8 trillion tokens across diverse datasets including The Pile, RedPajama, and C4.

## Key Results
- HLAT-7B and HLAT-70B achieve competitive performance with similarly sized models (LLaMA, OpenLLaMA) on standard benchmarks
- Training cost approximately 60% lower than GPU alternatives
- Successful scaling to 4096 accelerators with sustained throughput
- End-to-end training pipeline validated from pre-training to inference

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Layer coalescing reduces communication volume and increases matrix operation efficiency in distributed training.
- Mechanism: By merging Q, K, V linear projections and SwiGLU layers into single matrix multiplications, the number of separate tensor operations and associated inter-node communications is reduced.
- Core assumption: The compiler can optimize the coalesced operations without losing numerical precision or creating computational bottlenecks.
- Evidence anchors:
  - [abstract] "We propose multiple techniques to improve training efficiency on AWS Trainium such as a novel online dataloader, layer coalescing, selective activation checkpointing, precision strategy, and fault recovery mechanisms."
  - [section] "Coalescing Layers with Same Inputs: We coalesced linear layers with the same inputs to reduce the communication in tensor and sequence parallelism, and increase efficiency of matrix operations. Specifically, the Q,K,V layers in an attention block are coalesced, and the two linear projections layers in SwiGLU [28] are also coalesced."
  - [corpus] Weak - no direct corpus evidence for this specific optimization.
- Break condition: If the fused operations exceed available on-chip memory or if the compiler cannot effectively optimize the fused operations.

### Mechanism 2
- Claim: Selective activation checkpointing improves training efficiency by reducing memory usage while maintaining throughput.
- Mechanism: Only certain layers are checkpointed during forward pass, reducing memory overhead compared to full activation checkpointing while still allowing for gradient computation through backpropagation.
- Core assumption: The selective checkpointing strategy identifies the optimal set of layers to checkpoint without significantly impacting numerical stability or training convergence.
- Evidence anchors:
  - [abstract] "We propose multiple techniques to improve training efficiency on AWS Trainium such as a novel online dataloader, layer coalescing, selective activation checkpointing, precision strategy, and fault recovery mechanisms."
  - [section] "Selective Activation Checkpointing : We use selective activation checkpointing [18] to improve the training efficiency. It has slightly higher memory cost as full activation checkpointing, but increases the overall training throughput."
  - [corpus] Weak - no direct corpus evidence for this specific optimization.
- Break condition: If the selective checkpointing introduces numerical instability or if the memory savings are insufficient to offset the computational overhead.

### Mechanism 3
- Claim: Online dataloader with on-the-fly tokenization reduces preprocessing time and computational resources for large datasets.
- Mechanism: The dataloader tokenizes samples during training rather than pre-tokenizing the entire dataset, eliminating the need for separate preprocessing jobs and reducing storage requirements.
- Core assumption: The on-the-fly tokenization does not introduce significant latency during training and can be efficiently parallelized across CPU and accelerator resources.
- Evidence anchors:
  - [abstract] "We designed a novel dataloader which performs both tokenization and packing online during training."
  - [section] "We designed a novel dataloader which performs both tokenization and packing online during training. The dataloader takes one or more dataset files in Apache Arrow format [24]. All samples are randomly shuffled and split into several subsets according to the total DP ranks."
  - [corpus] Weak - no direct corpus evidence for this specific optimization.
- Break condition: If the on-the-fly tokenization becomes a bottleneck during training or if the parallelization overhead exceeds the benefits.

## Foundational Learning

- Concept: Distributed training with 3D parallelism (Tensor, Pipeline, Data)
  - Why needed here: HLAT-70B uses 4096 accelerators, requiring sophisticated parallelism to distribute model parameters, optimizer states, and training data across the cluster.
  - Quick check question: What are the three types of parallelism used in NxDT, and how do they each contribute to scaling large models?

- Concept: Mixed precision training strategies
  - Why needed here: Training large models requires balancing numerical stability with computational efficiency, especially when using accelerators with different native precision capabilities.
  - Quick check question: Why did HLAT-7B use BF16 with stochastic rounding while HLAT-70B used standard mixed precision training?

- Concept: Activation checkpointing and memory optimization
  - Why needed here: Large models exceed the memory capacity of individual accelerators, requiring techniques to trade computation for memory during backpropagation.
  - Quick check question: How does selective activation checkpointing differ from full activation checkpointing in terms of memory usage and computational overhead?

## Architecture Onboarding

- Component map: NeuronX Distributed Training (NxDT) library -> AWS Trainium accelerators (16 GB HBM each) -> Neuron SDK -> Online dataloader -> Layer coalescing -> Selective activation checkpointing -> Mixed precision training

- Critical path: Data loading and on-the-fly tokenization -> Forward pass with layer coalescing optimizations -> Backward pass with selective activation checkpointing -> Gradient synchronization and optimizer updates -> Checkpoint saving and fault recovery

- Design tradeoffs:
  - Memory vs. computation: Layer coalescing reduces communication but increases per-operation memory usage
  - Precision vs. stability: BF16 with SR provides efficiency but may introduce non-determinism on larger models
  - Preprocessing vs. runtime: Online dataloader eliminates preprocessing but requires efficient parallelization

- Failure signatures:
  - Communication timeouts: May indicate insufficient network bandwidth or improper sharding configuration
  - Memory OOM errors: Could result from inadequate activation checkpointing or improper parallelism configuration
  - Training divergence: May be caused by numerical instability from precision choices or improper weight initialization

- First 3 experiments:
  1. Test layer coalescing on a small model to verify compiler optimizations and measure memory/throughput impact
  2. Compare BF16 with SR vs. mixed precision training on a medium-sized model to identify stability thresholds
  3. Implement and benchmark the online dataloader with varying dataset sizes to identify parallelization bottlenecks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does stochastic rounding in BF16 precision consistently improve or harm training stability for different model sizes and architectures on AWS Trainium?
- Basis in paper: [explicit] The paper notes that BF16 with stochastic rounding showed similar convergence as mixed precision for HLAT-7B but converged slower on HLAT-70B, with potential nondeterministic normalization errors on larger models.
- Why unresolved: The paper only tested this on two specific model sizes (7B and 70B) and doesn't provide a systematic study across different architectures or explain the exact conditions under which stochastic rounding becomes problematic.
- What evidence would resolve it: A comprehensive study testing stochastic rounding across multiple model sizes, architectures, and training configurations to identify the threshold where it becomes detrimental and the underlying causes.

### Open Question 2
- Question: What is the optimal parallelism configuration for maximizing training throughput across different model sizes on AWS Trainium?
- Basis in paper: [explicit] The paper states that TP=8, PP=1 provided highest throughput for 7B models but was not optimal for 70B models, suggesting configuration needs vary by model size.
- Why unresolved: The paper only tested specific configurations and doesn't provide a methodology for determining optimal parallelism for arbitrary model sizes or explain the relationship between model characteristics and optimal sharding strategies.
- What evidence would resolve it: A systematic exploration of parallelism configurations across a wide range of model sizes and architectures, identifying patterns in optimal configurations and developing guidelines for practitioners.

### Open Question 3
- Question: How does the online dataloader approach impact long-term model performance compared to offline pre-tokenization, beyond just training efficiency?
- Basis in paper: [inferred] The paper highlights the online dataloader's efficiency benefits but doesn't evaluate whether on-the-fly tokenization affects model quality, convergence patterns, or downstream task performance compared to traditional offline approaches.
- Why unresolved: The paper focuses on computational efficiency metrics without investigating potential quality trade-offs or differences in how models learn from data processed in different ways.
- What evidence would resolve it: Comparative studies training identical models with online versus offline dataloaders while measuring both efficiency and final model performance across multiple benchmarks.

## Limitations
- Evaluation lacks comprehensive ablation studies to isolate the contribution of each optimization technique
- Performance claims of 60% cost reduction rely on vendor comparisons without independent verification
- Scalability testing limited to specific configurations without evidence for different model sizes or architectures
- Does not address potential hardware-specific limitations or vendor lock-in concerns

## Confidence

**High Confidence:** The core claim that HLAT models achieve competitive performance with established baselines (LLaMA, OpenLLaMA) on standard benchmarks is well-supported by empirical results. The demonstration of successful end-to-end pre-training on Trainium with 4096 accelerators represents a significant technical achievement.

**Medium Confidence:** The efficiency improvements from layer coalescing, selective activation checkpointing, and online dataloader are plausible based on the described mechanisms, but the paper lacks detailed ablation studies or micro-benchmarking to quantify the individual contributions of each optimization. The 60% cost reduction claim requires independent verification given the vendor-specific context.

**Low Confidence:** The generalization of HLAT's optimization techniques to other model architectures, datasets, or training configurations remains unproven. The paper does not provide evidence for how these optimizations would perform with different attention mechanisms, tokenization strategies, or curriculum learning approaches.

## Next Checks
1. **Ablation Study on Optimization Components:** Conduct controlled experiments systematically disabling each optimization (layer coalescing, selective activation checkpointing, online dataloader) to quantify their individual contributions to training efficiency and model quality. This would validate whether the claimed improvements are additive or synergistic.

2. **Independent Cost Analysis:** Perform a third-party cost comparison between Trainium and GPU/TPU training, including total cost of ownership calculations that account for preprocessing time, debugging overhead, power consumption, and potential vendor lock-in. This would verify the 60% cost reduction claim with real-world data.

3. **Generalization Testing Across Architectures:** Evaluate HLAT's optimization techniques on different model architectures (e.g., architectures with different attention mechanisms, non-transformer designs) and training objectives (e.g., masked language modeling, causal language modeling) to assess the broader applicability of the proposed optimizations beyond the specific transformer-based LLMs tested.