---
ver: rpa2
title: Proxy Methods for Domain Adaptation
arxiv_id: '2403.07442'
source_url: https://arxiv.org/abs/2403.07442
tags:
- domain
- target
- source
- adaptation
- shift
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces proxy methods for domain adaptation under
  latent shift, where the shift is due to an unobserved latent variable that confounds
  both covariates and labels. The authors propose using proximal causal learning with
  bridge functions to adapt without explicitly modeling or recovering the latent variable.
---

# Proxy Methods for Domain Adaptation

## Quick Facts
- arXiv ID: 2403.07442
- Source URL: https://arxiv.org/abs/2403.07442
- Reference count: 40
- Key outcome: Introduces proxy methods for domain adaptation under latent shift using proximal causal learning with bridge functions, outperforming methods requiring explicit latent confounder recovery

## Executive Summary
This paper addresses the challenge of domain adaptation when distribution shifts are caused by unobserved latent variables that confound both covariates and labels. The authors propose a novel approach using proximal causal learning with bridge functions that allows adaptation without explicitly modeling or recovering the latent confounder. They consider two settings: concept bottleneck, where an additional concept variable mediates the relationship between covariates and labels, and multi-domain, where training data from multiple source domains with different latent confounder distributions is available. The proposed two-stage kernel estimation approach demonstrates superior performance compared to baseline methods, including those that explicitly recover the latent confounder, in both simulated and real data experiments.

## Method Summary
The method employs proximal causal learning to estimate causal effects in the presence of unobserved confounders using proxy variables. It uses a two-stage kernel estimation approach: first estimating the conditional mean embedding of the proxy variable given the concept (or domain) and covariates, then solving for the bridge function that connects the proxy to the label by implicitly marginalizing over the unobserved confounder. This allows adaptation to novel target domains without requiring explicit recovery of the latent confounder distribution. The method is applied in two settings: concept bottleneck adaptation where a mediating concept variable is observed, and multi-domain adaptation where multiple source domains with different latent confounder distributions are available.

## Key Results
- The concept adaptation method recovers much of the performance of models trained on target domain data on the MIMIC-CXR dataset
- The multi-domain method shows limited success compared to baselines on the MIMIC-CXR dataset
- Two-stage kernel estimation approach outperforms other methods including those requiring explicit latent confounder recovery in simulated experiments
- For discrete latent confounders, multiple source domains can provide sufficient information to identify the bridge function

## Why This Works (Mechanism)

### Mechanism 1
Bridge functions allow identification of the optimal target predictor without explicitly recovering the latent confounder by connecting the proxy variable to the label through implicit marginalization over the unobserved confounder. This enables adaptation to distribution shifts in the latent variable without needing to identify or model its distribution. The mechanism relies on completeness and positivity assumptions being satisfied. Break condition: If completeness fails, meaning the proxy does not have sufficient variability related to changes in the latent confounder, the bridge function cannot be identified.

### Mechanism 2
Multi-domain data provides sufficient information to identify the bridge function when the latent confounder is discrete by observing data from multiple source domains with different latent confounder distributions. This allows the method to span the space of vectors on the latent confounder and identify a domain-invariant bridge matrix. The mechanism requires the number of source domains to be greater than or equal to the dimensionality of the latent confounder. Break condition: For continuous latent confounders, the method may not guarantee full identification, and convergence of the adaptation procedure is unclear.

### Mechanism 3
Kernel methods estimate bridge functions and conditional mean embeddings in both concept bottleneck and multi-domain settings through a two-stage estimation process. First, the conditional mean embedding of the proxy given concept (or domain) and covariates is estimated using kernel ridge regression. Then, this estimate is used to solve for the bridge function. The method assumes the bridge function lies in a reproducing kernel Hilbert space associated with a positive semidefinite kernel function. Break condition: If the kernel function is inappropriate for the data distribution or regularization parameters are poorly tuned, bridge function estimation may be inaccurate.

## Foundational Learning

- **Causal inference with unobserved confounders**: Understanding causal inference with unobserved confounders is crucial for developing methods to adapt to latent shift settings. Quick check: What is the difference between covariate shift and latent shift, and why does latent shift violate the assumptions of traditional domain adaptation methods?

- **Proximal causal learning**: This technique estimates causal effects in the presence of unobserved confounders with observed proxies, forming the foundation of the bridge function approach. Quick check: How does proximal causal learning differ from traditional causal inference methods, and what role do proxy variables play in this framework?

- **Reproducing kernel Hilbert spaces (RKHS)**: Understanding RKHS is essential for grasping how kernel methods work for estimating bridge functions and conditional mean embeddings. Quick check: What is the significance of the representer theorem in the context of kernel methods, and how does it relate to the estimation of the bridge function in this paper?

## Architecture Onboarding

- **Component map**: Data preprocessing -> Kernel estimation (conditional mean embedding) -> Bridge function estimation -> Adaptation (apply bridge function to target domain) -> Evaluation

- **Critical path**: 1) Preprocess data and extract relevant variables 2) Estimate conditional mean embeddings for each source domain 3) Solve for the bridge function using estimated conditional mean embeddings 4) Estimate conditional mean embedding for target domain 5) Apply bridge function to target domain's conditional mean embedding to obtain adapted predictor 6) Evaluate performance of adapted predictor

- **Design tradeoffs**: Choice of kernel function (Gaussian for continuous, binary for discrete), regularization parameter selection, number of source domains (more domains provide more information but increase computational complexity)

- **Failure signatures**: Poor target domain performance (kernel estimation, bridge function estimation, or hyperparameter issues), high variance in estimated bridge function (sensitivity to hyperparameters or insufficient data), non-convergence of adaptation procedure (kernel estimation or kernel function choice issues)

- **First 3 experiments**: 1) Implement kernel estimation procedure for simple synthetic dataset with known bridge function 2) Test bridge function estimation on dataset with multiple source domains and discrete latent confounder 3) Evaluate adaptation performance on real-world dataset with concept bottleneck structure

## Open Questions the Paper Calls Out

### Open Question 1
Under what precise conditions does the multi-domain adaptation method with continuous latent variables fail to provide full identification of the optimal predictor? The paper discusses identification challenges with continuous U and mentions Example 4.4 illustrating failure with continuous U, but does not provide complete characterization. This remains unresolved because while the paper notes that no finite set of projections will completely characterize square integrable functions on U for continuous cases, it doesn't provide precise conditions under which this leads to identification failure. A complete characterization of the function space and conditions on smoothness/regularity that guarantee or prevent identification with continuous U would resolve this.

### Open Question 2
How many source domains are theoretically necessary for reliable multi-domain adaptation, and how does this scale with the dimensionality of the latent variable? While Proposition 4.2 states kW, kZ ≥ kU is sufficient for identification with discrete variables, the paper doesn't provide bounds for continuous cases or analyze how the number of domains needed scales with U's dimensionality. This remains unresolved because the paper shows kZ ≥ kU is necessary for discrete U but doesn't analyze the scaling relationship for continuous U. Theoretical bounds on required number of domains as a function of U's dimensionality and empirical studies showing the relationship between domain diversity and adaptation performance would resolve this.

### Open Question 3
What are the precise conditions under which the completeness assumption (Assumption 4) holds in practice for both concept and multi-domain settings? The paper states Assumption 4 is commonly used but doesn't provide specific conditions or tests for when it holds. This remains unresolved because the paper references completeness as a common assumption in proximal causal inference but doesn't specify what conditions on the data generating process or variable relationships guarantee it. A set of verifiable conditions on the data distribution and relationships between variables that guarantee the completeness assumption holds, along with statistical tests to verify these conditions, would resolve this.

## Limitations
- Method's performance heavily depends on completeness and positivity assumptions which may not hold in all real-world scenarios
- Two-stage kernel estimation approach requires careful tuning of hyperparameters and kernel selection
- Method's effectiveness is limited when the latent confounder is continuous, as full identification of the bridge function is not guaranteed

## Confidence

**High**: The conceptual framework of using bridge functions for adaptation without explicitly recovering the latent confounder is sound and theoretically grounded in proximal causal learning.

**Medium**: The two-stage kernel estimation approach is a reasonable method for estimating the bridge function and conditional mean embeddings, but its effectiveness may depend on the specific data distribution and kernel choice.

**Low**: The experimental results on the MIMIC-CXR dataset show limited success for the multi-domain method, suggesting that the proposed approach may not generalize well to all real-world scenarios.

## Next Checks

1. **Assumption Validation**: Empirically evaluate the completeness and positivity assumptions in real-world datasets to assess their validity and impact on the method's performance.

2. **Hyperparameter Sensitivity**: Conduct a thorough sensitivity analysis of the kernel estimation and bridge function estimation procedures to different hyperparameter choices and kernel functions.

3. **Continuous Latent Confounder Extension**: Investigate potential extensions of the method to handle continuous latent confounders, such as using approximate inference techniques or assuming a specific parametric form for the bridge function.