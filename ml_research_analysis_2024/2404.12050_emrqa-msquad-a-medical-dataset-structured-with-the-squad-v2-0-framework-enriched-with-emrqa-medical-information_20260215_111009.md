---
ver: rpa2
title: 'emrQA-msquad: A Medical Dataset Structured with the SQuAD V2.0 Framework,
  Enriched with emrQA Medical Information'
arxiv_id: '2404.12050'
source_url: https://arxiv.org/abs/2404.12050
tags:
- medical
- dataset
- roberta
- baseline
- score
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This research addresses the challenge of complex medical terminology
  and question ambiguity in Medical Question Answering Systems (QAS). The authors
  created a specialized dataset called emrQA-msquad by restructuring the emrQA dataset
  into 163,695 questions and 4,136 manually obtained answers.
---

# emrQA-msquad: A Medical Dataset Structured with the SQuAD V2.0 Framework, Enriched with emrQA Medical Information

## Quick Facts
- arXiv ID: 2404.12050
- Source URL: https://arxiv.org/abs/2404.12050
- Reference count: 29
- Key outcome: Fine-tuning BERT, RoBERTa, and Tiny RoBERTa on emrQA-msquad dataset significantly improves F1-score in medical question answering (increases of 27.3%, 26.0%, and 30.8% respectively)

## Executive Summary
This research addresses the challenge of complex medical terminology and question ambiguity in Medical Question Answering Systems (QAS) by creating a specialized dataset called emrQA-msquad. The dataset restructures the emrQA dataset into 163,695 questions and 4,136 manually obtained answers using the SQuAD V2.0 framework. Fine-tuning transformer-based models (BERT, RoBERTa, and Tiny RoBERTa) on this medical dataset significantly improved response accuracy, with F1-score ranges increasing from 10.1% to 37.4% for BERT, 18.7% to 44.7% for RoBERTa, and 16.0% to 46.8% for Tiny RoBERTa.

## Method Summary
The authors restructured the emrQA dataset into the SQuAD V2.0 format, creating 163,695 questions and 4,136 manually extracted answers. They fine-tuned BERT, RoBERTa, and Tiny RoBERTa models on this medical dataset using standard transformer fine-tuning procedures with specific hyperparameters (max_length=512, learning_rate=2e-5, num_train_epochs=3, weight_decay=0.01, doc_stride=172, batch_size=16). The emrQA-msquad dataset is publicly available at https://huggingface.co/datasets/Eladio/emrqa-msquad.

## Key Results
- F1-score range improvement from 0.75 to 1.00 increased from 10.1% to 37.4% for BERT
- F1-score range improvement from 0.75 to 1.00 increased from 18.7% to 44.7% for RoBERTa
- F1-score range improvement from 0.75 to 1.00 increased from 16.0% to 46.8% for Tiny RoBERTa

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning transformer-based models on the emrQA-msquad dataset significantly improves F1-score in medical question answering. The emrQA-msquad dataset restructures medical EMR data into the SQuAD V2.0 format, providing contextually rich and structured medical text that aligns with the pre-training objectives of transformer models. Fine-tuning on this specialized dataset enables the models to better understand and extract medically relevant spans. Core assumption: The SQuAD V2.0 structure is sufficient to represent the span extraction task in medical EMRs. Evidence: The fine-tuning of models such as BERT, RoBERTa, and Tiny RoBERTa for medical contexts significantly improved response accuracy within the F1-score range of 0.75 to 1.00 from 10.1% to 37.4%, 18.7% to 44.7% and 16.0% to 46.8%, respectively.

### Mechanism 2
Manual summarization and answer extraction ensure higher-quality, contextually accurate training data compared to fully automated methods. The authors used OpenAI's text-davinci-003 to summarize medical reports, but found that automated answer extraction introduced noise. Therefore, they manually extracted answers to create the ground truth, preserving answer accuracy in the summaries. Core assumption: Manual extraction provides higher fidelity labels that improve model performance compared to automated methods prone to hallucination or omission. Evidence: Attempts were made to obtain the new ground truth using the text-davinci-003 model, but it proved insufficient for accurate span extraction of answers within the summaries. These models introduced additional information that compromised answer accuracy. As a result, the only viable approach for creating the new ground truth was a time-consuming manual process, where answers were collected one by one.

### Mechanism 3
Reducing dataset dimensionality by deduplicating questions and answers improves training efficiency and model generalization. After summarization, the authors applied deduplication to remove redundant questions and answers, which reduces noise and focuses the model on diverse, informative examples. Core assumption: Duplicate or near-duplicate examples do not contribute meaningful variation to the training process and may cause overfitting to specific phrasings rather than underlying concepts. Evidence: Optimization measures were applied to minimize repeated responses, reducing the dataset file's dimensionality.

## Foundational Learning

- Concept: Span extraction in MRC
  - Why needed here: The task is to locate the exact substring in the context that answers the question; understanding this is fundamental to building and evaluating QA models.
  - Quick check question: In SQuAD V2.0, if the answer is not in the context, what label is used? (Answer: NULL/No answer)

- Concept: Fine-tuning vs. pre-training
  - Why needed here: The models were pre-trained on general corpora and then fine-tuned on the medical dataset; understanding this distinction is key to interpreting performance gains.
  - Quick check question: What is the primary difference between pre-training and fine-tuning in the context of transformer models? (Answer: Pre-training learns general language representations; fine-tuning adapts them to a specific downstream task.)

- Concept: F1-score in QA evaluation
  - Why needed here: The main metric reported for model performance is F1-score; understanding how it is computed (precision and recall of token overlap) is essential for interpreting results.
  - Quick check question: If a model predicts an answer that matches exactly with the ground truth, what is the F1-score? (Answer: 1.0)

## Architecture Onboarding

- Component map: Input pipeline (context, question) -> Transformer encoder (BERT/RoBERTa/Tiny RoBERTa) -> Span prediction head (start/end logits) -> Output (predicted answer span)
- Critical path: Data loading -> tokenization (max_length=512, doc_stride=172) -> model forward pass -> loss computation (span cross-entropy) -> backpropagation -> optimizer step (lr=2e-5, weight_decay=0.01, epochs=3)
- Design tradeoffs: Larger models (BERT, RoBERTa) have higher capacity but slower inference; Tiny RoBERTa is faster but may underfit. doc_stride allows handling of long contexts but introduces overlap; batch_size=16 balances memory and convergence speed.
- Failure signatures: Low F1-score with high variance suggests poor generalization or noisy labels; perfect F1 on training but low on validation suggests overfitting; high EM but low F1 suggests exact matches are rare but partial matches are common.
- First 3 experiments:
  1. Run baseline models (BERT, RoBERTa, Tiny RoBERTa) on the test split of emrQA-msquad without fine-tuning to confirm the reported baseline F1 scores.
  2. Fine-tune each model for 1 epoch with a smaller learning rate (1e-5) to check for overfitting and verify training stability.
  3. Evaluate model performance on a held-out validation set after each epoch to monitor for overfitting and select the best checkpoint.

## Open Questions the Paper Calls Out

### Open Question 1
What specific fine-tuning techniques were employed to adapt BERT, RoBERTa, and Tiny RoBERTa models to the medical domain, and how did these techniques impact model performance? The paper mentions fine-tuning but does not provide detailed insights into the specific techniques used or their direct impact on performance metrics. What evidence would resolve it: Detailed documentation of the fine-tuning process, including hyperparameter settings and techniques, alongside comparative performance metrics before and after fine-tuning.

### Open Question 2
How does the emrQA-msquad dataset address the issue of complex medical terminology and question ambiguity, and what methods were used to ensure its effectiveness? The paper states that the emrQA-msquad dataset was created to address complex medical terminology and question ambiguity, but it does not elaborate on the specific methods used to achieve this. What evidence would resolve it: A comprehensive explanation of the dataset's structure, including how it manages terminology and ambiguity, possibly through examples or case studies.

### Open Question 3
What are the limitations of using Large Language Models (LLMs) like text-davinci-003 for summarizing medical reports, and how were these limitations addressed in the dataset creation process? The paper mentions the use of text-davinci-003 for summarizing medical reports but notes issues with additional information and the need for manual extraction of answers. What evidence would resolve it: An analysis of the challenges faced with LLMs in summarization, including examples of errors and the steps taken to mitigate these issues, along with the impact on dataset accuracy.

## Limitations
- Manual answer extraction process is not scalable and may introduce human bias without reported inter-annotator agreement metrics
- Performance improvements are reported only on emrQA-msquad dataset without external validation on clinical benchmarks or real-world data
- Dataset size (163,695 questions, 4,136 answers) is relatively small for fine-tuning large language models, potentially limiting generalization

## Confidence
- **High confidence**: The dataset restructuring methodology and general approach of fine-tuning transformer models on medical text are well-supported by the literature and internally consistent
- **Medium confidence**: The reported F1-score improvements are plausible given the fine-tuning process, but lack of baseline scores and external validation reduces confidence in magnitude of gains
- **Low confidence**: Scalability of manual answer extraction process and model performance on real-world clinical data are uncertain without further evidence

## Next Checks
1. **External Validation**: Evaluate the fine-tuned models on at least two external medical QA datasets (e.g., CliCR, emrQA original test set) to assess generalization beyond the emrQA-msquad corpus.

2. **Baseline Reporting**: Report the F1 scores of the base (un-fine-tuned) BERT, RoBERTa, and Tiny RoBERTa models on the emrQA-msquad test set to provide context for the reported improvements.

3. **Robustness Testing**: Test model robustness by introducing adversarial or out-of-distribution questions (e.g., questions with ambiguous terminology or requiring multi-hop reasoning) and report performance degradation or error analysis.