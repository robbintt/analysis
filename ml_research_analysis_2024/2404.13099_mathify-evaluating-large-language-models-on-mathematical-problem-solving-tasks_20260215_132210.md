---
ver: rpa2
title: 'Mathify: Evaluating Large Language Models on Mathematical Problem Solving
  Tasks'
arxiv_id: '2404.13099'
source_url: https://arxiv.org/abs/2404.13099
tags:
- mathematical
- dataset
- language
- large
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of improving mathematical problem-solving
  capabilities of large language models (LLMs). The authors introduce MathQuest, a
  comprehensive dataset of mathematical problems sourced from NCERT textbooks, and
  fine-tune three prominent LLMs (LLaMA-2, WizardMath, and MAmmoTH) on this dataset
  and an augmented version of the Math-401 dataset.
---

# Mathify: Evaluating Large Language Models on Mathematical Problem Solving Tasks

## Quick Facts
- arXiv ID: 2404.13099
- Source URL: https://arxiv.org/abs/2404.13099
- Reference count: 40
- Primary result: MAmmoTH-13B achieves 24.0% accuracy on MathQuest dataset after fine-tuning

## Executive Summary
This paper addresses the challenge of improving mathematical problem-solving capabilities of large language models (LLMs). The authors introduce MathQuest, a comprehensive dataset of mathematical problems sourced from NCERT textbooks, and fine-tune three prominent LLMs (LLaMA-2, WizardMath, and MAmmoTH) on this dataset and an augmented version of the Math-401 dataset. The primary result is that MAmmoTH-13B emerges as the most proficient model, achieving the highest accuracy on the MathQuest dataset (24.0%) after fine-tuning, outperforming the other models. The authors demonstrate significant improvements in mathematical problem-solving capabilities of LLMs through their fine-tuning approach.

## Method Summary
The authors developed MathQuest, a dataset of mathematical problems derived from NCERT textbooks, and combined it with an augmented version of the Math-401 dataset. They fine-tuned three large language models (LLaMA-2, WizardMath, and MAmmoTH) on this combined dataset to improve their mathematical problem-solving capabilities. The evaluation measured accuracy on the MathQuest test set, with MAmmoTH-13B achieving the highest performance at 24.0% accuracy after fine-tuning.

## Key Results
- MAmmoTH-13B achieves highest accuracy (24.0%) on MathQuest dataset after fine-tuning
- Fine-tuning on MathQuest and augmented Math-401 improves mathematical problem-solving capabilities
- MAmmoTH-13B outperforms LLaMA-2 and WizardMath in mathematical reasoning tasks

## Why This Works (Mechanism)
The paper does not provide explicit mechanistic explanations for why the fine-tuning approach improves mathematical problem-solving capabilities.

## Foundational Learning
The paper does not explicitly outline foundational learning concepts, but the following concepts are implicitly relevant:

1. **Mathematical problem representation**: Understanding how mathematical problems are structured and represented in natural language
   - Why needed: Essential for parsing and solving mathematical word problems
   - Quick check: Verify that problems are correctly tokenized and parsed

2. **Fine-tuning methodology**: Process of adapting pre-trained LLMs to domain-specific tasks
   - Why needed: Core technique for improving model performance on specialized tasks
   - Quick check: Confirm fine-tuning hyperparameters and dataset splits

3. **Mathematical reasoning evaluation**: Metrics and methods for assessing mathematical problem-solving capabilities
   - Why needed: Critical for measuring and comparing model performance
   - Quick check: Validate evaluation metrics and test set construction

## Architecture Onboarding

**Component Map**: Raw mathematical problems (NCERT textbooks) -> MathQuest dataset creation -> Combined dataset (MathQuest + Math-401 augmentation) -> LLM fine-tuning -> Performance evaluation on test set

**Critical Path**: Dataset preparation (MathQuest + Math-401 augmentation) → Fine-tuning process → Model evaluation → Performance analysis

**Design Tradeoffs**: The study uses relatively small fine-tuning datasets compared to the original pre-training data, balancing computational efficiency with performance gains. The selection of NCERT-based problems provides curriculum alignment but may limit generalizability to broader mathematical reasoning tasks.

**Failure Signatures**: Low accuracy (24.0%) suggests limitations in handling complex mathematical reasoning. The performance gap between models indicates architectural differences in mathematical reasoning capabilities. Dataset construction issues could lead to overfitting on specific problem types.

**First Experiments**: 
1. Baseline evaluation of unfine-tuned models on MathQuest to establish performance delta
2. Ablation study comparing performance with MathQuest alone vs. combined dataset
3. Cross-dataset evaluation to test generalization beyond MathQuest

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- The 24.0% accuracy, while presented as significant, represents relatively low absolute performance with limited practical applicability
- Dataset construction methodology lacks transparency regarding sampling strategy and potential biases
- Limited evaluation scope focused only on MathQuest without broader mathematical reasoning benchmarks
- Study focuses on three specific models without exploring performance patterns across a wider range of LLMs

## Confidence

| Claim | Confidence |
|-------|------------|
| Technical methodology of fine-tuning LLMs on mathematical datasets is sound | High |
| Relative ranking of models (MAmmoTH-13B > WizardMath > LLaMA-2) is accurate | Medium |
| Absolute performance improvements and practical significance of 24.0% accuracy | Low |

## Next Checks
1. Conduct ablation studies to isolate the contribution of MathQuest versus augmented Math-401 data to model performance
2. Test the fine-tuned models on additional mathematical reasoning benchmarks beyond MathQuest to assess generalization
3. Implement human evaluation of model responses to verify that high-confidence answers are mathematically correct and not pattern-matching artifacts