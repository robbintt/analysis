---
ver: rpa2
title: 'Automating Data Annotation under Strategic Human Agents: Risks and Potential
  Solutions'
arxiv_id: '2405.08027'
source_url: https://arxiv.org/abs/2405.08027
tags:
- group
- data
- agents
- retraining
- strategic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how ML models retrained with model-annotated
  data evolve when facing strategic human agents. It shows that the acceptance rate
  of agents increases over time, but the true qualification rate may decrease.
---

# Automating Data Annotation under Strategic Human Agents: Risks and Potential Solutions

## Quick Facts
- arXiv ID: 2405.08027
- Source URL: https://arxiv.org/abs/2405.08027
- Authors: Tian Xie; Xueru Zhang
- Reference count: 40
- Primary result: Shows that model-annotated samples inflate acceptance rates while qualification rates may decline, with classifier bias potentially increasing unless systematic bias is negative.

## Executive Summary
This paper studies how ML models retrained with model-annotated data evolve when facing strategic human agents. The authors find that acceptance rates increase over time while qualification rates may decrease, leading to potential bias amplification. They propose a refined retraining method using probabilistic samplers to stabilize these dynamics. Experiments on synthetic and real datasets validate the theoretical findings about how strategic behavior affects model performance and fairness.

## Method Summary
The paper proposes a refined retraining process using probabilistic samplers when producing model-annotated samples. Logistic regression models are trained using stochastic gradient descent over T steps. The method mixes human-annotated samples (K) with model-annotated samples (N-K) to form the training set. The refined approach uses estimated posterior probabilities from the latest training data rather than deterministic labeling, aiming to stabilize the dynamics of acceptance rates, qualification rates, and classifier bias.

## Key Results
- Acceptance rates increase over time under deterministic model annotation, regardless of true qualification
- Qualification rates may decrease as models become more lenient
- Classifier bias (acceptance rate minus qualification rate) increases monotonically unless systematic bias is negative
- Refined retraining with probabilistic samplers stabilizes dynamics but preserves systematic bias
- Fairness interventions can help disadvantaged groups maintain positive outcomes when agent perception is accurate

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Model-annotated samples systematically shift the feature distribution toward higher acceptance rates, regardless of true qualification.
- Mechanism: When agents best respond to the current classifier, they move features to the decision boundary. Model-annotation labels these moved features as positive with high probability, inflating the training set's positive label ratio and causing the next classifier to be more lenient.
- Core assumption: Agents respond to the latest model they know (delayed response) and feature-label relationship remains unchanged.
- Evidence anchors:
  - [abstract] "We find that agents are increasingly likely to receive positive decisions as the model gets retrained, whereas the proportion of agents with positive labels may decrease over time."
  - [section 3.1] "When agents best respond, the decision-maker tends to accept more agents."
- Break condition: If agents do not know the current model (no delayed response) or feature-label relationship changes with strategic behavior.

### Mechanism 2
- Claim: Classifier bias (acceptance rate minus qualification rate) increases monotonically unless systematic bias is negative.
- Mechanism: The difference between acceptance and qualification rates grows because model-annotated samples over-represent qualified labels among post-best-response agents. If decision-maker's systematic bias underestimates true qualification, it can counteract this effect.
- Core assumption: Model annotations reflect current classifier's decision rule without adjustment.
- Evidence anchors:
  - [abstract] "Classifier bias, the gap between acceptance and qualification rates, may increase or decrease depending on systematic bias."
  - [section 3.3] "Originally, the purpose of retraining the classifier was to ensure accurate decisions... However, when agents behave strategically, the retraining may lead to adverse outcomes by amplifying the classifier bias."
- Break condition: If systematic bias is negative and sufficiently large, or if model annotations are adjusted probabilistically (refined retraining).

### Mechanism 3
- Claim: Refining model annotations with probabilistic samplers stabilizes dynamics by reducing bias amplification.
- Mechanism: Instead of deterministic labeling, probabilistic sampler uses estimated posterior probabilities from the latest training data, diluting the effect of strategic shifts on the label distribution.
- Core assumption: Probabilistic sampler approximates true label distribution from the latest training data.
- Evidence anchors:
  - [section 3.4] "This alleviates the influence of agents' best responses to stabilize the dynamics of at, qt, ∆t."
  - [abstract] "We thus propose a refined retraining process to stabilize the dynamics."
- Break condition: If probabilistic sampler is poorly calibrated or training data is too small to estimate posteriors reliably.

## Foundational Learning

- Concept: Strategic classification with label changes
  - Why needed here: Understanding how agents' strategic feature shifts can alter their true labels under causal mechanisms is key to predicting retraining dynamics.
  - Quick check question: In a strategic setting, if an agent changes features to get accepted, under what condition does the true label also change?

- Concept: Retraining with model-annotated samples
  - Why needed here: The paper's core mechanism relies on mixing human and model-annotated data; knowing how model labels differ from true labels is critical.
  - Quick check question: What is the effect on training distribution when model-annotated samples are added at each round?

- Concept: Demographic parity and fairness intervention dynamics
  - Why needed here: The paper investigates how short-term fairness constraints interact with long-term strategic dynamics and whether disadvantaged groups benefit.
  - Quick check question: If a fairness constraint forces equal acceptance rates between groups, what might happen to long-term qualification rates for each group?

## Architecture Onboarding

- Component map:
  Agent population -> Best response module -> Feature/label distributions -> Model retraining module -> Classifier -> Decision -> Feedback loop

- Critical path:
  Best response -> Model annotation -> Retraining -> Deployment -> Next best response
  Each iteration must correctly sample from current distributions and apply the appropriate annotation method.

- Design tradeoffs:
  Deterministic vs probabilistic model annotation: Deterministic amplifies strategic shifts; probabilistic stabilizes but preserves systematic bias.
  Frequency of human annotation (K) vs model annotation (N): Higher K reduces amplification but is costlier; low K risks runaway acceptance rates.
  Fairness intervention timing: Enforcing fairness at each round may harm disadvantaged groups long-term; early stopping may be preferable.

- Failure signatures:
  Acceptance rate approaching 1 while qualification rate declines -> runaway bias amplification.
  Classifier bias oscillating or diverging sharply between groups -> improper annotation or fairness intervention.
  Training loss diverging or model failing to converge -> distribution shift too extreme or label noise too high.

- First 3 experiments:
  1. Run basic retraining with deterministic model annotation, track at, qt, ∆t over time; verify at increases and qt decreases.
  2. Introduce refined retraining with probabilistic sampler; confirm stabilization of at, qt, ∆t but persistence of systematic bias.
  3. Apply fairness constraints at each round; observe whether disadvantaged group qualification rate improves or declines long-term.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions can classifier bias be minimized or stabilized in the long run?
- Basis in paper: [explicit] The paper proposes a refined retraining process using probabilistic samplers to stabilize dynamics and shows it preserves systematic bias but does not eliminate it.
- Why unresolved: The theoretical analysis focuses on monotonic trends rather than identifying precise parameter regimes where bias is minimized.
- What evidence would resolve it: Empirical studies varying systematic bias magnitude, cost matrices, and sampling ratios to identify conditions where ∆t remains bounded.

### Open Question 2
- Question: How does the timing of fairness interventions affect long-term outcomes for disadvantaged groups?
- Basis in paper: [explicit] Theorem 4.2 shows fairness intervention helps maintain disadvantaged groups when agent perception is accurate, but the paper doesn't explore optimal intervention timing.
- Why unresolved: The analysis assumes interventions occur at every round, but real-world applications may benefit from strategic timing.
- What evidence would resolve it: Simulation experiments comparing continuous intervention versus periodic or conditional interventions on fairness metrics over time.

### Open Question 3
- Question: What are the impacts of model retraining dynamics in high-dimensional feature spaces?
- Basis in paper: [inferred] All theoretical and experimental results assume low-dimensional settings (2-3 features), but real-world applications often involve dozens of features.
- Why unresolved: High-dimensional spaces may exhibit different strategic response patterns and retraining dynamics.
- What evidence would resolve it: Comparative analysis of retraining dynamics between low and high-dimensional synthetic datasets with varying feature correlations.

## Limitations
- Theoretical framework relies on strong assumptions (logistic regression with monotone likelihood ratio property) that may not hold in real-world settings
- Convergence analysis assumes infinite sample sizes, but experiments use finite datasets where estimation errors could impact results
- Refined retraining method's effectiveness depends on accurate posterior probability estimation, which becomes unreliable with scarce or imbalanced training data

## Confidence

- **High confidence**: The core finding that acceptance rates increase while qualification rates may decrease under deterministic model annotation is theoretically sound and empirically validated across multiple datasets.
- **Medium confidence**: The refined retraining method's ability to stabilize dynamics while preserving systematic bias is supported by experiments, but the theoretical guarantees for this stabilization are less rigorous.
- **Low confidence**: The generalization of results to non-linear classifiers and multi-dimensional feature spaces remains untested.

## Next Checks

1. **Ablation study on sample size**: Systematically vary N and K to quantify how finite sample effects impact the convergence of acceptance/qualification rates and the effectiveness of refined retraining.

2. **Cross-classifier validation**: Replicate key experiments using non-linear classifiers (e.g., random forests, neural networks) to test the robustness of findings beyond logistic regression assumptions.

3. **Long-horizon fairness simulation**: Extend experiments beyond the tested time horizon to observe whether short-term fairness interventions lead to unintended equilibrium states that disadvantage protected groups in the long run.