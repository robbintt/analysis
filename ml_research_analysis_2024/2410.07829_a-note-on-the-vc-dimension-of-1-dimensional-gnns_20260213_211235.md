---
ver: rpa2
title: A note on the VC dimension of 1-dimensional GNNs
arxiv_id: '2410.07829'
source_url: https://arxiv.org/abs/2410.07829
tags:
- gnns
- graph
- dimension
- graphs
- activation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the generalization capabilities of graph
  neural networks (GNNs) by analyzing their Vapnik-Chervonenkis (VC) dimension. The
  authors extend previous results to show that 1-dimensional GNNs with a single parameter
  have infinite VC dimension for unbounded graphs.
---

# A note on the VC dimension of 1-dimensional GNNs

## Quick Facts
- arXiv ID: 2410.07829
- Source URL: https://arxiv.org/abs/2410.07829
- Authors: Noah Daniëls; Floris Geerts
- Reference count: 3
- 1-dimensional GNNs with a single parameter have infinite VC dimension for unbounded graphs

## Executive Summary
This paper investigates the generalization capabilities of graph neural networks (GNNs) by analyzing their Vapnik-Chervonenkis (VC) dimension. The authors prove that even the simplest 1-dimensional GNNs with only a single parameter can shatter arbitrarily large numbers of graphs, leading to infinite VC dimension for unbounded graphs. This result holds for both piecewise linear activation functions and analytic non-polynomial activation functions, including those used in recently proposed GNNs that are as expressive as the Weisfeiler-Leman isomorphism test. The findings suggest inherent limitations in GNN generalization when viewed from the VC dimension perspective, showing that increasing model complexity (width and depth) does not improve generalization for unbounded graphs.

## Method Summary
The authors construct specific graph families and GNNs that can shatter arbitrarily large numbers of graphs using carefully engineered vertex degrees and activation functions. For piecewise linear activation functions, they use a single parameter γx = Σxₖ·2ᵏ⁻¹ to encode binary representations, where vertex degrees are chosen to extract individual bits through controlled output values. For analytic non-polynomial activation functions like sine, they leverage the periodic nature of the function to extract binary bits from the parameter. The construction involves three-part graphs with carefully chosen vertex degrees, where the "result vertex" has degree depending on powers of 4 while other vertices have degrees designed to make their sine outputs zero.

## Key Results
- 1-dimensional GNNs with a single parameter have infinite VC dimension for unbounded graphs
- This result applies to both piecewise linear and analytic non-polynomial activation functions
- Increasing width and depth beyond 1 does not improve VC dimension for unbounded graphs
- Even GNNs that are as expressive as the Weisfeiler-Leman isomorphism test suffer from this limitation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: 1-dimensional GNNs with piecewise linear activation functions and only a single parameter can shatter arbitrarily many graphs, leading to infinite VC dimension
- Mechanism: The authors construct a family of graphs where vertex degrees are carefully chosen to encode binary representations. Using a single parameter γx = Σxₖ·2ᵏ⁻¹, the activation function can extract individual bits through controlled output values. For each graph Gⱼ, vertex degrees are set so that when γx equals specific values, exactly one vertex in Gⱼ will activate (output 1), while all others output 0. Summing these outputs gives ξx(Gⱼ) = xⱼ, allowing the GNN to distinguish all 2ᵇ possible binary strings across b graphs.
- Core assumption: The piecewise linear activation function can be designed to have distinct output regions that correspond to specific parameter values, and vertex degrees can be engineered to create these precise conditions.
- Evidence anchors:
  - [abstract]: "1-dimensional GNNs with a single parameter have an infinite VC dimension for unbounded graphs"
  - [section]: "These results suggest inherent limitations in the generalization ability of even the most simple GNNs, when viewed from the VC dimension perspective"
  - [corpus]: Weak evidence - corpus contains related papers on GNN generalization and VC dimension but none specifically validating this mechanism
- Break condition: If the piecewise linear activation function cannot create sufficiently distinct output regions, or if vertex degree constraints prevent constructing the required graph families, the shattering argument fails.

### Mechanism 2
- Claim: GNNs with analytic non-polynomial activation functions (like sine) can also shatter arbitrarily many graphs, achieving infinite VC dimension
- Mechanism: The authors leverage the periodic nature of sine to extract binary bits from the parameter γx = Σxₖ/4ᵏ. They show that sin(4ʲπγx/2) equals 0 when j > b, is ≥ 2/3 when xⱼ = 1, and ≤ 1/2 when xⱼ = 0. They construct three-part graphs where the "result vertex" has degree depending on 4ʲ, while other vertices have degrees designed to make their sine outputs zero. This allows the readout to distinguish xⱼ values across multiple graphs.
- Core assumption: The sine function's periodicity and the ability to control its argument through parameter and degree choices can reliably extract individual bits.
- Evidence anchors:
  - [abstract]: "Furthermore, we show that this also holds for GNNs using analytic non-polynomial activation functions, including the 1-dimensional GNNs that were recently shown to be as expressive as the 1-WL test"
  - [section]: "We prove a similar result for GNNs restricted to analytic non-polynomial activation functions"
  - [corpus]: Weak evidence - corpus contains related papers but none specifically validating this sine-based bit extraction mechanism
- Break condition: If the sine function cannot reliably distinguish between the required bit values due to numerical precision or if the degree construction fails to create the necessary output conditions, the shattering argument fails.

### Mechanism 3
- Claim: Increasing width and depth beyond 1 does not improve VC dimension for unbounded graphs
- Mechanism: The authors provide an alternative proof showing that even GNNs of width and depth 1 have infinite VC dimension, which is stronger than previous results requiring width and depth ≥ 2. This demonstrates that the fundamental limitation is not about model complexity but about the graph structure and activation functions. The same graph construction techniques work regardless of whether additional layers are added, as long as the final layer performs the critical bit extraction.
- Core assumption: The infinite VC dimension is an inherent property of GNNs on unbounded graphs, not dependent on model complexity beyond what's needed for bit extraction.
- Evidence anchors:
  - [abstract]: "These results suggest inherent limitations in the generalization ability of even the most simple GNNs, when viewed from the VC dimension perspective"
  - [section]: "Furthermore, we do so using 1-dimensional GNNs and just a single parameter, making this result applicable to the class of GNNs considered by Bravo et al. [2024]"
  - [corpus]: Weak evidence - corpus contains related papers but none specifically validating that increased complexity doesn't improve VC dimension
- Break condition: If additional layers or width could somehow constrain the function class to have finite VC dimension (contrary to the authors' construction), or if the bit extraction could be made more efficient with additional parameters, the claim would be weakened.

## Foundational Learning

- Concept: Vapnik-Chervonenkis (VC) dimension
  - Why needed here: VC dimension measures the capacity of a model class to shatter sets of points, providing a theoretical bound on generalization ability. Understanding VC dimension is crucial for interpreting the paper's main result about GNNs having infinite VC dimension.
  - Quick check question: If a model class can shatter sets of size 3 but not size 4, what is its VC dimension?

- Concept: Graph Neural Networks (GNN) architecture
  - Why needed here: The paper analyzes specific GNN architectures (1-dimensional, width 1, depth 1) and their generalization properties. Understanding how GNNs aggregate information and compute graph invariants is essential for following the proofs.
  - Quick check question: In a standard GNN with aggregation, what operation combines information from neighboring nodes?

- Concept: Weisfeiler-Leman (WL) isomorphism test
  - Why needed here: The paper relates GNN expressiveness to the WL test, which is a classical graph isomorphism heuristic. Understanding this connection helps contextualize why GNNs with certain activation functions can simulate the WL test while still having poor generalization.
  - Quick check question: What is the key operation in the 1-WL algorithm that distinguishes non-isomorphic graphs?

## Architecture Onboarding

- Component map:
  Input Graph -> Initial Features (h₀ᵛ = 1) -> Layer Computation (h₁ᵛ = f((1 + deg(v))γₓ)) -> Readout (ξₓ(G) = Σᵥ∈ᵥ h₁ᵛ) -> Output Graph Embedding

- Critical path:
  1. Graph construction with carefully chosen vertex degrees
  2. Parameter assignment based on desired binary pattern
  3. Layer computation using activation function to extract bits
  4. Readout summation to produce final graph embedding
  5. Thresholding to convert to binary classification

- Design tradeoffs:
  - Simplicity vs. expressiveness: 1-dimensional GNNs are simple but can simulate WL test
  - Parameter efficiency: Single parameter can shatter many graphs but requires complex graph construction
  - Activation function choice: Piecewise linear vs. analytic non-polynomial affects bit extraction mechanism
  - Graph size vs. shattering capacity: Larger graphs needed for more bits, but still bounded by bit length

- Failure signatures:
  - Incorrect vertex degree assignments leading to ambiguous bit extraction
  - Activation function not providing sufficient separation between bit values
  - Parameter values not matching the required binary pattern
  - Readout not properly summing the extracted bits
  - Graph construction not creating the required three-part structure

- First 3 experiments:
  1. Implement the piecewise linear activation function and verify it produces the required output regions for bit extraction with different parameter values
  2. Construct the three-part graphs (result vertex, regular graph, regular graph) and verify vertex degrees match the specification for small values of b
  3. Run the GNN on constructed graphs and verify that ξₓ(Gⱼ) correctly equals xⱼ for all j, demonstrating the shattering property for b=2 or b=3

## Open Questions the Paper Calls Out
None

## Limitations
- Results apply specifically to "unbounded graphs" and demonstrate worst-case scenarios rather than typical performance
- Carefully engineered graph constructions may not reflect typical real-world graph data distributions
- The theoretical results may not directly translate to practical performance limitations

## Confidence
- Main claim (infinite VC dimension): Medium confidence
- Extension to analytic non-polynomial activation functions: Medium-Low confidence
- Claim about complexity not improving VC dimension: High confidence

## Next Checks
1. Implement the graph construction algorithms and verify the shattering property experimentally for small values of b (e.g., b=2,3,4)
2. Test the sine-based bit extraction mechanism across a range of parameter values to ensure numerical stability and reliable bit discrimination
3. Evaluate GNN performance on real-world graph datasets to determine whether the infinite VC dimension translates to poor generalization in practice, comparing against theoretical predictions