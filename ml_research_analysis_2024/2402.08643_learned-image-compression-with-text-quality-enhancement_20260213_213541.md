---
ver: rpa2
title: Learned Image Compression with Text Quality Enhancement
arxiv_id: '2402.08643'
source_url: https://arxiv.org/abs/2402.08643
tags:
- text
- loss
- image
- compression
- logit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of text quality degradation in
  learned image compression, particularly for screen content images (SCIs). The authors
  propose a novel text logit loss function that leverages Optical Character Recognition
  (OCR) and scene text recognition models to quantify the disparity in text between
  the original and reconstructed images.
---

# Learned Image Compression with Text Quality Enhancement

## Quick Facts
- arXiv ID: 2402.08643
- Source URL: https://arxiv.org/abs/2402.08643
- Authors: Chih-Yu Lai, Dung Tran, Kazuhito Koishida
- Reference count: 0
- Primary result: Achieves -32.64% BD-CER and -28.03% BD-WER on average for text quality in image compression

## Executive Summary
This paper addresses text quality degradation in learned image compression, particularly for screen content images. The authors propose a novel text logit loss function that leverages OCR and scene text recognition models to quantify the disparity in text between original and reconstructed images. This approach significantly improves perceptual text quality while maintaining compression efficiency, with improvements being more pronounced at lower bitrates.

## Method Summary
The method introduces a text logit loss term to the total loss function of learned image compression models. During training, text regions are detected in the original image using CRAFT, then corresponding regions are cropped from both original and reconstructed images. These regions are processed through a pre-trained text recognition model (PARSeq) to obtain character-based logits, and the mean squared difference between these logits forms the text logit loss. This loss is weighted by κ and added to the standard rate and distortion losses, allowing the model to optimize for both compression efficiency and text quality simultaneously.

## Key Results
- Achieves -32.64% BD-CER and -28.03% BD-WER on average across two screenshot datasets
- Method effective across five state-of-the-art compression algorithms (Ballé 2018, Cheng 2020, He 2022, Qian 2022, Xie 2021)
- Improvements more pronounced at lower bitrates, demonstrating value for ultra-low bitrate compression
- Introduces BD-CER and BD-WER metrics for quantitative assessment of text quality in compression tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The text logit loss improves reconstructed text quality by directly optimizing character-based logits during training.
- Mechanism: The loss measures the dissimilarity between logits of recognized characters in original and reconstructed images, encouraging the model to preserve text information even at low bitrates.
- Core assumption: The character recognition model (PARSeq) provides stable and meaningful logit outputs for both original and reconstructed images.
- Evidence anchors:
  - [abstract] "we propose to minimize a novel text logit loss designed to quantify the disparity in text between the original and reconstructed images"
  - [section 2.1] "We then obtain the text logit loss by calculating the mean squared value of the text logit difference between x and ˆx"
  - [corpus] Weak evidence - no direct comparison with other logit-based losses
- Break condition: Recognition model fails to provide consistent logits for compressed text regions, or the difference between original and compressed text logits becomes too large for the loss to be meaningful.

### Mechanism 2
- Claim: Using bounding boxes from the original image ensures consistent comparison between text regions in original and reconstructed images.
- Mechanism: The TextRegion function extracts text regions from the original image, and these same regions are cropped from the reconstructed image for comparison, avoiding issues with text region detection on compressed outputs.
- Core assumption: Text regions detected in the original image remain valid and meaningful in the reconstructed image, even with compression artifacts.
- Evidence anchors:
  - [section 2.1] "This approach ensures that the same regions between x and ˆx are compared"
  - [section 2.2] "Since T(x, ˆx) is calculated solely from the cropped text regions, its direct impact is confined to the quality of text reconstruction in those specific regions"
  - [corpus] No direct evidence on alternative region comparison methods
- Break condition: Severe compression artifacts cause text regions to become unrecognizable, making the comparison meaningless.

### Mechanism 3
- Claim: The text logit loss acts as a parameter-free loss function that guides training without introducing additional degrees of freedom.
- Mechanism: Because T(x, ˆx) is calculated using pre-trained OCR and STR models, it doesn't require learning additional parameters, allowing the model to focus on optimizing compression while maintaining text quality.
- Core assumption: The pre-trained recognition models provide accurate and stable outputs across different compression levels and image qualities.
- Evidence anchors:
  - [section 2.2] "If TextRegion(·) and Recognize(·) are fixed, T(x, ˆx) serves as a parameter-free loss function"
  - [section 2.1] "we utilize the PARSeq Scene Text Recognition (STR) model [17] that is pre-trained for recognizing English text"
  - [corpus] No evidence comparing with learned recognition models during training
- Break condition: The recognition models fail to maintain consistent performance across the range of compression levels, or the fixed models become a bottleneck in optimization.

## Foundational Learning

- Concept: Character Error Rate (CER) and Word Error Rate (WER)
  - Why needed here: These metrics quantify the accuracy of reconstructed text, providing a way to measure the effectiveness of the text logit loss
  - Quick check question: How is CER calculated when there are deletions, insertions, and substitutions in the recognized text?

- Concept: Bjontegaard Delta (BD) Rate
  - Why needed here: BD-CER and BD-WER metrics measure the average reduction in error rates at the same bitrate, allowing comparison of compression methods
  - Quick check question: What does a negative BD-CER value indicate about the performance of the text logit loss compared to the reference method?

- Concept: Entropy-based image compression
  - Why needed here: The proposed method builds on entropy-based learned image compression models, requiring understanding of how rate and distortion losses work
  - Quick check question: How do the rate loss (R) and distortion loss (D) components typically balance in entropy-based image compression?

## Architecture Onboarding

- Component map: CRAFT text detection -> PARSeq STR recognition -> Text logit loss calculation -> Total loss integration -> Compression model optimization

- Critical path:
  1. Detect text regions in original image
  2. Crop corresponding regions from original and reconstructed images
  3. Recognize text and obtain logits
  4. Calculate text logit loss
  5. Add weighted loss to total loss function
  6. Backpropagate through compression model

- Design tradeoffs:
  - Using fixed recognition models vs. learning recognition during training
  - Computing text logit loss during training vs. pre-computing for efficiency
  - Balancing text quality improvement against overall compression performance

- Failure signatures:
  - CER/WER improving while PSNR deteriorates significantly
  - Model training becoming unstable with high κ values
  - Text quality improvement only visible at specific bitrate ranges

- First 3 experiments:
  1. Implement text logit loss with κ=0.1 on Ballé 2018 model and compare CER/WER with baseline
  2. Test different κ values (0.01, 0.1, 1.0) to find optimal balance
  3. Evaluate on both lossless and lossy compressed datasets to test generalization

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the text logit loss generalize to languages other than English, given that the recognition model used in the paper is pre-trained on English text?
- Basis in paper: [explicit] The paper mentions that "using any recognition model that is pre-trained on other languages will also work" but does not explore this.
- Why unresolved: The paper only evaluates the method using an English text recognition model, leaving the performance on other languages untested.
- What evidence would resolve it: Testing the method with recognition models pre-trained on other languages and comparing the results to those obtained with the English model.

### Open Question 2
- Question: What is the optimal balance between the text logit loss weight (κ) and the distortion loss weight (λ) for maximizing text quality without significantly compromising overall image quality?
- Basis in paper: [explicit] The paper discusses the effect of κ on text quality but notes that "we need to strike a balance between distortion, rate, and text loss" and that the balancing weights are "algorithm-specific to some extent."
- Why unresolved: The paper does not provide a systematic approach to finding the optimal balance between κ and λ for different algorithms and datasets.
- What evidence would resolve it: A comprehensive study that tests various combinations of κ and λ across different algorithms and datasets, identifying the optimal balance for each case.

### Open Question 3
- Question: How does the proposed text logit loss perform on images with text in multiple languages or mixed scripts, which are common in real-world scenarios?
- Basis in paper: [inferred] The paper focuses on English text and does not address the challenge of multi-language or mixed-script text.
- Why unresolved: The paper does not explore the performance of the text logit loss on images containing text in multiple languages or mixed scripts.
- What evidence would resolve it: Evaluating the method on a dataset containing images with text in multiple languages or mixed scripts and comparing the results to those obtained on single-language text images.

## Limitations
- The method's effectiveness depends heavily on the stability and accuracy of pre-trained OCR and STR models across different compression levels
- The optimal weight κ = 0.1 appears somewhat arbitrary without systematic sensitivity analysis
- The paper does not explore how the method performs on text in languages other than English or on mixed-script scenarios

## Confidence
**High confidence**: The fundamental mechanism of using logit differences between original and reconstructed text regions is technically sound and well-defined.

**Medium confidence**: The generalizability of results across different datasets and compression algorithms is supported but could be stronger.

**Low confidence**: The optimal weight κ = 0.1 appears somewhat arbitrary without systematic sensitivity analysis.

## Next Checks
1. **Recognition Model Stability Test**: Evaluate the consistency of CER/WER improvements when the text regions are compressed to different levels by creating controlled tests with independent compression of text regions at various quality factors.

2. **Cross-Dataset Generalization**: Test the method on a third, diverse screen content dataset containing different fonts, languages, or UI elements to validate that improvements are not specific to the two tested datasets.

3. **Ablation Study on Loss Components**: Perform systematic ablation by removing the text logit loss and measuring degradation in text quality, then incrementally adding it back with different κ values to quantify exact contribution and identify optimal balance point.