---
ver: rpa2
title: 'WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural
  Visual Question Answering on Global Cuisines'
arxiv_id: '2410.12705'
source_url: https://arxiv.org/abs/2410.12705
tags:
- dish
- language
- question
- food
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces WORLD CUISINES, a massive-scale benchmark
  for evaluating multilingual and multicultural visual question answering (VQA) on
  global cuisines. It addresses the challenge that vision language models (VLMs) often
  struggle with culture-specific knowledge, particularly in underrepresented languages
  and cultural contexts.
---

# WorldCuisines: A Massive-Scale Benchmark for Multilingual and Multicultural Visual Question Answering on Global Cuisines

## Quick Facts
- arXiv ID: 2410.12705
- Source URL: https://arxiv.org/abs/2410.12705
- Authors: 50 researchers including Genta Indra Winata, Frederikus Hudi, Patrick Amadeus Irawan, and others
- Reference count: 21
- Key outcome: Introduced a benchmark with over 1 million text-image pairs across 30 languages/dialects, 9 language families, showing that VLMs perform better with correct location context but struggle with adversarial contexts and predicting specific regional cuisines.

## Executive Summary
WORLD CUISINES is a massive-scale benchmark designed to evaluate multilingual and multicultural visual question answering (VQA) on global cuisines. The benchmark addresses the challenge that vision language models often struggle with culture-specific knowledge, particularly in underrepresented languages and cultural contexts. It includes over 1 million text-image pairs across 30 languages and dialects spanning 9 language families, with tasks for identifying dish names and their origins. The benchmark provides evaluation datasets in two sizes (12k and 60k instances) alongside a training dataset (1 million instances), along with a comprehensive knowledge base containing 2,414 dishes with images and metadata.

## Method Summary
The WORLD CUISINES benchmark consists of two main components: WC-VQA, a VQA dataset with 1 million text-image pairs across 30 languages/dialects, and WC-KB, a knowledge base with 2,414 dishes, 6,045 images, and metadata. The VQA dataset includes two tasks: dish name prediction (with no-context, contextualized, and adversarial contexts) and location prediction, with multiple-choice and open-ended question formats. Evaluation uses accuracy metrics for MCQ and BERTScore for OEQ. The benchmark provides code and datasets on Hugging Face for reproduction.

## Key Results
- VLMs perform significantly better with correct location context compared to no-context settings
- Models struggle with adversarial contexts, showing vulnerability to misleading information
- Larger models generally perform better, demonstrating scaling laws in multilingual VQA
- Performance varies significantly across language families and regions
- Open-ended questions are more challenging than multiple-choice questions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Context improves VQA accuracy when the context is relevant to the dish being queried
- Mechanism: Providing culturally relevant location or cuisine context helps align the model's attention with the correct dish, reducing ambiguity between similar dishes from different cultures
- Core assumption: The model can leverage contextual cues to disambiguate between culturally similar dishes
- Evidence anchors: Abstract states VLMs perform better with correct location context; section shows incorporating relevant context significantly enhances performance
- Break condition: If the context provided is adversarial or misleading, the model's performance drops significantly

### Mechanism 2
- Claim: Multilingual translation and cultural knowledge base improve model understanding of diverse cuisines
- Mechanism: Translating questions and context into 30 languages and providing a comprehensive knowledge base with dish names, descriptions, and cultural metadata helps models understand regional variations
- Core assumption: Models can effectively use multilingual information and cultural metadata to improve reasoning about food-related VQA tasks
- Evidence anchors: Abstract mentions knowledge base with annotated food entries; section highlights cultural diversity and broader language coverage
- Break condition: If translations are inaccurate or knowledge base lacks coverage of certain regions, performance in those languages/regions will suffer

### Mechanism 3
- Claim: Scaling model size improves VQA performance across different tasks and languages
- Mechanism: Larger models have more parameters and capacity to learn complex relationships between visual features and cultural-linguistic patterns
- Core assumption: Model performance scales with parameter count in a predictable way across multilingual and multicultural tasks
- Evidence anchors: Abstract states VLMs perform better with correct location context; section shows larger models perform better demonstrating scaling laws
- Break condition: If model size becomes too large relative to training data, performance may plateau or degrade due to overfitting

## Foundational Learning

- Concept: Vision-Language Models (VLMs) architecture
  - Why needed here: Understanding how VLMs process and integrate visual and textual information is crucial for interpreting benchmark results
  - Quick check question: What are the key components of a typical VLM architecture and how do they interact during inference?

- Concept: Multilingual text processing and translation
  - Why needed here: The benchmark involves 30 languages and dialects, requiring understanding of multilingual challenges like script variations and cultural nuances
  - Quick check question: What are the main challenges in translating culinary terms across languages with different scripts and cultural contexts?

- Concept: Visual question answering (VQA) task formulation
  - Why needed here: The benchmark evaluates models on two VQA tasks with different question types, requiring understanding of task-specific evaluation metrics
  - Quick check question: How do multiple-choice and open-ended question formats differ in terms of evaluation difficulty and model performance expectations?

## Architecture Onboarding

- Component map: Knowledge Base (WC-KB) -> VQA Dataset (WC-VQA) -> Evaluation Framework -> Model Interface

- Critical path:
  1. Load dish information from WC-KB
  2. Generate VQA pairs using similarity search and template-based question generation
  3. Translate questions/context into 30 languages
  4. Run inference on target VLMs
  5. Evaluate predictions using accuracy and BERTScore

- Design tradeoffs:
  - Coverage vs. Quality: Including more languages increases diversity but may reduce translation quality for low-resource languages
  - Size vs. Accessibility: Large datasets provide better evaluation but require significant computational resources
  - Context vs. Ambiguity: Providing contextual information helps disambiguation but may introduce bias if context is misleading

- Failure signatures:
  - Low accuracy on OEQ tasks suggests issues with text generation capabilities rather than cultural understanding
  - Poor performance on non-Latin scripts indicates problems with multilingual text processing
  - High variance in MCQ accuracy suggests inconsistent model reasoning across similar dishes

- First 3 experiments:
  1. Test a baseline VLM on the no-context MCQ task to establish baseline performance
  2. Evaluate the same model with contextualized questions to measure context impact
  3. Run adversarial context experiments to assess model robustness to misleading information

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the inclusion of additional languages and dialects in WORLD CUISINES affect the performance of VLMs on multilingual visual question answering tasks?
- Basis in paper: The paper mentions plans to extend the benchmark to encompass additional languages in the future
- Why unresolved: The paper evaluates VLMs on 30 languages and dialects but does not explore the impact of adding more languages
- What evidence would resolve it: Conducting experiments with the benchmark extended to include additional languages and dialects, and comparing performance with original results

### Open Question 2
- Question: What are the specific challenges and limitations of using Wikipedia as a data source for constructing a knowledge base of global cuisines?
- Basis in paper: The paper acknowledges that food entries are currently sourced from English Wikipedia, which may limit coverage of some regions
- Why unresolved: The paper does not provide a detailed analysis of challenges and limitations associated with using Wikipedia as a data source
- What evidence would resolve it: Conducting a comprehensive analysis of Wikipedia data used, identifying specific biases or gaps, and proposing solutions

### Open Question 3
- Question: How effective are the current evaluation metrics, such as accuracy and BERTScore, in assessing the cultural understanding of VLMs in the context of multilingual and multicultural visual question answering?
- Basis in paper: The paper uses accuracy and BERTScore as evaluation metrics but acknowledges that evaluating VQA model performance on multicultural data remains an open challenge
- Why unresolved: The paper does not provide a detailed analysis of the strengths and weaknesses of current evaluation metrics in capturing cultural understanding nuances
- What evidence would resolve it: Conducting a comparative study of different evaluation metrics to assess their effectiveness in capturing cultural understanding in multilingual and multicultural VQA tasks

## Limitations
- Data Representation Bias: The benchmark may exhibit systematic coverage gaps for certain regions and languages, particularly in the knowledge base which contains 2,414 dishes
- Translation Quality Variability: Translation quality across 30 languages and dialects may vary significantly, introducing uncertainty about whether performance differences reflect model capabilities or translation quality issues
- Context Dependency Overfitting: Strong performance improvement with correct contextual information raises concerns about whether models are truly learning cross-cultural food recognition or simply memorizing location-dish associations

## Confidence
- High Confidence: The benchmark successfully provides a large-scale, multilingual dataset for VQA evaluation, and the core observation that context improves model performance is well-supported
- Medium Confidence: Claims about model performance scaling with size and the effectiveness of the knowledge base are reasonable but would benefit from more rigorous ablation studies
- Low Confidence: The assertion that models struggle specifically with "culture-specific knowledge" lacks sufficient evidence, as performance differences could stem from various factors

## Next Checks
1. **Ablation Study on Knowledge Base Components**: Remove the knowledge base component and evaluate model performance to determine whether it genuinely improves cultural understanding or if models can perform well using only visual and textual cues
2. **Translation Quality Assessment**: Conduct blind evaluation of translations by multiple native speakers to quantify translation quality variance across language families, then correlate this with model performance
3. **Adversarial Context Robustness Testing**: Systematically vary the type and degree of adversarial context to measure model robustness and determine whether failures stem from context over-reliance or fundamental cultural knowledge gaps