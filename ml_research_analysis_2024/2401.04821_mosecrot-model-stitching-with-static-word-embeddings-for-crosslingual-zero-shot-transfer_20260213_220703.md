---
ver: rpa2
title: 'MoSECroT: Model Stitching with Static Word Embeddings for Crosslingual Zero-shot
  Transfer'
arxiv_id: '2401.04821'
source_url: https://arxiv.org/abs/2401.04821
tags:
- language
- languages
- embeddings
- target
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the problem of crosslingual zero-shot transfer
  for low-resource languages by introducing MoSECroT (Model Stitching with Static
  Word Embeddings for Crosslingual Zero-shot Transfer). The core method idea is to
  leverage relative representations to construct a common space for the embeddings
  of a source language pre-trained language model (PLM) and the static word embeddings
  of a target language.
---

# MoSECroT: Model Stitching with Static Word Embeddings for Crosslingual Zero-shot Transfer

## Quick Facts
- **arXiv ID**: 2401.04821
- **Source URL**: https://arxiv.org/abs/2401.04821
- **Reference count**: 14
- **Primary result**: Proposes MoSECroT framework for crosslingual zero-shot transfer using relative representations, but fails to outperform strong baselines like mBERT

## Executive Summary
This paper introduces MoSECroT, a novel framework for crosslingual zero-shot transfer that leverages relative representations to construct a common embedding space between a source language PLM and target language static word embeddings. The method enables zero-shot transfer by simply swapping the embedding layer after fine-tuning on source data. Experiments on two classification datasets show that while the proposed framework is competitive with weak baselines, it fails to achieve competitive results compared to strong baselines like mBERT. The authors provide a detailed analysis of this negative result and suggest possible improvements.

## Method Summary
MoSECroT addresses crosslingual zero-shot transfer by mapping source language PLM embeddings to target language static embeddings using relative representations. The method extracts parallel anchors between languages, computes cosine similarities between word embeddings and anchors to create relative representations, and then maps these back to the source PLM's embedding dimension. The source model is fine-tuned on source language data, and zero-shot transfer is achieved by swapping the embedding layer with the transformed target embeddings. The framework includes variants using different weighting schemes (standard, softmax, sparsemax) and anchor selection strategies.

## Key Results
- The proposed framework fails to outperform strong baselines like mBERT on high-resource languages
- On low-resource languages not covered by mBERT, relative representations consistently outperform mBERT
- Simple baseline projection methods (like least squares) are almost always beaten by the proposed method under multiple RR settings
- The method shows competitive performance against weak baselines but struggles against strong baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed method achieves competitive results on low-resource languages not covered by mBERT.
- Mechanism: Relative representations (RRs) align source and target language embeddings by using parallel anchors to construct a common semantic space. This allows zero-shot transfer by swapping embedding layers after fine-tuning on source data.
- Core assumption: Parallel anchors between source and target languages are of sufficient quality and quantity to enable meaningful alignment of embedding spaces.
- Evidence anchors:
  - [abstract] "we present the first framework that leverages relative representations to construct a common space for the embeddings of a source language PLM and the static word embeddings of a target language."
  - [section] "We propose a novel method that leverages relative representations for embedding space mapping."
- Break condition: If parallel anchors are of poor quality or insufficient quantity, the alignment will fail and transfer performance will degrade.

### Mechanism 2
- Claim: Relative representations capture semantic similarity better than simple projection methods.
- Mechanism: RRs transform absolute embeddings into similarity-based representations relative to parallel anchors, capturing semantic relationships that simple linear projection (like least squares) might miss.
- Core assumption: Semantic similarity encoded in different types of embeddings (PLMs vs static) can be better leveraged through relative coordinates than absolute coordinate alignment.
- Evidence anchors:
  - [section] "Although none of the RR settings outperforms mBERT on high-resource languages... for all five low-resource languages not seen by mBERT (mt, sah, fo, nds, tt), RRs outperform mBERT consistently."
  - [section] "We notice that the naive LS baseline is almost always beaten by the proposed method under multiple RR settings on both datasets."
- Break condition: If the semantic similarity structure is not preserved across embedding types, RRs will not provide an advantage over simpler methods.

### Mechanism 3
- Claim: Zero-shot transfer is possible without target language training data by leveraging model stitching.
- Mechanism: After aligning embedding spaces through RRs, the source language model can be fine-tuned on source language data and then deployed on target language data by simply swapping the embedding layer.
- Core assumption: The rest of the model architecture (Transformer body) can generalize to target language inputs after being trained on source language data and having its embeddings swapped.
- Evidence anchors:
  - [abstract] "we can train the PLM on source-language training data and perform zero-shot transfer to the target language by simply swapping the embedding layer."
  - [section] "We can simply fine-tune the model (F_s and the Transformer body) on the source-language train set of a downstream task and then assemble a target-language model for zero-shot transfer, without training on the target language."
- Break condition: If the model architecture cannot adapt to the new embedding space after reinitialization, zero-shot transfer will fail.

## Foundational Learning

- Concept: Parallel anchors
  - Why needed here: Parallel anchors provide the bridge between source and target language embedding spaces, enabling alignment through relative representations.
  - Quick check question: How are parallel anchors selected and what happens if they contain noisy translations?

- Concept: Relative representations
  - Why needed here: RRs transform absolute word embeddings into similarity-based coordinates relative to anchors, capturing semantic relationships that enable cross-lingual alignment.
  - Quick check question: What is the mathematical definition of a relative representation and how does it differ from absolute embeddings?

- Concept: Model stitching
  - Why needed here: Model stitching allows the pre-trained source language model to be adapted for target language use without additional training on target data.
  - Quick check question: What are the key assumptions behind model stitching and when might it fail?

## Architecture Onboarding

- Component map: Parallel anchor extraction and filtering -> Relative representation computation and embedding mapping -> Model fine-tuning and zero-shot transfer through embedding swapping

- Critical path: 1. Extract parallel anchors between source and target languages, 2. Compute relative representations for both languages, 3. Map target language RRs to source language embedding space dimension, 4. Fine-tune source model on source language data, 5. Swap embedding layers for zero-shot transfer to target language

- Design tradeoffs: The method trades off the need for expensive multilingual model training against the requirement for high-quality parallel anchors and the assumption that relative representations will provide better alignment than simpler projection methods

- Failure signatures: Poor zero-shot transfer performance, especially on low-resource languages; failure to outperform simple baselines like logistic regression; inconsistent results across different target languages

- First 3 experiments:
  1. Verify parallel anchor quality by manually inspecting a sample of anchor pairs and their translations
  2. Test relative representation computation by comparing RR-based alignment to baseline projection methods on a small validation set
  3. Validate zero-shot transfer by performing ablation studies with different numbers of parallel anchors and different weighting schemes (softmax vs sparsemax)

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but the discussion section raises several important considerations for future work, including the need for better parallel anchor selection methods, exploration of different dimensionalities for relative representations, and evaluation on task types beyond text classification.

## Limitations
- Evaluation scope is limited to two classification datasets and nine target languages
- Performance on high-resource languages consistently lags behind strong baselines like mBERT
- Reliance on parallel anchor quality is a critical bottleneck without systematic evaluation
- Domain mismatch between static embeddings and PLM embeddings is not addressed

## Confidence

**High Confidence**: The experimental methodology is sound and the negative results are well-documented. The comparison against baselines is comprehensive, including both simple (logistic regression) and strong (mBERT) baselines.

**Medium Confidence**: The proposed method's theoretical foundation is reasonable, but the empirical validation is limited to two datasets and nine target languages. The explanation for why the method underperforms mBERT is plausible but not definitively proven.

**Low Confidence**: The paper's claims about the method's potential for low-resource languages not covered by mBERT are based on theoretical reasoning rather than extensive empirical validation. The suggestion that the method "might work better with other tasks or datasets" is speculative without supporting evidence.

## Next Checks
1. **Anchor Quality Analysis**: Conduct a systematic evaluation of parallel anchor quality by manually inspecting anchor pairs for all target languages, measuring translation accuracy, and testing performance sensitivity to anchor set size and quality thresholds.

2. **Cross-Task Generalization**: Evaluate the method on additional task types beyond classification (e.g., sequence labeling, question answering) using established benchmarks like XTREME to assess whether the negative results are task-specific or method-specific.

3. **Ablation on Alignment Method**: Perform controlled experiments comparing relative representations against simpler alignment methods (like direct projection) while keeping all other factors constant, to definitively establish whether the added complexity of RRs provides any benefit over simpler approaches.