---
ver: rpa2
title: Entropic Hetero-Associative Memory
arxiv_id: '2411.02438'
source_url: https://arxiv.org/abs/2411.02438
tags:
- memory
- objects
- mnist
- such
- relation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces the Entropic Hetero-Associative Memory (EHAM),
  an extension of the Entropic Associative Memory (EAM) model to support hetero-associative
  recall, where cues and targets belong to different domains (e.g., digits to letters).
  The memory stores pairs of objects in a 4D weighted relation and retrieves the target
  object by selecting an indeterminate 2D memory plane specific to the cue.
---

# Entropic Hetero-Associative Memory

## Quick Facts
- arXiv ID: 2411.02438
- Source URL: https://arxiv.org/abs/2411.02438
- Authors: Rafael Morales; Luis A. Pineda
- Reference count: 20
- Primary result: EHAM achieves 65-70% recognition precision/recall and up to 60% retrieval precision/recall on digit-letter hetero-associative tasks

## Executive Summary
This paper introduces the Entropic Hetero-Associative Memory (EHAM), an extension of the Entropic Associative Memory (EAM) model that supports hetero-associative recall between different domains. The memory stores pairs of objects (e.g., digits and letters) in a 4D weighted relation and retrieves target objects by selecting an indeterminate 2D memory plane specific to the cue. Three incremental methods—random sampling, sample-and-test, and sample-and-search—address the missing cue problem in the target domain. Tested with MNIST digits and EMNIST letters, the model demonstrates recognition precision/recall around 65-70% and retrieval precision/recall up to 60% with the sample-and-search method, showing promise for large-scale storage and retrieval using limited computing resources.

## Method Summary
EHAM extends EAM to hetero-associative recall by storing pairs of objects from different domains in a 4D weighted relation h ⊂ A × B × V × Z. Retrieval begins by selecting a 2D memory plane specific to the input cue from the source domain, then reconstructing the target object from that plane. Since no cue exists for the target plane, three incremental methods are proposed: random sampling from the target plane, sample-and-test by comparing the backward relation to the original cue, and sample-and-search with local optimization to minimize distance to the original cue. The model was tested using 10-fold cross-validation on paired digit-letter associations from MNIST and EMNIST datasets.

## Key Results
- Recognition precision/recall: 65-70% on hetero-associated digit-letter pairs
- Retrieval precision/recall: up to 60% with sample-and-search method
- Sample-and-search outperforms random sampling and sample-and-test methods
- Model demonstrates viability for large-scale storage and retrieval with limited resources

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The memory stores hetero-associated pairs in a 4D relation and uses cue-based selection to retrieve the target object.
- Mechanism: Objects are encoded as functions and stored in a weighted relation h ⊂ A × B × V × Z. Retrieval begins by selecting a 2D memory plane specific to the cue from the source domain, then reconstructing the target object from that plane.
- Core assumption: The 4D weighted relation preserves the associations between cue and target objects, and a cue in one domain uniquely determines a relevant 2D plane in the other domain.
- Evidence anchors:
  - [abstract] "Pairs of hetero-associated objects, possibly of different domain and/or modalities, are held in a 4D relation."
  - [section] "The memory retrieval operation selects a largely indeterminate 2D memory plane that is specific to the input cue."
  - [corpus] Weak evidence - no direct mention of 4D relation or hetero-associative retrieval in neighbor papers.
- Break condition: If the 4D relation does not preserve associations, or if cues from one domain do not uniquely select a 2D plane in the other domain, retrieval will fail.

### Mechanism 2
- Claim: Three incremental methods (random, sample-and-test, sample-and-search) address the missing cue problem in the target domain.
- Mechanism: Since there is no cue available in the target domain to complete the retrieval, the model proposes: 1) random selection from the target plane, 2) sampling and testing by comparing the backward relation to the original cue, and 3) sampling, testing, and local search to minimize the distance to the original cue.
- Core assumption: The target 2D plane contains the correct object, but additional selection criteria are needed to identify it.
- Evidence anchors:
  - [abstract] "However, since no cue exists for the target plane, three incremental methods—random sampling, sample-and-test, and sample-and-search—are proposed to address the missing cue problem."
  - [section] "The memory retrieval operation selects a largely indeterminate 2D memory plane that is specific to the input cue; however, there is no cue left to retrieve an object from such latter plane."
  - [corpus] Weak evidence - neighbor papers do not discuss the missing cue problem or these specific methods.
- Break condition: If the target plane does not contain the correct object, or if the selection criteria do not effectively identify it, retrieval performance will degrade.

### Mechanism 3
- Claim: The entropy of the memory relation reflects the indeterminacy of stored objects and influences retrieval performance.
- Mechanism: The memory's entropy is calculated as the average entropy of all objects in the domain, using Shannon's entropy formula. This entropy value indicates the level of indeterminacy in the memory, which affects the effectiveness of the retrieval operation.
- Core assumption: Higher entropy corresponds to higher indeterminacy, which impacts the ability to retrieve the correct object.
- Evidence anchors:
  - [abstract] "Stored objects are 'overlapped' on the medium, hence the memory is indeterminate and has an entropy value at each state."
  - [section] "The entropy of the whole relation h is the average entropy of all objects in the domain A × B."
  - [corpus] No direct evidence in neighbor papers about entropy and memory indeterminacy.
- Break condition: If the entropy calculation does not accurately reflect the indeterminacy of stored objects, or if retrieval performance is not correlated with entropy, this mechanism will not hold.

## Foundational Learning

- Concept: Hebb's learning rule
  - Why needed here: The memory stores objects by reinforcing simultaneously the cells used by the cue, which is a form of Hebb's learning rule.
  - Quick check question: How does the model reinforce the cells used by the cue during storage?

- Concept: Distributed representations
  - Why needed here: Objects are "overlapped" on the memory medium, making the representation distributed and the memory indeterminate.
  - Quick check question: What is the consequence of having distributed representations for memory retrieval?

- Concept: Entropy and indeterminacy
  - Why needed here: The memory's entropy reflects the indeterminacy of stored objects, which influences retrieval performance.
  - Quick check question: How is the entropy of the memory relation calculated, and what does it indicate?

## Architecture Onboarding

- Component map:
  - 4D weighted relation h ⊂ A × B × V × Z for storing hetero-associated pairs
  - 2D memory planes for cue-based selection
  - Three incremental methods (random, sample-and-test, sample-and-search) for addressing the missing cue problem
  - Entropy calculation for assessing memory indeterminacy

- Critical path:
  1. Store hetero-associated pairs in the 4D relation
  2. Select a 2D memory plane based on the cue from the source domain
  3. Apply one of the three methods to identify the target object from the selected plane
  4. Calculate the entropy of the memory relation to assess indeterminacy

- Design tradeoffs:
  - Increased memory capacity and distributed representations vs. higher indeterminacy and more complex retrieval
  - Choice of method for addressing the missing cue problem (random, sample-and-test, sample-and-search) based on desired precision and computational resources

- Failure signatures:
  - Low recognition precision and recall indicate the system is not within its operational range
  - Poor retrieval performance suggests the target plane does not contain the correct object or the selection criteria are ineffective

- First 3 experiments:
  1. Test the recognition performance using both correct and incorrect associations as testing corpus
  2. Evaluate the retrieval performance using the three incremental methods (random, sample-and-test, sample-and-search)
  3. Analyze the relationship between the memory's entropy and the retrieval performance to assess the impact of indeterminacy

## Open Questions the Paper Calls Out
None

## Limitations
- Entropy-based memory indeterminacy mechanism lacks direct evidence from the literature
- Sample-and-search local optimization algorithm details are not fully specified
- Conversion functions between image representations and abstract functions are not clearly defined

## Confidence
- High confidence in the basic architecture and retrieval methods (random sampling, sample-and-test)
- Medium confidence in the entropy-based memory indeterminacy mechanism
- Low confidence in the exact implementation details of the sample-and-search method and conversion functions

## Next Checks
1. Verify the entropy calculation and its correlation with retrieval performance by conducting additional experiments with varying levels of memory indeterminacy.
2. Implement and test the sample-and-search method with different local optimization algorithms to determine the impact on retrieval precision and recall.
3. Develop and evaluate alternative conversion functions between image representations and abstract functions to assess their effect on the overall performance of the EHAM model.