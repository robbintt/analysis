---
ver: rpa2
title: (Ir)rationality and Cognitive Biases in Large Language Models
arxiv_id: '2402.09193'
source_url: https://arxiv.org/abs/2402.09193
tags:
- llms
- tasks
- reasoning
- responses
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper uses cognitive psychology tasks to assess the rational
  reasoning of seven large language models (LLMs), including GPT-3.5, GPT-4, Bard,
  Claude 2, and three versions of Llama 2. The study found that, like humans, LLMs
  display irrational reasoning, but not in the same way humans do.
---

# (Ir)rationality and Cognitive Biases in Large Language Models

## Quick Facts
- arXiv ID: 2402.09193
- Source URL: https://arxiv.org/abs/2402.09193
- Reference count: 40
- This paper uses cognitive psychology tasks to assess the rational reasoning of seven large language models, finding that they display inconsistent and often illogical reasoning rather than human-like cognitive biases.

## Executive Summary
This study evaluates seven large language models (GPT-3.5, GPT-4, Bard, Claude 2, and three versions of Llama 2) using 12 cognitive psychology tasks designed to probe rational reasoning and cognitive biases. The researchers found that LLMs, like humans, display irrational reasoning but in different ways - most incorrect responses stemmed from illogical reasoning or factual inaccuracies rather than cognitive biases. GPT-4 achieved the highest proportion of correct responses through proper reasoning, while all models showed significant inconsistency, often providing different answers to identical tasks.

## Method Summary
The researchers evaluated seven large language models using 12 cognitive psychology tasks, including the Linda Problem, Conjunction Fallacy, Gambler's Fallacy, and various heuristic-based reasoning tasks. They employed a three-shot prompting approach with 20 replicates per task per model. Each task was presented with instructions, a few examples, and the actual prompt. The responses were then analyzed to determine whether they were correct, incorrect, or unrelated to the question. Correct responses were further categorized based on whether they demonstrated correct reasoning or were correct for incorrect reasons.

## Key Results
- GPT-4 achieved the highest proportion of correct responses with proper reasoning, while other models showed lower accuracy rates
- Most incorrect responses were due to illogical reasoning or factual inaccuracies rather than cognitive biases
- All models demonstrated significant inconsistency, often providing different answers to identical tasks
- When models failed, they were more likely to give incorrect answers than to express uncertainty or refuse to answer

## Why This Works (Mechanism)
Unknown: The paper does not explicitly detail the mechanisms behind why LLMs exhibit inconsistent and often illogical reasoning patterns. This may be due to the inherent complexity of transformer architectures and their autoregressive generation processes, which can lead to varying outputs for the same input.

## Foundational Learning
- Cognitive psychology tasks - These standardized tests probe human reasoning patterns and biases; needed to establish baseline expectations for rational thinking
- Three-shot prompting - A few-shot learning approach where models are given three examples before the task; needed to provide context while maintaining task generalization
- Conjunction fallacy - A cognitive bias where people judge specific conditions as more probable than general ones; needed as a benchmark for irrational reasoning patterns
- Gambler's fallacy - The mistaken belief that past random events affect future probabilities; needed to test probabilistic reasoning
- Base rate neglect - Tendency to ignore general statistical information in favor of specific case details; needed to evaluate statistical reasoning capabilities
- Availability heuristic - Judging probability based on how easily examples come to mind; needed to assess heuristic-based decision making

## Architecture Onboarding

**Component Map**
Input text -> Tokenizer -> Embedding layer -> Transformer blocks -> Output layer -> Response generation

**Critical Path**
Token generation relies on self-attention mechanisms in transformer blocks, which process context through multiple layers to produce probability distributions over vocabulary.

**Design Tradeoffs**
- Larger models (GPT-4) show better reasoning but require more computational resources
- Autoregressive generation allows flexible responses but can lead to inconsistency
- Fixed context windows limit reasoning on complex tasks requiring extensive background knowledge

**Failure Signatures**
- Inconsistent responses to identical prompts
- Illogical reasoning chains despite coherent surface structure
- Factual inaccuracies presented confidently
- Complete misses where responses are unrelated to the question

**First 3 Experiments**
1. Test consistency by running identical prompts multiple times and measuring response variance
2. Evaluate response quality with and without chain-of-thought prompting
3. Compare performance on numerical vs. abstract reasoning tasks

## Open Questions the Paper Calls Out
None

## Limitations
- The study used only 12 cognitive tasks, mostly involving small numbers and familiar objects, which may not represent the full range of human cognitive biases or complex reasoning scenarios
- Only three-shot prompting was tested, potentially missing optimal performance with different prompting strategies
- The study cannot determine the underlying causes of significant response inconsistency across models

## Confidence

**Confidence Labels:**
- High confidence in the finding that LLMs show significant inconsistency in responses
- Medium confidence in the claim that most errors stem from illogical reasoning rather than cognitive biases
- Medium confidence in the comparative performance rankings between models

## Next Checks
1. Test the same tasks with a wider range of prompting strategies (zero-shot, few-shot with different examples, chain-of-thought prompting) to assess whether performance differences are prompting-dependent
2. Expand the task corpus to include more complex reasoning scenarios involving larger numbers, abstract concepts, and real-world decision-making contexts
3. Conduct follow-up tests with modified versions of the same tasks to systematically isolate whether inconsistencies arise from task interpretation, reasoning processes, or generation randomness