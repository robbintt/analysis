---
ver: rpa2
title: Guiding Multi-agent Multi-task Reinforcement Learning by a Hierarchical Framework
  with Logical Reward Shaping
arxiv_id: '2411.01184'
source_url: https://arxiv.org/abs/2411.01184
tags:
- reward
- learning
- tasks
- agent
- agents
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-agent hierarchical reinforcement
  learning algorithm (MHLRS) designed to address the challenge of multi-task learning
  in complex environments. MHLRS leverages Linear Temporal Logic (LTL) to express
  the internal logic of tasks, enabling agents to understand and complete multiple
  tasks with improved interpretability.
---

# Guiding Multi-agent Multi-task Reinforcement Learning by a Hierarchical Framework with Logical Reward Shaping

## Quick Facts
- arXiv ID: 2411.01184
- Source URL: https://arxiv.org/abs/2411.01184
- Reference count: 40
- One-line primary result: MHLRS outperforms baseline algorithms in multi-task environments using LTL-based reward shaping and hierarchical structure

## Executive Summary
This paper introduces a multi-agent hierarchical reinforcement learning algorithm (MHLRS) designed to address the challenge of multi-task learning in complex environments. MHLRS leverages Linear Temporal Logic (LTL) to express the internal logic of tasks, enabling agents to understand and complete multiple tasks with improved interpretability. The algorithm incorporates a logic reward shaping (LRS) mechanism, using LTL progression and value iteration to enhance coordination among agents. Experimental results on a Minecraft-like environment demonstrate that MHLRS outperforms baseline algorithms, achieving higher rewards and better performance in sequential, interleaving, and constrained tasks.

## Method Summary
MHLRS uses a hierarchical structure where each agent has a meta-controller for high-level task decomposition and a controller for low-level action execution. The key innovation is the Logic Reward Shaping (LRS) mechanism that uses LTL progression to convert non-Markovian reward structures into Markovian ones, enabling standard RL algorithms to handle complex task specifications. Value iteration is employed to evaluate agent actions and shape rewards that promote coordination. The algorithm is tested on a Minecraft-like environment with tasks specified using LTL formulas, including sequential, interleaving, and safety-constrained variants.

## Key Results
- MHLRS achieves higher rewards and success rates compared to baseline algorithms (FALCON, I-LPOPL, I-DQN-L) in multi-task environments
- The algorithm demonstrates improved performance in handling sequential, interleaving, and constrained tasks
- LTL-based reward shaping and the hierarchical structure contribute to better coordination and task completion among agents

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** LTL progression converts non-Markovian reward structures into Markovian ones, enabling standard RL algorithms to handle complex task specifications.
- **Mechanism:** The paper uses LTL progression to update the task formula after each action, retaining only the unsatisfied parts of the formula. This process, defined in Section III-A, transforms the reward function into a Markovian form by evaluating the progressed formula at each step.
- **Core assumption:** The LTL formula can be progressively rewritten without losing the logical semantics required for task completion.
- **Evidence anchors:**
  - [abstract]: "LRS uses Linear Temporal Logic (LTL) to express the internal logic relation of subtasks within a complex task. Then, it evaluates whether the subformulae of the LTL expressions are satisfied based on a designed reward structure."
  - [section]: "LTL progression [13], [35] is a rewriting process that retains LTL's semantics. It receives an LTL formula and the current state as input and returns a formula that needs to be further dealt with."
  - [corpus]: Weak evidence - The corpus contains related work on LTL for RL, but none directly anchor the specific progression mechanism used here.
- **Break condition:** If the progression rules are incorrectly implemented or the LTL formula contains constructs that cannot be progressively evaluated, the Markov property may be lost.

### Mechanism 2
- **Claim:** Value iteration-based reward shaping provides negative feedback that encourages coordination among agents by penalizing deviations from optimal collaboration.
- **Mechanism:** After each action, agents receive evaluation values (Vj) based on how close their actions bring them to task completion. The reward function is shaped by adding the difference between the minimum and discounted maximum evaluation values, as shown in Equation (11) of Section III-A.
- **Core assumption:** The evaluation value V(s') accurately reflects the progress toward task completion and can be used to shape rewards that promote coordination.
- **Evidence anchors:**
  - [abstract]: "To enhance coordination and cooperation among multiple agents, a value iteration technique is designed to evaluate the actions taken by each agent. Based on this evaluation, a reward function is shaped for coordination."
  - [section]: "V (s′) = ξ(R(s, a, s′) + γV (s|At = a)). In Equation (10), s′ is the transitioned state and V (s′) is the evaluation value after the agent acts an action a at state s."
  - [corpus]: Weak evidence - The corpus includes works on multi-agent RL, but none specifically use value iteration for reward shaping in the context of LTL tasks.
- **Break condition:** If the evaluation values do not accurately reflect task progress, or if the discount factor γ is not appropriately set, the reward shaping may fail to promote coordination.

### Mechanism 3
- **Claim:** The hierarchical structure with meta-controllers and controllers allows agents to decompose complex tasks into manageable sub-goals, improving learning efficiency.
- **Mechanism:** Each agent has a two-layer structure: a meta-controller that proposes sub-goals (options) based on the environment state, and a controller that executes actions to achieve these sub-goals. This is described in Section III-B and Algorithm 1.
- **Core assumption:** Decomposing tasks into sub-goals simplifies the learning problem and allows for more efficient exploration of the action space.
- **Evidence anchors:**
  - [abstract]: "Each agent has its own meta-controller, which learns sub-goal strategies based on the state of the environment."
  - [section]: "The low-level component of the agent is a controller modeled by Double Deep Q-Learning (DDQN) [36]. It is responsible for executing actions by following the option generated by the meta-controller."
  - [corpus]: Moderate evidence - The corpus includes works on hierarchical RL, supporting the general idea, but not the specific LTL-based approach.
- **Break condition:** If the sub-goals are not well-defined or the meta-controller fails to propose effective strategies, the hierarchical structure may not improve learning efficiency.

## Foundational Learning

- **Concept:** Linear Temporal Logic (LTL)
  - **Why needed here:** LTL is used to formally specify complex tasks with temporal and logical constraints, which is essential for the algorithm's ability to handle multi-task learning.
  - **Quick check question:** Can you explain the difference between LTL and propositional logic, and give an example of a task that can be specified using LTL but not propositional logic?

- **Concept:** Markov Decision Process (MDP)
  - **Why needed here:** The algorithm transforms the non-Markovian reward structure into a Markovian one, which is a key step in enabling standard RL algorithms to handle the task specifications.
  - **Quick check question:** What is the Markov property, and why is it important for reinforcement learning algorithms?

- **Concept:** Value Iteration
  - **Why needed here:** Value iteration is used to evaluate the actions taken by each agent, which is then used to shape the reward function and promote coordination.
  - **Quick check question:** How does value iteration work, and what is its role in reinforcement learning?

## Architecture Onboarding

- **Component map:** Environment -> LRS Module -> Agents (Meta-controller -> Controller) -> Environment

- **Critical path:**
  1. The environment provides the initial state and LTL task specification.
  2. Each agent's meta-controller proposes a sub-goal based on the current state and task.
  3. The controller executes actions to achieve the sub-goal.
  4. The LRS module evaluates the actions using value iteration and shapes the reward.
  5. The agents update their policies based on the shaped reward and repeat the process.

- **Design tradeoffs:**
  - Using LTL for task specification provides a formal and interpretable way to define complex tasks, but it also increases the complexity of the reward function.
  - The hierarchical structure allows for more efficient learning by decomposing tasks into sub-goals, but it also introduces additional complexity in the agent architecture.

- **Failure signatures:**
  - If the LTL progression is not correctly implemented, the reward function may not be Markovian, leading to poor learning performance.
  - If the value iteration-based reward shaping is not effective, the agents may not learn to coordinate properly.
  - If the hierarchical structure is not well-designed, the agents may not be able to decompose tasks effectively, leading to inefficient learning.

- **First 3 experiments:**
  1. **Sequential Tasks:** Test the algorithm's ability to handle tasks with a predetermined order of sub-tasks.
  2. **Interleaving Tasks:** Test the algorithm's ability to handle tasks where sub-tasks can be completed in any order.
  3. **Constrained Tasks:** Test the algorithm's ability to handle tasks with safety constraints, such as avoiding certain states or actions.

## Open Questions the Paper Calls Out
The paper does not explicitly call out any open questions. However, based on the content, several potential open questions arise regarding scalability, robustness to imperfect specifications, impact of different progression strategies, and handling of conflicting objectives.

## Limitations
- The exact network architecture details beyond "two hidden layers with 64 neurons" and ReLU activation are not specified.
- The specific LTL formulas for all 10 tasks are not fully detailed in the paper.
- The paper lacks experiments testing scalability with larger numbers of agents.

## Confidence
- Mechanism 1: Medium - The LTL progression concept is well-established, but the specific implementation details are not fully provided.
- Mechanism 2: Medium - Value iteration for reward shaping is a reasonable approach, but its effectiveness in the specific context is not empirically anchored.
- Mechanism 3: Medium - Hierarchical RL is a well-known technique, but the paper doesn't provide enough detail on the specific implementation or its impact on learning efficiency.

## Next Checks
1. Verify the correctness of LTL progression implementation by testing it with various LTL formulas and checking if the Markov property is maintained.
2. Test the robustness of the value iteration-based reward shaping across different task types by introducing variations in task complexity and agent interactions.
3. Evaluate the scalability of the approach to larger numbers of agents by conducting experiments with 5, 10, and 20 agents in complex environments, measuring performance metrics like reward, learning time, and communication overhead.