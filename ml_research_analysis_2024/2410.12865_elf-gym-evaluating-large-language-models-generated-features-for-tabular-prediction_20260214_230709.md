---
ver: rpa2
title: 'ELF-Gym: Evaluating Large Language Models Generated Features for Tabular Prediction'
arxiv_id: '2410.12865'
source_url: https://arxiv.org/abs/2410.12865
tags:
- features
- feature
- llms
- data
- golden
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'ELF-Gym addresses the challenge of evaluating LLM-generated features
  for tabular prediction by providing a framework that measures both semantic and
  functional similarity to human-crafted "golden" features. The framework curates
  a dataset of 251 expert features from 8 Kaggle competitions and uses two similarity
  measures: semantic similarity (assessed by LLM) and functional similarity (assessed
  by output comparison).'
---

# ELF-Gym: Evaluating Large Language Models Generated Features for Tabular Prediction

## Quick Facts
- arXiv ID: 2410.12865
- Source URL: https://arxiv.org/abs/2410.12865
- Reference count: 40
- Primary result: LLMs capture ~56% of golden features semantically but only 13% functionally

## Executive Summary
ELF-Gym is a framework for evaluating LLM-generated features for tabular prediction tasks. It measures both semantic and functional similarity between LLM-generated features and expert-crafted "golden" features. The framework curates 251 expert features from 8 Kaggle competitions and uses two similarity measures: semantic similarity (assessed by LLM) and functional similarity (assessed by output comparison). Results show that while LLMs can capture approximately 56% of golden features semantically, this drops to only 13% functionally, highlighting significant gaps in implementation accuracy.

## Method Summary
ELF-Gym uses a two-stage feature generation process: first generating feature descriptions from dataset context, then using those descriptions to generate executable code. The framework evaluates LLM-generated features against 251 expert-crafted features from Kaggle competitions using both semantic similarity (via LLM assessment) and functional similarity (via output comparison). It also measures downstream model performance impact. The evaluation employs recall metrics to quantify how well LLM-generated features align with golden features, providing a comprehensive assessment of both conceptual understanding and practical implementation capabilities.

## Key Results
- LLMs capture approximately 56% of golden features semantically but only 13% functionally
- Performance gaps are most significant for complex features involving multi-variable functions, custom aggregations, and table joins
- LLM-generated features can improve model performance over raw features but still struggle to match expert-level feature engineering

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The dual measurement approach (semantic + functional similarity) captures both conceptual understanding and practical implementation gaps in LLM-generated features.
- Mechanism: By measuring both semantic similarity (through LLM assessment) and functional similarity (through output comparison), the framework identifies where LLMs understand the intent of golden features but fail to implement them correctly.
- Core assumption: Semantic similarity alone is insufficient to evaluate feature engineering quality, as it doesn't capture implementation accuracy.
- Evidence anchors:
  - [abstract]: "ELF-Gym then quantitatively evaluates LLM-generated features by measuring their impact on downstream model performance as well as their alignment with expert-crafted features through semantic and functional similarity assessments."
  - [section]: "ELF-Gym uses two measurement functions ð‘€. ð‘€ð‘ ð‘’ð‘š measures the semantic similarity of two feature descriptions by prompting a GPT-4o to assess and score the similarity. ð‘€ð‘“ ð‘¢ð‘›ð‘ checks if two features are functionally equivalent by comparing the outputs of feature functions applied to the input data."

### Mechanism 2
- Claim: The two-stage feature generation process (description then code) improves reliability of LLM-generated features.
- Mechanism: First generating feature descriptions from dataset context, then using those descriptions to generate executable code, allows for validation and refinement at each step.
- Core assumption: Breaking down feature engineering into description and implementation phases helps LLMs better handle complex feature patterns.
- Evidence anchors:
  - [section]: "Similar to how golden features are curated, the LLM-generated features also take two steps... First, the candidate LLM receives the dataset description and prediction target and is asked to generate a list of feature descriptions."
  - [corpus]: "Found 25 related papers... Average neighbor FMR=0.435" (indicating active research in this area)

### Mechanism 3
- Claim: The recall-based evaluation metric provides meaningful comparison between LLM-generated and expert-crafted features.
- Mechanism: Using recall metrics with both semantic and functional similarity measures allows for quantitative assessment of how well LLMs capture expert knowledge.
- Core assumption: Recall metrics are appropriate for evaluating feature engineering as they measure coverage of expert knowledge.
- Evidence anchors:
  - [section]: "To measure the alignment between LLM-engineered features and golden features, ELF-Gym employs a recall metric... The recall metric is then defined as the proportion of golden features for which the ð‘€ returns 1 when compared to features in ð¹ð¿ð¿ð‘€."
  - [section]: "For RQ1, we find that LLMs can capture at best approximately 56% of the 'golden' features by description, but only 13% at best when coming to implementation"

## Foundational Learning

- Concept: Feature engineering and its role in machine learning pipelines
  - Why needed here: Understanding feature engineering is crucial for evaluating the framework's approach to measuring LLM performance
  - Quick check question: What is the difference between raw features and engineered features in machine learning?

- Concept: Large Language Model capabilities and limitations
  - Why needed here: Understanding LLM strengths and weaknesses helps explain the framework's evaluation approach
  - Quick check question: What are the main challenges LLMs face when generating executable code?

- Concept: Tabular data and Kaggle competition formats
  - Why needed here: Understanding the data context helps explain why specific evaluation metrics were chosen
  - Quick check question: Why are Kaggle competitions particularly relevant for evaluating feature engineering approaches?

## Architecture Onboarding

- Component map: Dataset Curator -> Feature Description Generator -> Feature Code Generator -> Semantic Similarity Evaluator -> Functional Similarity Evaluator -> Model Evaluator
- Critical path: Dataset â†’ Feature Generation â†’ Similarity Evaluation â†’ Performance Assessment
- Design tradeoffs:
  - Accuracy vs. computation cost in functional similarity evaluation
  - Completeness vs. noise in feature generation
  - Semantic vs. functional similarity weight
- Failure signatures:
  - Low functional similarity despite high semantic similarity
  - Poor downstream model performance despite high similarity scores
  - Inconsistent results across different LLM models
- First 3 experiments:
  1. Test semantic similarity evaluation with simple feature transformations
  2. Verify functional similarity measurement with controlled dataset samples
  3. Compare performance impact of LLM-generated features vs. raw features on a simple dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve LLMs' ability to generate functionally equivalent features, not just semantically similar ones?
- Basis in paper: [explicit] The paper shows that while LLMs can capture approximately 56% of golden features semantically, this drops to only 13% functionally, indicating a significant gap in code generation capabilities.
- Why unresolved: The paper identifies this gap but doesn't explore specific techniques to bridge it, such as better prompt engineering, fine-tuning on code synthesis, or architectural modifications to improve functional understanding.
- What evidence would resolve it: Comparative studies showing improved functional recall after implementing specific techniques like enhanced code verification, constraint-based generation, or specialized fine-tuning on tabular data transformation tasks.

### Open Question 2
- Question: What specific patterns or types of complex features are most challenging for LLMs to generate, and why?
- Basis in paper: [explicit] The paper identifies that LLMs struggle with multi-variable functions, custom aggregations, complex table joins, nested group-by operations, and non-builtin aggregations, but doesn't deeply analyze why these specific patterns are difficult.
- Why unresolved: While the paper categorizes challenging feature types, it doesn't investigate whether the difficulty stems from lack of domain knowledge, limited reasoning capabilities, or insufficient training data for these specific patterns.
- What evidence would resolve it: Detailed analysis of LLM failure modes for different complex feature patterns, including error analysis of generated code and comparison with human reasoning processes for the same features.

### Open Question 3
- Question: Can specialized training or fine-tuning improve LLMs' feature engineering capabilities for tabular data?
- Basis in paper: [inferred] The paper demonstrates significant performance gaps between LLM-generated and expert-crafted features, suggesting that current general-purpose LLMs may benefit from domain-specific adaptation for feature engineering tasks.
- Why unresolved: The study uses off-the-shelf LLMs without exploring whether targeted training on tabular data, feature engineering examples, or code synthesis tasks could improve performance.
- What evidence would resolve it: Empirical comparison of fine-tuned versus general-purpose LLMs on the same feature engineering tasks, showing whether specialized training leads to improved semantic and functional similarity to golden features.

## Limitations
- The framework's evaluation reveals significant gaps between LLM-generated features and expert-crafted features, with only 56% semantic and 13% functional similarity to golden features.
- The functional similarity assessment becomes computationally expensive for large datasets, necessitating sampling strategies that may miss important feature behaviors.
- The framework relies heavily on GPT-4o for both semantic similarity assessment and code generation, raising questions about generalizability to other LLM models.

## Confidence
- Semantic similarity results: **High** (supported by concrete metrics and multiple evaluation runs)
- Functional similarity results: **Medium** (computationally intensive evaluation with sampling introduces uncertainty)
- Generalizability claims: **Low** (limited to specific LLM models and competition datasets)

## Next Checks
1. Test the framework with additional LLM models (e.g., Claude, Gemini) to assess generalizability of semantic and functional similarity measures.

2. Evaluate feature engineering performance on non-Kaggle tabular datasets to verify real-world applicability beyond competition settings.

3. Implement an automated sampling strategy validation to ensure functional similarity assessments remain reliable when dealing with large datasets.