---
ver: rpa2
title: 'On The Planning Abilities of OpenAI''s o1 Models: Feasibility, Optimality,
  and Generalizability'
arxiv_id: '2409.19924'
source_url: https://arxiv.org/abs/2409.19924
tags:
- planning
- o1-preview
- tasks
- plan
- gpt-4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study evaluates the planning capabilities of OpenAI\u2019\
  s o1 models across diverse benchmark tasks, focusing on feasibility, optimality,\
  \ and generalizability. The authors assess performance on constraint-heavy tasks\
  \ (Barman, Tyreworld) and spatially complex environments (Termes, Floortile), comparing\
  \ o1-preview against GPT-4 and o1-mini."
---

# On The Planning Abilities of OpenAI's o1 Models: Feasibility, Optimality, and Generalizability

## Quick Facts
- arXiv ID: 2409.19924
- Source URL: https://arxiv.org/abs/2409.19924
- Authors: Kevin Wang; Junbo Li; Neel P. Bhatt; Yihan Xi; Qiang Liu; Ufuk Topcu; Zhangyang Wang
- Reference count: 2
- Primary result: o1-preview outperforms GPT-4 in constraint adherence but generates suboptimal plans with redundant actions and struggles in spatially complex tasks

## Executive Summary
This study evaluates the planning capabilities of OpenAI's o1 models (o1-preview and o1-mini) across diverse benchmark tasks, focusing on feasibility, optimality, and generalizability. The authors assess performance on constraint-heavy tasks like Barman and Tyreworld, as well as spatially complex environments like Termes and Floortile, comparing o1-preview against GPT-4 and o1-mini. Results show o1-preview achieves perfect success rates on constraint-heavy tasks but generates suboptimal plans with redundant actions, while struggling significantly in spatially complex tasks with 0% success rates. The study also reveals challenges in generalizing to abstract tasks and highlights the need for improved memory management and decision-making in LLM-based planning systems.

## Method Summary
The study employs empirical evaluation using OpenAI API to generate plans for benchmark planning tasks from PlanBench, including Barman, Tyreworld, Termes, Floortile, Blocksworld, and Grippers. The researchers compare three models (GPT-4, o1-mini, and o1-preview) using standardized prompts and evaluate success rates, optimality, and generalization performance. Success is measured by constraint adherence and task completion, while optimality is assessed by plan efficiency and redundant actions. The methodology involves running each model on all tasks and analyzing results across feasibility, optimality, and generalization dimensions.

## Key Results
- o1-preview achieves 100% success rates on constraint-heavy tasks (Barman and Tyreworld) while GPT-4 fails on Tyreworld
- All models struggle with spatially complex tasks, with 0% success rates in Termes and Floortile
- o1-preview generates feasible plans but with significant optimality issues, including redundant actions in Blocksworld and suboptimal steps in Floortile
- Generalization to abstract tasks degrades performance, with o1-preview maintaining only 33.3% success compared to 50% for GPT-4 on abstract Grippers tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The o1 model's self-evaluation mechanism improves constraint adherence by allowing the model to check and correct its actions during plan generation.
- Mechanism: The model generates a plan, then reviews it against task constraints, identifying and correcting violations before finalizing the output. This iterative checking process catches errors that would otherwise persist.
- Core assumption: The model's self-evaluation capability is reliable enough to identify constraint violations and generate corrections that maintain plan feasibility.
- Evidence anchors:
  - [abstract] "we highlight o1-preview's strengths in self-evaluation and constraint-following"
  - [section] "o1-preview's internal self-evaluation mechanism allowed it to better track the rules and adjust its actions accordingly"
  - [corpus] Weak - no direct corpus evidence for the specific self-evaluation mechanism
- Break condition: If the model cannot accurately evaluate its own constraints or if the self-correction process introduces new errors, the mechanism fails.

### Mechanism 2
- Claim: The o1 model demonstrates improved reasoning capabilities compared to GPT-4, particularly in structured environments with clear rules.
- Mechanism: Through reinforcement learning on complex reasoning tasks, o1 develops better internal representations for understanding task structure and applying appropriate actions within constraints.
- Core assumption: The reinforcement learning process effectively trains the model to recognize and apply complex reasoning patterns across different task domains.
- Evidence anchors:
  - [abstract] "o1-preview outperforms GPT-4 in adhering to task constraints and managing state transitions in structured environments"
  - [section] "o1-preview achieved a perfect 100% success rate, reflecting its stronger reasoning capabilities"
  - [corpus] Weak - no direct corpus evidence for the reinforcement learning mechanism
- Break condition: If task complexity exceeds the model's learned reasoning patterns or if environmental changes invalidate the learned representations.

### Mechanism 3
- Claim: The o1 model's chain-of-thought reasoning enables better problem understanding and solution planning compared to standard prompting approaches.
- Mechanism: The model breaks down complex problems into sequential reasoning steps, maintaining intermediate states and evaluating progress toward goals throughout the planning process.
- Core assumption: The chain-of-thought approach effectively captures the sequential nature of planning tasks and maintains coherence across multiple reasoning steps.
- Evidence anchors:
  - [abstract] "trained with reinforcement learning to naturally employ chain-of-thought reasoning"
  - [section] "models like o1, which demonstrate superior reasoning capabilities, tend to perform better, as they provide more thorough analysis and structured plans"
  - [corpus] Weak - no direct corpus evidence for the chain-of-thought mechanism specifics
- Break condition: If the reasoning chain becomes too long or complex, causing the model to lose track of intermediate states or goal relationships.

## Foundational Learning

- Concept: Constraint satisfaction in planning tasks
  - Why needed here: Understanding how models identify and enforce task-specific constraints is crucial for evaluating planning performance
  - Quick check question: What happens when a planning model violates a constraint in a task like Tyreworld?

- Concept: State space representation and management
  - Why needed here: Planning involves maintaining and transitioning between different states, which is a core challenge for language models
  - Quick check question: How does a model track the current state of a Barman task with multiple containers and ingredients?

- Concept: Optimality vs feasibility in planning
  - Why needed here: The study distinguishes between generating valid plans and generating efficient ones, which requires understanding optimization concepts
  - Quick check question: Why might a model generate a feasible plan that is not optimal?

## Architecture Onboarding

- Component map: OpenAI o1 model architecture includes transformer-based language model enhanced with reinforcement learning for reasoning, incorporating self-evaluation mechanisms and chain-of-thought processing capabilities
- Critical path: Problem understanding → Constraint identification → Plan generation → Self-evaluation → Output refinement
- Design tradeoffs: The model prioritizes reasoning depth over speed, using iterative evaluation that increases computational cost but improves accuracy in constraint adherence
- Failure signatures: Constraint violations (IR errors), failure to generate complete plans (IP errors), suboptimal action sequences (LO errors), and misinterpretation of goal states (MG errors)
- First 3 experiments:
  1. Test o1-preview on Barman task to verify constraint adherence and self-evaluation capabilities
  2. Compare o1-preview vs GPT-4 on Blocksworld with increasing block counts to measure reasoning scalability
  3. Evaluate generalization performance by replacing action symbols with abstract representations in Grippers task

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLMs be enhanced to achieve true optimality in planning tasks, beyond merely generating feasible solutions?
- Basis in paper: [explicit] The paper highlights that while o1-preview generates feasible plans, it frequently includes redundant actions and suboptimal steps, especially in tasks like Blocksworld and Floortile. The authors suggest that incorporating cost-based reasoning or learning from expert demonstrations could help.
- Why unresolved: Achieving true optimality in LLMs remains a challenge due to their current limitations in decision-making and resource minimization. The paper points out that while LLMs can understand constraints, they struggle with minimizing steps and optimizing resource usage.
- What evidence would resolve it: Developing and testing models that integrate cost-sensitive reasoning or expert demonstrations, and evaluating their performance on complex planning tasks to demonstrate improved optimality.

### Open Question 2
- Question: How can LLMs be improved to generalize effectively in abstract and high-dimensional problem spaces?
- Basis in paper: [explicit] The paper notes that while o1-preview shows promise in structured environments, its performance degrades in tasks with abstract representations, such as Termes and generalized tasks with random symbols. The authors suggest that enhancing generalization in high-dimensional spaces is crucial.
- Why unresolved: Current models struggle with reasoning in abstract and spatially dynamic environments, as they are often too closely tied to specific task domains. The paper indicates a need for more robust generalization mechanisms.
- What evidence would resolve it: Creating and evaluating models that incorporate advanced memory management and abstraction techniques, and testing their ability to generalize in diverse, abstract planning tasks.

### Open Question 3
- Question: How can LLMs be adapted to handle dynamic and unpredictable environments in real-world planning scenarios?
- Basis in paper: [explicit] The paper suggests that testing LLMs in dynamic environments with unpredictable elements would provide insights into their robustness and adaptability. This is particularly important as many real-world planning problems involve changing rules or constraints.
- Why unresolved: Current evaluations focus on structured environments, leaving a gap in understanding how LLMs perform under dynamic conditions. The paper calls for more extensive testing in such settings.
- What evidence would resolve it: Designing and conducting experiments where LLMs are tested in dynamic environments with changing rules or constraints, and analyzing their ability to adapt and maintain performance.

## Limitations

- The evaluation relies on synthetic benchmark tasks that may not fully capture real-world planning complexity
- Lack of direct corpus evidence for specific mechanisms like self-evaluation and chain-of-thought processing
- Comparison uses the same prompts for all models, which may not be optimal for each model's unique capabilities

## Confidence

- **High confidence**: Claims about o1-preview outperforming GPT-4 in constraint adherence on Barman and Tyreworld tasks
- **Medium confidence**: Claims about self-evaluation mechanism improving performance, due to lack of direct corpus evidence
- **Low confidence**: Claims about the specific chain-of-thought mechanism, as implementation details remain unclear

## Next Checks

1. **Prompt optimization study**: Test whether task-specific prompts improve o1-mini and GPT-4 performance to levels approaching o1-preview, controlling for prompt quality differences.

2. **Mechanism isolation**: Design experiments to specifically test whether the self-evaluation mechanism works as described by comparing plans generated with and without explicit self-evaluation steps.

3. **Real-world task extension**: Apply the evaluation framework to planning tasks from robotics or logistics domains to assess whether benchmark results translate to practical applications.