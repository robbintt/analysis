---
ver: rpa2
title: 'AI Safety: A Climb To Armageddon?'
arxiv_id: '2405.19832'
source_url: https://arxiv.org/abs/2405.19832
tags:
- safety
- will
- argument
- more
- failure
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a counterintuitive argument that AI safety measures
  may exacerbate existential risk rather than mitigate it. Under key assumptions -
  certainty of AI failure, correlation between failure severity and AI power at failure
  point, and tendency of safety measures to enable greater AI power before failure
  - safety efforts have negative expected utility.
---

# AI Safety: A Climb To Armageddon?

## Quick Facts
- arXiv ID: 2405.19832
- Source URL: https://arxiv.org/abs/2405.19832
- Reference count: 0
- One-line primary result: AI safety measures may increase existential risk by enabling more powerful AI systems before failure occurs

## Executive Summary
This paper presents a counterintuitive argument that certain AI safety measures may exacerbate existential risk rather than mitigate it. Under specific assumptions - including certainty of AI failure, correlation between failure severity and AI power at failure, and safety measures enabling greater AI power before failure - the expected utility of providing safety measures is negative. The authors examine three response strategies (Optimism, Mitigation, and Holism) but find each faces challenges from intrinsic AI safety landscape features they term Bottlenecking, Perfection Barrier, and Equilibrium Fluctuation. The argument proves remarkably robust even when central assumptions are weakened, forcing re-examination of core AI safety assumptions.

## Method Summary
The paper uses logical argumentation and decision theory to analyze the relationship between AI safety measures and existential risk. It presents a theoretical framework examining premises, counterexamples, and potential objections to an anti-safety argument. The method involves exploring both deterministic and non-deterministic versions of the argument, analyzing three response strategies, and identifying intrinsic features of AI safety landscapes that challenge conventional safety approaches.

## Key Results
- Safety measures can have negative expected utility by enabling AI systems to reach higher power levels before failing
- Three intrinsic features (Bottlenecking, Perfection Barrier, Equilibrium Fluctuation) make optimism about AI safety unlikely to succeed
- The anti-safety argument remains valid even when central assumptions are weakened
- All three examined response strategies (Optimism, Mitigation, Holism) face significant challenges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Safety measures enable AI systems to reach higher power levels before failure, increasing expected harm.
- Mechanism: The paper's core argument hinges on a "Cliff-Plateau-Climb" pattern where safety interventions delay failure to higher power states, where the severity of consequences increases disproportionately.
- Core assumption: Safety measures do not prevent failure but merely postpone it to a more powerful state.
- Evidence anchors:
  - [abstract] "Under certain key assumptions... safety measures to enable AI systems to become more powerful before failing"
  - [section 2] "the expected level of power when the AI first fails is higher if you provide safety than if you don't"
- Break condition: If safety measures can prevent failure entirely rather than just delay it, the mechanism fails.

### Mechanism 2
- Claim: Bottlenecking creates fundamental limits on safety measure effectiveness.
- Mechanism: Safety mechanisms must route through human implementers, regulators, and physical devices, creating reliability caps that cannot approach perfection as required for advanced AI safety.
- Core assumption: Human and organizational bottlenecks cannot achieve arbitrarily high reliability.
- Evidence anchors:
  - [section 3.1.1] "Safety mechanisms must route through fallible bottlenecks -- human implementers, regulators setting rules, people programming safety code"
  - [section 3.3] "Mitigation and Bottlenecking: the same bottlenecks that limit the effectiveness of safety mechanisms also constrain our ability to mitigate the risks"
- Break condition: If AI systems can develop and implement safety measures autonomously without human bottlenecks, this mechanism breaks down.

### Mechanism 3
- Claim: Perfection Barrier makes safety measures asymptotically harder to achieve than harm potential.
- Mechanism: As safety measures approach perfection, each incremental improvement requires exponentially more control, while harm can be achieved through random exertion of power without fine-tuning.
- Core assumption: Achieving near-perfect safety is fundamentally harder than causing harm.
- Evidence anchors:
  - [section 3.1.1] "Perfection gets asymptotically harder to achieve because it requires very fine control"
  - [section 3.3] "The Perfection Barrier poses an even greater challenge for Mitigation... undoing that ripple requires incredibly fine-tuned interventions"
- Break condition: If AI systems develop capabilities that make perfect safety achievable with reasonable effort, the mechanism fails.

## Foundational Learning

- Concept: Expected utility calculation in decision theory
  - Why needed here: The paper's argument relies on comparing expected utilities of providing vs. not providing safety measures
  - Quick check question: How do you calculate expected utility when outcomes have different probabilities and values?

- Concept: Bottlenecking in system reliability
  - Why needed here: Understanding how human and organizational bottlenecks limit system reliability is crucial for evaluating the paper's claims
  - Quick check question: What is the weakest link principle in reliability engineering?

- Concept: Power-law relationships in scaling
  - Why needed here: The paper discusses how harm potential scales with power in non-linear ways
  - Quick check question: How does a power-law relationship differ from linear scaling?

## Architecture Onboarding

- Component map:
  - Safety measure implementation pipeline (human bottlenecks → regulatory framework → code deployment)
  - AI capability assessment system (power measurement → failure prediction → harm estimation)
  - Risk evaluation module (bottleneck analysis → perfection barrier calculation → equilibrium fluctuation modeling)

- Critical path: Safety measure → Power increase → Failure point → Harm assessment → Utility comparison

- Design tradeoffs:
  - Safety vs. capability: More safety measures enable higher power but increase failure severity
  - Human vs. automated safety: Human bottlenecks limit safety effectiveness but may provide necessary oversight
  - Prevention vs. mitigation: Prevention aims to stop failures, mitigation aims to reduce their impact

- Failure signatures:
  - Premature failure at low power levels
  - Delayed catastrophic failure at high power levels
  - Ineffective safety measures that enable dangerous capabilities
  - Bottleneck failures in safety implementation

- First 3 experiments:
  1. Simulate AI systems with varying safety measures to measure power levels at failure and resulting harm
  2. Model bottleneck effects on safety measure implementation success rates
  3. Test perfection barrier effects by measuring control precision requirements vs. harm potential scaling

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can empirical evidence definitively establish whether safety measures for AI systems exhibit the Bottlenecking, Perfection Barrier, and Equilibrium Fluctuation effects described in the paper?
- Basis in paper: [explicit] The paper discusses these three features as intrinsic properties of AI safety landscapes that make optimism about AI safety measures unlikely to succeed.
- Why unresolved: These are theoretical claims about the nature of AI development and safety measures that require extensive empirical validation across different AI systems and safety approaches.
- What evidence would resolve it: Systematic analysis of multiple AI safety measures showing whether they consistently face these three constraints, including quantitative data on failure rates, improvement trajectories, and correlation between safety and power.

### Open Question 2
- Question: Under what specific conditions, if any, could the Non-Deterministic Argument's conclusion that safety measures are harmful be avoided or reversed?
- Basis in paper: [explicit] The authors acknowledge that their argument might be defeated by certain "ultra-optimistic" scenarios or specific configurations of AI development.
- Why unresolved: The paper identifies some potential escape routes but doesn't provide a comprehensive analysis of all possible conditions that might invalidate the argument.
- What evidence would resolve it: Concrete scenarios or mathematical proofs demonstrating that certain safety measures can increase overall expected utility despite the premises of the Non-Deterministic Argument.

### Open Question 3
- Question: How should AI governance and policy frameworks be structured if the Non-Deterministic Argument's conclusions are taken seriously?
- Basis in paper: [explicit] The authors note this as an open question, suggesting potential regulatory responses including bans on certain AI research or redirection of safety work.
- Why unresolved: The paper identifies this as a practical implication but doesn't explore the specific policy mechanisms or their trade-offs in detail.
- What evidence would resolve it: Analysis of specific governance frameworks that account for the argument's premises, including their feasibility, effectiveness, and potential unintended consequences.

## Limitations

- Theoretical framework relies on assumptions about AI failure inevitability that lack empirical validation
- Key concepts like "super-human capacities" and "existential threat" are not fully specified
- The argument may be overly sensitive to definitional choices and modeling assumptions

## Confidence

- High confidence in the logical structure and internal consistency of the argument
- Medium confidence in the applicability of the theoretical framework to real-world AI systems
- Low confidence in the empirical validity of key premises about AI failure patterns and power-severity correlations

## Next Checks

1. **Empirical validation**: Design experiments or simulations to test whether AI safety measures actually correlate with increased power at failure points, as the paper's core mechanism requires.

2. **Bottleneck quantification**: Develop metrics to measure the actual impact of human and organizational bottlenecks on safety measure effectiveness in real AI development contexts.

3. **Perfection barrier testing**: Create controlled scenarios to test whether achieving near-perfect safety does require exponentially more resources than causing harm, as claimed.