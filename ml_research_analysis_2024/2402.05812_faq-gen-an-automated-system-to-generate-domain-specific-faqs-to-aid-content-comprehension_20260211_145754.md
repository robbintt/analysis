---
ver: rpa2
title: 'FAQ-Gen: An automated system to generate domain-specific FAQs to aid content
  comprehension'
arxiv_id: '2402.05812'
source_url: https://arxiv.org/abs/2402.05812
tags:
- question
- system
- generation
- answer
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes an end-to-end automated system for generating
  domain-specific Frequently Asked Questions (FAQs) to aid content comprehension.
  The system leverages text-to-text transformation models, specifically T5, to perform
  domain identification, question generation, answer keyword extraction, answer completion,
  and FAQ ranking.
---

# FAQ-Gen: An automated system to generate domain-specific FAQs to aid content comprehension
## Quick Facts
- arXiv ID: 2402.05812
- Source URL: https://arxiv.org/abs/2402.05812
- Reference count: 11
- Proposes an end-to-end automated system using T5 models for generating domain-specific FAQs

## Executive Summary
This paper introduces FAQ-Gen, an automated system that generates domain-specific Frequently Asked Questions to improve content comprehension. The system uses a modular architecture with T5-based text-to-text transformation models to perform domain identification, question generation, answer keyword extraction, answer completion, and FAQ ranking. To enhance domain-specificity, the authors create custom datasets by extending SQuAD with domain-relevant content. Human evaluation across 17 domains shows high scores for syntactic quality and relevance to source text.

## Method Summary
The FAQ-Gen system employs a pipeline of T5-based models for automated FAQ generation. The process begins with domain identification, followed by question generation using the identified domain context. Answer keyword extraction identifies key information from the source text, which is then used for answer completion. Finally, an FAQ ranking module organizes the generated questions and answers. The system's modular design allows for flexibility and domain-specific tuning, with custom datasets created by extending SQuAD with domain-relevant content to improve performance across different domains.

## Key Results
- Human evaluation shows high syntactic quality scores (8.1/10 for questions, 8.0/10 for answers)
- Generated FAQs demonstrate strong relevance to source text across 17 domains
- Modular design enables flexibility and domain-specific tuning capabilities

## Why This Works (Mechanism)
The system works by leveraging the power of T5 text-to-text transformation models in a modular pipeline. Each component addresses a specific aspect of FAQ generation: domain identification ensures context-appropriate questions, question generation creates relevant queries, answer extraction identifies key information, answer completion fills in gaps, and ranking organizes the final output. The approach of extending SQuAD with domain-specific content allows the models to learn domain-appropriate patterns while maintaining the quality benchmarks established by the original dataset.

## Foundational Learning
- **T5 text-to-text transformation models**: Why needed - Provide unified framework for multiple NLP tasks; Quick check - Can handle both question generation and answer extraction within same architecture
- **Domain identification**: Why needed - Ensures generated questions match content context; Quick check - Must accurately classify content before question generation
- **Answer keyword extraction**: Why needed - Identifies essential information for concise answers; Quick check - Should capture core concepts without excessive detail
- **Modular architecture**: Why needed - Allows independent improvement and domain-specific tuning; Quick check - Components should function independently yet integrate seamlessly

## Architecture Onboarding
- **Component map**: Domain Identification -> Question Generation -> Answer Keyword Extraction -> Answer Completion -> FAQ Ranking
- **Critical path**: The most time-consuming step is likely question generation, as it requires understanding domain context and creating natural language questions that capture key information
- **Design tradeoffs**: The modular approach offers flexibility but introduces potential error propagation where failures in earlier stages compound downstream
- **Failure signatures**: Poor domain identification leads to irrelevant questions; weak keyword extraction results in incomplete answers; ranking failures cause disorganized FAQ output
- **First experiments**: 1) Test domain identification accuracy on diverse content samples, 2) Evaluate question generation quality with different domain contexts, 3) Assess answer completion performance with varying keyword extraction results

## Open Questions the Paper Calls Out
None

## Limitations
- Human evaluation conducted with only 5 evaluators per FAQ set, potentially insufficient for generalization
- Evaluation focused on readability and relevance but did not measure actual comprehension improvement
- No inter-rater reliability scores reported to assess consistency of human judgments

## Confidence
- High Confidence: System architecture and technical implementation details are well-described and sound
- Medium Confidence: Human evaluation scores are promising but based on limited sample size and narrow criteria
- Low Confidence: Claims about "comprehensive" FAQs not well-supported without direct comprehension measurement

## Next Checks
1. Conduct user studies measuring actual content comprehension improvement when using generated FAQs versus baseline approaches
2. Perform ablation studies to quantify contribution of each modular component and assess error propagation
3. Evaluate system performance across domains with limited training data and highly specialized terminology