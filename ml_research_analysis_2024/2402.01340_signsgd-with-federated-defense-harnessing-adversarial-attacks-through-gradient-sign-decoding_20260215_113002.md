---
ver: rpa2
title: 'SignSGD with Federated Defense: Harnessing Adversarial Attacks through Gradient
  Sign Decoding'
arxiv_id: '2402.01340'
source_url: https://arxiv.org/abs/2402.01340
tags:
- sign
- gradient
- workers
- learning
- decoding
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a new algorithm for distributed learning that
  is resilient to adversarial attacks. The key idea is to use a weighted majority
  voting aggregation where the weights are dynamically learned by the server based
  on the reliability of the workers' transmitted gradients.
---

# SignSGD with Federated Defense: Harnessing Adversarial Attacks through Gradient Sign Decoding

## Quick Facts
- arXiv ID: 2402.01340
- Source URL: https://arxiv.org/abs/2402.01340
- Reference count: 40
- New algorithm for distributed learning resilient to adversarial attacks using weighted majority voting aggregation with dynamically learned reliability weights

## Executive Summary
This paper introduces signSGD with federated defense (signSGD-FD), a novel approach to distributed learning that maintains robustness against adversarial attacks through a weighted majority voting aggregation scheme. The key innovation is the use of dynamically estimated log-likelihood ratio weights to prioritize gradients from reliable workers while filtering out adversarial contributions. The authors prove that signSGD-FD maintains the same convergence rate regardless of the number of adversarial workers, provided there are more benign workers than malicious ones.

## Method Summary
The paper proposes signSGD-FD with weighted majority voting (WMV) aggregation where the server dynamically estimates the reliability of each worker's transmitted gradient using log-likelihood ratios (LLRs). During the initial phase, the server estimates decoding error probabilities to compute LLR weights, then recursively updates these weights throughout training. The method uses signSGD for gradient quantization, sending only the sign of gradients to reduce communication costs. The aggregation combines gradient signs from benign workers using reliability-based weights, allowing the system to maintain convergence even when up to half of the workers are adversarial.

## Key Results
- signSGD-FD maintains convergence rate independent of the number of adversarial workers (L < M/2)
- Achieves better or comparable test accuracy than baselines (signSGD-MV, election-signSGD, sto-signSGD, noisy-signSGD) under SIA (r=1) and SSFA (r=0.5) attacks
- Reduces communication costs by transmitting only sign information (1 bit per gradient coordinate) compared to full precision gradients
- Shows graceful degradation with increasing adversarial workers, maintaining performance when L < M/2

## Why This Works (Mechanism)
The core mechanism leverages the fact that benign workers' gradients have higher probability of correct decoding compared to adversarial workers who intentionally send misleading gradient signs. By estimating the log-likelihood ratio of correct vs. incorrect decoding for each worker, the server can assign higher weights to reliable workers in the majority voting aggregation. This creates a federated defense that naturally filters out adversarial contributions while amplifying the signal from trustworthy workers, enabling convergence even in the presence of significant adversarial presence.

## Foundational Learning
- SignSGD gradient quantization: Only the sign of gradients is transmitted, reducing communication from 32/64 bits to 1 bit per coordinate. Why needed: Dramatically reduces communication costs in distributed learning.
- Log-likelihood ratio estimation: Computing the probability ratio of correct vs. incorrect gradient decoding for each worker. Why needed: Provides quantitative measure of worker reliability for weighted aggregation.
- Weighted majority voting: Aggregation that weights each worker's contribution by their estimated reliability score. Why needed: Allows the system to prioritize benign workers while minimizing adversarial impact.

## Architecture Onboarding
Component map: Workers -> Gradient sign transmission -> Server aggregation (WMV with LLR weights) -> Model update
Critical path: Worker gradient computation → Sign quantization → Server reliability estimation → Weighted aggregation → Model parameter update
Design tradeoffs: SignSGD-FD trades minimal additional computation for significant communication reduction and attack resilience. The weighted aggregation adds overhead but enables defense against powerful adversarial attacks.
Failure signatures: signSGD-FD fails to converge when L ≥ M/2 due to decoding error probability exceeding 0.5, making it impossible to distinguish benign from adversarial workers.
First experiments:
1. Implement signSGD-FD with WMV aggregation using LLR weights on MNIST with 15 workers, testing L=3 adversarial workers
2. Compare convergence under SIA (r=1) vs. SSFA (r=0.5) attacks to validate attack-specific resilience
3. Measure communication cost savings (MB) compared to full-precision gradient aggregation baselines

## Open Questions the Paper Calls Out
None

## Limitations
- The theoretical bound of L < M/2 is only validated experimentally for L=3 with M=15, leaving uncertainty about performance at the theoretical boundary
- Exact quantization scheme for sto-signSGD with b=0.012 parameter is unspecified, making precise reproduction challenging
- The noisy signSGD noise standard deviation of b=1e-3 is not clearly defined in terms of source distribution

## Confidence
- High confidence in core signSGD-FD algorithm and its resilience claims under stated conditions
- Medium confidence in exact reproduction due to quantization/noise parameter ambiguities
- Medium confidence in empirical validation given limited attack scenario testing

## Next Checks
1. Test signSGD-FD with L=M/2-1 adversarial workers to validate the theoretical bound
2. Implement alternative quantization schemes for sto-signSGD and compare results
3. Vary the noise level in noisy signSGD to understand its impact on convergence and defense