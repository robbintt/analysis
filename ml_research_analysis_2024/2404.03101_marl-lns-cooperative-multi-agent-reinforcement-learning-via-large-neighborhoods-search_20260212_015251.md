---
ver: rpa2
title: 'MARL-LNS: Cooperative Multi-agent Reinforcement Learning via Large Neighborhoods
  Search'
arxiv_id: '2404.03101'
source_url: https://arxiv.org/abs/2404.03101
tags:
- neighborhood
- training
- algorithm
- agents
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a framework to reduce training time for cooperative
  multi-agent reinforcement learning (MARL) by using subsets of agents during training.
  The proposed MARL-LNS framework divides training into iterations, each focusing
  on a subset of agents, and then alternates between different subsets.
---

# MARL-LNS: Cooperative Multi-agent Reinforcement Learning via Large Neighborhoods Search

## Quick Facts
- arXiv ID: 2404.03101
- Source URL: https://arxiv.org/abs/2404.03101
- Authors: Weizhe Chen; Sven Koenig; Bistra Dilkina
- Reference count: 19
- Primary result: Reduces MARL training time by at least 10% using subset-based training

## Executive Summary
This paper introduces MARL-LNS, a framework that reduces training time for cooperative multi-agent reinforcement learning by focusing on subsets of agents during training. The approach divides training into iterations, each concentrating on a subset of agents, and alternates between different subsets. Three algorithm variants are provided: RLNS (random selection), BLNS (batch-based selection), and ALNS (adaptive selection). Theoretical analysis demonstrates that convergence is maintained despite the reduced joint action space. Experiments on SMAC and GRF benchmarks show at least 10% reduction in training time while preserving final performance compared to the original algorithm.

## Method Summary
MARL-LNS operates by partitioning the set of agents into subsets and training on these subsets sequentially. The framework includes three selection strategies: RLNS selects subsets randomly, BLNS uses a batch-based approach, and ALNS employs an adaptive selection mechanism. During each iteration, only the selected subset of agents is trained while others remain fixed. This reduces the complexity of the joint action space and accelerates learning. The method is designed to be orthogonal to existing optimization techniques, allowing for potential compounding performance improvements when combined with other approaches.

## Key Results
- Achieves at least 10% reduction in training time on SMAC and GRF benchmarks
- Maintains the same final performance as full-agent training
- Demonstrates convergence despite reduced joint action space
- Method is orthogonal to other optimization techniques

## Why This Works (Mechanism)
The framework reduces training complexity by limiting the number of agents involved in joint decision-making at any given time. This decreases the dimensionality of the joint action space, which is a major bottleneck in MARL. By cycling through different subsets, the algorithm ensures that all agents eventually learn to coordinate with each other, while benefiting from faster per-iteration learning. The theoretical analysis confirms that convergence properties are preserved despite the reduced action space, as the alternating subset approach still covers all necessary coordination patterns over time.

## Foundational Learning
- Multi-agent reinforcement learning (MARL): Framework for multiple agents learning simultaneously in a shared environment; needed to understand the problem domain and why joint action spaces are computationally expensive
- Large Neighborhoods Search (LNS): Optimization technique that iteratively solves subproblems; needed to understand the inspiration behind subset-based training
- Joint action space complexity: The exponential growth of possible action combinations as agent count increases; needed to understand the computational bottleneck being addressed
- Convergence analysis in MARL: Mathematical proofs showing learning algorithms reach optimal policies; needed to validate that subset training doesn't compromise final performance
- Orthogonal optimization techniques: Methods that can be combined without interfering with each other; needed to understand the framework's compatibility with other improvements

## Architecture Onboarding

Component Map:
Training Loop -> Subset Selection (RLNS/BLNS/ALNS) -> Agent Training (subset only) -> Policy Update -> Coordination with other subsets

Critical Path:
The critical path involves the iterative process of selecting agent subsets, training those agents while freezing others, and then cycling to new subsets. Each iteration updates policies based on the subset's experiences, gradually building coordination knowledge across the entire agent population.

Design Tradeoffs:
- Subset size vs. training speed: Smaller subsets train faster but may slow convergence due to less frequent coordination practice
- Selection strategy complexity vs. performance: ALNS provides better adaptive selection but requires additional computation
- Training stability vs. exploration: Freezing agents during subset training can reduce exploration diversity but improves stability

Failure Signatures:
- If subsets are too small or training cycles too infrequent, coordination may never fully develop
- Random selection might miss critical coordination patterns between specific agent pairs
- Adaptive selection could get stuck in local optima if the selection metric is poorly designed

3 First Experiments:
1. Test RLNS with varying subset sizes (10%, 25%, 50% of agents) to find optimal balance
2. Compare BLNS and ALNS performance to establish if adaptive selection provides measurable benefits
3. Combine MARL-LNS with curriculum learning to test orthogonal optimization claims

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical convergence guarantee needs validation across diverse environments beyond SMAC and GRF
- 10% training time reduction claim requires scrutiny as performance gains vary by domain and agent configuration
- Orthogonal nature to other optimization techniques may not hold when combined with methods that also modify agent selection patterns

## Confidence
High: The framework's core concept of subset-based training and its theoretical convergence properties
Medium: The 10% training time reduction claim and the orthogonal nature of the method
Low: Performance in extremely large-scale multi-agent systems and the method's effectiveness with heterogeneous agent capabilities

## Next Checks
1. Test MARL-LNS across a broader range of multi-agent environments, including those with heterogeneous agents and varying communication topologies
2. Conduct ablation studies to quantify the individual contributions of RLNS, BLNS, and ALNS variants
3. Evaluate the framework's performance when combined with other state-of-the-art MARL optimization techniques to verify the orthogonal nature claim