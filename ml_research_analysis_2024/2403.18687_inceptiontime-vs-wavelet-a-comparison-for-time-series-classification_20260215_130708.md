---
ver: rpa2
title: InceptionTime vs. Wavelet -- A comparison for time series classification
arxiv_id: '2403.18687'
source_url: https://arxiv.org/abs/2403.18687
tags:
- data
- wavelet
- time
- network
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper compares two neural network approaches for classifying
  infrasound signals into 8 event categories. The direct approach uses a custom InceptionTime
  network trained on raw time series data, while the wavelet approach applies wavelet
  transformation to create 2D images that are classified using ResNet.
---

# InceptionTime vs. Wavelet -- A comparison for time series classification

## Quick Facts
- arXiv ID: 2403.18687
- Source URL: https://arxiv.org/abs/2403.18687
- Reference count: 7
- Primary result: Direct InceptionTime approach achieves 95.2% accuracy vs. wavelet approach's 90.2% accuracy on infrasound signal classification

## Executive Summary
This study compares two neural network approaches for classifying infrasound signals into 8 event categories. The direct approach uses a custom InceptionTime network trained on raw time series data, while the wavelet approach applies wavelet transformation to create 2D images that are classified using ResNet. Both methods achieve high accuracy, with the direct approach reaching 95.2% accuracy compared to the wavelet approach's 90.2%. The direct approach demonstrates superior performance and faster training due to processing fewer data points per signal. The study shows the potential for applying these AI methods to various time-series classification problems in scientific and engineering domains.

## Method Summary
The study uses 2400 simulated infrasound signals with 94 data points each, randomly split into 1920 training and 480 validation samples (20%). Two approaches are compared: (1) a direct approach using InceptionTime network on raw time series data, and (2) a wavelet approach that transforms signals into 2D images using Morlet wavelet with full frequency band, then classifies with ResNet50. Both approaches leverage transfer learning through pre-trained models using fastai and tsai libraries, with the direct approach using learning rate range [1e-6, 1e-2] for 33 epochs and the wavelet approach using learning rate 2e-2.

## Key Results
- Direct InceptionTime approach achieves 95.2% validation accuracy
- Wavelet + ResNet approach achieves 90.2% validation accuracy
- Direct approach processes only 94 data points per signal vs. 8,836 data points for wavelet approach, resulting in faster training
- Both approaches leverage transfer learning for accelerated training convergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The direct approach outperforms the wavelet approach because it processes fewer data points per signal.
- Mechanism: The InceptionTime network operates directly on the 94 data points of each time series signal, while the wavelet approach first transforms the signal into a 2D image representation containing 94 frequency steps x 94 time steps = 8,836 data points that must be processed by ResNet.
- Core assumption: The computational efficiency gained by processing fewer data points directly translates to better performance.
- Evidence anchors:
  - [section] "due to these advantages, the direct approach with the InceptionTime network shows a better performance. Concerning the splits between training and validation sets, only random splits were used to ensure no underlying systematic trend in the data is exploited."
  - [section] "since in the direct approach only 94 datapoints had to be processed for each signal, compared to 942 = 8836 datapoints in the wavelet approach"

### Mechanism 2
- Claim: Transfer learning with pre-trained models significantly accelerates training convergence.
- Mechanism: Both approaches leveraged pre-trained neural networks (InceptionTime and ResNet) through the fastai and tsai libraries, allowing them to start with weights already optimized for similar tasks rather than training from scratch.
- Core assumption: The pre-trained models' learned features are sufficiently transferable to the infrasound classification task.
- Evidence anchors:
  - [section] "we used the fastai [2] and tsai libraries [3] extensively. Both libraries support GPU utilization significantly speeding up the training process compared to pure CPU calculations. Additionally, they also provide pre-trained neural networks, which we used for transfer learning to reduce the number of calculations necessary to train the network."

### Mechanism 3
- Claim: Standardization of input data improves neural network training stability and convergence.
- Mechanism: The TSDataLoaders class groups data into batches of 64 signals and standardizes each batch according to the mean and standard deviation of its data points, ensuring consistent input scaling across training iterations.
- Core assumption: Neural networks perform better when input features have similar scales and distributions.
- Evidence anchors:
  - [section] "TSDataLoaders groups both data sets into batches of 64 signals and standardizes each batch according to the mean and standard deviation of the data points it contains."

## Foundational Learning

- Concept: Time series classification fundamentals
  - Why needed here: Understanding how to extract meaningful patterns from sequential data is essential for both approaches
  - Quick check question: What distinguishes time series classification from other classification tasks?

- Concept: Wavelet transformation and spectral analysis
  - Why needed here: The wavelet approach relies on converting time series into 2D spectral representations
  - Quick check question: How does a wavelet transform differ from a Fourier transform in analyzing time series data?

- Concept: Transfer learning in neural networks
  - Why needed here: Both approaches utilize pre-trained models to accelerate training
  - Quick check question: What are the key considerations when applying transfer learning to a new domain?

## Architecture Onboarding

- Component map: Data preprocessing pipeline (TSDataLoaders/ImageDataLoaders) -> Neural network architecture (InceptionTime for direct, ResNet for wavelet) -> Training loop with validation (fastai/tsai framework) -> Hyperparameter optimization (learning rate, epochs)

- Critical path: Data preprocessing → Model training → Validation accuracy evaluation → Hyperparameter tuning

- Design tradeoffs:
  - Direct approach: Faster training, simpler pipeline, but potentially less feature extraction
  - Wavelet approach: More complex feature extraction, slower training, but may capture different signal characteristics

- Failure signatures:
  - Low accuracy with high variance across runs suggests overfitting or insufficient model capacity
  - Training accuracy much higher than validation accuracy indicates overfitting
  - Very slow training with poor convergence suggests inappropriate learning rate or insufficient computational resources

- First 3 experiments:
  1. Run both approaches with default hyperparameters to establish baseline performance
  2. Tune learning rate using lr_find() to identify optimal learning rate range
  3. Experiment with different batch sizes to balance training stability and memory usage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the classification accuracy change if the model were trained and evaluated on real-world infrasound data instead of simulated data?
- Basis in paper: [inferred] The paper used simulated infrasound signals for training and validation, but mentions that the approach can be generalized to real-world signals
- Why unresolved: The study only tested on simulated data, so the performance on actual field recordings remains unknown
- What evidence would resolve it: Testing the trained models on real infrasound datasets from monitoring stations and comparing the accuracy metrics

### Open Question 2
- Question: Would combining the direct and wavelet approaches through ensemble methods improve classification accuracy beyond what either method achieves individually?
- Basis in paper: [inferred] The paper compares two fundamentally different approaches but doesn't explore combining them
- Why unresolved: The study focused on comparing approaches separately rather than investigating hybrid methods
- What evidence would resolve it: Training an ensemble model that uses both InceptionTime on raw data and ResNet on wavelet-transformed images, then measuring if the combined accuracy exceeds 95.2%

### Open Question 3
- Question: How sensitive are the results to the choice of wavelet function and frequency band parameters in the wavelet approach?
- Basis in paper: [explicit] The paper states "using the full available frequency band... works best" and "with the standard viridis colormap and the morlet wavelet the best results could be achieved"
- Why unresolved: The paper only tested one wavelet type (Morlet) and frequency band configuration, despite acknowledging that different parameters were tested
- What evidence would resolve it: Systematic testing of multiple wavelet families (Haar, Daubechies, etc.) and various frequency band selections with corresponding accuracy measurements

## Limitations

- The study uses simulated infrasound signals rather than real-world measurements, limiting generalizability to actual sensor data
- Only random train/validation splits are used without cross-validation or temporal splits that might better represent real deployment scenarios
- The comparison is limited to a single problem domain with specific signal characteristics (94 data points per signal)

## Confidence

**High Confidence**: The direct approach achieves superior accuracy (95.2% vs 90.2%) and faster training due to processing fewer data points.

**Medium Confidence**: The superiority of the direct approach generalizes to other time series classification problems.

**Medium Confidence**: The computational efficiency gains directly translate to better performance.

## Next Checks

1. **Cross-domain validation**: Test both approaches on real-world infrasound datasets and other time series classification problems (e.g., medical signals, sensor data) to assess generalizability.

2. **Hyperparameter sensitivity analysis**: Systematically vary learning rates, batch sizes, and network architectures for both approaches to determine if the performance gap persists across different configurations.

3. **Ablation study on preprocessing**: Compare the wavelet approach with and without the wavelet transformation to quantify how much of the performance difference stems from the preprocessing step versus the neural network architecture itself.