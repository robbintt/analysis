---
ver: rpa2
title: Does ChatGPT Have a Poetic Style?
arxiv_id: '2410.15299'
source_url: https://arxiv.org/abs/2410.15299
tags:
- poems
- poetry
- lines
- gpt-4
- forms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether ChatGPT exhibits a distinct poetic
  style by generating 5.7k poems across 24 forms, 40 subjects, and 3 prompt templates,
  then comparing them to 3.7k human poems. The analysis reveals that GPT models, particularly
  GPT-4, can produce poems of appropriate lengths for forms like sonnets (14 lines),
  villanelles (19 lines), and sestinas (39 lines).
---

# Does ChatGPT Have a Poetic Style?

## Quick Facts
- arXiv ID: 2410.15299
- Source URL: https://arxiv.org/abs/2410.15299
- Authors: Melanie Walsh; Anna Preus; Elizabeth Gronski
- Reference count: 40
- Primary result: GPT models exhibit strong stylistic uniformity in poetry generation, favoring quatrains, iambic meter, end rhyme, and specific vocabulary despite form-specific prompts

## Executive Summary
This study investigates whether ChatGPT exhibits a distinct poetic style by generating 5.7k poems across 24 forms, 40 subjects, and 3 prompt templates, then comparing them to 3.7k human poems. The analysis reveals that GPT models, particularly GPT-4, can produce poems of appropriate lengths for forms like sonnets (14 lines), villanelles (19 lines), and sestinas (39 lines). However, the models demonstrate strong stylistic uniformity, favoring quatrains, iambic meter, end rhyme, first-person plural perspective, and vocabulary like "heart," "embrace," "echo," and "whisper." This "default poetic mode" persists even when prompted for different forms, indicating constrained and formulaic generation compared to human poetry.

## Method Summary
The researchers generated 5.7k poems using GPT-3.5 and GPT-4 across 24 poetic forms, 40 subjects, and 3 prompt templates, then compared these to 3.7k human poems from Poetry Foundation and Academy of American Poets. They analyzed structural features including line counts, stanza patterns, pronoun usage, distinctive vocabulary using "fightin' words" algorithm, and prosody (rhyme and meter) using CMU Pronouncing Dictionary. The study employed both quantitative analysis and manual annotation to identify patterns and compare GPT-generated poetry to human-authored works.

## Key Results
- GPT models produce poems of correct conventional lengths for fixed forms (sonnets=14 lines, villanelles=19 lines, sestinas=39 lines), with GPT-4 showing higher precision than GPT-3.5
- Over 80% of GPT-generated poems contain end rhyme patterns compared to 50% of human poems
- GPT models exhibit strong stylistic uniformity, favoring quatrains, iambic meter, end rhyme, first-person plural perspective, and vocabulary like "heart," "embrace," "echo," and "whisper"
- The default poetic mode persists across different prompt templates and subject matters, indicating constrained generation compared to human poetry

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT models produce poetry with constrained stylistic uniformity despite form-specific prompts.
- Mechanism: The models default to a recognizable poetic mode—quatrains, iambic meter, end rhyme, first-person plural pronouns, and specific vocabulary (e.g., "heart," "echo," "whisper").
- Core assumption: LLMs have learned and internalized a dominant stylistic template from training data, which overrides fine-grained form instructions.
- Evidence anchors:
  - [abstract] "But the GPT models also exhibit their own distinct stylistic tendencies, both within and outside of these specific forms."
  - [section 4.3] "Overall, the distinguishing vocabulary for the GPT models consists of words associated with love ("heart," "love," "souls"), words that rhyme ("grace", "embrace"), and words that are acoustic ("echo," "whisper")."
  - [corpus] Strong z-score peaks for "in," "upon," "beneath," "behold," "within" in GPT-4; these are typical iambic starters.
- Break condition: If prompts explicitly override defaults (e.g., "do not use iambic meter") or if fine-tuning is applied, the default mode weakens.

### Mechanism 2
- Claim: GPT models can generate poems of the correct conventional length for fixed forms (sonnet=14 lines, villanelle=19 lines, sestina=39 lines) with higher precision in GPT-4.
- Mechanism: The model has memorized length conventions from training data and applies them automatically when prompted for fixed forms.
- Core assumption: Memorization of canonical form lengths is sufficient for accurate length generation without explicit instruction.
- Evidence anchors:
  - [abstract] "such as by producing poems of appropriate lengths for sonnets (14 lines), villanelles (19 lines), and sestinas (39 lines)."
  - [section 4.1] "GPT-4 hues closer to "conventional" lengths than our sample of poems from the Poetry Foundation and the Academy of American Poets."
  - [corpus] Boxplot shows GPT-4 median exactly 14 lines for sonnets vs GPT-3.5 with wider IQR and max outliers at 55 lines.
- Break condition: If form is non-canonical or rarely seen in training data, the model may default to generic length (~36 lines).

### Mechanism 3
- Claim: GPT-generated poetry is significantly more uniform in rhyme and meter usage than human poetry.
- Mechanism: The models' training and architecture favor regularity; they apply rhyme and iambic meter as a default style unless explicitly prompted otherwise.
- Core assumption: Training data distribution biases the model toward metered, rhymed verse, especially older, canonical poetry.
- Evidence anchors:
  - [abstract] "GPT poetry is much more constrained and uniform than human poetry, showing a strong penchant for rhyme, quatrains (4-line stanzas), iambic meter..."
  - [section 4.4] "Over 80% of the GPT-generated poems in our random sample contain patterns of end rhyme, as compared with around 50% of the human-authored poems."
  - [corpus] Quantitative rhyme analysis: GPT-3.5 ~90% poems with at least one rhyme vs 65% human; GPT-4 ~90% vs 65%.
- Break condition: Prompting for "free verse" or "blank verse" reduces iambic dominance in GPT-4; GPT-4 also shows slightly less iambic meter than GPT-3.5.

## Foundational Learning

- Concept: Poetic forms and their defining characteristics (e.g., sonnet=14 lines, iambic pentameter, rhyme scheme; villanelle=19 lines, repeating lines, ABA rhyme; sestina=39 lines, end-word repetition).
  - Why needed here: Understanding these forms is essential to evaluate whether GPT models actually generate poems in the correct form.
  - Quick check question: What is the standard number of lines in a villanelle, and what is its rhyme scheme?

- Concept: Prosody—patterns of sound in poetry, including rhyme (end, internal, slant) and meter (iambic, trochaic, anapestic, etc.).
  - Why needed here: The study relies on identifying meter and rhyme automatically, which requires understanding what these terms mean and how they are measured.
  - Quick check question: What is an iamb, and how does iambic meter differ from trochaic meter?

- Concept: Stanza forms and their typical usage (e.g., quatrain=4 lines, couplet=2 lines, tercet=3 lines).
  - Why needed here: The study shows that GPT models disproportionately favor quatrains; understanding stanzas is key to interpreting this bias.
  - Quick check question: How many lines are in a tercet, and how does this compare to a quatrain?

## Architecture Onboarding

- Component map: Scrape Poetry Foundation + Academy of American Poets → clean prefatory text → annotate forms/subjects → Combine 3 templates × 24 styles × 40 subjects → zero-shot prompts to GPT-3.5 and GPT-4 → Store generated poems per model, per prompt combination → Parse line breaks for stanza/line counts → Extract pronouns → Apply "fightin' words" algorithm for distinctive vocabulary → Use CMU Pronouncing Dictionary for rhyme → Manual prosody annotation of random samples → Compare with human poetry corpus

- Critical path: Generate prompts and run through GPT models → Parse and structure generated poems → Compute structural features (length, stanzas, rhyme, meter) → Compare distributions and patterns to human corpus → Interpret results and document default stylistic tendencies

- Design tradeoffs: Scraping only tagged poems limits corpus diversity but ensures labeled forms; using zero-shot prompts avoids fine-tuning but may underestimate model capability; manual prosody annotation is slow but necessary due to ambiguity in GPT outputs

- Failure signatures: If line parsing fails, stanza counts will be incorrect; if rhyme dictionary lacks pronunciations, rhyme detection will miss matches; if pronoun frequency analysis miscounts (e.g., "we" as a noun), perspective metrics will be skewed

- First 3 experiments: Run GPT-3.5 and GPT-4 with the "General" prompt template only; compare line counts and stanza distributions → Extract the 10 most distinctive words from GPT-3.5 poems and verify z-score ranking manually → Select 10 random GPT-4 poems in "free verse" style and manually annotate for iambic meter vs. other meters; compute percentage

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does memorization of canonical poetry in GPT models influence their ability to produce original poetic forms and styles?
- Basis in paper: [explicit] The paper discusses D'Souza and Mimno's finding that memorization is the key factor for poem memorization and notes that 41% of sampled human poems are likely memorized by GPT-4, suggesting canonical poetry is overrepresented in training data.
- Why unresolved: The paper acknowledges memorization may enhance form classification but states "more work is needed to evaluate the impact that memorization may have on poetry generation."
- What evidence would resolve it: Comparative analysis of GPT-generated poetry when trained on different poetry corpora (canonical vs. contemporary vs. diverse sources) to measure variations in originality and form adherence.

### Open Question 2
- Question: How does prompt engineering affect the stylistic constraints observed in GPT-generated poetry?
- Basis in paper: [explicit] The authors note that prompting significantly impacts the poetry produced and suggest that different prompts (including author names) might yield more interesting results, but their study used standardized prompts to understand "broad contours" rather than maximize creativity.
- Why unresolved: The study deliberately used zero-shot prompts without fine-tuning or specific author name prompts, leaving the question of how different prompting strategies might overcome observed limitations.
- What evidence would resolve it: Systematic testing of various prompt structures, including author-style prompts, iterative refinement prompts, and meta-prompts that explicitly address the model's default tendencies.

### Open Question 3
- Question: What explains the persistent use of first-person plural perspective in GPT-generated poetry, and does this reflect genuine inclusivity preferences or other factors?
- Basis in paper: [explicit] The paper identifies a "curiously dominant first-person plural perspective" and suggests it might reflect "pre-programmed attitudes toward inclusivity" or the model's "lack of first-person singular experiences."
- Why unresolved: The authors acknowledge this as an interesting pattern requiring further exploration but don't investigate the underlying mechanisms or test alternative explanations.
- What evidence would resolve it: Analysis of GPT's training data for collective vs. individual perspective representation, experiments with prompts explicitly requesting different perspectives, and comparison with other LLM poetry generation models.

## Limitations

- The study relies on zero-shot prompting without exploring fine-tuned models or few-shot examples, potentially underestimating the models' full capabilities
- Automated metrics (rhyme detection via CMU Pronouncing Dictionary, pronoun frequency analysis) may miss nuanced poetic devices or contain false positives
- The human poetry corpus is limited to Poetry Foundation and Academy of American Poets selections, potentially biasing comparisons toward contemporary American poetry

## Confidence

**High Confidence**: GPT models produce poems of correct conventional lengths for fixed forms; GPT-generated poetry shows significantly higher rates of rhyme and iambic meter than human poetry

**Medium Confidence**: Distinctive vocabulary patterns and "default poetic mode" preferences reflect training data bias but show stability across prompts

**Low Confidence**: Claims about "constrained and formulaic generation" involve subjective judgments about creativity and artistic merit that are difficult to quantify

## Next Checks

1. **Fine-tuning impact**: Generate poetry using GPT models with few-shot examples or fine-tuning on diverse poetic styles to test whether default stylistic tendencies persist when models receive explicit style guidance

2. **Cross-cultural validation**: Expand the human poetry corpus to include non-Western poetic traditions (haiku, tanka, ghazals, etc.) and test whether GPT models can accurately generate these forms or default to Western conventions

3. **Longitudinal analysis**: Compare poetry generated by different GPT versions over time to determine whether default stylistic tendencies evolve with model updates, and test whether newer versions show more stylistic diversity