---
ver: rpa2
title: On the VC dimension of deep group convolutional neural networks
arxiv_id: '2410.15800'
source_url: https://arxiv.org/abs/2410.15800
tags:
- gcnn
- function
- layer
- class
- group
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper establishes nearly-tight bounds for the Vapnik-Chervonenkis
  (VC) dimension of group convolutional neural networks (GCNNs), providing a theoretical
  framework to analyze their generalization capabilities. The authors derive upper
  and lower bounds for the VC dimension, showing that it scales linearly with the
  number of layers and weights, and logarithmically with the resolution of the input
  group.
---

# On the VC dimension of deep group convolutional neural networks

## Quick Facts
- arXiv ID: 2410.15800
- Source URL: https://arxiv.org/abs/2410.15800
- Authors: Anna Sepliarskaia; Sophie Langer; Johannes Schmidt-Hieber
- Reference count: 5
- Primary result: Establishes nearly-tight bounds for VC dimension of GCNNs, showing linear scaling with layers/weights plus logarithmic resolution term

## Executive Summary
This paper establishes theoretical bounds for the Vapnik-Chervonenkis (VC) dimension of group convolutional neural networks (GCNNs), providing a framework to analyze their generalization capabilities. The authors derive both upper and lower bounds that scale linearly with network depth and parameter count, but include an additional logarithmic term in the resolution of the discretized group. This result quantifies how GCNNs' sensitivity to group discretization impacts their complexity compared to standard deep neural networks.

## Method Summary
The authors employ mathematical analysis of the hypothesis space of GCNNs with ReLU activations to compute VC dimension bounds. The approach involves characterizing the output functions as piecewise polynomials and counting the maximum number of sign patterns that can be realized. Upper bounds are derived using combinatorial arguments about the number of linear regions created by ReLU activations across the group convolution operations. Lower bounds are established through construction of specific indicator networks that shatter particular sets of group elements. The analysis leverages the correspondence between GCNNs and deep feedforward networks while accounting for the group structure's impact on hypothesis space complexity.

## Key Results
- Upper bound: U_B(H) = L + 1 + 4(∑ℓ=1L W_ℓ) log_2(8er/∑ℓ=1L m_ℓ)
- Lower bound: V_C(H_W,L,r) ≥ max{c V_C(F_W,L), 1/4 W ⌊log_2 r⌋}
- Bounds are nearly tight, differing by constant factors and the resolution term
- VC dimension scales linearly with layers/weights like DNNs but includes logarithmic resolution dependence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The VC dimension of GCNNs scales linearly with the number of layers and weights, similar to DNNs, but includes an additional logarithmic term in the resolution.
- Mechanism: The structural equivariance in GCNNs constrains the hypothesis space more than general DNNs, leading to tighter bounds when the group structure is respected.
- Core assumption: The discretized group operations behave predictably under composition and do not introduce pathological cases.
- Evidence anchors:
  - [abstract] "the upper bound is expressed as U_B(H) = L + 1 + 4(∑ℓ=1L W_ℓ) log_2(8er/∑ℓ=1L m_ℓ)"
  - [section] "The VC dimension further depends on the network depth, as discussed in (Bartlett et al., 2019)"
- Break condition: If the discretization of the group introduces discontinuities or if the equivariance property fails to constrain the hypothesis space effectively.

### Mechanism 2
- Claim: The logarithmic dependence on resolution captures the sensitivity of GCNNs to the granularity of group discretization.
- Mechanism: Finer discretization increases the number of possible sign patterns the network can generate, effectively expanding the hypothesis space.
- Core assumption: The group convolution operations remain well-defined and stable under increasing resolution.
- Evidence anchors:
  - [abstract] "an additional logarithmic term in the resolution, highlighting their sensitivity to discretization of the group"
  - [section] "The group convolution computes a weighted average over the group elements... heavily depends on the discretization of the group"
- Break condition: If the discretized group becomes too fine, the VC dimension may become infinite as suggested by prior work.

### Mechanism 3
- Claim: The bounds are nearly tight, with lower bounds matching upper bounds up to constant factors and the resolution term.
- Mechanism: The construction of shattering sets using indicator networks and the correspondence between DNNs and GCNNs ensures that the bounds capture the true complexity.
- Core assumption: The indicator networks can approximate any desired function within the group structure.
- Evidence anchors:
  - [section] "VC(HW,L,r ) ≥ max{c VC(FW,L ), 1/4 W ⌊log_2 r⌋}"
  - [section] "there exist universal constants c and C such that c(VC(FW,L ) + W log2(r)) ≤ VC(HW,L,r ) ≤ C(VC(FW,L ) + LW log2(r))"
- Break condition: If the construction of shattering sets fails or if the correspondence between DNNs and GCNNs breaks down.

## Foundational Learning

- Concept: Group theory and group actions
  - Why needed here: GCNNs rely on group structures to define equivariant operations; understanding group actions is essential to grasp how the networks process data.
  - Quick check question: What is the difference between a left regular representation and a right regular representation of a group?

- Concept: VC dimension and growth function
  - Why needed here: The paper uses VC dimension to quantify the generalization capability of GCNNs; understanding growth functions and shattering is crucial.
  - Quick check question: How does the VC dimension relate to the growth function for a hypothesis class?

- Concept: Piecewise polynomial functions and ReLU activations
  - Why needed here: ReLU activations make the network outputs piecewise linear/polynomial, which is key to bounding the number of sign patterns.
  - Quick check question: Why does applying ReLU to a piecewise linear function increase the number of pieces but not the degree?

## Architecture Onboarding

- Component map:
  Input -> G-convolution layer (with learnable kernels and ReLU) -> Feature maps -> ... (repeated L times) -> Global average pooling -> Output

- Critical path:
  1. Discretize the group to obtain Gr
  2. Apply G-convolution layers with ReLU activations
  3. Compute outputs via global average pooling
  4. Optimize parameters to minimize loss

- Design tradeoffs:
  - Resolution vs. VC dimension: Higher resolution increases expressive power but also complexity
  - Number of layers vs. generalization: More layers increase capacity but may require more data
  - Kernel dimension vs. parameter efficiency: Higher-dimensional kernels increase expressiveness but also parameters

- Failure signatures:
  - High VC dimension relative to sample size → overfitting
  - Discretization artifacts → poor generalization on continuous groups
  - Vanishing/exploding gradients in deep architectures

- First 3 experiments:
  1. Verify the upper bound by constructing GCNNs with varying numbers of layers and measuring empirical VC dimension via growth function.
  2. Test sensitivity to resolution by training on increasingly fine discretizations and measuring performance degradation.
  3. Compare generalization of GCNNs vs. DNNs on equivariant data to validate the theoretical bounds.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the logarithmic term in the VC dimension bound (W ⌊log2 r⌋) impact the practical sample complexity of GCNNs compared to DNNs when r is large?
- Basis in paper: [explicit] The paper establishes that the VC dimension of GCNNs includes an additional term W ⌊log2 r⌋ compared to DNNs, highlighting the sensitivity of GCNNs to the discretization of the group.
- Why unresolved: While the theoretical bound is established, empirical studies are needed to quantify the practical impact of this term on sample complexity in real-world scenarios with varying resolutions r.
- What evidence would resolve it: Experiments comparing the sample complexity of GCNNs and DNNs on tasks with different group resolutions, showing how the logarithmic term affects the required training data.

### Open Question 2
- Question: Can the bounds on the VC dimension of GCNNs be tightened for specific types of groups (e.g., compact vs. non-compact) or architectures (e.g., sparse connections)?
- Basis in paper: [inferred] The current bounds are general and apply to any discretized group. The paper does not explore whether these bounds can be improved for specific cases or architectures.
- Why unresolved: The generality of the bounds may lead to loose estimates for certain group types or architectures. Tailored analysis could yield tighter bounds.
- What evidence would resolve it: Deriving and proving tighter bounds for specific group types (e.g., compact Lie groups) or architectural constraints (e.g., sparse GCNNs).

### Open Question 3
- Question: How does the choice of kernel basis functions in GCNNs affect the VC dimension and generalization capabilities?
- Basis in paper: [explicit] The paper mentions that kernel functions are expressed as linear combinations of basis functions, but does not explore how the choice of these basis functions impacts the VC dimension.
- Why unresolved: Different basis functions may lead to different expressive power and generalization properties, but their effect on the VC dimension is not analyzed.
- What evidence would resolve it: Comparative analysis of GCNNs with different kernel basis functions, showing how the choice affects the VC dimension and generalization performance on benchmark tasks.

## Limitations
- The logarithmic dependence on resolution assumes discretization artifacts remain bounded, but may break down with very fine discretization
- Tightness of bounds relies on specific indicator network constructions that may not generalize to all GCNN architectures
- Universal constants in bounds are not explicitly derived, limiting precise quantitative comparisons

## Confidence
- High: The linear scaling with layers and weights, based on well-established results for deep feedforward networks
- Medium: The logarithmic resolution term, as it depends on assumptions about discretization behavior
- Medium: The nearly-tightness claim, contingent on the specific indicator network constructions

## Next Checks
1. **Discretization Sensitivity Test**: Implement a systematic study of GCNN performance and empirical VC dimension across varying resolutions of a common group (e.g., SO(2) or SE(2)) to verify the logarithmic scaling prediction and identify any thresholds where bounds break down.

2. **Bound Tightness Verification**: Construct specific shattering sets for small GCNNs (2-3 layers) and compare the actual VC dimension against both theoretical bounds to quantify the gap and validate the indicator network approach.

3. **Cross-Architecture Comparison**: Benchmark generalization error and sample complexity of GCNNs versus standard DNNs on rotation/translation equivariant tasks, controlling for parameter count and depth, to empirically test whether the additional logarithmic term in VC dimension translates to measurable performance differences.