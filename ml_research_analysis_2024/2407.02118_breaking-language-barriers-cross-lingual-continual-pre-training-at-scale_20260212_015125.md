---
ver: rpa2
title: 'Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale'
arxiv_id: '2407.02118'
source_url: https://arxiv.org/abs/2407.02118
tags:
- training
- pre-training
- data
- language
- scaling
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates continual pre-training (CPT) as an alternative
  to training language models from scratch for new languages. The authors conduct
  extensive experiments across 40 model sizes (40M to 5B parameters), comparing CPT
  with pre-training from scratch on Chinese language tasks.
---

# Breaking Language Barriers: Cross-Lingual Continual Pre-Training at Scale

## Quick Facts
- **arXiv ID**: 2407.02118
- **Source URL**: https://arxiv.org/abs/2407.02118
- **Reference count**: 10
- **Primary result**: CPT converges faster and saves 25-50% of training tokens compared to training from scratch

## Executive Summary
This paper investigates continual pre-training (CPT) as an efficient alternative to training language models from scratch for new languages. Through extensive experiments across 40 model sizes (40M to 5B parameters), the authors demonstrate that CPT consistently outperforms training from scratch, particularly during initial stages, while achieving lower final loss. The work proposes an extended scaling law that captures CPT behavior with a joint data-parameter scaling term, and shows that compute-optimal allocation for CPT favors larger parameter sizes over larger datasets compared to training from scratch. The study also addresses catastrophic forgetting in CPT, finding that replaying 10-30% of source language data effectively mitigates this issue.

## Method Summary
The authors compare continual pre-training (CPT) against training from scratch for Chinese language models initialized from English pre-trained checkpoints. They experiment with model sizes ranging from 40M to 5B parameters, using identical training configurations for both approaches. The study evaluates cross-entropy loss on held-out validation sets and downstream multilingual benchmark performance. To address catastrophic forgetting, they test different ratios of source language data replay during target language training. The work also extends the Chinchilla scaling law to account for the joint scaling effect of data and parameter size in CPT scenarios.

## Key Results
- CPT converges faster and achieves lower validation loss than training from scratch across all 40 model sizes tested
- CPT saves 25-50% of training tokens while maintaining or improving performance
- Replaying 10-30% of source language data effectively mitigates catastrophic forgetting in CPT
- Compute-optimal allocation for CPT favors larger parameter sizes over larger datasets compared to training from scratch

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CPT preserves lower loss throughout training compared to training from scratch
- Mechanism: By initializing with a pre-trained English model, the Chinese CPT model benefits from transferred meta-knowledge, reducing the loss earlier in training
- Core assumption: The source and target languages share transferable linguistic knowledge
- Evidence anchors:
  - [abstract] "CPT converges faster and saves significant resources in a scalable manner"
  - [section 4.1] "CPT consistently outperforms training from scratch, particularly during the initial stages"
- Break condition: If the source and target languages are too dissimilar, the transfer benefit diminishes or disappears

### Mechanism 2
- Claim: The extended scaling law captures the joint scaling effect of data and parameter size in CPT
- Mechanism: Introducing a multiplicative term $D^{\beta'}N^{\gamma}$ in the loss function accounts for the positive correlation between parameter size and transfer effectiveness
- Core assumption: Larger models store more transferable knowledge, enhancing cross-lingual transfer
- Evidence anchors:
  - [abstract] "CPT adheres to an extended scaling law derived from Hoffmann et al. (2022) with a joint data-parameter scaling term"
  - [section 3.2] "we extend the D term to include a multiplicative joint effect of both D and N"
- Break condition: If the parameter size does not correlate with transferable knowledge, the joint scaling term may not improve the model

### Mechanism 3
- Claim: Replaying 10-30% of source language data mitigates catastrophic forgetting in CPT
- Mechanism: Mixing source language data during target language training preserves knowledge of the source distribution
- Core assumption: The model can retain source language performance while adapting to the target language
- Evidence anchors:
  - [abstract] "replaying 10-30% of source language data effectively mitigates this issue"
  - [section 5.2] "different ratios of replaying only affect the early stage of training" and "a large amount of original knowledge is preserved"
- Break condition: If the replay ratio is too low (<10%) or too high (>30%), catastrophic forgetting may not be effectively mitigated or target language performance may degrade

## Foundational Learning

- **Scaling laws in neural network training**
  - Why needed here: Understanding how loss scales with model size and training data is crucial for analyzing CPT efficiency
  - Quick check question: What is the relationship between model parameters, training tokens, and validation loss according to the Chinchilla scaling law?

- **Catastrophic forgetting in continual learning**
  - Why needed here: Identifying how CPT can cause the model to forget source language knowledge is essential for developing mitigation strategies
  - Quick check question: How does replaying source language data during target language training help prevent catastrophic forgetting?

- **Cross-lingual transfer learning**
  - Why needed here: Recognizing how knowledge transfers between languages explains why CPT can be more efficient than training from scratch
  - Quick check question: What factors influence the effectiveness of cross-lingual transfer in CPT?

## Architecture Onboarding

- **Component map**: English pre-trained checkpoint -> Chinese CPT model -> Validation loss monitoring -> Downstream benchmark evaluation -> Data replaying for forgetting mitigation
- **Critical path**: 1) Initialize Chinese model with English pre-trained checkpoint 2) Train on Chinese corpus with specified configurations 3) Monitor validation loss and downstream task performance 4) Apply data replaying if catastrophic forgetting is observed
- **Design tradeoffs**: Parameter size vs. training data (larger models may require less data due to stronger transfer effects), Replay ratio (balancing between preserving source language performance and target language adaptation), Tokenizer design (ensuring proper representation of both source and target languages)
- **Failure signatures**: High validation loss throughout training indicates ineffective transfer, Significant degradation in source language performance suggests catastrophic forgetting, Poor downstream task performance implies insufficient adaptation or transfer
- **First 3 experiments**: 1) Compare CPT vs. training from scratch for a small model (e.g., 50M parameters) to observe initial transfer effects 2) Test different replay ratios (e.g., 5%, 20%, 50%) to find the optimal balance for preventing catastrophic forgetting 3) Vary the similarity between source and target languages (e.g., English-French vs. English-Chinese) to assess transfer effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the extended scaling law for CPT behave when the source language and target language are linguistically very dissimilar?
- Basis in paper: [explicit] The paper mentions that the transfer scaling effect is stronger when the target language is more similar to the source language, but doesn't explore the extreme case of very dissimilar languages
- Why unresolved: The experiments focused on English to Chinese transfer, which are somewhat different but share some similarities. The effect of extreme linguistic dissimilarity remains unexplored
- What evidence would resolve it: Experiments comparing CPT with pre-training from scratch for linguistically distant language pairs (e.g., English to Japanese, English to Arabic) would reveal whether the extended scaling law still holds and how the transfer effect changes

### Open Question 2
- Question: What is the optimal amount of English data to replay during CPT for different model sizes and training durations?
- Basis in paper: [inferred] The paper found that 10-30% English data replay effectively mitigates catastrophic forgetting, but doesn't explore the optimal amount for different model sizes and training durations
- Why unresolved: The paper only tested a limited range of English replay ratios (1%, 5%, 10%, 20%, 50%, 80%) and didn't explore how this optimal ratio changes with model size and training duration
- What evidence would resolve it: A systematic study varying English replay ratios across different model sizes and training durations would reveal the optimal amount of source language data to replay for each scenario

### Open Question 3
- Question: How does vocabulary extension during CPT affect the scaling behavior and transfer effectiveness?
- Basis in paper: [explicit] The paper explicitly states that vocabulary extension during CPT was not tested and suggests this is a limitation
- Why unresolved: The experiments used a fixed vocabulary trained on both English and Chinese text, not reflecting practical scenarios where extending the vocabulary to include new tokens is necessary
- What evidence would resolve it: Experiments comparing CPT with and without vocabulary extension for various target languages would reveal how vocabulary changes affect scaling behavior and transfer effectiveness

## Limitations
- Findings based on a single language pair (English to Chinese), limiting generalizability across diverse language families
- Catastrophic forgetting mitigation relies on replaying 10-30% source data, but optimal replay ratio may vary significantly based on task similarity
- Scaling law extension lacks extensive empirical validation across diverse model architectures beyond LLaMA2 decoder-only Transformer

## Confidence
**High Confidence:**
- CPT converges faster than training from scratch (supported by validation loss curves across multiple model sizes)
- CPT achieves lower final loss than training from scratch (consistent across 40 model sizes)
- 25-50% token savings with CPT (directly measured from training curves)

**Medium Confidence:**
- Extended scaling law with joint data-parameter term (theoretical extension of Hoffmann et al., limited empirical validation)
- Compute-optimal allocation favors larger parameters in CPT (scaling analysis based on single language pair)
- Replay ratio of 10-30% effectively mitigates forgetting (demonstrated on specific Chinese-English scenario)

**Low Confidence:**
- Transfer scaling effect strength predictions (limited cross-linguistic validation)
- Catastrophic forgetting mitigation across diverse language pairs (single pair validation)
- Generalization of scaling law to other model architectures (only LLaMA2 tested)

## Next Checks
1. **Cross-linguistic Transfer Validation**: Test CPT efficiency across language pairs with varying linguistic distances (e.g., English→Spanish, English→Japanese, English→Arabic) to validate the claimed transfer scaling effects and identify break conditions where source-target similarity diminishes transfer benefits

2. **Scaling Law Robustness**: Validate the extended scaling law across diverse model architectures (encoder-decoder, decoder-only, and encoder-only variants) and training objectives to confirm whether the joint data-parameter scaling term generalizes beyond LLaMA2 decoder-only models

3. **Catastrophic Forgetting Mitigation Across Domains**: Evaluate catastrophic forgetting and replay ratio effectiveness in domain adaptation scenarios (e.g., general→medical, general→legal) rather than just language adaptation to test whether the 10-30% replay heuristic generalizes to non-linguistic continual learning tasks