---
ver: rpa2
title: Offline Evaluation of Set-Based Text-to-Image Generation
arxiv_id: '2410.17331'
source_url: https://arxiv.org/abs/2410.17331
tags:
- images
- prompts
- evaluation
- systems
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of evaluating Text-to-Image (TTI)
  generation systems, which are often used for ideation tasks. The authors argue that
  existing evaluation metrics, like FID, are insufficient because they focus on distributional
  similarity and don't capture how users interact with generated image sets during
  ideation.
---

# Offline Evaluation of Set-Based Text-to-Image Generation

## Quick Facts
- **arXiv ID:** 2410.17331
- **Source URL:** https://arxiv.org/abs/2410.17331
- **Reference count:** 40
- **Key outcome:** New offline evaluation metrics for TTI systems show better agreement with human preferences by incorporating novelty, diversity, and visual salience

## Executive Summary
This paper addresses the challenge of evaluating text-to-image (TTI) generation systems for ideation tasks. Traditional metrics like FID focus on distributional similarity but fail to capture how users interact with and assess sets of generated images. The authors propose new offline evaluation metrics grounded in information retrieval methods and explicit models of user behavior during image grid browsing. These metrics incorporate relevance, diversity, novelty, and visual salience to better align with human preferences for ideation tasks.

## Method Summary
The paper adapts information retrieval metrics (RBP and ERR) to evaluate sets of TTI-generated images, incorporating novelty discounting and visual salience modeling. The method involves generating images for prompts, predicting saliency maps, sampling user trajectories based on salience distributions, and computing metric scores. Six metric variants are evaluated: RBP, ERR, RBP with novelty (RBPùúÇ), and ERR with novelty (ERRùúÇ), each with uniform and saliency-aware trajectory sampling. Human preference annotations on image grid pairs serve as ground truth for comparison.

## Key Results
- RBPùúÇ with saliency-aware sampling shows highest agreement with human preferences across all prompt datasets
- Metrics incorporating novelty demonstrate significantly better performance than standard RBP/ERR, especially for complex prompts
- Visual salience modeling improves metric performance compared to uniform trajectory sampling
- FID fails to distinguish between systems in many cases where the proposed metrics show clear differences

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The proposed metrics better align with human preferences by modeling user behavior during ideation.
- Mechanism: The metrics incorporate explicit models of how users browse and interact with spatially arranged image grids, including position-based and cascade models for user persistency, novelty discounting, and visual salience prediction.
- Core assumption: Users inspect images in a predictable pattern based on position and visual salience, and this pattern affects their judgment of image set quality.
- Evidence anchors:
  - [abstract] "develop TTI evaluation metrics with explicit models of how users browse and interact with sets of spatially arranged generated images"
  - [section] "We define a family of metrics based on how the user might engage with the images in linear order" and "users tend to look at the most salient images first"
  - [corpus] Weak - corpus papers focus on biases and fairness rather than user behavior modeling
- Break condition: If user browsing patterns are highly variable or context-dependent, the model-based metrics may not generalize well.

### Mechanism 2
- Claim: Incorporating novelty and diversity measures improves metric performance for ideation tasks.
- Mechanism: The metrics discount relevance based on similarity to previously seen images (variety) and incorporate novelty through diversity-aware ranking, addressing the ideation need for exploring design space.
- Core assumption: Users value seeing different and novel images during ideation, not just highly relevant ones.
- Evidence anchors:
  - [section] "In order to measure novelty within a list, we leverage existing methods designed to measure diversity" and "we consider both of these criteria together"
  - [section] "Fluency refers to the total number of relevant items generated. Variety refers to the number of unique types of relevant items generated. Novelty refers to how different relevant items are from all previously generated items"
  - [corpus] Weak - corpus focuses on bias and fairness rather than novelty in ideation
- Break condition: If users primarily seek highly relevant images rather than diverse ones, novelty discounting could reduce metric accuracy.

### Mechanism 3
- Claim: Saliency-aware trajectory sampling better predicts user preferences than uniform sampling.
- Mechanism: The metrics sample user trajectories based on predicted image salience using Plackett-Luce models, rather than assuming uniform distribution over all possible viewing orders.
- Core assumption: Visual salience strongly predicts which images users will inspect first and influences their overall assessment.
- Evidence anchors:
  - [section] "users tend to look at the most salient images first" and "we model Pr(œÄ) using a Plackett-Luce model based on predicted image salience"
  - [section] "Although a na√Øve model might consider a uniform distribution over trajectories, we know that users tend to be more attracted to certain images based on their position and visual features"
  - [corpus] Weak - corpus papers don't address saliency modeling in TTI evaluation
- Break condition: If saliency prediction is inaccurate or users don't consistently prefer more salient images, the trajectory sampling may mispredict user behavior.

## Foundational Learning

- Concept: Information Retrieval evaluation metrics (RBP, ERR, diversity measures)
  - Why needed here: The paper adapts established IR evaluation methods to TTI, so understanding these metrics is crucial for grasping the proposed approach
  - Quick check question: How does rank-biased precision (RBP) differ from expected reciprocal rank (ERR) in modeling user behavior?
  
- Concept: Visual saliency prediction and attention models
  - Why needed here: The metrics incorporate saliency-based trajectory sampling, requiring understanding of how visual attention is modeled
  - Quick check question: What is the difference between bottom-up and top-down visual attention models?
  
- Concept: Text-to-image generation evaluation challenges
  - Why needed here: Understanding why traditional metrics like FID fail for TTI helps motivate the proposed approach
  - Quick check question: Why does FID struggle to capture user preferences in TTI systems?

## Architecture Onboarding

- Component map:
  - Prompt preprocessing ‚Üí Image generation ‚Üí Saliency prediction ‚Üí Trajectory sampling ‚Üí Metric computation ‚Üí Human preference comparison
  - Key components: IR metric variants (RBP, ERR), diversity/novelty functions, saliency model, trajectory sampler
  
- Critical path:
  1. Generate images for prompts using TTI systems
  2. Compute saliency maps for each image
  3. Sample trajectories based on saliency distribution
  4. Calculate metric scores for each system
  5. Compare against human preference annotations
  
- Design tradeoffs:
  - Position-based vs cascade user models: Simpler but less realistic vs more complex but better captures satisfaction behavior
  - Saliency prediction accuracy vs computational cost: Better predictions improve metric quality but increase runtime
  - Novelty discounting strength: Too weak misses diversity benefits, too strong penalizes relevant similar images
  
- Failure signatures:
  - Low agreement with human preferences despite high metric scores
  - Metrics fail to distinguish between systems with obvious quality differences
  - Performance varies dramatically across different prompt datasets
  
- First 3 experiments:
  1. Compare RBP vs RBP with novelty discounting on MS-COCO dataset to verify diversity improves agreement
  2. Test saliency-aware vs uniform trajectory sampling on simple prompts to confirm salience importance
  3. Evaluate position-based vs cascade models on challenging prompts to see which better captures user satisfaction

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we develop customized salience and browsing models specifically tailored for ideation tasks, beyond using general web page gaze data?
- Basis in paper: [explicit] The paper mentions that existing saliency models trained on web pages could be improved by developing models customized for ideation tasks.
- Why unresolved: The paper uses a saliency model trained on web page gaze data, which may not perfectly capture user attention patterns when browsing grids of generated images for ideation purposes.
- What evidence would resolve it: Developing and validating a saliency model trained specifically on gaze data from users browsing TTI image grids during ideation tasks, and demonstrating improved alignment with human preferences compared to general web page models.

### Open Question 2
- Question: How can we extend evaluation metrics to account for the interactive and multi-turn nature of ideation and generation, where users build upon previous generations and interact with the system over time?
- Basis in paper: [explicit] The paper acknowledges that ideation and generation are often interactive processes with multiple turns, and current metrics only capture single-turn effectiveness.
- Why unresolved: The proposed metrics in the paper focus on evaluating single sets of generated images for individual prompts, not capturing the iterative and interactive nature of real-world ideation workflows.
- What evidence would resolve it: Developing and validating evaluation metrics that incorporate information from multiple turns of interaction, such as user modifications to prompts, feedback on previous generations, and the evolution of ideas over time, and demonstrating their ability to better predict user satisfaction with the overall ideation process.

### Open Question 3
- Question: Can the proposed offline evaluation metrics provide insights into designing effective online evaluation metrics for TTI systems that go beyond traditional information retrieval metrics?
- Basis in paper: [explicit] The paper suggests that the results may provide insights into designing online evaluation metrics for TTI based on more elaborate behavior and interaction than found in classic information retrieval settings.
- Why unresolved: While the paper demonstrates the effectiveness of offline metrics for TTI, it does not explore how these findings can be translated into practical online evaluation metrics that capture user behavior and preferences in real-time.
- What evidence would resolve it: Conducting user studies to validate online evaluation metrics derived from the proposed offline metrics, demonstrating their ability to accurately reflect user satisfaction and guide system improvements in real-time interactions with TTI systems.

## Limitations
- The proposed metrics rely heavily on accurate saliency prediction, which the paper does not extensively validate
- The user behavior models (position-based and cascade) are based on assumptions about user persistence that may not generalize across different user populations or task contexts
- The human preference study, while substantial, may not fully capture the diversity of user preferences in real-world ideation scenarios

## Confidence

- **High Confidence:** The adaptation of IR metrics (RBP, ERR) to TTI evaluation is methodologically sound and well-justified
- **Medium Confidence:** The incorporation of novelty and visual salience improves metric alignment with human preferences, though validation of saliency prediction is limited
- **Medium Confidence:** The comparison with FID shows improved performance, but the study design doesn't rule out all alternative explanations

## Next Checks

1. Conduct ablation studies removing saliency prediction to quantify its actual contribution to metric performance
2. Test the metrics across additional TTI systems and datasets to assess generalizability beyond the three systems studied
3. Perform user studies with eye-tracking to directly validate the assumed user browsing patterns and saliency effects