---
ver: rpa2
title: 'Promptly Predicting Structures: The Return of Inference'
arxiv_id: '2401.06877'
source_url: https://arxiv.org/abs/2401.06877
tags:
- inference
- language
- linguistics
- association
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a framework for structured prediction using
  large language models (LLMs) in a zero- or few-shot manner. The key insight is to
  use structural constraints and combinatorial inference to filter out inconsistent
  predictions from LLMs.
---

# Promptly Predicting Structures: The Return of Inference

## Quick Facts
- arXiv ID: 2401.06877
- Source URL: https://arxiv.org/abs/2401.06877
- Authors: Maitrey Mehta; Valentina Pyatkin; Vivek Srikumar
- Reference count: 40
- One-line primary result: A framework for structured prediction using LLMs that guarantees structural consistency through combinatorial inference, achieving accuracy improvements and reduced inconsistency across SRL and coreference tasks.

## Executive Summary
This work introduces a framework for structured prediction using large language models (LLMs) in a zero- or few-shot manner. The key insight is to use structural constraints and combinatorial inference to filter out inconsistent predictions from LLMs. The framework breaks down a structured prediction task into component subtasks, converts each into a prompt-friendly format, and implements an inference algorithm that takes LLM scores and produces valid structured outputs. Experiments on two tasks (Semantic Role Labeling and Coreference Resolution) across five datasets show that the framework guarantees structural consistency and provides performance gains over unconstrained variants.

## Method Summary
The framework factorizes structured prediction into independent local decisions, each scored by an LLM via prompts. These scores become unary potentials feeding into combinatorial inference algorithms. For SRL, K-shortest paths find valid structures in a directed graph of span choices. For coreference, an ILP formulation enforces transitivity constraints. The approach enables predicting valid structured outputs without expensive labeled data.

## Key Results
- Framework guarantees structural consistency across all tested datasets
- Performance improvements in accuracy metrics over unconstrained variants
- Reduction in inconsistency percentages, particularly as model size increases
- Gains are consistent across both Semantic Role Labeling and Coreference Resolution tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-based methods can replace expensive supervised training for structured prediction by converting each local decision into a scored prompt.
- Mechanism: The framework factorizes the structured prediction distribution into independent local decisions P(y_i|X), each scored by a large language model via prompts. These scores become unary potentials that can feed into combinatorial inference algorithms.
- Core assumption: LLMs can reliably generate candidate spans with meaningful scores for each prompt-based question.
- Evidence anchors:
  - [abstract] "Prompt-based methods have been used extensively across NLP to build zero- and few-shot label predictors."
  - [section] "Prompt-based methods can help bypass the need for supervised training. For every local decision y_i, we construct a query q_i to convert the decision into a prompt-friendly format."
  - [corpus] Weak evidence - the related papers focus on graph neural networks and energy-based models, not prompt-based scoring of spans.
- Break condition: If LLM scoring becomes unreliable or highly variable across runs, the unary potentials will be noisy and inference will fail to produce valid structures.

### Mechanism 2
- Claim: Combinatorial inference algorithms can filter out structurally inconsistent LLM predictions to produce valid outputs.
- Mechanism: After generating candidate spans with scores for each question, the framework constructs a directed graph where edges represent span choices and scores encode preference. Inference algorithms (K-shortest paths, ILPs) then search for valid structures that satisfy task constraints.
- Core assumption: Task constraints can be encoded as graph structures or ILP constraints that can be efficiently solved.
- Evidence anchors:
  - [abstract] "Our key insight is that we can use structural constraints -- and combinatorial inference derived from them -- to filter out inconsistent structures predicted by large language models."
  - [section] "Inference algorithms—realized as beam search, integer linear programs (ILPs, e.g., Roth and Yih, 2004), weighted graph optimization (e.g., Täckström et al., 2015)—take scored candidate label sub-structures as input and optimize a global score while satisfying structural constraints."
  - [corpus] No direct evidence - related papers discuss graph machine learning but not inference over LLM-generated scores.
- Break condition: If the inference problem becomes computationally intractable (e.g., with many overlapping spans), the algorithm may fail to find valid structures in reasonable time.

### Mechanism 3
- Claim: Larger language models produce more consistent predictions that benefit more from inference constraints.
- Mechanism: As model size increases, LLM predictions become more accurate and less inconsistent. Inference then provides increasing performance gains by correcting the remaining errors and enforcing constraints.
- Core assumption: LLM prediction consistency improves monotonically with model size.
- Evidence anchors:
  - [abstract] "Results demonstrate improvements in accuracy metrics and reduction in inconsistency percentages, particularly as model size increases."
  - [section] "Across the datasets, we see that performance almost always increases and constraints provide steady gains."
  - [corpus] No direct evidence - related papers don't analyze LLM size effects on consistency.
- Break condition: If very large models become so accurate that constraint violations become rare, the marginal benefit of inference may diminish.

## Foundational Learning

- Concept: Factor graph representation of structured prediction
  - Why needed here: The framework decomposes the output structure into local decisions with unary potentials, which maps naturally to factor graphs
  - Quick check question: In a factor graph for SRL, what would represent the interaction between two argument spans that must not overlap?

- Concept: Graph search algorithms (K-shortest paths)
  - Why needed here: The SRL inference uses Yen's algorithm to find the K shortest paths in a directed graph representing span choices
  - Quick check question: If you have 3 roles with 2 candidate spans each, how many possible complete structures exist before applying constraints?

- Concept: Integer Linear Programming formulation
  - Why needed here: Coreference resolution uses ILP to enforce transitivity constraints while maximizing link scores
  - Quick check question: How would you encode the transitivity constraint "if A links to B and B links to C, then A must link to C" as an ILP constraint?

## Architecture Onboarding

- Component map: Prompt generation -> LLM scoring -> Graph construction -> Inference -> Output
- Critical path: Prompt generation → LLM scoring → Graph construction → Inference → Output
- Design tradeoffs:
  - Beam size vs. inference time: Larger beams give more candidates but increase graph size
  - K value in K-shortest paths: Larger K increases chance of finding valid structures but increases computation
  - Context window: Providing more context can improve LLM predictions but may exceed model limits
- Failure signatures:
  - High inconsistency percentages indicate LLM scoring problems
  - Empty or incomplete outputs suggest inference failure to find valid structures
  - Performance degradation with larger K may indicate inefficient graph construction
- First 3 experiments:
  1. Run SRL on QA-SRLwiki with T5-3B without inference to establish baseline inconsistency
  2. Apply K-shortest paths inference with K=5 and measure consistency improvement
  3. Vary K from 5 to 50 and plot consistency vs. inference time tradeoff

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of constrained inference models compare to fine-tuned models across different tasks and datasets?
- Basis in paper: Explicit. The paper mentions that "A fine-tuned constrained T5 model gets a Heads of 58.12" for QA-SRLwiki and provides results for other datasets, but does not provide a comprehensive comparison with fine-tuned models.
- Why unresolved: The paper focuses on comparing zero/few-shot constrained models with unconstrained ones, but does not extensively compare with fine-tuned models across all datasets and tasks.
- What evidence would resolve it: A detailed comparison table showing performance of fine-tuned models vs. zero/few-shot constrained models across all tasks and datasets would provide a clear answer.

### Open Question 2
- Question: What is the impact of different prompt engineering techniques (e.g., iterative prompting, instruction tuning) on the performance and consistency of structured prediction tasks?
- Basis in paper: Explicit. The paper mentions various prompt engineering techniques and their impacts on performance, but does not provide a comprehensive analysis of their effects across all tasks and datasets.
- Why unresolved: While the paper discusses the impact of prompt engineering techniques on some datasets, it does not provide a systematic analysis across all tasks and datasets.
- What evidence would resolve it: A detailed analysis showing the performance and consistency metrics for different prompt engineering techniques across all tasks and datasets would provide a clear answer.

### Open Question 3
- Question: How does the choice of inference algorithm (e.g., beam search, ILP, weighted graph search) affect the performance and consistency of structured prediction tasks?
- Basis in paper: Explicit. The paper mentions different inference algorithms and their use in the framework, but does not provide a comprehensive comparison of their effects on performance and consistency.
- Why unresolved: The paper focuses on using a specific inference algorithm for each task, but does not compare the performance and consistency of different inference algorithms across tasks.
- What evidence would resolve it: A detailed comparison showing the performance and consistency metrics for different inference algorithms across all tasks would provide a clear answer.

## Limitations

- Scalability concerns with very large K values for complex structures
- Limited analysis of prompt engineering sensitivity across tasks
- Unclear generalizability to structured prediction tasks beyond SRL and coreference

## Confidence

- **Low** confidence in scalability claims for very large K values
- **Medium** confidence in LLM scoring reliability
- **Medium** confidence regarding generalizability to other structured prediction tasks

## Next Checks

1. **Prompt Sensitivity Analysis**: Systematically vary prompt formulations (question phrasing, context window size, temperature settings) and measure the impact on both raw LLM scores and final inference performance.

2. **Scaling Experiment**: Run experiments with progressively larger K values (5, 10, 20, 50, 100) on the largest SRL dataset, measuring both consistency improvements and wall-clock inference time.

3. **Cross-Task Transferability Test**: Apply the exact same framework (prompts, inference algorithms, and parameters) to a different structured prediction task like dependency parsing or named entity recognition.