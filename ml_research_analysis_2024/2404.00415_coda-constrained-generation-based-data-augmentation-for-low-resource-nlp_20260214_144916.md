---
ver: rpa2
title: 'CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP'
arxiv_id: '2404.00415'
source_url: https://arxiv.org/abs/2404.00415
tags:
- dataset
- coda
- data
- document
- constraints
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents CoDa, a novel training-free data augmentation
  technique for low-resource NLP that leverages large language models (LLMs) to generate
  diverse and task-specific synthetic data. CoDa extracts simple heuristic-based constraints
  from source documents and verbalizes them into natural language instructions, which
  are then used to prompt LLMs for generating augmentations.
---

# CoDa: Constrained Generation based Data Augmentation for Low-Resource NLP

## Quick Facts
- arXiv ID: 2404.00415
- Source URL: https://arxiv.org/abs/2404.00415
- Authors: Chandra Kiran Reddy Evuru; Sreyan Ghosh; Sonal Kumar; Ramaneswaran S; Utkarsh Tyagi; Dinesh Manocha
- Reference count: 40
- Outperforms baselines by 0.12%-7.19% across 11 datasets spanning 3 tasks and 3 low-resource settings

## Executive Summary
CoDa is a novel training-free data augmentation technique for low-resource NLP that leverages large language models (LLMs) to generate diverse and task-specific synthetic data. The approach extracts simple heuristic-based constraints from source documents and verbalizes them into natural language instructions, which are then used to prompt LLMs for generating augmentations. CoDa addresses the challenge of generating effective augmentations that maintain consistency with the target data distribution while providing explicit control over the generation process. The method demonstrates superior performance in both quantitative metrics and qualitative generation quality across multiple NLP tasks and domains.

## Method Summary
CoDa extracts simple heuristic constraints (keywords, POS sequences, label constraints, length constraints, and abstract concept negations) from source documents and verbalizes them into natural language instructions for prompting LLMs. The approach works with any off-the-shelf instruction-tuned LLM in a training-free fashion, providing explicit control over generated augmentations. Constraints are extracted using simple heuristics including keyword extraction, POS tagging, label consistency checking, length calculation, and abstract concept detection. These constraints are then verbalized into readable instructions that guide the LLM to generate novel training instances while maintaining semantic consistency with the original data distribution.

## Key Results
- CoDa outperforms existing baselines by 0.12%-7.19% across 11 datasets spanning 3 tasks and 3 low-resource settings
- The method achieves superior performance in both quantitative metrics (F1 scores) and qualitative generation quality measures
- CoDa demonstrates consistent improvements across sequence classification, named entity recognition, and question answering tasks

## Why This Works (Mechanism)

### Mechanism 1
- Simple heuristic constraints from source documents guide LLM generation to produce augmentations that are both diverse and consistent with target data distribution
- Core assumption: LLMs can reliably follow natural language instructions containing simple constraints without requiring complex decoding-time techniques
- Evidence: CoDa outperforms baselines in SC by 0.12%-5.94%, NER by 0.47%-7.19%, and QA by 1.10%-3.06%

### Mechanism 2
- Verbalizing constraints into natural language instructions provides more intuitive and flexible control than technical decoding-time methods
- Core assumption: Natural language instructions are sufficient for controlling LLM generation quality and maintaining task consistency
- Evidence: CoDa provides a simpler and more intuitive natural language-based interface for constrained generation

### Mechanism 3
- Combining lexical, syntactic, semantic, length, and concept constraints creates balanced augmentations that avoid spurious features while maintaining diversity
- Core assumption: Multiple simple constraints work synergistically better than single complex constraints
- Evidence: CoDa consistently outperforms these methods in all domains with varying semantic and syntactic properties

## Foundational Learning

- Natural language instruction following by LLMs
  - Why needed: CoDa relies on LLMs correctly interpreting and executing natural language instructions containing multiple constraints
  - Quick check: Can you describe how an LLM might handle conflicting constraints in a natural language instruction?

- Constraint extraction and verbalization
  - Why needed: The method requires extracting meaningful constraints from source documents and converting them into effective instructions
  - Quick check: What are the key differences between lexical, syntactic, and semantic constraints in this context?

- Data augmentation evaluation metrics
  - Why needed: Understanding perplexity, diversity, and label consistency measures is crucial for assessing CoDa's effectiveness
  - Quick check: How would you measure whether generated augmentations maintain label consistency with source examples?

## Architecture Onboarding

- Component map: Document → Constraint extraction → Instruction construction → LLM generation → Augmentation integration → Downstream training
- Critical path: Document → Constraint extraction → Instruction construction → LLM generation → Augmentation integration → Downstream training
- Design tradeoffs: Simplicity vs. control complexity; LLM size vs. performance; Template-based vs. learned instruction construction
- Failure signatures: Low instruction-following accuracy; semantically inconsistent examples; poor diversity metrics; high perplexity scores
- First 3 experiments:
  1. Test constraint extraction accuracy on a small sample dataset by manually verifying extracted keywords and POS sequences
  2. Evaluate LLM instruction-following by providing simple instructions and measuring constraint adherence rates
  3. Compare augmentation quality using different constraint combinations on a single dataset to identify most effective constraint sets

## Open Questions the Paper Calls Out

Open Question 1
- How does CoDa's performance scale with the size of the LLM used for augmentation generation?
- Basis: The paper mentions performance differences between LLaMa-7B and LLaMa-13B but doesn't explore the full spectrum of possible model sizes
- What evidence would resolve it: Systematic study comparing CoDa's performance using a range of LLM sizes on the same datasets and tasks

Open Question 2
- How does CoDa's effectiveness vary across different types of NLP tasks beyond those studied in the paper?
- Basis: The paper demonstrates effectiveness on three task types but doesn't explore other common NLP tasks
- What evidence would resolve it: Evaluating CoDa on a broader range of NLP tasks with different input/output formats

Open Question 3
- What is the optimal balance between constraint adherence and generation diversity in CoDa?
- Basis: The paper acknowledges the trade-off but doesn't explore how different constraint strictness levels affect augmentation quality
- What evidence would resolve it: Ablation study varying constraint strictness and measuring impact on diversity metrics and downstream performance

Open Question 4
- How does CoDa perform in low-resource settings with extremely small datasets (e.g., fewer than 100 examples)?
- Basis: The paper evaluates CoDa on 100, 200, and 500 example splits but doesn't explore smaller dataset sizes
- What evidence would resolve it: Evaluating CoDa's performance on dataset sizes below 100 examples to reveal effectiveness in truly extreme low-resource settings

Open Question 5
- How does CoDa's computational cost compare to other data augmentation methods when scaled to large datasets?
- Basis: The paper acknowledges CoDa is computationally more expensive but doesn't provide detailed cost comparison
- What evidence would resolve it: Comprehensive benchmark measuring inference time, GPU memory usage, and cost for CoDa versus other augmentation methods

## Limitations

- Constraint extraction methods, particularly for abstract concept detection, are not fully detailed, making precise reproduction challenging
- The method's effectiveness relies heavily on the instruction-following capabilities of the underlying LLM, which may vary across different model architectures
- Evaluation focuses primarily on F1 scores and perplexity metrics without extensive ablation studies on individual constraint types or detailed error analysis

## Confidence

- **High Confidence**: The core claim that CoDa provides training-free data augmentation with explicit constraint control is well-supported by experimental results across 11 datasets
- **Medium Confidence**: The assertion that verbalized natural language instructions provide more intuitive control than technical decoding methods is plausible but lacks direct comparative evidence
- **Medium Confidence**: The multi-constraint approach's superiority over single-constraint methods is demonstrated through aggregate performance but could benefit from more granular analysis

## Next Checks

1. Test the LLM's ability to follow simple constraint instructions by measuring constraint adherence rates on a held-out validation set before full-scale augmentation
2. Systematically remove individual constraint types to quantify their individual contributions to augmentation quality and downstream performance
3. Evaluate CoDa's performance using different instruction-tuned LLMs to assess the method's dependence on specific model capabilities and identify potential limitations across model architectures