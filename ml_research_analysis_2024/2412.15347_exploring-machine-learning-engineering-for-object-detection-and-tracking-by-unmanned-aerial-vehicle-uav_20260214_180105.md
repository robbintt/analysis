---
ver: rpa2
title: Exploring Machine Learning Engineering for Object Detection and Tracking by
  Unmanned Aerial Vehicle (UAV)
arxiv_id: '2412.15347'
source_url: https://arxiv.org/abs/2412.15347
tags:
- detection
- object
- learning
- dataset
- yolov4
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed a machine learning pipeline for real-time
  object detection and tracking using a Parrot Mambo drone to autonomously locate
  Roomba vacuum cleaners indoors, emulating search and rescue operations. A new dataset
  of 17,983 labeled images was created from drone-captured video, annotated initially
  through manual labeling and then refined via YOLOv4-based automated labeling with
  quality assurance.
---

# Exploring Machine Learning Engineering for Object Detection and Tracking by Unmanned Aerial Vehicle (UAV)

## Quick Facts
- arXiv ID: 2412.15347
- Source URL: https://arxiv.org/abs/2412.15347
- Reference count: 18
- Primary result: Successfully developed ML pipeline for real-time object detection and tracking using Parrot Mambo drone to autonomously locate Roomba vacuum cleaners indoors

## Executive Summary
This study presents a comprehensive machine learning pipeline for real-time object detection and tracking using a Parrot Mambo drone in indoor environments. The researchers created a novel dataset of 17,983 labeled images by capturing video footage of Roomba vacuum cleaners, then manually annotating and refining the data using YOLOv4-based automated labeling. Two models (YOLOv4 and Mask R-CNN) were trained on this dataset and deployed on the drone for autonomous tracking. The system demonstrated 96% accuracy with an average loss of 0.1942 in detecting and tracking the Roomba across multiple trials, successfully emulating search and rescue operations in controlled indoor settings.

## Method Summary
The researchers developed a complete ML pipeline starting with dataset creation through drone-captured video of Roomba vacuum cleaners. Initial manual labeling was performed on the 17,983 images, followed by refinement using YOLOv4-based automated labeling with quality assurance processes. Both YOLOv4 and Mask R-CNN models were trained on the refined dataset, with the trained models then deployed on the Parrot Mambo drone for real-time object detection and tracking. The experimental setup involved multiple trials where the drone autonomously located and tracked the Roomba vacuum cleaner in indoor environments, with performance metrics including accuracy, average loss, and tracking reliability being measured throughout testing.

## Key Results
- Successfully developed and deployed ML pipeline on Parrot Mambo drone for real-time object detection
- Achieved 96% accuracy with average loss of 0.1942 in autonomous Roomba tracking
- Created novel dataset of 17,983 labeled images from drone-captured video footage

## Why This Works (Mechanism)
The system's effectiveness stems from the combination of high-quality training data, appropriate model selection, and successful deployment on embedded hardware. The automated labeling refinement process using YOLOv4 helps ensure consistency across the dataset while reducing manual annotation burden. The choice of both YOLOv4 and Mask R-CNN provides complementary detection capabilities, with YOLOv4 offering real-time performance and Mask R-CNN providing detailed segmentation. The drone's onboard processing capability allows for immediate decision-making and tracking without requiring constant communication with external servers, enabling autonomous operation in real-world scenarios.

## Foundational Learning
- Dataset creation and labeling techniques: Essential for generating quality training data from raw video footage; quick check involves verifying annotation consistency and coverage across different scenarios
- Automated labeling with quality assurance: Reduces manual effort while maintaining data quality; quick check requires comparing automated annotations against ground truth samples
- Model selection and comparison (YOLOv4 vs Mask R-CNN): Different architectures offer tradeoffs between speed and accuracy; quick check involves benchmarking both models on same validation set
- Embedded ML deployment: Critical for real-time inference on resource-constrained hardware; quick check includes measuring inference latency and memory usage on target device
- Drone-based computer vision: Combines aerial perspective with object detection; quick check involves validating camera calibration and image quality under various lighting conditions

## Architecture Onboarding

Component map: Drone camera -> Image preprocessing -> Object detection model -> Tracking algorithm -> Control system -> Actuators

Critical path: Camera capture → Model inference → Tracking update → Flight control adjustment

Design tradeoffs: YOLOv4 prioritized for speed (real-time performance) vs Mask R-CNN for accuracy (detailed segmentation); automated labeling balances efficiency with potential bias; onboard processing enables autonomy but limits model complexity

Failure signatures: Detection failure in low light, tracking loss during rapid movement, false positives from similar-shaped objects, inference timeout on drone hardware

Three first experiments:
1. Test inference latency on drone hardware under various load conditions
2. Validate tracking accuracy across different indoor lighting scenarios
3. Measure detection performance when Roomba is partially occluded

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Dataset size of 17,983 images may be insufficient for complex real-world scenarios
- Single object type (Roomba vacuum cleaners) limits generalizability to diverse search and rescue situations
- Automated labeling refinement may propagate inconsistencies from initial manual annotations
- Environmental factors like lighting, occlusions, and drone motion not thoroughly addressed

## Confidence

High confidence: Successful development and deployment of ML pipeline on Parrot Mambo drone
Medium confidence: Reported 96% accuracy based on specific indoor environment and single object type
Low confidence: Generalizability to other search and rescue scenarios or outdoor environments

## Next Checks
1. Test model performance in diverse indoor environments with varying lighting conditions and obstacles
2. Evaluate model's ability to detect and track different types of objects relevant to search and rescue operations
3. Conduct outdoor tests to assess model robustness in varying weather conditions and with potential visual interferences