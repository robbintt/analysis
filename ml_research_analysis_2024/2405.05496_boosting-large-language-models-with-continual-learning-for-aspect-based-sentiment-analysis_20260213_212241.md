---
ver: rpa2
title: Boosting Large Language Models with Continual Learning for Aspect-based Sentiment
  Analysis
arxiv_id: '2405.05496'
source_url: https://arxiv.org/abs/2405.05496
tags:
- domain
- knowledge
- learning
- continual
- sentiment
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of catastrophic forgetting in
  continual learning for aspect-based sentiment analysis (ABSA). The authors propose
  a Large Language Model-based Continual Learning (LLM-CL) model that decouples domain-invariant
  and domain-variant knowledge using an orthogonal constraint and a domain knowledge
  warmup strategy.
---

# Boosting Large Language Models with Continual Learning for Aspect-based Sentiment Analysis

## Quick Facts
- arXiv ID: 2405.05496
- Source URL: https://arxiv.org/abs/2405.05496
- Authors: Xuanwen Ding, Jie Zhou, Liang Dou, Qin Chen, Yuanbin Wu, Chengcai Chen, Liang He
- Reference count: 15
- Key outcome: LLM-CL achieves 0.9491 accuracy and 0.9143 Macro-F1 on ABSC, surpassing previous methods by significant margins.

## Executive Summary
This paper addresses catastrophic forgetting in continual learning for aspect-based sentiment analysis (ABSA) by proposing a Large Language Model-based Continual Learning (LLM-CL) framework. The approach decouples domain-invariant and domain-variant knowledge using an orthogonal constraint and domain knowledge warmup strategy. Evaluated across 19 datasets spanning three ABSA subtasks, LLM-CL demonstrates state-of-the-art performance with significant improvements over strong baselines.

## Method Summary
The LLM-CL framework trains domain-variant adapters sequentially while maintaining a domain-invariant adapter through replay data. An orthogonal constraint enforces separation between domain-specific and shared knowledge, while domain knowledge warmup aligns the domain-invariant adapter after all domains are seen. Domain positioning uses Mahalanobis distance to select appropriate adapters during test time without requiring domain IDs. The model uses LLaMA-7B with LoRA fine-tuning, maintaining 8 replay samples per domain throughout training.

## Key Results
- Achieves 0.9491 accuracy and 0.9143 Macro-F1 on aspect-based sentiment classification
- Outperforms strong baselines across all three ABSA subtasks (aspect extraction, sentiment classification, joint extraction and classification)
- Demonstrates effectiveness on 19 diverse datasets from product reviews and SemEval challenges

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Orthogonal constraint separates domain-invariant and domain-variant adapters, reducing catastrophic forgetting.
- Mechanism: Enforces Bi^T BS = 0 and Ai^T AS = 0 to ensure domain-variant and domain-invariant knowledge live in orthogonal subspaces.
- Core assumption: Domain-variant knowledge is largely non-overlapping with domain-invariant knowledge, and their representations can be cleanly decoupled.
- Evidence anchors: [abstract]: "First, we design a domain knowledge decoupling module to learn a domain-invariant adapter and separate domain-variant adapters dependently with an orthogonal constraint." [section]: "We utilize an orthogonal constraint to enforce the model to learn the difference between domain-invariant and domain-variant knowledge. To make sure Bi and Ai is orthogonal to BS and AS, we need to constrains them with BT_i BS = 0 and AT_i AS = 0."

### Mechanism 2
- Claim: Domain knowledge warmup aligns domain-invariant adapter with domain-variant adapters after all domains are seen.
- Mechanism: Fine-tunes domain-invariant adapter (BS, AS) using replay data from all domains while freezing domain-variant adapters.
- Core assumption: Domain-invariant adapter drifts during training; realigning it after all domain-variant adapters are fixed improves downstream task performance.
- Evidence anchors: [abstract]: "Then, we introduce a domain knowledge warmup strategy to align the representation between domain-invariant and domain-variant knowledge." [section]: "To address this, we leverage the replay data to fine-tune the invariant adapter for each variant adapter with frozen variant adapters."

### Mechanism 3
- Claim: Domain positioning infers correct domain-variant adapter without explicit domain ID.
- Mechanism: Learns domain prototypes (mean and shared covariance) and uses Mahalanobis distance to select the best matching adapter.
- Core assumption: Each domain has a distinct, learnable representation in hidden space; nearest prototype selection works in test time.
- Evidence anchors: [abstract]: "In the test phase, we index the corresponding domain-variant knowledge via domain positioning to not require each sample’s domain ID." [section]: "For a test sample x, we select the most matching domain-variant adapter using Mahalanobis distance."

## Foundational Learning

- Concept: Orthogonal decomposition of parameter space
  - Why needed here: To separate shared (domain-invariant) and specific (domain-variant) knowledge without interference.
  - Quick check question: If two subspaces are orthogonal, can you recover their original vectors after projection?

- Concept: Mahalanobis distance in multivariate space
  - Why needed here: To match test samples to learned domain prototypes in a way that accounts for feature correlations.
  - Quick check question: How does Mahalanobis distance differ from Euclidean distance when features are correlated?

- Concept: Parameter-efficient fine-tuning (LoRA)
  - Why needed here: To adapt large language models without full fine-tuning, enabling scalability across many domains.
  - Quick check question: In LoRA, what is the role of the low-rank matrices A and B relative to the frozen backbone?

## Architecture Onboarding

- Component map: LLM backbone (frozen) -> Domain-invariant LoRA adapter (BS, AS) -> Domain-variant LoRA adapters (Bi, Ai) per domain -> Domain prototype learner (mean μi, shared covariance Σ) -> Domain positioning module (Mahalanobis distance selector) -> Replay buffer (8 samples per domain)

- Critical path: 1. Train domain-invariant adapter on replay data. 2. Train domain-variant adapter on current domain data with orthogonal constraint. 3. After all domains: domain knowledge warmup. 4. At test: domain positioning selects adapter, then forward pass.

- Design tradeoffs:
  - Orthogonal constraint vs. no constraint: trade-off between clean separation and potential underfitting if constraints too strong.
  - Replay size (8 samples) vs. memory: small replay may miss rare domain patterns; larger replay increases memory usage.
  - Domain prototype complexity (mean+covariance) vs. simple mean: covariance captures feature correlations but adds compute.

- Failure signatures:
  - Performance collapse across domains → possible orthogonality violation or poor warmup alignment.
  - Misclassification of domain → domain positioning may fail due to overlapping prototypes.
  - Slow convergence → insufficient replay or too strong orthogonal constraint.

- First 3 experiments:
  1. Verify orthogonal constraint by checking Bi^T BS ≈ 0 and Ai^T AS ≈ 0 after training.
  2. Test domain positioning by holding out domain IDs and measuring adapter selection accuracy.
  3. Ablation: train with and without domain knowledge warmup and compare final domain-invariant adapter performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the orthogonal constraint between domain-invariant and domain-variant adapters affect the model's ability to generalize to unseen domains in ABSA tasks?
- Basis in paper: [explicit] The authors mention using an orthogonal constraint to enforce the model to learn the difference between domain-invariant and domain-variant knowledge, but do not explore its impact on generalization to unseen domains.
- Why unresolved: The paper focuses on performance within the tested domains and does not investigate the model's ability to adapt to entirely new, unseen domains without further fine-tuning.
- What evidence would resolve it: Experiments testing the model's performance on a completely new domain not seen during training, compared to a baseline model without the orthogonal constraint, would provide evidence.

### Open Question 2
- Question: What is the optimal balance between domain-invariant and domain-variant knowledge for maximizing performance in ABSA tasks across different domains?
- Basis in paper: [inferred] The authors propose a method to decouple domain-invariant and domain-variant knowledge, but do not explore how different balances between these types of knowledge affect performance.
- Why unresolved: The paper does not systematically vary the relative importance of domain-invariant and domain-variant knowledge in the model's architecture or training process to determine an optimal balance.
- What evidence would resolve it: Conducting experiments with different weightings or architectures that emphasize either domain-invariant or domain-variant knowledge more heavily, and comparing performance across various ABSA tasks and domains, would provide evidence.

### Open Question 3
- Question: How does the performance of LLM-CL compare to other continual learning methods when applied to ABSA tasks with highly imbalanced class distributions across domains?
- Basis in paper: [explicit] The authors mention using Macro-F1 as a metric to mitigate biases introduced by imbalanced class distributions, but do not compare their method's performance to other continual learning approaches in such scenarios.
- Why unresolved: The paper does not explore how different continual learning methods, including LLM-CL, perform when faced with highly imbalanced class distributions across domains in ABSA tasks.
- What evidence would resolve it: Conducting experiments comparing LLM-CL to other continual learning methods on ABSA datasets with varying degrees of class imbalance, and analyzing performance using metrics such as Macro-F1 and accuracy, would provide evidence.

## Limitations

- The orthogonal constraint may be overly restrictive if domain-invariant and domain-variant knowledge overlap significantly in practice.
- The replay buffer size (8 samples per domain) appears minimal and may not capture rare domain patterns adequately.
- Domain positioning effectiveness is uncertain when domain distributions overlap heavily, as Mahalanobis distance relies on well-separated domain prototypes.

## Confidence

- **High confidence**: The experimental setup and evaluation methodology are clearly specified, with results showing consistent improvements across all three ABSA subtasks and strong baseline comparisons.
- **Medium confidence**: The orthogonal constraint mechanism is theoretically sound but lacks extensive empirical validation of whether it truly achieves clean separation of domain knowledge in practice.
- **Medium confidence**: The domain positioning approach is novel but may struggle with ambiguous domain boundaries or heavily overlapping domain distributions.

## Next Checks

1. **Orthogonal constraint validation**: Measure the actual orthogonality of learned adapters (Bi^T BS and Ai^T AS) after training to verify if the constraint is being properly enforced and contributing to performance gains.

2. **Domain positioning robustness test**: Hold out domain IDs during test time and measure adapter selection accuracy across domains, particularly focusing on domains with similar characteristics to identify failure cases.

3. **Replay buffer sensitivity analysis**: Systematically vary replay buffer size (e.g., 4, 8, 16 samples per domain) and measure impact on catastrophic forgetting and overall performance to determine if the current setting is optimal.