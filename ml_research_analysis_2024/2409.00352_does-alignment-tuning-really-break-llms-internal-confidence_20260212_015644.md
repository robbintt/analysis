---
ver: rpa2
title: Does Alignment Tuning Really Break LLMs' Internal Confidence?
arxiv_id: '2409.00352'
source_url: https://arxiv.org/abs/2409.00352
tags:
- calibration
- llms
- confidence
- alignment
- wang
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates whether alignment tuning breaks large
  language models'' internal confidence by analyzing calibration degradation across
  four dimensions: models, calibration metrics, tasks, and confidence extraction methods.
  The authors conduct a comprehensive analysis using multiple open LLMs (Llama2, Llama3,
  Mistral, Gemma, OLMo, Qwen), two calibration metrics (Expected Calibration Error
  and Static Calibration Error), various tasks (ARC-easy, HellaSwag, MedMCQA, MMLU,
  PIQA), and three confidence extraction methods (continuation-sum, continuation-min,
  choice).'
---

# Does Alignment Tuning Really Break LLMs' Internal Confidence?

## Quick Facts
- **arXiv ID:** 2409.00352
- **Source URL:** https://arxiv.org/abs/2409.00352
- **Reference count:** 5
- **Primary result:** Alignment tuning consistently harms LLM calibration, particularly with SCE metric and choice confidence extraction method

## Executive Summary
This study investigates whether alignment tuning degrades large language models' internal confidence calibration across four dimensions: models, metrics, tasks, and confidence extraction methods. The authors conduct comprehensive analysis using multiple open LLMs (Llama2, Llama3, Mistral, Gemma, OLMo, Qwen) and find that while initial results showed inconsistent effects, stricter analysis conditions revealed that alignment consistently harms calibration. The degradation is particularly pronounced when using the Static Calibration Error (SCE) metric combined with the choice confidence extraction method. The study demonstrates that model rankings for calibration degradation remain robust across tasks, suggesting that training data and algorithms used in alignment primarily drive these effects.

## Method Summary
The study employs a comprehensive experimental framework analyzing four dimensions: model families (Llama2, Llama3, Mistral, Gemma, OLMo, Qwen), calibration metrics (Expected Calibration Error and Static Calibration Error), diverse tasks (ARC-easy, HellaSwag, MedMCQA, MMLU, PIQA), and confidence extraction methods (continuation-sum, continuation-min, choice). The authors conduct comparative analysis between base and aligned models under varying conditions to identify systematic patterns in calibration degradation. The experimental design emphasizes robustness by testing across multiple conditions and verifying consistency of findings.

## Key Results
- Alignment tuning consistently harms calibration when using SCE metric with choice confidence extraction method across multiple model families
- Model rankings for calibration degradation remain robust across tasks, suggesting training data and algorithms primarily drive effects
- Calibration degradation patterns vary significantly based on choice of metric and confidence extraction method

## Why This Works (Mechanism)
The study demonstrates that alignment tuning affects model calibration through the interplay of training data composition, optimization objectives, and confidence estimation methods. The degradation mechanism appears to stem from how alignment training modifies the probability distributions produced by models, particularly when using certain confidence extraction techniques. The consistency of findings across different model families suggests that the alignment process itself, rather than specific model architectures, drives calibration changes.

## Foundational Learning
- **Expected Calibration Error (ECE):** Measures discrepancy between predicted confidence and actual accuracy; needed to quantify calibration quality, quick check involves comparing predicted probabilities with empirical accuracy bins
- **Static Calibration Error (SCE):** Alternative metric focusing on calibration at specific confidence levels; needed for robustness validation, quick check examines calibration curves across confidence thresholds
- **Confidence Extraction Methods:** Techniques for deriving confidence scores from model outputs; needed to understand how different extraction approaches affect calibration measurements, quick check involves comparing calibration across continuation-sum, continuation-min, and choice methods

## Architecture Onboarding
- **Component Map:** Base Model -> Alignment Tuning -> Confidence Extraction -> Calibration Metric -> Performance Evaluation
- **Critical Path:** Model output probabilities → Confidence extraction → Calibration calculation → Performance comparison
- **Design Tradeoffs:** Choice between ECE and SCE metrics affects sensitivity to calibration degradation; different confidence extraction methods trade off computational efficiency versus calibration accuracy
- **Failure Signatures:** Inconsistent calibration across tasks indicates metric sensitivity; uniform degradation across models suggests alignment process issues
- **First Experiments:** 1) Compare base vs aligned models using SCE with choice method, 2) Test calibration across all three confidence extraction methods, 3) Verify robustness across multiple task domains

## Open Questions the Paper Calls Out
None identified in the provided material.

## Limitations
- Analysis restricted to specific model families and tasks, potentially limiting generalizability
- Focus on static calibration measures without examining dynamic performance across different contexts
- Calibration metrics and confidence extraction methods may not capture all aspects of calibration behavior

## Confidence
- **High Confidence:** Alignment tuning consistently harms calibration with SCE metric and choice confidence extraction method
- **Medium Confidence:** Model rankings for calibration degradation remain robust across tasks
- **Medium Confidence:** Training data and algorithms in alignment primarily drive calibration effects

## Next Checks
1. Test calibration degradation patterns across additional diverse tasks beyond current benchmark suite
2. Conduct controlled experiments varying specific alignment training components to isolate individual effects
3. Evaluate temporal stability of calibration metrics during different stages of alignment training