---
ver: rpa2
title: 'Online Learning and Information Exponents: On The Importance of Batch size,
  and Time/Complexity Tradeoffs'
arxiv_id: '2406.02157'
source_url: https://arxiv.org/abs/2406.02157
tags:
- learning
- loss
- correlation
- target
- batch
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies the impact of batch size on the iteration time
  of training two-layer neural networks with one-pass stochastic gradient descent
  (SGD) on multi-index target functions. The authors characterize the optimal batch
  size that minimizes the training time as a function of the hardness of the target,
  as quantified by the information exponent.
---

# Online Learning and Information Exponents: On The Importance of Batch size, and Time/Complexity Tradeoffs

## Quick Facts
- arXiv ID: 2406.02157
- Source URL: https://arxiv.org/abs/2406.02157
- Reference count: 40
- Key outcome: Optimal batch size nb ≲ d^(ℓ/2) minimizes training time for SGD on multi-index targets without increasing sample complexity

## Executive Summary
This paper analyzes how batch size affects the time complexity of one-pass stochastic gradient descent (SGD) for training two-layer neural networks on multi-index target functions. The authors characterize the optimal batch size that minimizes training time as a function of the target's information exponent ℓ, showing that batches of size nb ≲ d^(ℓ/2) achieve the best tradeoff. For larger batches, negative feedback terms prevent further time complexity improvements. To overcome this fundamental limitation, the authors propose Correlation Loss SGD, which suppresses auto-correlation terms and enables polylogarithmic training time for batch sizes nb = O(d^(ℓ-1)). The theoretical results are validated through numerical experiments.

## Method Summary
The paper studies two-layer neural networks trained with one-pass SGD on multi-index target functions. The method involves: (1) initializing first layer weights from a uniform distribution on the sphere while keeping second layer weights fixed, (2) partitioning data into disjoint batches of size nb and performing projected SGD updates on the hidden layer weights, (3) tracking sufficient statistics (overlap and norm matrices) to monitor learning progress, and (4) using low-dimensional ODEs to characterize the training dynamics. Two training protocols are compared: standard SGD with MSE loss and Correlation Loss SGD, which suppresses auto-correlation terms by effectively using a vanishing initialization scale for second layer weights.

## Key Results
- Optimal batch size nb ≲ d^(ℓ/2) minimizes training time without increasing sample complexity for standard SGD
- Correlation Loss SGD overcomes the fundamental limitation of standard SGD for larger batch sizes (nb > d^(ℓ/2))
- Training dynamics can be exactly characterized by low-dimensional ODEs tracking sufficient statistics
- Theoretical predictions are validated with numerical experiments on synthetic data

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient updates with batch sizes nb ≲ d^(ℓ/2) minimize training time without increasing total sample complexity.
- Mechanism: In this regime, the negative feedback terms that contract network-target correlation are suppressed, allowing larger batch sizes to accelerate learning without harming convergence.
- Core assumption: The information exponent ℓ characterizes the hardness of learning the target function, and the batch size is scaled with input dimension d as nb = n0·d^µ.
- Evidence anchors:
  - [abstract] "We show that performing gradient updates with large batches nb ≲ d^(ℓ/2) minimizes the training time without changing the total sample complexity..."
  - [section 3.1] "If we optimally choose the learning rate δ⋆(µ), the number of time iterations needed to weakly recover the teacher direction w⋆ is simply T(nb) = I(ℓ)/nb, rescaling straightforwardly the time complexity..."
- Break condition: When nb > d^(ℓ/2), negative feedback terms dominate and prevent further reduction in training iterations.

### Mechanism 2
- Claim: Correlation Loss SGD overcomes the fundamental limitation of standard SGD for nb > d^(ℓ/2).
- Mechanism: By suppressing auto-correlation terms in the loss function, this protocol reduces the number of steps needed to weakly recover the target to polylog(d) when using batch sizes nb = O(d^(ℓ-1)).
- Core assumption: The correlation loss effectively mimics a vanishing initialization of the second layer weights a0 of the network.
- Evidence anchors:
  - [abstract] "We provably overcome this fundamental limitation via a different training protocol, Correlation loss SGD, which suppresses the auto-correlation terms in the loss function."
  - [section 3.3] "The above-described protocol is equivalent to consider a vanishing initialization scale for the second layer weights a0 of the network."
- Break condition: When nb becomes too large (nb = O(d^ℓ)), one-step learning becomes optimal instead.

### Mechanism 3
- Claim: The training dynamics can be exactly characterized by low-dimensional ODEs tracking sufficient statistics.
- Mechanism: The overlaps between neurons and target subspace, along with their norms, concentrate to a deterministic process described by a system of ODEs, enabling precise characterization of learning regimes.
- Core assumption: High-dimensional concentration properties hold, allowing the full SGD dynamics to be approximated by a low-dimensional deterministic system.
- Evidence anchors:
  - [section 4.1] "We are now in the position to state our proposition that provides a set of deterministic ODEs to describe one-pass SGD in high-dimensions."
  - [corpus] "Weak Convergence and Empirical Processes: With Applications to Statistics" provides theoretical foundation for concentration in high dimensions.
- Break condition: When learning rate is too large (γ = ω(1)), the deterministic ODE approximation breaks down due to unpredictable behavior from random weight decay.

## Foundational Learning

- Concept: Information Exponent (ℓ)
  - Why needed here: Quantifies the hardness of learning multi-index target functions, determining how batch size affects optimal training time
  - Quick check question: What does the information exponent represent in the context of learning multi-index models?

- Concept: Hermite Expansion
  - Why needed here: The information exponent is defined as the lowest degree Hermite polynomial with non-zero coefficient in the target function's expansion
  - Quick check question: How is the information exponent ℓ formally defined using Hermite polynomials?

- Concept: Sufficient Statistics
  - Why needed here: The dynamics of SGD can be tracked using low-dimensional statistics (overlaps and norms) rather than full weight vectors
  - Quick check question: What are the key sufficient statistics that characterize the SGD dynamics in high dimensions?

## Architecture Onboarding

- Component map: Two-layer neural network (W, a) -> Multi-index target function (ℓ) -> Batch processing (nb) -> Training protocols (SGD/Correlation Loss SGD)

- Critical path:
  1. Initialize first layer weights W from uniform distribution on sphere, fix second layer weights a
  2. Partition data into batches and perform projected SGD updates on W
  3. Track sufficient statistics (overlap matrix M and norm matrix Q) to monitor learning progress
  4. Adjust learning rate and batch size according to information exponent ℓ to optimize training time

- Design tradeoffs:
  - Larger batch sizes (nb ≲ d^(ℓ/2)) reduce training iterations but beyond this threshold cause negative feedback
  - Correlation Loss SGD enables larger batch sizes (nb = O(d^(ℓ-1))) at the cost of requiring polylogarithmic iterations
  - Vanishing initialization of second layer weights enables Correlation Loss SGD but may not be optimal for full target learning

- Failure signatures:
  - Training stalls with correlation decreasing toward zero (negative feedback dominance)
  - Dynamics not captured by low-dimensional ODEs (learning rate too large)
  - Insufficient sample complexity despite large batch size (exceeding d^(ℓ-1) threshold)

- First 3 experiments:
  1. Compare training time for different batch sizes nb = O(1), O(d^(ℓ/2)), and O(d^(ℓ-1)) on a multi-index target with known ℓ
  2. Validate that Correlation Loss SGD achieves polylog(d) iterations for nb = O(d^(ℓ-1)) while standard SGD fails
  3. Test whether the low-dimensional ODE system accurately predicts the evolution of sufficient statistics for various (δ, µ) parameter choices

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between the leap index and the information exponent in multi-index target functions?
- Basis in paper: [inferred] The paper mentions that the leap index generalizes the information exponent for multi-index targets, but does not provide a detailed analysis.
- Why unresolved: The paper focuses on single-index targets for simplicity and does not explore the generalization to multi-index targets in depth.
- What evidence would resolve it: A rigorous mathematical proof showing how the leap index relates to the information exponent in the context of multi-index target functions.

### Open Question 2
- Question: How does the choice of activation function affect the optimal batch size and learning rate in the context of the correlation loss SGD?
- Basis in paper: [explicit] The paper discusses the impact of batch size and learning rate on the learning dynamics, but does not explore the role of different activation functions in detail.
- Why unresolved: The paper uses a specific activation function (erf) for numerical experiments and does not systematically study the effect of other activation functions.
- What evidence would resolve it: Numerical experiments comparing the performance of correlation loss SGD with different activation functions, showing how the optimal batch size and learning rate vary.

### Open Question 3
- Question: What is the precise form of the non-asymptotic corrections to the asymptotic dynamics, and how do they depend on the batch size and input dimension?
- Basis in paper: [explicit] The paper mentions that intra-batch correlations introduce non-asymptotic corrections to the asymptotic dynamics, but does not provide a detailed analysis.
- Why unresolved: The paper focuses on the asymptotic description of the dynamics and does not explore the finite-dimensional corrections in depth.
- What evidence would resolve it: A rigorous mathematical analysis of the non-asymptotic corrections, showing how they depend on the batch size and input dimension.

## Limitations
- Theoretical analysis relies heavily on high-dimensional Gaussian approximations that may not hold for non-isotropic data distributions
- Correlation loss SGD requires vanishing initialization of second layer weights, which could be problematic for training deeper networks
- Current numerical experiments are limited to synthetic data with known information exponents

## Confidence
- **High confidence**: The characterization of optimal batch sizes (nb ≲ d^(ℓ/2)) for standard SGD minimizing training time without increasing sample complexity
- **Medium confidence**: The correlation loss SGD mechanism for overcoming the fundamental limitation when nb > d^(ℓ/2), based on theoretical analysis but limited empirical validation
- **Low confidence**: Practical effectiveness of these findings on real-world datasets with complex, unknown target functions

## Next Checks
1. **Real data generalization**: Test the optimal batch size scaling law (nb = Θ(d^(ℓ/2))) on CIFAR-10/100 with multi-index approximations of the classification functions to measure weak recovery correlation and training time.

2. **Correlation loss SGD ablation**: Systematically vary the initialization scale of second layer weights in correlation loss SGD to quantify the tradeoff between negative feedback suppression and final target recovery accuracy.

3. **Distribution mismatch study**: Evaluate how deviations from isotropic Gaussian inputs (e.g., correlated features or non-Gaussian tails) affect the concentration properties and the validity of the low-dimensional ODE approximation.