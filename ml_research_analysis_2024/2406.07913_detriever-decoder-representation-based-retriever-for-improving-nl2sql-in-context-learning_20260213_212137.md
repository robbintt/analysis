---
ver: rpa2
title: 'DeTriever: Decoder-representation-based Retriever for Improving NL2SQL In-Context
  Learning'
arxiv_id: '2406.07913'
source_url: https://arxiv.org/abs/2406.07913
tags:
- examples
- language
- layer
- learning
- nl2sql
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes DeTriver, a demonstration retrieval framework
  for improving the performance of Large Language Models (LLMs) on the natural language
  to SQL (NL2SQL) task using in-context learning (ICL). The key idea is to learn a
  weighted combination of LLM hidden states to effectively retrieve relevant demonstration
  examples for a given task.
---

# DeTriever: Decoder-representation-based Retriever for Improving NL2SQL In-Context Learning

## Quick Facts
- arXiv ID: 2406.07913
- Source URL: https://arxiv.org/abs/2406.07913
- Reference count: 9
- One-line primary result: DeTriever achieves up to 11.58 and 8.94 points of improvement in execution accuracy over random one-shot baseline on Spider and BIRD datasets respectively.

## Executive Summary
This paper proposes DeTriever, a demonstration retrieval framework that improves Large Language Models' performance on NL2SQL tasks using in-context learning. The key innovation is learning a weighted combination of LLM hidden states to retrieve relevant demonstration examples, addressing the challenge of selecting beneficial examples for ICL. Experiments on Spider and BIRD benchmarks show DeTriever significantly outperforms state-of-the-art baselines, achieving substantial improvements in execution accuracy.

## Method Summary
DeTriever uses a CodeLlama-13b LLM to generate SQL queries based on database schema and natural language questions. For retrieval, it processes the input through the LLM to obtain hidden states, then applies a 3-layer MLP to transform these states into a weighted combination across different layers. The retrieval model is trained using supervised contrastive loss to align the problem description embedding space with the similarity between concatenated problem descriptions and ground-truth SQL queries. During inference, the retriever selects the most similar demonstration example from the training set to prepend to the input prompt, improving the LLM's SQL generation accuracy.

## Key Results
- DeTriever achieves 11.58 points improvement over random one-shot baseline on Spider dataset
- DeTriever achieves 8.94 points improvement over random one-shot baseline on BIRD dataset
- Weighted combination of LLM layers (10-20) outperforms last layer (40) for retrieval representation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LLM's own hidden states encode richer semantic information for retrieval than external encoders like Sentence-BERT or Contriever.
- Mechanism: By using the hidden states of the LLM itself, the retrieval representation is inherently aligned with the semantic space the LLM uses for generation, avoiding the representational gap between external retrievers and the LLM.
- Core assumption: The hidden states of the LLM contain information that is more discriminative for the NL2SQL task than generic embeddings from external models.
- Evidence anchors:
  - [abstract] "there exists a fundamental gap between the retrieval encoders and the LLM due to the inherently more sophisticated representational capabilities of the LLM to differentiate the intricate nuances embedded within the task prompt"
  - [section 3.2] "Since the probability of predicting the correct query can be measured through execution accuracy by comparing the predicted query ˆy = M ([I(x); x]) against the ground-truth query y over an evaluation corpus Dtest, where Acc(x, ˆy, y) ∈ {0, 1}, the objective of optimizing the retriever R can be expressed in Equation 2."
  - [corpus] Weak evidence. The corpus mentions similar retrieval works but does not directly compare internal LLM states vs external encoders.
- Break condition: If the LLM's hidden states do not capture the semantic nuances relevant to the NL2SQL task, or if the task requires features that are not encoded in the hidden states.

### Mechanism 2
- Claim: A weighted combination of multiple LLM layers captures more granular and complementary information than using a single layer.
- Mechanism: Different layers of the LLM encode information at different levels of abstraction. By learning a weighted combination of these layers, the model can leverage the strengths of each layer for retrieval.
- Core assumption: The optimal retrieval representation is not confined to a single layer but benefits from a combination of information across multiple layers.
- Evidence anchors:
  - [section 3.2] "we see that the hidden representations from lower-mid layers (10-20) have a significantly better execution accuracy compared to the last layer (40)"
  - [section 3.2] "Motivated by this finding, we propose to learn a weighted combination over the layers (Bahdanau et al., 2015) to effectively leverage the different granularity of information encoded in each layer of the LLM"
  - [corpus] Weak evidence. The corpus does not directly address the benefits of combining multiple layers for retrieval.
- Break condition: If the weighted combination does not improve over the best single layer, or if the learned weights are not stable across different tasks or datasets.

### Mechanism 3
- Claim: The similarity between the concatenated problem description and ground-truth SQL query is a good proxy for the relative benefits of demonstration examples for ICL.
- Mechanism: By training the retrieval model to align the problem description embedding space with the similarity between the concatenated problem description and ground-truth SQL query, the model learns to retrieve examples that are beneficial for generating the correct SQL query.
- Core assumption: The similarity between the problem description and the ground-truth SQL query is correlated with the improvement in execution accuracy when using that example as an ICL demonstration.
- Evidence anchors:
  - [section 3.3] "we propose an indirect approach to predict Sim([xi; yi], [x; y]) by training the retrieval model R using supervised contrastive loss"
  - [section 4.5.1] "Since using the problem description does not require knowledge of the ground-truth query, we simply use the best-performing layer from Table 1 without the need for training"
  - [corpus] Weak evidence. The corpus does not directly address the use of query similarity as a proxy for ICL benefits.
- Break condition: If the similarity between the problem description and the ground-truth SQL query is not correlated with the improvement in execution accuracy, or if the contrastive loss does not effectively align the embedding space.

## Foundational Learning

- Concept: In-context learning (ICL)
  - Why needed here: ICL is the core technique used to improve LLM performance on NL2SQL by providing demonstration examples in the prompt.
  - Quick check question: What is the main difference between ICL and fine-tuning?
- Concept: Contrastive learning
  - Why needed here: Contrastive learning is used to train the retrieval model to align the problem description embedding space with the similarity between the concatenated problem description and ground-truth SQL query.
  - Quick check question: How does contrastive learning work in the context of representation learning?
- Concept: Execution accuracy
  - Why needed here: Execution accuracy is the primary metric used to evaluate the performance of the NL2SQL model, as it measures whether the generated SQL query correctly retrieves the expected database records.
  - Quick check question: What are the limitations of using execution accuracy as an evaluation metric for NL2SQL?

## Architecture Onboarding

- Component map:
  - Input prompt (database schema + natural language question) -> LLM (CodeLlama-13b) -> Hidden states -> 3-layer MLP -> Weighted combination -> DeTriever retrieval -> Retrieved demonstration example -> Augmented input prompt -> LLM generation -> SQL query -> Execution accuracy evaluation
- Critical path:
  - Input prompt (database schema + natural language question) is processed by the LLM to generate hidden states.
  - DeTriever uses the hidden states to retrieve the most similar demonstration example from the training set.
  - The retrieved demonstration example is prepended to the input prompt.
  - The LLM generates the SQL query based on the augmented input prompt.
  - The generated SQL query is evaluated using execution accuracy.
- Design tradeoffs:
  - Using the LLM's own hidden states vs external encoders: Pros - better alignment with the LLM's semantic space; Cons - requires access to the LLM's internal states.
  - Weighted combination of multiple layers vs single layer: Pros - captures more granular and complementary information; Cons - increases model complexity and training time.
  - Using query similarity as a proxy vs direct execution accuracy: Pros - allows for training without requiring pairwise inference; Cons - assumes correlation between query similarity and ICL benefits.
- Failure signatures:
  - Low execution accuracy: Indicates that the retrieved demonstration examples are not beneficial for generating the correct SQL query.
  - High error rate: Indicates that the generated SQL queries are not compilable, suggesting issues with the generation process or the demonstration examples.
  - Unstable learned weights: Indicates that the weighted combination of layers is not consistent across different tasks or datasets, suggesting issues with the model's ability to generalize.
- First 3 experiments:
  1. Evaluate the performance of using different single layers of the LLM for retrieval.
  2. Evaluate the performance of using the weighted combination of layers vs the best single layer.
  3. Evaluate the performance of using query similarity as a proxy vs direct execution accuracy for training the retrieval model.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LLM architectures and sizes affect the performance of DeTriever, particularly when considering models with varying numbers of layers and hidden state representations?
- Basis in paper: [explicit] The paper uses CodeLlama-13b, which has 41 layers, and focuses on the hidden states from every 5th layer. However, it does not explore the impact of using different architectures or sizes of LLMs.
- Why unresolved: The paper does not provide comparative analysis with other LLM architectures or sizes, leaving open the question of how generalizable the findings are across different models.
- What evidence would resolve it: Experiments comparing DeTriever's performance using different LLM architectures (e.g., GPT-3, Llama-2) and sizes would provide insights into the model's adaptability and effectiveness.

### Open Question 2
- Question: Can the DeTriever approach be effectively adapted for other NLP tasks beyond NL2SQL, such as text summarization or sentiment analysis, where the output space and complexity might differ significantly?
- Basis in paper: [inferred] The paper suggests potential generalizability to other tasks but does not provide empirical evidence or analysis for tasks other than NL2SQL.
- Why unresolved: The paper does not explore or validate the approach on other tasks, leaving uncertainty about its applicability and effectiveness in different contexts.
- What evidence would resolve it: Conducting experiments on various NLP tasks with different output complexities and comparing the performance with task-specific baselines would demonstrate the approach's versatility.

### Open Question 3
- Question: What is the impact of varying the number of demonstration examples (ICL examples) on the performance of DeTriever, and is there an optimal number that maximizes accuracy?
- Basis in paper: [explicit] The paper mentions the study of the effects of different numbers of positive examples in the contrastive loss but does not provide a comprehensive analysis of varying the total number of demonstration examples used during inference.
- Why unresolved: The paper does not explore how the number of ICL examples affects performance, which is crucial for understanding the scalability and efficiency of the approach.
- What evidence would resolve it: Systematic experimentation with different numbers of ICL examples during inference, analyzing performance metrics such as accuracy and computational efficiency, would clarify the optimal number of examples.

## Limitations

- The improvement claims are measured against random baselines rather than state-of-the-art retrieval methods, making it difficult to assess the true magnitude of improvement.
- The paper doesn't address computational overhead—retrieving demonstrations using LLM hidden states likely incurs additional latency compared to simpler methods.
- The claim that LLM hidden states are superior to external encoders for retrieval is supported primarily by qualitative reasoning rather than direct ablation studies comparing different encoder types.

## Confidence

- **High Confidence**: The core finding that weighted combinations of LLM layers outperform single layers for retrieval. This is directly supported by Table 1's empirical comparison and the ablation study.
- **Medium Confidence**: The claim that LLM hidden states are superior to external encoders for retrieval. While the reasoning is sound, direct comparative experiments with alternative retrievers would strengthen this claim.
- **Medium Confidence**: The effectiveness of using query similarity as a proxy for ICL benefits. The method works empirically, but the paper doesn't establish the theoretical relationship between query similarity and execution accuracy improvements.

## Next Checks

1. **Ablation Study with Alternative Encoders**: Replace DeTriever's use of LLM hidden states with established retrieval models like Sentence-BERT or Contriever while keeping all other components (weighted layer combination, contrastive loss) constant. This would isolate whether the improvement comes from the specific choice of encoder or from the overall retrieval framework design.

2. **Cross-Dataset Generalization Test**: Train DeTriever on one dataset (e.g., Spider) and evaluate its retrieval performance on another (e.g., BIRD) without fine-tuning. This would test whether the learned weighted layer combination and retrieval representations generalize beyond the training distribution or are overfit to specific dataset characteristics.

3. **Computational Overhead Analysis**: Measure and report the additional latency introduced by DeTriever's retrieval process compared to baseline methods, including both the time to compute hidden state representations and the retrieval operation itself. This would help assess the practical deployment costs of the approach.