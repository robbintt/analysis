---
ver: rpa2
title: Supplier Recommendation in Online Procurement
arxiv_id: '2403.01301'
source_url: https://arxiv.org/abs/2403.01301
tags:
- event
- suppliers
- events
- supplier
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a personalized supplier recommendation system
  for online procurement, specifically targeting road freight events. The system uses
  Factorization Machines (FMs) to recommend suppliers to events, incorporating both
  historical interaction data and event metadata (e.g., purchaser identity, event
  descriptions, timezone).
---

# Supplier Recommendation in Online Procurement

## Quick Facts
- arXiv ID: 2403.01301
- Source URL: https://arxiv.org/abs/2403.01301
- Reference count: 17
- One-line primary result: Factorization Machines outperform popularity and MF-like baselines in personalized supplier recommendation for road freight events, achieving ~60% precision at k=5.

## Executive Summary
This paper presents a personalized supplier recommendation system for online procurement, specifically targeting road freight events. The system uses Factorization Machines (FMs) to recommend suppliers to events, incorporating both historical interaction data and event metadata (e.g., purchaser identity, event descriptions, timezone). A key challenge addressed is the cold-start problem for new events, where no interaction history exists. Experiments using real-world data showed that FMs outperformed baseline methods (popularity-based and MF-like) across precision, recall, and NDCG metrics. For example, at k=5, FM achieved ~60% precision, meaning 3 out of 5 recommendations were relevant on average, compared to ~35% for the popularity baseline. The results are promising, especially given the limited metadata used, and suggest potential for further improvement with richer data and more advanced filtering and fairness mechanisms.

## Method Summary
The proposed method employs Factorization Machines (FMs) with Bayesian Personalized Ranking (BPR) as the loss function to recommend suppliers for road freight events. The model uses a participation matrix (events × suppliers) and event metadata (purchaser ID, descriptions, timezone) to learn latent representations. BPR optimizes the relative ranking of suppliers, comparing those who participated in an event against negative samples. The system handles cold-start scenarios by leveraging event metadata when no interaction history exists. Experiments were conducted on real-world data, comparing FM against non-personalized popularity and MF-like baselines across precision, recall, and NDCG metrics.

## Key Results
- FM achieved ~60% precision at k=5, outperforming the popularity baseline (~35%) and MF-like baseline.
- FM consistently outperformed baselines across all metrics (precision, recall, NDCG) and all recommendation list sizes (k).
- The model effectively handles cold-start scenarios by leveraging event metadata, demonstrating the value of incorporating rich features beyond historical interactions.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Factorization Machines (FMs) can leverage both interaction data and event metadata to provide personalized supplier recommendations, even in cold-start scenarios.
- Mechanism: FMs model the interaction between features using latent vectors, allowing them to capture complex relationships between event characteristics (like purchaser identity, event descriptions, and timezone) and supplier participation history. This enables predictions for new events where no interaction history exists, by inferring relevance from the event's metadata.
- Core assumption: The meta-features of an event (purchaser identity, description, timezone) are predictive of which suppliers will be relevant, even without direct interaction history for that event.
- Evidence anchors:
  - [abstract] "Our system is able to provide personalized supplier recommendations, taking into account customer needs and preferences."
  - [section] "The interaction data takes the form of a participation matrix PE×S, where E is the number of past events and S is the number of suppliers...Our meta-data also includes simple features such as the event timezone. Finally, the meta-data includes a short textual description of the event."
  - [corpus] Weak or missing. The corpus neighbors do not directly address the specific FM mechanism for cold-start supplier recommendation.
- Break condition: If the event metadata is not predictive of supplier relevance (e.g., descriptions are too vague or features are irrelevant), the FM's ability to generalize from past interactions to new events will degrade.

### Mechanism 2
- Claim: Bayesian Personalized Ranking (BPR) as a loss function optimizes the relative ordering of suppliers for an event, improving the relevance of the top-k recommendations.
- Mechanism: BPR compares positive instances (suppliers who participated in an event) against sampled negative instances (suppliers who did not participate) and optimizes the model to rank the positive suppliers higher. This rank-aware approach directly targets the quality of the top-k list, which is the practical output of the recommender.
- Core assumption: The relative ordering of suppliers (which ones are more relevant than others) is more important than absolute relevance scores for the final recommendation list.
- Evidence anchors:
  - [section] "We use Bayesian Personalized Ranking (BPR) [8] as our rank-aware loss function...Let Se denote the set of suppliers who participated in event e...The log-likelihood of the observed preferences is:..."
  - [corpus] Weak or missing. The corpus neighbors do not specifically discuss BPR or its application in this supplier recommendation context.
- Break condition: If the negative sampling strategy does not accurately represent truly irrelevant suppliers, or if the number of negative samples per positive is not tuned correctly, BPR may optimize for a ranking that does not reflect true user preferences.

### Mechanism 3
- Claim: The combination of FM with BPR creates a model that is more effective than simpler baselines (popularity-based and MF-like) because it learns both the interaction patterns and the importance of metadata features.
- Mechanism: The FM with BPR not only learns latent representations of suppliers and events but also learns the weights of the meta-features. This allows the model to capture supplier-event affinity that is specific to certain purchasers or event types, which simpler models that ignore metadata cannot do. The experiments show that FM outperforms the baselines across all metrics.
- Core assumption: The metadata features contain signal that, when combined with historical interaction data, improves the accuracy of supplier recommendations beyond what interaction data alone can provide.
- Evidence anchors:
  - [section] "Our preliminary results show that there is great promise in tackling this problem in the way that we propose...Figure 1 shows the results. As we would expect, precision falls but recall and NDCG rise as k increases. But what is important is the comparison with the baselines: FM outperforms them for every metric, at every recommendation list size, k."
  - [corpus] Weak or missing. The corpus neighbors do not provide direct evidence for the specific claim about FM+BPR outperforming baselines in this domain.
- Break condition: If the metadata features are noisy or irrelevant, or if the historical interaction data is insufficient to learn meaningful latent representations, the advantage of the FM+BPR model over simpler baselines will diminish.

## Foundational Learning

- Concept: Factorization Machines (FMs)
  - Why needed here: FMs are chosen because they can handle both the sparse interaction data (who participated in which event) and the rich event metadata (purchaser, description, timezone) in a unified model, which is crucial for the cold-start problem in supplier recommendation.
  - Quick check question: How does an FM differ from a standard matrix factorization model, and why is this difference important for this problem?

- Concept: Cold-start recommendation
  - Why needed here: The primary use case is recommending suppliers for new events where there is no historical interaction data, making it a classic cold-start problem that requires leveraging metadata and learned patterns from past data.
  - Quick check question: What are the main challenges of cold-start recommendation, and how does using event metadata help address them?

- Concept: Bayesian Personalized Ranking (BPR)
  - Why needed here: BPR is a rank-aware loss function that optimizes the relative ordering of items in the recommendation list, which is more aligned with the goal of providing a useful top-k list of suppliers than a point-wise loss.
  - Quick check question: How does BPR optimize for a better ranking, and what is the role of negative sampling in this process?

## Architecture Onboarding

- Component map: Data Ingestion -> Preprocessing -> FM Model -> BPR Loss Function -> Candidate Scoring -> Top-k Selection -> Recommendation Output
- Critical path: Data Ingestion → Preprocessing → FM Model Training (with BPR) → Candidate Scoring → Top-k Selection → Recommendation Output
- Design tradeoffs:
  - Using a large latent dimension D in the FM can capture more complex patterns but increases computational cost and the risk of overfitting, especially with limited data.
  - Relying heavily on text descriptions for metadata is risky if the descriptions are uninformative; adding more structured features (e.g., supplier location, service types) could improve performance.
  - A simple top-k selection by score is fast but may ignore important criteria like diversity or fairness; a more complex selection algorithm could address these but at the cost of increased complexity.
- Failure signatures:
  - Low precision and recall across all k values, even after hyperparameter tuning, suggests the model is not learning meaningful patterns from the data.
  - A large gap between training and validation performance indicates overfitting, possibly due to a high latent dimension or insufficient regularization.
  - Consistently recommending the same popular suppliers regardless of event metadata suggests the model is ignoring the rich features and relying too heavily on popularity.
- First 3 experiments:
  1. **Baseline Comparison**: Train and evaluate the FM with BPR against the non-personalized popularity baseline and the MF-like baseline (FM without metadata) using the same cross-validation setup to confirm the reported performance improvements.
  2. **Metadata Ablation**: Systematically remove different types of metadata (e.g., descriptions, purchaser ID, timezone) from the FM model to quantify the contribution of each feature type to the overall performance.
  3. **Cold-start Stress Test**: Simulate a more extreme cold-start scenario by holding out events from certain purchasers or with certain keywords in their descriptions, and evaluate how well the model generalizes to these unseen patterns.

## Open Questions the Paper Calls Out
None

## Limitations
- The system relies on a limited set of metadata features (purchaser ID, event descriptions, timezone) for cold-start recommendations, without exploring richer supplier-side features.
- The evaluation focuses on offline metrics without assessing real-world business impact, such as cost savings or supplier diversity.
- The cold-start scenario is relatively mild, as the model assumes at least one past event from the same purchaser exists.

## Confidence

**High Confidence**: The FM+BPR model outperforms the simpler baselines (popularity and MF-like) across all reported metrics. This is directly supported by the experimental results presented in the paper.

**Medium Confidence**: The FM model's ability to generalize to new events based on metadata is plausible given the experimental results, but the paper does not provide strong evidence for the specific mechanism by which this generalization occurs, nor does it test the model's performance in a more extreme cold-start scenario.

**Low Confidence**: The paper's claim about the potential for future improvement with more advanced filtering and fairness mechanisms is speculative, as these were not implemented or evaluated in the current study.

## Next Checks
1. **Metadata Ablation Study**: Systematically remove different types of metadata (e.g., descriptions, purchaser ID, timezone) from the FM model to quantify the contribution of each feature type to the overall performance.
2. **Extreme Cold-start Evaluation**: Simulate a more extreme cold-start scenario by holding out events from certain purchasers or with certain keywords in their descriptions, and evaluate how well the model generalizes to these unseen patterns.
3. **Offline-to-Online Gap Assessment**: Conduct a small-scale online experiment or a user study to validate that the offline improvements in precision and recall translate to tangible business benefits, such as reduced procurement costs or improved supplier selection.