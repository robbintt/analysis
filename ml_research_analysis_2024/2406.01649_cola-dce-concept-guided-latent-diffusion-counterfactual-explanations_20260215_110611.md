---
ver: rpa2
title: CoLa-DCE -- Concept-guided Latent Diffusion Counterfactual Explanations
arxiv_id: '2406.01649'
source_url: https://arxiv.org/abs/2406.01649
tags:
- counterfactual
- image
- cola-dce
- concept
- concepts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Concept-guided Latent Diffusion Counterfactual
  Explanations (CoLa-DCE), a novel approach for generating interpretable counterfactual
  explanations for image classifiers. CoLa-DCE leverages diffusion models to create
  realistic counterfactuals while providing transparency through concept-based guidance
  and spatial conditioning.
---

# CoLa-DCE -- Concept-guided Latent Diffusion Counterfactual Explanations

## Quick Facts
- arXiv ID: 2406.01649
- Source URL: https://arxiv.org/abs/2406.01649
- Reference count: 40
- Key outcome: Novel method for generating interpretable counterfactual explanations for image classifiers using concept-based guidance and spatial conditioning in diffusion models

## Executive Summary
This paper introduces CoLa-DCE, a novel approach for generating interpretable counterfactual explanations for image classifiers. The method leverages diffusion models to create realistic counterfactuals while providing transparency through concept-based guidance and spatial conditioning. CoLa-DCE selects semantically meaningful concepts from the classifier's latent space and uses them to guide the image generation process, resulting in counterfactuals with minimal feature changes. The approach enables better understanding of model behavior and potential misclassifications by visualizing the specific concepts and spatial regions that influence the classifier's decisions.

## Method Summary
CoLa-DCE generates counterfactual explanations by combining diffusion models with concept-based guidance. The method extracts semantic concepts from a classifier's latent space using local attribution methods (LRP/CRP), selects the most important concepts through binary masking, and applies spatial constraints to focus modifications on relevant regions. The diffusion model is guided by gradients aligned with the selected concepts, producing counterfactuals that flip the classifier's prediction while maintaining minimal and interpretable changes. Target samples are selected based on the classifier's internal representation rather than external semantic similarity, improving the quality of the counterfactuals.

## Key Results
- CoLa-DCE produces more focused and interpretable explanations compared to existing methods, as evidenced by improved FID scores and higher flip ratios
- Concept-based guidance reduces the number of features that change while preserving semantic meaning
- Spatial conditioning increases the focus and locality of feature changes, improving transparency
- The method enables better understanding of model behavior by visualizing specific concepts and spatial regions that influence classifier decisions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Concept-based guidance improves interpretability by reducing the number of features that change while preserving semantic meaning
- Mechanism: By selecting the top-k most important concepts from the classifier's latent space and applying binary masks to the gradient, only the relevant semantic features are modified during diffusion, resulting in counterfactuals that are both minimal and comprehensible
- Core assumption: Semantic concepts encoded in classifier channels correspond to human-interpretable features, and modifying only these channels produces meaningful counterfactuals
- Evidence anchors:
  - [abstract] "CoLa-DCE generates concept-guided counterfactuals for any classifier with a high degree of control regarding concept selection and spatial conditioning. The counterfactuals comprise an increased granularity through minimal feature changes."
  - [section 4.3] "The conditioned gradient with regards to the selected concepts λ1, ...,λk with binary constraints θ1, ..., θk is computed."
  - [corpus] Weak - the related papers focus on diffusion-based counterfactuals but don't directly address concept-based guidance as implemented here
- Break condition: If the classifier's latent space does not encode semantically meaningful concepts, or if the top-k concepts are not the most relevant for changing the prediction

### Mechanism 2
- Claim: Spatial conditioning increases the focus and locality of feature changes, improving transparency
- Mechanism: Binary masking based on gradient thresholds applied to spatial dimensions restricts concept modifications to the most probable regions, preventing changes in irrelevant areas and creating more focused explanations
- Core assumption: Each semantic feature should only be modified in specific local regions where it's most relevant to the counterfactual class
- Evidence anchors:
  - [section 4.4] "Assuming each feature is locally restricted and may only be modified in the most probable region(s), we add spatial constraints per concept by thresholding the gradient."
  - [section 5.3] "With added spatial constraints, a stronger focus in the explanation becomes apparent, either having more sparse explanations or reflecting a stronger focus on single semantic features."
  - [corpus] Weak - related work on diffusion counterfactuals doesn't mention spatial conditioning at the concept level
- Break condition: If spatial constraints are too restrictive and prevent necessary feature changes, or if the gradient thresholding is poorly calibrated

### Mechanism 3
- Claim: Local counterfactual target selection based on model perception improves the quality of generated counterfactuals
- Mechanism: Instead of using semantic similarity from WordNet, CoLa-DCE finds the reference sample with minimal distance in the classifier's feature space that has a different class prediction, creating counterfactuals closer to the decision boundary
- Core assumption: The classifier's internal representation of samples better captures semantic similarity than external label-based similarity measures
- Evidence anchors:
  - [section 5.1] "Selecting a target layer, the classifier-internal representation of a data point can be extracted via the activation or the attribution... the sample with minimal distance and differing class prediction to the encoded target sample is extracted."
  - [section 5.1] "Using the intermediate LRP [4] attribution yields substantial improvements in the minimal change needed while simultaneously achieving high flip ratios."
  - [corpus] Weak - the related papers use different target selection strategies but don't evaluate the impact of using model perception
- Break condition: If the feature space encoding doesn't accurately represent semantic similarity, or if the reference dataset is not representative

## Foundational Learning

- Concept: Diffusion models and the denoising process
  - Why needed here: Understanding how the diffusion process works is essential for implementing the concept and spatial conditioning mechanisms
  - Quick check question: How does the DDIM sampling method speed up the image generation process in diffusion models?

- Concept: Local attribution methods (LRP, CRP)
  - Why needed here: These methods are used to extract the classifier's perception of samples and to visualize the semantic concepts that influence predictions
  - Quick check question: What is the key difference between using activation values versus attribution scores for target selection in CoLa-DCE?

- Concept: Counterfactual explanation principles
  - Why needed here: Understanding the requirements for good counterfactuals (minimality, semantic similarity, plausibility) guides the design of CoLa-DCE
  - Quick check question: Why is the "near miss" approach used for target selection, and how does it relate to the minimality requirement?

## Architecture Onboarding

- Component map: Classifier -> Latent space extraction -> Concept selection -> Spatial conditioning -> Gradient alignment -> Diffusion model (encoder-decoder) -> Counterfactual output
- Critical path: Sample → classifier encoding → target selection → concept selection → gradient conditioning → diffusion generation → counterfactual output
- Design tradeoffs: The number of concepts (k) creates a tradeoff between accuracy (flip ratio) and comprehensibility (FID score), while spatial conditioning improves focus but may reduce flip ratio
- Failure signatures: Poor counterfactual quality may result from misconfigured gradient alignment parameters, inadequate concept selection (wrong k value), or insufficient spatial conditioning
- First 3 experiments:
  1. Generate counterfactuals using different numbers of concepts (k=10, 50, 100) and compare flip ratios and FID scores
  2. Compare target selection using activation vs. attribution (LRP) on a subset of the validation data
  3. Evaluate the impact of spatial conditioning by generating counterfactuals with and without spatial constraints and measuring the focus of the explanations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the quality of counterfactual explanations change when using more advanced diffusion models beyond the ones tested in the paper?
- Basis in paper: [inferred] The paper mentions using class-conditional LDM and miniSD diffusion models, but acknowledges that the main limitation is reliance on well-trained diffusion models and suggests that optimizing parameters based on the used model could positively affect generated counterfactuals.
- Why unresolved: The paper only tests a limited set of diffusion models and does not explore how using more advanced or recent diffusion architectures would impact the quality of counterfactual explanations.
- What evidence would resolve it: Conducting experiments using state-of-the-art diffusion models like Stable Diffusion XL or other recent advancements, and comparing the quality metrics (FID, flip ratio) and interpretability of counterfactuals generated with these models against the ones presented in the paper.

### Open Question 2
- Question: Can the concept-based guidance approach be extended to other types of explanations beyond counterfactuals, such as feature importance or prototype-based explanations?
- Basis in paper: [explicit] The paper introduces a concept-based approach for generating counterfactual explanations, but does not explore its potential application to other types of explanations in explainable AI.
- Why unresolved: The paper focuses specifically on counterfactual explanations and does not investigate whether the concept-based guidance and visualization techniques could be applied to other explanation methods.
- What evidence would resolve it: Developing and testing concept-based guidance methods for generating feature importance maps or prototype-based explanations, and comparing their effectiveness and interpretability against existing methods in those domains.

### Open Question 3
- Question: How does the choice of concept attribution method (e.g., LRP vs. other methods) impact the quality and interpretability of counterfactual explanations?
- Basis in paper: [explicit] The paper uses LRP for concept attribution and mentions that using intermediate LRP attribution yields substantial improvements in minimal change needed while achieving high flip ratios. However, it does not explore other attribution methods or compare their impact on the quality of explanations.
- Why unresolved: The paper relies on LRP for concept attribution without exploring alternative methods or comparing their effectiveness in generating counterfactual explanations.
- What evidence would resolve it: Conducting experiments using different concept attribution methods (e.g., Integrated Gradients, DeepLIFT, SHAP) and comparing the resulting counterfactual explanations in terms of their quality metrics, interpretability, and alignment with human understanding.

## Limitations
- Concept selection reliability depends on classifier's latent space encoding semantically meaningful features
- Spatial conditioning may overly restrict necessary changes for concepts with complex spatial distributions
- Target selection robustness depends on having a representative reference dataset that captures the classifier's decision boundary

## Confidence
- Concept selection reliability: Medium
- Spatial conditioning effectiveness: Medium
- Target selection robustness: Medium

## Next Checks
1. **Concept importance validation** - Conduct ablation studies by systematically varying k (number of concepts) and measuring both flip ratio and FID score across different datasets and classifier architectures to establish optimal trade-offs and verify concept selection reliability.

2. **Spatial constraint sensitivity analysis** - Test different gradient thresholding strategies and spatial constraint configurations on challenging cases where concepts span multiple regions, measuring the impact on both counterfactual quality and explanation focus.

3. **Cross-dataset generalization** - Evaluate CoLa-DCE on datasets with different visual characteristics (medical imaging, satellite imagery, synthetic data) to assess whether the concept-based guidance approach generalizes beyond natural images.