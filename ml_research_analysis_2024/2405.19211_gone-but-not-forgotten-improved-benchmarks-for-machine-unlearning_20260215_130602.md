---
ver: rpa2
title: 'Gone but Not Forgotten: Improved Benchmarks for Machine Unlearning'
arxiv_id: '2405.19211'
source_url: https://arxiv.org/abs/2405.19211
tags:
- unlearning
- privacy
- machine
- algorithms
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes improved benchmarks for evaluating machine
  unlearning algorithms, focusing on three key areas: using worst-case metrics instead
  of average-case, considering model update-based attacks that may leak information
  during unlearning, and analyzing performance over repeated (iterative) unlearning
  applications. The authors implement stronger membership inference attacks, including
  an offline version of LiRA, to better estimate privacy.'
---

# Gone but Not Forgotten: Improved Benchmarks for Machine Unlearning

## Quick Facts
- arXiv ID: 2405.19211
- Source URL: https://arxiv.org/abs/2405.19211
- Reference count: 26
- Primary result: This paper proposes improved benchmarks for evaluating machine unlearning algorithms, focusing on worst-case metrics, update-leakage attacks, and iterative performance evaluation.

## Executive Summary
This paper addresses critical gaps in machine unlearning evaluation by proposing three key improvements to benchmarking methodology. The authors argue that current evaluations rely too heavily on average-case metrics, fail to account for privacy leakage through model updates, and don't consider long-term performance across repeated unlearning applications. They implement stronger membership inference attacks (including an offline version of LiRA) and update-leakage attacks, creating an iterative unlearning pipeline to study performance degradation patterns. Preliminary results on CIFAR10 with ResNet18 reveal significant differences in how various unlearning algorithms maintain accuracy over time, with some degrading rapidly while others improve.

## Method Summary
The authors implement three key improvements to machine unlearning benchmarking: worst-case privacy metrics using stronger membership inference attacks (offline LiRA with 256 shadow models), update-leakage attacks that exploit information from model differences during unlearning, and an iterative evaluation framework that tracks performance across multiple unlearning iterations. They evaluate multiple unlearning algorithms (Identity, Retrain, Finetune, RandLabel, BadTeach, SCRUB+R, SSD, SSD+FT) on CIFAR10 with ResNet18, using forget sets constructed by sampling 1% of training data conditioned on not being previously forgotten. Hyperparameter tuning is performed using Optuna optimized for MIA performance and validation accuracy.

## Key Results
- Preliminary results show notable discrepancies in test accuracy among algorithms during iterative unlearning, with some degrading rapidly while others maintain or improve performance
- Stronger membership inference attacks (offline LiRA) provide more realistic privacy estimates than simple logistic regression classifiers
- Iterative evaluation reveals performance degradation patterns not visible in single-iteration assessments
- Update-leakage attacks successfully exploit information from model differences during unlearning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using worst-case metrics instead of average-case metrics provides a more realistic estimate of privacy risks in unlearning algorithms.
- Mechanism: Worst-case metrics capture the highest potential privacy leakage that could occur for any individual data point, rather than averaging across all points. This aligns with how users would care about their own privacy outcomes.
- Core assumption: Privacy is primarily a worst-case concern, and users care more about the maximum potential exposure than the average exposure.
- Evidence anchors:
  - [abstract] "we argue that researchers and practitioners should consider the following three essential characteristics: 1) Emphasis of worst-case metrics over average-case metrics"
  - [section] "We argue that worst-case measures of privacy are crucial for effective evaluations of unlearning algorithms"
- Break condition: If the distribution of privacy leakage is uniform across all data points, worst-case metrics may not provide additional value over average-case metrics.

### Mechanism 2
- Claim: Using stronger membership inference attacks (like offline LiRA) provides better upper bounds on privacy leakage than simpler attacks.
- Mechanism: Stronger attacks are more likely to succeed in identifying forgotten data, thus providing a more conservative (and realistic) estimate of privacy protection.
- Core assumption: The strength of the attack correlates with the likelihood of a real-world adversary being able to extract information about forgotten data.
- Evidence anchors:
  - [section] "Through our literature review we have found that the most common MIA used to evaluate algorithms is a simple Logistic Regression classifier... we adapt the offline version of LiRA for unlearning"
  - [section] "The effectiveness of LiRA as an MIA makes it a much more realistic estimate of privacy"
- Break condition: If the stronger attack becomes too computationally expensive to be practical for evaluation, it may not be useful for benchmarking.

### Mechanism 3
- Claim: Iterative unlearning analysis reveals performance degradation patterns that single-iteration evaluations miss.
- Mechanism: Repeated unlearning applications can accumulate effects that impact both privacy and accuracy over time, which aren't visible in one-off evaluations.
- Core assumption: Unlearning effects compound over multiple iterations, affecting long-term model viability.
- Evidence anchors:
  - [section] "Few papers consider this set-up... We have not encountered an iterative test accuracy evaluation in the vision domain"
  - [section] "In the iterative setting, it is required at each iteration to both ensure effective forgetting and, crucially, maintain model performance"
- Break condition: If unlearning algorithms are designed to be stateless or have no memory of previous iterations, iterative analysis may not reveal additional insights.

## Foundational Learning

- Concept: Differential Privacy
  - Why needed here: Many unlearning evaluations borrow concepts from DP literature, and understanding DP is crucial for interpreting privacy metrics in unlearning
  - Quick check question: What is the difference between (ε, δ)-DP and approximate unlearning in terms of guarantees provided?

- Concept: Membership Inference Attacks
  - Why needed here: MIAs are the primary tool for empirically evaluating privacy in unlearning algorithms
  - Quick check question: How does a membership inference attack determine whether a data point was part of the training set?

- Concept: Update-leakage attacks
  - Why needed here: Unlearning may actually increase information leakage through model updates, making this a critical evaluation dimension
  - Quick check question: How can the difference between two model versions be used as an attack vector?

## Architecture Onboarding

- Component map:
  Dataset preprocessing and partitioning module -> Base model training component -> Multiple unlearning algorithm implementations -> Membership inference attack implementations -> Update-leakage attack implementation -> Iterative evaluation pipeline -> Hyperparameter optimization framework -> Result aggregation and visualization tools

- Critical path:
  1. Train base model on training data
  2. Apply unlearning algorithm with forget set
  3. Run membership inference attacks to evaluate privacy
  4. Run update-leakage attacks if applicable
  5. Evaluate test accuracy
  6. Repeat for multiple iterations if doing iterative evaluation

- Design tradeoffs:
  - Strong attacks (online LiRA) provide better privacy estimates but are computationally expensive
  - Offline LiRA is more practical but may underestimate privacy risks
  - Iterative evaluation reveals long-term effects but multiplies computational costs
  - Hyperparameter tuning for privacy may conflict with performance optimization

- Failure signatures:
  - Privacy metrics improve while accuracy degrades rapidly (algorithm is over-forgetting)
  - Privacy metrics worsen after unlearning (algorithm is ineffective)
  - Performance degradation accelerates over iterations (algorithm has cumulative negative effects)
  - Update-leakage attacks succeed better than base attacks (unlearning is counterproductive)

- First 3 experiments:
  1. Run identity baseline (no unlearning) with simple MIA to establish baseline privacy and accuracy
  2. Run one unlearning algorithm (e.g., SSD) with offline LiRA to compare privacy estimates with simple MIA
  3. Run iterative evaluation of the same algorithm for 5-10 iterations to observe performance trends over time

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different unlearning algorithms compare in their ability to maintain test accuracy over many iterative unlearning applications?
- Basis in paper: [explicit] The paper discusses the need to analyze unlearning algorithm performance over repeated applications and shows preliminary results on CIFAR10 with ResNet18, finding notable discrepancies in test accuracy among algorithms.
- Why unresolved: The paper only presents preliminary results on one dataset and model architecture. The long-term behavior of various algorithms across different datasets and model architectures remains unknown.
- What evidence would resolve it: Systematic evaluation of multiple unlearning algorithms across various datasets (e.g., CIFAR100, ImageNet) and architectures (e.g., ResNet50, Vision Transformer) over 50+ iterative unlearning applications, measuring test accuracy degradation.

### Open Question 2
- Question: What is the optimal trade-off between privacy (measured by membership inference attack success) and computational efficiency for different unlearning algorithms?
- Basis in paper: [explicit] The paper states that machine unlearning involves balancing privacy, cost, and long-term performance, and mentions that hyperparameter tuning trades off performance and privacy differently.
- Why unresolved: The paper's preliminary results only analyze test-set accuracy and do not characterize privacy or runtime. The relationship between these three key requirements across different algorithms and datasets is not fully understood.
- What evidence would resolve it: Comprehensive benchmarking of multiple unlearning algorithms on various datasets measuring membership inference attack success rates, wall-clock runtime, and test accuracy, with Pareto-optimal curves showing the privacy-efficiency trade-off.

### Open Question 3
- Question: How does update-leakage attack performance differ between unlearning algorithms and full retraining, and under what conditions might unlearning actually harm privacy?
- Basis in paper: [explicit] The paper discusses update-leakage attacks and their potential to provide information to attackers, stating that unlearning may find an optimal middle ground between update-leakage and standard attacks.
- Why unresolved: The paper mentions implementing the update-leakage attack from [5] but does not provide detailed results. The conditions under which unlearning could be worse than retraining for privacy are not fully characterized.
- What evidence would resolve it: Detailed comparison of update-leakage attack success rates on unlearned models versus their corresponding retrained models across multiple unlearning algorithms, datasets, and attack parameters, identifying specific scenarios where unlearning increases vulnerability.

## Limitations

- Experimental scope is limited to CIFAR10 with ResNet18, which may not generalize to other datasets or architectures
- The paper provides preliminary results without comprehensive benchmarking across multiple algorithms and datasets
- Update-leakage attack implementation details are not fully specified beyond referencing prior work

## Confidence

- Claims about worst-case metric importance: **Medium** - theoretically sound but lacks broad empirical validation across different unlearning scenarios
- Claims about stronger MIA effectiveness: **High** - directly demonstrated through implementation and comparison with simple attacks
- Claims about iterative evaluation insights: **Medium** - preliminary results show clear patterns but need more extensive validation across algorithms and datasets

## Next Checks

1. Test the benchmarking framework on a text classification task with a transformer-based model to assess domain generalizability
2. Implement additional unlearning algorithms from recent literature to validate the comprehensive nature of the benchmark
3. Conduct sensitivity analysis on forget set size and composition to understand how dataset characteristics affect evaluation outcomes