---
ver: rpa2
title: Do Unlearning Methods Remove Information from Language Model Weights?
arxiv_id: '2410.08827'
source_url: https://arxiv.org/abs/2410.08827
tags:
- unlearning
- information
- accuracy
- unlearn
- facts
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new adversarial evaluation framework to
  test whether unlearning methods actually remove knowledge from LLM weights or merely
  hide it. The key idea is to give an attacker access to some unlearned facts (T)
  and have them recover other independent facts (V) from the same distribution via
  fine-tuning on T.
---

# Do Unlearning Methods Remove Information from Language Model Weights?

## Quick Facts
- arXiv ID: 2410.08827
- Source URL: https://arxiv.org/abs/2410.08827
- Authors: Aghyad Deeb; Fabien Roger
- Reference count: 40
- Key finding: State-of-the-art unlearning methods hide rather than remove knowledge, with recovery rates exceeding 88% for pretraining information

## Executive Summary
This paper introduces a new adversarial evaluation framework called Retraining on T (RTT) to test whether unlearning methods actually remove knowledge from LLM weights or merely hide it. The key insight is that if knowledge is hidden rather than removed, an attacker with access to related facts (T) can recover the hidden facts (V) through fine-tuning. Using this approach, the authors find that state-of-the-art unlearning methods like RMU and gradient descent only hide knowledge, with recovery rates exceeding 88% when unlearning pretraining knowledge while maintaining performance on other tasks. The framework also reveals that evaluations using fine-tuning-acquired knowledge overestimate robustness compared to pretraining knowledge.

## Method Summary
The paper proposes a new evaluation framework where knowledge to be unlearned is split into two datasets (T and V) with minimal mutual information. After applying an unlearning method, the attacker performs fine-tuning on T (RTT) to attempt recovery of V. Recovery Rate is calculated as the accuracy on V after RTT divided by the accuracy on V of the original model after RTT. Lower recovery rates indicate better knowledge removal. The method tests three unlearning approaches (RMU, gradient descent, RIA) on multiple datasets including years, MMLU categories, and cybersecurity questions.

## Key Results
- Recovery rates for pretrained information exceeded 88% across all tested unlearning methods
- RTT failed to recover fine-tuned information, suggesting fine-tuning-acquired knowledge is easier to unlearn than pretraining knowledge
- The recoverability of unlearned information depends on the type of knowledge being unlearned (pretraining vs fine-tuning)
- Stress testing with high-granularity hiding models showed RTT could still recover knowledge in most cases

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unlearning methods hide knowledge by altering model weights without removing underlying representations.
- Mechanism: The attacker fine-tunes on accessible facts (T) to recover hidden facts (V) from the same distribution, exploiting the fact that information remains encoded in weights even after unlearning.
- Core assumption: Facts in T and V share minimal mutual information, so training on T doesn't directly teach V but rather reverses the hiding mechanism.
- Evidence anchors:
  - [abstract] "using fine-tuning on the accessible facts can recover 88% of the pre-unlearning accuracy"
  - [section 3.2] "Training on T might reveal information that was hidden by increasing the model's propensity to output the unlearned facts without teaching the model the facts again"
  - [corpus] Weak evidence - corpus contains related papers but none directly address this specific mechanism
- Break condition: If facts in T and V have significant shared information, RTT would directly teach V rather than recover hidden knowledge.

### Mechanism 2
- Claim: Unlearning methods that maintain retain accuracy cannot effectively remove information from weights.
- Mechanism: When unlearning strength is adjusted to preserve performance on non-unlearned tasks, the method can only hide information rather than remove it, as evidenced by RTT recovery rates.
- Core assumption: There exists a fundamental tradeoff between information removal and retention of other capabilities.
- Evidence anchors:
  - [section 5.1] "recovery rates for pretrained information were greater than 88%, implying poor performance at removing information"
  - [section 5] "Even if we do not include a retain loss, RTT is often able to recover forget accuracy"
  - [corpus] Weak evidence - related papers discuss unlearning tradeoffs but not this specific relationship
- Break condition: If a method can remove information without affecting retain accuracy, this mechanism would not hold.

### Mechanism 3
- Claim: Fine-tuning learned information is easier to unlearn than pretraining knowledge.
- Mechanism: Information learned during fine-tuning is more superficial and easier to reverse, while pretraining knowledge is deeply embedded and harder to remove.
- Core assumption: Fine-tuning introduces changes that are more localized and less fundamental than pretraining.
- Evidence anchors:
  - [section 5.2] "RTT does not recover information for the fine-tuned dataset... but a recovery rate less than 35% for GD and RIA"
  - [section 5.2] "changes introduced by fine-tuning might be relatively superficial and easy to reverse"
  - [corpus] Weak evidence - no direct corpus support for this specific claim
- Break condition: If fine-tuning can deeply integrate knowledge similar to pretraining, this mechanism would not hold.

## Foundational Learning

- Concept: Mutual information and independence of datasets
  - Why needed here: The framework requires T and V to have minimal shared information to ensure RTT reveals hidden knowledge rather than directly teaching V
  - Quick check question: If two facts share high mutual information, would training on one fact necessarily improve performance on the other?

- Concept: Adversarial evaluation framework
  - Why needed here: The paper uses an attacker model where the attacker has access to T and tries to recover V, which is a specific type of adversarial evaluation
  - Quick check question: How does this attacker model differ from traditional adversarial attacks in machine learning?

- Concept: Fine-tuning and hyperparameter optimization
  - Why needed here: RTT involves fine-tuning with hyperparameter search, requiring understanding of learning rates, optimizers, and evaluation metrics
  - Quick check question: Why is it important to search over multiple learning rates when performing RTT?

## Architecture Onboarding

- Component map:
  Dataset creation and preprocessing (splitting facts into T and V) -> Unlearning method implementation (RMU, GD, RIA) -> RTT fine-tuning pipeline with hyperparameter search -> Evaluation framework for measuring recovery rates -> Analysis and visualization tools

- Critical path:
  1. Create datasets with minimal information leakage between splits
  2. Apply unlearning method to forget dataset
  3. Perform RTT on unlearned model using T
  4. Evaluate accuracy on V
  5. Calculate recovery rate and analyze results

- Design tradeoffs:
  - Dataset quality vs. computational cost (more splits = better independence but more expensive)
  - Unlearning strength vs. retain accuracy (stronger unlearning = more knowledge removal but may hurt other capabilities)
  - RTT hyperparameters (longer fine-tuning = better recovery but more expensive)

- Failure signatures:
  - High recovery rates (>80%) indicate knowledge was hidden rather than removed
  - Low recovery rates on fine-tuned information vs. high rates on pretraining information
  - RTT failing to recover knowledge despite it being present (suggests high-granularity hiding)

- First 3 experiments:
  1. Apply RTT to a baseline model (no unlearning) to establish baseline recovery rate
  2. Apply RTT to models unlearned with different methods (RMU, GD, RIA) and compare recovery rates
  3. Test RTT on both pretraining and fine-tuned information to compare unlearning effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do unlearning methods perform on real-world dangerous knowledge vs. controlled synthetic datasets?
- Basis in paper: [explicit] The paper notes their datasets use synthetic facts like years and birthdays, but real-world dangerous knowledge has different properties
- Why unresolved: The authors state they tested on controlled datasets but don't evaluate actual dangerous knowledge scenarios
- What evidence would resolve it: Testing unlearning methods on real-world dangerous knowledge (cybersecurity, bioweapons) using the same RTT framework

### Open Question 2
- Question: Are there fundamentally different approaches to unlearning that would remove information rather than just hide it?
- Basis in paper: [explicit] The paper concludes current methods only hide knowledge and asks what alternative approaches might be needed
- Why unresolved: All tested methods (RMU, GD, RIA) showed similar limitations; no alternative approaches were identified
- What evidence would resolve it: Demonstrating an unlearning method that achieves low recovery rates on pretrained knowledge while maintaining performance

### Open Question 3
- Question: What is the relationship between unlearning strength and retention of useful capabilities?
- Basis in paper: [explicit] Figure 3 shows unlearning strength can be increased until retain accuracy drops significantly
- Why unresolved: The paper only tests up to 100% retain accuracy drop; the full tradeoff curve is unknown
- What evidence would resolve it: Complete mapping of unlearning strength vs. forget/retain accuracy across multiple methods and datasets

## Limitations

- The computational expense of RTT limits its practical applicability, with some cases potentially failing to recover knowledge that remains present
- Framework effectiveness depends critically on the assumption that T and V datasets have minimal mutual information, which may be difficult to guarantee
- The paper uses specific unlearning methods (RMU, GD, RIA) and findings may not generalize to other approaches
- Evaluation focuses on factual knowledge recovery but may not capture other forms of information removal

## Confidence

- **High confidence**: The core finding that current unlearning methods hide rather than remove knowledge, supported by consistent recovery rates >88% across multiple datasets and methods
- **Medium confidence**: The claim that fine-tuning-acquired knowledge is easier to unlearn than pretraining knowledge, based on limited experimental evidence
- **Medium confidence**: The assertion that maintaining retain accuracy fundamentally limits unlearning effectiveness, though the full tradeoff space wasn't explored

## Next Checks

1. **Dataset independence verification**: Systematically measure and report the mutual information between T and V subsets across all datasets to validate the independence assumption underlying the RTT method

2. **Method generalization study**: Apply the RTT framework to additional unlearning methods (e.g., recent embedding-space approaches) to determine if the hiding phenomenon is universal or method-specific

3. **Computational optimization**: Develop and test approximations or sampling strategies for RTT that reduce computational cost while maintaining evaluation fidelity, making the method more practical for widespread adoption