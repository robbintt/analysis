---
ver: rpa2
title: OpenAI o1 System Card
arxiv_id: '2412.16720'
source_url: https://arxiv.org/abs/2412.16720
tags:
- evaluations
- o1-preview
- gpt-4o
- evaluation
- safety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The o1 model series employs large-scale reinforcement learning
  to enable reasoning using chain-of-thought, improving safety and robustness by allowing
  the model to reason about safety policies before responding to prompts. Evaluations
  demonstrate that o1 models achieve state-of-the-art performance in reducing disallowed
  content, resisting jailbreaks, and minimizing hallucinations compared to GPT-4o.
---

# OpenAI o1 System Card

## Quick Facts
- **arXiv ID**: 2412.16720
- **Source URL**: https://arxiv.org/abs/2412.16720
- **Reference count**: 40
- **Key outcome**: o1 uses chain-of-thought reasoning to improve safety and reduce disallowed content compared to GPT-4o

## Executive Summary
The o1 model series introduces a novel approach to AI safety through large-scale reinforcement learning that enables chain-of-thought reasoning. This allows the model to reason about safety policies before responding to prompts, resulting in significant improvements in safety metrics compared to previous models. The system demonstrates enhanced resistance to jailbreaks, reduced hallucinations, and improved fairness in stereotyped responses. While representing a substantial advancement in both capability and safety, the model requires ongoing research to address potential risks from its heightened intelligence.

## Method Summary
The o1 model employs large-scale reinforcement learning to develop reasoning capabilities that enable chain-of-thought processing. This approach allows the model to internally deliberate on safety policies and potential risks before generating responses. The system was evaluated through internal red teaming exercises, safety benchmarks, and comparative analysis against GPT-4o. Key evaluation areas included resistance to jailbreaks, reduction of disallowed content, hallucination minimization, fairness in stereotyped responses, and adherence to instruction hierarchies. Chain-of-thought monitoring was implemented to detect potential deceptive behaviors during the reasoning process.

## Key Results
- Achieves state-of-the-art performance in reducing disallowed content compared to GPT-4o
- Demonstrates improved resistance to jailbreak attempts through reasoning capabilities
- Shows minimal instances of deceptive behavior detected through chain-of-thought monitoring
- Exhibits better fairness in stereotyped responses and instruction hierarchy adherence

## Why This Works (Mechanism)
The o1 model's safety improvements stem from its ability to perform chain-of-thought reasoning before responding to prompts. This reasoning process allows the model to internally evaluate safety policies, consider potential risks, and develop appropriate responses that align with safety guidelines. By thinking through consequences and safety implications before answering, the model can better resist adversarial prompts and avoid generating disallowed content. The reinforcement learning framework trains the model to prioritize safety considerations during its internal deliberation process.

## Foundational Learning
- **Reinforcement Learning**: Trains models through reward-based optimization, enabling learning of complex behaviors including safety reasoning
  - Why needed: Allows models to learn safety behaviors through trial and error rather than explicit programming
  - Quick check: Compare RL-trained vs non-RL safety performance on benchmark tasks
- **Chain-of-Thought Reasoning**: Enables step-by-step internal deliberation before response generation
  - Why needed: Provides time for safety evaluation and risk assessment before committing to outputs
  - Quick check: Measure latency vs safety improvement tradeoff
- **Safety Policy Integration**: Embeds safety guidelines into the reasoning process
  - Why needed: Ensures safety considerations are part of the decision-making framework
  - Quick check: Test against adversarial safety prompts designed to bypass policies
- **Instruction Hierarchy**: Prioritizes safety instructions over other commands
  - Why needed: Prevents users from overriding safety measures through clever prompting
  - Quick check: Evaluate response to conflicting safety vs user instructions

## Architecture Onboarding

**Component Map**: Input Processing -> Chain-of-Thought Reasoning -> Safety Policy Evaluation -> Response Generation

**Critical Path**: Input reception → internal reasoning → safety policy check → response formulation → output

**Design Tradeoffs**: The system trades computational efficiency for enhanced safety, as chain-of-thought reasoning increases response time but improves safety outcomes. This represents a deliberate prioritization of safety over speed.

**Failure Signatures**: Potential failures include incomplete reasoning chains, safety policy misinterpretation, or reasoning loops that prevent response generation. The model may also exhibit over-caution, refusing benign requests due to excessive safety considerations.

**First Experiments**:
1. Test safety policy adherence with adversarial jailbreak attempts
2. Measure response quality degradation under heavy safety filtering
3. Evaluate chain-of-thought reasoning depth vs safety improvement correlation

## Open Questions the Paper Calls Out
The paper does not explicitly identify specific open questions, focusing instead on the demonstrated capabilities and current safety improvements of the o1 model series.

## Limitations
- Relies on internal red teaming and company-controlled metrics without independent verification
- Safety improvements benchmarked only against GPT-4o rather than industry standards
- Chain-of-thought monitoring methodology and criteria remain undisclosed
- Claims of state-of-the-art safety performance cannot be independently confirmed

## Confidence

**Safety metric improvements over GPT-4o**: Medium (internal data only)
**Chain-of-thought deception detection**: Low (methodology undisclosed)
**State-of-the-art safety performance**: Low (no independent verification)
**Fairness and instruction adherence**: Medium (limited public details)

## Next Checks

1. Request independent red teaming data and prompt sets used in safety evaluations
2. Obtain documentation of chain-of-thought monitoring methodology and false positive rates
3. Compare o1 safety metrics against publicly available third-party benchmarks