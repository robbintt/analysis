---
ver: rpa2
title: 'Thai Winograd Schemas: A Benchmark for Thai Commonsense Reasoning'
arxiv_id: '2405.18375'
source_url: https://arxiv.org/abs/2405.18375
tags:
- thai
- language
- winograd
- were
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Thai Winograd Schemas (Thai-WS), the first
  benchmark dataset for evaluating commonsense reasoning in the Thai language. The
  dataset was created by translating 285 English Winograd schemas into Thai with the
  help of professional translators and native speakers, while preserving cultural
  and linguistic nuances.
---

# Thai Winograd Schemas: A Benchmark for Thai Commonsense Reasoning

## Quick Facts
- arXiv ID: 2405.18375
- Source URL: https://arxiv.org/abs/2405.18375
- Authors: Phakphum Artkaew
- Reference count: 10
- Large language models achieve 93.68% accuracy on English Winograd schemas but only 51.58% on Thai versions

## Executive Summary
This paper introduces Thai Winograd Schemas (Thai-WS), the first benchmark dataset for evaluating commonsense reasoning in Thai language. The dataset contains 285 translated Winograd schemas created with professional translators and native speakers to preserve both linguistic nuances and cultural contexts. Evaluation of popular language models shows a significant performance drop from English (up to 93.68% accuracy) to Thai (as low as 51.58%), highlighting the challenges of cross-lingual commonsense reasoning and the need for language-specific benchmarks in low-resource languages.

## Method Summary
The Thai-WS dataset was created by translating 285 English Winograd schemas into Thai using professional translators who adapted cultural references while preserving semantic ambiguity. Three native Thai speakers validated the translations for clarity and accuracy. Models including GPT-4, Claude-3 variants, and Typhoon were evaluated using a prompt-based approach with standardized system prompts, user prompts, and exact match scoring criteria. The evaluation was conducted with temperature=0 and seed=77 where possible to ensure reproducibility.

## Key Results
- GPT-4 achieves 93.68% accuracy on English Winograd schemas
- GPT-4 achieves only 51.58% accuracy on Thai Winograd schemas
- Performance drop suggests significant challenges in cross-lingual commonsense reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The performance drop in Thai Winograd schemas is primarily due to the models' limited exposure to Thai linguistic structures and cultural contexts during training.
- Mechanism: When models are trained predominantly on English data, they develop strong patterns for English syntax, idioms, and cultural references. Thai language has distinct grammatical structures, different pronoun usage patterns, and unique cultural contexts that aren't captured in English training data. This creates a gap where models can resolve English ambiguities well but struggle with Thai-specific ambiguities.
- Core assumption: The models' architecture and training data distribution create language-specific reasoning capabilities that don't transfer equally across languages.
- Evidence anchors:
  - [abstract] "Results indicate that while models like GPT-4 and Claude-3-Opus achieve high accuracy in English, their performance significantly drops in Thai"
  - [section] "The performance drop in Thai suggests that these models may have more difficulty handling the nuances and linguistic structures of Thai"
  - [corpus] Weak evidence - the corpus neighbors don't provide direct comparison data between English and Thai model performance

### Mechanism 2
- Claim: The translation methodology preserves semantic ambiguity while adapting cultural references, making the Thai dataset a valid cross-linguistic evaluation tool.
- Mechanism: Professional translators adapted names, contexts, and phrases to Thai while maintaining the core ambiguity structure of Winograd schemas. This preserves the reasoning challenge while making it culturally relevant to Thai speakers. The validation process with native speakers ensures that the translated schemas maintain their intended difficulty level.
- Core assumption: Cultural adaptation doesn't fundamentally alter the logical structure of the reasoning task, just makes it more accessible to Thai speakers.
- Evidence anchors:
  - [section] "The translated Winograd Schemas were reviewed with three native Thai speakers, and final adjustments were made to ensure clarity"
  - [section] "Translation guidelines were provided, instructing them to adapt names and contexts to suit the Thai language while preserving the ambiguity and nuances"
  - [corpus] Moderate evidence - the related papers on Thai benchmarks suggest growing interest in culturally adapted NLP tasks

### Mechanism 3
- Claim: The prompt-based evaluation method provides consistent and reproducible results across different model architectures despite not using the traditional probability-based approach.
- Mechanism: By standardizing system prompts, temperature settings, and requiring exact matches to correct answers, the evaluation method reduces variability in model responses. This allows fair comparison between models with different architectures (GPT, Claude, Typhoon) using a consistent methodology.
- Core assumption: The prompt structure and exact-match evaluation criteria are sufficient to capture the models' true reasoning capabilities on Winograd schemas.
- Evidence anchors:
  - [section] "Language models were evaluated using a prompt structure approach. A system prompt and user prompt structure were utilized"
  - [section] "Only exact matches to the correct choice were counted as correct answers"
  - [corpus] Weak evidence - the corpus doesn't provide validation of this evaluation methodology

## Foundational Learning

- Concept: Cross-linguistic transfer learning
  - Why needed here: Understanding why models perform differently on Thai vs English requires knowledge of how language models transfer learned patterns across languages
  - Quick check question: What architectural features enable or limit cross-linguistic reasoning capabilities in transformer-based models?

- Concept: Winograd schema resolution mechanisms
  - Why needed here: The core task requires understanding how models resolve pronoun reference ambiguities using context and commonsense knowledge
  - Quick check question: How do probability-based approaches differ from prompt-based approaches in evaluating Winograd schema resolution?

- Concept: Cultural adaptation in NLP
  - Why needed here: The translation process involves adapting cultural references while preserving reasoning challenges, requiring understanding of how culture affects language understanding
  - Quick check question: What are the key differences between direct translation and cultural adaptation in NLP tasks?

## Architecture Onboarding

- Component map: Translation pipeline -> Validation process -> Prompt-based evaluation -> Model interfaces -> Performance comparison
- Critical path: Dataset creation → Translation and validation → Model evaluation setup → Performance comparison → Analysis of cross-linguistic gaps
- Design tradeoffs: The exact-match evaluation criteria ensures consistency but may miss valid reasoning patterns. The prompt-based approach is reproducible but doesn't leverage the models' full probabilistic capabilities. The translation maintains cultural relevance but may introduce subtle biases.
- Failure signatures: If models show high variance in responses to identical prompts, if Thai speakers disagree on correct answers, or if performance patterns don't align with expected cross-linguistic transfer capabilities.
- First 3 experiments:
  1. Evaluate the same models on a different low-resource language benchmark to see if performance drops are language-specific or general
  2. Fine-tune a model on Thai Winograd schemas and test if performance improves on the same benchmark
  3. Create a hybrid evaluation using both prompt-based and probability-based methods to compare consistency and identify any missed valid responses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do Thai-specific linguistic features (such as pronoun usage and idiomatic expressions) impact the performance of multilingual language models on commonsense reasoning tasks?
- Basis in paper: [explicit] The paper notes that Thai language structures differ significantly from English, particularly in pronoun usage (e.g., "they" can refer to both people and objects in English but not in Thai), requiring translators to adapt phrases to sound natural in Thai.
- Why unresolved: While the paper demonstrates performance drops in Thai, it does not isolate which specific linguistic features contribute most to the difficulty.
- What evidence would resolve it: A controlled study comparing model performance on schemas with varying linguistic complexity (e.g., those requiring pronoun disambiguation vs. those relying on cultural context) would clarify which features pose the greatest challenge.

### Open Question 2
- Question: Can the Thai Winograd Schemas dataset be used to improve the cross-lingual transfer learning capabilities of multilingual language models?
- Basis in paper: [inferred] The paper highlights the performance gap between English and Thai versions of the Winograd Schemas, suggesting that models struggle with low-resource languages like Thai.
- Why unresolved: The paper evaluates existing models but does not explore whether fine-tuning on the Thai dataset improves performance on other Thai language tasks or cross-lingual transfer.
- What evidence would resolve it: Experiments fine-tuning multilingual models on the Thai Winograd Schemas and evaluating their performance on other Thai language tasks (e.g., XNLI or XCOPA) would demonstrate the dataset's utility for cross-lingual learning.

### Open Question 3
- Question: How does the performance of Thai language models on the Thai Winograd Schemas compare to human performance, and what are the implications for evaluating commonsense reasoning in low-resource languages?
- Basis in paper: [explicit] The paper mentions that a human baseline for English Winograd Schemas is approximately 90%, but no Thai human baseline has been established.
- Why unresolved: Without a Thai human baseline, it is difficult to assess whether the performance drop observed in models reflects a true challenge or a limitation in the dataset's alignment with human reasoning.
- What evidence would resolve it: Conducting a human evaluation study on the Thai Winograd Schemas would provide a baseline for comparison and insights into the alignment between human and machine reasoning in Thai.

## Limitations

- Translation process details and validation methodology are not fully specified, making it difficult to assess potential cultural or linguistic biases
- Prompt-based evaluation approach may not capture the full reasoning capabilities of models compared to probability-based methods
- Model selection focuses on commercial APIs without exploring open-source alternatives that could be fine-tuned on Thai data

## Confidence

- High confidence: Core finding that cross-linguistic performance drops exist, aligning with established research on multilingual model limitations
- Medium confidence: Specific performance numbers, given potential evaluation methodology constraints and API access variability
- Low confidence: Attributing the performance gap solely to linguistic differences without exploring alternative explanations

## Next Checks

1. Compare model performance on Thai-WS with performance on Winograd schemas in other non-English languages to determine if the gap is Thai-specific or reflects general cross-linguistic transfer limitations
2. Implement probability-based evaluation alongside the current prompt-based approach to assess whether exact-match criteria are missing valid reasoning patterns
3. Fine-tune an open-source model on the Thai-WS training split and evaluate on the test set to measure the impact of language-specific adaptation