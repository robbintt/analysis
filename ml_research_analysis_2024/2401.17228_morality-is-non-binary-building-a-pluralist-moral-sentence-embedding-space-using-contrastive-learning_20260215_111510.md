---
ver: rpa2
title: 'Morality is Non-Binary: Building a Pluralist Moral Sentence Embedding Space
  using Contrastive Learning'
arxiv_id: '2401.17228'
source_url: https://arxiv.org/abs/2401.17228
tags:
- moral
- simcse
- supervised
- embedding
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a method to create a moral sentence embedding
  space using contrastive learning. The authors argue that morality is not binary
  but can be decomposed into a finite number of elements, and aim to capture these
  elements in an embedding space.
---

# Morality is Non-Binary: Building a Pluralist Moral Sentence Embedding Space using Contrastive Learning

## Quick Facts
- arXiv ID: 2401.17228
- Source URL: https://arxiv.org/abs/2401.17228
- Reference count: 40
- Key result: Supervised contrastive learning creates moral embedding spaces that capture relationships between moral elements better than unsupervised approaches

## Executive Summary
This paper proposes a method to create a moral sentence embedding space using contrastive learning. The authors argue that morality is not binary but can be decomposed into a finite number of elements, and aim to capture these elements in an embedding space. They train three different approaches: an off-the-shelf model, an unsupervised SimCSE model, and a supervised SimCSE model using label information. The embedding spaces are evaluated intrinsically by visualizing the relationships between moral elements and computing moral similarity, and extrinsically by testing generalizability on a held-out test set and comparing to an independently collected moral dictionary. Results show that the supervised approach outperforms the other two, successfully capturing the relationships between moral elements.

## Method Summary
The authors create a moral sentence embedding space using supervised contrastive learning with SimCSE on the Moral Foundations Twitter Corpus (MFTC). They implement three training strategies: using an off-the-shelf model, unsupervised SimCSE, and supervised SimCSE with label information. The supervised approach constructs training triples using two policies for negative instance selection: "opposite" (selecting opposite virtue/vice pairs) and "outside" (selecting elements from different moral foundations). The model is evaluated through intrinsic methods (UMAP visualization, moral similarity matrices) and extrinsic methods (test set performance, comparison with Moral Foundations Dictionary 2.0).

## Key Results
- Supervised SimCSE outperforms both off-the-shelf and unsupervised SimCSE approaches
- The embedding space successfully captures measurable relationships between moral elements
- Geometric clustering in the embedding space reflects virtue-vice dualities from Moral Foundations Theory

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Supervised contrastive learning with label information enables the embedding space to disentangle different moral elements from a pluralist moral theory.
- Mechanism: The training strategy uses label information to construct positive and negative pairs for contrastive learning. Positive pairs are sentences with the same moral label(s), while negative pairs are sentences with either opposite moral elements (opposite policy) or elements from different moral foundations (outside policy). This forces the embedding space to cluster sentences with the same moral elements together and separate those with different elements.
- Core assumption: The structure of the Moral Foundations Theory (MFT) taxonomy, with its virtue-vice dualities, provides a meaningful basis for selecting positive and negative pairs that will lead to disentangled clusters in the embedding space.

### Mechanism 2
- Claim: The contrastive learning objective, by minimizing distance between positive pairs and maximizing distance between negative pairs, creates meaningful geometric relationships between moral elements in the embedding space.
- Mechanism: The supervised contrastive loss function encourages the model to learn an embedding space where sentences with the same moral label(s) are close together, and sentences with different moral label(s) are far apart. This creates clusters for each moral element and relationships between these clusters that reflect the moral structure of the data.
- Core assumption: The cosine similarity used in the contrastive loss is an appropriate measure of semantic similarity for moral sentences, and the temperature hyperparameter is set correctly to balance the positive and negative terms.

### Mechanism 3
- Claim: The use of a pre-trained BERT model as the base encoder for SimCSE provides a strong semantic foundation that, when fine-tuned with contrastive learning, can capture nuanced moral distinctions.
- Mechanism: The pre-trained BERT model already encodes a rich semantic representation of language. Fine-tuning it with the contrastive learning objective on the MFTC dataset adapts this representation to the specific task of distinguishing between different moral elements, leveraging the existing semantic knowledge while learning the moral distinctions.
- Core assumption: The pre-trained BERT model's semantic representation is sufficiently general and transferable to the task of moral sentence embedding, and the fine-tuning process does not destroy this general semantic knowledge.

## Foundational Learning

- Concept: Contrastive Learning
  - Why needed here: Contrastive learning is the core training paradigm used to create the moral embedding space. It allows the model to learn meaningful representations by comparing similar and dissimilar examples.
  - Quick check question: What is the key difference between the contrastive learning objective used in supervised SimCSE and unsupervised SimCSE in this paper?

- Concept: Moral Foundations Theory (MFT)
  - Why needed here: MFT provides the theoretical framework for understanding morality as a set of discrete elements, which is the basis for the supervised training strategy and the evaluation of the embedding space.
  - Quick check question: How many moral foundations are there in the MFT, and what is the relationship between virtues and vices within each foundation?

- Concept: Sentence Embeddings
  - Why needed here: Sentence embeddings are the output of the model, representing each sentence as a point in a high-dimensional space. Understanding their properties and evaluation is crucial for interpreting the results.
  - Quick check question: What is the dimensionality of the SimCSE sentence embeddings used in this paper, and why is dimensionality reduction (e.g., UMAP) necessary for visualization?

## Architecture Onboarding

- Component map: Pre-trained BERT model -> SimCSE contrastive learning framework -> MFTC dataset (training/evaluation) -> MFD2.0 dictionary (extrinsic evaluation) -> Visualization (UMAP) + Similarity computation + Classification evaluation
- Critical path: Training the supervised SimCSE model by loading pre-trained BERT, constructing training triples from MFTC with opposite/outside policies, defining supervised contrastive loss, and optimizing parameters
- Design tradeoffs: Label information vs. annotation effort; training data size vs. missing labels; classifier complexity vs. general-purpose embedding goal
- Failure signatures: No discernible clusters in UMAP visualization; low and inconsistent moral similarity scores; poor classification performance
- First 3 experiments:
  1. Train supervised SimCSE on MFTC training set and visualize embedding space with UMAP to check for moral element clusters
  2. Compute moral similarity table for trained embedding space to assess relationships between moral elements
  3. Evaluate embedding space generalizability on MFTC test set using same visualization and similarity analysis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of negative instance policy (opposite vs. outside) affect the quality of the moral embedding space?
- Basis in paper: The authors propose two policies for selecting negative instances in the supervised SimCSE approach and evaluate their impact on the resulting embedding space.
- Why unresolved: The paper does not provide a direct comparison of the two policies, only stating that they were applied to different halves of the training set.
- What evidence would resolve it: A direct comparison of the embedding spaces generated using the opposite and outside policies, evaluated using the same intrinsic and extrinsic metrics.

### Open Question 2
- Question: Can the moral embedding space be improved by incorporating additional contextual information, such as the domain of the tweets?
- Basis in paper: The MFTC dataset includes tweets from seven different domains, suggesting that domain-specific information could be relevant for moral judgment.
- Why unresolved: The paper does not explore the potential benefits of incorporating domain information into the training process.
- What evidence would resolve it: An experiment comparing the performance of the moral embedding space with and without domain-specific information, evaluated using the same intrinsic and extrinsic metrics.

### Open Question 3
- Question: How does the moral embedding space generalize to other moral value taxonomies, such as the Schwartz value theory?
- Basis in paper: The paper focuses on the Moral Foundations Theory (MFT), but mentions that other moral value taxonomies exist and could be explored.
- Why unresolved: The paper only evaluates the embedding space using the MFTC dataset and the MFD2.0 dictionary, both based on the MFT.
- What evidence would resolve it: An experiment evaluating the moral embedding space on a dataset annotated with a different moral value taxonomy, such as the Schwartz value theory, and comparing the results to those obtained with the MFTC dataset.

## Limitations

- The framework relies on contested assumptions about Moral Foundations Theory as a valid representation of human morality
- The MFTC corpus consists primarily of tweets, which may not represent the full diversity of moral language
- The "opposite" and "outside" policies for negative sample selection are heuristic approaches without systematic evaluation of alternatives

## Confidence

**High Confidence (7/10)**:
- Supervised SimCSE outperforms off-the-shelf and unsupervised baselines on MFTC test set
- Embedding space captures measurable relationships between moral elements
- Geometric clustering reflects virtue-vice dualities in MFT

**Medium Confidence (4/10)**:
- Claims about morality being "non-binary" and decomposable rest on contested theoretical assumptions
- Effectiveness of specific negative sample selection policies demonstrated but not rigorously compared
- Embedding space's ability to generalize beyond MFTC domain suggested but not thoroughly validated

**Low Confidence (2/10)**:
- Assertion that human labels are "necessary" doesn't establish this is the only viable approach
- Claims about capturing full complexity of pluralist morality limited by theoretical and data constraints

## Next Checks

1. **Ablation Study on Pair Selection**: Systematically compare the "opposite" and "outside" policies against random negative sampling and other heuristic approaches to determine their relative contribution to embedding quality.

2. **Cross-Domain Generalization**: Evaluate the embeddings on moral text from different sources (news articles, literature, academic writing) and languages to assess their broader applicability beyond Twitter.

3. **Theoretical Sensitivity Analysis**: Test how sensitive the embedding quality is to different moral theories by training separate models on subsets of MFTC data annotated with different theoretical lenses.