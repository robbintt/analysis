---
ver: rpa2
title: Closed-form Filtering for Non-linear Systems
arxiv_id: '2402.09796'
source_url: https://arxiv.org/abs/2402.09796
tags:
- gaussian
- where
- such
- then
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces a new class of filters based on Gaussian
  PSD Models (GPMs) for approximating the filtering distribution in Hidden Markov
  Models. The key contributions include: Closed-form filtering operations: GPMs allow
  for efficient closed-form computation of filtering operations like products, marginals,
  and integrals, which are typically intractable for general non-linear systems.'
---

# Closed-form Filtering for Non-linear Systems

## Quick Facts
- **arXiv ID**: 2402.09796
- **Source URL**: https://arxiv.org/abs/2402.09796
- **Reference count**: 40
- **Primary result**: Introduces PSDFilter, a closed-form filtering algorithm for non-linear systems with O(ε^{-1.5}) computational complexity for very regular kernels

## Executive Summary
This paper introduces Gaussian PSD Models (GPMs) as a new class of filters for approximating the filtering distribution in Hidden Markov Models. The key innovation is that GPMs enable closed-form computation of filtering operations like products, marginals, and integrals, which are typically intractable for general non-linear systems. The proposed PSDFilter algorithm provides strong theoretical guarantees on stability and robustness, with estimation error that adapts to the regularity of transition probabilities. For very regular kernels, the algorithm achieves better computational efficiency than particle filtering methods.

## Method Summary
The method involves learning approximations of transition and observation kernels as GPMs using kernel ridge regression on square roots of target functions. The learned models are then used in a recursive filtering algorithm that computes closed-form operations (Product, Marginal, Integral, PartialEvaluation) on GPMs to approximate the filtering distribution. The approach relies on structural assumptions about the target functions being sums of squared smooth functions and mixing properties of the transition kernels. The method includes an extension to Generalized GPMs for handling more complex distributions.

## Key Results
- PSDFilter achieves computational complexity of O(ε^{-1.5}) and memory complexity of O(ε^{-1}) for very regular kernels
- The filter is stable and robust with estimation error depending on approximation quality and adaptive to transition kernel regularity
- Closed-form operations for GPMs enable efficient filtering without particle sampling
- Extension to Generalized GPMs allows handling of distributions with non-diagonal precision matrices

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gaussian PSD Models enable closed-form computation of filtering operations that are typically intractable for general non-linear systems
- Mechanism: The model structure f(x) = Φη(x)⊤AΦη(x) allows matrix operations to represent probability density operations
- Core assumption: Target density can be approximated by sum of squared smooth functions, kernel evaluations kη(x,u) can be computed efficiently
- Evidence anchors: [abstract] "PSDFilter, an approximate filtering algorithm which plugs ˆQ and ˆG in the iteration above"; [section 2.1] "Proposition 1 (Closed form operations for Gaussian PSD Models)"

### Mechanism 2
- Claim: The filter is stable and robust with estimation error that depends on approximation quality and adapts to transition kernel regularity
- Mechanism: Stability follows from mixing assumptions on optimal kernel Rn and Hilbert metric contraction properties
- Core assumption: Transition kernel Q satisfies mixing conditions and can be approximated by GPMs with sufficient precision
- Evidence anchors: [abstract] "estimation error that depends on the quality of the approximation and is adaptive to the regularity of the transition probabilities"; [section 4.3] "Theorem 6 (PSD filter robustness and stability)"

### Mechanism 3
- Claim: Computational complexity scales as O(ε^-1.5) and memory as O(ε^-1) for very regular kernels, outperforming particle filtering
- Mechanism: Order of GPM remains constant through filtering steps, learning rates for smooth functions are optimal in L∞ norm
- Core assumption: Transition/observation kernels have high smoothness and learning algorithm achieves optimal rates for sum-of-squares functions
- Evidence anchors: [abstract] "computational complexity of O(ε^-3/2) respectively, including the offline learning step"; [section 3.2] "Theorem 4 proves that ˆf converges to f in L∞, with optimal rates"

## Foundational Learning

- **Hidden Markov Models and Bayesian filtering recursion**: Why needed here - The entire filtering framework builds on the HMM structure and the recursive Bayes update formula (1). Quick check question: What are the two steps in the recursive filtering equation and what do they represent?

- **Reproducing kernel Hilbert spaces and kernel ridge regression**: Why needed here - The learning algorithm for GPMs reduces to kernel ridge regression in Hη, and approximation guarantees depend on RKHS properties. Quick check question: How does the choice of kernel bandwidth η relate to the desired approximation precision ε?

- **Stability analysis using Hilbert metric and mixing coefficients**: Why needed here - Theoretical guarantees for filter stability rely on comparing measures using the Hilbert metric and exploiting mixing properties of the optimal kernel. Quick check question: What is the relationship between the mixing constant σ and the contraction coefficient of the Hilbert metric?

## Architecture Onboarding

- **Component map**: Learning module (Algorithm 1) → Filter computation module (Algorithm 2) → Operations module (Proposition 1 operations)
- **Critical path**: Learn Q and G approximations → Initialize filter → For each observation: Product → Marginal → Partial evaluation → Normalization
- **Design tradeoffs**: Model order M vs. approximation accuracy (higher M → better accuracy but more computation); kernel bandwidth η vs. learning rates (smaller η → better approximation but requires more data)
- **Failure signatures**: Diverging filter estimates (poor mixing or bad initialization); numerical instability in matrix operations (ill-conditioned A matrices); learning failure (inadequate anchor points or samples)
- **First 3 experiments**:
  1. Test closed-form operations on simple Gaussian Mixture Models to verify correctness of Product/Marginal/Integral implementations
  2. Verify learning rates by approximating known smooth functions with increasing model orders M
  3. Compare filter stability on a linear Gaussian system (where Kalman filter is exact) to establish baseline accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of PSDFilter compare to particle filtering methods in practice for non-linear systems with multi-modal distributions?
- Basis in paper: [inferred] The paper states that PSDFilter can handle multi-modality and provides theoretical guarantees, but does not provide empirical comparisons to particle filtering methods
- Why unresolved: The paper focuses on theoretical analysis and does not include experimental results comparing PSDFilter to particle filtering methods
- What evidence would resolve it: Empirical results comparing the accuracy and computational efficiency of PSDFilter and particle filtering methods on benchmark non-linear filtering problems with multi-modal distributions

### Open Question 2
- Question: What are the limitations of Gaussian PSD Models in approximating transition kernels with complex dependencies between states?
- Basis in paper: [explicit] The paper discusses the extension to Generalized Gaussian PSD Models to handle non-diagonal precision matrices, but does not provide a comprehensive analysis of the limitations of Gaussian PSD Models
- Why unresolved: The paper does not explore the limitations of Gaussian PSD Models in detail, particularly in cases where the transition kernel has complex dependencies between states
- What evidence would resolve it: A theoretical analysis of the approximation capabilities of Gaussian PSD Models for transition kernels with different types of dependencies, and empirical results demonstrating the performance of PSDFilter on such problems

### Open Question 3
- Question: How does the choice of anchor points and precision parameters affect the performance of PSDFilter?
- Basis in paper: [inferred] The paper mentions that the order of the approximation depends on the regularity of the transition kernels, but does not discuss the impact of the choice of anchor points and precision parameters
- Why unresolved: The paper does not provide guidance on how to choose anchor points and precision parameters for optimal performance of PSDFilter
- What evidence would resolve it: A theoretical analysis of the impact of anchor points and precision parameters on the approximation quality and computational efficiency of PSDFilter, and empirical results demonstrating the performance of PSDFilter with different choices of these parameters

## Limitations

- Theoretical guarantees critically depend on structural assumptions about target functions being sums of squared smooth functions and mixing properties of transition kernels, which may not hold for many practical non-linear systems
- Computational complexity claims are asymptotic and assume optimal learning rates are achieved; practical performance may vary significantly based on implementation details and system characteristics
- The paper provides limited empirical validation across diverse system types, focusing primarily on theoretical analysis

## Confidence

- **High confidence**: Closed-form operations for GPMs (Product, Marginal, Integral) are mathematically sound and well-defined, following directly from model structure and trace computations
- **Medium confidence**: Stability and robustness guarantees hold under stated mixing assumptions, but practical relevance depends on how well real systems satisfy these conditions
- **Medium confidence**: Computational complexity advantages are theoretically established for very regular kernels, but practical performance may vary significantly based on implementation details and system characteristics

## Next Checks

1. **Assumption testing**: Systematically evaluate the performance of PSDFilter on systems with varying degrees of smoothness and mixing properties to identify boundaries where theoretical guarantees break down

2. **Empirical complexity validation**: Implement PSDFilter and particle filtering on benchmark non-linear filtering problems, measuring actual computational time and memory usage across different ε targets to verify claimed complexity advantages

3. **Generalization testing**: Test PSDFilter on systems with non-smooth transitions, multimodal distributions, and non-diagonal precision matrices to assess robustness beyond theoretical assumptions and evaluate the effectiveness of the Generalized GPM extension