---
ver: rpa2
title: A Bayesian Approach to Online Planning
arxiv_id: '2406.02103'
source_url: https://arxiv.org/abs/2406.02103
tags:
- search
- bayesian
- tree
- planning
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a Bayesian approach to online planning using
  Thompson sampling to search decision trees, addressing the problem of imperfect
  neural network approximations in Monte Carlo tree search. The core method idea involves
  formulating tree search as a sequential discovery of rewards, where Thompson sampling
  is used to select which branches to explore based on posterior probabilities.
---

# A Bayesian Approach to Online Planning

## Quick Facts
- arXiv ID: 2406.02103
- Source URL: https://arxiv.org/abs/2406.02103
- Reference count: 40
- The paper proposes Thompson sampling for tree search with Bayesian regret bounds of O(√H(z*)T) and shows significant performance gains in ProcGen environments when ground-truth uncertainty is known.

## Executive Summary
This paper addresses the challenge of planning in decision processes where neural network approximations may be imperfect by introducing a Bayesian approach to online planning using Thompson sampling for tree search. The authors formulate tree search as a sequential discovery of rewards and prove a finite-time Bayesian regret bound that depends on the entropy of the prior distribution. They propose practical implementations using max-backups for posterior propagation and forward sampling for action selection. Empirical results on ProcGen Maze and Leaper environments demonstrate that the Bayesian approach significantly outperforms standard MCTS when accurate uncertainty estimates are available, though learned uncertainty estimates alone were not sufficient to yield improvements.

## Method Summary
The method introduces Thompson Sampling Tree Search (TSTS) and Bayes-UCB Tree Search (BTS) algorithms that use neural network uncertainty estimates to guide exploration in decision trees. The approach maintains posterior distributions over Q-values at each state-action pair, updating them using max-backup propagation when rewards are observed at leaf nodes. Action selection is performed via Thompson sampling or Bayes-UCB with a state-dependent quantile schedule. The algorithms are evaluated through self-play, where the neural network is trained using annotations from the current planner, creating an iterative improvement loop.

## Key Results
- Bayesian regret bound of O(√H(z*)T) where H(z*) is the Shannon entropy of the prior distribution
- On ProcGen Maze and Leaper environments, Bayesian planners outperform standard MCTS when ground-truth uncertainty is available
- BTS with max-backups and forward sampling shows superior performance compared to TSTS in practice
- Learned uncertainty estimates were not accurate enough to yield performance improvements, highlighting the challenge of uncertainty estimation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Thompson sampling on tree branches achieves lower Bayesian regret when prior entropy is low.
- Mechanism: The algorithm samples branches according to their posterior probability of being optimal, focusing search on high-value regions while maintaining exploration.
- Core assumption: Posterior distributions over Q-values are independent across actions at each state.
- Evidence anchors:
  - [abstract]: "Our bound shows that when the Shannon entropy of the prior is small (equivalent to high certainty in the neural net approximation), the expected regret is small."
  - [section 2.1]: "Theorem 1. The regret of the leaf selection rule defined in Eq. (4) satisfies: E [Regret(T )] ≤ HR max √(1/2 |Z|H(z*)T)."
- Break condition: If posterior independence assumption fails, the theoretical regret bound may not hold.

### Mechanism 2
- Claim: Max-backup propagation maintains correct posterior distributions under independence assumption.
- Mechanism: When a reward is revealed at a leaf, the posterior for parent Q-values is updated by backing up the maximum order statistic of child posteriors.
- Core assumption: Reward observations at leaves are independent given the history.
- Evidence anchors:
  - [section 2.2]: "If we further assume that the posterior for branches that do not involve (st-1, at-1) does not change, we can apply the rule in (5) recursively to update all the posteriors in the tree."
  - [section 2.2]: "We refer to this update as the max-backup method."
- Break condition: Correlated rewards across branches would invalidate the max-backup update rule.

### Mechanism 3
- Claim: Bayes-UCB with state-dependent quantile schedule outperforms Thompson sampling in tree search.
- Mechanism: Actions are selected based on optimistic quantiles of posterior Q-values, with exploration decreasing as visit counts increase.
- Core assumption: The quantile schedule α(s) = 1 - (1-α0)·e^(-N(s)-1/β) provides appropriate exploration-exploitation tradeoff.
- Evidence anchors:
  - [section 2.3]: "In our experiments, we found the BTS schedule to outperform both Bayes-UCB and Bayes-UCT2."
  - [section 2.3]: "The Bayes-UCB algorithm of Kaufmann et al. (2012) applied a selection rule α(s) = 1 - β/N(s) in the MAB setting."
- Break condition: If the neural network uncertainty estimates are inaccurate, the quantile-based selection may explore suboptimal branches excessively.

## Foundational Learning

- Concept: Bayesian inference and posterior updating
  - Why needed here: The entire algorithm relies on maintaining and updating posterior distributions over Q-values as rewards are observed during tree search.
  - Quick check question: If you observe a reward of 5 at a leaf state-action pair, how would you update the posterior distribution for the parent Q-value?

- Concept: Thompson sampling and probability matching
  - Why needed here: The algorithm selects actions during tree search according to their probability of being optimal, which is the Thompson sampling principle applied to branches.
  - Quick check question: In a 2-armed bandit where P(Q1 > Q2) = 0.7, what is the probability that Thompson sampling selects action 1?

- Concept: Information theory and mutual information
  - Why needed here: The regret analysis uses information-theoretic concepts like Shannon entropy and mutual information to bound the expected regret.
  - Quick check question: If the prior distribution over the optimal branch has low entropy, what does this imply about the difficulty of the planning problem?

## Architecture Onboarding

- Component map:
  Neural network module -> Tree search module -> Learning module -> Evaluation module

- Critical path:
  1. Neural network predicts Q-value distributions for root state
  2. Tree search explores using uncertainty-aware selection
  3. Action is committed based on search results
  4. Environment feedback updates neural network via self-play

- Design tradeoffs:
  - Independence assumption vs. correlated posteriors: Independence simplifies implementation but may be violated in practice
  - Forward sampling vs. exact posterior computation: Forward sampling is efficient but approximate
  - Deterministic vs. stochastic action commitment: Stochastic helps avoid getting stuck but adds randomness

- Failure signatures:
  - Poor performance despite accurate uncertainty: May indicate independence assumption violation or inappropriate exploration schedule
  - Tree search depth much shallower than expected: Could suggest incorrect max-backup implementation or posterior update issues
  - Neural network training diverges: Might indicate exploration-exploitation imbalance in self-play

- First 3 experiments:
  1. Implement and test max-backup on a simple 2-level tree with known posteriors
  2. Compare TSTS with standard MCTS on a small maze where ground truth values are computable
  3. Train neural network with uncertainty head and test uncertainty prediction accuracy on training data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the independence assumption for posterior distributions be relaxed to allow for dependent value estimates between different actions at the same state?
- Basis in paper: [explicit] The authors acknowledge that designing an efficient posterior update and sampling method for dependent posteriors is non-trivial and leave it as an open problem for future work.
- Why unresolved: The current method assumes independent posteriors for computational efficiency, but real-world problems may exhibit significant dependencies between value estimates.
- What evidence would resolve it: A concrete algorithm that efficiently handles dependent posteriors while maintaining reasonable computational complexity and demonstrating improved performance over the independent case.

### Open Question 2
- Question: How can the accuracy of learned uncertainty estimates be improved to consistently yield performance gains in planning?
- Basis in paper: [explicit] The authors found that popular uncertainty estimation methods were not accurate enough to translate to performance gains, suggesting more research is needed.
- Why unresolved: While the theoretical framework shows promise with ground-truth uncertainty, current methods for learning uncertainty from data are insufficient.
- What evidence would resolve it: A method that learns uncertainty estimates with significantly lower error rates, validated through planning tasks where the improved uncertainty estimates lead to consistent performance improvements.

### Open Question 3
- Question: Can the Bayesian regret bound be extended to account for the error in learned uncertainty estimates?
- Basis in paper: [explicit] The current regret bound assumes access to true posterior distributions, which are not available in practice when using learned uncertainty estimates.
- Why unresolved: The theoretical analysis assumes perfect knowledge of the posterior, but practical implementations rely on approximations.
- What evidence would resolve it: A modified regret bound that explicitly incorporates the error in uncertainty estimates and shows how this affects the overall performance guarantee.

## Limitations

- The theoretical regret bound relies on the independence assumption between posterior distributions at different states, which may not hold in practice.
- Neural network uncertainty estimates may be systematically inaccurate, leading to suboptimal tree search behavior.
- The generalization of results to complex, high-dimensional environments beyond ProcGen Maze and Leaper is uncertain.

## Confidence

- **High confidence**: The core algorithm mechanics (Thompson sampling for tree search, max-backup propagation) are well-defined and implementable.
- **Medium confidence**: The theoretical regret bound is valid under stated assumptions, but practical performance depends heavily on uncertainty estimation accuracy.
- **Low confidence**: The generalization of results to complex, high-dimensional environments beyond ProcGen Maze and Leaper.

## Next Checks

1. Implement a synthetic environment with known ground truth Q-values and test whether TSTS outperforms standard MCTS when accurate uncertainty estimates are provided.
2. Analyze the correlation structure of learned posteriors in practice to verify the independence assumption, measuring the impact of violations on regret bounds.
3. Conduct ablation studies varying the exploration schedule parameters (α0, β) in BTS to identify optimal settings for different environment types.