---
ver: rpa2
title: Interpretable Prediction and Feature Selection for Survival Analysis
arxiv_id: '2404.14689'
source_url: https://arxiv.org/abs/2404.14689
tags:
- survival
- feature
- features
- data
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DyS, an interpretable Generalized Additive
  Model (GAM) with interactions for survival analysis. The method uses a ranked probability
  score (RPS) loss function to directly optimize survival predictions, improving discrimination
  compared to Cox-based losses.
---

# Interpretable Prediction and Feature Selection for Survival Analysis

## Quick Facts
- arXiv ID: 2404.14689
- Source URL: https://arxiv.org/abs/2404.14689
- Authors: Mike Van Ness; Madeleine Udell
- Reference count: 40
- One-line primary result: DyS achieves competitive discrimination performance while providing exact feature selection and natural interpretability in survival analysis.

## Executive Summary
This paper introduces DyS, an interpretable Generalized Additive Model with interactions for survival analysis. The method uses a ranked probability score (RPS) loss function to directly optimize survival predictions, improving discrimination compared to Cox-based losses. DyS also supports feature sparsity by learning smooth-step functions for each feature and interaction, enabling joint feature selection and prediction. A two-stage training procedure further improves scalability to large datasets. On benchmark datasets, DyS achieves competitive or state-of-the-art discrimination performance while providing natural feature importance and feature impact plots without post-hoc approximation. On a large heart failure dataset, DyS performs within one standard deviation of the best black-box models while using far fewer features.

## Method Summary
DyS is a Generalized Additive Model with interactions (GA2M) that uses neural networks to parameterize shape functions for each main effect and interaction. The model employs smooth-step gating functions that can converge to exactly 0 or 1, enabling exact feature selection. Training uses a two-stage approach: first fitting main effects, then adding interactions only for features selected in the first stage. The model is trained using Ranked Probability Score (RPS) loss, which directly optimizes survival probability predictions rather than ranking risks. This approach handles non-proportional hazards better than traditional Cox-based losses while maintaining interpretability through exact feature selection and natural feature importance measures.

## Key Results
- DyS achieves competitive or state-of-the-art discrimination performance on benchmark survival analysis datasets.
- On a large heart failure dataset with 2,410 features and ~670,000 samples, DyS performs within one standard deviation of the best black-box models.
- DyS naturally provides feature importance and feature impact plots without post-hoc approximation.
- The two-stage training procedure enables scalability to large datasets with thousands of features.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The RPS loss directly optimizes survival predictions, improving discrimination over Cox loss.
- Mechanism: By minimizing squared errors in survival probability estimates rather than ranking risks, RPS handles non-proportional hazards better.
- Core assumption: The event times can be discretized without significant loss of information.
- Evidence anchors:
  - [abstract] "uses a ranked probability score (RPS) loss function to directly optimize survival predictions, improving discrimination compared to Cox-based losses."
  - [section 3.2] "We choose the Ranked Probability Score (RPS) loss function [...] Instead of Cox-style losses, the RPS loss directly optimizes survival predictions, the usual quantity of interest."
  - [corpus] Weak evidence; no corpus entries directly discuss RPS loss.
- Break condition: If the time-to-event distribution is highly skewed with many unique times, discretization may be too coarse.

### Mechanism 2
- Claim: Smooth-step functions enable exact feature sparsity in the model.
- Mechanism: Binary gates are replaced with smooth-step functions that can converge to exactly 0 or 1, allowing hard feature selection.
- Core assumption: The smooth-step function can be optimized to converge to 0 or 1 without getting stuck in intermediate values.
- Evidence anchors:
  - [section 3.3] "the smooth-step function is a continuous function with range [0 , 1], but unlike other such functions like the sigmoid function, the smooth-step function can actually reach 0, producing an exactly sparse model."
  - [section 3.3] "To induce feature sparsity, the smooth-step parameters µj, µj,ℓ are learned via the RPS loss LRPS along with the sparsity regularizer LSS(µ)."
  - [corpus] Weak evidence; no corpus entries directly discuss smooth-step functions.
- Break condition: If the entropy regularizer is too weak, smooth-step parameters may not converge to 0 or 1.

### Mechanism 3
- Claim: Two-stage fitting enables scalability to large datasets.
- Mechanism: By fitting main effects first and then interactions only for active features, the number of parameters to optimize is drastically reduced.
- Core assumption: The main effects capture most of the signal, so interactions between inactive features are less important.
- Evidence anchors:
  - [section 3.4] "Training GA 2Ms is computationally and memory intensive when the number of features in a dataset is large. [...] Instead, DyS takes advantage of the feature sparsity in the model."
  - [section 3.4] "We first fit only the main effects of the model. The fitting procedure naturally chooses an active subset of the main effects by learning the smooth-step function parameters."
  - [section 5.3] "As aforemenioned, we omit CoxPH and RSF as they are too slow, and also omit PseudoNAM as computing pseudo-values is similarly slow."
- Break condition: If important interactions exist between initially inactive features, two-stage fitting may miss them.

## Foundational Learning

- Concept: Survival analysis basics (censored vs uncensored data, survival function, hazard function)
  - Why needed here: The entire model is built for time-to-event prediction with censoring.
  - Quick check question: What is the difference between a censored and uncensored sample in survival analysis?
- Concept: Generalized Additive Models (GAMs) and their extension to GA2Ms
  - Why needed here: DyS is a GA2M model, so understanding the base structure is essential.
  - Quick check question: How does a GA2M differ from a standard GAM?
- Concept: Neural networks as shape functions
  - Why needed here: DyS uses MLPs to parameterize the shape functions in the GA2M.
  - Quick check question: Why might neural networks be preferred over splines for shape functions in this context?

## Architecture Onboarding

- Component map: Input -> Shape functions (MLP) -> Smooth-step gates -> Aggregation -> Softmax -> Survival probabilities
- Critical path: Input → shape functions → smooth-step gates → aggregation → softmax → survival predictions
- Design tradeoffs:
  - Using MLPs vs splines for shape functions: MLPs are more flexible but less interpretable for individual features
  - Discretizing time: Simplifies the problem but may lose granularity
  - Two-stage vs one-stage fitting: Two-stage is more scalable but may miss some interactions
- Failure signatures:
  - Poor discrimination: Could indicate issues with the RPS loss implementation or insufficient model capacity
  - No feature sparsity: May indicate the entropy regularizer is too weak or the smooth-step parameters are not converging
  - Slow training: Could be due to too many interactions being considered or inefficient implementation
- First 3 experiments:
  1. Train DyS on synthetic data that violates proportional hazards to verify RPS loss improves performance over Cox loss.
  2. Train DyS with and without feature sparsity on a small dataset to verify the smooth-step functions are working as intended.
  3. Train DyS with one-stage and two-stage fitting on a medium-sized dataset to verify the two-stage approach is faster with minimal performance loss.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ranked probability score (RPS) loss function perform compared to the Cox proportional hazards loss on datasets that satisfy the proportional hazards assumption?
- Basis in paper: [explicit] The authors compare RPS loss to Cox loss on synthetic data that violates proportional hazards assumption, showing RPS performs better. They suggest RPS could be better even when proportional hazards holds.
- Why unresolved: The paper only tests on synthetic data violating proportional hazards. No empirical comparison is provided on real datasets where proportional hazards holds.
- What evidence would resolve it: Direct comparison of RPS vs Cox loss on benchmark datasets known to satisfy proportional hazards assumption, measuring discrimination performance and calibration.

### Open Question 2
- Question: What is the optimal number of features to select when using DyS for feature selection, and how does this vary across different types of survival analysis problems?
- Basis in paper: [explicit] The authors demonstrate feature selection capability but use heuristic approaches (selecting ~50 features or using bisection to find exactly 10 features). They don't systematically explore optimal feature counts.
- Why unresolved: The paper shows feature selection works but doesn't provide guidance on determining optimal feature counts for different problem domains or dataset characteristics.
- What evidence would resolve it: Systematic experiments varying feature selection budgets across multiple datasets, analyzing performance trade-offs between model complexity and discrimination accuracy.

### Open Question 3
- Question: How does the two-stage fitting approach compare to joint fitting in terms of computational efficiency and performance for very large survival datasets with millions of samples?
- Basis in paper: [inferred] The authors propose two-stage fitting to handle large datasets and show it performs nearly as well as joint fitting on smaller datasets. However, they don't test on truly massive datasets.
- Why unresolved: The computational benefits are theoretically sound but not empirically validated on datasets with millions of samples and thousands of features.
- What evidence would resolve it: Performance and timing comparisons of one-stage vs two-stage fitting on datasets with n > 1,000,000 and p > 5,000, measuring both training time and prediction accuracy.

## Limitations
- Discretization of time may lose granularity in temporal patterns, especially for datasets with many unique event times.
- The paper lacks ablation studies to isolate the contributions of RPS loss, smooth-step gating, and two-stage fitting.
- Potential overfitting concerns given the high model complexity when many interactions are included.

## Confidence
- High confidence: RPS loss directly optimizing survival predictions is clearly specified with mathematical formulation.
- Medium confidence: Two-stage fitting approach is described but exact implementation details for scaling to 2,410 features are not fully specified.
- Low confidence: Interpretation of heart failure dataset results, as standard deviations are not shown and model tuning details are missing.

## Next Checks
1. **Verify discretization impact**: Train DyS with different numbers of time bins on synthetic data to assess how discretization granularity affects both discrimination performance and interpretability.
2. **Ablation of key components**: Implement DyS variants without RPS loss (using Cox loss), without smooth-step gating (using standard sigmoid gates), and without two-stage fitting to quantify each component's contribution to performance.
3. **Standard deviation analysis**: Re-run experiments on the heart failure dataset with multiple random seeds and report standard deviations for all metrics to validate the claim of being "within one standard deviation" of black-box models.