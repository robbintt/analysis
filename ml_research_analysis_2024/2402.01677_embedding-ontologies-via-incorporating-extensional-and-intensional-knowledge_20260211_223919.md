---
ver: rpa2
title: Embedding Ontologies via Incorporating Extensional and Intensional Knowledge
arxiv_id: '2402.01677'
source_url: https://arxiv.org/abs/2402.01677
tags:
- knowledge
- intensional
- extensional
- embedding
- triple
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes EIKE, a novel ontology embedding approach that
  models both extensional knowledge (instances and their types) and intensional knowledge
  (concept properties and semantic relationships) in separate spaces. EIKE uses geometric
  regions to represent concepts and point vectors for instances in the extensional
  space, while employing a pre-trained language model to encode intensional knowledge
  in the intensional space.
---

# Embedding Ontologies via Incorporating Extensional and Intensional Knowledge

## Quick Facts
- arXiv ID: 2402.01677
- Source URL: https://arxiv.org/abs/2402.01677
- Reference count: 40
- Key outcome: EIKE significantly outperforms state-of-the-art methods in triple classification and link prediction by modeling both extensional and intensional knowledge in separate spaces.

## Executive Summary
This paper introduces EIKE, a novel ontology embedding approach that addresses the limitations of existing methods by modeling both extensional knowledge (instances and their types) and intensional knowledge (concept properties and semantic relationships) in separate spaces. EIKE uses geometric regions to represent concepts and point vectors for instances in the extensional space, while employing a pre-trained language model to encode intensional knowledge in the intensional space. The approach combines structure-based and text-based embeddings to comprehensively represent ontologies, achieving significant performance improvements over state-of-the-art methods on triple classification and link prediction tasks.

## Method Summary
EIKE embeds ontologies by learning two distinct spaces: an extensional space that models instances and their types using geometric regions (ellipsoids for concepts, point vectors for instances), and an intensional space that captures semantic properties and relationships using pre-trained language model embeddings. The method jointly trains on three types of triples (InstanceOf, SubClassOf, and Relational) using separate loss functions for each type, with a weighting parameter α to balance the contributions from both spaces. This dual-space approach allows EIKE to preserve both extensional semantics (instance-concept relationships) and intensional semantics (concept properties and semantic associations) simultaneously.

## Key Results
- EIKE achieves 2-5% accuracy improvements in triple classification for InstanceOf and SubClassOf triples compared to state-of-the-art methods
- Link prediction performance shows Hit@10 improvements of 2-15% on YAGO39K, M-YAGO39K, and DB99K-242 datasets
- The pre-trained language model component contributes significantly to performance, with the PRE variant outperforming the UNP variant across all tasks
- EIKE demonstrates superior performance on simple ontologies while maintaining competitive results on more complex datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Dual-space representation allows separate modeling of extensional (instance-level) and intensional (concept-level) knowledge, preventing information interference.
- Mechanism: Extensional space uses geometric regions (ellipsoids) to model concepts and point vectors for instances, while intensional space uses pre-trained language model embeddings to capture semantic properties and relationships.
- Core assumption: Extensional and intensional knowledge have fundamentally different characteristics that benefit from distinct modeling approaches.
- Evidence anchors:
  - [abstract] "EIKE presents a unified framework for embedding instances, concepts and their relations in an ontology, applying a geometry-based method to model extensional knowledge and a pretrained language model to model intensional knowledge"
  - [section 4.1] "In this work, we follow [43] to represent concept as ellipsoid and instance as point vector in the extensional space"
  - [corpus] Weak evidence - corpus neighbors don't directly address dual-space methodology
- Break condition: When the distinction between extensional and intensional knowledge becomes blurred or when one type dominates the other significantly.

### Mechanism 2
- Claim: Pre-trained language models effectively capture intensional knowledge by encoding concept properties and semantic relationships.
- Mechanism: Concept names and descriptions are fed into Sentence-BERT to generate embeddings that represent inherent properties and associations between concepts.
- Core assumption: Textual descriptions of concepts contain sufficient information to represent their intensional properties and relationships.
- Evidence anchors:
  - [section 4.1.2] "We input the name of each concept or concatenation of name and description of the concept exists to a pre-trained language model to encode its properties and characteristics"
  - [abstract] "applying a pre-trained language model to encode intensional knowledge from properties and characteristics of the concepts"
  - [corpus] Weak evidence - corpus neighbors don't provide direct evidence for PLM effectiveness in ontology embedding
- Break condition: When concepts lack textual descriptions or when the language model fails to capture relevant semantic relationships.

### Mechanism 3
- Claim: Joint training across both spaces with separate loss functions for each triple type preserves both extensional and intensional semantics.
- Mechanism: Distinct loss functions for InstanceOf, SubClassOf, and Relational triples are combined with weighting parameter α to balance learning across spaces.
- Core assumption: Different types of triples require different optimization objectives to properly capture their semantic relationships.
- Evidence anchors:
  - [section 4.3] "Finally, we define the overall loss function as linear combinations of these three functions: L = Lsub + Lins + Lrel"
  - [abstract] "distinct loss functions are proposed for SubClassOf, InstanceOf, and Relational Triples, culminating in joint training"
  - [section 4.2] "For a relational triple (h, r, t), we utilize the widely used TransE [2] model to calculate the loss function"
- Break condition: When one space's loss dominates training or when the weighting parameter α cannot effectively balance the objectives.

## Foundational Learning

- Concept: Description Logic (DL) and its variants (EL++, DL-Lite)
  - Why needed here: The ontology representation follows specific DL constraints, and understanding these logics is crucial for interpreting the triple structure and relations
  - Quick check question: What is the difference between EL and DL-Lite ontologies in terms of expressivity?

- Concept: Geometric embeddings (ellipsoids, point vectors)
  - Why needed here: The extensional space representation relies on geometric concepts to model concepts as regions and instances as points
  - Quick check question: How does representing concepts as ellipsoids differ from using simple point vectors?

- Concept: Pre-trained language models for semantic encoding
  - Why needed here: The intensional space uses PLMs to capture semantic properties and relationships from textual descriptions
  - Quick check question: Why might Sentence-BERT be preferred over other PLMs for encoding concept descriptions?

## Architecture Onboarding

- Component map:
  Input layer -> Extensional space (geometric modeling) -> Intensional space (PLM encoding) -> Loss functions -> Joint training

- Critical path:
  1. Initialize embeddings (geometric for extensional, PLM for intensional)
  2. Compute scores for all triple types using appropriate functions
  3. Calculate individual losses (Lsub, Lins, Lrel)
  4. Combine losses with weighting α
  5. Backpropagate through both spaces simultaneously

- Design tradeoffs:
  - Two spaces vs. single unified space: Better separation of concerns vs. increased complexity
  - PLM vs. random initialization: Better semantic capture vs. computational overhead
  - Separate vs. shared instance embeddings: Clearer distinction vs. parameter efficiency

- Failure signatures:
  - One space's loss dominates training: Imbalanced learning objectives
  - Poor link prediction performance: Insufficient relational modeling
  - Random initialization performs similarly to PLM: Textual information not contributing

- First 3 experiments:
  1. Compare EIKE with and without PLM (UNP vs PRE variants) on YAGO39K
  2. Test different mapping matrices (EYE vs MAT) for virtual instance embeddings
  3. Evaluate the effect of varying α parameter on triple classification performance

## Open Questions the Paper Calls Out

The paper acknowledges that its focus on simple ontologies with only SubClassOf and InstanceOf relations represents a limitation, noting that more complex logical constructs like disjointness, property restrictions, and cardinality constraints are not addressed by the current framework.

## Limitations

- The evaluation relies on relatively small ontology datasets (YAGO39K: 39K triples, DB99K-242: 99K triples), raising questions about scalability to real-world ontologies with millions of triples.
- The paper does not report computational complexity or training times, which could be substantial given the dual-space approach and pre-trained language model usage.
- Performance improvements over baseline methods show diminishing returns on larger datasets, suggesting potential saturation effects.

## Confidence

- High confidence: The dual-space methodology for separating extensional and intensional knowledge is technically sound and well-supported by the theoretical framework and experimental design.
- Medium confidence: The effectiveness of the pre-trained language model for capturing intensional knowledge is supported by results but could be further validated through ablation studies comparing different PLM architectures.
- Medium confidence: The scalability claims are supported by experiments on multiple dataset sizes but lack explicit analysis of computational complexity or training time.

## Next Checks

1. Conduct scalability experiments on larger ontologies (e.g., DBpedia 2016-10 or custom datasets with >1M triples) to verify performance scaling and identify potential bottlenecks.
2. Perform ablation studies comparing EIKE's performance using different PLM architectures (e.g., Sentence-BERT vs. RoBERTa vs. BERT) to isolate the contribution of the language model component.
3. Analyze training time and memory usage across different dataset sizes to establish practical computational requirements and identify optimization opportunities for real-world deployment.