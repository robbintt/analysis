---
ver: rpa2
title: Bottleneck-Minimal Indexing for Generative Document Retrieval
arxiv_id: '2405.10974'
source_url: https://arxiv.org/abs/2405.10974
tags:
- document
- indexing
- information
- queries
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a bottleneck-minimal indexing method for generative
  document retrieval (GDR), which addresses the limitation of previous indexing methods
  that only consider document distribution while ignoring query distribution. The
  authors apply the information bottleneck theory to analyze GDR and show that the
  optimal indexing is determined by both document and query distributions.
---

# Bottleneck-Minimal Indexing for Generative Document Retrieval

## Quick Facts
- arXiv ID: 2405.10974
- Source URL: https://arxiv.org/abs/2405.10974
- Authors: Xin Du; Lixin Xiu; Kumiko Tanaka-Ishii
- Reference count: 40
- This paper proposes a bottleneck-minimal indexing method for generative document retrieval (GDR), which addresses the limitation of previous indexing methods that only consider document distribution while ignoring query distribution.

## Executive Summary
This paper addresses a fundamental limitation in generative document retrieval (GDR) by proposing a bottleneck-minimal indexing method. The key insight is that existing indexing approaches only consider document distributions when selecting representative document vectors, while ignoring the query distribution. The authors apply information bottleneck theory to show that optimal indexing requires considering both document and query distributions. Their proposed method applies hierarchical k-means clustering to query mean vectors instead of document vectors, resulting in up to 7.06 points improvement in recall@1 score on standard benchmarks.

## Method Summary
The proposed bottleneck-minimal indexing method fundamentally rethinks how document vectors are selected for GDR systems. Instead of the traditional approach of clustering document vectors to select representative points, this method applies hierarchical k-means clustering directly to query mean vectors. The authors argue that since GDR systems generate document identifiers conditioned on both queries and documents, the indexing should be optimized for the joint distribution rather than just the document distribution. This approach allows the system to better handle the actual query-document matching patterns that occur during retrieval, resulting in more effective index selection and improved retrieval performance, particularly for smaller models.

## Key Results
- The proposed method achieves up to 7.06 points improvement in recall@1 score compared to existing methods
- Outperforms existing methods especially with smaller models
- Achieves competitive performance with state-of-the-art methods using textual ID strings and sophisticated index-learning strategies on NQ320K and MARCO Lite datasets

## Why This Works (Mechanism)
The proposed method works by addressing a fundamental mismatch in how traditional GDR indexing approaches the problem. Information bottleneck theory reveals that optimal indexing should be determined by both document and query distributions, not just document distribution as previous methods assume. By clustering query mean vectors instead of document vectors, the method creates an index that better reflects the actual patterns of query-document interactions during retrieval. This query-aware indexing allows the generative model to more effectively identify relevant documents because the index structure aligns with how queries actually interact with the document collection.

## Foundational Learning

**Information Bottleneck Theory**: A framework for finding optimal representations that preserve relevant information while compressing irrelevant details. Needed to formally analyze the GDR indexing problem and identify why existing methods are suboptimal. Quick check: verify that the mutual information terms in the theoretical analysis correctly capture the indexing objective.

**Generative Document Retrieval**: A retrieval paradigm where the model generates document identifiers directly rather than ranking documents. Needed to understand the specific challenges in indexing for GDR versus traditional retrieval. Quick check: confirm that the document representation and generation process matches the standard GDR formulation.

**Hierarchical K-means Clustering**: An approach that builds cluster hierarchies by recursively applying k-means clustering. Needed to efficiently create a multi-level index structure that can handle large document collections. Quick check: verify that the clustering depth and branching factor are appropriate for the dataset size.

## Architecture Onboarding

**Component Map**: Raw documents -> Document encoder -> Document vectors -> Query mean vectors -> Hierarchical k-means clustering -> Bottleneck-minimal index -> GDR model

**Critical Path**: The most important components are the query mean vector extraction, hierarchical k-means clustering, and the integration of the resulting index with the GDR model. The bottleneck occurs at the indexing stage where the choice of clustering target (queries vs documents) determines retrieval effectiveness.

**Design Tradeoffs**: The main tradeoff is between index size/complexity and retrieval accuracy. Clustering query vectors may create more complex index structures but provides better alignment with actual retrieval patterns. The hierarchical approach allows for efficient search while maintaining good coverage.

**Failure Signatures**: Poor clustering of query vectors would lead to suboptimal index selection. If the query distribution is not well-represented in the training data, the index may not generalize well to unseen queries. The method may struggle if query-document relationships are highly diverse or if the document collection changes significantly over time.

**First Experiments**:
1. Compare retrieval performance using document-vector clustering versus query-vector clustering on a small dataset
2. Test different numbers of clusters in the hierarchical k-means to find the optimal balance between index size and accuracy
3. Evaluate the method's sensitivity to query distribution shifts by testing on held-out query types

## Open Questions the Paper Calls Out
None

## Limitations
- The analysis relies heavily on theoretical arguments about information bottlenecks without extensive empirical validation across diverse retrieval scenarios
- Evaluation focuses primarily on recall@1 improvements, which may not fully capture end-to-end retrieval effectiveness including precision and user satisfaction
- Computational overhead of hierarchical k-means clustering on query mean vectors is not thoroughly analyzed, leaving questions about scalability for very large document collections

## Confidence

High: The observation that existing indexing methods ignore query distribution is clearly supported by the literature review and theoretical analysis.

Medium: The theoretical analysis of optimal indexing through information bottleneck principles provides a solid foundation but requires more empirical validation across different scenarios.

Medium: The experimental results showing performance improvements are promising but are limited to specific datasets and evaluation metrics, requiring broader validation.

## Next Checks

1. Conduct ablation studies isolating the contribution of query-aware indexing versus other components like the textual ID strings and index-learning strategies to quantify the specific impact of the proposed indexing approach.

2. Evaluate the method on additional retrieval benchmarks beyond NQ320K and MARCO Lite to test generalizability across different domains, query types, and document collections.

3. Perform a detailed computational complexity analysis comparing the proposed hierarchical k-means approach against existing indexing methods to quantify practical scalability constraints and identify potential bottlenecks for large-scale deployment.