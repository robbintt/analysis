---
ver: rpa2
title: Exploring Complex Mental Health Symptoms via Classifying Social Media Data
  with Explainable LLMs
arxiv_id: '2412.10414'
source_url: https://arxiv.org/abs/2412.10414
tags:
- lyme
- health
- mental
- adhd
- anxiety
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a pipeline for gaining insights into complex
  diseases by training LLMs on challenging social media text classification tasks,
  obtaining explanations for the classification outputs, and performing qualitative
  and quantitative analysis on the explanations. The authors report initial results
  on predicting, explaining, and systematizing the explanations of predicted reports
  on mental health concerns in people reporting Lyme disease concerns.
---

# Exploring Complex Mental Health Symptoms via Classifying Social Media Data with Explainable LLMs

## Quick Facts
- **arXiv ID**: 2412.10414
- **Source URL**: https://arxiv.org/abs/2412.10414
- **Reference count**: 7
- **Primary result**: Achieved F1 scores of 0.73 for classifying Lyme-related posts as mental-health-related and 0.89 for predicting future ADHD posting behavior in anxiety users

## Executive Summary
This paper presents a pipeline for gaining insights into complex diseases by training LLMs on challenging social media text classification tasks, obtaining explanations for classification outputs, and performing qualitative and quantitative analysis on the explanations. The authors report initial results on predicting, explaining, and systematizing explanations of predicted reports on mental health concerns in people reporting Lyme disease concerns, as well as predicting future ADHD concerns for people reporting anxiety disorder concerns. The approach demonstrates how explainable LLMs can reveal patterns in social media discussions about complex medical conditions.

## Method Summary
The pipeline involves collecting Reddit posts from relevant subreddits, manually labeling a subset of posts for mental health relevance, fine-tuning RoBERTa models for classification tasks, generating explanations through phrase masking, and analyzing the explanations to identify patterns. For the Lyme disease task, posts containing "Lyme" were classified as mental-health-related or not, achieving an F1 score of 0.73. For the ADHD/anxiety task, users who posted in r/anxiety but not r/ADHD were classified as likely to post in r/ADHD in the future, achieving an F1 score of 0.89. Explanations were generated by masking individual phrases and measuring their impact on classification, with results compared to a reference AskDocs dataset to identify disease-specific patterns.

## Key Results
- Achieved F1 score of 0.73 for classifying Lyme-related posts as mental-health-related or not
- Achieved F1 score of 0.89 for predicting future ADHD posting behavior in anxiety users
- Identified mold-related concerns as more prevalent in the Lyme dataset compared to the AskDocs reference dataset

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The explainable RoBERTa model can identify mental health symptoms in social media posts about Lyme disease by learning from manually labeled examples.
- Mechanism: The model is fine-tuned on a small labeled dataset where posts are categorized as mental-health-related or not. The explainability method (phrase masking) highlights which phrases contribute most to the classification, revealing symptom descriptions and concerns.
- Core assumption: Social media posts contain sufficient linguistic cues about mental health symptoms that can be learned by a transformer-based classifier.
- Evidence anchors:
  - [abstract] "We classify Lyme disease-related posts as either mental-health-related or not using RoBERTa models"
  - [section] "We train a RoBERTa classifier to predict whether a post is mental-health-related or not, and classify 58,398 posts containing the keyword 'Lyme' as mental-health-related or not"
- Break condition: If the linguistic cues are too subtle or varied across users, the model may not generalize beyond the manually labeled examples.

### Mechanism 2
- Claim: The explainability method (phrase masking) effectively identifies symptom descriptions that are unique to Lyme disease discussions.
- Mechanism: By masking individual phrases and measuring the impact on classification, the method highlights symptom descriptions that are most indicative of mental health concerns. These highlighted phrases can then be analyzed for patterns specific to Lyme disease.
- Core assumption: The masked phrase method reliably captures the most relevant symptom descriptions for classification.
- Evidence anchors:
  - [section] "each post is broken up into phrases... Each phrase in turn is masked out, and the phrases that influence the classifier output the most are highlighted"
  - [section] "we demonstrate preliminary results showing a proof-of-concept of a pipeline where the explanations of the classifier are used in order to identify interesting patterns"
- Break condition: If the phrase boundaries don't align well with symptom descriptions, important context may be lost during masking.

### Mechanism 3
- Claim: Comparing highlighted symptoms from Lyme posts to a reference medical dataset (AskDocs) reveals disease-specific patterns.
- Mechanism: The top 300 most similar posts in both the Lyme dataset and reference dataset are compared to identify which symptom descriptions are more prevalent in Lyme discussions, revealing potential disease-specific patterns.
- Core assumption: A general medical discussion dataset serves as an appropriate reference for identifying Lyme-specific patterns.
- Evidence anchors:
  - [section] "Look through the top 300 matches for each group of explanations in the reference r/AskDocs corpus"
  - [section] "we identify that the mold theme appears more prevalent in the Lyme dataset than in the baseline AskDocs dataset"
- Break condition: If the reference dataset doesn't adequately represent general medical discussions, the comparison may not reveal meaningful patterns.

## Foundational Learning

- Concept: Understanding of transformer-based language models and their fine-tuning process
  - Why needed here: The entire pipeline relies on fine-tuning RoBERTa for a specific classification task and understanding how transformer attention mechanisms work for explainability
  - Quick check question: How does RoBERTa differ from BERT, and why might it be better suited for this task?

- Concept: Explainability techniques for NLP models
  - Why needed here: The phrase masking method is central to the pipeline's ability to generate insights from the classification model
  - Quick check question: What are the advantages and limitations of using phrase masking compared to other explainability methods like attention visualization?

- Concept: Statistical significance testing
  - Why needed here: The analysis compares symptom prevalence between datasets, requiring understanding of how to test for significant differences
  - Quick check question: What statistical test would be appropriate for comparing the prevalence of symptom descriptions between two datasets?

## Architecture Onboarding

- Component map: Data collection -> Manual labeling -> Model fine-tuning -> Explanation generation -> Pattern analysis -> Comparison with reference dataset
- Critical path: Manual labeling -> Model fine-tuning -> Explanation generation -> Pattern analysis
- Design tradeoffs: Using a small manually labeled dataset limits generalizability but enables precise control over labels; phrase masking is interpretable but may miss contextual information
- Failure signatures: Low F1 scores indicate poor model performance; inconsistent patterns in explanations suggest model confusion or data quality issues
- First 3 experiments:
  1. Fine-tune RoBERTa on the manually labeled Lyme dataset and evaluate performance metrics
  2. Generate explanations for a subset of correctly classified posts to validate the explainability method
  3. Compare symptom descriptions between the Lyme dataset and AskDocs reference to identify preliminary patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the prevalence of mold-related concerns in the Lyme dataset compare to other medical conditions or the general population?
- Basis in paper: [explicit] The authors found a higher prevalence of mold-related concerns in the Lyme dataset compared to the AskDocs reference dataset.
- Why unresolved: The comparison is only made with one reference dataset, and no statistical analysis is provided to determine if the difference is significant compared to the general population or other medical conditions.
- What evidence would resolve it: A comprehensive study comparing the prevalence of mold-related concerns across various medical conditions and the general population, along with statistical analysis to determine significance.

### Open Question 2
- Question: What are the specific mental health symptoms associated with Lyme disease, and how do they differ from other medical conditions?
- Basis in paper: [explicit] The authors mention that Lyme disease is known for its significant neurological and psychological impacts, but they do not provide a detailed list of specific mental health symptoms.
- Why unresolved: The paper only mentions that Lyme disease can lead to mental health disorders but does not delve into the specific symptoms or how they differ from other medical conditions.
- What evidence would resolve it: A comprehensive study identifying and categorizing the specific mental health symptoms associated with Lyme disease, along with a comparison to other medical conditions.

### Open Question 3
- Question: How effective is the RoBERTa classifier in identifying mental health-related posts in the Lyme dataset, and how does it compare to other classification methods?
- Basis in paper: [explicit] The authors report an F1 score of 0.73 for the RoBERTa classifier in identifying mental health-related posts in the Lyme dataset, but they do not compare it to other classification methods.
- Why unresolved: The paper only presents results for the RoBERTa classifier and does not compare its performance to other classification methods or provide information on its effectiveness in identifying mental health-related posts.
- What evidence would resolve it: A comparative study evaluating the performance of the RoBERTa classifier against other classification methods in identifying mental health-related posts in the Lyme dataset, along with an analysis of its effectiveness in different scenarios.

## Limitations
- Small sample size for manual labeling (343 posts) may limit generalizability
- Phrase masking explainability method may miss contextual information spanning phrase boundaries
- Reference dataset choice (AskDocs) may not adequately represent general medical discussions

## Confidence
- Classification performance metrics (F1 scores of 0.73 and 0.89): **Medium**
- Qualitative pattern identification results: **Low**

## Next Checks
1. **Replicate classification results** with a larger manually labeled dataset to assess whether the F1 scores remain stable with increased sample size.
2. **Validate explanation method** by conducting a human evaluation where multiple annotators independently assess whether the masked phrases accurately capture symptom descriptions.
3. **Test reference dataset sensitivity** by comparing results using different baseline datasets (e.g., general medical forums vs. symptom-specific communities) to determine if the identified patterns are robust to the choice of reference.