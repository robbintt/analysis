---
ver: rpa2
title: Construction and Evaluation of Mandarin Multimodal Emotional Speech Database
arxiv_id: '2401.07336'
source_url: https://arxiv.org/abs/2401.07336
tags:
- data
- emotion
- speech
- emotional
- database
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: A multi-modal Mandarin emotional speech database including articulatory
  kinematics, acoustics, glottal and facial micro-expressions was designed and established.
  Subjects expressed seven emotions (neutral, happy, pleasant, indifferent, angry,
  sad, grief) while being recorded with an electromagnetic articulograph (EMA-AG501),
  electronic glottograph (EGG-D100), and high-resolution video camera.
---

# Construction and Evaluation of Mandarin Multimodal Emotional Speech Database

## Quick Facts
- arXiv ID: 2401.07336
- Source URL: https://arxiv.org/abs/2401.07336
- Reference count: 32
- A multimodal Mandarin emotional speech database with articulatory, acoustic, glottal, and facial data across seven emotions

## Executive Summary
This paper presents the creation and evaluation of a comprehensive Mandarin multimodal emotional speech database. The database captures seven emotional states (neutral, happy, pleasant, indifferent, angry, sad, grief) using simultaneous recordings from electromagnetic articulography, electronic glottography, and high-resolution video. The dataset comprises 2,943 samples with acoustic, kinematic, and glottal data plus 24.5 hours of video, all annotated with both discrete emotion labels and dimensional (PAD) labels. The construction process included dimensional annotation validation, outlier filtering, and correlation analysis with psychological symptom scores, followed by emotion recognition experiments across multiple machine learning models.

## Method Summary
The database was constructed by recording subjects expressing seven emotions while being captured with an electromagnetic articulograph (EMA-AG501), electronic glottograph (EGG-D100), and high-resolution video camera. Dimensional (PAD) annotations were validated through statistical analysis of intra-annotator dispersion and spatial clustering, with outliers identified using box-plot thresholds. Recognition experiments employed SVM, CNN, and DNN models to evaluate the discriminability of acoustic, glottal, and kinematic modalities separately.

## Key Results
- Recognition accuracies achieved: 82% (acoustic), 72% (glottal), 55.7% (kinematic) across seven emotions
- Database includes 2,943 samples of multimodal emotional speech data plus 24.5 hours of video
- Multiple linear regression linked outliers to SCL-90 psychological symptom scores (somatization, interpersonal sensitivity, depression)

## Why This Works (Mechanism)
The multimodal approach captures complementary emotional expression channels - articulatory kinematics reveal speech production patterns, glottal data captures voice source characteristics, and facial expressions provide visual emotional cues. Dimensional annotation validation through statistical dispersion analysis ensures labeling consistency, while outlier filtering based on psychological correlations helps maintain dataset quality. The combination of machine learning models (SVM, CNN, DNN) provides complementary strengths for different data modalities, with acoustic features showing highest discriminability due to their rich emotional information content.

## Foundational Learning
- Electromagnetic articulography (EMA): Measures tongue and lip movements during speech; needed for articulatory kinematic data; quick check: verify spatial accuracy of sensor placement
- Electronic glottography (EGG): Records vocal fold contact patterns; needed for glottal source analysis; quick check: confirm signal-to-noise ratio meets threshold
- Dimensional emotion models (PAD): Pleasure, Arousal, Dominance framework; needed for continuous emotion representation; quick check: validate annotation consistency across raters
- SVM/CNN/DNN architectures: Different ML approaches for pattern recognition; needed to leverage modality-specific advantages; quick check: compare training convergence rates
- Outlier detection using box-plots: Statistical method for identifying anomalous samples; needed for quality control; quick check: verify outlier criteria sensitivity

## Architecture Onboarding

**Component map:** Speech production -> EMA (kinematic) + EGG (glottal) + Video (facial) -> Feature extraction -> ML models (SVM/CNN/DNN) -> Emotion classification

**Critical path:** Emotion expression → synchronized multimodal recording → feature extraction → model training → recognition evaluation

**Design tradeoffs:** Multimodal capture provides comprehensive emotional information but increases complexity and cost; dimensional vs. discrete labeling balances granularity with practical usability; strict outlier filtering improves dataset consistency but may reduce natural variation

**Failure signatures:** Poor recognition accuracy in specific modalities indicates data quality issues or model mismatch; inconsistent dimensional annotations suggest labeling ambiguity; outlier correlation with psychological symptoms may indicate recording protocol problems

**First experiments:** 1) Test feature extraction pipelines on single samples to verify data integrity; 2) Train simple baseline models before complex architectures; 3) Validate dimensional annotation consistency with inter-rater reliability tests

## Open Questions the Paper Calls Out
None

## Limitations
- Recognition rates validated only on the constructed database, not tested for cross-corpus generalization
- Outlier filtering based on box-plot thresholds may remove physiologically meaningful atypical expressions
- Correlation between outliers and psychological scores reported without detailed statistical significance or effect size metrics

## Confidence

| Major Claim | Confidence |
|-------------|------------|
| Database completeness and multimodal integration | High |
| Recognition performance benchmarks | Medium |
| Psychological outlier correlation | Low |

## Next Checks
1. Evaluate recognition models trained on this database against external Mandarin emotional speech datasets to assess cross-corpus generalization
2. Conduct inter-rater reliability analysis for the dimensional (PAD) labeling across multiple annotators to confirm consistency
3. Perform ablation studies to determine the impact of outlier filtering on both the dataset's representativeness and model robustness