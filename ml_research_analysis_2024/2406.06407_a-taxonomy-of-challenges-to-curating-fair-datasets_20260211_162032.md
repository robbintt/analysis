---
ver: rpa2
title: A Taxonomy of Challenges to Curating Fair Datasets
arxiv_id: '2406.06407'
source_url: https://arxiv.org/abs/2406.06407
tags:
- data
- dataset
- fairness
- datasets
- fair
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a taxonomy of challenges dataset curators face
  when creating fair machine learning datasets, identified through interviews with
  30 practitioners. The authors categorize challenges across five phases of the dataset
  lifecycle (requirements, design, implementation, evaluation, maintenance) and five
  levels of the broader fairness landscape (individual, discipline, organization,
  regulatory, socio-political).
---

# A Taxonomy of Challenges to Curating Fair Datasets

## Quick Facts
- arXiv ID: 2406.06407
- Source URL: https://arxiv.org/abs/2406.06407
- Reference count: 40
- One-line primary result: A taxonomy of challenges dataset curators face when creating fair machine learning datasets, identified through interviews with 30 practitioners

## Executive Summary
This paper presents a taxonomy of challenges dataset curators face when creating fair machine learning datasets, identified through interviews with 30 practitioners. The authors categorize challenges across five phases of the dataset lifecycle (requirements, design, implementation, evaluation, maintenance) and five levels of the broader fairness landscape (individual, discipline, organization, regulatory, socio-political). Key findings include difficulties balancing dataset scope with fairness goals, challenges in creating taxonomies that reflect fairness definitions, limited availability of diverse data, and systemic issues around recognition and incentives for fair dataset work. The paper provides recommendations for enabling fair dataset curation through systemic changes at multiple levels of the ML ecosystem.

## Method Summary
The study uses semi-structured interviews with 30 ML dataset curators to identify challenges in creating fair datasets. Researchers conducted thematic analysis of interview transcripts to develop a taxonomy categorizing challenges across dataset lifecycle phases and broader fairness landscape levels. The methodology focuses on capturing practitioners' lived experiences rather than theoretical concerns, enabling discovery of practical, context-specific challenges that literature reviews might miss.

## Key Results
- Challenges identified across all five dataset lifecycle phases: requirements, design, implementation, evaluation, and maintenance
- Three dimensions of fairness identified: composition (dataset diversity), process (labor equity, recognition), and release (transparency, accessibility)
- Systemic issues at organizational and regulatory levels create barriers to fair dataset curation
- Trade-offs between dataset utility and fairness goals emerge as a central challenge

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Taxonomy-driven challenge mapping enables systematic identification of dataset curation barriers across lifecycle phases.
- **Mechanism**: The paper structures challenges into lifecycle phases (requirements, design, implementation, evaluation, maintenance) and broader fairness landscape levels (individual, discipline, organization, regulatory, socio-political). This dual-axis taxonomy surfaces where trade-offs and constraints occur.
- **Core assumption**: Lifecycle and landscape categorizations are both exhaustive and mutually informative; challenges map cleanly to one phase and one landscape level.
- **Evidence anchors**:
  - [abstract] presents a "comprehensive taxonomy of challenges and trade-offs encountered throughout the dataset curation lifecycle."
  - [section] maps each challenge to phases and subphases, explicitly showing lifecycle coverage.
  - [corpus] does not directly support this; evidence is internal to the paper.
- **Break Condition**: If a challenge spans multiple phases or landscape levels in a way that cannot be meaningfully categorized, the taxonomy loses utility.

### Mechanism 2
- **Claim**: Multi-dimensional fairness definitions (composition, process, release) expose trade-offs that single-axis definitions obscure.
- **Mechanism**: By allowing participants to define fairness themselves and then categorizing into composition (dataset diversity), process (labor equity, recognition), and release (transparency, accessibility), the study captures nuanced trade-offs that single definitions would miss.
- **Core assumption**: These three dimensions are both necessary and sufficient to capture the practical fairness concerns curators face.
- **Evidence anchors**:
  - [abstract] notes "three dimensions of fairness—composition, process, and release—that participants considered during curation."
  - [section] provides examples where participants navigated trade-offs between dimensions (e.g., balancing diversity with utility, fair labor vs. resource constraints).
  - [corpus] does not provide external validation.
- **Break Condition**: If a fairness concern cannot be mapped to any of these three dimensions, the framework is incomplete.

### Mechanism 3
- **Claim**: Empirical interview methodology surfaces tacit challenges that literature reviews miss.
- **Mechanism**: The study uses semi-structured interviews with 30 dataset curators, enabling discovery of practical, lived experiences rather than theoretical concerns. This approach surfaces trade-offs and contextual factors that prior guideline-focused work overlooks.
- **Core assumption**: Interview-based qualitative methods can capture challenges that participants themselves may not have explicitly articulated before.
- **Evidence anchors**:
  - [abstract] states "Drawing from interviews with 30 ML dataset curators, we present a comprehensive taxonomy of the challenges and trade-offs encountered throughout the dataset curation lifecycle."
  - [section] describes how interviews revealed specific practical challenges (e.g., vendor transparency, data collector availability).
  - [corpus] does not provide external validation.
- **Break Condition**: If interview responses are heavily biased toward participants' current organizational contexts, the findings may not generalize.

## Foundational Learning

- **Concept**: Lifecycle-phase mapping
  - Why needed here: Understanding where challenges occur in the dataset creation process is essential for targeting interventions.
  - Quick check question: Can you list all five phases of the dataset lifecycle as defined in this paper?

- **Concept**: Multi-dimensional fairness
  - Why needed here: Recognizing that fairness has composition, process, and release dimensions prevents oversimplification of fairness challenges.
  - Quick check question: What are the three dimensions of fairness identified in this study, and how do they differ?

- **Concept**: Socio-ecological modeling
  - Why needed here: The nested landscape model (individual → discipline → organization → regulatory → socio-political) shows how challenges propagate across levels.
  - Quick check question: In the socio-ecological model, which level would vendor transparency challenges be categorized under?

## Architecture Onboarding

- **Component map**: Interview protocol → Participant recruitment → Thematic coding → Taxonomy development → Challenge categorization → Recommendation formulation
- **Critical path**: Interview → Thematic coding → Taxonomy mapping → Recommendation formulation. Each step depends on the previous one's output quality.
- **Design tradeoffs**: The study prioritizes depth (30 detailed interviews) over breadth (fewer participants but more detailed insights). This increases confidence in specific challenge identification but may limit generalizability.
- **Failure signatures**: If taxonomy categories are too rigid, some challenges may be forced into inappropriate categories. If interview questions are leading, findings may reflect researcher bias rather than practitioner reality.
- **First 3 experiments**:
  1. Replicate the taxonomy with a different sample of dataset curators to test generalizability.
  2. Apply the three-dimensional fairness framework to evaluate existing dataset documentation practices.
  3. Test whether recommendations improve fair dataset curation practices in controlled organizational settings.

## Open Questions the Paper Calls Out
None

## Limitations
- Findings based primarily on industry practitioners (24 industry vs. 6 academia), limiting generalizability to other contexts
- Taxonomy relies on self-reported experiences and may not capture all possible challenges
- Dual-axis categorization assumes challenges can be meaningfully mapped to discrete categories, though some challenges may span multiple dimensions

## Confidence
- Taxonomy structure: Medium
- Identified challenges: High
- Three-dimensional fairness framework: Medium

## Next Checks
1. **Generalizability Test**: Replicate the interview methodology with a different sample of dataset curators, particularly those from non-industry contexts, to assess whether the taxonomy captures a broader range of challenges.

2. **Framework Validation**: Apply the three-dimensional fairness framework to evaluate existing dataset documentation practices and assess whether it adequately captures the fairness considerations practitioners report.

3. **Intervention Effectiveness**: Test whether the recommended systemic changes (at individual, organizational, and regulatory levels) actually improve fair dataset curation practices through controlled organizational studies.