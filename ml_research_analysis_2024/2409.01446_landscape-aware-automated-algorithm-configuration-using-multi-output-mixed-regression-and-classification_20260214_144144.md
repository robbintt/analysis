---
ver: rpa2
title: Landscape-Aware Automated Algorithm Configuration using Multi-output Mixed
  Regression and Classification
arxiv_id: '2409.01446'
source_url: https://arxiv.org/abs/2409.01446
tags:
- functions
- optimization
- bbob
- configurations
- configuration
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates the potential of randomly generated functions
  (RGF) for training feature-based predictive models in landscape-aware algorithm
  selection. To improve representativeness compared to the widely-used BBOB suite,
  we implement a selection process to identify appropriate RGF and focus on automated
  algorithm configuration (AAC) by combining algorithm selection and hyperparameter
  optimization.
---

# Landscape-Aware Automated Algorithm Configuration using Multi-output Mixed Regression and Classification

## Quick Facts
- arXiv ID: 2409.01446
- Source URL: https://arxiv.org/abs/2409.01446
- Reference count: 40
- Primary result: Near-optimal configurations for black-box optimization problems can be identified using landscape-aware neural network models trained on randomly generated functions

## Executive Summary
This work addresses landscape-aware automated algorithm configuration (AAC) by combining algorithm selection and hyperparameter optimization. The authors investigate using randomly generated functions (RGF) instead of traditional benchmark suites for training predictive models. Dense neural networks are employed to handle multi-output mixed regression and classification tasks, predicting optimal configurations for unseen black-box optimization problems based on their landscape features. The approach demonstrates competitive performance against state-of-the-art methods, with neural network models sometimes outperforming traditional random forest approaches.

## Method Summary
The method generates 1000 randomly generated functions (RGF) and 1000 many-affine BBOB (MA-BBOB) functions as training data. A selection process filters RGF based on Kendall rank correlation and global optimum z-score to ensure appropriateness for AAC. Exploratory landscape analysis (ELA) features are computed from 50*d samples for 5D and 20*d samples for 20D problems. Dense neural networks with separate output layers for regression and classification handle the multi-output task. Configurations are found via TPE optimization with 500 evaluations. The approach is evaluated on 24 BBOB functions in 5D and 20D, measuring performance using the area under the convergence curve (AUC).

## Key Results
- Near-optimal configurations identified using the proposed approach outperform default configurations in most cases
- Neural network models sometimes perform better than random forest models typically used in landscape-aware ASP
- Configurations with better performance can be best identified using NN models trained on a combination of RGF and MA-BBOB functions
- The predicted configurations are competitive against the single best solver (SBS) on BBOB test problems

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Randomly generated functions (RGF) provide broader coverage of optimization landscape characteristics than the BBOB suite, improving the generalizability of predictive models.
- Mechanism: RGF are generated using a tree-structured function generator that selects operands and operators from a predefined pool, creating diverse problem classes. This diversity covers optimization problems not represented in BBOB, such as automotive crashworthiness optimization.
- Core assumption: Landscape features extracted from RGF are sufficiently representative of real-world BBO problems to train effective predictive models.
- Evidence anchors:
  - [abstract] "we investigate the potential of randomly generated functions (RGF) for the model training, which cover a much more diverse set of optimization problem classes compared to the widely-used black-box optimization benchmarking (BBOB) suite."
  - [section] "RGF can better cover the ELA feature space, highlighting the benefits of using RGF as training data."
- Break condition: If RGF landscape features are too different from target application domains, or if the function generator produces RGF with unrealistic optimization properties, the representativeness assumption fails.

### Mechanism 2
- Claim: Dense neural network models can effectively handle multi-output mixed regression and classification tasks in landscape-aware automated algorithm configuration.
- Mechanism: The dense NN architecture uses separate output layers for regression and classification tasks, with linear activation for regression and softmax for classification. This allows the model to predict both continuous hyperparameters and categorical algorithm choices simultaneously.
- Core assumption: The mixed-output nature of algorithm configuration can be effectively learned by separating regression and classification pathways within a single neural network.
- Evidence anchors:
  - [abstract] "we consider dense neural network (NN) models, which can easily handle multi-output mixed regression and classification tasks."
  - [section] "different layers are assigned for the mixed regression and classification tasks."
- Break condition: If the relationships between landscape features and optimal configurations are too complex for the chosen NN architecture, or if the mixed-output nature creates conflicting gradients during training, the model performance degrades.

### Mechanism 3
- Claim: A pre-selection process for RGF ensures only functions appropriate for automated algorithm configuration are used for training, improving model accuracy.
- Mechanism: The selection process identifies RGF with clear configuration rankings and avoids functions where the global optimum is an extreme outlier or where all configurations perform equally well.
- Core assumption: The quality of training data directly impacts the performance of predictive models, and RGF with ambiguous rankings provide poor learning signals.
- Evidence anchors:
  - [section] "we implement an intermediate step to select RGF that are appropriate for the model training purposes."
  - [section] "the following measures are implemented to identify RGF that are appropriate for AAC purposes."
- Break condition: If the selection criteria are too restrictive and exclude useful RGF, or if the selection process fails to identify inappropriate RGF, the training data quality suffers.

## Foundational Learning

- Concept: Exploratory Landscape Analysis (ELA) features
  - Why needed here: ELA features quantify the landscape characteristics of optimization problems, providing the input representation for the predictive model.
  - Quick check question: What are the six fundamental feature classes initially proposed in ELA, and how do they capture different aspects of optimization landscapes?

- Concept: Hyperparameter optimization (HPO) and configuration search space
  - Why needed here: HPO is used to identify optimal configurations for each RGF, creating the ground truth labels for the predictive model training.
  - Quick check question: How does the modular CMA-ES configuration search space in Table 1 differ from a standard CMA-ES configuration, and why is this important for the AAC problem?

- Concept: Neural network multi-output architecture design
  - Why needed here: The NN must handle both regression (for continuous hyperparameters) and classification (for categorical hyperparameters) in a single model.
  - Quick check question: Why is it advantageous to use separate output layers with different activation functions (linear for regression, softmax for classification) rather than a single output layer?

## Architecture Onboarding

- Component map: RGF generation -> ELA feature computation -> HPO for optimal configurations -> NN model training -> Configuration prediction for BBO problems
- Critical path: The most critical path is RGF generation -> HPO for optimal configurations, as this determines the quality of training data. Without good training data, even the best NN architecture will fail.
- Design tradeoffs: The choice between RGF and MA-BBOB training data involves a tradeoff between diversity and representativeness. RGF provides broader coverage but less predictable properties, while MA-BBOB offers more focused but potentially limited diversity.
- Failure signatures: Poor model performance often manifests as configurations that underperform the default configuration or fail to generalize to new BBO functions. This typically indicates issues with training data quality or feature representation.
- First 3 experiments:
  1. Train and evaluate NN models using only BBOB functions as training data to establish a baseline comparison.
  2. Generate and test a small set of RGF (e.g., 50 functions) with the selection process to verify it effectively filters inappropriate functions.
  3. Compare NN model performance against random forest models on a subset of BBOB functions to validate the architectural choice.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific landscape characteristics of randomly generated functions (RGF) make them more representative of real-world optimization problems compared to the BBOB suite?
- Basis in paper: [explicit] The paper discusses how RGF can cover a diverse set of optimization problem classes and potentially represent real-world applications like automotive crashworthiness optimization better than BBOB functions.
- Why unresolved: The paper does not provide a detailed analysis of which specific landscape characteristics of RGF make them more representative. It only mentions that RGF are "much more diverse" without quantifying or explaining this diversity.
- What evidence would resolve it: A detailed comparative analysis of landscape characteristics (e.g., modality, global structure, search space topology) between RGF and BBOB functions, with examples of how RGF better match real-world problem landscapes.

### Open Question 2
- Question: How does the selection process for appropriate RGF functions impact the performance of the neural network models in landscape-aware automated algorithm configuration?
- Basis in paper: [explicit] The paper describes an intermediate selection step to identify RGF appropriate for AAC purposes, suggesting this step is crucial for improving model accuracy.
- Why unresolved: While the paper mentions the importance of this selection process, it does not provide empirical evidence on how different selection criteria affect model performance or compare the performance of models trained on selected vs. non-selected RGF.
- What evidence would resolve it: An empirical study comparing the performance of neural network models trained on RGF selected using different criteria, including a control group using randomly selected RGF, to quantify the impact of the selection process on model accuracy.

### Open Question 3
- Question: How do the performance improvements of predicted configurations scale with increasing dimensionality, and what are the limitations of the current approach in higher dimensions?
- Basis in paper: [explicit] The paper mentions extending investigations to 20d and observes that performance improvements are less than in 5d, but does not provide a detailed analysis of scaling behavior.
- Why unresolved: The paper only briefly mentions the extension to 20d and notes reduced performance improvements, but does not explore how the approach scales with dimensionality or identify specific limitations that arise in higher dimensions.
- What evidence would resolve it: A comprehensive study of the approach's performance across a range of dimensions (e.g., 5d, 10d, 20d, 50d), analyzing how configuration prediction accuracy and optimization performance scale with dimensionality, and identifying specific challenges or limitations that arise in higher dimensions.

## Limitations

- The approach is limited to BBOB functions in only 5D and 20D dimensions, which may not generalize to higher-dimensional problems common in practice.
- The selection process for RGF lacks specific threshold values and implementation details, making reproducibility challenging.
- The evaluation does not include real-world optimization problems to validate practical applicability of RGF-trained models.

## Confidence

- **High**: The dense neural network architecture can handle multi-output mixed regression and classification tasks when properly configured with separate output layers.
- **Medium**: Randomly generated functions provide broader coverage of optimization landscape characteristics than the BBOB suite, improving generalizability of predictive models.
- **Medium**: A pre-selection process for RGF ensures only functions appropriate for automated algorithm configuration are used for training, improving model accuracy.

## Next Checks

1. **RGF Representativeness Analysis**: Generate a diverse set of 100 RGF, compute their ELA features, and perform statistical analysis comparing the feature distributions to those of BBOB and MA-BBOB functions. This will quantify how well RGF covers the feature space and validate the representativeness assumption.

2. **Architecture Sensitivity Study**: Systematically vary the neural network architecture (number of layers, neurons per layer, regularization parameters) and measure the impact on prediction accuracy for both regression and classification tasks. This will identify optimal architectural choices and robustness to design decisions.

3. **Cross-Domain Generalization Test**: Apply the trained models to a set of real-world optimization problems from different domains (e.g., engineering design, hyperparameter tuning for machine learning) and measure performance degradation compared to BBOB test functions. This will validate the practical applicability of RGF-trained models.