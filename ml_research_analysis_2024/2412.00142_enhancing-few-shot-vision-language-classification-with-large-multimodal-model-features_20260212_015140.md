---
ver: rpa2
title: Enhancing Few-Shot Vision-Language Classification with Large Multimodal Model
  Features
arxiv_id: '2412.00142'
source_url: https://arxiv.org/abs/2412.00142
tags:
- tasks
- arxiv
- classification
- image
- attention
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of adapting generative Large
  Multimodal Models (LMMs) to vision-language classification tasks. The core method,
  Sparse Attention Vectors (SAVs), identifies and leverages a sparse set of attention
  heads from the LMM's activation space that serve as effective features for classification.
---

# Enhancing Few-Shot Vision-Language Classification with Large Multimodal Model Features

## Quick Facts
- arXiv ID: 2412.00142
- Source URL: https://arxiv.org/abs/2412.00142
- Reference count: 40
- One-line primary result: SAVs outperform few-shot and finetuned baselines across diverse vision-language tasks without requiring model finetuning

## Executive Summary
This paper introduces Sparse Attention Vectors (SAVs), a method for adapting generative Large Multimodal Models (LMMs) to vision-language classification tasks without finetuning. SAVs identify a small subset of attention heads that serve as effective features for classification, extracted from few-shot labeled examples and selected based on their predictive performance. The method achieves strong results across safety, VQA, image-text retrieval, and image classification tasks, demonstrating up to +62.9% improvement on safety tasks and +16.4% on image classification compared to LLaVA-OneVision-7B, while using fewer than 5% of the model's attention heads.

## Method Summary
The core approach involves extracting attention vectors from few-shot labeled examples and selecting the top-performing heads based on their classification accuracy. These sparse attention vectors are then used for direct classification via majority voting, eliminating the need for expensive finetuning. The method leverages the observation that only a small subset of attention heads in LMMs are responsible for effective vision-language classification, allowing for efficient and targeted use of the model's capabilities.

## Key Results
- SAVs achieve up to +62.9% improvement on safety tasks compared to LLaVA-OneVision-7B
- SAVs show +16.4% improvement on image classification tasks without finetuning
- The method uses fewer than 5% of the model's attention heads while outperforming strong baselines

## Why This Works (Mechanism)
SAVs work by identifying and leveraging the most informative attention heads within LMMs for vision-language classification. By focusing on a sparse set of heads that capture task-relevant features, the method bypasses the need for full-model adaptation while maintaining strong performance. The approach exploits the inherent structure of LMM attention mechanisms, where only a small fraction of heads contribute meaningfully to classification tasks.

## Foundational Learning
- **Large Multimodal Models (LMMs)**: Neural networks that process and generate both visual and textual information. Needed to understand the model family being adapted. Quick check: LMMs like LLaVA combine vision encoders with language models.
- **Attention Mechanisms**: Components that weigh the importance of different input elements when generating outputs. Needed to understand how SAVs extract useful features. Quick check: Attention heads in transformers process different aspects of input data.
- **Few-Shot Learning**: Learning from very limited labeled examples. Needed to contextualize the method's applicability. Quick check: Few-shot learning is crucial when labeled data is scarce or expensive to obtain.
- **Feature Selection**: Identifying the most informative components of a model for a specific task. Needed to understand SAVs' core mechanism. Quick check: Feature selection improves efficiency and reduces overfitting.
- **Majority Voting Classification**: Making predictions based on the most common output among selected features. Needed to understand how SAVs make final predictions. Quick check: Majority voting is a simple yet effective ensemble method.

## Architecture Onboarding

**Component Map**: Input Images -> Vision Encoder -> LMM Attention Layers -> Selected Attention Heads (SAVs) -> Majority Voting -> Classification Output

**Critical Path**: The critical path involves extracting attention vectors from few-shot examples, selecting top-performing heads, and using these for majority voting classification. Each step is essential for the method's success.

**Design Tradeoffs**: The method trades model adaptation flexibility (via finetuning) for efficiency and simplicity. While finetuning can potentially achieve higher performance, SAVs offer a parameter-efficient alternative that works across diverse tasks without task-specific adaptation.

**Failure Signatures**: Performance may degrade if few-shot examples are noisy, unrepresentative, or if the selected attention heads are not truly task-relevant. The method may also struggle with tasks requiring complex reasoning beyond the capabilities of the selected sparse heads.

**First Experiments**:
1. Test SAVs on a simple vision-language classification benchmark (e.g., CIFAR-100 with text descriptions) to validate basic functionality
2. Compare SAVs against random attention head selection to confirm the importance of the selection process
3. Evaluate sensitivity to the number of few-shot examples to understand data requirements

## Open Questions the Paper Calls Out
None

## Limitations
- Performance depends heavily on the quality and representativeness of few-shot examples
- Theoretical understanding of why specific attention heads are effective remains limited
- Computational efficiency compared to lightweight finetuning approaches is not fully explored

## Confidence
- **High**: Core empirical findings showing SAVs outperform few-shot baselines across multiple benchmarks
- **Medium**: Claims about generalizability and efficiency across diverse vision-language tasks
- **Low**: Theoretical claims about why specific attention heads are effective for classification

## Next Checks
1. Test SAVs on out-of-distribution or novel vision-language tasks not included in the original benchmarks to assess true generalizability
2. Evaluate the sensitivity of SAVs to the quality and size of few-shot training data, including noisy or biased examples
3. Compare the computational efficiency of SAVs against lightweight finetuning approaches to provide a more complete picture of trade-offs