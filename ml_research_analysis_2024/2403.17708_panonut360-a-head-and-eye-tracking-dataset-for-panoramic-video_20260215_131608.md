---
ver: rpa2
title: 'Panonut360: A Head and Eye Tracking Dataset for Panoramic Video'
arxiv_id: '2403.17708'
source_url: https://arxiv.org/abs/2403.17708
tags:
- video
- dataset
- gaze
- videos
- head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Panonut360, a head and eye tracking dataset
  for panoramic video. It addresses the challenge of understanding user attention
  and behavior in immersive VR/AR environments.
---

# Panonut360: A Head and Eye Tracking Dataset for Panoramic Video

## Quick Facts
- **arXiv ID**: 2403.17708
- **Source URL**: https://arxiv.org/abs/2403.17708
- **Reference count**: 28
- **Primary result**: Introduces Panonut360 dataset with 50 users watching 15 panoramic videos, revealing anisotropic gaze distribution patterns challenging traditional Gaussian assumptions

## Executive Summary
This paper presents Panonut360, a comprehensive dataset for head and eye tracking in panoramic video environments. The dataset addresses the growing need for understanding user attention patterns in immersive VR/AR applications by providing high-frequency (120 Hz) tracking data from 50 participants viewing 15 panoramic videos. The study reveals that gaze attention deviates from the center of the Field of View with a consistent downward offset, forming what the authors term an "anisotropic donut" (Panonut) distribution rather than the traditionally assumed Gaussian pattern. This dataset and its findings have significant implications for improving adaptive streaming systems and content delivery in panoramic video applications.

## Method Summary
The dataset was collected using an HTC VIVE Pro Eye HMD with SRanipal SDK to capture head and gaze tracking data at 120 Hz. Fifty participants (25 male, 25 female) watched 15 panoramic videos while their head and eye movements were recorded. The data includes 3D coordinates of head and gaze positions in panoramic space, viewport and gaze attention locations, and pre-generated saliency maps. The authors developed scripts to analyze the deviation between head and eye movements and generate saliency distributions using Gaussian filtering around gaze fixation points. The dataset is publicly available with accompanying analysis tools.

## Key Results
- Gaze fixations show a consistent downward offset relative to the Field of View center across multiple users and videos
- The gaze distribution follows an anisotropic "donut" pattern (Panonut) rather than Gaussian distribution
- 120 Hz sampling rate captures more detailed behavioral insights compared to previous 30-60 Hz datasets
- The dataset provides a foundation for developing better adaptive streaming systems for panoramic videos

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Gaze fixations deviate from the center of the Field of View (FoV) with a consistent downward offset.
- **Mechanism**: The human eye's optical structure causes gaze to shift slightly downward relative to the head direction when users watch panoramic videos.
- **Core assumption**: The downward offset is not due to user preference for content positioning but rather the inherent biomechanics of human vision.
- **Evidence anchors**:
  - [abstract]: "Our analysis reveals a consistent downward offset in gaze fixations relative to the FoV in experimental settings involving multiple users and videos."
  - [section]: "Our analysis reveals a consistent downward offset in gaze fixations relative to the FoV in experimental settings involving multiple users and videos."
  - [corpus]: Weak - no direct mention of biomechanical eye structure in corpus neighbors.
- **Break condition**: If the downward offset is primarily due to content positioning rather than optical structure, the mechanism would fail.

### Mechanism 2
- **Claim**: The gaze distribution does not follow a Gaussian distribution but rather an anisotropic "donut" shape (Panonut).
- **Mechanism**: Gaze attention is concentrated in an annular region around the FoV center with higher weighting toward the lower portion, creating a non-Gaussian distribution.
- **Core assumption**: The "donut" shape represents a more accurate model of human visual attention in panoramic video viewing than traditional Gaussian models.
- **Evidence anchors**:
  - [abstract]: "the deviation between head and eye movements challenges the widely held assumption that gaze attention decreases from the center of the FoV following a Gaussian distribution."
  - [section]: "Our analysis reveals a consistent downward offset in gaze fixations relative to the FoV in experimental settings involving multiple users and videos."
  - [corpus]: Weak - corpus neighbors do not directly address gaze distribution models.
- **Break condition**: If empirical data shows gaze distribution follows Gaussian patterns, the Panonut model would be invalid.

### Mechanism 3
- **Claim**: High-frequency sampling (120 Hz) captures more valuable insights into user behavior than lower frequencies.
- **Mechanism**: Higher sampling rates provide finer temporal resolution of rapid eye movements and head transitions, revealing patterns invisible at lower frequencies.
- **Core assumption**: The additional data captured at 120 Hz provides meaningful behavioral insights rather than just noise.
- **Evidence anchors**:
  - [abstract]: "we collect head/gaze tracking data at a frequency of 120 Hz."
  - [section]: "Most of previous datasets are sampled at 30-60 Hz, but to obtain more valuable insights, higher sampling rates are necessary, so we collect head/gaze tracking data at a frequency of 120 Hz."
  - [corpus]: Weak - corpus neighbors do not compare different sampling frequencies.
- **Break condition**: If 120 Hz sampling does not reveal additional behavioral patterns beyond what 60 Hz would capture, the benefit would be negligible.

## Foundational Learning

- **Concept**: Equirectangular projection and its distortion effects
  - **Why needed here**: Understanding how 3D panoramic coordinates map to 2D planes is crucial for correctly interpreting gaze and head tracking data
  - **Quick check question**: How does equirectangular projection distort areas near the poles compared to the equator?

- **Concept**: Quaternion mathematics for 3D rotation representation
  - **Why needed here**: Quaternions are used to represent head and gaze orientations in the dataset, avoiding gimbal lock issues
  - **Quick check question**: How do you convert a quaternion (x, y, z, w) to a rotation matrix?

- **Concept**: Gaussian vs. non-Gaussian probability distributions
  - **Why needed here**: The paper challenges the assumption that gaze follows Gaussian distribution, introducing the Panonut model
  - **Quick check question**: What are the key differences between Gaussian and anisotropic distributions in terms of probability density?

## Architecture Onboarding

- **Component map**: HTC VIVE Pro Eye HMD → SRanipal SDK → Unity → Data Collection → CSV Storage → Analysis Scripts → Public Website
- **Critical path**: HMD tracks head/eye → Unity processes data → CSV files generated → Analysis scripts process data → Saliency maps generated → Public repository updated
- **Design tradeoffs**: Higher sampling rate (120 Hz) increases data volume and processing requirements but captures finer behavioral details
- **Failure signatures**: Missing or corrupted gaze data during blinks, calibration errors causing systematic offsets, data synchronization issues between head and eye tracking
- **First 3 experiments**:
  1. Verify data collection by plotting head position trajectories for a single user across all videos
  2. Test the gaze offset calculation by computing angle differences between head and eye vectors for calibration data
  3. Validate the Panonut model by comparing empirical gaze distributions against Gaussian and Panonut predictions on a subset of data

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the downward offset in gaze fixations relative to the FoV vary across different types of panoramic video content (e.g., landscapes vs. sports vs. narrative videos)?
- **Basis in paper**: [explicit] The paper mentions that gaze offset exhibits a consistent downward offset and is influenced by the inherent structure of the human eye, but does not analyze content-specific variations.
- **Why unresolved**: The dataset includes various video categories, but the paper does not provide a detailed breakdown of gaze behavior across these categories.
- **What evidence would resolve it**: Detailed analysis of gaze offset patterns segmented by video genre, showing statistical differences in downward gaze bias across content types.

### Open Question 2
- **Question**: To what extent does audio content influence user gaze patterns and attention distribution in panoramic videos?
- **Basis in paper**: [explicit] The paper notes that most studies use videos without audio, while their dataset includes audio, suggesting potential advantages for understanding audio's effects on user behavior.
- **Why unresolved**: The paper does not conduct specific experiments or analyses comparing gaze patterns with and without audio.
- **What evidence would resolve it**: Comparative studies of gaze data from the same videos with and without audio, measuring differences in attention distribution and fixation patterns.

### Open Question 3
- **Question**: How well do current Transformer-based models perform in predicting long-term FoV trajectories using the Panonut360 dataset compared to traditional RNN-based approaches?
- **Basis in paper**: [inferred] The paper discusses the limitations of RNN-based models and suggests that Transformer models are more suitable for long-term prediction tasks in panoramic video applications.
- **Why unresolved**: The paper does not provide experimental results comparing different model architectures on the dataset.
- **What evidence would resolve it**: Benchmark results comparing FoV prediction accuracy of Transformer models versus RNN models on the Panonut360 dataset, including both short-term and long-term prediction tasks.

## Limitations
- Dataset size limited to 50 users watching 15 videos may not capture full diversity of user behavior
- Analysis focuses on technical metrics without exploring cognitive or psychological factors
- Unclear whether 120 Hz sampling rate is necessary versus sufficient for capturing behavioral patterns
- The underlying mechanism for downward gaze offset (biomechanical vs. content-driven) requires further investigation

## Confidence

- **High Confidence**: The dataset construction methodology and technical implementation details (data collection hardware, sampling rate, format specifications) are well-documented and reproducible.
- **Medium Confidence**: The observation of downward gaze offset is supported by experimental data, but the underlying mechanism (whether biomechanical or content-driven) requires further investigation.
- **Medium Confidence**: The Panonut model as an alternative to Gaussian distribution is theoretically plausible but needs validation across diverse datasets and content types.

## Next Checks
1. **Cross-validation with independent datasets**: Test whether the Panonut model accurately predicts gaze distributions in datasets collected with different hardware (e.g., Oculus Quest, Varjo headsets) and content categories (gaming, educational, entertainment).
2. **Content-specific analysis**: Analyze whether the downward gaze offset and Panonut distribution persist across different video genres and motion types, or if they are specific to the video selection used in this study.
3. **Physiological mechanism validation**: Conduct controlled experiments with eye-tracking specialists to determine whether the downward offset is primarily due to optical structure, head posture, or content positioning effects.