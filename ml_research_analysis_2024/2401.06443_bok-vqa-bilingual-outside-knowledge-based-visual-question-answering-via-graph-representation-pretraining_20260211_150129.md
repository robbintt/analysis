---
ver: rpa2
title: 'BOK-VQA: Bilingual outside Knowledge-Based Visual Question Answering via Graph
  Representation Pretraining'
arxiv_id: '2401.06443'
source_url: https://arxiv.org/abs/2401.06443
tags:
- question
- knowledge
- information
- triple
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a bilingual outside-knowledge VQA (BOK-VQA)
  dataset and a graph-embedded learning-based VQA (GEL-VQA) model to address the challenge
  of multilingual VQA by leveraging external knowledge. The BOK-VQA dataset contains
  17K Korean-English question-answer pairs and 280K knowledge triples, enabling multilingual
  extensibility.
---

# BOK-VQA: Bilingual outside Knowledge-Based Visual Question Answering via Graph Representation Pretraining

## Quick Facts
- arXiv ID: 2401.06443
- Source URL: https://arxiv.org/abs/2401.06443
- Reference count: 10
- Primary result: 25.97% bilingual VQA improvement over baseline

## Executive Summary
This paper introduces BOK-VQA, a bilingual dataset for outside-knowledge visual question answering in Korean and English, and GEL-VQA, a graph-embedded learning-based VQA model. The dataset contains 17K Korean-English QA pairs with 280K knowledge triples, addressing the challenge of multilingual VQA by leveraging external knowledge. The proposed model employs multitask learning combining VQA and triple prediction tasks, utilizing knowledge graph embeddings to incorporate external knowledge. Experimental results demonstrate significant performance improvements in bilingual settings, with Korean (less-resourced) showing 3.7% higher performance than English.

## Method Summary
The paper proposes a bilingual VQA approach that combines dataset construction with model innovation. BOK-VQA dataset creation involves collecting image-question-answer triples in both Korean and English, enriched with knowledge triples connecting visual and textual information. The GEL-VQA model architecture integrates vision transformers for image understanding with language models for question processing, while incorporating knowledge graph embeddings through a multitask learning framework. The model simultaneously predicts answers and relevant knowledge triples, allowing it to leverage external knowledge for improved reasoning. The bilingual nature enables cross-lingual knowledge transfer and addresses resource limitations for less-resourced languages like Korean.

## Key Results
- 25.97% performance improvement over baseline in bilingual VQA settings
- Korean performance exceeds English by 3.7%, demonstrating effectiveness for less-resourced languages
- Successfully leverages 280K knowledge triples to enhance VQA reasoning capabilities
- Demonstrates multilingual extensibility through the bilingual dataset architecture

## Why This Works (Mechanism)
The model works by integrating external knowledge through knowledge graph embeddings into the VQA pipeline. By predicting both answers and relevant knowledge triples simultaneously, the model learns to retrieve and utilize pertinent external information during inference. The bilingual dataset enables cross-lingual transfer learning, where knowledge gained from one language can benefit the other, particularly advantageous for less-resourced languages. The graph representation pretraining allows the model to understand complex relationships between visual elements, textual questions, and external knowledge concepts.

## Foundational Learning
- Knowledge Graph Embeddings (KGEs): Vector representations of entities and relationships in knowledge graphs, essential for incorporating structured external knowledge into neural models
  - Why needed: Enables integration of symbolic knowledge with neural representations
  - Quick check: Verify embeddings capture semantic relationships between entities
- Multitask Learning: Training a single model on multiple related tasks (VQA + triple prediction)
  - Why needed: Encourages shared representations and leverages task relationships
  - Quick check: Monitor performance on individual tasks during joint training
- Cross-lingual Transfer: Knowledge transfer between languages during training
  - Why needed: Addresses resource limitations for less-resourced languages
  - Quick check: Compare performance improvements when training with/without bilingual data

## Architecture Onboarding

Component Map: Image Encoder -> Text Encoder -> Knowledge Graph Encoder -> Multitask Fusion -> Answer Predictor + Triple Predictor

Critical Path: Visual features → multimodal fusion → knowledge integration → answer prediction

Design Tradeoffs:
- Multitask learning enables knowledge sharing but may dilute task-specific optimization
- Knowledge graph integration improves reasoning but increases computational complexity
- Bilingual approach enhances generalization but requires careful alignment between languages

Failure Signatures:
- Performance degradation when knowledge triples are irrelevant or incorrect
- Suboptimal results when language alignment between Korean and English is poor
- Limited gains when external knowledge coverage is incomplete

First 3 Experiments:
1. Ablation study: Remove knowledge graph component to measure its contribution
2. Monolingual comparison: Train separate models for Korean and English vs. bilingual approach
3. Knowledge coverage analysis: Evaluate performance correlation with external knowledge completeness

## Open Questions the Paper Calls Out
None

## Limitations
- Dataset not publicly released, preventing independent validation and replication
- Reliance on GPT-4V for Korean-English translation without validation of translation quality
- Model performance dependent on completeness and accuracy of external knowledge triples
- Baseline comparison lacks specification, making performance gains difficult to contextualize

## Confidence
- Dataset construction and multilingual validity: Medium
- Performance improvement claims: Medium
- Model architecture effectiveness: High
- Knowledge graph integration methodology: High

## Next Checks
1. Release the dataset publicly to enable independent replication and cross-lingual knowledge transfer experiments
2. Conduct ablation studies removing the knowledge graph component to quantify its specific contribution to performance gains
3. Test the model on additional low-resource languages beyond Korean to verify generalizability of the multilingual advantage claims