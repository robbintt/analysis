---
ver: rpa2
title: 'MM-PhyRLHF: Reinforcement Learning Framework for Multimodal Physics Question-Answering'
arxiv_id: '2404.12926'
source_url: https://arxiv.org/abs/2404.12926
tags:
- dataset
- llms
- image
- rlhf
- llava
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The paper proposes an RLHF-based framework (MM-PhyRLHF) for enhancing\
  \ a large multimodal model\u2019s ability to answer physics multiple-choice questions\
  \ that include both text and images. The authors leverage the MM-PhyQA dataset of\
  \ Indian high-school-level multimodal physics problems, and experiment with three\
  \ settings: adding image captions, applying RLHF, and combining both."
---

# MM-PhyRLHF: Reinforcement Learning Framework for Multimodal Physics Question-Answering

## Quick Facts
- arXiv ID: 2404.12926
- Source URL: https://arxiv.org/abs/2404.12926
- Reference count: 40
- The best model (LLaVA 13B with captions and RLHF) achieves substantially higher accuracy than baselines

## Executive Summary
This paper proposes an RLHF-based framework (MM-PhyRLHF) to enhance large multimodal models' ability to answer physics multiple-choice questions containing both text and images. The authors leverage the MM-PhyQA dataset of Indian high-school-level multimodal physics problems and experiment with three settings: adding image captions, applying RLHF, and combining both. Image captions are generated via Infi-MM to provide richer context and reduce hallucinations, while RLHF is implemented using a preference dataset ranked by Gemini Pro and optimized with PPO. Results show significant accuracy improvements with image captioning and RLHF, with the best performing model achieving substantially higher accuracy than baselines.

## Method Summary
The authors fine-tune LLaVA models (7B, 13B, and LoRA-enhanced 13B) on the MM-PhyQA dataset using different combinations of text, image, and caption inputs. They create a preference dataset by generating responses using multiple models and ranking them with Gemini Pro, then train a reward model on this dataset. RLHF is applied using the Proximal Policy Optimization (PPO) algorithm to the fine-tuned models, comparing performance across six training settings.

## Key Results
- Image captioning improves LLaVA-7B accuracy from 53.3% to 82.52%
- RLHF with preference pairs enhances human-aligned reasoning
- Best model (LLaVA 13B with captions and RLHF) achieves substantially higher accuracy than baselines

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Adding image captions improves LMM accuracy by providing richer textual context that mitigates visual hallucinations and processing errors.
- Mechanism: Infi-MM generates detailed explanations for each diagram in physics questions. These captions are concatenated with the original question and options, giving the model more structured information to reason from instead of raw images alone.
- Core assumption: The LMM can integrate textual explanations of images effectively and will use them as primary reasoning cues.
- Evidence anchors:
  - [abstract] states image captions "minimize hallucinations and image processing errors."
  - [section] reports 82.52% accuracy for 7B model with captions vs 53.3% without.
  - [corpus] has no direct evidence; weak correlation.
- Break condition: If captions are too generic or misaligned with the image, the model may ignore them or be misled, reducing performance.

### Mechanism 2
- Claim: RLHF with preference pairs enhances the model's human-aligned reasoning by training a reward model to score responses based on coherence and prompt adherence.
- Mechanism: A preference dataset of 8000 pairs is built by ranking five model responses per sample using Gemini Pro. A reward model (LLaVA2-13B) learns to predict which response is better. PPO then optimizes the policy to maximize this reward.
- Core assumption: Gemini Pro rankings are consistent and reflect human preferences well enough for reward model training.
- Evidence anchors:
  - [abstract] links RLHF to "human-like problem-solving abilities."
  - [section] describes the 3-stage RLHF pipeline and pairing method.
  - [corpus] shows no direct citations; assumption.
- Break condition: If the reward model overfits to Gemini Pro's quirks or if preference pairs are too noisy, RLHF may degrade rather than improve performance.

### Mechanism 3
- Claim: Multi-image chain-of-thought prompting improves physics reasoning by decomposing complex problems into sequential reasoning steps.
- Mechanism: Each physics problem can involve multiple diagrams. CoT prompting forces the model to reason step-by-step across all images, ensuring no visual detail is skipped.
- Core assumption: The model can maintain coherence across multiple reasoning steps and images without drifting off-topic.
- Evidence anchors:
  - [abstract] references "multi-image CoT prompting" as part of the approach.
  - [section] cites prior work "[4, 15]" on CoT prompting for physics QA.
  - [corpus] no direct support; weak.
- Break condition: If reasoning steps become too verbose or redundant, the model may lose focus or run into token limits, hurting accuracy.

## Foundational Learning

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: Standard supervised fine-tuning cannot capture nuanced human preferences for answer quality; RLHF aligns outputs with human judgment.
  - Quick check question: What are the three main stages of an RLHF pipeline?

- Concept: Vision-Language Model (VLM) fine-tuning
  - Why needed here: Off-the-shelf VLMs lack domain-specific physics reasoning; fine-tuning on MM-PhyQA adapts them to multimodal physics MCQs.
  - Quick check question: Why is PEFT (Parameter Efficient Fine-Tuning) used instead of full fine-tuning?

- Concept: Image captioning for VLM context
  - Why needed here: Raw images can be ambiguous or incomplete; captions provide explicit context to reduce hallucination and improve reasoning.
  - Quick check question: What model is used for generating captions in this work?

## Architecture Onboarding

- Component map: Dataset loader -> Caption generator (Infi-MM) -> Base LLaVA model (7B/13B/LoRA) -> Preference dataset builder -> Reward model (LLaVA2-13B) -> PPO optimizer -> Evaluation harness

- Critical path:
  1. Load raw dataset
  2. Generate and append captions
  3. Fine-tune base model (SFT)
  4. Build preference dataset
  5. Train reward model
  6. Apply PPO with reward model
  7. Evaluate accuracy

- Design tradeoffs:
  - Caption granularity vs. noise: detailed captions help but may mislead if inaccurate
  - Preference pair size vs. quality: more pairs improve reward model robustness but cost annotation time
  - LoRA rank vs. adaptation speed: higher rank improves performance but increases compute

- Failure signatures:
  - Accuracy drops after RLHF: reward model overfits or preference data is noisy
  - Captions ignored: generated captions are too generic or misaligned
  - Training instability: PPO step size too large; reduce clipping range

- First 3 experiments:
  1. Baseline: Fine-tune LLaVA-7B with only (Q&A, Image) inputs
  2. Caption test: Fine-tune LLaVA-7B with (Q&A, Image, Caption) inputs
  3. RLHF test: Apply RLHF to the best caption model from experiment 2

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the integration of image captioning with RLHF compare to using either technique alone in terms of improving physics question-answering accuracy?
- Basis in paper: [explicit] The paper experiments with combining image captioning and RLHF, comparing their performance against using each technique individually.
- Why unresolved: The paper presents results for different combinations of techniques but does not provide a direct comparison of the combined approach versus individual techniques in terms of accuracy improvement.
- What evidence would resolve it: A detailed analysis showing the accuracy improvements when combining image captioning with RLHF compared to using each technique separately, with statistical significance testing.

### Open Question 2
- Question: What is the impact of different LoRA rank values on the performance of the LLaVA models in the physics question-answering task?
- Basis in paper: [explicit] The paper mentions experimenting with different LoRA values for the LLaVA models, but does not provide detailed results comparing the impact of different ranks on performance.
- Why unresolved: While the paper uses different LoRA configurations, it does not explicitly analyze or compare the performance impact of varying LoRA rank values.
- What evidence would resolve it: A comprehensive comparison of LLaVA model performance across different LoRA rank values, showing how the rank affects accuracy and training efficiency.

### Open Question 3
- Question: How does the RLHF approach affect the model's ability to handle out-of-distribution physics questions compared to traditional fine-tuning methods?
- Basis in paper: [inferred] The paper mentions that RLHF improves reasoning capabilities and aligns responses with human preferences, suggesting potential benefits for handling diverse question types.
- Why unresolved: The paper does not explicitly test or compare the model's performance on out-of-distribution questions with and without RLHF.
- What evidence would resolve it: An experiment comparing the model's accuracy on out-of-distribution physics questions using RLHF-enhanced models versus traditionally fine-tuned models, with a diverse set of unseen question types.

## Limitations
- No external validation of Gemini Pro rankings as reliable human preference signals
- Absence of caption quality metrics or error analysis for Infi-MM generated captions
- No statistical significance testing or variance reporting for accuracy improvements

## Confidence
- **High confidence**: The core methodology of using RLHF with PPO and image captioning for physics QA is clearly described and technically sound
- **Medium confidence**: The reported accuracy improvements appear significant but lack statistical testing and variance reporting
- **Low confidence**: The assumption that Gemini Pro rankings perfectly reflect human preferences is not empirically verified

## Next Checks
1. Test the reward model's predictions against human-annotated preference labels on a held-out validation set to verify alignment quality
2. Manually inspect 50 generated captions for accuracy, relevance, and potential hallucination risks using expert physics educators
3. Run controlled experiments isolating the effects of image captions, RLHF, and CoT prompting to quantify individual contribution to accuracy gains