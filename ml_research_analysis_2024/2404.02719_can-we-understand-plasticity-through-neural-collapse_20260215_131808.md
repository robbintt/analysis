---
ver: rpa2
title: Can We Understand Plasticity Through Neural Collapse?
arxiv_id: '2404.02719'
source_url: https://arxiv.org/abs/2404.02719
tags:
- neural
- collapse
- loss
- plasticity
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the relationship between plasticity loss
  and neural collapse in deep learning. Plasticity loss refers to a neural network's
  reduced ability to adapt to new tasks, while neural collapse describes the tendency
  of feature representations to converge to class means.
---

# Can We Understand Plasticity Through Neural Collapse?

## Quick Facts
- arXiv ID: 2404.02719
- Source URL: https://arxiv.org/abs/2404.02719
- Authors: Guglielmo Bonifazi; Iason Chalas; Gian Hess; Jakub Åucki
- Reference count: 9
- Primary result: Neural collapse and plasticity loss show complex correlation patterns in continual learning, with regularization of collapse improving adaptability to new tasks.

## Executive Summary
This paper investigates the relationship between plasticity loss (reduced ability to adapt to new tasks) and neural collapse (feature convergence to class means) in deep learning models. Through experiments in Permuted MNIST and Warm Starting settings, the authors find that while plasticity loss can prevent neural collapse in continual learning scenarios, there exists a positive correlation when focusing on post-first-task-change phases. In Warm Starting, a strong initial positive correlation is observed that diminishes as training progresses. The study also demonstrates that regularizing neural collapse can alleviate plasticity loss, improving test accuracy on new tasks. The paper highlights the complex interplay between these phenomena and emphasizes the need for further exploration of underlying factors.

## Method Summary
The paper employs two experimental settings to study the relationship between plasticity loss and neural collapse. In Permuted MNIST, a 3-layer MLP is trained sequentially on 140 tasks with varying epochs for the first task. In Warm Starting, an 18-layer ResNet is trained on a warm-up dataset and then on the full dataset, with and without neural collapse regularization. Neural collapse metrics (NC1-NC4) and plasticity loss are measured and compared across different experimental setups to analyze their correlation. The authors introduce a regularization approach targeting neural collapse to assess its impact on alleviating plasticity loss.

## Key Results
- Plasticity loss can prevent neural collapse in continual learning scenarios with Permuted MNIST
- Strong initial positive correlation between plasticity loss and neural collapse in Warm Starting, diminishing over training
- Regularizing neural collapse improves test accuracy on new tasks and alleviates plasticity loss
- Correlation patterns suggest complex interplay rather than simple causal relationship

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Excessive reduction in within-class variability leads to decreased model adaptability to new tasks
- Mechanism: Neural collapse causes features to collapse to class means, reducing the model's ability to distinguish between samples within a class. This reduced representational richness limits the model's capacity to adapt to new tasks that may require finer distinctions
- Core assumption: The information discarded during neural collapse is necessary for plasticity
- Evidence anchors:
  - [abstract]: "We hypothesize that an excessive reduction in in-class variability of activations might compromise the plasticity of the model"
  - [section]: "The collapse of features to class means implies a form of forgetting, where information distinguishing samples within a class is discarded"
  - [corpus]: Weak evidence - corpus papers focus on spectral collapse and churn but don't directly address the mechanism linking within-class variability to plasticity
- Break condition: If new tasks require the same coarse-grained features that remain after collapse

### Mechanism 2
- Claim: Training duration correlates with both neural collapse and plasticity loss
- Mechanism: Longer training on initial tasks drives both phenomena independently. Extended optimization reduces within-class variability (neural collapse) while simultaneously causing the model to overfit to task-specific features (plasticity loss)
- Core assumption: Training duration is the common underlying factor rather than a causal relationship between the two phenomena
- Evidence anchors:
  - [abstract]: "Both PL and NC seem to exacerbate with the number of training steps taken"
  - [section]: "we computed correlations within a sliding window of 100 warm-up epochs" showing correlation deteriorates as training progresses
  - [corpus]: Weak evidence - corpus papers mention spectral collapse and churn but don't establish the temporal correlation mechanism
- Break condition: If training is interrupted before either phenomenon fully develops

### Mechanism 3
- Claim: Regularizing neural collapse can mitigate plasticity loss
- Mechanism: By preventing excessive reduction in within-class variability, regularization maintains representational richness needed for adapting to new tasks
- Core assumption: The regularization preserves information that would otherwise be lost during collapse
- Evidence anchors:
  - [abstract]: "we introduce a regularization approach to mitigate neural collapse, demonstrating its effectiveness in alleviating plasticity loss"
  - [section]: "the results indicated a marked improvement over classical warm-up, reflected in better test accuracy for both the Warm-Up and Full datasets"
  - [corpus]: Weak evidence - corpus papers don't mention regularization approaches
- Break condition: If regularization is too strong and prevents adequate convergence on initial tasks

## Foundational Learning

- Concept: Neural collapse phenomenon
  - Why needed here: Understanding the mechanism requires knowing how features converge to class means and what metrics measure this collapse
  - Quick check question: What does NC1 metric measure and why is lower value indicative of more severe collapse?

- Concept: Plasticity loss in continual learning
  - Why needed here: The paper investigates how model adaptability degrades when trained sequentially on multiple tasks
  - Quick check question: How does plasticity loss manifest differently in Permuted MNIST versus Warm Starting settings?

- Concept: Correlation vs causation
  - Why needed here: The paper emphasizes caution in interpreting observed correlations as causal relationships
  - Quick check question: What evidence would be needed to establish neural collapse as a causal factor for plasticity loss rather than just correlated?

## Architecture Onboarding

- Component map:
  - Experiment controller -> Model manager -> Metric calculator -> Regularization module -> Data pipeline

- Critical path:
  1. Initialize model and dataset
  2. Train on initial task while monitoring neural collapse metrics
  3. Continue to subsequent tasks while measuring plasticity loss
  4. Analyze correlation between metrics across tasks
  5. Apply regularization if testing that component

- Design tradeoffs:
  - Simpler models (MLPs) vs complex models (ResNets) for different task types
  - Computational cost of running multiple seeds vs statistical confidence
  - Regularization strength balancing between preventing collapse and maintaining convergence

- Failure signatures:
  - Negative correlation between plasticity loss and neural collapse indicates collapse prevented by poor performance
  - Correlation disappearing after initial training phase suggests confounding factors
  - Regularization harming Warm-Up dataset performance indicates over-regularization

- First 3 experiments:
  1. Replicate neural collapse in continual learning with 140 Permuted MNIST tasks
  2. Test variable length of training on first task with 5 seeds
  3. Implement NC1 regularization in Warm Starting setting and measure accuracy impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the relationship between plasticity loss and neural collapse causal, or are both phenomena driven by an underlying factor such as training duration or network capacity?
- Basis in paper: [inferred] The authors note a strong correlation between neural collapse and plasticity loss in the early stages of training, but caution that training duration is inherently correlated with both phenomena, potentially acting as an underlying factor. They also demonstrate that regularizing neural collapse can alleviate plasticity loss, hinting at a potential causal relationship, but acknowledge that other regularization methods like L2 could have the same effect.
- Why unresolved: The experiments conducted in the paper do not definitively establish causality. While regularizing neural collapse improves plasticity loss, this does not rule out the possibility of a common underlying factor driving both phenomena.
- What evidence would resolve it: Further experiments manipulating neural collapse and plasticity loss independently, while controlling for other factors like training duration and network capacity, could help establish causality. Additionally, theoretical analysis of the mechanisms underlying both phenomena could provide insights into their relationship.

### Open Question 2
- Question: How do network architecture and optimization schedule influence the relationship between plasticity loss and neural collapse?
- Basis in paper: [explicit] The authors mention that factors such as network size and optimization schedules determine the model's capacity to overfit on the first task, which in turn affects the relationship between plasticity loss and neural collapse.
- Why unresolved: The experiments in the paper focus on specific architectures (3-layer MLP for Permuted MNIST and 18-layer ResNet for CIFAR-10) and optimization settings. The impact of different architectures and optimization schedules on the relationship between plasticity loss and neural collapse remains unexplored.
- What evidence would resolve it: Conducting experiments with a variety of network architectures (e.g., different depths, widths, and types of layers) and optimization schedules (e.g., different learning rates, momentum, and regularization techniques) could shed light on how these factors influence the relationship between plasticity loss and neural collapse.

### Open Question 3
- Question: How does the similarity between subsequent tasks affect the relationship between plasticity loss and neural collapse?
- Basis in paper: [explicit] The authors mention that the similarity between subsequent tasks can lead to divergent outcomes, such as the one witnessed with plasticity loss or a successful fine-tuning.
- Why unresolved: The experiments in the paper focus on specific task sequences (Permuted MNIST and Warm Starting with CIFAR-10). The impact of task similarity on the relationship between plasticity loss and neural collapse remains unexplored.
- What evidence would resolve it: Designing experiments with varying degrees of task similarity (e.g., using different datasets or modifying the input space) and analyzing the resulting patterns of plasticity loss and neural collapse could provide insights into how task similarity influences their relationship.

## Limitations
- Synthetic scenarios (Permuted MNIST and Warm Starting) may not fully capture real-world continual learning challenges
- Correlation analysis rather than establishing causal mechanisms
- Potential confounding factors not fully explored

## Confidence
- **High confidence**: The empirical observations of neural collapse metrics (NC1-NC4) in continual learning settings are reproducible and well-documented.
- **Medium confidence**: The correlation patterns between plasticity loss and neural collapse, while statistically significant, require further validation across diverse architectures and tasks.
- **Low confidence**: The proposed regularization mechanism's effectiveness needs more rigorous ablation studies to isolate its specific impact from other training dynamics.

## Next Checks
1. Test the correlation patterns on more diverse continual learning benchmarks beyond Permuted MNIST, including task-incremental and class-incremental scenarios.

2. Conduct controlled experiments varying the number of training epochs independently for each task to better isolate the temporal relationship between plasticity loss and neural collapse.

3. Implement additional regularization strategies targeting different aspects of neural collapse (e.g., NC2-NC4) to determine which mechanisms most effectively preserve plasticity.