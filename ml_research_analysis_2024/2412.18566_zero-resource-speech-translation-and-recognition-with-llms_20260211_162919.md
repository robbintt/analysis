---
ver: rpa2
title: Zero-resource Speech Translation and Recognition with LLMs
arxiv_id: '2412.18566'
source_url: https://arxiv.org/abs/2412.18566
tags:
- speech
- language
- languages
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper tackles the challenge of zero-resource speech translation\
  \ and automatic speech recognition in languages without paired audio-text data.\
  \ The authors propose a novel method that leverages a multilingual Large Language\
  \ Model (LLM) by adapting audio representations from a pre-trained multilingual\
  \ speech encoder to the LLM\u2019s token embedding space using a lightweight CNN\
  \ adapter."
---

# Zero-resource Speech Translation and Recognition with LLMs

## Quick Facts
- arXiv ID: 2412.18566
- Source URL: https://arxiv.org/abs/2412.18566
- Reference count: 32
- Key outcome: BLEU scores over 23 for zero-resource speech translation in CoVoST2, WER up to 28.2% for ASR

## Executive Summary
This paper addresses the challenge of zero-resource speech translation and automatic speech recognition for languages without paired audio-text data. The authors propose a novel method that leverages a multilingual Large Language Model (LLM) by adapting audio representations from a pre-trained multilingual speech encoder to the LLM's token embedding space using a lightweight CNN adapter. The system demonstrates strong performance on previously unseen languages, achieving BLEU scores over 23 for speech translation and WER up to 28.2% for ASR, while highlighting that LLM language generation capability is a key performance bottleneck.

## Method Summary
The method employs a pre-trained Conformer speech encoder (630M parameters) to process audio input, followed by a weighted average layer combining all encoder outputs. A 2-layer 1D CNN adapter (18.9M or 62.9M parameters) maps these audio representations to the LLM's token embedding space. LoRA adapters (9.4M or 18.8M additional parameters) are then used to adapt the mT0 LLM (3.7B or 12.9B parameters) for the specific tasks. The system is trained using multi-task learning for both speech translation (ST) and automatic speech recognition (ASR), with a sequential training strategy where the CNN adapter is pre-trained on ASR data before joint fine-tuning with LoRA parameters.

## Key Results
- Achieved BLEU scores over 23 for zero-resource speech translation in CoVoST2
- Obtained WER up to 28.2% for zero-resource ASR tasks
- Demonstrated that LLM language generation capability bounds ASR performance in zero-resource settings

## Why This Works (Mechanism)

### Mechanism 1
The CNN adapter successfully bridges the representation gap between speech encoder and LLM token embeddings. The 2-layer 1D CNN first upsamples audio features from 1024 to 2048 dimensions (matching LLM hidden size) while downsampling time resolution from 40ms to 80ms frame length, then adds depth through a second convolutional layer. Core assumption: Speech representations can be linearly transformed to match the semantic space of LLM embeddings through this lightweight architecture.

### Mechanism 2
Pre-training the CNN adapter on ASR data before joint training improves zero-resource performance. Initial pre-training allows the CNN to learn basic speech-to-text feature mapping before introducing LoRA parameters, creating a more stable optimization path. Core assumption: Early adaptation of audio features to textual space reduces catastrophic forgetting and improves cross-lingual transfer.

### Mechanism 3
The LLM's language generation capability bounds ASR performance in zero-resource settings. Even with perfect audio-to-embedding mapping, the LLM must be capable of generating text in the target language for accurate ASR output. Core assumption: The multilingual LLM has sufficient exposure to and capacity for the target language during pre-training.

## Foundational Learning

- **Cross-lingual transfer learning**
  - Why needed here: The system must leverage knowledge from high-resource languages to perform well on zero-resource languages without direct training data
  - Quick check question: How does the model transfer knowledge from seen languages to unseen languages when no paired audio-text data exists for the target languages?

- **Adapter-based fine-tuning**
  - Why needed here: Instead of full model fine-tuning, lightweight adapters allow efficient adaptation while preserving pre-trained capabilities
  - Quick check question: Why use a CNN adapter instead of fine-tuning the entire speech encoder or LLM?

- **Multi-task learning dynamics**
  - Why needed here: The model must balance learning both ST and ASR tasks simultaneously without degrading performance on either
  - Quick check question: What happens to zero-resource performance when ASR data is included in multi-task training versus ST-only training?

## Architecture Onboarding

- **Component map**: Audio → Conformer → Weighted average → CNN adapter → LoRA → LLM → Text output
- **Critical path**: Audio → Conformer → Weighted average → CNN adapter → LoRA → LLM → Text output
- **Design tradeoffs**: Using adapters vs full fine-tuning: reduced parameter count but potentially lower performance; Pre-training CNN vs joint training: better initialization but longer overall training time; Task-specific vs universal prompts: better performance but requires prompt engineering
- **Failure signatures**: Poor ASR performance despite good ST: likely LLM language generation limitation; Degraded performance on seen languages: adapter over-fitting or catastrophic forgetting; Random output languages: language confusion in the LLM's multilingual space
- **First 3 experiments**: Test the weighted average layer by varying the weights and measuring impact on ST performance; Evaluate CNN adapter performance with and without pre-training on a small subset of data; Compare multi-task vs sequential training by training on ST only, then ASR only, then both tasks

## Open Questions the Paper Calls Out

### Open Question 1
How do different language families and phonetic similarities impact the zero-resource cross-lingual transfer performance of multilingual LLMs in speech translation and recognition? While the paper provides initial observations that Dutch (less phonetic overlap with training languages) shows significantly worse performance compared to Catalan (higher similarity), it does not systematically investigate this across a broader range of languages.

### Open Question 2
What is the optimal training strategy (multi-task vs. sequential training) for zero-resource speech translation and recognition when using multilingual LLMs? The authors observe conflicting results for ST and ASR tasks, suggesting that the optimal training strategy may depend on the specific task, but further investigation is needed to determine the best approach for each task.

### Open Question 3
How does the size and diversity of the training dataset impact the zero-resource performance of multilingual LLMs in speech translation and recognition? While the paper demonstrates the positive impact of larger datasets, it does not provide a detailed analysis of how dataset size and diversity affect zero-resource performance across different tasks and language pairs.

## Limitations

- The system's ASR performance is fundamentally limited by the LLM's language generation capability - it cannot perform ASR in languages the LLM cannot generate
- The pre-trained speech encoder (Conformer + BEST-RQ) appears to be a non-public or custom checkpoint, making exact reproduction challenging
- Experimental scope is limited to only three zero-resource language pairs with paraphrased prompts during training and fixed prompts during testing

## Confidence

**High Confidence Claims:**
- The CNN adapter architecture successfully bridges the representation gap between speech encoder and LLM token embeddings
- Pre-training the CNN adapter on ASR data before joint training improves zero-resource performance
- The LLM's language generation capability bounds ASR performance in zero-resource settings

**Medium Confidence Claims:**
- The weighted average layer combining all encoder outputs provides optimal feature representation
- The sequential training strategy (ASR pre-training → ST fine-tuning) is superior to multi-task training for zero-resource scenarios
- The 2-layer 1D CNN architecture is optimal for the adaptation task

**Low Confidence Claims:**
- The reported BLEU scores over 23 for previously unseen languages will generalize to other zero-resource language pairs beyond the tested three
- The LoRA adapters for LLM (9.4M or 18.8M parameters) are optimally sized for the adaptation task
- The specific training hyperparameters are optimal across all tasks and language pairs

## Next Checks

1. **Language Coverage Validation**: Test the system on a broader set of zero-resource language pairs (at least 10-15 additional pairs) to verify that the LLM language generation limitation is the primary bottleneck and that the adaptation framework generalizes beyond the three tested languages.

2. **Adapter Architecture Ablation**: Systematically compare the 2-layer 1D CNN adapter against alternative adaptation strategies including full fine-tuning, different adapter types (e.g., MLP-based adapters), and varying CNN depths to establish whether the proposed architecture is optimal or simply adequate.

3. **Pre-training Data Impact Analysis**: Conduct controlled experiments varying the quantity and quality of ASR pre-training data to quantify the relationship between pre-training data characteristics and zero-resource performance, particularly for languages with varying degrees of similarity to the pre-training languages.