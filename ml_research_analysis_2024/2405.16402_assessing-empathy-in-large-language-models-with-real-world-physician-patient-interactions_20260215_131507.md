---
ver: rpa2
title: Assessing Empathy in Large Language Models with Real-World Physician-Patient
  Interactions
arxiv_id: '2405.16402'
source_url: https://arxiv.org/abs/2405.16402
tags:
- patient
- empathy
- response
- chatgpt
- evaluation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether ChatGPT can deliver more empathetic
  responses than physicians in patient interactions. A de-identified dataset of patient
  messages and physician responses from Mayo Clinic was collected, and ChatGPT was
  prompted to generate alternative responses.
---

# Assessing Empathy in Large Language Models with Real-World Physician-Patient Interactions

## Quick Facts
- arXiv ID: 2405.16402
- Source URL: https://arxiv.org/abs/2405.16402
- Reference count: 15
- ChatGPT responses perceived as more empathetic than physician responses in 72.85% of cases by human evaluators

## Executive Summary
This study investigates whether ChatGPT can deliver more empathetic responses than physicians in patient interactions. Using a de-identified dataset of patient messages and physician responses from Mayo Clinic, the researchers prompted ChatGPT to generate alternative responses. They developed an automatic empathy ranking evaluation (EMRank) using LLaMA and conducted human assessments. The results demonstrate that ChatGPT responses were perceived as more empathetic than physician responses in 72.85% of cases by human evaluators, and achieved higher empathy scores across all automatic metrics. The ensemble LLaMA-EMRank metric showed the strongest correlation with human judgment, suggesting that LLM-powered chatbots have potential to enhance patient care by providing more empathetic communication.

## Method Summary
The study collected de-identified patient messages and physician responses from Mayo Clinic's patient portal, focusing on prostate cancer patients who underwent radical prostatectomy. ChatGPT was prompted to generate alternative responses to patient messages using a 100-word limit. The researchers developed LLaMA-EMRank, a family of automatic empathy ranking metrics using LLaMA's in-context learning capabilities without fine-tuning. Human evaluation was conducted with three male prostate cancer patients rating 70 questions in a randomized order. The study employed both automatic metrics (zero-shot, one-shot, few-shot, and ensemble LLaMA-EMRank) and human judgment to assess empathy levels in responses.

## Key Results
- ChatGPT responses were perceived as more empathetic than physician responses in 72.85% of cases by human evaluators
- Achieved higher empathy scores across all automatic metrics (92.41% for zero-shot LLaMA-EMRank)
- Ensemble LLaMA-EMRank metric showed strongest correlation with human judgment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ChatGPT responses are perceived as more empathetic than physician responses in 72.85% of cases by human evaluators.
- Mechanism: ChatGPT leverages its instruction-tuned architecture and extensive pretraining to generate responses that better address patient emotional concerns while maintaining domain accuracy.
- Core assumption: ChatGPT's ability to balance empathy and clinical accuracy is superior to physicians' natural responses under time constraints.
- Evidence anchors:
  - [abstract] "ChatGPT responses were perceived as more empathetic than physician responses in 72.85% of cases by human evaluators"
  - [section 6.1] "human evaluation revealed that ChatGPT's responses are more empathetic than those of humans in 72.85% of cases"
  - [corpus] Weak - no direct citations supporting this specific comparison
- Break condition: If ChatGPT's responses become too verbose or lose domain specificity, the empathy advantage may diminish.

### Mechanism 2
- Claim: LLaMA-EMRank automatic metrics correlate strongly with human judgment for empathy assessment.
- Mechanism: LLaMA's in-context learning capabilities enable zero-shot, one-shot, and few-shot empathy ranking without requiring fine-tuning on domain-specific data.
- Core assumption: LLaMA can effectively evaluate empathy through prompt engineering despite being a smaller model than ChatGPT.
- Evidence anchors:
  - [abstract] "achieved higher empathy scores across all automatic metrics (e.g., 92.41% for zero-shot LLaMA-EMRank)"
  - [section 5.1] "We introduce multiple metrics using LLaMA, named LLaMA-EMRank"
  - [section 6.2] "LLaMA-EMRank achieve positive Pearson'r value, suggesting that the introduced EMRank metrics serve as reliable indicators"
- Break condition: If the in-context examples provided to LLaMA are not representative of the evaluation task, correlation with human judgment may degrade.

### Mechanism 3
- Claim: Ensemble LLaMA-EMRank metric shows strongest correlation with human judgment.
- Mechanism: Combining multiple evaluation approaches (zero-shot, one-shot, few-shot) captures different aspects of empathy assessment, reducing individual method weaknesses.
- Core assumption: Different LLaMA-EMRank methods have complementary strengths that, when combined, provide more robust evaluation than any single method.
- Evidence anchors:
  - [abstract] "The ensemble LLaMA-EMRank metric showed the strongest correlation with human judgment"
  - [section 6.2] "Among all metrics, ensemble metric aligns most closely with human judgment, followed by the order of one-shot voting, few-shot voting, and zero-shot"
  - [section 6.3] Discussion of individual method limitations and ensemble solution
- Break condition: If one method dominates the ensemble or if methods are too correlated, the ensemble benefit may not materialize.

## Foundational Learning

- Concept: In-context learning
  - Why needed here: Enables LLaMA to perform empathy ranking without fine-tuning, maintaining model independence from ChatGPT
  - Quick check question: How does providing examples in the prompt allow LLaMA to adapt to new tasks without parameter updates?

- Concept: Zero-shot vs few-shot learning
  - Why needed here: Different evaluation approaches balance inference efficiency against performance, with zero-shot being most efficient but potentially less accurate
  - Quick check question: What trade-offs exist between using zero-shot evaluation (no examples) versus few-shot evaluation (multiple examples)?

- Concept: Perplexity as fluency metric
  - Why needed here: Provides quantitative measure of response quality that complements empathy assessment
  - Quick check question: Why might lower perplexity correlate with perceived empathy in physician-patient communication?

## Architecture Onboarding

- Component map: ChatGPT API -> Response generation, LLaMA model -> Empathy evaluation, Patient evaluators -> Human judgment validation, Data pipeline -> Mayo Clinic de-identified messages
- Critical path: Patient message -> ChatGPT response generation -> LLaMA-EMRank evaluation -> Human validation -> Result aggregation
- Design tradeoffs: Used smaller LLaMA for evaluation to maintain independence from ChatGPT generation, accepted potential performance gap for evaluation reliability
- Failure signatures: Low correlation between automatic and human metrics indicates prompt engineering issues; inconsistent one-shot results suggest example selection problems
- First 3 experiments:
  1. Test zero-shot LLaMA evaluation with varying prompt formulations to optimize empathy assessment accuracy
  2. Compare one-shot evaluation results using different in-context examples to identify most effective demonstration
  3. Implement ensemble evaluation and measure improvement in correlation with human judgments compared to individual methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do the proposed LLaMA-EMRank metrics compare to existing empathy evaluation methods in terms of reliability and generalizability across different healthcare domains?
- Basis in paper: [explicit] The paper mentions that existing evaluation metrics primarily focus on empathy detection tasks and introduces LLaMA-EMRank as a novel approach. It also states that LLaMA-EMRank eliminates the need for fine-tuning and is generalizable to other domains.
- Why unresolved: The paper does not provide a direct comparison of LLaMA-EMRank with existing empathy evaluation methods in terms of reliability and generalizability across different healthcare domains.
- What evidence would resolve it: A comparative study evaluating LLaMA-EMRank against existing methods using datasets from various healthcare domains, measuring reliability and generalizability.

### Open Question 2
- Question: What are the potential limitations of using ChatGPT and LLaMA models in assessing empathy in physician-patient interactions, and how might these limitations impact the study's conclusions?
- Basis in paper: [inferred] The paper discusses the use of ChatGPT for response generation and LLaMA for empathy evaluation. It mentions potential biases and the need for manual review of outputs, indicating possible limitations.
- Why unresolved: The paper does not explicitly discuss the potential limitations of using these models in assessing empathy or how these limitations might impact the study's conclusions.
- What evidence would resolve it: An analysis of the potential biases and limitations of ChatGPT and LLaMA models in empathy assessment, including their impact on the study's conclusions.

### Open Question 3
- Question: How does the length of responses from ChatGPT and physicians influence the perceived empathy, and what strategies could be employed to ensure a fair comparison?
- Basis in paper: [explicit] The paper mentions that ChatGPT responses tend to be more lengthy on average than those of human physicians, and a word count restriction was implemented to ensure a fair comparison.
- Why unresolved: The paper does not explore how the length of responses influences the perceived empathy or discuss strategies beyond word count restriction to ensure a fair comparison.
- What evidence would resolve it: An investigation into the relationship between response length and perceived empathy, along with alternative strategies for ensuring a fair comparison, such as response content analysis or standardized response templates.

## Limitations

- Study limited to prostate cancer patients who underwent radical prostatectomy, potentially limiting generalizability
- Small human evaluation sample (3 patients rating 70 questions) raises concerns about statistical power
- Low inter-annotator agreement (Fleiss' kappa = -0.15) indicates substantial variability in empathy perception

## Confidence

- **High Confidence**: ChatGPT's superior performance in automatic LLaMA-EMRank metrics and correlation with human judgment
- **Medium Confidence**: Human evaluation results showing 72.85% preference for ChatGPT responses
- **Low Confidence**: Generalizability of results to other medical specialties and patient populations

## Next Checks

1. **Cross-Specialty Validation**: Replicate the study across multiple medical specialties (cardiology, oncology, primary care) to assess generalizability of empathy advantages.

2. **Multi-Rater Agreement Improvement**: Conduct a larger-scale human evaluation with 10-15 diverse evaluators per case to improve inter-annotator agreement and assess stability of the 72.85% finding.

3. **Physician Perspective Integration**: Add physician evaluation of responses focusing on clinical accuracy and appropriateness alongside empathy to provide a more complete assessment of response quality.