---
ver: rpa2
title: 'Position: Stop Making Unscientific AGI Performance Claims'
arxiv_id: '2402.03962'
source_url: https://arxiv.org/abs/2402.03962
tags:
- sentences
- against
- claims
- inflation
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that patterns found in the latent spaces of AI
  models, including LLMs, should not be interpreted as evidence of AGI or intrinsic
  understanding. Through experiments with random projections, PCA, and linear probes
  on LLM activations, the authors show that such patterns can arise even in models
  with no training or understanding.
---

# Position: Stop Making Unscientific AGI Performance Claims

## Quick Facts
- arXiv ID: 2402.03962
- Source URL: https://arxiv.org/abs/2402.03962
- Reference count: 22
- Primary result: Patterns in AI latent spaces don't indicate AGI understanding

## Executive Summary
This paper argues that patterns found in the latent spaces of AI models, including LLMs, should not be interpreted as evidence of AGI or intrinsic understanding. Through experiments with random projections, PCA, and linear probes on LLM activations, the authors demonstrate that such patterns can arise even in models with no training or understanding. The authors show that current methodological setups and human biases (anthropomorphism, confirmation bias) make it easy to over-interpret such patterns, calling for more rigorous testing and cautious communication in AI research to avoid spurious claims of intelligence.

## Method Summary
The authors conduct experiments using linear probes, PCA, and random projections on LLM activations and untrained neural networks. They test whether interpretable patterns emerge from random features and untrained models, demonstrating that human-interpretable patterns can appear without any training. They also test linear probes on FOMC-RoBERTa LLM activations and show these can be fooled by irrelevant sentences about birds, suggesting correlations are spurious rather than evidence of understanding.

## Key Results
- Linear probes on untrained neural networks can predict world coordinates from random features
- PCA components on Treasury yield curve data capture interpretable features without implying economic intelligence
- LLM probes can be fooled by irrelevant sentences, showing correlations don't indicate understanding
- Human biases toward anthropomorphism make over-interpretation of AI capabilities likely

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Patterns found in the latent spaces of AI models can be misleading indicators of intelligence.
- **Mechanism:** Random projections in latent spaces can produce patterns that are interpretable by humans, even in untrained models.
- **Core assumption:** The presence of interpretable patterns in latent spaces does not imply intrinsic understanding or intelligence.
- **Evidence anchors:**
  - [abstract] "Through experiments with random projections, PCA, and linear probes on LLM activations, the authors show that such patterns can arise even in models with no training or understanding."
  - [section 2.1] "we demonstrate that the discovery of human-interpretable patterns in latent spaces should not be a surprising outcome."
  - [corpus] Weak - no direct evidence in corpus.
- **Break condition:** If empirical evidence shows that only trained models produce interpretable patterns, the claim would be weakened.

### Mechanism 2
- **Claim:** Human biases, such as anthropomorphism and confirmation bias, lead to over-interpretation of AI capabilities.
- **Mechanism:** Humans tend to attribute human-like qualities to non-human agents, especially when the agent's behavior resembles human behavior.
- **Core assumption:** The interpretation of AI outputs is influenced by human cognitive biases.
- **Evidence anchors:**
  - [abstract] "we review literature from the social sciences that shows that humans are prone to seek such patterns and anthropomorphize."
  - [section 3.2] "Research on anthropomorphism has repeatedly shown the human tendency to attribute human-like characteristics to non-human agents and/or objects."
  - [corpus] Weak - no direct evidence in corpus.
- **Break condition:** If studies show that humans can objectively evaluate AI outputs without bias, the claim would be weakened.

### Mechanism 3
- **Claim:** The current methodological setup and public image of AI are ideal for misinterpretation of spurious cues as hints towards AGI.
- **Mechanism:** The combination of complex AI models, human biases, and media portrayal creates a fertile ground for misinterpretation.
- **Core assumption:** The context in which AI research is conducted and communicated influences the interpretation of results.
- **Evidence anchors:**
  - [abstract] "we argue and empirically demonstrate that the finding of meaningful patterns in latent spaces of models cannot be seen as evidence in favor of AGI."
  - [section 4] "As discussed above, AI research and development outcomes can very easily be over-interpreted, both from a data perspective, and because of human biases and interests."
  - [corpus] Weak - no direct evidence in corpus.
- **Break condition:** If the methodological setup and public image of AI change to reduce the potential for misinterpretation, the claim would be weakened.

## Foundational Learning

- **Concept:** Linear regression and PCA
  - Why needed here: To understand how patterns can be extracted from data and how they can be misleading.
  - Quick check question: Can you explain how PCA works and why it might produce misleading results?

- **Concept:** Anthropomorphism and confirmation bias
  - Why needed here: To understand how human biases can influence the interpretation of AI outputs.
  - Quick check question: Can you give an example of how anthropomorphism might affect the interpretation of AI behavior?

- **Concept:** Hypothesis testing and statistical significance
  - Why needed here: To understand how to rigorously test claims about AI capabilities.
  - Quick check question: Can you explain the difference between a null hypothesis and an alternative hypothesis?

## Architecture Onboarding

- **Component map:** Random projections -> PCA -> Linear probes -> LLM activations -> Human interpretation
- **Critical path:** Latent space analysis -> Pattern detection -> Human interpretation -> AGI claims
- **Design tradeoffs:** Complexity of AI models vs. potential for misinterpretation
- **Failure signatures:** Over-interpretation of AI outputs, reliance on spurious correlations, confirmation bias
- **First 3 experiments:**
  1. Apply linear regression to latent spaces of untrained models to see if interpretable patterns emerge.
  2. Test human interpretation of AI outputs with and without priming to see if biases influence interpretation.
  3. Analyze the relationship between AI model complexity and the potential for misinterpretation of outputs.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific experimental design would definitively distinguish between genuine understanding and spurious pattern matching in LLM representations?
- Basis in paper: [explicit] The authors discuss the need for more rigorous testing and mention their "Parrot Test" as a minimum viable framework, but acknowledge it's insufficient.
- Why unresolved: Current methods like linear probes can show patterns even in untrained models, and human biases make it difficult to interpret results objectively. The paper demonstrates this with experiments showing linear probes on untrained networks and random PCA components can predict meaningful variables.
- What evidence would resolve it: A test that could consistently distinguish between models that truly understand concepts versus those that merely memorize correlations, perhaps through adversarial testing with impossible or nonsensical scenarios that would fool a pattern-matching model but not a genuinely understanding one.

### Open Question 2
- Question: How can we quantify the degree of anthropomorphism in AI research claims and develop metrics to assess the validity of intelligence claims?
- Basis in paper: [explicit] The authors extensively discuss how human tendencies toward anthropomorphism and confirmation bias affect the interpretation of AI capabilities, and how this is exacerbated by professional incentives in the field.
- Why unresolved: While the paper identifies these biases, it doesn't provide concrete metrics or frameworks for measuring or mitigating them. The challenge lies in creating objective measures that can be applied across different AI systems and research contexts.
- What evidence would resolve it: Development of standardized assessment tools that can measure the degree to which research claims rely on anthropomorphic interpretations versus objective technical evidence, potentially including peer review protocols specifically designed to identify and mitigate these biases.

### Open Question 3
- Question: What would constitute a scientifically valid demonstration of Artificial General Intelligence that avoids the pitfalls of spurious pattern detection and confirmation bias?
- Basis in paper: [explicit] The authors argue that current methodological setups are prone to misinterpretation and call for more rigorous testing, while providing examples of how even simple models can produce seemingly meaningful patterns.
- Why unresolved: The paper highlights the challenges in distinguishing genuine intelligence from pattern matching but doesn't propose a definitive framework for what would constitute valid evidence of AGI. This remains a fundamental challenge in the field.
- What evidence would resolve it: A comprehensive framework that combines multiple lines of evidence, including successful performance on novel tasks that require true generalization, consistent results across different testing methodologies, and the ability to explain its reasoning in ways that demonstrate genuine understanding rather than memorized patterns.

## Limitations
- Empirical evidence relies heavily on synthetic demonstrations rather than direct testing of published AGI claims
- Limited scope of inflation/birds experiment may not comprehensively test all types of understanding claims
- Social science literature on anthropomorphism is cited but not directly connected to specific AGI discourse examples

## Confidence

| Claim | Confidence Level |
|-------|------------------|
| That interpretable patterns can emerge from random features in latent spaces | High |
| That human cognitive biases systematically lead to over-interpretation of AI capabilities | Medium |
| That the current methodological and communication context creates fertile ground for misinterpretation | Medium |

## Next Checks
1. **Systematic literature review**: Examine a sample of published AGI performance claims to identify whether they actually rely on the methodological patterns (linear probes on latent spaces) that the authors demonstrate can be misleading.
2. **Extended probe robustness testing**: Replicate the inflation/birds experiment with a broader range of semantic domains and probe architectures to determine the prevalence and severity of this failure mode across different types of understanding claims.
3. **Human bias validation study**: Conduct controlled experiments where participants evaluate LLM outputs with and without priming about AGI capabilities, measuring the impact of anthropomorphism and confirmation bias on their assessment of model understanding.