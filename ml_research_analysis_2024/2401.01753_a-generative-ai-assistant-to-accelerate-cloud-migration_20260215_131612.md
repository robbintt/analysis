---
ver: rpa2
title: A Generative AI Assistant to Accelerate Cloud Migration
arxiv_id: '2401.01753'
source_url: https://arxiv.org/abs/2401.01753
tags: []
core_contribution: This paper presents a generative AI tool designed to automate cloud
  migration strategy generation. The tool uses a large language model to produce migration
  profiles in JSON format, accompanied by architecture diagrams and documentation
  links.
---

# A Generative AI Assistant to Accelerate Cloud Migration

## Quick Facts
- **arXiv ID**: 2401.01753
- **Source URL**: https://arxiv.org/abs/2401.01753
- **Reference count**: 6
- **Primary result**: Generative AI tool achieving 87% accuracy in cloud migration strategy generation

## Executive Summary
This paper presents a generative AI tool designed to automate cloud migration strategy generation using large language models. The tool produces migration profiles in JSON format with architecture diagrams and documentation links. Two prompt strategies were evaluated: a single API call for full JSON output versus multiple API calls for individual profile values. The structured approach achieved 87% accuracy compared to 83% for the default method, with tradeoffs in latency (55.51s vs 34.27s) and cost (305 vs 403 tokens). User studies indicated the tool was particularly helpful for complex migration scenarios.

## Method Summary
The tool uses a serverless architecture with caching to optimize performance and cost. It employs a large language model to generate migration profiles in JSON format, accompanied by architecture diagrams and documentation links. Two prompt strategies were evaluated: a single API call for full JSON output versus multiple API calls for individual profile values. The structured approach showed improved accuracy at the expense of increased latency and token usage. The implementation leverages a caching mechanism to reduce redundant API calls and optimize resource utilization.

## Key Results
- Structured prompts achieved 87% accuracy versus 83% for default method
- Performance tradeoffs: 55.51s latency (vs 34.27s) and 305 tokens (vs 403 tokens)
- User studies showed tool particularly helpful for complex migration scenarios

## Why This Works (Mechanism)
The tool works by leveraging structured prompt engineering to guide the LLM toward producing consistent, JSON-formatted migration profiles. The multi-call approach breaks down the migration assessment into discrete components (architecture, complexity, migration strategy), allowing the model to focus on specific aspects with tailored prompts. This structured approach reduces ambiguity and improves accuracy by constraining the output format. The serverless architecture with caching minimizes latency and cost by avoiding redundant API calls for similar migration profiles.

## Foundational Learning
1. **Prompt Engineering for Structured Output**: Why needed - To guide LLM toward consistent JSON format; Quick check - Verify output schema compliance
2. **Multi-Call vs Single-Call Strategies**: Why needed - Balance accuracy vs latency/cost tradeoffs; Quick check - Compare response times and token usage
3. **Serverless Architecture Benefits**: Why needed - Scalability and cost optimization for cloud migration tool; Quick check - Monitor cold start times and concurrent request handling
4. **Caching Mechanisms**: Why needed - Reduce redundant API calls and improve response times; Quick check - Validate cache hit ratios and invalidation policies
5. **Accuracy Metrics for LLM Outputs**: Why needed - Quantify performance against manual domain expert profiles; Quick check - Establish inter-rater reliability for ground truth data

## Architecture Onboarding

Component Map: User Request -> Prompt Engineering Layer -> LLM API -> JSON Parser -> Cache -> Response

Critical Path: User request triggers prompt generation → LLM API call(s) → Response parsing → Cache storage → Formatted output delivery

Design Tradeoffs: Structured prompts improve accuracy (87% vs 83%) but increase latency (55.51s vs 34.27s) and token usage (305 vs 403 tokens)

Failure Signatures: Inaccurate migration strategies, malformed JSON responses, cache misses causing repeated API calls, latency spikes during high concurrent usage

First Experiments:
1. Benchmark accuracy improvement from structured vs default prompts across 50+ applications
2. Test cache effectiveness under realistic concurrent request patterns
3. Evaluate cost optimization through token usage analysis across different prompt strategies

## Open Questions the Paper Calls Out
None

## Limitations
- Small sample size (20 applications from one enterprise) limits generalizability
- Accuracy metrics measured against manual profiles lack inter-rater reliability details
- User study sample of 10 participants may not represent diverse skill levels
- Security and compliance implications of processing data through external LLM APIs not addressed

## Confidence
- **High Confidence**: Technical implementation of prompt strategies and comparative performance metrics
- **Medium Confidence**: User study results showing preference for structured prompts in complex scenarios
- **Low Confidence**: Generalizability claims across different enterprise contexts and application types

## Next Checks
1. Conduct larger-scale evaluation with 100+ applications across multiple industries
2. Perform A/B testing with different LLM configurations and temperature settings
3. Implement and test caching mechanism under realistic production workloads