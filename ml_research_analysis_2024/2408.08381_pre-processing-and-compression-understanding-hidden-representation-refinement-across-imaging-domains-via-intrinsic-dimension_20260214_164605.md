---
ver: rpa2
title: 'Pre-processing and Compression: Understanding Hidden Representation Refinement
  Across Imaging Domains via Intrinsic Dimension'
arxiv_id: '2408.08381'
source_url: https://arxiv.org/abs/2408.08381
tags:
- repr
- intrinsic
- depth
- relative
- network
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies how the intrinsic dimension (ID) of neural network
  hidden representations evolves through network depth, and how this behavior differs
  between natural and medical image domains. Using eleven natural and medical image
  datasets and six network architectures, the authors find that medical image models
  peak in representation ID earlier in the network than natural image models, implying
  a difference in the abstractness of prediction-relevant features between domains.
---

# Pre-processing and Compression: Understanding Hidden Representation Refinement Across Imaging Domains via Intrinsic Dimension

## Quick Facts
- arXiv ID: 2408.08381
- Source URL: https://arxiv.org/abs/2408.08381
- Authors: Nicholas Konz; Maciej A. Mazurowski
- Reference count: 40
- This paper studies how the intrinsic dimension (ID) of neural network hidden representations evolves through network depth, and how this behavior differs between natural and medical image domains.

## Executive Summary
This study investigates how neural network hidden representations evolve in complexity across layers by measuring intrinsic dimension (ID), a proxy for the number of independent factors needed to describe the data. Using eleven natural and medical image datasets and six network architectures, the authors find that medical image models peak in representation ID earlier in the network than natural image models, suggesting domain-specific differences in feature abstraction. They also discover a strong correlation between the peak representation ID and the ID of the input data, indicating that the intrinsic information content of learned representations is guided by the data's intrinsic complexity.

## Method Summary
The authors use the two-nearest neighbor (2-NN) method to estimate the intrinsic dimension of hidden representations at each layer of trained image classification models. They evaluate eleven datasets spanning natural and medical imaging, using six different architectures. By tracking how ID changes through the network, they identify patterns in representation refinement and compare behaviors across domains. The peak representation ID is then correlated with the ID of the input data to understand the relationship between data complexity and learned features.

## Key Results
- Medical image models peak in representation ID earlier in the network than natural image models, indicating earlier abstraction of prediction-relevant features.
- There is a strong correlation between the peak representation ID and the ID of the input data, suggesting that learned representations are shaped by the intrinsic complexity of the training data.
- These findings provide insights into how network behavior differs across imaging domains and how learned features are shaped by training data.

## Why This Works (Mechanism)
The intrinsic dimension (ID) of a representation reflects the number of independent factors needed to describe it. By measuring ID at each layer, the authors can track how information is compressed and refined as it flows through the network. Natural images, with their rich and complex visual features, require deeper layers to abstract prediction-relevant information, leading to later ID peaks. Medical images, often simpler and more task-specific, reach abstraction earlier. The correlation between peak ID and input ID suggests that the network's ability to compress and abstract is fundamentally constrained by the intrinsic complexity of the training data.

## Foundational Learning
- **Intrinsic Dimension (ID)**: The minimum number of parameters needed to describe a dataset or representation. *Why needed*: Serves as a quantitative measure of information complexity and abstraction in neural networks. *Quick check*: Can be estimated using methods like two-nearest neighbor or maximum likelihood estimation.
- **Two-Nearest Neighbor (2-NN) Method**: A technique to estimate intrinsic dimension by analyzing the distribution of distances to the nearest and second-nearest neighbors in the data. *Why needed*: Provides a robust, widely-used approach to measure ID in high-dimensional spaces. *Quick check*: Compare results with other ID estimation methods to ensure consistency.
- **Representation Refinement**: The process by which neural networks progressively transform and abstract input data through successive layers. *Why needed*: Central to understanding how models learn and generalize from data. *Quick check*: Visualize feature maps or use techniques like t-SNE to qualitatively assess abstraction across layers.
- **Domain-Specific Feature Abstraction**: The idea that different data domains (e.g., natural vs. medical images) require different depths or mechanisms for extracting relevant features. *Why needed*: Explains observed differences in network behavior across domains. *Quick check*: Compare ID curves across domains and architectures to identify consistent patterns.

## Architecture Onboarding
- **Component Map**: Input Data -> Network Layers (Sequential/Residual) -> Hidden Representations (measured at each layer) -> Output Predictions
- **Critical Path**: Measuring ID at each layer is essential to track representation refinement and identify peak ID behavior.
- **Design Tradeoffs**: The choice of ID estimation method and layer sampling frequency can affect the resolution and accuracy of the analysis. Balancing computational cost with measurement granularity is key.
- **Failure Signatures**: Inconsistent ID estimates across similar architectures or domains may indicate issues with the estimation method or data preprocessing. Large variance in ID across layers could suggest instability in representation refinement.
- **First Experiments**:
  1. Reproduce ID curves for a simple CNN on a small natural image dataset to validate the measurement pipeline.
  2. Compare ID estimates using 2-NN and MLE-based methods on the same dataset to assess sensitivity.
  3. Visualize hidden representations (e.g., via PCA or t-SNE) at peak ID and final layers to qualitatively confirm abstraction trends.

## Open Questions the Paper Calls Out
None

## Limitations
- The study's conclusions are based on a limited number of network architectures and dataset combinations, which may limit generalizability.
- The analysis focuses on image classification tasks and may not extend to other modalities or tasks.
- The choice of ID estimation method and hyperparameters could influence the results, though standard approaches are used.

## Confidence
- High confidence in the methodological framework for measuring intrinsic dimension across network depths
- Medium confidence in the comparative analysis between natural and medical imaging domains
- Medium confidence in the correlation between peak representation ID and input data ID
- Low confidence in the broader generalizability of findings beyond the studied architectures and datasets

## Next Checks
1. Replicate the study using additional network architectures (e.g., Vision Transformers, MLPs) and more diverse medical imaging tasks (segmentation, detection) to test robustness of the domain differences.
2. Conduct ablation studies varying the intrinsic dimension estimation method (e.g., MLE-based, PCA-based) and hyperparameters to assess sensitivity of the findings.
3. Investigate whether the observed ID patterns correlate with specific architectural choices (depth, width, activation functions) or training characteristics (batch size, optimization method) rather than domain-specific factors.