---
ver: rpa2
title: A Study on How Attention Scores in the BERT Model are Aware of Lexical Categories
  in Syntactic and Semantic Tasks on the GLUE Benchmark
arxiv_id: '2403.16447'
source_url: https://arxiv.org/abs/2403.16447
tags:
- attention
- bert
- words
- lexical
- categories
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates how attention scores in BERT vary by lexical
  category during fine-tuning for different downstream tasks. Using a novel extraction
  algorithm, the authors analyze attention distributions across six GLUE tasks, finding
  that semantic tasks increase attention to content words while syntactic tasks enhance
  focus on function words.
---

# A Study on How Attention Scores in the BERT Model are Aware of Lexical Categories in Syntactic and Semantic Tasks on the GLUE Benchmark

## Quick Facts
- arXiv ID: 2403.16447
- Source URL: https://arxiv.org/abs/2403.16447
- Reference count: 0
- This study investigates how attention scores in BERT vary by lexical category during fine-tuning for different downstream tasks.

## Executive Summary
This study investigates how attention scores in BERT vary by lexical category during fine-tuning for different downstream tasks. Using a novel extraction algorithm, the authors analyze attention distributions across six GLUE tasks, finding that semantic tasks increase attention to content words while syntactic tasks enhance focus on function words. The results demonstrate that BERT models develop task-specific attention patterns aligned with lexical categories during fine-tuning. Additionally, certain layers consistently prefer specific lexical categories regardless of task, suggesting inherent layer-wise biases. These findings provide insights into how BERT captures syntactic and semantic information through attention mechanisms, validating the hypothesis that lexical category awareness emerges during fine-tuning based on task requirements.

## Method Summary
The study fine-tuned BERT-base-cased models on six GLUE benchmark tasks (CoLA, MRPC, SST-2, QQP, MNLI, WiC). A novel attention extraction algorithm processed attention score matrices by computing mean attention across heads, excluding special tokens, averaging subtoken weights, and selecting the most attended token per category. The authors compared attention distributions across lexical categories before and after fine-tuning, focusing on the last layer and identifying the top three layers with the highest attention to each category.

## Key Results
- Semantic tasks (SST, QQP, WiC) increase attention to content words during fine-tuning
- Syntactic tasks (CoLA, MRPC, MNLI) enhance focus on function words during fine-tuning
- Certain layers (1, 10, 11, 12) consistently prefer content words while others (2, 4, 8, 9) favor function words across all tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning causes BERT to dynamically shift attention weights toward task-relevant lexical categories.
- Mechanism: The fine-tuning process updates BERT's parameters such that attention scores become preferentially allocated to content words in semantic tasks and to function words in syntactic tasks.
- Core assumption: Lexical categories (content vs. function words) are differentially relevant to syntactic vs. semantic tasks, and the model can adjust attention accordingly during fine-tuning.
- Evidence anchors:
  - [abstract] "semantic tasks increase attention to content words while syntactic tasks enhance focus on function words"
  - [section] "When fine-tuning BERT for the CoLA task, which requires understanding of syntactic structures, there is an increase in attention devoted to function words, while content words experience a decrease"
- Break condition: If the task's lexical category distribution does not correlate with its syntactic/semantic emphasis, or if the fine-tuning objective does not directly reward such attention shifts.

### Mechanism 2
- Claim: Certain BERT layers consistently favor specific lexical categories across all tasks, indicating layer-wise lexical biases.
- Mechanism: Layers 1, 10, 11, 12 predominantly attend to content words, while layers 2, 4, 8, 9 focus more on function words, regardless of the fine-tuning task.
- Core assumption: The self-attention mechanism in BERT has inherent layer-wise specialization that is not fully overwritten by task-specific fine-tuning.
- Evidence anchors:
  - [abstract] "certain layers consistently prefer specific lexical categories regardless of task"
  - [section] "Layers 1, 10, 11, and 12 exhibit a pronounced emphasis on content words, while Layers 2, 4, 8, and 9 demonstrate a predominant focus on function words"
- Break condition: If layer-wise preferences vary significantly with different model architectures or if the fine-tuning dataset is too small to manifest consistent patterns.

### Mechanism 3
- Claim: The extracting algorithm isolates meaningful attention patterns without distorting values, enabling accurate lexical category analysis.
- Mechanism: The algorithm computes mean attention across heads, excludes special tokens, averages subtoken weights, and selects the most attended token per category, preserving the original attention structure.
- Core assumption: Averaging attention across heads and excluding special tokens yields an accurate representation of lexical category attention without introducing bias.
- Evidence anchors:
  - [section] "The proposed method aims to unravel the attention distribution at each layer within a multi-layer model...not distorting or compromising the values"
  - [abstract] The method allows "decryption of token relationships without altering attention values during the information extraction process"
- Break condition: If subtoken segmentation or averaging across heads introduces significant information loss or distortion in attention distribution.

## Foundational Learning

- Concept: Lexical category distinction (content vs. function words)
  - Why needed here: The study's hypothesis and analysis hinge on whether attention shifts are correlated with content/function word types.
  - Quick check question: What POS tags are classified as function words in this study, and why?

- Concept: Self-attention mechanism in Transformers
  - Why needed here: Understanding how attention scores are computed and distributed is essential for interpreting the study's findings.
  - Quick check question: How does the multi-head attention mechanism aggregate information from multiple attention heads?

- Concept: Fine-tuning in NLP models
  - Why needed here: The core experiment involves fine-tuning BERT for specific downstream tasks and observing changes in attention.
  - Quick check question: What changes in BERT's parameters occur during fine-tuning that could affect attention distributions?

## Architecture Onboarding

- Component map: Tokenization -> Fine-tuning -> Attention extraction -> Layer-wise analysis -> Interpretation
- Critical path: Tokenization → Fine-tuning → Attention extraction → Layer-wise analysis → Interpretation
- Design tradeoffs:
  - Using bert-base-cased vs. uncased: Preserves case information but may affect tokenization and attention.
  - Averaging attention across heads: Simplifies analysis but may lose head-specific patterns.
  - Excluding special tokens: Focuses on meaningful content but ignores potential meta-level patterns.
- Failure signatures:
  - If attention shifts are inconsistent across runs, check for random seed issues or insufficient fine-tuning epochs.
  - If layer-wise preferences are not observed, verify POS tagging accuracy and the extracting algorithm's implementation.
- First 3 experiments:
  1. Verify POS tagging: Run the NLTK POS tagger on sample sentences and confirm correct classification of function vs. content words.
  2. Test attention extraction: Apply the extracting algorithm to a pre-trained BERT model and check if attention distributions make intuitive sense (e.g., higher attention to content words in semantic tasks).
  3. Fine-tune sanity check: Fine-tune BERT on a small subset of CoLA and verify that attention to function words increases, as predicted.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do attention patterns differ across BERT model variants (e.g., BERT-large, RoBERTa) when analyzing lexical category awareness?
- Basis in paper: [inferred] The study only used BERT-base and did not compare with other model variants or architectures.
- Why unresolved: Different model sizes and pre-training objectives might lead to varying attention patterns across lexical categories.
- What evidence would resolve it: Systematic comparison of attention distributions across multiple BERT variants and alternative transformer models during fine-tuning on the same tasks.

### Open Question 2
- Question: Do attention patterns for lexical categories change differently during fine-tuning on tasks not included in GLUE (e.g., coreference resolution, named entity recognition)?
- Basis in paper: [explicit] The study only examined six GLUE benchmark tasks, acknowledging this as a limitation.
- Why unresolved: The specific tasks chosen may not capture the full range of syntactic and semantic demands that could influence attention patterns.
- What evidence would resolve it: Fine-tuning BERT on a broader range of NLP tasks and analyzing attention distributions across lexical categories for each.

### Open Question 3
- Question: How do attention patterns evolve during the fine-tuning process over time, rather than just comparing pre-trained and final fine-tuned models?
- Basis in paper: [inferred] The study only compared attention distributions between pre-trained and fully fine-tuned models, not examining the training dynamics.
- Why unresolved: The temporal evolution of attention patterns during training could reveal when and how lexical category awareness emerges.
- What evidence would resolve it: Monitoring and analyzing attention distributions at multiple checkpoints throughout the fine-tuning process.

## Limitations

- The study relies on a binary classification of POS tags into content vs. function words, which oversimplifies the nuanced roles words can play in different contexts.
- The analysis focuses on layer-wise patterns but does not account for head-level variations that could reveal more granular attention mechanisms.
- The extracting algorithm's averaging process across heads and subtokens may introduce artifacts that affect the interpretation of attention distributions.

## Confidence

- **High Confidence**: The observation that BERT models develop task-specific attention patterns aligned with lexical categories during fine-tuning is well-supported by the comparative analysis of pre- and post-fine-tuning attention distributions.
- **Medium Confidence**: The claim that certain layers consistently prefer specific lexical categories regardless of task is supported but could benefit from testing across different BERT variants and tasks to confirm robustness.
- **Low Confidence**: The mechanism by which fine-tuning induces attention shifts toward task-relevant lexical categories is theoretically plausible but lacks direct causal evidence linking parameter updates to specific attention changes.

## Next Checks

1. **Cross-Architecture Validation**: Apply the same analysis to BERT variants (BERT-large, RoBERTa, ALBERT) to determine if layer-wise lexical category preferences are consistent across architectures or specific to bert-base-cased.

2. **Head-Level Analysis**: Extend the analysis to examine attention patterns at the individual head level rather than averaging across heads, to identify whether specific heads drive the observed lexical category preferences.

3. **Intervention Study**: Conduct controlled experiments where attention weights are artificially manipulated during fine-tuning to test whether forcing attention toward specific lexical categories improves task performance, thereby establishing causal links between attention patterns and task success.