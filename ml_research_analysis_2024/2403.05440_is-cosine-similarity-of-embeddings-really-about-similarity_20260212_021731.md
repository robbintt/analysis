---
ver: rpa2
title: Is Cosine-Similarity of Embeddings Really About Similarity?
arxiv_id: '2403.05440'
source_url: https://arxiv.org/abs/2403.05440
tags:
- cosine-similarity
- embeddings
- matrix
- learned
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper shows that cosine-similarity between embeddings learned
  from regularized linear matrix factorization models can yield arbitrary or meaningless
  results. For certain objectives, the similarities are not unique and depend implicitly
  on the regularization scheme.
---

# Is Cosine-Similarity of Embeddings Really About Similarity?

## Quick Facts
- **arXiv ID:** 2403.05440
- **Source URL:** https://arxiv.org/abs/2403.05440
- **Reference count:** 8
- **Primary result:** Cosine-similarity in regularized linear matrix factorization can be arbitrary or meaningless depending on the training objective and regularization scheme.

## Executive Summary
This paper demonstrates that cosine-similarity between embeddings learned from regularized linear matrix factorization models can yield arbitrary results. The authors show that when training with denoising objectives like dropout, the solution becomes invariant to column rescalings of the embeddings, leading to meaningless cosine-similarity values. In contrast, weight decay regularization produces unique embeddings but doesn't necessarily optimize for cosine-similarity. Through experiments on simulated data, the paper illustrates how different training objectives and regularization choices can drastically affect the cosine-similarity structure, even when the underlying dot-product is fixed. The findings caution against blindly using cosine-similarity and suggest alternatives like training directly for cosine-similarity or projecting embeddings back to the original space before applying similarity measures.

## Method Summary
The authors analyze regularized linear matrix factorization models, examining how different training objectives and regularization schemes affect the properties of learned embeddings. They theoretically prove that denoising objectives (like dropout) lead to solutions that are invariant to column rescalings, making cosine-similarity arbitrary. For weight decay regularization, they show embeddings are unique but not necessarily optimized for cosine-similarity. The theoretical analysis is complemented by experiments on simulated data that demonstrate how different training regimes produce vastly different similarity structures despite maintaining the same dot-product values. The paper compares these findings against the assumption that cosine-similarity meaningfully captures similarity in embedding spaces.

## Key Results
- Cosine-similarity can be arbitrary when training with denoising objectives due to invariance to column rescalings
- Weight decay regularization yields unique embeddings but doesn't optimize for cosine-similarity
- Different training objectives can produce vastly different similarity structures even with identical dot-products
- Blind use of cosine-similarity in embedding spaces may lead to misleading results

## Why This Works (Mechanism)
The core mechanism stems from the mathematical properties of regularized linear matrix factorization. When using denoising objectives, the optimization problem becomes invariant to column-wise rescalings of the embedding matrices, meaning multiple solutions with different cosine-similarity structures can achieve the same objective value. This invariance arises because denoising objectives like dropout focus on reconstructing the original data rather than constraining the geometry of the embedding space. In contrast, weight decay regularization imposes constraints that make the solution unique but doesn't explicitly optimize for the angular relationships captured by cosine-similarity. The paper shows that cosine-similarity is only meaningful when the training objective explicitly accounts for the normalization inherent in the cosine measure.

## Foundational Learning

**Matrix Factorization** - Decomposing a matrix into lower-dimensional components
*Why needed:* The paper's analysis centers on linear matrix factorization models
*Quick check:* Can you explain how X ≈ UV^T represents a factorization?

**Regularization in Optimization** - Techniques to prevent overfitting by adding penalty terms
*Why needed:* Different regularization schemes (weight decay vs. dropout) drive the core findings
*Quick check:* What's the difference between L2 regularization and dropout in terms of their effect on solutions?

**Cosine Similarity** - Measure of angular difference between vectors: cos(θ) = (a·b)/(||a|| ||b||)
*Why needed:* The paper questions whether this measure is meaningful for embeddings
*Quick check:* Calculate cosine similarity between [1,0] and [0,1] versus [2,0] and [0,2]

**Denoising Objectives** - Training approaches that reconstruct clean data from corrupted versions
*Why needed:* Show how these objectives lead to arbitrary cosine similarities
*Quick check:* How does dropout during training affect the optimization landscape?

**Column Rescaling Invariance** - Property where solutions remain optimal under certain transformations
*Why needed:* Explains why cosine similarity becomes arbitrary with denoising
*Quick check:* If U is a solution, why is diag(α)U also a solution when using dropout?

## Architecture Onboarding

**Component Map:** Data Matrix -> Matrix Factorization Model -> Embeddings -> Cosine Similarity Computation -> Similarity-based Tasks

**Critical Path:** The theoretical analysis establishes that regularization choice → solution uniqueness → cosine similarity properties. Experiments validate this by showing different regularization → different similarity structures despite same dot-products.

**Design Tradeoffs:** Using denoising objectives trades off meaningful cosine similarity for better reconstruction, while weight decay trades off potentially better similarity structure for unique solutions. Direct optimization for cosine similarity trades off reconstruction quality for better angular relationships.

**Failure Signatures:** Arbitrary or inconsistent cosine similarities across different runs with same data, dramatic changes in nearest neighbors when switching regularization schemes, or high reconstruction accuracy paired with meaningless similarity rankings.

**First Experiments:**
1. Train linear matrix factorization with dropout vs. weight decay on synthetic data and compare resulting cosine similarity structures
2. Verify that column rescaling leaves the objective value unchanged under dropout regularization
3. Test whether projecting embeddings back to original space before computing similarities resolves the arbitrariness issue

## Open Questions the Paper Calls Out
The paper acknowledges that its findings are based on linear matrix factorization models and synthetic data, raising questions about applicability to non-linear deep learning embeddings commonly used in practice. It also notes that while alternatives are suggested (direct cosine training or projection methods), these lack extensive empirical validation in real-world scenarios. The paper doesn't address how these findings might affect downstream tasks that rely on cosine similarity, such as retrieval systems or clustering algorithms in embedding spaces.

## Limitations
- Analysis limited to linear matrix factorization models, not deep learning embeddings
- Experiments conducted on simulated rather than real-world data
- Limited empirical validation of proposed alternatives
- Doesn't investigate impact on downstream tasks using cosine similarity

## Confidence
**High:** The core theoretical claim that cosine-similarity can be arbitrary under denoising objectives in regularized linear matrix factorization is mathematically sound and well-proven.

**Medium:** The experimental demonstrations showing how different regularization choices affect similarity structures are convincing but limited to synthetic data settings.

**Low:** The practical recommendations about alternatives (direct cosine training or projection methods) are largely theoretical and lack comprehensive real-world validation.

## Next Checks
1. Test whether similar cosine-similarity ambiguities arise in non-linear deep learning embeddings from transformer models
2. Evaluate the proposed alternatives (direct cosine training or projection methods) on benchmark retrieval tasks using real-world datasets
3. Investigate whether the identified issues affect downstream tasks that rely on cosine-similarity, such as nearest neighbor search or clustering in embedding spaces