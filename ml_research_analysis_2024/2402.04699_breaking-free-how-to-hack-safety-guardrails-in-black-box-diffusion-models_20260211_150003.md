---
ver: rpa2
title: 'Breaking Free: How to Hack Safety Guardrails in Black-Box Diffusion Models!'
arxiv_id: '2402.04699'
source_url: https://arxiv.org/abs/2402.04699
tags:
- adversarial
- images
- diffusion
- evoseed
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "EvoSeed is a black-box framework that uses CMA-ES to find adversarial\
  \ initial seed vectors for diffusion models, producing natural adversarial images\
  \ that maintain photorealism while deceiving classifiers. It operates with auxiliary\
  \ conditional diffusion and classifier models, searching within an L\u221E constraint\
  \ around random seed vectors."
---

# Breaking Free: How to Hack Safety Guardrails in Black-Box Diffusion Models!

## Quick Facts
- arXiv ID: 2402.04699
- Source URL: https://arxiv.org/abs/2402.04699
- Reference count: 40
- Key outcome: EvoSeed uses CMA-ES to find adversarial seeds in black-box diffusion models, achieving over 92% attack success on CIFAR-10-like data with maintained photorealism.

## Executive Summary
EvoSeed is a novel black-box framework for generating adversarial images from diffusion models without access to their internal parameters. It leverages CMA-ES to search for seed vectors that, when fed into a conditional diffusion model, produce images that fool safety classifiers while remaining visually indistinguishable from benign images. The approach sidesteps the need for white-box access by optimizing only the seed vector within an L∞ constraint, using auxiliary models for guidance. Results show high attack success rates and strong image fidelity, outperforming random search and matching white-box methods like SD-NAE.

## Method Summary
EvoSeed operates by optimizing initial seed vectors for a conditional diffusion model using CMA-ES, an evolutionary strategy well-suited for black-box optimization. The framework employs an auxiliary classifier to guide the search toward adversarial seeds, iterating within an L∞ constraint to ensure perturbations remain imperceptible. Generated images are evaluated for both attack success (misclassification by safety classifiers) and photorealism (via FID and human perception checks). The method is designed to be unrestricted, requiring no changes to the diffusion model's distribution and functioning without white-box access.

## Key Results
- Over 92% attack success rate on CIFAR-10-like datasets with EvoSeed, outperforming random search and matching white-box approaches.
- Generated adversarial images maintain high photorealism, as evidenced by low FID scores and qualitative human assessments.
- Transferability tests show L2 robust classifiers are most vulnerable to EvoSeed-generated samples, though results vary by architecture.

## Why This Works (Mechanism)
EvoSeed exploits the latent space of diffusion models by searching for seed vectors that, when diffused, yield images misclassified by safety classifiers. The evolutionary search via CMA-ES efficiently navigates this high-dimensional space, guided by an auxiliary classifier, to find adversarial seeds without requiring gradient access to the target model. This black-box approach leverages the inherent robustness and photorealism of diffusion models to produce unrestricted adversarial samples.

## Foundational Learning
- **CMA-ES (Covariance Matrix Adaptation Evolution Strategy)**: An evolutionary algorithm for black-box optimization, needed for searching high-dimensional latent spaces without gradients. Quick check: Can handle non-convex, noisy objectives and adapts search distribution over iterations.
- **Conditional Diffusion Models**: Generative models that produce images conditioned on class labels or other metadata, needed as the backbone for controllable image synthesis. Quick check: Ability to steer image generation toward desired classes via conditioning.
- **L∞ Norm Constraint**: Limits the maximum change to any pixel/channel in the seed vector, needed to keep adversarial perturbations imperceptible. Quick check: Ensures perturbations do not exceed a specified bound in any dimension.
- **Transferability of Adversarial Examples**: The phenomenon where adversarial samples crafted for one model also fool others, needed to assess robustness across architectures. Quick check: Evaluate attack success on multiple, independently trained classifiers.

## Architecture Onboarding
- **Component Map**: Auxiliary Classifier -> CMA-ES Optimizer -> Seed Vector -> Conditional Diffusion Model -> Generated Image -> Safety Classifier
- **Critical Path**: Seed vector optimization (CMA-ES) guided by auxiliary classifier → conditional diffusion → image generation → classifier evaluation
- **Design Tradeoffs**: Black-box optimization avoids white-box access but is computationally heavier; reliance on auxiliary models introduces potential brittleness.
- **Failure Signatures**: High FID scores or classifier accuracy near baseline suggest optimization failure or seed space saturation.
- **First Experiments**: 1) Test CMA-ES convergence on synthetic seed spaces; 2) Validate auxiliary classifier guidance with known vulnerabilities; 3) Measure attack success on a held-out safety classifier.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to CIFAR-10-like datasets; real-world, high-resolution image performance unclear.
- Reliance on auxiliary models introduces overhead and potential brittleness in deployment.
- No exploration of defenses or detection mechanisms against such attacks.

## Confidence
- High confidence in technical feasibility, demonstrated by controlled experiments and quantitative metrics (attack success rate, FID scores).
- Medium confidence in real-world applicability due to limited dataset diversity and absence of adversarial defense evaluations.
- Medium confidence in transferability findings, as results depend on specific classifier architectures.
- Low confidence in scalability to higher-resolution images or complex domains without further empirical validation.

## Next Checks
1. Test EvoSeed on high-resolution datasets (e.g., ImageNet) and diverse safety classifiers to assess scalability and robustness.
2. Evaluate the impact of EvoSeed-generated samples on adversarial training and model hardening techniques.
3. Measure computational efficiency and optimization convergence for larger latent spaces and real-time attack scenarios.