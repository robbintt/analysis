---
ver: rpa2
title: Deep UAV Path Planning with Assured Connectivity in Dense Urban Setting
arxiv_id: '2406.15225'
source_url: https://arxiv.org/abs/2406.15225
tags:
- flight
- dupac
- path
- rsrp
- distance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes DUPAC, a Deep Reinforcement Learning (DRL)
  framework for UAV path planning with assured connectivity in dense urban environments.
  DUPAC balances flight distance and signal quality by learning optimal routes that
  minimize distance while maintaining stable communication links with cellular base
  stations.
---

# Deep UAV Path Planning with Assured Connectivity in Dense Urban Setting

## Quick Facts
- arXiv ID: 2406.15225
- Source URL: https://arxiv.org/abs/2406.15225
- Authors: Jiyong Oh; Syed M. Raza; Lusungu J. Mwasinga; Moonseong Kim; Hyunseung Choo
- Reference count: 15
- Key outcome: DUPAC achieves 7.5% better RSRP than baseline with only 2% increased distance in NYC urban simulation

## Executive Summary
This study proposes DUPAC, a Deep Reinforcement Learning (DRL) framework for UAV path planning with assured connectivity in dense urban environments. The framework addresses the challenge of balancing flight distance and signal quality by learning optimal routes that minimize distance while maintaining stable communication links with cellular base stations. Using a Unity-simulated 3D urban environment of New York City with 30 base stations, DUPAC demonstrates improved communication reliability with minimal distance trade-off through differentiated reward functions based on signal quality ranges.

## Method Summary
DUPAC uses Proximal Policy Optimization (PPO) with continuous action space for precise UAV maneuvering in 3D urban environments. The method incorporates RSRP-based rewards with differentiated weights depending on signal quality ranges (-80 ≤ RSRP, -100 ≤ RSRP < -80, RSRP < -100) to prioritize connectivity when needed while optimizing path length. The state vector includes UAV position, velocity, obstacle distance/direction, and associated base station ID. Training uses generalized advantage estimation (GAE) for stable policy updates in this dynamic environment.

## Key Results
- Achieves 7.5% better RSRP than baseline method while only increasing flight distance by 2%
- Maintains excellent RSRP for 9% longer periods during UAV flights compared to baseline
- Demonstrates effective balance between path optimization and communication reliability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DUPAC learns to balance flight distance and RSRP by using differentiated rewards based on signal quality ranges.
- Mechanism: The reward function assigns different weight combinations (µ1, µ2, µ3) depending on whether RSRP is excellent (-80 ≤ RSRP), mediocre (-100 ≤ RSRP < -80), or poor (RSRP < -100). This enables the agent to prioritize signal quality when RSRP drops below -80 dB, while still optimizing for shorter paths when signal is good.
- Core assumption: The agent can learn to trade off path length and connectivity by adjusting its policy according to RSRP-based reward shaping.
- Evidence anchors: Abstract states "DUPAC achieves an autonomous UAV flight path similar to base method with only 2% increment while maintaining an average 9% better connection quality throughout the flight." Section III defines the reward function with differentiated weights for RSRP ranges.

### Mechanism 2
- Claim: Continuous action space enables more precise UAV maneuvering compared to discrete grid-based methods.
- Mechanism: Instead of selecting from predefined grid points, DUPAC generates continuous 3D movements by adding values in [-1,1] to current coordinates. This allows fine-grained control and reduces collision risk in dense urban environments.
- Core assumption: Continuous actions provide sufficient granularity to navigate complex 3D urban environments without excessive computational overhead.
- Evidence anchors: Section III specifies that "For the 3D movement, new values for xt+1, yt+1, and zt+1 are calculated by adding values between [-1,1] to xt, yt, and zt." Abstract notes that discrete methods "sacrifices the precise maneuvering of a UAV in a dense urban environment and increases risk of collisions."

### Mechanism 3
- Claim: PPO-based training with generalized advantage estimation (GAE) improves policy stability and convergence.
- Mechanism: PPO uses a clipped probability ratio to limit policy updates, while GAE balances bias and variance in advantage estimation. This combination reduces training instability and improves learning efficiency in the dynamic UAV environment.
- Core assumption: The clipped PPO objective and GAE provide stable learning signals in environments with sparse and delayed rewards.
- Evidence anchors: Section III describes "PPO uses parameter ϵ and clip function for curtailing the variance of estimated rewards and gradually updating the policy network" and "modified advantage value function vGAE which operates as a Generalized Advantage Estimator (GAE)." Abstract states the agent "calculates the reward rt based on st+1 resulted from at performed in st" and aims to "maximize the accumulative discounted reward over all time steps."

## Foundational Learning

- Concept: Markov Decision Process (MDP)
  - Why needed here: The UAV path planning problem is formulated as an MDP where the agent observes states, takes actions, and receives rewards to learn optimal policies.
  - Quick check question: What are the four components of an MDP, and which ones are explicitly defined in the DUPAC framework?

- Concept: Antenna radiation patterns and path loss models
  - Why needed here: Understanding how RSRP is calculated from base station antenna gain and path loss is crucial for interpreting the reward function and performance metrics.
  - Quick check question: How do the urban micro and macro models differ in terms of antenna height and path loss calculation?

- Concept: Deep Reinforcement Learning algorithms (PPO vs DQN)
  - Why needed here: Knowing the differences between on-policy (PPO) and off-policy (DQN) methods helps explain why PPO was chosen for this continuous control problem.
  - Quick check question: What are the key differences between PPO and DQN in terms of policy updates and action space handling?

## Architecture Onboarding

- Component map: Unity 3D simulation environment with NYC urban model -> 30 base stations with antenna radiation patterns -> UAV agent with continuous action space -> PPO neural network (policy and value networks) -> Reward calculation module with differentiated RSRP weights -> State vector: UAV position, velocity, obstacle distance, direction, associated BS ID

- Critical path: Unity environment generates state st (UAV position, RSRP, obstacles) -> PPO agent selects action at (continuous movement + BS handover) -> Environment executes action, transitions to st+1 -> Reward rt calculated based on distance change and RSRP -> PPO updates policy/value networks using GAE and clipped objective -> Repeat until destination reached or timeout

- Design tradeoffs: Continuous vs discrete actions (continuous provides precision but requires more complex policy networks), RSRP-based rewards (improves connectivity but may increase path length), PPO vs other RL algorithms (PPO offers stability but may converge slower than DQN)

- Failure signatures: UAV gets stuck in local minima (poor path planning), excessive handovers between base stations (unstable connectivity), policy collapse (training instability), overshooting targets (action bounds too large)

- First 3 experiments: 1) Validate basic navigation: Run UAV from source to destination with minimal obstacles and verify it reaches the target, 2) Test reward sensitivity: Adjust µ1, µ2, µ3 weights and observe changes in path length vs RSRP performance, 3) Compare action spaces: Implement discrete actions as baseline and measure collision rates and path precision

## Open Questions the Paper Calls Out

- Question: How does DUPAC perform under varying urban environments with different building densities and heights?
  - Basis in paper: [explicit] The paper mentions "in-depth evaluation of DUPAC for diverse urban environments is on-going to be included in the extended article."
  - Why unresolved: The current evaluation is limited to a single 3D dense urban environment of New York City around Times Square, and results for other urban environments are not yet available.
  - What evidence would resolve it: Performance results of DUPAC in multiple urban environments with varying building densities and heights, demonstrating its generalization capability.

- Question: What is the impact of different antenna configurations on the RSRP and UAV path planning?
  - Basis in paper: [inferred] The paper uses a specific antenna model with 8 elements vertically arranged, but does not explore the impact of varying antenna configurations.
  - Why unresolved: The paper does not evaluate the effect of different antenna configurations on the RSRP and UAV path planning.
  - What evidence would resolve it: Comparative results showing the impact of different antenna configurations on RSRP and UAV path planning, demonstrating the sensitivity of DUPAC to antenna settings.

- Question: How does the handover reduction functionality affect the overall performance of DUPAC?
  - Basis in paper: [explicit] The paper mentions that "we are adding handover reduction functionality in DUPAC to minimize connectivity disruptions."
  - Why unresolved: The handover reduction functionality is mentioned as a future addition, but its impact on the overall performance of DUPAC is not yet evaluated.
  - What evidence would resolve it: Performance results of DUPAC with and without the handover reduction functionality, showing the impact on connectivity stability and flight distance.

## Limitations

- Evaluation limited to single urban scenario (NYC simulation) with 30 base stations, limiting generalizability
- Specific hyperparameter tuning process for reward weights and PPO parameters remains unclear
- Does not address edge cases such as extreme weather conditions, dynamic urban environments, or UAV failures

## Confidence

- High Confidence: The core mechanism of using differentiated RSRP-based rewards to balance path length and connectivity is well-supported by the mathematical formulation and performance metrics
- Medium Confidence: The claim that continuous action space provides superior precision compared to discrete methods is supported by the theoretical framework, but lacks direct empirical comparison
- Medium Confidence: The assertion that PPO with GAE provides stable learning is reasonable given the algorithm's known properties, but the study does not provide detailed training curves or convergence analysis

## Next Checks

1. **Reward Sensitivity Analysis**: Systematically vary the reward weights (µ1, µ2, µ3) across a wider range and measure their impact on the trade-off between path length and RSRP performance to identify optimal tuning strategies

2. **Cross-Environment Generalization**: Evaluate DUPAC in multiple urban environments with varying densities of buildings and base station configurations to test robustness and identify failure modes in different scenarios

3. **Real-World Deployment Readiness**: Conduct field tests with actual UAVs in controlled urban environments to validate simulation results and identify practical challenges not captured in the Unity 3D model