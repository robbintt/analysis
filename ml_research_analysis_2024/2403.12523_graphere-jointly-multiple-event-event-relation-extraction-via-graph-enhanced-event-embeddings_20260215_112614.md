---
ver: rpa2
title: 'GraphERE: Jointly Multiple Event-Event Relation Extraction via Graph-Enhanced
  Event Embeddings'
arxiv_id: '2403.12523'
source_url: https://arxiv.org/abs/2403.12523
tags:
- event
- graph
- relation
- embeddings
- graphs
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of extracting multiple types of
  event-event relations (Coreference, Temporal, Causal, and Subevent) in natural language
  documents. The key innovation is the GraphERE framework, which enriches event embeddings
  by incorporating event argument and structure features using static AMR and IE graphs.
---

# GraphERE: Jointly Multiple Event-Event Relation Extraction via Graph-Enhanced Event Embeddings

## Quick Facts
- arXiv ID: 2403.12523
- Source URL: https://arxiv.org/abs/2403.12523
- Authors: Haochen Li; Di Geng
- Reference count: 14
- Key outcome: GraphERE achieves 0.26% to 3.57% F1 score improvements across four event relation types on MAVEN-ERE

## Executive Summary
This paper addresses the challenge of extracting multiple types of event-event relations (Coreference, Temporal, Causal, and Subevent) in natural language documents. The GraphERE framework innovatively enriches event embeddings by incorporating event argument and structure features from static AMR and IE graphs. To jointly extract multiple relations, GraphERE employs task-specific dynamic event graphs that learn optimal edge structures for each relation type. Experiments demonstrate significant performance improvements over existing methods, with ablation studies validating the effectiveness of both graph-enhanced embeddings and the joint extraction strategy.

## Method Summary
GraphERE enhances event embeddings by constructing static event graphs from AMR and IE parsers, which capture event arguments and structural relationships. These static graphs are encoded using Graph Attention Networks to produce graph-enhanced event representations. A Node Transformer module with multi-head attention processes event interactions, followed by task-specific dynamic graph construction where similarity metrics weighted by relation-specific vectors determine edge weights. Graph sparsification with relation-specific thresholds removes noisy edges, and GCNs refine node embeddings within each dynamic graph. The framework uses multi-task learning with four classifiers trained on these relation-specific dynamic graphs.

## Key Results
- GraphERE outperforms state-of-the-art methods on MAVEN-ERE dataset
- Achieves F1 score improvements ranging from 0.26% to 3.57% across all four relation types
- Ablation studies confirm effectiveness of graph-enhanced embeddings and joint extraction strategy
- Particularly strong performance gains on Causal (3.57%) and Subevent (2.38%) relations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Graph-enhanced event embeddings improve precision by incorporating event argument and structure information that token-level embeddings miss.
- Mechanism: Static AMR and IE graphs provide nodes for event triggers and arguments, with edges encoding subordination relationships. Graph Attention Networks aggregate neighbor information to enrich event representations.
- Core assumption: Event arguments and their structural relationships contain discriminative features for relation extraction not captured by trigger-only embeddings.
- Evidence anchors:
  - [abstract] "we enrich the event embeddings with event argument and structure features by using static AMR graphs and IE graphs"
  - [section] "Adding the features of event argument and structure to event embeddings can better encode events, benefiting the downstream event relation extraction."
  - [corpus] Weak evidence - corpus contains no citation count data for validation.

### Mechanism 2
- Claim: Task-specific dynamic graphs enable better modeling of relation interactions by learning optimal edge structures for each relation type.
- Mechanism: Initial fully-connected graphs are constructed using similarity metrics weighted by relation-specific vectors. Graph sparsification with relation-specific thresholds removes noisy edges, and GCNs refine node embeddings within each dynamic graph.
- Core assumption: Different event relation types benefit from distinct graph structures optimized through training rather than shared static structures.
- Evidence anchors:
  - [abstract] "we use Node Transformer and construct Task-specific Dynamic Event Graphs for each type of relation"
  - [section] "Unlike static event graphs, which are fixed, the connection of dynamic graphs will change concerning the event embeddings."
  - [corpus] Weak evidence - corpus contains no citation count data for validation.

### Mechanism 3
- Claim: Joint multi-task learning with task-specific dynamic graphs outperforms shared-embedding approaches by preventing interference between relation types.
- Mechanism: Four classifiers operate on relation-specific dynamic graphs rather than shared event embeddings, allowing each task to optimize its own graph structure without interfering with others.
- Core assumption: Different event relations have distinct feature requirements that are better served by specialized graph structures than by shared embeddings optimized for all tasks.
- Evidence anchors:
  - [abstract] "we use a multi-task learning strategy to train the whole framework"
  - [section] "In contrast, GraphEREjoint uses task-specific dynamic graphs for joint training. The dynamic graphs for each event relation are optimized through training, thus preventing the event embeddings from being interfered by a particular relation."
  - [corpus] Weak evidence - corpus contains no citation count data for validation.

## Foundational Learning

- Concept: Graph Attention Networks
  - Why needed here: To aggregate information from event arguments and structural relationships encoded in static graphs
  - Quick check question: How does GAT compute attention coefficients between connected nodes in the event-argument graph?

- Concept: Multi-head Attention (Transformer)
  - Why needed here: To capture diverse semantic aspects of event-event interactions across different relation types
  - Quick check question: What is the purpose of using multiple attention heads in the Node Transformer module?

- Concept: Graph Convolutional Networks
  - Why needed here: To refine event embeddings within task-specific dynamic graphs by propagating information through learned edge structures
  - Quick check question: How does GCN update node representations based on neighbor information in the dynamic graph?

## Architecture Onboarding

- Component map: Sequence Encoder (RoBERTa) → Static Graph Encoder (GAT on AMR/IE) → Node Transformer (Multi-head Attention) → Dynamic Graph Construction → Task-specific GCNs → Relation Classifiers → Multi-task Loss
- Critical path: Document → RoBERTa embeddings → Static graphs → Graph-enhanced embeddings → Node Transformer → Dynamic graphs → GCN refinement → Classification
- Design tradeoffs: Static graphs provide prior knowledge but may contain noise; dynamic graphs adapt to task needs but require more parameters and data; joint training captures interactions but risks interference
- Failure signatures: Poor performance on specific relations suggests static graph extraction issues; degradation in causal/subevent relations indicates dynamic graph optimization problems; inconsistent improvements across relation types suggest joint training interference
- First 3 experiments:
  1. Train with static graphs only (no dynamic graphs) to measure impact of graph-enhanced embeddings
  2. Train with shared embeddings across all relations (no task-specific dynamic graphs) to validate joint training benefits
  3. Vary β mixing ratio between AMR and IE graphs to find optimal balance of structural vs entity information

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the GraphERE framework perform on event-event relation extraction tasks beyond the four types (Coreference, Temporal, Causal, and Subevent) covered in the paper?
- Basis in paper: [inferred] The paper focuses on four event relation types but does not explore the framework's generalizability to other relation types.
- Why unresolved: The paper does not provide any experiments or discussions on the framework's performance with other event relation types.
- What evidence would resolve it: Conducting experiments on additional event relation types and comparing the results with baseline methods would provide evidence for the framework's generalizability.

### Open Question 2
- Question: How would the performance of GraphERE be affected by using different NLP tools for extracting event arguments and constructing static event graphs?
- Basis in paper: [explicit] The paper uses AMR and OpenIE parsers for constructing static event graphs but does not explore the impact of using different parsers or tools.
- Why unresolved: The paper does not provide any experiments or discussions on the impact of using different NLP tools on the framework's performance.
- What evidence would resolve it: Conducting experiments with different NLP tools for extracting event arguments and constructing static event graphs, and comparing the results with the current approach, would provide evidence for the impact of different tools on the framework's performance.

### Open Question 3
- Question: How would the performance of GraphERE be affected by incorporating additional features or information beyond event arguments and structure, such as event types or contextual information?
- Basis in paper: [inferred] The paper focuses on incorporating event argument and structure features but does not explore the impact of additional features or information on the framework's performance.
- Why unresolved: The paper does not provide any experiments or discussions on the impact of additional features or information on the framework's performance.
- What evidence would resolve it: Conducting experiments with additional features or information beyond event arguments and structure, and comparing the results with the current approach, would provide evidence for the impact of additional features on the framework's performance.

## Limitations

- Performance heavily dependent on quality of AMR and IE parsers for static graph construction
- Limited training data for Causal and Subevent relations (1,010 and 1,185 examples) may lead to overfitting
- Computational overhead of dynamic graph construction not reported, raising efficiency concerns

## Confidence

**High Confidence**: Core architectural components clearly specified, substantial consistent improvements across all relation types

**Medium Confidence**: Graph-enhanced embeddings effectiveness supported by ablation studies, but optimal AMR/IE mixing ratio lacks systematic exploration

**Low Confidence**: Generalization to other datasets/domains uncertain, robustness to parser failures not tested

## Next Checks

1. **Parser Robustness Test**: Systematically corrupt AMR/IE graph inputs with varying noise levels to measure framework sensitivity to graph extraction failures

2. **Data Efficiency Analysis**: Train GraphERE with progressively smaller subsets of training data for each relation type to assess whether improvements stem from better feature learning or overfitting limited data

3. **Cross-Dataset Generalization**: Evaluate GraphERE on alternative event relation extraction benchmarks (TimeBank, Causal-TimeBank) to verify transfer across different annotation schemas and domains