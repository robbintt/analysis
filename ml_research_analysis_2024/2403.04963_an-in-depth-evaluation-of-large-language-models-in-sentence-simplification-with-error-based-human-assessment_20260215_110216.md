---
ver: rpa2
title: An In-depth Evaluation of Large Language Models in Sentence Simplification
  with Error-based Human Assessment
arxiv_id: '2403.04963'
source_url: https://arxiv.org/abs/2403.04963
tags:
- simplification
- sentence
- gpt-4
- evaluation
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the sentence simplification capabilities of
  large language models (LLMs), focusing on GPT-4, using a novel error-based human
  evaluation framework. The framework identifies and categorizes key errors in LLM-generated
  simplifications, including lack of simplicity, altered meaning, coreference issues,
  repetition, and hallucination.
---

# An In-depth Evaluation of Large Language Models in Sentence Simplification with Error-based Human Assessment

## Quick Facts
- arXiv ID: 2403.04963
- Source URL: https://arxiv.org/abs/2403.04963
- Authors: Xuanxin Wu; Yuki Arase
- Reference count: 40
- Key outcome: GPT-4 generates fewer erroneous outputs compared to Control-T5 in sentence simplification, but struggles with lexical paraphrasing

## Executive Summary
This study evaluates the sentence simplification capabilities of large language models, particularly GPT-4, using a novel error-based human evaluation framework. The framework identifies and categorizes key errors in LLM-generated simplifications, including lack of simplicity, altered meaning, coreference issues, repetition, and hallucination. Results show that GPT-4 outperforms the state-of-the-art supervised model, Control-T5, by generating fewer erroneous outputs and better preserving original meaning. However, GPT-4 struggles with lexical paraphrasing. The study also reveals that current automatic metrics lack sensitivity to assess the high-quality simplifications generated by GPT-4.

## Method Summary
The study employs an error-based human evaluation framework to assess GPT-4's sentence simplification capabilities. The framework identifies specific error types in generated simplifications through human annotation. GPT-4 is optimized for the task using dataset-specific prompt engineering, and its performance is compared against Control-T5, a state-of-the-art supervised model. The evaluation is conducted on three English datasets: Turk, ASSET, and Newsela. Additionally, a meta-evaluation of widely used automatic metrics is performed using human annotations to assess their effectiveness in evaluating high-quality LLM-generated simplifications.

## Key Results
- GPT-4 generates fewer erroneous outputs compared to Control-T5 across all datasets
- GPT-4 better preserves the original meaning in simplifications than Control-T5
- GPT-4 struggles with lexical paraphrasing, producing less diverse vocabulary in simplifications
- Automatic metrics lack sensitivity to assess the high-quality simplifications generated by GPT-4

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Error-based human evaluation framework provides clearer interpretability than conventional rating-based approaches while maintaining reliability.
- Mechanism: By focusing on identifying specific error types rather than overall quality ratings, the framework enables more granular analysis of model failures and reduces ambiguity in assessments.
- Core assumption: Annotators can consistently identify and categorize errors when given clear definitions and examples.
- Evidence anchors:
  - [abstract] "our error-based human evaluation framework to identify key failures in important aspects of sentence simplification"
  - [section 5.1.2] "Our approach aligns closely with human intuition by focusing on outcome-based assessments rather than linguistic details"
  - [corpus] Weak - the paper reports inter-annotator agreement but does not provide correlation with gold standard error annotations
- Break Condition: If error definitions remain ambiguous or annotators show inconsistent agreement across sessions

### Mechanism 2
- Claim: GPT-4 with prompt engineering outperforms Control-T5 in sentence simplification quality.
- Mechanism: Through dataset-specific instructions and example selection, GPT-4 can generate fewer erroneous outputs and better preserve meaning compared to supervised models.
- Core assumption: Prompt engineering can effectively leverage GPT-4's capabilities for task-specific performance.
- Evidence anchors:
  - [abstract] "GPT-4 generates fewer erroneous outputs compared to the state-of-the-art supervised model, Control-T5"
  - [section 6.2] "GPT-4 again consistently outperforms Control-T5 across all datasets"
  - [corpus] Moderate - the paper provides quantitative comparisons but relies on human evaluation data which may have bias
- Break Condition: If prompt engineering fails to account for dataset-specific simplification strategies

### Mechanism 3
- Claim: Current automatic metrics lack sensitivity to assess high-quality simplifications generated by advanced LLMs.
- Mechanism: Metrics like LENS and BERTScore can differentiate significant quality differences but fail to capture subtle improvements in already high-quality outputs.
- Core assumption: The evaluation metrics are not designed to handle the generally high performance of advanced LLMs.
- Evidence anchors:
  - [abstract] "while these metrics are effective for significant quality differences, they lack sufficient sensitivity to assess the overall high-quality simplification by GPT-4"
  - [section 7.2.2] "while these metrics can detect significant quality variations, they fall short when overall quality is high"
  - [corpus] Strong - the paper provides point-biserial correlation analysis comparing metrics scores with human evaluations
- Break Condition: If new metrics are developed specifically for LLM-generated outputs

## Foundational Learning

- Concept: Error-based evaluation frameworks
  - Why needed here: Traditional rating-based approaches are too superficial for advanced LLM evaluation and may miss subtle quality issues
  - Quick check question: What are the advantages of identifying specific error types versus providing overall quality ratings?

- Concept: Prompt engineering for task-specific performance
  - Why needed here: LLMs like GPT-4 are sensitive to prompt variations and can be optimized for specific tasks through careful prompt design
  - Quick check question: How do dataset-specific instructions and example selection impact GPT-4's simplification outputs?

- Concept: Meta-evaluation of automatic metrics
  - Why needed here: To assess whether existing metrics can adequately evaluate the high-quality outputs generated by advanced LLMs
  - Quick check question: What are the limitations of current automatic metrics in evaluating LLM-generated simplifications?

## Architecture Onboarding

- Component map: Error-based human evaluation framework (Task 1) + Likert scale rating (Task 2) + Automatic metrics meta-evaluation
- Critical path: Prompt engineering → GPT-4 simplification generation → Error identification → Likert scale rating → Automatic metrics calculation → Analysis
- Design tradeoffs: Detailed error categorization vs. annotator agreement consistency; comprehensive error coverage vs. annotation complexity
- Failure signatures: Low inter-annotator agreement; metrics scores not aligning with human evaluation; GPT-4 struggling with specific simplification strategies
- First 3 experiments:
  1. Test prompt variations on validation sets to identify optimal configuration for each dataset
  2. Conduct pilot annotation with sample outputs to refine error definitions and guidelines
  3. Compare automatic metrics scores with human evaluations to identify sensitivity gaps

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the error-based human evaluation framework perform when applied to other large language models beyond GPT-4 and Control-T5?
- Basis in paper: [explicit] The authors mention that their error-based framework could be applied to "both closed-source and open-source LLMs" but only evaluated GPT-4 and Control-T5 specifically
- Why unresolved: The study only evaluated two specific models, so the framework's generalizability to other LLMs remains untested
- What evidence would resolve it: Applying the framework to evaluate additional LLMs like Claude, Gemini, or other transformer models and comparing error patterns across models

### Open Question 2
- Question: Would the error patterns identified in the study differ if evaluated by native English speakers rather than second-language learners with advanced proficiency?
- Basis in paper: [inferred] The authors chose second-language learners as annotators expecting they would be "more sensitive to variations in textual difficulty" but this assumption wasn't validated
- Why unresolved: The study used non-native speakers as annotators without comparing their evaluations to native speaker assessments
- What evidence would resolve it: Conducting parallel evaluations with both native and non-native speakers and comparing inter-annotator agreement and error identification patterns

### Open Question 3
- Question: How would the evaluation results change if the error-based framework included additional error categories or refined the existing ones?
- Basis in paper: [explicit] The authors state they "refined the categories for altered meaning and lack of simplicity by looking into the specific types of changes they involve" suggesting potential for further refinement
- Why unresolved: The framework was developed through preliminary investigation but not iteratively refined based on annotation results
- What evidence would resolve it: Conducting additional rounds of annotation to identify patterns in error categorization, then refining the framework and reassessing model performance

### Open Question 4
- Question: Would automatic metrics show improved sensitivity to high-quality simplifications if they were trained or calibrated specifically on LLM-generated outputs?
- Basis in paper: [inferred] The meta-evaluation found automatic metrics lack sensitivity for high-quality LLM outputs, suggesting they may be optimized for lower-quality system outputs
- Why unresolved: The study only tested existing metrics without exploring whether retraining or recalibration could improve performance
- What evidence would resolve it: Retraining automatic metrics using human evaluation data from high-quality LLM outputs and testing their improved ability to distinguish quality levels

### Open Question 5
- Question: How would the simplification performance change if the models were evaluated on languages other than English?
- Basis in paper: [inferred] The study focuses exclusively on English sentence simplification without exploring cross-linguistic applicability
- Why unresolved: The research was limited to English datasets and didn't investigate whether findings generalize to other languages
- What evidence would resolve it: Applying the same evaluation framework to simplification models across multiple languages and comparing error patterns and metric performance across languages

## Limitations

- The error-based human evaluation framework relies heavily on annotator interpretation, which may vary across different linguistic backgrounds
- The prompt engineering approach for GPT-4 is dataset-specific, suggesting performance may degrade on unseen domains or languages
- Current automatic metrics lack sensitivity to assess high-quality simplifications generated by advanced LLMs

## Confidence

**High Confidence:** The comparative performance of GPT-4 versus Control-T5 is well-supported by quantitative human evaluation data. The finding that GPT-4 generates fewer erroneous outputs and better preserves meaning has strong empirical backing.

**Medium Confidence:** The effectiveness of the error-based evaluation framework is moderately supported. While the framework shows promise in identifying specific error types, the lack of correlation analysis with gold standards and potential annotator bias limits confidence in its universal applicability.

**Low Confidence:** The meta-evaluation conclusions about automatic metric sensitivity have limited support. The paper demonstrates that metrics struggle with high-quality outputs, but doesn't explore whether alternative metrics or evaluation approaches might address these limitations.

## Next Checks

1. **Cross-linguistic validation:** Test the error-based evaluation framework with annotators from different linguistic backgrounds to assess its robustness and identify potential cultural or linguistic biases in error categorization.

2. **Metric sensitivity enhancement:** Develop and test enhanced versions of existing metrics that are specifically calibrated for high-quality LLM-generated outputs, potentially using machine learning approaches to improve sensitivity.

3. **Prompt engineering generalization:** Conduct experiments to evaluate whether the dataset-specific prompt engineering approach for GPT-4 generalizes to other text simplification tasks or domains beyond the tested datasets.