---
ver: rpa2
title: Personalized Federated Instruction Tuning via Neural Architecture Search
arxiv_id: '2402.16919'
source_url: https://arxiv.org/abs/2402.16919
tags:
- uni00000011
- uni00000014
- uni00000013
- uni00000015
- uni00000056
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses challenges in federated instruction tuning
  of large language models (LLMs), specifically data and resource heterogeneity. The
  authors propose a personalized federated instruction tuning (PerFIT) framework that
  allows each client to search for a personalized fine-tuning architecture based on
  its local data and resources.
---

# Personalized Federated Instruction Tuning via Neural Architecture Search

## Quick Facts
- arXiv ID: 2402.16919
- Source URL: https://arxiv.org/abs/2402.16919
- Reference count: 4
- Key outcome: Up to 23% perplexity reduction on LLMs in non-IID federated scenarios through personalized architecture search

## Executive Summary
This paper addresses the challenges of federated instruction tuning for large language models (LLMs) in the presence of data and resource heterogeneity. The authors propose PerFIT, a framework that enables each client to search for a personalized fine-tuning architecture based on its local data and resources. By combining neural architecture search through iterative pruning with a personalized aggregation mechanism, PerFIT achieves significant performance improvements over state-of-the-art methods in non-IID scenarios while addressing resource heterogeneity.

## Method Summary
PerFIT operates by first expanding the trainable parameter space of LoRA adapters, then performing neural architecture search through iterative pruning based on importance scores derived from Taylor expansion of the loss function. Each client finds a sparse architecture tailored to its local data distribution. A personalized aggregation mechanism then combines parameters across clients, aggregating only those marked as "shared" while preserving "exclusive" parameters locally. The framework uses symmetric initialization of LoRA adapters to prevent Measurement Vanishing during the NAS process.

## Key Results
- Achieves up to 23% decrease in perplexity compared to state-of-the-art methods in non-IID scenarios
- Demonstrates effectiveness on two LLM architectures (Alpaca-7B and Vicuna-7B)
- Shows improved convergence speed compared to standard federated fine-tuning approaches

## Why This Works (Mechanism)

### Mechanism 1
PerFIT allows each client to search for a personalized fine-tuning architecture by expanding the trainable parameter space and then pruning it back to the original size. The method uses Taylor expansion-based importance scores to iteratively prune parameters, enabling personalized instruction fine-tuning within expanded parameter spaces while preserving the same number of trainable parameters.

### Mechanism 2
Personalized parameter-wise aggregation enhances information interaction across clients with various architectures by only aggregating parameters that are marked as "shared" between clients. This approach allows clients with different data distributions to maintain architectural differences while still benefiting from global knowledge transfer.

### Mechanism 3
Symmetric initialization of LoRA adapters prevents Measurement Vanishing by ensuring gradients are non-zero, enabling effective importance score computation during NAS. This addresses the problem where standard zero-initialization causes gradients to be zero, preventing meaningful parameter importance evaluation.

## Foundational Learning

- **Neural Architecture Search (NAS) basics**: Understanding NAS is crucial because PerFIT uses a lightweight NAS approach through iterative pruning to find personalized architectures for each client. *Quick check: What is the difference between traditional NAS and the iterative pruning approach used in PerFIT?*

- **Federated Learning fundamentals**: The entire framework operates in a federated setting where clients collaborate without sharing data. *Quick check: How does PerFIT's personalized aggregation differ from standard FedAvg aggregation?*

- **Parameter-efficient fine-tuning (LoRA) principles**: PerFIT builds upon LoRA as the fine-tuning method. *Quick check: Why does PerFIT modify the standard LoRA initialization approach, and what problem does this solve?*

## Architecture Onboarding

- **Component map**: Client-side (Local NAS module → Local fine-tuning module → Sparse mask transmission) → Server-side (Personalized aggregation module → Global module generation → Personalized module distribution) → Client-side (Client fine-tuning)

- **Critical path**: Client NAS → Client fine-tuning → Server aggregation → Server personalization → Client distribution → Client fine-tuning (next round)

- **Design tradeoffs**: Pruning vs. accuracy (more aggressive pruning reduces computation but may hurt performance), communication overhead (transmitting sparse masks vs. dense models), personalization vs. generalization (too much personalization may prevent useful global knowledge transfer)

- **Failure signatures**: If all clients end up with similar architectures despite data heterogeneity, the NAS is not working effectively; if perplexity increases instead of decreases compared to baseline, the pruning or aggregation is harming performance; if convergence is very slow, the personalized aggregation may be preventing effective knowledge transfer

- **First 3 experiments**:
  1. Implement the iterative pruning on a single client with synthetic data to verify that the importance scores correctly identify parameters to prune and that performance is maintained
  2. Test the symmetric initialization by comparing gradient magnitudes with zero initialization to confirm Measurement Vanishing is prevented
  3. Validate the personalized aggregation by creating two clients with clearly different data distributions and verifying that they learn different architectures and that aggregation works as intended

## Open Questions the Paper Calls Out

- **Open Question 1**: How does the performance of PerFIT scale with the number of local clients participating in each round? The paper only tested two specific percentages (5% and 20%) of participants, leaving the optimal number of participants for different scenarios unclear.

- **Open Question 2**: What is the impact of different importance score metrics (first-order, second-order, or mixed) on the effectiveness of the pruning process? While the paper observed that all metrics exhibit similar behavior, it did not provide a detailed analysis of which metric is most effective under different conditions.

- **Open Question 3**: How does the proposed PerFIT framework perform when applied to other types of foundation models beyond LLMs, such as vision transformers or multimodal models? The paper's evaluation is limited to two specific LLM architectures.

## Limitations

- Performance improvements appear heavily dependent on extreme data heterogeneity scenarios that may not reflect realistic federated learning deployments
- The personalized aggregation mechanism introduces significant algorithmic complexity without clear evidence that simpler personalization approaches wouldn't achieve similar results
- The Measurement Vanishing problem and its proposed solution lack empirical comparison against standard LoRA implementations

## Confidence

- **High Confidence**: The general framework of combining NAS with federated learning is technically sound and the iterative pruning approach for finding sparse architectures is well-established
- **Medium Confidence**: The experimental results showing performance improvements over baselines are likely valid for the extreme heterogeneity scenarios tested
- **Low Confidence**: The claim that the personalized aggregation mechanism is necessary and superior to simpler approaches lacks sufficient empirical support

## Next Checks

1. **Ablation Study on Aggregation Methods**: Implement and compare PerFIT against simpler personalization approaches like pFedMe or LG-FedAvg using the same experimental setup to determine if the complex personalized aggregation provides measurable benefits

2. **Heterogeneity Sensitivity Analysis**: Systematically vary the data heterogeneity levels (using different β values in Dirichlet distribution and less pathological non-IID splits) to identify the threshold at which PerFIT's personalization benefits outweigh its added complexity

3. **Measurement Vanishing Quantification**: Conduct controlled experiments measuring gradient magnitudes and importance score distributions with standard zero-initialized LoRA versus symmetric initialization across different fine-tuning scenarios to empirically validate the severity of the Measurement Vanishing problem