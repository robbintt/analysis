---
ver: rpa2
title: Understanding Cross-Lingual Alignment -- A Survey
arxiv_id: '2404.06228'
source_url: https://arxiv.org/abs/2404.06228
tags:
- language
- cross-lingual
- alignment
- association
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Cross-lingual alignment is essential for zero-shot cross-lingual
  transfer in multilingual NLP models. This survey examines methods to improve alignment,
  focusing on encoder models like XLM-R and mBERT.
---

# Understanding Cross-Lingual Alignment -- A Survey

## Quick Facts
- arXiv ID: 2404.06228
- Source URL: https://arxiv.org/abs/2404.06228
- Reference count: 33
- Primary result: Contrastive training is effective for improving cross-lingual alignment in multilingual NLP models, with parallel data usage also showing significant benefits.

## Executive Summary
This survey examines cross-lingual alignment in multilingual language models, focusing on encoder models like XLM-R and mBERT. The authors propose two views of alignment - weak alignment (translation pairs as nearest neighbors) and strong alignment (translations as nearest neighbors among all words). They analyze various methods including parallel data objectives, contrastive learning, adapter tuning, data augmentation, and representation transformations. The survey identifies contrastive learning as particularly effective and highlights the growing importance of alignment in encoder-decoder models as the field shifts toward generative approaches.

## Method Summary
The survey collected papers from 2019-2023 by searching ACL Anthology, Semantic Scholar, and arXiv.org using terms like "zero-shot cross-lingual transfer" and "cross-lingual alignment." Papers were selected by following citation graphs while excluding those without PDF versions or focusing on static cross-lingual word embeddings. The methods were categorized by data requirements (parallel data, contrastive learning, etc.) and initialization approaches (training from scratch vs. modifying existing models). The survey synthesized findings across these categories to provide insights and discuss future research directions.

## Key Results
- Contrastive learning objectives significantly improve cross-lingual alignment by increasing similarity of positive examples and dissimilarity of negative examples
- Parallel data usage provides explicit alignment signals that improve cross-lingual representation quality
- Encoder-decoder models face unique challenges in balancing language-neutral and language-specific information
- The field is shifting toward generative models, presenting new challenges for cross-lingual alignment

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Cross-lingual alignment enables zero-shot transfer by ensuring semantic similarity of representations across languages
- Mechanism: When representations of equivalent words or sentences across languages are more similar to each other than to representations of different meanings, a prediction head trained on one language can effectively classify or process data in another language
- Core assumption: Semantic similarity in representation space directly translates to functional transfer capability in downstream tasks
- Evidence anchors:
  - [abstract] "Cross-lingual alignment, the meaningful similarity of representations across languages in multilingual language models, has been an active field of research in recent years."
  - [section] "Cross-lingual alignment means that words or sentences with similar semantics: 1) Are more similar in the representation space than words or sentences with dissimilar semantics."
  - [corpus] Corpus evidence is present with related papers focusing on cross-lingual transfer and alignment
- Break condition: The alignment breaks if the representation space becomes too language-specific or if the semantic similarity does not translate to functional transfer capability

### Mechanism 2
- Claim: Contrastive learning improves cross-lingual alignment by increasing the similarity of positive examples and the dissimilarity of negative examples
- Mechanism: By using contrastive learning objectives, the model is trained to make representations of equivalent translations closer while pushing apart representations of different meanings, thus improving the alignment
- Core assumption: Contrastive learning can effectively differentiate between similar and dissimilar representations across languages
- Evidence anchors:
  - [section] "Contrastive learning has become popular in NLP for a variety of use cases. For cross-lingual alignment, it has also been used in several papers, since it aims to increase the similarity of positive examples and the dissimilarity of negative examples jointly."
  - [corpus] Related papers in the corpus discuss the effectiveness of contrastive learning for cross-lingual tasks
- Break condition: The mechanism breaks if the contrastive learning objective does not adequately capture the semantic differences or if the negative examples are not informative enough

### Mechanism 3
- Claim: Parallel data usage improves cross-lingual alignment by providing explicit alignment signals
- Mechanism: The use of parallel data allows the model to learn explicit mappings between representations of equivalent words or sentences across languages, thereby improving alignment
- Core assumption: Parallel data provides sufficient and accurate alignment signals for the model to learn meaningful cross-lingual representations
- Evidence anchors:
  - [section] "First, we discuss models using external parallel dataâ€”sentence-parallel or word-parallel. These make up a plurality of methods in this survey."
  - [corpus] Corpus evidence includes papers that utilize parallel data for improving cross-lingual alignment
- Break condition: The mechanism breaks if the parallel data is insufficient, noisy, or if the alignment signals are not effectively utilized by the model

## Foundational Learning

- Concept: Cross-lingual alignment in multilingual NLP models
  - Why needed here: Understanding cross-lingual alignment is crucial for zero-shot cross-lingual transfer, which is a key focus of the paper
  - Quick check question: What are the two main views of cross-lingual alignment discussed in the paper?

- Concept: Contrastive learning objectives
  - Why needed here: Contrastive learning is a popular method for improving cross-lingual alignment by differentiating between similar and dissimilar representations
  - Quick check question: How does contrastive learning help in reducing the hubness problem in cross-lingual alignment?

- Concept: Parallel data usage in multilingual models
  - Why needed here: Parallel data provides explicit alignment signals that can improve the cross-lingual alignment of representations
  - Quick check question: Why is the use of parallel data considered effective for cross-lingual alignment?

## Architecture Onboarding

- Component map: Encoder layers (XLM-R/mBERT) -> Alignment objectives (contrastive learning, parallel data objectives) -> Task-specific heads for downstream tasks
- Critical path: Pre-train encoder on multilingual data -> Apply alignment objectives to improve cross-lingual alignment -> Fine-tune on specific tasks for zero-shot transfer
- Design tradeoffs: Balancing language-neutral and language-specific information, choice between parallel data vs. monolingual data, complexity of alignment objectives vs. model performance
- Failure signatures: Poor zero-shot transfer performance, misaligned representations leading to incorrect classifications, model generating text in wrong language
- First 3 experiments:
  1. Implement basic cross-lingual alignment using cosine similarity to measure similarity of translation pairs
  2. Apply contrastive learning objective to multilingual encoder model and evaluate impact on alignment
  3. Use parallel data to fine-tune multilingual model and assess improvements in zero-shot transfer performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does "strong" cross-lingual alignment as defined in Equation 2 actually improve performance on all downstream tasks, or is it task-dependent?
- Basis in paper: [explicit] The paper questions whether strong alignment is necessary for downstream tasks, noting that Gaschi et al. (2023) show a correlation between strong alignment and better performance on classification tasks, but this does not prove necessity
- Why unresolved: The paper highlights that most cross-lingual retrieval tasks query only the target language space, potentially making strong alignment unnecessary for these tasks. However, the impact on other task types remains unclear
- What evidence would resolve it: Systematic evaluation of a diverse set of downstream tasks (classification, retrieval, generation) comparing models with varying degrees of alignment strength, measuring both alignment metrics and task performance

### Open Question 2
- Question: How can cross-lingual alignment methods be effectively integrated into the pre-training of multilingual generative models to balance language-neutral and language-specific information?
- Basis in paper: [explicit] The paper argues that multilingual generative models need to trade off language-neutral and language-specific information effectively, and suggests that cross-lingual alignment methods from encoder models could be applied, but with risks of suppressing necessary language-specific information for the decoder
- Why unresolved: The paper identifies this as a key challenge for future research, noting that the "implicit encoder" in decoder-only models could be a target for alignment methods, but the optimal approach is unknown
- What evidence would resolve it: Development and evaluation of specific cross-lingual alignment techniques integrated into the pre-training of multilingual generative models, measuring both cross-lingual transfer performance and generation quality across languages

### Open Question 3
- Question: To what extent does the "Curse of Multilinguality" explain the underperformance of intentionally multilingual generative models like BLOOM and XGLM compared to English-centric models?
- Basis in paper: [explicit] The paper notes that models like BLOOM and XGLM underperform compared to newer English-centric models, and suggests exploring why beyond the "Curse of Multilinguality," which is often cited as an explanation
- Why unresolved: While the "Curse of Multilinguality" is mentioned, the paper calls for further exploration of the underlying reasons for this performance gap, suggesting that the explanation may be more complex
- What evidence would resolve it: Detailed analysis of the training data, model architectures, and training procedures of both multilingual and English-centric models, identifying specific factors contributing to the performance difference

## Limitations
- Conclusions primarily based on papers from 2019-2023, potentially missing recent developments
- Exclusion criteria may omit relevant foundational work on static cross-lingual word embeddings
- Inconsistent reporting of results across different papers, particularly for lower-resource languages
- Limited availability of code and model downloads for reproducibility

## Confidence

- **High Confidence:** The fundamental mechanisms of cross-lingual alignment enabling zero-shot transfer through semantic similarity in representation space
- **Medium Confidence:** The effectiveness of contrastive learning for improving alignment, as results vary across different implementations and datasets
- **Medium Confidence:** The utility of parallel data for alignment, given variations in parallel data quality and quantity across studies

## Next Checks
1. Replicate the basic cosine similarity alignment experiment on a held-out dataset to verify semantic similarity claims across languages
2. Implement and test a contrastive learning objective on a standard multilingual encoder (XLM-R or mBERT) to measure alignment improvements
3. Conduct a systematic comparison of parallel data vs. monolingual data approaches using the same base model and evaluation metrics to quantify relative effectiveness