---
ver: rpa2
title: Gaussian-Smoothed Sliced Probability Divergences
arxiv_id: '2404.03273'
source_url: https://arxiv.org/abs/2404.03273
tags:
- sliced
- wasserstein
- distance
- divergence
- divergences
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the theoretical properties of Gaussian-smoothed
  sliced probability divergences, which are used to compare probability distributions
  while preserving privacy. The authors introduce the concept of a double empirical
  distribution for the smoothed-projected distribution and prove that the Gaussian-smoothed
  sliced Wasserstein distance converges with a rate of O(n^-1/2).
---

# Gaussian-Smoothed Sliced Probability Divergences

## Quick Facts
- arXiv ID: 2404.03273
- Source URL: https://arxiv.org/abs/2404.03273
- Authors: Mokhtar Z. Alaya; Alain Rakotomamonjy; Maxime Berar; Gilles Gasso
- Reference count: 13
- Key outcome: Gaussian-smoothed sliced probability divergences achieve O(n^-1/2) convergence rate while preserving privacy, with only slight performance loss in domain adaptation tasks.

## Executive Summary
This paper introduces Gaussian-smoothed sliced probability divergences for comparing probability distributions while preserving privacy. The authors establish theoretical properties including metric preservation, weak topology maintenance, and convergence rates for the smoothed Wasserstein distance. They demonstrate that these divergences provide a practical framework for privacy-preserving machine learning applications, particularly in domain adaptation scenarios. The method leverages Gaussian convolution to achieve differential privacy while maintaining statistical efficiency through the sliced approach.

## Method Summary
The method constructs Gaussian-smoothed sliced divergences by first projecting high-dimensional distributions onto random directions, then applying Gaussian convolution to the projected distributions, and finally computing the base divergence on these smoothed projections. A key innovation is the double empirical distribution that separates sampling from the original distribution and sampling from the smoothed-projected distribution. The approach uses Monte Carlo approximation with multiple random projections to estimate the divergence. For domain adaptation applications, the smoothed divergence serves as a regularizer to align source and target distributions while preserving privacy through the Gaussian noise.

## Key Results
- Gaussian-smoothed sliced Wasserstein distance achieves O(n^-1/2) convergence rate with respect to sample size
- The divergence is continuous with respect to the smoothing parameter σ, enabling efficient fine-tuning across privacy levels
- Empirical studies show only slight performance loss in domain adaptation tasks while preserving privacy
- The method maintains metric properties and weak topology of the base divergence

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The double empirical distribution enables accurate sample complexity analysis by separating randomness from sampling the original distribution and from sampling the smoothed-projected distribution.
- Mechanism: By defining the double empirical distribution as a result of two independent sampling processes, the authors can apply standard concentration inequalities to each layer separately, leading to a clean decomposition of error terms.
- Core assumption: The convolution of the Radon transform with Gaussian smoothing results in a continuous mixture Gaussian measure that can be empirically approximated.
- Evidence anchors:
  - [abstract]: "We then introduce ˆˆµn the double empirical distribution for the smoothed-projected µ"
  - [section]: "Sampling i.i.d. {T x i }i=1,...,n is given by the following scheme: for i = 1, . . . , n, we first choose the component N (u⊤Xi, σ2) from the mixture 1 n Pn i=1 N (u⊤Xi, σ2) then we generate T x i = u⊤Xi + Zx i , where Zx i ∼ N σ"
- Break condition: If the smoothing operation does not produce a tractable mixture of Gaussians, the double sampling approach becomes intractable.

### Mechanism 2
- Claim: Gaussian smoothing preserves the metric property and weak topology of the base divergence while adding privacy guarantees.
- Mechanism: The convolution with Gaussian noise smooths the distributions in a way that maintains their metric properties (via the identity of indiscernibles and triangle inequality) while also satisfying differential privacy requirements.
- Core assumption: The base divergence satisfies the required properties (non-negativity, identity of indiscernibles, triangle inequality) and is bounded.
- Evidence anchors:
  - [abstract]: "We first show that smoothing and slicing preserve the metric property and the weak topology"
  - [section]: "Theorem 1. For any σ > 0, p ≥ 1, the following properties hold: 1. if D(·, ·) is non-negative (or symmetric), then GσSD(·, ·) is non-negative (or symmetric)"
- Break condition: If the base divergence is unbounded or does not satisfy the identity of indiscernibles, the smoothing operation may not preserve metric properties.

### Mechanism 3
- Claim: The continuity of the smoothed sliced divergences with respect to the smoothing parameter σ enables efficient fine-tuning across different privacy levels.
- Mechanism: Since the divergence is continuous in σ, one can train a model at a high privacy level (large σ) and then fine-tune it for smaller σ values with minimal additional training, leveraging the topological properties established in the paper.
- Core assumption: The base divergence is continuous with respect to the smoothing parameter.
- Evidence anchors:
  - [abstract]: "We also derive other properties, including continuity, of different divergences with respect to the smoothing parameter"
  - [section]: "Proposition 8. For any two distributions µ and ν for which the sliced Wasserstein is well-defined, the Gaussian-smoothed sliced Wasserstein distance is continuous w.r.t. to σ"
- Break condition: If the base divergence is not continuous with respect to the smoothing parameter, the fine-tuning strategy may not be effective.

## Foundational Learning

- Concept: Weak topology and convergence
  - Why needed here: The paper establishes that the smoothed sliced divergences metrize the weak topology, which is crucial for understanding their statistical properties and convergence behavior.
  - Quick check question: What does it mean for a divergence to "metrize the weak topology" and why is this important for statistical estimation?

- Concept: Sample complexity and concentration inequalities
  - Why needed here: The authors derive sample complexity bounds using concentration inequalities applied to the double empirical distribution, requiring understanding of how empirical measures converge to true measures.
  - Quick check question: How does the sample complexity of O(n^-1/2) compare to other common rates in statistical learning, and what does this imply about the efficiency of the method?

- Concept: Differential privacy and Gaussian mechanism
  - Why needed here: The motivation for using Gaussian smoothing is to achieve differential privacy, so understanding the privacy-utility tradeoff is essential for practical applications.
  - Quick check question: How does the amount of Gaussian smoothing (parameter σ) affect both the privacy guarantees and the statistical performance of the divergence?

## Architecture Onboarding

- Component map: High-dimensional distributions -> Random projections -> Gaussian smoothing -> Base divergence computation -> Averaging over projections
- Critical path: The most critical computational path is the evaluation of the divergence, which involves (1) projecting samples onto random directions, (2) applying Gaussian smoothing to the projected samples, (3) computing the base divergence on the smoothed projections, and (4) averaging over multiple projections.
- Design tradeoffs: The choice of σ involves a tradeoff between privacy (larger σ provides stronger privacy) and statistical accuracy (smaller σ provides better estimation). The number of projections L trades off computational cost with approximation accuracy.
- Failure signatures: If the base divergence is not bounded or does not satisfy the required properties, the metric properties may not be preserved. If the smoothing is too large, the sample complexity will degrade significantly.
- First 3 experiments:
  1. Verify the sample complexity by computing the divergence between two normal distributions with increasing sample sizes and checking the convergence rate.
  2. Test the continuity property by computing the divergence for a fixed pair of distributions with varying σ values and confirming smooth variation.
  3. Evaluate the privacy-utility tradeoff on a domain adaptation task by training models with different σ values and measuring both accuracy and privacy level.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the statistical properties of Gaussian-smoothed sliced Sinkhorn divergence and MMD, particularly regarding sample complexity and convergence rates?
- Basis in paper: [inferred] The authors mention that while the properties of Gaussian-smoothed Wasserstein distance have been extensively studied, the properties of Gaussian-smoothed sliced Wasserstein distance and other divergences like Sinkhorn and MMD have not been fully investigated. They also conjecture that certain properties hold for smoothed Sinkhorn and MMD but leave the proofs for future work.
- Why unresolved: The paper focuses primarily on Gaussian-smoothed sliced Wasserstein distance and does not provide a complete analysis for other divergences like Sinkhorn and MMD.
- What evidence would resolve it: A comprehensive theoretical analysis establishing the sample complexity, convergence rates, and other statistical properties for Gaussian-smoothed sliced Sinkhorn divergence and MMD would resolve this question.

### Open Question 2
- Question: How do different smoothing distributions, other than Gaussian, affect the statistical properties and convergence rates of sliced divergences?
- Basis in paper: [explicit] The authors mention in the conclusion that an important direction for future research is to consider non-Gaussian smoothing distributions that enjoy certain properties, which could potentially improve the statistical bounds.
- Why unresolved: The paper only considers Gaussian smoothing and does not explore the effects of other smoothing distributions on the statistical properties of sliced divergences.
- What evidence would resolve it: Conducting a theoretical and empirical analysis of sliced divergences with different smoothing distributions (e.g., Laplace, Cauchy) and comparing their statistical properties and convergence rates would resolve this question.

### Open Question 3
- Question: How does the choice of base divergence (e.g., Wasserstein, Sinkhorn, MMD) impact the sample complexity and performance of Gaussian-smoothed sliced divergences in practical applications like domain adaptation?
- Basis in paper: [inferred] The authors perform empirical studies comparing different base divergences (Wasserstein, Sinkhorn, MMD) and their Gaussian-smoothed versions in the context of domain adaptation. However, they do not provide a comprehensive analysis of how the choice of base divergence affects the sample complexity and performance.
- Why unresolved: While the authors compare different base divergences empirically, they do not provide a theoretical analysis of how the choice of base divergence impacts the sample complexity and performance of Gaussian-smoothed sliced divergences.
- What evidence would resolve it: Conducting a theoretical analysis of the sample complexity and performance of Gaussian-smoothed sliced divergences for different base divergences, along with empirical studies validating the theoretical findings, would resolve this question.

## Limitations

- The theoretical analysis relies on boundedness assumptions for the base divergence and support, which may not hold for many practical distributions
- The continuity properties depend on bounded support assumptions that may not generalize to unbounded domains
- Empirical validation focuses on specific synthetic distributions and domain adaptation tasks, with limited exploration of diverse real-world scenarios

## Confidence

- **High Confidence:** The metric preservation properties (Theorem 1) - these follow directly from the mathematical properties of Gaussian smoothing and have straightforward proofs.
- **Medium Confidence:** Sample complexity bounds (Theorem 2) - while the methodology is sound, the bounds assume bounded support and Lipschitz continuity which may not hold in practice.
- **Medium Confidence:** Continuity with respect to σ - the proofs rely on bounded support assumptions that may not generalize to all practical applications.

## Next Checks

1. **Generalization to unbounded distributions:** Test the sample complexity bounds on distributions with heavy tails or unbounded support to verify if the O(n^-1/2) rate still holds or degrades gracefully.

2. **Robustness to base divergence choice:** Systematically evaluate the method's performance across a broader range of base divergences (e.g., KL divergence, Jensen-Shannon divergence) to verify that the metric preservation properties extend beyond the bounded cases considered.

3. **Privacy-utility tradeoff characterization:** Conduct a comprehensive empirical study varying both σ and sample size n to map out the full privacy-utility tradeoff space, particularly for privacy-sensitive applications where both metrics are critical.