---
ver: rpa2
title: 'JEP-KD: Joint-Embedding Predictive Architecture Based Knowledge Distillation
  for Visual Speech Recognition'
arxiv_id: '2403.18843'
source_url: https://arxiv.org/abs/2403.18843
tags: []
core_contribution: The paper introduces JEP-KD, a knowledge distillation framework
  that incorporates a Joint-Embedding Predictive Architecture (JEPA) into visual speech
  recognition (VSR) models. The method aims to address the inherent limitations of
  VSR compared to audio speech recognition (ASR) by aligning video features with audio
  features through a generative network in the embedding layer.
---

# JEP-KD: Joint-Embedding Predictive Architecture Based Knowledge Distillation for Visual Speech Recognition

## Quick Facts
- arXiv ID: 2403.18843
- Source URL: https://arxiv.org/abs/2403.18843
- Authors: Chang Sun; Hong Yang; Bo Qin
- Reference count: 26
- Character Error Rate reduced from 19.92% to 11.97% on CMLR dataset

## Executive Summary
This paper introduces JEP-KD, a knowledge distillation framework that incorporates Joint-Embedding Predictive Architecture (JEPA) to enhance visual speech recognition (VSR) models. The approach addresses the inherent limitations of VSR compared to audio speech recognition (ASR) by aligning video features with audio features through a generative network in the embedding layer. The method employs a comprehensive three-stage training regimen with four models to ensure stability and effectiveness. Experiments on the CMLR dataset demonstrate significant performance improvements, reducing character error rate from 19.92% to 11.97%, bringing VSR performance closer to ASR levels.

## Method Summary
JEP-KD introduces a generative network within the embedding layer to enhance video encoder capacity for semantic feature extraction and align it with audio features. The framework uses a four-model, three-stage training process: a warm-up phase (20 epochs) to establish basic VSR capability, an enhancement phase (10 epochs) to train generator/discriminator pairs for modality alignment, and a refinement phase (2 epochs) to adapt the decoder to enhanced features. The approach leverages a pre-trained ASR model to provide audio feature references and employs GAN-based training with CTC, CE, and L1 loss functions. This staged approach addresses GAN training instability while progressively transferring knowledge from audio to visual modalities.

## Key Results
- Character Error Rate reduced from 19.92% to 14.26% using JEP-KD framework
- Further reduction to 11.97% with additional pretraining data
- Achieves state-of-the-art performance on CMLR dataset
- Successfully bridges semantic gaps between video and audio modalities

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The generative network in the embedding layer helps bridge semantic gaps between video and audio modalities.
- Mechanism: By introducing a generator G(z,v) that takes video features v and random noise z to predict audio features a, the model learns to fill in missing semantic information that lip movements cannot convey.
- Core assumption: Video modality inherently lacks certain semantic information that audio modality contains, and this missing information follows predictable patterns.
- Evidence anchors:
  - [abstract] "Central to JEP-KD is the inclusion of a generative network within the embedding layer, which enhances the video encoder's capacity for semantic feature extraction and brings it into closer alignment with the audio features"
  - [section] "Inspired by the JEPA [9, 10], introducing a tailored generative network into the semantic domain to augment the video semantic features could not only allow the video encoder to more foucsed on video feature extraction but also promote better alignment with the audio semantic features"
  - [corpus] No direct evidence found in corpus about generative network bridging semantic gaps; this appears to be a novel contribution not yet discussed in related literature.

### Mechanism 2
- Claim: The three-stage training process ensures stability and effectiveness of knowledge distillation.
- Mechanism: The staged approach first establishes basic VSR capability, then trains the generator/discriminator pair to align modalities, and finally refines the decoder to adapt to enhanced features.
- Core assumption: GAN training is prone to instability, and a controlled, staged approach can mitigate these issues while allowing progressive knowledge transfer.
- Evidence anchors:
  - [section] "Because GAN models are prone to imbalance during training, we designed this three-stage training process to ensure that the input to the generator is a stable and regular vector in feature space"
  - [section] "The first stage we call the warm-up phase... The second stage is the enhancement stage... The third stage is called the refinement phase"
  - [corpus] No direct evidence found in corpus about three-stage training for VSR knowledge distillation; this appears to be a novel contribution.

### Mechanism 3
- Claim: JEPA-based architecture can reduce the performance gap between VSR and ASR.
- Mechanism: By using predictive modeling in the embedding space, the system learns to complete missing semantic information in video features, bringing VSR performance closer to ASR levels.
- Core assumption: The semantic gaps between video and audio modalities follow systematic patterns that can be learned and predicted.
- Evidence anchors:
  - [abstract] "This approach aims to progressively reduce the performance gap between VSR and ASR"
  - [section] "On one side, these methods inadequately exploit the auditory features, thus failing to facilitate a robust knowledge transfer; on the other end of the spectrum, studies have identified an intrinsic disparity across modalities akin to interlingual translation [8]"
  - [corpus] No direct evidence found in corpus about JEPA-based approaches specifically for VSR-ASR gap reduction; this appears to be a novel contribution.

## Foundational Learning

- Concept: Generative Adversarial Networks (GANs)
  - Why needed here: The generator-discriminator pair is central to the JEP-KD architecture for aligning video and audio features
  - Quick check question: What is the role of the discriminator in a GAN framework, and how does it differ from the generator's role?

- Concept: Knowledge Distillation
  - Why needed here: The paper builds on existing knowledge distillation techniques but enhances them with predictive modeling
  - Quick check question: How does traditional knowledge distillation differ from the JEP-KD approach in terms of feature alignment?

- Concept: Multimodal Representation Learning
  - Why needed here: Understanding how to align and transfer knowledge between video and audio modalities is fundamental to this work
  - Quick check question: What are the key challenges in aligning features from different modalities like video and audio?

## Architecture Onboarding

- Component map:
  - Video frames → Video Encoder (feV) → Generator (G) → Discriminator (D) → Video Decoder → Text Output
  - Pre-trained Audio Encoder (feA) provides reference audio features

- Critical path: Video → Video Encoder → Generator → Discriminator → Video Decoder → Text Output

- Design tradeoffs:
  - The three-stage training process adds complexity but provides stability
  - Using a pre-trained ASR model limits flexibility but provides strong supervision
  - The generative approach adds computational overhead but potentially improves alignment quality

- Failure signatures:
  - GAN instability during training (mode collapse, vanishing gradients)
  - Poor alignment between generated and real audio features
  - Limited improvement over baseline knowledge distillation approaches
  - Overfitting to the specific training dataset

- First 3 experiments:
  1. Baseline comparison: Train without JEP-KD (only stage 1) vs. full three-stage training to measure improvement
  2. Generator ablation: Train with and without the generator component to isolate its contribution
  3. Modality analysis: Evaluate performance on different types of utterances (homophonic vs. visually distinct) to understand where JEP-KD helps most

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the predictive model's capabilities be further enhanced to reduce the performance gap between VSR and ASR?
- Basis in paper: [explicit] The paper acknowledges that there is still a significant gap compared to semantic recognition models with similar structures, suggesting substantial room for research in the completion of semantic features.
- Why unresolved: The current predictive model, while effective, still falls short of the performance achieved by ASR models, indicating that the semantic features are not fully captured.
- What evidence would resolve it: Empirical studies demonstrating improved performance metrics (e.g., reduced CER) in VSR models after implementing advanced predictive models or architectures.

### Open Question 2
- Question: How universal is the JEP-KD structure across different lip-reading models?
- Basis in paper: [explicit] The paper mentions the need to verify the universality of the JEP-KD structure among different lip-reading models as part of future research.
- Why unresolved: The current experiments are limited to a specific model and dataset, and it is unclear how well the JEP-KD structure would perform with other models or in different contexts.
- What evidence would resolve it: Comparative studies showing consistent performance improvements across various lip-reading models and datasets when using the JEP-KD structure.

### Open Question 3
- Question: Can an Audio-Visual Speech Recognition (AVSR) model as a teacher network, combined with the JEP-KD structure, further improve the performance of lip-reading models?
- Basis in paper: [explicit] The paper suggests exploring the use of an AVSR model as a teacher network combined with the JEP-KD structure to enhance lip-reading performance.
- Why unresolved: This approach has not been tested, and it is uncertain whether the integration of AVSR would provide additional benefits over the current setup.
- What evidence would resolve it: Experimental results demonstrating enhanced performance metrics in lip-reading models when using an AVSR model as a teacher network in conjunction with the JEP-KD structure.

## Limitations

- The method requires a pre-trained ASR model and audio features, limiting applicability to languages or domains where these resources are unavailable
- The three-stage training process significantly increases training complexity and computational requirements
- The CMLR dataset used for evaluation is relatively small (61 hours of training data), potentially limiting generalizability to larger-scale applications
- The paper doesn't thoroughly explore the impact of different noise distributions in the generator or investigate potential overfitting to specific dataset characteristics

## Confidence

- **High Confidence**: The experimental results showing CER reduction from 19.92% to 11.97% are well-supported by the data and methodology. The three-stage training approach is clearly defined and implemented.
- **Medium Confidence**: The mechanism by which the generative network bridges semantic gaps between modalities is plausible but relies on several assumptions about the predictability of these gaps. The effectiveness of JEPA-based architecture for reducing VSR-ASR performance gaps is supported by results but needs broader validation.
- **Low Confidence**: Claims about the generalizability of the approach to other languages or domains are not substantiated. The specific architectural choices (ResNet2D block configurations, discriminator architecture) lack detailed justification.

## Next Checks

1. **Ablation study on noise injection**: Systematically vary the noise distribution z in the generator to determine its optimal characteristics and verify that it's not just serving as a regularizer.

2. **Cross-dataset validation**: Test the trained JEP-KD model on different VSR datasets (e.g., LRS3, LRW) to evaluate generalization beyond the CMLR dataset and identify potential overfitting.

3. **Modality importance analysis**: Conduct controlled experiments where either video or audio features are degraded or removed to quantify the relative importance of each modality and better understand where JEP-KD provides the most benefit.