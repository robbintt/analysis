---
ver: rpa2
title: 'Surgical Feature-Space Decomposition of LLMs: Why, When and How?'
arxiv_id: '2405.13039'
source_url: https://arxiv.org/abs/2405.13039
tags:
- sfsd
- decomposition
- compression
- performance
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Surgical Feature-Space Decomposition (SFSD),
  a novel method for compressing large language models (LLMs) through low-rank approximations
  of latent features. The authors compare SFSD with traditional weight space decomposition
  and structured pruning methods like LLM-Pruner.
---

# Surgical Feature-Space Decomposition of LLMs: Why, When and How?
## Quick Facts
- arXiv ID: 2405.13039
- Source URL: https://arxiv.org/abs/2405.13039
- Reference count: 19
- Primary result: SFSD achieves superior performance in accuracy and perplexity on commonsense reasoning tasks compared to weight space decomposition and structured pruning methods

## Executive Summary
This paper introduces Surgical Feature-Space Decomposition (SFSD), a novel method for compressing large language models (LLMs) through low-rank approximations of latent features. The authors compare SFSD with traditional weight space decomposition and structured pruning methods like LLM-Pruner. Results on LLaMA-7B and Mistral-7B show that SFSD achieves superior performance in terms of accuracy and perplexity on commonsense reasoning tasks, even at high compression levels. For example, at a 5.4B parameter budget, SFSD achieves an average downstream task accuracy of 65.4% on LLaMA-7B, compared to 59.5% for LLM-Pruner. Additionally, SFSD reduces model bias, as measured by the ICAT score, indicating its potential for ethical AI development. The method is computationally efficient, requiring only CPU resources for decomposition and a single GPU for rank search. Overall, SFSD presents a promising approach for compressing LLMs while maintaining or even improving performance and reducing bias.

## Method Summary
SFSD decomposes the latent features of LLMs using low-rank approximations, which allows for efficient compression while preserving model performance. Unlike traditional weight space decomposition methods that directly compress the model weights, SFSD operates in the feature space, enabling more effective compression. The method involves three main steps: feature extraction, low-rank approximation, and rank search. Feature extraction involves computing the latent features of the model using a small subset of the training data. Low-rank approximation then approximates these features using a low-rank matrix factorization technique. Finally, rank search determines the optimal rank for the approximation, balancing compression and performance. The entire process is computationally efficient, requiring only CPU resources for decomposition and a single GPU for rank search.

## Key Results
- SFSD achieves an average downstream task accuracy of 65.4% on LLaMA-7B at a 5.4B parameter budget, compared to 59.5% for LLM-Pruner.
- SFSD reduces model bias, as measured by the ICAT score, indicating its potential for ethical AI development.
- The method is computationally efficient, requiring only CPU resources for decomposition and a single GPU for rank search.

## Why This Works (Mechanism)
SFSD works by decomposing the latent features of LLMs using low-rank approximations, which allows for efficient compression while preserving model performance. This approach is more effective than traditional weight space decomposition methods because it operates in the feature space, where the model's representational power is concentrated. By approximating the latent features with a low-rank matrix factorization, SFSD can compress the model while maintaining its ability to capture complex patterns in the data. Additionally, the rank search step ensures that the optimal balance between compression and performance is achieved, further enhancing the method's effectiveness.

## Foundational Learning
- **Low-rank matrix factorization**: This technique is used to approximate the latent features of the model with a lower-dimensional representation. It is essential for compressing the model while preserving its performance.
  - Why needed: To reduce the dimensionality of the latent features and achieve compression.
  - Quick check: Verify that the low-rank approximation accurately captures the essential information in the latent features.
- **Feature extraction**: This step involves computing the latent features of the model using a small subset of the training data. It is crucial for identifying the most informative features for compression.
  - Why needed: To obtain a representative sample of the model's latent features for decomposition.
  - Quick check: Ensure that the extracted features are diverse and capture the model's behavior across different inputs.
- **Rank search**: This step determines the optimal rank for the low-rank approximation, balancing compression and performance.
  - Why needed: To find the best trade-off between model size and accuracy.
  - Quick check: Verify that the selected rank achieves the desired level of compression without significantly degrading performance.

## Architecture Onboarding
- **Component map**: Feature extraction -> Low-rank approximation -> Rank search
- **Critical path**: The critical path involves extracting the latent features, performing low-rank approximation, and determining the optimal rank. This sequence is essential for achieving efficient compression while preserving model performance.
- **Design tradeoffs**: The main tradeoff is between compression level and model performance. Higher compression levels may lead to a loss in accuracy, while lower compression levels may not achieve the desired reduction in model size.
- **Failure signatures**: Potential failure modes include suboptimal rank selection, which can lead to either excessive compression or insufficient size reduction. Additionally, if the feature extraction step fails to capture representative features, the low-rank approximation may not be effective.
- **First experiments**:
  1. Evaluate the impact of different rank values on model performance to identify the optimal compression level.
  2. Compare the performance of SFSD with traditional weight space decomposition methods on a small-scale benchmark.
  3. Test the bias reduction capabilities of SFSD on a diverse set of datasets to assess its effectiveness in promoting fairness.

## Open Questions the Paper Calls Out
None

## Limitations
- Practical scalability to larger models remains uncertain, as the evaluation focuses on LLaMA-7B and Mistral-7B.
- The method's generalization across diverse task types is unclear, as the evaluation primarily focuses on commonsense reasoning benchmarks.
- The bias reduction analysis relies on a single ICAT metric, and a more comprehensive evaluation of fairness is needed.

## Confidence
- High confidence in the core compression-accuracy trade-off findings, as the results show consistent improvements over baselines across multiple compression levels.
- Medium confidence in the bias reduction claims, given the limited scope of bias evaluation and the need for more comprehensive fairness metrics.
- High confidence in the novelty and technical soundness of the feature-space decomposition approach based on the methodological presentation.

## Next Checks
1. Evaluate SFSD on specialized benchmarks (e.g., code generation, mathematical reasoning) to assess task generalization.
2. Conduct ablation studies to isolate which components of the feature-space decomposition contribute most to performance gains.
3. Test scalability on larger models (e.g., LLaMA-70B) and measure wall-clock training time across different hardware configurations.