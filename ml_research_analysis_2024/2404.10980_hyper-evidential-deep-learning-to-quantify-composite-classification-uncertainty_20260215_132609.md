---
ver: rpa2
title: Hyper Evidential Deep Learning to Quantify Composite Classification Uncertainty
arxiv_id: '2404.10980'
source_url: https://arxiv.org/abs/2404.10980
tags:
- composite
- evidence
- upce
- class
- singleton
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of quantifying predictive uncertainty
  in deep neural networks (DNNs) when training data contains composite class labels,
  i.e., when some images can belong to multiple classes simultaneously due to visual
  similarity. The proposed Hyper-Evidential Neural Network (HENN) framework explicitly
  models this composite classification uncertainty using the theory of Subjective
  Logic and grouped Dirichlet distributions.
---

# Hyper Evidential Deep Learning to Quantify Composite Classification Uncertainty

## Quick Facts
- **arXiv ID**: 2404.10980
- **Source URL**: https://arxiv.org/abs/2404.10980
- **Reference count**: 40
- **One-line primary result**: HENN achieves over 90% Jaccard Similarity on composite set prediction and 2-5% accuracy improvements on singleton class prediction compared to competitive baselines

## Executive Summary
This paper addresses the challenge of quantifying predictive uncertainty in deep neural networks when training data contains composite class labels - cases where images belong to multiple classes simultaneously due to visual similarity. The proposed Hyper-Evidential Neural Network (HENN) framework explicitly models this composite classification uncertainty using Subjective Logic theory and grouped Dirichlet distributions. HENN predicts hyper-opinions that assign belief masses to both singleton classes and composite sets, enabling quantification of a new uncertainty type called vagueness. Extensive experiments on four image datasets demonstrate HENN's superior performance in both composite set prediction (achieving over 90% Jaccard Similarity on some datasets) and singleton class prediction (with 2-5% accuracy improvements), while successfully quantifying vagueness as the most effective indicator for discriminating between composite and singleton samples.

## Method Summary
HENN treats predictions as hyper-opinions in Subjective Logic, which assign belief masses to both singleton classes and composite sets. The framework uses a grouped Dirichlet distribution to represent belief mass distribution over composite sets and quantify vagueness. It employs an Uncertainty Partial Cross Entropy (UPCE) loss function that generalizes standard cross-entropy to handle composite labels by computing expected partial cross-entropy over the predicted grouped Dirichlet distribution. A KL-divergence regularization term addresses inconsistent evidence output by enforcing flat grouped Dirichlet distributions. The model is trained using Adam optimizer with hyperparameter tuning via grid search across four image datasets (CIFAR100, tinyImageNet, Living17, Nonliving26) with composite labels created through Gaussian blurring and label replacement.

## Key Results
- HENN achieves over 90% Jaccard Similarity on composite set prediction for Living17 and Nonliving26 datasets
- HENN improves singleton class prediction accuracy by 2-5% compared to baseline methods across all datasets
- Vagueness uncertainty measure shows superior performance in discriminating composite samples from singleton samples (AUC values ranging from 0.94 to 0.99)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: HENN can effectively quantify a new type of uncertainty called "vagueness" that arises from composite class labels in training data.
- **Mechanism**: HENN treats predictions as hyper-opinions in Subjective Logic, which assign belief masses to both singleton classes and composite sets. By placing a grouped Dirichlet distribution on class probabilities, HENN can explicitly model evidence supporting composite labels and quantify the resulting vagueness.
- **Core assumption**: The composite sets in the training data form a partition of the singleton classes, and the grouped Dirichlet distribution can accurately represent the belief mass distribution over these sets.
- **Evidence anchors**:
  - [abstract] "We introduce a new uncertainty type called vagueness originally designed for hyper-opinions in SL to quantify composite classification uncertainty for DNNs."
  - [section 3.2] "The vagueness uncertainty measure (also named total vague belief mass) of a hyper-opinion can be estimated as: vag(ω) = ΣS∈C(Y) bS."
  - [corpus] Weak - only related papers are about evidential deep learning in general, not specifically about vagueness quantification.
- **Break condition**: If the composite sets do not form a partition of singleton classes, or if the grouped Dirichlet distribution cannot adequately represent the belief mass distribution.

### Mechanism 2
- **Claim**: The Uncertainty Partial Cross Entropy (UPCE) loss function enables effective training of HENN on composite-labeled data.
- **Mechanism**: UPCE generalizes the standard cross-entropy loss to handle composite labels by computing the expected partial cross-entropy over the predicted grouped Dirichlet distribution. This ensures that the model learns to assign high projected probabilities to classes belonging to the composite set label.
- **Core assumption**: The training data contains composite labels that can be represented as binary vectors, and the UPCE loss can be minimized effectively during training.
- **Evidence anchors**:
  - [abstract] "We propose a new loss function, uncertainty partial cross entropy (UPCE), for HENN training."
  - [section 4.1] "Our proposed UPCE loss function has the lower bound: UPCE(x, ˜y; θ) ≥ PCE(Ep∼GDD(p|α,c)[p], ˜y; θ)."
  - [corpus] Weak - related papers discuss evidential deep learning but not specifically the UPCE loss for composite labels.
- **Break condition**: If the binary vector representation of composite labels is inadequate, or if the UPCE loss cannot be minimized effectively due to computational issues.

### Mechanism 3
- **Claim**: The KL-divergence regularization term addresses the issue of inconsistent evidence output in HENN.
- **Mechanism**: The regularization term enforces a flat grouped Dirichlet distribution by penalizing evidence on non-ground-truth classes, ensuring that the model predicts high evidence for the ground-truth class and low evidence for others.
- **Core assumption**: The KL-divergence between the predicted grouped Dirichlet and a flat grouped Dirichlet can effectively regularize the evidence output.
- **Evidence anchors**:
  - [section 4.1] "To address the previous issue, we propose the following KL-divergence regularization term to make the evidence output more flat."
  - [section 4.1] "Proposition 2 (Effectiveness of the regularization term Reg(x, ˜y; θ)). Following the UAP assumption, the regularized empirical UPCE risk defined in Eq. (15) approaches the infimum 0 if the solution θ⋆ satisfies the following properties."
  - [corpus] Weak - related papers discuss evidential deep learning but not specifically the KL-divergence regularization for composite labels.
- **Break condition**: If the KL-divergence regularization term is too strong and prevents the model from learning useful evidence, or if the UAP assumption does not hold.

## Foundational Learning

- **Concept**: Subjective Logic and hyper-opinions
  - Why needed here: HENN is built upon the theory of Subjective Logic to model belief masses on both singleton classes and composite sets.
  - Quick check question: Can you explain the difference between a hyper-opinion and a multinomial opinion in Subjective Logic?

- **Concept**: Grouped Dirichlet distribution
  - Why needed here: HENN uses a grouped Dirichlet distribution to represent the belief mass distribution over composite sets and quantify vagueness.
  - Quick check question: How does a grouped Dirichlet distribution differ from a standard Dirichlet distribution, and why is it suitable for modeling composite labels?

- **Concept**: Uncertainty quantification in deep learning
  - Why needed here: HENN aims to quantify a new type of uncertainty (vagueness) that arises from composite class labels, which is different from traditional uncertainty measures like entropy or confidence.
  - Quick check question: What are the different types of uncertainty in deep learning, and how does vagueness differ from them?

## Architecture Onboarding

- **Component map**: Input → Backbone network → Evidence prediction layer → Loss function → Model parameters update
- **Critical path**: Input → Backbone network → Evidence prediction layer → Loss function → Model parameters update
- **Design tradeoffs**:
  - Using a grouped Dirichlet distribution allows modeling composite labels but increases complexity compared to standard Dirichlet distributions.
  - The KL-divergence regularization term helps prevent inconsistent evidence output but may also limit the model's expressiveness if too strong.
  - The choice of backbone network affects the feature extraction quality and overall performance.
- **Failure signatures**:
  - High vagueness values for singleton samples may indicate that the model is not distinguishing well between singleton and composite labels.
  - Low accuracy on singleton class prediction despite good composite set prediction may suggest that the evidence distribution is not well-calibrated.
  - The model fails to converge during training, possibly due to the complexity of the UPCE loss or the regularization term.
- **First 3 experiments**:
  1. Train HENN on a small dataset with known composite labels and evaluate the vagueness values for singleton vs. composite samples.
  2. Compare the performance of HENN with and without the KL-divergence regularization term on a validation set.
  3. Analyze the evidence distribution output by HENN for a few sample inputs to understand how it assigns belief masses to singleton and composite sets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does HENN perform when composite labels are generated based on semantic similarity rather than visual blurring?
- Basis in paper: [explicit] The paper generates composite labels by applying Gaussian blurring to visually similar classes. It mentions WordNet hierarchy is based on semantic relationships rather than visual similarities.
- Why unresolved: The paper only evaluates HENN on composite labels created through visual blurring, not semantic similarity. It's unclear if HENN would perform equally well on semantically-based composite labels.
- What evidence would resolve it: Experiments applying HENN to datasets where composite labels are defined based on semantic similarity (e.g., WordNet hierarchy) rather than visual blurring, comparing performance metrics.

### Open Question 2
- Question: What is the computational complexity of HENN compared to ENN when handling large numbers of composite classes?
- Basis in paper: [inferred] The paper mentions combinatorial complexity as a limitation but doesn't provide detailed computational complexity analysis. HENN predicts over hyper-domains (power sets) while ENN only predicts over singleton classes.
- Why unresolved: While the paper demonstrates HENN's effectiveness, it doesn't analyze the computational overhead of predicting over composite sets versus singletons, especially as the number of classes grows.
- What evidence would resolve it: Detailed computational complexity analysis comparing HENN and ENN runtime and memory usage as a function of the number of classes and composite sets, plus empirical timing results.

### Open Question 3
- Question: How robust is HENN's vagueness measure to different types of uncertainty beyond composite labels?
- Basis in paper: [explicit] The paper introduces vagueness as a measure for composite label uncertainty but shows it outperforms other uncertainty measures in distinguishing composite from singleton samples.
- Why unresolved: While vagueness performs well for composite label uncertainty, the paper doesn't explore whether it generalizes to other uncertainty types like out-of-distribution detection or adversarial examples.
- What evidence would resolve it: Experiments testing HENN's vagueness measure on datasets with different uncertainty sources (OOD samples, adversarial examples, noisy labels) and comparing its performance to established uncertainty measures for each case.

### Open Question 4
- Question: What is the optimal number of composite classes to include in training for best performance?
- Basis in paper: [explicit] The paper varies the number of composite classes (M=10, 15, 20) in experiments and observes that performance improves with more composite classes, but doesn't determine an optimal point.
- Why unresolved: The paper shows performance trends but doesn't identify when adding more composite classes stops providing benefits or potentially harms performance due to overfitting or computational constraints.
- What evidence would resolve it: Systematic experiments varying M across a wider range, analyzing performance saturation points and trade-offs between composite prediction accuracy and computational efficiency.

## Limitations
- The empirical validation relies on synthetic composite datasets created through Gaussian blurring rather than real-world datasets with naturally occurring composite labels
- The paper doesn't analyze computational complexity or runtime overhead compared to baseline methods when handling large numbers of composite classes
- The hyperparameter sensitivity, particularly to the KL regularization coefficient, may limit practical applicability

## Confidence
- **High confidence**: The theoretical foundation of vagueness quantification and its mathematical connections to Subjective Logic are well-established and empirically validated
- **Medium confidence**: The effectiveness of UPCE loss function and KL-divergence regularization is supported by theoretical analysis and experimental comparisons, but computational complexity and hyperparameter sensitivity may limit practical use
- **Medium confidence**: Claims about HENN's ability to quantify vagueness and improve composite classification performance are supported by extensive experiments, but rely on synthetic rather than real-world composite datasets

## Next Checks
1. Evaluate HENN on real-world datasets with naturally occurring composite labels (e.g., multi-label datasets with ambiguous cases) to test generalizability beyond synthetic blurring scenarios
2. Perform ablation studies to quantify the individual contributions of the UPCE loss and KL-divergence regularization, particularly examining the impact of regularization strength on evidence calibration
3. Test HENN's uncertainty quantification capabilities on out-of-distribution samples and corrupted data to assess robustness beyond the in-distribution composite classification task