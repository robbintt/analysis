---
ver: rpa2
title: Learning Cluster Representatives for Approximate Nearest Neighbor Search
arxiv_id: '2412.05921'
source_url: https://arxiv.org/abs/2412.05921
tags:
- function
- search
- query
- points
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This thesis addresses the challenge of improving accuracy in clustering-based
  Approximate Nearest Neighbor (ANN) search. The key insight is that the routing function,
  which determines the most promising clusters for a query, solves a ranking problem.
---

# Learning Cluster Representatives for Approximate Nearest Neighbor Search

## Quick Facts
- arXiv ID: 2412.05921
- Source URL: https://arxiv.org/abs/2412.05921
- Authors: Thomas Vecchiato
- Reference count: 0
- Primary result: Learnt cluster representatives via LTR significantly improve ANN search accuracy over baseline centroids

## Executive Summary
This thesis addresses the challenge of improving accuracy in clustering-based Approximate Nearest Neighbor (ANN) search. The key insight is that the routing function, which determines the most promising clusters for a query, solves a ranking problem. By leveraging this observation, the thesis introduces a novel state-of-the-art method that learns cluster representatives using a simple linear function via Learning-to-Rank (LTR). This approach significantly enhances the accuracy of ANN search by generating more informative and discriminative representative points for each cluster. Extensive experiments on diverse datasets, embedding models, and clustering algorithms demonstrate that the proposed method consistently outperforms the baseline, achieving substantial improvements in both top-1 and top-k accuracy.

## Method Summary
The method learns cluster representatives by formulating the routing function as a ranking problem amenable to Learning-to-Rank techniques. Given a dataset partitioned into L clusters, the approach trains a linear routing function that scores each cluster's representative using the inner product with the query vector. The training data consists of query-document pairs where the ground truth is the cluster containing the nearest neighbor. The routing function is trained using cross-entropy loss with a softmax layer to predict the most relevant cluster. Once trained, the learnt representatives replace standard centroids, and the new routing function is used for ANN search by selecting the top-ℓ clusters with highest scores.

## Key Results
- Learnt representatives consistently outperform standard centroids across multiple datasets and embedding models
- The method achieves substantial improvements in both top-1 and top-k accuracy
- Linear routing functions provide the best balance of accuracy and computational efficiency

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Learning-to-Rank formulation transforms the routing problem into a ranking problem amenable to standard LTR techniques.
- Mechanism: The routing function selects top-ℓ clusters by scoring each cluster's representative. By treating this as a ranking problem, we can use LTR to learn a scoring function that orders clusters by their probability of containing the nearest neighbor.
- Core assumption: Each query has exactly one relevant cluster (containing the nearest neighbor) with probability 1.
- Evidence anchors:
  - [abstract] "The routing function solves a ranking problem, making the function amenable to learning-to-rank"
  - [section 5.1] "The routing function τ is a ranking function that addresses the problem of ordering partitions from the most similar, which has the highest probability of containing the actual top-k documents for a given query, to the least similar"
  - [corpus] Weak evidence - no direct mention of LTR formulation in neighbor papers
- Break condition: If the assumption of single relevant cluster fails, the cross-entropy loss formulation becomes invalid.

### Mechanism 2
- Claim: Learnt representative points significantly outperform standard cluster centroids.
- Mechanism: By learning representative points through LTR optimization, we obtain vectors that better capture cluster semantics and improve routing accuracy.
- Core assumption: The learnt representatives {νi}L
i=1 can better capture the semantic meaning of clusters than the standard centroids {µi}L
i=1
- Evidence anchors:
  - [abstract] "learning cluster representatives using a simple linear function significantly boosts the accuracy"
  - [section 5.4.1] "our proposed Learnt method consistently outperforms the Baseline, significantly improving the accuracy"
  - [corpus] Weak evidence - no direct comparison of learnt vs standard representatives in neighbor papers
- Break condition: If learnt representatives don't improve accuracy or degrade performance compared to centroids.

### Mechanism 3
- Claim: Linear routing functions are optimal for this problem.
- Mechanism: The learnt linear routing function ´τ(q; W) = Wq achieves better accuracy than nonlinear alternatives while maintaining computational efficiency.
- Core assumption: Linear functions provide sufficient expressiveness for routing while being computationally efficient.
- Evidence anchors:
  - [section 5.5.1] "the learnt linear routing function, ´τ, is the most suitable choice compared to the nonlinear functions examined"
  - [abstract] "learning cluster representatives using a simple linear function significantly boosts the accuracy"
  - [corpus] Weak evidence - no direct comparison of linear vs nonlinear routing functions in neighbor papers
- Break condition: If nonlinear functions consistently outperform linear ones in accuracy.

## Foundational Learning

- Concept: Learning-to-Rank (LTR) fundamentals
  - Why needed here: The entire method relies on formulating routing as a ranking problem and applying LTR techniques
  - Quick check question: What is the difference between pointwise, pairwise, and listwise LTR approaches?

- Concept: Clustering algorithms (Standard KMeans, Spherical KMeans, Shallow KMeans)
  - Why needed here: The method builds upon these clustering algorithms to create partitions, then learns better representatives
  - Quick check question: How does Spherical KMeans differ from Standard KMeans in terms of objective function?

- Concept: Maximum Inner Product Search (MIPS) problem
  - Why needed here: The routing function uses inner product similarity to rank clusters, and the overall goal is MIPS-based ANN search
  - Quick check question: Why is MIPS different from Euclidean distance-based nearest neighbor search?

## Architecture Onboarding

- Component map:
  - Embedding model -> Clustering algorithm -> Base routing function -> Learnt routing function -> ANN search pipeline

- Critical path:
  1. Embed dataset and queries
  2. Cluster document vectors into L partitions
  3. Create training data: (query, ground-truth cluster)
  4. Train linear routing function via LTR
  5. Replace centroids with learnt representatives
  6. Use new routing function for ANN search

- Design tradeoffs:
  - Linear vs nonlinear routing functions (accuracy vs complexity)
  - Cluster count L vs accuracy (more clusters = finer granularity but higher computational cost)
  - Training data size vs generalization (more queries = better learned routing but longer training)

- Failure signatures:
  - No accuracy improvement over baseline
  - Training loss doesn't converge
  - Learnt representatives are identical or very similar to centroids
  - Routing function performs poorly on validation set

- First 3 experiments:
  1. Verify that learnt routing function matches baseline accuracy on small dataset
  2. Test with different cluster counts (L) to find sweet spot
  3. Compare linear vs nonlinear routing functions on validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the dimensionality of query and document vectors impact the accuracy of the learnt representative points?
- Basis in paper: [explicit] The paper notes that when comparing 768-dimensional embeddings to 384-dimensional embeddings, the accuracy gap between the learnt and baseline methods is less pronounced for higher dimensions.
- Why unresolved: The paper does not provide a detailed analysis of how dimensionality affects the learning process or the effectiveness of the learnt representatives.
- What evidence would resolve it: Systematic experiments varying the dimensionality of embeddings and measuring the impact on accuracy for different cluster sizes and datasets would clarify this relationship.

### Open Question 2
- Question: Can the learning approach for the routing function be generalized to top-k retrieval with k > 1, and what would be the impact on accuracy?
- Basis in paper: [explicit] The paper mentions that generalizing the learning approach to top-k with k > 1 is straightforward but does not provide experimental results or detailed analysis.
- Why unresolved: The paper only discusses the top-1 case and provides preliminary results for top-k without exploring the full potential of the generalization.
- What evidence would resolve it: Implementing and testing the generalized learning approach for top-k retrieval, comparing the results to the top-1 case and baseline methods, would provide insights into its effectiveness.

### Open Question 3
- Question: How does the proposed method perform with distance functions other than inner product, such as Euclidean distance, cosine distance, or Manhattan distance?
- Basis in paper: [explicit] The paper focuses on the k-MIPS problem using the inner product distance and does not explore other distance functions.
- Why unresolved: The paper does not investigate the applicability of the method to other distance functions, which are commonly used in ANN search.
- What evidence would resolve it: Conducting experiments using different distance functions and evaluating the performance of the learnt representative points compared to the baseline would determine the method's versatility.

## Limitations
- The method assumes exactly one relevant cluster per query, which may not hold in practice
- Linear routing functions may lack expressiveness for complex datasets
- No comparison with state-of-the-art ANN methods like HNSW or IVF-PQ

## Confidence
- High: The LTR formulation of routing as a ranking problem is well-grounded and supported by experimental results
- Medium: The claim that learnt representatives significantly outperform standard centroids is supported by experiments but lacks direct comparisons in literature
- Medium: The assertion that linear routing functions are optimal is based on limited experimental comparisons and may not generalize to all scenarios

## Next Checks
1. Test the method's robustness with varying numbers of relevant clusters per query to assess the impact on accuracy
2. Compare the performance of the proposed method against state-of-the-art ANN techniques like HNSW or IVF-PQ on the same datasets
3. Evaluate the method's scalability and efficiency on large-scale datasets to determine its practicality in production environments