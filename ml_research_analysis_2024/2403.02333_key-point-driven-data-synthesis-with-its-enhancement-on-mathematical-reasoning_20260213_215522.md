---
ver: rpa2
title: Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning
arxiv_id: '2403.02333'
source_url: https://arxiv.org/abs/2403.02333
tags:
- math
- arxiv
- data
- reasoning
- number
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a key-point-driven data synthesis framework
  (KPDDS) for enhancing mathematical reasoning in large language models (LLMs). The
  method extracts key points and topics from seed problems, constructs a topic co-occurrence
  probability matrix, and generates new problems and solutions using a scoring and
  consensus assessment mechanism.
---

# Key-Point-Driven Data Synthesis with its Enhancement on Mathematical Reasoning

## Quick Facts
- arXiv ID: 2403.02333
- Source URL: https://arxiv.org/abs/2403.02333
- Reference count: 40
- Qwen1.5-72B achieves 87.0% PASS@1 on GSM8K and 58.3% on MATH benchmarks

## Executive Summary
This paper introduces KPDDS (Key-Point-Driven Data Synthesis), a framework for generating high-quality mathematical reasoning datasets by extracting key points and topics from seed problems and using them to guide synthetic data generation. The method constructs a topic co-occurrence probability matrix and employs a scoring and consensus assessment mechanism to ensure quality. When fine-tuned on the resulting KPMath-Plus dataset, the Qwen1.5-72B model achieves state-of-the-art results on both GSM8K (87.0%) and MATH (58.3%) benchmarks, outperforming competitors in the 7B to 70B model range.

## Method Summary
KPDDS extracts key points and topics from seed problems using GPT-4, constructs a topic co-occurrence probability matrix (TCPM), and generates novel problems using this matrix as a guide. Generated questions are assessed using a scoring model based on key point coverage and error detection. Multiple solutions are generated for each problem using nucleus sampling, and a voting mechanism aggregates these solutions to ensure correctness. The framework produces KPMath (800K+ pairs) and KPMath-Plus (1.5M+ data points), which are used to fine-tune the Qwen1.5-72B model for 3 epochs with a linear learning rate schedule.

## Key Results
- Qwen1.5-72B achieves 87.0% PASS@1 accuracy on GSM8K benchmark
- Qwen1.5-72B achieves 58.3% PASS@1 accuracy on MATH benchmark
- Model surpasses competitors in the 7B to 70B model size range on mathematical reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: KPDDS leverages key points and exemplar problems to generate novel, diverse mathematical questions while maintaining high quality.
- Mechanism: By extracting key points and topics from seed problems, constructing a topic co-occurrence probability matrix, and using this matrix to guide probabilistic sampling of topics and key points, KPDDS ensures generated questions are both novel and relevant. The scoring model assesses question quality based on the presence of provided key points and absence of logical/factual errors.
- Core assumption: The extracted key points and topics from seed problems are representative and sufficient to generate high-quality, diverse questions.
- Evidence anchors:
  - [abstract]: "KPDDS ensures the generation of novel questions with rigorous quality control and substantial scalability."
  - [section]: "The scoring model then assesses the quality of these questions, allowing only those with high scores to proceed."
  - [corpus]: "Average neighbor FMR=0.441, average citations=0.0." (Weak evidence of related work; KPDDS approach appears novel)
- Break condition: If the key points extracted from seed problems are not representative or sufficient, the generated questions may lack diversity or relevance.

### Mechanism 2
- Claim: The consensus assessment mechanism minimizes the effects of noisy data and enhances the reliability of the answer-generation process.
- Mechanism: By generating multiple potential solutions using nucleus sampling and applying a voting mechanism to aggregate solutions, KPDDS ensures the correctness of generated answers. The voting mechanism leverages packages like sympy to recognize equivalent answers in different forms.
- Core assumption: The generated solutions are sufficiently diverse to allow for a meaningful consensus to emerge.
- Evidence anchors:
  - [abstract]: "To ensure the correctness of generated answers, we employ a few-shot strategy... Subsequently, a voting mechanism... is employed to aggregate the solutions."
  - [section]: "By integrating a voting protocol, our methodology is designed to minimize the effects of noisy data and enhance the reliability of the answer-generation process."
  - [corpus]: "Average neighbor FMR=0.441, average citations=0.0." (Weak evidence of related work; KPDDS approach appears novel)
- Break condition: If the generated solutions are not sufficiently diverse, the voting mechanism may not be able to identify the correct answer.

### Mechanism 3
- Claim: The combination of KPMath and KPMath-Plus datasets significantly improves the mathematical reasoning capabilities of the Qwen1.5-72B model.
- Mechanism: By fine-tuning the Qwen1.5-72B model on the comprehensive KPMath-Plus dataset, which includes KPMath (derived from MATH and GSM8K datasets) and MixMath (curated from various high-quality open-source mathematical reasoning datasets), the model achieves state-of-the-art results on GSM8K and MATH benchmarks.
- Core assumption: The KPMath-Plus dataset is sufficiently diverse and high-quality to improve the model's mathematical reasoning capabilities.
- Evidence anchors:
  - [abstract]: "By fine-tuning the Qwen1.5-72B model on KPMath-Plus, we achieved zero-shot PASS@1 accuracies of 87.0% on GSM8K and 58.3% on MATH, surpassing competitors in the 7B to 70B model size range."
  - [section]: "The Qwen1.5-72B model, fine-tuned on KPMath-Plus, achieves 87.0% PASS@1 accuracy on GSM8K and 58.3% on MATH, surpassing competitors in the 7B to 70B range and best commercial models like GPT-4 across multiple math reasoning datasets."
  - [corpus]: "Average neighbor FMR=0.441, average citations=0.0." (Weak evidence of related work; KPDDS approach appears novel)
- Break condition: If the KPMath-Plus dataset is not sufficiently diverse or high-quality, the model's performance may not improve significantly.

## Foundational Learning

- Concept: Key points and topics extraction
  - Why needed here: To identify the essential mathematical concepts and their relationships in seed problems, which are then used to generate novel questions.
  - Quick check question: Given a simple math problem, can you identify the key points and topics involved?

- Concept: Topic co-occurrence probability matrix
  - Why needed here: To understand the frequency and distribution of topic pairs within the dataset, which guides the generation of novel questions with appropriate topic combinations.
  - Quick check question: Given a small dataset of math problems, can you construct a topic co-occurrence probability matrix?

- Concept: Consensus assessment mechanism
  - Why needed here: To ensure the correctness of generated answers by aggregating multiple potential solutions using a voting mechanism.
  - Quick check question: Given multiple potential solutions to a math problem, can you implement a voting mechanism to identify the correct answer?

## Architecture Onboarding

- Component map: Knowledge Extraction -> TCPM Construction -> Question Generation with Quality Assessment -> Solution Generation with Consensus Assessment

- Critical path: Knowledge Extraction → TCPM Construction → Question Generation with Quality Assessment → Solution Generation with Consensus Assessment

- Design tradeoffs:
  - Tradeoff between diversity and relevance of generated questions: Using a broader range of key points and topics may increase diversity but decrease relevance.
  - Tradeoff between quality and quantity of generated questions: Stricter quality assessment may decrease the number of generated questions but increase their overall quality.

- Failure signatures:
  - If the generated questions are not diverse or relevant, the Knowledge Extraction or TCPM Construction components may be failing.
  - If the generated answers are incorrect, the Solution Generation with Consensus Assessment component may be failing.

- First 3 experiments:
  1. Test the Knowledge Extraction component by manually verifying the extracted key points and topics from a small set of seed problems.
  2. Test the TCPM Construction component by comparing the generated topic co-occurrence probability matrix with a manually constructed one for a small dataset.
  3. Test the Question Generation with Quality Assessment component by manually evaluating the quality of generated questions for a small set of input key points and topics.

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions. However, based on the methodology and results, several questions arise regarding the scalability, robustness, and generalizability of the KPDDS approach that could be explored in future work.

## Limitations

- The framework's reliance on GPT-4 for key point extraction and quality assessment introduces a potential bottleneck, as the quality of synthesized data is fundamentally limited by GPT-4's performance on mathematical reasoning tasks.
- The consensus assessment mechanism's effectiveness depends heavily on the diversity of generated solutions, but the paper does not provide detailed analysis of solution diversity metrics or failure cases where voting mechanisms might produce incorrect results.
- The assertion that KPDDS "surpasses competitors in the 7B to 70B model size range" is based on comparisons with unspecified competitors and lacks detailed benchmarking data.

## Confidence

**High Confidence:** The technical implementation of the TCPM construction and the basic synthesis pipeline is well-documented and reproducible. The reported benchmark results (87.0% on GSM8K, 58.3% on MATH) are specific and verifiable through the provided code.

**Medium Confidence:** The claim that KPDDS ensures "novel questions with rigorous quality control" is supported by the methodology but lacks comprehensive empirical validation. The paper does not provide detailed analysis of question diversity or quality distribution across the synthesized dataset.

**Low Confidence:** The assertion that KPDDS "surpasses competitors in the 7B to 70B model size range" is based on comparisons with unspecified competitors and lacks detailed benchmarking data. The paper also does not address potential biases introduced during the synthesis process or the generalizability of the approach to other mathematical domains.

## Next Checks

1. **Diversity Analysis:** Conduct a comprehensive analysis of the generated questions' diversity by measuring topic coverage, key point distribution, and similarity metrics between synthesized and original problems to verify the claim of novel question generation.

2. **Quality Validation:** Implement an independent quality assessment pipeline using multiple evaluators to verify the scoring model's effectiveness and identify failure modes in the quality control mechanism.

3. **Robustness Testing:** Test the consensus assessment mechanism's performance across different levels of solution diversity and error rates to establish its reliability boundaries and identify conditions under which it may fail.