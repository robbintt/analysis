---
ver: rpa2
title: Toward Robust Multimodal Learning using Multimodal Foundational Models
arxiv_id: '2401.13697'
source_url: https://arxiv.org/abs/2401.13697
tags:
- modality
- multimodal
- missing
- modalities
- text
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a simple yet robust multimodal learning framework
  that extends the capabilities of CLIP-based multimodal foundational models to handle
  scenarios with missing modalities. The framework generates virtual modalities to
  replace missing ones and aligns the semantic spaces between generated and missing
  modalities.
---

# Toward Robust Multimodal Learning using Multimodal Foundational Models

## Quick Facts
- arXiv ID: 2401.13697
- Source URL: https://arxiv.org/abs/2401.13697
- Reference count: 9
- Primary result: Framework extends CLIP-based multimodal models to handle missing modalities by generating virtual modalities and aligning semantic spaces

## Executive Summary
This paper addresses the challenge of missing modalities in multimodal learning by proposing a robust framework that extends CLIP-based multimodal foundational models. The framework generates virtual modalities to replace missing ones and aligns the semantic spaces between generated and missing modalities. It consists of three components: a multimodal foundational model to learn latent semantic correlations, a missing modality inference module to generate virtual modalities, and a semantic matching learning module to align semantic spaces. Experiments on three multimodal sentiment analysis benchmark datasets (CMU-MOSI, CMU-MOSEI, and MELD) demonstrate the effectiveness and robustness of the proposed framework.

## Method Summary
The proposed framework consists of three main components: a multimodal foundational model (MFM) using pre-trained CLIP encoders, a missing modality inference module with Text-2-Visual and Visual-2-Text encoders (MLPs), and a semantic matching learning module. The framework generates virtual modalities when one modality is missing and uses a similarity matrix with cross-entropy loss to align the semantic spaces between generated and original modalities. The model is trained with a combined loss function that balances task-specific objectives (MSE or cross-entropy) with semantic matching loss.

## Key Results
- The model achieves significant improvements over previous methods, with performance closer to the upper bound when visual modality is missing
- Improvements of 1-11% are observed when text modality is missing
- The framework is flexible and can enhance the robustness of any CLIP-based multimodal foundational model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Missing modality inference leverages cross-modal semantic alignment from CLIP to generate semantically consistent virtual modalities
- Core assumption: The CLIP embedding space preserves sufficient semantic correlations to allow one modality to reconstruct missing information from another
- Evidence anchors:
  - [abstract] "The framework extends the capabilities of the multimodal foundational models to handle scenarios with missing modalities by utilizing the latent semantic association"
  - [section 3.4] "Previous work [Radford et al., 2021] has shown that contrastive learning has a powerful ability to align deep semantic associations across modalities"
  - [corpus] Weak - neighbors focus on diffusion-based recovery or invariant learning, not CLIP-based semantic reconstruction
- Break condition: If the semantic correlation between modalities is too weak or domain-specific, the generated virtual modality will be semantically incoherent

### Mechanism 2
- Claim: Semantic matching learning enforces the generated virtual modality to retain the original modality's semantic properties
- Core assumption: The semantic matching loss can effectively guide the virtual modality generation to capture missing semantic content
- Evidence anchors:
  - [section 3.4] "the semantic matching learning module aligns the cross-modal semantic space under the prompt of original modality"
  - [section 4.3] "Figures (c) ∼(d) suggest a very high semantic similarity between our generated modalities and the original modalities"
  - [corpus] Weak - neighbors focus on reconstruction accuracy but not explicit semantic space alignment
- Break condition: If the semantic space alignment is insufficient, virtual modalities will not capture the missing modality's semantic content

### Mechanism 3
- Claim: The framework's flexibility allows swapping different CLIP-based multimodal foundational models without retraining the entire pipeline
- Core assumption: Different CLIP-based models learn compatible semantic spaces that the semantic matching module can align
- Evidence anchors:
  - [abstract] "Our model achieves a more prominent performance than previous works. In addition, compared with the state-of-the-art work, our model can deal with scenes that are missing at the frame level"
  - [section 4.2] "Figure 4 illustrates the performance of various multimodal foundational models embedded in TRML"
  - [corpus] Weak - neighbors focus on modality reconstruction but not framework modularity
- Break condition: If different foundational models produce incompatible embedding spaces, semantic matching will fail

## Foundational Learning

- Concept: Cross-modal contrastive learning
  - Why needed here: It provides the aligned embedding space where missing modalities can be inferred from present ones
  - Quick check question: Does the embedding space preserve semantic correlations when one modality is missing?

- Concept: Semantic space alignment via similarity matrices
  - Why needed here: It ensures generated virtual modalities capture the original modality's semantic properties
  - Quick check question: Does the similarity matrix capture meaningful semantic relationships between original and virtual modalities?

- Concept: Modality-specific inference through MLPs
  - Why needed here: It generates modality-specific virtual representations conditioned on the present modality
  - Quick check question: Can the MLPs generate coherent virtual modalities when prompted by the remaining modality?

## Architecture Onboarding

- Component map: Multimodal Foundational Model (MFM) -> Missing Modality Inference -> Semantic Matching -> Task layers
- Critical path: MFM → Missing Modality Inference → Semantic Matching → Task layers
- Design tradeoffs:
  - Using CLIP's fixed embeddings vs. fine-tuning: Fixed embeddings provide better generalization but may limit adaptation to specific domains
  - Simple MLPs vs. complex generative models: MLPs are faster and more stable but may produce less realistic virtual modalities
  - Symmetric vs. asymmetric semantic matching: Symmetric matching provides balanced alignment but may over-constrain the model
- Failure signatures:
  - Poor performance on missing modality scenarios: Indicates semantic matching loss is ineffective
  - Virtual modalities far from original in embedding space: Indicates inference module is not capturing semantic correlations
  - Overfitting to training data: Indicates semantic matching is too strong relative to task loss
- First 3 experiments:
  1. Test semantic matching loss ablation: Compare performance with and without semantic matching to verify its importance
  2. Test inference module alone: Generate virtual modalities and measure similarity to original modalities without training task-specific layers
  3. Test different CLIP variants: Swap CLIP with X-CLIP or ImageBind to verify framework flexibility

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed framework scale with increasing missing modality ratios beyond the 90% tested in the paper?
- Basis in paper: [explicit] The paper tests missing modality ratios up to 90% (p=10%), but does not explore higher ratios
- Why unresolved: The paper focuses on demonstrating effectiveness at high but not complete missing modality ratios, leaving the performance at extreme ratios unexplored
- What evidence would resolve it: Experiments testing the framework's performance at missing modality ratios of 95% and 99% on the same datasets would provide insights into its scalability and limitations

### Open Question 2
- Question: How does the framework's performance compare to other methods when handling missing modalities in real-time scenarios?
- Basis in paper: [inferred] The paper focuses on static datasets and does not address the computational efficiency or latency of the framework in real-time applications
- Why unresolved: Real-time processing of missing modalities is a critical requirement for many practical applications, but the paper does not evaluate this aspect
- What evidence would resolve it: Benchmarking the framework's inference time and resource usage on streaming data compared to other methods would provide insights into its real-time capabilities

### Open Question 3
- Question: Can the framework be extended to handle more than two modalities effectively?
- Basis in paper: [explicit] The paper focuses on scenarios with two modalities (text and visual) and does not explore the framework's performance with additional modalities like audio
- Why unresolved: Many real-world applications involve more than two modalities, but the paper does not investigate the framework's scalability to handle such cases
- What evidence would resolve it: Experiments testing the framework's performance on datasets with three or more modalities (e.g., adding audio to the CMU-MOSI dataset) would demonstrate its generalizability

## Limitations

- The framework relies heavily on the semantic correlation strength captured by pre-trained CLIP embeddings, which may not hold for weak or domain-specific correlations
- The assumption that semantic matching loss alone is sufficient to align complex semantic spaces may not generalize to all scenarios
- The evaluation focuses on multimodal sentiment analysis and emotion recognition, limiting generalizability to other multimodal tasks

## Confidence

- High Confidence: The framework's ability to generate virtual modalities using MLPs and semantic matching loss is well-supported by the experiments and ablation studies
- Medium Confidence: The claim that the framework can enhance any CLIP-based multimodal foundational model is supported by experiments but limited by the evaluation scope
- Low Confidence: The assumption that semantic matching loss alone is sufficient to align complex semantic spaces may not generalize to all scenarios

## Next Checks

1. Test semantic matching loss ablation: Compare performance with and without semantic matching loss to verify its importance and effectiveness in aligning semantic spaces

2. Test inference module alone: Generate virtual modalities and measure their similarity to original modalities without training task-specific layers to assess the quality of the generated representations

3. Test different CLIP variants: Swap CLIP with X-CLIP or ImageBind to verify framework flexibility and ensure compatibility with different semantic spaces