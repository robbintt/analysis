---
ver: rpa2
title: Multi-modal Auto-regressive Modeling via Visual Words
arxiv_id: '2403.07720'
source_url: https://arxiv.org/abs/2403.07720
tags:
- visual
- multi-modal
- image
- information
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Visual Tokens (VT-LMM), a novel approach
  that enables Large Multi-modal Models (LMMs) to perform auto-regressive modeling
  on both visual and language sequences. The core challenge addressed is that existing
  LMMs only optimize for language components during training, neglecting visual information
  supervision.
---

# Multi-modal Auto-regressive Modeling via Visual Words

## Quick Facts
- **arXiv ID**: 2403.07720
- **Source URL**: https://arxiv.org/abs/2403.07720
- **Reference count**: 40
- **Primary result**: VT-LMM-Mistral-7B achieves competitive or superior performance to larger LMMs (13B-33B) across multiple VQA and LMM evaluation benchmarks

## Executive Summary
This paper introduces Visual Tokens (VT-LMM), a novel approach that enables Large Multi-modal Models (LMMs) to perform auto-regressive modeling on both visual and language sequences. The core challenge addressed is that existing LMMs only optimize for language components during training, neglecting visual information supervision. VT-LMM proposes mapping visual features into probability distributions over the LLM's vocabulary, creating "visual tokens" that serve as supervised labels for visual modeling. The method demonstrates strong performance across 5 VQA benchmarks and 4 LMM evaluation toolkits, with the 7B parameter model achieving results competitive with or exceeding larger models (13B-33B). Notably, the VT-LMM-Mistral-7B achieved best or second-best performance across all evaluated metrics, including 87.3% accuracy on adversarial object hallucination evaluation.

## Method Summary
VT-LMM addresses the fundamental limitation in LMMs where visual information is not explicitly modeled during training. The method maps visual features extracted from an image encoder into probability distributions over the LLM's vocabulary, treating these distributions as "visual tokens" that can be predicted alongside language tokens. During training, the model learns to predict both language and visual tokens using the LLM's language modeling objective, enabling end-to-end optimization of visual information. The approach leverages a visual encoder (CLIP-ViT) to extract features, which are then mapped to token probabilities using a learned projection. During inference, images are split into patches and encoded into visual tokens, which are then processed by the LLM in an auto-regressive manner alongside language tokens. The method is implemented through adapter-based finetuning of existing LMMs, requiring no additional training parameters or architectural changes.

## Key Results
- VT-LMM-Mistral-7B achieved best or second-best performance across all evaluated LMM metrics
- 87.3% accuracy on adversarial object hallucination evaluation
- Outperformed larger models (13B-33B) on multiple VQA benchmarks
- Successfully addressed visual modeling while maintaining language modeling capabilities

## Why This Works (Mechanism)
The method works by bridging the semantic gap between visual and language modalities through probability distribution mapping. Visual features are projected into the LLM's vocabulary space, creating a common semantic representation that allows the language model to directly model visual information. This approach enables the LLM to learn visual concepts using its existing language understanding capabilities, avoiding the need for specialized visual modeling architectures. The adapter-based finetuning approach allows the method to be applied to existing LMMs without architectural modifications, making it practical and scalable.

## Foundational Learning

**Visual Feature Extraction**: Understanding how CLIP-ViT extracts visual features from images
- *Why needed*: Visual features form the basis for creating visual tokens
- *Quick check*: Verify feature extraction produces meaningful representations

**Probability Distribution Mapping**: Converting visual features to probability distributions over vocabulary
- *Why needed*: Enables semantic bridging between visual and language modalities
- *Quick check*: Ensure distributions capture relevant visual information

**Auto-regressive Modeling**: Sequential prediction of both visual and language tokens
- *Why needed*: Allows the model to generate coherent multi-modal sequences
- *Quick check*: Validate token prediction accuracy and sequence coherence

**Adapter-based Finetuning**: Adding task-specific adapters to existing LMMs
- *Why needed*: Enables adaptation without modifying base architecture
- *Quick check*: Confirm adapter integration doesn't degrade base model performance

**Multi-modal Evaluation**: Assessing model performance across visual and language tasks
- *Why needed*: Validates effectiveness of visual modeling approach
- *Quick check*: Verify evaluation metrics capture both visual and language capabilities

## Architecture Onboarding

**Component Map**: Visual Encoder (CLIP-ViT) -> Visual Feature Projection -> LLM with Adapters -> Token Prediction

**Critical Path**: Image → Visual Encoder → Feature Projection → LLM Input → Token Generation

**Design Tradeoffs**: Adapter-based approach trades some parameter efficiency for easier integration with existing models; visual feature resolution vs. computational cost; vocabulary size vs. semantic granularity

**Failure Signatures**: 
- Poor visual token prediction accuracy
- Degradation in language modeling performance
- Inconsistent visual representations across similar images
- High computational overhead during inference

**First Experiments**:
1. Validate visual token prediction accuracy on held-out visual features
2. Compare language modeling performance before and after adapter integration
3. Test visual representation consistency across similar image pairs

## Open Questions the Paper Calls Out
None

## Limitations
- The semantic correspondence between visual regions and language tokens lacks theoretical grounding
- Limited evaluation on diverse multi-modal tasks beyond VQA and LMM-specific benchmarks
- No ablation studies on visual feature extraction methods or vocabulary size impact
- Claims about "strong visual modeling capability" are based on downstream task performance rather than direct visual metrics

## Confidence

**High Confidence**: Empirical results showing VT-LMM's strong performance across multiple VQA benchmarks and LMM evaluation tools, including the 87.3% accuracy on adversarial object hallucination evaluation.

**Medium Confidence**: Claims about the effectiveness of the visual token mapping strategy, which are supported by results but lack theoretical justification and comprehensive ablation studies.

**Low Confidence**: Broader claims about VT-LMM being a "powerful tool for multi-modal auto-regressive modeling" and achieving "state-of-the-art results," given the limited scope of evaluations and lack of comparison to diverse multi-modal tasks.

## Next Checks

1. **Ablation studies on visual feature extraction**: Evaluate the impact of different visual backbone architectures (CLIP, DINOv2, SigLIP) and feature resolutions on VT-LMM's performance.

2. **Direct visual representation evaluation**: Conduct controlled experiments using visual-only tasks (image classification, object detection) to assess whether VT-LMM truly learns meaningful visual representations.

3. **Generalization to diverse multi-modal tasks**: Test VT-LMM on a broader range of multi-modal benchmarks including image generation, video understanding, and complex reasoning tasks to validate claims beyond VQA.