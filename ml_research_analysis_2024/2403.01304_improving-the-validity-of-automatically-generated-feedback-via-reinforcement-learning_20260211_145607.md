---
ver: rpa2
title: Improving the Validity of Automatically Generated Feedback via Reinforcement
  Learning
arxiv_id: '2403.01304'
source_url: https://arxiv.org/abs/2403.01304
tags:
- feedback
- gpt-4
- answer
- incorrect
- student
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors address the challenge of automatically generating pedagogically
  valid feedback for incorrect student responses in math. They propose a rubric-based
  evaluation framework and show that GPT-4 can effectively use it to annotate both
  human-written and LLM-generated feedback with high agreement to human annotators.
---

# Improving the Validity of Automatically Generated Feedback via Reinforcement Learning

## Quick Facts
- arXiv ID: 2403.01304
- Source URL: https://arxiv.org/abs/2403.01304
- Reference count: 39
- The authors propose a rubric-based evaluation framework showing GPT-4 can effectively annotate feedback with high human agreement, enabling DPO fine-tuning of Llama 2 to improve feedback correctness and alignment

## Executive Summary
This paper addresses the challenge of automatically generating pedagogically valid feedback for incorrect student responses in math problems. The authors develop a rubric-based evaluation framework and demonstrate that GPT-4 can effectively use this rubric to annotate both human-written and LLM-generated feedback with high agreement to human annotators. Using GPT-4's annotations, they construct a preference dataset and employ direct preference optimization (DPO) to fine-tune Llama 2 for feedback generation. Experiments on a math multiple-choice dataset show that their method significantly improves feedback correctness and alignment compared to baselines, approaching GPT-4's performance with a much smaller model.

## Method Summary
The authors propose a rubric-based evaluation framework to assess feedback validity in educational contexts. They first demonstrate that GPT-4 can effectively annotate feedback using this rubric with high agreement to human annotators. This two-stage process creates a preference dataset by having GPT-4 compare pairs of feedback annotations. The preference data is then used to fine-tune Llama 2 through direct preference optimization (DPO), a reinforcement learning technique that optimizes models based on human preferences. The resulting fine-tuned model generates feedback that is evaluated against baselines for correctness and alignment with pedagogical principles, showing significant improvements over standard fine-tuning approaches.

## Key Results
- GPT-4 annotations of feedback show high agreement with human annotators when using the rubric-based evaluation framework
- DPO fine-tuning of Llama 2 using GPT-4's preference dataset significantly improves feedback correctness and alignment
- The fine-tuned Llama 2 model approaches GPT-4's performance in feedback generation while being substantially smaller
- Rubric-based evaluation provides a scalable method for assessing and improving automated feedback quality

## Why This Works (Mechanism)
The approach works by creating a structured evaluation framework that translates subjective pedagogical quality into objective rubric criteria. GPT-4 serves as a scalable annotator that can process large volumes of feedback against these criteria, creating preference pairs that capture subtle quality differences. DPO then optimizes the Llama 2 model to generate feedback that maximizes these preferences, effectively learning the patterns of high-quality feedback. This two-stage process leverages GPT-4's reasoning capabilities for evaluation while producing a smaller, deployable model that embodies the learned feedback quality patterns.

## Foundational Learning
- **Rubric-based evaluation frameworks**: Needed to standardize subjective pedagogical assessment; quick check: test rubric applicability across different subject domains
- **Direct Preference Optimization (DPO)**: A reinforcement learning technique that optimizes models based on pairwise preference data; quick check: compare DPO against other RLHF variants
- **Feedback quality dimensions**: Correctness, alignment with learning objectives, and pedagogical soundness; quick check: validate these dimensions with educational experts
- **LLM annotation capabilities**: Using large models to evaluate and compare outputs; quick check: measure inter-annotator agreement between GPT-4 and human experts
- **Preference dataset construction**: Creating training data from comparative judgments; quick check: analyze preference distribution for bias

## Architecture Onboarding

**Component Map:**
Student Response -> Feedback Generator -> Rubric Evaluator (GPT-4) -> Preference Dataset -> DPO Fine-tuning -> Optimized Feedback Generator

**Critical Path:**
The most critical path is: Feedback Generator → Rubric Evaluator → Preference Dataset → DPO Fine-tuning. This path determines the quality of the final model, as errors in evaluation or preference ranking will propagate through the fine-tuning process.

**Design Tradeoffs:**
- Using GPT-4 for evaluation enables scalability but introduces dependency on a black-box model and potential evaluation bias
- DPO provides efficient fine-tuning compared to full RLHF but may be less flexible in handling complex reward structures
- The rubric-based approach standardizes evaluation but may miss nuanced pedagogical aspects not captured in the rubric

**Failure Signatures:**
- Poor feedback quality despite high evaluation scores suggests rubric misalignment with actual pedagogical needs
- Inconsistent preference rankings indicate evaluation instability or ambiguous rubric criteria
- Limited improvement during fine-tuning suggests preference data lacks sufficient diversity or signal

**First 3 Experiments:**
1. Validate rubric effectiveness by comparing human vs. GPT-4 evaluations on a held-out feedback set
2. Test baseline feedback generation quality without fine-tuning to establish performance floor
3. Conduct ablation study removing rubric criteria to identify which aspects most impact feedback quality

## Open Questions the Paper Calls Out
None

## Limitations
- Heavy reliance on GPT-4 for annotation creates potential stability and generalizability concerns across domains
- Focus on multiple-choice math problems may limit applicability to open-ended questions or other subject areas
- The nature of agreement between GPT-4 and human annotators, including potential systematic biases, remains unclear

## Confidence
- **High confidence**: Technical implementation of DPO fine-tuning and experimental design for comparing model variants
- **Medium confidence**: Effectiveness of GPT-4 as an evaluator for feedback quality
- **Medium confidence**: Generalizability of results to non-mathematical domains or different educational levels

## Next Checks
1. Conduct cross-domain validation by applying the same methodology to open-ended questions in humanities or social sciences to test rubric adaptability and feedback quality across educational contexts
2. Perform ablation studies removing GPT-4 annotations to assess whether human-only annotations could achieve similar results
3. Test model performance across multiple student ability levels and question difficulties to determine whether improvements hold consistently