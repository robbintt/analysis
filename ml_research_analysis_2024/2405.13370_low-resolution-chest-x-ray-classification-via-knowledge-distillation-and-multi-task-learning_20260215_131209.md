---
ver: rpa2
title: Low-Resolution Chest X-ray Classification via Knowledge Distillation and Multi-task
  Learning
arxiv_id: '2405.13370'
source_url: https://arxiv.org/abs/2405.13370
tags: []
core_contribution: This paper introduces MLCAK, a novel knowledge distillation framework
  designed to enhance low-resolution chest X-ray classification. The method transfers
  discriminative attention knowledge from a high-resolution teacher model to a low-resolution
  student model using Vision Transformers, employing both multi-label and multi-class
  classification tasks.
---

# Low-Resolution Chest X-ray Classification via Knowledge Distillation and Multi-task Learning

## Quick Facts
- arXiv ID: 2405.13370
- Source URL: https://arxiv.org/abs/2405.13370
- Authors: Yasmeena Akhter; Rishabh Ranjan; Richa Singh; Mayank Vatsa
- Reference count: 0
- Key outcome: MLCAK improves low-resolution chest X-ray classification with AUC increases up to 0.0411 for multi-label and 0.0211 for multi-class tasks

## Executive Summary
This paper introduces MLCAK, a knowledge distillation framework that transfers diagnostic knowledge from high-resolution chest X-rays to low-resolution versions using Vision Transformers. The method employs a multi-task learning approach combining multi-label and multi-class classification tasks, along with attention knowledge transfer through averaging self-attention maps across transformer layers. Experiments on the Vindr CXR dataset demonstrate substantial improvements in diagnostic accuracy at resolutions as low as 28×28 pixels.

## Method Summary
The MLCAK framework uses a high-resolution teacher ViT and low-resolution student ViT, transferring knowledge through soft logits and averaged self-attention maps. The method employs multi-task learning with BCE loss for both multi-label (local pathological findings) and multi-class (global normal/abnormal) classification tasks. Knowledge distillation occurs through MSE loss between teacher and student representations, with the student model trained on resolutions ranging from 112×112 down to 28×28 pixels.

## Key Results
- AUC improvements of up to 0.0411 for multi-label classification at 28×28 resolution
- AUC improvements of up to 0.0211 for multi-class classification at 28×28 resolution
- Enhanced model explainability through identification of diseased pixels in low-resolution images
- Consistent performance gains across multiple ViT architectures (Tiny, Small, Base)

## Why This Works (Mechanism)

### Mechanism 1
Multilevel Collaborative Attention Knowledge (MLCAK) improves student performance by averaging self-attention maps across all 12 encoder layers, creating consolidated attention representations that capture critical spatial relationships in high-resolution images. This assumes that averaging preserves discriminative spatial features while filtering noise. Evidence shows MLCAK "incorporates local pathological findings to boost model explainability" through mean attention map calculation. The break condition occurs if attention maps contain contradictory information, potentially diluting important features.

### Mechanism 2
Multi-task learning enhances both local and global disease detection through complementary BCE loss functions for multi-label (local pathological findings) and multi-class (global normal/abnormal) classification. This assumes local and global tasks provide complementary information improving overall diagnostic performance. Evidence indicates MLCT "focuses on local label information, while MCCT captures global information" with joint optimization. Break conditions include task dissimilarity creating conflicting gradients that harm performance.

### Mechanism 3
Knowledge distillation transfers critical diagnostic knowledge from high-resolution teacher to low-resolution student through three components: soft logits from both tasks and MLCAK attention maps, optimized using MSE loss. This assumes teacher high-resolution attention and prediction patterns contain valuable information transferable to improve low-resolution performance. Evidence shows MLCAK "transfer critical diagnostic knowledge from high-resolution images to enhance the diagnostic efficacy of low-resolution CXRs" through dual-input architecture. Break conditions include poorly trained teachers or large domain gaps introducing harmful biases.

## Foundational Learning

- Concept: Vision Transformer architecture and self-attention mechanisms
  - Why needed here: MLCAK leverages self-attention maps from ViT to transfer spatial information between teacher and student models
  - Quick check question: What is the difference between self-attention in ViT and convolutional filters in CNNs?

- Concept: Knowledge distillation principles and multi-task learning
  - Why needed here: The method combines response-based and feature-based distillation within a multi-task framework to improve low-resolution classification
  - Quick check question: How does multi-task learning with BCE loss differ from single-task classification in terms of gradient flow?

- Concept: Medical image analysis and domain-specific challenges
  - Why needed here: Understanding why low-resolution CXRs lose spatial information and how this affects diagnostic accuracy is crucial for appreciating MLCAK's value
  - Quick check question: Why are small anomalies like nodules particularly challenging to detect in low-resolution medical images?

## Architecture Onboarding

- Component map: Input → Patch embedding → Encoder blocks → MLCAK attention extraction → Multi-task classification → Joint loss optimization
- Critical path: Teacher Model (T) with HR images → Student Model (S) with LR images → MLCAK module → MLCT and MCCT branches → BCE and MSE losses
- Design tradeoffs: Fixed model complexity vs. adaptive student architecture; N=12 attention layers to average vs. computational efficiency; weight parameters (α, β, γ) vs. task prioritization
- Failure signatures: Performance degradation from inconsistent attention maps; poor transfer from dissimilar HR/LR resolutions; suboptimal multi-task balance from task dominance
- First 3 experiments: 1) Baseline comparison: Train student without KD on low-res images; 2) Individual block transfer: Transfer from single attention blocks vs. averaging; 3) Ablation study: Remove MLCT or MCCT task to measure impact

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implications arise from the methodology and results.

## Limitations
- Computational complexity of averaging attention maps across 12 encoder layers may be prohibitive for resource-constrained deployment
- Method's generalization to other medical imaging modalities beyond chest X-rays remains untested
- Attention map averaging mechanism lacks theoretical justification for why layer-wise averaging specifically enhances diagnostic performance

## Confidence
- High Confidence: Performance improvements on Vindr CXR dataset (AUC increases of 0.0411 for multi-label and 0.0211 for multi-class classification)
- Medium Confidence: MLCAK enhances model explainability through identification of diseased pixels, supported by qualitative visualizations but lacking quantitative metrics
- Low Confidence: Claims about method's generalizability to other medical imaging domains are unsupported by experimental evidence beyond chest X-rays

## Next Checks
1. Ablation Study on Attention Aggregation Methods: Compare MLCAK's layer-wise averaging against alternative techniques (max pooling, weighted averaging, specific layer selection) to validate optimal performance
2. Cross-Domain Generalization Test: Evaluate MLCAK on other medical imaging datasets (dermatology images, retinal scans) to assess broader applicability claims
3. Computational Efficiency Analysis: Quantify MLCAK's computational overhead and evaluate whether performance gains justify additional complexity in resource-constrained deployment scenarios