---
ver: rpa2
title: Learning under Label Noise through Few-Shot Human-in-the-Loop Refinement
arxiv_id: '2401.14107'
source_url: https://arxiv.org/abs/2401.14107
tags:
- noise
- label
- labels
- learning
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces FHLR, a method for learning deep models from
  wearable sensor data with noisy labels. FHLR combines weak label training, few-shot
  expert label refinement, and weighted model averaging to improve robustness and
  generalization.
---

# Learning under Label Noise through Few-Shot Human-in-the-Loop Refinement

## Quick Facts
- arXiv ID: 2401.14107
- Source URL: https://arxiv.org/abs/2401.14107
- Reference count: 7
- Achieves up to 19% accuracy improvement under label noise

## Executive Summary
This paper introduces FHLR, a method for learning deep models from wearable sensor data with noisy labels. FHLR combines weak label training, few-shot expert label refinement, and weighted model averaging to improve robustness and generalization. Evaluated on four tasks—sleep scoring, activity recognition, cardiac arrhythmia detection, and artifact detection—FHLR outperforms eight baselines, achieving up to 19% accuracy improvement under label noise. Notably, FHLR remains robust even with high noise levels (up to 60%), unlike prior methods that degrade significantly. The approach is effective for real-world wearable sensing, where obtaining clean labels is costly and challenging.

## Method Summary
FHLR is a three-stage method for learning from noisy labels in wearable sensor data. First, it trains a seed model using label smoothing to create softened labels that prevent overfitting to noise. Second, it fine-tunes this seed model using a small number of expert-labeled examples (100 in experiments) to correct systematic errors. Finally, it merges the seed and fine-tuned models via weighted parameter averaging to achieve better generalizability and robustness. The method is evaluated on four tasks using different wearable sensor datasets and demonstrates superior performance compared to eight baseline approaches across various noise levels.

## Key Results
- Outperforms eight baselines including CE, LS, Mixup, PL, Bi-T, LC, CL, and FL
- Achieves up to 19% accuracy improvement under label noise
- Maintains robustness at high noise levels (60%) where other methods fail
- Requires only 100 expert labels for fine-tuning, making it cost-effective

## Why This Works (Mechanism)

### Mechanism 1: Label Smoothing as Weak Label Generation
- Claim: Weak label smoothing during seed training prevents overfitting to noisy labels by creating soft label distributions that reflect class ambiguity.
- Mechanism: Label smoothing replaces hard one-hot labels with softened distributions where the correct class retains high probability but other classes receive small non-zero probabilities.
- Core assumption: The smoothing parameter α (typically 0.05) is appropriately tuned to balance between preserving true label information and introducing beneficial noise.
- Evidence anchors: [abstract] mentions weak label training; [section] describes generating weak labels through label smoothing; [corpus] shows weak evidence from related papers.

### Mechanism 2: Few-Shot Expert Label Refinement
- Claim: Few-shot expert label acquisition provides targeted refinement that corrects systematic errors in the seed model without requiring extensive manual labeling.
- Mechanism: The method selects a small subset of examples (100 in experiments) and obtains expert labels for these instances to create a clean dataset for fine-tuning.
- Core assumption: The selected examples for expert labeling are representative of the overall data distribution and the expert labels are accurate.
- Evidence anchors: [abstract] and [section] both describe the two-stage process; [corpus] provides moderate evidence from related human-in-the-loop learning papers.

### Mechanism 3: Model Parameter Averaging
- Claim: Model parameter averaging combines complementary strengths of seed and fine-tuned models, creating a more robust final model.
- Mechanism: The method performs weighted averaging of the parameters from the seed model (trained with weak labels) and the fine-tuned model (trained with expert labels).
- Core assumption: The seed and fine-tuned models have similar architectures and were trained on the same task, allowing meaningful parameter averaging.
- Evidence anchors: [abstract] and [section] describe the merging process; [corpus] provides strong evidence from model averaging literature.

## Foundational Learning

- Concept: Label noise modeling and noise types
  - Why needed here: Understanding symmetric vs asymmetric noise, noise levels, and sparsity is crucial for designing appropriate mitigation strategies
  - Quick check question: What is the difference between symmetric and asymmetric label noise in terms of how they affect model training?

- Concept: Transfer learning and fine-tuning
  - Why needed here: The method relies on transferring knowledge from seed model to fine-tuned model, requiring understanding of how pre-training affects downstream adaptation
  - Quick check question: Why is fine-tuning a pre-trained model more effective than training from scratch when limited clean labels are available?

- Concept: Model ensembling and parameter averaging
  - Why needed here: The final model merging step uses parameter averaging, which requires understanding how to combine multiple models effectively
  - Quick check question: How does parameter averaging differ from model ensembling in terms of inference cost and model combination?

## Architecture Onboarding

- Component map: Seed model training → Few-shot expert label acquisition → Model parameter averaging
- Critical path: The most critical components are the label smoothing implementation during seed training and the weighted averaging parameters
- Design tradeoffs: The method trades computational cost of fine-tuning for improved accuracy, and uses simple averaging instead of complex ensembling techniques
- Failure signatures: Poor performance if label smoothing parameter is misconfigured, if expert labels are noisy, or if averaging weights are poorly chosen
- First 3 experiments:
  1. Test different label smoothing parameters (α) on a validation set to find optimal value
  2. Experiment with different numbers of expert-labeled examples (5, 10, 50, 100) to find sweet spot
  3. Test different averaging weights for seed vs fine-tuned models on validation set to optimize final performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of few-shot label acquisition strategy (e.g., entropy, least confidence, stratified sampling) impact FHLR's performance across different noise levels and task types?
- Basis in paper: [explicit] Section 5 discusses ablation of label acquisition strategies, showing entropy performs well on sleep scoring and activity recognition, but the paper does not explore performance variation across noise levels or task types.
- Why unresolved: The paper only evaluates a single noise level (0.4) and sparsity (0.2), limiting understanding of how acquisition strategy effectiveness varies.
- What evidence would resolve it: Comparative evaluation of acquisition strategies across a broader range of noise levels and task types, identifying patterns or task-specific optimal strategies.

### Open Question 2
- Question: What is the optimal weighting scheme for merging seed and fine-tuned models in FHLR, and can this be learned dynamically based on noise characteristics?
- Basis in paper: [explicit] Section 4.3 mentions fixed weights (wB = 0.15 for high noise, wB = 0.9 for low noise) but acknowledges these are selected empirically and suggests investigating a more principled approach.
- Why unresolved: The paper uses empirically determined weights rather than a principled method, and does not explore whether weights should adapt based on noise level or task characteristics.
- What evidence would resolve it: A method for dynamically determining optimal weights based on noise characteristics, validated across multiple tasks and noise profiles.

### Open Question 3
- Question: How does FHLR perform when expert disagreement rates exceed 20%, and what strategies can mitigate the impact of high disagreement on model performance?
- Basis in paper: [explicit] Section 5 mentions an experiment with 10 virtual annotators at 10% and 20% disagreement rates, but does not explore performance degradation beyond 20% or strategies to handle higher disagreement.
- Why unresolved: The paper only tests disagreement rates up to 20%, which may not reflect real-world scenarios where expert disagreement could be higher.
- What evidence would resolve it: Evaluation of FHLR at disagreement rates exceeding 20%, along with strategies (e.g., weighted averaging of expert labels, disagreement-aware training) to maintain performance.

## Limitations

- Performance may not generalize beyond the four evaluated datasets with specific noise characteristics
- Exact noise injection methodology for creating noisy labels is not fully specified, affecting reproducibility
- Expert label acquisition process details (cost, inter-rater reliability) are not fully elaborated

## Confidence

- High confidence: The core mechanism of combining weak label training with few-shot expert refinement and parameter averaging is well-supported
- Medium confidence: Specific parameter choices (α=0.05, 100 expert labels, averaging weights) are reasonable but may not be optimal across all settings
- Low confidence: Claims about robustness to high noise levels (60%) should be interpreted cautiously due to limited experimental scope

## Next Checks

1. Systematically evaluate FHLR's performance across different noise types (symmetric vs. asymmetric, instance-dependent vs. instance-independent) to identify its strengths and limitations.

2. Conduct experiments varying the number of expert labels (5, 10, 25, 50, 100, 200) to determine the minimum effective sample size and cost-benefit tradeoffs.

3. Test FHLR on datasets from different domains or with different data characteristics to assess generalizability beyond the four evaluated tasks.