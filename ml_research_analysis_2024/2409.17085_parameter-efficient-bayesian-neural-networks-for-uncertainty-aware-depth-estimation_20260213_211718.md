---
ver: rpa2
title: Parameter-efficient Bayesian Neural Networks for Uncertainty-aware Depth Estimation
arxiv_id: '2409.17085'
source_url: https://arxiv.org/abs/2409.17085
tags:
- lora
- colora
- methods
- bayesian
- peft
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates parameter-efficient Bayesian neural networks
  for uncertainty-aware depth estimation in safety-critical applications. The authors
  combine popular parameter-efficient fine-tuning (PEFT) methods (BitFit, DiffFit,
  LoRA) with Bayesian inference techniques (Stochastic Weight Averaging Gaussians
  and checkpoint ensembles) on a large-scale Transformer-based vision model.
---

# Parameter-efficient Bayesian Neural Networks for Uncertainty-aware Depth Estimation

## Quick Facts
- arXiv ID: 2409.17085
- Source URL: https://arxiv.org/abs/2409.17085
- Reference count: 40
- One-line primary result: PEFT subspaces enable parameter-efficient Bayesian inference, improving predictive performance and providing well-calibrated uncertainty estimates for depth estimation.

## Executive Summary
This work investigates parameter-efficient Bayesian neural networks for uncertainty-aware depth estimation in safety-critical applications. The authors combine popular parameter-efficient fine-tuning (PEFT) methods (BitFit, DiffFit, LoRA) with Bayesian inference techniques (Stochastic Weight Averaging Gaussians and checkpoint ensembles) on a large-scale Transformer-based vision model. A novel PEFT method, CoLoRA, is introduced for convolutional layers by applying Tucker decomposition to convolutional kernels. Experiments on NYU and KITTI datasets demonstrate that PEFT subspaces enable parameter-efficient Bayesian inference, improving predictive performance and providing well-calibrated uncertainty estimates.

## Method Summary
The paper combines parameter-efficient fine-tuning methods (BitFit, DiffFit, LoRA, and CoLoRA) with Bayesian inference techniques (DeepEnsembles, Checkpoint Ensembles, SWAG-Diagonal, SWAG-Low-Rank) for monocular depth estimation. The method uses a pre-trained DINOv2 vision transformer backbone with a DPT decoder, applying PEFT to reduce the number of trainable parameters while maintaining performance. CoLoRA extends LoRA to convolutional layers through Tucker decomposition. Bayesian inference is performed on the PEFT-adapted parameters using 100 checkpoints over 20 epochs, with evaluation metrics including negative log-likelihood and test loss, and calibration assessed via test loss per quantile of most certain predictions.

## Key Results
- PEFT subspaces enable parameter-efficient Bayesian inference, improving predictive performance and providing well-calibrated uncertainty estimates.
- CoLoRA performs competitively with other PEFT methods, particularly at higher ranks, while requiring significantly fewer parameters than full fine-tuning.
- Bayesian inference on PEFT-adapted parameters improves calibration compared to deterministic baselines across multiple methods and datasets.

## Why This Works (Mechanism)

### Mechanism 1
- Parameter-efficient fine-tuning subspaces preserve predictive performance while reducing computational cost by constructing low-rank or sparse perturbations of pre-trained weights, enabling fine-tuning in a smaller parameter space that still captures task-relevant features.
- Core assumption: The pre-trained model's weights contain generalizable features that can be adapted efficiently with small perturbations.
- Break condition: If the pre-trained model lacks relevant features for the downstream task, PEFT will fail to achieve competitive performance.

### Mechanism 2
- Bayesian inference on PEFT subspaces provides well-calibrated uncertainty estimates by approximating the posterior distribution over the PEFT-adapted parameters, enabling uncertainty quantification through Monte Carlo sampling.
- Core assumption: The PEFT subspace is sufficiently expressive to capture the uncertainty in the model's predictions.
- Break condition: If the PEFT subspace is too restrictive, the uncertainty estimates may be overconfident or poorly calibrated.

### Mechanism 3
- CoLoRA extends LoRA to convolutional layers by applying Tucker decomposition to convolutional kernels, decomposing the convolutional kernel tensor into a core tensor and factor matrices.
- Core assumption: The convolutional kernel tensor can be effectively approximated by a low-rank decomposition without significant loss of information.
- Break condition: If the Tucker decomposition does not capture the essential features of the convolutional kernel, CoLoRA will not provide effective adaptation.

## Foundational Learning

- **Bayesian deep learning**: Needed to quantify uncertainty in depth estimation, which is crucial for safety-critical applications. Quick check: What is the difference between aleatoric and epistemic uncertainty in Bayesian deep learning?
- **Parameter-efficient fine-tuning (PEFT)**: Needed to adapt large pre-trained models to the depth estimation task without the computational cost of full fine-tuning. Quick check: How does LoRA achieve parameter efficiency compared to full fine-tuning?
- **Tucker decomposition**: Needed to extend LoRA to convolutional layers, enabling parameter-efficient adaptation of the convolutional prediction head. Quick check: What is the difference between Tucker-1 and Tucker-2 decomposition?

## Architecture Onboarding

- **Component map**: Pre-trained vision transformer backbone (DINOv2) -> Convolutional prediction head (DPT decoder) -> PEFT adapters (LoRA, CoLoRA, BitFit, DiffFit) -> Bayesian inference methods (SWAG, checkpoint ensembles)
- **Critical path**: Load pre-trained model -> Apply PEFT to relevant layers -> Fine-tune on depth estimation task -> Perform Bayesian inference on PEFT-adapted parameters -> Evaluate predictive performance and uncertainty calibration
- **Design tradeoffs**: Higher PEFT rank provides better performance but increases computational cost; more complex Bayesian inference methods may provide better uncertainty estimates but are more computationally expensive; using a larger number of checkpoints for Bayesian inference improves uncertainty estimates but increases storage and computational cost
- **Failure signatures**: Poor predictive performance (PEFT subspace too restrictive or pre-trained model lacks relevant features); poorly calibrated uncertainty (Bayesian inference method may not adequately capture posterior or PEFT subspace too restrictive); numerical instability (Low-rank plus diagonal SWAG may fail due to ill-conditioning)
- **First 3 experiments**: 1) Apply LoRA with rank 1 to transformer backbone and evaluate on NYU dataset; 2) Apply CoLoRA with rank 4 to convolutional prediction head and compare to LoRA; 3) Perform Bayesian inference using SWAG with diagonal covariance on PEFT-adapted parameters and evaluate uncertainty calibration on KITTI dataset

## Open Questions the Paper Calls Out

### Open Question 1
- How does the performance of CoLoRA compare to LoRA when applied to fully convolutional architectures without a transformer backbone?
- Basis in paper: The paper introduces CoLoRA as a novel LoRA-inspired PEFT method specifically designed for convolutional layers, but only evaluates it in combination with a transformer backbone
- Why unresolved: The paper focuses exclusively on a hybrid transformer-convolutional architecture, leaving open whether CoLoRA's advantages are specific to this architecture or generalize to purely convolutional models
- What evidence would resolve it: Direct comparison of CoLoRA vs LoRA on purely convolutional architectures (e.g., U-Net based models) for depth estimation or other dense prediction tasks

### Open Question 2
- What is the relationship between the rank parameter in CoLoRA and the resulting approximation quality for different kernel sizes and input/output channel configurations?
- Basis in paper: The paper shows that CoLoRA performs competitively with LoRA at higher ranks but doesn't provide a systematic analysis of how rank requirements scale with kernel dimensions or channel counts
- Why unresolved: The paper only tests a limited set of rank values (1-64) without analyzing how these choices relate to the intrinsic dimensionality of the convolutional kernels being approximated
- What evidence would resolve it: Analysis showing how optimal rank scales with kernel size (k1Ã—k2), input channels (cin), and output channels (cout) across different architectures

### Open Question 3
- How do the uncertainty estimates from PEFT-based Bayesian inference methods compare to those obtained from full Bayesian methods in terms of detecting out-of-distribution samples in depth estimation?
- Basis in paper: The paper demonstrates that PEFT methods with Bayesian inference provide well-calibrated uncertainty estimates on in-distribution test data but doesn't evaluate their effectiveness at detecting distribution shifts
- Why unresolved: The paper focuses on calibration and predictive performance metrics but doesn't test the practical utility of the uncertainty estimates for safety-critical applications like autonomous driving
- What evidence would resolve it: Evaluation of uncertainty estimates on deliberately shifted datasets (e.g., different weather conditions, times of day, or geographic locations) and comparison with full Bayesian methods

### Open Question 4
- What is the computational overhead of CoLoRA compared to standard LoRA when considering both training time and inference latency?
- Basis in paper: While the paper claims CoLoRA requires significantly fewer parameters than full fine-tuning, it doesn't provide a detailed analysis of computational efficiency compared to LoRA or the impact of the Tucker decomposition on training speed
- Why unresolved: The paper mentions that CoLoRA can be implemented without allocating the full kernel matrix during training, but doesn't quantify the practical performance implications of this approach
- What evidence would resolve it: Benchmarking of training time per epoch, inference latency, and memory usage for CoLoRA versus LoRA across different rank settings and architectures

## Limitations
- Evaluation restricted to two datasets (NYU and KITTI), which may not represent the diversity of real-world scenarios where uncertainty-aware depth estimation is critical.
- Does not extensively explore robustness of uncertainty estimates to distribution shift or adversarial examples, which are crucial for safety-critical applications.
- Computational savings of CoLoRA compared to full fine-tuning are not quantified in terms of wall-clock time or memory usage.

## Confidence
- **High Confidence**: The core claim that PEFT methods can reduce the number of trainable parameters while maintaining competitive predictive performance is well-supported by extensive experimental results.
- **Medium Confidence**: The assertion that Bayesian inference on PEFT subspaces provides well-calibrated uncertainty estimates is supported by experimental results, but the analysis of calibration could be more comprehensive.
- **Low Confidence**: The novelty and effectiveness of CoLoRA specifically are supported by the results, but the claim that it "performs competitively with other PEFT methods" is based on a limited set of experiments.

## Next Checks
1. Evaluate uncertainty estimates on out-of-distribution data (e.g., from different datasets or with synthetic corruptions) to assess robustness to distribution shift.
2. Test model's uncertainty estimates under adversarial attacks to determine if Bayesian PEFT approach provides more reliable uncertainty estimates in the presence of adversarial examples.
3. Quantify wall-clock time and memory usage of PEFT methods compared to full fine-tuning during both training and inference to provide comprehensive understanding of computational benefits.