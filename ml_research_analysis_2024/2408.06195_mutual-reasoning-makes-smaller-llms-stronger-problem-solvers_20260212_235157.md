---
ver: rpa2
title: Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers
arxiv_id: '2408.06195'
source_url: https://arxiv.org/abs/2408.06195
tags:
- reasoning
- question
- answer
- rstar
- step
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: rStar improves small language models' reasoning by decoupling reasoning
  into a self-play generation-discrimination process using MCTS with rich human-like
  actions and mutual consistency verification. On GSM8K, it boosts LLaMA2-7B accuracy
  from 12.51% to 63.91%, Mistral-7B from 36.46% to 81.88%, and LLaMA3-8B-Instruct
  from 74.53% to 91.13%, matching or exceeding results from domain-specialized fine-tuning.
---

# Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers

## Quick Facts
- arXiv ID: 2408.06195
- Source URL: https://arxiv.org/abs/2408.06195
- Reference count: 23
- Key outcome: rStar improves small language models' reasoning by decoupling reasoning into a self-play generation-discrimination process using MCTS with rich human-like actions and mutual consistency verification

## Executive Summary
rStar introduces a novel self-play framework that significantly enhances the reasoning capabilities of small language models (SLMs) without requiring fine-tuning or superior models. The method employs a target SLM to generate reasoning trajectories using Monte Carlo Tree Search (MCTS) with a rich set of human-like actions, while a separate SLM acts as a discriminator to validate these trajectories through mutual consistency verification. This decoupling of generation and verification reduces error propagation and leads to more reliable answer selection. On diverse reasoning benchmarks, rStar demonstrates substantial accuracy improvements across multiple SLM architectures, often matching or exceeding the performance of domain-specialized fine-tuning approaches.

## Method Summary
rStar enhances SLM reasoning through a self-play mutual generation-discrimination process using MCTS with rich human-like actions. The target SLM generates candidate reasoning trajectories while a separate SLM with similar capabilities acts as a discriminator, evaluating each trajectory by completing partial reasoning steps. Trajectories that mutually agree on their reasoning are deemed more likely to be correct. The method introduces five specific actions in MCTS (propose one-step thought, propose remaining thought steps, propose next sub-question with answer, re-answer sub-question, and rephrase the question) to maximize the SLM's potential for solving complex reasoning problems. The final answer is selected based on a combined score from both the target SLM's generation and the discriminator's verification.

## Key Results
- LLaMA2-7B accuracy improves from 12.51% to 63.91% on GSM8K
- Mistral-7B accuracy improves from 36.46% to 81.88% on GSM8K
- LLaMA3-8B-Instruct accuracy improves from 74.53% to 91.13% on GSM8K

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Self-play mutual reasoning decouples solution generation from verification, reducing error propagation.
- **Mechanism:** The target SLM generates candidate reasoning trajectories using MCTS with a rich action space. A separate SLM with similar capabilities acts as a discriminator, evaluating each trajectory by completing partial reasoning steps. Trajectories mutually agreed upon are deemed higher quality.
- **Core assumption:** Two SLMs with similar capabilities can effectively validate each other's reasoning without introducing new errors.
- **Evidence anchors:**
  - [abstract]: "rStar decouples reasoning into a self-play mutual generation-discrimination process."
  - [section]: "rStar employs a second SLM with the similar capability, acting as a discriminator to provide unsupervised feedback on each candidate reasoning trajectory."
  - [corpus]: "Found 25 related papers... mutual reasoning framework (rStar)"
- **Break condition:** If either SLM's reasoning capabilities are too weak, the mutual agreement may not reflect true correctness.

### Mechanism 2
- **Claim:** Rich human-like actions in MCTS enable more effective exploration of the solution space.
- **Mechanism:** Instead of single actions, rStar uses five actions: propose one-step thought, propose remaining thought steps, propose next sub-question with answer, re-answer sub-question, and rephrase the question. This allows the SLM to simulate diverse human reasoning behaviors.
- **Core assumption:** Diverse actions mirror human problem-solving and lead to better trajectory generation.
- **Evidence anchors:**
  - [abstract]: "rStar advocates a richer set of reasoning actions in the self-exploration."
  - [section]: "we introduce a richer set of 5 actions to maximize the SLM's potential for correctly solving complex reasoning problems."
  - [section]: "Table 1: Ablation study on the effectiveness of our rich action space."
- **Break condition:** If the action space is too broad, MCTS may struggle to prioritize effective actions.

### Mechanism 3
- **Claim:** Mutual consistency verification provides more reliable answer selection than self-consistency.
- **Mechanism:** For each candidate trajectory, rStar masks part of the reasoning steps and asks the discriminator SLM to complete them. If the completed steps match the original, the trajectory is considered valid. The target SLM then selects the final answer based on a combined score.
- **Core assumption:** Agreement between two SLMs on the same reasoning steps indicates higher likelihood of correctness.
- **Evidence anchors:**
  - [abstract]: "The mutually agreed reasoning trajectories are considered mutual consistent, thus are more likely to be correct."
  - [section]: "we use another SLM as a discriminator to augment the MCTS process, mutually verifying the correctness of each trajectory."
  - [section]: "Mutual reasoning consistency by Discriminator SLM 2."
- **Break condition:** If the discriminator SLM is significantly weaker or stronger, the mutual agreement may be unreliable.

## Foundational Learning

- **Concept: Monte Carlo Tree Search (MCTS)**
  - Why needed here: MCTS enables systematic exploration of the solution space by building a search tree with actions and states.
  - Quick check question: What are the four key steps in MCTS?
- **Concept: Chain-of-Thought (CoT) prompting**
  - Why needed here: CoT provides a framework for breaking down complex reasoning into intermediate steps, which aligns with MCTS's tree structure.
  - Quick check question: How does CoT differ from direct question answering?
- **Concept: Self-consistency**
  - Why needed here: Self-consistency is used as a baseline and informs the design of the reward function in rStar.
  - Quick check question: What is the core idea behind self-consistency in reasoning tasks?

## Architecture Onboarding

- **Component map:**
  Target SLM -> MCTS engine -> Discriminator SLM -> Reward function -> Final answer selection
- **Critical path:**
  1. Target SLM generates initial trajectory using MCTS.
  2. Discriminator SLM validates each trajectory.
  3. Target SLM selects final answer based on scores.
- **Design tradeoffs:**
  - Action space breadth vs. MCTS efficiency: More actions increase exploration but may slow convergence.
  - Discriminator strength vs. mutual agreement reliability: Similar capabilities ensure fair validation.
  - Rollout count vs. inference cost: More rollouts improve accuracy but increase computational expense.
- **Failure signatures:**
  - Low accuracy improvement: Indicates weak generator or ineffective discriminator.
  - High inference cost: Suggests excessive rollouts or inefficient MCTS.
  - Inconsistent results: Points to unreliable mutual agreement.
- **First 3 experiments:**
  1. Ablation study on action space: Remove one action at a time to measure impact on accuracy.
  2. Discriminator model selection: Test different SLMs as discriminators to find optimal pairing.
  3. Rollout count optimization: Vary the number of MCTS rollouts to balance accuracy and cost.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does rStar's performance scale with different numbers of MCTS rollouts, and is there an optimal number beyond which returns diminish?
- Basis in paper: [explicit] The paper shows performance with different rollouts in Figure 5 but doesn't analyze optimal rollout numbers or diminishing returns.
- Why unresolved: The paper demonstrates that rStar works with as few as 2 rollouts but doesn't explore the full tradeoff between inference cost and accuracy across a wider range of rollout numbers.
- What evidence would resolve it: Systematic experiments testing accuracy vs. rollout numbers from 2 to 64+ on multiple datasets, with analysis of cost-benefit tradeoffs.

### Open Question 2
- Question: How does the choice of discriminator model affect rStar's performance, and what is the minimum capability required for effective discrimination?
- Basis in paper: [explicit] The paper shows that different discriminator models yield similar results (Table 5) but doesn't explore the minimum capability threshold or whether stronger models provide any benefit.
- Why unresolved: The paper demonstrates that even weaker models can work as discriminators but doesn't systematically test the lower bound of discriminator capability or explore why capability doesn't strongly correlate with performance.
- What evidence would resolve it: Experiments testing discriminators with varying capabilities (including smaller models and models with different pretraining objectives) across multiple reasoning tasks.

### Open Question 3
- Question: Can rStar's rich action space be further optimized, and which specific actions contribute most to performance gains across different reasoning domains?
- Basis in paper: [explicit] Table 1 shows ablation study on action space effectiveness but doesn't analyze which actions are most critical for specific reasoning domains or explore potential new actions.
- Why unresolved: The paper demonstrates that the full action space works well but doesn't provide detailed analysis of which actions are most important for different types of reasoning problems or whether domain-specific action spaces could be more effective.
- What evidence would resolve it: Detailed ablation studies across different reasoning domains (math, logic, commonsense) showing which actions are most critical, plus experiments testing new potential actions or domain-specific action sets.

## Limitations
- The effectiveness of mutual reasoning heavily depends on the relative capabilities of the target and discriminator SLMs, but the paper does not thoroughly explore scenarios where the SLMs have significantly different capabilities.
- While the action space is described as "rich," the paper does not provide detailed analysis of whether all five actions contribute meaningfully or if some could be redundant.
- The computational overhead of MCTS with multiple rollouts and mutual verification is substantial, but the paper lacks a comprehensive cost-benefit analysis across different hardware configurations.

## Confidence
- **High confidence** in the core claim that mutual reasoning improves SLM reasoning accuracy, as evidenced by consistent gains across multiple models and tasks.
- **Medium confidence** in the mechanism claim about mutual consistency verification being more reliable than self-consistency, as the paper shows improved results but does not directly compare failure cases between the two approaches.
- **Medium confidence** in the claim about rich human-like actions improving exploration, as the ablation study shows positive impact but doesn't explore alternative action spaces or their relative contributions.

## Next Checks
1. **Cross-capability validation test:** Systematically vary the relative capabilities between target and discriminator SLMs to determine the optimal pairing ratio and identify failure modes when capabilities diverge significantly.
2. **Action space efficiency analysis:** Conduct a detailed ablation study that measures not just accuracy impact but also computational overhead for each action, potentially identifying a minimal effective action set.
3. **Cost-benefit scaling study:** Measure accuracy gains against inference costs across different hardware configurations and rollout counts to establish practical deployment thresholds for various use cases.