---
ver: rpa2
title: 'LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of
  Large Language Models'
arxiv_id: '2404.15522'
source_url: https://arxiv.org/abs/2404.15522
tags:
- context
- reasoning
- question
- will
- logical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'LogicBench systematically evaluates logical reasoning in LLMs
  using 25 inference rules across propositional, first-order, and non-monotonic logics.
  The dataset is generated via a three-stage pipeline: sentence generation with diverse
  ontologies, natural language conversion of logical templates, and task-specific
  instance creation for binary and multiple-choice QA formats.'
---

# LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models

## Quick Facts
- arXiv ID: 2404.15522
- Source URL: https://arxiv.org/abs/2404.15522
- Reference count: 40
- Primary result: LLMs achieve below-human performance on LogicBench's 25 logical inference rules, with GPT-4 scoring 63.98% accuracy

## Executive Summary
LogicBench is a systematically constructed dataset that evaluates large language models' logical reasoning abilities across 25 distinct inference rules spanning propositional, first-order, and non-monotonic logics. The dataset employs natural language contexts and two task formats (binary and multiple-choice question answering) to test models' ability to perform single-step logical inference. Evaluations with leading LLMs including GPT-4, ChatGPT, Gemini, Llama-2, and Mistral using chain-of-thought prompting reveal significant performance gaps, particularly on rules involving negations and longer inference chains. Fine-tuning T5-large on synthetically augmented LogicBench data yields modest improvements (~2% average) on other logic reasoning datasets.

## Method Summary
LogicBench is generated through a three-stage pipeline: first, sentences are created using diverse ontologies and predicates; second, these sentences are converted into natural language contexts; third, task-specific instances are generated for binary and multiple-choice question answering formats. The dataset covers 25 inference rules across propositional, first-order, and non-monotonic logics, with each rule evaluated independently. Models are assessed using zero-shot chain-of-thought prompting with three different prompt formulations, and performance is compared against human baselines from 61 college students. The framework also includes synthetic data augmentation and fine-tuning of T5-large to test transfer capabilities to other logic datasets.

## Key Results
- GPT-4 achieves 63.98% accuracy on propositional logic tasks, indicating significant challenges even for large models
- Models struggle particularly with inference rules involving negations and longer chains of inference
- Fine-tuning T5-large on augmented LogicBench data improves performance on other logic datasets by approximately 2% on average
- Binary question-answering tasks prove more challenging than multiple-choice formats for most models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LogicBench exposes fundamental logical reasoning gaps in LLMs by isolating single inference rules in natural language contexts.
- Mechanism: The dataset separates reasoning patterns into 25 distinct rules across propositional, first-order, and non-monotonic logics, forcing models to perform targeted logical inference without conflating other reasoning skills.
- Core assumption: LLMs pretrained on general text lack sufficient exposure to explicit logical rule patterns, especially those involving negations and longer chains of inference.
- Evidence anchors:
  - [abstract]: "we comprehensively evaluate the logical reasoning ability of LLMs on 25 different reasoning patterns spanning over propositional, first-order, and non-monotonic logics."
  - [section 4.3]: "ChatGPT achieves 48.04%, and GPT-4 shows a performance of 63.98% A(Y es) on average which indicates the challenge of classical logical reasoning (PL) even for larger LLMs."
  - [corpus]: FMR=0.692 for "logical reasoning" indicates moderate corpus overlap, suggesting the dataset targets a less explored capability space.
- Break condition: If models are pretrained with sufficient logical reasoning examples or fine-tuned explicitly on similar patterns, performance may improve significantly.

### Mechanism 2
- Claim: Chain-of-thought prompting enables LLMs to verbalize intermediate reasoning steps, revealing their internal logical processing.
- Mechanism: By providing prompts that ask models to "think step-by-step logically," the approach forces them to expose reasoning chains, making it possible to diagnose where logical failures occur.
- Core assumption: Verbalizing intermediate reasoning steps improves accuracy by making implicit reasoning explicit and allowing for error detection/correction.
- Evidence anchors:
  - [abstract]: "We conduct detailed analysis with a range of LLMs such as GPT-4, ChatGPT, Gemini-Pro, Llama-2-7B-Chat, and Mistral-7B-Instruct using chain-of-thought prompting."
  - [section 4.3]: "We investigate the LLMs' logical reasoning ability in natural language, not in artificial logical formulations. Hence, we note that LLMs sometimes hallucinate information and overlook contextual information, leading to incorrect conclusions."
  - [corpus]: Related work on "Chain-of-Thought" (0.577 FMR) supports this mechanism's relevance.
- Break condition: If models become overconfident or if step-by-step reasoning introduces compounding errors, the benefit may diminish.

### Mechanism 3
- Claim: Finetuning on synthetically augmented LogicBench improves logical reasoning transfer to other logic datasets.
- Mechanism: Augmenting LogicBench with variations and fine-tuning T5-large creates LogicT5, which shows improved performance on LogicNLI and FOLIO (~2% average gain).
- Core assumption: Exposure to diverse logical reasoning patterns during training generalizes to unseen logical reasoning tasks.
- Evidence anchors:
  - [abstract]: "We synthetically augment it and fine-tune T5-large. Our preliminary results... show that this improves the logical reasoning ability of existing models leading to performance improvement on other logic datasets, LogicNLI, and FOLIO (~2% on an average)."
  - [section 4.3]: "We trained the T5-large model on the LogicBench(Aug) resulting in a model named LogicT5. Furthermore, we performed fine-tuning on four other logical reasoning datasets: LogiQA, Reclor, LogicNLI, and FOLIO."
  - [corpus]: No direct corpus evidence; this is an internal experimental claim.
- Break condition: If augmented data introduces noise or if the target datasets require different reasoning skills, transfer gains may be limited.

## Foundational Learning

- Concept: Inference rules and their natural language representation
  - Why needed here: The dataset relies on translating formal logic inference rules into natural language contexts and questions, so understanding these rules is essential for both dataset construction and model evaluation.
  - Quick check question: Can you explain how modus tollens is represented in natural language versus formal logic?

- Concept: Chain-of-thought prompting and step-by-step reasoning
  - Why needed here: This is the primary method used to evaluate and diagnose LLMs' logical reasoning, so understanding how to construct effective prompts is critical.
  - Quick check question: How would you structure a chain-of-thought prompt to evaluate modus ponens in natural language?

- Concept: Dataset construction and variation generation
  - Why needed here: The three-stage pipeline (sentence generation → NL conversion → task instance generation) is central to LogicBench's design, so understanding this process is key for replication or extension.
  - Quick check question: What are the key differences between generating BQA and MCQA instances in the LogicBench pipeline?

## Architecture Onboarding

- Component map: LogicBench dataset with 25 reasoning patterns → Two task formats (BQA and MCQA) → Chain-of-thought prompting for LLMs (GPT-4, ChatGPT, Gemini, Llama-2, Mistral) → Human evaluation baseline → Synthetic augmentation and fine-tuning (T5-large)
- Critical path: Dataset generation → Prompt design → Model evaluation → Error analysis → Fine-tuning (optional) → Transfer evaluation
- Design tradeoffs: The choice to focus on single inference rules simplifies evaluation but may not capture multi-step reasoning; natural language contexts improve realism but may introduce ambiguity
- Failure signatures: Models may hallucinate information, overlook contextual premises, or struggle with negations and longer inference rules; performance drops significantly on complex reasoning patterns
- First 3 experiments:
  1. Evaluate a small subset of LogicBench instances with GPT-4 using chain-of-thought prompting to verify the basic evaluation pipeline
  2. Compare performance on BQA vs. MCQA tasks for a single reasoning pattern to understand task-specific challenges
  3. Fine-tune T5-large on a small augmented subset of LogicBench and evaluate on LogicNLI to test transfer capability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal model size for logical reasoning tasks, and does simply scaling model parameters lead to diminishing returns in this domain?
- Basis in paper: Explicit - The paper observes that larger models (GPT-4, Gemini) outperform smaller ones (Llama-2, Mistral) but notes this is based on a limited comparison between 7B and much larger models.
- Why unresolved: The study only compares two model sizes. A systematic study varying model size (e.g., 1B, 7B, 34B, 70B) while controlling for other factors would be needed to determine if performance gains continue linearly or plateau.
- What evidence would resolve it: A comprehensive ablation study training and evaluating models of multiple sizes (1B-70B) on LogicBench and tracking accuracy gains per parameter increase.

### Open Question 2
- Question: How do negations in logical premises affect model reasoning, and can this be mitigated through targeted training?
- Basis in paper: Explicit - The paper notes that models struggle significantly with inference rules involving negations (e.g., MT, DD) and observes this pattern in the reasoning chains.
- Why unresolved: The paper identifies the problem but doesn't test whether targeted training on negated logical statements improves performance or whether models can learn to better handle negation scope.
- What evidence would resolve it: Training experiments where models are fine-tuned on augmented datasets with varying proportions of negated statements, followed by evaluation on both negated and non-negated test sets.

### Open Question 3
- Question: Does the performance gap between BQA and MCQA tasks indicate a fundamental limitation in how models process logical reasoning versus pattern matching?
- Basis in paper: Explicit - The paper observes that models perform better on MCQA than BQA for PL/FOL but worse for NM, suggesting different reasoning mechanisms may be at play.
- Why unresolved: The paper doesn't investigate whether this gap stems from models using elimination strategies in MCQA or whether the task format itself changes how logical reasoning is performed.
- What evidence would resolve it: Comparative studies where models are tested on modified MCQA tasks (e.g., with more options, different distributions of correct answers) and BQA tasks with confidence scores to distinguish true reasoning from pattern matching.

## Limitations
- The evaluation methodology relies heavily on chain-of-thought prompting, which may artificially inflate performance by providing additional reasoning scaffolding
- The synthetic augmentation approach for fine-tuning may introduce artifacts that don't generalize to naturally occurring logical reasoning tasks
- The human evaluation baseline represents a small sample size (61 college students) that may not capture the full diversity of human logical reasoning abilities

## Confidence
- High confidence: The core finding that existing LLMs struggle with logical reasoning tasks involving negations and longer inference rules is well-supported by systematic evaluation across multiple models and reasoning patterns
- Medium confidence: The effectiveness of chain-of-thought prompting for logical reasoning is supported but may be overestimated due to the artificial nature of the prompting approach
- Low confidence: The claim that LogicBench comprehensively evaluates "logical reasoning ability" is overstated given the narrow focus on single inference rules

## Next Checks
1. Extend the evaluation framework to include complex reasoning chains that combine multiple inference rules, testing whether models can handle realistic logical deduction scenarios beyond single-rule applications
2. Conduct a systematic comparison between zero-shot evaluation, chain-of-thought prompting, and few-shot prompting to quantify the contribution of prompting strategy versus inherent model reasoning capabilities
3. Test the LogicBench evaluation on multilingual LLMs and datasets to determine whether logical reasoning abilities transfer across languages or if performance gaps are language-specific