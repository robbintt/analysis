---
ver: rpa2
title: 'LongHealth: A Question Answering Benchmark with Long Clinical Documents'
arxiv_id: '2401.14490'
source_url: https://arxiv.org/abs/2401.14490
tags:
- information
- clinical
- accuracy
- documents
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study presents LongHealth, a new benchmark designed to evaluate
  large language models'' (LLMs) ability to extract and interpret information from
  lengthy clinical documents. The benchmark consists of 20 detailed fictional patient
  cases, each containing 5,090-6,754 words, accompanied by 400 multiple-choice questions
  across three categories: information extraction, negation, and sorting.'
---

# LongHealth: A Question Answering Benchmark with Long Clinical Documents

## Quick Facts
- arXiv ID: 2401.14490
- Source URL: https://arxiv.org/abs/2401.14490
- Reference count: 34
- Nine open-source LLMs and GPT-3.5 Turbo evaluated on processing 5,000-6,754 word clinical documents

## Executive Summary
This study introduces LongHealth, a benchmark designed to evaluate large language models' ability to process and extract information from lengthy clinical documents. The benchmark consists of 20 fictional patient cases with 400 multiple-choice questions across three categories: information extraction, negation, and identifying missing information. Nine open-source LLMs plus GPT-3.5 Turbo were tested, requiring minimum 16,000 token context windows. Mixtral-8x7B-Instruct-v0.1 achieved the highest accuracy at 77% for information retrieval tasks, while all models struggled significantly with negation and missing information detection.

## Method Summary
The LongHealth benchmark uses 20 fictional patient cases containing 5,090-6,754 words each, accompanied by 400 multiple-choice questions across three task categories. Nine open-source LLMs with minimum 16,000 token context lengths were evaluated, along with GPT-3.5 Turbo as a high-performance baseline. Models were assessed on their ability to extract information, handle negations, and identify missing information. Performance was measured through automated scoring of multiple-choice responses, with results aggregated across multiple runs to assess consistency.

## Key Results
- Mixtral-8x7B-Instruct-v0.1 achieved 77% accuracy in information retrieval tasks
- All models struggled significantly with identifying missing information in clinical documents
- Models with adequate context windows (16,000+ tokens) can process full 5,000+ word documents in a single pass

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large language models can process long clinical documents when given sufficient context length.
- Mechanism: The benchmark tests models with documents of 5,090-6,754 words, requiring context windows of at least 16,000 tokens to fit the full text. Models with adequate context length can process the entire document in one pass, maintaining coherence across the full narrative.
- Core assumption: The information needed to answer questions is distributed throughout the document and requires understanding the full clinical context.
- Evidence anchors:
  - [abstract] "We evaluated nine open-source LLMs with a minimum of 16,000 tokens"
  - [section] "The included open-source models were selected based on their ability to handle a minimum context length of 16,000 tokens"
  - [corpus] Weak evidence - only mentions token length requirements but not actual processing capability demonstration
- Break condition: If critical information is fragmented across sections that cannot all fit in the context window, or if the model cannot maintain coherence across such long documents.

### Mechanism 2
- Claim: Models can accurately extract information from single-patient documents when the relevant data is present.
- Mechanism: When all required information for a question exists within the provided documents, models can locate and retrieve it through attention mechanisms and pattern matching, achieving reasonable accuracy.
- Core assumption: The documents contain all necessary information for the questions, and the information is distributed in a way that allows the model to identify relevant sections.
- Evidence anchors:
  - [section] "Task 1: This task evaluates the ability of the model to extract relevant information from a set of clinical documents of a single patient"
  - [section] "Mixtral-8x7B-Instruct-v0.1 achieved the highest accuracy at 77% in information retrieval tasks"
  - [corpus] Moderate evidence - shows some models achieve 77% accuracy but doesn't explain why this specific mechanism works
- Break condition: When information is missing, requires negation reasoning, or when the model must identify that data is insufficient to answer.

### Mechanism 3
- Claim: Models struggle significantly with negation and identifying missing information in clinical contexts.
- Mechanism: Negation requires the model to understand what is NOT stated in the text, which involves more complex reasoning than simple information extraction. Identifying missing information requires the model to recognize gaps in the data and appropriately refuse to answer.
- Core assumption: Negation and missing information detection require higher-order reasoning beyond pattern matching and information retrieval.
- Evidence anchors:
  - [abstract] "all models struggled significantly with identifying missing information"
  - [section] "all models struggled significantly in tasks requiring the identification of missing information"
  - [corpus] Strong evidence - explicitly states all models struggled with these tasks
- Break condition: If models are trained with specific negation detection techniques or if they are fine-tuned on clinical datasets that emphasize missing information detection.

## Foundational Learning

- Concept: Context window limitations in transformer models
  - Why needed here: Understanding why 16,000 tokens is the minimum requirement and how models handle long sequences
  - Quick check question: What happens when you try to process a 10,000 word document with a model that only has a 4,000 token context window?

- Concept: Negation reasoning in natural language processing
  - Why needed here: The benchmark specifically tests models' ability to handle negations, which is a known weakness in LLMs
  - Quick check question: How would you design a prompt to help an LLM identify that a medication was NOT prescribed when it's not explicitly mentioned?

- Concept: Information extraction vs. information retrieval
  - Why needed here: The benchmark distinguishes between simple extraction tasks and more complex reasoning tasks, requiring understanding of different NLP capabilities
  - Quick check question: What's the difference between extracting "the patient was given 500mg of aspirin" and determining "the patient was NOT given warfarin"?

## Architecture Onboarding

- Component map: Document preprocessing -> Tokenization -> Model inference -> Answer extraction -> Scoring -> Result aggregation
- Critical path: The model inference step is most performance-critical as it requires processing long documents (5,000+ words) and maintaining context across multiple questions
- Design tradeoffs: Using multiple-choice questions simplifies automated scoring but doesn't reflect real-world clinical queries; including GPT-3.5 Turbo provides a high-performance baseline but introduces a proprietary component
- Failure signatures: Low accuracy on Task 3 (identifying missing information) indicates the model is prone to hallucination; decreasing accuracy with longer context suggests context window limitations; high variance across runs suggests sensitivity to document ordering
- First 3 experiments:
  1. Test model accuracy on Task 1 with varying context lengths (4K, 8K, 12K, 16K tokens) to identify the minimum required context for acceptable performance
  2. Evaluate model performance on negation questions specifically to quantify the weakness in negation reasoning
  3. Compare single-run vs. multi-run performance to assess model consistency and sensitivity to document shuffling

## Open Questions the Paper Calls Out
None

## Limitations
- Fictional patient cases may not capture the complexity and noise of real clinical data
- Multiple-choice format doesn't reflect the open-ended nature of real clinical queries
- Limited evaluation to nine open-source models without exploring parameter tuning or prompting strategies

## Confidence
- **High confidence**: Models can process long clinical documents when given sufficient context length
- **Medium confidence**: Mixtral-8x7B-Instruct-v0.1 achieves superior performance on information retrieval tasks (77% accuracy)
- **Medium confidence**: All models struggle with negation and missing information tasks
- **Low confidence**: Current LLMs are insufficient for reliable clinical use

## Next Checks
1. Evaluate the same models on real patient records from multiple healthcare systems to assess performance differences between fictional and actual clinical documents
2. Systematically test whether different prompting strategies, few-shot examples, or chain-of-thought reasoning improve model performance on negation and missing information tasks
3. Compare standard LLM performance against retrieval-augmented generation approaches where relevant document sections are extracted and provided as context