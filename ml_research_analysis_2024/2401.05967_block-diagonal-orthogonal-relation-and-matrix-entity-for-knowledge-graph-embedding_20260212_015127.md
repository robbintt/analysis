---
ver: rpa2
title: Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph Embedding
arxiv_id: '2401.05967'
source_url: https://arxiv.org/abs/2401.05967
tags:
- relation
- orthogonale
- entity
- orthogonal
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces OrthogonalE, a novel knowledge graph embedding
  model that employs matrices for entities and block-diagonal orthogonal matrices
  with Riemannian optimization for relations. The model addresses two key limitations
  of existing rotation-based methods: the need for proportional increases in relation
  parameters with entity dimension and difficulties in generalizing to higher-dimensional
  rotations.'
---

# Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph Embedding

## Quick Facts
- arXiv ID: 2401.05967
- Source URL: https://arxiv.org/abs/2401.05967
- Authors: Yihua Zhu; Hidetoshi Shimodaira
- Reference count: 15
- Key outcome: OrthogonalE achieves greater flexibility and generality than rotation-based methods by using matrices for entities and block-diagonal orthogonal matrices with Riemannian optimization for relations, significantly outperforming state-of-the-art models while reducing relation parameters.

## Executive Summary
This paper introduces OrthogonalE, a novel knowledge graph embedding model that addresses key limitations of existing rotation-based methods. By transforming entity vectors into matrices and utilizing block-diagonal orthogonal matrices for relations with Riemannian optimization, OrthogonalE achieves both greater flexibility and generality. The model captures essential relation patterns including symmetry, antisymmetry, inversion, and non-commutative composition while significantly reducing the number of relation parameters compared to state-of-the-art approaches.

## Method Summary
OrthogonalE represents entities as matrices (n×m) and relations as block-diagonal orthogonal matrices composed of d×d orthogonal blocks. The model uses Euclidean distance as a scoring function and employs alternating optimization: RiemannianAdam for relation matrices and SGD for entity matrices. This design allows for increased entity representation capacity without proportionally increasing relation parameters, while enabling exploration of higher-dimensional rotations that were computationally intractable with previous rotation-based methods.

## Key Results
- OrthogonalE significantly outperforms RotatE, QuatE, and other state-of-the-art models on WN18RR and FB15K-237 datasets
- The model reduces relation parameters by orders of magnitude compared to RotatE while maintaining or improving performance
- Different block sizes (2×2 to 10×10) show varying effectiveness across datasets, with WN18RR requiring more complex blocks than FB15K-237
- OrthogonalE successfully captures symmetry, antisymmetry, inversion, and non-commutative composition relation patterns

## Why This Works (Mechanism)

### Mechanism 1
- Claim: OrthogonalE achieves greater flexibility by transforming entity vectors into matrices while keeping relation matrix size fixed.
- Mechanism: By changing entity representation from vectors (ev ∈ R^n) to matrices (eV ∈ R^(n×m)), OrthogonalE can increase entity representation capacity through m without proportionally increasing relation parameters.
- Core assumption: The scoring function using matrix multiplication (eR · eH) remains valid and effective with matrix entity representations.
- Evidence anchors:
  - [abstract]: "by transforming entity vectors into matrices and utilizing orthogonal matrices for relations, OrthogonalE achieves greater flexibility and generality"
  - [section 4.2]: "to enhance OrthogonalE's flexibility, we aim to regulate entity dimension using variable m and transform entity vectors ev ∈ R^n into matrices eV ∈ R^(n×m)"
  - [corpus]: Weak - no direct corpus evidence for this specific transformation mechanism
- Break condition: If matrix multiplication in scoring function becomes computationally prohibitive or loses meaningful entity-relation relationships when m increases.

### Mechanism 2
- Claim: OrthogonalE gains generality by replacing rotation matrices with orthogonal matrices of adaptable dimensions.
- Mechanism: Using orthogonal matrices Xi ∈ R^(d×d) instead of rotation matrices allows exploration of higher-dimensional rotations (d > 3) that were computationally intractable before.
- Core assumption: Orthogonal matrices can capture the same relation patterns (symmetry, antisymmetry, inversion, composition) as rotation matrices while being more general.
- Evidence anchors:
  - [abstract]: "by transforming entity vectors into matrices and utilizing orthogonal matrices for relations, OrthogonalE achieves greater flexibility and generality"
  - [section 4.1]: "we exploit the orthogonality of rotation matrices, substituting rotation matrices (Bi ∈ R^(d×d)) with orthogonal matrices (Xi ∈ R^(d×d)) of corresponding dimensions d"
  - [corpus]: Weak - no direct corpus evidence for this specific orthogonality-to-generality mechanism
- Break condition: If higher-dimensional orthogonal matrices fail to capture essential relation patterns or become too computationally expensive to optimize.

### Mechanism 3
- Claim: Riemannian optimization enables effective training of block-diagonal orthogonal matrices for relations.
- Mechanism: Using RiemannianAdam for relation optimization while keeping entities fixed during relation updates, then optimizing entities with SGD, allows stable training of orthogonal relation matrices.
- Core assumption: Separating relation and entity optimization while using manifold-aware optimization for orthogonal matrices leads to better convergence than joint optimization.
- Evidence anchors:
  - [section 4.4]: "we employ Riemannian optimization for the relation matrix eR ∈ R^(n×n) and SGD for the entity matrix eV ∈ R^(n×m)"
  - [section 3.2]: "recent studies suggest optimization of the orthogonal manifold with retractions as an effective approach"
  - [corpus]: Weak - no direct corpus evidence for this specific separation of optimization strategy
- Break condition: If alternating optimization leads to poor convergence or if Riemannian optimization becomes too computationally expensive compared to benefits.

## Foundational Learning

- Concept: Orthogonal matrices and their properties
  - Why needed here: Understanding that orthogonal matrices preserve distances and angles, making them suitable for relation transformations in KGE
  - Quick check question: Why are orthogonal matrices preferred over general matrices for relation representations in this model?

- Concept: Riemannian optimization on manifolds
  - Why needed here: The relation matrices lie on the orthogonal manifold, requiring specialized optimization techniques that respect manifold constraints
  - Quick check question: What is the key difference between Riemannian gradient descent and standard gradient descent?

- Concept: Block-diagonal matrix decomposition
  - Why needed here: The relation embedding is constructed as a block-diagonal matrix, requiring understanding of how to compose and optimize such structures
  - Quick check question: How does the block-diagonal structure of relation matrices affect both parameter efficiency and model expressiveness?

## Architecture Onboarding

- Component map:
  - Entity representation: Matrix form (n × m)
  - Relation representation: Block-diagonal orthogonal matrix (n × n composed of d × d blocks)
  - Optimization: Alternating Riemannian optimization for relations, SGD for entities
  - Scoring: Euclidean distance between transformed head entity and tail entity

- Critical path: Entity matrix → Relation block-diagonal orthogonal matrix → Scoring function → Loss computation → Alternating optimization

- Design tradeoffs:
  - Flexibility vs parameter efficiency: Matrix entities provide flexibility but increase entity parameter count
  - Generality vs computational complexity: Higher d enables more general rotations but increases optimization cost
  - Alternating optimization vs joint optimization: Separation improves stability but may slow convergence

- Failure signatures:
  - Poor convergence: May indicate issues with Riemannian optimization parameters or alternating schedule
  - Degraded performance with higher m: Could suggest matrix entity representation is not capturing useful information
  - No improvement with higher d: May indicate relation patterns don't benefit from higher-dimensional rotations

- First 3 experiments:
  1. Compare OrthogonalE(2×2) with fixed m against RotatE with same entity dimension to verify parameter efficiency claim
  2. Test OrthogonalE with varying m (1, 5, 10) to demonstrate flexibility in entity representation
  3. Compare OrthogonalE(2×2), OrthogonalE(3×3), and OrthogonalE(4×4) to validate generality across different dimensionalities

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does OrthogonalE perform on larger, more complex knowledge graph datasets compared to the relatively small WN18RR and FB15K-237 datasets used in this study?
- Basis in paper: [inferred] The paper evaluates OrthogonalE on two small datasets and notes that performance differences among OrthogonalE models with different block sizes are not significant due to the small dataset size. The paper also mentions that WN18RR requires more complex blocks for adequate representation compared to FB15K-237, suggesting dataset size and complexity may impact performance.
- Why unresolved: The experiments were limited to small datasets, and the paper does not provide results or analysis for larger, more complex knowledge graphs.
- What evidence would resolve it: Experimental results comparing OrthogonalE's performance on larger, more complex knowledge graph datasets (e.g., YAGO, Freebase) against state-of-the-art models would provide insights into its scalability and effectiveness in real-world scenarios.

### Open Question 2
- Question: What is the computational complexity of the exponential retraction in Riemannian optimization used in OrthogonalE, and how does it impact training time and scalability for very large knowledge graphs?
- Basis in paper: [explicit] The paper mentions that the computation of exponential retraction in the orthogonal manifold for Riemannian optimization is costly, noting that OrthogonalE(2×2) training time is 4 times longer than RotatE for the same entity dimension.
- Why unresolved: While the paper acknowledges the computational cost, it does not provide a detailed analysis of the complexity or explore optimization techniques to reduce the computational burden for large-scale applications.
- What evidence would resolve it: A thorough analysis of the computational complexity of the exponential retraction and experimental results comparing training times of OrthogonalE with other models on very large knowledge graphs would clarify the scalability challenges and potential solutions.

### Open Question 3
- Question: How does the choice of block-diagonal orthogonal matrix size (d×d) in OrthogonalE affect its ability to capture different types of relation patterns (symmetry, antisymmetry, inversion, and non-commutative composition) across various knowledge graph domains?
- Basis in paper: [explicit] The paper demonstrates that OrthogonalE can capture the four relation patterns and that the block size affects performance, with WN18RR requiring more complex blocks (3×3 to 10×10) compared to FB15K-237 (2×2 is sufficient). However, it does not provide a comprehensive analysis of how different block sizes impact the capture of specific relation patterns across diverse domains.
- Why unresolved: The paper only shows that different block sizes perform differently on two specific datasets but does not explore the relationship between block size and relation pattern capture across various knowledge graph domains or relation types.
- What evidence would resolve it: Experimental results analyzing OrthogonalE's performance with different block sizes on knowledge graphs from various domains (e.g., biomedical, social networks) and with diverse relation patterns would reveal the optimal block size for capturing specific relation types and domain characteristics.

## Limitations

- Empirical evaluation is limited to only two small benchmark datasets (WN18RR and FB15K-237), which may not demonstrate generalizability across diverse knowledge graph types.
- Computational complexity analysis is incomplete, with no detailed runtime comparisons or memory usage analysis across different dimensionalities.
- The alternating optimization strategy lacks theoretical justification and comparative analysis against joint optimization approaches.

## Confidence

**High Confidence**: The parameter efficiency claim (reduced relation parameters compared to RotatE) is well-supported by the mathematical formulation and experimental results.

**Medium Confidence**: The claims about flexibility and generality through matrix entities and orthogonal relations are supported by experimental results but lack deeper theoretical analysis.

**Low Confidence**: The assertion that Riemannian optimization with alternating updates is superior to other optimization strategies lacks comparative analysis.

## Next Checks

1. **Parameter Efficiency Verification**: Implement RotatE with equivalent entity dimensions and directly compare relation parameter counts across different dimensionalities to verify the claimed efficiency gains.

2. **Generalizability Testing**: Evaluate OrthogonalE on additional knowledge graph datasets (e.g., YAGO3-10, NELL-995) to assess whether performance gains generalize beyond the two benchmark datasets used.

3. **Optimization Strategy Comparison**: Compare the alternating Riemannian optimization approach against joint optimization methods and standard gradient descent to determine if the separation of relation and entity optimization is indeed optimal.