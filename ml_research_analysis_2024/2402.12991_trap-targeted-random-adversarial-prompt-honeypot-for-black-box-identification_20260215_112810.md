---
ver: rpa2
title: 'TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification'
arxiv_id: '2402.12991'
source_url: https://arxiv.org/abs/2402.12991
tags: []
core_contribution: This paper introduces the novel task of black-box identity verification
  (BBIV) to detect whether a third-party application is using a specific proprietary
  large language model (LLM) without access to its weights. The authors propose a
  method called Targeted Random Adversarial Prompt (TRAP) that leverages adversarial
  suffixes, originally developed for jailbreaking, to force the target LLM to produce
  a predefined response (e.g., a specific number) while other models generate random
  outputs.
---

# TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification

## Quick Facts
- arXiv ID: 2402.12991
- Source URL: https://arxiv.org/abs/2402.12991
- Reference count: 40
- Primary result: Novel method achieves >95% true positive detection with <0.2% false positive rates for black-box LLM identity verification

## Executive Summary
This paper introduces the novel task of black-box identity verification (BBIV) to detect whether a third-party application is using a specific proprietary large language model without access to its weights. The authors propose a method called Targeted Random Adversarial Prompt (TRAP) that leverages adversarial suffixes, originally developed for jailbreaking, to force the target LLM to produce a predefined response (e.g., a specific number) while other models generate random outputs. TRAP achieves over 95% true positive detection rates with less than 0.2% false positive rates even after a single interaction, outperforming perplexity-based approaches. The method remains effective even when the LLM undergoes minor modifications that do not significantly alter its core functionality.

## Method Summary
The TRAP method works by crafting adversarial suffixes that, when appended to benign prompts, force the target LLM to consistently output a specific predefined response (such as a particular number) while other models produce random or unrelated outputs. The approach exploits the unique response patterns of different LLMs to adversarial inputs, treating the problem as a binary classification task. The method requires only black-box access through an API, making no assumptions about model weights or architecture. The authors demonstrate that this approach outperforms traditional perplexity-based methods and naive identity prompting techniques, achieving high detection accuracy even with single interactions.

## Key Results
- TRAP achieves >95% true positive detection rates with <0.2% false positive rates
- Outperforms perplexity-based approaches for black-box identity verification
- Remains effective against minor model modifications that preserve core functionality
- Single interaction sufficient for reliable detection

## Why This Works (Mechanism)
The method exploits the fact that different LLMs respond uniquely to adversarial suffixes due to their distinct training data, architecture, and optimization processes. When presented with carefully crafted adversarial prompts, the target model consistently produces a specific predefined response while other models generate random outputs. This creates a reliable fingerprinting mechanism that can distinguish between models even without access to their internal parameters.

## Foundational Learning
- **Adversarial suffixes**: Short strings appended to prompts that trigger specific model behaviors, essential for creating the differential response patterns needed for identification
- **Perplexity-based fingerprinting**: Traditional approach measuring output probability distributions, less effective than TRAP for identity verification
- **Black-box identity verification**: Task of determining which model is behind an API without weight access, requiring novel approaches beyond traditional model comparison
- **Prompt-based attacks**: Techniques for forcing models to produce specific outputs, repurposed here for legitimate identification purposes
- **Model fingerprinting**: Process of creating unique identifiers for models based on their response patterns to specific inputs

## Architecture Onboarding

**Component Map:** Adversarial suffix generation -> Prompt construction -> API interaction -> Response classification -> Identity determination

**Critical Path:** The adversarial suffix generation and response classification stages are critical, as they directly determine the method's effectiveness and reliability.

**Design Tradeoffs:** The method trades computational efficiency (requiring API calls) for simplicity and effectiveness, avoiding the need for complex model analysis or weight access.

**Failure Signatures:** Poor adversarial suffix design leads to high false positives/negatives; API rate limits can prevent sufficient sampling; model updates may invalidate existing suffixes.

**First Experiments:**
1. Test TRAP effectiveness across different prompt categories (math, reasoning, creative writing)
2. Measure detection accuracy when models are fine-tuned with safety alignment
3. Evaluate performance when multiple models are used in an ensemble configuration

## Open Questions the Paper Calls Out
None

## Limitations
- Experimental validation limited to only four commercial LLM APIs and two open-source models
- Effectiveness uncertain against fine-tuned models with safety training or instruction-following modifications
- No analysis of resilience to adversarial countermeasures like response filtering or ensemble approaches
- Unclear generalization to smaller or specialized model families

## Confidence

**High confidence:** The experimental methodology and evaluation framework are sound and reproducible

**Medium confidence:** The core technical approach of using adversarial suffixes for identity verification

**Low confidence:** Generalization claims to broader model families and robustness against active countermeasures

## Next Checks
1. Test TRAP across a broader range of model sizes (1B-70B parameters) and architectures to establish effectiveness boundaries
2. Evaluate the method against fine-tuned models with safety training and instruction-following modifications
3. Design and implement countermeasures (response filtering, ensemble approaches) to assess vulnerability to evasion techniques