---
ver: rpa2
title: Embedded Multi-label Feature Selection via Orthogonal Regression
arxiv_id: '2403.00307'
source_url: https://arxiv.org/abs/2403.00307
tags:
- feature
- multi-label
- selection
- data
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of embedded multi-label feature
  selection, where state-of-the-art methods based on least squares regression struggle
  to preserve sufficient discriminative information in multi-label data. The proposed
  method, GRROOR, introduces orthogonal regression with feature weighting to retain
  more statistical and structural information related to local label correlations.
---

# Embedded Multi-label Feature Selection via Orthogonal Regression

## Quick Facts
- arXiv ID: 2403.00307
- Source URL: https://arxiv.org/abs/2403.00307
- Reference count: 34
- GRROOR achieves superior classification performance on 10 multi-label datasets compared to 9 state-of-the-art methods

## Executive Summary
This paper addresses the limitations of least squares regression-based multi-label feature selection methods by proposing GRROOR, which employs orthogonal regression with feature weighting. The method aims to retain more statistical and structural information related to local label correlations while considering both global feature redundancy and global label relevance. GRROOR formulates an unbalanced orthogonal Procrustes problem on the Stiefel manifold, solved using an efficient iterative optimization algorithm. Experimental results demonstrate that GRROOR outperforms nine state-of-the-art multi-label feature selection methods across six performance metrics on ten datasets.

## Method Summary
GRROOR uses orthogonal regression to preserve local structural information in multi-label data, addressing the limitation of least squares regression methods that may lose discriminative information. The method incorporates feature weighting to minimize global redundancy and optimize global label relevance through a low-dimensional label subspace projection. The optimization problem is formulated as an unbalanced orthogonal Procrustes problem on the Stiefel manifold, solved using an efficient iterative algorithm that alternates between updating the projection matrix, feature weighting matrix, and label subspace.

## Key Results
- GRROOR achieves superior classification performance across six metrics (redundancy, coverage, hamming loss, average precision, macro-F1, micro-F1)
- The method outperforms nine state-of-the-art multi-label feature selection techniques on ten benchmark datasets
- Orthogonal regression effectively preserves local label correlations compared to least squares regression approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Orthogonal regression retains more local structural information than least squares regression in multi-label feature selection.
- Mechanism: Orthogonal regression minimizes the perpendicular distance from data points to the regression line rather than horizontal distance, capturing more local geometric structure in the projection subspace.
- Core assumption: The perpendicular distance better preserves local label correlations in multi-label data than horizontal distance.
- Evidence anchors:
  - [abstract] "The method employs orthogonal regression with feature weighting to retain sufficient statistical and structural information related to local label correlations"
  - [section] "Instead of minimizing the horizontal distance in the LSR, orthogonal regression aims to minimize the perpendicular distance from the data points to the regression line"

### Mechanism 2
- Claim: Feature weighting matrix Θ with global redundancy minimization (θT Aθ) accurately evaluates feature importance while reducing redundancy.
- Mechanism: The diagonal matrix Θ contains feature scores θ that are optimized to minimize global redundancy while maintaining discrimination. High redundancy between features leads to reduced scores for less important redundant features.
- Core assumption: The global redundancy matrix A accurately captures feature redundancy relationships.
- Evidence anchors:
  - [abstract] "both global feature redundancy and global label relevancy information have been considered in the orthogonal regression model"
  - [section] "the feature weighting matrix Θ with global redundancy minimization constraint θT Aθ is added into the orthogonal regression model"

### Mechanism 3
- Claim: Low-dimensional label subspace V with global label relevance optimization (tr[RBTB]) improves feature selection by capturing label correlations.
- Mechanism: Projecting high-dimensional labels to low-dimensional space V while optimizing for global label relevance helps the feature selection process focus on features that discriminate between correlated labels.
- Core assumption: Label correlations in the original high-dimensional space are preserved in the low-dimensional projection V.
- Evidence anchors:
  - [abstract] "the high-dimensional label space Y is projected into a low-dimensional subspace V with global label relevance optimization constraint tr[RBTB]"
  - [section] "To exploit global label relevance, a regularizer for the coefficient matrix B is defined as: ΣRijbTbj"

## Foundational Learning

- Concept: Orthogonal regression vs. least squares regression
  - Why needed here: Understanding why perpendicular distance minimization is better for preserving local structure in multi-label data
  - Quick check question: What is the mathematical difference between orthogonal and least squares regression, and how does this affect the preserved information?

- Concept: Stiefel manifold and orthogonal constraints
  - Why needed here: The algorithm optimizes over the Stiefel manifold (W^TW = Ic), which is critical for understanding the optimization approach
  - Quick check question: What properties does the Stiefel manifold impose on the projection matrix W, and why are these constraints useful?

- Concept: Graph Laplacian for local structure preservation
  - Why needed here: The algorithm uses graph Laplacian L to ensure consistency between original feature space and latent semantics space
  - Quick check question: How does the graph Laplacian matrix L help preserve local geometry structures between X and V?

## Architecture Onboarding

- Component map: Data matrix X (d×n) → Feature weighting matrix Θ (d×d) → Orthogonal projection matrix W (d×c) → Latent semantics matrix V (n×c) → Coefficient matrix B (c×k). The algorithm alternates between updating these components while maintaining orthogonality constraints on W.

- Critical path: 1) Initialize W, Θ, V, B randomly 2) Update W using GPI on Stiefel manifold 3) Update Θ using ALM optimization 4) Update V by solving Sylvester equation 5) Update B by solving Sylvester equation 6) Repeat until convergence

- Design tradeoffs: Orthogonal constraints ensure non-redundant features but increase computational complexity (O(dkn + n³ + c³)). The method trades computational efficiency for better preservation of local structure compared to least squares approaches.

- Failure signatures: 1) Slow convergence indicating poor initialization 2) High redundancy values suggesting the redundancy minimization isn't working 3) Degraded classification performance on test sets indicating overfitting to training data structure

- First 3 experiments:
  1. Run on a small synthetic multi-label dataset with known feature-label relationships to verify that GRROOR recovers the correct features
  2. Compare GRROOR's convergence behavior on a medium-sized dataset with different initialization strategies
  3. Perform ablation study removing the global redundancy term to measure its contribution to performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the GRROOR method's performance compare to other state-of-the-art multi-label feature selection methods when dealing with extremely large sample sizes?
- Basis in paper: [inferred] The paper mentions that GRROOR often requires higher computational time costs compared to filter methods, and its computational complexity is O(dkn + n^3 + c^3), which could be a limitation for extremely large sample sizes.
- Why unresolved: The paper does not provide experimental results or analysis specifically focusing on the performance of GRROOR with extremely large sample sizes.
- What evidence would resolve it: Experimental results comparing the performance of GRROOR with other state-of-the-art methods on datasets with extremely large sample sizes would help resolve this question.

### Open Question 2
- Question: How effective is the GRROOR method in handling missing labels in multi-label learning tasks?
- Basis in paper: [explicit] The paper mentions that multi-label feature selection with missing labels has attracted extensive attention, and existing LSR-based methods may not accurately model the complex relationship between features and incomplete labels. It suggests investigating orthogonal regression-based methods for multi-label feature selection under the circumstance of missing labels.
- Why unresolved: The paper does not provide any experimental results or analysis on the effectiveness of GRROOR in handling missing labels.
- What evidence would resolve it: Experimental results demonstrating the performance of GRROOR on datasets with missing labels, compared to other methods specifically designed for handling missing labels, would help resolve this question.

### Open Question 3
- Question: How sensitive is the performance of GRROOR to the choice of parameters λ, β, and η?
- Basis in paper: [explicit] The paper mentions that the performance of GRROOR is sensitive to the values of control parameters and shows the average classification results (ACR) of the Image data set with varying parameters.
- Why unresolved: The paper does not provide a comprehensive analysis of the parameter sensitivity across different datasets or a systematic approach to selecting optimal parameter values.
- What evidence would resolve it: A more extensive analysis of the parameter sensitivity across multiple datasets, along with guidelines or heuristics for selecting optimal parameter values, would help resolve this question.

## Limitations
- High computational complexity (O(dkn + n³ + c³)) may limit scalability to very high-dimensional data
- Performance depends on proper construction of the global redundancy matrix A
- Effectiveness relies on meaningful local neighborhoods existing in the feature space

## Confidence

- Medium: Claims about local structure preservation via orthogonal regression
- High: Claims about algorithmic convergence based on established optimization techniques
- Medium: Claims about redundancy minimization effectiveness given proper similarity matrix construction

## Next Checks

1. Conduct controlled experiments on synthetic multi-label datasets with known ground truth feature-label relationships to quantify feature recovery accuracy
2. Perform sensitivity analysis on the dimensionality of the label subspace V across different data distributions to determine optimal c values
3. Test GRROOR's performance on high-dimensional datasets (d > 10,000) to evaluate scalability limitations and potential approximations