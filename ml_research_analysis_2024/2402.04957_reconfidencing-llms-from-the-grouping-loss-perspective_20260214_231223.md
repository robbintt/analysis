---
ver: rpa2
title: Reconfidencing LLMs from the Grouping Loss Perspective
arxiv_id: '2402.04957'
source_url: https://arxiv.org/abs/2402.04957
tags:
- calibration
- confidence
- loss
- llms
- grouping
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This work analyzes confidence scores of large language models\
  \ (LLMs) on knowledge-based questions. Using a new dataset derived from YAGO knowledge\
  \ base, it demonstrates that models like Mistral and LLaMA are overconfident in\
  \ their answers, with significant grouping loss\u2014calibration errors vary across\
  \ sub-groups like popularity or nationality."
---

# Reconfidencing LLMs from the Grouping Loss Perspective

## Quick Facts
- arXiv ID: 2402.04957
- Source URL: https://arxiv.org/abs/2402.04957
- Authors: Lihu Chen; Alexandre Perez-Lebel; Fabian M. Suchanek; Gaël Varoquaux
- Reference count: 40
- One-line primary result: Proposed reconfidencing method improves LLM confidence calibration and reduces grouping loss by training sub-group specific isotonic regressors via decision tree partitions.

## Executive Summary
This work analyzes confidence scores of large language models (LLMs) on knowledge-based questions and identifies significant overconfidence and grouping loss—calibration errors that vary across sub-groups like entity popularity or nationality. Using a new YAGO-derived dataset, the authors demonstrate that standard recalibration methods do not address these subgroup-specific miscalibrations. They propose reconfidencing, a method that uses decision tree partitions to identify latent sub-groups and applies separate isotonic regressors for each, resulting in better Brier scores and reduced grouping loss.

## Method Summary
The authors construct a new evaluation dataset from YAGO triples and use two black-box confidence elicitation methods (JAFC and SelfCheckGPT) to obtain confidence scores from LLaMA and Mistral models. They apply global isotonic recalibration and then propose reconfidencing: a decision tree partitions the embedding space into leaves, each assigned its own isotonic regressor. This sub-group specific recalibration is shown to outperform global recalibration in both Brier score and grouping loss.

## Key Results
- Mistral and LLaMA models exhibit overconfidence on knowledge-based questions.
- Grouping loss is significant: calibration errors vary across sub-groups (e.g., by entity popularity or nationality).
- Reconfidencing via decision tree-based sub-group partitions reduces both Brier score and grouping loss more effectively than global recalibration.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Grouping loss exists when a model is well-calibrated overall but has large errors in specific sub-groups.
- Mechanism: Confidence scores are aggregated across all samples, masking large errors in certain sub-groups. Partitioning the data into sub-groups and calibrating separately reduces the error within each group.
- Core assumption: The sources of overconfidence are systematic and tied to specific features (e.g., entity popularity, nationality).
- Evidence anchors:
  - [abstract] "predicted scores may deviate significantly from the actual posterior probabilities due to the impact of grouping loss."
  - [section] "we show that they are more overconfident on some answers than others, eg depending on the nationality of the person in the query."
  - [corpus] Found 25 related papers; FMR 0.417 indicates moderate relevance, but no direct citations to this work yet.
- Break condition: If the sources of overconfidence are random or not tied to identifiable features, the decision tree partition will not improve calibration.

### Mechanism 2
- Claim: A decision tree can partition the input space to identify latent sub-groups where overconfidence patterns are similar.
- Mechanism: The decision tree uses LLM embeddings as input and partitions samples into leaves where the model's confidence error is homogeneous. Each leaf gets its own isotonic regressor for recalibration.
- Core assumption: The embedding space captures the relevant features that influence confidence calibration.
- Evidence anchors:
  - [abstract] "we propose a solution to reconfidence LLMs, canceling not only calibration but also grouping loss."
  - [section] "we follow Perez-Lebel et al. (2023) to employ a decision tree, using a loss related to the squared loss for the Brier score on labels ( Y )."
  - [corpus] No direct evidence in corpus yet; related work on uncertainty quantification exists.
- Break condition: If the embedding space does not capture the features causing overconfidence, the tree will not identify meaningful partitions.

### Mechanism 3
- Claim: Sub-group specific recalibration (reconfidencing) reduces both overall Brier score and grouping loss more effectively than global recalibration.
- Mechanism: Separate isotonic regressors for each sub-group allow fine-grained adjustment of confidence scores, reducing dispersion in each group's error rate.
- Core assumption: Each sub-group has a consistent calibration error that can be corrected with a local model.
- Evidence anchors:
  - [abstract] "Our refined reconfidencing solution has a better performance in terms of Brier score and grouping loss."
  - [section] "Our proposed solution, we propose a more refined method to reconfidence LLMs from the sub-group perspective."
  - [corpus] Related papers on calibration and confidence elicitation exist but do not cite this specific method.
- Break condition: If sub-groups are too small or inconsistent, the local regressors will overfit or fail to generalize.

## Foundational Learning

- Concept: Calibration curves and Brier score
  - Why needed here: To evaluate whether confidence scores match true probabilities and to quantify overall calibration performance.
  - Quick check question: What does it mean if a calibration curve lies above the diagonal?

- Concept: Grouping loss
  - Why needed here: To understand why calibration alone is insufficient and why sub-group analysis is necessary.
  - Quick check question: How is grouping loss different from calibration loss?

- Concept: Isotonic regression
  - Why needed here: To map raw confidence scores to calibrated probabilities in a non-parametric way.
  - Quick check question: What property of isotonic regression makes it suitable for recalibration?

## Architecture Onboarding

- Component map: LLM → Confidence extraction method (JAFC or SelfCheckGPT) → Decision tree classifier → Isotonic regressors (one per leaf) → Calibrated confidence scores
- Critical path: Confidence extraction → Decision tree training → Sub-group identification → Local recalibration → Evaluation on test set
- Design tradeoffs: Global recalibration is simpler but misses sub-group errors; reconfidencing is more complex but improves both Brier score and grouping loss.
- Failure signatures: If reconfidencing does not reduce grouping loss, the decision tree may not be finding meaningful partitions; if local regressors overfit, calibration curves will be noisy.
- First 3 experiments:
  1. Compare Brier score and grouping loss before and after global recalibration on the test set.
  2. Visualize calibration curves for user-defined sub-groups (popularity, nationality) before and after reconfidencing.
  3. Evaluate whether the decision tree-identified sub-groups correspond to known sources of overconfidence (e.g., long-tail entities).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do confidence elicitation methods perform across different knowledge domains (e.g., science, history, geography)?
- Basis in paper: [explicit] The paper evaluates methods on knowledge base-derived questions about people, organizations, and creative works, but does not systematically vary knowledge domains.
- Why unresolved: The dataset construction focused on YAGO triples for specific relations, and domain-specific calibration errors are not explored.
- What evidence would resolve it: Evaluations of JAFC and SelfCheckGPT across a broader set of knowledge domains with varying complexity and entity types.

### Open Question 2
- Question: Can the grouping loss identified in LLMs be mitigated by fine-tuning on datasets that explicitly balance sub-group representation?
- Basis in paper: [inferred] The paper identifies significant grouping loss in user-defined sub-groups (e.g., nationality, popularity) and suggests the need for more balanced training data.
- Why unresolved: The study only applies post-hoc recalibration/reconfidencing without addressing data imbalance during training.
- What evidence would resolve it: Controlled experiments comparing standard fine-tuning with balanced sub-group sampling versus the proposed reconfidencing approach.

### Open Question 3
- Question: How do white-box confidence elicitation methods (requiring access to internal states or logits) compare to black-box methods in terms of calibration and grouping loss?
- Basis in paper: [explicit] The paper focuses on black-box methods (JAFC and SelfCheckGPT) and notes that white-box methods are not always available for certain LLMs like ChatGPT.
- Why unresolved: Direct comparison of white-box versus black-box methods on the same dataset is not performed.
- What evidence would resolve it: Benchmarking white-box methods (e.g., analyzing token logits or internal states) against black-box methods using the same evaluation dataset and metrics.

## Limitations

- Dataset construction details (e.g., question templates, entity feature extraction) are not fully specified, raising concerns about sampling bias.
- Decision tree partitions rely on LLM embeddings, which may not fully capture the latent sources of overconfidence.
- The method's robustness and generalizability across different LLMs and knowledge tasks is not fully established.

## Confidence

- **High Confidence**: The demonstration that LLMs (Mistral, LLaMA) exhibit overconfidence on knowledge-based questions, and that global recalibration improves Brier score. These are well-established phenomena in the calibration literature.
- **Medium Confidence**: The claim that grouping loss is a significant issue and that sub-group specific recalibration (reconfidencing) is more effective than global recalibration. The analysis is compelling, but the method's robustness across different datasets and models is not fully established.
- **Low Confidence**: The decision tree's ability to discover meaningful latent subgroups that drive grouping loss. Without direct comparison to known subgroups (e.g., popularity, nationality), it's unclear if the tree finds true sources of miscalibration or merely artifacts of the data.

## Next Checks

1. **Dataset Representation Check**: Audit the YAGO-derived dataset to confirm the distribution of entity popularity and nationality matches the real world. Verify that long-tail entities are adequately represented, as their absence could skew grouping loss estimates.

2. **Latent vs. Known Subgroup Comparison**: Compare the decision tree-identified subgroups to known subgroups (popularity, nationality). Evaluate whether the tree partitions align with or improve upon these known sources of overconfidence. If the tree's subgroups do not correspond to meaningful features, its utility is questionable.

3. **Robustness Across Models and Tasks**: Apply the reconfidencing method to a different LLM (e.g., GPT-3.5) and a different knowledge-based task (e.g., SQuAD or TriviaQA). If improvements in Brier score and grouping loss are not replicated, the method's generalizability is limited.