---
ver: rpa2
title: Learning to Rank Pre-trained Vision-Language Models for Downstream Tasks
arxiv_id: '2412.20682'
source_url: https://arxiv.org/abs/2412.20682
tags:
- performance
- vega
- downstream
- class
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of selecting the best-performing
  pre-trained vision-language model (VLM) for a given unlabeled downstream task without
  relying on additional supervised datasets or large language models. The proposed
  method, Visual-tExtual Graph Alignment (VEGA), constructs fully connected graphs
  for visual and textual modalities based on the feature distributions of class names
  and unlabeled images.
---

# Learning to Rank Pre-trained Vision-Language Models for Downstream Tasks
## Quick Facts
- arXiv ID: 2412.20682
- Source URL: https://arxiv.org/abs/2412.20682
- Reference count: 40
- Proposes VEGA, a method that ranks VLMs for zero-shot classification using unlabeled data without additional supervision

## Executive Summary
This paper addresses the challenge of selecting the most effective pre-trained vision-language model (VLM) for a specific downstream task without access to labeled data or large language models. The proposed Visual-tExtual Graph Alignment (VEGA) method constructs fully connected graphs for visual and textual modalities based on feature distributions of class names and unlabeled images. By computing node-level similarity and edge-level structural alignment between these graphs, VEGA predicts zero-shot classification performance. The method achieves state-of-the-art results with an average Top-5 recall of 0.64, Kendall correlation of 0.62, and Top-1 accuracy of 0.66 across three benchmarks, outperforming existing unsupervised VLM selection methods.

## Method Summary
VEGA operates by constructing fully connected graphs for both visual and textual modalities. For the visual modality, it generates image embeddings using each VLM on a set of unlabeled images, then builds a graph where nodes represent images and edges encode similarity. For the textual modality, it processes class name tokens to create embeddings, then builds a corresponding graph. The method computes alignment scores at two levels: node-level similarity between visual and textual node embeddings, and edge-level alignment comparing graph structures. These combined scores predict zero-shot classification performance, enabling unsupervised VLM ranking without labeled data or additional supervision.

## Key Results
- Achieves state-of-the-art performance with average Top-5 recall of 0.64 across benchmarks
- Demonstrates strong Kendall correlation of 0.62 between predicted and actual VLM rankings
- Reaches Top-1 accuracy of 0.66 in selecting the best-performing VLM for downstream tasks
- Outperforms existing unsupervised VLM selection methods in comprehensive experimental evaluation

## Why This Works (Mechanism)
VEGA leverages the intrinsic relationship between visual and textual feature spaces that VLMs learn during pre-training. By constructing modality-specific graphs and measuring their alignment, the method captures how well a VLM's learned representations bridge vision and language for a specific task. The node-level alignment ensures individual concept representations match, while edge-level structural alignment verifies the relational consistency between modalities. This dual-level alignment effectively predicts zero-shot performance because it directly measures the semantic coherence that VLMs must maintain to perform well on downstream tasks without fine-tuning.

## Foundational Learning
- Graph Construction Methods: Understanding how fully connected graphs encode feature relationships is essential for grasping VEGA's architecture
  - Why needed: VEGA's core innovation relies on comparing visual and textual graph structures
  - Quick check: Can you explain how node embeddings become graph edges through similarity metrics?

- Zero-Shot Classification: The task of classifying images into categories without model training on those categories
  - Why needed: VEGA specifically targets ranking VLMs for zero-shot scenarios where no task-specific training data exists
  - Quick check: Do you understand how VLMs can classify without gradient updates?

- Modality Alignment: Measuring correspondence between different data representations (visual vs textual)
  - Why needed: VEGA's effectiveness depends on accurately quantifying how well visual and textual features align
  - Quick check: Can you describe what constitutes "good" alignment between visual and textual embeddings?

## Architecture Onboarding
Component Map: Unlabeled Images -> Visual Graph -> Node Similarity -> Edge Alignment -> Combined Score
Critical Path: Class Names + Unlabeled Images → Graph Construction → Alignment Computation → Performance Prediction
Design Tradeoffs: Fully connected graphs provide comprehensive feature relationships but increase computational complexity; dual-level alignment captures both local and structural correspondences but requires careful weighting
Failure Signatures: Poor node alignment indicates VLM struggles with individual concept representations; weak edge alignment suggests relational understanding issues between modalities
First Experiments: 1) Test VEGA on a single benchmark with 2-3 VLMs to verify basic functionality, 2) Vary the number of unlabeled images to assess sensitivity, 3) Compare node-level vs edge-level alignment contributions to performance prediction

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation scope limited to zero-shot classification tasks; effectiveness for detection, segmentation, retrieval, or captioning remains unverified
- Assumes access to 50-100 unlabeled images per task but doesn't explore sensitivity to this requirement or minimum viable sample size
- Fully connected graph construction may become computationally expensive for large vocabularies or datasets

## Confidence
- VEGA's ability to rank VLMs for zero-shot classification: High
- VEGA's state-of-the-art performance claims on benchmarks: High
- Generalization to other VLM tasks (detection, retrieval, etc.): Low
- Robustness to varying amounts of unlabeled data: Medium

## Next Checks
1. Evaluate VEGA on non-classification vision-language tasks including image-text retrieval, visual grounding, and zero-shot detection to assess cross-task generalization
2. Conduct ablation studies varying the number of unlabeled images used to construct graphs, determining minimum viable samples for reliable ranking
3. Test VEGA's performance when class names contain ambiguous or polysemous terms that may create multiple valid visual interpretations, measuring robustness to semantic uncertainty