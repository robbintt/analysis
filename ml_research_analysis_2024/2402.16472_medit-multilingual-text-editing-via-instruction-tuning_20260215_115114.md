---
ver: rpa2
title: 'mEdIT: Multilingual Text Editing via Instruction Tuning'
arxiv_id: '2402.16472'
source_url: https://arxiv.org/abs/2402.16472
tags:
- language
- text
- association
- linguistics
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: MEDIT is a multilingual text editing model that can perform tasks
  like grammatical error correction, paraphrasing, and text simplification in multiple
  languages. It is trained by fine-tuning multilingual language models on curated
  instruction-tuning datasets.
---

# mEdIT: Multilingual Text Editing via Instruction Tuning

## Quick Facts
- arXiv ID: 2402.16472
- Source URL: https://arxiv.org/abs/2402.16472
- Reference count: 40
- MEDIT outperforms multilingual baselines on GEC, paraphrasing, and simplification tasks across 7 languages

## Executive Summary
MEDIT is a multilingual text editing model that achieves strong performance on grammatical error correction, paraphrasing, and text simplification tasks across multiple languages. The model is trained by fine-tuning multilingual large language models via instruction tuning on curated datasets. MEDIT outperforms other multilingual models and generalizes well to new languages not seen during fine-tuning, with scaling model size leading to improved performance.

## Method Summary
MEDIT is trained by fine-tuning multilingual LLMs (mT5, mT0, BLOOMZ, PolyLM, Bactrian-X) on curated instruction-tuning datasets for three text editing tasks across seven languages. The model uses instruction prompts in English, native, or random languages, and is fine-tuned for 5 epochs on 8xA100 GPUs. Evaluation uses task-specific metrics (GLEU, SARI, BLEU, Self-BLEU, Semantic Similarity) aggregated via harmonic mean.

## Key Results
- MEDIT outperforms multilingual baselines on GEC, paraphrasing, and simplification tasks across Arabic, Chinese, English, German, Japanese, Korean, and Spanish
- Scaling model size improves performance, with larger models achieving better results
- MEDIT generalizes effectively to new languages not seen during training
- Human evaluations rate MEDIT outputs highly on fluency, adequacy, and accuracy across languages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multilingual instruction tuning improves task-specific performance over zero-shot baselines
- Mechanism: Fine-tuning multilingual LLMs on curated parallel corpora of task-language pairs provides explicit examples of how to follow instructions in different languages
- Core assumption: The curated datasets are high-quality and representative of the editing tasks across languages
- Evidence anchors: [abstract] "MEDIT models are trained by fine-tuning multilingual large, pre-trained language models (LLMs) via instruction tuning"; [section] "We build MEDIT by curating data from multiple publicly available human-annotated text editing datasets"
- Break condition: If the curated datasets contain significant noise or are not representative of the target tasks, fine-tuning will not improve performance over zero-shot baselines

### Mechanism 2
- Claim: Scaling model size improves multilingual text editing performance
- Mechanism: Larger models have more capacity to learn complex mappings between instructions, input text, and desired output across multiple languages and tasks
- Core assumption: The training data is sufficient in quantity and quality to leverage the increased capacity of larger models
- Evidence anchors: [abstract] "Scaling model size improves performance, with the largest models achieving the best results"; [section] "Figure 7: Aggregated model performance on different tasks broken down by parameter size"
- Break condition: If the training data is limited in quantity or quality, scaling model size will not lead to improved performance due to overfitting or underutilization of capacity

### Mechanism 3
- Claim: MEDIT generalizes well to new languages not seen during fine-tuning
- Mechanism: Multilingual pre-training exposes the model to a wide range of languages, enabling it to leverage shared linguistic patterns and transfer knowledge to new languages during fine-tuning
- Core assumption: The pre-training corpus covers a diverse set of languages and the model learns transferable representations
- Evidence anchors: [abstract] "MEDIT generalizes well to new languages not seen during fine-tuning"; [section] "Table 2: Zero-shot evaluation results on the language generalization experiments"
- Break condition: If the pre-training corpus lacks diversity or the model fails to learn transferable representations, generalization to new languages will be poor

## Foundational Learning

- Concept: Instruction tuning
  - Why needed here: MEDIT models need to learn how to follow natural language instructions for text editing tasks in multiple languages
  - Quick check question: How does instruction tuning differ from standard supervised fine-tuning?
- Concept: Multilingual pre-training
  - Why needed here: MEDIT models need a strong foundation in multiple languages to enable cross-lingual transfer and generalization
  - Quick check question: What are the key benefits of multilingual pre-training for text editing tasks?
- Concept: Task-specific datasets
  - Why needed here: MEDIT models need high-quality, task-specific data to learn the nuances of each text editing task across languages
  - Quick check question: How does the quality and quantity of task-specific data impact the performance of instruction-tuned models?

## Architecture Onboarding

- Component map: Multilingual pre-trained LLMs (mT5, mT0, BLOOMZ, PolyLM, Bactrian-X) -> Task-specific instruction datasets (GEC, paraphrasing, simplification) -> Evaluation metrics (GLEU, SARI, BLEU, Self-BLEU, Semantic Similarity)
- Critical path: 1) Curate high-quality task-specific datasets in multiple languages 2) Fine-tune multilingual LLMs on the curated datasets using instruction tuning 3) Evaluate model performance on held-out test sets and compare against baselines 4) Analyze results and identify areas for improvement
- Design tradeoffs: Model size vs. computational resources; Dataset size vs. annotation costs; Task diversity vs. model complexity
- Failure signatures: Poor performance on individual tasks or languages; Inability to generalize to new languages; Overfitting to training data
- First 3 experiments: 1) Fine-tune a small multilingual LLM on a single task-language pair and evaluate performance 2) Scale up to a larger LLM and multiple task-language pairs, comparing against the baseline 3) Evaluate zero-shot generalization to a new language not seen during fine-tuning

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does MEDIT's performance compare to monolingual models on each language and task?
- Basis in paper: [explicit] The paper states that MEDIT achieves strong performance on text editing benchmarks across languages and tasks, outperforming other multilingual models. However, it does not provide a direct comparison to monolingual models.
- Why unresolved: The paper focuses on comparing MEDIT to other multilingual models, but does not explore its performance relative to monolingual models which could be the current state-of-the-art for each language and task.
- What evidence would resolve it: Direct comparisons of MEDIT's performance to the best monolingual models on each language and task, using the same evaluation metrics.

### Open Question 2
- Question: How does the size of the underlying LLM (e.g., mT5 vs mT0 vs BLOOMZ) impact MEDIT's performance?
- Basis in paper: [explicit] The paper states that scaling model size generally improves performance, but it does not analyze the impact of the underlying LLM's architecture and size on MEDIT's performance.
- Why unresolved: The paper uses different variants of mT5, mT0, and BLOOMZ as the underlying LLMs for MEDIT, but does not provide insights into how the choice of LLM affects the final performance.
- What evidence would resolve it: An ablation study comparing MEDIT models trained on the same data but with different underlying LLMs (e.g., mT5-large vs mT0-large vs BLOOMZ-3b), analyzing the impact on performance.

### Open Question 3
- Question: How does MEDIT's performance generalize to languages not seen during training?
- Basis in paper: [explicit] The paper evaluates MEDIT on new languages not seen during training and finds that it generalizes effectively, but the analysis is limited to a few languages and tasks.
- Why unresolved: The paper only evaluates MEDIT on a limited set of new languages (Romanian, Hindi, Italian, French) and tasks (GEC, Simplification, Paraphrasing). It is unclear how well MEDIT would perform on other languages and tasks not seen during training.
- What evidence would resolve it: Extensive evaluations of MEDIT on a diverse set of languages and tasks not seen during training, using appropriate evaluation metrics for each language and task.

## Limitations

- The quality and representativeness of the curated datasets across languages and tasks remains uncertain
- The exact composition of instruction prompts for all 231 task-language combinations is not fully specified
- The impact of different instruction languages (English, native, random) on model performance is not thoroughly analyzed

## Confidence

- **High Confidence**: Claims about model scaling benefits and task-specific performance improvements over baselines are well-supported by quantitative results across multiple benchmarks
- **Medium Confidence**: Generalization to new languages claims are supported by zero-shot evaluation results, but the sample of unseen languages is limited
- **Low Confidence**: Claims about dataset quality and curation process, given the weak corpus evidence and lack of detailed validation metrics

## Next Checks

1. Conduct external validation of dataset quality by having multiple annotators evaluate a sample of instruction-target pairs across languages
2. Perform ablation studies on instruction language (English vs native vs random) to quantify the impact on performance
3. Extend zero-shot generalization evaluation to a broader set of languages, including typologically diverse languages not present in any training data