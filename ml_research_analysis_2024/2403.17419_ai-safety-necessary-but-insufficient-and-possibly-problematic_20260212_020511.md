---
ver: rpa2
title: 'AI Safety: Necessary, but insufficient and possibly problematic'
arxiv_id: '2403.17419'
source_url: https://arxiv.org/abs/2403.17419
tags:
- safety
- https
- good
- harm
- such
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper argues that AI safety is a necessary but insufficient
  and potentially problematic concept. While AI safety aims to prevent unexpected
  or undesirable AI operation, it has a nuanced relationship with transparency and
  addressing societal harm.
---

# AI Safety: Necessary, but insufficient and possibly problematic

## Quick Facts
- arXiv ID: 2403.17419
- Source URL: https://arxiv.org/abs/2403.17419
- Authors: Deepak P
- Reference count: 13
- Primary result: AI safety is necessary but insufficient and potentially problematic, as it may allow continuation of structural harm while providing cover for harmful AI systems

## Executive Summary
This paper critically examines the recent hype around AI safety, arguing that while preventing unexpected or undesirable AI operation is necessary, the current framing of AI safety is insufficient and potentially problematic. The author contends that AI safety discourse, dominated by governments and corporations, may deprioritize fundamental rights protections and transparency mechanisms needed to address structural harm like algorithmic discrimination. The paper warns that this shallow notion of safety could normalize and provide cover for intentionally harmful AI systems used in predictive policing, insurance, and the gig economy.

## Method Summary
The paper presents a conceptual analysis of AI safety discourse, contrasting the current hype dominated by governments and corporations with scholarly research on AI for social good. It examines the evolution of regulatory language, particularly in the EU AI Act, to demonstrate how safety framing may have shifted focus away from protecting individuals' fundamental rights. The analysis draws on examples of problematic AI applications to illustrate how safety definitions that focus on technical robustness rather than ethical outcomes can enable harmful systems.

## Key Results
- AI safety as currently defined allows continuation of structural harm by focusing on preventing visible system failures while enabling invisible algorithmic discrimination
- The AI safety debate has influenced regulatory efforts like the EU AI Act, potentially deprioritizing protection of individuals' fundamental rights
- AI safety hype, driven by corporate and government interests, diverges from the academic community's decade-long focus on fairness, accountability, and social good

## Why This Works (Mechanism)

### Mechanism 1
- Claim: AI safety framing allows continuation of structural harm by shifting regulatory focus away from fundamental rights protections
- Mechanism: The paper argues that the current AI safety discourse, dominated by governments and corporations, deprioritizes addressing invisible harms like algorithmic discrimination in favor of preventing visible system failures. This shift is evidenced by how the EU AI Act's language evolved after AI safety hype emerged, moving from protecting "natural persons" to framing AI as needing to be "safe" itself.
- Core assumption: The AI safety debate deliberately or inadvertently deprioritizes fundamental rights protections in favor of technical safety concerns
- Evidence anchors: Analysis of EU AI Act language evolution showing shift from protecting "natural persons" to AI being "safe"

### Mechanism 2
- Claim: AI safety discourse enables harmful AI systems by providing them with legitimacy and cover
- Mechanism: The paper posits that framing AI systems as "safe" allows problematic applications (like discriminatory predictive policing or exploitative gig economy algorithms) to continue operating under the guise of safety compliance. This occurs because safety definitions focus on technical robustness rather than ethical outcomes or societal impact.
- Core assumption: The concept of AI safety can be co-opted to legitimize harmful systems that meet technical safety criteria but perpetuate structural harm
- Evidence anchors: Examples of racist predictive policing algorithms, discriminatory insurance AI, and gig economy algorithms optimized for platform profits operating under safety frameworks

### Mechanism 3
- Claim: AI safety hype is corporate-driven and diverges from scholarly research on AI for social good
- Mechanism: The paper argues that the current AI safety movement is dominated by government and corporate interests, contrasting with the academic community's decade-long focus on fairness, accountability, transparency, and social good. This corporate framing shapes public understanding and regulatory priorities.
- Core assumption: The AI safety discourse is primarily shaped by corporate and government interests rather than academic or civil society perspectives
- Evidence anchors: Contrast between corporate-dominated AI safety movement and academic work on fairness, accountability, transparency, and social good

## Foundational Learning

- Concept: Software quality control vs. AI safety
  - Why needed here: The paper distinguishes between traditional software quality control (testing for bugs) and AI safety, arguing that AI safety has expanded beyond quality control to include robustness to malicious actors and societal impact
  - Quick check question: How does the paper differentiate between software quality control and AI safety? (Answer: Quality control ensures software works as intended; AI safety also addresses robustness to malicious actors and structural harm)

- Concept: Transparency in AI systems
  - Why needed here: The paper identifies three types of transparency (technological, objective, and decision-level) that are largely absent from current AI safety discussions, which is problematic for identifying structural harm
  - Quick check question: What are the three types of transparency the paper identifies as missing from AI safety discussions? (Answer: Technological transparency, objective transparency, and decision-level transparency)

- Concept: Structural harm vs. visible harm
  - Why needed here: The paper argues that AI safety focuses on preventing visible system failures while allowing invisible structural harms like algorithmic discrimination to continue, creating a false sense of security
  - Quick check question: According to the paper, what is the key difference between visible and structural harm in AI systems? (Answer: Visible harm is obvious and easily identified, while structural harm is often invisible and requires transparency mechanisms to detect)

## Architecture Onboarding

- Component map: Corporate/government AI safety hype -> Insufficient safety definitions -> Normalization of harmful AI systems and regulatory capture
- Critical path: The core argument flows from establishing the corporate dominance of AI safety discourse → demonstrating how current safety definitions are insufficient → showing how this enables harmful systems and regulatory capture
- Design tradeoffs: The paper trades technical depth for accessibility, using concrete examples (predictive policing, insurance algorithms, gig economy) rather than technical specifications. It also prioritizes regulatory analysis over technical implementation details
- Failure signatures: The argument could fail if: (1) Evidence of corporate influence is overstated, (2) Alternative interpretations of AI safety exist that address the concerns raised, or (3) The regulatory examples are cherry-picked
- First 3 experiments:
  1. Map the evolution of regulatory language in AI safety discussions across multiple jurisdictions to verify the claim about deprioritizing fundamental rights
  2. Analyze corporate funding patterns in AI safety research to quantify the corporate influence claim
  3. Survey AI safety definitions from different organizations to assess whether they include structural harm prevention mechanisms

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the prioritization of AI safety over other concerns like transparency and fundamental rights affect the overall societal impact of AI systems?
- Basis in paper: [explicit] The paper argues that the focus on AI safety may deprioritize the protection of individuals' fundamental rights and lead to opaque AI systems that can perpetuate structural harm.
- Why unresolved: The paper highlights the potential risks of prioritizing AI safety but does not provide concrete evidence or case studies showing the extent of this impact in real-world scenarios.
- What evidence would resolve it: Empirical studies comparing the societal outcomes of AI systems developed under strict AI safety protocols versus those with a focus on transparency and fundamental rights protection.

### Open Question 2
- Question: To what extent does the current global enthusiasm for AI safety reflect a genuine concern for preventing harm, versus a strategic move by governments and corporations to maintain control over AI development?
- Basis in paper: [explicit] The paper suggests that the AI safety movement is dominated by governments and corporations, which may use it as a veneer to continue their profit-driven goals without addressing deeper societal issues.
- Why unresolved: The paper raises suspicions about the motivations behind the AI safety movement but does not provide direct evidence of ulterior motives or analyze the intentions of key stakeholders.
- What evidence would resolve it: Investigative reports or leaked documents revealing the strategic objectives of governments and corporations in promoting AI safety, along with analyses of their actions in relation to other AI ethics initiatives.

### Open Question 3
- Question: How can AI safety frameworks be designed to incorporate transparency and accountability measures without compromising the efficiency and innovation of AI systems?
- Basis in paper: [inferred] The paper argues that AI safety as currently defined does not mandate transparency, which could allow harmful AI systems to operate under the guise of safety. This suggests a need for a more balanced approach.
- Why unresolved: The paper critiques the current state of AI safety but does not propose specific mechanisms or frameworks that could integrate transparency and accountability while maintaining innovation.
- What evidence would resolve it: Case studies of AI systems or regulatory frameworks that successfully balance safety, transparency, and innovation, along with comparative analyses of their performance and societal impact.

## Limitations

- The paper lacks direct empirical evidence for some of its claims about corporate and government influence on AI safety discourse
- The analysis of regulatory language evolution relies on selective textual analysis that could be subject to alternative interpretations
- The connection between AI safety framing and the normalization of harmful AI systems is primarily theoretical rather than systematically validated

## Confidence

- High confidence: Observation that current AI safety discourse differs from academic work on fairness, accountability, and social good
- Medium confidence: Claim that AI safety framing shifts regulatory focus away from fundamental rights protections
- Low confidence: Assertion that AI safety discourse deliberately enables harmful systems

## Next Checks

1. Conduct a systematic content analysis of AI safety discussions across regulatory documents, corporate communications, and academic literature to quantify the relative emphasis on technical safety versus fundamental rights protections.

2. Map the funding sources and authorship patterns of prominent AI safety publications and organizations to empirically assess the claim of corporate dominance in the discourse.

3. Perform a comparative analysis of AI safety definitions across major AI companies, governments, and academic institutions to determine whether they include provisions for addressing structural harm and requiring transparency mechanisms.