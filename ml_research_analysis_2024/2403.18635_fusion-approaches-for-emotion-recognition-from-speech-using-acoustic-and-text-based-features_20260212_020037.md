---
ver: rpa2
title: Fusion approaches for emotion recognition from speech using acoustic and text-based
  features
arxiv_id: '2403.18635'
source_url: https://arxiv.org/abs/2403.18635
tags:
- speech
- audio
- fusion
- iemocap
- folds
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies emotion recognition from speech using acoustic
  and text-based features. The authors propose using contextualized word embeddings
  with BERT to represent speech transcriptions, showing better performance than using
  Glove embeddings.
---

# Fusion approaches for emotion recognition from speech using acoustic and text-based features

## Quick Facts
- arXiv ID: 2403.18635
- Source URL: https://arxiv.org/abs/2403.18635
- Reference count: 0
- Primary result: Fusing audio and text information leads to significant improvements of approximately 16% on both datasets relative to using the best single modality.

## Executive Summary
This paper studies emotion recognition from speech using acoustic and text-based features. The authors propose using contextualized word embeddings with BERT to represent speech transcriptions, showing better performance than using Glove embeddings. They compare different strategies to combine audio and text modalities on IEMOCAP and MSP-PODCAST datasets, finding that fusing acoustic and text-based systems is beneficial on both datasets. The study also highlights the large effect that the criteria used to define cross-validation folds have on results, with the standard way of creating folds for IEMOCAP resulting in a highly optimistic estimation of performance for the text-based system.

## Method Summary
The authors train individual audio and text models on IEMOCAP and MSP-PODCAST datasets, then fuse them using early fusion (EF), late fusion (LF), and various training strategies (cold-start, pre-trained, warm-start). Audio features include MFCCs, pitch, loudness, jitter, shimmer, and logHNR, while text features use BERT or Glove embeddings. The models are trained using cross-entropy loss and Adam optimizer, with performance evaluated using average recall and average area under the ROC for four emotion classes (happy, sad, angry, neutral).

## Key Results
- Contextualized word embeddings from BERT lead to significant improvements over Glove embeddings
- Fusing acoustic and text information improves performance by approximately 16% on both datasets
- Speaker+script-balanced cross-validation is essential for fair evaluation on IEMOCAP, as standard speaker-only folds inflate text-based performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Contextualized embeddings (BERT) capture emotion-relevant word meanings better than static embeddings (GloVe).
- Mechanism: BERT embeddings are computed from the entire sentence, so words like "sad" have different embeddings depending on context ("I am very sad" vs "I am not sad at all"), allowing the model to distinguish sentiment shifts.
- Core assumption: The emotional content of a word depends on its surrounding context, and the model can learn to use this contextual variation.
- Evidence anchors:
  - [abstract] "contextualized word embeddings obtained with BERT leads to significant improvements in performance compared to using standard word embeddings obtained with Glove"
  - [section] "contextualized word embeddings like those extracted by BERT take into account the whole phrase in which the word is found. As a consequence, the embedding corresponding to the word "sad" in those two phrases would most likely be different."
- Break condition: If the training corpus has very few examples of context-dependent emotional words, or if the model is too small to capture these nuances.

### Mechanism 2
- Claim: Speaker- and script-balanced cross-validation folds prevent inflated text-based performance.
- Mechanism: IEMOCAP contains scripted dialogues; if the same script appears in both train and test folds, the text model memorizes script-specific wording, giving unrealistically high scores. By excluding repeated speakers and scripts across folds, evaluation reflects true generalization.
- Core assumption: Text-based emotion cues are often tied to specific wording in scripts, and scripts are reused across speakers.
- Evidence anchors:
  - [section] "creating folds by speaker is not sufficient to obtain fair performance predictions on IEMOCAP, since the data contains scripted dialogues which greatly affect the performance of text-based systems when the same script is observed in training and testing"
  - [section] "This last point is very important for the text-based model, as has been noted in [7], because dialogues from the same script are very similar"
- Break condition: If all scripted content is removed from the dataset, or if the task uses spontaneous speech only.

### Mechanism 3
- Claim: Early and late fusion of audio and text modalities improves emotion classification over single modalities.
- Mechanism: Audio captures prosody and tone, while text captures semantics; combining them at either feature level (early) or decision level (late) lets the model leverage complementary signals.
- Core assumption: Emotion is encoded in both acoustic-prosodic and lexical channels, and these channels are not perfectly redundant.
- Evidence anchors:
  - [abstract] "fusing acoustic and text-based systems is beneficial on both datasets"
  - [section] "All fusion approaches perform similarly, in agreement with previous results in the literature"
- Break condition: If one modality is consistently noisy or if the model cannot learn to integrate modalities effectively.

## Foundational Learning

- Concept: BERT architecture and contextual embeddings
  - Why needed here: Understanding how BERT generates word representations that depend on sentence context is key to interpreting why it outperforms static embeddings in emotion recognition.
  - Quick check question: What part of BERT's architecture allows it to generate different embeddings for the same word in different contexts?

- Concept: Cross-validation design and data leakage
  - Why needed here: Recognizing how improper fold design (e.g., reusing scripts) can inflate performance metrics is essential for fair evaluation, especially with scripted datasets.
  - Quick check question: Why does using speaker-only folds still risk data leakage in IEMOCAP?

- Concept: Multimodal fusion strategies
  - Why needed here: Knowing the difference between early and late fusion, and their respective tradeoffs, is necessary to design and troubleshoot multimodal emotion recognition systems.
  - Quick check question: In what scenario might early fusion outperform late fusion, and vice versa?

## Architecture Onboarding

- Component map:
  Text → BERT extraction → embedding reduction → conv → pooling → fusion → output (or same for audio, then fusion)

- Critical path:
  Text → BERT extraction → embedding reduction → conv → pooling → fusion → output (or same for audio, then fusion)

- Design tradeoffs:
  - Early fusion requires aligned feature dimensions and may merge noisy features; late fusion keeps modalities separate longer but may miss fine-grained interactions
  - Using contextualized embeddings increases model complexity and compute but improves context capture
  - Cross-validation by speaker+script is more conservative but prevents optimistic bias

- Failure signatures:
  - Overfitting in text branch: High train accuracy but poor test accuracy, especially if scripts leak between folds
  - Modality imbalance: One modality dominates predictions; check attention or weight distribution
  - Fusion underperformance: Similar performance to best single modality; may indicate poor alignment or lack of complementary information

- First 3 experiments:
  1. Train text and audio models separately on IEMOCAP with speaker+script folds; compare to speaker-only folds to observe leakage effect.
  2. Implement early fusion and late fusion; measure if combining modalities yields statistically significant gains over single best modality.
  3. Swap GloVe for BERT in the text branch; quantify improvement and analyze failure cases where context matters.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of contextualized word embeddings from BERT compare to other state-of-the-art contextual embedding models (e.g., ELMo, GPT) for emotion recognition from speech transcriptions?
- Basis in paper: [explicit] The paper shows that BERT outperforms Glove embeddings, but does not compare BERT to other contextual embedding models.
- Why unresolved: The study focuses solely on BERT and Glove, leaving the comparison with other contextual embedding approaches unexplored.
- What evidence would resolve it: Conducting experiments using other contextual embedding models (e.g., ELMo, GPT) and comparing their performance to BERT on the same emotion recognition tasks and datasets.

### Open Question 2
- Question: What is the impact of using different feature extraction methods for audio (e.g., spectrograms, embeddings from pre-trained audio models) on the performance of emotion recognition systems?
- Basis in paper: [inferred] The paper uses MFCCs, pitch, loudness, jitter, shimmer and logHNR features for the audio branch, but does not explore other feature extraction methods.
- Why unresolved: The study is limited to a specific set of audio features, leaving the potential benefits of alternative feature extraction methods unexplored.
- What evidence would resolve it: Conducting experiments using different audio feature extraction methods and comparing their performance to the baseline features used in the study.

### Open Question 3
- Question: How does the performance of emotion recognition systems vary across different emotion classification schemes (e.g., dimensional vs. categorical models)?
- Basis in paper: [inferred] The paper uses a categorical model with four emotion classes (happy, sad, angry, and neutral), but does not explore other classification schemes.
- Why unresolved: The study is limited to a specific emotion classification scheme, leaving the potential benefits of alternative schemes unexplored.
- What evidence would resolve it: Conducting experiments using different emotion classification schemes and comparing their performance to the baseline scheme used in the study.

### Open Question 4
- Question: What is the effect of incorporating additional modalities (e.g., facial expressions, body gestures) on the performance of emotion recognition systems?
- Basis in paper: [inferred] The paper focuses on fusing audio and text modalities, but does not explore the incorporation of additional modalities.
- Why unresolved: The study is limited to two modalities, leaving the potential benefits of incorporating additional modalities unexplored.
- What evidence would resolve it: Conducting experiments using additional modalities (e.g., facial expressions, body gestures) and comparing their performance to the baseline system using only audio and text.

## Limitations
- Lack of complete implementation details for cross-validation fold creation, particularly criteria for avoiding repeated speakers and scripts
- Underspecified BERT embedding extraction process and specific text model architecture details
- Results based on only two datasets, limiting generalizability to other domains or languages
- No statistical significance testing reported for performance differences between fusion strategies

## Confidence
- High confidence in the mechanism explaining why BERT outperforms GloVe (contextual embeddings capture emotion-relevant word meanings)
- High confidence in the importance of speaker+script-balanced cross-validation for preventing text-based performance inflation
- Medium confidence in the fusion benefit claims, as all fusion approaches performed similarly and statistical significance is not reported
- Medium confidence in the relative performance numbers due to incomplete methodological details

## Next Checks
1. Implement and compare cross-validation strategies: Reproduce the study using both speaker-only and speaker+script-balanced folds on IEMOCAP to verify the claimed performance inflation when scripts leak between train/test sets.

2. Statistical significance testing: Apply appropriate statistical tests (e.g., paired t-tests or McNemar's test) to the AvRec and AvAUC results across all fusion strategies to determine which performance differences are statistically significant.

3. Ablation study on BERT embeddings: Systematically remove contextual information by training on single words versus full sentences to quantify exactly how much the context-awareness of BERT contributes to the performance gains over GloVe.