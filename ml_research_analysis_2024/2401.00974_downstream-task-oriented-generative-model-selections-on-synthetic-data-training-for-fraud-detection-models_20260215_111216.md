---
ver: rpa2
title: Downstream Task-Oriented Generative Model Selections on Synthetic Data Training
  for Fraud Detection Models
arxiv_id: '2401.00974'
source_url: https://arxiv.org/abs/2401.00974
tags:
- data
- synthetic
- generative
- training
- fraud
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the problem of downstream task-oriented generative
  model selection for synthetic data training in fraud detection. It compares neural
  network-based (NN) and Bayesian network-based (BN) generative models in terms of
  accuracy, AUROC, recall, precision, and F1 score.
---

# Downstream Task-Oriented Generative Model Selections on Synthetic Data Training for Fraud Detection Models

## Quick Facts
- arXiv ID: 2401.00974
- Source URL: https://arxiv.org/abs/2401.00974
- Reference count: 40
- Primary result: Bayesian network-based models excel in F1 score and precision, while neural network-based models are preferred for AUROC and recall in utility-oriented fraud detection tasks.

## Executive Summary
This paper addresses the critical challenge of selecting appropriate generative models for synthetic data training in fraud detection systems. The authors systematically compare neural network-based (NN) and Bayesian network-based (BN) generative models across multiple performance metrics and model interpretability levels. Their empirical study reveals that different generative model families excel under different evaluation criteria: NN-based models perform better for AUROC and recall, while BN-based models show superior performance in F1 score and precision metrics. The research provides practical guidance for machine learning practitioners by mapping specific generative model choices to downstream task requirements and interpretability constraints.

## Method Summary
The study uses the Credit Card Fraud Dataset (31 features, 0.1727% fraud cases) to evaluate generative model performance. Synthetic datasets are generated using SDV library components (CTGAN, TVAE, GaussianCopula, CopulaGAN) and DataSynthesizer with epsilon settings (0, 0.1). The authors train nine fraud detection models (Logistic Regression, Decision Tree, KNN, Naive Bayes, SVM, Random Forest, GAM, XGBoost, NAM) on both real and synthetic training data, evaluating performance on a held-out test set using accuracy, AUROC, F1 score, recall, precision, and precision-recall curves. The experiment pipeline involves data synthesis, model training on synthetic data, and comprehensive performance evaluation across utility and interpretability dimensions.

## Key Results
- For utility-oriented selection, NN-based models (CTGAN, TVAE) are preferred for AUROC and recall metrics, while BN-based models excel in F1 score and precision.
- Interpretability-oriented selection favors BN-based models for intrinsically interpretable models (Decision Tree, KNN) and medium interpretable models (GAM).
- Both NN-based and BN-based models perform similarly for not-easy interpretable models (XGBoost, NAM).
- GaussianCopula and CopulaGAN were excluded from analysis as they failed to generate any fraud cases in synthetic datasets.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Neural network-based generative models are preferred for utility-oriented downstream tasks when the evaluation metric is AUROC or recall.
- Mechanism: NN-based models (CTGAN, TVAE) better capture complex data distributions and can generate synthetic samples that improve the classifier's ability to distinguish between fraud and non-fraud classes, leading to higher AUROC and recall scores.
- Core assumption: The synthetic data generated by NN-based models sufficiently represents the underlying data distribution to improve model discrimination.
- Evidence anchors:
  - [abstract]: "for utility-oriented selection, NN-based models are preferred for AUROC and recall"
  - [section 4.2]: "CTGAN is a suitable data generative technique to synthesize data considering the performance of AUROC" and "CTGAN surpasses all of the other methods and even distinctly exceeds the original data" for recall.
- Break condition: If the real data distribution is simple or if the NN-based model overfits the training data, the advantage in AUROC and recall may disappear.

### Mechanism 2
- Claim: Bayesian network-based generative models are preferred for utility-oriented downstream tasks when the evaluation metric is F1 score or precision.
- Mechanism: BN-based models (DS with epsilon 0) generate synthetic data that better balances the precision-recall tradeoff, leading to higher F1 scores and precision metrics.
- Core assumption: The synthetic data generated by BN-based models maintains the correlation structure of the original data, which is crucial for achieving high precision and F1 score.
- Evidence anchors:
  - [abstract]: "BN-based models excel in F1 score and precision"
  - [section 4.2]: "DS with epsilon 0 improves the performance of NaÃ¯ve Bayes for fraud detection" for F1 score and "DS with epsilon 0 can synthesize data comparable to the original data when focusing on precision" for precision.
- Break condition: If the correlation structure of the original data is not important for the downstream task, or if the BN-based model fails to capture the data distribution accurately, the advantage in F1 score and precision may be lost.

### Mechanism 3
- Claim: Bayesian network-based generative models are preferred for interpretability-oriented downstream tasks when the fraud detection model is intrinsically or medium interpretable.
- Mechanism: BN-based models generate synthetic data that preserves the interpretability of the fraud detection model, allowing the model to maintain its decision-making transparency.
- Core assumption: The synthetic data generated by BN-based models does not introduce noise or complexity that would compromise the interpretability of the fraud detection model.
- Evidence anchors:
  - [abstract]: "Interpretability-oriented selection favors BN-based models for intrinsic and medium interpretable models"
  - [section 4.3]: "all medium interpretable models select Bayesian network-based generative models to synthesize data" and "Decision Tree and KNN suggest Bayesian network-based generative models as better tools for synthetic data generation" for intrinsically interpretable models.
- Break condition: If the fraud detection model is not-easy interpretable (e.g., XGBoost, NAM), or if the BN-based model fails to capture the data distribution accurately, the advantage in interpretability may be lost.

## Foundational Learning

- Concept: Generative model selection for synthetic data training
  - Why needed here: The paper aims to provide practical guidance for selecting the best family of generative models for synthetic training tasks given a specific combination of machine learning model class and performance metric.
  - Quick check question: What are the key factors to consider when selecting a generative model for synthetic data training in fraud detection tasks?

- Concept: Utility-interpretability tradeoff in fraud detection models
  - Why needed here: The paper investigates the best practice given different combinations of model interpretability and model performance constraints, highlighting the importance of considering both model interpretability and performance metrics when selecting generative models.
  - Quick check question: How does the utility-interpretability tradeoff impact the selection of generative models for synthetic data training in fraud detection tasks?

- Concept: Performance metrics for imbalanced datasets
  - Why needed here: The paper evaluates the performance of synthesized data on fraud detection using metrics suitable for imbalanced datasets, such as AUROC, recall, precision, F1 score, and precision-recall curve.
  - Quick check question: Why are metrics like accuracy misleading for imbalanced datasets, and what alternative metrics should be used instead?

## Architecture Onboarding

- Component map: Synthetic Data Generation (CTGAN, TVAE, DataSynthesizer) -> Fraud Detection Models (9 models) -> Performance Evaluation (Accuracy, AUROC, F1, Recall, Precision, PR Curve)

- Critical path:
  1. Generate synthetic data using SDV and DS
  2. Split original data into training (70%) and test (30%) sets
  3. Train fraud detection models on synthetic training data
  4. Evaluate model performance on test data using various metrics
  5. Analyze results to determine the best generative model for each combination of model interpretability and performance metric

- Design tradeoffs:
  - NN-based vs. BN-based generative models: NN-based models may better capture complex data distributions but can be harder to interpret, while BN-based models may preserve interpretability but struggle with complex distributions.
  - Epsilon value in DS: Higher epsilon values provide more privacy but may result in less accurate synthetic data.
  - Model interpretability vs. performance: More interpretable models may have lower performance, while less interpretable models may have higher performance.

- Failure signatures:
  - Poor performance across all metrics: The generative model may not be capturing the data distribution accurately.
  - High variance in performance across different models or metrics: The synthetic data may not be representative of the original data distribution.
  - Inconsistent results across different runs: The generative model may be sensitive to random initialization or hyperparameters.

- First 3 experiments:
  1. Generate synthetic data using CTGAN and train all 9 fraud detection models on the synthetic data. Evaluate performance using all metrics and compare to the original data.
  2. Generate synthetic data using DS with epsilon 0 and train all 9 fraud detection models on the synthetic data. Evaluate performance using all metrics and compare to the original data.
  3. Generate synthetic data using TVAE and train all 9 fraud detection models on the synthetic data. Evaluate performance using all metrics and compare to the original data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the long-term performance implications of using synthetic data for fraud detection model training?
- Basis in paper: [inferred] The paper mentions that performance degradation is a price of using synthetic data, but does not explore long-term effects or model drift.
- Why unresolved: The study focuses on immediate performance metrics and does not consider how synthetic data might impact model performance over time or as fraud patterns evolve.
- What evidence would resolve it: Longitudinal studies comparing fraud detection models trained on synthetic data versus real data, tracking performance changes over extended periods and under varying fraud scenarios.

### Open Question 2
- Question: How do different data augmentation techniques affect the interpretability and performance of fraud detection models trained on synthetic data?
- Basis in paper: [explicit] The paper mentions synthetic augmented training but does not extensively explore various data augmentation techniques.
- Why unresolved: The study primarily focuses on the selection of generative models but does not delve into how different augmentation strategies might impact model interpretability and performance.
- What evidence would resolve it: Comparative studies of various data augmentation techniques (e.g., SMOTE, ADASYN, GAN-based methods) on the interpretability and performance of fraud detection models trained on synthetic data.

### Open Question 3
- Question: How do the results of this study generalize to other domains beyond fraud detection?
- Basis in paper: [explicit] The paper focuses specifically on fraud detection models and does not explore applicability to other domains.
- Why unresolved: The study's findings are limited to the fraud detection context, and it is unclear how well these results would translate to other domains with different data characteristics and model requirements.
- What evidence would resolve it: Replicating similar studies in various domains (e.g., healthcare, finance, cybersecurity) to compare the performance and interpretability of models trained on synthetic data versus real data across different use cases.

## Limitations

- The study's findings may not generalize to fraud datasets with different characteristics or fraud rates outside the 0.1727% range examined.
- Hyperparameter sensitivity is not explored, leaving uncertainty about how stable the observed tradeoffs are under different configuration settings.
- Temporal aspects of fraud pattern evolution are not considered, limiting understanding of synthetic data's effectiveness over time.

## Confidence

**High Confidence**: The core finding that Bayesian network-based models excel in F1 score and precision metrics for utility-oriented selection, supported by consistent results across multiple interpretable models (Decision Tree, KNN, Naive Bayes) and the medium interpretability GAM model.

**Medium Confidence**: The assertion that neural network-based models are superior for AUROC and recall metrics, as this conclusion is based primarily on CTGAN performance and may be influenced by specific hyperparameter choices or dataset characteristics.

**Low Confidence**: The interpretability-oriented selection guidance for not-easy interpretable models (XGBoost, NAM), as the paper notes that both model families perform similarly in these cases, providing limited practical guidance for practitioners using complex models.

## Next Checks

1. **Cross-dataset validation**: Replicate the experiment using multiple fraud detection datasets with varying characteristics (different feature types, fraud rates, and temporal patterns) to assess the robustness of the proposed selection guidelines.

2. **Hyperparameter sensitivity analysis**: Systematically vary key hyperparameters for both generative models and fraud detection algorithms to determine the stability of the observed utility-interpretability tradeoffs under different configuration settings.

3. **Temporal validation**: Implement a time-based validation framework where synthetic data is generated from historical data and evaluated on subsequent time periods to assess the impact of evolving fraud patterns on model performance and the validity of synthetic training data over time.