---
ver: rpa2
title: A Novel Multi-Stage Prompting Approach for Language Agnostic MCQ Generation
  using GPT
arxiv_id: '2401.07098'
source_url: https://arxiv.org/abs/2401.07098
tags:
- generation
- language
- https
- languages
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors propose a multi-stage prompting (MSP) approach for
  generating multiple choice questions (MCQs) in multiple languages using large language
  models like GPT-4 and text-davinci-003. The MSP approach consists of four stages:
  paraphrase generation, keyword extraction, question generation, and distractor generation.'
---

# A Novel Multi-Stage Prompting Approach for Language Agnostic MCQ Generation using GPT

## Quick Facts
- arXiv ID: 2401.07098
- Source URL: https://arxiv.org/abs/2401.07098
- Authors: Subhankar Maity; Aniket Deroy; Sudeshna Sarkar
- Reference count: 34
- Primary result: Multi-stage prompting consistently produces higher-quality distractors than single-stage prompting across multiple languages

## Executive Summary
This paper introduces a multi-stage prompting (MSP) approach for generating high-quality multiple-choice questions (MCQs) in multiple languages using GPT models. The approach breaks down the complex MCQ generation task into four sequential stages: paraphrase generation, keyword extraction, question generation, and distractor generation. By providing step-by-step guidance through chain-of-thought prompting, the MSP method significantly improves distractor quality compared to traditional single-stage prompting. The approach demonstrates effectiveness across English, German, Bengali, and Hindi, with particular benefits for low-resource languages when using one-shot examples.

## Method Summary
The method employs a four-stage prompting pipeline using GPT-4 and text-davinci-003. First, contexts are paraphrased to create multiple versions. Keywords are then extracted from these paraphrases, followed by question generation using the keywords and original contexts. Finally, distractors are generated for each question-answer pair. The approach is tested in both zero-shot and one-shot settings, with the latter providing a demonstration example to guide the model. The MSP approach is compared against a single-stage prompting baseline that attempts to generate complete MCQs in one step. Evaluation uses automated metrics (BLEU, ROUGE-L, cosine similarity) and human assessment of grammaticality, answerability, and difficulty.

## Key Results
- MSP outperforms single-stage prompting in automated metrics (BLEU, ROUGE-L, cosine similarity) for distractor quality across all tested languages
- One-shot examples significantly improve results, particularly for low-resource languages like Hindi and Bengali
- Human evaluations confirm that MSP-generated questions have better grammaticality, answerability, and appropriate difficulty levels
- The approach successfully generates quality MCQs across multiple languages including English, German, Bengali, and Hindi

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-stage prompting (MSP) outperforms single-stage prompting (SSP) in generating higher quality distractors for MCQs.
- Mechanism: MSP breaks down the complex task of MCQ generation into sequential stages (paraphrase generation, keyword extraction, question generation, distractor generation) allowing GPT models to focus on one subtask at a time with guided reasoning.
- Core assumption: Sequential decomposition of the MCQ generation task into smaller, focused subtasks improves overall quality compared to attempting all subtasks simultaneously.
- Evidence anchors:
  - [abstract] "Automated evaluations consistently demonstrate the superiority of our proposed MSP method over the traditional single-stage prompting (SSP) baseline, resulting in the production of high-quality distractors."
  - [section] "Our approach incorporates the innovative concept of chain-of-thought prompting, a progressive technique in which the GPT model is provided with a series of interconnected cues to guide the MCQ generation process."
  - [corpus] Weak evidence - no direct corpus support for MSP vs SSP comparison found
- Break condition: If the intermediate steps introduce cumulative errors that compound rather than improve final output quality.

### Mechanism 2
- Claim: One-shot examples improve MCQ generation quality, especially for low-resource languages.
- Mechanism: Providing a single demonstration example (one-shot) helps guide the model's generation process by showing the expected output format and reasoning pattern.
- Core assumption: Even a single demonstration example provides sufficient guidance to improve model performance, particularly when pre-training data is limited for certain languages.
- Evidence anchors:
  - [abstract] "Furthermore, the one-shot MSP technique enhances automatic evaluation results, contributing to improved distractor generation in multiple languages"
  - [section] "In addition, we have tried the one-shot technique to improve the quality of the distractors. The one-shot technique for DG involves the following prompt: 'The distractors for the question <questioni> and the correct answer <correct answeri> are <distractori1, distractori2, distractori3> in language x. Create three plausible distractors for the question <questionj> and the correct answer <keyword j> in language x.'"
  - [corpus] No direct evidence found in corpus
- Break condition: If the one-shot example is not representative of the target task or introduces bias that degrades rather than improves quality.

### Mechanism 3
- Claim: MSP approach effectively handles multilingual MCQ generation, with better performance on high-resource languages but showing improvement for low-resource languages.
- Mechanism: The MSP approach leverages GPT models' multilingual capabilities while the multi-stage structure helps compensate for data scarcity in low-resource languages by providing structured guidance.
- Core assumption: The structured approach of MSP can partially compensate for limited pre-training data in low-resource languages.
- Evidence anchors:
  - [abstract] "Our approach incorporates the innovative concept of chain-of-thought prompting... in multiple languages, including English, German, Bengali, and Hindi."
  - [section] "This occurs because showing a one-shot example to the GPT model helps to improve human evaluation criteria more for low-resource languages than for high-resource languages."
  - [corpus] Weak evidence - no direct corpus support for multilingual performance comparison found
- Break condition: If language-specific limitations of the underlying models cannot be overcome through prompting techniques alone.

## Foundational Learning

- Concept: Chain-of-thought prompting
  - Why needed here: Enables the model to perform step-by-step reasoning rather than jumping directly to the final answer, which is crucial for complex tasks like MCQ generation
  - Quick check question: How does chain-of-thought prompting differ from direct prompting in terms of intermediate reasoning steps?

- Concept: Few-shot vs zero-shot learning
  - Why needed here: Understanding when to use one-shot examples versus pure zero-shot prompting based on task complexity and available resources
  - Quick check question: What are the key differences in model behavior between one-shot and zero-shot settings?

- Concept: Language model evaluation metrics
  - Why needed here: BLEU, ROUGE-L, and cosine similarity are used to evaluate distractor quality, requiring understanding of what these metrics measure
  - Quick check question: What aspects of text quality does BLEU-4 capture that BLEU-1 does not?

## Architecture Onboarding

- Component map: Paraphrase Generator → Keyword Extractor → Question Generator → Distractor Generator, with optional one-shot example injection at each stage
- Critical path: The question quality depends on the entire pipeline working correctly; a failure in any stage can propagate downstream and degrade final output
- Design tradeoffs: MSP adds complexity through multiple API calls and intermediate processing versus the simplicity of SSP; this increases latency but potentially improves quality
- Failure signatures: Low-quality distractors may indicate issues with either the question generation stage or distractor generation stage; poor question quality suggests problems earlier in the pipeline
- First 3 experiments:
  1. Compare MSP vs SSP on a small English dataset using automated metrics to verify the baseline improvement claim
  2. Test one-shot versus zero-shot settings on a low-resource language to confirm the improvement effect
  3. Run human evaluation on a subset of generated questions to validate automated metric results and establish quality benchmarks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the MSP approach scale with different numbers of paraphrases (M) in the paraphrase generation stage?
- Basis in paper: [explicit] The paper mentions that M=3 paraphrases are used in the experiments, but does not explore the impact of varying M.
- Why unresolved: The paper does not provide any analysis or discussion on how the number of paraphrases affects the overall quality of the generated MCQs.
- What evidence would resolve it: Conducting experiments with different values of M and comparing the results in terms of automated and human evaluation metrics would provide insights into the optimal number of paraphrases.

### Open Question 2
- Question: How does the MSP approach perform when applied to other low-resource languages not covered in this study, such as Arabic or Swahili?
- Basis in paper: [inferred] The paper demonstrates the effectiveness of the MSP approach for Hindi and Bengali, but these are only two examples of low-resource languages.
- Why unresolved: The paper does not explore the generalizability of the MSP approach to a wider range of low-resource languages.
- What evidence would resolve it: Evaluating the MSP approach on additional low-resource languages and comparing the results with those of high-resource languages would provide insights into its cross-lingual applicability.

### Open Question 3
- Question: What is the impact of using different GPT models (e.g., GPT-3.5, GPT-4) on the performance of the MSP approach for MCQ generation?
- Basis in paper: [explicit] The paper uses text-davinci-003 and GPT-4 for experiments, but does not compare their performance directly.
- Why unresolved: The paper does not provide a comprehensive analysis of how different GPT models affect the quality of the generated MCQs.
- What evidence would resolve it: Conducting experiments using various GPT models and comparing their performance in terms of automated and human evaluation metrics would provide insights into the optimal model for the MSP approach.

## Limitations

- Lack of detailed prompt specifications makes exact reproduction difficult and limits generalizability
- Automated metrics (BLEU, ROUGE-L) may not fully capture pedagogical quality of distractors or educational effectiveness
- Human evaluation criteria are subjective and may vary across evaluators, introducing potential bias
- The paper does not address potential biases in generated questions or verify factual accuracy against source contexts

## Confidence

- **High confidence**: The core finding that multi-stage prompting improves distractor quality over single-stage prompting is well-supported by both automated and human evaluations across multiple languages
- **Medium confidence**: The claim that one-shot examples improve performance for low-resource languages is supported but lacks detailed analysis of why this occurs and how representative the one-shot examples were
- **Medium confidence**: The multilingual performance comparison shows trends but lacks statistical significance testing and deeper analysis of language-specific challenges

## Next Checks

1. Reproduce the MSP vs SSP comparison using a small English dataset with the same automated metrics (BLEU, ROUGE-L, cosine similarity) to verify the baseline quality improvement claim independently

2. Test the one-shot effect on low-resource languages by generating MCQs with and without one-shot examples for Hindi and Bengali, then comparing both automated metrics and human evaluation scores

3. Analyze distractor semantic quality by having human experts rate distractor appropriateness and plausibility beyond surface-level similarity metrics, focusing on whether distractors represent plausible but incorrect alternatives that test genuine understanding