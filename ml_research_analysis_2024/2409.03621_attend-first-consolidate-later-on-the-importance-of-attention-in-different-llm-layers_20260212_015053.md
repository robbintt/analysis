---
ver: rpa2
title: 'Attend First, Consolidate Later: On the Importance of Attention in Different
  LLM Layers'
arxiv_id: '2409.03621'
source_url: https://arxiv.org/abs/2409.03621
tags:
- layers
- hidden
- tokens
- layer
- previous
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the role of attention mechanisms in different
  layers of transformer-based language models, specifically examining how much earlier
  tokens' representations matter at various depths. The authors conduct experiments
  with four LLMs (Llama2-7B, Mistral-7B, Yi-6B, and Llemma-7B) across four tasks,
  manipulating hidden states of previous tokens in various ways including replacing
  with random vectors, freezing, and swapping with states from other prompts.
---

# Attend First, Consolidate Later: On the Importance of Attention in Different LLM Layers

## Quick Facts
- **arXiv ID**: 2409.03621
- **Source URL**: https://arxiv.org/abs/2409.03621
- **Reference count**: 3
- **Primary result**: Transformer models show a two-phase processing pattern where early layers gather information from previous tokens while upper layers process internally with minimal attention requirements

## Executive Summary
This paper investigates how attention mechanisms function across different layers of transformer-based language models. Through systematic manipulation experiments with four 7B-parameter models (Llama2-7B, Mistral-7B, Yi-6B, and Llemma-7B) across four tasks, the authors reveal that transformers don't uniformly process all tokens throughout all layers. Instead, they exhibit a two-phase pattern where the first 50-70% of layers are attention-critical for gathering information from previous tokens, while the remaining layers primarily process this information internally. This finding has important implications for understanding transformer architecture and potentially making these models more efficient by reducing attention computations in upper layers.

## Method Summary
The authors conducted controlled manipulation experiments on four transformer models, systematically altering the hidden states of previous tokens at various depths. They employed three manipulation techniques: replacing previous token representations with random vectors, freezing previous token states (preventing gradient updates), and swapping previous token states with those from different prompts. These manipulations were applied at different layer depths across four tasks (text continuation, classification, sentiment analysis, and summarization). Performance degradation was measured to assess the importance of attention to previous tokens at each layer depth. The experiments were repeated across multiple model architectures to verify consistency of findings.

## Key Results
- Transformer models exhibit a two-phase processing pattern: early layers (50-70%) are attention-critical while upper layers are attention-resilient
- Freezing the top 50% of layers often results in no performance loss across multiple tasks
- Attention mechanisms become less critical in upper layers, with some tasks showing robustness to skipping attention blocks entirely after certain depths
- The pattern is consistent across four different 7B-parameter models and multiple manipulation methods

## Why This Works (Mechanism)
The two-phase processing pattern emerges from the hierarchical nature of information processing in transformers. Early layers function as information gatherers, using attention mechanisms to collect and integrate context from previous tokens. As representations become more abstract and task-relevant through layer depth, upper layers transition to information processors that refine and consolidate these representations without requiring continuous access to the full token history. This suggests that once information is properly encoded in early layers, upper layers can operate on this compressed representation through feed-forward networks and residual connections, making continuous attention to previous tokens unnecessary for many tasks.

## Foundational Learning

**Attention Mechanism**: A mechanism that allows models to weigh the importance of different input elements when processing each position. Why needed: Understanding how attention distributes importance across tokens is crucial for interpreting layer-wise processing patterns. Quick check: Verify that attention weights in early layers show high entropy (distributed importance) while upper layers show more focused patterns.

**Transformer Layer Architecture**: Each layer consists of multi-head self-attention followed by feed-forward networks with residual connections. Why needed: The component-wise structure determines how information flows and transforms through the model. Quick check: Confirm that freezing attention outputs still allows information propagation through residual connections.

**Hidden State Representations**: The internal vector representations of tokens that evolve through layer depth. Why needed: These representations carry the semantic content that upper layers process. Quick check: Analyze representation similarity between early and late layers to understand information compression.

**Layer-wise Information Processing**: The hypothesis that different layers perform different computational roles. Why needed: This concept frames the interpretation of why attention becomes less important in upper layers. Quick check: Measure information-theoretic properties (entropy, mutual information) across layer depths.

**Task-specific Processing Requirements**: Different tasks may require different amounts of context and processing depth. Why needed: Understanding task variability helps explain why the 50-70% threshold isn't universal. Quick check: Compare attention criticality thresholds across task types (generation vs. classification).

## Architecture Onboarding

**Component Map**: Input Tokens -> Embedding Layer -> Layer 1 Attention -> Layer 1 FFN -> Layer 2 Attention -> ... -> Layer N Attention -> Layer N FFN -> Output Layer

**Critical Path**: Token embeddings flow through sequential attention and feed-forward blocks, with attention blocks being critical in early layers for context gathering and less critical in upper layers for internal processing.

**Design Tradeoffs**: The architecture balances between context gathering (attention-heavy early layers) and efficient processing (attention-light upper layers), trading off memory bandwidth for computation depth.

**Failure Signatures**: Performance degradation when freezing early attention layers indicates insufficient context gathering; degradation when freezing upper layers suggests tasks require continuous attention to history.

**First 3 Experiments**:
1. Apply the three manipulation methods (random replacement, freezing, cross-prompt swapping) to early vs. late layers to confirm the two-phase pattern
2. Measure attention weight distributions across layers to quantify the shift from distributed to focused attention
3. Test the pattern on complex reasoning tasks to verify generalizability beyond simple classification and generation

## Open Questions the Paper Calls Out
None

## Limitations
- Results based on relatively simple tasks may not generalize to complex reasoning or multi-step problem solving
- Manipulation methods create abrupt discontinuities that may not reflect gradual information processing in actual inference
- Experiments limited to 7B parameter models, with patterns potentially differing in larger architectures
- Paper doesn't investigate whether the two-phase pattern is learned during training or emerges from architectural constraints

## Confidence
- Two-phase processing pattern (50-70% attention-critical vs. attention-resilient layers): **High confidence** - consistent across four models and multiple manipulation methods
- Upper layers processing information internally: **Medium confidence** - plausible but not directly measured; requires activation analysis
- Practical efficiency of skipping upper attention layers: **Low confidence** - theoretical robustness demonstrated but no implementation or benchmarking validation

## Next Checks
1. Test the two-phase pattern on complex reasoning tasks (math word problems, code generation, multi-hop QA) to verify generalizability beyond simple classification and generation
2. Implement and benchmark an actual inference optimization that skips upper attention layers, measuring both accuracy retention and memory/compute savings
3. Conduct layer-wise activation analysis (e.g., probing classifier on intermediate representations) to determine what upper layers are actually computing when attention manipulations no longer affect output