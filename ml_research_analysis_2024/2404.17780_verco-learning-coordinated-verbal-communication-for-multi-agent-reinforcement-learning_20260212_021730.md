---
ver: rpa2
title: 'Verco: Learning Coordinated Verbal Communication for Multi-agent Reinforcement
  Learning'
arxiv_id: '2404.17780'
source_url: https://arxiv.org/abs/2404.17780
tags:
- uni00000013
- communication
- action
- agents
- message
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Verco proposes a multi-agent reinforcement learning framework that
  integrates large language models to enable agents to generate human-understandable
  verbal communication. The approach uses a teacher LLM (e.g., GPT-4) to generate
  message labels from global observations, then trains a student message module via
  supervised fine-tuning.
---

# Verco: Learning Coordinated Verbal Communication for Multi-agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2404.17780
- Source URL: https://arxiv.org/abs/2404.17780
- Authors: Dapeng Li; Hang Dong; Lu Wang; Bo Qiao; Si Qin; Qingwei Lin; Dongmei Zhang; Qi Zhang; Zhiwei Xu; Bin Zhang; Guoliang Fan
- Reference count: 40
- Primary result: Verco achieves significant performance gains in Overcooked game through interpretable verbal communication, outperforming baseline methods like Symbolic PPO, CommNet, and TWOSOME.

## Executive Summary
Verco introduces a novel multi-agent reinforcement learning framework that integrates large language models (LLMs) to enable coordinated verbal communication between agents. The framework uses a teacher LLM to generate message labels from global observations, which are then used to train a student message module via supervised fine-tuning. The action module is trained through reinforcement learning using PPO, receiving both local observations and verbal messages from other agents. By employing separate LoRA parameters for communication and action modules, Verco prevents interference between modules while maintaining interpretability of agent interactions.

## Method Summary
Verco proposes a two-module architecture where agents learn to communicate through language-like messages. The teacher LLM (e.g., GPT-4) generates message labels from global observations during training, which are used to train a student message module via supervised fine-tuning. The action module is trained using PPO reinforcement learning, taking both local observations and received verbal messages as inputs. The framework uses separate LoRA parameters for communication and action modules to prevent interference. During inference, the student LLM generates messages based on local observations, enabling interpretable coordination between agents.

## Key Results
- Verco significantly outperforms baseline methods (Symbolic PPO, CommNet, TWOSOME) on the Overcooked game environment
- The framework achieves interpretable communication patterns that reveal agents' collaborative strategies
- Verco demonstrates improved task performance through coordinated verbal communication between agents

## Why This Works (Mechanism)
The framework leverages the reasoning capabilities of large language models to generate meaningful communication labels from global observations. By training a student message module to replicate this behavior using local observations, agents learn to communicate effectively without requiring centralized information during inference. The separation of communication and action modules through LoRA parameters prevents interference while allowing each component to specialize in its respective task. The combination of supervised learning for communication and reinforcement learning for actions creates a robust learning framework that balances interpretability with performance.

## Foundational Learning
- **Large Language Model Integration**: LLM-based communication generation provides human-understandable messages; quick check: verify message quality through human evaluation
- **Supervised Fine-tuning for Communication**: Teacher-student approach transfers communication knowledge; quick check: measure message similarity between teacher and student outputs
- **Reinforcement Learning with PPO**: Action optimization using both observations and communication; quick check: validate reward improvement during training
- **LoRA Parameter Separation**: Prevents interference between communication and action modules; quick check: compare performance with and without LoRA separation
- **Multi-agent Coordination**: Agents learn to work together through interpretable communication; quick check: analyze message patterns for coordination evidence
- **Global-to-Local Observation Transfer**: Enables decentralized communication using centralized training data; quick check: test communication effectiveness with only local observations

## Architecture Onboarding

**Component Map**: Teacher LLM -> Message Label Generation -> Student Message Module Training -> Local Message Generation -> Action Module -> PPO RL Training

**Critical Path**: Global observations → Teacher LLM → Message labels → Student message module → Local messages → Action module → Agent actions → Environment feedback → PPO update

**Design Tradeoffs**: Uses supervised learning for stable communication training vs. end-to-end RL for communication-action integration; separates modules with LoRA to prevent interference vs. unified parameter sharing for simplicity; requires teacher LLM supervision vs. self-supervised communication learning

**Failure Signatures**: Communication breakdown when teacher LLM generates inconsistent messages; performance degradation if message module overfits to training distribution; coordination failure when local observations differ significantly from training scenarios

**First Experiments**:
1. Verify message quality by comparing teacher vs. student LLM outputs on held-out observations
2. Test communication effectiveness with ablated action module (fixed actions, only communication varies)
3. Measure performance impact of varying communication frequency and message length

## Open Questions the Paper Calls Out
None

## Limitations
- Relies on teacher LLM supervision which may introduce biases based on model knowledge and reasoning capabilities
- Experimental validation limited to single Overcooked game environment, generalizability to other domains uncertain
- Computational overhead of LLM-based communication may limit scalability to scenarios with many agents or real-time constraints

## Confidence
- High confidence: Technical implementation details and experimental methodology are clearly described and reproducible
- Medium confidence: Performance improvements over baselines demonstrated, but domain generalizability uncertain
- Medium confidence: Interpretability benefits shown qualitatively, but systematic analysis of communication patterns across scenarios limited

## Next Checks
1. Test Verco across multiple diverse multi-agent environments (e.g., StarCraft II, particle-world tasks) to assess domain generalization
2. Conduct ablation studies removing LLM supervision to quantify contribution of teacher-generated message labels versus alternative communication initialization methods
3. Evaluate communication efficiency by measuring message length, frequency, and computational overhead under varying bandwidth constraints