---
ver: rpa2
title: Nearest Neighbor Speculative Decoding for LLM Generation and Attribution
arxiv_id: '2405.19325'
source_url: https://arxiv.org/abs/2405.19325
tags:
- nest
- language
- retrieval
- should
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of hallucination and lack of attribution
  in large language models (LLMs) by introducing a semi-parametric approach called
  Nearest Neighbor Speculative Decoding (NEST). NEST combines token-level retrieval
  with a novel confidence-based interpolation method and relaxed speculative decoding
  to incorporate real-world text spans into LLM generations while providing direct
  attribution to their sources.
---

# Nearest Neighbor Speculative Decoding for LLM Generation and Attribution

## Quick Facts
- arXiv ID: 2405.19325
- Source URL: https://arxiv.org/abs/2405.19325
- Authors: Minghan Li; Xilun Chen; Ari Holtzman; Beidi Chen; Jimmy Lin; Wen-tau Yih; Xi Victoria Lin
- Reference count: 40
- Key outcome: NEST achieves 1.8x speedup in inference time while significantly enhancing generation quality and attribution rate across knowledge-intensive tasks.

## Executive Summary
This paper introduces Nearest Neighbor Speculative Decoding (NEST), a semi-parametric approach that addresses hallucination and attribution problems in large language models. NEST combines token-level retrieval with confidence-based interpolation and relaxed speculative decoding to incorporate real-world text spans into LLM generations while providing direct attribution. The method demonstrates significant improvements in both generation quality and attribution accuracy across various knowledge-intensive tasks.

## Method Summary
NEST implements a two-stage k-NN search with passage retrieval followed by token-level retrieval, uses a novel Relative Retrieval Confidence (RRC) score for dynamic interpolation between LM and k-NN distributions, and employs relaxed speculative decoding to accept longer spans from the corpus. The approach retrieves relevant passages using DRAGON+ and BM25, constructs token-level key-value pairs on-the-fly, computes mixture probabilities, and selects spans through dynamic span selection. This design achieves a balance between generation latency and attribution quality.

## Key Results
- Achieves 1.8x speedup in inference time when applied to Llama-2-Chat 70B
- Outperforms conventional kNN-LM and performs competitively with in-context retrieval augmentation (CRISS)
- Significantly enhances generation quality and attribution rate across knowledge-intensive tasks including question answering, text completion, and factuality-aware generation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The two-stage k-NN search improves retrieval efficiency by reducing the search space from the entire corpus to relevant passages before performing token-level retrieval.
- Mechanism: First, dense and sparse retrievers (DRAGON+ and BM25) retrieve the top-b relevant passages from the corpus. Then, token-level key-value pairs are constructed on-the-fly from these passages for k-NN search.
- Core assumption: Relevant passages contain the tokens needed for accurate generation, and limiting search to these passages maintains sufficient accuracy while reducing computational cost.
- Evidence anchors:
  - [section] "To provide a better trade-off between latency and accuracy, we adopt the two-stage design, which is widely applied in information retrieval and search engines."
  - [section] "First-stage passage retrieval Given the corpus D, we segment the documents into separate passages of less than m tokens each. We then encode the corpus and use a hybrid retrieval system to select the relevant passages..."
  - [section] "The two-stage design provides a trade-off between search latency and accuracy and the passage-level index only takes a fraction of the token-level index in Section 2.2."

### Mechanism 2
- Claim: The Relative Retrieval Confidence (RRC) score dynamically adjusts the interpolation between LM and k-NN distributions based on the uncertainty of the token retriever.
- Mechanism: RRC computes a confidence score from the similarity range of retrieved neighbors (min/max ratio) and uses this as the interpolation coefficient λt between pLM and pk-NN.
- Core assumption: The uncertainty in token retrieval (measured by similarity score variation) is a reliable indicator of when to trust the k-NN distribution versus the LM distribution.
- Evidence anchors:
  - [section] "We use a novel Relative Retrieval Confidence (RRC) score to measure the uncertainty of the token retriever and use it as the interpolation coefficient of the output probability mixture."
  - [section] "λt = σ( (mini |s(qt, ki)| / maxi |s(qt, ki)| - α) / τ )"
  - [section] "This enables flexible adaptation of the LM's output to different downstream tasks through dynamic interpolation with the token retrieval results."

### Mechanism 3
- Claim: Relaxed speculative decoding accepts longer spans from the corpus when they have high probability under the mixture distribution, reducing generation latency while maintaining quality.
- Mechanism: If a span is selected from the corpus, it undergoes evaluation using a relaxed acceptance probability based on the mixture distribution. If accepted, multiple tokens are generated in parallel.
- Core assumption: The mixture distribution pM provides a reliable probability estimate for determining whether a retrieved span should be accepted or rejected.
- Evidence anchors:
  - [section] "If a span of more than one token is selected, it undergoes evaluation based on the mixture probability. Through a rejection procedure similar to that in speculative decoding (Leviathan et al., 2023), only a prefix deemed highly likely by the mixture probability is accepted."
  - [section] "The probability of accepting the token w(i)t in a span is: P(accept token w(i)t) = min(1, pM(w = w(i)t | x, y<t, w(1)t, w(2)t, ..., w(i-1)t) / (γ · maxw pM(w | x, y<t, w(1)t, w(2)t, ..., w(i-1)t)))"
  - [section] "Combined with Figure 2a, we can reach the conclusion that fetching longer spans from the corpus results in lower generation latency per query."

## Foundational Learning

- Concept: Token-level retrieval and interpolation with k-NN language models
  - Why needed here: NEST builds upon kNN-LM but extends it with passage retrieval and dynamic confidence-based interpolation
  - Quick check question: What is the key difference between standard kNN-LM and NEST's approach to combining LM and k-NN distributions?

- Concept: Speculative decoding and rejection sampling
  - Why needed here: NEST uses a variant of speculative decoding to accept or reject proposed spans from the corpus based on their probability
  - Quick check question: How does NEST's relaxed speculative decoding differ from standard speculative decoding approaches?

- Concept: Two-stage retrieval systems (coarse-to-fine search)
  - Why needed here: The passage retrieval stage filters the search space before token-level retrieval, improving efficiency
  - Quick check question: Why does NEST use both dense (DRAGON+) and sparse (BM25) retrievers in the first stage?

## Architecture Onboarding

- Component map:
  - Input prompt → Passage retriever (DRAGON+ + BM25) → Top-b passages
  - Passages → Token encoder → Token-level key-value store (K', V')
  - Input + history → LM hidden state encoder → Query vector
  - Query vector → Token search in K', V' → Top-r neighbors
  - Neighbors + query → Mixture distribution (pLM, pk-NN, pM)
  - Mixture distribution → Span selection → Relaxed speculative decoding → Output

- Critical path:
  1. Passage retrieval (parallel dense and sparse search)
  2. Token encoding of passages and construction of K', V'
  3. LM encoding of current prefix
  4. Token search in K', V'
  5. Mixture probability computation
  6. Span selection and speculative decoding
  7. Output generation

- Design tradeoffs:
  - Number of passages (b) vs. search accuracy and latency
  - Number of tokens per passage vs. context relevance
  - Relaxation factor (γ) vs. acceptance rate and quality
  - Span length (n) vs. attribution granularity and coherence

- Failure signatures:
  - Low attribution ratio: Passage retrieval is not finding relevant passages
  - Poor generation quality: Mixture distribution is poorly calibrated or RRC is not working
  - High latency: Too many passages being retrieved or relaxed speculative decoding is rejecting too often
  - Non-fluent text: Dynamic span selection is choosing inappropriate spans

- First 3 experiments:
  1. Baseline comparison: Run NEST with all components enabled and compare against base LM and kNN-LM on WikiText-103 perplexity
  2. Ablation study: Disable RRC, dynamic span selection, and relaxed speculative decoding one at a time to measure their individual contributions
  3. Hyperparameter sensitivity: Vary the relaxation factor (γ) and measure the tradeoff between latency and FACTSCORE on Biography

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the interpolation coefficient λ_t in NEST behave differently across various domains (e.g., technical vs. creative writing) and what is the optimal range of α and τ for each domain?
- Basis in paper: [explicit] Section 3.2 describes the interpolation coefficient λ_t using the Relative Retrieval Confidence (RRC) score with hyper-parameters α and τ.
- Why unresolved: The paper tunes α and τ on Wikipedia-based tasks and Pile of Law, but does not explore how these parameters should be adjusted for different domains or types of content.
- What evidence would resolve it: Systematic experiments showing how different α and τ values affect performance across diverse domains, with analysis of the resulting λ_t behavior.

### Open Question 2
- Question: What is the trade-off between retrieval accuracy and generation fluency when using longer n-grams in the dynamic span selection process?
- Basis in paper: [explicit] Section 3.3 describes dynamic span selection with a fixed n-gram length of 64 tokens, and mentions that the final output depends on the interpolation coefficient λ_t.
- Why unresolved: The paper does not explore how varying the n-gram length affects the balance between maintaining attribution accuracy and ensuring fluent text generation.
- What evidence would resolve it: Controlled experiments varying n-gram lengths and measuring both attribution accuracy (e.g., ratio of tokens from corpus) and generation quality metrics (e.g., ROUGE scores, MAUVE scores).

### Open Question 3
- Question: How does NEST's performance change when applied to specialized domains with limited training data (e.g., medical or legal domains) compared to general knowledge domains?
- Basis in paper: [explicit] Section 4.2 mentions using Wikipedia and Pile of Law as knowledge sources, and Section 4.4 shows results on MedMCQA (medical domain).
- Why unresolved: While the paper evaluates on MedMCQA, it doesn't systematically compare performance across domains with varying data availability or explore strategies for adapting NEST to data-scarce domains.
- What evidence would resolve it: Comparative experiments across multiple specialized domains with varying amounts of training data, measuring both accuracy and attribution metrics.

## Limitations
- The evaluation relies heavily on factuality-aware metrics that may not fully capture semantic quality or practical utility in real-world applications.
- The computational overhead of maintaining dual indexes (passage-level and token-level) and performing two-stage retrieval may limit practical deployment.
- The RRC-based interpolation assumes that similarity score variation is a reliable indicator of retrieval quality, which may not hold uniformly across different types of knowledge.

## Confidence
- High confidence: The core architectural design of NEST is well-specified and the reported performance improvements on knowledge-intensive tasks are likely reproducible.
- Medium confidence: The attribution quality improvements are well-supported by the metrics, but the practical significance in real-world applications requires further validation.
- Low confidence: The claim that NEST outperforms in-context retrieval augmentation (CRISS) is based on a single comparison and may not generalize across all task types or model sizes.

## Next Checks
1. **Robustness across domains**: Validate NEST performance on specialized domains (scientific literature, legal documents, technical manuals) where retrieval accuracy and attribution quality are critical. Measure FACTSCORE and attribution accuracy while varying the knowledge source characteristics and query complexity.

2. **Latency characterization**: Conduct comprehensive latency profiling of NEST under different system loads and hardware configurations. Measure the impact of varying passage count (b), token limit per passage, and relaxation factor (γ) on both throughput and quality metrics to establish practical deployment boundaries.

3. **Failure mode analysis**: Systematically evaluate NEST under adversarial conditions including contradictory passages, noisy inputs, and ambiguous queries. Measure the degradation in generation quality, attribution accuracy, and RRC calibration to identify specific failure modes and their severity across different task types.