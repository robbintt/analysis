---
ver: rpa2
title: 'Cheetah: Natural Language Generation for 517 African Languages'
arxiv_id: '2401.01053'
source_url: https://arxiv.org/abs/2401.01053
tags:
- languages
- language
- african
- english
- chrf
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We introduce Cheetah, a massively multilingual natural language
  generation (NLG) language model for African languages. Cheetah supports 517 African
  languages and language varieties, addressing the scarcity of NLG resources for these
  languages.
---

# Cheetah: Natural Language Generation for 517 African Languages

## Quick Facts
- arXiv ID: 2401.01053
- Source URL: https://arxiv.org/abs/2401.01053
- Authors: Ife Adebara; AbdelRahim Elmadany; Muhammad Abdul-Mageed
- Reference count: 33
- Primary result: Cheetah achieves 1.5 BLEU score improvement over baselines across 7 NLG tasks for African languages

## Executive Summary
Cheetah is a massively multilingual natural language generation model supporting 517 African languages and language varieties. The model is pretrained on a 42GB multi-domain corpus using a denoising objective and achieves state-of-the-art performance across seven downstream NLG tasks. Human evaluations confirm superior faithfulness and fluency compared to baseline models. The work addresses the critical scarcity of NLG resources for African languages and establishes a new evaluation benchmark called AfroNLG.

## Method Summary
Cheetah uses an encoder-decoder architecture similar to T5 with 12 layers, 12 attention heads, and 768 hidden units. The model is pretrained on a 42GB multi-domain corpus covering 517 African languages using a denoising objective where 15% of tokens are masked and the model reconstructs the original sentence. For downstream tasks, Cheetah is fine-tuned on task-specific datasets for 20 epochs with early stopping. The AfroNLG benchmark evaluates performance across seven tasks: cloze tests, machine translation, paraphrase, question answering, summarization, and title generation using automatic metrics (BLEU, ROUGE, F1) and human evaluation on faithfulness and fluency.

## Key Results
- Cheetah outperforms baseline models in 5 of 7 downstream tasks
- Achieves an average improvement of 1.5 BLEU score points over baselines
- Human evaluation confirms superior faithfulness and fluency across Hausa, Swahili, and Yorùbá
- Supports the largest number of African languages (517) in a single NLG model

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pretraining on a massively multilingual corpus improves downstream task performance across many African languages.
- Mechanism: By exposing the model to broad linguistic diversity during pretraining, it captures cross-linguistic patterns that transfer to low-resource languages.
- Core assumption: Low-resource languages benefit from shared linguistic structures learned during pretraining.
- Evidence anchors:
  - [abstract] "Cheetah supports 517 African languages and language varieties, allowing us to address the scarcity of NLG resources and provide a solution to foster linguistic diversity."
  - [section] "We pretrain Cheetah using the encoder-decoder architecture... Each of the encoder and decoder components is similar in size and configuration to T5, with 12 layers each with 12 attention heads, and 768 hidden units for the base model."
  - [corpus] Weak - no explicit analysis of pretraining data overlap with downstream tasks provided.
- Break condition: If pretraining data lacks sufficient representation of a target language's unique features, downstream performance will suffer.

### Mechanism 2
- Claim: Finetuning on task-specific datasets improves model performance on downstream NLG tasks.
- Mechanism: Task-specific finetuning allows the model to adapt general linguistic knowledge to specific generation tasks like machine translation, summarization, etc.
- Core assumption: Task-specific finetuning data is representative of the target domain and task.
- Evidence anchors:
  - [abstract] "We demonstrate the effectiveness of Cheetah through comprehensive evaluations across seven generation downstream tasks."
  - [section] "For all models, we finetune on the training data split (Train) for 20 epochs with an early stopping of 5 epochs, learning-rate of 5e − 5, batch size of 16, and sequence length of 512."
  - [corpus] Weak - no explicit analysis of finetuning data quality or domain coverage provided.
- Break condition: If finetuning data is noisy, biased, or unrepresentative, model performance will degrade.

### Mechanism 3
- Claim: Human evaluation reveals model behaviors not captured by automatic metrics.
- Mechanism: Human evaluators assess faithfulness and fluency, capturing nuanced linguistic phenomena that automatic metrics like BLEU miss.
- Core assumption: Human evaluators are competent in the target languages and can reliably assess model outputs.
- Evidence anchors:
  - [abstract] "We additionally conduct a detailed human evaluation to delve deeper into the linguistic capabilities of Cheetah."
  - [section] "To evaluate the effectiveness of each model across different languages, we assess the generated output's faithfulness and fluency using a five-point Likert scale."
  - [corpus] Weak - no explicit discussion of annotator qualifications or inter-annotator agreement beyond Kappa scores.
- Break condition: If human evaluators lack language competence or evaluation criteria are unclear, results will be unreliable.

## Foundational Learning

- Concept: Transfer learning
  - Why needed here: Cheetah leverages knowledge from a massive multilingual pretraining corpus to improve performance on low-resource African languages.
  - Quick check question: How does transfer learning differ from training a model from scratch on a target language?

- Concept: Encoder-decoder architecture
  - Why needed here: Cheetah uses an encoder-decoder architecture to perform NLG tasks like machine translation and text summarization.
  - Quick check question: What are the key differences between encoder-decoder and decoder-only architectures?

- Concept: Tokenization and vocabulary
  - Why needed here: Cheetah uses SentencePiece to tokenize text into WordPiece tokens with a 250K vocabulary size.
  - Quick check question: How does subword tokenization help handle morphological complexity in African languages?

## Architecture Onboarding

- Component map: Pretraining corpus (42GB, 517 languages) -> Encoder-decoder model (T5-like, 12L/12H/768H) -> SentencePiece tokenization (250K WordPieces) -> Denoising pretraining objective -> Task-specific finetuning

- Critical path:
  1. Download and preprocess pretraining corpus
  2. Train Cheetah on pretraining corpus
  3. Prepare finetuning datasets for target tasks
  4. Finetune Cheetah on each task
  5. Evaluate model performance using automatic and human metrics

- Design tradeoffs:
  - Larger model size vs. training/inference efficiency
  - More diverse pretraining data vs. data quality and curation effort
  - Longer pretraining vs. risk of overfitting to pretraining data
  - More finetuning data vs. annotation cost and data availability

- Failure signatures:
  - Poor performance on specific languages: likely due to lack of representation in pretraining data
  - Inconsistent results across tasks: likely due to task-specific data quality or domain mismatch
  - High variance in automatic metrics: likely due to noisy or ambiguous reference translations

- First 3 experiments:
  1. Train Cheetah on a subset of the pretraining corpus and evaluate on a held-out dev set
  2. Finetune Cheetah on a single downstream task and compare to baseline models
  3. Conduct human evaluation of Cheetah outputs on a controlled test set to identify strengths and weaknesses

## Open Questions the Paper Calls Out

The paper identifies several key limitations and areas for future work:

1. Limited bias analysis due to restricted access to native speakers for comprehensive evaluation
2. Need for more extensive human evaluation across the full range of 517 supported languages
3. Potential for improving model performance through curriculum learning or adaptive pretraining strategies
4. Challenges in data collection and quality assessment for low-resource African languages

## Limitations

- Pretraining corpus composition and language distribution remain underspecified
- Benchmark covers only 67 test sets across 7 tasks, representing a small fraction of the 517 supported languages
- Human evaluation was limited to only three languages (Hausa, Swahili, Yorùbá)
- No analysis of potential biases in pretraining data or their impact on generated outputs

## Confidence

**High Confidence Claims:**
- Cheetah supports 517 African languages and language varieties
- Pretraining used a 42GB multi-domain corpus
- Model architecture details are well-specified

**Medium Confidence Claims:**
- Cheetah outperforms other models in 5 of 7 tasks
- Average 1.5 point BLEU improvement over baselines
- Human evaluation confirms superior faithfulness and fluency

**Low Confidence Claims:**
- Pretraining data adequately represents all 517 languages
- Results generalize to all 517 languages
- Cross-linguistic transfer benefits are substantial

## Next Checks

1. **Data Distribution Analysis**: Request detailed statistics on the 42GB pretraining corpus showing language-specific data volumes and domains to verify claims about comprehensive coverage of all 517 languages.

2. **Significance Testing**: Re-analyze the BLEU/ROUGE score differences between Cheetah and baselines with proper statistical significance tests (e.g., bootstrap resampling) to confirm the reported 1.5 point average improvement is not due to random variation.

3. **Cross-Lingual Transfer Study**: Design and conduct ablation experiments comparing Cheetah's performance on low-resource languages against models trained only on language-specific data to quantify actual transfer learning benefits versus pretraining on the target language alone.