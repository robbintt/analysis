---
ver: rpa2
title: Deep Sketched Output Kernel Regression for Structured Prediction
arxiv_id: '2406.09253'
source_url: https://arxiv.org/abs/2406.09253
tags:
- kernel
- learning
- output
- neural
- dsokr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work tackles the problem of learning neural networks for structured
  output prediction tasks, where the outputs lie in an infinite-dimensional feature
  space induced by kernel-induced losses. The proposed Deep Sketched Output Kernel
  Regression (DSOKR) method addresses this by designing a neural architecture whose
  last layer predicts in a data-dependent finite-dimensional subspace of the infinite-dimensional
  output feature space.
---

# Deep Sketched Output Kernel Regression for Structured Prediction

## Quick Facts
- arXiv ID: 2406.09253
- Source URL: https://arxiv.org/abs/2406.09253
- Authors: Tamim El Ahmad; Junjie Yang; Pierre Laforgue; Florence d'Alché-Buc
- Reference count: 28
- Key outcome: DSOKR achieves state-of-the-art performance on molecular identification tasks, outperforming existing approaches.

## Executive Summary
This paper introduces Deep Sketched Output Kernel Regression (DSOKR), a method for training neural networks on structured output prediction tasks where outputs lie in infinite-dimensional feature spaces induced by kernel-induced losses. The key innovation is reducing the infinite-dimensional output space to a finite, data-dependent subspace using random sketching of the output kernel covariance operator. This enables standard gradient-based training while preserving the expressiveness of kernel-induced losses.

DSOKR is evaluated on synthetic least squares regression, SMILES to Molecule prediction, and Text to Molecule identification tasks. The method demonstrates strong performance, particularly on molecular identification problems where it achieves state-of-the-art results. The approach effectively bridges the gap between the flexibility of kernel methods and the scalability of deep learning for structured prediction.

## Method Summary
DSOKR addresses structured output prediction by designing a neural architecture whose last layer predicts in a finite-dimensional subspace of the infinite-dimensional output feature space. This subspace is constructed as the span of eigenfunctions from a randomly-approximated version of the empirical kernel covariance operator. The method first computes a sketching matrix to approximate the output Gram matrix, then performs SVD to obtain the basis functions. The input neural network is trained to map inputs to coefficients in this finite basis using standard MSE loss. At inference, candidate outputs are scored by projecting them into the basis and selecting the highest-scoring candidate.

## Key Results
- DSOKR achieves state-of-the-art performance on molecular identification tasks (SMI2Mol and ChEBI-20 datasets)
- The method successfully trains on synthetic least squares regression with infinite-dimensional outputs
- Performance demonstrates the relevance of data-dependent basis selection over random basis approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DSOKR unlocks gradient-based training for structured prediction by reducing the infinite-dimensional output feature space to a finite data-dependent subspace.
- Mechanism: The method constructs a finite basis by computing the eigenfunctions of a sketched empirical covariance operator of the output kernel features. This basis spans a low-dimensional subspace of the original RKHS, allowing predictions to be made via linear combinations of these basis functions.
- Core assumption: The covariance operator has a fast eigendecay, so a small random sketch captures most of the relevant variance.
- Evidence anchors:
  - [abstract] "This subspace is chosen as the span of the eigenfunctions of a randomly-approximated version of the empirical kernel covariance operator."
  - [section 2.1] "KPCA provides the eigenbasis of the empirical covariance operator by computing the SVD of the output Gram matrix... it is often the case that the so-called capacity condition holds... it is then possible to efficiently approximate the eigenbasis of the empirical covariance operator using random projections techniques."
  - [corpus] Weak; no direct mention of sketching or covariance operator approximation.
- Break condition: If the output kernel covariance operator lacks rapid eigendecay, then the sketch dimension m required for good approximation becomes too large, negating computational benefits.

### Mechanism 2
- Claim: DSOKR's architecture allows standard deep learning optimizers to train on structured output tasks without custom loss engineering.
- Mechanism: The last layer is fixed to a finite basis derived from the sketched kernel covariance, while the preceding layers are trained normally. This transforms structured prediction into a standard regression problem in the finite basis space, solvable with MSE loss and SGD.
- Core assumption: The finite basis sufficiently approximates the infinite-dimensional feature space so that the induced loss remains meaningful.
- Evidence anchors:
  - [abstract] "This approach unlocks the use of gradient descent algorithms (and consequently of any neural architecture) for structured prediction."
  - [section 2.1] "Equipped with the basis ˜E, we can compute a novel expression of the loss L(θ)... learning the full network hθ boils down to learning the input neural network gW."
  - [corpus] Weak; no direct mention of gradient descent applicability to structured outputs.
- Break condition: If the basis approximation error is too large, the learned mapping may not correspond well to the true structured prediction task.

### Mechanism 3
- Claim: The pre-image problem at inference is made tractable by restricting candidate outputs to a known set and scoring them via the finite basis representation.
- Mechanism: For each test input, the model computes scores for all candidate outputs by projecting them into the finite basis and taking the inner product with the network output. This avoids costly optimization over the full output space.
- Core assumption: The candidate set is representative enough to contain the correct output, and the scoring function is discriminative.
- Evidence anchors:
  - [section 2.2] "we replace gW(x) by the 'perfect' coefficients... the prediction is given by fˆθ(xte i) = yc j where j = arg max 1≤j≤nc g ˆW (xte i)⊤ ˜ψ(yc j)."
  - [section 3.2] "the decoding is particularly suited to problems for which we have some knowledge of the possible outcomes, such as molecular identification problems."
  - [corpus] Weak; no direct mention of candidate-based decoding strategies.
- Break condition: If the candidate set is incomplete or the scoring function fails to discriminate, the method will yield incorrect predictions.

## Foundational Learning

- Concept: Kernel trick in output space
  - Why needed here: Enables defining structured output prediction tasks via kernel-induced losses without explicit feature maps.
  - Quick check question: How does the kernel trick allow computation of the squared loss without accessing infinite-dimensional features directly?

- Concept: Reproducing Kernel Hilbert Space (RKHS)
  - Why needed here: Provides the mathematical framework for the output feature space where predictions are made.
  - Quick check question: What property of RKHS allows the last layer to be a linear combination of basis functions?

- Concept: Random sketching / Nyström approximation
  - Why needed here: Reduces the computational burden of eigen-decomposition in high-dimensional output spaces.
  - Quick check question: Why does sketching preserve the essential spectral properties of the covariance operator?

## Architecture Onboarding

- Component map: Input neural network (gW) -> Fixed basis layer (gE) -> MSE loss -> Inference decoder (candidate scoring)

- Critical path:
  1. Compute output kernel matrix K from training outputs
  2. Apply sketching matrix R to obtain eK = R K R⊤
  3. Compute SVD of eK to obtain eigenfunctions (basis functions)
  4. Train input neural network gW to minimize finite-dimensional MSE
  5. At inference, score candidates and select best

- Design tradeoffs:
  - Larger sketching size m → better approximation but higher memory/computation
  - Choice of sketching method (subsample vs Gaussian) → statistical vs computational efficiency tradeoff
  - Basis dimension p vs model capacity → risk of underfitting if p too small

- Failure signatures:
  - High validation loss despite low training loss → overfitting in gW or poor basis approximation
  - Performance plateaus early → sketching size m too small to capture output space structure
  - Slow convergence → inappropriate learning rate or network architecture for the task

- First 3 experiments:
  1. Train DSOKR on a simple synthetic dataset where the output dimension is known; verify that performance matches a baseline NN when m is set appropriately.
  2. Vary sketching size m on a validation set; plot validation loss to identify optimal m.
  3. Replace the fixed basis with random Fourier features; compare performance to assess importance of data-dependent basis.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal sketching size m for DSOKR in practice, and how does it depend on the specific task and dataset characteristics?
- Basis in paper: [explicit] The paper discusses strategies for selecting the sketching size m, including Approximate Leverage Scores (ALS) and Perfect h. The experiments show that Perfect h can be a relevant strategy to fine-tune m, but the optimal value may vary depending on the dataset.
- Why unresolved: The optimal sketching size m is crucial for the performance of DSOKR. While the paper provides strategies for selecting m, the optimal value may depend on the specific task and dataset characteristics, such as the dimensionality of the output space and the amount of noise in the data.
- What evidence would resolve it: Further experiments on a wider range of datasets and tasks, with different output spaces and noise levels, could provide insights into the optimal sketching size m for different scenarios.

### Open Question 2
- Question: How does the choice of output kernel affect the performance of DSOKR, and are there any guidelines for selecting the most appropriate kernel for a given task?
- Basis in paper: [explicit] The paper mentions that the versatility of kernel-induced losses stems from the large variety of kernels that have been designed to compare structured objects. However, it does not provide guidelines for selecting the most appropriate kernel for a given task.
- Why unresolved: The choice of output kernel can significantly impact the performance of DSOKR. Different kernels may be more suitable for different types of structured outputs, such as graphs, sequences, or trees. Guidelines for selecting the most appropriate kernel for a given task could help improve the performance of DSOKR.
- What evidence would resolve it: Experiments comparing the performance of DSOKR with different output kernels on various tasks and datasets could provide insights into the impact of kernel choice on performance and help develop guidelines for kernel selection.

### Open Question 3
- Question: How does the performance of DSOKR compare to other state-of-the-art methods for structured prediction, such as energy-based models or direct risk minimization techniques?
- Basis in paper: [explicit] The paper mentions that energy-based approaches and direct risk minimization techniques are alternative approaches to structured prediction. However, it does not provide a comprehensive comparison of DSOKR with these methods.
- Why unresolved: Understanding how DSOKR compares to other state-of-the-art methods for structured prediction is important for assessing its strengths and limitations. A comprehensive comparison could help identify the scenarios where DSOKR is most advantageous and provide insights for further improvements.
- What evidence would resolve it: Experiments comparing the performance of DSOKR with other state-of-the-art methods for structured prediction on a wide range of tasks and datasets could provide a comprehensive assessment of its performance relative to alternative approaches.

## Limitations
- The method requires a finite candidate set for inference, which may not be available or may be too large in many structured prediction scenarios.
- Computational complexity analysis is incomplete, particularly for the full training pipeline including candidate set scoring.
- The approach assumes rapid eigendecay of the output kernel covariance operator, which may not hold for all structured prediction tasks.

## Confidence
- Mechanism 1 (sketching reduces infinite-dimensional space): Medium - The theoretical framework is sound but empirical validation of eigendecay assumptions is limited.
- Mechanism 2 (gradient-based training enabled): High - The transformation to finite-dimensional space is well-established and the training procedure is straightforward.
- Mechanism 3 (candidate-based inference): Medium - The approach is valid for molecular datasets but its generalizability to other structured prediction tasks is unclear.

## Next Checks
1. Systematically measure and report the eigendecay rates of output kernel covariance operators across different structured prediction datasets to validate the sketching approximation assumptions.
2. Evaluate the method's performance and runtime as the candidate set size varies across multiple orders of magnitude to understand practical limitations.
3. Conduct ablation studies varying the sketching dimension m on held-out validation sets for each task to determine the minimum m required for good performance and quantify the approximation error.