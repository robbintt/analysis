---
ver: rpa2
title: 'LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction'
arxiv_id: '2404.00913'
source_url: https://arxiv.org/abs/2404.00913
tags:
- llama-excitor
- arxiv
- excitor
- visual
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: LLaMA-Excitor introduces an indirect feature interaction method
  for fine-tuning large language models to improve instruction-following ability without
  compromising pre-trained knowledge. The approach modifies attention similarity matrices
  using learnable prompts while keeping hidden states within the original model distribution,
  unlike direct modification methods that can cause performance degradation.
---

# LLaMA-Excitor: General Instruction Tuning via Indirect Feature Interaction

## Quick Facts
- arXiv ID: 2404.00913
- Source URL: https://arxiv.org/abs/2404.00913
- Authors: Bo Zou, Chao Yang, Yu Qiao, Chengbin Quan, Youjian Zhao
- Reference count: 40
- Primary result: 3.12% relative improvement on MMLU while preserving pre-trained knowledge

## Executive Summary
LLaMA-Excitor introduces an indirect feature interaction method for fine-tuning large language models to improve instruction-following ability without compromising pre-trained knowledge. The approach modifies attention similarity matrices using learnable prompts while keeping hidden states within the original model distribution, unlike direct modification methods that can cause performance degradation. Evaluated on language-only instruction-following tasks, LLaMA-Excitor achieves a 3.12% relative improvement on the MMLU benchmark while maintaining baseline capabilities, whereas other parameter-efficient fine-tuning methods show performance drops. In multi-modal settings, it reaches state-of-the-art image captioning performance of 157.5 CIDEr on MSCOCO without complex visual-language alignment modules and achieves 88.39% accuracy on ScienceQA, comparable to larger models with extensive pretraining.

## Method Summary
LLaMA-Excitor employs a parameter-efficient fine-tuning approach that modifies attention similarity matrices through learnable prompts rather than directly altering hidden states. The method introduces a generator network that produces prompts which are then processed by a processor network to generate attention modifications. These modifications are applied indirectly to the attention mechanism, preserving the original hidden state distributions. The approach uses both local and global prompts - local prompts focus on individual token relationships while global prompts capture broader contextual information. The model is trained using a combination of cross-entropy loss for instruction-following tasks and auxiliary losses to maintain pre-trained knowledge stability.

## Key Results
- Achieves 3.12% relative improvement on MMLU benchmark compared to baseline LLaMA
- Maintains pre-trained knowledge while improving instruction-following capabilities
- State-of-the-art image captioning performance of 157.5 CIDEr on MSCOCO without complex visual-language alignment modules
- 88.39% accuracy on ScienceQA, comparable to larger models with extensive pretraining

## Why This Works (Mechanism)
The indirect feature interaction mechanism works by modifying attention similarity matrices rather than directly changing hidden states. This approach keeps the model's internal representations within the distribution learned during pre-training, preventing catastrophic forgetting. The learnable prompts act as soft constraints that guide the attention mechanism toward instruction-following behaviors while maintaining the underlying knowledge structure. The separation between local and global prompts allows the model to capture both fine-grained token relationships and broader contextual understanding simultaneously.

## Foundational Learning
- **Attention mechanisms**: Why needed - core to transformer architectures and language understanding. Quick check - verify understanding of scaled dot-product attention.
- **Parameter-efficient fine-tuning**: Why needed - enables adapting large models without full fine-tuning costs. Quick check - compare LoRA, prefix tuning, and adapter methods.
- **Cross-modal learning**: Why needed - enables language models to process and generate multi-modal outputs. Quick check - understand how text and image features are integrated.
- **Knowledge preservation**: Why needed - prevents catastrophic forgetting during adaptation. Quick check - recognize difference between direct and indirect parameter modification.
- **Multi-task learning**: Why needed - enables single model to handle diverse instruction types. Quick check - identify how different tasks share representations.

## Architecture Onboarding

Component map: Input text -> Tokenizer -> Embedding layer -> LLaMA backbone -> Attention modules with Excitor modification -> Output layer

Critical path: Input sequence flows through standard LLaMA layers with Excitor modules inserted at attention blocks. The Excitor modules modify attention similarity matrices before applying softmax.

Design tradeoffs: Indirect modification vs direct hidden state changes - indirect approach preserves pre-trained distributions but adds computational overhead for prompt generation.

Failure signatures: Performance degradation on pre-training tasks indicates over-modification; poor instruction-following indicates under-modification; training instability suggests prompt-generator imbalance.

First experiments:
1. Baseline LLaMA performance on MMLU to establish reference point
2. Excitor module insertion at single attention layer to test impact
3. Ablation study removing either local or global prompts to measure contribution

## Open Questions the Paper Calls Out
None

## Limitations
- Performance improvements demonstrated primarily on single benchmark (MMLU)
- Limited multi-modal task evaluation beyond image captioning and ScienceQA
- Ablation studies for indirect feature interaction mechanism are not comprehensive
- Direct controlled comparisons with other parameter-efficient methods lack identical experimental conditions

## Confidence
- High confidence in technical approach and methodology for indirect feature interaction
- Medium confidence in claimed performance improvements, particularly for multi-modal tasks
- Medium confidence in preservation of pre-trained knowledge claim due to limited evaluation scope

## Next Checks
1. Conduct ablation studies systematically removing individual components of the indirect feature interaction mechanism to quantify each component's contribution to performance
2. Test the method on additional instruction-following benchmarks beyond MMLU to verify generalizability of improvements
3. Compare directly with other parameter-efficient fine-tuning methods (LoRA, Prefix Tuning, etc.) using identical model sizes, training data, and compute budgets