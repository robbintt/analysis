---
ver: rpa2
title: Locally Convex Global Loss Network for Decision-Focused Learning
arxiv_id: '2403.01875'
source_url: https://arxiv.org/abs/2403.01875
tags:
- loss
- task
- surrogate
- problem
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes the Locally Convex Global Loss Network (LCGLN)
  for decision-focused learning (DFL), which integrates prediction and optimization
  to achieve better decisions. LCGLN addresses the challenge of computing gradients
  of optimal decisions with respect to parameters by using a partial input convex
  neural network (ICNN) as a global surrogate loss model.
---

# Locally Convex Global Loss Network for Decision-Focused Learning

## Quick Facts
- arXiv ID: 2403.01875
- Source URL: https://arxiv.org/abs/2403.01875
- Authors: Haeun Jeon; Hyunglip Bae; Minsu Park; Chanyeong Kim; Woo Chang Kim
- Reference count: 28
- Key outcome: LCGLN achieves 18.52% relative task loss on portfolio optimization with only 600 samples vs 18.90% for best benchmark with 10M samples

## Executive Summary
This paper proposes the Locally Convex Global Loss Network (LCGLN) for decision-focused learning, addressing the challenge of computing gradients through optimization problems. LCGLN uses a partial input convex neural network as a global surrogate loss model, guaranteeing convexity for sampled predictions while maintaining non-convexity for instance variables. This approach enables effective gradient flow for model updates without requiring specific parametric forms for different optimization problems.

## Method Summary
LCGLN integrates prediction and optimization by learning a surrogate loss model that maps predictions to task loss values. The method uses a partial input convex neural network architecture that is convex in the sampled prediction variables but non-convex in the instance variables. For each instance, predictions are sampled around the true value, and the surrogate loss model is trained to predict task loss from these samples. The predictive model is then updated using gradients from the surrogate loss. LCGLN offers two variants: ICNN-L (local model per instance) and ICNN-G (global model for all instances), with the global variant achieving significant sample efficiency improvements.

## Key Results
- LCGLN outperforms existing methods on three stochastic decision-making problems: inventory stock, knapsack, and portfolio optimization
- ICNN-G requires only 600 samples (less than 0.01% of LODL) while achieving better performance (18.52% vs 18.90% relative task loss) on portfolio optimization
- LCGLN demonstrates flexibility by handling general DFL problems without requiring problem-specific parametric forms

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ICLN's convexity guarantee for specific inputs ensures better gradient flow than non-convex surrogates
- Mechanism: The partial input convex neural network structure ensures convexity in sampled prediction variables while maintaining non-convex global structure for instance variables, providing informative gradients
- Core assumption: Task loss has a well-defined minimum at true prediction value that can be approximated by convex function
- Evidence anchors: [abstract], [section 4.1], [corpus] (weak)

### Mechanism 2
- Claim: Learning a global surrogate loss model reduces computational complexity compared to instance-specific local models
- Mechanism: ICNN-G learns a single global model mapping both instance identifier and prediction to task loss, reducing samples from K·N to KG
- Core assumption: Task loss function has regularity across instances capturable by single global model
- Evidence anchors: [abstract], [section 5.2], [corpus] (weak)

### Mechanism 3
- Claim: Partial ICNN architecture allows selective convexity enforcement while maintaining model expressiveness
- Mechanism: ICNN enforces convexity on prediction variables needing gradient information while keeping instance variables non-convex
- Core assumption: Prediction variables need convexity for gradient computation while instance variables can remain non-convex
- Evidence anchors: [section 4.1], [section 4.3], [corpus] (weak)

## Foundational Learning

- Concept: Convex optimization and convexity in neural networks
  - Why needed here: Core innovation relies on using input convex neural networks to guarantee convexity in specific dimensions
  - Quick check question: What is the difference between a fully input convex neural network and a partially input convex neural network?

- Concept: Decision-focused learning paradigm and task loss
  - Why needed here: Paper operates within DFL framework where goal is minimizing task loss rather than prediction loss
  - Quick check question: How does task loss differ from prediction loss in decision-focused learning?

- Concept: Gradient computation through optimization problems
  - Why needed here: Addresses challenge of computing gradients of optimal decisions with respect to parameters
  - Quick check question: Why is it challenging to compute gradients through discrete optimization problems?

## Architecture Onboarding

- Component map: Instance identifier → non-convex subnet → hidden layers → convex subnet → sampled predictions → output layer
- Critical path: Instance identifier → non-convex subnet → hidden layers → convex subnet → sampled predictions → output layer
- Design tradeoffs:
  - Convexity vs expressiveness: Enforcing convexity limits function class but guarantees gradient quality
  - Global vs local models: Global models reduce sample complexity but may lose instance-specific details
  - Partial vs full convexity: Partial convexity maintains flexibility while ensuring gradient quality for most important variables
- Failure signatures:
  - Poor gradient quality: Likely due to insufficient convexity enforcement or bad initialization
  - Overfitting to training instances: Global model may not generalize well across diverse instances
  - High computational cost: May indicate inefficient sampling strategy or model architecture
- First 3 experiments:
  1. Implement ICLN-L on simple knapsack problem and compare with PFL baseline
  2. Test ICLN-G on same problem to verify sample efficiency gains
  3. Apply ICLN to portfolio optimization problem to validate performance on high-dimensional data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does LCGLN performance compare on continuous vs discrete optimization problems?
- Basis in paper: [inferred] Paper mentions LCGLN handles general DFL problems but doesn't compare continuous vs discrete performance
- Why unresolved: No direct comparison of LCGLN performance across problem types
- What evidence would resolve it: Study comparing LCGLN on continuous (portfolio) and discrete (knapsack) problems with varying sizes

### Open Question 2
- Question: What is the impact of 'c' factor in sampling process on LCGLN performance?
- Basis in paper: [explicit] Mentions 'c' controls sample variance but doesn't discuss performance impact
- Why unresolved: No empirical evidence or analysis on sensitivity to 'c' parameter
- What evidence would resolve it: Experiments with different 'c' values and corresponding performance analysis

### Open Question 3
- Question: How does LCGLN time complexity scale with input data size and problem dimensionality?
- Basis in paper: [inferred] Mentions reduced time complexity vs benchmarks but lacks scaling analysis
- Why unresolved: No comprehensive study on LCGLN scalability to large-scale problems
- What evidence would resolve it: Analysis of time complexity as function of input size and dimensionality with theoretical and empirical validation

## Limitations
- Performance gains demonstrated primarily on synthetic stochastic optimization problems with limited real-world validation
- Assumes task loss functions can be effectively approximated by convex functions near true predictions
- Computational complexity analysis focuses on sample efficiency without full characterization of training time or scalability

## Confidence
- Convexity guarantees and sample efficiency improvements: High confidence
- Generalizability to real-world problems: Medium confidence
- Scalability to large-scale problems: Low confidence

## Next Checks
1. Ablation study on convexity: Test LCGLN performance when removing convexity guarantees to quantify actual contribution
2. Cross-domain validation: Apply LCGLN to at least two real-world optimization problems from different domains
3. Sample efficiency scaling: Systematically vary training samples to determine precise relationship between sample size and performance improvements