---
ver: rpa2
title: The evaluation of a code-switched Sepedi-English automatic speech recognition
  system
arxiv_id: '2403.07947'
source_url: https://arxiv.org/abs/2403.07947
tags:
- speech
- data
- training
- recognition
- corpus
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated an end-to-end CTC-based automatic speech recognition
  system for Sepedi-English code-switched speech. The model was trained on the Sepedi
  Prompted Code Switching corpus and tested on both the NCHLT Sepedi corpus and the
  training corpus.
---

# The evaluation of a code-switched Sepedi-English automatic speech recognition system

## Quick Facts
- arXiv ID: 2403.07947
- Source URL: https://arxiv.org/abs/2403.07947
- Reference count: 33
- Primary result: CTC-based ASR with 16 filters achieved lowest WER (41.9%) but struggled with monolingual Sepedi text

## Executive Summary
This study evaluated an end-to-end CTC-based automatic speech recognition system for Sepedi-English code-switched speech. The model was trained on the Sepedi Prompted Code Switching corpus and tested on both the NCHLT Sepedi corpus and the training corpus. Experimental results showed that using 16 filters in the convolutional layers yielded the lowest validation loss (21.96%) and Word Error Rate (41.9%). However, the system struggled with recognizing Sepedi-only text, achieving WERs of 50.05% on the training corpus and 84.59% on the NCHLT corpus. These findings highlight the importance of carefully tuning the number of filters and the challenges of code-switching in under-resourced languages like Sepedi.

## Method Summary
The study evaluated a CTC-based ASR system for Sepedi-English code-switched speech using the Sepedi Prompted Code Switching corpus. The model architecture consisted of CNN layers with 16, 32, and 64 filters, followed by 3 GRU layers with 256 units each. The system was trained for 150 epochs using CTC loss and greedy decoding. Performance was evaluated using Word Error Rate on both the training corpus and the NCHLT Sepedi corpus test sets.

## Key Results
- 16 filters in CNN layers produced the lowest validation loss (21.96%) and WER (41.9%)
- System achieved WER of 50.05% on Sepedi-only text from training corpus
- System achieved WER of 84.59% on Sepedi-only text from NCHLT corpus

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Varying the number of convolutional filters directly affects the model's ability to capture relevant acoustic features in code-switched speech.
- Mechanism: More filters increase the model's capacity to learn complex patterns but also raise the risk of overfitting, especially with limited data.
- Core assumption: The optimal filter count balances feature extraction with generalization to unseen data.
- Evidence anchors:
  - [abstract] "The model produced the lowest WER of 41.9%, however, the model faced challenges in recognizing the Sepedi only text."
  - [section] "The number of filters is a hyperparameter that influences the model's capacity to learn and generalize from acoustic features in audio signals."
- Break condition: If validation loss spikes or WER increases with more filters, overfitting is likely occurring.

### Mechanism 2
- Claim: Code-switching introduces linguistic complexity that degrades ASR performance, particularly for low-resource languages like Sepedi.
- Mechanism: Mixing languages in speech creates unpredictable patterns that challenge models trained on monolingual or limited bilingual data.
- Core assumption: The model's training data must reflect the diversity of code-switching patterns present in the target domain.
- Evidence anchors:
  - [abstract] "These findings highlight the importance of carefully tuning the number of filters and the challenges of code-switching in under-resourced languages like Sepedi."
  - [section] "In real-world scenarios, particularly in multilingual societies characterized by the prevalent practice of code-switching."
- Break condition: If WER remains high across all filter settings, the issue is likely data scarcity or mismatch rather than model capacity.

### Mechanism 3
- Claim: CTC-based models perform better when the number of filters is tuned to the dataset size and complexity.
- Mechanism: CTC's sequence-to-sequence mapping benefits from a balanced filter count that avoids both underfitting and overfitting.
- Core assumption: The relationship between filter count and performance is monotonic up to an optimal point, after which performance degrades.
- Evidence anchors:
  - [section] "The results demonstrate that the number of filters in a convolutional layer affects the capacity of the network to capture different features in the input data."
  - [corpus] No direct evidence in corpus about filter count; this is inferred from experimental results.
- Break condition: If performance plateaus or worsens with additional filters, the optimal filter count has been exceeded.

## Foundational Learning

- Concept: Connectionist Temporal Classification (CTC)
  - Why needed here: CTC enables sequence-to-sequence mapping without requiring frame-level alignments, critical for end-to-end ASR.
  - Quick check question: How does CTC handle variable-length input and output sequences in speech recognition?

- Concept: Convolutional Neural Networks (CNNs) in ASR
  - Why needed here: CNNs extract local acoustic features from spectrograms, forming the input to RNN layers in the model.
  - Quick check question: What role do convolutional layers play in preprocessing speech signals for RNN-based ASR?

- Concept: Word Error Rate (WER)
  - Why needed here: WER quantifies transcription accuracy by measuring substitutions, deletions, and insertions relative to reference text.
  - Quick check question: How is WER calculated, and why is it a standard metric for evaluating ASR systems?

## Architecture Onboarding

- Component map:
  Input spectrograms -> CNN layers (16/32/64 filters) -> GRU layers (256 units) -> Character predictions -> CTC loss

- Critical path:
  1. Preprocess audio to spectrograms
  2. Pass through CNN layers for feature extraction
  3. Process features with RNN layers for sequence modeling
  4. Generate character predictions
  5. Compute CTC loss and update weights
  6. Evaluate WER on validation/test sets

- Design tradeoffs:
  - Filter count vs. model complexity: More filters increase capacity but risk overfitting.
  - Dataset size vs. model performance: Limited data amplifies the need for careful hyperparameter tuning.
  - Language coverage vs. accuracy: Code-switching adds complexity, reducing accuracy for monolingual segments.

- Failure signatures:
  - High WER on both training and validation sets: Underfitting or insufficient model capacity.
  - Low training WER but high validation WER: Overfitting, likely due to excessive filters or limited data.
  - Spikes in validation loss during training: Model instability or poor generalization.

- First 3 experiments:
  1. Train with 16 filters and monitor validation loss/WER to establish baseline performance.
  2. Increase to 32 filters and compare validation loss/WER to detect overfitting.
  3. Increase to 64 filters and evaluate if performance degrades, confirming optimal filter count.

## Open Questions the Paper Calls Out
None

## Limitations
- Limited dataset size and domain specificity restrict generalizability of findings
- Significant performance degradation on monolingual Sepedi text indicates challenges with language boundaries
- Narrow focus on filter count without exploring other architectural hyperparameters

## Confidence
- Confidence in core findings: Medium
  - Experimental design is methodologically sound with systematic hyperparameter variation
  - Results are reproducible given access to the specific corpora
  - However, small dataset and lack of cross-validation introduce uncertainty about generalizability

## Next Checks
1. **Cross-corpus validation**: Test the trained model on additional code-switched datasets from different language pairs to verify if the filter count optimization generalizes beyond the Sepedi-English context.

2. **Language boundary detection**: Implement and evaluate explicit language identification modules to determine if separating code-switched segments before ASR processing improves monolingual language recognition accuracy.

3. **Architectural ablation study**: Systematically vary learning rate, batch size, and layer configurations alongside filter count to identify whether the observed performance patterns are robust to broader architectural changes.