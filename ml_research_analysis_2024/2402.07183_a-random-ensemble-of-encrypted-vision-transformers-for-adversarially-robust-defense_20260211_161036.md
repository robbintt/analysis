---
ver: rpa2
title: A Random Ensemble of Encrypted Vision Transformers for Adversarially Robust
  Defense
arxiv_id: '2402.07183'
source_url: https://arxiv.org/abs/2402.07183
tags:
- adversarial
- ensemble
- attacks
- encrypted
- against
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes a method to improve adversarial robustness in
  deep learning models, specifically vision transformers (ViTs), by using a random
  ensemble of encrypted models. The method addresses the vulnerability of ViTs to
  adversarial examples (AEs), which are inputs designed to fool the model.
---

# A Random Ensemble of Encrypted Vision Transformers for Adversarially Robust Defense

## Quick Facts
- arXiv ID: 2402.07183
- Source URL: https://arxiv.org/abs/2402.07183
- Reference count: 40
- Primary result: Proposed method achieves high clean accuracy and robustness against various attacks including AutoAttack on CIFAR-10 and ImageNet datasets.

## Executive Summary
This paper introduces a novel defense against adversarial attacks by combining encrypted vision transformers with a random ensemble method. The approach addresses the vulnerability of vision transformers (ViTs) to adversarial examples by training multiple sub-models on block-wise encrypted images and randomly selecting outputs during inference. Experiments demonstrate that this method achieves strong performance on clean images while maintaining robustness against both white-box and black-box attacks, outperforming state-of-the-art defenses in terms of clean and robust accuracy.

## Method Summary
The proposed method employs block-wise encryption using pixel shuffling to create multiple encrypted sub-models from a pre-trained ViT. Each sub-model is trained on images encrypted with a unique key. During inference, test images are encrypted with all keys and passed through corresponding sub-models. A random selection mechanism then chooses S outputs from N sub-models to generate the final prediction. This approach leverages the patch embedding structure of ViTs and introduces randomness to disrupt adversarial attack strategies.

## Key Results
- High clean accuracy on CIFAR-10 and ImageNet datasets
- Robust accuracy against various attack methods including AutoAttack
- Outperforms state-of-the-art defenses in both clean and robust accuracy metrics
- Demonstrated effectiveness against both white-box and black-box attacks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Random ensemble of encrypted sub-models provides robustness against both white-box and black-box attacks.
- Mechanism: By using multiple sub-models trained on images encrypted with different keys, and randomly selecting outputs from these sub-models, the system prevents attackers from reliably estimating gradients needed for white-box attacks and reduces transferability of adversarial examples across sub-models, making black-box attacks harder.
- Core assumption: Each encrypted sub-model is individually robust to white-box attacks when its encryption key is unknown, and the randomness in output selection disrupts gradient-based optimization for black-box attacks.
- Evidence anchors:
  - [abstract] "The proposed method was demonstrated to be robust against not only white-box attacks but also black-box ones in an image classification task..."
  - [section] "In contrast, key-based attacks are not robust against black-box attacks such as Square Attack [22]."
  - [corpus] Weak - no direct citations in corpus supporting this specific mechanism.
- Break condition: If the attacker can recover or bypass the encryption keys, or if the randomness in ensemble selection can be modeled or predicted, the defense degrades to individual encrypted model robustness.

### Mechanism 2
- Claim: Block-wise encryption with pixel shuffling disrupts the gradient flow used in adversarial attack generation.
- Mechanism: The encryption process splits images into non-overlapping blocks, flattens and permutes pixel values within each block using a secret key, then reconstructs the encrypted image. This transformation alters the spatial structure in a way that is preserved under the vision transformer's patch embedding, but makes gradient-based perturbations less effective when the key is unknown.
- Core assumption: The vision transformer's patch embedding treats encrypted blocks similarly to plain patches, preserving model accuracy while breaking gradient-based attacks.
- Evidence anchors:
  - [section] "The properties are obtained from the patch embedding structure of ViT. Accordingly, it is expected that ViT models encrypted with a block-wise encryption method are robust against various attacks under some requirements."
  - [section] "Property (d) means that it is easy to prepare encrypted models with high performance by using a block-wise encryption method."
  - [corpus] Weak - corpus does not discuss block-wise encryption effects on gradients.
- Break condition: If the attacker has access to the encryption key or can approximate the permutation, the defense reverts to standard model vulnerability.

### Mechanism 3
- Claim: Increasing the number of sub-models in the random ensemble improves robustness against black-box attacks.
- Mechanism: More sub-models increase the difficulty for black-box attacks like Square Attack that rely on repeated queries and averaging predictions, because the random selection of S outputs from N sub-models introduces unpredictability and variance in the model's response.
- Core assumption: Black-box attacks that repeatedly query the model are disrupted by the stochastic output selection, which prevents the attacker from converging on effective perturbations.
- Evidence anchors:
  - [section] "From these results, increasing the number of sub-models can improve the robustness against black-box attacks under only the use of a random ensemble of sub-models..."
  - [section] "By using a random selection of S outputs, the random ensemble model provides a different estimation value every time, even if the same image is input to the model."
  - [corpus] Weak - no direct citations in corpus supporting this specific claim about ensemble size.
- Break condition: If the attacker can query enough times to average out the randomness, or if the ensemble selection becomes predictable, the defense effectiveness decreases.

## Foundational Learning

- Concept: Vision Transformers (ViT) and patch embedding
  - Why needed here: The method relies on ViT's ability to process encrypted images without significant accuracy loss due to its patch embedding structure.
  - Quick check question: How does ViT's patch embedding structure allow it to handle block-wise encrypted images effectively?

- Concept: Adversarial examples and transferability
  - Why needed here: Understanding why encrypted models are robust to white-box attacks but vulnerable to black-box attacks is key to designing the random ensemble defense.
  - Quick check question: What is adversarial transferability and why does it pose a challenge for defenses relying on model encryption?

- Concept: Random ensemble methods and their role in robustness
  - Why needed here: The core novelty is combining multiple encrypted sub-models with random output selection to disrupt both white-box and black-box attacks.
  - Quick check question: How does random selection of outputs from multiple sub-models improve robustness against black-box attacks like Square Attack?

## Architecture Onboarding

- Component map:
  Pre-trained ViT model (patch size MxM, e.g., 16x16) -> Encryption module using pixel shuffling with block size M -> N sub-models, each fine-tuned on images encrypted with a unique key -> Random ensemble selector choosing S outputs from N sub-models -> Evaluation module using AutoAttack benchmark

- Critical path:
  1. Encrypt training images with N different keys to generate N encrypted datasets
  2. Fine-tune pre-trained ViT on each encrypted dataset to create N sub-models
  3. For inference, encrypt test image with N keys, pass each through corresponding sub-model
  4. Randomly select S outputs and average predictions for final result

- Design tradeoffs:
  - More sub-models (N) increases robustness but also computational cost
  - Larger S (number of outputs selected) increases stability but reduces randomness
  - Encryption key management is critical; leakage compromises defense
  - Randomness in ensemble selection trades off determinism for robustness

- Failure signatures:
  - High accuracy on clean images but low robust accuracy against AutoAttack
  - Degraded performance if encryption keys are leaked or guessed
  - Ineffective if S is too small or N is too small relative to attack strength
  - Potential overfitting if sub-models are trained too long on encrypted data

- First 3 experiments:
  1. Verify that a single encrypted ViT is robust to white-box attacks but vulnerable to black-box attacks
  2. Test random ensemble with N=4, S=3 on CIFAR-10 to confirm improved black-box robustness
  3. Measure impact of increasing N (e.g., N=5) on robust accuracy against Square Attack and AutoAttack

## Open Questions the Paper Calls Out
None

## Limitations
- The defense relies heavily on encryption key secrecy, with potential vulnerability if keys are compromised
- Computational overhead scales linearly with the number of sub-models, potentially limiting practical deployment
- Lacks ablation studies to isolate contributions of encryption versus random ensemble to overall robustness
- Effectiveness against adaptive attacks targeting the random ensemble mechanism remains untested

## Confidence

- **High confidence**: The mechanism of using multiple encrypted models to improve robustness against black-box attacks is theoretically sound and supported by the experimental results showing improved performance against Square Attack.
- **Medium confidence**: The claim that random ensemble selection disrupts gradient-based optimization for white-box attacks is plausible but not thoroughly validated through targeted experiments.
- **Low confidence**: The assertion that increasing N always improves robustness against black-box attacks needs more rigorous testing across different attack scenarios and ensemble configurations.

## Next Checks

1. Conduct adaptive white-box attack experiments specifically designed to exploit the random ensemble mechanism (e.g., by modeling the selection process or attacking multiple sub-models simultaneously)
2. Perform ablation studies comparing: (a) encrypted single model, (b) random ensemble of plain models, and (c) encrypted random ensemble to isolate the contribution of each component
3. Test the defense's robustness to encryption key recovery attacks, including statistical attacks that might infer permutation patterns from multiple queries