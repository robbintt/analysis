---
ver: rpa2
title: 'Evaluating Students'' Open-ended Written Responses with LLMs: Using the RAG
  Framework for GPT-3.5, GPT-4, Claude-3, and Mistral-Large'
arxiv_id: '2405.05444'
source_url: https://arxiv.org/abs/2405.05444
tags:
- llms
- temperature
- evaluation
- grade
- grading
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluated the performance of GPT-3.5, GPT-4, Claude-3,
  and Mistral-Large in grading 54 open-ended student responses, each evaluated 10
  times under temperature settings 0.0 and 0.5. Using a RAG framework and reference
  material, the models showed significant variation in grading consistency and accuracy.
---

# Evaluating Students' Open-ended Written Responses with LLMs: Using the RAG Framework for GPT-3.5, GPT-4, Claude-3, and Mistral-Large

## Quick Facts
- arXiv ID: 2405.05444
- Source URL: https://arxiv.org/abs/2405.05444
- Reference count: 2
- GPT-3.5 showed 39.88% inaccurate grades and lowest consistency; Claude-3 and GPT-4 performed better; Mistral-Large had highest consistency at temperature 0.0 (83.33%)

## Executive Summary
This study evaluated the performance of GPT-3.5, GPT-4, Claude-3, and Mistral-Large in grading 54 open-ended student responses, each evaluated 10 times under temperature settings 0.0 and 0.5. Using a RAG framework and reference material, the models showed significant variation in grading consistency and accuracy. GPT-3.5 was fastest but least reliable, with 39.88% inaccurate grades and low consistency. Claude-3 and GPT-4 demonstrated better accuracy and consistency, while Mistral-Large showed the highest consistency at temperature 0.0 (83.33%). Processing times varied up to five-fold, with GPT-3.5 fastest (3.5–3.9 s) and Claude-3 slowest (20.6–21.5 s). Temperature setting 0.0 yielded more consistent results. Results highlight the importance of model selection and benchmarking in automated grading.

## Method Summary
The study evaluated four large language models (GPT-3.5, GPT-4, Claude-3, and Mistral-Large) using a RAG framework with reference materials to grade 54 open-ended student responses. Each response was graded 10 times at temperature settings 0.0 and 0.5. The RAG framework retrieved relevant information from provided materials to support grading decisions. Processing times and accuracy were measured across all models, with GPT-3.5 showing fastest processing but lowest accuracy and consistency.

## Key Results
- GPT-3.5 had 39.88% inaccurate grades and lowest consistency across all models
- Claude-3 and GPT-4 demonstrated superior accuracy and consistency compared to GPT-3.5
- Mistral-Large achieved highest consistency (83.33%) at temperature 0.0
- Processing times varied up to five-fold, with GPT-3.5 fastest (3.5–3.9 s) and Claude-3 slowest (20.6–21.5 s)
- Temperature setting 0.0 produced more consistent grading results than 0.5

## Why This Works (Mechanism)
The RAG framework enables accurate grading by combining external knowledge retrieval with language model generation. When grading open-ended responses, the retriever component fetches relevant reference materials and grading criteria, which the language model then uses to generate context-specific evaluations. This approach addresses the knowledge limitations of standalone LLMs by grounding their responses in domain-specific information. The framework's effectiveness depends on the quality of retrieved materials and the model's ability to synthesize this information with the student response. Temperature settings modulate the randomness of outputs, with lower temperatures (0.0) producing more deterministic and consistent grading decisions across multiple evaluations of the same response.

## Foundational Learning
- RAG framework: Retrieval-augmented generation combines external knowledge retrieval with language model generation to improve accuracy. Needed for providing context-specific grading criteria and reference materials.
- Temperature settings: Control randomness in model outputs (0.0 = deterministic, 0.5 = more varied). Critical for understanding consistency in grading.
- Grading consistency metrics: Statistical measures of agreement between multiple evaluations of the same response. Essential for validating automated grading reliability.
- Processing time benchmarking: Measuring computational efficiency across different model architectures and implementations. Important for practical deployment considerations.

## Architecture Onboarding
Component map: Student response -> RAG retriever -> Language model -> Grading output
Critical path: Retrieval of reference materials → Model inference → Consistency evaluation
Design tradeoffs: Speed vs accuracy (GPT-3.5 fast but inaccurate) vs consistency (Mistral-Large most consistent)
Failure signatures: Inconsistent grades across temperature settings, high inaccuracy rates, processing timeouts
First experiments: 1) Test single-response grading with varying temperature settings, 2) Compare retrieval quality with different reference materials, 3) Measure consistency across model versions

## Open Questions the Paper Calls Out
- How do different RAG retriever implementations affect grading accuracy and consistency?
- What is the optimal balance between retrieval quality and processing speed for automated grading?
- How do temperature settings interact with different model architectures for grading tasks?
- Can the observed performance differences be generalized across different subject domains?

## Limitations
- Small sample size (54 responses) limits generalizability across educational contexts
- Focus on chemistry restricts applicability to other subjects
- Does not address potential biases in reference materials or RAG framework impact
- Limited exploration of different retriever architectures and their effects on grading quality

## Confidence
- Model performance rankings: Medium confidence - reliable relative comparisons but specific percentages may not generalize
- Temperature effect findings: Medium confidence - reasonable but could vary with different datasets
- Processing time differences: Low confidence - could vary significantly with computational conditions

## Next Checks
1. Replicate with larger, multi-subject dataset (minimum 500 responses across STEM and humanities)
2. Test RAG framework impact by comparing results with and without retrieval augmentation
3. Conduct cross-linguistic validation for non-English responses and different educational systems