---
ver: rpa2
title: 'Comparing the Efficacy of GPT-4 and Chat-GPT in Mental Health Care: A Blind
  Assessment of Large Language Models for Psychological Support'
arxiv_id: '2405.09300'
source_url: https://arxiv.org/abs/2405.09300
tags: []
core_contribution: A blind clinical psychologist evaluation compared GPT-4 and Chat-GPT
  on 18 psychological prompts. GPT-4 achieved an average score of 8.29 out of 10,
  while Chat-GPT scored 6.52, with GPT-4 consistently outperforming Chat-GPT across
  all mental health topics.
---

# Comparing the Efficacy of GPT-4 and Chat-GPT in Mental Health Care: A Blind Assessment of Large Language Models for Psychological Support

## Quick Facts
- **arXiv ID**: 2405.09300
- **Source URL**: https://arxiv.org/abs/2405.09300
- **Reference count**: 5
- **Primary result**: GPT-4 achieved 8.29/10 average score vs Chat-GPT's 6.52/10 in blind psychologist evaluation of mental health responses

## Executive Summary
This blind assessment compared GPT-4 and Chat-GPT across 18 psychological prompts evaluated by a clinical psychologist. GPT-4 consistently outperformed Chat-GPT, achieving an average score of 8.29 out of 10 compared to 6.52 for Chat-GPT. Both models demonstrated overconfidence relative to human ratings, with Chat-GPT showing higher overconfidence levels. The study reveals GPT-4's superior effectiveness in generating clinically relevant, empathetic responses for mental health applications.

## Method Summary
A single clinical psychologist conducted blind evaluations of GPT-4 and Chat-GPT responses to 18 psychological prompts across various mental health topics. Each model's responses were rated on a 10-point scale for clinical relevance and empathy. The evaluator was blinded to which model generated each response. The assessment focused on response quality metrics without testing real-world clinical outcomes or patient safety impacts.

## Key Results
- GPT-4 achieved average score of 8.29/10 vs Chat-GPT's 6.52/10 across all mental health topics
- GPT-4 consistently outperformed Chat-GPT in every evaluated mental health scenario
- Both models exhibited overconfidence, with Chat-GPT showing higher overconfidence relative to human ratings

## Why This Works (Mechanism)
None

## Foundational Learning
Why needed: Understanding core concepts helps interpret AI performance in clinical contexts
Quick check: Can you explain the difference between confidence ratings and actual clinical effectiveness?

- **Blind evaluation methodology**: Prevents evaluator bias when comparing AI model outputs
- **Clinical relevance scoring**: Measures how applicable AI responses are to real therapeutic contexts
- **Overconfidence metrics**: Identifies when AI systems rate their responses more favorably than human experts
- **Psychological prompt design**: Ensures evaluation covers diverse mental health scenarios
- **Single-blind vs double-blind protocols**: Different approaches to controlling evaluator bias
- **Response quality metrics**: Standardized measures for assessing AI-generated therapeutic content

## Architecture Onboarding
Component map: Clinical Psychologist -> Evaluation Rubric -> Model Responses -> Score Aggregation
Critical path: Prompt generation → Model response → Blind evaluation → Score calculation → Comparative analysis
Design tradeoffs: Single evaluator provides consistency but limits generalizability; 18 prompts balance depth with breadth
Failure signatures: Overconfidence in AI responses, potential evaluator bias, limited scenario coverage
First experiments: 1) Multi-evaluator validation, 2) Real patient outcome testing, 3) Expanded prompt library testing

## Open Questions the Paper Calls Out
None

## Limitations
- Single-blind evaluator design limits generalizability and introduces potential bias
- Small sample size of 18 prompts may not capture full spectrum of mental health scenarios
- No assessment of real-world clinical outcomes or patient safety impacts
- Confidence ratings showed both models overconfident, but clinical implications not fully explored

## Confidence
- **High confidence**: GPT-4 demonstrated superior performance metrics compared to Chat-GPT across all evaluated mental health topics
- **Medium confidence**: The superiority of GPT-4 translates to meaningful clinical utility (requires real-world validation)
- **Low confidence**: The observed overconfidence has significant clinical safety implications (needs larger-scale patient outcome studies)

## Next Checks
1. Conduct multi-evaluator blind assessments with clinical psychologists to verify consistency and reduce individual bias
2. Test model responses with actual patient populations to measure real-world effectiveness and safety outcomes
3. Expand prompt library to include diverse mental health scenarios, crisis situations, and edge cases to ensure comprehensive evaluation