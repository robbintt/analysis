---
ver: rpa2
title: 'Probing Ranking LLMs: A Mechanistic Analysis for Information Retrieval'
arxiv_id: '2410.18527'
source_url: https://arxiv.org/abs/2410.18527
tags:
- features
- probing
- llms
- ranking
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper employs probing techniques to investigate whether statistical
  IR features are encoded in the activations of fine-tuned ranking LLMs. The authors
  probe four different LLM architectures (Llama2-7b, Llama2-13b, Llama3.1-8b, and
  Pythia-6.9b) fine-tuned for passage reranking using the MS MARCO dataset.
---

# Probing Ranking LLMs: A Mechanistic Analysis for Information Retrieval

## Quick Facts
- arXiv ID: 2410.18527
- Source URL: https://arxiv.org/abs/2410.18527
- Authors: Tanya Chowdhury; Atharva Nijasure; James Allan
- Reference count: 40
- Primary result: Probing fine-tuned ranking LLMs reveals that certain statistical IR features are prominently encoded in MLP layer activations, while others show no representation

## Executive Summary
This paper investigates whether statistical information retrieval features are encoded in the activations of fine-tuned ranking language models. The authors employ sparse probing techniques to examine four different LLM architectures (Llama2-7b, Llama2-13b, Llama3.1-8b, and Pythia-6.9b) that were fine-tuned for passage reranking using the MS MARCO dataset. By capturing MLP layer activations and applying Lasso regression probes, they identify which human-engineered IR features are represented in the models' internal representations, revealing both consistent patterns across architectures and potential overfitting behaviors.

## Method Summary
The study fine-tunes four base LLMs for passage reranking using MS MARCO with LoRA rank 32 for one epoch. The authors create balanced probing datasets with 500 queries from MS MARCO dev set and top 100 documents per query, computing 19 MSLR features and 7 similarity metrics. They capture MLP activations across all layers using PyTorch hooks, aggregate by mean, and apply Lasso regression probes with 5-fold cross-validation to measure R² scores indicating feature presence in the activation space.

## Key Results
- Covered query term number, covered query term ratio, mean of stream length normalized term frequency, and variance of tf*idf are prominently encoded in LLM activations
- Sum of stream length normalized term frequency, max of stream length normalized term frequency, and BM25 show no discernible representation in MLP layers
- Different LLM architectures encode similar statistical IR features when fine-tuned using the same dataset
- Activation patterns remain consistent for in-distribution queries but not for certain features in RankLlama2-13b when encountering out-of-distribution data, suggesting potential overfitting

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM MLP layers act as key-value memories that store interpretable statistical IR features
- Mechanism: The MLP layers perform pointwise nonlinear transformations on token representations, creating internal activations that can be linearly mapped back to human-engineered features via sparse probing with Lasso regularization
- Core assumption: The relevant features are linearly decodable from the activation space and sparsity constraints can isolate the contributing neurons
- Evidence anchors:
  - [abstract] "Our analysis concentrates on extracting activations from the MLP unit of each transformer block, which is posited to contain the key feature extractors [23]."
  - [section] "This MLP layer applies pointwise nonlinearities to each token independently and therefore performs the majority of feature extraction for the LLM."
  - [corpus] Weak - neighboring papers focus on SAEs and interpretability but not specifically on MLP key-value memory claims
- Break condition: If feature representations are highly nonlinear or distributed across many neurons, linear probes will fail to capture them accurately

### Mechanism 2
- Claim: Fine-tuning on MS MARCO creates consistent feature representations across different LLM architectures
- Mechanism: The LoRA fine-tuning process adapts the same statistical IR patterns to different base architectures, resulting in similar MLP activation patterns for the same features despite architectural differences
- Core assumption: The ranking objective and training data dominate the learned feature representations, making them architecture-independent
- Evidence anchors:
  - [abstract] "Furthermore, we analyze how these models respond to out-of-distribution queries and documents, revealing distinct generalization behaviors."
  - [section] "The results revealed broadly consistent outcomes across all models, suggesting that different LLM architectures encode latent features in a comparable way within their activations."
  - [corpus] Weak - neighboring papers discuss interpretability across models but don't specifically address architecture-independent feature learning
- Break condition: If architectural differences significantly impact how features are represented in activation space, the consistency would break down

### Mechanism 3
- Claim: Overfitting to MS MARCO manifests as inconsistent feature representations on out-of-distribution data
- Mechanism: When models overfit, they learn dataset-specific feature representations that don't generalize, causing different activation patterns for the same features when encountering out-of-distribution queries
- Core assumption: The model's ability to generalize is reflected in the stability of feature representations across training and test distributions
- Evidence anchors:
  - [abstract] "However, they do not remain consistent for certain features of RankLlama2-13b when encountering out-of-distribution data, suggesting potential overfitting in the LLM during fine-tuning."
  - [section] "A likely reason for this might be that RankLlama 13b has overfit to the MS MARCO dev set and is hence seeking features like stream length which are known to not generalize for reflecting query-document relevance."
  - [corpus] Weak - neighboring papers don't discuss overfitting patterns in mechanistic interpretability contexts
- Break condition: If the model generalizes well, feature representations would remain stable across in-distribution and out-of-distribution data

## Foundational Learning

- Concept: Sparse probing with Lasso regularization
  - Why needed here: To identify which specific neurons in MLP layers encode particular IR features while maintaining sparsity
  - Quick check question: How does Lasso regularization help isolate the most important neurons for each feature?

- Concept: Layer-wise activation analysis
  - Why needed here: To understand where in the transformer architecture specific features are extracted and represented
  - Quick check question: Why might certain features be encoded in early layers while others appear only in later layers?

- Concept: R² score interpretation in mechanistic interpretability
  - Why needed here: To quantify how well linear probes can recover feature values from neural activations
  - Quick check question: What does a high R² score tell us about the relationship between a feature and neural activations?

## Architecture Onboarding

- Component map: Tokenized query-document pairs -> Forward pass with hooks -> MLP activation capture -> Lasso regression probe fitting -> R² evaluation
- Critical path: Tokenization → Forward pass with hooks → Activation capture → Probe fitting → R² evaluation
- Design tradeoffs:
  - Mean vs max aggregation: Mean aggregation showed similar results to max with slightly better stability
  - Feature selection: Limited to features computable from MS MARCO data (no anchors, links, URLs)
  - Dataset balance: Critical for avoiding biased probe results
- Failure signatures:
  - Negative R² scores indicate anti-correlation or probe failure
  - Inconsistent R² across distributions suggests overfitting
  - Low R² for expected features suggests architectural limitations
- First 3 experiments:
  1. Verify MLP layer extraction by probing for simple features like stream length
  2. Test probing stability by running the same probe multiple times
  3. Compare mean vs max activation aggregation on a single feature

## Open Questions the Paper Calls Out

- Question: Do ranking LLMs encode features beyond those identified in the MSLR dataset, and if so, what are their characteristics?
  - Basis in paper: [explicit] The authors acknowledge that their study focuses on a subset of MSLR features and state "We do not claim that this is a definitive list of features universally acknowledged as important for ranking."
  - Why unresolved: The paper only probes for 19 MSLR features and 7 similarity scores, leaving open the possibility that other important features exist but were not examined.
  - What evidence would resolve it: Systematic probing experiments that expand the feature set to include other known ranking features, or analysis of the residual unexplained variance in LLM activations after accounting for the studied features.

- Question: What is the causal relationship between identified features and ranking decisions in LLMs?
  - Basis in paper: [inferred] The authors note that "Probing techniques reveal correlations between specific features and neuron activations, but not a causal analysis of the decision making process."
  - Why unresolved: The study uses correlation-based probing methods that identify associations but cannot establish whether the identified features actually drive ranking decisions.
  - What evidence would resolve it: Ablation studies that systematically disable or modify neurons encoding specific features and measure the impact on ranking performance, or causal intervention experiments.

- Question: How do feature representations in LLM activations generalize across different domains and datasets?
  - Basis in paper: [explicit] The authors find that "certain features like stream length and sum of term frequency show a strong presence in the in-distribution dataset probe, even though they are unlikely features to influence a ranking decision" and suggest "RankLlama 13b has overfit to the MS MARCO dev set."
  - Why unresolved: While the paper examines in-distribution versus out-of-distribution performance for some features, it does not provide a comprehensive analysis of feature generalization across diverse domains.
  - What evidence would resolve it: Extensive probing experiments using multiple out-of-domain datasets spanning different domains (academic, e-commerce, social media) to determine which features consistently generalize versus those that are dataset-specific.

## Limitations
- Study only examines four specific LLM architectures fine-tuned on a single dataset (MS MARCO)
- Feature selection was constrained by available data in MS MARCO, potentially missing important IR features
- Probing methodology relies on linear relationships between activations and features

## Confidence
- **High confidence**: MLP layers serve as primary feature extractors; linear probes can successfully recover some IR features from activations
- **Medium confidence**: Consistency of feature encoding across different LLM architectures; overfitting manifests as inconsistent feature representations
- **Low confidence**: Specific architectural reasons for feature encoding patterns; exact mechanisms of feature extraction in MLP layers

## Next Checks
1. Replicate probing experiments on additional ranking datasets (e.g., TREC Deep Learning tracks) to verify whether observed feature encoding patterns generalize beyond MS MARCO
2. Compare MLP probing results with alternative feature extraction mechanisms (attention heads, residual streams) to determine if MLP layers are uniquely suited for IR feature encoding
3. Systematically test model performance on progressively more distant out-of-distribution queries to establish clear thresholds for overfitting detection and generalization limits