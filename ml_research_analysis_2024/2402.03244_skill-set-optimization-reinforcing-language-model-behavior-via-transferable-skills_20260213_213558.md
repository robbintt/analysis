---
ver: rpa2
title: 'Skill Set Optimization: Reinforcing Language Model Behavior via Transferable
  Skills'
arxiv_id: '2402.03244'
source_url: https://arxiv.org/abs/2402.03244
tags:
- skill
- skills
- task
- actor
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Skill Set Optimization (SSO) improves LLM actor performance by
  constructing and refining transferable skills from high-reward subtrajectories.
  SSO extracts similar subtrajectories, scores them by similarity, reward, and length,
  and generates abstract subgoals and instructions.
---

# Skill Set Optimization: Reinforcing Language Model Behavior via Transferable Skills

## Quick Facts
- arXiv ID: 2402.03244
- Source URL: https://arxiv.org/abs/2402.03244
- Reference count: 40
- Primary result: SSO achieves state-of-the-art performance in ScienceWorld, outperforming CLIN by 35%, and improves NetHack task performance by 40% over baselines

## Executive Summary
Skill Set Optimization (SSO) is a method for improving language model (LLM) actor performance in interactive environments by constructing and refining transferable skills from high-reward subtrajectories. SSO extracts similar subtrajectories, scores them by similarity, reward, and length, and generates abstract subgoals and instructions. Skills are refined by pruning those that don't lead to high rewards. In ScienceWorld, SSO achieves state-of-the-art performance, outperforming CLIN by 35%. In a custom NetHack task, SSO outperforms baselines by 40%, demonstrating effective skill-based in-context policy improvement.

## Method Summary
SSO iteratively constructs and refines skills that reinforce high-reward behaviors by extracting common subtrajectories with high rewards, generating subgoals and instructions to represent each skill, and providing them in-context to the LLM actor. The method extracts similar subtrajectories, scores them by similarity, reward, and length, and generates abstract subgoals and instructions. Skills are refined by pruning those that don't lead to high rewards, enabling continual in-context policy improvement without expensive fine-tuning. SSO generalizes across tasks by learning abstract, transferable skills that can be reused in new task variants by matching initial states.

## Key Results
- SSO achieves state-of-the-art performance in ScienceWorld, outperforming CLIN by 35%
- In a custom NetHack task, SSO outperforms baselines by 40%
- SSO demonstrates effective skill-based in-context policy improvement without fine-tuning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SSO improves LLM actor performance by iteratively constructing and refining skills that reinforce high-reward behaviors.
- Mechanism: SSO extracts similar subtrajectories with high rewards, generates abstract subgoals and instructions, and provides them in-context. Poorly performing skills are pruned based on observed discounted rewards.
- Core assumption: Similar subtrajectories can be abstracted into generalizable skills, and environment rewards reliably indicate skill usefulness.
- Evidence anchors:
  - [abstract] "SSO constructs skills by extracting common subtrajectories with high rewards and generating subgoals and instructions to represent each skill."
  - [section 4.1] "To identify transferable skills, we extract multiple similar subtrajectories to be used to generate each skill."
  - [corpus] Weak; no direct citation of reward-based pruning or subtrajectory abstraction in neighbors.
- Break condition: If subtrajectories are too dissimilar, abstraction fails. If rewards are sparse or noisy, pruning becomes unreliable.

### Mechanism 2
- Claim: SSO enables continual in-context policy improvement without expensive fine-tuning.
- Mechanism: By providing relevant skills in-context at each decision step, the LLM actor adapts its policy based on learned behaviors. Skills are retrieved via cosine similarity of initial states.
- Core assumption: LLM in-context learning can generalize from skill examples to new situations; cosine similarity effectively retrieves relevant skills.
- Evidence anchors:
  - [abstract] "These skills are provided to the LLM actor in-context to reinforce behaviors with high rewards."
  - [section 3] "Instead, we develop a scalable and transferable memory structure for in-context policy improvement that we call skills."
  - [corpus] No direct evidence; neighbors focus on skill space design or library learning, not in-context retrieval.
- Break condition: If context window is exceeded, retrieval fails. If similarity metric is poor, irrelevant skills are provided.

### Mechanism 3
- Claim: SSO generalizes across tasks by learning abstract, transferable skills.
- Mechanism: Skills are generated from multiple similar subtrajectories, making them abstract. They can be reused in new task variants by matching initial states.
- Core assumption: Abstract instructions generalize across task variants; initial states provide sufficient retrieval cues.
- Evidence anchors:
  - [abstract] "Because each skill subgoal and instructions are generated from multiple subtrajectories, the resulting subgoals and instructions are often abstract (as in Figure 1) and further facilitate task transfer."
  - [section 4.1.4] "We prompt the actor LLM with the pair of subtrajectories and ask it to generate ... a single target observation that would indicate the success of the skill."
  - [corpus] Weak; neighbors discuss skill libraries but not abstraction from subtrajectories.
- Break condition: If tasks require highly specific knowledge, abstraction reduces usefulness. If initial states are too general, retrieval becomes ambiguous.

## Foundational Learning

- Concept: Subtrajectory extraction and similarity scoring
  - Why needed here: SSO relies on identifying similar sequences of states and actions to build generalizable skills.
  - Quick check question: Given two subtrajectories, how would you compute their similarity using state and action embeddings?

- Concept: In-context learning with examples
  - Why needed here: SSO provides skills as examples in the prompt to guide the LLM actor's behavior.
  - Quick check question: What are the limitations of in-context learning for sequential decision making compared to fine-tuning?

- Concept: Reinforcement learning basics (reward signals, credit assignment)
  - Why needed here: SSO uses environment rewards to score and prune skills, mimicking RL policy improvement.
  - Quick check question: How does SSO's skill refinement step relate to policy gradient methods in RL?

## Architecture Onboarding

- Component map:
  - Environment -> LLM Actor -> SSO Module -> Skill Set
  - SSO Module: Extracts, scores, samples, and generates skills; retrieves relevant skills; refines skill set

- Critical path:
  1. Run trajectory with current LLM actor and skill set.
  2. Extract similar subtrajectory pairs from trajectory history.
  3. Score subtrajectory pairs by similarity, reward, and length.
  4. Sample non-overlapping high-scoring pairs.
  5. Generate skills (subgoal + instructions) from sampled pairs.
  6. Retrieve relevant skills for next iteration via state similarity.
  7. Refine skill set by pruning low-observed-value skills.

- Design tradeoffs:
  - Granularity of skills (short vs. long subtrajectories) affects abstraction and usefulness.
  - Number of retrieved skills vs. context window limits.
  - Frequency of skill refinement vs. computational cost.

- Failure signatures:
  - Skills become too specific (no abstraction) or too general (no usefulness).
  - Skill set grows too large, causing retrieval inefficiency or context overflow.
  - Refinement prunes all skills, reverting to baseline performance.

- First 3 experiments:
  1. Run SSO on a simple ScienceWorld task with known subgoals; verify skill generation and retrieval.
  2. Compare performance with and without skill refinement on a held-out task variant.
  3. Test skill transfer by training on one task set and evaluating on a different set.

## Open Questions the Paper Calls Out
- How does SSO performance change with different similarity metrics for subtrajectory extraction?
- Can SSO effectively handle environments without intermediate rewards?
- How does SSO compare to other memory-based methods like Voyager or ExpeL in terms of long-term task generalization?

## Limitations
- Effectiveness heavily depends on quality of subtrajectory abstraction and reliability of environment rewards for skill pruning
- Reliance on cosine similarity for skill retrieval assumes initial states provide sufficient cues, which may not hold in environments with ambiguous state features
- Claim of task transfer through abstract skills is not robustly supported with empirical evidence across diverse task sets

## Confidence
- **High Confidence**: SSO improves LLM actor performance by constructing and refining transferable skills from high-reward subtrajectories
- **Medium Confidence**: SSO enables continual in-context policy improvement without fine-tuning
- **Low Confidence**: SSO generalizes across tasks by learning abstract, transferable skills

## Next Checks
1. Conduct experiments to measure the abstractness of generated skills and their effectiveness across task variants
2. Test SSO's performance under varying reward signal qualities (sparse, noisy, delayed) to assess reliability of skill pruning
3. Measure effectiveness of cosine similarity-based skill retrieval in environments with ambiguous or repetitive state features