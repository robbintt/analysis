---
ver: rpa2
title: 'Distinguished In Uniform: Self Attention Vs. Virtual Nodes'
arxiv_id: '2405.11951'
source_url: https://arxiv.org/abs/2405.11951
tags:
- graph
- mpgnn
- graphs
- function
- layer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the relative expressive power of Graph
  Transformers (GTs) and Message-Passing GNNs with Virtual Nodes (MPGNN+VN) in the
  uniform setting, where a single network must work for graphs of all sizes. The authors
  show that both architectures are not universal approximators and that neither subsumes
  the other in terms of uniform expressivity.
---

# Distinguished In Uniform: Self Attention Vs. Virtual Nodes
## Quick Facts
- arXiv ID: 2405.11951
- Source URL: https://arxiv.org/abs/2405.11951
- Reference count: 40
- Primary result: Graph Transformers and MPGNN+VN are incomparable in uniform expressivity

## Executive Summary
This paper investigates the relative expressive power of Graph Transformers (GTs) and Message-Passing GNNs with Virtual Nodes (MPGNN+VN) in the uniform setting, where a single network must work for graphs of all sizes. The authors show that both architectures are not universal approximators and that neither subsumes the other in terms of uniform expressivity. They prove that GTs cannot uniformly approximate functions requiring unbounded aggregation (like counting nodes), while MPGNN+VN cannot uniformly approximate functions requiring intricate computation of self-attention. Synthetic experiments confirm these theoretical findings, demonstrating that MPGNN+VN can learn to predict |V|^2 while GTs fail to generalize to larger graph sizes. On real-world datasets, the results are mixed, with neither architecture consistently outperforming the other.

## Method Summary
The paper employs a theoretical analysis approach combined with synthetic and real-world experiments. The theoretical framework establishes impossibility results for both architectures under uniform parameterization assumptions. The authors construct specific functions that demonstrate the limitations of each architecture: GT cannot uniformly approximate functions requiring unbounded aggregation, while MPGNN+VN cannot uniformly approximate functions requiring intricate computation of self-attention. Synthetic experiments test the ability to learn |V|^2 on graphs of varying sizes, while real-world experiments evaluate performance on standard graph datasets.

## Key Results
- GTs and MPGNN+VN are incomparable in uniform expressivity - neither can uniformly approximate all functions the other can
- GTs cannot uniformly approximate functions requiring unbounded aggregation (like counting nodes) due to attention weight normalization
- MPGNN+VN cannot uniformly approximate functions requiring intricate computation of self-attention, as virtual nodes lack global connectivity patterns
- Synthetic experiments confirm theoretical findings: MPGNN+VN learns |V|^2 while GTs fail to generalize beyond training sizes
- Real-world results show mixed performance with no consistent winner between architectures

## Why This Works (Mechanism)
The key insight is that uniform expressivity requires architectures to handle all graph sizes with fixed parameters. GTs rely on attention mechanisms that normalize across nodes, making it impossible to distinguish between graphs with different numbers of nodes using bounded parameters. MPGNN+VN, while able to aggregate information through virtual nodes, cannot implement the intricate computation patterns required for certain attention-based functions. The virtual node mechanism provides global connectivity but lacks the fine-grained control over attention weights that GTs possess, while GTs' attention normalization prevents them from performing unbounded aggregations necessary for functions like node counting.

## Foundational Learning
- Uniform vs. non-uniform architectures: Understanding why a single parameter set must work across all graph sizes is crucial for grasping the theoretical limitations
- Attention weight normalization: Why needed - prevents unbounded information flow; Quick check - verify attention scores sum to 1 across nodes
- Virtual node aggregation: Why needed - provides global context to local computations; Quick check - confirm virtual node messages reach all graph nodes
- Bounded vs. unbounded functions: Why needed - determines whether architectures can use fixed parameters; Quick check - test if function output grows with graph size

## Architecture Onboarding
Component map: Input Graph -> GT/MPGNN+VN -> Virtual Node Integration -> Output Prediction

Critical path: Node features → Local message passing (MPGNN) OR Self-attention (GT) → Virtual node aggregation → Final readout

Design tradeoffs: GT offers fine-grained attention control but cannot perform unbounded aggregations; MPGNN+VN provides global connectivity through virtual nodes but lacks attention mechanism flexibility

Failure signatures: GT fails on size-dependent functions (e.g., |V|^2) due to normalization; MPGNN+VN fails on complex attention-based computations requiring intricate weight patterns

First experiments:
1. Test GT on synthetic function f(G) = |V| to confirm inability to count nodes
2. Test MPGNN+VN on attention-based function requiring specific weight patterns
3. Compare training and test performance on |V|^2 across different size ranges

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions in the main text, but the results raise several important questions about practical implications of uniform expressivity limitations.

## Limitations
- Strong assumptions about bounded graph size and attribute values may not hold in practice
- The uniform vs. non-uniform distinction relies on identical parameterization across sizes, which may not reflect real implementations
- Synthetic experiments use simple functions that may not capture full complexity of real-world graph problems
- Design choices for attention mechanisms and aggregation strategies significantly impact results but are not thoroughly explored

## Confidence
- Theoretical impossibility results: High confidence
- Synthetic experiment conclusions: Medium confidence
- Real-world dataset results: Medium confidence

## Next Checks
1. Test architectures on broader range of synthetic functions requiring different computation types
2. Investigate impact of different attention mechanisms and aggregation strategies
3. Conduct experiments with varying bounds on graph size and attribute values