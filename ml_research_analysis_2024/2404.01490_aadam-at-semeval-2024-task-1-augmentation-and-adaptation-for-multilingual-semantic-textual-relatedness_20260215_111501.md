---
ver: rpa2
title: 'AAdaM at SemEval-2024 Task 1: Augmentation and Adaptation for Multilingual
  Semantic Textual Relatedness'
arxiv_id: '2404.01490'
source_url: https://arxiv.org/abs/2404.01490
tags:
- language
- data
- association
- languages
- linguistics
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a system for the SemEval-2024 Task 1 on Semantic
  Textual Relatedness for African and Asian Languages. The system uses machine translation
  for data augmentation and task-adaptive pre-training to address limited training
  data.
---

# AAdaM at SemEval-2024 Task 1: Augmentation and Adaptation for Multilingual Semantic Textual Relatedness

## Quick Facts
- arXiv ID: 2404.01490
- Source URL: https://arxiv.org/abs/2404.01490
- Reference count: 40
- Ranked first in both supervised learning and cross-lingual transfer subtasks for SemEval-2024 Task 1

## Executive Summary
This paper presents AAdaM, a system designed for Semantic Textual Relatedness (STR) in African and Asian languages as part of SemEval-2024 Task 1. The system addresses the challenge of limited training data through machine translation-based data augmentation and task-adaptive pre-training (TAPT). It explores both full fine-tuning and adapter-based tuning approaches, ultimately adopting the MAD-X framework for effective zero-shot cross-lingual transfer. The system achieves top performance in both supervised and cross-lingual transfer subtasks.

## Method Summary
The AAdaM system uses a cross-encoder architecture with AfroXLMR-large-61L as the backbone. It applies machine translation (NLLB) to augment training data from English to target languages, addresses domain adaptation through task-adaptive pre-training on unlabeled task data, and employs adapter-based tuning with MAD-X for efficient parameter updates. The system implements a two-phase training strategy, first training on augmented noisy data then on clean task data, with source language selection based on development set performance for cross-lingual transfer.

## Key Results
- Achieved the best performance among all ranked teams in both supervised learning (subtask A) and cross-lingual transfer (subtask C) subtasks
- Demonstrated effectiveness of data augmentation through machine translation for low-resource languages
- Showed adapter-based tuning provides competitive performance to full fine-tuning while being more parameter-efficient

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data augmentation using machine translation compensates for limited labeled data in low-resource languages.
- Mechanism: Translation of high-resource language (English) data to target languages creates additional training samples that preserve semantic content.
- Core assumption: The translated sentences maintain the semantic relatedness properties of the original sentences.
- Evidence anchors:
  - [abstract] "We propose using machine translation for data augmentation to address the low-resource challenge of limited training data."
  - [section] "As the provided task data for non-English languages is relatively limited, we perform data augmentation for these languages via machine translation."
  - [corpus] "To leverage data in varied qualities, Zhu et al. (2023) shows that a two-phase approach is beneficial, in which the model is trained on noisy data first and then trained on clean data."
- Break condition: Translation quality degrades or introduces semantic drift that misaligns with relatedness scoring objectives.

### Mechanism 2
- Claim: Task-adaptive pre-training (TAPT) improves model performance by bridging the gap between general pre-training and specific task requirements.
- Mechanism: Continued pre-training on unlabeled task data using masked language modeling adapts the model's representations to the target domain.
- Core assumption: The unlabeled task data shares sufficient domain characteristics with the labeled data to be beneficial for adaptation.
- Evidence anchors:
  - [abstract] "we apply task-adaptive pre-training on unlabeled task data to bridge the gap between pre-training and task adaptation."
  - [section] "To better adapt PLMs to downstream tasks, Gururangan et al. (2020) propose task-adaptive pre-training (TAPT), i.e., continued pre-training on task-specific unlabeled data, and show that it can effectively improve downstream task performance."
  - [corpus] "For MLM, we set the learning rate to 5e-5 and train models for 10 epochs."
- Break condition: Domain mismatch between unlabeled task data and labeled training data reduces the effectiveness of adaptation.

### Mechanism 3
- Claim: Adapter-based tuning provides competitive performance to full fine-tuning while being more parameter-efficient.
- Mechanism: Small adapter modules inserted between transformer layers are trained while keeping the base model frozen, allowing task-specific adaptation without updating all parameters.
- Core assumption: The adapter modules can capture the necessary task-specific information without full parameter updates.
- Evidence anchors:
  - [abstract] "For model training, we investigate both full fine-tuning and adapter-based tuning, and adopt the adapter framework for effective zero-shot cross-lingual transfer."
  - [section] "adapter-based tuning (Houlsby et al., 2019) only updates small modules known as adapters inserted between the layers of PLMs while keeping the remaining parameters frozen."
  - [corpus] "For adapter-based tuning, we utilize the MAD-X framework (Pfeiffer et al., 2020) which consists of language-specific adapters and task-specific adapters."
- Break condition: The task requires fine-grained adjustments that cannot be captured by small adapter modules alone.

## Foundational Learning

- Concept: Cross-encoder vs Bi-encoder architectures
  - Why needed here: Understanding the architecture choice helps explain the trade-off between performance and efficiency
  - Quick check question: Why does a cross-encoder architecture generally outperform bi-encoders for semantic relatedness tasks?

- Concept: Zero-shot cross-lingual transfer
  - Why needed here: Critical for understanding how the system performs well on languages without direct supervision
  - Quick check question: How does the MAD-X framework enable zero-shot cross-lingual transfer?

- Concept: Data augmentation quality vs quantity trade-off
  - Why needed here: Helps explain why two-phase training (noisy then clean) is beneficial
  - Quick check question: Why might training on noisy augmented data first be beneficial before training on clean task data?

## Architecture Onboarding

- Component map: 
  - Input layer: Concatenation of sentence pairs
  - Backbone: AfroXLMR-large-61L transformer model
  - Regression head: Predicts relatedness score
  - Optional components: Language adapters, task adapters, TAPT stage
  - Data augmentation pipeline: English to target language translation

- Critical path:
  1. Load pre-trained AfroXLMR-large-61L model
  2. Apply TAPT on unlabeled task data (optional)
  3. Perform data augmentation via NLLB translation
  4. Train adapters (or fine-tune) on augmented data (warmup)
  5. Train adapters (or fine-tune) on original task data
  6. For cross-lingual transfer: Replace source language adapter with target language adapter

- Design tradeoffs:
  - Cross-encoder vs bi-encoder: Better performance vs efficiency
  - Fine-tuning vs adapter tuning: Full parameter updates vs parameter efficiency
  - Augmentation quality: Translation artifacts vs increased training data
  - Source language selection: Linguistic distance vs development set performance

- Failure signatures:
  - Poor performance on low-relatedness pairs: May indicate data annotation issues or model bias
  - Negative correlations in cross-lingual transfer: Likely source language selection problem
  - Marginal gains from TAPT: Domain mismatch between unlabeled and labeled data
  - Degradation when switching between fine-tuning and adapter tuning: Task may require full parameter updates

- First 3 experiments:
  1. Compare baseline (no training) vs fine-tuned model on development set to establish performance baseline
  2. Test data augmentation effectiveness by training on augmented data vs original data only
  3. Evaluate impact of TAPT by comparing with and without task-adaptive pre-training on same model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do specific linguistic features and typological distances between source and target languages quantitatively impact cross-lingual transfer performance in semantic textual relatedness tasks?
- Basis in paper: [inferred] The paper discusses linguistic distance as a metric for source language selection but finds it insufficient to explain transfer results. They note contradictory conclusions in previous studies about factors affecting cross-lingual transfer.
- Why unresolved: The paper concludes that linguistic distance alone doesn't reliably predict transfer performance and that multiple factors interact in complex ways. The specific contribution of individual linguistic features remains unclear.
- What evidence would resolve it: Systematic experiments isolating individual linguistic features (e.g., syntax, morphology, phonology) while controlling for other variables, combined with correlation analysis between feature similarity and transfer performance across multiple language pairs.

### Open Question 2
- Question: What is the optimal balance between data augmentation quality and quantity for improving semantic textual relatedness model performance across different languages?
- Basis in paper: [explicit] The paper acknowledges that machine translation-based data augmentation introduces artifacts and semantic mismatches between "similarity" and "relatedness" concepts, but still achieves improvements.
- Why unresolved: The paper doesn't investigate how different qualities of augmented data (e.g., human vs. machine translation, different translation models) affect performance, nor does it explore the relationship between augmentation quality and quantity.
- What evidence would resolve it: Controlled experiments comparing performance across varying levels of data augmentation quality and quantity, potentially including human-annotated augmented data for comparison.

### Open Question 3
- Question: How does task-adaptive pre-training affect the generalization capabilities of language models for semantic textual relatedness across diverse language families?
- Basis in paper: [explicit] The paper applies task-adaptive pre-training on unlabeled task data but notes potential domain mismatch between pre-training data and task data, suggesting this needs further investigation.
- Why unresolved: The paper doesn't explore how TAPT affects model performance across different language families or investigate optimal pre-training strategies for diverse linguistic structures.
- What evidence would resolve it: Comparative analysis of model performance across language families with and without TAPT, including ablation studies on pre-training data domain, quantity, and task-specific objectives.

## Limitations
- Reliance on machine translation quality for data augmentation, which may introduce semantic drift in low-resource languages
- Two-phase training strategy requires careful calibration of when to switch between noisy and clean data
- Limited exploration of optimal source language selection beyond development set performance

## Confidence
- **High confidence**: The effectiveness of adapter-based tuning for parameter efficiency and cross-lingual transfer performance
- **Medium confidence**: The data augmentation strategy's impact, as translation quality can vary significantly across language pairs
- **Low confidence**: The exact contribution of task-adaptive pre-training (TAPT) to overall performance

## Next Checks
1. Conduct ablation studies comparing model performance with and without each component (TAPT, data augmentation, adapter tuning) to isolate their individual contributions to the overall system performance.

2. Perform quality assessment of machine-translated augmented data by measuring semantic drift between source and target sentences using established metrics like BERTScore or BLEURT.

3. Test the model's robustness to translation quality variations by deliberately using lower-quality translation services and measuring the impact on downstream relatedness scoring performance.