---
ver: rpa2
title: 'CARTE: Pretraining and Transfer for Tabular Learning'
arxiv_id: '2402.16785'
source_url: https://arxiv.org/abs/2402.16785
tags:
- carte
- learning
- data
- tables
- table
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CARTE, a novel neural architecture for tabular
  learning that addresses the challenge of data integration by eliminating the need
  for schema and entity matching. CARTE represents table entries as graphs, where
  each row is a star-like graphlet with nodes and edges initialized using language
  models.
---

# CARTE: Pretraining and Transfer for Tabular Learning

## Quick Facts
- arXiv ID: 2402.16785
- Source URL: https://arxiv.org/abs/2402.16785
- Authors: Myung Jun Kim; Léo Grinsztajn; Gaël Varoquaux
- Reference count: 40
- Primary result: CARTE consistently outperforms strong baselines including CatBoost on tabular learning tasks by 10-30% without requiring schema or entity matching

## Executive Summary
This paper introduces CARTE, a novel neural architecture for tabular learning that addresses the challenge of data integration by eliminating the need for schema and entity matching. CARTE represents table entries as graphs, where each row is a star-like graphlet with nodes and edges initialized using language models. This graph representation enables pretraining on large knowledge bases like YAGO without requiring column or entity matching. Extensive experiments demonstrate that CARTE consistently outperforms strong baselines, including tree-based models like CatBoost, on both single-table and multi-table settings. The method achieves normalized scores of 0.783 on regression tasks and 0.615 on classification tasks with 1024 training samples, significantly surpassing other methods.

## Method Summary
CARTE converts table rows into star-like graphlets where nodes represent entries and edges represent column names, with both initialized using language model embeddings (FastText). The architecture uses a 12-layer graph-attention network with 12 attention heads and 300-dimensional hidden states to contextualize entries using column names and neighboring entries. Pretraining is performed on YAGO3 knowledge base using a contrastive loss that generates positive samples through truncated graphlets. For downstream tasks, CARTE supports both single-table learning and joint learning across multiple tables with unmatched columns, using early stopping on validation sets with a bagging strategy for ensemble predictions.

## Key Results
- CARTE achieves normalized scores of 0.783 on regression and 0.615 on classification tasks with 1024 training samples
- CARTE consistently outperforms CatBoost and other baselines across 51 datasets with 10-30% improvement
- Joint learning across tables with unmatched columns provides additional performance gains without requiring data integration
- The method enables pretraining on large knowledge bases without schema matching, opening possibilities for large pretrained models in tabular learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CARTE bypasses schema and entity matching by representing table rows as star-like graphlets.
- Mechanism: Each row becomes a small graph where nodes are entries and edges are column names. This normalization allows joint learning across tables with different schemas without alignment.
- Core assumption: The graphlet representation preserves sufficient relational context for downstream prediction.
- Evidence anchors:
  - [abstract] "CARTE represents table entries as graphs, where each row is a star-like graphlet with nodes and edges initialized using language models."
  - [section] "The graph representation of table entities play a crucial role in facilitating the generalization power of CARTE."
- Break condition: If column names or neighboring entries do not provide meaningful context for the task, the graphlet structure may not capture discriminative features.

### Mechanism 2
- Claim: Language model embeddings of column names and entries enable open-vocabulary learning across tables.
- Mechanism: FastText embeddings provide dense representations of strings, avoiding the need for exact matching between entity names across sources.
- Core assumption: The semantic similarity captured by embeddings is sufficient to bridge naming variations.
- Evidence anchors:
  - [abstract] "CARTE represents table entries as graphs, where each row is a star-like graphlet with nodes and edges initialized using language models."
  - [section] "CARTE benefits from the use of language models on non-numerical entries... the graph transformation in CARTE does not require extensive intervention on discrete entries with data preprocessing or data-cleaning."
- Break condition: If embedding vocabulary is too limited or if column names are highly ambiguous, embeddings may not distinguish entities well.

### Mechanism 3
- Claim: Pretraining on YAGO captures broad knowledge that improves downstream tabular learning.
- Mechanism: Contrastive loss on graphlets from YAGO teaches the model to aggregate contextual information across entity relations.
- Core assumption: YAGO contains enough relevant entities and relations to provide useful priors for typical tabular tasks.
- Evidence anchors:
  - [abstract] "The architecture –CARTE for Context Aware Representation of Table Entries– uses a graph representation of tabular (or relational) data to process tables with different columns, string embedding of entries and columns names to model an open vocabulary, and a graph-attentional network to contextualize entries with column names and neighboring entries."
  - [section] "CARTE is pretrained on YAGO3 (Mahdisoltani et al., 2013), a large knowledgebase built from Wikidata and other sources that contain facts about real-world."
- Break condition: If downstream tables use domains poorly represented in YAGO, the pretraining benefit may be minimal.

## Foundational Learning

- Graph neural networks and attention mechanisms
  - Why needed here: CARTE's core architecture is a graph-attention network that contextualizes table entries via column and neighbor information.
  - Quick check question: Can you explain how the attention score in equation (1) incorporates both node and edge features?

- Contrastive learning for representation learning
  - Why needed here: Pretraining uses a contrastive loss to learn representations that capture contextual relationships in the knowledge graph.
  - Quick check question: How does the positive sample generation (truncated graphlets) enforce invariance to partial information?

- Knowledge graph embedding techniques
  - Why needed here: The element-wise product of node and edge features in the attention mechanism is inspired by knowledge graph embedding methods.
  - Quick check question: Why might element-wise multiplication be preferred over translational operations for tabular data?

## Architecture Onboarding

- Component map:
  Input -> Star-like graphlet conversion -> FastText embedding initialization -> 12-layer graph-attention network -> Center node extraction -> Supervised loss (fine-tune) or contrastive loss (pretrain)

- Critical path:
  1. Convert rows to graphlets
  2. Initialize node/edge features with language model
  3. Apply graph-attention layers
  4. Extract center node embedding
  5. Train with contrastive loss (pretrain) or supervised loss (fine-tune)

- Design tradeoffs:
  - Graph representation vs flat feature matrix: More expressive but heavier computation
  - Pretraining on YAGO vs training from scratch: Better priors but domain mismatch risk
  - Joint learning vs single-table: More data but possible noise from unrelated tables

- Failure signatures:
  - Overfitting on small tables: Watch validation curves; use early stopping
  - Poor transfer between tables: Check if column names share semantic similarity
  - Slow training: Graph-attention layers are expensive; consider reducing layers or heads

- First 3 experiments:
  1. Train CARTE on a single small table with varying numbers of training samples; compare to CatBoost baseline
  2. Jointly train CARTE on two tables with mismatched columns; measure transfer benefit
  3. Pretrain on YAGO, then fine-tune on a downstream table; ablate pretraining to measure impact

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions but identifies several areas for future research:

- **Domain-specific pretraining:** The paper mentions that CARTE enables pretraining on background data without requiring matching, opening possibilities for domain-specific knowledge bases (medical, financial, etc.) to improve performance on specialized tabular tasks.

- **Scalability to larger knowledge graphs:** While the paper demonstrates pretraining on YAGO3, it doesn't explore how performance scales with larger knowledge bases or different graph structures.

- **Integration with existing tabular learning frameworks:** The paper establishes CARTE as a standalone method but doesn't explore how it could be integrated with existing tabular learning pipelines or combined with other pretraining strategies.

## Limitations

- Evaluation uses non-standard normalized score metric that combines performance across different datasets
- Pretraining on YAGO knowledge graph may not generalize well to tabular data with different characteristics
- Significant computational overhead compared to traditional tree-based methods like CatBoost
- Limited exploration of how performance varies with table size, column count, and relationship complexity

## Confidence

- High confidence in the core architectural contribution and mechanism design
- Medium confidence in the empirical results and comparisons to baselines
- Low confidence in the pretraining effectiveness across diverse tabular domains and computational practicality

## Next Checks

1. **Cross-domain pretraining evaluation:** Test CARTE pretraining on multiple knowledge bases (not just YAGO) and evaluate transfer to different tabular domains (business, healthcare, social science) to assess domain generalizability.

2. **Efficiency benchmarking:** Measure wall-clock training time, memory usage, and inference latency for CARTE versus CatBoost and other baselines on the same hardware to quantify the computational overhead.

3. **Ablation on graph representation:** Compare CARTE performance when using alternative graph constructions (different node/edge initialization schemes, varying graphlet sizes) to isolate the contribution of the star-like graphlet design.