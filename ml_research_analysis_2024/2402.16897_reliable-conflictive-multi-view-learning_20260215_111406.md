---
ver: rpa2
title: Reliable Conflictive Multi-View Learning
arxiv_id: '2402.16897'
source_url: https://arxiv.org/abs/2402.16897
tags: []
core_contribution: The paper introduces the Reliable Conflictive Multi-view Learning
  (RCML) problem, which requires models to provide both decisions and reliability
  estimates for conflictive multi-view data where views may disagree. Existing methods
  either remove conflictive instances or replace views, but real-world applications
  need decisions for such cases.
---

# Reliable Conflictive Multi-View Learning

## Quick Facts
- arXiv ID: 2402.16897
- Source URL: https://arxiv.org/abs/2402.16897
- Authors: Cai Xu; Jiajun Si; Ziyu Guan; Wei Zhao; Yue Wu; Xiyue Gao
- Reference count: 11
- Primary result: ECML achieves 2.99% accuracy improvement on normal test sets and up to 23.24% on conflictive test sets

## Executive Summary
This paper addresses the challenge of reliable conflictive multi-view learning where views may disagree. The authors propose ECML, which learns view-specific evidence and constructs opinions with belief masses and uncertainties. A key innovation is the conflictive opinion aggregation strategy that increases uncertainty when views conflict, rather than averaging them. This approach provides both decisions and reliability estimates for conflictive multi-view data, outperforming state-of-the-art methods on 6 real-world datasets.

## Method Summary
The ECML method uses view-specific evidential DNNs to learn evidence for each category, which is converted to Dirichlet distribution parameters. The conflictive opinion aggregation combines views by weighting belief masses and increasing uncertainty when views disagree. The model is trained with a composite loss function that includes uncertainty-based accuracy (Lace), KL divergence (LKL), and conflictive degree minimization (Lcon). The method provides both predictions and reliability estimates that increase with noise intensity in conflictive data.

## Key Results
- 2.99% accuracy improvement on normal test sets compared to state-of-the-art baselines
- Up to 23.24% accuracy improvement on conflictive test sets
- Reliable uncertainty estimates that correlate with noise intensity in conflictive data
- Theoretical proof that conflictive opinion aggregation correctly models view reliability relationships

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Conflictive opinion aggregation increases uncertainty when views disagree, preventing overconfidence in unreliable predictions.
- Mechanism: When combining two opinions with different projected probabilities but both have non-zero uncertainty, the conjunctive certainty (product of reliabilities) is low. The aggregation formula gives more weight to the uncertain masses, resulting in higher final uncertainty.
- Core assumption: The uncertainty masses from individual views accurately reflect their reliability, and conflictive views have non-zero uncertainty.
- Evidence anchors: [abstract] "we propose a conflictive opinion aggregation strategy and theoretically prove this strategy can exactly model the relation of multi-view common and view-specific reliabilities"; [section] "when a highly uncertain opinion is combined, the uncertainty of the new opinion is larger than the original opinion"

### Mechanism 2
- Claim: View-specific evidential DNNs learn evidence that serves as support for each category, enabling uncertainty estimation.
- Mechanism: Each view's DNN outputs evidence values (non-negative) for each class. These are converted to Dirichlet parameters, which model the probability distribution over classes. Higher evidence leads to lower uncertainty (more certainty in prediction).
- Core assumption: Evidence can be meaningfully learned from data for each view independently, and the Dirichlet distribution properly captures uncertainty.
- Evidence anchors: [abstract] "ECML first learns view-specific evidence, which could be termed as the amount of support to each category collected from data"; [section] "We calculate the Dirichlet distribution parameters α by α = e + 1 to guarantee the parameters are larger than one"

### Mechanism 3
- Claim: Minimizing conflictive degree during training ensures consistency across views for normal instances, reducing false positives for conflictive classification.
- Mechanism: The conflictive degree metric combines projected distance and conjunctive certainty between views. By minimizing this during training on normal instances, the model learns to produce consistent predictions across views, making conflictive instances more distinguishable.
- Core assumption: Normal instances should have consistent predictions across views, and the conflictive degree metric properly captures this consistency.
- Evidence anchors: [abstract] "Specifically, we calculate the conflictive degree according to the projected distance and conjunctive certainty among views"; [section] "we introduce a measure named the conflictive degree in Definition 2, which is established according to opinion entropy"

## Foundational Learning

- Concept: Dirichlet distribution for uncertainty modeling
  - Why needed here: Traditional softmax only provides point estimates; Dirichlet allows modeling of both aleatoric and epistemic uncertainty in multi-view settings
  - Quick check question: Why do we add 1 to evidence to get Dirichlet parameters (α = e + 1)?

- Concept: Subjective logic and belief masses
  - Why needed here: Provides theoretical framework for combining uncertain opinions from multiple views, essential for the conflictive aggregation strategy
  - Quick check question: What's the relationship between belief mass, uncertainty mass, and base rate in a multinomial opinion?

- Concept: Conflict detection and measurement
  - Why needed here: Real-world multi-view data often contains conflicting views; we need to quantify this conflict to properly handle it during aggregation
  - Quick check question: How does the conflictive degree metric distinguish between conflicting and non-conflicting opinions?

## Architecture Onboarding

- Component map: View-specific evidential DNNs → Conflictive opinion aggregation layer → Final prediction
- Critical path: Input views → View-specific DNNs (evidence learning) → Opinion construction → Conflictive aggregation → Output
- Design tradeoffs: Simple average pooling for aggregation vs. more complex weighted methods; explicit uncertainty modeling vs. traditional softmax
- Failure signatures: Poor performance on conflictive test sets; uncertainty estimates not correlating with noise levels; high conflictive degree on normal instances
- First 3 experiments:
  1. Train on normal instances only, test on normal test set to verify baseline performance
  2. Add Gaussian noise to one view, check if uncertainty increases appropriately
  3. Modify one view's content to create unaligned views, verify conflictive degree increases and aggregation handles it correctly

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the conflictive opinion aggregation method be extended to handle more than two views in a principled way?
- Basis in paper: [explicit] The paper mentions that the method can be extended to handle more than two views, but does not provide details on how to do so.
- Why unresolved: The paper only proves the method for two views and does not explore the theoretical implications or practical implementation for multiple views.
- What evidence would resolve it: A theoretical proof showing that the conflictive opinion aggregation method maintains its properties (e.g., uncertainty increases with conflicts) when extended to more than two views, along with experimental validation on multi-view datasets.

### Open Question 2
- Question: How does the conflictive degree metric perform in cases where views have different levels of noise or conflicts?
- Basis in paper: [explicit] The paper introduces the conflictive degree metric but does not extensively evaluate its performance in scenarios with varying noise levels or conflicts across views.
- Why unresolved: The paper only demonstrates the metric's ability to capture conflicts but does not analyze its robustness or sensitivity to different noise intensities or conflict patterns.
- What evidence would resolve it: Experiments showing the conflictive degree's performance under various noise levels and conflict patterns, along with a sensitivity analysis to understand its behavior in different scenarios.

### Open Question 3
- Question: Can the ECML method be adapted to handle streaming or online multi-view data where views may arrive at different times or have varying quality?
- Basis in paper: [inferred] The paper focuses on batch learning and does not address the challenges of streaming or online scenarios.
- Why unresolved: Real-world applications often involve streaming data, and the current method may not be suitable for such cases without modifications.
- What evidence would resolve it: An extension of the ECML method to handle streaming or online multi-view data, along with experiments demonstrating its effectiveness in scenarios where views arrive at different times or have varying quality.

## Limitations

- The method's computational overhead for conflictive degree calculation and uncertainty propagation is not thoroughly analyzed
- The theoretical proofs assume well-behaved Dirichlet distributions, but empirical validation across diverse data distributions is limited
- Generalization to extreme noise levels and very high-dimensional multi-view data remains untested

## Confidence

- High confidence: The theoretical framework for conflictive opinion aggregation is mathematically sound
- Medium confidence: Experimental results show consistent improvements, but limited to 6 datasets
- Low confidence: Generalization to extreme noise levels and very high-dimensional multi-view data remains untested

## Next Checks

1. Test on datasets with artificially injected noise levels ranging from 10% to 50% to evaluate uncertainty estimation robustness
2. Compare computational efficiency against non-evidential baselines on large-scale multi-view datasets
3. Conduct ablation studies removing the conflictive degree component to isolate its contribution to performance gains