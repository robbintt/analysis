---
ver: rpa2
title: Cooperative Advisory Residual Policies for Congestion Mitigation
arxiv_id: '2407.00553'
source_url: https://arxiv.org/abs/2407.00553
tags:
- policies
- driver
- policy
- residual
- congestion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel class of cooperative advisory residual
  policies for congestion mitigation in traffic networks. The key idea is to learn
  personalized residual policies that can provide real-time driving advice to human
  drivers, while accounting for diverse driver behaviors and preferences.
---

# Cooperative Advisory Residual Policies for Congestion Mitigation

## Quick Facts
- arXiv ID: 2407.00553
- Source URL: https://arxiv.org/abs/2407.00553
- Reference count: 40
- One-line primary result: Proposed policies improve congestion metrics by up to 40% over baselines while maintaining human compatibility in user studies.

## Executive Summary
This paper introduces cooperative advisory residual policies for mitigating traffic congestion by providing real-time driving advice to human drivers. The approach learns personalized residual policies that adapt to individual driver behaviors and preferences while accounting for imperfect instruction-following. The policies are trained in simulation using a novel driver policy that models how drivers react to instructions, and are evaluated both in simulation and through a user study with 16 participants in a driving simulator. Results demonstrate significant congestion reduction while maintaining human compatibility and personalization.

## Method Summary
The method combines Piecewise Constant Policies (PCP) as a base with Residual Policies (RP) trained to provide advisory actions. An improved reward function explicitly balances speed maximization, headway minimization, and action smoothness to avoid tailgating while encouraging uniform traffic flow. The driver policy models three real driver traitsâ€”noise, reaction delay, and intentional offsetâ€”to improve sim-to-real transfer. A Driver Trait Inference (DTI) module using a VAE encoder learns latent representations of driver behavior from trajectories, enabling personalization without ground-truth traits. Policies are trained in SUMO simulation and evaluated in both SUMO and CARLA driving simulator with human participants.

## Key Results
- Residual policies improve congestion factor by up to 40% compared to baseline OSL policies in simulation
- User study (N=16) shows residual policies are preferred by humans and personalize effectively to individual driving styles
- PeRP (with DTI) performs comparably to oracle TA-RP, suggesting learned latents capture useful personalization signals
- Policies successfully balance congestion mitigation with human compatibility, receiving positive feedback on NASA TLX and SUS questionnaires

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Improved reward function explicitly balances speed, headway, and action smoothness to avoid tailgating while encouraging uniform traffic flow.
- Mechanism: Reward ð‘…ð‘…ð‘ƒ combines speed maximization (ð›¼ð‘ ð‘ð‘’ð‘’ð‘‘ ð‘£ð‘–ð‘¡), headway minimization (ð›¼â„Žð‘’ð‘Žð‘‘ð‘¤ð‘Žð‘¦ â„Žð‘–ð‘¡), and action change penalization (ð›¼ð‘Žð‘ð‘¡ð‘–ð‘œð‘› |ð‘Žð‘Žð‘‘ð‘£ð‘–ð‘ð‘’ð‘¡ âˆ’ ð‘Žð‘Žð‘‘ð‘£ð‘–ð‘ð‘’ð‘¡âˆ’ð›¿|) to shape policy toward congestion mitigation without forcing unsafe tailgating.
- Core assumption: Maximizing average speed while minimizing headway uniformly across all vehicles reduces stop-and-go waves.
- Evidence anchors:
  - [abstract]: "introduce an improved reward function that explicitly addresses congestion mitigation and driver attitudes to advice."
  - [section 4.1]: Definition of ð‘…ð‘…ð‘ƒ with three terms.
- Break condition: If headway term is too small relative to speed term, policy may revert to tailgating; if action smoothness term is too large, policy becomes overly conservative.

### Mechanism 2
- Claim: Driver policy ðœ‹ð‘‘ð‘Ÿð‘–ð‘£ð‘’ð‘Ÿ models three real driver traitsâ€”noise, reaction delay, and intentional offsetâ€”to improve sim-to-real transfer.
- Mechanism: At each step, the advised action is modified by adding Gaussian noise (imperfect following), a uniform delay (reaction time), and a uniform offset (intentional deviation).
- Core assumption: These three traits capture enough variability in real driver instruction-following to make trained policies robust when deployed to humans.
- Evidence anchors:
  - [section 4.2]: Formal definition of ðœ‹ð‘‘ð‘Ÿð‘–ð‘£ð‘’ð‘Ÿ combining ð‘˜, ð‘š, and ðœŽ.
  - [section 6.1.2]: User study results showing residual policies outperform baseline despite trait dormancy.
- Break condition: If real drivers exhibit traits outside the modeled ranges, policies may fail to adapt; if traits interact nonlinearly, the additive model may be insufficient.

### Mechanism 3
- Claim: Driver Trait Inference (DTI) module learns latent representations of driver behavior from trajectories, enabling personalization without ground-truth traits.
- Mechanism: Unsupervised VAE encoder maps observed trajectory sequences to latent vectors; residual policy conditions on these latents to adjust advice.
- Core assumption: Trajectory differences caused by trait variations are separable in latent space, allowing the policy to infer and adapt to driver style.
- Evidence anchors:
  - [section 4.4]: DTI model architecture and loss function.
  - [section 6.1.1]: PeRP performance comparable to or better than oracle TA-RP, suggesting learned latents are informative.
- Break condition: If driver behavior is dominated by context rather than traits, latents may capture noise instead of useful personalization signals.

## Foundational Learning

- Concept: Reinforcement learning with reward shaping.
  - Why needed here: Policies must learn to mitigate congestion in a dynamic, multi-vehicle environment without explicit supervision.
  - Quick check question: What happens to congestion if reward only maximizes ego speed without headway term?

- Concept: Variational autoencoders for unsupervised representation learning.
  - Why needed here: DTI module must infer driver traits from raw trajectories without labeled trait data.
  - Quick check question: How does the KL term in the DTI loss encourage useful latent representations?

- Concept: Residual policy learning.
  - Why needed here: Avoids training from scratch instability by building on PC policies while allowing personalization.
  - Quick check question: Why might a residual policy converge faster than a pure from-scratch policy in this setting?

## Architecture Onboarding

- Component map: PCP policy -> Residual policy -> Driver policy (simulation) / Human driver (user study) -> Environment
- Critical path: 1. Train PC policy (ð›¿ hold-length, discrete speed action space). 2. Train residual policy conditioned on PC policy actions and optionally DTI latents. 3. Deploy: PC policy action + residual offset -> advised speed -> human driver.
- Design tradeoffs: Action space granularity vs. training stability; Hold-length ð›¿ vs. human reaction time tolerance; DTI latent dimensionality vs. overfitting risk.
- Failure signatures: Large speed jumps between ð›¿ periods -> PC policy base instability; Residual adding large offsets -> Policy overcompensating for traits; DTI latents unchanging -> Trait inference not capturing useful variance.
- First 3 experiments: 1. Evaluate base PC policy vs. OSL to confirm congestion mitigation baseline. 2. Train residual policy with only reward shaping (no DTI) and measure improvement. 3. Add DTI module, train PeRP, compare personalization vs. RP.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do residual policies perform in more complex traffic scenarios with lane changes and intersections?
- Basis in paper: [explicit] The authors note that their policies were only tested on a simple ring road without lane changes and suggest testing on other networks such as open ring or intersection environments.
- Why unresolved: The study only evaluated the policies on a single-lane ring road, limiting generalizability to more complex traffic scenarios.
- What evidence would resolve it: Testing the policies in environments with lane changes, intersections, and varying road configurations to compare performance against baselines.

### Open Question 2
- Question: How does the performance of residual policies scale with increasing penetration rates beyond a single vehicle?
- Basis in paper: [explicit] The authors mention that testing the penetration power of their policies from one vehicle to multiple vehicles holds promise but was not evaluated.
- Why unresolved: The study only evaluated the policies with a single ego vehicle providing advice, not exploring the impact of multiple vehicles using the same or similar policies.
- What evidence would resolve it: Simulating and testing the policies with varying numbers of vehicles providing advice to assess the scalability and potential benefits of increased penetration rates.

### Open Question 3
- Question: How do the policies perform in real-world naturalistic driving conditions compared to simulation?
- Basis in paper: [explicit] The authors acknowledge that their user study was conducted in a driving simulator environment and that the sim-to-real gap necessitates naturalistic studies.
- Why unresolved: The study relied on simulation and a controlled user study, which may not fully capture the complexities and variability of real-world driving.
- What evidence would resolve it: Conducting field tests with the policies deployed in real vehicles on actual roads to evaluate their performance and robustness in naturalistic driving conditions.

## Limitations

- Driver policy's three-trait additive model may not capture complex, nonlinear interactions in real human instruction-following behavior
- User study sample size (N=16) may be underpowered to detect subtle policy differences across diverse driver populations
- Policy evaluation limited to controlled ring road environment without lane changes, limiting generalizability to complex traffic scenarios

## Confidence

- **High**: Congestion mitigation performance in simulation (40% improvement metrics are clearly demonstrated)
- **Medium**: User study results showing human preference and compatibility (small sample size, limited trait variation)
- **Medium**: DTI module effectiveness (qualitative comparisons show promise but lack ablation studies)

## Next Checks

1. **Trait Interaction Validation**: Systematically vary combinations of driver traits beyond the additive model to test for nonlinear effects that could break policy performance.
2. **Sample Size Power Analysis**: Conduct statistical power analysis on the user study data to determine minimum detectable effects and identify whether larger N would reveal additional insights.
3. **Context-Agnosticity Test**: Evaluate policy performance across diverse traffic scenarios (merges, intersections, varying densities) to verify congestion mitigation generalizes beyond the controlled ring road setting.