---
ver: rpa2
title: 'CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for
  RAG systems'
arxiv_id: '2404.02103'
source_url: https://arxiv.org/abs/2404.02103
tags:
- clap
- answer
- passage
- answers
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CLAP NQ is a new benchmark dataset for evaluating the full RAG
  pipeline in long-form question answering. It consists of 4,946 questions with grounded
  gold passages from Natural Questions, where answers are concise, complete, and cohesive
  by integrating non-contiguous text fragments.
---

# CLAPNQ: Cohesive Long-form Answers from Passages in Natural Questions for RAG systems

## Quick Facts
- arXiv ID: 2404.02103
- Source URL: https://arxiv.org/abs/2404.02103
- Reference count: 21
- Key outcome: CLAP NQ is a new benchmark dataset for evaluating the full RAG pipeline in long-form question answering, consisting of 4,946 questions with grounded gold passages from Natural Questions, where answers are concise, complete, and cohesive by integrating non-contiguous text fragments.

## Executive Summary
CLAP NQ is a novel benchmark dataset designed to evaluate the full retrieval-augmented generation (RAG) pipeline in long-form question answering. The dataset consists of 4,946 questions derived from Natural Questions, each paired with grounded gold passages. Unlike existing benchmarks, CLAP NQ requires models to generate answers that are not only accurate but also cohesive, complete, and concise by integrating non-contiguous text fragments from the passages. The dataset includes both answerable and unanswerable questions, making it suitable for comprehensive RAG evaluation. Experiments with state-of-the-art models reveal significant room for improvement, particularly in faithfulness to passages, conciseness, and handling unanswerable questions.

## Method Summary
CLAP NQ is constructed by first extracting questions from the Natural Questions dataset and then annotating grounded gold passages for each question. Annotators were tasked with creating answers that are cohesive, complete, and concise, often requiring the integration of non-contiguous text fragments from the passages. The dataset includes both answerable and unanswerable questions, with the latter being explicitly marked to evaluate model robustness. The annotation process involved multiple rounds of quality checks to ensure consistency and accuracy. The resulting dataset is designed to test the full RAG pipeline, including retrieval, generation, and end-to-end performance.

## Key Results
- State-of-the-art models struggle with faithfulness, conciseness, and handling unanswerable questions in CLAP NQ.
- Human evaluation confirms that models often fail to generate cohesive and complete answers from non-contiguous text fragments.
- The dataset reveals significant gaps in RAG system performance, particularly in integrating fragmented information and maintaining answer quality.

## Why This Works (Mechanism)
The CLAP NQ dataset works by addressing the limitations of existing RAG evaluation benchmarks. By requiring models to generate answers that are cohesive, complete, and concise from non-contiguous text fragments, it challenges the ability of RAG systems to integrate fragmented information effectively. The inclusion of unanswerable questions further tests model robustness and ability to handle uncertainty. The grounded gold passages ensure that evaluation is based on reliable and consistent evidence, providing a more accurate assessment of RAG system performance.

## Foundational Learning
1. **Retrieval-Augmented Generation (RAG)**: Combines information retrieval with text generation to produce informed responses.
   - Why needed: Enables models to access external knowledge for answering questions.
   - Quick check: Verify that the RAG system can retrieve relevant passages for a given query.

2. **Long-form Question Answering**: Involves generating detailed, coherent answers to complex questions.
   - Why needed: Tests the model's ability to provide comprehensive and contextually appropriate responses.
   - Quick check: Ensure the model can generate answers that are both informative and concise.

3. **Faithfulness in RAG**: Measures how accurately the generated answer reflects the content of the retrieved passages.
   - Why needed: Ensures that the model's output is grounded in the provided evidence.
   - Quick check: Compare the generated answer to the gold passages to assess faithfulness.

## Architecture Onboarding
- **Component Map**: Retrieval Module -> Generation Module -> Evaluation Module
- **Critical Path**: The retrieval module fetches relevant passages, which are then processed by the generation module to produce a cohesive answer. The evaluation module assesses the quality of the answer based on faithfulness, conciseness, and completeness.
- **Design Tradeoffs**: Balancing the trade-off between retrieval accuracy and generation quality. High retrieval accuracy may not always lead to better answers if the generation module cannot effectively integrate the retrieved information.
- **Failure Signatures**: Models may fail by producing answers that are either too verbose or too fragmented, or by ignoring unanswerable questions.
- **First Experiments**: 1) Evaluate retrieval accuracy using standard metrics like Recall@k. 2) Assess generation quality by measuring faithfulness to the passages. 3) Test model robustness by evaluating performance on unanswerable questions.

## Open Questions the Paper Calls Out
None

## Limitations
- The dataset is derived from Natural Questions, which may introduce inherent biases in question types or domains.
- The manual annotation process, while thorough, was conducted by a limited number of annotators, potentially introducing subjectivity in defining "cohesive" and "complete" answers.
- The evaluation metrics focus heavily on faithfulness and conciseness but may not fully capture other important aspects of answer quality, such as informativeness or relevance to the user's intent.

## Confidence
- **High**: Dataset construction and basic statistics are well-documented and reproducible.
- **Medium**: Evaluation results rely on specific models and metrics, which may not generalize to all RAG systems.
- **Low**: Claims about the dataset's ability to improve RAG evaluation require long-term validation across diverse applications.

## Next Checks
1. **Generalizability Test**: Evaluate CLAP NQ with a broader range of RAG models to assess whether performance gaps persist across different architectures.
2. **Bias Analysis**: Conduct a systematic analysis of potential biases introduced by the Natural Questions corpus.
3. **Annotation Consistency**: Replicate the manual annotation process with a new set of annotators to verify the consistency and objectivity of the "cohesive" and "complete" answer definitions.