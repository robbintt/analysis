---
ver: rpa2
title: 'PyTAG: Tabletop Games for Multi-Agent Reinforcement Learning'
arxiv_id: '2405.18123'
source_url: https://arxiv.org/abs/2405.18123
tags:
- games
- agents
- game
- against
- player
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: PyTAG extends a tabletop game framework with a Python API to support
  multi-agent reinforcement learning, enabling training and evaluation of RL agents
  in a diverse collection of modern board and card games. The work introduces a self-play
  mechanism to train agents against past versions of themselves, addressing the challenges
  of multi-agent dynamics and non-trivial turn orders.
---

# PyTAG: Tabletop Games for Multi-Agent Reinforcement Learning

## Quick Facts
- arXiv ID: 2405.18123
- Source URL: https://arxiv.org/abs/2405.18123
- Reference count: 36
- Introduces PyTAG, a Python API extending a tabletop game framework for multi-agent RL training and evaluation.

## Executive Summary
PyTAG extends a tabletop game framework with a Python API to support multi-agent reinforcement learning, enabling training and evaluation of RL agents in a diverse collection of modern board and card games. The work introduces a self-play mechanism to train agents against past versions of themselves, addressing the challenges of multi-agent dynamics and non-trivial turn orders. Agents are trained using PPO and evaluated against baseline agents (Random, OSLA, MCTS) across games like Tic Tac Toe, Diamant, Love Letter, Exploding Kittens, Stratego, Dots and Boxes, and Sushi Go!. Results show that self-play improves robustness and performance, though some games remain challenging due to hidden information, high stochasticity, or sparse rewards. The framework supports flexible observation/action representations and opens avenues for future research including LLM integration and league-based training.

## Method Summary
PyTAG provides a Python API for training RL agents in tabletop games, using PPO with self-play. Agents are trained against a pool of past versions of themselves, with periodic evaluation against baseline agents (Random, OSLA, MCTS). The framework supports flexible observation and action spaces tailored to each game, vectorised environments for high-throughput training, and turn-order management for games with non-regular player sequences. Training runs for 1M steps with evaluation every 20k steps, using 3 random seeds.

## Key Results
- Self-play agents outperform baseline agents (Random, OSLA, MCTS) in most evaluated games, with win rates and score improvements.
- Games with hidden information (Love Letter, Exploding Kittens) remain challenging, though self-play improves performance over fixed opponents.
- Strategic games like Stratego and Dots and Boxes show that self-play agents can learn competitive strategies within a few hours on a standard laptop.

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The self-play mechanism improves agent robustness by continually exposing the learner to previously discovered strategies.
- **Mechanism:** The learner policy is periodically checkpointed and added to a pool of past policies. During training, the learner plays against sampled opponents from this pool, ensuring it must adapt to evolving strategies rather than overfitting to a single fixed opponent.
- **Core assumption:** Past versions of the learner policy are sufficiently diverse and strong to challenge the current learner without causing catastrophic forgetting.
- **Evidence anchors:**
  - [abstract] "The work introduces a self-play mechanism to train agents against past versions of themselves, addressing the challenges of multi-agent dynamics..."
  - [section] "By using self-play, the comparisons against the baseline agents are fairer."
- **Break condition:** If the opponent pool is too homogeneous (e.g., always the most recent checkpoint), the learner may converge to a narrow strategy that exploits only those versions. If the pool is too large, the learner may face opponents too weak to drive meaningful improvement.

### Mechanism 2
- **Claim:** Flexible observation and action space interfaces enable the framework to support diverse tabletop games with minimal engineering overhead.
- **Mechanism:** PyTAG delegates the construction of observations and action masks to game-specific extractor functions. This allows each game to encode its state and legal actions in a representation best suited to its mechanics (e.g., grid-based for Stratego, card counts for Love Letter), while keeping the core training loop generic.
- **Core assumption:** Each game can be expressed as a vector or JSON observation and a fixed-size action mask, even if the underlying action space is combinatorial or dynamic.
- **Evidence anchors:**
  - [section] "PyTAG aims to keep the framework accessible to a wider community, hence we aim to keep the observation and action spaces flexible."
  - [section] "To add support for a new game, the user is only required to write two functions: 1, a function returning a standardised observation from the game state... and 2, an action-masking function..."
- **Break condition:** If a game's state or action space cannot be meaningfully flattened into a vector or enumerated action mask (e.g., natural language negotiation in Diplomacy), the interface will fail to capture essential information.

### Mechanism 3
- **Claim:** Vectorised environments with turn-order management allow high-throughput training despite arbitrary player action sequences.
- **Mechanism:** At each step, the framework determines which player(s) must act using a turn-order function, routes observations and action masks to the appropriate policies, collects actions, and updates the environment. Training updates occur as soon as any player's buffer reaches the desired length, preventing idle waiting.
- **Core assumption:** The overhead of managing per-player buffers and dynamic routing is outweighed by the parallelisation gains from vectorisation.
- **Evidence anchors:**
  - [section] "With varying turn-orders, this is often not the case, as some players may take more actions than others. To tackle this, instead of waiting for all agents’ transitions to fill up a predefined length, we do an update as soon as one player in the environment reaches the desired length."
  - [section] "All the experiments were run on a machine with 16 CPU cores without making use of a GPU. With self-play, we introduce an additional overhead by using multiple neural networks for inference during training, but even in this setting an agent can be trained in a few hours on a normal laptop."
- **Break condition:** If the turn-order function introduces frequent player switches, the routing overhead may dominate, eroding the throughput benefit of vectorisation.

## Foundational Learning

- **Concept:** Multi-agent reinforcement learning (MARL) in non-simultaneous, non-regular turn-taking environments
  - Why needed here: Tabletop games often have asymmetric or stochastic turn orders, so the agent must handle cases where different players act at different times or in different orders each episode.
  - Quick check question: What happens if the turn-order function returns the same player twice in a row? (Answer: The same agent acts again without the environment state advancing.)

- **Concept:** Self-play as a curriculum for opponent strength
  - Why needed here: Training against a fixed baseline can lead to overfitting; self-play ensures the agent faces increasingly challenging strategies derived from its own past performance.
  - Quick check question: How does the opponent sampling probability affect the difficulty of training? (Answer: Biasing toward recent checkpoints raises difficulty; uniform sampling may leave the agent under-challenged.)

- **Concept:** Action masking to enforce legality in large combinatorial action spaces
  - Why needed here: Many tabletop games have hundreds or thousands of possible actions; action masks ensure the RL agent only selects from currently legal moves, preventing wasted exploration and invalid policy outputs.
  - Quick check question: What is the consequence of an incorrectly constructed action mask? (Answer: The agent may be unable to act or may attempt illegal actions, causing environment errors or poor learning.)

## Architecture Onboarding

- **Component map:**
  - PyTAG core: manages game state, turn-order, observation/action extraction, and vectorised step execution.
  - TAG backend: implements the actual tabletop games, forward model, and baseline agents.
  - RL agent: neural network policy (e.g., PPO) that receives observations, outputs actions, and is trained on collected transitions.
  - Self-play pool: stores past policy checkpoints for opponent sampling.
  - Evaluation module: periodically runs the current policy against baseline agents to measure progress.

- **Critical path:**
  1. Environment step → determine acting player(s) via turn-order.
  2. Route observations/masks to correct policy(ies).
  3. Collect actions, update environment, store transitions.
  4. Trigger training update when any buffer reaches target length.
  5. Periodically evaluate against baselines and save checkpoints.

- **Design tradeoffs:**
  - Flexible observation/action interfaces vs. unified tensor pipelines (more flexibility, but potential inefficiency).
  - Opponent pool size vs. computational overhead (larger pool = more diverse but slower sampling).
  - Buffer length trigger vs. update frequency (shorter buffers = faster updates but noisier gradients).

- **Failure signatures:**
  - Training stalls: likely caused by opponent pool too weak or homogeneous.
  - Exploding gradients or NaNs: check observation scaling and reward clipping.
  - Low evaluation win rates despite high self-play win rates: overfitting to self-play opponents; need stronger baselines or league-based training.

- **First 3 experiments:**
  1. **Tic Tac Toe sanity check:** Train PPO with self-play for 100k steps; verify >90% win rate against Random and OSLA.
  2. **Diamant reward ablation:** Compare Terminal vs. Score reward functions; measure final scores and win rates.
  3. **Love Letter memory test:** Train with full card history in observation; evaluate against Random and MCTS to see if memory improves performance.

## Open Questions the Paper Calls Out

- **Open Question 1:** How can PyTAG's self-play mechanism be improved to avoid convergence to suboptimal strategies and explore the strategic depth of tabletop games more effectively?
  - Basis in paper: [explicit] The paper discusses challenges with self-play converging to suboptimal policies and mentions exploring league-based systems or weighted sampling based on recency as future directions.
  - Why unresolved: Current self-play implementations in PyTAG may not fully explore the strategic depth of games, leading to convergence on less optimal strategies.
  - What evidence would resolve it: Experimental results comparing different self-play strategies (e.g., league-based systems, weighted sampling) and their impact on learning outcomes and convergence in various tabletop games.

- **Open Question 2:** What are the most effective ways to incorporate memory mechanisms into RL agents within PyTAG to improve performance in games with hidden information and complex decision-making processes?
  - Basis in paper: [explicit] The paper highlights the challenges of hidden information in tabletop games and suggests that adding explicit memory to RL agents could boost performance.
  - Why unresolved: Current RL agents in PyTAG may struggle with games requiring memory for tracking hidden information and making informed decisions based on past observations.
  - What evidence would resolve it: Comparative studies of RL agents with and without memory mechanisms, demonstrating improvements in performance across various games with hidden information.

- **Open Question 3:** How can PyTAG be extended to support games that require cooperation or mixed competitive-cooperative dynamics, and what are the implications for MARL research?
  - Basis in paper: [explicit] The paper mentions the potential for PyTAG to support cooperative games like "Hanabi" and "Pandemic," and mixed dynamics, but does not explore these extensively.
  - Why unresolved: PyTAG's current focus is on competitive games, and extending it to cooperative or mixed dynamics would require new interfaces and algorithms.
  - What evidence would resolve it: Development and testing of PyTAG's support for cooperative and mixed dynamics games, with analysis of how these changes impact MARL research and agent performance.

## Limitations

- The framework's flexibility comes at the cost of reduced standardisation, making cross-game comparisons and transfer learning difficult.
- Self-play assumes past checkpoints form a meaningful curriculum, but there is no theoretical guarantee of monotonic improvement.
- Reported performance gains may be architecture-dependent, as only high-level neural network descriptions are provided.

## Confidence

- **High confidence:** Self-play mechanism implementation and vectorised environment step execution are clearly specified and directly reproducible.
- **Medium confidence:** Claim that self-play improves robustness is supported by win-rate trends but not by ablation studies isolating the effect of opponent pool diversity.
- **Low confidence:** Claims about scalability to games with hidden information or stochastic turn orders lack quantitative evidence; reported performance gains may be architecture-dependent.

## Next Checks

1. **Architecture sensitivity test:** Vary neural network size and type (fully-connected vs. convolutional) for a single game (e.g., Tic Tac Toe) and measure impact on convergence speed and final win rates.

2. **Opponent pool ablation:** Train identical agents with different opponent pool sizes (1, 5, 10, 20) and sampling strategies (uniform, recent-biased) to quantify the effect of self-play diversity on robustness.

3. **Hidden information stress test:** Implement a simple hidden-info variant of Tic Tac Toe (e.g., random fog-of-war) and verify that the observation/action extractor can still produce meaningful representations and that training remains stable.