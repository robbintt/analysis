---
ver: rpa2
title: Is Flash Attention Stable?
arxiv_id: '2405.02803'
source_url: https://arxiv.org/abs/2405.02803
tags:
- attention
- training
- deviation
- numeric
- flash
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates training instability in large-scale Generative
  AI models, hypothesizing that numeric deviation from optimization techniques like
  Flash Attention could be a contributing factor. The authors develop a microbenchmark
  to quantify numeric deviation during the forward pass and use Wasserstein distance
  to bound its impact on model weights during training.
---

# Is Flash Attention Stable?

## Quick Facts
- arXiv ID: 2405.02803
- Source URL: https://arxiv.org/abs/2405.02803
- Authors: Alicia Golden; Samuel Hsia; Fei Sun; Bilge Acun; Basil Hosmer; Yejin Lee; Zachary DeVito; Jeff Johnson; Gu-Yeon Wei; David Brooks; Carole-Jean Wu
- Reference count: 12
- This paper investigates training instability in large-scale Generative AI models, hypothesizing that numeric deviation from optimization techniques like Flash Attention could be a contributing factor.

## Executive Summary
This paper investigates training instability in large-scale Generative AI models, hypothesizing that numeric deviation from optimization techniques like Flash Attention could be a contributing factor. The authors develop a microbenchmark to quantify numeric deviation during the forward pass and use Wasserstein distance to bound its impact on model weights during training. Their analysis shows that Flash Attention introduces roughly 10x more numeric deviation than Baseline Attention at BF16 precision during a forward pass. However, data-driven analysis of weight changes suggests this deviation is 2-5x less significant than low-precision training. The study provides a principled framework for quantifying numeric deviation and contextualizing its potential impact on training stability.

## Method Summary
The authors develop a numerical microbenchmark to isolate and study numeric deviation in Flash Attention compared to Baseline Attention. They implement a re-implementation of Flash Attention that allows varying numerical precision (BF16, FP16, FP32, FP64) and block configurations. The microbenchmark generates random query, key, and value vectors to simulate attention computations and measure output differences. They also conduct training experiments using Text-to-Image models on NVIDIA A100 GPUs with the Shutterstock dataset, comparing model weights during training using maximum difference and Wasserstein distance metrics. The Wasserstein distance analysis provides upper bounds on how numeric deviation impacts model weights during training.

## Key Results
- Flash Attention introduces roughly an order of magnitude more numeric deviation than Baseline Attention at BF16 precision during an isolated forward pass.
- The numeric deviation present in Flash Attention is 2-5 times less significant than low-precision training when measured by its impact on model weights.
- As sequence length increases, the numerical deviation between Flash Attention and Baseline Attention increases due to more rescaling calculations being needed.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Flash Attention introduces roughly an order of magnitude more numeric deviation than Baseline Attention during the forward pass when using BF16 precision.
- Mechanism: Flash Attention uses tiling and recomputation to reduce memory overhead, but this introduces additional rescaling factors that accumulate approximation errors, especially with fewer mantissa bits in BF16.
- Core assumption: The numeric deviation observed in the forward pass is primarily due to the rescaling factors required by the tiling mechanism in Flash Attention.
- Evidence anchors:
  - [abstract] "We find that Flash Attention sees roughly an order of magnitude more numeric deviation as compared to Baseline Attention at BF16 when measured during an isolated forward pass."
  - [section] "We find that numerical precision has an impact on the output of Flash Attention, causing it to deviate from the output of Baseline Attention."
  - [corpus] Weak - corpus neighbors don't directly address numeric deviation in Flash Attention.
- Break condition: If the rescaling factors in Flash Attention were optimized to reduce approximation errors, or if higher precision formats were used, the numeric deviation would decrease significantly.

### Mechanism 2
- Claim: The numeric deviation introduced by Flash Attention during training is bounded and less significant than the deviation caused by low-precision training.
- Mechanism: Using Wasserstein distance, the authors quantify the impact of numeric deviation on model weights and find it to be 2-5 times less significant than the deviation from low-precision training.
- Core assumption: The Wasserstein distance metric accurately captures the distributional similarity of model weights and provides a meaningful upper bound on the impact of numeric deviation.
- Evidence anchors:
  - [abstract] "We then use a data-driven analysis based on the Wasserstein Distance to provide upper bounds on how this numeric deviation impacts model weights during training, finding that the numerical deviation present in Flash Attention is 2-5 times less significant than low-precision training."
  - [section] "We utilize two metrics to measure the model weight difference between a model trained with Baseline Attention as compared to Flash Attention."
  - [corpus] Weak - corpus neighbors don't directly address the use of Wasserstein distance in quantifying training instability.
- Break condition: If the model weights show significantly different behavior in downstream tasks (e.g., accuracy), the bound provided by Wasserstein distance would be insufficient to capture the true impact of numeric deviation.

### Mechanism 3
- Claim: The numeric deviation in Flash Attention increases with sequence length due to more rescaling calculations being needed.
- Mechanism: As sequence length increases, the N×N intermediate matrix becomes larger, requiring more tiles and thus more rescaling operations, which accumulate more approximation errors.
- Core assumption: The accumulation of approximation errors in rescaling operations is linear with the number of tiles required.
- Evidence anchors:
  - [section] "We find that as sequence length increases, the numerical deviation between Flash Attention and Baseline Attention increases."
  - [section] "Since a larger sequence length implies a larger N × N intermediate matrix that must be tiled while the tile size stays the same, more rescaling is needed."
  - [corpus] Weak - corpus neighbors don't directly address the relationship between sequence length and numeric deviation in Flash Attention.
- Break condition: If the model dimension or other architectural factors changed in a way that reduced the number of tiles needed, the relationship between sequence length and numeric deviation might not hold.

## Foundational Learning

- Concept: Attention mechanism in Transformers
  - Why needed here: Understanding how attention works is crucial to grasping why Flash Attention was developed and how it differs from baseline attention.
  - Quick check question: What is the key computation in the attention mechanism that causes quadratic scaling with sequence length?

- Concept: Numeric precision and floating-point representation
  - Why needed here: The paper heavily relies on understanding how different numeric precisions (BF16, FP16, FP64) affect the accumulation of approximation errors.
  - Quick check question: How does the number of mantissa bits in a floating-point format relate to the precision of numerical calculations?

- Concept: Wasserstein distance metric
  - Why needed here: The paper uses Wasserstein distance to quantify the distributional similarity of model weights, which is key to contextualizing the impact of numeric deviation.
  - Quick check question: How does Wasserstein distance differ from other metrics like maximum difference in measuring the similarity between tensors?

## Architecture Onboarding

- Component map: Microbenchmark -> Forward pass simulation -> Numeric deviation measurement -> Training experiments -> Weight difference analysis
- Critical path: Forward pass with Flash Attention → Numeric deviation in attention output → Weight updates during training → Model behavior and potential instability
- Design tradeoffs:
  - Speed vs. precision: Flash Attention offers speedup but introduces more numeric deviation
  - Memory efficiency vs. accuracy: Tiling reduces memory usage but requires rescaling that accumulates errors
  - Complexity vs. interpretability: Microbenchmark allows detailed analysis but requires re-implementation of the algorithm
- Failure signatures:
  - Loss spikes during training
  - Model weights diverging significantly from expected values
  - Reduced accuracy in downstream tasks
- First 3 experiments:
  1. Implement the microbenchmark to measure numeric deviation in Flash Attention at different precisions and sequence lengths
  2. Train a small model with both Flash and Baseline Attention to observe weight differences using maximum difference and Wasserstein distance
  3. Vary the tile size in Flash Attention to see how it affects numeric deviation and training stability

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How significant is the numeric deviation introduced by Flash Attention in terms of training instability?
- Basis in paper: [explicit] The authors quantify that Flash Attention introduces roughly 10x more numeric deviation than Baseline Attention at BF16 precision during a forward pass, but their data-driven analysis suggests this deviation is 2-5x less significant than low-precision training in terms of model weight changes.
- Why unresolved: While the authors provide a principled framework to quantify numeric deviation, they do not directly link this deviation to training instability. Training instability is costly to study and isolate, making it difficult to establish a direct causal relationship.
- What evidence would resolve it: Conducting controlled experiments where models are trained with Flash Attention and Baseline Attention under identical conditions, measuring the frequency and severity of loss spikes, and correlating these with the quantified numeric deviation.

### Open Question 2
- Question: How do other training optimizations, such as the Winograd Algorithm or kernel fusion techniques, impact numeric deviation?
- Basis in paper: [inferred] The authors suggest that their methodology can be applied to analyze other training optimizations and their numeric deviation from the appropriate baseline.
- Why unresolved: The paper focuses on Flash Attention as a case study, leaving the impact of other optimizations unexplored. The complex nature of these optimizations and their varying implementations make it challenging to generalize findings.
- What evidence would resolve it: Applying the authors' microbenchmark and data-driven analysis framework to a range of training optimizations, quantifying their numeric deviation, and comparing these results to identify patterns or significant differences.

### Open Question 3
- Question: What is the relationship between hardware reliability, checkpointing, and training instability?
- Basis in paper: [explicit] The authors mention that hardware failures contribute to the starting/stopping of model training across datacenters and suggest that future work should investigate this relationship.
- Why unresolved: The interplay between hardware reliability, checkpointing strategies, and training instability is complex and multifaceted. The stochastic nature of training and the variability in hardware performance make it difficult to isolate and study these factors.
- What evidence would resolve it: Conducting large-scale experiments that track hardware failures, checkpointing events, and instances of training instability, analyzing the data to identify correlations and potential causal relationships.

### Open Question 4
- Question: How does training instability impact the sustainability and power efficiency of datacenters?
- Basis in paper: [explicit] The authors note that frequent interruptions to the training procedure cause major power swings across the datacenter, leading to sustainability implications for designing low-carbon, power-efficient infrastructure.
- Why unresolved: The relationship between training instability and datacenter sustainability is complex and depends on various factors, including the frequency and severity of instabilities, the power consumption of different training stages, and the efficiency of power management systems.
- What evidence would resolve it: Collecting power consumption data from datacenters during training runs with varying levels of instability, analyzing the data to quantify the impact on power usage and carbon emissions, and developing models to predict the sustainability implications of different training strategies.

## Limitations
- Model Architecture Specificity: Findings may be highly dependent on the specific attention mechanism implementation and model architecture used.
- Hardware and Precision Dependencies: Numeric deviation measurements are specific to BF16 precision and NVIDIA A100 GPUs, potentially varying on different hardware or precision formats.
- Single-Dataset Analysis: Training experiments use the Shutterstock dataset, and observed stability characteristics may not generalize to other datasets or domains.

## Confidence
- High Confidence: The microbenchmark approach for quantifying numeric deviation is methodologically sound with clear implementation details and reproducible metrics.
- Medium Confidence: The claim that numeric deviation from Flash Attention is less significant than low-precision training (2-5x difference) is based on specific training conditions and may vary with different model scales, datasets, or training procedures.
- Low Confidence: The relationship between sequence length and numeric deviation accumulation assumes linear scaling of approximation errors, which may not hold for all model configurations or attention mechanisms.

## Next Checks
1. Replicate the numeric deviation measurements across multiple precision formats (BF16, FP16, FP32, FP64) on different GPU architectures to establish how hardware and precision choices affect the magnitude of deviation in Flash Attention.

2. Apply the same analysis framework to different transformer variants (e.g., Longformer, Linformer, Performer) to determine if the numeric deviation patterns observed for standard attention generalize to attention mechanisms with different computational properties.

3. Conduct controlled experiments measuring the actual impact of numeric deviation on downstream task performance (accuracy, perplexity, etc.) rather than relying solely on weight distribution metrics, to validate whether Wasserstein distance bounds meaningfully predict real-world training stability.