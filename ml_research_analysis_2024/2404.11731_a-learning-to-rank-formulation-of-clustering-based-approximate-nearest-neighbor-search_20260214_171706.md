---
ver: rpa2
title: A Learning-to-Rank Formulation of Clustering-Based Approximate Nearest Neighbor
  Search
arxiv_id: '2404.11731'
source_url: https://arxiv.org/abs/2404.11731
tags:
- function
- search
- query
- learning
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper formulates clustering-based approximate nearest neighbor
  search (ANN) as a learning-to-rank (LTR) problem. The key insight is that the routing
  step in ANN can be viewed as ranking partitions by their likelihood of containing
  the nearest neighbor, making it amenable to LTR methods.
---

# A Learning-to-Rank Formulation of Clustering-Based Approximate Nearest Neighbor Search

## Quick Facts
- arXiv ID: 2404.11731
- Source URL: https://arxiv.org/abs/2404.11731
- Reference count: 38
- Key outcome: Learning-to-rank improves clustering-based MIPS accuracy, especially when probing fewer partitions

## Executive Summary
This paper introduces a novel learning-to-rank formulation for clustering-based approximate nearest neighbor search (ANN), specifically targeting Maximum Inner Product Search (MIPS). The key insight is that the routing step in ANN can be viewed as a ranking problem, where partitions are ranked by their likelihood of containing the nearest neighbor. This approach leverages readily available ground-truth relevance labels by identifying the partition containing the exact nearest neighbor for each query. Experiments demonstrate that a learned linear routing function consistently outperforms the baseline centroid-based routing in MIPS, with more pronounced improvements when fewer partitions are probed. The learned function also benefits top-k retrieval accuracy beyond just top-1, despite being trained for top-1 optimization.

## Method Summary
The authors formulate the routing step in clustering-based ANN as a learning-to-rank (LTR) problem. Given a query, the routing function ranks partitions by their likelihood of containing the nearest neighbor. Ground-truth relevance labels are obtained by identifying the partition containing the exact nearest neighbor for each query. The routing function is learned using a simple linear model with cross-entropy loss, a consistent surrogate for Mean Reciprocal Rank (MRR). The learned routing function is then used in the ANN pipeline to improve the accuracy of MIPS. Experiments on various text datasets and embedding models show consistent improvements over the baseline centroid-based routing, especially when fewer partitions are probed.

## Key Results
- Learning-to-rank improves clustering-based MIPS accuracy compared to centroid-based routing
- Improvements are more pronounced when fewer partitions are probed
- Learned routing function benefits top-k retrieval accuracy beyond top-1, despite being trained for top-1

## Why This Works (Mechanism)
The proposed approach works by leveraging the learning-to-rank framework to optimize the routing step in clustering-based ANN. By formulating the routing as a ranking problem, the authors can learn a function that directly optimizes the likelihood of selecting the partition containing the nearest neighbor. The use of cross-entropy loss as a surrogate for MRR ensures that the learned routing function is optimized for the desired ranking metric. Additionally, the availability of ground-truth relevance labels from the exact nearest neighbor simplifies the training process and allows for direct supervision of the routing function.

## Foundational Learning
- Learning-to-Rank (LTR): LTR is needed to formulate the routing step as a ranking problem and learn an optimized routing function. Quick check: LTR is commonly used in information retrieval and recommender systems to rank items based on their relevance to a query.
- Maximum Inner Product Search (MIPS): MIPS is the specific ANN problem targeted in this paper, where the goal is to find the items with the highest inner product similarity to a query. Quick check: MIPS is widely used in recommendation systems, natural language processing, and computer vision tasks.
- Mean Reciprocal Rank (MRR): MRR is a ranking metric used to evaluate the performance of the learned routing function. Quick check: MRR measures the average reciprocal rank of the first relevant item in a ranked list of items.

## Architecture Onboarding

### Component Map
Query -> Learned Routing Function -> Partition Selection -> ANN Search

### Critical Path
The critical path in the proposed approach involves the learned routing function, which ranks partitions based on their likelihood of containing the nearest neighbor. The quality of the routing function directly impacts the accuracy of the ANN search, making it a crucial component in the pipeline.

### Design Tradeoffs
The authors choose a simple linear routing function to balance expressiveness and computational efficiency. While more complex models might potentially yield better results, they could also increase the training time and computational overhead. The use of cross-entropy loss as a surrogate for MRR is another tradeoff, as it simplifies the optimization process but may not directly optimize for the desired ranking metric.

### Failure Signatures
Potential failure modes include:
- Overfitting of the learned routing function to the training data, leading to poor generalization
- Suboptimal routing function leading to incorrect partition selection and reduced ANN accuracy
- Computational overhead of learning the routing function outweighing the benefits in accuracy

### First 3 Experiments
1. Evaluate the learned routing function on diverse data types (e.g., images, audio) to assess its generalization capabilities.
2. Investigate the impact of varying cluster sizes and the choice of number of clusters on the learned routing function's performance.
3. Compare the computational overhead of learning the routing function with the baseline, including training time and memory requirements.

## Open Questions the Paper Calls Out
None

## Limitations
- Scalability to very large datasets is not explored
- Robustness of the learned routing function across different data distributions is not thoroughly investigated
- Generalization to other data types beyond text (e.g., images, audio) is not demonstrated
- Impact of varying cluster sizes and choice of number of clusters on performance is not discussed in detail

## Confidence
- Core claims: High (empirical evidence of improved accuracy in clustering-based MIPS)
- Broader applicability: Medium (limited scope of experiments and lack of discussion on potential limitations)

## Next Checks
1. Evaluate the learned routing function on diverse data types (e.g., images, audio) to assess its generalization capabilities.
2. Investigate the impact of varying cluster sizes and the choice of number of clusters on the learned routing function's performance.
3. Compare the computational overhead of learning the routing function with the baseline, including training time and memory requirements.