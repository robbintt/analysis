---
ver: rpa2
title: 'Control-DAG: Constrained Decoding for Non-Autoregressive Directed Acyclic
  T5 using Weighted Finite State Automata'
arxiv_id: '2404.06854'
source_url: https://arxiv.org/abs/2404.06854
tags:
- decoding
- length
- wfsa
- beam
- constrained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors present Control-DAG, a constrained decoding algorithm
  for non-autoregressive Directed Acyclic T5 (DA-T5) models. The main problems addressed
  are frequent Out-Of-Vocabulary (OOV) errors and inability to faithfully generate
  entity names in non-autoregressive text generation.
---

# Control-DAG: Constrained Decoding for Non-Autoregressive Directed Acyclic T5 using Weighted Finite State Automata

## Quick Facts
- arXiv ID: 2404.06854
- Source URL: https://arxiv.org/abs/2404.06854
- Authors: Jinghong Chen, Weizhe Lin, Jingbiao Mei, Bill Byrne
- Reference count: 38
- Primary result: DA-T5 with Control-DAG achieves 22.9 BLEU and 0.0 SER on SGD, 19.0 BLEU and 0.0 EOR on DART

## Executive Summary
Control-DAG introduces a constrained decoding algorithm for non-autoregressive Directed Acyclic T5 (DA-T5) models that addresses Out-Of-Vocabulary (OOV) errors and faithful entity name generation through Weighted Finite State Automata (WFSA). The approach enforces lexical, vocabulary, and length constraints during decoding to ensure controlled generation while maintaining non-autoregressive speed advantages. The method demonstrates strong performance on dialogue state generation tasks while significantly reducing slot and exact occurrence errors to zero.

## Method Summary
Control-DAG uses Weighted Finite State Automata to constrain non-autoregressive decoding in DA-T5 models. The algorithm constructs WFSA representations for required constraints (vocabulary, lexical patterns, length) and intersects them with the model's output distribution space. This intersection ensures that only valid token sequences satisfying all constraints are generated. The decoding process operates in parallel across positions, maintaining the efficiency benefits of non-autoregressive generation while incorporating control mechanisms that typically require autoregressive approaches.

## Key Results
- Achieves 22.9 BLEU and 0.0 Slot Error Rate on Schema Guided Dialogue (SGD) dataset
- Achieves 19.0 BLEU and 0.0 Exact Occurrence Error Rate on DART dataset
- Maintains speed advantage over autoregressive baselines while enforcing strict generation constraints

## Why This Works (Mechanism)
Control-DAG works by leveraging WFSA theory to enforce hard constraints during non-autoregressive decoding. By constructing automata that represent required lexical patterns, vocabulary restrictions, and length constraints, the method ensures valid outputs without sacrificing parallel generation. The WFSA intersection operation effectively prunes the search space before decoding begins, allowing DA-T5 to operate within a constrained but efficient output space that guarantees constraint satisfaction.

## Foundational Learning
- **Weighted Finite State Automata (WFSA)**: Weighted automata that assign probabilities to paths, enabling constraint representation with uncertainty modeling. Needed to encode generation constraints with probability distributions.
- **Non-autoregressive Text Generation**: Generation methods that predict all tokens simultaneously rather than sequentially. Needed for efficiency gains in decoding speed.
- **Directed Acyclic T5 (DA-T5)**: T5 architecture modification enabling non-autoregressive generation through independent position prediction. Needed as the base model for Control-DAG.
- **BLEU Score**: Bilingual Evaluation Understudy metric measuring n-gram overlap between generated and reference text. Needed for quantitative evaluation of generation quality.
- **Slot Error Rate (SER)**: Metric measuring the percentage of slots incorrectly predicted in dialogue state generation. Needed to evaluate entity name generation accuracy.
- **Exact Occurrence Error Rate (EOR)**: Metric measuring the percentage of required phrases/entities not exactly matched in output. Needed for precise constraint satisfaction evaluation.

## Architecture Onboarding

Component Map: Constraints -> WFSA Construction -> WFSA Intersection -> DA-T5 Model -> Parallel Decoding

Critical Path: Constraint specification → WFSA construction → WFSA intersection → Constrained parallel decoding

Design Tradeoffs: Speed vs. constraint flexibility, constraint complexity vs. decoding efficiency, model capacity vs. constraint expressiveness

Failure Signatures: Constraint violations in output, increased decoding latency, memory exhaustion during WFSA operations

First Experiments:
1. Test WFSA intersection with simple vocabulary constraints on toy sequences
2. Measure decoding speed with increasing constraint complexity
3. Evaluate constraint satisfaction rate on synthetic datasets with known constraints

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to dialogue state generation datasets (SGD and DART)
- Performance metrics restricted to BLEU and slot/occurrence error rates
- No analysis of computational overhead for WFSA construction and intersection operations
- Limited exploration of scalability with multiple or complex constraints
- No assessment of open-domain generation scenarios with dynamic constraints

## Confidence
High for core technical contribution and SGD/DART results; Medium for general applicability claims; Low for scalability and open-domain performance assertions

## Next Checks
1. Benchmark Control-DAG on diverse text generation tasks beyond dialogue (e.g., summarization, translation, long-form generation) to assess generalization.
2. Measure wall-clock inference time and memory consumption across varying sequence lengths and constraint complexity to quantify the claimed speed advantages.
3. Evaluate performance with dynamically generated or user-defined constraints to test robustness in open-domain scenarios.