---
ver: rpa2
title: Grokking Modular Polynomials
arxiv_id: '2406.03495'
source_url: https://arxiv.org/abs/2406.03495
tags:
- modular
- logg
- networks
- network
- solution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work extends analytical solutions for modular arithmetic tasks
  to modular multiplication and addition with many terms. The authors construct networks
  using these "expert" solutions to solve arbitrary modular polynomials, including
  previously un-learnable ones.
---

# Grokking Modular Polynomials

## Quick Facts
- arXiv ID: 2406.03495
- Source URL: https://arxiv.org/abs/2406.03495
- Reference count: 40
- Key outcome: Extends analytical solutions for modular arithmetic to modular multiplication and addition with many terms, constructing networks that achieve 100% accuracy on learnable modular polynomials.

## Executive Summary
This paper extends analytical solutions for modular arithmetic tasks to handle modular multiplication and addition with many terms. The authors construct networks using these "expert" solutions to solve arbitrary modular polynomials, including previously un-learnable ones. They propose a hypothesis classifying modular polynomials into learnable and non-learnable categories, supported by experimental evidence. The approach achieves 100% accuracy on learnable modular polynomials using 2-layer MLPs with sufficient width, opening possibilities for Mixture-of-Experts models to learn arbitrary modular arithmetic tasks.

## Method Summary
The method constructs analytical solutions for modular arithmetic by exploiting the algebraic structure of finite fields GF(p). For modular addition, it uses periodic weight matrices with frequencies determined by the modulus p. For modular multiplication, it applies exponential and logarithmic maps to transform multiplication into addition. These solutions are combined to handle arbitrary modular polynomials by decomposing them into terms handled by expert networks, with softmax approximation ensuring proper summation.

## Key Results
- Analytical solutions extend to modular multiplication via exponential/logarithmic maps in finite fields
- Networks constructed from these solutions achieve 100% accuracy on learnable modular polynomials
- Clear experimental evidence supports classification of modular polynomials into learnable and non-learnable categories
- Width requirements grow exponentially with number of terms due to cross-term suppression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The analytical solution extends to modular multiplication by exploiting the bijection between addition and multiplication in finite fields via exponential and logarithmic maps.
- Mechanism: Modular multiplication in GF(p) has a cycle of length p-1 (excluding zero). The exponential map (g^r) and its inverse (log_g r) reshuffle non-zero elements of Z_p\{0}, transforming multiplication into addition. This allows reusing the analytical solution for modular addition.
- Core assumption: The existence of a primitive root g in GF(p) such that g^(p-1) ≡ 1 mod p, making the exponential map bijective.
- Evidence anchors:
  - [section] "Note that both modular addition and multiplication are defined over finite field of prime order p... multiplication has a cycle of length p − 1; i.e. (g · g · · · · (n − 1 times)) mod p = 1 , where g is a primitive root of GF (p)."
  - [section] "Reshuffling the numbers according to the logarithmic map turns the multiplication task into the addition task. This is the equivalent of the resultlogg(n1n2) = log g n1 + logg n2 on finite fields."
- Break condition: If p is not prime, GF(p) does not form a field and the bijection between multiplication and addition via exponential/logarithmic maps fails.

### Mechanism 2
- Claim: The analytical solutions for modular addition and multiplication can be combined to construct networks that generalize on arbitrary modular polynomials.
- Mechanism: Each term of the polynomial is handled by an "expert" network (either addition or multiplication), and their outputs are summed using another addition expert. Softmax with high temperature approximates one-hot vectors, allowing the subsequent addition expert to work correctly.
- Core assumption: The outputs of term-wise expert networks can be made sufficiently one-hot (via high-temperature softmax) to preserve the structure needed for correct summation.
- Evidence anchors:
  - [section] "We utilize the solutions presented in previous sections to construct a simple network that generalizes on this task... The expert networks are designed to perform best when their inputs are close to one hot vectors; so we apply (low temperature) Softmax function to the outputs of the each term."
  - [section] "We select a high value for the inverse temperature β ∼ 100 so that the intermediate outputs u get close to one hot vectors. This results in a higher accuracy in the summation of monomials to be performed in the subsequent layers."
- Break condition: If the polynomial contains terms that cannot be decomposed into sums/products of variables (e.g., cross-terms like n1*n2 + n1 + n2 without coefficients), the expert combination fails.

### Mechanism 3
- Claim: There exists a classification of modular polynomials into learnable and non-learnable categories based on whether they can be expressed in the form h(g1(n1) + g2(n2)) mod p.
- Mechanism: Polynomials of this form can be solved by composing analytical solutions for addition and multiplication. The hypothesis is supported by experimental evidence showing that networks generalize on learnable polynomials but fail on others.
- Core assumption: The learnability of a modular polynomial by a 2-layer MLP is determined by its algebraic structure, specifically whether it can be reduced to nested additions after applying field-specific transformations.
- Evidence anchors:
  - [section] "Hypothesis 5.1 (Weak generalization). Consider training a 2-layer MLP network... The network achieves 100% test accuracy if the modular polynomial is of the following form: h(g1(n1) + g2(n2)) mod p."
  - [section] "In Appendix C, we provide experimental evidence supporting this claim. We show the results of training experiments on various modular polynomials belonging to both categories. We see a clear difference in test performance on tasks that fall under the form equation 14 and those that do not."
- Break condition: If the hypothesis does not extend to deeper architectures or non-standard training procedures, the classification may not hold universally.

## Foundational Learning

- Concept: Finite fields GF(p) and their properties (cyclic groups, primitive roots).
  - Why needed here: The entire analytical solution relies on the algebraic structure of GF(p), particularly the bijection between multiplication and addition via exponential/logarithmic maps.
  - Quick check question: What is the order of the multiplicative group of GF(p) for prime p, and why does this matter for modular multiplication?

- Concept: Fourier analysis and periodicity in neural network weights.
  - Why needed here: The analytical solutions construct periodic weight matrices whose structure is verified using Inverse Participation Ratio (IPR) to measure similarity between analytical and trained networks.
  - Quick check question: How does the IPR measure the concentration of Fourier spectrum in a weight vector, and what value indicates perfect periodicity?

- Concept: Grokking phenomenon and generalization dynamics.
  - Why needed here: The paper's context is understanding why some modular polynomials are learnable while others are not, tied to the delayed generalization observed in grokking.
  - Quick check question: What distinguishes the training dynamics of learnable vs. non-learnable modular polynomials in terms of test accuracy over time?

## Architecture Onboarding

- Component map: Input → Term experts → Softmax → Summation expert → Output
- Critical path: Input layer (one-hot encoded variables) → Expert networks (addition/multiplication) → High-temperature softmax → Addition expert → Output layer (softmax over p classes)
- Design tradeoffs:
  - Width vs. Accuracy: Exponential increase in required width with number of terms due to cross-term suppression
  - Temperature vs. Approximation: Higher softmax temperature improves one-hot approximation but may introduce noise
  - Analytical vs. Learned weights: Analytical weights guarantee generalization but may be less robust to noise than learned ones
- Failure signatures:
  - Test accuracy plateaus at random chance level for non-learnable polynomials
  - IPR values remain low in trained networks, indicating lack of periodic structure
  - Softmax outputs are not sufficiently one-hot, causing errors in summation
- First 3 experiments:
  1. Train a 2-layer MLP on (n1 + n2) mod p with analytical weights vs. random initialization; compare IPR distributions.
  2. Construct and evaluate the expert network for (n1*n2) mod p; verify periodicity via IPR.
  3. Test the combined expert network on (n1*n2 + n1 + n2) mod p; measure accuracy and compare to standard MLP training.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the analytical solutions for modular arithmetic tasks be extended to other network architectures beyond 2-layer MLPs, such as deeper networks or transformers?
- Basis in paper: [inferred] The paper discusses extending analytical solutions to modular multiplication and addition with many terms, and constructing networks that generalize on arbitrary modular polynomials. It also mentions that deeper MLPs and transformers have been unable to solve general modular arithmetic tasks that do not fall under the proposed learnable form.
- Why unresolved: The paper only presents analytical solutions for 2-layer MLPs and hypothesizes that these solutions can be extended to other architectures, but does not provide experimental evidence or a formal proof.
- What evidence would resolve it: Experimental results showing that the analytical solutions can be successfully applied to deeper networks or transformers, achieving similar performance on modular arithmetic tasks.

### Open Question 2
- Question: What is the precise classification of modular polynomials into learnable and non-learnable categories, and how does this classification depend on the choice of network architecture and training methods?
- Basis in paper: [explicit] The paper hypothesizes a classification of modular polynomials into learnable and non-learnable categories via neural network training, and provides experimental evidence supporting the claim. It also mentions that the classification might depend on the network architecture.
- Why unresolved: The paper provides experimental evidence for the classification but does not offer a comprehensive proof or a formal definition of the learnable and non-learnable categories. It also does not explore how the classification might change with different network architectures or training methods.
- What evidence would resolve it: A formal proof or a more comprehensive experimental study that explores the classification across different network architectures and training methods, providing a clear definition of the learnable and non-learnable categories.

### Open Question 3
- Question: How do the analytical solutions for modular arithmetic tasks relate to the phenomenon of grokking, and can they provide insights into the mechanisms underlying this phenomenon?
- Basis in paper: [explicit] The paper mentions that the analytical solutions lead to networks that generalize on modular arithmetic tasks, which is related to the grokking phenomenon. It also discusses the use of inverse participation ratio (IPR) to quantify the similarity between the analytical solution and real trained networks.
- Why unresolved: The paper does not provide a detailed analysis of the relationship between the analytical solutions and the grokking phenomenon, nor does it explore how the solutions can shed light on the mechanisms underlying grokking.
- What evidence would resolve it: A detailed study that investigates the relationship between the analytical solutions and the grokking phenomenon, including an analysis of the training dynamics and the role of the solutions in enabling generalization.

## Limitations

- The learnability classification remains largely empirical without complete theoretical characterization of why the specific algebraic form enables generalization
- Analytical solutions assume prime fields GF(p), limiting applicability to composite moduli where exponential/logarithmic bijection fails
- Width requirements grow exponentially with number of terms, creating practical limitations for complex polynomials

## Confidence

- **High Confidence**: The mechanism for extending analytical solutions from modular addition to multiplication via exponential/logarithmic maps is well-supported by number-theoretic properties of finite fields. The IPR evidence for periodicity in trained networks is also strong.
- **Medium Confidence**: The experimental evidence supporting the learnability classification is convincing but limited in scope. The paper demonstrates clear performance differences between learnable and non-learnable polynomials but does not provide a complete theoretical characterization.
- **Low Confidence**: The claim that the expert combination approach generalizes to arbitrary modular polynomials is plausible but untested on more complex examples beyond the simple cases shown.

## Next Checks

1. **Cross-term suppression analysis**: Systematically measure how the required network width scales with the number of terms in the polynomial, particularly for polynomials with many cross-terms (e.g., n1*n2 + n1*n3 + n2*n3). This would quantify the practical limitations of the width explosion problem.

2. **Generalization to composite moduli**: Test whether the expert network approach can be adapted to work with composite moduli (Z_n for n composite) by developing alternative field-like structures or proving why such generalization is impossible.

3. **Deeper architecture validation**: Evaluate whether the learnability classification holds for deeper MLPs (3+ layers) or transformer architectures, which might bypass the algebraic constraints through different inductive biases.