---
ver: rpa2
title: Enhancing Taiwanese Hokkien Dual Translation by Exploring and Standardizing
  of Four Writing Systems
arxiv_id: '2403.12024'
source_url: https://arxiv.org/abs/2403.12024
tags:
- hokkien
- translation
- data
- language
- chinese
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops the first dual translation model between Taiwanese
  Hokkien and both Mandarin Chinese and English. It leverages orthographic similarities
  between Hokkien Han and Mandarin Chinese by continuing pre-training a LLaMA 2-7B
  model specialized in Mandarin Chinese.
---

# Enhancing Taiwanese Hokkien Dual Translation by Exploring and Standardizing of Four Writing Systems

## Quick Facts
- arXiv ID: 2403.12024
- Source URL: https://arxiv.org/abs/2403.12024
- Reference count: 0
- First dual translation model between Taiwanese Hokkien and both Mandarin Chinese and English

## Executive Summary
This paper introduces the first dual translation model between Taiwanese Hokkien and both Mandarin Chinese and English. The authors leverage orthographic similarities between Hokkien Han and Mandarin Chinese by pre-training on a LLaMA 2-7B model specialized in Traditional Mandarin Chinese. They conduct comprehensive experiments across various Hokkien writing systems and find that using monolingual corpora covering all systems improves translation performance. The study also demonstrates that standardizing all Hokkien monolingual corpora into Hokkien Han before continued pre-training yields slight performance improvements.

## Method Summary
The method involves continued pre-training of a pre-trained LLaMA 2-7B model specialized in Traditional Mandarin Chinese on monolingual Hokkien corpora across four writing systems (Hokkien Han, Tâi-lô, Peh-oe-jī, and Hàn-lô). The model is then fine-tuned on parallel datasets involving Hokkien and high-resource languages (ZH and EN). An optional standardization step converts all Hokkien monolingual data to Hokkien Han before continued pre-training. The model is evaluated using BLEU score, chrF++, and a GPT-4-based evaluation method incorporating back-translation.

## Key Results
- Pre-training on Mandarin Chinese specialized LLaMA 2 improves Hokkien Han translation through orthographic similarity
- Including monolingual corpora covering all Hokkien writing systems improves translation performance across all directions
- Standardizing all Hokkien monolingual corpora into Hokkien Han before continued pre-training results in slight performance improvements

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Pre-training on a Mandarin Chinese specialized LLaMA 2 model improves Hokkien Han translation by leveraging orthographic similarity.
- Mechanism: The shared Chinese character system between Mandarin Chinese and Hokkien Han allows the model to transfer knowledge of character meanings and usage patterns, reducing the learning burden for Hokkien Han.
- Core assumption: Hokkien Han and Mandarin Chinese share a significant portion of their vocabulary and character usage, making pre-training on Mandarin Chinese beneficial.
- Evidence anchors:
  - [abstract] "We employ a pre-trained LLaMA 2-7B model specialized in Traditional Mandarin Chinese to leverage the orthographic similarities between Taiwanese Hokkien Han and Traditional Mandarin Chinese."
  - [section 2.2] "Despite the commonality of Chinese characters between HAN and ZH, many homographs differ semantically."
  - [corpus] Limited evidence of shared character frequency analysis; assumption based on stated orthographic similarity.
- Break condition: If the semantic divergence of shared characters is too high, the benefit of orthographic similarity diminishes, and the model may learn incorrect mappings.

### Mechanism 2
- Claim: Including monolingual corpora covering all Hokkien writing systems improves translation performance across all directions.
- Mechanism: Exposure to diverse writing systems during pre-training allows the model to learn the relationships between different scripts and their corresponding pronunciations, improving its ability to translate between them.
- Core assumption: The model can learn the mapping between different writing systems by observing their usage in context within the monolingual data.
- Evidence anchors:
  - [abstract] "We find that the use of a limited monolingual corpus still further improves the model's Taiwanese Hokkien capabilities."
  - [section 5.1.1] "Incorporating all Hokkien data yields the best performance, particularly in POJ-related translations, with a 10 to 20 point increase in GPT-4 score."
  - [corpus] Monolingual data covers HAN, POJ, and HL, but lacks TL data (converted to POJ).
- Break condition: If the monolingual data is too small or unrepresentative of the language, the model may not learn the relationships between writing systems effectively.

### Mechanism 3
- Claim: Standardizing all Hokkien monolingual corpora into Hokkien Han before pre-training slightly improves HAN↔ZH and HAN↔EN translation performance.
- Mechanism: By standardizing all data to a single writing system, the model can focus on learning the relationships between Hokkien Han and other languages without the added complexity of multiple scripts.
- Core assumption: The model benefits from a consistent input format during pre-training, reducing the cognitive load of handling multiple writing systems.
- Evidence anchors:
  - [abstract] "Standardizing all Taiwanese Hokkien monolingual corpora into Hokkien Han before continued pre-training results in slight performance improvements."
  - [section 5.2] "The model pre-trained on standardized monolingual data on par or slightly outperforms the other models on average across both pairs of languages."
  - [corpus] Standardization process uses the best POJ-HAN translation model, but the quality of this model may impact the effectiveness of standardization.
- Break condition: If the standardization process introduces errors or loses important information, the performance improvement may be negated or reversed.

## Foundational Learning

- Concept: Jensen-Shannon Divergence (JSD) for corpus similarity analysis.
  - Why needed here: To assess the domain similarity of the HAN dataset and identify potential biases or inconsistencies in the data.
  - Quick check question: How does JSD differ from other similarity metrics like cosine similarity, and why is it more appropriate for this analysis?

- Concept: Back-translation for evaluation of low-resource language translation.
  - Why needed here: To enable reliable evaluation of translation quality for Hokkien, which GPT-4 may not fully understand.
  - Quick check question: What are the limitations of back-translation, and how does the proposed method address them?

- Concept: Few-shot learning and instruction tuning for fine-tuning LLaMA 2.
  - Why needed here: To adapt the pre-trained LLaMA 2 model to the specific task of Hokkien translation using limited parallel data.
  - Quick check question: How does instruction tuning differ from standard fine-tuning, and what are its advantages for this task?

## Architecture Onboarding

- Component map: Pre-trained LLaMA 2-7B model -> Monolingual Hokkien corpora -> Parallel Hokkien-HRL datasets -> Translation model fine-tuning pipeline -> GPT-4-based evaluation system

- Critical path: 1. Pre-train LLaMA 2 on monolingual Hokkien data 2. Fine-tune on parallel Hokkien-HRL datasets 3. Standardize monolingual data to HAN (optional) 4. Re-pre-train on standardized data (optional) 5. Evaluate using GPT-4-based method

- Design tradeoffs:
  - Vocabulary extension: Extending vocabulary improves POJ translation but may degrade other directions due to limited pre-training data.
  - Standardization: Standardizing to HAN simplifies the model's input but may lose information from other writing systems.
  - Parallel data selection: Focusing on HRL data improves cross-lingual alignment but may neglect Hokkien script translation.

- Failure signatures: Degraded performance in directions not involving the target language of the pre-trained model (e.g., ZH-POJ), overfitting to specific domains in the monolingual data, inability to handle out-of-vocabulary words or rare characters

- First 3 experiments: 1. Compare the performance of the base LLaMA 2 model with the pre-trained Hokkien model on a held-out test set. 2. Ablation study: Evaluate the impact of including different combinations of monolingual data (HAN only, HAN+POJ, HAN+POJ+HL) on translation performance. 3. Compare the performance of the model with and without vocabulary extension for POJ translation.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the translation performance vary when the monolingual Hokkien corpus is extended beyond the current size?
- Basis in paper: [inferred] The paper notes that extending the model's vocabulary for Taiwanese Hokkien did not yield improvements, which they attribute to the limited size of the pre-training corpus. They suggest that collecting a larger POJ corpus could be beneficial.
- Why unresolved: The study did not explore the impact of a significantly larger monolingual corpus on translation performance.
- What evidence would resolve it: Conducting experiments with a substantially larger monolingual Hokkien corpus and comparing the translation performance to the current results would provide insights into the impact of corpus size.

### Open Question 2
- Question: What is the impact of standardizing Hokkien monolingual corpora into Hokkien Han on translation performance for other Hokkien writing systems (e.g., POJ, HL)?
- Basis in paper: [explicit] The paper investigates the benefits of standardizing all Hokkien monolingual data into HAN before continued pre-training and observes slight improvements in HAN-related translations.
- Why unresolved: The study focuses on HAN translation improvements and does not explore the effects on other Hokkien writing systems.
- What evidence would resolve it: Evaluating the translation performance of models pre-trained on standardized Hokkien data for other writing systems (e.g., POJ, HL) would reveal the impact on these systems.

### Open Question 3
- Question: How does the translation quality differ when using different GPT-based evaluation methods, such as the one proposed by Kocmi and Federmann (2023), compared to the modified evaluation method incorporating back-translation?
- Basis in paper: [explicit] The paper introduces an evaluation method incorporating back-translation and GPT-4 to ensure reliable translation quality assessment for low-resource languages.
- Why unresolved: The study does not compare the proposed evaluation method with other GPT-based evaluation approaches.
- What evidence would resolve it: Conducting a comparative analysis of translation quality assessments using different GPT-based evaluation methods would provide insights into their effectiveness and reliability.

## Limitations
- Limited monolingual corpus size (approximately 500K sentences) may constrain the model's ability to fully capture the nuances of Hokkien
- Standardization process may introduce noise, particularly when converting TL to POJ using an existing tool
- GPT-4 evaluation method, while innovative, may not fully capture the nuances of Hokkien dialects and introduces uncertainties in assessing translation quality

## Confidence
- High Confidence: Pre-training on Mandarin Chinese specialized LLaMA 2 improves Hokkien Han translation; including monolingual corpora covering all Hokkien writing systems improves translation performance; standardizing monolingual corpora to Hokkien Han provides slight improvements
- Medium Confidence: GPT-4-based evaluation method reliably assesses translation quality for low-resource languages; vocabulary extension improves POJ translation

## Next Checks
1. Conduct detailed error analysis of the standardization process to quantify conversion errors and their impact on translation quality
2. Validate the GPT-4-based evaluation method by comparing its assessments with human evaluations on a subset of translations
3. Perform a granular ablation study varying the amount of monolingual data for each writing system to determine the optimal data mix for balanced performance across all translation directions