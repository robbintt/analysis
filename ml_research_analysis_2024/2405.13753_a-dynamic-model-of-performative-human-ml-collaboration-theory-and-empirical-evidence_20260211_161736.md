---
ver: rpa2
title: 'A Dynamic Model of Performative Human-ML Collaboration: Theory and Empirical
  Evidence'
arxiv_id: '2405.13753'
source_url: https://arxiv.org/abs/2405.13753
tags:
- performance
- human
- collaborative
- utility
- knapsack
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops a theoretical framework for understanding dynamic
  human-ML collaboration where ML models are trained on human decisions that may deviate
  from ground truth, and human decisions are influenced by ML recommendations. The
  authors introduce a "collaborative characteristic function" that maps ML performance
  to human+ML performance, showing that this dynamic system can converge to stable
  points that may be suboptimal relative to ground truth.
---

# A Dynamic Model of Performative Human-ML Collaboration: Theory and Empirical Evidence

## Quick Facts
- arXiv ID: 2405.13753
- Source URL: https://arxiv.org/abs/2405.13753
- Reference count: 40
- Primary result: Dynamic human-ML collaboration can converge to stable equilibria that are suboptimal relative to ground truth

## Executive Summary
This paper develops a theoretical framework for understanding dynamic human-ML collaboration where ML models are trained on human decisions that may deviate from ground truth, and human decisions are influenced by ML recommendations. The authors introduce a "collaborative characteristic function" that maps ML performance to human+ML performance, showing that this dynamic system can converge to stable points that may be suboptimal relative to ground truth. As a proof of concept, they conduct a user study with 1,408 participants solving knapsack problems with varying quality ML recommendations and monetary incentives. Key findings include humans improving upon ML recommendations for many performance levels, monetary incentives not affecting human performance, and empirical data suggesting the system would reach an equilibrium at approximately 92% of maximum knapsack value.

## Method Summary
The authors develop a theoretical framework for performative human-ML collaboration by modeling the system as a dynamic process where ML models are trained on human decisions that may deviate from ground truth, while human decisions are influenced by ML recommendations. They introduce a collaborative characteristic function that captures the relationship between ML performance and combined human-ML performance. The empirical validation involves a user study with 1,408 participants solving knapsack problems under different conditions: varying quality of ML recommendations and the presence or absence of monetary incentives. The study measures human performance with and without ML recommendations to understand how humans interact with and potentially improve upon ML suggestions.

## Key Results
- Humans improve upon ML recommendations for many performance levels
- Monetary incentives do not affect human performance in the knapsack task
- Empirical data suggests the system would reach an equilibrium at approximately 92% of maximum knapsack value

## Why This Works (Mechanism)
The framework works by modeling the recursive feedback loop between human decisions and ML model updates. When humans make decisions that deviate from ground truth (due to various factors like bounded rationality or strategic behavior), and these decisions are used to train the ML model, the model learns to predict human behavior rather than ground truth. As the model improves at predicting human decisions, it provides recommendations that humans find increasingly useful, creating a self-reinforcing cycle. The collaborative characteristic function captures how improvements in ML performance translate to improvements in the combined human-ML system performance, allowing the identification of stable equilibria where neither component has incentive to change their behavior.

## Foundational Learning

**Performative Prediction** - Why needed: To understand how ML models change the data distribution through their predictions when humans act on recommendations. Quick check: Can we identify when the model is predicting human behavior vs. ground truth?

**Collaborative Characteristic Function** - Why needed: To map the relationship between ML performance and combined human+ML performance in a single mathematical object. Quick check: Does the function capture diminishing returns in human-ML collaboration?

**Stable Equilibria in Dynamic Systems** - Why needed: To identify when the human-ML collaboration system reaches a state where neither component changes behavior. Quick check: Are there multiple equilibria and how do we identify the globally optimal one?

## Architecture Onboarding

Component Map: Human Decision-Maker -> ML Model -> Recommendation -> Human Decision-Maker

Critical Path: Human decision → ML training data → model update → new recommendation → human decision → ...

Design Tradeoffs: Ground truth access vs. realistic deployment scenarios; theoretical tractability vs. empirical complexity; controlled experiments vs. real-world generalizability

Failure Signatures: Convergence to suboptimal equilibria; humans becoming overly dependent on ML recommendations; ML models learning to predict human biases rather than ground truth

First Experiments:
1. Test the framework with a simple linear regression task where humans can easily verify ground truth
2. Implement a synthetic environment with known ground truth to validate convergence properties
3. Conduct a small-scale pilot study with domain experts to assess the framework's applicability to professional decision-making

## Open Questions the Paper Calls Out
None

## Limitations

- The theoretical framework relies on assumptions about human rationality and the stability of the collaborative characteristic function that may not hold in real-world settings
- The empirical validation is based on a single experimental paradigm (knapsack problems) that may not generalize to other domains
- The study cannot definitively distinguish between learning trajectories that improve versus deteriorate relative to ground truth without ground truth access

## Confidence

**High Confidence:** The mathematical framework for performative prediction with human decision-makers is well-developed and internally consistent. The convergence properties and existence of stable equilibria are theoretically sound given the stated assumptions.

**Medium Confidence:** The empirical evidence from the user study provides reasonable support for the theoretical predictions, particularly regarding the equilibrium behavior. However, the sample size and task specificity limit generalizability.

**Low Confidence:** The claim that human performance improves upon ML recommendations across many performance levels needs more diverse task validation. The conclusion about monetary incentives having no effect requires replication with different incentive structures and domains.

## Next Checks

1. Replicate the study across multiple task domains (e.g., medical diagnosis, financial decision-making, content moderation) to assess generalizability of the collaborative characteristic function and equilibrium predictions.

2. Conduct experiments with ground truth access to empirically validate whether the identified stable points are indeed suboptimal relative to ground truth, and measure the magnitude of potential degradation.

3. Test the framework with alternative incentive structures (performance-based rewards, team-based incentives, time pressure) to determine the boundary conditions for when monetary incentives affect human-ML collaboration dynamics.