---
ver: rpa2
title: Empowering Graph Invariance Learning with Deep Spurious Infomax
arxiv_id: '2407.11083'
source_url: https://arxiv.org/abs/2407.11083
tags:
- spurious
- learning
- features
- graph
- invariant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles out-of-distribution (OOD) generalization in
  graph neural networks by proposing a novel learning paradigm and framework called
  EQuAD. The key insight is that maximizing mutual information (infomax) provably
  encourages the model to learn spurious features, regardless of their correlation
  strength with labels.
---

# Empowering Graph Invariance Learning with Deep Spurious Infomax

## Quick Facts
- arXiv ID: 2407.11083
- Source URL: https://arxiv.org/abs/2407.11083
- Reference count: 40
- Key outcome: EQuAD achieves state-of-the-art performance on 7 synthetic datasets (up to 31.76% improvement) and 8 real-world graph datasets, outperforming environment inference, augmentation, and other invariance learning approaches.

## Executive Summary
This paper addresses out-of-distribution (OOD) generalization in graph neural networks by introducing EQuAD, a novel framework that leverages the observation that mutual information maximization (infomax) provably encourages learning spurious features regardless of their correlation strength with labels. The key insight is that when the spurious subgraph dominates the invariant subgraph, infomax preferentially encodes spurious features. EQuAD exploits this by first using infomax-based self-supervised learning to extract spurious features, then quantifying their correlation with labels using classifiers, and finally decorrelating them from invariant features through a tailored loss function. This approach achieves state-of-the-art performance on both synthetic and real-world graph datasets without requiring assumptions about spurious correlation strengths.

## Method Summary
EQuAD is a three-stage framework that addresses OOD generalization in graph neural networks by learning to decorrelate spurious and invariant features. The method consists of: (1) Encoding - using infomax-based self-supervised learning to obtain representations dominated by spurious features; (2) Quantifying - training classifiers on these representations to generate logits that reflect spurious-label correlation; and (3) Decorrelating - retraining a GNN with sample-specific and model-specific reweighting to maximize entropy of logits conditioned on invariant representations, forcing decorrelation. The framework is built on the theoretical insight that infomax representations preferentially capture spurious features when the spurious subgraph is larger than the invariant subgraph, and it achieves this without requiring prior knowledge of correlation strengths.

## Key Results
- Achieves up to 31.76% improvement on synthetic datasets compared to state-of-the-art invariance learning methods
- Outperforms environment inference and augmentation approaches on 8 real-world graph datasets
- Demonstrates robust performance across varying degrees of spurious correlation without requiring assumptions about correlation strengths
- Shows stable performance on challenging real-world datasets including DrugOOD, MolBACE, and MolBBBP

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Maximizing mutual information (infomax) provably encourages the model to learn spurious features regardless of their correlation strength with labels.
- Mechanism: The infomax principle maximizes mutual information between global and local representations. When the spurious subgraph (|Gs|) is larger than the invariant subgraph (|Gc|), the global representations preferentially encode spurious features because the infomax objective rewards information present across all nodes, which spurious structures dominate.
- Core assumption: The spurious subgraph is typically larger than the invariant subgraph in real-world graphs (|Gs|/|Gc| > δr/δl).
- Evidence anchors:
  - [abstract]: "The paradigm is built upon the observation that the infomax principle encourages learning spurious features regardless of spurious correlation strengths."
  - [section]: Theorem 4.1 formalizes this claim, showing that under certain entropy conditions, infomax representations exclusively contain spurious features when the spurious subgraph dominates.
  - [corpus]: Weak evidence - corpus neighbors focus on invariant learning but don't directly discuss infomax's spurious feature capture.
- Break condition: If |Gc| > |Gs| (invariant subgraph dominates), infomax would instead preferentially learn invariant features.

### Mechanism 2
- Claim: EQuAD learns invariant features by first extracting spurious features via infomax, then decorrelating them from invariant features.
- Mechanism: The framework follows three steps: (1) Encoding - use infomax-based SSL to obtain representations dominated by spurious features; (2) Quantifying - train classifiers on these representations to generate logits that reflect spurious-label correlation; (3) Decorrelation - train a new GNN to maximize entropy of logits conditioned on invariant representations, forcing decorrelation.
- Core assumption: Spurious features can be isolated through infomax and used as targets for decorrelation without needing prior knowledge of correlation strengths.
- Evidence anchors:
  - [abstract]: "EQuAD leverages this by first encoding spurious features via infomax-based self-supervised learning, then quantifying their correlation with labels using classifiers, and finally decorrelating them from invariant features through a tailored loss function."
  - [section]: The three-step framework is detailed with theoretical justification (Theorem 5.1) showing the decorrelation loss achieves unique invariant solutions.
  - [corpus]: Weak evidence - corpus neighbors discuss invariant learning but not the specific infomax-then-decorrelate approach.
- Break condition: If the quantifying step fails to accurately reflect spurious-label correlation (e.g., classifiers capture invariant features), decorrelation won't work.

### Mechanism 3
- Claim: Sample and model-specific reweighting in the decorrelation step mitigates data imbalance and improves OOD generalization.
- Mechanism: Minority samples (those with low spurious-label correlation) are upweighted using w(siy;γ) = 1 - sγiy, and model-specific reweighting uses validation performance to prioritize higher-quality logits matrices. This balances the entropy maximization objective across correlation levels.
- Core assumption: Minority samples (low spurious correlation) are underrepresented and need upweighting to achieve balanced decorrelation.
- Evidence anchors:
  - [section]: "To address this, we propose a novel sample reweighting approach that increases the weight of the minority group to achieve a more balanced distribution of data across different levels of correlation under the same target label Y."
  - [abstract]: "EQuAD shows stable and enhanced performance across different degrees of bias in synthetic datasets and challenging real-world datasets up to 31.76%."
  - [corpus]: Weak evidence - corpus neighbors don't discuss reweighting strategies for invariant learning.
- Break condition: If the reweighting function is too aggressive (γ too small) or too conservative (γ too large), it may either destabilize training or fail to address imbalance.

## Foundational Learning

- Concept: Mutual Information Maximization (Infomax)
  - Why needed here: Forms the theoretical basis for spurious feature extraction - understanding how maximizing MI between global and local representations preferentially captures spurious features when they dominate the graph.
  - Quick check question: Why does the infomax principle favor spurious features when |Gs| > |Gc|?

- Concept: Invariance Principle from Causality
  - Why needed here: The core motivation - invariant features remain stable across environments while spurious features change. EQuAD builds on this by learning to identify and remove spurious features.
  - Quick check question: What distinguishes invariant features from spurious features in the data generating process?

- Concept: Data Imbalance and Reweighting
  - Why needed here: Real datasets often have imbalanced spurious correlation levels. Understanding reweighting schemes is crucial for the decorrelation step's effectiveness.
  - Quick check question: How does the sample-specific reweighting function w(siy;γ) address the minority group problem?

## Architecture Onboarding

- Component map: Encoding (GNN + infomax SSL) -> Quantifying (Linear classifiers + probability calibration) -> Decorrelation (New GNN + sample/model reweighting)
- Critical path: The success critically depends on Step 1 producing spurious-dominated representations, Step 2 accurately quantifying spurious-label correlation, and Step 3 effectively decorrelating without losing invariant information.
- Design tradeoffs: Using infomax SSL for spurious feature extraction is theoretically elegant but computationally heavier than direct approaches. The multi-stage design adds complexity but provides robustness to varying correlation strengths.
- Failure signatures: Poor performance on OOD data suggests Step 1 failed to capture spurious features, Step 2 misquantified correlations, or Step 3's reweighting is unbalanced. High training accuracy but low OOD accuracy indicates spurious feature leakage.
- First 3 experiments:
  1. Run encoding step on SPMotif-0.60 and verify that feature reweighting post-infomax SSL yields much lower test accuracy than ERM (confirming spurious feature capture).
  2. Test decorrelation with and without sample reweighting on EC50 datasets to confirm reweighting improves minority group performance.
  3. Vary λ in decorrelation loss and observe performance degradation when λ ≥ 1.0, validating the theoretical constraint.

## Open Questions the Paper Calls Out

- Question: Does the EQuAD framework generalize effectively to other data modalities beyond graph-structured data, such as vision or natural language?
  - Basis in paper: [explicit] The authors state that the learning paradigm "holds great potential for adaptation to other data modalities, such as vision and natural language, which we leave for our future work."
  - Why unresolved: The paper focuses solely on graph-structured data and does not provide any empirical evidence or theoretical analysis for other modalities.
  - What evidence would resolve it: Empirical results demonstrating the effectiveness of EQuAD on vision or natural language tasks, or theoretical analysis showing how the framework can be adapted to these modalities.

- Question: How does the performance of EQuAD vary with different graph neural network architectures as the backbone encoder?
  - Basis in paper: [inferred] The paper uses GIN as the backbone encoder and mentions that "more expressive GNN architectures... may further enhances as the expressive power increases," but does not provide empirical results.
  - Why unresolved: The paper does not explore the impact of different GNN architectures on EQuAD's performance, leaving this question open for investigation.
  - What evidence would resolve it: Empirical results comparing EQuAD's performance using different GNN architectures (e.g., GAT, Gated GNN, subgraph-based GNNs) on the same datasets.

- Question: What is the impact of different self-supervised learning algorithms in the Encoding stage on the quality of spurious features learned and overall EQuAD performance?
  - Basis in paper: [explicit] The authors mention that "off-the-shelf algorithms can serve as plug-ins for specific implementations for each step" and provide an ablation study comparing GraphCL and Infomax in the Encoding stage.
  - Why unresolved: While the authors show that Infomax outperforms GraphCL, they do not explore other self-supervised learning algorithms or provide a comprehensive comparison.
  - What evidence would resolve it: Empirical results comparing EQuAD's performance using various self-supervised learning algorithms (e.g., contrastive learning, masked autoencoders, generative models) in the Encoding stage.

## Limitations

- Theoretical assumptions about entropy conditions and correlation structures have not been empirically validated across diverse graph topologies
- Three-stage framework significantly increases computational overhead compared to standard GNN training
- Effectiveness on graphs with novel spurious patterns not captured during the infomax encoding phase remains unclear

## Confidence

- High Confidence: The core mechanism that infomax-based SSL preferentially captures spurious features when |Gs| > |Gc| is theoretically grounded and supported by experimental results on synthetic datasets
- Medium Confidence: The effectiveness of the quantifying and decorrelation steps is supported by results on synthetic and real-world datasets, but theoretical guarantees assume idealized conditions
- Low Confidence: The robustness of EQuAD to varying spurious correlation strengths across completely different graph domains has limited validation

## Next Checks

1. **Ablation on Infomax Component**: Remove the infomax SSL step and directly apply sample reweighting to a standard GNN. Compare performance to validate whether the infomax encoding is essential for capturing spurious features.

2. **Cross-Domain Generalization**: Test EQuAD on a dataset where the spurious correlation structure differs significantly from training environments (e.g., different graph motifs or node attributes) to assess robustness to novel spurious patterns.

3. **Theoretical Constraint Validation**: Systematically vary λ in the decorrelation loss (including values ≥ 1.0) and measure the impact on invariant feature learning and OOD performance to empirically verify the theoretical constraint that λ < 1.0.