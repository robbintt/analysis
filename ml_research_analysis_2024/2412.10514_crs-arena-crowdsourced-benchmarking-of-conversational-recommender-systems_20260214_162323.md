---
ver: rpa2
title: 'CRS Arena: Crowdsourced Benchmarking of Conversational Recommender Systems'
arxiv_id: '2412.10514'
source_url: https://arxiv.org/abs/2412.10514
tags:
- crss
- arena
- systems
- user
- users
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: CRS Arena is a crowdsourced benchmarking platform for conversational
  recommender systems (CRSs) that enables scalable evaluation through pairwise "battles"
  between anonymous CRSs. Users interact with two CRSs sequentially before declaring
  a winner or draw, collecting conversations and feedback.
---

# CRS Arena: Crowdsourced Benchmarking of Conversational Recommender Systems

## Quick Facts
- **arXiv ID**: 2412.10514
- **Source URL**: https://arxiv.org/abs/2412.10514
- **Reference count**: 21
- **Primary result**: Collected 474 conversations across open and closed crowdsourcing environments; Elo-based rankings showed strong correlation between environments (r = 0.763, ρ = 0.700) but poor correlation with recommendation metrics

## Executive Summary
CRS Arena is a crowdsourced benchmarking platform that evaluates conversational recommender systems through pairwise "battles" where users interact with two anonymous CRSs and declare a winner. The platform collected 474 conversations from nine CRSs across open and closed crowdsourcing environments, generating the CRSArena-Dial dataset. While Elo-based rankings were highly consistent across environments and aligned with user satisfaction, they showed negative correlation with traditional recommendation metrics, suggesting that user satisfaction doesn't directly reflect recommendation performance. User satisfaction remained low across all systems, with top performers achieving only 52.1% satisfaction.

## Method Summary
The platform implements a pairwise battle format where users interact sequentially with two anonymous CRSs before selecting a winner or draw. This design enables scalable evaluation while collecting both conversation data and explicit feedback. The system uses Elo ratings to rank CRS performance based on battle outcomes. The study deployed in both open and closed crowdsourcing environments, comparing results across these settings. Nine CRSs were evaluated using this framework, with conversations collected and analyzed for both quantitative metrics and qualitative insights about user experience.

## Key Results
- Elo-based rankings showed strong correlation between open and closed crowdsourcing environments (r = 0.763, ρ = 0.700)
- Elo rankings aligned with user satisfaction ratings (ρ = 0.917) but negatively correlated with R@10 recall metrics (ρ = -0.238)
- User satisfaction remained low across all CRSs, with top performers achieving only 52.1% satisfaction
- The CRSArena-Dial dataset containing 474 real user conversations is publicly available for research

## Why This Works (Mechanism)
The pairwise battle format creates a controlled comparison environment where users directly experience multiple CRSs, generating rich feedback about relative performance. By anonymizing the systems and collecting explicit winner selections, the platform captures genuine user preferences rather than just task completion metrics. The Elo rating system provides a robust ranking mechanism that adapts to the competitive nature of pairwise comparisons while handling varying numbers of battles per system. This approach scales effectively compared to traditional one-on-one evaluations while maintaining meaningful comparative insights.

## Foundational Learning

**Crowdsourced benchmarking** - Why needed: Enables scalable, diverse evaluation of CRSs across different user populations and use cases. Quick check: Compare result consistency between open and closed crowdsourcing environments.

**Pairwise comparison methodology** - Why needed: Reduces cognitive load on users while providing relative performance insights. Quick check: Verify that winner selections align with qualitative feedback about system strengths.

**Elo rating system** - Why needed: Provides adaptive ranking that accounts for varying numbers of comparisons and opponent strengths. Quick check: Monitor rating stability over time with additional battles.

**User satisfaction metrics** - Why needed: Captures subjective experience beyond objective task completion. Quick check: Correlate satisfaction scores with specific interaction patterns or system behaviors.

**Recommendation performance metrics** - Why needed: Provides objective baseline for comparing subjective user feedback. Quick check: Analyze divergence between satisfaction and recall metrics to identify evaluation gaps.

## Architecture Onboarding

**Component map**: User Interface -> CRS Pair Selection -> Battle Execution -> Elo Update -> Feedback Collection -> Dataset Storage

**Critical path**: User Interface -> CRS Pair Selection -> Battle Execution -> Elo Update -> Winner Selection

**Design tradeoffs**: The pairwise format trades absolute performance measurement for scalable relative comparison, potentially introducing comparison effects that don't reflect isolated system capabilities.

**Failure signatures**: Low participation rates suggest interface issues; inconsistent Elo rankings across environments indicate sampling bias; poor correlation between satisfaction and metrics suggests evaluation misalignment.

**3 first experiments**:
1. Run controlled battles between two well-understood CRSs to validate Elo ranking behavior
2. Compare satisfaction scores with detailed conversation analysis to identify satisfaction drivers
3. Test correlation between battle outcomes and traditional one-on-one evaluation results

## Open Questions the Paper Calls Out

The paper highlights several uncertainties: whether Elo-based rankings accurately capture long-term CRS performance given the relatively small sample size of 474 conversations; whether low user satisfaction scores reflect fundamental CRS limitations or experimental setup issues; whether the pairwise battle format introduces bias through comparison effects that over- or under-estimate individual system capabilities; and whether the discrepancy between recommendation performance metrics and user satisfaction indicates that Elo rankings represent true user experience or are influenced by factors beyond recommendation quality.

## Limitations

- Relatively small sample size (474 conversations) may limit Elo ranking stability and generalizability
- Pairwise battle format may introduce comparison effects that bias individual system evaluation
- Low user satisfaction across all systems raises questions about CRS technology readiness or experimental conditions
- Discrepancy between satisfaction metrics and recommendation performance suggests evaluation framework limitations

## Confidence

**High confidence**: Platform implementation and data collection methodology, correlation between crowdsourcing environments, negative correlation between Elo and recall metrics

**Medium confidence**: User satisfaction measurements, comparative performance rankings, dataset quality and utility

**Low confidence**: Long-term validity of Elo rankings, generalizability to different CRS types, interpretation of satisfaction-recall disconnect

## Next Checks

1. Conduct longitudinal studies with repeated user interactions to verify Elo ranking stability over time
2. Implement isolated system evaluations to disentangle comparison effects from absolute performance
3. Perform ablation studies to identify which aspects of CRS interaction most strongly influence user satisfaction versus recommendation accuracy