---
ver: rpa2
title: 'Fighting crime with Transformers: Empirical analysis of address parsing methods
  in payment data'
arxiv_id: '2404.05632'
source_url: https://arxiv.org/abs/2404.05632
tags: []
core_contribution: This paper addresses the challenge of address parsing in financial
  payment data, where extracting fields like street, postal code, or country from
  free text is crucial for regulatory compliance. The authors compare various state-of-the-art
  methods, including rule-based approaches, deep learning models, and generative Large
  Language Models (LLMs), using both synthetic and real-world noisy transactional
  data.
---

# Fighting crime with Transformers: Empirical analysis of address parsing methods in payment data

## Quick Facts
- arXiv ID: 2404.05632
- Source URL: https://arxiv.org/abs/2404.05632
- Reference count: 17
- A well fine-tuned Transformer model using early stopping significantly outperforms other approaches on synthetic zero-shot data and production data.

## Executive Summary
This paper addresses the challenge of address parsing in financial payment data, where extracting fields like street, postal code, or country from free text is crucial for regulatory compliance. The authors compare various state-of-the-art methods, including rule-based approaches, deep learning models, and generative Large Language Models (LLMs), using both synthetic and real-world noisy transactional data. They introduce an augmented dataset to better reflect real-world conditions and benchmark the performance of different models. The results show that a well fine-tuned Transformer model using early stopping significantly outperforms other approaches, achieving state-of-the-art performance on synthetic zero-shot data and production data.

## Method Summary
The authors compare rule-based approaches (LibPostal, DeepParse), deep learning models (including Transformer-based models), and generative LLMs on synthetic and real-world noisy transactional data. They introduce an augmented dataset (V0, V1, V2) with address structure masking and labeling augmentation to better reflect real-world conditions. The evaluation uses F1 score on synthetic zero-shot data and production data, with precision and recall averaged over all tags. The paper employs BIO tagging schema for token classification and explores early stopping on zero-shot validation loss to prevent overfitting during training.

## Key Results
- Transformer models with early stopping outperform rule-based and CRF models on multilingual address parsing.
- Data augmentation with address structure masking and OOA labels improves model robustness to real-world address variability.
- Generative LLMs demonstrate strong zero-shot performance but do not match encoder transformer models in overall accuracy.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Early stopping on zero-shot validation loss prevents overfitting and improves generalization to noisy production data.
- Mechanism: The model monitors cross-entropy loss on zero-shot data during training and halts when performance degrades, preserving weights that best generalize beyond the training distribution.
- Core assumption: Zero-shot data distribution is sufficiently similar to production data to serve as a proxy for overfitting detection.
- Evidence anchors:
  - [abstract]: "Our results suggest that a well fine-tuned Transformer model using early-stopping significantly outperforms other approaches."
  - [section 4.3.2]: "Figure 1 suggests overfitting, prompting us to experiment with early stopping, using cross-entropy loss on zero-shot data as the stopping metric."
  - [corpus]: Weak evidence; corpus does not contain direct validation of early stopping.

### Mechanism 2
- Claim: Data augmentation with address structure masking and OOA labels improves model robustness to real-world address variability.
- Mechanism: By randomly reordering address elements and inserting non-address tokens, the model learns to handle out-of-order fields and irrelevant content, reducing overfitting to clean, structured formats.
- Core assumption: The synthetic augmented dataset sufficiently captures the distribution of noise and variability in real-world payment messages.
- Evidence anchors:
  - [section 3.1]: "This variation aims to prevent the models from overfitting to country-specific address structures, addressing a concern highlighted by Yassine et al. (2021)."
  - [section 4.3.2]: "Removing the prefix yields a slightly better score, but a more notable improvement comes with the early stopping."
  - [corpus]: Weak evidence; corpus neighbors discuss crime prediction and AML, not address parsing robustness.

### Mechanism 3
- Claim: Transformer models outperform rule-based and CRF models on multilingual address parsing due to their ability to learn contextual representations from large datasets.
- Mechanism: Transformers capture complex dependencies between words in an address, enabling accurate parsing even when the format deviates from country-specific standards.
- Core assumption: The training data is sufficiently large and diverse to allow the model to learn robust patterns across multiple countries and address formats.
- Evidence anchors:
  - [abstract]: "Our results suggest that a well fine-tuned Transformer model using early-stopping significantly outperforms other approaches."
  - [section 4.3.1]: "The results reported in 5... show that the only variation that substantially improved the model's performance is the addition of early stopping."
  - [corpus]: Weak evidence; corpus neighbors discuss unrelated tasks like crime prediction and generative advertising.

## Foundational Learning

- Concept: Token classification with BIO tagging schema
  - Why needed here: Address parsing requires labeling each word in a free-text address with its corresponding field (e.g., street name, postal code), and the BIO schema captures both the start and continuation of multi-word entities.
  - Quick check question: What is the difference between "B-StreetName" and "I-StreetName" in the BIO schema?

- Concept: Data augmentation for robustness
  - Why needed here: Real-world payment data contains noise such as missing fields, out-of-order elements, and irrelevant content; augmentation simulates these conditions during training.
  - Quick check question: How does the "OOA" (Out-Of-Address) tag help the model handle irrelevant content in payment messages?

- Concept: Early stopping to prevent overfitting
  - Why needed here: Training on synthetic data risks overfitting to its structure; early stopping on zero-shot validation loss helps preserve generalization.
  - Quick check question: Why is zero-shot data used as the validation set for early stopping instead of the test set?

## Architecture Onboarding

- Component map: Tokenizer -> Transformer Encoder -> Linear Classifier -> Output Labels
- Critical path: Input text -> Tokenization -> Model forward pass -> Label prediction -> Post-processing (BIO to structured fields)
- Design tradeoffs: Larger models (e.g., XLM-RoBERTa-Large) achieve higher accuracy but increase inference cost; smaller models (e.g., DistilBERT) are faster but slightly less accurate.
- Failure signatures: High variance between folds indicates instability; poor zero-shot performance suggests overfitting to synthetic data; hallucinations in LLM outputs indicate unreliable parsing.
- First 3 experiments:
  1. Train DistilBERT on V0 data and evaluate on zero-shot data to establish baseline performance.
  2. Apply early stopping on zero-shot validation loss and compare performance to baseline.
  3. Test data augmentation (V2) with early stopping to measure robustness improvements.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the address parsing performance change if the models were trained on data with varying levels of address noise and structure complexity?
- Basis in paper: [inferred] The paper discusses the importance of training robust models that can handle real-world noisy transactional data and mentions the use of data augmentation techniques to mimic real-life data structure.
- Why unresolved: The paper does not provide a detailed analysis of how different levels of noise and structure complexity in the training data would impact the model's performance.
- What evidence would resolve it: Conducting experiments with training data that varies in noise levels and structure complexity, and comparing the model's performance across these variations.

### Open Question 2
- Question: What is the impact of using context-free grammars or knowledge graphs to improve the output structure and accuracy of generative LLMs in address parsing?
- Basis in paper: [explicit] The paper suggests that generative LLMs have potential for improvement and mentions the use of context-free grammars such as the LMQL library and the introduction of geography knowledge with a LlamaIndex as possible directions for further improvement.
- Why unresolved: The paper does not explore or provide results on the use of context-free grammars or knowledge graphs to enhance the performance of generative LLMs in address parsing.
- What evidence would resolve it: Implementing and testing generative LLMs with context-free grammars or knowledge graphs and evaluating their impact on the output structure and accuracy.

### Open Question 3
- Question: How does the performance of address parsing models vary across different languages and writing systems beyond the Latin alphabet?
- Basis in paper: [inferred] The paper mentions that the data used for training and testing is limited to addresses written in the Latin alphabet, and there is no indication of the country in the dataset, which is a limitation for regulatory purposes.
- Why unresolved: The paper does not address the performance of address parsing models on addresses written in non-Latin alphabets or different languages.
- What evidence would resolve it: Conducting experiments with address parsing models trained on multilingual data that includes various writing systems and evaluating their performance across different languages.

## Limitations

- The evaluation relies heavily on synthetic data augmented with address structure masking and OOA labels, with uncertain alignment to actual production data distribution.
- The comparison between generative LLMs and encoder-only transformers may be incomplete due to inconsistent fine-tuning approaches and evaluation protocols.
- The study does not address address parsing performance on non-Latin writing systems or languages beyond those represented in the training data.

## Confidence

**High Confidence**: The core finding that Transformer models outperform rule-based approaches (LibPostal, DeepParse) on address parsing tasks is well-supported by direct experimental comparisons across multiple metrics and datasets.

**Medium Confidence**: The claim that early stopping based on zero-shot validation loss prevents overfitting is supported by observed performance improvements, but the mechanism's effectiveness depends on the zero-shot data distribution matching production data, which is not empirically validated.

**Low Confidence**: The assertion that generative LLMs "warrant further investigation" based on strong zero-shot performance is premature given the methodological differences in fine-tuning approaches and evaluation protocols between generative and encoder models.

## Next Checks

1. **Distribution Alignment Analysis**: Quantitatively compare the statistical properties of synthetic augmented data versus real production data to validate that augmentation techniques capture the actual noise patterns observed in payment messages.

2. **Controlled Generative Model Comparison**: Re-run the generative LLM experiments with consistent fine-tuning approaches (either fully fine-tuned or adapter-based) across all models to isolate the architectural differences from training methodology effects.

3. **Cross-Country Performance Validation**: Evaluate model performance across different country address formats not represented in the training data to assess true multilingual generalization beyond the reported improvements from country name insertion augmentation.