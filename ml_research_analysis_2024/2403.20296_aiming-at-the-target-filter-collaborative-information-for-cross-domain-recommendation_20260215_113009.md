---
ver: rpa2
title: 'Aiming at the Target: Filter Collaborative Information for Cross-Domain Recommendation'
arxiv_id: '2403.20296'
source_url: https://arxiv.org/abs/2403.20296
tags:
- target
- user
- domain
- transfer
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses negative transfer in cross-domain recommendation,
  where irrelevant information from source domains degrades target domain performance.
  The proposed Collaborative information regularized User Transformation (CUT) framework
  explicitly filters source-domain collaborative information by using target-domain
  user similarity as a constraint.
---

# Aiming at the Target: Filter Collaborative Information for Cross-Domain Recommendation

## Quick Facts
- **arXiv ID**: 2403.20296
- **Source URL**: https://arxiv.org/abs/2403.20296
- **Reference count**: 40
- **Primary result**: CUT achieves up to 15% better Recall and 18% better NDCG compared to best baseline

## Executive Summary
Cross-domain recommendation systems often suffer from negative transfer, where irrelevant information from source domains degrades target domain performance. This paper proposes the Collaborative information regularized User Transformation (CUT) framework, which explicitly filters source-domain collaborative information by using target-domain user similarity as a constraint. CUT consists of two phases: learning user similarity from the target domain and transferring information from the source domain while preserving target-domain user relationships through a user transformation layer and contrastive loss. Experiments on six cross-domain tasks from Amazon and Douban datasets show significant improvements over state-of-the-art methods.

## Method Summary
CUT addresses negative transfer in cross-domain recommendation through a two-phase framework. First, it learns user similarity relationships from the target domain using cosine similarity with a threshold Œ≥. Second, it transfers information from the source domain while preserving these target-domain relationships through a user transformation layer and contrastive loss. For overlapping users, CUT learns separate source and target embeddings connected by a transformation layer that models domain differences. The contrastive loss ensures users who are similar in the target domain remain similar after transformation, even when incorporating source-domain information. The framework combines target loss, source loss, and contrastive regularization to optimize for target-domain performance while filtering irrelevant source information.

## Key Results
- CUT-LightGCN achieves up to 15% better Recall and 18% better NDCG compared to the best baseline
- CUT achieves up to 7% better NDCG and 4.6% better Recall compared to single-domain methods trained on both domains
- The framework effectively alleviates negative transfer by retaining target-domain user similarity relationships

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: CUT directly filters irrelevant source-domain collaborative information by using target-domain user similarity as a constraint
- **Mechanism**: CUT first learns user similarity relationships from the target domain, then transfers information from the source domain while preserving these target-domain relationships through a user transformation layer and contrastive loss
- **Core assumption**: User similarity relationships in the target domain should remain stable even when incorporating information from the source domain
- **Evidence anchors**: [abstract] "The proposed Collaborative information regularized User Transformation (CUT) framework explicitly filters source-domain collaborative information by using target-domain user similarity as a constraint"

### Mechanism 2
- **Claim**: User transformation layer allows different representations for overlapping users across domains while preserving shared information
- **Mechanism**: For overlapping users, CUT learns separate source and target embeddings but connects them through a transformation layer that models domain differences
- **Core assumption**: Users exhibit different behavior patterns across domains but share some underlying characteristics that can be preserved through transformation
- **Evidence anchors**: [section] "For an overlapping user ùë¢ùëú ùëñ , we directly learn its source-domain embedding uùëú Œòùëú ùëñ by the backbone model and adopt a transformation layer F to learn its target-domain embedding F(uùëú Œòùëú ùëñ )"

### Mechanism 3
- **Claim**: Contrastive negative transfer regularization preserves target-domain user similarity relationships during source-target information transfer
- **Mechanism**: CUT uses a contrastive loss that ensures users who are similar in the target domain remain similar after transformation, even when incorporating source-domain information
- **Core assumption**: Maintaining target-domain similarity relationships is more important than incorporating all available source-domain information
- **Evidence anchors**: [section] "We propose an additional loss term based on the supervised contrastive learning mechanism [12] to regularize the collaborative information (i.e., similarity) of user representations"

## Foundational Learning

- **User similarity in collaborative filtering**: Understanding how user similarity is measured and used in recommendation systems is crucial for CUT's approach to filtering information based on similarity relationships
  - **Quick check**: How does cosine similarity differ from Jaccard similarity when measuring user similarity in recommendation systems?

- **Negative transfer in machine learning**: Understanding negative transfer is crucial for grasping why CUT's approach to filtering source-domain information is necessary
  - **Quick check**: What are the key differences between negative transfer and catastrophic forgetting in transfer learning?

- **Contrastive learning**: CUT uses contrastive loss to maintain user similarity relationships during information transfer
  - **Quick check**: How does supervised contrastive loss differ from traditional contrastive loss in self-supervised learning?

## Architecture Onboarding

- **Component map**: Single-domain backbone recommender ‚Üí User similarity learning phase ‚Üí User transformation layer ‚Üí Contrastive loss ‚Üí Cross-domain training
- **Critical path**: TARGET phase (learn user similarity) ‚Üí TRANSFER phase (transform and regularize) ‚Üí Final model training
- **Design tradeoffs**: Simpler transformation layer vs. more complex architectures; strict similarity preservation vs. flexibility in information transfer
- **Failure signatures**: Degradation in target domain performance; user similarity relationships becoming distorted; source domain information overwhelming target domain signals
- **First 3 experiments**:
  1. Test CUT with different similarity thresholds (Œ≥) to see impact on performance
  2. Compare CUT performance with and without the user transformation layer
  3. Evaluate how sensitive CUT is to the weight of the contrastive loss term (Œª)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does CUT's contrastive loss term perform with different similarity thresholds (Œ≥) and what is the optimal threshold range for various datasets?
- **Basis in paper**: [explicit] The paper mentions Œ≥ as a hyper-parameter and conducts experiments with different thresholds, but does not provide detailed analysis of threshold sensitivity or optimal ranges
- **Why unresolved**: The paper only reports using Œ≥=0.9 without exploring its sensitivity or comparing different threshold values
- **What evidence would resolve it**: Comprehensive ablation studies showing CUT performance across various Œ≥ values for different datasets and domain relationships

### Open Question 2
- **Question**: How does CUT handle scenarios with very sparse target domains where user similarity cannot be reliably computed?
- **Basis in paper**: [inferred] The paper briefly mentions that sparse target domains might undermine performance but doesn't provide detailed analysis or solutions for extreme sparsity cases
- **Why unresolved**: The paper only notes that performance drops in sparse cases without explaining mitigation strategies or performance bounds
- **What evidence would resolve it**: Empirical results showing CUT performance across target domain sparsity levels from 1% to 50% of interactions

### Open Question 3
- **Question**: What is the computational overhead of CUT compared to single-domain baselines during inference, and how does it scale with dataset size?
- **Basis in paper**: [explicit] The paper mentions that CUT induces "controllable time and space consumption" but only provides one training epoch comparison without inference time analysis
- **Why unresolved**: The paper lacks detailed runtime analysis or scaling studies for inference phase
- **What evidence would resolve it**: Detailed profiling showing inference time and memory usage comparisons between CUT and single-domain methods across datasets of varying sizes

## Limitations
- Limited empirical validation of intermediate steps - the paper demonstrates end-to-end performance improvements but doesn't provide ablation studies showing how much each component contributes individually
- The assumption that target-domain user similarity relationships are stable and should be preserved is not rigorously tested
- No analysis of CUT's performance when source-domain information contradicts target-domain relationships

## Confidence
- **High confidence**: End-to-end performance improvements on benchmark datasets
- **Medium confidence**: The general approach of using target-domain constraints to filter source information
- **Low confidence**: The specific mechanism claims about why each component works as described

## Next Checks
1. **Ablation study**: Test CUT performance with only the transformation layer (no contrastive loss) and with only the contrastive loss (no transformation layer) to quantify individual contributions
2. **Similarity robustness test**: Intentionally corrupt the target-domain user similarity matrix with noise and measure impact on CUT performance to validate the assumption about similarity stability
3. **Domain difference analysis**: Compare CUT performance across domains with varying degrees of user behavior differences to understand when the transformation layer is most effective