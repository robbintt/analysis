---
ver: rpa2
title: Reinforcement Learning for High-Level Strategic Control in Tower Defense Games
arxiv_id: '2406.07980'
source_url: https://arxiv.org/abs/2406.07980
tags:
- uni00000013
- levels
- uni00000048
- game
- agent
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a hybrid reinforcement learning approach that
  combines RL with heuristic AI to improve automated gameplay testing in tower defense
  games. The method uses RL as a high-level decision maker to select strategies while
  leveraging the existing HAI for low-level action execution.
---

# Reinforcement Learning for High-Level Strategic Control in Tower Defense Games

## Quick Facts
- arXiv ID: 2406.07980
- Source URL: https://arxiv.org/abs/2406.07980
- Reference count: 22
- Hybrid RL+HAI approach achieved 57.12% success rate vs 47.95% for HAI alone in Plants vs. Zombies

## Executive Summary
This paper presents a hybrid reinforcement learning approach that combines RL with heuristic AI to improve automated gameplay testing in tower defense games. The method uses RL as a high-level decision maker to select strategies while leveraging the existing HAI for low-level action execution. Tested on Plants vs. Zombies across 40 levels, the hybrid approach achieved a 57.12% success rate compared to 47.95% for HAI alone, demonstrating improved performance and robustness to difficulty changes. However, the results also showed that tower defense games require level-specific strategies, as agents trained on subsets of levels failed to generalize well to unseen levels.

## Method Summary
The approach implements a hybrid RL+HAI system where an RL agent (PPO-based) selects from 5 high-level strategies (Sow Sun, Attack, Defensive, Prepare, No-op) while the HAI executor handles detailed unit placement and cooldown management. Action masking eliminates invalid actions based on current sun tokens and cooldowns. The system uses 88-dimensional state features and trains separate RL agents for each of the 40 Plants vs. Zombies levels at fixed difficulty 100K for 10K episodes per level.

## Key Results
- Hybrid RL+HAI achieved 57.12% success rate versus 47.95% for HAI alone
- Action masking improved training efficiency by reducing invalid action exploration
- Level-specific training outperformed generalization attempts across multiple levels
- Hybrid approach showed improved robustness to difficulty scaling

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Hybrid RL + HAI outperforms both pure HAI and random agents in tower defense games by using RL for high-level strategy selection while deferring low-level execution to HAI.
- **Mechanism:** RL agent learns to choose between 4 high-level strategies (Sow Sun, Attack, Defensive, Prepare) and a no-op action, while HAI handles the detailed execution of unit placement and cooldown management. This splits the decision space into manageable parts.
- **Core assumption:** High-level strategy selection is more tractable for RL than low-level placement, and HAI's execution is sufficient if given better strategic input.
- **Evidence anchors:**
  - [abstract] "combining a learned approach, such as reinforcement learning, with a scripted AI produces a higher-performing and more robust agent"
  - [section] "The hybrid RL agent leverages the knowledge of HAI and determines which low-level strategy the HAI should execute"
- **Break condition:** If the strategy space is too large or poorly defined, or if HAI's execution becomes a bottleneck, the hybrid approach fails.

### Mechanism 2
- **Claim:** Action masking enables efficient training by eliminating invalid actions based on current game state (sun tokens, cooldowns).
- **Mechanism:** At each step, actions corresponding to strategies whose units are either too expensive or on cooldown are masked out, reducing the effective action space.
- **Core assumption:** Invalid actions can be reliably detected and masked before RL training, so the agent never wastes policy capacity on them.
- **Evidence anchors:**
  - [section] "At each simulation step, for a given semantic unit type tied to a specific strategy, each unit of said group is checked for... the action corresponding to this group is labeled as available"
- **Break condition:** If masking logic is buggy or incomplete, the agent may attempt invalid actions or the effective action space becomes too small.

### Mechanism 3
- **Claim:** Training one RL agent per level yields better performance than training a single agent across multiple levels due to the puzzle-like nature of tower defense levels.
- **Mechanism:** Each level has a unique optimal strategy; training per level allows the RL agent to specialize rather than generalize poorly.
- **Core assumption:** The optimal high-level strategy is level-specific and non-transferable.
- **Evidence anchors:**
  - [abstract] "agents trained on subsets of levels failed to generalize well to unseen levels"
  - [section] "some levels require dynamic values to be actually played, and a particular play-style – e.g., preferring defensive plants over attacking ones – can work in one level but not in another"
- **Break condition:** If levels share underlying strategic patterns, per-level training becomes inefficient and generalization becomes possible.

## Foundational Learning

- **Concept:** Reinforcement Learning (RL) basics (states, actions, rewards, policy optimization)
  - Why needed here: The hybrid approach replaces HAI's static priority system with an RL agent that learns to choose high-level strategies based on game states.
  - Quick check question: What does the RL agent output in this system, and how does it differ from the action space in pure RL methods?

- **Concept:** Action masking in RL
  - Why needed here: Ensures the RL agent only considers valid strategies given current resources and cooldowns, making training more efficient.
  - Quick check question: How does the action masking logic determine which strategies are available at each step?

- **Concept:** Multi-armed bandit / strategy selection problems
  - Why needed here: The RL agent essentially solves a sequential decision problem of when to switch between Sow Sun, Attack, Defensive, and Prepare strategies.
  - Quick check question: Why is choosing between 5 high-level actions easier for RL than choosing among all possible grid placements?

## Architecture Onboarding

- **Component map:** Game state → state feature vector → RL policy → strategy action → action masking → HAI executor → game state update
- **Critical path:** Game state → state feature vector → RL policy → strategy action → action masking → HAI executor → game state update
- **Design tradeoffs:**
  - Splitting decision space into high-level (RL) and low-level (HAI) makes RL tractable but adds complexity
  - Training per level maximizes performance but prevents generalization
  - Fixed difficulty training (100K) simplifies evaluation but may not reflect all player skill levels
- **Failure signatures:**
  - RL agent never learns: check if state space is informative enough, or if masking eliminates too many actions
  - HAI executor fails: check if strategy choice is incompatible with current game state
  - Poor generalization: check if training levels are too diverse or strategy patterns are truly non-transferable
- **First 3 experiments:**
  1. Replace RL agent with random strategy selection; compare success rate to baseline HAI and random agents
  2. Train RL agent on a single level; compare performance to HAI on that level only
  3. Train RL agent on 5 random levels; test on all 40 levels to measure generalization capability

## Open Questions the Paper Calls Out
1. What is the optimal level of generalization for RL agents in puzzle-like tower defense games?
2. Can transfer learning techniques improve generalization across tower defense levels?
3. How does the hybrid RL+HAI approach perform in other game genres beyond tower defense?

## Limitations
- Level-specific training prevents generalization across levels, requiring separate training for each level
- Hybrid approach adds implementation complexity compared to pure HAI systems
- Success rates remain relatively low (57.12%) despite improvement over baseline

## Confidence
- **High confidence**: The hybrid RL+HAI architecture works as described and achieves the reported performance improvements on the tested levels
- **Medium confidence**: The action masking mechanism is correctly implemented and contributes to training efficiency
- **Low confidence**: The claim that tower defense games fundamentally require level-specific strategies rather than generalizable patterns

## Next Checks
1. **Generalization testing**: Train the RL agent on random subsets of 10-30 levels and measure performance on the remaining unseen levels to quantify the extent of strategy-specific versus generalizable behavior
2. **Ablation study**: Remove the action masking component and compare training efficiency and final performance to determine the actual contribution of this mechanism
3. **HAI implementation verification**: Reimplement the HAI executor from scratch using only the described priority system and verify it achieves the baseline 47.95% success rate before adding the RL component