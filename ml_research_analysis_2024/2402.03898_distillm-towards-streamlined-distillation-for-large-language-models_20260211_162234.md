---
ver: rpa2
title: 'DistiLLM: Towards Streamlined Distillation for Large Language Models'
arxiv_id: '2402.03898'
source_url: https://arxiv.org/abs/2402.03898
tags:
- disti
- training
- student
- language
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DISTI LLM, a streamlined distillation framework
  for compressing large language models (LLMs) into smaller student models. It addresses
  key limitations in existing methods, such as instability from Kullback-Leibler divergence
  (KLD) loss and inefficiency from using student-generated outputs (SGOs).
---

# DistiLLM: Towards Streamlined Distillation for Large Language Models

## Quick Facts
- arXiv ID: 2402.03898
- Source URL: https://arxiv.org/abs/2402.03898
- Authors: Jongwoo Ko; Sungnyun Kim; Tianyi Chen; Se-Young Yun
- Reference count: 40
- Key outcome: Introduces DISTI LLM, a streamlined distillation framework achieving state-of-the-art performance with up to 4.3× speedup

## Executive Summary
DISTI LLM addresses key limitations in large language model (LLM) distillation by introducing a skew Kullback-Leibler divergence loss that stabilizes gradients and an adaptive off-policy approach that efficiently leverages student-generated outputs. The framework consists of two main components: a skew KLD that interpolates teacher and student distributions to prevent gradient explosion, and an adaptive scheduler that manages when to use student-generated outputs based on validation loss trends. Extensive experiments demonstrate that DISTI LLM achieves superior performance across instruction-following, text summarization, and machine translation tasks while significantly improving training efficiency.

## Method Summary
DISTI LLM is a distillation framework designed to compress large language models into smaller, more efficient student models. The method combines a skew Kullback-Leibler divergence loss with an adaptive off-policy approach for utilizing student-generated outputs. The skew KLD introduces a mixing parameter α that interpolates between teacher and student distributions, stabilizing gradients during training. The adaptive off-policy approach employs a replay buffer to store student-generated outputs and uses an adaptive scheduler to dynamically adjust the probability of using these outputs based on validation loss trends, improving sample efficiency while minimizing noisy feedback.

## Key Results
- Achieves state-of-the-art performance on instruction-following, text summarization, and machine translation tasks
- Demonstrates up to 4.3× speedup compared to recent knowledge distillation methods
- Shows stable training dynamics with reduced gradient explosion risk compared to vanilla KLD
- Improves sample efficiency through adaptive off-policy sampling of student-generated outputs

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Skew KLD stabilizes gradients during distillation by reducing the risk of gradient explosion that occurs with vanilla KLD.
- **Mechanism**: The skew KLD mixes the teacher distribution with the student distribution (αp + (1−α)qθ), which smooths the ratio term in the gradient and prevents the denominator from approaching zero.
- **Core assumption**: The student distribution will be reasonably close to the teacher distribution for the skew mixture to remain effective.
- **Evidence anchors**:
  - [abstract]: "a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties"
  - [section]: "SKL offers a reduced gradient norm compared to KLD, due to p and qθ interpolation preventing the denominator of rp,˜qθ from reaching zero"
  - [corpus]: Weak — no direct citations yet for gradient stability claims
- **Break condition**: If the student distribution diverges significantly from the teacher, the skew term may not sufficiently stabilize gradients.

### Mechanism 2
- **Claim**: Adaptive off-policy sampling improves sample efficiency by reusing student-generated outputs without constantly generating new ones.
- **Mechanism**: A replay buffer stores student-generated outputs (SGOs) and samples from it during training, reducing the need for on-policy SGO generation each iteration.
- **Core assumption**: Stored SGOs remain relevant enough to train the student without causing high bias errors.
- **Evidence anchors**:
  - [abstract]: "an adaptive off-policy approach that efficiently leverages SGOs by minimizing noisy feedback and improving sample efficiency"
  - [section]: "we replace the recently adopted on-policy approach ... with an off-policy approach, employing a replay buffer"
  - [corpus]: Weak — no citations yet for off-policy efficiency in KD
- **Break condition**: If the replay buffer contains outdated or noisy SGOs that no longer represent the student's current policy, performance may degrade.

### Mechanism 3
- **Claim**: The adaptive SGO scheduler dynamically adjusts the probability of using SGOs based on validation loss, balancing performance and stability.
- **Mechanism**: If validation loss increases, the scheduler raises the probability of using SGOs to reduce training-inference mismatch; otherwise, it lowers the probability to avoid noisy feedback.
- **Core assumption**: Validation loss is a reliable proxy for training-inference mismatch and noisy feedback.
- **Evidence anchors**:
  - [abstract]: "an adaptive off-policy approach ... designed to enhance the efficiency in utilizing student-generated outputs"
  - [section]: "we adjust ϕ by comparing the current and previous validation losses; an increase in validation loss leads to an increase in ϕ"
  - [corpus]: Weak — no direct evidence yet for validation loss as a proxy
- **Break condition**: If validation loss does not correlate with the actual mismatch or noise in feedback, the scheduler may make suboptimal adjustments.

## Foundational Learning

- **Concept**: Kullback-Leibler divergence and its asymmetric properties
  - **Why needed here**: Understanding KLD's mode-seeking behavior and its tendency to cause mode averaging or collapse is critical to grasping why skew KLD was proposed.
  - **Quick check question**: Why does vanilla KLD cause the student distribution to become overly smooth in generative tasks?
- **Concept**: Knowledge distillation objectives and sequence-level vs. token-level decomposition
  - **Why needed here**: The paper hinges on decomposing sequence-level distillation into token-wise components, which is enabled by tractable divergence measures like KLD.
  - **Quick check question**: How does the decomposition of sequence-level KLD into token-level terms enable efficient training?
- **Concept**: Policy optimization and off-policy learning in reinforcement learning
  - **Why needed here**: The adaptive off-policy approach borrows concepts from RL, particularly the idea of reusing past experiences to improve sample efficiency.
  - **Quick check question**: What is the key difference between on-policy and off-policy learning in terms of data usage?

## Architecture Onboarding

- **Component map**: Forward pass (teacher or SGO) -> Skew KLD computation -> Backward pass -> Update student parameters -> Adaptive scheduler check -> Off-policy buffer update
- **Critical path**: Forward pass (teacher or SGO) → Skew KLD computation → Backward pass → Update student parameters → Adaptive scheduler check → Off-policy buffer update
- **Design tradeoffs**:
  - **α value tuning**: Higher α reduces gradient instability but may slow convergence; lower α speeds convergence but risks instability.
  - **Replay buffer size**: Larger buffers improve sample efficiency but may introduce stale SGOs; smaller buffers reduce bias but increase on-policy generation cost.
  - **SGO usage probability ϕ**: Higher ϕ improves performance but increases noise risk; lower ϕ reduces noise but may slow convergence.
- **Failure signatures**:
  - **Gradient explosion**: Loss values become NaN or extremely large during training.
  - **Degraded performance**: Validation loss increases despite training progress, indicating noisy SGOs or stale buffer samples.
  - **Slow convergence**: Training loss plateaus early, suggesting insufficient exploration of SGOs.
- **First 3 experiments**:
  1. **Gradient stability test**: Compare gradient norms and loss smoothness between vanilla KLD and α-SKL with α=0.1.
  2. **Sample efficiency test**: Measure training time and performance with and without the off-policy buffer.
  3. **Scheduler sensitivity test**: Vary initial ϕ and observe validation loss trends to tune the adaptive scheduler.

## Open Questions the Paper Calls Out

- **Cross-tokenizer distillation**: How does DISTI LLM perform when applied to models with different tokenizers? The paper suggests future work could explore transferring knowledge between models with different tokenizers, but performance in such scenarios remains unexplored.
- **Human preference optimization**: Can DISTI LLM be extended to accommodate human preference optimization setups? While the paper is primarily designed for supervised fine-tuning, extending it to human preference optimization could be valuable future work.
- **Replay buffer capacity optimization**: How does the performance of DISTI LLM vary with different capacities of the replay buffer? While the paper provides insights into the impact of replay buffer capacity, it does not explore the full range of capacities or provide a comprehensive analysis of the trade-offs involved.

## Limitations
- **Empirical validation gaps**: Limited direct evidence for gradient stability claims and the correlation between validation loss and training-inference mismatch
- **Component isolation**: Ablation studies do not clearly isolate the individual contributions of skew KLD versus adaptive off-policy sampling
- **Generalization concerns**: Performance in cross-tokenizer distillation and human preference optimization scenarios remains unexplored

## Confidence
- **High**: The overall framework architecture and experimental results demonstrating performance improvements
- **Medium**: The theoretical justification for skew KLD's gradient stability properties
- **Low**: The specific mechanism by which validation loss reliably indicates training-inference mismatch

## Next Checks
1. **Ablation study isolation**: Measure performance differences when removing either skew KLD or adaptive off-policy components to quantify individual contributions
2. **Gradient analysis**: Compare gradient norms and variance between vanilla KLD and skew KLD across different α values during training
3. **Scheduler correlation test**: Empirically verify the relationship between validation loss trends and actual training-inference mismatch by measuring output distribution divergence