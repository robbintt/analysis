---
ver: rpa2
title: Unlearning Traces the Influential Training Data of Language Models
arxiv_id: '2401.15241'
source_url: https://arxiv.org/abs/2401.15241
tags:
- dataset
- training
- influence
- datasets
- untrac
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces UnTrac and UnTrac-Inv, methods for tracing
  the influence of training datasets on a language model's outputs. The core idea
  is to unlearn each training dataset using gradient ascent and measure the change
  in test loss, directly quantifying the dataset's influence.
---

# Unlearning Traces the Influential Training Data of Language Models

## Quick Facts
- arXiv ID: 2401.15241
- Source URL: https://arxiv.org/abs/2401.15241
- Reference count: 16
- Key outcome: UnTrac methods accurately trace training dataset influence by directly measuring loss changes after unlearning, outperforming existing gradient-based influence tracing approaches.

## Executive Summary
This paper introduces UnTrac and UnTrac-Inv, novel methods for tracing the influence of training datasets on language model outputs. The core innovation is using gradient ascent unlearning to directly measure how removing each dataset affects model performance on test datasets. UnTrac-Inv provides a more scalable approximation by unlearning test data instead of training data. Experiments demonstrate these methods significantly outperform existing approaches like TracIn, GradDot, GradCos, and Hessian-based influence functions in accurately estimating dataset influence on generating toxic, biased, and untruthful content.

## Method Summary
UnTrac and UnTrac-Inv trace training dataset influence by measuring how unlearning each dataset affects model performance. UnTrac unlearns each training dataset using gradient ascent and measures the resulting change in test loss, directly quantifying the dataset's influence. UnTrac-Inv reverses this process by unlearning the test dataset and measuring changes in training loss across all datasets, providing computational efficiency when dealing with many training datasets. Both methods require only standard training memory and avoid the computational burden of multiple checkpoints or excessive storage.

## Key Results
- UnTrac and UnTrac-Inv accurately estimate dataset influence with Pearson/Spearman correlations significantly higher than existing methods (TracIn, GradDot, GradCos, HIF).
- UnTrac-Inv performs well with large batch sizes (256) and few unlearning steps (5 epochs), providing computational efficiency.
- The methods work robustly across different hyperparameter settings, though performance depends on appropriate learning rates and batch sizes.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unlearning with gradient ascent directly measures dataset influence by quantifying loss increase when dataset is "forgotten"
- Mechanism: For each training dataset Z, perform gradient ascent unlearning steps, measure how much test loss L(z', θ) increases after unlearning. The magnitude of this increase directly quantifies the dataset's influence on test performance.
- Core assumption: Gradient ascent unlearning effectively removes the learned patterns associated with that dataset without introducing significant noise from other datasets.
- Evidence anchors:
  - [abstract]: "UnTrac is extremely simple; each training dataset is unlearned by gradient ascent, and we evaluate how much the model's predictions change after unlearning."
  - [section 3.1]: "UnTrac unlearns each training dataset and estimates its influence by assessing the unlearned model's performance on a test dataset."
  - [corpus]: Weak evidence - corpus papers focus on unlearning methods but don't directly validate the tracing mechanism described here.
- Break condition: If gradient ascent introduces too much noise or if the model catastrophically forgets patterns unrelated to the target dataset, the measured influence becomes unreliable.

### Mechanism 2
- Claim: UnTrac-Inv approximates UnTrac by reversing the direction of unlearning - unlearning test data instead of training data
- Mechanism: Instead of unlearning each training dataset and measuring test loss change, unlearning the test dataset and measuring training loss change. This is computationally efficient when many training datasets exist.
- Core assumption: The influence measured by unlearning test data and measuring training loss change approximates the influence measured by unlearning training data and measuring test loss change.
- Evidence anchors:
  - [section 3.2]: "UnTrac-Inv resembles UnTrac, while being efficient for massive training datasets."
  - [section 6.1]: Experimental results show UnTrac-Inv performs well when using large batch sizes and few unlearning steps, supporting the approximation claim.
  - [corpus]: Weak evidence - related papers on unlearning don't explicitly discuss this reverse-direction approximation.
- Break condition: When the number of unlearning steps is large or batch size is small, UnTrac-Inv deviates from UnTrac and becomes unreliable.

### Mechanism 3
- Claim: UnTrac methods outperform influence functions by directly measuring unlearning effects rather than approximating them
- Mechanism: Existing methods (HIF, TracIn, GradDot, GradCos) approximate influence through gradient analysis and Hessian approximations. UnTrac directly measures the effect of removing dataset influence through actual unlearning.
- Core assumption: Direct measurement through unlearning is more accurate than approximations based on gradients and Hessians.
- Evidence anchors:
  - [abstract]: "Experimental results demonstrate that our method estimates their influence much more accurately than existing methods."
  - [section 5.2]: "UnTrac and UnTrac-Inv correlates well with the ground-truth influence" while other methods show lower performance.
  - [corpus]: Weak evidence - related papers don't directly compare unlearning-based influence tracing to gradient-based methods.
- Break condition: If unlearning is computationally prohibitive or if the model architecture makes unlearning ineffective, the direct measurement advantage disappears.

## Foundational Learning

- Concept: Gradient ascent optimization
  - Why needed here: Unlearning requires maximizing loss rather than minimizing it, which is the opposite of standard training.
  - Quick check question: What happens to model parameters when you perform gradient ascent instead of gradient descent on a loss function?

- Concept: Influence functions and their approximations
  - Why needed here: The paper builds on and compares to existing influence function methods, so understanding their mechanisms is crucial.
  - Quick check question: How does Hessian-based influence function estimate the effect of removing training examples?

- Concept: Leave-one-out validation
  - Why needed here: The ground-truth influence is measured using leave-dataset-out, which is the gold standard but computationally expensive.
  - Quick check question: What is the computational complexity of measuring ground-truth influence using leave-dataset-out for N training datasets?

## Architecture Onboarding

- Component map: Trained language model -> Unlearning procedure (gradient ascent) -> Loss evaluation -> Influence measurement
- Critical path: For UnTrac: select training dataset → perform gradient ascent unlearning → measure test loss change. For UnTrac-Inv: select test dataset → perform gradient ascent unlearning → measure training loss change on each training dataset.
- Design tradeoffs: UnTrac is accurate but computationally expensive with many datasets; UnTrac-Inv is efficient but requires careful hyperparameter tuning. The choice depends on the number of training datasets and computational resources available.
- Failure signatures: Poor correlation with ground-truth influence, unstable performance across runs, sensitivity to learning rate and batch size, failure to converge during unlearning.
- First 3 experiments:
  1. Implement UnTrac on a small synthetic dataset to verify basic functionality and measure influence on test performance.
  2. Compare UnTrac with existing methods (GradDot, GradCos) on the same synthetic dataset to validate accuracy improvements.
  3. Implement UnTrac-Inv and verify it approximates UnTrac when using large batch sizes and few unlearning steps.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of UnTrac and UnTrac-Inv change when the number of training steps is significantly increased or decreased from the optimal setting?
- Basis in paper: [explicit] The paper discusses the sensitivity of UnTrac and UnTrac-Inv to the number of training steps in Section 6.1.
- Why unresolved: The paper does not explore the performance of these methods under extreme variations in the number of training steps.
- What evidence would resolve it: Experimental results showing the performance of UnTrac and UnTrac-Inv with significantly more or fewer training steps than the optimal setting.

### Open Question 2
- Question: Can UnTrac and UnTrac-Inv be effectively applied to larger language models, such as GPT-3 or GPT-4, and what are the potential limitations or challenges?
- Basis in paper: [inferred] The paper mentions that UnTrac and UnTrac-Inv require the same memory footprint as standard training, suggesting potential scalability issues with larger models.
- Why unresolved: The paper does not provide experiments or analysis on the application of these methods to larger language models.
- What evidence would resolve it: Successful application of UnTrac and UnTrac-Inv to larger language models, along with a discussion of any limitations or challenges encountered.

### Open Question 3
- Question: How do the hyperparameters of UnTrac and UnTrac-Inv, such as learning rate and batch size, interact with each other, and how can an optimal combination be determined?
- Basis in paper: [explicit] The paper discusses the sensitivity of UnTrac and UnTrac-Inv to various hyperparameters in Section 6.2.
- Why unresolved: The paper does not explore the interaction between different hyperparameters or provide a method for determining an optimal combination.
- What evidence would resolve it: A comprehensive analysis of the interaction between different hyperparameters and a method for determining an optimal combination based on the specific task and model architecture.

## Limitations

- The paper's empirical validation is limited to only 8 pretraining datasets and 3 test tasks, which may not generalize to larger, more diverse model training scenarios.
- Claims about outperforming all existing methods are based solely on correlation metrics without exploring whether different methods might be more accurate in specific scenarios.
- The computational efficiency claims for UnTrac-Inv depend heavily on specific hyperparameter choices that may not transfer to other model architectures or dataset compositions.

## Confidence

- **High Confidence**: The core mechanism of using gradient ascent unlearning to measure influence is well-defined and mathematically sound. The paper clearly articulates how loss changes after unlearning directly quantify dataset influence, and the computational approach is straightforward to implement.

- **Medium Confidence**: The approximation quality of UnTrac-Inv relative to UnTrac is demonstrated empirically but lacks theoretical guarantees. While experiments show good correlation with ground-truth influence, the paper doesn't establish bounds on approximation error or identify conditions where the approximation breaks down.

- **Low Confidence**: Claims about outperforming all existing methods (HIF, TracIn, GradDot, GradCos) are based on correlation metrics alone. The paper doesn't explore whether these methods might be more accurate in specific scenarios or provide ablation studies showing which components of UnTrac drive performance improvements.

## Next Checks

1. **Hyperparameter Sensitivity Analysis**: Systematically vary learning rates, batch sizes, and unlearning epochs for both UnTrac and UnTrac-Inv to identify the stability boundaries and optimal ranges. This would reveal whether the reported performance is robust or dependent on narrow hyperparameter windows.

2. **Cross-Dataset Generalization Test**: Apply UnTrac methods to a model trained on 50+ diverse datasets (including overlapping and similar-content datasets) to evaluate whether the methods maintain accuracy and whether the UnTrac-Inv approximation still holds under more complex training conditions.

3. **Failure Mode Investigation**: Deliberately create scenarios where datasets share significant content overlap or where unlearning steps are excessive, then measure how UnTrac and UnTrac-Inv performance degrades compared to ground-truth influence. This would establish the practical limits of each method's applicability.