---
ver: rpa2
title: 'Time-MMD: Multi-Domain Multimodal Dataset for Time Series Analysis'
arxiv_id: '2406.08627'
source_url: https://arxiv.org/abs/2406.08627
tags:
- data
- series
- multi
- multimodal
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Time-MMD introduces a high-quality, multi-domain multimodal time
  series dataset covering 9 domains with aligned numerical and textual data, addressing
  key gaps in existing datasets. The dataset includes carefully curated numerical
  sequences and corresponding textual data from reports and web searches, preprocessed
  using LLMs to ensure relevance and eliminate data contamination.
---

# Time-MMD: Multi-Domain Multimodal Dataset for Time Series Analysis

## Quick Facts
- **arXiv ID:** 2406.08627
- **Source URL:** https://arxiv.org/abs/2406.08627
- **Reference count:** 40
- **Primary result:** Multimodal models outperform unimodal by >15% average, up to 40% in text-rich domains

## Executive Summary
Time-MMD is a novel multi-domain multimodal time series dataset that addresses key gaps in existing datasets by providing aligned numerical and textual data across 9 diverse domains. The dataset includes carefully curated numerical sequences and corresponding textual data from reports and web searches, preprocessed using LLMs to ensure relevance and eliminate data contamination. A novel binary timestamp alignment method supports diverse time series analysis tasks. Experiments using the accompanying MM-TSFlib library demonstrate that multimodal forecasting models significantly outperform unimodal counterparts, validating both the dataset quality and the effectiveness of multimodal integration.

## Method Summary
Time-MMD dataset construction involves collecting numerical sequences and corresponding textual data from 9 domains, then preprocessing using Llama3-70B to filter, disentangle predictions from facts, and summarize text. Binary timestamps align text and numerical data at the same temporal granularity. The MM-TSFlib library provides an end-to-end pipeline that independently models numerical and textual series using TSF backbones and LLMs, then combines outputs via learnable linear weighting. Experiments run across 12 TSF backbones and 7 LLM models to evaluate multimodal forecasting performance.

## Key Results
- Multimodal forecasting models outperformed unimodal counterparts by over 15% on average
- Up to 40% improvement observed in text-rich domains
- Multimodal versions outperformed unimodal in 95% of cases across 12 TSF backbones
- Improvements robust across different horizon window sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-grained modality alignment improves forecasting accuracy.
- Mechanism: By aligning text and numerical data at the same temporal granularity using binary timestamps, the model can more effectively associate relevant textual context with corresponding numerical patterns.
- Core assumption: Text and numerical series share the same event timelines and domain-specific references.
- Evidence anchors:
  - [abstract] "ensures fine-grained modality alignment"
  - [section 2.3] "use binary timestamps to mark the start and end dates as a universal temporal alignment method"
- Break condition: If text sources report events at different granularities than the numerical series, alignment breaks down and relevance drops.

### Mechanism 2
- Claim: LLM preprocessing improves textual data quality.
- Mechanism: Using Llama3-70B to filter, disentangle predictions from facts, and summarize text reduces noise and data contamination, leading to cleaner input for forecasting models.
- Core assumption: LLMs can reliably distinguish factual content from predictions and irrelevant material in domain-specific reports.
- Evidence anchors:
  - [section 2.2] "curate the collected raw text data... by LLMs"
  - [section 2.2] "Table 2 presents a comparison of the token count before and after preprocessing"
- Break condition: If LLM hallucinations are not filtered out or context understanding fails, noise increases and model performance degrades.

### Mechanism 3
- Claim: Multimodal integration framework enables end-to-end forecasting.
- Mechanism: Independently modeling numerical and textual series, then combining via learnable linear weighting allows the model to leverage both modalities without sacrificing training efficiency.
- Core assumption: Numerical and textual features are complementary and can be linearly combined to improve prediction.
- Evidence anchors:
  - [section 3.2] "numerical and textual series are independently modeled... combined using a learnable linear weighting mechanism"
  - [section 4.2] "multimodal versions outperformed corresponding unimodal versions in 95% of cases"
- Break condition: If modalities are not complementary or weighting fails to capture interactions, multimodal performance does not improve.

## Foundational Learning

- Concept: Temporal alignment in multimodal time series
  - Why needed here: Ensures that text and numerical data correspond to the same time windows, critical for accurate forecasting
  - Quick check question: If a report covers Q1 2023, which numerical time steps should it align with in a weekly series?

- Concept: LLM-based text preprocessing
  - Why needed here: Cleans and structures noisy raw text so it can be effectively used as input to forecasting models
  - Quick check question: What is the difference between "extracted facts" and "extracted predictions" in the context of preprocessing?

- Concept: Multimodal integration via linear weighting
  - Why needed here: Combines independently modeled numerical and textual predictions into a single forecast without heavy architectural changes
  - Quick check question: How does freezing LLM parameters affect training time and model complexity?

## Architecture Onboarding

- Component map:
  - Numerical series → TSF backbone (e.g., Transformer, Informer)
  - Textual series → LLM + projection layer
  - Output combination → Learnable linear weighting
  - End-to-end pipeline → MM-TSFlib wrapper

- Critical path:
  1. Load Time-MMD dataset with binary timestamps
  2. Preprocess numerical and textual series
  3. Run unimodal and multimodal experiments via MM-TSFlib
  4. Compare MSE results across models and modalities

- Design tradeoffs:
  - Simple linear combination vs. complex attention-based fusion
  - Freezing LLM vs. fine-tuning full model
  - High-quality preprocessing vs. computational cost

- Failure signatures:
  - MSE unimodal ≈ MSE multimodal: Modalities not complementary
  - Training instability: Projection layer or weighting poorly initialized
  - Data leakage: Text includes future information beyond target horizon

- First 3 experiments:
  1. Run unimodal Transformer on numerical series only
  2. Run multimodal version with GPT-2-Small text modeling
  3. Compare MSE and check for >15% reduction in multimodal case

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the effectiveness of LLM preprocessing for textual data quality assurance be further improved beyond the current filtering, disentangling, and summarizing steps?
- Basis in paper: [explicit] The paper acknowledges the use of LLM preprocessing steps (filtering, disentangling, and summarizing) but also mentions that these steps are preliminary explorations and that more effective and efficient matching approaches tailored for time series forecasting tasks could be developed.
- Why unresolved: The current preprocessing method, while effective, is described as a simple preliminary exploration. The paper suggests that more advanced techniques could be developed to enhance the quality of textual data further.
- What evidence would resolve it: Empirical results demonstrating the performance improvements of alternative or enhanced preprocessing methods compared to the current approach would provide evidence to resolve this question.

### Open Question 2
- Question: What is the optimal horizon window size for multimodal time series forecasting across different domains, and how does it vary with domain characteristics?
- Basis in paper: [inferred] The paper explores the influence of horizon window size on multimodal forecasting performance but does not provide a definitive answer on the optimal size. It mentions that improvements via multimodality are robust to horizon window size but does not specify the best size for each domain.
- Why unresolved: The paper indicates that the effectiveness of multimodal forecasting is stable across different horizon window sizes but does not determine the optimal size for each domain. This could vary based on domain-specific characteristics and data patterns.
- What evidence would resolve it: Experimental results showing the performance of multimodal forecasting models across various horizon window sizes for each domain would help identify the optimal size for different domains.

### Open Question 3
- Question: How can the multimodal integration framework be extended to incorporate attention mechanisms or other advanced techniques to improve forecasting accuracy?
- Basis in paper: [explicit] The paper mentions that using attention mechanisms has not yet demonstrated a clear advantage over the current simple combining approach. It also suggests that there is significant room for improvement in methodological research for multimodal time series forecasting.
- Why unresolved: The current multimodal integration framework is described as effective but simple. The paper suggests that more advanced techniques, such as attention mechanisms, could potentially enhance performance but has not yet demonstrated their effectiveness.
- What evidence would resolve it: Comparative experiments showing the performance of multimodal forecasting models using attention mechanisms or other advanced techniques against the current framework would provide evidence to resolve this question.

## Limitations
- Dataset construction methodology: Exact filtering criteria and validation process for ensuring data quality across all 9 domains are not fully detailed
- Generalization across domains: Distribution of domain sizes and characteristics not provided, limiting assessment of generalizability
- Hyperparameter sensitivity: No ablation studies on critical hyperparameters like projection layer dimensions or weighting coefficients

## Confidence

- **High confidence:** The dataset's multimodal structure and binary timestamp alignment method are well-specified and directly supported by the methodology section
- **Medium confidence:** The reported performance improvements (15-40%) are supported by experimental results but lack detailed statistical significance testing across domains
- **Low confidence:** The LLM preprocessing pipeline's effectiveness is claimed but not independently verified, and potential hallucination risks are not quantified

## Next Checks

1. **Alignment verification:** Implement a script to verify binary timestamp alignment between numerical and textual series for a sample of records across multiple domains to confirm fine-grained temporal correspondence
2. **Preprocessing audit:** Run the described LLM preprocessing pipeline on raw text samples from each domain and compare token counts and content categories before/after to validate the claimed data quality improvements
3. **Statistical significance testing:** Perform paired t-tests on MSE results across all 12 TSF backbones and 7 domains to confirm that multimodal improvements are statistically significant beyond the reported average percentages