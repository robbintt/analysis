---
ver: rpa2
title: The NPU-ASLP-LiAuto System Description for Visual Speech Recognition in CNVSRC
  2023
arxiv_id: '2401.06788'
source_url: https://arxiv.org/abs/2401.06788
tags:
- visual
- speech
- data
- system
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents the NPU-ASLP-LiAuto system for the CNVSRC 2023
  visual speech recognition challenge. The system employs a multi-scale lip motion
  video extraction approach and utilizes various data augmentation techniques including
  speed perturbation, random rotation, horizontal flipping, and color transformation.
---

# The NPU-ASLP-LiAuto System Description for Visual Speech Recognition in CNVSRC 2023

## Quick Facts
- arXiv ID: 2401.06788
- Source URL: https://arxiv.org/abs/2401.06788
- Reference count: 0
- Primary result: First place in all three CNVSRC 2023 tracks with 34.76% CER (Single-Speaker) and 41.06% CER (Multi-Speaker) after multi-system fusion

## Executive Summary
This paper presents the NPU-ASLP-LiAuto system for the CNVSRC 2023 visual speech recognition challenge, achieving first place in all three tracks participated. The system employs a multi-scale lip motion video extraction approach and utilizes various data augmentation techniques including speed perturbation, random rotation, horizontal flipping, and color transformation. The VSR model is based on an end-to-end architecture with joint CTC/attention loss, consisting of a ResNet3D visual frontend, an E-Branchformer encoder, and a Transformer decoder. The system achieves 34.76% CER for the Single-Speaker Task and 41.06% CER for the Multi-Speaker Task after multi-system fusion.

## Method Summary
The system extracts lip motion videos at multiple scales (48, 64, 96, 112 pixels) and applies speed perturbation (0.9, 1.0, 1.1) to triple training data. The VSR model uses an end-to-end architecture with joint CTC/attention loss, combining ResNet3D visual frontend, E-Branchformer encoder, and Transformer decoder. Dynamic augmentation during training includes random rotation, horizontal flipping, and color transformation. The system builds diverse VSR models with different encoders (E-Branchformer, Branchformer, Conformer) and fuses them using ROVER for final recognition. A 24-layer Transformer language model is trained for shallow fusion during decoding.

## Key Results
- Achieved 34.76% CER for Single-Speaker Task
- Achieved 41.06% CER for Multi-Speaker Task
- Ranked first place in all three CNVSRC 2023 tracks participated

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Joint CTC/attention loss improves visual speech recognition performance by balancing monotonic alignment and sequence-to-sequence modeling.
- Mechanism: CTC loss provides sequence-level alignment supervision, helping with monotonic alignment and handling repetitions, while attention-based decoder captures long-range dependencies and contextual information. The weighted combination allows the model to leverage strengths of both approaches.
- Core assumption: Visual speech recognition benefits from both monotonic alignment (CTC) and contextual modeling (attention), and these objectives are complementary rather than conflicting.
- Evidence anchors:
  - [abstract] "The VSR model adopts an end-to-end architecture with joint CTC/attention loss"
  - [section] "The final model loss is composed of the CTC loss derived from the encoder and the cross-entropy (CE) loss computed on the decoder"
- Break condition: If the two loss components compete rather than complement each other, or if one dominates training to the detriment of the other, performance may degrade rather than improve.

### Mechanism 2
- Claim: Multi-scale lip motion video extraction improves model robustness to speaker variations and viewing distances.
- Mechanism: By extracting lip motion videos at multiple scales (48, 64, 96, 112), the model learns features that are invariant to scale changes and can handle speakers at different distances from the camera. This provides diverse spatial context for lip shape recognition.
- Core assumption: Lip reading performance benefits from scale-invariant features and the model can effectively learn from multi-scale inputs without explicit scale normalization.
- Evidence anchors:
  - [abstract] "we leverage the lip motion extractor from the baseline1 to produce multi-scale video data"
  - [section] "we extract lip motion video data by crop sizes of 48, 64, 96, and 112 to build multi-scale systems"
- Break condition: If the model cannot effectively integrate features across scales or if scale variation is not a significant factor in the dataset, the benefit may be minimal.

### Mechanism 3
- Claim: E-Branchformer encoder architecture provides superior performance for visual speech recognition compared to standard Transformers.
- Mechanism: E-Branchformer combines local and global context modeling through its parallel MLP-attention structure with enhanced merging, allowing it to capture both fine-grained lip movement patterns and broader temporal dependencies in speech.
- Core assumption: Visual speech recognition benefits from architectures that can efficiently capture both local (lip shape) and global (speech rhythm) patterns, and E-Branchformer is better suited for this than standard Transformers.
- Evidence anchors:
  - [abstract] "we adopt the recently proposed E-Branchformer [2]"
  - [section] "we also build Conformer and Branchformer VSR systems with similar param sizes to the E-Branchformer one"
- Break condition: If the dataset characteristics don't favor the specific architectural choices of E-Branchformer, or if the increased complexity doesn't translate to better generalization.

## Foundational Learning

- Concept: Lip motion extraction and video preprocessing
  - Why needed here: Visual speech recognition depends on accurately extracting lip motion features from raw video, which forms the foundation for all downstream modeling
  - Quick check question: How does lip motion extraction handle variations in lighting, camera angle, and speaker distance?

- Concept: End-to-end sequence modeling with joint objectives
  - Why needed here: VSR requires mapping variable-length visual sequences to text sequences, necessitating architectures that can handle alignment and context simultaneously
  - Quick check question: What are the trade-offs between CTC-only, attention-only, and joint CTC/attention approaches for sequence modeling?

- Concept: Multi-scale feature extraction and fusion
  - Why needed here: Visual speech recognition benefits from features at different spatial resolutions to capture both fine lip movements and broader facial context
  - Quick check question: How do you fuse features from different scales without introducing computational overhead or information loss?

## Architecture Onboarding

- Component map: Input video → Dynamic augmentation (rotation, flipping, color) → ResNet3D visual frontend → E-Branchformer encoder → Transformer decoder → CTC/attention joint loss → Output text
- Critical path: Visual frontend → Encoder → Decoder → Loss computation → Parameter update
- Design tradeoffs: Multi-scale inputs increase data diversity but also computational cost; joint CTC/attention provides better alignment but requires careful loss weighting; E-Branchformer offers better performance but is more complex than standard Transformers
- Failure signatures: Poor CER on evaluation sets despite good training performance (overfitting); degradation when using multi-scale inputs (integration issues); CTC and attention objectives fighting each other (loss weighting problems)
- First 3 experiments:
  1. Test single-scale vs multi-scale performance with the same encoder to isolate the benefit of scale variation
  2. Compare E-Branchformer with Conformer and Branchformer encoders while keeping all other components constant
  3. Evaluate the impact of joint CTC/attention vs CTC-only or attention-only training objectives on the validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the multi-scale lip motion video extraction approach impact recognition performance across different crop sizes?
- Basis in paper: [explicit] The paper mentions extracting lip motion video data with crop sizes of 48, 64, 96, and 112 to build multi-scale systems and observes performance improvements as crop size increases from 40 to 112.
- Why unresolved: The paper does not provide detailed analysis of the impact of each specific crop size on recognition performance or explain the optimal crop size for different scenarios.
- What evidence would resolve it: Detailed ablation studies showing recognition performance for each crop size independently, along with analysis of why certain sizes perform better than others.

### Open Question 2
- Question: What is the relative contribution of each data augmentation technique (speed perturbation, random rotation, horizontal flipping, color transformation) to overall system performance?
- Basis in paper: [explicit] The paper mentions applying various augmentation techniques during training but does not provide individual analysis of their contributions.
- Why unresolved: The paper presents results with all augmentations combined but lacks ablation studies to determine the impact of each technique independently.
- What evidence would resolve it: Systematic ablation studies showing recognition performance with different combinations of augmentation techniques.

### Open Question 3
- Question: How does the performance of different encoder architectures (E-Branchformer, Branchformer, Conformer) vary across different speaking styles or noise conditions?
- Basis in paper: [explicit] The paper mentions building diverse VSR systems with different encoders and notes that E-Branchformer outperforms Branchformer and Conformer, but does not analyze performance across different conditions.
- Why unresolved: The paper only provides overall performance metrics without analyzing how different encoders handle various speaking styles or noise conditions.
- What evidence would resolve it: Detailed analysis of encoder performance across different speaking styles, noise conditions, and challenging scenarios.

## Limitations
- Architectural specifications lack detail, particularly for ResNet3D visual frontend beyond basic layer dimensions
- Multi-system fusion effectiveness presented without ablation studies showing individual component contributions
- Computational cost analysis for multi-scale processing is not provided, making practical trade-offs difficult to assess

## Confidence

- High confidence: The ranking achievement (first place in all three tracks) is clearly stated and verifiable against the challenge results
- Medium confidence: The effectiveness of joint CTC/attention loss is supported by established literature but specific implementation details are limited
- Medium confidence: The multi-scale lip motion extraction benefit is theoretically sound but lacks comparative ablation studies

## Next Checks

1. Conduct controlled experiments comparing single-scale vs multi-scale training while keeping all other variables constant to isolate the contribution of scale variation
2. Perform ablation studies on the loss weighting parameters for CTC/attention joint training to identify optimal balance
3. Implement cross-validation on different speaker subsets to evaluate the robustness of the multi-scale approach to speaker-specific variations