---
ver: rpa2
title: Can Large Language Models abstract Medical Coded Language?
arxiv_id: '2403.10822'
source_url: https://arxiv.org/abs/2403.10822
tags:
- codes
- medical
- llms
- these
- encounter
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates whether Large Language Models (LLMs) can accurately
  interpret medical codes used in healthcare, such as ICD-10 and CPT codes. The research
  tests general and biomedical-specific LLMs (e.g., GPT-4, LLaMA-2, Meditron) on tasks
  including predicting medical conditions from ordered and randomly ordered codes,
  and identifying fake adversarial codes.
---

# Can Large Language Models abstract Medical Coded Language?

## Quick Facts
- arXiv ID: 2403.10822
- Source URL: https://arxiv.org/abs/2403.10822
- Reference count: 40
- Primary result: LLMs struggle to accurately interpret medical codes, frequently producing hallucinations and failing to distinguish real from fake codes

## Executive Summary
This study evaluates whether Large Language Models (LLMs) can accurately interpret medical codes used in healthcare, such as ICD-10 and CPT codes. The research tests general and biomedical-specific LLMs (e.g., GPT-4, LLaMA-2, Meditron) on tasks including predicting medical conditions from ordered and randomly ordered codes, and identifying fake adversarial codes. Results show that LLMs struggle with coded language, frequently producing inaccurate or hallucinated outputs, even when trained on biomedical data. For instance, GPT-4 had lower hallucination rates but still produced errors. Models like Meditron performed poorly regardless of prompting. The findings highlight the need for improved tokenization strategies and better representations of medical codes in LLMs to enhance their reliability in healthcare applications.

## Method Summary
The study evaluates off-the-shelf LLMs (GPT-4, LLaMA-2, Meditron, etc.) on their ability to interpret medical codes from the MIMIC clinical database. Three experiments are conducted: predicting medical conditions from ordered codes, from randomly ordered codes, and detecting fake adversarial codes. Models run without internet access, and predictions are evaluated using word match proportion against ground truth medical terminology. The evaluation includes 1000 medical codes spanning ICD-9/10, CPT, NDC, and LPOINC codes.

## Key Results
- LLMs consistently struggle to accurately predict medical conditions from coded language, with significant hallucination rates
- Biomedical-specific models like Meditron performed worse than general LLMs like GPT-4
- Models failed to reliably detect adversarial fake codes, often hallucinating plausible but incorrect responses
- Tokenization strategies optimized for natural language appear ill-suited for structured medical codes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs struggle with medical code abstraction because their tokenization strategies are optimized for natural language rather than structured alphanumeric codes.
- Mechanism: Subword tokenizers like BytePair Encoding (BPE) or WordPiece split words into smaller units based on frequency in natural language corpora. Medical codes have a unique structured format that doesn't align with these frequency patterns, leading to suboptimal tokenization and poor representation.
- Core assumption: Medical codes follow a rule-based structure that differs fundamentally from natural language text.
- Evidence anchors:
  - [abstract] "However, the frequent use of specialized coded languages like ICD-10, which are regularly updated and deviate from natural language formats, presents potential challenges for LLMs"
  - [section] "One hypothesis surrounding an LLMs inability to correctly predict an ICD code could be due to the tokenization designs that are not optimal for numerical or code like language"
  - [corpus] Weak - corpus doesn't directly address tokenization issues
- Break condition: If tokenization were redesigned specifically for medical codes, performance would improve significantly.

### Mechanism 2
- Claim: LLMs produce hallucinations when they encounter medical codes they cannot properly interpret.
- Mechanism: When faced with unfamiliar structured data, LLMs attempt to generate responses by pattern-matching or filling gaps, resulting in incorrect or fabricated outputs that appear plausible but lack grounding in actual code meanings.
- Core assumption: LLMs will always attempt to generate an answer rather than admit uncertainty when encountering unfamiliar input formats.
- Evidence anchors:
  - [abstract] "However, the frequent use of specialized coded languages like ICD-10... presents potential challenges for LLMs in creating accurate and meaningful latent representations"
  - [section] "This issue is significant in both machine learning and healthcare, as these LLMs are known to produce 'hallucinations' that are challenging to distinguish from actual ground truth"
  - [corpus] Weak - corpus doesn't directly address hallucination mechanisms
- Break condition: If models were trained to recognize their limitations with structured codes and respond appropriately.

### Mechanism 3
- Claim: Medical code ontologies are not well-represented in LLM parameter space, even for biomedical-specific models.
- Mechanism: The hierarchical and rule-based nature of medical coding systems requires specific knowledge representation that general pretraining on biomedical text doesn't capture, leading to poor performance even in specialized models.
- Core assumption: Biomedical pretraining should confer advantage in medical code interpretation tasks.
- Evidence anchors:
  - [abstract] "we evaluate whether large language models (LLMs) are aware of medical code ontologies and can accurately generate names from these codes"
  - [section] "Surprisingly, our findings also reveal that models pre-trained on medical tasks performed far worse than the state-of-the-art general LLMs"
  - [corpus] Weak - corpus doesn't directly address ontology representation
- Break condition: If biomedical pretraining included explicit representation of medical code ontologies.

## Foundational Learning

- Concept: Tokenization and subword segmentation
  - Why needed here: Understanding how LLMs break down input text is crucial for grasping why medical codes pose challenges
  - Quick check question: What tokenization method does your LLM use, and how would it handle "A120.1" versus "A12" versus "A1"?

- Concept: Hallucination detection and measurement
  - Why needed here: The study's evaluation relies on detecting when models generate incorrect information
  - Quick check question: How would you distinguish between a correct prediction and a hallucination when the ground truth is "Fracture of forearm"?

- Concept: Medical coding hierarchies and ontologies
  - Why needed here: Understanding the structured nature of medical codes is essential for interpreting why LLMs struggle
  - Quick check question: In ICD-10, what does the first digit "A" represent, and how does the code structure progress from there?

## Architecture Onboarding

- Component map: Input preprocessing → Tokenizer → Model layers → Output decoder → Post-processing → Medical code database → Ground truth mapping → Evaluation metrics → Experimental design framework → Randomization logic → Adversarial example generation

- Critical path: Input medical code → Tokenization → Model prediction → Output mapping to medical terminology → Evaluation against ground truth

- Design tradeoffs:
  - General vs. specialized models: General models may lack domain knowledge but avoid overfitting; specialized models may be too narrow
  - Prompt engineering vs. model architecture: Better prompts may help, but fundamental tokenization issues persist
  - Evaluation granularity: Chapter-level vs. code-level accuracy tradeoffs

- Failure signatures:
  - Consistent hallucination across different prompt formulations
  - Correct chapter prediction but wrong specific code mapping
  - Pattern of ignoring numerical structure in codes
  - Poor performance on adversarial examples despite success on real codes

- First 3 experiments:
  1. Test tokenizer behavior on a sample of medical codes to understand segmentation patterns
  2. Evaluate model performance on ordered vs. random code sequences to assess structural understanding
  3. Create a controlled experiment with known good and bad codes to measure hallucination rates systematically

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do tokenization strategies specifically impact the ability of LLMs to process and understand medical codes?
- Basis in paper: [explicit] The paper discusses tokenization strategies as a potential reason for LLMs' struggles with medical codes, noting that subword tokenizers like BytePair Encoding are optimized for natural language and may not be well-suited for numerical values or codes.
- Why unresolved: The paper highlights this as a hypothesis but does not provide empirical evidence or specific experiments to test the impact of different tokenization strategies on medical code understanding.
- What evidence would resolve it: Comparative studies evaluating the performance of LLMs using various tokenization strategies (e.g., subword tokenizers vs. character-level tokenization) on tasks involving medical codes.

### Open Question 2
- Question: What are the specific limitations of current LLMs in handling adversarial medical codes, and how can these be addressed?
- Basis in paper: [explicit] The paper includes an adversarial attack experiment where LLMs were tested on identifying fake medical codes, revealing that models often hallucinate responses instead of correctly identifying the malicious examples.
- Why unresolved: The paper shows that LLMs struggle with adversarial examples but does not explore methods to improve their ability to detect or ignore fake codes.
- What evidence would resolve it: Experiments testing whether fine-tuning LLMs on datasets containing adversarial examples improves their ability to detect or reject fake medical codes.

### Open Question 3
- Question: Can LLMs be effectively fine-tuned on biomedical data to improve their understanding of medical codes, and if so, what are the optimal training approaches?
- Basis in paper: [inferred] The paper notes that biomedical-specific LLMs like Meditron performed poorly, suggesting that current fine-tuning approaches may not be sufficient. It also calls for better representation of medical codes in LLMs.
- Why unresolved: The paper does not explore whether different fine-tuning strategies, such as curriculum learning or domain-specific pre-training, could enhance LLMs' performance on medical code tasks.
- What evidence would resolve it: Comparative studies evaluating the performance of LLMs fine-tuned using various strategies (e.g., domain-specific pre-training, curriculum learning) on medical code prediction tasks.

## Limitations

- The study demonstrates systematic failures but doesn't definitively establish whether these stem from tokenization, training data gaps, or fundamental architectural limitations
- Most results are based on GPT-4 with limited comparative data across the full range of tested models
- The evaluation methodology focuses on surface-level word matching rather than semantic accuracy

## Confidence

- High confidence that LLMs struggle with medical code interpretation across multiple dimensions
- Medium confidence that tokenization issues are the primary driver
- Low confidence in claims about biomedical-specific models performing worse than general models

## Next Checks

1. Conduct controlled experiments isolating tokenization effects by testing custom tokenizers designed specifically for medical code structures against standard tokenizers
2. Implement semantic similarity metrics (e.g., clinical concept matching) rather than word-level matching to better capture model understanding
3. Test model performance on progressively obfuscated code formats to determine the boundary between structured and natural language processing capabilities