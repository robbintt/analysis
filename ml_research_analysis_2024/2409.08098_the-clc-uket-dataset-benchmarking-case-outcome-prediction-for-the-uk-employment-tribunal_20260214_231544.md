---
ver: rpa2
title: 'The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK Employment
  Tribunal'
arxiv_id: '2409.08098'
source_url: https://arxiv.org/abs/2409.08098
tags:
- case
- claimant
- legal
- prediction
- cases
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces the CLC-UKET dataset to facilitate outcome
  prediction for UK Employment Tribunal cases. The dataset includes approximately
  19,000 cases with detailed legal annotations covering facts, claims, legal references,
  case outcomes, and jurisdiction codes.
---

# The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK Employment Tribunal

## Quick Facts
- **arXiv ID**: 2409.08098
- **Source URL**: https://arxiv.org/abs/2409.08098
- **Reference count**: 25
- **Primary result**: Fine-tuned T5 transformer model achieves highest F-score (0.564) on UKET case outcome prediction task

## Executive Summary
This study introduces the CLC-UKET dataset for predicting case outcomes in the UK Employment Tribunal. The dataset contains approximately 19,000 cases with detailed legal annotations including facts, claims, legal references, outcomes, and jurisdiction codes. The researchers employed GPT-4 for automatic annotation to overcome the challenge of extensive manual labeling. The study investigates a multi-class prediction task where models must predict whether a claimant wins, loses, partly wins, or other outcomes based on case facts and claims. Experiments compare fine-tuned transformer models (BERT and T5) against zero-shot and few-shot GPT models, with fine-tuned T5 achieving the highest performance and human experts serving as the baseline.

## Method Summary
The method involves using GPT-4 to automatically annotate UK Employment Tribunal cases, extracting facts, claims, legal references, and outcomes. The dataset is split into training (11,838 cases), validation (1,373 cases), and test (1,371 cases) sets. Fine-tuned transformer models (BERT and T5) are trained using Adam optimizer with learning rate 1e-4 for 5 epochs, sequence length 512. GPT-3.5 and GPT-4 models are evaluated in zero-shot and few-shot settings with jurisdiction code-based sampling. All models are evaluated against gold-standard test labels using weighted F-score, precision, and recall metrics.

## Key Results
- Fine-tuned T5 transformer model achieves highest F-score of 0.564 on the UKET prediction task
- Human experts achieve F-score of 0.672, outperforming all automated models
- GPT-4 achieves highest precision among all baseline models in zero-shot setting
- GPT-3.5 performance improves when few-shot examples share similar jurisdiction codes with target cases

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Fine-tuned transformer models outperform zero-shot and few-shot LLMs on the UKET prediction task.
- **Mechanism**: Fine-tuning adapts the pre-trained model parameters specifically to the domain and task, optimizing them for the distribution of facts, claims, and outcomes in UKET cases.
- **Core assumption**: The task-specific data distribution in CLC-UKETpred is sufficiently distinct from general language to benefit from fine-tuning.
- **Evidence anchors**:
  - [abstract] "Empirical results from baseline models indicate that finetuned transformer models outperform zero-shot and few-shot LLMs on the UKET prediction task."
  - [section] "The fine-tuned T5 emerges as the best performer overall, achieving the highest F-score."
  - [corpus] Weak - corpus neighbors do not provide strong evidence for fine-tuning superiority in legal prediction tasks.
- **Break condition**: If the task data is too small or too similar to the pre-training data, fine-tuning may overfit or offer minimal gains.

### Mechanism 2
- **Claim**: Including task-related information in few-shot examples enhances GPT-3.5's prediction performance.
- **Mechanism**: Providing examples with similar jurisdiction codes gives the model relevant context, aligning its reasoning process with the specific legal domain of the target case.
- **Core assumption**: Jurisdictional similarity correlates with case outcome similarity, making it a useful signal for model adaptation.
- **Evidence anchors**:
  - [abstract] "The performance of zero-shot LLMs can be enhanced by integrating task-related information into few-shot examples."
  - [section] "Using examples that share similar jurisdiction codes with the target case enhances the F-score of GPT-3.5's predictions more effectively than randomly sampled examples."
  - [corpus] Weak - no direct corpus evidence supports jurisdiction-based few-shot learning effectiveness.
- **Break condition**: If jurisdiction codes do not correlate with outcome patterns, the few-shot benefit may disappear.

### Mechanism 3
- **Claim**: GPT-4 in zero-shot setting already achieves the highest precision among all baseline models.
- **Mechanism**: GPT-4's larger model capacity and stronger reasoning abilities allow it to make more accurate predictions without task-specific tuning or examples.
- **Core assumption**: The model's pre-training data includes sufficient legal reasoning patterns to generalize to UKET outcomes.
- **Evidence anchors**:
  - [section] "GPT-4, in its zero-shot setting, already achieves the highest precision among all baseline models."
  - [corpus] Weak - corpus neighbors do not provide evidence of GPT-4's precision in legal judgment prediction.
- **Break condition**: If the legal domain requires highly specialized knowledge not covered in pre-training, precision may degrade without fine-tuning or few-shot adaptation.

## Foundational Learning

- **Concept**: Multi-class classification with imbalanced classes
  - **Why needed here**: The prediction task has four outcome labels with uneven distribution (e.g., "claimant wins" vs. "other"), requiring careful metric selection and model calibration.
  - **Quick check question**: Why is weighted average F-score used instead of macro F-score in Table 2?

- **Concept**: Automatic annotation using LLMs
  - **Why needed here**: Manual annotation of legal texts is costly and time-consuming; GPT-4 is used to extract facts, claims, and outcomes efficiently.
  - **Quick check question**: What are potential risks of relying on GPT-4 for legal information extraction?

- **Concept**: Fine-tuning vs. few-shot learning
  - **Why needed here**: The study compares fine-tuned transformer models (BERT, T5) against zero-shot and few-shot GPT models to evaluate adaptation strategies.
  - **Quick check question**: Under what conditions might few-shot learning outperform fine-tuning?

## Architecture Onboarding

- **Component map**:
  - CLC-UKET dataset -> GPT-4 annotation extraction -> Prediction task (facts + claims â†’ outcome) -> Baseline models (BERT, T5, GPT-3.5, GPT-4) -> Evaluation metrics (accuracy, precision, recall, F-score)

- **Critical path**:
  1. Extract facts and claims from court decisions using GPT-4.
  2. Construct training, validation, and test splits.
  3. Fine-tune transformer models on training set.
  4. Evaluate models on test set with gold-standard labels.

- **Design tradeoffs**:
  - Fine-tuning vs. few-shot: Fine-tuning offers higher performance but requires more data and compute; few-shot is faster but less accurate.
  - Zero-shot vs. few-shot: Zero-shot avoids example bias but may underperform when domain-specific context is critical.

- **Failure signatures**:
  - Low precision on "claimant loses" may indicate the model is overly conservative in predicting negative outcomes.
  - Poor recall on "partly wins" suggests the model struggles with nuanced or multi-faceted cases.

- **First 3 experiments**:
  1. Train BERT on the training set and evaluate on validation set to check overfitting.
  2. Test GPT-4 zero-shot on a small subset to establish baseline precision.
  3. Compare GPT-3.5 few-shot with jurisdiction-matched examples vs. random examples to measure contextual benefit.

## Open Questions the Paper Calls Out

- **Open Question 1**
  - Question: How does the performance of prediction models change when actual facts and claims are used instead of extracted facts and claims from court decisions?
  - Basis in paper: [explicit] The paper discusses that facts and claims used in the dataset were extracted from tribunal decisions, which may introduce information biases as judges write knowing the case outcome. The paper acknowledges this limitation and suggests exploring alternative methods of identifying facts and claims to better approximate original submissions to the court.
  - Why unresolved: The current study relies on extracted facts and claims from court decisions, which may contain biases. The paper explicitly states this limitation and suggests future research to explore alternative methods of identifying facts and claims.
  - What evidence would resolve it: Comparing model performance using both extracted facts/claims and actual facts/claims from case submissions would provide evidence on the impact of information bias on prediction accuracy.

- **Open Question 2**
  - Question: What is the impact of providing more detailed facts compared to the relatively concise fact statements present in the current CLC-UKET dataset on prediction performance?
  - Basis in paper: [explicit] The paper mentions that there is room to explore the effect of extracting and providing more detailed facts compared to the relatively concise fact statements present in the current CLC-UKET dataset. It suggests that this could be an area for future exploration.
  - Why unresolved: The current dataset uses relatively concise fact statements extracted by GPT-4. The paper acknowledges this limitation and suggests exploring the impact of more detailed facts on prediction performance.
  - What evidence would resolve it: Conducting experiments with datasets containing more detailed facts and comparing the prediction performance to the current dataset would provide evidence on the impact of fact detail on model accuracy.

- **Open Question 3**
  - Question: How does the inclusion of temporal information (case decision dates) affect the prediction performance of models?
  - Basis in paper: [explicit] The paper notes that predicting case outcomes without knowing the precise decision date may lead to mistakes, as employment and procedural law has evolved during the period covered by the dataset (2011-2023). It mentions that models and human predictors did not have direct access to the date at which the underlying case was decided.
  - Why unresolved: The current prediction task does not include temporal information, which could be relevant given the evolution of employment law during the dataset period. The paper identifies this as a potential source of error but does not explore its impact.
  - What evidence would resolve it: Experiments comparing prediction performance with and without temporal information would provide evidence on the importance of case decision dates for accurate predictions.

- **Open Question 4**
  - Question: How does the performance of prediction models vary across different types of employment tribunal cases (jurisdiction codes)?
  - Basis in paper: [explicit] The paper mentions that each UKET case can be associated with multiple jurisdiction codes identifying the dispute matter. It also notes that GPT-3.5 performance improves when task-specific examples are included in prompts, particularly those selected according to jurisdiction codes.
  - Why unresolved: While the paper explores few-shot learning with jurisdiction-specific examples, it does not provide a comprehensive analysis of how prediction performance varies across different types of employment cases.
  - What evidence would resolve it: Analyzing prediction performance separately for each jurisdiction code or grouping of codes would provide insights into how well models perform for different types of employment disputes.

## Limitations
- The automatic annotation process using GPT-4 introduces potential labeling errors that could affect model performance evaluation.
- The dataset size of approximately 19,000 cases may be insufficient for fine-tuning large transformer models effectively.
- The focus on UK Employment Tribunal cases limits generalizability to other legal domains or jurisdictions.

## Confidence

- **High confidence**: The observation that fine-tuned transformer models outperform zero-shot LLMs is well-supported by the experimental results and aligns with established machine learning principles.
- **Medium confidence**: The finding that GPT-4 achieves highest precision in zero-shot setting is based on the reported experiments but could be influenced by annotation artifacts.
- **Medium confidence**: The claim about jurisdiction-matched few-shot examples improving GPT-3.5 performance is supported but requires further validation with larger sample sizes.

## Next Checks

1. Conduct error analysis on GPT-4 automatic annotations to quantify labeling accuracy and identify systematic biases.
2. Perform cross-validation with multiple random seeds to assess the stability of model performance differences.
3. Test model generalization by evaluating performance on a held-out subset of cases from different time periods or legal sub-domains.