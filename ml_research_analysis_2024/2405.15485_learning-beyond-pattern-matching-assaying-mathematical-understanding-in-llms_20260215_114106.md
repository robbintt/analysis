---
ver: rpa2
title: Learning Beyond Pattern Matching? Assaying Mathematical Understanding in LLMs
arxiv_id: '2405.15485'
source_url: https://arxiv.org/abs/2405.15485
tags:
- skills
- skill
- accuracy
- learning
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the mathematical understanding of large language
  models (LLMs) by examining how they learn and adapt to mathematical skills. The
  authors propose a method called NTKEval, inspired by Neural Tangent Kernel (NTK),
  to measure changes in LLM probability distributions during training.
---

# Learning Beyond Pattern Matching? Assaying Mathematical Understanding in LLMs

## Quick Facts
- arXiv ID: 2405.15485
- Source URL: https://arxiv.org/abs/2405.15485
- Authors: Siyuan Guo; Aniket Didolkar; Nan Rosemary Ke; Anirudh Goyal; Ferenc Huszár; Bernhard Schölkopf
- Reference count: 40
- Key outcome: In-context learning enables LLMs to differentiate between deep mathematical structures and surface formats, while instruction-tuning leads to pattern matching rather than true mathematical comprehension

## Executive Summary
This paper evaluates whether large language models (LLMs) truly understand mathematical concepts or merely pattern match. The authors introduce NTKEval, a method inspired by Neural Tangent Kernel theory, to measure changes in LLM probability distributions during training. Using a synthetic dataset with controlled variables, they compare in-context learning (ICL) and instruction-tuning (IT) approaches. The study reveals that ICL allows LLMs to differentiate between deep mathematical structures (core skills) and surface presentation formats, while IT produces similar performance changes regardless of training data type, suggesting pattern matching rather than genuine understanding.

## Method Summary
The authors create a synthetic dataset of 3rd-grade math problems with controlled variables, isolating deep mathematical structures from surface presentation formats. They introduce NTKEval, which measures probability distribution changes using importance sampling to efficiently compare base and fine-tuned models without requiring thousands of generations. The method computes the difference in probability of generating correct solutions between models. They then conduct experiments comparing ICL (providing skill-targeted examples in context) with IT (fine-tuning on skill-focused datasets), analyzing how each approach affects the model's ability to distinguish between skill-matched and format-matched test questions.

## Key Results
- ICL with skill-targeted examples significantly improves performance on test questions sharing the same mathematical skill more than on those sharing only surface format
- IT on skill-focused datasets leads to similar performance changes regardless of whether training data shares deep skills or surface formats with test data
- NTKEval requires fewer generations than accuracy counting methods while maintaining comparable reliability for detecting domain understanding

## Why This Works (Mechanism)

### Mechanism 1
- Claim: NTKEval measures changes in LLM probability distributions more efficiently than accuracy-based methods.
- Mechanism: NTKEval uses importance sampling to compute the difference in probability of generating correct solutions between a fine-tuned model and base model, avoiding the need for thousands of generations per question.
- Core assumption: The probability difference between models on the same input correlates with accuracy differences, and this correlation converges with fewer samples than accuracy counting.
- Evidence anchors:
  - [abstract] "We propose NTKEval to assess changes in LLM's probability distribution via training on different kinds of math data"
  - [section] "NTKEval requires fewer generations compared to the standard metric in counting accuracy differences"
  - [corpus] Weak evidence; no corpus papers directly discuss NTKEval efficiency
- Break condition: If the probability distributions of base and fine-tuned models diverge too much, importance sampling becomes unreliable.

### Mechanism 2
- Claim: In-context learning (ICL) enables LLMs to differentiate between deep mathematical structures and surface presentation formats.
- Mechanism: ICL with skill-targeted examples improves performance on test questions sharing the same mathematical skill more than on those sharing only surface format, indicating the model learns the underlying skill rather than pattern matching.
- Core assumption: Performance improvement scales with the relevance of in-context examples to the test question's deep structure.
- Evidence anchors:
  - [abstract] "Our systematic analysis finds evidence of domain understanding during in-context learning"
  - [section] "in-context learning differentiates deep structures from surface structures (Table 3)"
  - [corpus] Weak evidence; corpus lacks direct studies comparing ICL skill vs. format targeting
- Break condition: If LLMs rely on superficial statistical correlations rather than genuine skill extraction.

### Mechanism 3
- Claim: Instruction-tuning (IT) on skill-focused datasets leads to pattern matching rather than true mathematical understanding.
- Mechanism: IT produces similar performance changes regardless of whether the training data shares deep mathematical skills or surface formats with the test data, suggesting the model adapts to format rather than skill.
- Core assumption: True mathematical understanding would manifest as differential performance improvement based on skill relevance, not format similarity.
- Evidence anchors:
  - [abstract] "certain instruction-tuning leads to similar performance changes irrespective of training on different data, suggesting a lack of domain understanding"
  - [section] "instruction-tuning on skill-focused dataset leads to similar performance change irrespective of training on different data types"
  - [corpus] Weak evidence; corpus papers focus on general IT effectiveness rather than skill vs. format differentiation
- Break condition: If IT methodology changes to better isolate skill-specific learning.

## Foundational Learning

- Concept: Neural Tangent Kernel (NTK)
  - Why needed here: NTKEval builds on NTK theory to measure how training on one input affects predictions on another, adapted for language models.
  - Quick check question: How does NTK theoretically capture the generalization relationship between training and test inputs in neural networks?

- Concept: Importance sampling
  - Why needed here: NTKEval uses importance sampling to efficiently compare probability distributions between base and fine-tuned models without generating thousands of completions.
  - Quick check question: Why is importance sampling computationally more efficient than direct probability estimation when comparing two related models?

- Concept: In-context learning vs. instruction-tuning
  - Why needed here: The paper compares these two learning paradigms to understand how LLMs acquire mathematical understanding differently.
  - Quick check question: What is the fundamental difference in how information is presented to the model during inference between ICL and IT?

## Architecture Onboarding

- Component map: NTKEval -> Importance sampling -> Probability difference computation -> Skill differentiation analysis; ICL/IT -> Dataset preparation -> Model training/fine-tuning -> Performance evaluation
- Critical path: Generate completions -> Extract deterministic solutions -> Compute probability differences -> Analyze skill vs. format effects
- Design tradeoffs: NTKEval trades some accuracy for sample efficiency; ICL trades storage for computation; IT trades computation for potentially better generalization
- Failure signatures: NTKEval fails when probability distributions diverge too much; ICL fails when examples don't share sufficient deep structure; IT fails when format dominates skill learning
- First 3 experiments:
  1. Implement NTKEval and verify it converges faster than accuracy counting on a simple synthetic dataset
  2. Test ICL skill vs. format targeting on elementary math problems to confirm differential performance
  3. Compare IT performance changes when training on skill-matched vs. format-matched data to identify pattern matching behavior

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does NTKEval's sample efficiency advantage persist across different language model architectures and scales beyond the 7B parameter models tested?
- Basis in paper: [explicit] The paper demonstrates NTKEval's sample efficiency on three 7B parameter models (Code Llama, Llemma, Mistral) but explicitly states "we hope to help design better and more transparent scientific assistants" suggesting broader applicability
- Why unresolved: The evaluation was limited to a specific parameter range and model families. NTK behavior may vary with model depth, width, or architectural differences (transformers vs other architectures)
- What evidence would resolve it: Systematic evaluation of NTKEval across diverse model scales (1B to 70B+), different architectures (decoder-only, encoder-decoder, MoE), and training objectives would establish generalizability

### Open Question 2
- Question: What is the precise relationship between NTK changes measured by NTKEval and downstream mathematical reasoning performance across non-synthetic tasks?
- Basis in paper: [inferred] The paper uses synthetic datasets with controlled variables to isolate deep vs surface structures, but acknowledges this is a limitation: "We only considered QA data rather than open-ended texts"
- Why unresolved: The controlled synthetic setting may not capture the complexity of real mathematical reasoning where multiple skills interact and surface features provide legitimate contextual cues
- What evidence would resolve it: Evaluation of NTKEval on established mathematical reasoning benchmarks (GSM8K, MATH, AMPS) would reveal whether NTK changes predict actual problem-solving ability in naturalistic settings

### Open Question 3
- Question: How do NTK dynamics differ between in-context learning and instruction tuning when both methods achieve comparable final performance?
- Basis in paper: [explicit] The paper observes different NTK patterns between ICL (differentiates deep vs surface structures) and IT (similar performance changes regardless of training data type), but notes "Both ICL and IT are capable of learning to learn"
- Why unresolved: The paper shows ICL and IT have different learning mechanisms but doesn't investigate whether these differences persist when both achieve equivalent accuracy
- What evidence would resolve it: Head-to-head comparison where ICL and IT are tuned to match performance on the same task, followed by NTKEval analysis to see if mechanistic differences remain when surface-level outcomes are equalized

### Open Question 4
- Question: What is the minimum amount of training data needed for NTKEval to reliably detect domain understanding versus pattern matching?
- Basis in paper: [inferred] The paper demonstrates NTKEval's efficiency compared to accuracy-based metrics but doesn't systematically investigate the data requirements for reliable detection of understanding
- Why unresolved: While NTKEval requires fewer generations than accuracy counting, the paper doesn't establish the relationship between training dataset size and the reliability of the NTK signal
- What evidence would resolve it: Controlled experiments varying training dataset sizes while measuring NTK signal stability and correlation with ground-truth understanding would establish practical requirements

### Open Question 5
- Question: How does the inclusion of chain-of-thought reasoning in training data affect NTK measurements of mathematical understanding?
- Basis in paper: [explicit] The paper mentions "the answer may contain CoT reasoning (Wei et al., 2022b) where an exact solution is extractable" but doesn't investigate its impact on NTK measurements
- Why unresolved: The marginalization approach for CoT in NTKEval assumes CoT can be separated from the core mathematical understanding, but this may not reflect how models actually use reasoning traces
- What evidence would resolve it: Comparative NTKEval analysis on datasets with and without CoT, examining whether the presence of reasoning traces changes the NTK signal for mathematical skills versus surface structures

## Limitations
- Synthetic dataset limits generalizability to real-world mathematical reasoning
- NTKEval efficiency claims lack extensive empirical validation across diverse model architectures
- Focus on 3rd-grade arithmetic restricts applicability to advanced mathematical domains

## Confidence
- **High Confidence**: ICL enables skill differentiation between deep structures and surface formats
- **Medium Confidence**: IT leads to pattern matching rather than domain understanding
- **Low Confidence**: NTKEval efficiency relative to accuracy counting across different model families

## Next Checks
1. **Cross-Domain Validation**: Replicate the ICL vs. IT comparison using more complex mathematical domains (algebra, calculus) and real-world mathematical problems to verify that the observed pattern matching vs. domain understanding distinction holds beyond elementary arithmetic.

2. **NTKEval Robustness Testing**: Conduct systematic experiments comparing NTKEval's efficiency and accuracy against traditional accuracy counting across multiple model sizes, families, and problem types to establish the method's reliability boundaries and identify conditions where it may fail.

3. **Mechanism Isolation Study**: Design experiments that control for confounding factors such as instruction format, example quality, and model architecture to more precisely identify which aspects of ICL specifically enable skill extraction versus pattern matching.