---
ver: rpa2
title: 'Learning Control Barrier Functions and their application in Reinforcement
  Learning: A Survey'
arxiv_id: '2404.16879'
source_url: https://arxiv.org/abs/2404.16879
tags: []
core_contribution: The paper provides a comprehensive survey of safe reinforcement
  learning (SRL) methods that use control barrier functions (CBFs) to ensure safety
  during robot training and deployment. It categorizes safety constraints into soft,
  hard, and probabilistic types, and reviews methods that enforce each.
---

# Learning Control Barrier Functions and their application in Reinforcement Learning: A Survey

## Quick Facts
- arXiv ID: 2404.16879
- Source URL: https://arxiv.org/abs/2404.16879
- Reference count: 40
- Primary result: Survey of safe reinforcement learning methods using control barrier functions to ensure safety during robot training and deployment

## Executive Summary
This survey comprehensively examines safe reinforcement learning (SRL) methods that leverage control barrier functions (CBFs) to ensure safety during robot training and deployment. The paper categorizes safety constraints into soft, hard, and probabilistic types, and reviews methods that enforce each. CBF-based safety methods are highlighted for their ability to provide formal safety guarantees, but challenges include model dependence and difficulty in constructing CBFs. To address this, the paper reviews data-driven approaches for learning CBFs from expert demonstrations, simulation data, or via refinement of initial handcrafted CBFs using machine learning techniques such as Gaussian Processes, neural networks, and reinforcement learning.

## Method Summary
The paper provides a comprehensive survey of existing literature on SRL methods using CBFs, categorizing safety constraints and reviewing data-driven approaches for learning CBFs. The survey methodology examines various techniques for learning CBFs from expert demonstrations, simulation data, or by refining initial handcrafted CBFs using machine learning techniques like Gaussian Processes, neural networks, and reinforcement learning. The paper also discusses the trade-offs between model-based and model-free RL in terms of safety guarantees, sample efficiency, and generalizability.

## Key Results
- CBF-based safety methods provide formal safety guarantees by ensuring forward invariance of safe sets
- Data-driven approaches can learn CBFs from expert demonstrations, simulation data, or by refining handcrafted CBFs
- Trade-offs exist between model-based and model-free RL regarding safety guarantees, sample efficiency, and generalizability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CBFs provide safety by ensuring forward invariance of a safe set defined by a continuously differentiable function h(x) ≥ 0.
- Mechanism: The system state is kept within the safe set by requiring the Lie derivative of h(x) along the system dynamics to satisfy Lf h + Lgh u ≥ -α(h(x)), where α is an extended class K function. This constraint is enforced during RL policy training via a safety filter that modifies the agent's action before execution.
- Core assumption: The system dynamics are control affine and the Lie derivatives Lf h and Lgh can be computed (either analytically or learned).
- Evidence anchors:
  - [abstract] "These functions provide a framework to ensure that the system remains in a safe state during the learning process."
  - [section II.A] "Most of CBF methods are concerned with control affine systems... This advantage due to the existence of several numerically efficient methods for solving QP in real time."
  - [corpus] Weak: Corpus neighbors discuss CBFs in robotics but do not provide specific anchor for the forward invariance mechanism.
- Break condition: If the system is not control affine, or if the Lie derivatives cannot be computed/learned accurately, the CBF safety filter will fail.

### Mechanism 2
- Claim: Learning CBFs from data addresses the challenge of manually constructing CBFs for complex systems.
- Mechanism: Machine learning techniques (e.g., neural networks, Gaussian Processes) are used to parameterize a CBF candidate function h(x). The parameters are trained to satisfy the CBF condition (4) using expert demonstrations or sampled safe/unsafe trajectories. The learned CBF can then be used in a safety filter during RL training.
- Core assumption: There exists a function h(x) that can be represented by the chosen ML model and that satisfies the CBF properties for the system.
- Evidence anchors:
  - [abstract] "This challenge motivates the exploration of data-driven methods for automatically defining control barrier functions, which is highly appealing."
  - [section IV.A] "Different ML can be used to design and infer CBFs from data... Loss functions are designed to mimic the behavior of an actual CBF, and the minimization of which leads to a CBF that can discriminate between safe and unsafe sets."
  - [corpus] Weak: Corpus neighbors mention learning CBFs but do not provide specific anchor for the data-driven learning mechanism.
- Break condition: If the ML model cannot represent the true CBF, or if insufficient data is available to train it, the learned CBF may not provide safety guarantees.

### Mechanism 3
- Claim: CBF-based safety filters enable hard and probabilistic safety constraints during RL training, improving sample efficiency and safety performance compared to soft constraints.
- Mechanism: The CBF safety filter modifies the agent's action to ensure the CBF condition is satisfied, preventing safety violations. This active enforcement of safety allows the agent to explore the state space more efficiently, as it is guided away from unsafe regions. Probabilistic safety is achieved by incorporating uncertainty models (e.g., Gaussian Processes) in the CBF formulation.
- Core assumption: The safety filter can compute a safe action that is close to the agent's original action, and that the agent can learn from the modified actions.
- Evidence anchors:
  - [section III-B2] "Unlike methods adopting soft constraints, methods enforcing hard safety constraints aim to give guarantees that ensure system safety throughout training and deployment..."
  - [section III-B2] "The core idea of as safety shield is that it monitors an action before being applied to the system, and it modifies this action only in case it is expected to violate safety."
  - [corpus] Weak: Corpus neighbors discuss safety in RL but do not provide specific anchor for the CBF safety filter mechanism.
- Break condition: If the safety filter is too conservative, it may prevent the agent from exploring effectively. If the agent cannot learn from the modified actions, the final policy may not achieve the desired task.

## Foundational Learning

- Concept: Control Barrier Functions (CBFs)
  - Why needed here: CBFs are the core safety mechanism used in the surveyed SRL methods. Understanding their properties and how they enforce safety is crucial for implementing and extending these methods.
  - Quick check question: What is the condition that a function h(x) must satisfy to be a valid CBF for a control affine system?
- Concept: Reinforcement Learning (RL) and Markov Decision Processes (MDPs)
  - Why needed here: SRL methods build upon RL to learn safe policies. Knowledge of RL concepts (e.g., policies, value functions, exploration-exploitation trade-off) and MDP formalism is necessary to understand how safety is incorporated into the learning process.
  - Quick check question: How does the Markov property simplify the RL problem, and why is it often assumed in practice?
- Concept: Learning from Demonstrations and Data-Driven CBF Construction
  - Why needed here: Manually constructing CBFs for complex systems can be challenging. Learning CBFs from expert demonstrations or sampled data is a key approach to overcome this challenge. Familiarity with imitation learning, supervised learning, and data-driven control is beneficial.
  - Quick check question: What are the key differences between learning a CBF from safe demonstrations versus learning from both safe and unsafe demonstrations?

## Architecture Onboarding

- Component map: RL agent -> Environment, CBF safety filter modifies agent actions, Data-driven CBF learning module updates CBF parameters
- Critical path: RL agent proposes action -> CBF safety filter modifies action (if needed) -> modified action is executed in the environment. Data-driven CBF learning: collect data -> update CBF parameters -> new CBF is used in the safety filter
- Design tradeoffs: Model-based vs. model-free RL: Model-based methods can provide stronger safety guarantees but may suffer from model bias. Model-free methods are more flexible but may require more data. Hard vs. soft constraints: Hard constraints provide stronger safety guarantees but may limit exploration. Soft constraints are more flexible but may not prevent all safety violations.
- Failure signatures: If the CBF is not valid, the safety filter may fail to prevent safety violations. If the CBF is too conservative, the agent may not explore effectively. If the data-driven CBF learning is not sample-efficient, the CBF may not be learned accurately.
- First 3 experiments:
  1. Implement a basic CBF safety filter for a simple control affine system (e.g., double integrator) and verify that it prevents safety violations.
  2. Learn a CBF from expert demonstrations for a more complex system (e.g., quadrotor) and evaluate its safety performance during RL training.
  3. Compare the sample efficiency and safety performance of an RL agent with a CBF safety filter versus an RL agent with only soft safety constraints (e.g., reward shaping).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we ensure that safe RL policies trained in simulation maintain their safety guarantees when deployed in the real world, given the inherent simulation-to-reality gap?
- Basis in paper: [explicit] The paper discusses the simulation-to-reality gap as a challenge for deploying safe RL policies and mentions the need for methods to certify safety during deployment.
- Why unresolved: While the paper mentions the issue, it doesn't provide specific solutions for ensuring safety guarantees during deployment, focusing mainly on safety during training.
- What evidence would resolve it: Developing and testing methods that can effectively bridge the simulation-to-reality gap and maintain safety guarantees during deployment would resolve this question.

### Open Question 2
- Question: What is the most effective way to learn control barrier functions (CBFs) from data, considering the trade-off between sample efficiency, generalizability, and the quality of the learned CBFs?
- Basis in paper: [explicit] The paper extensively reviews various methods for learning CBFs from data, highlighting the challenges and trade-offs involved.
- Why unresolved: The paper presents different approaches but doesn't definitively conclude which method is superior, as the effectiveness depends on the specific application and constraints.
- What evidence would resolve it: Comparative studies evaluating the performance of different CBF learning methods across various tasks and environments would help determine the most effective approach.

### Open Question 3
- Question: How can we design safe RL algorithms that can generalize well to unseen environments and tasks while maintaining safety guarantees?
- Basis in paper: [explicit] The paper discusses the importance of generalizability and zero-shot performance for safe RL policies but acknowledges the limited research in this area.
- Why unresolved: Achieving generalizability while maintaining safety is a complex challenge that requires further research and development of new algorithms and techniques.
- What evidence would resolve it: Developing and testing safe RL algorithms that demonstrate strong performance and safety guarantees in diverse and unseen environments would address this question.

## Limitations
- Limited empirical validation across diverse robotic systems for data-driven CBF learning methods
- Effectiveness of data-driven CBF learning highly dependent on data quality and quantity
- Simulation-to-reality gap poses challenges for maintaining safety guarantees during deployment

## Confidence

**High Confidence**: Control Barrier Functions provide a theoretical framework for safety through forward invariance properties (Mechanism 1)

**Medium Confidence**: Learning CBFs from data can address manual construction challenges, though empirical success varies significantly by method and application (Mechanism 2)

**Medium Confidence**: CBF safety filters improve sample efficiency by preventing safety violations during exploration, but trade-offs with conservatism exist (Mechanism 3)

## Next Checks

1. Verify the accuracy of Lie derivative computations for learned CBFs in a control-affine system with known dynamics
2. Test data-driven CBF learning performance with limited expert demonstrations versus both safe/unsafe data
3. Compare exploration efficiency and safety violation rates between CBF-filtered RL and reward-shaping approaches in a benchmark environment