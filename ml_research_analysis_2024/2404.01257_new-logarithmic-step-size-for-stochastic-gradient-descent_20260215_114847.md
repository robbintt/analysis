---
ver: rpa2
title: New logarithmic step size for stochastic gradient descent
arxiv_id: '2404.01257'
source_url: https://arxiv.org/abs/2404.01257
tags: []
core_contribution: This paper introduces a novel logarithmic step size with warm restarts
  for stochastic gradient descent (SGD). The proposed step size addresses the issue
  of slow convergence in the final iterations of existing methods by maintaining a
  higher probability of selection for these iterations.
---

# New logarithmic step size for stochastic gradient descent

## Quick Facts
- arXiv ID: 2404.01257
- Source URL: https://arxiv.org/abs/2404.01257
- Reference count: 7
- Primary result: Introduces logarithmic step size with warm restarts for SGD, achieving 0.9% accuracy improvement on CIFAR100

## Executive Summary
This paper introduces a novel logarithmic step size with warm restarts for stochastic gradient descent (SGD) to address slow convergence in final iterations. The proposed step size maintains higher probability of selection for final iterations compared to cosine step size, while still achieving the optimal O(1/√T) convergence rate for smooth non-convex functions. Empirical results on FashionMNIST, CIFAR10, and CIFAR100 datasets demonstrate improved test accuracy and superior performance compared to nine state-of-the-art step size methods.

## Method Summary
The method introduces a logarithmic step size schedule defined as ηt = η0[1 - ln(t)/ln(T)] combined with warm restarts. The step size decays logarithmically rather than exponentially, maintaining larger values longer in training while still meeting convergence requirements. Warm restarts periodically reset the learning rate to its initial value, allowing the algorithm to escape local minima. The approach is evaluated on image classification tasks using convolutional neural networks and ResNet architectures.

## Key Results
- Achieves O(1/√T) convergence rate for smooth non-convex functions, matching the best-known rate
- Demonstrates 0.9% test accuracy improvement on CIFAR100 dataset
- Shows superior performance compared to nine baseline step size methods including cosine, constant, Adam, and Armijo line search
- The logarithmic step size provides higher probability of selection for final iterations compared to cosine step size

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The logarithmic step size maintains higher probability of selection for final iterations compared to cosine step size.
- Mechanism: The probability distribution ηt/PT ηt decreases more slowly for final iterations in logarithmic step size than in cosine step size, giving final iterations higher selection probability.
- Core assumption: The probability distribution ηt/PT ηt directly influences the likelihood of selecting specific iteration outputs as final solutions.
- Evidence anchors:
  - [abstract] "the new step size proves to be more effective for the final iterations, as they enjoy a higher probability of selection"
  - [section] "Fig. 1 (Left) illustrates that the cosine step size assigns a higher probability of selection to the initial iterations but substantially reduces the probability for the final iterations"
  - [corpus] Weak - no direct evidence found about probability distributions affecting final iteration selection
- Break condition: If the probability distribution doesn't actually influence final solution selection, this mechanism fails.

### Mechanism 2
- Claim: The logarithmic step size converges more slowly to zero than many other step sizes while still providing O(1/√T) convergence rate.
- Mechanism: The logarithmic decay pattern ηt = η0(1 - ln(t)/ln T) creates slower step size reduction than exponential or polynomial decay, maintaining learning capability longer while still meeting convergence requirements.
- Core assumption: Maintaining larger step sizes longer in training helps escape local minima while still achieving optimal convergence rate.
- Evidence anchors:
  - [section] "we introduce a logarithmic step size that exhibits slower convergence to zero compared to many other step sizes"
  - [section] "when considering c ∝ O(√T/ln T), we have the following result: E∥∇f(¯xT)∥2 ≤ 4L√T(f(x1) − f∗) + 4σ2L√T"
  - [corpus] Weak - no direct evidence found about logarithmic vs exponential decay rates
- Break condition: If slower convergence to zero prevents reaching optimal solution, this mechanism fails.

### Mechanism 3
- Claim: Warm restarts combined with logarithmic step size provide better generalization by periodically resetting to larger step sizes.
- Mechanism: The warm restart schedule allows the algorithm to periodically escape local minima by resetting to larger step sizes, while the logarithmic decay provides smooth convergence within each cycle.
- Core assumption: Periodic resets to larger step sizes help escape local minima and improve generalization performance.
- Evidence anchors:
  - [section] "The key idea behind warm restarts is that in each restart, the learning rate is initialized to a specific value (denoted as η0) and scheduled to decrease"
  - [section] "it has been shown that a warm restarted SGD takes 2–4 times less time than a traditional learning rate adjustment strategy"
  - [corpus] Weak - no direct evidence found about warm restarts improving generalization
- Break condition: If warm restarts cause instability or prevent convergence, this mechanism fails.

## Foundational Learning

- Concept: Convergence analysis for stochastic gradient descent
  - Why needed here: The paper establishes O(1/√T) convergence rate for smooth non-convex functions using the new step size
  - Quick check question: What are the key assumptions needed to prove convergence for SGD with different step size schedules?

- Concept: Probability distributions in stochastic optimization
  - Why needed here: The paper analyzes the probability distribution ηt/PT ηt and how it affects which iteration outputs are selected
  - Quick check question: How does the probability distribution of step sizes influence the likelihood of selecting specific iteration outputs?

- Concept: Warm restart techniques in optimization
  - Why needed here: The paper combines the new logarithmic step size with warm restarts, requiring understanding of how periodic resets affect optimization dynamics
  - Quick check question: What are the theoretical and practical benefits of using warm restarts in stochastic gradient descent?

## Architecture Onboarding

- Component map: The system consists of three main components: (1) logarithmic step size scheduler, (2) warm restart controller, and (3) SGD optimizer. The step size scheduler generates ηt values based on current iteration t and total iterations T. The warm restart controller manages cycle boundaries and resets. The SGD optimizer uses these values to update model parameters.

- Critical path: The critical path is: data batch → gradient computation → step size calculation → parameter update → loss evaluation → checkpoint (every T iterations). Any delay in step size calculation directly impacts training speed.

- Design tradeoffs: The logarithmic step size provides slower decay but requires computing logarithms at each iteration. The warm restart schedule adds overhead for tracking cycle boundaries but provides better generalization. The choice of T (inner loop length) affects both convergence speed and memory usage.

- Failure signatures: If training loss plateaus early, check if step size is decaying too quickly. If accuracy is poor, verify warm restart intervals are appropriate. If convergence is slow, check if initial step size η0 is too small.

- First 3 experiments:
  1. Compare convergence curves of logarithmic vs constant step sizes on a simple convex problem
  2. Test different T values (inner loop lengths) to find optimal warm restart frequency
  3. Compare final accuracy with and without warm restarts on CIFAR100 dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the logarithmic step size perform on other types of non-convex functions, such as those satisfying the PL condition?
- Basis in paper: [explicit] The paper mentions that the convergence rate for smooth non-convex functions that satisfy the PL condition was not examined.
- Why unresolved: The current analysis focuses on smooth non-convex functions without the PL condition.
- What evidence would resolve it: Convergence rate analysis and empirical results for the logarithmic step size on non-convex functions satisfying the PL condition.

### Open Question 2
- Question: How does the logarithmic step size compare to other step sizes for convex and strongly convex objective functions?
- Basis in paper: [explicit] The paper suggests further investigation into the convergence rate for convex and strongly convex functions is warranted.
- Why unresolved: The current analysis is limited to smooth non-convex functions.
- What evidence would resolve it: Convergence rate analysis and empirical results comparing the logarithmic step size to other step sizes for convex and strongly convex functions.

### Open Question 3
- Question: How does the logarithmic step size perform on larger and more diverse datasets, beyond the FashionMNIST, CIFAR10, and CIFAR100 datasets used in the experiments?
- Basis in paper: [inferred] The paper only evaluates the logarithmic step size on three specific datasets, and there is a need for broader evaluation.
- Why unresolved: The current experiments are limited in scope and may not capture the full range of performance across different datasets.
- What evidence would resolve it: Empirical results comparing the logarithmic step size to other step sizes on a diverse set of larger datasets from various domains.

## Limitations

- Theoretical analysis restricted to smooth non-convex functions while empirical validation uses deep neural networks which may have non-smooth components
- Limited documentation of hyperparameter tuning for baseline methods makes fair comparison uncertain
- Warm restart mechanism implementation details not fully specified, particularly cycle length determination and reset strategies

## Confidence

- **High Confidence**: The O(1/√T) convergence rate proof for smooth non-convex functions (theoretical claim)
- **Medium Confidence**: The 0.9% accuracy improvement on CIFAR100 (empirical claim with limited seed runs)
- **Medium Confidence**: The superiority over 9 baseline methods (comparative claim with potential hyperparameter tuning issues)

## Next Checks

1. **Reproduce convergence curves**: Implement the logarithmic step size with warm restarts and verify O(1/√T) convergence rate on a simple non-convex test function (e.g., Rosenbrock function)
2. **Ablation study**: Run CIFAR100 experiments with and without warm restarts using identical hyperparameters to isolate the effect of the logarithmic step size versus the restart mechanism
3. **Baseline parameter sensitivity**: Systematically vary learning rates for all 9 baseline methods and re-run CIFAR100 experiments to ensure fair comparison across different step size schedules