---
ver: rpa2
title: 'Chinchilla Scaling: A replication attempt'
arxiv_id: '2404.10102'
source_url: https://arxiv.org/abs/2404.10102
tags:
- hoffmann
- scaling
- loss
- training
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors attempt to replicate Hoffmann et al.'s (2022) Approach
  3 for estimating compute-optimal scaling laws in large language models. They extract
  data from Hoffmann et al.'s plots and fit a parametric loss function to model the
  final pre-training loss.
---

# Chinchilla Scaling: A replication attempt

## Quick Facts
- arXiv ID: 2404.10102
- Source URL: https://arxiv.org/abs/2404.10102
- Authors: Tamay Besiroglu; Ege Erdil; Matthew Barnett; Josh You
- Reference count: 1
- Primary result: Authors find substantial differences from Hoffmann et al.'s compute-optimal scaling law estimates, with their model providing better fit to the data and more plausible confidence intervals.

## Executive Summary
This paper attempts to replicate Hoffmann et al.'s (2022) Approach 3 for estimating compute-optimal scaling laws in large language models. The authors extract data from Hoffmann et al.'s published figures and fit a parametric loss function to model the final pre-training loss. Their results show significant differences from Hoffmann et al.'s reported estimates, with their model providing substantially better fit to the data and more plausible confidence intervals. The scaling policy derived from their estimates is consistent with both the Chinchilla training ratio and Hoffmann et al.'s other approaches, unlike Hoffmann et al.'s Approach 3.

## Method Summary
The authors extracted data from Hoffmann et al.'s Figure 4 using SVG parsing to reconstruct scatter plot data points. They then fit the parametric scaling law L(N, D) = E + A/N^α + B/D^β using Huber loss minimization over log-transformed variables. The fitting used a grid of initializations to avoid local optima. They performed bootstrapping to estimate standard errors and tested the significance of differences between their estimates and Hoffmann et al.'s using χ² tests. The authors also analyzed the plausibility of Hoffmann et al.'s reported confidence intervals given their dataset size.

## Key Results
- The authors' estimated scaling law differs substantially from Hoffmann et al.'s, with highly significant differences in the E and β parameters (p < 10^-60).
- Hoffmann et al.'s estimated scaling law fails to fit the reconstructed data well, with 98% of Huber loss values being higher than those of the authors' model.
- The confidence intervals reported by Hoffmann et al. are implausibly narrow, requiring over 600,000 experiments to achieve, while they likely only ran fewer than 500.
- The scaling policy derived from Hoffmann et al.'s estimated parameters is inconsistent with their other approaches and the 20-tokens-per-parameter rule-of-thumb used to train their Chinchilla model, while the authors' model is consistent with both.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The parametric scaling law can be fit to observed loss data by minimizing Huber loss, allowing robust parameter estimation.
- Mechanism: The model assumes loss L(N,D) = E + A/N^α + B/D^β, where N is parameters, D is tokens. The fitting minimizes Huber loss over log-transformed variables to handle outliers and non-linearity.
- Core assumption: The final pre-training losses across different model sizes and training lengths follow the assumed parametric form with independent noise.
- Evidence anchors:
  - [abstract] "fitting a parametric loss function to model the final pre-training loss" and "[they] fit the Hoffmann et al. parametric scaling law to our reconstructed subset of the data: L(N, D) = E + A/N^α + B/D^β"
  - [section] "We fit the Hoffmann et al. parametric scaling law to our reconstructed subset of the data: L(N, D) = E + A/N^α + B/D^β (1) with N being the number of parameters and D being the number of training tokens. To fit this function used by Hoffmann et al, we minimize the Huber loss"
- Break condition: If the true loss surface deviates significantly from the assumed parametric form, the fit will be poor and parameter estimates unreliable.

### Mechanism 2
- Claim: The χ² test can detect statistically significant differences between parameter estimates from different datasets.
- Mechanism: Under the assumption that one set of estimates is the true population value, differences between estimates from a subset should follow a multivariate normal distribution, enabling χ² significance testing.
- Core assumption: The dataset used by the authors is a random subset of Hoffmann et al.'s full dataset, and parameter estimates follow a normal sampling distribution.
- Evidence anchors:
  - [section] "if µ denotes the Hoffmann et al. estimates and ν denotes our best fit, the difference µ − ν should follow N (0, Σ) for some covariance matrix Σ. Given that this is the case, we expect (µ−ν)T Σ−1(µ−ν) to follow a χ2 distribution"
  - [section] "the p-value of the resulting χ2 test ends up being < 10−60. This means that the difference between our parameters and Hoffmann et al.'s parameters is extremely statistically significant"
- Break condition: If the datasets are systematically different or the normality assumption fails, the χ² test becomes invalid.

### Mechanism 3
- Claim: Narrow confidence intervals reported by Hoffmann et al. are statistically implausible given their dataset size.
- Mechanism: Standard error decreases with √n, so achieving very tight confidence intervals requires many more observations than reported.
- Core assumption: The reported confidence intervals were computed using standard statistical methods without special data augmentation.
- Evidence anchors:
  - [abstract] "the confidence intervals reported by Hoffmann et al. are implausibly tight and unlikely to be obtained from proper statistical procedures given the size of their dataset"
  - [section] "Obtaining confidence intervals that tight would require many hundreds of thousands of observations, while they likely had only ∼400"
- Break condition: If Hoffmann et al. used intermediate loss values or special clustering methods not reported, the confidence intervals could be narrower.

## Foundational Learning

- Concept: Statistical significance testing using χ² distributions
  - Why needed here: To determine whether parameter differences between studies are due to random sampling or systematic issues
  - Quick check question: What distributional assumption underlies the χ² test used to compare parameter estimates?

- Concept: Huber loss function for robust regression
  - Why needed here: To fit the scaling law while being robust to outliers in the loss data
  - Quick check question: How does the Huber loss function balance between squared error and absolute error?

- Concept: Compute-optimal scaling relationships
  - Why needed here: To interpret the parameters a and b in terms of optimal model size and training token allocation
  - Quick check question: How do the parameters a and b relate to the optimal allocation of compute between model size and training tokens?

## Architecture Onboarding

- Component map:
  - Data extraction -> Model fitting -> Statistical testing -> Confidence interval analysis -> Scaling policy derivation

- Critical path:
  1. Extract data from published figures
  2. Fit parametric scaling law using Huber loss
  3. Compute statistical significance of parameter differences
  4. Analyze confidence interval plausibility
  5. Derive and compare scaling policies

- Design tradeoffs:
  - Using Huber loss vs. standard squared error: More robust to outliers but introduces a hyperparameter
  - Bootstrap vs. analytical confidence intervals: More flexible but computationally expensive
  - χ² test vs. other significance tests: Appropriate for multivariate normal differences but assumes specific distributional properties

- Failure signatures:
  - Poor model fit (large residuals) suggests parametric form is wrong
  - Implausibly narrow confidence intervals suggest methodological issues
  - Inconsistent scaling policies across methods suggest parameter estimation problems

- First 3 experiments:
  1. Replicate the data extraction process on a simpler figure to verify the SVG parsing method
  2. Fit the scaling law to synthetic data with known parameters to verify the optimization works
  3. Perform bootstrap analysis on a small dataset to understand the variance structure before applying to the full data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Did Hoffmann et al. use only final loss values or intermediate training losses when fitting their parametric scaling law model?
- Basis in paper: [explicit] The paper states Hoffmann et al. explicitly reported using "final losses from experiments" in Approach 3, but also mentions they might have used intermediate losses which could explain their implausibly narrow confidence intervals.
- Why unresolved: The authors of this replication attempt find the narrow confidence intervals reported by Hoffmann et al. to be implausible given their dataset size, suggesting they may have used intermediate losses. However, Hoffmann et al. did not clearly specify this in their paper.
- What evidence would resolve it: A clear statement from Hoffmann et al. clarifying whether intermediate training losses were used in their Approach 3 analysis would resolve this question.

### Open Question 2
- Question: What is the cause of the outlier models with high losses relative to their compute budgets in Hoffmann et al.'s dataset?
- Basis in paper: [explicit] The replication attempt notes a column of points with training FLOP around 10^19 where larger models obtained unusually high loss values, up to 70% higher than models with similar compute budgets.
- Why unresolved: The authors of the replication attempt exclude these outliers from their analysis and note that the cause is unclear, suggesting it could be due to data quality, training instability, or other factors.
- What evidence would resolve it: Analysis of the training process, data quality, or other factors for these specific models could explain why they performed poorly relative to their compute budgets.

### Open Question 3
- Question: How would different parametric forms for the scaling law affect the conclusions about optimal model size and training token allocation?
- Basis in paper: [inferred] The replication attempt uses the same parametric form as Hoffmann et al. (L(N, D) = E + A/N^α + B/D^β) but obtains significantly different parameter estimates, leading to different conclusions about optimal scaling.
- Why unresolved: The authors only explore one parametric form, so it's unclear if their conclusions would hold under different model specifications or if the discrepancies with Hoffmann et al. are due to the choice of parametric form.
- What evidence would resolve it: Fitting alternative parametric forms to the same data and comparing the resulting optimal scaling policies would show how sensitive the conclusions are to model specification.

### Open Question 4
- Question: How robust are the compute-optimal scaling laws to variations in data quality and model architecture?
- Basis in paper: [explicit] The replication attempt notes that Bi et al. (2024) found the optimal token-to-parameter ratio is sensitive to data quality, and Anil et al. (2023) replicated the finding that model size and training tokens should scale proportionally but may have used different architectures or data.
- Why unresolved: The original Hoffmann et al. study and this replication attempt only consider dense transformer architectures trained on a specific dataset, so it's unclear how general the scaling laws are.
- What evidence would resolve it: Systematic experiments varying data quality, model architectures (e.g., sparse vs dense, different attention mechanisms), and other factors while measuring the resulting scaling laws would show their robustness to these variations.

## Limitations

- The analysis depends on extracting data from published figures, which introduces reconstruction uncertainty.
- The exact methodology Hoffmann et al. used for fitting and computing confidence intervals is not fully known.
- The comparison is limited to the same parametric form, so differences could stem from the choice of model specification.

## Confidence

**High Confidence Claims**:
- The authors' fitted model provides better fit to the reconstructed data than Hoffmann et al.'s reported estimates.
- The difference between parameter estimates is statistically significant (p < 10^-60).

**Medium Confidence Claims**:
- Hoffmann et al.'s confidence intervals are implausibly narrow given their dataset size.
- The scaling policy derived from Hoffmann et al.'s parameters is inconsistent with their other approaches.

**Low Confidence Claims**:
- Any claims about the absolute correctness of either model, since both depend on data reconstruction and methodological assumptions that cannot be fully verified.

## Next Checks

1. **Cross-validation on Synthetic Data**: Generate synthetic datasets following the assumed parametric form with known parameters and noise levels. Apply both the data reconstruction process and fitting methodology to verify that the pipeline recovers the true parameters within expected uncertainty bounds.

2. **Bootstrap Analysis of Confidence Intervals**: Perform bootstrap resampling on the reconstructed dataset to empirically estimate confidence intervals using the same methodology that would have been applied to Hoffmann et al.'s data. Compare these empirical intervals with those reported to verify the plausibility calculations.

3. **Independent Data Source Verification**: If possible, obtain access to the raw data used by Hoffmann et al. (either through their code release or direct request) and re-run the entire analysis pipeline. This would definitively resolve whether the discrepancies stem from data reconstruction issues or fundamental methodological differences.