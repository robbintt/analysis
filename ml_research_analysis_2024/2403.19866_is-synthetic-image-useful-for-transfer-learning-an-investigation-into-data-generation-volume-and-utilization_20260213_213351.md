---
ver: rpa2
title: Is Synthetic Image Useful for Transfer Learning? An Investigation into Data
  Generation, Volume, and Utilization
arxiv_id: '2403.19866'
source_url: https://arxiv.org/abs/2403.19866
tags:
- transfer
- data
- synthetic
- images
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether synthetic images generated by text-to-image
  models can improve transfer learning from ImageNet. It finds that naively mixing
  synthetic and real images degrades performance due to distribution gaps.
---

# Is Synthetic Image Useful for Transfer Learning? An Investigation into Data Generation, Volume, and Utilization

## Quick Facts
- arXiv ID: 2403.19866
- Source URL: https://arxiv.org/abs/2403.19866
- Reference count: 40
- Primary result: Bridged transfer improves classification accuracy by up to 30% compared to naive mixing of synthetic and real images

## Executive Summary
This paper investigates whether synthetic images generated by text-to-image models can enhance transfer learning from ImageNet to downstream tasks. The authors find that naively mixing synthetic and real images degrades performance due to distribution gaps. To address this, they propose "bridged transfer," a two-stage fine-tuning method that first trains on synthetic data to improve transferability, then adapts on real data. They also introduce Dataset Style Inversion to align synthetic and real image styles. Evaluated across 10 datasets and 5 models, bridged transfer consistently improves accuracy, with benefits increasing as more synthetic data is used.

## Method Summary
The paper proposes a two-stage transfer learning framework called "bridged transfer" to utilize synthetic images for improving transfer learning from ImageNet. The method first fine-tunes a pre-trained model on synthetic images generated by Stable Diffusion V1.5, then adapts the model on real target dataset images. To reduce the distribution gap between synthetic and real images, the authors introduce Dataset Style Inversion (DSI), which learns a style token representing the target dataset's characteristics and uses it to guide synthetic image generation. The approach is compared against vanilla transfer (only real images) and mixed transfer (real + synthetic images) across 10 classification datasets.

## Key Results
- Bridged transfer achieves up to 30% accuracy improvement over naive mixing of synthetic and real images
- Benefits increase with more synthetic data, with no saturation point identified within tested ranges (up to 3,000 images per class)
- Dataset Style Inversion reduces the need for large volumes of synthetic data while maintaining performance gains
- Bridged transfer shows consistent improvements across 10 different datasets and 5 model architectures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mixing synthetic and real images degrades performance due to distribution gap
- Mechanism: Synthetic images have different style and artifact characteristics that conflict with real image distribution, causing the model to overfit to synthetic artifacts instead of learning real image features
- Core assumption: Synthetic and real images follow different underlying distributions that cannot be easily aligned by naive mixing
- Evidence anchors:
  - [abstract] "we observe that their naive incorporation into existing real-image datasets does not consistently enhance model performance due to the inherent distribution gap between synthetic and real images"
  - [section] "Employing a naive approach to mix these distributions tends to deteriorate the quality of the training data"
- Break condition: If synthetic images are generated with perfect style alignment to real images, the distribution gap would be eliminated

### Mechanism 2
- Claim: Bridged transfer improves transferability by first learning synthetic data then adapting to real data
- Mechanism: Fine-tuning on synthetic data first allows the model to learn more diverse features that are transferable, then fine-tuning on real data adapts these features to the real distribution without the synthetic artifacts interfering
- Core assumption: Synthetic data contains useful features that are transferable to real data, but the model needs to first learn these features before adapting to real data
- Evidence anchors:
  - [abstract] "initially employs synthetic images for fine-tuning a pre-trained model to improve its transferability and subsequently uses real data for rapid adaptation"
  - [section] "bridged transfer consistently achieves faster convergence on the real training set as compared to vanilla transfer"
- Break condition: If synthetic data does not contain transferable features or the distribution gap is too large, the bridged transfer approach would fail

### Mechanism 3
- Claim: Dataset Style Inversion (DSI) improves alignment between synthetic and real images
- Mechanism: Learning a style token that captures the dataset's stylistic characteristics and using it during image generation creates synthetic images with similar style to real images, reducing the distribution gap
- Core assumption: Style is a major component of the distribution gap between synthetic and real images, and it can be captured by a single token
- Evidence anchors:
  - [abstract] "We propose dataset style inversion strategy to improve the stylistic alignment between synthetic and real images"
  - [section] "DSI effectively concentrates the downstream dataset into a style token, subsequently employed to steer the image generation process"
- Break condition: If style is not the primary source of distribution gap, or if it cannot be captured by a single token, DSI would not be effective

## Foundational Learning

- Concept: Transfer learning
  - Why needed here: The paper is about improving transfer learning from ImageNet to downstream datasets using synthetic data
  - Quick check question: What is the difference between transfer learning and training from scratch?

- Concept: Domain adaptation
  - Why needed here: The paper addresses the domain gap between synthetic and real images, which is a domain adaptation problem
  - Quick check question: How does domain adaptation differ from standard transfer learning?

- Concept: Style transfer and image generation
  - Why needed here: The paper uses Stable Diffusion to generate synthetic images and introduces DSI to control the style of generated images
  - Quick check question: What is the difference between style transfer and image generation?

## Architecture Onboarding

- Component map: Stable Diffusion model for image generation → Bridged transfer framework (two-stage fine-tuning) → Dataset Style Inversion (style token learning) → Evaluation on downstream datasets
- Critical path: Image generation → Fine-tune on synthetic data → Fine-tune on real data → Evaluate on test set
- Design tradeoffs: Generating more synthetic images improves performance but increases computational cost; DSI reduces computational cost but may not capture all style variations
- Failure signatures: Performance degradation when mixing synthetic and real data; convergence issues in bridged transfer; DSI failing to capture dataset style
- First 3 experiments:
  1. Compare vanilla transfer vs mixed transfer vs bridged transfer on a small dataset
  2. Test bridged transfer with different amounts of synthetic data
  3. Evaluate DSI vs single template prompt on a fine-grained dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does bridged transfer with synthetic data generalize to other types of transfer learning beyond classification tasks, such as object detection or semantic segmentation?
- Basis in paper: [inferred] The paper focuses exclusively on classification tasks and does not explore other transfer learning applications.
- Why unresolved: The study is limited to classification datasets and does not test the framework on other computer vision tasks.
- What evidence would resolve it: Empirical results showing the effectiveness of bridged transfer on object detection and semantic segmentation datasets, compared to vanilla transfer.

### Open Question 2
- Question: What is the optimal number of synthetic images per class that maximizes transfer learning performance without reaching a saturation point?
- Basis in paper: [explicit] The paper states that the benefits of synthetic data were not yet saturated, indicating that performance may further increase with more data, but does not identify a specific saturation point.
- Why unresolved: The study only tests up to 3,000 synthetic images per class and does not explore higher volumes.
- What evidence would resolve it: Experiments testing synthetic data volumes beyond 3,000 images per class to identify when performance gains plateau.

### Open Question 3
- Question: How does the choice of guidance scale in Stable Diffusion affect the quality and diversity of synthetic images, and what is the optimal balance for transfer learning?
- Basis in paper: [explicit] The paper experiments with different guidance scales and finds that bridged transfer is robust to changes, but does not identify an optimal value.
- Why unresolved: The study finds no fixed optimal guidance scale across datasets and only tests a limited range of values.
- What evidence would resolve it: Systematic experiments testing a wider range of guidance scales and their impact on transfer learning performance across multiple datasets.

### Open Question 4
- Question: Can dataset style inversion be extended to learn multiple style tokens for different subsets of classes within a dataset, rather than a single token for the entire dataset?
- Basis in paper: [inferred] The paper introduces a single style token for the entire dataset but does not explore the possibility of multiple tokens.
- Why unresolved: The study focuses on a single style token approach and does not investigate the potential benefits of multiple tokens.
- What evidence would resolve it: Experiments comparing the performance of single-token versus multi-token style inversion on datasets with diverse class styles.

## Limitations

- The findings are based on Stable Diffusion V1.5 and may not generalize to other generation models or prompt strategies
- Computational cost of generating large volumes of synthetic images remains a practical limitation
- The study focuses primarily on classification tasks and ResNet architectures, leaving uncertainty about performance on other vision tasks or architectures
- The distribution gap mechanism is theoretically sound but the relative contribution of style versus other factors remains unquantified

## Confidence

**High Confidence**: The core finding that naive mixing of synthetic and real images degrades performance is well-supported by experimental evidence across 10 datasets and multiple architectures. The bridged transfer framework showing consistent improvements is also highly reliable.

**Medium Confidence**: The Dataset Style Inversion technique's effectiveness is demonstrated but relies on a single style token to capture dataset characteristics, which may oversimplify complex style variations. The claim that bridged transfer is consistently beneficial across all tested scenarios is supported but may have edge cases not explored.

**Low Confidence**: The theoretical explanation that style is the primary component of the distribution gap lacks quantitative validation. The paper does not systematically ablate other potential gap sources (e.g., semantic fidelity, object pose distribution, background statistics).

## Next Checks

1. **Ablation study on distribution gap components**: Systematically measure the relative contribution of style alignment versus semantic fidelity and object distribution alignment to the performance gap between synthetic and real images.

2. **Cross-model generalization test**: Evaluate bridged transfer performance when using synthetic images generated by different text-to-image models (e.g., DALL-E 2, Midjourney) or with different prompt engineering strategies to assess robustness.

3. **Task and architecture diversity validation**: Test bridged transfer on non-classification tasks (object detection, semantic segmentation) and modern architectures (Vision Transformers) to determine if benefits extend beyond the current experimental scope.