---
ver: rpa2
title: 'QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs'
arxiv_id: '2404.00456'
source_url: https://arxiv.org/abs/2404.00456
tags:
- quantization
- quarot
- hadamard
- llama
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: QuaRot introduces a new quantization method for large language
  models (LLMs) that enables end-to-end 4-bit inference by eliminating outlier features
  in activations and KV caches using Hadamard transformations. The method rotates
  weights, activations, and KV caches in a way that removes outliers without changing
  the model output, making quantization easier.
---

# QuaRot: Outlier-Free 4-Bit Inference in Rotated LLMs

## Quick Facts
- **arXiv ID**: 2404.00456
- **Source URL**: https://arxiv.org/abs/2404.00456
- **Reference count**: 40
- **Primary result**: Achieves end-to-end 4-bit inference with 99% zero-shot performance preservation on LLaMA-2-70B models

## Executive Summary
QuaRot introduces a novel quantization method for large language models that enables efficient 4-bit inference by eliminating outlier features through Hadamard transformations. The approach rotates weights, activations, and KV caches to remove outliers without altering model outputs, making quantization more effective. This technique preserves 99% of zero-shot performance on LLaMA-2-70B models while achieving up to 3.33× speedup in prefill stage and 3.89× memory savings during decoding.

## Method Summary
QuaRot employs randomized Hadamard transformations to rotate weight matrices and eliminate outlier features in activations and KV caches. The method fuses these transformations into the model structure, enabling all matrix multiplications to be performed in 4 bits. The approach uses round-to-nearest quantization without requiring calibration data, supporting lossless 6 and 8-bit quantization. By transforming the weight matrices and fusing them into the model, QuaRot maintains model accuracy while significantly reducing memory footprint and computation requirements.

## Key Results
- Preserves 99% of zero-shot performance on LLaMA-2-70B models
- Achieves up to 3.33× speedup in prefill stage
- Provides 3.89× memory savings during decoding

## Why This Works (Mechanism)
QuaRot leverages Hadamard transformations to rotate weight matrices and eliminate outlier features that typically hinder low-bit quantization. By removing these outliers from activations and KV caches, the method creates a more uniform distribution of values that can be effectively quantized to 4 bits without significant precision loss. The randomized nature of the transformations ensures that outliers are distributed across the matrix, making them easier to handle during quantization.

## Foundational Learning
- **Hadamard Transformations**: Orthogonal transformations used to rotate data without changing its intrinsic properties. Why needed: To redistribute outlier features across the matrix for easier quantization. Quick check: Verify that the transformation preserves vector norms.
- **Outlier Elimination**: Process of removing extreme values that cause quantization errors. Why needed: To create a more uniform value distribution suitable for low-bit quantization. Quick check: Measure the reduction in value range before and after transformation.
- **Round-to-Nearest Quantization**: Simple quantization method that maps values to the nearest representable point. Why needed: To minimize quantization error without complex calibration. Quick check: Compare quantization error rates with other methods.
- **KV Cache Optimization**: Techniques to reduce memory usage in attention mechanisms. Why needed: To minimize the memory footprint of long-context processing. Quick check: Measure memory usage at different context lengths.

## Architecture Onboarding
- **Component Map**: Input -> Hadamard Rotation -> Quantized Matrix Multiplication -> Output
- **Critical Path**: Token embedding → Attention computation → Feed-forward network → Output prediction
- **Design Tradeoffs**: Precision vs. performance, memory vs. speed, complexity vs. accuracy
- **Failure Signatures**: Increased quantization error, degraded model accuracy, unexpected runtime behavior
- **First Experiments**:
  1. Test basic Hadamard rotation on a small weight matrix
  2. Verify quantization error rates with and without rotation
  3. Measure memory usage at different context lengths

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Generalizability to different model architectures beyond LLaMA-2-70B remains uncertain
- Effectiveness in long-context processing scenarios not thoroughly explored
- Potential computational overhead in resource-constrained environments not fully characterized

## Confidence
- **High Confidence**: Core mathematical framework of Hadamard transformations for outlier elimination is sound
- **Medium Confidence**: 99% performance preservation demonstrated but may not hold across all benchmarks
- **Low Confidence**: Long-term stability effects on continuously fine-tuned models

## Next Checks
1. **Cross-Architecture Validation**: Implement and test QuaRot on diverse LLM architectures (OPT, BLOOM) and varying parameter counts (1B, 7B, 13B)
2. **Long-Context Performance Analysis**: Test with context lengths exceeding 4096 tokens to evaluate effectiveness in long-document processing
3. **Production Deployment Simulation**: Create benchmarks simulating real-world deployment with varying batch sizes, concurrent requests, and hardware configurations