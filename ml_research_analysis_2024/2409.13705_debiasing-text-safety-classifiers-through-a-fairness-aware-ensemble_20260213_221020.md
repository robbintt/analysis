---
ver: rpa2
title: Debiasing Text Safety Classifiers through a Fairness-Aware Ensemble
arxiv_id: '2409.13705'
source_url: https://arxiv.org/abs/2409.13705
tags:
- counterfactual
- sentence
- seed
- identity
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper presents a post-processing debiasing method for text
  safety classifiers that builds an ensemble model on top of existing closed-source
  classifiers to mitigate counterfactual fairness issues. The approach uses Fair Data
  Reweighting (FDW) to reweight training data based on fairness metrics, training
  a small ensemble model on source model outputs.
---

# Debiasing Text Safety Classifiers through a Fairness-Aware Ensemble

## Quick Facts
- arXiv ID: 2409.13705
- Source URL: https://arxiv.org/abs/2409.13705
- Reference count: 30
- Method achieves 66.2% reduction in Average Counterfactual Variance for hate and 61.9% for violence while maintaining model performance

## Executive Summary
This paper introduces a post-processing debiasing method for text safety classifiers that builds an ensemble model on top of existing closed-source classifiers to mitigate counterfactual fairness issues. The approach uses Fair Data Reweighting (FDW) to reweight training data based on fairness metrics, training a small ensemble model on source model outputs. The method introduces two threshold-agnostic metrics (Average Counterfactual Variance and Sliced Averages) to measure counterfactual fairness and creates two new counterfactually balanced datasets covering four harm categories. Results show improved counterfactual fairness with minimal performance degradation.

## Method Summary
The approach involves building an ensemble that leverages outputs from multiple closed-source text safety classifiers (Detoxify and two proprietary models) as features. A baseline ensemble is first trained on the original dataset, then Fair Data Reweighting (FDW) is applied using Sliced Averages metrics computed from the baseline model to resample training examples proportional to bias levels. The debiased ensemble is then trained on this reweighted dataset. The method introduces two new counterfactually balanced datasets (expanded OpenAI dataset and LLM-generated dataset) and two threshold-agnostic metrics for evaluating counterfactual fairness.

## Key Results
- Achieved 66.2% reduction in Average Counterfactual Variance for hate and 61.9% for violence categories
- Maintained model performance with only 1.8% and 0.1% drops in AU-PRC on original test sets
- Achieved 13.9% and 12.8% gains in AU-PRC on counterfactual test sets
- Successfully mitigated counterfactual fairness biases across multiple identity subgroups (race, religion, gender, sexual orientation)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The ensemble approach mitigates counterfactual fairness biases by reweighting training data based on fairness metrics.
- Mechanism: Fair Data Reweighting (FDW) computes Sliced Averages (SA) metrics from a baseline model, then resamples training examples proportional to the level of bias across subgroups. This creates a more balanced training distribution.
- Core assumption: Source model outputs contain sufficient information to predict the final task while being less biased than raw text inputs.
- Evidence anchors:
  - [abstract] "Our approach involves building an ensemble that not only outperforms the input classifiers and policy-aligns them, but also acts as a debiasing regularizer"
  - [section 4] "Using SA evaluation of the baseline model per subgroup slice as a proxy for model fairness, FDW resamples training examples from these slices proportional to the level of bias"
  - [corpus] Weak - related papers focus on fairness-aware methods but don't specifically validate this reweighting mechanism

### Mechanism 2
- Claim: The two threshold-agnostic metrics (ACV and SA) effectively measure counterfactual fairness and guide the debiasing process.
- Mechanism: ACV measures variance in predictions across counterfactual sets, while SA measures average scores per subgroup. These metrics identify problematic identity categories and subgroups respectively.
- Core assumption: Variance in predictions across counterfactuals indicates bias, and subgroup-specific average scores reveal which groups are most affected.
- Evidence anchors:
  - [abstract] "We introduce two threshold-agnostic metrics to assess the counterfactual fairness of a model"
  - [section 3] "We propose two quantitative metrics to measure counterfactual fairness... ACV is a broad measure which reveals problematic identity categories for a harm category"
  - [corpus] Weak - related papers discuss fairness metrics but don't specifically validate ACV and SA for text safety classifiers

### Mechanism 3
- Claim: The ensemble architecture outperforms individual source models while inheriting and mitigating their biases.
- Mechanism: A small ensemble model trained on source model outputs can leverage complementary strengths of different classifiers while regularizing against their biases through the FDW process.
- Core assumption: Source classifiers provide complementary information and their outputs contain sufficient semantic information for the ensemble to learn the task.
- Evidence anchors:
  - [abstract] "Our approach involves building an ensemble that not only outperforms the input classifiers and policy-aligns them, but also acts as a debiasing regularizer"
  - [section 4] "Our ensemble approach is also motivated by an ability to better leverage complementary strengths of existing classifiers and outperform them"
  - [corpus] Weak - related papers discuss ensemble methods but don't specifically validate this approach for debiasing text safety classifiers

## Foundational Learning

- Concept: Counterfactual fairness
  - Why needed here: The entire paper focuses on mitigating counterfactual fairness issues in text safety classifiers
  - Quick check question: What distinguishes counterfactual fairness from traditional group fairness metrics?

- Concept: Fair Data Reweighting (FDW)
  - Why needed here: FDW is the core mechanism for mitigating biases by reweighting training data based on fairness metrics
  - Quick check question: How does FDW use Sliced Averages as a proxy for model fairness to determine sample weights?

- Concept: Threshold-agnostic metrics
  - Why needed here: The paper introduces metrics that work across different classification thresholds, which is important for industrial applications
  - Quick check question: Why would threshold-agnostic metrics be more useful than threshold-dependent metrics for debiasing?

## Architecture Onboarding

- Component map:
  - Source classifiers (Detoxify and two proprietary models)
  - Ensemble model (Random Forest with 34 input features, 4 outputs)
  - Fair Data Reweighting module
  - Dataset generation pipeline (templated LLM generation and identity injection)
  - Evaluation framework (ACV and SA metrics)

- Critical path: Data generation → Baseline ensemble training → SA metric computation → FDW reweighting → Debiased ensemble training → Evaluation

- Design tradeoffs:
  - Using source model outputs as features vs. raw text inputs
  - Computational efficiency of post-processing vs. in-training debiasing
  - Balancing between model performance and fairness improvements

- Failure signatures:
  - High feature contributions from biased attributes in the ensemble
  - Minimal improvement in ACV and SA metrics after debiasing
  - Significant performance degradation on original test set

- First 3 experiments:
  1. Train baseline ensemble on original training set and compute SA metrics
  2. Apply FDW with different λ and β hyperparameters to create reweighted datasets
  3. Train debiased ensembles on reweighted datasets and compare ACV and SA metrics against baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the ensemble debiasing approach perform when applied to source models with overlapping biases across multiple identity categories?
- Basis in paper: [explicit] The paper notes that "significant updates in underlying source models may call for retraining the ensemble" and acknowledges that "in scenarios where all of the input features that may be useful in predicting a harm exhibit significant biases, it is possible that our approach may experience significant performance degradation."
- Why unresolved: The paper only demonstrates results on two harm categories (Hate and Violence) and doesn't explore scenarios where all source model features are similarly biased. The authors acknowledge this limitation but don't provide empirical evidence of how the approach would perform in such extreme cases.
- What evidence would resolve it: Experiments showing the approach's performance when source models have overlapping biases across multiple identity categories, or when all input features are biased, would clarify the method's limitations and robustness.

### Open Question 2
- Question: How does the method's effectiveness vary across different languages beyond English, and what adaptations would be necessary for multilingual deployment?
- Basis in paper: [explicit] The paper states "In this study, we focus on the English language, we plan to test on more languages in the future" and acknowledges this as a limitation.
- Why unresolved: The authors explicitly identify this as a limitation but haven't tested the approach on other languages. The effectiveness of the ensemble approach, data generation techniques, and fairness metrics across languages remains unexplored.
- What evidence would resolve it: Empirical results showing the approach's performance on multilingual datasets, including comparisons of debiasing effectiveness across languages and any necessary modifications to the methodology.

### Open Question 3
- Question: What is the optimal balance between the hyperparameters λ (example weight) and β (sampling sharpness) for maximizing fairness gains while minimizing performance degradation?
- Basis in paper: [explicit] The paper mentions these hyperparameters and provides controlled experiments showing their individual effects on fairness metrics, but doesn't explore their combined optimization or the trade-off between them.
- Why unresolved: While the paper shows how each hyperparameter affects fairness individually, it doesn't provide guidance on how to balance them together or determine optimal values for different use cases.
- What evidence would resolve it: A systematic exploration of the parameter space showing the combined effects of λ and β on both fairness metrics and model performance, along with recommendations for setting these parameters based on different priorities.

## Limitations

- Results are demonstrated only on four specific harm categories (Hate, Violence, Sexual, Toxicity) and may not generalize to other types of content moderation tasks
- The approach relies on closed-source models whose individual biases cannot be fully examined, limiting understanding of how they contribute to the ensemble
- The effectiveness of the debiasing approach depends on source model outputs containing complementary information, which may not hold in all scenarios

## Confidence

- High confidence: The experimental methodology and dataset creation process are well-documented
- Medium confidence: The effectiveness of the debiasing approach given the improvements in counterfactual fairness metrics
- Low confidence: The generalizability of results to other harm categories and identity subgroups beyond those tested

## Next Checks

1. Validate the ACV and SA metrics by testing them on established benchmark datasets with known fairness issues
2. Test the approach with different ensemble architectures (beyond Random Forest) to assess robustness of the debiasing effect
3. Conduct ablation studies removing the FDW component to quantify its specific contribution to counterfactual fairness improvements