---
ver: rpa2
title: 'BEnQA: A Question Answering and Reasoning Benchmark for Bengali and English'
arxiv_id: '2403.10900'
source_url: https://arxiv.org/abs/2403.10900
tags:
- english
- bengali
- questions
- dataset
- question
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: BEnQA is a new parallel dataset of Bengali and English science
  exam questions covering factual, application, and reasoning types across middle
  and high school subjects. Experiments show that large language models perform notably
  worse on Bengali questions than English ones, with open-source models lagging far
  behind proprietary models.
---

# BEnQA: A Question Answering and Reasoning Benchmark for Bengali and English

## Quick Facts
- arXiv ID: 2403.10900
- Source URL: https://arxiv.org/abs/2403.10900
- Reference count: 40
- Large language models perform notably worse on Bengali questions than English ones, with open-source models lagging far behind proprietary models.

## Executive Summary
BEnQA introduces a parallel dataset of 5,161 Bengali-English science exam questions spanning factual, application, and reasoning types across middle and high school subjects. The benchmark reveals significant performance gaps between languages and model types, with proprietary models substantially outperforming open-source ones in Bengali. Experiments demonstrate that Chain-of-Thought prompting primarily benefits reasoning and application questions, while appending English translations to Bengali prompts provides consistent accuracy improvements across subjects.

## Method Summary
The study benchmarks large language models on a manually curated parallel dataset of Bengali-English science questions from Bangladeshi school exams. Models are evaluated using zero-shot and few-shot approaches with Chain-of-Thought prompting and translation-augmented prompts. Performance is measured by accuracy on multiple-choice questions, analyzed separately for factual, application, and reasoning types across subjects. Manual evaluation is used to handle non-standard Bengali outputs.

## Key Results
- Proprietary models (GPT-4, Claude 2.1) significantly outperform open-source models (LLaMA-2, Mistral) on Bengali questions
- Chain-of-Thought prompting improves reasoning and application question accuracy by 10-20%, but has minimal effect on factual questions
- Appending English translations to Bengali prompts improves performance across all subjects, especially in Biology

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Appending English translations to Bengali prompts improves LLM performance in Bengali question answering.
- Mechanism: Providing translations bridges the vocabulary gap in scientific terminology and improves comprehension of non-Latin scripts, leveraging the stronger language model performance on English.
- Core assumption: The LLM's performance bottleneck in Bengali is due to unfamiliarity with domain-specific terminology and script, not fundamental reasoning ability.
- Evidence anchors:
  - [abstract] "We also find that appending English translation helps to answer questions in Bengali."
  - [section] "We expect that having access to a translation of the question would likely help the model to understand the context better." Experiments showed improvements in all subjects, especially Biology.
  - [corpus] Weak evidence: only experiments in BEnQA, COPA, and Big-Bench-Hard are mentioned, no broader multilingual dataset validation.
- Break condition: If the model fails to understand the appended English translation or if the Bengali question requires culturally specific context not captured in English.

### Mechanism 2
- Claim: Chain-of-Thought (CoT) prompting is more beneficial for reasoning and application questions than for factual questions.
- Mechanism: CoT encourages step-by-step reasoning, which aligns with the multi-step analysis required for reasoning and application questions but is unnecessary for direct factual recall.
- Core assumption: The model's reasoning ability is latent and can be elicited by structured prompting, while factual knowledge retrieval does not benefit from reasoning scaffolding.
- Evidence anchors:
  - [abstract] "We also investigate some prompting methods, and find that Chain-of-Thought (CoT) prompting is beneficial mostly on reasoning questions, but not so much on factual ones."
  - [section] "Reasoning and application questions particularly benefit from CoT prompting, with accuracy improvements ranging by 10–20%. In contrast, we observed a minimal improvement in factual questions."
  - [corpus] Weak evidence: only BEnQA dataset used, no cross-dataset validation of this effect.
- Break condition: If the question type categorization is incorrect or if the model overfits to CoT format without genuine reasoning.

### Mechanism 3
- Claim: The performance gap between proprietary and open-source models is substantial, with open-source models lagging far behind in Bengali.
- Mechanism: Proprietary models have larger model sizes, better training data, and more optimized tokenization for multilingual tasks, leading to superior performance.
- Core assumption: Model size and training data quality are the primary drivers of performance disparity, not architectural differences.
- Evidence anchors:
  - [abstract] "Experiments show that large language models perform notably worse on Bengali questions than English ones, with open-source models lagging far behind proprietary models."
  - [section] "Our result shows that proprietary LLMs are far ahead of open-source LLMs such as LLaMA-2 and Mistral." GPT-4 showed the largest performance gap.
  - [corpus] Weak evidence: only a few models compared, no ablation on model size or training data specifics.
- Break condition: If a new open-source model with comparable size and data quality is introduced, or if tokenization inefficiencies are addressed.

## Foundational Learning

- Concept: Prompt engineering (Chain-of-Thought, translation appending)
  - Why needed here: To elicit reasoning behavior and bridge language gaps in low-resource settings.
  - Quick check question: How does CoT prompting differ from direct answering in terms of model output structure?

- Concept: Multilingual benchmarking
  - Why needed here: To fairly compare model performance across languages and identify specific weaknesses.
  - Quick check question: Why is a parallel corpus important for measuring language-specific performance gaps?

- Concept: Low-resource language challenges
  - Why needed here: To understand why Bengali lags behind English and what interventions can help.
  - Quick check question: What are the main barriers to LLM performance in low-resource languages?

## Architecture Onboarding

- Component map: Dataset curation (Bengali→English parallel questions) → Model benchmarking (zero/few-shot) → Prompt engineering experiments (CoT, translation appending) → Performance analysis by subject/type
- Critical path: Curate parallel dataset → Benchmark models → Identify performance gaps → Test prompt interventions → Analyze results
- Design tradeoffs: Zero-shot vs. few-shot (cost vs. performance), Bengali-only vs. translation-augmented prompts (purity vs. accuracy), open-source vs. proprietary models (accessibility vs. capability)
- Failure signatures: No improvement from CoT (question categorization wrong), translation appending hurts performance (translation quality poor), open-source models match proprietary (benchmark flawed)
- First 3 experiments:
  1. Benchmark GPT-4 on BEnQA English vs. Bengali to confirm performance gap
  2. Apply CoT prompting on Bengali subset to test reasoning improvement
  3. Append English translations to Bengali prompts and measure accuracy change

## Open Questions the Paper Calls Out

### Open Question 1
- Question: To what extent does the Chain-of-Thought prompting technique generalize to other low-resource languages beyond Bengali?
- Basis in paper: [inferred] The paper investigates the effectiveness of Chain-of-Thought prompting on Bengali questions and finds it beneficial for reasoning and application questions but not so much for factual ones. It also mentions the need for future research to see how such prompting methods utilizing high-resource languages generalize to other datasets and languages.
- Why unresolved: The study focuses on Bengali as a case study for low-resource languages. The impact of Chain-of-Thought prompting on other low-resource languages remains unexplored.
- What evidence would resolve it: Conducting experiments with Chain-of-Thought prompting on question-answering datasets in other low-resource languages and comparing the results with Bengali would provide evidence of its generalizability.

### Open Question 2
- Question: How does the performance of open-source language models in Bengali compare to proprietary models in the long term, and what are the key factors influencing this performance gap?
- Basis in paper: [explicit] The paper observes a substantial performance gap between open-source and proprietary language models in Bengali, with open-source models lagging far behind proprietary models.
- Why unresolved: The study provides a snapshot of the current performance gap but does not explore the long-term trends or the underlying factors contributing to the disparity.
- What evidence would resolve it: Longitudinal studies tracking the performance of open-source and proprietary models in Bengali over time, along with in-depth analyses of the factors influencing their performance, would shed light on the long-term trends and key factors.

### Open Question 3
- Question: What are the optimal strategies for incorporating English translations into prompts to improve the performance of language models in low-resource languages, and how do these strategies vary across different task types?
- Basis in paper: [explicit] The paper investigates the impact of appending English translations to Bengali prompts and finds that it helps improve performance, particularly in subjects with more scientific terminology. It suggests further research to optimize and refine the use of language models in low-resource languages.
- Why unresolved: The study provides initial evidence of the benefits of appending English translations but does not explore the optimal strategies or how they vary across different task types.
- What evidence would resolve it: Systematic experiments comparing different strategies for incorporating English translations (e.g., appending vs. interleaving, using human vs. machine translations) across various task types in low-resource languages would identify the optimal approaches.

## Limitations

- Dataset size of 5,161 questions may not generalize to broader scientific or general knowledge domains
- Focus on middle and high school science exams limits applicability to other domains
- Findings based on a single dataset and limited set of models may not reflect universal principles

## Confidence

- Translation-augmented prompts improve performance: Medium
- CoT prompting benefits reasoning over factual questions: Medium
- Proprietary models outperform open-source models in Bengali: High

## Next Checks

1. Test the translation-augmented prompt approach on additional multilingual QA datasets (e.g., XCOPA, multilingual variants of Big-Bench) to verify generalizability.
2. Conduct ablation studies varying model size and training data composition for open-source models to isolate the sources of performance gaps.
3. Perform human evaluation of translation quality and its impact on reasoning performance to distinguish between vocabulary assistance and genuine comprehension improvement.