---
ver: rpa2
title: Increasing the Robustness of Model Predictions to Missing Sensors in Earth
  Observation
arxiv_id: '2407.15512'
source_url: https://arxiv.org/abs/2407.15512
tags:
- missing
- sensor
- data
- sensors
- multi-sensor
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces two novel methods to improve model robustness
  to missing sensors in multi-sensor Earth Observation (EO) settings: Input Sensor
  Dropout (ISensD) and Ensemble Sensor Invariant (ESensI). ISensD randomly masks sensor
  features during training to simulate missing data, while ESensI uses shared prediction
  heads across sensor-dedicated models to create sensor-invariant predictions.'
---

# Increasing the Robustness of Model Predictions to Missing Sensors in Earth Observation

## Quick Facts
- **arXiv ID:** 2407.15512
- **Source URL:** https://arxiv.org/abs/2407.15512
- **Reference count:** 40
- **Primary result:** Two novel methods (ISensD and ESensI) significantly improve model robustness to missing sensors in multi-sensor EO settings compared to baselines

## Executive Summary
This paper addresses the critical challenge of missing sensor data in multi-sensor Earth Observation (EO) applications, where sensor failures or acquisitions issues can degrade model performance. The authors introduce two novel methods: Input Sensor Dropout (ISensD) and Ensemble Sensor Invariant (ESensI). ISensD applies random masking of entire sensor feature sets during training to simulate missing data, while ESensI uses shared prediction heads across sensor-dedicated models to create sensor-invariant predictions. Evaluated on three multi-sensor EO datasets (CropHarvest, LFMC, PM25), both methods significantly increase robustness to missing sensors compared to baselines, with ensemble-based approaches showing the highest robustness. However, these methods show a trade-off between robustness and full-sensor predictive performance.

## Method Summary
The paper introduces two novel approaches for improving model robustness to missing sensors in multi-sensor EO settings. Input Sensor Dropout (ISensD) randomly masks entire sensor feature sets during training using Bernoulli masking, teaching the model to predict without relying on any single sensor. Ensemble Sensor Invariant (ESensI) employs a shared prediction head over sensor-specific encoders, forcing the network to learn sensor-invariant intermediate representations. Both methods are evaluated against four baselines using 10-fold cross-validation on three multi-sensor EO datasets, with performance measured using Performance Robustness Score (PRS) and predictive performance metrics (F1 for classification, R2 for regression).

## Key Results
- ISensD and ESensI significantly increase robustness to missing sensors compared to baseline methods across all three datasets
- Ensemble-based methods (Ensemble and ESensI) show the highest robustness when sensor availability decreases
- Both proposed methods demonstrate a trade-off between robustness and full-sensor predictive performance
- The sensor dropout component in ISensD shows promising robustness results, particularly with a 60% dropout ratio

## Why This Works (Mechanism)

### Mechanism 1
Random masking of entire sensor feature sets during training teaches the model to predict without relying on any single sensor. Input Sensor Dropout (ISensD) applies Bernoulli masking to whole sensors at the input level, exposing the model to many combinations of missing sensors during training. This forces the model to distribute prediction responsibility across available sensors rather than memorizing sensor-specific cues. The core assumption is that the model can still produce reasonable predictions when any subset of sensors is missing, as long as at least one sensor remains.

### Mechanism 2
Sharing a prediction head across sensor-dedicated models forces the network to learn sensor-invariant intermediate representations. Ensemble Sensor Invariant (ESensI) uses a shared prediction head over sensor-specific encoders, with sensor encoding vectors added to maintain awareness of which sensor is being processed. This forces each encoder to produce features compatible with the shared head, encouraging sensor-invariant learning. The core assumption is that different sensor encoders can produce meaningfully comparable intermediate representations within a shared head.

### Mechanism 3
Ensembling sensor-dedicated models and ignoring missing predictions reduces performance drop when sensors are absent. The baseline ensemble method trains separate models per sensor and averages their outputs, omitting any missing sensor's prediction during inference. This naturally handles missing sensors without requiring imputation or reconstruction. The core assumption is that each sensor model is competent on its own and the ensemble average remains stable even with some models missing.

## Foundational Learning

- **Concept: Dropout as data augmentation**
  - Why needed here: Simulates missing sensor conditions during training so the model generalizes to them at inference
  - Quick check question: If you apply dropout to 50% of the input features at training, what proportion of sensors will be missing on average for a dataset with 4 sensors of equal feature size?

- **Concept: Sensor-invariant representation learning**
  - Why needed here: Forces the model to learn features that are useful regardless of which sensor provided them, improving robustness to sensor absence
  - Quick check question: In ESensI, what is the role of the sensor encoding vector added to the shared prediction head?

- **Concept: Ensemble methods and missing data handling**
  - Why needed here: Provides a baseline robustness strategy that naturally accommodates missing predictions by omitting them from the aggregation
  - Quick check question: If two of three sensor models are missing during inference, how does the ensemble method combine the remaining prediction?

## Architecture Onboarding

- **Component map:**
  - Input layer: Concatenated multi-sensor features (temporal + static)
  - ISensD path: Random sensor masking → 1D CNN encoder → prediction head
  - ESensI path: Sensor-specific 1D CNN encoders → shared prediction head with sensor encodings
  - Baseline ensemble: Separate 1D CNN encoders per sensor → per-sensor prediction heads → average (ignoring missing)

- **Critical path:** ISensD/Feature encoders → prediction head(s) → loss computation → backprop with masked inputs

- **Design tradeoffs:**
  - ISensD vs ESensI: ISensD simpler but may sacrifice full-sensor accuracy; ESensI more complex but better balances robustness and full-sensor performance
  - Imputation vs masking vs ignoring: Masking (ISensD) simulates missingness; ignoring (ensemble) naturally handles it; imputation adds bias

- **Failure signatures:**
  - PRS drops sharply with few missing sensors → model overfits to full sensor set
  - Full-sensor performance worse than Input baseline → masking or shared head hurts complete data utilization
  - No robustness gain over Input → masking ratio too low or shared head not properly integrated

- **First 3 experiments:**
  1. Train Input model, evaluate PRS with incremental missing sensor percentages
  2. Add ISensD with 60% dropout ratio, compare PRS to Input across all missing levels
  3. Implement ESensI with sensor encodings, evaluate both PRS and full-sensor F1/R2 against Input and Ensemble

## Open Questions the Paper Calls Out

### Open Question 1
What are the specific sources of robustness in the proposed methods (ISensD and ESensI) - is it due to model adaptability, disregard for sensor information, or the irrelevance of sensor data for prediction? The paper explicitly states that future work will focus on detecting these sources of robustness, as it is unclear whether the robustness comes from the model's adaptability, disregard for sensor information, or the irrelevance of sensor data for prediction.

### Open Question 2
How does the Input Sensor Dropout (ISensD) method perform with different SensD ratio values, and what is the optimal ratio for maximizing robustness without significantly compromising full-sensor predictive performance? While the paper shows results for different SensD ratios, it doesn't provide a comprehensive analysis of the optimal ratio or its impact on the trade-off between robustness and full-sensor performance.

### Open Question 3
How do the proposed methods (ISensD and ESensI) compare to other state-of-the-art approaches for handling missing sensor data in multi-sensor Earth Observation models? The paper introduces two novel methods and compares them to four baselines, but doesn't extensively explore other recent approaches in the field that could outperform or complement the proposed methods.

## Limitations

- The robustness-performance trade-off observed may be dataset-dependent, with relative performance of ISensD vs ESensI varying across different multi-sensor EO tasks and sensor configurations
- The effectiveness of sensor encodings in ESensI relies on the assumption that sensor modalities can share meaningful intermediate representations, which may not hold for highly heterogeneous sensor types
- The optimal masking ratio for ISensD (60% used in experiments) may need tuning for different datasets or sensor combinations

## Confidence

- **High confidence**: The core mechanism of ISensD (training-time sensor masking improves robustness to missing sensors) is well-supported by the PRS improvements shown across all three datasets
- **Medium confidence**: The superiority of ensemble-based methods (Ensemble and ESensI) for robustness is demonstrated, but the magnitude of improvement may depend on specific dataset characteristics
- **Medium confidence**: The trade-off between robustness and full-sensor performance is observed, though the optimal balance point may vary by application requirements

## Next Checks

1. Test ISensD with varying dropout ratios (40%, 60%, 80%) on each dataset to verify the 60% ratio is near-optimal and understand sensitivity to this hyperparameter
2. Evaluate ESensI on a fourth multi-sensor EO dataset with different sensor types (e.g., combining optical, SAR, and hyperspectral data) to test generalizability across heterogeneous modalities
3. Compare PRS and full-sensor performance when using the same model architecture (1D CNN) but different training strategies (ISensD, ESensI, input-level feature concatenation) to isolate the effect of the proposed methods from architectural differences