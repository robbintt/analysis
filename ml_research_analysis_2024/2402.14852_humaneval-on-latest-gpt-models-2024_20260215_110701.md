---
ver: rpa2
title: HumanEval on Latest GPT Models -- 2024
arxiv_id: '2402.14852'
source_url: https://arxiv.org/abs/2402.14852
tags: []
core_contribution: This paper revisits GPT models on HumanEval in 2024, focusing on
  advancements in program synthesis using large language models (LLMs) like GPT-4.
  The authors introduce a repository that connects GPT models to HumanEval, a dataset
  originally developed for CODEGEN on natural and programming language data.
---

# HumanEval on Latest GPT Models -- 2024

## Quick Facts
- arXiv ID: 2402.14852
- Source URL: https://arxiv.org/abs/2402.14852
- Authors: Daniel Li; Lincoln Murr
- Reference count: 15
- Key outcome: GPT-4 achieves pass@1 score of 85.73 and pass@10 score of 98.17 on HumanEval, significantly outperforming previous models

## Executive Summary
This paper revisits GPT models on HumanEval in 2024, focusing on advancements in program synthesis using large language models like GPT-4. The authors introduce a repository connecting GPT models to HumanEval, demonstrating competitive performance in zero-shot Python code generation. The study shows GPT-4 achieves impressive pass@1 and pass@10 scores without extensive prompt engineering, and explores multi-step paradigm synthesis that significantly improves program synthesis over single-turn inputs.

## Method Summary
The authors created a repository to connect GPT models to HumanEval, a dataset of 164 Python coding problems. Using the OpenAI API, they generated code solutions for each problem and evaluated performance using pass@1 and pass@10 metrics through the OpenAI grading framework. The evaluation employed multiple threads for iterations and compared GPT-4 against previous models. The methodology focused on zero-shot generation without extensive prompt engineering, though some basic prompt templates were used.

## Key Results
- GPT-4 achieves pass@1 score of 85.73 and pass@10 score of 98.17 on HumanEval
- Multi-step paradigm synthesis significantly improves program synthesis over single-turn inputs
- GPT-4 outperforms previous GPT models in zero-shot Python code generation on HumanEval tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 achieves high pass@1 and pass@10 scores on HumanEval without extensive prompt engineering
- Mechanism: The model leverages large-scale pretraining on diverse code and natural language data to directly map problem descriptions to functional code
- Core assumption: Pretraining data and model architecture capture sufficient programming knowledge for zero-shot problem solving
- Evidence anchors: [abstract] "GPT-4 achieves an impressive pass@1 score of 85.73 and a pass@10 score of 98.17"
- Break condition: Performance may degrade when encountering problems outside pretraining distribution or requiring complex reasoning

### Mechanism 2
- Claim: Multi-step paradigm synthesis significantly improves program synthesis
- Mechanism: Breaking complex problems into smaller, factorized prompts reduces cognitive load and enables more accurate code generation
- Core assumption: Decomposing problems aligns with model's processing capabilities
- Evidence anchors: [abstract] "This benchmark features 160 diverse problem sets factorized into multistep prompts"
- Break condition: Benefits diminish if problem decomposition introduces additional complexity

### Mechanism 3
- Claim: Future LLMs may exhibit increasingly human-like capabilities and surpass need for prompt engineering
- Mechanism: Scaling LLMs and exposing them to diverse data will lead to emergent abilities mimicking human reasoning
- Core assumption: Scaling and diverse data exposure will enable human-like problem-solving
- Evidence anchors: [abstract] "Future LLMs have the potential to exhibit increasingly human-like capabilities"
- Break condition: If LLMs fail to generalize beyond training data or require human creativity

## Foundational Learning

- Concept: Large Language Models (LLMs)
  - Why needed here: Understanding LLM capabilities and limitations is crucial for evaluating code generation performance
  - Quick check question: What are the key differences between GPT-3.5 and GPT-4 in terms of architecture, training data, and performance?

- Concept: Prompt Engineering
  - Why needed here: Prompt engineering techniques play a significant role in guiding LLMs to generate accurate code
  - Quick check question: How do different prompt engineering strategies impact GPT-4's performance in code generation tasks?

- Concept: Evaluation Metrics
  - Why needed here: Pass@1 and pass@10 metrics assess code generation quality and problem-solving ability
  - Quick check question: What are the advantages and limitations of using pass@1 and pass@10 evaluation methods?

## Architecture Onboarding

- Component map: GPT-4 model -> HumanEval dataset -> Evaluation framework -> Multi-step synthesis
- Critical path: 1) Load GPT-4 and HumanEval dataset, 2) Generate code using predefined prompts, 3) Evaluate using pass@1 and pass@10 metrics, 4) Analyze results
- Design tradeoffs: Model size vs. inference speed, prompt complexity vs. generalization, evaluation granularity vs. comprehensiveness
- Failure signatures: Low pass@1 scores indicate understanding issues, low pass@10 scores suggest lack of self-correction, inconsistent performance across problem types
- First 3 experiments: 1) Compare GPT-4 with previous versions on HumanEval, 2) Evaluate impact of different prompt engineering techniques, 3) Investigate effectiveness of multi-step synthesis vs. single-turn inputs

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does GPT-4's performance compare to previous versions of large language models in code generation tasks?
- Basis in paper: [explicit] GPT-4 showed significant improvements compared to predecessors with pass@1 score of 85.73
- Why unresolved: Paper provides comparison but doesn't explore specific reasons behind improvements or potential for future advancements
- What evidence would resolve it: Further research on underlying mechanisms and comparative studies with emerging language models

### Open Question 2
- Question: What impact does prompt engineering have on GPT-4's code generation abilities?
- Basis in paper: [explicit] Paper discusses benefits of prompt engineering techniques like LA TS and Reexion
- Why unresolved: Paper highlights benefits but doesn't explore limitations or drawbacks of relying on these techniques
- What evidence would resolve it: Experiments comparing performance with and without prompt engineering, investigating potential limitations

### Open Question 3
- Question: Will future LLMs demonstrate increasingly human-like capabilities and surpass the necessity for prompt engineering?
- Basis in paper: [explicit] Paper raises question about future LLMs exhibiting human-like capabilities
- Why unresolved: Paper acknowledges potential but doesn't provide definitive answer or explore contributing factors
- What evidence would resolve it: Longitudinal studies comparing future LLMs with current models, investigations into underlying mechanisms

## Limitations

- Lack of transparency in methodology with insufficient detail about specific prompts and parameter settings
- No comparison with recent specialized code generation models like AlphaCode or recent Codex iterations
- Limited analysis of failure modes and the types of problems where GPT-4 struggles

## Confidence

- High Confidence (8/10): GPT-4 achieves high pass@1 and pass@10 scores on HumanEval; outperforms previous GPT models; multi-step synthesis improves program synthesis
- Medium Confidence (5/10): GPT-4 exhibits foundational AGI behaviors; future LLMs may surpass need for prompt engineering; performance represents significant advance
- Low Confidence (3/10): Specific mechanisms behind GPT-4's performance; generalizability to other benchmarks; impact of specific prompt engineering techniques

## Next Checks

1. **Reproducibility audit**: Independently reproduce exact results using open-source code and HumanEval dataset, documenting all parameters, prompts, and API configurations across multiple runs

2. **Benchmark expansion**: Test GPT-4 on additional code generation benchmarks (MBPP, APPS) and real-world programming tasks to validate HumanEval performance generalization

3. **Error analysis**: Conduct systematic analysis of GPT-4's failures on HumanEval, categorizing error types and identifying patterns in problems where the model struggles