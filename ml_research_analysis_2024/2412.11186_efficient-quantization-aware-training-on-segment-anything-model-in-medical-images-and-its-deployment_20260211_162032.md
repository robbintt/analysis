---
ver: rpa2
title: Efficient Quantization-Aware Training on Segment Anything Model in Medical
  Images and Its Deployment
arxiv_id: '2412.11186'
source_url: https://arxiv.org/abs/2412.11186
tags:
- quantized
- training
- inference
- image
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a quantization-aware training pipeline for
  the Segment Anything Model (SAM) tailored to medical images. The authors address
  the challenge of high computational costs during inference by quantizing LiteMedSAM
  using quantization-aware training (QAT) with the Brevitas framework.
---

# Efficient Quantization-Aware Training on Segment Anything Model in Medical Images and Its Deployment

## Quick Facts
- arXiv ID: 2412.11186
- Source URL: https://arxiv.org/abs/2412.11186
- Reference count: 17
- Quantizes LiteMedSAM to reduce inference time from 38.78s to 11.78s while maintaining DSC scores

## Executive Summary
This paper presents a quantization-aware training (QAT) pipeline for the Segment Anything Model (SAM) tailored to medical images, specifically addressing the challenge of high computational costs during inference. The authors propose quantizing LiteMedSAM using the Brevitas framework to enable efficient deployment on resource-constrained platforms. The approach involves optimizing data preprocessing, training with a small subset of data to balance different imaging modalities, and deploying the quantized model using OpenVINO. Experimental results demonstrate that the quantized model achieves comparable accuracy to the baseline while significantly improving inference speed, making it suitable for clinical applications requiring real-time segmentation.

## Method Summary
The proposed method employs quantization-aware training on LiteMedSAM for medical image segmentation, using the Brevitas framework to quantize weights and activations. The pipeline begins with data preprocessing to optimize input for medical imaging contexts, followed by training on a carefully selected subset to balance different modalities and reduce computational load. The quantized model is then deployed using OpenVINO for efficient inference. The approach specifically addresses the challenge of modality imbalance by using a representative subset of data during training, ensuring that the quantized model maintains performance across different imaging types while achieving substantial speed improvements.

## Key Results
- Inference time reduced from 38.78s to 11.78s for CT images while maintaining competitive DSC scores
- Quantized model achieves comparable accuracy to baseline across tested modalities
- Successfully addresses modality imbalance through careful subset selection during training

## Why This Works (Mechanism)
The quantization-aware training approach works by simulating quantization effects during the training process, allowing the model to adapt to lower precision representations while maintaining accuracy. By using Brevitas for quantization, the model learns to operate within the constraints of reduced bit-width weights and activations, optimizing the trade-off between computational efficiency and performance. The careful selection of a balanced training subset ensures that the quantized model generalizes well across different medical imaging modalities, preventing bias toward any particular imaging type while maintaining segmentation accuracy.

## Foundational Learning
- Quantization-aware training (QAT): Simulates quantization during training to adapt model parameters - why needed: enables deployment on resource-constrained devices - quick check: compare quantized vs full-precision model performance
- Brevitas framework: Provides tools for training quantized neural networks - why needed: supports low-bit precision training for efficient inference - quick check: verify bit-width settings and calibration
- OpenVINO deployment: Intel's toolkit for optimizing and deploying AI models - why needed: enables efficient inference on various hardware platforms - quick check: measure actual inference time on target device
- Modality balancing: Ensuring representation across different imaging types during training - why needed: prevents bias toward specific imaging modalities - quick check: analyze distribution of training samples across modalities
- Dice Similarity Coefficient (DSC): Metric for evaluating segmentation accuracy - why needed: standard metric for medical image segmentation quality - quick check: compare DSC scores across different model versions
- LiteMedSAM: Lightweight version of SAM optimized for medical images - why needed: provides efficient base model for quantization - quick check: verify model architecture and parameter count

## Architecture Onboarding

Component Map: Data Preprocessing -> QAT with Brevitas -> OpenVINO Deployment

Critical Path: Input Image → Preprocessing → Quantized LiteMedSAM → Output Segmentation → OpenVINO Inference

Design Tradeoffs:
1. Precision vs Speed: Lower bit-width improves speed but may reduce accuracy
2. Subset Size vs Modality Coverage: Smaller subsets improve training efficiency but may miss rare cases
3. Hardware Optimization: OpenVINO optimization for specific hardware may limit portability

Failure Signatures:
1. Significant accuracy drop when quantizing to very low bit-widths
2. Poor performance on underrepresented modalities in training subset
3. Runtime errors during OpenVINO deployment due to unsupported operations

Three First Experiments:
1. Measure inference time and accuracy across different quantization bit-widths (8-bit, 6-bit, 4-bit)
2. Evaluate model performance on held-out test set from each modality (CT, US, etc.)
3. Compare OpenVINO deployment performance on different hardware targets (CPU vs GPU vs VPU)

## Open Questions the Paper Calls Out
None

## Limitations
- Limited evaluation across diverse medical imaging modalities beyond CT and US images
- Performance metrics primarily focused on DSC scores without additional clinical validation
- Unclear impact of quantization on model robustness to imaging artifacts and acquisition variations

## Confidence

High confidence in:
- Reported inference speed improvements from 38.78s to 11.78s
- Basic accuracy maintenance across tested modalities

Medium confidence in:
- Generalization of results to broader medical imaging applications due to limited modality coverage

Low confidence in:
- Model performance on rare or complex medical cases not represented in the training subset

## Next Checks

1. Evaluate the quantized model's performance across a broader range of medical imaging modalities, including MRI and X-ray, with varying resolutions and clinical conditions.

2. Conduct extensive testing on model robustness to common imaging artifacts, noise levels, and different acquisition protocols to assess real-world applicability.

3. Perform clinical validation studies comparing the quantized model's segmentation results against expert radiologist annotations across multiple anatomical structures and disease states.