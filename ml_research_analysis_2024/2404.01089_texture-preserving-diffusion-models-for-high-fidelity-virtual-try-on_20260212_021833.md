---
ver: rpa2
title: Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On
arxiv_id: '2404.01089'
source_url: https://arxiv.org/abs/2404.01089
tags:
- image
- garment
- try-on
- person
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a novel diffusion-based method for high-fidelity
  virtual try-on without relying on additional garment image encoders. The core idea
  is to concatenate the masked person and reference garment images along the spatial
  dimension and use the resulting image as input for the diffusion model's denoising
  UNet.
---

# Texture-Preserving Diffusion Models for High-Fidelity Virtual Try-On
## Quick Facts
- arXiv ID: 2404.01089
- Source URL: https://arxiv.org/abs/2404.01089
- Reference count: 40
- Proposed a diffusion-based method for virtual try-on without additional garment image encoders

## Executive Summary
This paper introduces a novel diffusion-based approach for high-fidelity virtual try-on that eliminates the need for additional garment image encoders. The method uses spatial concatenation of masked person and reference garment images as input to the diffusion model's denoising UNet, enabling efficient texture transfer through self-attention layers. A decoupled mask prediction method accurately determines the inpainting area based on specific person-garment pairs, improving synthesis quality.

## Method Summary
The proposed method leverages diffusion models for virtual try-on by concatenating the masked person image and reference garment image along the spatial dimension. This concatenated image serves as input to the diffusion model's UNet, where self-attention layers efficiently transfer garment textures to the person image. The approach introduces a decoupled mask prediction technique that determines the precise inpainting region based on the specific person-garment combination. This spatial concatenation strategy allows the model to capture garment-person interactions directly within the diffusion framework without requiring separate encoding mechanisms.

## Key Results
- Achieves SSIM of 0.90, FID of 8.54, and LPIPS of 0.07 on VITON-HD database
- Significantly outperforms state-of-the-art approaches in realism and coherence
- Eliminates need for additional garment image encoders while maintaining high fidelity

## Why This Works (Mechanism)
The method works by leveraging spatial attention mechanisms within the diffusion UNet to transfer textures from the garment to the person image. By concatenating the masked person and garment images along the spatial dimension, the self-attention layers can directly observe and process both inputs simultaneously. This allows the model to learn the mapping between garment features and the corresponding body regions that need to be inpainted. The decoupled mask prediction further enhances accuracy by determining the precise inpainting area based on the specific person-garment pair characteristics.

## Foundational Learning
- **Diffusion models**: Why needed - Generate high-quality images through iterative denoising process; Quick check - Can the model progressively denoise from random noise to realistic output
- **Self-attention mechanisms**: Why needed - Enable long-range dependency modeling between garment and person regions; Quick check - Does attention effectively capture garment-person spatial relationships
- **Spatial concatenation**: Why needed - Combine person and garment information in a single input tensor; Quick check - Does concatenation preserve spatial relationships between features
- **Mask prediction**: Why needed - Accurately identify inpainting regions for realistic garment synthesis; Quick check - Can the mask adapt to different body shapes and garment types
- **UNet architecture**: Why needed - Provides hierarchical feature extraction for image generation tasks; Quick check - Does the network maintain spatial resolution throughout the process
- **Texture transfer**: Why needed - Preserve garment details when mapping to person image; Quick check - Are garment patterns and textures accurately reproduced

## Architecture Onboarding
- **Component map**: Masked person image + Reference garment image (spatially concatenated) -> Diffusion UNet with self-attention -> Synthesized try-on image with decoupled mask prediction
- **Critical path**: Input concatenation → Self-attention layers → Texture transfer → Mask-guided inpainting → Final output
- **Design tradeoffs**: The spatial concatenation approach simplifies the pipeline but may limit scalability to multi-garment scenarios; the decoupled mask prediction improves accuracy but adds computational overhead
- **Failure signatures**: Poor texture transfer when garment patterns are complex; inaccurate mask prediction for unusual body-garment combinations; artifacts at garment boundaries
- **3 first experiments**: 1) Test spatial concatenation with simple geometric patterns; 2) Evaluate mask prediction accuracy on synthetic person-garment pairs; 3) Measure texture preservation on garments with varying complexity

## Open Questions the Paper Calls Out
None

## Limitations
- Method focuses primarily on front-view person images, potentially limiting generalization to profile or back views
- Spatial concatenation approach may not scale effectively to multi-garment scenarios or complex outfit combinations
- Decoupled mask prediction may not generalize well to diverse body shapes and garment types beyond tested datasets

## Confidence
- **High confidence**: The core technical contribution of using spatial concatenation for diffusion input and the self-attention mechanism for texture transfer
- **Medium confidence**: The effectiveness of the decoupled mask prediction method for inpainting area determination
- **Medium confidence**: The quantitative performance improvements over existing methods on the tested datasets

## Next Checks
1. Test the method on profile and back-view person images to evaluate generalization beyond front-view scenarios
2. Evaluate performance on multi-garment try-on scenarios to assess scalability beyond single-garment inputs
3. Conduct user studies comparing perceived realism and texture fidelity against state-of-the-art methods to complement quantitative metrics