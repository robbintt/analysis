---
ver: rpa2
title: 'LegalLens: Leveraging LLMs for Legal Violation Identification in Unstructured
  Text'
arxiv_id: '2402.04335'
source_url: https://arxiv.org/abs/2402.04335
tags:
- legal
- data
- text
- violation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces LegalLens, a novel approach for identifying
  legal violations and associating victims in unstructured text. The authors develop
  two specialized datasets using Large Language Models (LLMs) and domain expert validation,
  targeting Named Entity Recognition (NER) for violation detection and Natural Language
  Inference (NLI) for victim association.
---

# LegalLens: Leveraging LLMs for Legal Violation Identification in Unstructured Text

## Quick Facts
- arXiv ID: 2402.04335
- Source URL: https://arxiv.org/abs/2402.04335
- Authors: Dor Bernsohn; Gil Semo; Yaron Vazana; Gila Hayat; Ben Hagag; Joel Niklaus; Rohit Saha; Kyryl Truskovskyi
- Reference count: 30
- Primary result: F1-score of 62.69% for violation identification and 81.02% for victim association

## Executive Summary
LegalLens introduces a novel dual-task approach for identifying legal violations and associating victims in unstructured text. The system employs Named Entity Recognition (NER) to detect violations and Natural Language Inference (NLI) to associate these violations with potentially affected individuals. By leveraging Large Language Models (LLMs) for synthetic data generation and expert validation, LegalLens achieves promising results in legal NLP tasks, demonstrating the effectiveness of combining NER and NLI methodologies.

## Method Summary
LegalLens employs a dual-task methodology to identify legal violations and associate victims within unstructured text. The approach involves generating synthetic datasets using GPT-4, which are then validated by domain expert annotators to ensure realism and complexity. The system utilizes BERT-based models for Named Entity Recognition (NER) to detect legal violations and various models, including open-source and closed-source LLMs, for Natural Language Inference (NLI) to match violations with known resolved cases. The performance is evaluated using F1-scores, with experiments showing that BERT-based models outperform LLMs in the NER task due to their token-level optimization capabilities.

## Key Results
- F1-score of 62.69% for violation identification using NER
- F1-score of 81.02% for associating victims using NLI
- BERT-based models outperform LLMs in NER tasks due to token-level optimization
- Expert annotators achieved an average F1-score of 44.86% in distinguishing synthetic from real legal text

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** The dual-task setup (NER for violation detection, NLI for victim association) enables both identification and resolution of legal violations.
- **Mechanism:** NER extracts structured legal entities from unstructured text, while NLI matches these violations to known resolved cases through logical inference.
- **Core assumption:** Legal violations in text can be decomposed into identifiable entities (Law, Violation, Violated By, Violated On) that follow consistent patterns.
- **Evidence anchors:**
  - [abstract] "We focus on two main tasks, the first for detecting legal violations within unstructured textual data, and the second for associating these violations with potentially affected individuals."
  - [section] "We designed two setups, one for each task, the first for solving the legal violation identification task (a.k.a Identification Setup) using named entity recognition (NER), and the other for associating these violations with potentially affected individuals (a.k.a Resolution Setup) using natural language inference (NLI)."
- **Break condition:** If legal text doesn't follow entity patterns or if violations are too context-dependent for pattern recognition.

### Mechanism 2
- **Claim:** LLM-generated synthetic data, when validated by domain experts, creates high-quality training data that closely mimics real-world legal text.
- **Mechanism:** GPT-4 generates diverse legal scenarios based on real class action cases, then expert annotators validate and refine the data to ensure realism and complexity.
- **Core assumption:** LLMs can generate text that is indistinguishable from human-written legal content when properly prompted and validated.
- **Evidence anchors:**
  - [abstract] "We constructed two datasets using Large Language Models (LLMs) which were subsequently validated by domain expert annotators."
  - [section] "Our annotators' goal was to label each record based on its origin: machine-generated or human-written. The annotators achieved an average F1-score of 44.86%."
- **Break condition:** If expert annotators can consistently distinguish synthetic from real text, indicating the LLM fails to capture legal language nuances.

### Mechanism 3
- **Claim:** Fine-tuning BERT-based models on entity-level tasks (NER) provides better performance than fine-tuning LLMs due to the constrained label space and token-level optimization.
- **Mechanism:** BERT models use cross-entropy loss per token, providing stronger gradient signals for entity classification, while LLMs predict next tokens in a much larger label space.
- **Core assumption:** The legal entity classification task benefits more from token-level supervision than sequence-level generation.
- **Evidence anchors:**
  - [abstract] "Experiments with BERT-based models, open-source LLMs, and closed-source LLMs show promising results: an F1-score of 62.69% for violation identification."
  - [section] "Interestingly, BERT-based models with fewer parameters outperform LLMs by a significant margin. This disparity in performance is due to the difference in objective functions that the different model classes use."
- **Break condition:** If the task requires more contextual understanding beyond local token patterns, where LLMs' generation capabilities would be advantageous.

## Foundational Learning

- **Concept:** Named Entity Recognition (NER)
  - Why needed here: NER is essential for extracting structured legal entities (Law, Violation, Violated By, Violated On) from unstructured legal text.
  - Quick check question: What are the four entity types defined in the LegalLens NER dataset?
- **Concept:** Natural Language Inference (NLI)
  - Why needed here: NLI enables matching detected violations to known resolved cases by determining logical entailment between premise and hypothesis.
  - Quick check question: What are the three possible NLI classifications used in this system?
- **Concept:** Prompt Engineering for LLMs
  - Why needed here: Effective prompts are crucial for generating high-quality synthetic legal data that mimics real-world scenarios.
  - Quick check question: What are the two prompting strategies used for NER data generation (explicit vs implicit)?

## Architecture Onboarding

- **Component map:** Text → NER extraction → NLI matching → Victim association
- **Critical path:** Text → NER extraction → NLI matching → Victim association
- **Design tradeoffs:**
  - BERT vs LLM for NER: BERT provides better token-level accuracy but LLMs offer better contextual understanding
  - Synthetic vs real data: Synthetic data enables scalability but requires rigorous validation
  - Fine-tuning vs few-shot: Fine-tuning provides better performance but requires more data and compute resources
- **Failure signatures:**
  - Low NER F1 scores indicate entity extraction problems
  - High neutral classifications in NLI suggest difficulty in establishing logical connections
  - Expert annotators easily distinguishing synthetic from real text indicates data quality issues
- **First 3 experiments:**
  1. Compare BERT-based models vs LLMs on the NER task using the same training data
  2. Test NLI performance across different legal domains using leave-one-out validation
  3. Evaluate expert annotator agreement on distinguishing synthetic vs real legal text

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LegalLens compare when applied to legal texts from different jurisdictions, such as civil law systems?
- Basis in paper: [inferred] The paper acknowledges that the dataset is heavily focused on common law in US courts and mentions future work aims to integrate legal texts from various global jurisdictions.
- Why unresolved: The current dataset and experiments are limited to US common law, and the paper does not provide comparative results from other legal systems.
- What evidence would resolve it: Conducting experiments using LegalLens on legal texts from different jurisdictions, such as civil law systems, and comparing the performance metrics with those obtained from US common law texts.

### Open Question 2
- Question: Can the integration of fact matching algorithms improve the accuracy of legal violation identification in LegalLens, and if so, to what extent?
- Basis in paper: [inferred] The paper mentions future work on developing algorithms for cross-referencing facts across sources to enhance the accuracy of legal violation identification.
- Why unresolved: The paper does not provide any results or analysis on the impact of fact matching algorithms on the performance of LegalLens.
- What evidence would resolve it: Implementing fact matching algorithms in LegalLens and evaluating the changes in performance metrics, such as F1-score, precision, and recall, for both NER and NLI tasks.

### Open Question 3
- Question: How does the model's performance vary with different token lengths for providing context in complex legal scenarios, such as wage violations?
- Basis in paper: [inferred] The error analysis section mentions that the model struggles with accurately classifying statements related to wage areas and suggests investigating different token lengths to provide more context.
- Why unresolved: The paper does not explore the effect of varying token lengths on the model's performance in complex legal scenarios.
- What evidence would resolve it: Conducting experiments with different token lengths for providing context in complex legal scenarios and analyzing the changes in performance metrics for the NLI task, particularly in the wage domain.

## Limitations

- Data Quality Uncertainty: The modest F1-score (44.86%) for expert annotators distinguishing synthetic from real legal text suggests potential quality issues in synthetic data generation.
- Performance Gaps: While promising, the F1-score of 62.69% for violation identification indicates substantial room for improvement.
- Generalization Concerns: The approach hasn't been extensively tested across different legal domains or time periods, raising questions about its robustness in varied contexts.

## Confidence

**High Confidence:**
- The dual-task architecture (NER + NLI) is well-justified and follows established NLP patterns
- The performance comparison between BERT and LLM approaches for NER is empirically supported
- The use of expert validation for synthetic data is a sound methodological choice

**Medium Confidence:**
- The synthetic data generation process effectiveness, given the modest expert annotator distinction performance
- The scalability of the approach to real-world legal document volumes
- The robustness of results across different legal domains

**Low Confidence:**
- The long-term stability of model performance as legal language evolves
- The cost-effectiveness of the approach for practical deployment
- The handling of ambiguous or borderline legal violations

## Next Checks

1. **Cross-Domain Robustness Test:** Evaluate model performance on legal documents from different jurisdictions and time periods to assess generalization capabilities beyond the training domain.

2. **Cost-Benefit Analysis:** Compare the computational and annotation costs of the LegalLens approach against simpler baseline methods to determine practical viability for real-world deployment.

3. **Error Analysis Protocol:** Conduct a detailed error analysis focusing on false positives/negatives in both NER and NLI tasks, particularly examining cases where legal violations require extensive contextual understanding beyond entity recognition patterns.