---
ver: rpa2
title: Zero-shot Classification using Hyperdimensional Computing
arxiv_id: '2401.16876'
source_url: https://arxiv.org/abs/2401.16876
tags:
- attribute
- learning
- image
- zero-shot
- encoder
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a novel hybrid architecture for zero-shot
  classification (ZSC) that leverages hyperdimensional computing (HDC) to achieve
  Pareto-optimal results in terms of accuracy and model parameter count. The proposed
  HDC-ZSC model consists of a pre-trained image encoder, an HDC-based attribute encoder,
  and a similarity kernel.
---

# Zero-shot Classification using Hyperdimensional Computing

## Quick Facts
- arXiv ID: 2401.16876
- Source URL: https://arxiv.org/abs/2401.16876
- Reference count: 38
- Key outcome: Achieves 63.8% top-1 accuracy on CUB-200 with 26.6M parameters, outperforming non-generative SOTA by 4.3-9.9%

## Executive Summary
This work introduces a novel hybrid architecture for zero-shot classification (ZSC) that leverages hyperdimensional computing (HDC) to achieve Pareto-optimal results in terms of accuracy and model parameter count. The proposed HDC-ZSC model consists of a pre-trained image encoder, an HDC-based attribute encoder, and a similarity kernel. The attribute encoder uses fixed binary codebooks to compactly represent attributes, reducing memory requirements by 71%. HDC-ZSC achieves a top-1 classification accuracy of 63.8% on the CUB-200 dataset with only 26.6 million trainable parameters, outperforming state-of-the-art non-generative approaches by 4.3-9.9% in accuracy while requiring 1.72-1.85x fewer parameters.

## Method Summary
The HDC-ZSC model employs a three-phase training approach: pre-training a ResNet50 backbone on ImageNet, fine-tuning for attribute extraction using weighted BCEL loss, and finally ZSC fine-tuning with frozen backbone. The attribute encoder uses fixed binary codebooks to compactly represent attributes, reducing memory requirements by 71%. HDC-ZSC achieves high accuracy by exploiting the quasi-orthogonality of high-dimensional random vectors and the correlation between attributes and class labels. The model is trained in three phases: pre-training on ImageNet, attribute extraction, and ZSC fine-tuning, enabling efficient implementation in low-power embedded platforms.

## Key Results
- Achieves 63.8% top-1 classification accuracy on CUB-200 dataset
- Requires only 26.6 million trainable parameters
- Outperforms state-of-the-art non-generative approaches by 4.3-9.9% in accuracy
- Reduces memory requirements by 71% using fixed binary codebooks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Hyperdimensional Computing (HDC) with binary codebooks achieves high accuracy with low parameter count by exploiting quasi-orthogonality of high-dimensional random vectors.
- Mechanism: Randomly initialized high-dimensional (d-dimensional) binary hypervectors representing atomic symbols (attribute groups and values) are combined via elementwise XOR (binding) to form attribute-level codevectors. Because high-dimensional random binary vectors are quasi-orthogonal, these bound vectors preserve discriminative information while requiring only ~17 KB of memory for the codebook.
- Core assumption: Sufficient dimensionality (e.g., d=1536) ensures quasi-orthogonality; binary vectors suffice for the task.
- Evidence anchors:
  - [abstract] "The attribute encoder uses fixed binary codebooks to compactly represent attributes, reducing memory requirements by 71%."
  - [section III-A] "With a sufficiently high dimension, the atomic vectors tend to be quasi-orthogonal to each other."
  - [corpus] Weak/no direct experimental data comparing binary vs. bipolar hypervectors in HDC-ZSC context.
- Break condition: If dimensionality is too low, vectors lose quasi-orthogonality, leading to poor attribute discrimination and reduced accuracy.

### Mechanism 2
- Claim: Pre-training on ImageNet + attribute extraction + fine-tuning on ZSC yields higher accuracy than direct ZSC training by leveraging mature embeddings.
- Mechanism: Phase I pre-trains the image encoder on ImageNet to obtain generic visual features. Phase II fine-tunes this encoder for attribute prediction using a weighted BCEL loss, aligning image embeddings with attribute embeddings. Phase III freezes the backbone and fine-tunes only the FC layer on ZSC with training classes, ensuring the image embeddings are already aligned with attribute space.
- Core assumption: Attributes extracted from images are sufficiently correlated with class labels so that Phase II pre-training benefits Phase III ZSC fine-tuning.
- Evidence anchors:
  - [abstract] "HDC-ZSC achieves a top-1 classification accuracy of 63.8% on the CUB-200 dataset...outperforming state-of-the-art non-generative approaches by 4.3-9.9%."
  - [section III-A] "The backbone network after pre-training for the attribute extraction task has matured weights...so that it can be used as the starting point for ZSC."
  - [corpus] No explicit ablation showing direct vs. staged training accuracy.
- Break condition: If attribute extraction task is poorly aligned with ZSC task, pre-training may not transfer, hurting accuracy.

### Mechanism 3
- Claim: Fixed, stationary attribute encoder weights enable efficient hardware implementation without sacrificing accuracy.
- Mechanism: The attribute encoder ϕ(·) is defined as a static matrix product A × B (class attributes × attribute vectors), where B is generated from fixed binary codebooks. At inference, both image and attribute encoders are stationary (frozen), allowing implementation in non-von Neumann hardware accelerators without weight updates.
- Core assumption: The static attribute encoder is sufficiently expressive for accurate ZSC, so no retraining is needed at inference.
- Evidence anchors:
  - [abstract] "The attribute encoder uses fixed binary codebooks...providing opportunities for implementation in resource-constrained edge devices."
  - [section III-B] "All weights of the model remain stationary at this stage."
  - [corpus] No quantitative hardware performance or power measurements provided.
- Break condition: If task requires dynamic adaptation at inference, fixed weights become a bottleneck and accuracy drops.

## Foundational Learning

- Concept: Quasi-orthogonality of high-dimensional random vectors in HDC
  - Why needed here: Enables compact representation of attributes with minimal memory and maximal discriminative power.
  - Quick check question: What property of high-dimensional random vectors allows HDC to use fixed binary codebooks without losing information?

- Concept: Multi-stage pre-training pipeline (ImageNet → attribute extraction → ZSC)
  - Why needed here: Builds progressively richer and more aligned embeddings, improving ZSC accuracy compared to single-stage training.
  - Quick check question: Why does freezing the backbone after Phase II and only fine-tuning the FC layer in Phase III help maintain attribute-image alignment?

- Concept: Cosine similarity kernel for comparing embeddings
  - Why needed here: Measures alignment between image and attribute embeddings in a normalized way, enabling effective classification without additional learnable parameters.
  - Quick check question: How does cosine similarity differ from dot product in this context, and why is it preferred for ZSC?

## Architecture Onboarding

- Component map:
  - Input: Images (h×w×l) + Class attribute matrix (C×α)
  - Image encoder: ResNet50 backbone + FC projection → d-dimensional embedding
  - Attribute encoder: Fixed binary codebooks (G groups, V values) → B matrix → A×B product → C×d embedding
  - Similarity kernel: Cosine similarity between image and attribute embeddings → B×C logits
  - Output: Predicted class via argmax over similarity scores

- Critical path:
  1. Image → ResNet50 → FC projection → d-dim vector
  2. Attributes → bind group/value hypervectors → B matrix → A×B → d-dim per class
  3. Cosine similarity between image and each class embedding
  4. Argmax to get predicted class

- Design tradeoffs:
  - Fixed vs. trainable attribute encoder: Fixed saves memory and enables hardware acceleration but may limit expressiveness; trainable increases parameters and training cost.
  - Binary vs. bipolar hypervectors: Binary uses XOR for binding (efficient), bipolar uses multiplication; choice affects hardware mapping and orthogonality properties.
  - Pre-training depth: More pre-training stages improve accuracy but increase training time; fewer stages reduce time but may hurt performance.

- Failure signatures:
  - Low accuracy: Check dimensionality d, alignment between attribute extraction and ZSC, and correctness of binding operations.
  - Memory bloat: Verify binary codebook storage; ensure no accidental duplication of hypervectors.
  - Training instability: Monitor BCEL weighting in attribute extraction; ensure balanced attribute classes.

- First 3 experiments:
  1. Verify quasi-orthogonality: Generate random binary vectors of dimension d=1536, compute pairwise cosine similarities; confirm near-zero mean and small variance.
  2. Validate attribute binding: Bind a group and value hypervector; confirm the result is quasi-orthogonal to both operands.
  3. Test pre-training pipeline: Train Phases I and II separately, evaluate attribute extraction accuracy; ensure embeddings align before Phase III.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of HDC-ZSC scale with increasing dimensionality of hypervectors?
- Basis in paper: [inferred] The paper mentions that HDC benefits from high-dimensionality for quasi-orthogonality, but does not explore the impact of dimensionality on classification accuracy.
- Why unresolved: The authors do not provide empirical results showing the trade-off between dimensionality and accuracy or computational efficiency.
- What evidence would resolve it: Experiments varying hypervector dimensionality while measuring accuracy, model size, and inference speed.

### Open Question 2
- Question: Can HDC-ZSC be effectively extended to more complex attribute structures beyond the flat attribute matrices used in CUB-200?
- Basis in paper: [inferred] The paper uses a simple binding of attribute groups and values, but real-world applications may involve hierarchical or relational attributes.
- Why unresolved: The authors do not test HDC-ZSC on datasets with more complex attribute relationships or explore hierarchical HDC representations.
- What evidence would resolve it: Application of HDC-ZSC to datasets with nested or relational attributes and comparison of accuracy gains.

### Open Question 3
- Question: How does HDC-ZSC compare to generative ZSL methods when applied to datasets with fewer attribute annotations?
- Basis in paper: [explicit] The authors note that generative methods require 1.75×–2.58× more parameters but only achieve 3.9% higher accuracy, yet do not test HDC-ZSC on low-attribute datasets.
- Why unresolved: The paper does not evaluate HDC-ZSC on datasets with sparse or noisy attribute annotations where generative methods might excel.
- What evidence would resolve it: Experiments comparing HDC-ZSC and generative ZSL on datasets with varying levels of attribute annotation quality and quantity.

## Limitations

- The model's performance is only demonstrated on the CUB-200 dataset, limiting conclusions about its effectiveness on other ZSC benchmarks.
- The paper lacks quantitative hardware measurements to validate the claimed 71% memory reduction and efficiency gains.
- No ablation studies comparing staged vs. direct training to isolate the contribution of each pre-training phase.

## Confidence

- **High confidence**: The Pareto-optimal accuracy-parameter tradeoff claim (63.8% accuracy with 26.6M parameters outperforming state-of-the-art by 4.3-9.9%) is well-supported by the experimental results presented.
- **Medium confidence**: The effectiveness of the three-phase pre-training pipeline is supported by results but lacks direct ablation studies to confirm the necessity of each stage.
- **Low confidence**: The hardware efficiency claims (71% memory reduction, opportunities for low-power embedded implementation) lack quantitative validation through actual hardware measurements or power consumption data.

## Next Checks

1. **Ablation study on training phases**: Train the model with direct ZSC training (skipping Phases I and II) and compare accuracy to the three-phase approach to isolate the contribution of pre-training.
2. **Dimensionality sensitivity analysis**: Systematically vary the hypervector dimension d (e.g., 512, 1024, 1536, 2048) and measure accuracy and memory usage to identify the optimal configuration.
3. **Hardware validation**: Implement the fixed attribute encoder on an FPGA or ASIC and measure actual memory footprint and inference latency compared to conventional neural network approaches.