---
ver: rpa2
title: Realistic Evaluation of Toxicity in Large Language Models
arxiv_id: '2405.10659'
source_url: https://arxiv.org/abs/2405.10659
tags: []
core_contribution: This paper introduces the Thoroughly Engineered Toxicity (TET)
  dataset, which contains 2,546 manually crafted prompts designed to bypass safety
  mechanisms in large language models (LLMs). These prompts are derived from over
  1 million real-world interactions and include jailbreak scenarios to expose vulnerabilities
  in LLM toxicity defenses.
---

# Realistic Evaluation of Toxicity in Large Language Models

## Quick Facts
- arXiv ID: 2405.10659
- Source URL: https://arxiv.org/abs/2405.10659
- Reference count: 16
- Key outcome: Introduces TET dataset that outperforms existing benchmarks in exposing LLM toxicity vulnerabilities

## Executive Summary
This paper presents the Thoroughly Engineered Toxicity (TET) dataset, a manually crafted benchmark containing 2,546 prompts designed to bypass safety mechanisms in large language models. Derived from over 1 million real-world interactions, TET includes sophisticated jailbreak scenarios that expose vulnerabilities in LLM toxicity defenses. The dataset was used to evaluate seven prominent LLMs including ChatGPT, Gemini Pro, Llama 2, Mistral, OpenChat, Orca 2, and Zephyr, demonstrating significantly higher effectiveness at eliciting toxic responses compared to the existing ToxiGen dataset.

## Method Summary
The study constructed the TET dataset by filtering over 1 million real-world interactions from the chat-lmsys-1M dataset using HateBERT to identify prompts that elicited toxic responses from multiple open-source LLMs. The filtered subset was then scored across six toxicity dimensions using Perspective API, and the top 1000 prompts for each dimension were selected to create the final 2,546-prompt dataset. Seven LLMs were evaluated on this dataset, with their responses scored using the same multi-dimensional toxicity framework. Results were compared against the ToxiGen dataset using prompts with similar toxicity levels to measure relative effectiveness.

## Key Results
- Llama2-7B-Chat showed toxicity scores of 22.994 on TET versus 11.778 on ToxiGen-S
- ChatGPT 3.5 showed scores of 24.404 versus 5.284 on the same comparison
- TET is significantly more effective at eliciting toxic responses across all tested models
- Model defenses vary substantially depending on the jailbreak prompt template used

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The TET dataset is more effective at exposing LLM toxicity because it contains prompts derived from real-world interactions that bypass safety mechanisms through sophisticated jailbreak techniques
- Mechanism: By filtering over 1 million real-world interactions and selecting prompts that elicited toxic responses from multiple open-source LLMs, TET captures realistic adversarial scenarios that models encounter in the wild
- Core assumption: Real-world jailbreak prompts are more representative of actual attack vectors than synthetic prompts
- Evidence anchors: [abstract] "theThoroughly Engineered Toxicity (TET) dataset, comprising manually crafted prompts designed to nullify the protective layers of such models"; [section] "we utilize HateBERT to filter out prompts in chat-lmsys-1M that elicited toxic responses"
- Break condition: If safety mechanisms become robust against common jailbreak patterns or if models are trained with adversarial examples from similar real-world datasets

### Mechanism 2
- Claim: TET reveals subtler toxicity issues in LLMs that remain hidden when using normal prompts because it includes jailbreak scenarios that bypass protective layers
- Mechanism: The dataset contains prompts specifically engineered to circumvent safety filters, exposing vulnerabilities that standard toxicity benchmarks miss
- Core assumption: Safety mechanisms have blind spots that can be exploited through sophisticated prompt engineering
- Evidence anchors: [abstract] "these safeguards can be easily bypassed with minimal prompt engineering"; [section] "Incorporating such jailbreak scenarios into our dataset exposes the vulnerabilities of LLMs"
- Break condition: If models implement robust contextual understanding that prevents jailbreak techniques

### Mechanism 3
- Claim: The multi-dimensional toxicity analysis using Perspective API provides more granular insights into model behavior than single-score toxicity detection
- Mechanism: By evaluating responses across six distinct toxicity metrics (toxicity, severe toxicity, identity attack, insult, profanity, threat), TET captures nuanced patterns of harmful content generation that single metrics would miss
- Core assumption: Different models have varying strengths and weaknesses across different types of toxic content
- Evidence anchors: [abstract] "Perspective API stands as the state-of-the-art tool for multifaceted abusive content detection"; [section] "For each of the six toxicity criteria provided by Perspective API, we rank the prompts based on their corresponding scores"
- Break condition: If single-score toxicity detection becomes sufficiently accurate

## Foundational Learning

- Concept: Adversarial prompt engineering
  - Why needed here: Understanding how malicious actors craft prompts to bypass safety mechanisms is crucial for creating effective evaluation datasets and defensive strategies
  - Quick check question: What are the common techniques used in jailbreak prompts, and how do they exploit weaknesses in language model safety mechanisms?

- Concept: Multi-dimensional toxicity classification
  - Why needed here: Different types of harmful content require different detection approaches, and understanding the distinctions between toxicity, identity attacks, insults, etc. is essential for comprehensive safety evaluation
  - Quick check question: How does Perspective API differentiate between various types of toxic content, and why is this granularity important for LLM safety assessment?

- Concept: Dataset construction and filtering methodologies
  - Why needed here: The process of selecting and filtering prompts from large interaction datasets determines the quality and representativeness of the evaluation benchmark
  - Quick check question: What criteria and filtering methods are used to identify prompts that effectively test model safety boundaries?

## Architecture Onboarding

- Component map: chat-lmsys-1M interactions -> HateBERT filtering -> Perspective API scoring -> TET prompt selection -> LLM evaluation -> toxicity scoring -> analysis
- Critical path: Collect and preprocess real-world interaction data -> Filter prompts using toxicity detection models -> Score filtered prompts across multiple toxicity dimensions -> Select prompts that maximize toxicity exposure -> Benchmark multiple LLM models using selected prompts -> Analyze and compare results across different evaluation metrics
- Design tradeoffs: Real-world vs synthetic prompts (authenticity vs control); Single vs multi-dimensional toxicity analysis (simplicity vs comprehensiveness); Manual vs automated prompt selection (quality vs scalability)
- Failure signatures: Overfitting to specific prompt patterns; Metric gaming; False negatives; False positives
- First 3 experiments: 1) Run baseline toxicity evaluation using standard prompts (ToxiGen) to establish baseline toxicity scores for comparison; 2) Evaluate same models using TET dataset to measure relative effectiveness at exposing toxicity; 3) Conduct ablation study by removing jailbreak templates from TET to quantify their contribution to overall toxicity detection

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LLMs respond to jailbreak prompts in conversational scenarios?
- Basis in paper: [inferred] The paper mentions that further exploration in conversational contexts is needed to provide a more complete understanding of chat models' performance
- Why unresolved: The study focuses on static prompt evaluation rather than dynamic conversation-based scenarios
- What evidence would resolve it: A systematic evaluation of LLMs in multi-turn conversations using TET prompts, measuring how models' responses evolve and whether they become more vulnerable to toxicity over conversation history

### Open Question 2
- Question: How do larger LLMs (beyond 70B parameters) perform on TET compared to smaller models?
- Basis in paper: [explicit] The paper states that unavailability of computational resources prevented benchmarking of widely-used larger models
- Why unresolved: The study only includes models up to 70B parameters, potentially missing critical performance differences in larger architectures
- What evidence would resolve it: Comprehensive benchmarking of frontier models (e.g., GPT-4, Claude 3) on TET, comparing their toxicity resistance patterns to smaller models

### Open Question 3
- Question: What is the relationship between prompt toxicity and model vulnerability to jailbreak attacks?
- Basis in paper: [inferred] The paper observes that some models perform well against certain jailbreak templates while poorly against others
- Why unresolved: The study doesn't systematically analyze how different prompt characteristics correlate with jailbreak success rates
- What evidence would resolve it: A detailed analysis correlating specific prompt features with jailbreak effectiveness, potentially revealing patterns in which types of jailbreak attempts are most successful

## Limitations
- Manual crafting process may introduce selection bias in prompt selection
- Focus on English-language content limits generalizability to multilingual contexts
- Dataset construction relies on filtering responses from open-source models, which may not represent full diversity of real-world attack patterns

## Confidence
- High confidence in comparative effectiveness of TET vs ToxiGen for toxicity elicitation
- Medium confidence in claim that real-world prompts are more effective than synthetic ones
- Low confidence in generalizability of findings across different languages and cultural contexts

## Next Checks
1. Conduct cross-validation using alternative toxicity detection frameworks beyond Perspective API to verify consistency of toxicity scoring
2. Test model performance on a holdout set of real-world prompts not used in dataset construction to assess overfitting concerns
3. Evaluate the same models on multilingual toxic prompts to determine if findings generalize across languages