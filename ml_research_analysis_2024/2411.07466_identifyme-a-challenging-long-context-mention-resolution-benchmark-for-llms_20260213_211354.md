---
ver: rpa2
title: 'IdentifyMe: A Challenging Long-Context Mention Resolution Benchmark for LLMs'
arxiv_id: '2411.07466'
source_url: https://arxiv.org/abs/2411.07466
tags:
- mention
- entity
- mentions
- llms
- identifyme
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: IdentifyMe is a new benchmark for mention resolution in long narratives,
  presented in multiple-choice question format to better assess LLM referential understanding.
  The benchmark uses curated mentions from literary coreference datasets and includes
  challenging pronominal and nominal cases, along with nested mentions and None of
  the Above options.
---

# IdentifyMe: A Challenging Long-Context Mention Resolution Benchmark for LLMs

## Quick Facts
- **arXiv ID**: 2411.07466
- **Source URL**: https://arxiv.org/abs/2411.07466
- **Authors**: Kawshik Manikantan; Makarand Tapaswi; Vineet Gandhi; Shubham Toshniwal
- **Reference count**: 3
- **Primary result**: Top models achieve 81.9% accuracy, with significant performance gaps on pronominal mentions and NoA cases

## Executive Summary
IdentifyMe is a new benchmark designed to evaluate large language models' ability to resolve mentions in long literary narratives. The benchmark uses a multiple-choice question format to avoid the complexities of exact span identification, focusing on genuinely challenging pronominal and nominal mentions filtered through heuristics. It includes None of the Above (NoA) options to test model confidence in resolution decisions. Evaluations show that while top models like GPT-4o perform reasonably well, they still struggle significantly with pronominal mentions and cases where NoA is the correct answer, revealing important limitations in current LLM referential understanding capabilities.

## Method Summary
IdentifyMe creates MCQ-style mention resolution questions from literary coreference datasets (LitBank and FantasyCoref) by filtering out easily resolvable mentions using heuristics based on fuzzy string similarity and context disambiguation. The benchmark focuses on pronominal and nominal mentions in long contexts (1700-2000 words) and includes 10% NoA questions where the correct entity is removed. Models are evaluated using both direct answer prompts and chain-of-thought reasoning, with performance measured as accuracy across different mention types and dataset sources.

## Key Results
- GPT-4o achieves 81.9% accuracy on the benchmark, with open-source models lagging 20-30% behind
- Pronominal mentions are particularly challenging, with accuracy gaps of 10-20% compared to nominal mentions
- NoA questions prove especially difficult, with open-source models experiencing over 50% performance drops
- Errors cluster in nested mentions and cases where entities overlap, highlighting specific LLM limitations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The MCQ format improves LLM performance by eliminating the need for exact span identification, thus avoiding errors from dataset-specific annotation choices.
- **Mechanism**: Traditional coreference resolution requires precise mention boundaries, but MCQ options provide canonical entity labels that bypass this requirement.
- **Core assumption**: LLMs can accurately select from a fixed set of options even if they cannot precisely identify mention spans.
- **Evidence anchors**: [abstract] "it eliminates the need for exact antecedent span identification during mention resolution evaluation, thus mitigating errors caused by dataset-specific annotation choices"; [section 1] "The MCQ format is commonly used in large language model (LLM) evaluations (Hendrycks et al., 2021) and offers two key advantages. First, its widespread presence in pretraining datasets enables LLMs to answer questions in this format effectively"
- **Break condition**: If the MCQ options are poorly constructed or ambiguous, making the selection task itself difficult.

### Mechanism 2
- **Claim**: The heuristic filtering of easily identifiable mentions creates a more challenging benchmark that better exposes LLM limitations.
- **Mechanism**: By excluding mentions with high fuzzy similarity to entity labels and those with clear disambiguating context, the benchmark focuses on genuinely difficult resolution cases.
- **Core assumption**: The heuristics effectively identify and remove mentions that would be trivially resolved by any competent model.
- **Evidence anchors**: [section 2.1] "To make the benchmark challenging, we restrict it to pronominal and nominal mentions and apply heuristics for each mention type to filter out easily resolvable cases"; [section 2.3] "The performance drops of 9.5% for Mistral-7B and 7.2% for the more robust GPT-4o-mini demonstrate that IdentifyMe captures more challenging mentions compared to random sampling"
- **Break condition**: If the heuristics incorrectly filter out genuinely challenging mentions or retain some easy ones.

### Mechanism 3
- **Claim**: The inclusion of None of the Above (NoA) options creates a more realistic evaluation by testing the model's confidence in its resolution.
- **Mechanism**: NoA forces models to distinguish between cases where the mention truly has no valid antecedent versus cases where the correct antecedent is simply not in the provided options.
- **Core assumption**: Models can learn to recognize when a mention cannot be resolved to any provided entity.
- **Evidence anchors**: [abstract] "The instances where None of the Above is the correct answer prove particularly challenging for all the models, with open-source models experiencing a performance drop of more than 50%"; [section 3.2] "We remove the correct entity from 10% of the questions, making NoA the correct choice"
- **Break condition**: If models systematically choose NoA too frequently or too rarely, indicating miscalibrated confidence.

## Foundational Learning

- **Concept: Coreference resolution fundamentals**
  - Why needed here: Understanding the difference between mention detection and mention linking is crucial for interpreting benchmark results
  - Quick check question: What's the distinction between identifying mentions and clustering them by entity identity?

- **Concept: Fuzzy string matching**
  - Why needed here: The heuristic filtering relies on calculating similarity between mentions and entity labels
  - Quick check question: How would you calculate fuzzy similarity between "the old woman" and "old woman" to determine if a mention is too easy?

- **Concept: Chain-of-thought prompting**
  - Why needed here: The evaluation uses CoT to improve model performance, requiring understanding of how reasoning steps affect output
  - Quick check question: What's the difference between a direct answer prompt and a CoT prompt in terms of model behavior?

## Architecture Onboarding

- **Component map**: LitBank and FantasyCoref annotations → mention extraction → entity label generation → MCQ construction → model evaluation with CoT → accuracy calculation
- **Critical path**: Mention selection heuristics → MCQ construction → model evaluation with CoT → accuracy reporting
- **Design tradeoffs**: Domain specificity (literary texts only) vs. general applicability; MCQ format convenience vs. loss of detailed span information; heuristic filtering for difficulty vs. potential bias introduction
- **Failure signatures**: Low accuracy on NoA questions indicates confidence calibration issues; similar performance on nested vs. non-nested mentions suggests span resolution problems; large gaps between nominal and pronominal performance indicate surface form dependency
- **First 3 experiments**: 1) Compare performance with and without CoT prompting on the validation set; 2) Evaluate model accuracy when the correct answer is an entity vs. NoA; 3) Analyze nested mention resolution by checking if predicted entities correspond to overlapping mentions

## Open Questions the Paper Calls Out
None

## Limitations
- The benchmark's focus on literary texts limits generalizability to other domains like news, dialogue, or technical writing
- The MCQ format loses information about exact mention boundaries that may be important for downstream applications
- Heuristic filtering introduces potential bias that could systematically exclude certain mention types or contexts

## Confidence

**High Confidence (8/10)**: The overall methodology of using MCQ format to evaluate mention resolution is sound and well-justified. The performance differences between model families and mention types are clearly demonstrated and consistent across multiple evaluation conditions.

**Medium Confidence (6/10)**: The heuristic filtering approach effectively creates a challenging benchmark, but the specific impact of each heuristic on benchmark composition is not fully characterized. The claim that the benchmark better captures challenging mentions than random sampling is supported but could benefit from more rigorous statistical analysis.

**Low Confidence (4/10)**: The assertion that MCQ format eliminates errors from dataset-specific annotation choices is plausible but not empirically validated. The performance differences on NoA questions, while striking, may be influenced by factors beyond just confidence calibration.

## Next Checks

1. **Cross-domain validation**: Evaluate IdentifyMe performance on non-literary texts (news, dialogue, technical writing) to assess domain generalizability and identify whether the same model limitations persist across different discourse contexts.

2. **Span recovery analysis**: For a subset of questions where models answer correctly, analyze whether the models could also identify the correct mention span given the MCQ context, to quantify what information is truly lost by the MCQ format.

3. **Heuristic bias quantification**: Systematically analyze the distribution of mention types, entity properties, and discourse contexts in the filtered benchmark versus the original datasets to quantify potential biases introduced by the filtering heuristics.