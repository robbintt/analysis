---
ver: rpa2
title: Massively Multiagent Minigames for Training Generalist Agents
arxiv_id: '2406.05071'
source_url: https://arxiv.org/abs/2406.05071
tags:
- training
- level
- features
- agents
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Meta MMO introduces a collection of many-agent minigames as a reinforcement
  learning benchmark built on Neural MMO. It enables fine-grained control over game
  objectives, agent spawning, team assignments, and various game elements, with built-in
  domain randomization and adaptive difficulty.
---

# Massively Multiagent Minigames for Training Generalist Agents

## Quick Facts
- **arXiv ID**: 2406.05071
- **Source URL**: https://arxiv.org/abs/2406.05071
- **Reference count**: 40
- **Key outcome**: Meta MMO enables training generalist agents across multiple minigames, with performance matching specialists after training on the same samples plus auxiliary data, and up to 3x faster training than Neural MMO 2

## Executive Summary
This work introduces Meta MMO, a collection of many-agent minigames built on Neural MMO, as a reinforcement learning benchmark for training generalist agents. The framework provides fine-grained control over game objectives, agent spawning, team assignments, and game elements, along with built-in domain randomization and adaptive difficulty. Using a simple curriculum learning approach with PPO, the authors demonstrate that a generalist agent can match specialist performance after training on the same number of samples from the target task plus auxiliary data from other minigames. The training process is also significantly faster, completing 400M agent steps in under 20 hours on a single RTX 4090.

## Method Summary
Meta MMO is a reinforcement learning benchmark that extends Neural MMO by providing a collection of many-agent minigames with fine-grained control over game parameters. The framework allows for custom game objectives, agent spawning, team assignments, and various game elements, as well as built-in domain randomization and adaptive difficulty. The authors use a simple curriculum learning method combined with PPO to train a generalist agent across multiple minigames. The training process is designed to enable the agent to learn transferable skills and knowledge that can be applied to new tasks.

## Key Results
- Generalist agent trained on multiple minigames matches specialist performance after training on the same number of samples from the target task plus auxiliary data
- Training with minigames is up to 3x faster than Neural MMO 2, completing 400M agent steps in under 20 hours on a single RTX 4090
- Meta MMO provides a flexible framework for studying generalization and skill transfer in many-agent RL

## Why This Works (Mechanism)
The Meta MMO framework works by exposing agents to a diverse set of tasks and game variations through its many-agent minigames. The built-in domain randomization and adaptive difficulty ensure that agents encounter a wide range of scenarios during training, promoting the development of generalizable skills and knowledge. The curriculum learning approach helps agents gradually build upon their existing knowledge as they progress through increasingly complex tasks. By training on multiple minigames, the generalist agent can learn to identify and exploit common patterns and strategies across different tasks, enabling it to adapt quickly to new environments.

## Foundational Learning
- **Reinforcement Learning (RL)**: The core learning paradigm used in this work, where agents learn to maximize cumulative rewards through interactions with the environment. Understanding RL is crucial for grasping the training process and the agent's ability to learn from experience.
- **PPO (Proximal Policy Optimization)**: A popular RL algorithm used in this work for training the generalist agent. Familiarity with PPO's concepts, such as policy gradients and clipped objective functions, is essential for understanding the training methodology.
- **Curriculum Learning**: A training approach where agents are gradually exposed to more complex tasks as they progress. Understanding curriculum learning is important for grasping how the generalist agent builds upon its knowledge over time.
- **Domain Randomization**: A technique used to improve generalization by exposing agents to a wide range of environment variations during training. Familiarity with domain randomization is crucial for understanding how the Meta MMO framework promotes the development of robust, generalizable skills.
- **Transfer Learning**: The ability of an agent to apply knowledge learned from one task to another related task. Understanding transfer learning is essential for grasping how the generalist agent can leverage its experience across multiple minigames to quickly adapt to new tasks.

## Architecture Onboarding

### Component Map
Meta MMO Environment -> PPO Agent -> Minigame Manager -> Domain Randomizer -> Adaptive Difficulty Controller

### Critical Path
1. Initialize Meta MMO environment with desired minigames and parameters
2. PPO agent interacts with the environment, collecting experiences
3. Minigame Manager selects and configures the current minigame
4. Domain Randomizer introduces variations in the environment
5. Adaptive Difficulty Controller adjusts the challenge level based on agent performance
6. PPO agent updates its policy using collected experiences and rewards
7. Repeat steps 2-6 for multiple training iterations

### Design Tradeoffs
- **Generalist vs. Specialist**: Training a single generalist agent across multiple minigames may lead to slower initial learning compared to training specialists for each task, but can result in better long-term adaptability and transfer learning capabilities.
- **Curriculum Complexity**: Choosing the right curriculum for gradually increasing task difficulty is crucial for efficient learning. Too simple a curriculum may lead to slow progress, while too complex a curriculum may hinder learning altogether.
- **Domain Randomization Extent**: The degree of environment variation introduced through domain randomization can impact the agent's ability to generalize. Too little randomization may lead to overfitting, while too much randomization may make learning more challenging.

### Failure Signatures
- **Underfitting**: If the agent fails to learn any meaningful skills or strategies across the minigames, it may indicate that the curriculum is too complex or the domain randomization is too extensive.
- **Overfitting**: If the agent performs well on the training minigames but fails to generalize to new tasks, it may suggest that the domain randomization is insufficient or the curriculum is not diverse enough.
- **Slow Learning**: If the agent's performance improves very slowly across the minigames, it may indicate that the PPO hyperparameters need tuning or that the curriculum needs adjustment.

### First 3 Experiments
1. Train a PPO agent on a single minigame and evaluate its performance to establish a baseline.
2. Train a PPO agent on multiple minigames using the proposed curriculum learning approach and compare its performance to the baseline.
3. Evaluate the trained generalist agent's ability to adapt to a new, unseen minigame to assess its transfer learning capabilities.

## Open Questions the Paper Calls Out
None

## Limitations
- The work relies on PPO with curriculum learning, which may not be optimal for all minigame combinations or task distributions.
- The long-term stability and robustness of the generalist agent's performance across diverse task families remain unclear.
- While the framework enables fine-grained control over game parameters, the extent to which these controls translate to meaningful generalization across truly disparate domains is not fully characterized.

## Confidence
- **High confidence**: The framework's ability to train agents across multiple minigames with built-in domain randomization and adaptive difficulty
- **Medium confidence**: The claim that generalist agents can match specialist performance with auxiliary data
- **Medium confidence**: The 3x speed improvement over Neural MMO 2 in terms of agent steps per training time

## Next Checks
1. Test the generalist agent's performance when trained on a much larger variety of minigames to evaluate true generalization capabilities beyond similar task families.
2. Compare PPO with curriculum learning against alternative training methods (e.g., IMPALA, R2D2) to verify the approach is not method-specific.
3. Evaluate the agent's zero-shot transfer performance on entirely unseen minigames to better understand the limits of generalization.