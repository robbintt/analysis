---
ver: rpa2
title: Rethinking Top Probability from Multi-view for Distracted Driver Behaviour
  Localization
arxiv_id: '2411.12525'
source_url: https://arxiv.org/abs/2411.12525
tags:
- action
- recognition
- class
- video
- distracted
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of distracted driver behavior
  localization in naturalistic driving videos by proposing a multi-stage system combining
  action recognition, multi-view ensemble strategy, and conditional post-processing.
  The method employs a self-supervised video masked autoencoder (VideoMAE) for robust
  action classification, followed by an ensemble strategy that leverages three camera
  views (dashboard, rearview, and rightside) to improve prediction accuracy.
---

# Rethinking Top Probability from Multi-view for Distracted Driver Behaviour Localization

## Quick Facts
- arXiv ID: 2411.12525
- Source URL: https://arxiv.org/abs/2411.12525
- Reference count: 38
- Achieved 6th place in 2024 AI City Challenge with os score 0.7625

## Executive Summary
This paper presents a multi-stage system for distracted driver behavior localization using naturalistic driving videos from multiple synchronized camera views. The approach combines self-supervised video masked autoencoder (VideoMAE) for action recognition, a multi-view ensemble strategy leveraging dashboard, rearview, and rightside cameras, and a conditional post-processing module for temporal boundary refinement. The method demonstrates strong performance on the 2024 AI City Challenge dataset, achieving sixth place with an overlap score of 0.7625.

## Method Summary
The system employs a three-stage pipeline: (1) VideoMAE-based self-supervised action recognition on 64-frame video clips from three synchronized camera views, (2) multi-view ensemble strategy that combines predictions across dashboard, rearview, and rightside perspectives, and (3) conditional post-processing using top-1 and top-2 prediction confidences to refine temporal boundaries and resolve classification ambiguities. The approach addresses the challenge of distracted driver behavior localization in naturalistic driving scenarios by leveraging complementary information from multiple camera perspectives.

## Key Results
- Achieved 6th place on 2024 AI City Challenge leaderboard with os score of 0.7625
- Outperformed 7th-ranked method by nearly 8% in temporal localization accuracy
- Successfully localized 16 different distracting activities in naturalistic driving videos
- Demonstrated effectiveness of multi-view ensemble strategy and conditional post-processing

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-supervised learning via VideoMAE improves action recognition robustness by learning temporal coherence from masked video patches.
- Mechanism: VideoMAE uses masked autoencoding to train on unlabeled video data, forcing the model to reconstruct missing frames and thereby capture essential spatiotemporal features.
- Core assumption: Temporal coherence in video data provides meaningful supervision signals for learning robust representations.
- Evidence anchors:
  - [abstract] "we adopt an action recognition model based on self-supervise learning to detect distracted activities"
  - [section] "VideoMAE [22] employs Masked Autoencoders... encouraging the model to learn useful representations that capture the underlying structure of the data"
- Break condition: If video sequences lack sufficient temporal coherence or if masking ratio is too high, the model may learn spurious correlations instead of meaningful patterns.

### Mechanism 2
- Claim: Multi-view ensemble strategy reduces class-specific view dependency by leveraging complementary camera perspectives.
- Mechanism: Three camera views (dashboard, rearview, rightside) each capture different aspects of driver behavior, and ensemble averaging their predictions mitigates individual view limitations.
- Core assumption: Different camera views capture complementary information about the same actions, and combining them improves overall accuracy.
- Evidence anchors:
  - [section] "Each of these views has significance in different contexts... In addition, several specific classes can be integrated by all views"
  - [section] "Tab. 1 illustrate the effect of each of views on different classes"
- Break condition: If certain camera views are consistently unreliable or if the system cannot properly weight view contributions, ensemble may introduce noise rather than improve accuracy.

### Mechanism 3
- Claim: Conditional post-processing using top-1 and top-2 confidences smooths temporal boundaries and reduces classification noise.
- Mechanism: The system analyzes both highest and second-highest prediction confidences to merge similar actions, resolve ambiguous segments, and restore missing labels.
- Core assumption: Top-1 and top-2 confidences contain sufficient information to distinguish between genuine action transitions and classification uncertainty.
- Evidence anchors:
  - [section] "Our post-processing strategy leverages top 1 and top 2 of output probability to locate the actions and time boundary more accurately"
  - [section] "This process consists of three main steps: Conditional Merging, Conditional Decision and Missing Labels Restoring"
- Break condition: If top-1 and top-2 confidences are very close in value across many frames, the system may incorrectly merge distinct actions or create false boundaries.

## Foundational Learning

- Concept: Masked Autoencoders for Video Understanding
  - Why needed here: VideoMAE requires understanding how masking ratios and reconstruction objectives affect learned representations for action recognition.
  - Quick check question: How does increasing the masking ratio in VideoMAE affect the model's ability to reconstruct missing frames versus learning useful features?

- Concept: Temporal Action Localization Metrics
  - Why needed here: The os (overlap score) metric requires understanding how temporal intersection and union calculations measure localization accuracy.
  - Quick check question: If a predicted segment overlaps 80% with ground truth but is twice as long, what is the resulting os score?

- Concept: Ensemble Learning Theory
  - Why needed here: The multi-view ensemble strategy requires understanding how different models' predictions can be combined effectively.
  - Quick check question: Under what conditions does simple averaging of model predictions perform worse than weighted averaging in ensemble methods?

## Architecture Onboarding

- Component map:
  Input: 64-frame video segments from three camera views
  VideoMAE classifiers (3 instances, one per view)
  Ensemble module (averages predictions across views)
  Conditional post-processing (merging, decision, restoration)
  Output: Localized action segments with temporal boundaries

- Critical path: Video → VideoMAE → Ensemble → Post-processing → Output
  - Each stage must complete successfully for the system to produce valid results

- Design tradeoffs:
  - Masking ratio vs. reconstruction quality in VideoMAE
  - Simple averaging vs. weighted ensemble strategies
  - Sensitivity of conditional thresholds vs. robustness to noise

- Failure signatures:
  - Low accuracy on specific classes despite high overall accuracy (view-specific weaknesses)
  - Jittery temporal boundaries in output (post-processing threshold issues)
  - Missing labels despite confident predictions (restoration module failure)

- First 3 experiments:
  1. Test individual camera view accuracy on validation set to identify weakest views
  2. Vary VideoMAE masking ratio (e.g., 50%, 75%, 90%) and measure impact on action recognition
  3. Compare simple averaging vs. confidence-weighted ensemble strategies on validation set

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the ensemble strategy's performance vary across different action categories when combining multiple camera views?
- Basis in paper: [explicit] The paper mentions that different camera views excel at recognizing specific actions (e.g., right side view for hand movements, dashboard view for eating/drinking), but doesn't provide detailed analysis of how the ensemble strategy performs across all categories.
- Why unresolved: The paper only shows overall accuracy improvements from the ensemble strategy without breaking down performance by individual action categories.
- What evidence would resolve it: Detailed per-category accuracy metrics comparing single-view vs ensemble performance across all 16 action classes.

### Open Question 2
- Question: What is the optimal frame sampling rate and temporal window size for the VideoMAE action recognition model?
- Basis in paper: [inferred] The paper uses 64 frames per video with FPS 30 and mentions trimming videos, but doesn't explore how different temporal resolutions affect performance.
- Why unresolved: The temporal parameters appear fixed without justification or exploration of alternative configurations.
- What evidence would resolve it: Comparative results showing recognition accuracy using different frame rates (e.g., 15, 30, 60 FPS) and temporal window sizes (e.g., 32, 64, 128 frames).

### Open Question 3
- Question: How does the conditional post-processing module handle ambiguous cases where top-1 and top-2 predictions have similar confidence scores?
- Basis in paper: [explicit] The paper describes using both top-1 and top-2 confidences but doesn't detail the decision mechanism when these scores are close.
- Why unresolved: The paper mentions the threshold (>0.2) for top-2 consideration but doesn't explain the strategy for handling near-equal confidences.
- What evidence would resolve it: Analysis of cases where top-1 and top-2 confidences differ by less than 10%, showing how the model makes decisions in these ambiguous scenarios.

## Limitations

- Method relies heavily on quality and synchronization of multi-view camera inputs, with potential performance degradation if views are occluded or poorly calibrated
- Conditional post-processing thresholds were optimized for this specific dataset and may not generalize to different driving conditions or cultural contexts
- 64-frame input window (approximately 2 seconds at 30 FPS) may miss longer-duration distractions or fail to capture full context of complex behaviors

## Confidence

- High confidence in VideoMAE-based action recognition: The self-supervised learning approach has strong theoretical foundations and demonstrated effectiveness in prior work
- Medium confidence in multi-view ensemble strategy: While ensemble methods generally improve performance, the optimal combination weights for different camera views remain empirically determined
- Medium confidence in conditional post-processing: The approach shows promise but relies on specific threshold values that may not generalize across datasets

## Next Checks

1. Ablation study on masking ratios in VideoMAE to quantify the impact on action recognition accuracy and identify optimal parameters
2. Cross-dataset validation using different distracted driver datasets to test generalization of the conditional post-processing thresholds
3. Analysis of per-class performance breakdown to identify specific driver behaviors where the multi-view ensemble strategy underperforms compared to individual camera views