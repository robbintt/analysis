---
ver: rpa2
title: 'Leveraging ChatGPT in Pharmacovigilance Event Extraction: An Empirical Study'
arxiv_id: '2402.15663'
source_url: https://arxiv.org/abs/2402.15663
tags:
- event
- chatgpt
- data
- extraction
- argument
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of leveraging ChatGPT
  for pharmacovigilance event extraction, a task aimed at identifying and extracting
  adverse events or potential therapeutic events from medical text. The study explores
  various prompting and demonstration selection strategies for zero-shot and few-shot
  learning, comparing ChatGPT's performance to fully fine-tuned smaller models.
---

# Leveraging ChatGPT in Pharmacovigilance Event Extraction: An Empirical Study

## Quick Facts
- arXiv ID: 2402.15663
- Source URL: https://arxiv.org/abs/2402.15663
- Reference count: 11
- Primary result: ChatGPT shows reasonable performance in pharmacovigilance event extraction but falls short of fine-tuned smaller models, with data augmentation showing limited benefits due to annotation quality issues.

## Executive Summary
This paper investigates the effectiveness of leveraging ChatGPT for pharmacovigilance event extraction, a task aimed at identifying and extracting adverse events or potential therapeutic events from medical text. The study explores various prompting and demonstration selection strategies for zero-shot and few-shot learning, comparing ChatGPT's performance to fully fine-tuned smaller models. While ChatGPT demonstrates reasonable performance with appropriate demonstration selection strategies, it falls short of fully fine-tuned smaller models. Additionally, the paper explores the potential of leveraging ChatGPT for data augmentation, finding that filtering strategies can improve stability but not consistently enhance performance.

## Method Summary
The study evaluates ChatGPT's performance on the PHEE dataset using zero-shot and few-shot prompting strategies, comparing against fine-tuned models including Generative QA, UIE, and Flan-T5. The authors implement various prompt engineering techniques, use BM25 for demonstration selection in few-shot learning, and explore data augmentation with filtering strategies based on model confidence scores. The experiments involve systematic comparison of different prompting approaches, demonstration selection methods, and synthetic data filtering techniques to assess ChatGPT's capabilities and limitations for pharmacovigilance event extraction.

## Key Results
- ChatGPT with appropriate demonstration selection (BM25) achieves reasonable performance in few-shot learning but remains below fine-tuned models
- Detailed schema explanations in prompts significantly improve zero-shot performance compared to instruction-only approaches
- Data augmentation with synthetic examples from ChatGPT can lead to performance degradation, but filtering based on model confidence can improve stability
- BM25 demonstration selection outperforms other methods (random, SBERT, TreeKernel) in few-shot learning scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing detailed explanations of the event schema improves ChatGPT's zero-shot performance in pharmacovigilance event extraction.
- Mechanism: Detailed explanations help ChatGPT better understand the task requirements and the structure of the expected output, leading to more accurate event and argument extraction.
- Core assumption: ChatGPT can effectively utilize additional context provided in the prompt to improve its performance on the task.
- Evidence anchors:
  - [abstract]: "The findings demonstrate that while ChatGPT demonstrates reasonable performance with appropriate demonstration selection strategies..."
  - [section]: "Providing only instructions yields unsatisfactory performance, but including a detailed explanation of the event schema leads to noticeable improvement, highlighting the importance of comprehensive guidance."
  - [corpus]: Weak evidence - corpus search did not yield specific papers directly supporting this mechanism, but related work on prompt engineering suggests the importance of clear instructions for LLMs.
- Break condition: If the explanations become too complex or verbose, ChatGPT may struggle to extract the relevant information, potentially leading to decreased performance.

### Mechanism 2
- Claim: Using BM25 for in-context demonstration selection improves ChatGPT's few-shot performance in pharmacovigilance event extraction.
- Mechanism: BM25 retrieves examples that are lexically similar to the test sentence, which often contain matching entities such as drugs. This helps ChatGPT learn more effectively from the demonstrations, as the task is sensitive to entities.
- Core assumption: The task of pharmacovigilance event extraction is more sensitive to entities than to structural similarity.
- Evidence anchors:
  - [section]: "Incorporating structured information improves performance, while the simplest lexical-based retrieval strategy shows the most noticeable performance gains. Upon examining the samples retrieved by different example selection strategies, we observed that BM25 is more inclined to retrieve sentences containing matching entities such as drugs."
  - [corpus]: Weak evidence - corpus search did not yield specific papers directly supporting this mechanism, but related work on information retrieval suggests the effectiveness of BM25 for entity matching.
- Break condition: If the test sentence contains rare or unseen entities, BM25 may fail to retrieve relevant examples, leading to decreased performance.

### Mechanism 3
- Claim: Filtering synthetic data based on model confidence can improve the stability of performance when using ChatGPT for data augmentation.
- Mechanism: Filtering out low-confidence annotations from the synthetic data reduces the impact of noise, leading to more stable performance when the filtered data is combined with the original training data.
- Core assumption: A fine-tuned model has some discriminatory ability regarding the quality of annotations, and incorrect annotations may result in lower confidence scores from the model for the annotation sequence.
- Evidence anchors:
  - [abstract]: "To mitigate this, we explore different filtering strategies and find that, with the proper approach, more stable performance can be achieved..."
  - [section]: "Recognizing that directly incorporating generated samples into the training data can lead to performance decline, possibly due to issues related to data quality, we have introduced filtering strategies."
  - [corpus]: Weak evidence - corpus search did not yield specific papers directly supporting this mechanism, but related work on data filtering and quality control suggests its potential benefits.
- Break condition: If the filtering threshold is set too high, it may result in the removal of valuable data, leading to decreased performance.

## Foundational Learning

- Concept: Prompt engineering
  - Why needed here: Effective prompt engineering is crucial for leveraging ChatGPT's capabilities in pharmacovigilance event extraction, as it helps the model understand the task requirements and generate accurate outputs.
  - Quick check question: What are the key elements of a well-designed prompt for ChatGPT in the context of pharmacovigilance event extraction?

- Concept: In-context learning
  - Why needed here: In-context learning allows ChatGPT to learn from demonstrations provided in the prompt, enabling few-shot learning for the pharmacovigilance event extraction task.
  - Quick check question: How does the choice of demonstration examples impact ChatGPT's performance in few-shot learning for pharmacovigilance event extraction?

- Concept: Data augmentation and filtering
  - Why needed here: Data augmentation can help improve the performance of small models, but it is essential to filter the synthetic data to reduce noise and maintain data quality.
  - Quick check question: What are the potential benefits and challenges of using ChatGPT for data augmentation in pharmacovigilance event extraction?

## Architecture Onboarding

- Component map: ChatGPT -> BM25 Demonstration Selection -> Filtering Strategies -> Fine-tuned Models
- Critical path:
  1. Design prompts for ChatGPT (zero-shot and few-shot learning)
  2. Select in-context demonstration examples using BM25
  3. Generate synthetic data using ChatGPT
  4. Filter synthetic data based on model confidence
  5. Combine filtered synthetic data with original training data
  6. Fine-tune models on the combined dataset
  7. Evaluate model performance
- Design tradeoffs:
  - Zero-shot vs. few-shot learning: Zero-shot learning requires no demonstrations but may have lower performance, while few-shot learning can improve performance but requires careful selection of demonstrations.
  - Data augmentation vs. data quality: Data augmentation can help improve model performance, but it is essential to maintain data quality through filtering to avoid introducing noise.
- Failure signatures:
  - Poor performance in zero-shot learning may indicate the need for more detailed prompt instructions or the use of few-shot learning.
  - Decreased performance when using synthetic data may indicate the presence of noise or insufficient filtering.
- First 3 experiments:
  1. Compare ChatGPT's performance using zero-shot prompting with and without detailed schema explanations.
  2. Evaluate the impact of different in-context demonstration selection strategies (random, SBERT, TreeKernel, BM25) on ChatGPT's few-shot performance.
  3. Assess the effect of data filtering on the performance of models trained with synthetic data generated by ChatGPT.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different LLM architectures (e.g., LLaMA 30B, Flan-T5 XXL) compare to ChatGPT in zero-shot/few-shot pharmacovigilance event extraction?
- Basis in paper: [explicit] The paper mentions encountering limitations in exploring alternative open-source LLMs like LLaMA 30B and Flan-T5 XXL for zero-shot/few-shot prompting due to significant differences in generation quality and slow inference speeds.
- Why unresolved: The authors were unable to conduct a comprehensive evaluation of these models due to technical constraints.
- What evidence would resolve it: Systematic benchmarking of multiple LLM architectures on the pharmacovigilance event extraction task, comparing their performance, inference speed, and resource requirements.

### Open Question 2
- Question: Can Chain-of-Thought (CoT) reasoning improve few-shot learning performance for fine-grained event extraction tasks like pharmacovigilance?
- Basis in paper: [inferred] The paper acknowledges that CoT reasoning has shown enhanced performance in few-shot learning for biomedical NLP tasks but did not incorporate it due to the intricate nature of constructing reasoning steps for each argument in the fine-grained event extraction task.
- Why unresolved: The authors did not have annotated reasoning steps for all samples, hindering a comprehensive evaluation of the CoT method in this context.
- What evidence would resolve it: Experiments comparing the performance of few-shot pharmacovigilance event extraction with and without CoT reasoning, using models trained on data with annotated reasoning steps.

### Open Question 3
- Question: How can the coverage of rare arguments in ChatGPT-synthesized examples be improved to enhance data augmentation effectiveness?
- Basis in paper: [explicit] The qualitative analysis revealed that ChatGPT-synthesized examples have less coverage for rare arguments compared to the given templates, which may contribute to performance degradation in argument extraction when using synthetic data for augmentation.
- Why unresolved: The paper suggests that providing more diverse examples when synthesizing data may be a worthwhile direction to explore but does not investigate specific methods to achieve this.
- What evidence would resolve it: Development and evaluation of techniques to improve the diversity and coverage of rare arguments in ChatGPT-synthesized examples, followed by experiments assessing the impact on data augmentation effectiveness for pharmacovigilance event extraction.

## Limitations
- ChatGPT performance remains below fully fine-tuned smaller models despite various prompting strategies
- Data augmentation with synthetic examples shows limited benefits due to annotation quality issues
- The study focuses on a single proprietary model (ChatGPT), limiting generalizability to other LLMs
- Filtering strategies for synthetic data show only modest improvements in stability

## Confidence
- **High Confidence**: The observation that detailed prompt instructions improve zero-shot performance is well-supported by controlled experiments and aligns with established prompt engineering principles. The comparison between ChatGPT and fine-tuned models also shows consistent patterns across multiple evaluation metrics.
- **Medium Confidence**: The effectiveness of BM25 for demonstration selection in few-shot learning is supported by empirical results, though the study does not provide extensive ablation studies to rule out alternative explanations. The findings regarding data augmentation benefits with filtering also fall into this category, as results show variability across different filtering strategies.
- **Low Confidence**: The exact mechanisms by which ChatGPT generates annotations and the specific characteristics of noise in these annotations remain poorly understood. The study provides limited analysis of why certain filtering strategies work better than others or how the quality of synthetic data relates to specific types of errors.

## Next Checks
1. **Cross-LLM Validation**: Replicate the key experiments using multiple open-source LLMs (e.g., GPT-4, Claude, LLaMA) to determine whether the observed patterns are specific to ChatGPT or represent more general principles about LLM behavior in pharmacovigilance tasks.

2. **Extended Filtering Analysis**: Implement and evaluate additional filtering strategies beyond confidence-based methods, including uncertainty estimation through Monte Carlo dropout, ensemble methods, or quality scoring based on linguistic features, to determine if more sophisticated approaches can achieve consistent performance improvements.

3. **Error Analysis Framework**: Conduct a detailed error analysis categorizing types of annotation errors in ChatGPT-generated data (entity recognition failures, schema violations, semantic inconsistencies) and correlate these error types with specific filtering outcomes to develop more targeted quality control strategies.