---
ver: rpa2
title: 'WARM: On the Benefits of Weight Averaged Reward Models'
arxiv_id: '2401.12187'
source_url: https://arxiv.org/abs/2401.12187
tags:
- reward
- warm
- weight
- figure
- learning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper proposes Weight Averaged Reward Models (WARM), a method
  that combines multiple reward models (RMs) by averaging their weights in the weight
  space. This approach addresses two key challenges in reward modeling: distribution
  shifts during reinforcement learning (RL) and inconsistencies in human preferences.'
---

# WARM: On the Benefits of Weight Averaged Reward Models

## Quick Facts
- **arXiv ID**: 2401.12187
- **Source URL**: https://arxiv.org/abs/2401.12187
- **Reference count**: 40
- **Primary result**: WARM achieves 79.4% win rate against single RM in RL fine-tuning

## Executive Summary
This paper introduces WARM (Weight Averaged Reward Models), a method that combines multiple reward models by averaging their weights in weight space rather than averaging predictions. The approach addresses two key challenges in reward modeling: distribution shifts during reinforcement learning and inconsistencies in human preferences. By leveraging the observation that fine-tuned weights remain linearly mode connected when sharing the same pre-training, WARM provides an efficient alternative to traditional prediction ensembling while improving reliability under distribution shifts and robustness to preference inconsistencies.

## Method Summary
WARM works by fine-tuning multiple reward models from a shared pre-trained initialization with diverse hyperparameters and data orderings, then averaging their weights to produce a single, more robust reward model. This weight averaging approach approximates prediction ensembling without the memory and inference overhead. The method exploits linear mode connectivity, where fine-tuned weights from a shared pre-training remain in a region where linear interpolation between them does not significantly degrade performance. WARM is then used as the reward signal during RL fine-tuning, with experiments showing improved policy performance and reduced reward hacking compared to single reward models.

## Key Results
- WARM achieves a 79.4% win rate against a policy RL fine-tuned with a single reward model
- Weight averaging improves efficiency compared to traditional ensembling while enhancing reliability under distribution shifts
- WARM demonstrates robustness to preference inconsistencies and label corruption in human preference data
- The method reduces variance in reward estimates, which is the dominant issue under distribution shifts

## Why This Works (Mechanism)

### Mechanism 1: Reliability Under Distribution Shifts
Weight averaging improves reliability under distribution shifts by reducing variance in reward estimates. When multiple reward models are fine-tuned from a shared pre-trained initialization, their weights remain linearly mode connected. Averaging these weights produces a single model that approximates prediction ensembling without memory and inference overhead. This reduces variance, which is the dominant issue under distribution shifts. The core assumption is that fine-tuned weights from shared pre-training remain linearly connected, allowing effective interpolation without significant loss in accuracy.

### Mechanism 2: Robustness to Label Corruption
Weight averaging improves robustness to label corruption by prioritizing invariant predictive mechanisms across runs. Under label corruption, individual reward models may memorize corrupted samples differently. WA, by averaging weights, implicitly downweights features specific to individual runs (often related to memorization) and emphasizes features consistently learned across runs (more likely to be invariant and generalizable). This reduces reliance on spurious correlations introduced by corrupted labels. The core assumption is that features consistently learned across different fine-tuning runs are more likely to be invariant mechanisms rather than spurious correlations.

### Mechanism 3: RL Stability Through Smoother Rewards
Weight averaging facilitates stability in reinforcement learning by making the reward smoother in the input space. By reducing memorization and prioritizing invariant features, WA makes the reward model less sensitive to small input variations. This increased smoothness (related to Lipschitzness) is beneficial for reinforcement learning stability, as it prevents large reward differences from minor input changes that can cause instability in policy gradient updates. The core assumption is that a smoother reward function, with bounded differences for small input changes, leads to more stable policy gradient updates.

## Foundational Learning

- **Linear mode connectivity (LMC)**: This is the foundational observation that allows weight averaging to work effectively. It states that fine-tuned weights from a shared pre-training remain in a region where linear interpolation between them does not significantly degrade performance. *Quick check*: If two reward models are fine-tuned from the same pre-trained model but with different learning rates, will their weights necessarily be linearly connected?

- **Variance reduction in ensemble methods**: Understanding how ensembling reduces variance is crucial for grasping why weight averaging, as an efficient approximation of ensembling, can improve reliability under distribution shifts. *Quick check*: In an ensemble of diverse models, what is the dominant factor contributing to improved performance under distribution shift?

- **Memorization vs. generalization in deep learning**: The paper's argument about WA's robustness to label corruption relies on the distinction between features that lead to memorization (often spurious correlations) and those that lead to generalization (often invariant mechanisms). *Quick check*: How does the presence of label noise in a training dataset typically affect the features learned by a deep neural network?

## Architecture Onboarding

- **Component map**: Pre-trained LLM -> SFT on instructions -> Multiple RM fine-tunings -> Weight averaging -> WARM -> RL fine-tuning using WARM -> Evaluation with control reward and oracle preference metrics

- **Critical path**: 1) Initialize reward models from shared SFT checkpoint, 2) Fine-tune reward models with diverse configurations, 3) Average weights to obtain WARM, 4) Use WARM as reward in RL fine-tuning, 5) Evaluate policy performance

- **Design tradeoffs**: Diversity vs. LMC (too much diversity may break LMC), Number of models (M=6 is effective but increasing M increases training cost), Hyperparameter range (must preserve LMC)

- **Failure signatures**: Reward hacking (control reward collapse during RL), Poor generalization (policy performance degradation on out-of-distribution prompts), Overfitting to corrupted labels (policy exploiting spurious correlations)

- **First 3 experiments**: 1) Validate LMC by interpolating between two fine-tuned RMs and evaluating accuracy, 2) Compare WA vs. ENS under label corruption (25% corruption) on clean validation/test sets, 3) RL fine-tuning with WARM, monitoring control reward and policy performance

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does WARM's performance compare when using different types of architectures or pre-trainings for the individual reward models, compared to using the same architecture and pre-training?
- **Basis in paper**: The paper mentions that prediction ensembling can benefit from combining RMs from various architectures and pre-trainings, suggesting a potential advantage over WARM which requires shared pre-training.
- **Why unresolved**: The paper primarily focuses on using PaLM-XXS models with shared pre-training for the individual RMs in WARM and does not explore performance differences when using diverse architectures or pre-trainings.
- **What evidence would resolve it**: Experiments comparing WARM's performance using different architectures or pre-trainings for the individual RMs versus using the same architecture and pre-training.

### Open Question 2
- **Question**: How does WARM's robustness to label corruption change when the percentage of corrupted labels in the preference dataset is increased beyond 25%?
- **Basis in paper**: The paper mentions that label noise in preference datasets can be significant (72.6% inter-labeler agreement for InstructGPT) and conducts experiments with 25% label corruption but does not explore higher levels.
- **Why unresolved**: The paper does not provide insights into how WARM's robustness to label corruption scales with increasing levels of corruption.
- **What evidence would resolve it**: Experiments with varying levels of label corruption (e.g., 50%, 75%) to assess WARM's performance and robustness.

### Open Question 3
- **Question**: Can WARM be extended to handle multi-objective reward modeling, where different RMs are trained on different preference datasets or with different objectives?
- **Basis in paper**: The paper discusses the potential for extending WARM to combine RMs trained on different datasets, which could be useful for multi-objective scenarios.
- **Why unresolved**: The paper does not investigate the practical implementation or effectiveness of using WARM for multi-objective reward modeling.
- **What evidence would resolve it**: Experiments comparing WARM's performance in multi-objective scenarios versus traditional methods.

## Limitations
- WARM's effectiveness relies heavily on the assumption of linear mode connectivity between fine-tuned reward models, which may not hold in all scenarios
- The analysis of robustness to label corruption is based on theoretical arguments about invariant feature selection rather than extensive empirical validation
- RL experiments show improved win rates but don't directly measure whether improvement stems from reduced reward hacking or better preference alignment

## Confidence
- **High Confidence**: Efficiency benefits of weight averaging over prediction ensembling are well-established and supported by linear mode connectivity literature
- **Medium Confidence**: WARM improves reliability under distribution shifts is supported by variance reduction mechanism but empirical validation is limited
- **Low Confidence**: WARM's robustness to label corruption through invariant feature selection is primarily theoretical with limited direct empirical evidence

## Next Checks
1. **Linear Mode Connectivity Validation**: Conduct interpolation tests between fine-tuned reward models with varying hyperparameters to empirically verify that performance remains stable across the interpolation path, confirming the LMC assumption holds for the specific reward model architecture used.

2. **Distribution Shift Robustness Test**: Design a controlled experiment where reward models are evaluated on progressively out-of-distribution prompts during RL fine-tuning, comparing WARM against single models and prediction ensembling to isolate the distribution shift robustness benefit.

3. **Label Corruption Ablation Study**: Systematically vary the level of label corruption in the preference data (0%, 10%, 25%, 50%) and measure both reward model performance on clean validation sets and policy performance after RL fine-tuning to quantify the robustness benefit.