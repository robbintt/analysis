---
ver: rpa2
title: 'einspace: Searching for Neural Architectures from Fundamental Operations'
arxiv_id: '2405.20838'
source_url: https://arxiv.org/abs/2405.20838
tags:
- search
- architecture
- space
- architectures
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: einspace is a novel neural architecture search space based on a
  parameterised probabilistic context-free grammar (PCFG), designed to discover diverse,
  high-performing architectures from fundamental operations. It supports various sizes
  and complexities, modelling convolutions, attention, and other operations, and contains
  existing state-of-the-art architectures like ResNet, ViT, and MLP-Mixer.
---

# einspace: Searching for Neural Architectures from Fundamental Operations

## Quick Facts
- arXiv ID: 2405.20838
- Source URL: https://arxiv.org/abs/2405.20838
- Reference count: 40
- Primary result: einspace outperforms previous CFG-based search spaces and human-designed architectures on diverse tasks beyond traditional computer vision when initialised with strong baselines

## Executive Summary
einspace is a novel neural architecture search space based on a parameterised probabilistic context-free grammar (PCFG) designed to discover diverse, high-performing architectures from fundamental operations. It supports various sizes and complexities, modelling convolutions, attention, and other operations, and contains existing state-of-the-art architectures like ResNet, ViT, and MLP-Mixer. Experiments on Unseen NAS datasets show that einspace outperforms previous CFG-based search spaces and human-designed architectures on tasks beyond traditional computer vision. Simple search strategies like regularised evolution consistently improve upon baselines when initialised with strong priors (e.g., ResNet18 or a mix of SOTA models), achieving state-of-the-art results on datasets like NinaPro.

## Method Summary
einspace defines a neural architecture search space using a parameterised probabilistic context-free grammar (PCFG) with recursive production rules that can generate diverse architectures including convolutional networks, transformers, and MLPs. The grammar is constrained by parameters (shape, mode, branching factor) to ensure valid architectures, with feature modes (Im/Col) enabling adaptation to different data modalities. The search space is explored using regularised evolution with initialisation from strong baseline architectures like ResNet18, which consistently improves performance across diverse tasks.

## Key Results
- einspace outperforms previous CFG-based search spaces and human-designed architectures on Unseen NAS datasets
- Regularised evolution with ResNet18 initialisation achieves state-of-the-art results on NinaPro dataset
- The search space successfully models diverse architectures including ResNet, ViT, and MLP-Mixer in a single unified framework

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The parameterised probabilistic context-free grammar (PCFG) enables the search space to include diverse architectures (convolutional, transformer, MLP) in a single space.
- Mechanism: The grammar's recursive production rules and parameterised terminals allow construction of complex architectures while maintaining validity through parameter constraints.
- Core assumption: The branching rate of the PCFG is less than one, ensuring finite architecture generation.
- Evidence anchors:
  - [abstract] "...search space based on a parameterised probabilistic context-free grammar...containing diverse network operations which allow it to model convolutions, attention components and more."
  - [section] "...we can ensure that the CFG avoids generating infinitely long architecture strings by setting the probabilities such that the branching rate of the CFG is less than one [8]."
  - [corpus] Weak - the related papers focus on different NAS methods but don't specifically address PCFG-based search spaces.
- Break condition: If the branching rate exceeds one, the grammar could generate infinite architectures, making sampling impossible.

### Mechanism 2
- Claim: Initialising evolutionary search with strong baselines (e.g., ResNet18) consistently improves performance across diverse tasks.
- Mechanism: Seeding the population with high-performing architectures provides a strong starting point, allowing evolutionary search to explore variations while maintaining good performance.
- Core assumption: The search space is expressive enough to allow meaningful mutations from the baseline architectures.
- Evidence anchors:
  - [abstract] "...we consistently find large improvements when initialising the search with strong baselines..."
  - [section] "To fully utilise the powerful priors of existing human-designed structures, we now invoke search where the initial population of our evolutionary search is seeded with a collection of existing state-of-the-art architectures."
  - [corpus] Weak - the related papers discuss different search strategies but don't specifically address the impact of initialisation with strong baselines.
- Break condition: If the search space is not expressive enough, mutations from the baseline may not lead to significant improvements.

### Mechanism 3
- Claim: The feature mode (Im/Col) enables the same search space to handle different data modalities (images, text, audio) by adapting the available operations and tensor shapes.
- Mechanism: The mode parameter restricts available operations based on the expected input shape, ensuring architectural validity while maintaining flexibility across modalities.
- Core assumption: The mode transitions (Im→Col via im2col, Col→Im via col2im) are sufficient to handle the necessary transformations for different tasks.
- Evidence anchors:
  - [abstract] "...our space is versatile, supporting architectures of various sizes and complexities..."
  - [section] "Different neural architectures operate on different feature shapes. ConvNets maintain 3D features throughout most of the network while transformers have 2D features."
  - [corpus] Weak - the related papers don't discuss modality-specific adaptations in search spaces.
- Break condition: If the mode transitions are insufficient for a particular task, the search space may not be able to generate valid architectures.

## Foundational Learning

- Concept: Context-free grammars and their extension to probabilistic context-free grammars
  - Why needed here: The search space is defined using a PCFG, which allows for stochastic generation of architectures with controlled complexity
  - Quick check question: How does the branching rate of a PCFG affect the probability of generating infinite architectures?

- Concept: Neural architecture search (NAS) strategies (random search, evolutionary algorithms)
  - Why needed here: The paper compares different search strategies (random sampling, random search, regularised evolution) within the einspace
  - Quick check question: Why does random search often perform poorly in highly expressive search spaces like einspace?

- Concept: Different neural network architectures (ConvNets, Transformers, MLPs) and their characteristic operations
  - Why needed here: einspace is designed to represent diverse architectures from these families, understanding their components is crucial for grasping the search space's expressiveness
  - Quick check question: What are the key differences between the operations used in ConvNets versus Transformers that einspace needs to accommodate?

## Architecture Onboarding

- Component map: S → M M | B M A | P M R, M → M M | B M A | P M R | C
- Function groups: Branching (B), Aggregation (A), Routing (P, R), Computation (C)
- Parameter system: (s,m,b) values that constrain valid productions based on input shape, feature mode, and branching factor
- Feature modes: Im mode (3D tensors) and Col mode (2D tensors) with mode transitions via im2col/col2im

- Critical path: Grammar definition → Parameterisation → PCFG probability assignment → Sampling/mutation algorithm → Search strategy application

- Design tradeoffs:
  - Expressiveness vs. complexity: More expressive search spaces allow diverse architectures but increase search difficulty
  - Prior constraints vs. flexibility: Imposing priors (convolutional, branching) reduces search space size but may limit discovery of novel architectures
  - Parameterised rules vs. validation: Parameter constraints ensure validity but add complexity to the sampling algorithm

- Failure signatures:
  - Sampling failures: Backtracking occurs when no valid production rules match current parameters
  - Mode mismatches: Operations applied in wrong feature mode (e.g., im2col in Col mode)
  - Branching factor mismatches: Aggregation function branching factor doesn't match preceding branching function

- First 3 experiments:
  1. Implement the grammar and parameter system, then sample 100 random architectures and verify their validity
  2. Implement the evolutionary search algorithm with ResNet18 initialisation, run on a simple dataset (e.g., AddNIST), and compare performance to random search
  3. Implement the feature mode system, create a transformer-style architecture in einspace, and verify it can handle text data input

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the expressiveness of einspace impact the effectiveness of search strategies beyond regularised evolution, such as Bayesian optimisation or gradient-based methods?
- Basis in paper: [explicit] The paper notes that due to the large size and complexity of einspace, one-shot methods like ENAS or DARTS are challenging to implement. However, it suggests that developing an algorithm to learn the probabilities of the PCFG may be more feasible.
- Why unresolved: The paper primarily focuses on regularised evolution and simple search strategies. More sophisticated search strategies that can leverage the expressiveness of einspace have not been explored.
- What evidence would resolve it: Empirical results comparing the performance of various search strategies, including Bayesian optimisation and gradient-based methods, on einspace across multiple datasets.

### Open Question 2
- Question: What are the implications of the potential ambiguity arising from multiple derivation trees for a single architecture in einspace?
- Basis in paper: [explicit] The paper mentions that ambiguity can arise due to the possibility of multiple derivation trees for a single architecture, primarily due to the multiple ways of stacking sequential modules.
- Why unresolved: The paper does not explore the consequences of this ambiguity or propose methods to address it.
- What evidence would resolve it: Analysis of the frequency and impact of ambiguous architectures in einspace, and the development of techniques to identify and handle equivalent architectures.

### Open Question 3
- Question: How can recurrent computation be integrated into einspace to enable the representation of architectures like RNNs and state-space models?
- Basis in paper: [explicit] The paper acknowledges that einspace does not currently support recurrent computation, which is found in RNNs and state-space models like Mamba. It suggests that this could be integrated via the inclusion of a recurrent module that repeats the computation of the components within it.
- Why unresolved: The paper does not provide a concrete implementation or evaluate the effectiveness of incorporating recurrent computation into einspace.
- What evidence would resolve it: A modified version of einspace that includes recurrent computation, along with experiments demonstrating its ability to represent and search for architectures with recurrent components.

## Limitations

- The theoretical guarantees of the PCFG branching rate constraint are not fully validated experimentally
- The impact of initialisation with strong baselines may be overstated with limited statistical analysis
- Feature mode transitions between Im and Col modes are described but not thoroughly validated

## Confidence

- High confidence: The PCFG-based search space definition and its ability to represent diverse architectures (ResNet, ViT, MLP-Mixer)
- Medium confidence: The effectiveness of initialisation with strong baselines
- Low confidence: The generality of the search space across truly diverse tasks

## Next Checks

1. Implement a test to verify that the PCFG branching rate calculation prevents infinite architecture generation by measuring distribution of architecture depths from random sampling
2. Conduct ablation studies comparing search performance with different initialisation strategies (no initialisation, random architectures, only SOTA architectures) to quantify the exact benefit of ResNet18 seeding
3. Validate the feature mode system by attempting to construct architectures for tasks outside the 8 classification datasets (e.g., regression, sequence-to-sequence) and measuring validity rates