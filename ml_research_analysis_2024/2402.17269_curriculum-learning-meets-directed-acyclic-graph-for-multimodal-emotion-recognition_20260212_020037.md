---
ver: rpa2
title: Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion Recognition
arxiv_id: '2402.17269'
source_url: https://arxiv.org/abs/2402.17269
tags:
- multidag
- learning
- emotion
- training
- curriculum
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MultiDAG+CL, a multimodal emotion recognition
  model that integrates Directed Acyclic Graph (DAG) and Curriculum Learning (CL)
  to handle emotional shifts and data imbalance in conversation. The model uses DAG
  to combine textual, acoustic, and visual features, while CL progressively trains
  the model with increasingly complex samples.
---

# Curriculum Learning Meets Directed Acyclic Graph for Multimodal Emotion Recognition

## Quick Facts
- arXiv ID: 2402.17269
- Source URL: https://arxiv.org/abs/2402.17269
- Reference count: 11
- MultiDAG+CL outperforms state-of-the-art ERC models with 1.05% accuracy improvement on IEMOCAP and 0.34% on MELD

## Executive Summary
This paper introduces MultiDAG+CL, a multimodal emotion recognition model that integrates Directed Acyclic Graph (DAG) and Curriculum Learning (CL) to handle emotional shifts and data imbalance in conversation. The model uses DAG to combine textual, acoustic, and visual features while maintaining temporal order, while CL progressively trains the model with increasingly complex samples. Experimental results on IEMOCAP and MELD datasets show that MultiDAG+CL outperforms existing state-of-the-art models, achieving accuracy improvements of 1.05% on IEMOCAP and 0.34% on MELD.

## Method Summary
MultiDAG+CL combines DAG-based message passing for multimodal feature integration with curriculum learning for progressive training. The model employs modality-specific encoders (BiLSTM for text, FC for audio/visual) followed by DAG layers that perform attention-based message passing while preserving temporal order. Curriculum learning uses a difficulty measure function based on emotional shift frequency and speaker count to organize training samples into buckets, gradually exposing the model to more complex conversations. The approach uses 5 buckets for IEMOCAP and 12 buckets for MELD datasets.

## Key Results
- MultiDAG+CL achieves 1.05% accuracy improvement on IEMOCAP dataset compared to state-of-the-art models
- Model shows 0.34% accuracy improvement on MELD dataset
- Ablation studies confirm both DAG and CL components contribute to performance gains
- Best performance achieved when combining all three modalities with optimal bucket counts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DAG-based message passing preserves causal temporal order while enabling distant utterance interactions
- Mechanism: The Directed Acyclic Graph enforces unidirectional flow from predecessors to successors, preventing cycles that would corrupt temporal semantics. Each utterance's hidden state aggregates information from earlier utterances through attention-weighted edges, with edge type (same/speaker/different) modulating information flow.
- Core assumption: Emotional context propagates unidirectionally through conversation turns
- Evidence anchors: [abstract] "Directed Acyclic Graph (DAG) to integrate textual, acoustic, and visual features within a unified framework" and [section] "Each utterance in a conversation receives information exclusively from past utterances. This one-way information flow is effectively represented by a Directed Acyclic Graph (DAG)"
- Break condition: If conversation contains feedback loops or self-reference that violate DAG assumptions

### Mechanism 2
- Claim: Curriculum Learning improves emotional shift handling by prioritizing stable emotion sequences
- Mechanism: Difficulty Measure Function quantifies conversations by emotional shift frequency (Nshift) and speaker count (Nsp), normalizing by total utterances. Training scheduler progressively mixes easier conversations (low DIF) into training set, allowing model to first learn stable emotional patterns before handling complex shifts.
- Core assumption: Conversations with fewer emotional shifts are easier to learn from
- Evidence anchors: [abstract] "Curriculum Learning (CL) to address challenges related to emotional shifts and data imbalance" and [section] "Specifically, e(ui) ≠ e(uk), p(ui) = p(uk), ∄j : i < j < k, p(ui) = p(uj) = p(uk)" and DIF(ci) = (Nshift(ci) + Nsp(ci))/(Nu(ci) + Nsp(ci))
- Break condition: If difficulty metric poorly correlates with actual learning difficulty

### Mechanism 3
- Claim: Multimodal DAG-GNN enables complementary feature fusion across modalities
- Mechanism: Separate modality encoders (BiLSTM for text, FC for audio/visual) produce context-aware features. DAG layers perform cross-modal attention and edge-based aggregation, with GRU units combining node and context information. Final prediction uses concatenated multi-layer representations.
- Core assumption: Different modalities provide complementary emotional signals that benefit from unified graph processing
- Evidence anchors: [abstract] "employs Directed Acyclic Graph (DAG) to integrate textual, acoustic, and visual features within a unified framework" and [section] "We use modality-specific encoders... ha_i = EncA(ua_i); hv_i = EncV(uv_i); hl_i = EncL(ul_i)" and "The attention weight is further utilized in combination with edge relationships to aggregate information"
- Break condition: If modality encoders fail to produce compatible feature spaces

## Foundational Learning

- Concept: Directed Acyclic Graphs in neural networks
  - Why needed here: Ensures temporal coherence in conversation modeling by preventing backward information flow that would violate conversational causality
  - Quick check question: Why can't we use a regular graph for conversation modeling?

- Concept: Curriculum Learning scheduling
  - Why needed here: Gradually exposes model to increasingly complex emotional patterns, preventing overfitting to simple cases while building foundation for handling emotional shifts
  - Quick check question: What happens if we train with hardest examples first?

- Concept: Multimodal feature fusion strategies
  - Why needed here: Different modalities (text, audio, visual) capture distinct aspects of emotional expression that require coordinated processing rather than independent modeling
  - Quick check question: Why not just concatenate raw modality features?

## Architecture Onboarding

- Component map: Modality Encoders (BiLSTM for text, FC for audio/visual) -> DAG-GNN layers (attention-based message passing with edge relations) -> GRU units (node information and context information processing) -> Curriculum Learning pipeline (difficulty measurement and bucket-based scheduling) -> Final prediction layer (feed-forward network)

- Critical path: Modality encoding → DAG message passing → GRU processing → Curriculum scheduling → Final prediction

- Design tradeoffs:
  - BiLSTM vs transformer for text encoding: BiLSTM preserves sequential order better for DAG integration
  - Number of DAG layers: More layers capture longer-range dependencies but increase computational cost
  - Curriculum bucket count: More buckets provide finer difficulty granularity but require more epochs

- Failure signatures:
  - Poor emotional shift handling: Check curriculum scheduling and difficulty measure accuracy
  - Modality imbalance: Verify encoder outputs have comparable scales
  - Overfitting on IEMOCAP: Monitor validation performance across curriculum stages

- First 3 experiments:
  1. Ablation study: Remove DAG component, compare performance on emotional shift detection
  2. Curriculum sensitivity: Vary bucket counts (4, 5, 7, 10, 15) and measure w-F1 improvement
  3. Modality contribution: Train with individual modalities and pairwise combinations, analyze accuracy drops

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of difficulty measure function impact the performance of curriculum learning in multimodal emotion recognition?
- Basis in paper: [explicit] The paper mentions designing a difficulty measure function based on emotional shift frequency, but also notes the potential for exploring alternative training schedulers for curriculum learning.
- Why unresolved: The paper only uses one specific difficulty measure function and does not explore the impact of different difficulty measures on the performance of curriculum learning.
- What evidence would resolve it: Comparing the performance of MultiDAG+CL with different difficulty measure functions (e.g., based on emotional intensity, conversation length, or speaker diversity) on the IEMOCAP and MELD datasets.

### Open Question 2
- Question: What is the impact of incorporating a learning-based approach to model emotion label similarity in multimodal emotion recognition?
- Basis in paper: [explicit] The paper mentions incorporating a learning-based approach to model emotion label similarity as a potential future work.
- Why unresolved: The paper does not explore the impact of incorporating a learning-based approach to model emotion label similarity on the performance of multimodal emotion recognition.
- What evidence would resolve it: Implementing a learning-based approach to model emotion label similarity (e.g., using a neural network to learn the similarity between emotion labels) and comparing the performance of MultiDAG+CL with and without this approach on the IEMOCAP and MELD datasets.

### Open Question 3
- Question: How does the performance of MultiDAG+CL compare to other state-of-the-art multimodal emotion recognition models on different datasets?
- Basis in paper: [explicit] The paper compares the performance of MultiDAG+CL to several state-of-the-art multimodal ERC models on the IEMOCAP and MELD datasets.
- Why unresolved: The paper only evaluates the performance of MultiDAG+CL on two datasets (IEMOCAP and MELD) and does not compare its performance to other state-of-the-art models on different datasets.
- What evidence would resolve it: Evaluating the performance of MultiDAG+CL on other multimodal emotion recognition datasets (e.g., MOSEI, CMU-MOSEI, or EmoryNLP) and comparing its performance to other state-of-the-art models on these datasets.

## Limitations
- DAG assumption may not capture all conversational dynamics, particularly emotional feedback loops
- Modest performance gains (1.05% on IEMOCAP, 0.34% on MELD) suggest limited practical impact
- Difficulty measure function lacks validation against alternative metrics for learning difficulty

## Confidence
**High Confidence**: The core methodology combining DAG and CL is technically sound and well-implemented. The ablation studies showing performance degradation when removing either component provide strong evidence for their contributions.

**Medium Confidence**: The specific difficulty measure function (Nshift + Nsp)/(Nu + Nsp) is reasonable but not rigorously validated against alternatives. The optimal bucket counts (5 for IEMOCAP, 12 for MELD) appear dataset-specific without clear theoretical justification.

**Low Confidence**: The claim that DAG specifically addresses emotional shifts better than other graph architectures lacks comparative analysis with alternative graph structures like cyclic graphs or undirected graphs.

## Next Checks
1. **Difficulty Measure Validation**: Test alternative difficulty metrics (e.g., emotional intensity variance, turn-taking complexity) and compare their impact on CL performance to validate the proposed measure's effectiveness.

2. **Graph Structure Ablation**: Implement and compare against cyclic conversation graphs and undirected graphs to determine if the DAG constraint is truly necessary for the observed improvements.

3. **Cross-Dataset Generalization**: Evaluate the trained model on additional emotion recognition datasets (e.g., DailyDialog, EmoryNLP) to assess whether the curriculum learning benefits transfer beyond IEMOCAP and MELD.