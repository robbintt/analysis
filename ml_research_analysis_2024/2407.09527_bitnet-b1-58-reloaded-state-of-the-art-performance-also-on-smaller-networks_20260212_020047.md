---
ver: rpa2
title: 'BitNet b1.58 Reloaded: State-of-the-art Performance Also on Smaller Networks'
arxiv_id: '2407.09527'
source_url: https://arxiv.org/abs/2407.09527
tags:
- training
- quantization
- learning
- weights
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates 1.58-bit quantization-aware training for
  small language and vision models ranging from 100K to 48M parameters. The authors
  introduce a variant of BitNet b1.58 that uses median rather than mean in the quantization
  process.
---

# BitNet b1.58 Reloaded: State-of-the-art Performance Also on Smaller Networks

## Quick Facts
- arXiv ID: 2407.09527
- Source URL: https://arxiv.org/abs/2407.09527
- Reference count: 19
- Primary result: 1.58-bit quantization-aware training achieves state-of-the-art performance for small language and vision models

## Executive Summary
This paper investigates 1.58-bit quantization-aware training for small language and vision models ranging from 100K to 48M parameters. The authors introduce a median-based quantization variant that outperforms mean-based quantization in convergence speed. Their experiments show that 1.58-bit quantization requires approximately double the hidden layer size compared to 16-bit for language models, but achieves superior or comparable performance for vision tasks on CIFAR-10 and CIFAR-100 datasets.

## Method Summary
The authors use BitLinear layers replacing standard linear layers, with 1.58-bit weights quantized to {âˆ’1, 0, 1} and 8-bit quantized activations. Training employs Adam optimizer with batch size 128, testing learning rates from 0.0001 to 0.1 and weight decay from 0% to 10%. Two quantization schemes are compared: AbsMean and AbsMedian for weight scaling. The approach uses shadow 16-bit weights updated during training, with gradients computed via straight-through estimator.

## Key Results
- 1.58-bit quantization requires approximately 2x larger hidden layers for language models compared to 16-bit to achieve similar performance
- 1.58-bit vision models achieve or surpass state-of-the-art performance on CIFAR-10 and CIFAR-100 compared to 16-bit counterparts
- Median-based quantization generally converges faster than mean-based but shows higher sensitivity to learning rate changes

## Why This Works (Mechanism)

### Mechanism 1
The median-based quantization allows higher weight update variance without significant scaling factor changes, leading to faster convergence in some scenarios. When using median quantization, large weight changes of a few weights have less impact on the scaling factor compared to mean quantization, which averages all weight magnitudes. This flexibility allows the optimization process to explore the parameter space more freely without being constrained by outliers in weight magnitudes.

### Mechanism 2
Small models require approximately double the hidden layer size when using 1.58-bit quantization compared to 16-bit to achieve comparable performance. 1.58-bit quantization reduces the representational capacity of each weight from 16 bits to effectively 1.58 bits, which decreases the model's capacity. To compensate for this reduced per-weight capacity, the model needs more weights (larger hidden layers) to maintain the same total representational capacity.

### Mechanism 3
1.58-bit quantization provides inherent robustness to weight decay regularization compared to 16-bit models. The coarse quantization in 1.58-bit models (ternary weights) introduces a form of implicit regularization that makes the optimization landscape more robust to additional explicit regularization like weight decay. The 1.58-bit models are less sensitive to weight decay because the quantization itself already constrains the weight space significantly.

## Foundational Learning

- Quantization-aware training vs post-training quantization: Understanding the difference is crucial because this paper uses quantization-aware training where weights are quantized during training, not after. This affects how gradients flow and how the model learns. Quick check: What is the key difference between quantization-aware training and post-training quantization in terms of when the quantization occurs relative to the training process?

- Straight-through estimator (STE) in backpropagation: The paper mentions using STE for backward computations of gradients, which is essential for training quantized networks. Understanding STE is critical for grasping how gradients flow through non-differentiable quantization operations. Quick check: How does the straight-through estimator handle the non-differentiable quantization operation during backpropagation?

- Weight decay and its interaction with quantization: The paper investigates weight decay effects on both 1.58-bit and 16-bit models, finding different patterns. Understanding weight decay mechanics and how quantization might affect its impact is crucial for interpreting these results. Quick check: What is the primary purpose of weight decay in neural network training, and how might quantization affect its effectiveness?

## Architecture Onboarding

- Component map: Input normalization -> Activation quantization (8-bit) -> Weight quantization (1.58-bit) -> Matrix multiplication with quantized values -> Dequantization -> Gradient computation via STE -> Shadow weight update

- Critical path: 1) Input normalization 2) Activation quantization 3) Weight quantization 4) Matrix multiplication with quantized values 5) Dequantization 6) Gradient computation via STE 7) Shadow weight update

- Design tradeoffs:
  - Integer vs floating-point quantization: The paper uses integer quantization for both weights and activations, enabling multiplication-free kernels but potentially introducing more quantization error
  - Mean vs median for weight scaling: Mean is more sensitive to outliers but provides smoother scaling; median is more robust but can be less stable with learning rate changes
  - Hidden layer size scaling: 1.58-bit models need approximately 2x larger hidden layers compared to 16-bit for comparable performance

- Failure signatures: Training instability manifesting as sudden accuracy drops or training divergence, particularly with high learning rates or aggressive weight decay; convergence to poor minima if quantization scaling factors are poorly chosen; sensitivity to hyperparameters especially learning rate and weight decay

- First 3 experiments:
  1. Scale comparison: Train 16-bit and 1.58-bit models (mean and median variants) with hidden sizes 32, 64, 128, 256 on small language models to verify the 2x hidden size requirement
  2. Hyperparameter sensitivity: Tune learning rate and weight decay on a 12M parameter model to identify optimal settings and sensitivity patterns
  3. Vision task validation: Train on MNIST, CIFAR-10, and CIFAR-100 to confirm vision performance claims and investigate median vs mean quantization differences

## Open Questions the Paper Calls Out

### Open Question 1
How does 1.58-bit quantization performance scale with larger small models (beyond 48M parameters)? The authors state "We would expect the need for this [increasing parameters] to decrease as model-size grows, i.e., we would expect 1.58-bit to work well in networks from a certain size without increasing the number of parameters." This remains unresolved as the study only tested models up to 48M parameters.

### Open Question 2
Does 1.58-bit quantization-aware training provide benefits for encoder-based architectures (e.g., BERT, Vision Transformers)? The paper only tested decoder-only language models and standard CNNs. "We encourage future work to investigate 1.58-bit quantization-aware training on other networks such as object-detection networks in the vision domain and language models with encoders."

### Open Question 3
Which quantization scheme (mean vs median) is generally preferable for 1.58-bit quantization-aware training? "Our results on employing AbsMean vs AbsMedian quantization of the 16-bit shadow weights do not yield distinctive and conclusive results, leaving it as a hyperparameter for now." The experiments showed mixed results across different datasets and model types.

## Limitations
- Limited to 10 epochs of training, which may not be sufficient for full convergence
- Language model evaluation uses a relatively small 135M token corpus without detailed data description
- Focus on small models (100K to 48M parameters) leaves questions about scalability to larger architectures

## Confidence

- High confidence: The core finding that 1.58-bit quantization requires approximately 2x larger hidden layers for comparable performance
- Medium confidence: The claim that median-based quantization generally converges faster than mean-based
- Medium confidence: The assertion that 1.58-bit quantization outperforms 16-bit on vision tasks

## Next Checks
1. Test the scaling relationship between 1.58-bit and 16-bit models on a larger language model (e.g., 100M+ parameters) to verify if the 2x hidden layer requirement holds beyond the studied range.
2. Conduct longer training runs (50+ epochs) to determine if the performance gaps between quantization methods and bit-widths change with extended optimization.
3. Evaluate the robustness of median vs mean quantization across different learning rate schedules and weight decay combinations to better characterize the sensitivity claims.