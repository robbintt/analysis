---
ver: rpa2
title: Multimodal Reasoning with Multimodal Knowledge Graph
arxiv_id: '2406.02030'
source_url: https://arxiv.org/abs/2406.02030
tags:
- knowledge
- multimodal
- mr-mkg
- reasoning
- image
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses hallucinations and knowledge gaps in multimodal
  reasoning with LLMs by introducing Multimodal Reasoning with Multimodal Knowledge
  Graphs (MR-MKG), which integrates multimodal knowledge graphs into LLMs using a
  relation graph attention network and cross-modal alignment. MR-MKG achieves state-of-the-art
  performance on multimodal question answering (ScienceQA) and multimodal analogy
  reasoning (MARS), improving accuracy by 1.95% and Hits@1 by 10.4% respectively,
  while only updating ~2.25% of LLM parameters.
---

# Multimodal Reasoning with Multimodal Knowledge Graph

## Quick Facts
- **arXiv ID**: 2406.02030
- **Source URL**: https://arxiv.org/abs/2406.02030
- **Reference count**: 23
- **Primary result**: MR-MKG improves multimodal QA accuracy by 1.95% and MARS Hits@1 by 10.4% while updating only 2.25% of LLM parameters.

## Executive Summary
This paper addresses hallucinations and knowledge gaps in multimodal reasoning by integrating multimodal knowledge graphs (MMKGs) into large language models (LLMs). The proposed MR-MKG method uses a relation graph attention network (RGAT) to encode MMKG nodes, a cross-modal alignment module to optimize image-text alignment, and parameter-efficient fine-tuning via adapter layers. The approach achieves state-of-the-art performance on ScienceQA and MARS benchmarks while only updating a small fraction of LLM parameters.

## Method Summary
MR-MKG constructs multimodal knowledge graphs from FreeBase, DBpedia, and YAGO, then retrieves relevant subgraphs based on input text or images. The method employs RGAT to encode knowledge nodes while preserving graph structure, and uses a cross-modal alignment module with triplet loss to reduce modality gaps. The model freezes the LLM and visual encoder, updating only adapter layers (approximately 2.25% of parameters). Training occurs in two stages: pre-training on an MMKG-grounded dataset followed by task-specific fine-tuning on ScienceQA and MARS.

## Key Results
- Achieves 1.95% accuracy improvement on ScienceQA
- Improves MARS Hits@1 by 10.4%
- Trains only 2.25% of LLM parameters
- Outperforms baselines on both multimodal QA and analogy reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1
RGAT encoding of MMKGs preserves graph structure and improves knowledge retrieval for multimodal reasoning. The RGAT model uses node and relation embeddings initialized by CLIP, then applies attention layers over the graph to capture complex relationships. This structured representation allows the LLM to access rich, context-aware knowledge rather than flat triples. Core assumption: Visual modality provides discriminative embeddings that transfer well to multimodal entity representations. Break condition: If RGAT layers cannot learn meaningful graph embeddings due to insufficient training data or noise in the MMKG.

### Mechanism 2
Cross-modal alignment via triplet loss reduces modality gap and improves image-text consistency. By sampling image entities and their corresponding text entities from the MMKG, the model learns to minimize distance between matched pairs and maximize distance to negative samples. This aligns visual and textual representations in a shared space. Core assumption: The MMKG contains accurate paired image-text entities that reflect true correspondences. Break condition: If the MMKG lacks correct image-text pairs, the alignment task may reinforce wrong associations.

### Mechanism 3
Parameter-efficient fine-tuning (2.25% of LLM params) achieves state-of-the-art performance without full model retraining. Freezing the LLM backbone and only training adapter layers reduces overfitting risk and computational cost while retaining LLM reasoning power. Core assumption: The frozen LLM can still leverage new knowledge when properly formatted in its input space. Break condition: If the LLM's pretraining knowledge is too outdated or incompatible, small adapters may not suffice.

## Foundational Learning

- **Concept**: Graph Neural Networks (GNNs) and RGAT
  - Why needed here: To embed MMKG nodes while preserving relational structure for downstream reasoning.
  - Quick check question: What is the key difference between a GAT and an RGAT?

- **Concept**: Cross-modal alignment and metric learning
  - Why needed here: To bridge the modality gap between image and text embeddings for coherent multimodal reasoning.
  - Quick check question: What loss function is commonly used for contrastive learning in multimodal settings?

- **Concept**: Parameter-efficient fine-tuning (adapter layers)
  - Why needed here: To update only a small subset of parameters for cost-effective adaptation to new knowledge.
  - Quick check question: What is the typical percentage of parameters updated in adapter-based fine-tuning?

## Architecture Onboarding

- **Component map**: Inputs (Text, Image, MMKG Subgraph) -> Encoders (LLM embedder, CLIP-based Visual, RGAT-based KG) -> Adapters (Visual, Knowledge) -> Alignment Module (Triplet Loss) -> LLM Prompt -> Output

- **Critical path**: 1. Retrieve relevant MMKG subgraph 2. Encode text, image, KG via respective encoders 3. Apply adapters to align embeddings to LLM space 4. Apply cross-modal alignment loss during training 5. Concatenate embeddings -> LLM prompt

- **Design tradeoffs**: RGAT vs GAT: RGAT can explicitly model relations; heavier computation. Full fine-tuning vs adapters: Full fine-tuning risks overfitting; adapters save compute. Cross-modal alignment: Adds training complexity but improves multimodal coherence.

- **Failure signatures**: Low alignment loss but poor QA performance: KG alignment may be misaligned with task objective. High retrieval recall but low accuracy: MMKG quality or subgraph filtering needs improvement. Large variance in Hits@k: Retrieval quality unstable; need better subgraph selection strategy.

- **First 3 experiments**: 1. Ablation: Remove cross-modal alignment; measure impact on accuracy 2. Retrieval test: Vary number of triples (5, 10, 20); observe performance curve 3. Architecture swap: Replace RGAT with GAT; compare Hits@k and accuracy on MARS

## Open Questions the Paper Calls Out

### Open Question 1
How does the quality and relevance of retrieved multimodal knowledge graphs affect the performance of MR-MKG across different tasks and datasets? The paper discusses the effectiveness of retrieved sub-MMKGs and notes that insufficient or ambiguous knowledge can lead to errors, but does not quantify the impact of knowledge quality on performance across different tasks and datasets. A systematic study comparing performance using knowledge graphs of varying quality and relevance across multiple tasks and datasets would resolve this.

### Open Question 2
Can the MR-MKG approach be effectively scaled to even larger models and a broader range of multimodal reasoning tasks? The paper mentions limitations due to computational resources and suggests future work to scale up the method to larger model sizes and assess its performance on a broader range of tasks. Experiments demonstrating performance when applied to larger models (e.g., LLaMA-2 70B) and diverse multimodal reasoning tasks beyond ScienceQA and MARS would resolve this.

### Open Question 3
What is the optimal strategy for retrieving multimodal knowledge graphs, and how does it vary depending on the nature of the task? The paper discusses different subgraph retrieval methods and their impact on performance, noting that effectiveness depends on dataset characteristics. A detailed analysis comparing performance using various retrieval strategies tailored to specific task types and datasets, identifying the most effective approach for each scenario, would resolve this.

## Limitations

- MMKG construction relies on pre-existing knowledge bases without addressing potential biases or gaps
- RGAT implementation details are sparse, with critical hyperparameters unspecified
- Evaluation limited to only two datasets (ScienceQA and MARS), restricting generalizability claims
- Ablation studies focus on adapter presence but don't isolate contribution of individual components

## Confidence

- **High confidence**: The parameter-efficient fine-tuning claim (2.25% of parameters) is well-supported by the architecture description and matches established adapter literature patterns.
- **Medium confidence**: The performance improvements (1.95% accuracy gain on ScienceQA, 10.4% Hits@1 on MARS) are reported but lack statistical significance testing and comparison to baseline uncertainty ranges.
- **Low confidence**: The mechanism claims about RGAT preserving graph structure and cross-modal alignment reducing modality gaps are plausible but lack direct empirical validation through ablation studies or qualitative analysis.

## Next Checks

1. **Statistical validation**: Compute 95% confidence intervals for the reported accuracy and Hits@k metrics across multiple training runs to assess whether the performance gains are statistically significant versus baselines.

2. **Component isolation ablation**: Create systematic ablations that separately remove: (a) RGAT encoding, (b) cross-modal alignment, (c) knowledge retrieval subgraph size variation, and (d) adapter layers. This would quantify each component's marginal contribution.

3. **Generalization test**: Evaluate MR-MKG on a third multimodal dataset (e.g., VQA-CP, GQA, or a held-out subset of ScienceQA with different domain focus) to assess whether the method generalizes beyond the two reported datasets.