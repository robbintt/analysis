---
ver: rpa2
title: Deterministic Policies for Constrained Reinforcement Learning in Polynomial
  Time
arxiv_id: '2405.14183'
source_url: https://arxiv.org/abs/2405.14183
tags:
- definition
- algorithm
- proof
- lemma
- cost
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a polynomial-time approximation scheme (FPTAS)
  for computing near-optimal deterministic policies in constrained reinforcement learning
  (CRL) problems. The key idea is to convert the CRL problem into a covering problem
  using value-demand augmentation, then solve it using approximate dynamic programming
  over both time and space.
---

# Deterministic Policies for Constrained Reinforcement Learning in Polynomial Time

## Quick Facts
- arXiv ID: 2405.14183
- Source URL: https://arxiv.org/abs/2405.14183
- Reference count: 40
- Key result: Polynomial-time approximation scheme for near-optimal deterministic policies in constrained RL with expectation, almost sure, and anytime constraints

## Executive Summary
This paper presents the first polynomial-time approximation scheme (FPTAS) for computing near-optimal deterministic policies in constrained reinforcement learning problems. The algorithm achieves additive or relative approximation guarantees for three types of constraints (expectation, almost sure, and anytime) that have remained open for nearly 25 years. By converting the CRL problem into a covering problem through value-demand augmentation and solving it using approximate dynamic programming over both time and space, the approach provides deterministic policies that are approximately optimal with polynomial-time complexity.

## Method Summary
The method introduces a novel value-demand augmentation technique that transforms the constrained reinforcement learning problem into a covering problem. This augmented problem is then solved using approximate dynamic programming that operates simultaneously over the time horizon and the state space. The algorithm employs time-space rounding to discretize both temporal and spatial dimensions, enabling polynomial-time computation while maintaining approximation guarantees. The approach works for both non-negative and general reward settings, providing either relative or additive FPTAS guarantees respectively.

## Key Results
- First polynomial-time approximation scheme for CRL problems with expectation, almost sure, and anytime constraints
- Additive FPTAS with complexity O(H^7 S^5 A r_max^3 / ε^3) for general reward settings
- Relative FPTAS with complexity O(H^7 S^5 A log(r_max/r_min p_min)^3 / ε^3) for non-negative rewards
- Achieves deterministic policies that are approximately optimal with provable guarantees

## Why This Works (Mechanism)
The approach works by transforming the constrained optimization problem into a tractable covering problem through value-demand augmentation, which effectively encodes the constraints into the state space. This transformation allows the use of approximate dynamic programming techniques that can handle both the temporal structure and the augmented state space efficiently. The time-space rounding discretization ensures that the approximate solution remains close to the optimal solution while enabling polynomial-time computation.

## Foundational Learning
- Value-demand augmentation: Encodes constraints into state space to convert CRL to covering problem (needed for tractable approximation; check: problem constraints become part of state representation)
- Approximate dynamic programming over space: Extends standard ADP to handle augmented state space efficiently (needed for polynomial-time solution; check: maintains approximation bounds despite discretization)
- Time-space rounding: Discretizes both temporal and spatial dimensions to enable polynomial computation (needed for computational tractability; check: discretization error remains within ε bounds)

## Architecture Onboarding
- Component map: MDP problem -> Value-demand augmentation -> Time-space rounding -> Approximate DP over space-time
- Critical path: State representation → Constraint encoding → Discretization → Policy computation
- Design tradeoffs: Deterministic policies provide computational efficiency but may limit solution quality compared to stochastic policies
- Failure signatures: Poor performance when transition probabilities approach zero (violates p_min assumption), impractical for very large state spaces or long horizons
- First experiments: 1) Test on small CRL problems with known optimal solutions, 2) Vary ε to verify approximation quality, 3) Compare deterministic vs. optimal stochastic policies on benchmark problems

## Open Questions the Paper Calls Out
None

## Limitations
- Requires known, bounded state and action spaces with strictly positive transition probabilities
- Computational complexity grows exponentially with horizon length H
- Deterministic policy restriction may not be suitable for all constrained RL domains
- Assumes perfect knowledge of MDP model rather than learning from samples

## Confidence
- High confidence: Polynomial-time approximation framework and theoretical correctness under stated assumptions
- Medium confidence: Practical scalability claims given stated complexity bounds
- Low confidence: Applicability of deterministic policies to all constrained RL domains

## Next Checks
1. Empirical evaluation on benchmark constrained RL problems to verify practical running time matches theoretical bounds
2. Testing algorithm's robustness when transition probabilities approach zero (testing p_min assumption)
3. Extension to settings with model uncertainty or where only samples are available, comparing performance to existing sampling-based approaches