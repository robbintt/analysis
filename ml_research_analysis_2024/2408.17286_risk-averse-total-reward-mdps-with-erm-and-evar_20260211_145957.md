---
ver: rpa2
title: Risk-averse Total-reward MDPs with ERM and EVaR
arxiv_id: '2408.17286'
source_url: https://arxiv.org/abs/2408.17286
tags:
- risk
- optimal
- value
- policy
- policies
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of optimizing risk-averse objectives
  in Markov decision processes (MDPs) using the total reward criterion (TRC) with
  Entropic Risk Measure (ERM) and Entropic Value at Risk (EVaR) risk measures. The
  core method involves showing that risk-averse TRC with ERM and EVaR admits optimal
  stationary policies in transient MDPs, making them simple to analyze, interpret,
  and deploy.
---

# Risk-averse Total-reward MDPs with ERM and EVaR
## Quick Facts
- arXiv ID: 2408.17286
- Source URL: https://arxiv.org/abs/2408.17286
- Authors: Xihong Su; Julien Grand-Clément; Marek Petrik
- Reference count: 30
- One-line primary result: Risk-averse TRC with ERM and EVaR admits optimal stationary policies in transient MDPs

## Executive Summary
This paper tackles the challenge of optimizing risk-averse objectives in Markov decision processes using total reward criterion with Entropic Risk Measure (ERM) and Entropic Value at Risk (EVaR). The authors demonstrate that risk-averse total reward MDPs with these specific risk measures admit optimal stationary policies in transient MDPs, making them more practical for real-world deployment. The work relaxes previous constraints by allowing both positive and negative rewards, expanding the applicability of risk-averse reinforcement learning methods.

The proposed approach combines theoretical analysis with practical algorithms including exponential value iteration, policy iteration, and linear programming to compute optimal policies. The primary contribution is showing that the total reward criterion can be preferable to the discounted criterion in risk-averse reinforcement learning domains, with algorithms capable of computing δ-optimal EVaR policies.

## Method Summary
The authors develop a framework for risk-averse total reward MDPs by showing that ERM and EVaR risk measures preserve the existence of optimal stationary policies in transient MDPs. They formulate the problem using exponential utility functions and demonstrate that the optimal value function satisfies a modified Bellman equation. The proposed algorithms include exponential value iteration (which iterates on risk-adjusted value functions), policy iteration (which alternates between policy evaluation and improvement steps), and linear programming formulations that directly optimize for risk-averse criteria. These methods are shown to converge to optimal or δ-optimal policies under the specified conditions.

## Key Results
- Risk-averse total reward MDPs with ERM and EVaR admit optimal stationary policies in transient MDPs
- Proposed algorithms (exponential value iteration, policy iteration, linear programming) can compute δ-optimal EVaR policies
- The total reward criterion may be preferable to the discounted criterion in risk-averse reinforcement learning domains
- The framework allows both positive and negative rewards, relaxing previous non-negativity constraints

## Why This Works (Mechanism)
The mechanism works because ERM and EVaR risk measures have specific mathematical properties that preserve the structure needed for optimal stationary policies in transient MDPs. These risk measures use exponential utility functions that naturally handle risk aversion while maintaining the mathematical properties required for convergence. The transient nature of the MDP ensures that policies eventually reach absorbing states or exit, which is crucial for the existence of optimal stationary policies under these risk measures.

## Foundational Learning
- **Transient MDPs**: Why needed - The existence of optimal stationary policies relies on the transient property. Quick check - Verify the MDP eventually reaches terminal states with probability 1 under any policy.
- **Entropic Risk Measure (ERM)**: Why needed - Provides a specific form of risk aversion through exponential utility. Quick check - Confirm ERM satisfies the required convexity and coherence properties.
- **Entropic Value at Risk (EVaR)**: Why needed - Offers a coherent risk measure that generalizes VaR and CVaR. Quick check - Verify EVaR maintains the mathematical properties needed for MDP analysis.
- **Total Reward Criterion**: Why needed - Unlike discounted criterion, accumulates rewards without decay factor. Quick check - Ensure the total reward converges appropriately in transient settings.
- **Stationary Policies**: Why needed - Optimal policies that don't change over time simplify implementation. Quick check - Confirm that policy evaluation and improvement steps converge.

## Architecture Onboarding
- **Component Map**: Risk-averse objective -> ERM/EVaR formulation -> Bellman equation modification -> Algorithm selection (value iteration/policy iteration/LP) -> Optimal stationary policy
- **Critical Path**: Problem formulation -> Mathematical analysis of risk measures -> Algorithm design -> Convergence proof -> Implementation
- **Design Tradeoffs**: Transient MDP assumption vs. broader applicability, computational complexity of exponential operations vs. accuracy of risk aversion, theoretical guarantees vs. practical implementation challenges
- **Failure Signatures**: Non-convergence of value iteration indicates improper risk measure parameters, suboptimal policies suggest incorrect MDP modeling, numerical instability from exponential calculations
- **3 First Experiments**: 1) Test on simple transient MDP with known optimal solution to verify algorithm correctness, 2) Compare ERM vs EVaR performance on risk-sensitive tasks, 3) Validate handling of negative rewards in standard benchmark problems

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Analysis is limited to transient MDPs, excluding periodic or absorbing states common in practical applications
- Theoretical guarantees primarily developed for finite MDPs with countable state-action spaces
- Risk measures considered (ERM and EVaR) may not capture all types of risk aversion needed in specific domains
- Does not explore alternative risk measures that might be more suitable for certain applications

## Confidence
- Optimal stationary policies exist for risk-averse TRC with ERM and EVaR in transient MDPs: Medium confidence (relies on specific properties of risk measures and transient MDPs)
- Algorithmic results for computing δ-optimal EVaR policies: High confidence (builds on well-established techniques with clear convergence properties)
- Total reward criterion preferable to discounted criterion in risk-averse domains: Medium confidence (requires empirical validation across diverse domains)

## Next Checks
1. Test the proposed algorithms on MDPs with absorbing states to verify whether theoretical guarantees extend beyond transient MDPs
2. Implement and evaluate the methods on real-world risk-averse reinforcement learning problems with mixed reward signals
3. Compare the performance of ERM and EVaR risk measures against alternative risk-averse approaches in domains where risk sensitivity is critical