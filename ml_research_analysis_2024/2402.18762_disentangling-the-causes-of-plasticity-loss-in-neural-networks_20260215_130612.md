---
ver: rpa2
title: Disentangling the Causes of Plasticity Loss in Neural Networks
arxiv_id: '2402.18762'
source_url: https://arxiv.org/abs/2402.18762
tags:
- loss
- network
- plasticity
- networks
- normalization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper investigates plasticity loss in neural networks, a phenomenon
  where networks become less adaptable to new learning signals over time. It decomposes
  plasticity loss into three phases: learning dynamics, parameter/feature changes,
  and manifestation in empirical neural tangent kernels.'
---

# Disentangling the Causes of Plasticity Loss in Neural Networks

## Quick Facts
- arXiv ID: 2402.18762
- Source URL: https://arxiv.org/abs/2402.18762
- Reference count: 40
- Primary result: Identifies three independent mechanisms driving plasticity loss in neural networks and demonstrates that combining layer normalization with weight decay creates highly robust learning algorithms.

## Executive Summary
Plasticity loss in neural networks refers to the decreasing ability of networks to adapt to new learning signals over time. This paper decomposes plasticity loss into three distinct phases and identifies three independent mechanisms: preactivation distribution shift, parameter norm growth, and large regression target means. Through extensive experiments across synthetic nonstationary tasks, reinforcement learning benchmarks, and natural distribution shift datasets, the authors demonstrate that while each mechanism can cause plasticity loss individually, combining interventions targeting multiple mechanisms yields highly effective solutions. Specifically, the combination of layer normalization and weight decay is shown to be highly effective at maintaining plasticity across various learning scenarios.

## Method Summary
The study investigates plasticity loss through a decomposition approach, analyzing learning dynamics, parameter changes, and their manifestation in empirical neural tangent kernels. The authors identify three independent mechanisms causing plasticity loss and test interventions targeting each mechanism individually and in combination. The primary intervention tested is the combination of layer normalization and weight decay regularization, which addresses all three mechanisms simultaneously. Experiments span synthetic nonstationary tasks, reinforcement learning in the Arcade Learning Environment, and natural distribution shift datasets like iWildcam.

## Key Results
- Three independent mechanisms drive plasticity loss: preactivation distribution shift, parameter norm growth, and large regression target means
- Layer normalization and weight decay combination is highly effective at maintaining plasticity across various learning scenarios
- The approach improves performance on deep reinforcement learning benchmarks and natural distribution shift datasets
- Interventions targeting multiple mechanisms simultaneously yield more robust learning algorithms than single-mechanism approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preactivation distribution shift causes plasticity loss by pushing hidden units into linearized regimes.
- Mechanism: Shifts in preactivation statistics push ReLU units into saturated or linear regimes, reducing expressivity and harming signal propagation.
- Core assumption: Initial network design assumes specific preactivation statistics (mean 0, variance 1) for good signal propagation; deviation degrades optimization.
- Evidence anchors:
  - Preactivation distribution shift is identified as an independent mechanism
  - Preactivation distribution shift may result in more easily measurable pathologies, reducing network expressivity
  - Activation by Interval-wise Dropout suggests preventing such shifts is an active research direction
- Break condition: Normalization layers maintain preactivation statistics close to initial design assumptions, neutralizing this mechanism.

### Mechanism 2
- Claim: Growth of parameter norms creates ill-conditioned loss landscapes that impede plasticity.
- Mechanism: Increasing parameter magnitudes correlate with sharper loss landscapes and differential learning dynamics that destabilize optimization.
- Core assumption: Larger parameters increase output sensitivity to weight changes, making optimization unstable.
- Evidence anchors:
  - Parameter norm growth is identified as an independent mechanism
  - Parameter norm growth creates training instabilities and saturates network components
  - Recovering Plasticity of Neural Networks via Soft Weight Rescaling addresses unbounded weight growth
- Break condition: L2 regularization or explicit norm constraints keep parameter magnitudes bounded, mitigating this mechanism.

### Mechanism 3
- Claim: Large regression target means cause ill-conditioning of feature representations, leading to plasticity loss.
- Mechanism: Networks encode large constant offsets in single-dimensional feature subspaces, creating extreme ill-conditioning that makes fitting new tasks difficult.
- Core assumption: Networks can represent large-mean functions via bias terms, but instead encode offsets in features, causing rank-deficiency in the empirical NTK.
- Evidence anchors:
  - Large regression target means are identified as an independent mechanism
  - Target magnitude alone explains significant plasticity loss in deep reinforcement learning
  - Networks without layer normalization show this ill-conditioning effect, though somewhat mitigated with layer normalization
  - Plasticity Loss in Deep Reinforcement Learning: A Survey implies this is a recognized cause in RL settings
- Break condition: Scale-invariant losses or normalization layers constrain feature norms, neutralizing this mechanism.

## Foundational Learning

- Concept: Empirical Neural Tangent Kernel (eNTK)
  - Why needed here: The eNTK characterizes local optimization dynamics and reveals shared pathologies across different plasticity loss mechanisms; it is used as a diagnostic tool.
  - Quick check question: What does a rank-deficient eNTK indicate about a network's ability to generalize between inputs?

- Concept: Signal propagation and preactivation distributions
  - Why needed here: Good signal propagation depends on preactivation distributions matching design assumptions; deviations cause dead/zombie units and optimization difficulties.
  - Quick check question: How does layer normalization help maintain the conditions needed for good signal propagation?

- Concept: Catastrophic forgetting vs. plasticity loss
  - Why needed here: Plasticity loss is distinct from catastrophic forgetting; it refers to inability to optimize current objective, not forgetting past tasks.
  - Quick check question: In what way does plasticity loss manifest differently from catastrophic forgetting in the experiments?

## Architecture Onboarding

- Component map:
  Layer normalization -> Stabilize preactivation distributions
  Weight decay regularization -> Constrain parameter norm growth
  Loss function design -> Avoid large target mean pathologies
  Optimizer state management -> Prevent stale second-order estimates

- Critical path:
  1. Apply layer normalization before each nonlinearity
  2. Add L2 regularization with coefficient ~1e-5 to 1e-6
  3. For regression tasks with large targets, use scale-invariant losses (e.g., categorical over discretized target range)
  4. Monitor eNTK diagnostics during training to detect early signs of ill-conditioning

- Design tradeoffs:
  - Layer normalization vs. batch normalization: LN is more robust across domains (RL, NLP) but BN can help with unit centering in image tasks
  - L2 strength: Too high slows training; too low allows parameter explosion
  - Scale-invariant losses: Help with large targets but may require discretization and increase output dimensionality

- Failure signatures:
  - Sharp increase in dead or linearized units after task change
  - Growth of maximum Hessian eigenvalue or parameter norm
  - eNTK becoming close to low-rank (diagonal + rank-1)
  - Loss curves plateauing or becoming increasingly flat during later tasks

- First 3 experiments:
  1. Train a CNN on CIFAR-10 with random label resets; compare performance with/without layer normalization and L2.
  2. Train a DQN agent on image classification MDP; compare regression loss vs. categorical distributional loss with layer normalization.
  3. Train an MLP on regression with large target means; visualize feature SVD and eNTK before/after layer normalization.

## Open Questions the Paper Calls Out
None

## Limitations
- The paper identifies three mechanisms but lacks a unified theoretical framework explaining why these specific mechanisms are primary drivers
- Empirical evidence relies heavily on synthetic benchmarks and may not fully capture real-world nonstationary environment complexity
- Effectiveness of layer normalization and weight decay combinations may have undiscovered failure modes in more extreme nonstationarities

## Confidence
- High confidence: Identification of three distinct mechanisms causing plasticity loss is well-supported by theoretical analysis and empirical validation across multiple domains
- Medium confidence: Combining layer normalization and weight decay creates highly robust learning algorithms is supported by experiments but may not generalize to all nonstationary scenarios
- Medium confidence: Mechanisms are "independent" is demonstrated empirically but could benefit from more rigorous mathematical proof of orthogonality

## Next Checks
1. Test the proposed approach on more extreme nonstationarities with rapidly shifting distributions and evaluate whether layer normalization + weight decay maintains plasticity under such conditions
2. Conduct ablation studies on the three mechanisms to quantify relative contribution of each to overall plasticity loss in different network architectures and task types
3. Investigate whether identified mechanisms and mitigation strategies apply to other forms of optimization difficulty beyond plasticity loss, such as catastrophic forgetting or interference in multi-task learning scenarios