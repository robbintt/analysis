---
ver: rpa2
title: Towards Accurate Post-training Quantization for Reparameterized Models
arxiv_id: '2402.16121'
source_url: https://arxiv.org/abs/2402.16121
tags:
- quantization
- outliers
- accuracy
- output
- quantized
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Reparameterization improves inference efficiency but creates outlier-induced
  quantization errors. To address this, a new post-training quantization framework
  called RepAPQ was developed.
---

# Towards Accurate Post-training Quantization for Reparameterized Models

## Quick Facts
- arXiv ID: 2402.16121
- Source URL: https://arxiv.org/abs/2402.16121
- Reference count: 40
- Key outcome: RepAPQ achieves ~1% better accuracy for 8-bit and ~2% for 6-bit quantization on various models and tasks

## Executive Summary
This paper addresses the challenge of post-training quantization for reparameterized models, where outliers in activation maps cause significant quantization errors. The authors propose RepAPQ, a framework that uses Mean Absolute Error (MAE) instead of Mean Squared Error (MSE) to reduce outlier impact, introduces Quantization Protecting Reparameterization (QPRep) with an affine layer to preserve outliers, and employs Across-block Calibration (ABC) to improve gradient estimation. Experiments show RepAPQ outperforms prior methods by approximately 1% for 8-bit and 2% for 6-bit quantization on models like RepVGG, MobileOne, and ConvNeXt across classification and detection tasks.

## Method Summary
RepAPQ is a post-training quantization framework designed specifically for reparameterized models. It replaces the traditional MSE-based optimization with MAE to reduce the disproportionate influence of outliers on quantization parameters. The framework introduces QPRep, which adds an affine layer after reparameterized convolution layers to amplify features and preserve outlier values during quantization. Additionally, ABC leverages stage output as supervision to address gradient issues introduced by MAE and enhance interlayer correlation. The method is trained for 1000 iterations using Adam optimizer with CosineDecay learning rate scheduler on calibration datasets of 1024 ImageNet samples for classification and 512 COCO samples for detection.

## Key Results
- RepAPQ achieves approximately 1% better accuracy than prior methods for 8-bit quantization
- RepAPQ achieves approximately 2% better accuracy than prior methods for 6-bit quantization
- The framework demonstrates consistent improvements across multiple models including RepVGG, MobileOne, and ConvNeXt on both classification and detection tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using MAE instead of MSE for quantization reduces the impact of outliers on quantization parameter selection.
- Mechanism: MSE squares the error, so outliers have a disproportionately large effect. MAE uses absolute error, which treats all errors proportionally and thus outliers have less impact.
- Core assumption: The quantization distortion function with MAE behaves like a Gaussian distribution with scalar error for outliers, as proven in Proposition 2.
- Evidence anchors:
  - [abstract] "we utilize Mean Absolute Error (MAE) to mitigate the influence of outliers on quantization parameters"
  - [section III-B] "In regression problems, Mean Squared Error (MSE) is not suitable for data with outliers that are not crucial for the results"
  - [section V-D] "By choosing p smaller, we can preserve the effect of outliers. In our papers, we utilize the Mean Absolute Error (MAE) instead of the MSE"
- Break condition: If the outlier distribution is not well-modeled as Gaussian with scalar errors, or if the optimal quantization clipping values differ significantly between MAE and MSE, this mechanism may not hold.

### Mechanism 2
- Claim: The affine layer in QPRep amplifies features to better accommodate outliers and accelerates convergence.
- Mechanism: The affine layer with learnable scaling parameter η can amplify the output of reparameterized convolution layers. For channels with outliers, η learns to be larger, bringing outliers closer to their original values. This helps preserve accuracy during quantization.
- Core assumption: The gradient of η is relative to the output of the reparameterized convolution layers, which may have outliers. A larger η amplifies the output, bringing outliers closer to their original values.
- Evidence anchors:
  - [abstract] "The affine layer accelerates convergence and amplifies the output of the convolution to better accommodate samples with outliers"
  - [section IV-B] "To account for samples with outliers, we introduce a new degree of freedom to amplify the output of each layer, bringing it closer to the original value of the outliers"
  - [section V-D] "QPRep helps amplify the features and capture the outliers"
- Break condition: If the affine layer cannot learn to effectively amplify the outliers without causing other issues, or if the gradient of η is not sufficiently large for channels with outliers, this mechanism may fail.

### Mechanism 3
- Claim: ABC uses stage output as supervision to address gradient issues from MAE and enhance interlayer correlation.
- Mechanism: ABC minimizes the discrepancy between the output of each block and its estimate, as well as the discrepancy between the output of the entire stage and its estimate. This incorporates information from subsequent blocks to provide a more accurate gradient.
- Core assumption: Using the stage output as supervision can help overcome the element-wise gradient limitations of MAE and improve the accuracy of the quantized model.
- Evidence anchors:
  - [abstract] "Additionally, Across-block Calibration (ABC) leverages the measurement of stage output as supervision to address the gradient problem introduced by MAE and enhance the interlayer correlation with quantization parameters"
  - [section IV-C] "In the optimization process, despite the outlier issue, we continue to utilize MAE as the measurement of the block output. Specifically, for the k-th block within a stage consisting of m blocks, our objective is to minimize the discrepancy between the output of the k-th block and its estimate, as well as the discrepancy between the output of the entire stage and its estimate"
  - [section V-D] "Incorporating stage knowledge is beneficial for most networks"
- Break condition: If the stage output is not a reliable indicator of the block output, or if the additional complexity of ABC outweighs its benefits, this mechanism may not be effective.

## Foundational Learning

- Concept: Gaussian Mixture Model for outlier distribution
  - Why needed here: The paper uses a Gaussian mixture model to analyze the distribution of outliers in the activation maps. This is crucial for understanding the behavior of outliers and designing quantization strategies.
  - Quick check question: What is the form of the Gaussian mixture model used to represent activation maps with outliers?

- Concept: Mean Squared Error vs Mean Absolute Error
  - Why needed here: The paper replaces MSE with MAE as the distortion metric for quantization. Understanding the differences between these metrics is essential for grasping the rationale behind this choice.
  - Quick check question: How does the gradient of MAE differ from the gradient of MSE, and how does this affect the quantization process?

- Concept: Affine layer for feature amplification
  - Why needed here: The paper introduces an affine layer after the reparameterized convolution layers to amplify features and better accommodate outliers. Knowledge of how affine layers work and how they can be used for feature scaling is important.
  - Quick check question: How does the affine layer with learnable scaling parameter help amplify features and preserve the impact of outliers during quantization?

## Architecture Onboarding

- Component map: Reparameterized convolution layers -> Affine layer (QPRep) -> Batch normalization layers -> Activation quantization (BatchQuant) -> Stage output distillation (ABC)
- Critical path: Quantization protecting reparameterization (QPRep) -> Across-block calibration (ABC) -> Weight and activation quantization
- Design tradeoffs: Using MAE instead of MSE reduces outlier impact but may introduce more quantization noise. QPRep helps preserve outliers but adds complexity. ABC improves gradient accuracy but requires additional computation.
- Failure signatures: Significant accuracy degradation, outliers not properly handled, gradients not accurate enough, stage output not representative of block output.
- First 3 experiments:
  1. Compare quantization results using MSE vs MAE on a reparameterized model with known outliers.
  2. Evaluate the impact of the affine layer in QPRep on preserving outlier features during quantization.
  3. Assess the effectiveness of ABC in improving gradient accuracy and overall quantization performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can outliers in reparameterized depth-wise convolutions be effectively handled for lower-bit quantization?
- Basis in paper: [inferred] The paper notes that their method may not perform well on reparameterized depth-wise convolutions for lower-bit quantization and suggests exploring this as a promising direction for future research.
- Why unresolved: Depth-wise convolutions have different computational characteristics compared to standard convolutions, and the existing outlier handling techniques might not directly apply.
- What evidence would resolve it: Empirical results showing improved quantization accuracy for reparameterized depth-wise convolutions using a modified or new outlier handling technique.

### Open Question 2
- Question: Can the proposed RepAPQ framework be extended to more complex network architectures beyond VGG-like models?
- Basis in paper: [inferred] The paper focuses on VGG-like models and suggests exploring the application of their method in more complex network architectures as a promising direction for future research.
- Why unresolved: Complex architectures might have different reparameterization strategies, layer connections, and activation patterns that could affect the effectiveness of the proposed outlier handling and calibration techniques.
- What evidence would resolve it: Comparative experiments demonstrating the effectiveness of RepAPQ on a diverse set of complex network architectures, including residual networks, transformers, and other modern designs.

### Open Question 3
- Question: How does the choice of batch normalization technique (post-add vs. pre-add) impact the quantization performance of reparameterized models?
- Basis in paper: [explicit] The paper discusses the trade-off between post-add and pre-add batch normalization techniques and their impact on accuracy and outlier handling during quantization.
- Why unresolved: The paper suggests that post-add batch normalization is less compatible with the quantization process, resulting in larger accuracy degradation compared to pre-add. However, the underlying reasons and potential solutions are not fully explored.
- What evidence would resolve it: A comprehensive study comparing the quantization performance of reparameterized models using different batch normalization techniques, along with an analysis of the underlying mechanisms and potential modifications to improve post-add batch normalization compatibility.

## Limitations
- The framework may not perform well on reparameterized depth-wise convolutions for lower-bit quantization
- The paper focuses primarily on classification and object detection tasks, with limited exploration of other domains
- Scalability to very large models (>1B parameters) remains unexplored

## Confidence

**High Confidence**: The experimental results showing 1% improvement for 8-bit and 2% for 6-bit quantization across multiple models are directly measurable and reproducible. The mechanism of using MAE instead of MSE to reduce outlier impact is mathematically sound and well-validated.

**Medium Confidence**: The effectiveness of QPRep's affine layer in amplifying features and preserving outliers relies on assumptions about gradient behavior that, while reasonable, haven't been extensively validated across diverse model architectures. The benefits of ABC in handling gradient issues from MAE are demonstrated but could be architecture-dependent.

**Low Confidence**: The scalability of these approaches to very large models (e.g., >1B parameters) and their performance in non-vision tasks remain largely unexplored.

## Next Checks

1. **Distribution Analysis**: Analyze activation distributions in quantized models to verify that outliers are being properly preserved rather than simply suppressed. Compare histograms of activation values before and after QPRep application.

2. **Ablation Studies**: Systematically remove each component (MAE, QPRep, ABC) to quantify their individual contributions. This should include testing with synthetic outlier patterns to isolate each mechanism's effectiveness.

3. **Generalization Testing**: Apply RepAPQ to models outside the primary evaluation set (ResNet, MobileNet, EfficientNet, ConvNeXt) and to tasks beyond classification and detection (e.g., semantic segmentation) to assess broader applicability.