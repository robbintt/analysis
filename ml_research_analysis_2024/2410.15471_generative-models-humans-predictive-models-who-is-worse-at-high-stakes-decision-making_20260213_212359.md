---
ver: rpa2
title: 'Generative Models, Humans, Predictive Models: Who Is Worse at High-Stakes
  Decision Making?'
arxiv_id: '2410.15471'
source_url: https://arxiv.org/abs/2410.15471
tags:
- race
- compas
- human
- humans
- nemo
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared decisions made by large language models (LMs)
  to those made by humans and a predictive model on recidivism prediction using the
  COMPAS dataset. The research examined how different types of information (including
  race, photos, and other decision-makers' judgments) and bias mitigation techniques
  affect LM decisions.
---

# Generative Models, Humans, Predictive Models: Who Is Worse at High-Stakes Decision Making?

## Quick Facts
- arXiv ID: 2410.15471
- Source URL: https://arxiv.org/abs/2410.15471
- Reference count: 40
- Large language models are not more accurate than humans or predictive models at recidivism prediction, though they align more with human judgments

## Executive Summary
This study compares decisions made by large language models (LMs), humans, and a predictive model on recidivism prediction using the COMPAS dataset. The research examines how different types of information and bias mitigation techniques affect LM decisions. The primary finding is that LMs alone are not more accurate than humans or the COMPAS model at predicting recidivism, though their decisions align more closely with human judgments than with the predictive model. The study also reveals that adding photos to defendant descriptions surprisingly improves LM accuracy, even when photos shouldn't logically affect the decision, and that bias mitigation techniques have variable effects across different LM models.

## Method Summary
The study uses the COMPAS dataset (1,000 defendants) combined with human judgment data (20 annotations per defendant) and Chicago Face Database photos. Three LMs (GPT 4o, Llama 3.2 90B, Mistral NeMo) are tested using zero-shot prompting with temperature 0. The experiments test various conditions: baseline accuracy, in-context learning with human judgments and COMPAS scores, photo incorporation, and bias mitigation through anti-discrimination prompting. Performance is measured through accuracy, proportion of positive outcomes, agreement with human predictions and COMPAS model, and refusal rates.

## Key Results
- LMs are not more accurate than humans or COMPAS model at predicting recidivism
- LMs show higher agreement with human decisions than with COMPAS model predictions
- Adding photo information improves LM accuracy even when photos shouldn't affect the decision
- Bias mitigation techniques have variable and sometimes counterproductive effects across different LM models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LMs show higher agreement with human decisions than with COMPAS model predictions on recidivism prediction
- Mechanism: When LMs receive in-context information about human judgments or COMPAS scores, they adjust their predictions to align more closely with the provided information, with stronger alignment to human judgments
- Core assumption: LMs can effectively use in-context learning to incorporate external decision information and adjust their predictions accordingly
- Evidence anchors:
  - [abstract] "the LMs not exclusively in terms of accuracy, but also in terms of agreement with (imperfect, noisy, and sometimes biased) human predictions or existing predictive models"
  - [section 5.2] "The first observation is that the predictions of the LM models agree much more with human predictions than with COMPAS Th. predictions. The agreement with humans ranges between 0.83 and 0.87"
  - [corpus] Weak evidence - only 5 related papers found, none directly addressing human-LM agreement mechanisms

### Mechanism 2
- Claim: Adding photo information to defendant descriptions changes LM predictions, even when photos shouldn't logically affect the decision
- Mechanism: The presence of images triggers different processing pathways in multimodal LMs, causing them to alter their decision-making process regardless of image content
- Core assumption: Multimodal LMs process visual information differently than text, and the mere presence of images can influence their decision-making framework
- Evidence anchors:
  - [section 5.3] "Incorporating image features, unsettlingly, improve the accuracy of the models. However, we believe the root cause is not that the models are successfully leveraging the image information, but that the LM operates on a different 'regime' when images are provided"
  - [section 5.3] "we find that this does not translate to all the LMs we studied"
  - [corpus] Weak evidence - related papers focus on general LM decision-making but not specifically on multimodal regime changes

### Mechanism 3
- Claim: Bias mitigation techniques like anti-discrimination prompting have variable and sometimes counterproductive effects across different LM models
- Mechanism: Different LMs respond differently to the same bias mitigation prompts due to variations in their training, safety mechanisms, and model architectures
- Core assumption: LMs have heterogeneous responses to the same prompting techniques due to differences in their underlying architectures and training approaches
- Evidence anchors:
  - [section 5.4] "A recently proposed anti-discrimination prompting technique [46] can have unintended effects such as a catastrophic decrease in the number of predicted positives"
  - [section 5.4] "The results are, unfortunately, quite variable...results are variable. While on GPT accuracy mostly improves or remains neutral on all groups and setups, Mistral NeMo clearly degrades"
  - [section 5.4] "their use is therefore not straightforward. Practitioners should carefully evaluate these mitigations on the specific models and setups they plan to use"
  - [corpus] Moderate evidence - several related papers discuss bias in LMs but not specifically on mitigation technique generalizability

## Foundational Learning

- Concept: In-context learning and few-shot prompting
  - Why needed here: The study uses in-context learning to provide LMs with additional information about human judgments and COMPAS scores to see how this affects their predictions
  - Quick check question: How does providing examples or additional context in the prompt window influence an LM's output without fine-tuning?

- Concept: Multimodal model architecture
  - Why needed here: The study tests how LMs respond when given photos alongside text descriptions, requiring understanding of how multimodal models process different data types
  - Quick check question: What are the key architectural differences between text-only and multimodal language models?

- Concept: Fairness metrics and bias evaluation
  - Why needed here: The study evaluates both accuracy and fairness across different racial groups, requiring understanding of metrics like proportion of positive outcomes and agreement rates
  - Quick check question: What are the differences between accuracy, fairness gaps, and agreement metrics when evaluating model performance?

## Architecture Onboarding

- Component map: Prompt engineering -> LM inference (GPT 4o, Llama 3.2 90B, Mistral NeMo) -> Response parsing -> Metric calculation -> Analysis
- Critical path: Prompt construction -> LM inference -> Response parsing -> Metric calculation -> Analysis
- Design tradeoffs: Using zero-shot prompting avoids bias from training examples but may limit performance compared to fine-tuned approaches; multimodal experiments add complexity but provide insights into model behavior
- Failure signatures: High refusal rates indicate safety guardrails triggering; inconsistent accuracy across racial groups indicates bias; agreement rates significantly different from baseline indicate prompt issues
- First 3 experiments:
  1. Run baseline experiments with reference prompts to establish accuracy and bias without additional information
  2. Implement in-context learning experiments with human judgment information to test agreement mechanisms
  3. Add multimodal experiments with placeholder photos to test image processing effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does incorporating defendant race information improve LM accuracy for recidivism prediction across all racial groups?
- Basis in paper: [explicit] The paper found race information significantly altered LM performance, with different effects across racial groups - improving accuracy for some groups while reducing it for others.
- Why unresolved: The paper showed mixed results where race helped GPT and Llama on Hispanic group but decreased Mistral's accuracy, and GPT's accuracy on Black group didn't improve despite including race.
- What evidence would resolve it: Systematic experiments testing race information inclusion across multiple LM models and racial groups with larger sample sizes would clarify if race consistently improves accuracy for any specific groups.

### Open Question 2
- Question: How do different LM models respond to bias mitigation techniques like anti-discrimination prompting?
- Basis in paper: [explicit] The paper found variable effects - while GPT4o sometimes improved, Llama3.2 accuracy was consistently reduced, and Mistral showed mixed results with accuracy degrading on Black group.
- Why unresolved: The paper only tested one specific anti-discrimination prompting technique across three models, showing inconsistent results that suggest model-specific peculiarities.
- What evidence would resolve it: Testing multiple bias mitigation techniques across a broader range of LM models and settings would reveal whether these effects are model-specific or technique-specific.

### Open Question 3
- Question: What causes LM decisions to change when photos are added, even when photos shouldn't affect the decision?
- Basis in paper: [explicit] The paper found that adding photos (even placeholder images) significantly changed LM predictions, suggesting the models operate in a different "regime" when images are present.
- Why unresolved: The paper couldn't determine if LMs were using image content or if the mere presence of images triggered different decision-making patterns.
- What evidence would resolve it: Mechanistic interpretability studies examining internal model representations when processing text-only versus text-with-image inputs would reveal whether LMs are actually using image features or simply responding differently to multimodal prompts.

## Limitations

- The study relies on the COMPAS dataset, which has known biases and controversies that may affect the generalizability of findings
- The effectiveness of in-context learning may vary significantly across different decision-making domains beyond recidivism prediction
- The observed improvements when adding photos to descriptions suggest potential confounding factors or artifacts in how multimodal LMs process information

## Confidence

- High confidence in claims about LMs not outperforming humans or COMPAS in accuracy for recidivism prediction
- Medium confidence in claims about LM agreement with human decisions being higher than with COMPAS predictions
- Low confidence in claims about multimodal regime changes, as the underlying mechanism remains unclear

## Next Checks

1. Test the same experimental protocol on a different recidivism dataset or alternative high-stakes decision domain to verify if LM performance patterns hold across different contexts and data sources

2. Conduct ablation studies on the photo experiments to isolate whether visual features, multimodal processing, or other experimental artifacts drive the observed accuracy improvements

3. Implement and test additional bias mitigation techniques beyond the anti-discrimination prompting to establish whether the variable effectiveness is specific to that technique or represents a broader challenge in LM bias mitigation