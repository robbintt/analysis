---
ver: rpa2
title: Evaluating Fine-Tuning Efficiency of Human-Inspired Learning Strategies in
  Medical Question Answering
arxiv_id: '2408.07888'
source_url: https://arxiv.org/abs/2408.07888
tags: []
core_contribution: "This study evaluates the effectiveness of human-inspired learning\
  \ strategies for fine-tuning large language models in medical question answering.\
  \ The authors compare five data ordering strategies\u2014including curriculum learning,\
  \ blocked, and interleaved approaches\u2014across four language models, three datasets,\
  \ and both human- and LLM-generated difficulty labels."
---

# Evaluating Fine-Tuning Efficiency of Human-Inspired Learning Strategies in Medical Question Answering

## Quick Facts
- arXiv ID: 2408.07888
- Source URL: https://arxiv.org/abs/2408.07888
- Reference count: 40
- Primary result: Interleaved strategies achieve maximum 1.81% accuracy gain over random shuffle in fine-tuning LLMs for medical question answering

## Executive Summary
This study evaluates human-inspired learning strategies for fine-tuning large language models on medical question answering tasks. The authors compare five data ordering approaches—including curriculum learning, blocked, and interleaved methods—across four language models, three datasets, and both human- and LLM-generated difficulty labels. While interleaved strategies generally perform best on average, the optimal approach varies significantly across model-dataset combinations. Notably, curriculum learning using LLM-defined difficulty outperforms human-defined difficulty, demonstrating the potential of model-generated labels as a cost-effective alternative for optimizing fine-tuning.

## Method Summary
The study evaluates five human-inspired learning strategies (Blocked, Interleaved, Curriculum, Blocked Curriculum, Interleaved Curriculum) across four language models (TinyLlama 1.1B, Llama 2 7B/13B, Mistral 7B) using QLoRA-based fine-tuning. Three datasets are used: LEK (medical curriculum), MedMCQA, and MedQA. Difficulty labels are generated either by human experts based on test-taker performance or by LLMs using their own knowledge distribution. The evaluation measures accuracy gains over a Random Shuffle baseline, with each strategy-dataset-model combination repeated five times for stability.

## Key Results
- Interleaved strategies achieve maximum accuracy gain of 1.81% and average gain of 1.02% across datasets
- LLM-defined difficulty labels outperform human-defined difficulty in curriculum learning scenarios
- The optimal learning strategy varies significantly across model-dataset combinations, with no single approach dominating universally
- Curriculum Learning with LLM-defined difficulty showed the greatest improvements for MedMCQA in Blocked Curriculum (+0.92) and Interleaved Curriculum (+1.81)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-defined difficulty outperforms human-defined difficulty in curriculum learning.
- Mechanism: The LLM uses its own knowledge distribution to assess question difficulty, which aligns more closely with the model's learning path and error patterns.
- Core assumption: The LLM's self-assessment of difficulty correlates with the actual learning gradient for the model.
- Evidence anchors: [abstract], [section 4.3]
- Break condition: If the LLM's pre-training domain diverges significantly from the target task, its difficulty estimates may misalign with the actual model learning trajectory.

### Mechanism 2
- Claim: Interleaved learning strategies consistently outperform random shuffling across all models.
- Mechanism: By cycling through different categories, interleaved learning prevents catastrophic forgetting and reinforces long-term retention by forcing the model to re-engage with previously seen concepts.
- Core assumption: The interleaving interval is sufficient to induce retrieval practice without overwhelming the model's working memory.
- Evidence anchors: [abstract], [section 4.2]
- Break condition: If the dataset is too small or the categories too similar, the benefit of interleaving diminishes due to insufficient spacing between repetitions.

### Mechanism 3
- Claim: The optimal learning strategy varies significantly across model-dataset combinations.
- Mechanism: Each model's architecture and pre-training distribution create unique inductive biases, so the learning gradient differs; thus, data ordering that benefits one model may hinder another.
- Core assumption: Model architecture and pre-training knowledge fundamentally shape how the model perceives difficulty and category boundaries.
- Evidence anchors: [abstract], [section 4.2]
- Break condition: If all models share similar architectures and pre-training, the variation in optimal strategy would shrink.

## Foundational Learning

- Concept: Supervised fine-tuning of language models
  - Why needed here: The study evaluates fine-tuning strategies, so understanding how supervised fine-tuning works is essential to interpret the results.
  - Quick check question: What is the difference between supervised fine-tuning and parameter-efficient fine-tuning?

- Concept: Curriculum learning and data ordering
  - Why needed here: The paper tests various data ordering strategies; knowing how curriculum learning differs from random ordering is key to grasping the impact.
  - Quick check question: Why might ordering training data from easy to hard help model learning?

- Concept: Difficulty labeling and clustering
  - Why needed here: The study compares human-defined vs. LLM-defined difficulty and clustering-based categories; understanding these labeling methods is critical for interpreting results.
  - Quick check question: How can you automatically generate difficulty labels without human annotation?

## Architecture Onboarding

- Component map: Load data -> Generate labels -> Partition into strategies -> Fine-tune models -> Evaluate accuracy -> Aggregate results
- Critical path: Load data → Generate labels → Partition into strategies → Fine-tune models → Evaluate accuracy → Aggregate results
- Design tradeoffs:
  - Using QLoRA for memory efficiency vs. full fine-tuning for potential gains
  - Repeating data within blocks vs. single pass to preserve curriculum order
  - Five samples per strategy for stability vs. higher computational cost
- Failure signatures:
  - No improvement over random shuffle: strategy ineffective or model too large for small dataset
  - Large variance between runs: insufficient sampling or instability in data ordering
  - Overfitting to training set: model memorizing without generalizing to MedMCQA/MedQA
- First 3 experiments:
  1. Run Random Shuffle baseline for each model to establish baseline accuracy.
  2. Run Curriculum Learning with human-defined difficulty to test traditional curriculum effect.
  3. Run Interleaved Learning with LLM-defined difficulty to test the most consistently beneficial strategy under optimal labeling.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the effectiveness of human-inspired learning strategies vary based on the size of the medical curriculum (breadth of topics covered)?
- Basis in paper: [inferred] The paper notes that their medical dataset focuses solely on graduate-level medical exams, whereas previous work used a broader curriculum spanning secondary to graduate school levels.
- Why unresolved: The study only tested one medical curriculum (LEK dataset).
- What evidence would resolve it: A controlled experiment training on medical curricula of varying breadth while keeping other factors constant.

### Open Question 2
- Question: What is the optimal temporal spacing for interleaved learning strategies in fine-tuning LLMs?
- Basis in paper: [inferred] The authors mention that benefits of Interleaved Learning might become apparent over longer revision intervals.
- Why unresolved: The current study used a fixed interleaving pattern without exploring different temporal spacings.
- What evidence would resolve it: Experiments varying the number of interleaved items and repetition intervals.

### Open Question 3
- Question: How do model-generated difficulty measures compare to human-defined difficulty across different medical specialties?
- Basis in paper: [explicit] The paper found that LLM-defined difficulty measures outperformed human-defined difficulty for curriculum learning.
- Why unresolved: The study only used one medical dataset where human difficulty was based on general test-taker performance.
- What evidence would resolve it: Testing human vs. model difficulty measures on specialty-specific medical datasets.

## Limitations
- The observed accuracy gains (max 1.81%, average 1.02%) are modest, suggesting diminishing returns for data ordering strategies
- The optimal strategy varies significantly across model-dataset combinations, indicating limited generalizability
- The study uses only multiple-choice question answering datasets, not representing the full spectrum of medical language tasks

## Confidence
- High confidence: The finding that LLM-defined difficulty outperforms human-defined difficulty is well-supported by consistent results
- Medium confidence: The general superiority of interleaved strategies is supported, but magnitude varies considerably
- Low confidence: The claim that curriculum learning with LLM-defined difficulty is universally superior, as it only outperformed random shuffle for two of four models

## Next Checks
1. Test whether the superiority of LLM-defined difficulty extends to other medical domains beyond multiple-choice questions
2. Evaluate whether observed strategy variations persist when using parameter-efficient fine-tuning methods instead of QLoRA
3. Investigate whether combining data ordering strategies with other training techniques amplifies or diminishes observed benefits