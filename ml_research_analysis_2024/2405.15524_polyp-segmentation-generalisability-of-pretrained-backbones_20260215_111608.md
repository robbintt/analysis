---
ver: rpa2
title: Polyp Segmentation Generalisability of Pretrained Backbones
arxiv_id: '2405.15524'
source_url: https://arxiv.org/abs/2405.15524
tags:
- backbones
- pretraining
- polyp
- segmentation
- pretrained
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates how different backbone architectures and
  pretraining strategies affect polyp segmentation models' ability to generalise to
  unseen data distributions. The study evaluates 12 models with either ResNet50 or
  ViT-B backbones, pretrained using various self-supervised (MoCo v3, Barlow Twins,
  MAE) and supervised methods, and fine-tuned on Kvasir-SEG before testing on CVC-ClinicDB.
---

# Polyp Segmentation Generalisability of Pretrained Backbones

## Quick Facts
- arXiv ID: 2405.15524
- Source URL: https://arxiv.org/abs/2405.15524
- Reference count: 4
- Primary result: ResNet50 models with MoCo v3 pretraining show better generalization to unseen data distributions than ViT-B models

## Executive Summary
This paper investigates how different backbone architectures and pretraining strategies affect polyp segmentation models' ability to generalise to unseen data distributions. The study evaluates 12 models with either ResNet50 or ViT-B backbones, pretrained using various self-supervised (MoCo v3, Barlow Twins, MAE) and supervised methods, and fine-tuned on Kvasir-SEG before testing on CVC-ClinicDB. While models with ViT-B backbones generally outperform those with ResNet50 backbones on in-distribution test sets, the ResNet50 models demonstrate superior generalisation to the different data distribution in CVC-ClinicDB. The best generalisation was achieved by the ResNet50 model pretrained with MoCo v3 on ImageNet-1k, which significantly improved its ranking when tested on the new dataset. This finding challenges the assumption that larger, more complex ViT-B models with pretraining pipelines automatically provide better generalisation, suggesting that architecture complexity may lead to overfitting on training data distributions.

## Method Summary
The study evaluates 12 polyp segmentation models with either ResNet50 or ViT-B backbones, pretrained using self-supervised methods (MoCo v3, Barlow Twins, MAE) or supervised learning on either ImageNet-1k or Hyperkvasir-unlabelled datasets. These models are fine-tuned on Kvasir-SEG for polyp segmentation using DeepLabV3+ (for ResNet50) or DPT (for ViT-B) decoders, then tested on both in-distribution (Kvasir-SEG) and out-of-distribution (CVC-ClinicDB) datasets to measure generalization performance through mDice, mIoU, mPrecision, and mRecall metrics.

## Key Results
- ResNet50 models with MoCo v3 pretraining on ImageNet-1k achieved the best generalization to CVC-ClinicDB
- ViT-B models generally outperform ResNet50 models on in-distribution test sets
- Models with ResNet50 backbones typically improve their ranking when tested on out-of-distribution data
- Self-supervised pretraining on ImageNet-1k generally provides better generalization than pretraining on Hyperkvasir-unlabelled

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ViT-B backbones achieve higher performance on in-distribution test sets due to their larger model capacity and ability to capture complex visual patterns.
- Mechanism: The increased architectural complexity of ViT-B allows it to learn more nuanced feature representations from the training data, resulting in better segmentation accuracy when evaluated on the same distribution.
- Core assumption: Larger model capacity inherently leads to better performance when the test data distribution matches the training distribution.
- Evidence anchors:
  - [abstract] "models with ViT-B backbones generally outperform those with ResNet50 backbones on in-distribution test sets"
  - [section] "models with ViT-B backbones generally experience a drop in ranking" when tested on CVC-ClinicDB
- Break condition: If the test data distribution differs significantly from training, the model may overfit to training-specific patterns rather than learning generalizable features.

### Mechanism 2
- Claim: ResNet50 backbones demonstrate superior generalization because they are less prone to overfitting on training data distribution.
- Mechanism: The relatively simpler architecture of ResNet50 forces the model to learn more robust, generalizable features rather than memorizing training-specific patterns, making it perform better on data from different distributions.
- Core assumption: Architectural simplicity acts as a regularizer that prevents overfitting to specific data distributions.
- Evidence anchors:
  - [abstract] "the best generalisation was achieved by the ResNet50 model pretrained with MoCo v3 on ImageNet-1k"
  - [section] "models with ResNet50 backbones generally improve their ranking, implying greater generalisability"
- Break condition: If the training data distribution is very different from deployment scenarios, even simpler models may struggle to generalize effectively.

### Mechanism 3
- Claim: Self-supervised pretraining on ImageNet-1k provides better generalization than self-supervised pretraining on Hyperkvasir-unlabelled.
- Mechanism: ImageNet-1k contains a more diverse and extensive set of natural images, allowing models to learn more general visual representations that transfer better to different endoscopic data distributions.
- Core assumption: Larger, more diverse pretraining datasets lead to better feature representations that generalize across domains.
- Evidence anchors:
  - [abstract] "self-supervised pretraining on ImageNet-1k generally provides the best generalisation"
  - [section] "supervised pretraining on ImageNet-1k is generally better than self-supervised pretraining on Hyperkvasir-unlabelled"
- Break condition: If domain-specific pretraining data becomes significantly larger and more diverse, it may surpass general-purpose pretraining.

## Foundational Learning

- Concept: Domain adaptation and generalization
  - Why needed here: Understanding how models perform when shifted from one data distribution (Kvasir-SEG) to another (CVC-ClinicDB) is central to the paper's contribution
  - Quick check question: What key difference between Kvasir-SEG and CVC-ClinicDB might explain the performance differences observed?

- Concept: Self-supervised learning vs supervised learning
  - Why needed here: The paper compares multiple pretraining strategies, requiring understanding of how self-supervised methods learn without explicit labels
  - Quick check question: How does MoCo v3 differ fundamentally from supervised pretraining on ImageNet-1k?

- Concept: Backbone architecture comparison (ViT vs ResNet)
  - Why needed here: The core finding contrasts performance between transformer-based (ViT-B) and convolutional (ResNet50) architectures
  - Quick check question: What architectural feature of ViT-B contributes to its higher capacity compared to ResNet50?

## Architecture Onboarding

- Component map:
  - Encoder backbone (ResNet50 or ViT-B)
  - Pretraining strategy (MoCo v3, Barlow Twins, MAE, supervised, or none)
  - Pretraining dataset (ImageNet-1k or Hyperkvasir-unlabelled)
  - Fine-tuning dataset (Kvasir-SEG)
  - Evaluation dataset (CVC-ClinicDB)
  - Decoder (DeepLabV3+ for ResNet50, DPT for ViT-B)

- Critical path:
  1. Pretrain backbone using chosen algorithm and dataset
  2. Fine-tune on Kvasir-SEG for polyp segmentation
  3. Evaluate on both in-distribution (Kvasir-SEG test) and out-of-distribution (CVC-ClinicDB) datasets
  4. Compare performance metrics (mDice, mIoU, mPrecision, mRecall)

- Design tradeoffs:
  - Model complexity vs generalization capability
  - Pretraining dataset diversity vs domain relevance
  - Computational cost vs performance gains
  - In-distribution accuracy vs out-of-distribution robustness

- Failure signatures:
  - Large performance drop when testing on CVC-ClinicDB vs Kvasir-SEG
  - Increased variance in instance-wise Dice scores
  - Lower maximum Dice score achievement on out-of-distribution data
  - Worsening rank when evaluated on different distributions

- First 3 experiments:
  1. Train ResNet50 and ViT-B models with no pretraining on Kvasir-SEG, evaluate on both Kvasir-SEG and CVC-ClinicDB to establish baseline performance
  2. Compare supervised ImageNet-1k pretraining vs no pretraining for both architectures on generalization performance
  3. Implement MoCo v3 pretraining on ImageNet-1k for both architectures and measure generalization improvement relative to baseline

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific architectural features of ResNet50 backbones enable better generalisation compared to ViT-B backbones in polyp segmentation tasks?
- Basis in paper: [explicit] The authors observe that ResNet50 models generally improve their ranking when tested on CVC-ClinicDB compared to ViT-B models, which typically experience a drop in ranking.
- Why unresolved: The paper notes this phenomenon but does not provide a detailed analysis of which architectural characteristics (such as inductive biases, parameter efficiency, or feature representations) contribute to this improved generalisation.
- What evidence would resolve it: A controlled ablation study comparing specific architectural components between ResNet50 and ViT-B models, or experiments varying individual architectural parameters while keeping pretraining constant, would help identify which features contribute to generalisation differences.

### Open Question 2
- Question: How does the relationship between model complexity and generalisation manifest across different self-supervised pretraining methods?
- Basis in paper: [inferred] The authors suggest that "larger complexity of the models with ViT-B backbones allowing for overfitting on the distribution underlying the training data," but note this "challenges the assumption that the considered pretraining pipelines should help prevent this."
- Why unresolved: The paper demonstrates that ViT-B models overfit more than ResNet50 models, but doesn't investigate whether this relationship holds consistently across different self-supervised pretraining methods or if certain pretraining approaches can mitigate overfitting in more complex architectures.
- What evidence would resolve it: Systematic experiments comparing generalisation gaps across architectures using multiple self-supervised methods (MoCo v3, Barlow Twins, MAE) with varying model complexities, and measuring overfitting patterns, would clarify these relationships.

### Open Question 3
- Question: What are the specific distributional differences between Kvasir-SEG and CVC-ClinicDB that most significantly impact model generalisation?
- Basis in paper: [explicit] The authors note that their results reveal how well models "generalise to data of a somewhat different distribution to the training data, which will likely arise in deployment due to different cameras and demographics of patients."
- Why unresolved: While the paper demonstrates that models perform differently on these two datasets, it doesn't analyse what specific characteristics (image quality, camera properties, patient demographics, polyp appearance variations) drive the generalisation differences.
- What evidence would resolve it: Detailed statistical analysis of the two datasets' properties (e.g., lighting conditions, resolution, anatomical variations, demographic factors) combined with controlled experiments where individual characteristics are varied would identify the key factors affecting generalisation.

## Limitations

- The findings are based on a single out-of-distribution test set (CVC-ClinicDB), limiting generalizability across different data distributions
- Specific hyperparameters and implementation details for pretraining and fine-tuning procedures are not fully specified
- The study does not explore whether observed patterns hold across multiple different endoscopic modalities or medical imaging domains

## Confidence

- **High Confidence**: The observation that ResNet50 models with MoCo v3 pretraining show better generalization to CVC-ClinicDB than ViT-B models is supported by the experimental results and aligns with known principles of overfitting.
- **Medium Confidence**: The claim that self-supervised pretraining on ImageNet-1k generally provides better generalization than on Hyperkvasir-unlabelled is supported by the data but requires validation across more datasets.
- **Low Confidence**: The broader generalization of the finding that "larger, more complex ViT-B models with pretraining pipelines automatically provide better generalisation" to other medical imaging domains is not well-established.

## Next Checks

1. **Cross-dataset validation**: Test the same model architectures on at least two additional polyp segmentation datasets from different medical centers to verify if the generalization patterns hold consistently across multiple data distributions.

2. **Architecture ablation study**: Systematically vary the depth and width of both ResNet and ViT architectures to identify the specific architectural properties that contribute to generalization, isolating model capacity from architectural differences.

3. **Pretraining dataset size analysis**: Compare models pretrained on ImageNet-1k versus progressively larger subsets of Hyperkvasir-unlabelled to determine if dataset diversity or size is the primary driver of generalization improvements.