---
ver: rpa2
title: 'Rethinking Token-wise Feature Caching: Accelerating Diffusion Transformers
  with Dual Feature Caching'
arxiv_id: '2412.18911'
source_url: https://arxiv.org/abs/2412.18911
tags:
- caching
- tokens
- steps
- feature
- methods
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates two fundamental questions about token-wise
  feature caching in Diffusion Transformers (DiTs): whether computing "important"
  tokens in every caching step is necessary, and whether these tokens are truly important.
  Through empirical analysis, the authors discover that computing important tokens
  consistently is not required, and that token selection based on attention scores
  sometimes performs worse than random selection.'
---

# Rethinking Token-wise Feature Caching: Accelerating Diffusion Transformers with Dual Feature Caching

## Quick Facts
- arXiv ID: 2412.18911
- Source URL: https://arxiv.org/abs/2412.18911
- Reference count: 28
- Primary result: Achieves 3.45× acceleration on FLUX.1-dev with minimal quality degradation using Dual Feature Caching

## Executive Summary
This paper challenges fundamental assumptions about token-wise feature caching in Diffusion Transformers (DiTs) by investigating whether computing "important" tokens in every caching step is necessary and whether these tokens are truly important. Through empirical analysis, the authors discover that consistently computing important tokens is not required, and that token selection based on attention scores sometimes performs worse than random selection. These insights lead to the introduction of Dual Feature Caching (DuCa), which alternates aggressive and conservative caching strategies with random token selection, achieving significant speedups across multiple models while maintaining generation quality.

## Method Summary
Dual Feature Caching (DuCa) introduces a novel approach to token-wise feature caching by alternating between aggressive caching (high-ratio acceleration with full feature reuse) and conservative caching (selective token computation to fix cache errors). The method uses random token selection during conservative steps, which the authors find performs better than attention-based selection methods. DuCa is designed to be compatible with FlashAttention, enabling practical latency improvements in real-world deployment scenarios.

## Key Results
- Achieves 3.45× acceleration on FLUX.1-dev with minimal quality degradation
- Achieves 2.50× speedup on both OpenSora and PixArt-α models
- Achieves up to 2.93× speedup in latency through FlashAttention compatibility
- Random token selection outperforms attention-based selection methods in conservative caching steps

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Computing "important" tokens in every caching step is not necessary
- Mechanism: The caching error between aggressive and conservative caching is nearly identical at the first caching step, showing that computing important tokens doesn't provide additional benefit when error hasn't accumulated yet
- Core assumption: Token importance selection based on attention scores, spatial position, or frequency metrics is not reliable for reducing caching error
- Evidence anchors:
  - [abstract] "consistently computing the selected 'important tokens' in all steps is not necessary"
  - [section] "we find that the caching error of the aggressive and conservative caching exhibit a close value at the first caching step (i.e., timestep 27 in Figure 2)"
  - [corpus] Weak evidence - no direct corpus support for this specific claim about first-step caching error equivalence

### Mechanism 2
- Claim: Token selection based on attention scores sometimes performs worse than random selection
- Mechanism: Random token selection provides better diversity and avoids selecting tokens with similar semantic information, which is more important than selecting supposedly "important" tokens
- Core assumption: Diversity in token selection matters more than importance-based selection for maintaining generation quality
- Evidence anchors:
  - [abstract] "The selection of the so-called 'important tokens' is often ineffective, and even sometimes shows inferior performance than random selection"
  - [section] "selecting a subset of tokens that has the smallest similarity to other tokens is the only solution to beat random token selection"
  - [corpus] Weak evidence - no direct corpus support for this specific claim about random selection outperforming attention-based methods

### Mechanism 3
- Claim: Dual feature caching alternates aggressive and conservative strategies to balance efficiency and quality
- Mechanism: The method initializes with full computation, performs one-step aggressive caching for high-ratio acceleration, followed by one-step conservative caching to fix cache error, then repeats this cycle
- Core assumption: Aggressive caching can be used safely in early caching steps when error hasn't accumulated, while conservative caching is needed to correct errors before they propagate
- Evidence anchors:
  - [abstract] "DuCa, which performs aggressive caching strategy and conservative caching strategy iteratively"
  - [section] "In each caching cycle, DuCa initializes the cache by performing the full computation in the freshing timestep. Then, it performs the one-step aggressive caching for high-ratio acceleration, followed by one-step conservative caching to fix the cache error"
  - [corpus] Weak evidence - no direct corpus support for this specific dual caching alternating strategy

## Foundational Learning

- Concept: Diffusion Models and the denoising process
  - Why needed here: Understanding how DiTs generate images through iterative denoising is crucial for understanding why feature caching works and what errors it introduces
  - Quick check question: What is the mathematical formulation of the reverse process in diffusion models, and how does it relate to the denoising network ϵθ?

- Concept: Transformer architecture and attention mechanisms
  - Why needed here: DiTs use transformer blocks with self-attention, cross-attention, and MLP layers, and understanding these components is essential for implementing feature caching at the token level
  - Quick check question: How do self-attention and cross-attention layers process token sequences in DiTs, and what role do they play in feature caching?

- Concept: Feature caching in neural networks
  - Why needed here: The paper builds on existing feature caching concepts but introduces novel approaches, so understanding the basics of feature caching is essential
  - Quick check question: What is the fundamental principle behind feature caching, and how does it differ between aggressive and conservative approaches?

## Architecture Onboarding

- Component map: DiT model -> Caching mechanism (aggressive/conservative) -> Token selection logic -> FlashAttention compatibility layer
- Critical path: Token computation → Feature caching → Cache reuse → Generation output
- Design tradeoffs: Balancing acceleration ratio against generation quality, choosing between attention-based vs random token selection, compatibility with FlashAttention vs attention score computation
- Failure signatures: Quality degradation in generated images, increased latency, memory overhead issues, incompatibility with efficient attention implementations
- First 3 experiments:
  1. Compare caching error between aggressive and conservative caching at the first caching step to verify the core observation
  2. Test different token selection methods (attention-based, K-norm, V-norm, random, similarity-based) to identify the best performer
  3. Implement and evaluate the dual caching strategy with different cycle lengths and cache ratios to find optimal configuration

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Is the effectiveness of random token selection in DuCa due to its diversity-promoting properties, or are there other factors at play?
- Basis in paper: [explicit] The paper demonstrates that random token selection outperforms other methods based on specific metrics and maintains compatibility with efficient attention methods.
- Why unresolved: While the paper argues for the importance of diversity, it doesn't definitively prove that diversity is the sole reason for the effectiveness of random selection.
- What evidence would resolve it: Controlled experiments comparing random selection to other methods that explicitly prioritize diversity, such as selecting tokens with the lowest similarity to each other, would help isolate the impact of diversity.

### Open Question 2
- Question: Can the cycle length and cache ratio parameters in DuCa be dynamically adjusted during inference based on the characteristics of the input or the current denoising step?
- Basis in paper: [inferred] The paper conducts ablation studies on cycle length and cache ratio but uses fixed values for all experiments. The optimal values might vary depending on the specific model or input.
- Why unresolved: The paper doesn't explore the possibility of adaptive parameter selection, which could potentially further improve the efficiency and quality of DuCa.
- What evidence would resolve it: Experiments comparing fixed and adaptive parameter selection, using metrics like generation quality and computational cost, would demonstrate the potential benefits of dynamic adjustment.

### Open Question 3
- Question: How does DuCa's performance compare to other acceleration methods when applied to different types of diffusion models, such as those with different architectures or trained on different datasets?
- Basis in paper: [inferred] The paper evaluates DuCa on several diffusion models but doesn't explore its generalization to a wider range of models or tasks.
- Why unresolved: The effectiveness of DuCa might depend on the specific characteristics of the diffusion model, and it's unclear how well it would perform on models not included in the experiments.
- What evidence would resolve it: Extensive experiments applying DuCa to a diverse set of diffusion models, including those with different architectures, training datasets, and tasks, would provide insights into its generalizability and robustness.

## Limitations

- The paper's claims about token importance and random selection superiority are supported primarily by internal experimental evidence rather than external validation
- The effectiveness of the dual caching strategy depends on precise timing and error accumulation patterns that may vary significantly with different diffusion models
- Quality degradation metrics are reported as "minimal" without detailed breakdowns across different image characteristics or failure modes

## Confidence

**High Confidence**: The technical implementation of alternating aggressive and conservative caching strategies is well-defined and reproducible. The reported speedups and latency improvements are specific and measurable, though independent verification is needed.

**Medium Confidence**: The empirical observation that early-step caching errors are similar between aggressive and conservative approaches is supported by presented data, but the broader claim about token importance may not generalize. The random selection outperforming attention-based methods is counterintuitive and requires more extensive validation.

**Low Confidence**: The claim that random selection is generally superior to attention-based selection for token importance requires substantial additional evidence, particularly for different model types, sampling strategies, and generation tasks beyond the tested ones.

## Next Checks

1. **Cross-model Generalization Test**: Validate DuCa's performance on additional diffusion models (e.g., Stable Diffusion, Imagen) and different sampling methods (DDIM, Euler) to assess whether the token importance observations and random selection superiority hold across architectures and configurations.

2. **Error Propagation Analysis**: Conduct systematic ablation studies tracking how caching errors propagate through timesteps, particularly comparing aggressive vs conservative caching beyond the first caching step, to verify the claimed equivalence in early-stage error accumulation.

3. **Token Selection Ablation**: Implement and test a comprehensive set of token selection methods (attention-based, K-norm, V-norm, random, similarity-based, and hybrid approaches) across multiple datasets and quality metrics to independently verify that random selection consistently outperforms or matches attention-based methods.