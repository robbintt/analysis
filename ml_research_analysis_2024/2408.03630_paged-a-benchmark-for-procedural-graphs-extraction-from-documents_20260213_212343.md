---
ver: rpa2
title: 'PAGED: A Benchmark for Procedural Graphs Extraction from Documents'
arxiv_id: '2408.03630'
source_url: https://arxiv.org/abs/2408.03630
tags:
- procedural
- graphs
- actions
- llms
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper proposes PAGED, a new benchmark for automatic extraction
  of procedural graphs from documents. The core method is constructing a large high-quality
  dataset using a three-stage pipeline: decomposing graphs into natural language fragments,
  grouping and ordering fragments into sub-procedures, and aggregating and smoothing
  fragments into coherent documents.'
---

# PAGED: A Benchmark for Procedural Graphs Extraction from Documents

## Quick Facts
- arXiv ID: 2408.03630
- Source URL: https://arxiv.org/abs/2408.03630
- Reference count: 28
- Primary result: Existing state-of-the-art methods struggle with non-sequential actions and complex logic in procedural graph extraction tasks

## Executive Summary
The paper introduces PAGED, a new benchmark for automatic extraction of procedural graphs from documents. The core contribution is a large high-quality dataset constructed through a three-stage pipeline that decomposes procedural graphs into natural language fragments, groups and orders them into sub-procedures, and aggregates them into coherent documents. The authors demonstrate that existing methods perform poorly on this task, particularly in handling non-sequential actions and complex logical structures, and propose enhanced LLMs with a self-refine strategy that shows significant improvements in identifying sequential actions and constraints while still facing challenges with gateway logic.

## Method Summary
The authors construct a large high-quality dataset using a three-stage pipeline: decomposing procedural graphs into natural language fragments using breadth-first search and hand-written templates, grouping fragments into sub-procedures using a RoBERTa-based boundary identification model, and aggregating fragments into coherent documents with smoothing. They evaluate five state-of-the-art baselines and three advanced LLMs (Flan-T5, ChatGPT, Llama2) with few-shot in-context learning and supervised fine-tuning, enhanced by a self-refine strategy with condition and parallel verifiers. The evaluation uses automatic metrics (FINE, ESA) and human evaluation criteria to assess performance.

## Key Results
- Existing state-of-the-art methods struggle with non-sequential actions and complex logic in procedural graph extraction
- LLMs enhanced with self-refine strategy show advantages in identifying sequential actions and constraints
- The self-refine strategy improves LLMs' performance in reasoning about gateways and non-sequential actions
- Performance remains challenging for building logical structures and handling complex gateway scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing graphs into natural language fragments enables better alignment between procedural graph structures and document text.
- Mechanism: The pipeline breaks down complex procedural graphs into minimal meaningful units using breadth-first search and hand-written templates, preserving sequential execution relations while making the transformation tractable for language models.
- Core assumption: Procedural graph elements can be mapped to natural language spans without losing structural information.
- Evidence anchors: [section]: "We define a vocabulary with nineteen units, each of which consists of actions, gateways or constraints connected by flow"; [section]: "We also design nineteen hand-written templates for the vocabulary"
- Break condition: If the graph decomposition loses critical sequential or conditional relationships between actions, the transformation will fail to preserve the original procedural logic.

### Mechanism 2
- Claim: Grouping fragments into sub-procedures using boundary identification improves document coherence and readability.
- Mechanism: A pre-trained RoBERTa-large model with classifier layer predicts whether a fragment is the end of a group, organizing procedural information into logical sub-procedures that match human expression patterns.
- Core assumption: Natural language procedural documents naturally organize information into sub-procedures that can be learned from existing corpora.
- Evidence anchors: [section]: "we employ a simple boundary identification model, which employs the RoBERTa-large model (Liu et al., 2019) with a classifier layer, to predict whether a fragment is the end of a group"; [section]: "We train it on the WikiHow corpus Bolotova et al. (2023), which consists of procedural documents collected from the WikiHow website and format marks indicate different sub-procedures"
- Break condition: If the boundary identification model fails to learn meaningful sub-procedure boundaries, the resulting documents will lack logical organization and readability.

### Mechanism 3
- Claim: The self-refine strategy with condition and parallel verifiers enables LLMs to reason about non-sequential actions and gateway logic.
- Mechanism: Two verification systems examine extracted procedural graphs - condition verifier detects conflicts to determine exclusive vs inclusive gateways, parallel verifier uses object analysis to identify parallel actions, providing targeted feedback for iterative refinement.
- Core assumption: LLMs can improve their logical reasoning through iterative feedback when given explicit verification criteria.
- Evidence anchors: [section]: "we design a self-refine strategy to help LLMs gain logic reasoning ability among actions from iterative feedback and refinement"; [section]: "we suppose if the conditions hold conflict, the gateway should be the exclusive one; otherwise, it should be the inclusive one"
- Break condition: If the verification criteria are too simplistic or miss edge cases, the feedback will not effectively guide the LLM toward correct logical structures.

## Foundational Learning

- Concept: Breadth-first search traversal of procedural graphs
  - Why needed here: Ensures systematic decomposition of graph elements while preserving execution order relationships
  - Quick check question: What traversal method would you use to ensure that gateway pairs are processed together when decomposing a procedural graph?

- Concept: Natural language inference for conflict detection
  - Why needed here: Determines whether conditions on exclusive vs inclusive gateways conflict, enabling correct gateway classification
  - Quick check question: How would you determine if "need dishes" and "need drinks" conditions represent conflicting or compatible requirements?

- Concept: Semantic parsing for object extraction
  - Why needed here: Identifies objects associated with actions to determine whether actions can be performed in parallel
  - Quick check question: What semantic parsing approach would you use to extract "the profile" as the object from "create the profile" and "send the profile"?

## Architecture Onboarding

- Component map: Graph → Decomposition → Grouping → Ordering → Aggregation → Refinement → Evaluation
- Critical path: Graph Decomposition Engine → Boundary Identification Module → Fragment Ordering System → Aggregation & Smoothing Pipeline → Self-Refine Strategy → Evaluation Framework
- Design tradeoffs:
  - Hand-written templates vs learned transformations: Templates ensure consistency but lack flexibility
  - Pre-trained vs fine-tuned models: Pre-trained models provide generalization, fine-tuning improves task-specific performance
  - Single vs iterative refinement: Iterative refinement improves accuracy but increases computational cost
- Failure signatures:
  - Low FINE scores indicate omission or hallucination in generated documents
  - Poor gateway F1 scores suggest failure in logical structure organization
  - Inconsistent human evaluation scores reveal quality issues in readability or accuracy
- First 3 experiments:
  1. Test graph decomposition with a simple 5-node procedural graph to verify template mapping
  2. Evaluate boundary identification on WikiHow corpus to ensure sub-procedure detection works
  3. Run self-refine strategy on a small procedural graph with known gateway conflicts to verify feedback loop

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can procedural knowledge be effectively incorporated into the pre-training stage of LLMs to improve their ability to reason about non-sequential actions in procedural graphs?
- Basis in paper: [explicit] The paper discusses the challenge LLMs face in organizing non-sequential actions and suggests incorporating procedural knowledge during pre-training.
- Why unresolved: The paper acknowledges this as a direction for future work but does not provide a concrete method or implementation for incorporating procedural knowledge during pre-training.
- What evidence would resolve it: Developing and testing a pre-training approach that explicitly incorporates procedural knowledge, then evaluating its impact on LLM performance in extracting and reasoning about non-sequential actions in procedural graphs.

### Open Question 2
- Question: What additional factors beyond accuracy are limiting the practical usage of automatic procedural graph extraction systems in real-world scenarios?
- Basis in paper: [explicit] The paper mentions investigating factors beyond accuracy that limit practical usage but does not specify what these factors might be.
- Why unresolved: The paper identifies this as an area for future investigation but does not explore or hypothesize about potential limiting factors beyond accuracy.
- What evidence would resolve it: Conducting user studies or case studies in real-world applications to identify and quantify factors such as usability, interpretability, or integration challenges that affect the adoption of automatic procedural graph extraction systems.

### Open Question 3
- Question: How can the self-refine strategy be further improved to handle more complex logical structures and edge cases in procedural documents?
- Basis in paper: [explicit] The paper introduces a self-refine strategy to improve LLM performance on gateway extraction but acknowledges room for improvement.
- Why unresolved: While the self-refine strategy shows promise, the paper does not explore its limitations or potential enhancements for handling more complex scenarios.
- What evidence would resolve it: Developing and testing extensions to the self-refine strategy, such as incorporating more sophisticated logical reasoning modules or expanding the feedback mechanisms, and evaluating their impact on handling complex logical structures in procedural documents.

### Open Question 4
- Question: How does the quality and diversity of the training data impact the performance of LLMs in extracting procedural graphs from documents?
- Basis in paper: [inferred] The paper emphasizes the importance of high-quality data and shows improvements with larger, more diverse datasets, but does not systematically study the impact of data quality and diversity.
- Why unresolved: The paper demonstrates the benefits of a larger dataset but does not explore how different characteristics of the training data (e.g., domain specificity, complexity, or annotation quality) affect LLM performance.
- What evidence would resolve it: Conducting controlled experiments varying the quality, diversity, and characteristics of training data, and measuring the corresponding impact on LLM performance in extracting procedural graphs from documents across different domains and complexity levels.

### Open Question 5
- Question: Can the proposed three-stage pipeline for constructing procedural document-graph pairs be adapted or improved to generate more diverse or domain-specific procedural documents?
- Basis in paper: [explicit] The paper describes a three-stage pipeline for generating procedural documents but does not explore its adaptability or potential improvements for generating more diverse or domain-specific content.
- Why unresolved: While the pipeline is shown to be effective, the paper does not investigate its flexibility or potential modifications to generate procedural documents tailored to specific domains or use cases.
- What evidence would resolve it: Adapting the three-stage pipeline to incorporate domain-specific templates, language models, or constraints, and evaluating the quality and diversity of the generated procedural documents in various domains or application scenarios.

## Limitations

- Template dependency may limit generalization to novel procedural graph structures or domains
- Verification system simplicity may not capture full complexity of procedural logic
- Dataset construction bias may not reflect naturally occurring procedural text patterns

## Confidence

- Template-based decomposition: Medium - ensures consistency but may limit flexibility for edge cases
- Verification system effectiveness: Low - simple heuristics may be insufficient for complex logical reasoning
- Dataset representativeness: Medium - controlled construction process but unknown alignment with natural language patterns

## Next Checks

1. **Cross-Domain Generalization Test**: Evaluate the pipeline on procedural graphs from different domains (e.g., medical procedures, cooking recipes, technical instructions) to assess whether the template-based decomposition and boundary identification generalize beyond the original dataset distribution.

2. **Complex Logic Stress Test**: Create synthetic procedural graphs with nested conditional logic, multiple concurrent paths, and ambiguous gateway conditions to systematically stress-test the self-refine strategy's verification systems and identify failure modes in logical reasoning.

3. **Human-Generated vs Generated Document Comparison**: Compare the quality and structural fidelity of documents generated by the pipeline against naturally occurring procedural documents on the same underlying procedural graphs, measuring differences in readability, completeness, and logical accuracy.