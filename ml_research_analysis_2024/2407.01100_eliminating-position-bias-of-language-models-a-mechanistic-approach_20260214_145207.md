---
ver: rpa2
title: 'Eliminating Position Bias of Language Models: A Mechanistic Approach'
arxiv_id: '2407.01100'
source_url: https://arxiv.org/abs/2407.01100
tags:
- position
- bias
- attention
- pine
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the prevalent issue of position bias in large
  language models (LLMs), where models prioritize content based on its position within
  the given context. The authors attribute this bias to two key components of LLMs:
  causal attention and relative positional encodings.'
---

# Eliminating Position Bias of Language Models: A Mechanistic Approach

## Quick Facts
- arXiv ID: 2407.01100
- Source URL: https://arxiv.org/abs/2407.01100
- Reference count: 40
- Key outcome: Training-free approach (PINE) eliminates position bias in LLMs, achieving 8-10 percentage point gains on reasoning tasks and outperforming GPT-4 in some cases

## Executive Summary
This paper addresses position bias in large language models (LLMs), where models favor content based on its position in the context. The authors identify causal attention and relative positional encodings (like RoPE) as the primary sources of this bias. They propose PINE (Position-INvariant inferencE), a training-free approach that modifies causal attention to bidirectional attention between segments and uses attention similarity to determine relative positions. PINE significantly improves performance across multiple tasks including LM-as-a-judge, retrieval-augmented QA, molecule generation, and math reasoning.

## Method Summary
PINE is a training-free, zero-shot approach that eliminates position bias by modifying inference-time behavior. It first makes attention bidirectional between segments while maintaining causal attention within segments. Then it computes attention similarity between segments using attention weights without RoPE involvement (Simtoken = Softmax(QKT/√d)). Based on these similarities, PINE reassigns positions and applies the modified attention mask. This ensures that each segment treats itself as the last segment, with other segments positioned by their similarity rather than their original order.

## Key Results
- Eliminates position bias in LM-as-a-judge tasks, achieving 8-10 percentage point improvements
- Outperforms GPT-4 in reasoning pair evaluation
- Improves retrieval-augmented QA by 9.7% in top-1 retrieval accuracy
- Shows consistent performance across different input orders, with variance decreasing from 16.0% to 0.8% in reasoning tasks

## Why This Works (Mechanism)

### Mechanism 1: Causal Attention Bias
- **Claim:** Causal attention causes language models to favor distant content over nearby content
- **Mechanism:** In causal attention, tokens can only attend to previous tokens. This unidirectional propagation means that distant content is propagated forward through the attention mechanism, giving it disproportionate influence on later tokens
- **Core assumption:** The attention mechanism aggregates information from attended tokens, and this aggregation process preserves the relative influence of distant versus nearby content
- **Evidence anchors:**
  - [abstract] "causal attention generally causes models to favor distant content"
  - [section] "the causal attention propagates the distant content to the nearby content. Therefore, nearby tokens also contain information about distant tokens, potentially making models favor distant content after the aggregation"
  - [corpus] Weak - no direct corpus evidence, but supported by Figure 1 analysis showing yellow area (causal attention bias) is wider at the beginning

### Mechanism 2: RoPE Recency Bias
- **Claim:** Relative positional encodings like RoPE cause language models to exhibit recency bias
- **Mechanism:** RoPE uses sinusoidal functions that decay for larger relative positions, causing attention weights to decay as the distance between tokens increases. This mathematical property creates a recency bias where nearby tokens receive more attention
- **Core assumption:** The decay in RoPE attention weights is significant enough to affect the final output, and the model's decision-making process is sensitive to these weight differences
- **Evidence anchors:**
  - [abstract] "relative positional encodings like RoPE Su et al. (2024) prefer nearby ones based on the analysis of retrieval-augmented question answering (QA)"
  - [section] "RoPE has been shown to have recency bias due to its mathematical long-form weight decay [29, 24]"
  - [corpus] Weak - relies on cited works rather than direct evidence in the paper

### Mechanism 3: Bidirectional Attention with Position Reassignment
- **Claim:** Bidirectional inter-segment attention combined with position reassignment based on attention similarity eliminates position bias
- **Mechanism:** PINE first makes attention bidirectional between segments while keeping intra-segment attention causal. Then it computes similarity scores between segments using attention values, and re-sorts positions based on these similarities. This ensures that each segment treats itself as the last segment and other segments are positioned by their similarity
- **Core assumption:** The attention values computed without RoPE involvement (Simtoken = Softmax(QKT/√d)) are meaningful indicators of segment similarity that can be used for position reassignment
- **Evidence anchors:**
  - [section] "we compute the attentions without RoPE involved: Simtoken = Softmax(QKT/√d), where d is the hidden state dimension. Then, we obtain the similarity between segments by aggregation"
  - [section] "We reassign positions by similarities as shown in the rightmost part of Figure 3"
  - [corpus] Moderate - supported by experimental results showing PINE eliminates position bias in LM-as-a-judge and retrieval-augmented QA

## Foundational Learning

- **Concept:** Transformer attention mechanism
  - Why needed here: Understanding how attention works is fundamental to grasping why causal attention and positional encodings cause bias
  - Quick check question: How does the causal attention mask differ from a bidirectional attention mask in terms of which tokens can attend to which others?

- **Concept:** Positional encoding (specifically RoPE)
  - Why needed here: RoPE's mathematical properties directly cause the recency bias that PINE aims to eliminate
  - Quick check question: Why does RoPE's sinusoidal function create decay for larger relative positions?

- **Concept:** Zero-shot learning and inference-time adaptation
  - Why needed here: PINE is a training-free approach that modifies inference behavior without retraining
  - Quick check question: What are the key differences between zero-shot learning and few-shot learning in the context of language models?

## Architecture Onboarding

- **Component map:** Input segments -> Bidirectional inter-segment attention with causal intra-segment attention -> Attention similarity computation (Simtoken) -> Position reassignment -> Output position-invariant hidden states

- **Critical path:**
  1. Compute attention scores without RoPE
  2. Aggregate attention scores to get segment similarities
  3. Reassign positions based on similarities
  4. Apply bidirectional attention with new positions
  5. Generate position-invariant output

- **Design tradeoffs:**
  - PINE vs. PCW: PINE keeps contextual information by using bidirectional attention, while PCW loses it by masking all inter-segment attention
  - PINE vs. training-based solutions: PINE is training-free but has inference overhead, while training-based solutions require data and computation upfront
  - Re-sorting vs. no re-sorting: Re-sorting eliminates position bias but adds complexity; no re-sorting is simpler but may retain some bias

- **Failure signatures:**
  - If PINE performs worse than vanilla inference, it may indicate that the attention similarity computation is not capturing meaningful relationships
  - If PINE still shows variance across different input orders, it may indicate numerical instability in the softmax operations
  - If PINE causes significant performance drops, it may indicate that the bidirectional attention disrupts learned patterns

- **First 3 experiments:**
  1. Run PINE on a simple LM-as-a-judge task with 2 segments and verify that outputs are identical when segment order is swapped
  2. Compare PINE performance against vanilla inference on a retrieval-augmented QA task with 10 documents
  3. Test PINE with different segment similarity aggregation methods (mean, max, weighted) to see which performs best

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does the bidirectional attention in PINE affect the model's ability to generate coherent and contextually relevant outputs in tasks that require language modeling, such as dialogue generation or creative writing?
- Basis in paper: [explicit] The paper mentions that previous methods like PCW, which use bidirectional attention, suffer from performance drops in tasks requiring language modeling due to OOD operations and loss of contextual information.
- Why unresolved: The paper does not provide empirical evidence on how PINE's bidirectional attention impacts language modeling tasks beyond LM-as-a-judge and retrieval-augmented QA.
- What evidence would resolve it: Testing PINE on a variety of language modeling tasks (e.g., dialogue generation, story continuation, summarization) and comparing its performance to vanilla models and other baseline methods.

### Open Question 2
- Question: How does the similarity computation method used in PINE (based on attention weights) perform when dealing with segments of significantly different lengths or semantic content?
- Basis in paper: [inferred] The paper mentions that similarity is computed using aggregated attention scores and normalized by segment length, but does not explore the robustness of this method to varying segment characteristics.
- Why unresolved: The paper does not provide a detailed analysis of how the similarity computation handles edge cases, such as very long or short segments, or segments with low semantic overlap.
- What evidence would resolve it: Conducting experiments with segments of varying lengths and semantic content, and analyzing the impact on PINE's performance and the accuracy of the similarity computation.

### Open Question 3
- Question: Can PINE be extended to handle more complex input structures, such as hierarchical or nested segments, where position invariance is required at multiple levels?
- Basis in paper: [inferred] The paper focuses on eliminating position bias in flat segments (e.g., retrieved documents, candidate responses) but does not address more complex input structures.
- Why unresolved: The paper does not explore how PINE could be adapted to handle inputs with nested or hierarchical segment structures, where position invariance might be required at different levels of granularity.
- What evidence would resolve it: Developing and testing an extension of PINE that can handle hierarchical or nested input structures, and evaluating its effectiveness in tasks with such inputs (e.g., multi-turn dialogue, nested question answering).

## Limitations
- The effectiveness of PINE may be task-dependent, with modest gains in molecule generation and math reasoning compared to strong performance in LM-as-a-judge and retrieval-augmented QA
- The computational overhead of bidirectional inter-segment attention may be prohibitive for very long contexts, though this is not thoroughly explored
- The assumption that attention similarity computed via Simtoken is a reliable indicator of segment relevance lacks direct empirical validation

## Confidence
- **High Confidence**: The identification of causal attention and relative positional encodings as sources of position bias
- **Medium Confidence**: The effectiveness of PINE in eliminating position bias for LM-as-a-judge and retrieval-augmented QA tasks
- **Low Confidence**: The generalizability of PINE across all LLM applications

## Next Checks
1. **Attention Similarity Validation**: Conduct an ablation study where PINE is run with random position reassignment versus attention-based reassignment to quantify how much of the performance gain comes from the similarity computation versus simply breaking positional order.

2. **Numerical Stability Analysis**: Test PINE on increasingly long sequences (500, 1000, 2000 tokens) to identify at what point numerical instability in the softmax operations begins to affect position bias elimination reliability.

3. **Cross-Domain Generalization**: Apply PINE to code generation and translation tasks where position bias might manifest differently than in reasoning or retrieval tasks, comparing performance against both vanilla inference and training-based position bias mitigation approaches.