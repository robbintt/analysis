---
ver: rpa2
title: Automated Generation and Tagging of Knowledge Components from Multiple-Choice
  Questions
arxiv_id: '2405.20526'
source_url: https://arxiv.org/abs/2405.20526
tags:
- learning
- knowledge
- question
- questions
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper introduces an automated approach to generating and\
  \ tagging Knowledge Components (KCs) for multiple-choice questions (MCQs) using\
  \ GPT-4. The authors developed two prompting strategies\u2014simulated expert and\
  \ simulated textbook\u2014to generate KC labels and a clustering algorithm to organize\
  \ questions by shared KCs."
---

# Automated Generation and Tagging of Knowledge Components from Multiple-Choice Questions

## Quick Facts
- arXiv ID: 2405.20526
- Source URL: https://arxiv.org/abs/2405.20526
- Reference count: 40
- Key outcome: LLM-generated KCs preferred by experts 2/3 of time, with 56% accuracy in Chemistry and 35% in E-Learning

## Executive Summary
This paper introduces an automated approach to generating and tagging Knowledge Components (KCs) for multiple-choice questions (MCQs) using GPT-4. The authors developed two prompting strategies—simulated expert and simulated textbook—to generate KC labels and a clustering algorithm to organize questions by shared KCs. They evaluated the approach on Chemistry and E-Learning MCQs, comparing LLM-generated KCs to human-created ones. Results showed 56% KC matching accuracy in Chemistry and 35% in E-Learning, with even higher accuracy when considering top five suggestions. Domain experts preferred LLM-generated KCs over human-generated ones in two-thirds of cases. The clustering algorithm successfully grouped questions by underlying KCs, demonstrating a scalable solution for KC generation and classification without requiring student data or predefined labels.

## Method Summary
The authors developed an automated KC generation system using GPT-4 with two prompting strategies: simulated expert (using tree-of-thought with three expert instructors discussing skills needed) and simulated textbook (identifying topics a textbook page would cover). They tested this on 80 Chemistry MCQs and 80 E-Learning MCQs, each mapped to one KC by domain experts. The system compared LLM-generated KCs to human-assigned ones for matching accuracy, then used a clustering algorithm to iteratively partition questions based on LLM-generated learning objectives. Human experts evaluated KC quality through pairwise comparisons, and the clustering algorithm created ontologies without needing explicit labels or contextual information.

## Key Results
- LLM-generated KC labels matched human-created labels in 56% of Chemistry MCQs and 35% of E-Learning MCQs
- When considering top five suggestions, accuracy improved to 79% for Chemistry and 62% for E-Learning
- Domain experts preferred LLM-generated KCs over human-generated ones in approximately two-thirds of mismatched cases
- Clustering algorithm successfully grouped questions by shared KCs with 90% accuracy in Chemistry and 83% in E-Learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLM-generated KC labels are preferred by domain experts over human-generated labels in mismatched cases.
- Mechanism: LLMs produce more readable and appropriately verbose KC labels due to their advanced next-word prediction capabilities, which aligns better with human evaluative preferences than human-generated labels.
- Core assumption: Human evaluators prefer KC labels that are more detailed and readable, even when the semantic content is equivalent.
- Evidence anchors:
  - [abstract] "Human evaluators favored LLM-generated KCs, choosing them over human-assigned ones approximately two-thirds of the time, a preference that was statistically significant across both domains."
  - [section] "We observed a notable and statistically significant (two-thirds) preference for LLM-generated KC labels over human-generated ones in both domains."
  - [corpus] Weak evidence; corpus neighbors discuss KC generation but do not directly address preference studies.
- Break condition: If evaluators prioritize brevity or specific domain jargon over readability, or if LLM-generated labels become overly verbose and less precise.

### Mechanism 2
- Claim: LLMs can successfully generate KC labels that match human-created labels at moderate accuracy rates.
- Mechanism: LLMs leverage their training on educational content to produce KC labels that align with expert-defined KCs, especially in domains with common terminology like Chemistry.
- Core assumption: The LLM's training data includes sufficient educational content with common terminology to generate relevant KC labels.
- Evidence anchors:
  - [abstract] "Our most effective LLM strategy accurately matched KCs for 56% of Chemistry and 35% of E-Learning MCQs."
  - [section] "In the undergraduate Chemistry domain, we successfully matched more than half of the MCQs with their corresponding KCs."
  - [corpus] Weak evidence; corpus neighbors focus on KC generation but lack specific accuracy benchmarks.
- Break condition: If the domain uses highly specialized jargon not well-represented in LLM training data, leading to lower match rates.

### Mechanism 3
- Claim: The clustering algorithm can organize MCQs by shared KCs without explicit labels or contextual information.
- Mechanism: The algorithm uses iterative partitioning based on LLM-generated learning objectives to group questions with similar semantic content, refining KC ontologies to match or exceed expert-defined granularity.
- Core assumption: LLM-generated learning objectives can effectively partition questions into groups that reflect shared KCs.
- Evidence anchors:
  - [abstract] "Our clustering algorithm successfully grouped questions by their underlying KCs without needing explicit labels or contextual information."
  - [section] "The KC ontology induced by this process is similar to taxonomies such as the Common Core State Standards."
  - [corpus] Weak evidence; corpus neighbors discuss KC clustering but do not provide specific algorithmic details.
- Break condition: If LLM-generated learning objectives fail to capture the semantic nuances of KCs, leading to poor grouping accuracy.

## Foundational Learning

- Concept: Knowledge Components (KCs)
  - Why needed here: KCs are the foundational elements being generated and tagged in this research. Understanding their role in educational assessment and adaptive learning is crucial.
  - Quick check question: What are Knowledge Components, and why are they important in educational technology?

- Concept: Large Language Models (LLMs)
  - Why needed here: LLMs are the primary tool used for generating KC labels. Knowledge of their capabilities and limitations is essential for understanding the research approach.
  - Quick check question: How do LLMs generate text, and what are their strengths and weaknesses in educational applications?

- Concept: Clustering algorithms
  - Why needed here: The clustering algorithm is used to group questions by shared KCs. Understanding its mechanics is necessary for evaluating its effectiveness.
  - Quick check question: What is the purpose of a clustering algorithm, and how does iterative partitioning work?

## Architecture Onboarding

- Component map: Dataset (MCQs) -> LLM (KC generation via simulated expert/textbook prompts) -> Human evaluation (KC matching and preference) -> Clustering algorithm (ontology induction and question grouping)
- Critical path: LLM-based KC generation → Human evaluation → Clustering algorithm
- Design tradeoffs: Accuracy of LLM-generated KCs vs. human effort required for evaluation; granularity of KCs vs. usability
- Failure signatures: LLM generates overly general KCs instead of domain-specific ones; clustering creates too many fine-grained KCs; human evaluators prefer human-generated KCs consistently
- First 3 experiments:
  1. Test the LLM's ability to generate KC labels for a small set of MCQs in a known domain (e.g., Chemistry) and compare to human-generated labels
  2. Evaluate the preference of domain experts for LLM-generated vs. human-generated KC labels in mismatched cases
  3. Run the clustering algorithm on a small set of MCQs and assess the grouping accuracy against known KC groupings

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we determine the optimal level of granularity for knowledge components in different educational contexts and domains?
- Basis in paper: [explicit] The paper mentions that the Knowledge-Learning-Instruction (KLI) Framework provides flexibility in determining KC specificity, and that LLM-generated KCMs can have different levels of granularity (e.g., 63 vs 40 KCs in E-Learning). The paper also notes that domain experts might want to employ different levels of KC granularity.
- Why unresolved: Determining the optimal KC granularity requires balancing specificity for accurate assessment with practical usability. The paper shows LLMs can generate KCs at various granularities but doesn't establish clear criteria for choosing the right level.
- What evidence would resolve it: Comparative studies across multiple domains measuring student learning outcomes, system performance, and expert usability at different KC granularities; development of automated methods to recommend optimal granularity based on course characteristics.

### Open Question 2
- Question: How can we improve the accuracy of LLM-generated KC selection when multiple relevant KCs are identified for a single question?
- Basis in paper: [explicit] The paper notes that LLMs often favor general options over precise, domain-specific ones when selecting the most relevant KC from multiple candidates. This led to 21% of Chemistry and 38% of E-Learning MCQs not matching any of the top five suggested KCs.
- Why unresolved: Current prompting strategies generate relevant KCs but struggle with selection. The paper identifies this as a key limitation but doesn't provide solutions for improving the selection accuracy.
- What evidence would resolve it: Development and testing of enhanced prompting techniques, alternative ranking methods, or hybrid human-LLM approaches that consistently improve selection accuracy beyond the current 56% (Chemistry) and 35% (E-Learning) direct match rates.

### Open Question 3
- Question: How well do LLM-generated KCs generalize across different educational domains, assessment formats, and cultural contexts?
- Basis in paper: [explicit] The paper tested only two domains (Chemistry and E-Learning) and focused on multiple-choice questions. It notes that niche domains may have poorer performance due to limited representation in training data, and mentions potential future work with different assessment formats.
- Why unresolved: The current study provides promising results but is limited in scope. The paper acknowledges these limitations but doesn't explore how well the approach transfers to other contexts.
- What evidence would resolve it: Systematic evaluation across diverse domains (STEM, humanities, professional education), multiple assessment formats (short answer, essays, projects), and cultural contexts (different educational systems, languages); analysis of performance variations and identification of factors affecting generalization.

## Limitations
- KC matching accuracy varies significantly between domains (56% for Chemistry vs. 35% for E-Learning), suggesting domain dependency
- Clustering algorithm shows notable false positive rates (10% in Chemistry, 17% in E-Learning), indicating room for improvement
- Human evaluation relied on pairwise comparisons without standardized criteria beyond general guidelines

## Confidence

- KC generation accuracy claims: **Medium** confidence - While the 56% and 35% match rates are well-documented, the significant domain variation suggests the approach may not generalize equally well across all educational domains.
- Expert preference findings: **High** confidence - The two-thirds preference rate was statistically significant and consistently observed across both domains, providing strong evidence that LLM-generated KCs meet human evaluative standards.
- Clustering algorithm effectiveness: **Medium** confidence - The algorithm successfully grouped questions, but the high false positive rates and domain-dependent performance suggest it needs refinement before deployment.

## Next Checks

1. **Cross-domain validation**: Test the KC generation approach on additional domains (e.g., mathematics, biology) with varying complexity levels to establish generalizability patterns and identify which domains benefit most from this approach.

2. **Longitudinal expert evaluation**: Conduct follow-up studies where the same domain experts re-evaluate KC labels after a delay to test consistency and determine if preferences remain stable over time or change with familiarity.

3. **Student outcome correlation**: Measure whether MCQs with LLM-generated KCs that match human-generated ones show different student performance patterns compared to mismatched pairs, establishing whether the matching accuracy translates to educational effectiveness.