---
ver: rpa2
title: Keyword-Guided Adaptation of Automatic Speech Recognition
arxiv_id: '2406.02649'
source_url: https://arxiv.org/abs/2406.02649
tags:
- whisper
- speech
- prompt
- arxiv
- keyword
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a keyword-guided approach to improve Automatic
  Speech Recognition (ASR) performance on specialized jargon using the Whisper model.
  The method uses a keyword spotting model to identify domain-specific keywords and
  generates prompts to guide the ASR decoder.
---

# Keyword-Guided Adaptation of Automatic Speech Recognition

## Quick Facts
- arXiv ID: 2406.02649
- Source URL: https://arxiv.org/abs/2406.02649
- Authors: Aviv Shamsian; Aviv Navon; Neta Glazer; Gill Hetz; Joseph Keshet
- Reference count: 0
- Primary result: KG-Whisper-PT achieves 5.1% average WER improvement over Whisper on unseen languages

## Executive Summary
This paper proposes a keyword-guided approach to improve Automatic Speech Recognition (ASR) performance on specialized jargon using the Whisper model. The method uses a keyword spotting model (AdaKWS) to identify domain-specific keywords and generates prompts to guide the ASR decoder. Two approaches are introduced: KG-Whisper, which fine-tunes the decoder, and KG-Whisper-PT, which learns a prompt prefix. Results show significant improvements in word error rate (WER) and keyword recognition, with KG-Whisper-PT achieving a 5.1% average WER improvement over Whisper on unseen languages. The method also generalizes well to out-of-domain datasets with challenging acoustic environments.

## Method Summary
The method employs AdaKWS, an open-vocabulary keyword spotting model built on Whisper's acoustic encoder, to dynamically identify domain-specific keywords from audio. These keywords are then used to generate prompts that guide the Whisper decoder during transcription. Two approaches are proposed: KG-Whisper, which fine-tunes the Whisper decoder while keeping the encoder frozen, and KG-Whisper-PT, which learns a small prompt prefix while keeping the entire Whisper model frozen. The training involves dynamically generating positive and negative keywords for each audio input, concatenating them with a delimiter, and placing them between start tokens. The models are trained on the Voxpopuli dataset for 30K update steps with a batch size of 4 and specific learning rates for each approach.

## Key Results
- KG-Whisper-PT achieves a 5.1% average WER improvement over Whisper on unseen languages
- Both KG-Whisper and KG-Whisper-PT show significant improvements in keyword recognition F1 scores
- The methods generalize well to out-of-domain datasets including air traffic control communications and medical reports

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The keyword spotting model dynamically generates prompts that guide the decoder toward specific domain jargon, improving both WER and keyword recall.
- Mechanism: AdaKWS identifies relevant keywords from the audio, then the decoder receives these as part of the prompt, biasing its output toward those terms.
- Core assumption: The encoder representation contains sufficient signal to identify domain-specific keywords, and the decoder can be steered effectively using these prompts.
- Evidence anchors:
  - [abstract]: "We employ a keyword spotting model that leverages the Whisper encoder representation to dynamically generate prompts for guiding the decoder during the transcription process."
  - [section 2]: "The KWS prediction vector ˆy is then used to generate a prompt that is passed to the Whisper decoder."
  - [corpus]: Weak support; related work focuses on fine-tuning Whisper for domain adaptation but does not explicitly mention keyword-guided prompting.

### Mechanism 2
- Claim: Fine-tuning the decoder (KG-Whisper) allows the model to internalize keyword context during training, improving adaptation to specialized domains.
- Mechanism: During training, the decoder is updated to minimize cross-entropy loss with respect to keyword-injected prompts, while the encoder remains frozen.
- Core assumption: The decoder has sufficient capacity to learn the mapping from keyword context to transcription without catastrophic forgetting of general ASR capability.
- Evidence anchors:
  - [abstract]: "KG-Whisper, which is aimed at fine-tuning the Whisper decoder..."
  - [section 2.1]: "We fine-tune the decoder parameters to minimize the CE loss over pairs of speech representations and KWS prompts."
  - [corpus]: Weak support; no direct evidence of decoder fine-tuning for keyword guidance in the corpus.

### Mechanism 3
- Claim: Prompt tuning (KG-Whisper-PT) achieves similar or better generalization than full fine-tuning by learning only a small prefix of tokens, preserving most Whisper parameters.
- Mechanism: A small learned prompt prefix is prepended to the KWS-generated prompt, guiding the decoder without altering its weights.
- Core assumption: A small number of tunable parameters can effectively steer a large frozen model without losing its pre-trained generalization ability.
- Evidence anchors:
  - [abstract]: "KG-Whisper-PT, which learns a prompt prefix... achieves on-par or superior generalization results using only ∼15K trainable parameters."
  - [section 2.2]: "This is done by minimizing the CE loss, while keeping the Whisper's encoder and decoder frozen."
  - [corpus]: Weak support; related works focus on LoRA-style adaptation or prompt tuning for ASR, but not keyword-guided prompt tuning.

## Foundational Learning

- Concept: Keyword spotting and its integration with ASR
  - Why needed here: Understanding how KWS identifies domain terms and feeds them into ASR is central to the proposed approach.
  - Quick check question: How does AdaKWS use the encoder representation to predict keyword presence?

- Concept: Prompt tuning vs. full fine-tuning
  - Why needed here: KG-Whisper-PT's efficiency and generalization depend on the mechanics of prompt tuning.
  - Quick check question: What is the difference between learned prefix prompt tuning and traditional adapter-based fine-tuning?

- Concept: Cross-attention in transformer decoders
  - Why needed here: The decoder's cross-attention weights determine how prompts influence output token prediction.
  - Quick check question: How do cross-attention scores shift when keyword prompts are added?

## Architecture Onboarding

- Component map: Audio -> Mel-spectrogram -> Whisper encoder -> AdaKWS -> Prompt generator -> Whisper decoder -> transcription

- Critical path:
  1. Audio → Mel-spectrogram → Whisper encoder → AdaKWS
  2. AdaKWS outputs keyword presence vector
  3. Prompt generator creates keyword sequence
  4. Prompt (with learned prefix if PT) → Whisper decoder → transcription

- Design tradeoffs:
  - Full decoder fine-tuning (KG-Whisper) vs. prompt tuning (KG-Whisper-PT): parameter count vs. generalization
  - Prefix length: longer may capture more context but risk hallucination
  - Keyword sampling strategy: affects diversity and robustness of prompts

- Failure signatures:
  - Degraded WER on non-jargon words: indicates overfitting to keywords
  - Increased hallucination: suggests prompt injection issues
  - Poor keyword recall: KWS may be underperforming or prompts are ignored

- First 3 experiments:
  1. Validate KWS keyword detection accuracy on a held-out keyword set
  2. Test KG-Whisper-PT with varying prefix lengths (4, 8, 12, 16 tokens) on a validation split
  3. Compare KG-Whisper vs. KG-Whisper-PT on a low-resource language subset for generalization assessment

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, but several important ones emerge from the evaluation:

### Open Question 1
- Question: How does the performance of KG-Whisper and KG-Whisper-PT scale with increasing amounts of domain-specific training data?
- Basis in paper: [inferred] The paper mentions "expanding upon Whisper's architecture" and proposes fine-tuning approaches, but does not investigate the relationship between training data size and performance.
- Why unresolved: The paper focuses on evaluating the proposed methods with a fixed training dataset size and does not explore how performance changes with varying amounts of domain-specific data.
- What evidence would resolve it: Systematic experiments varying the amount of domain-specific training data and measuring the resulting WER and F1 scores for KG-Whisper and KG-Whisper-PT.

### Open Question 2
- Question: What is the impact of keyword length and complexity on the effectiveness of KG-Whisper and KG-Whisper-PT?
- Basis in paper: [explicit] The paper mentions that "positive keywords are sampled from the transcript of the input audio sample" and discusses keyword sampling strategies, but does not investigate how keyword characteristics affect performance.
- Why unresolved: While the paper presents keyword sampling methods, it does not explore how the length or complexity of keywords influences the effectiveness of the proposed approaches.
- What evidence would resolve it: Controlled experiments varying keyword length and complexity, and measuring the resulting WER and F1 scores for KG-Whisper and KG-Whisper-PT.

### Open Question 3
- Question: How do KG-Whisper and KG-Whisper-PT perform in real-time, streaming ASR scenarios?
- Basis in paper: [inferred] The paper evaluates the proposed methods on pre-recorded datasets and does not address their suitability for real-time, streaming applications.
- Why unresolved: The paper focuses on offline evaluation and does not investigate the latency or performance of KG-Whisper and KG-Whisper-PT in streaming ASR scenarios.
- What evidence would resolve it: Experiments evaluating the latency and accuracy of KG-Whisper and KG-Whisper-PT in real-time, streaming ASR setups.

## Limitations

- The effectiveness of AdaKWS keyword spotting is assumed but not directly validated, creating a potential single point of failure.
- The method's robustness to noisy or incomplete keyword predictions is not tested.
- The impact of prompt injection on general ASR capability degradation remains unclear.
- While KG-Whisper-PT shows promising generalization, the trade-off between prefix length and hallucination risk is not systematically explored.

## Confidence

**High Confidence**: The claim that KG-Whisper-PT achieves a 5.1% average WER improvement over Whisper on unseen languages is supported by experimental results across multiple datasets and language conditions.

**Medium Confidence**: The assertion that keyword-guided prompting improves keyword recall is reasonable given the mechanism but would benefit from direct measurement of KWS performance and ablation studies isolating the keyword contribution.

**Low Confidence**: The claim that the method generalizes well to out-of-domain datasets with challenging acoustic environments is weakly supported - the evaluation shows performance on specific out-of-domain datasets but doesn't test the limits of this generalization or compare against domain-specific baselines.

## Next Checks

1. **KWS Performance Validation**: Measure AdaKWS keyword detection precision, recall, and F1-score on a held-out keyword set to establish baseline keyword spotting capability before integration with ASR.

2. **Prompt Length Sensitivity Analysis**: Systematically test KG-Whisper-PT with varying prefix lengths (4, 8, 12, 16 tokens) on validation data to identify the optimal balance between contextual guidance and hallucination risk.

3. **Generalization Boundary Testing**: Evaluate both KG-Whisper and KG-Whisper-PT on increasingly challenging out-of-domain datasets with progressively different acoustic characteristics and vocabulary domains to map the limits of generalization.