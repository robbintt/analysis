---
ver: rpa2
title: Prompting Strategies for Enabling Large Language Models to Infer Causation
  from Correlation
arxiv_id: '2412.13952'
source_url: https://arxiv.org/abs/2412.13952
tags:
- graph
- given
- directed
- independent
- causal
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces PC-SubQ, a prompting strategy that improves
  LLMs' ability to infer causal relationships from correlation statements by decomposing
  the task into fixed subquestions following the PC algorithm steps. The approach
  guides LLMs through these steps by sequentially prompting with one subquestion at
  a time, using few-shot Chain-of-Thought examples for each.
---

# Prompting Strategies for Enabling Large Language Models to Infer Causation from Correlation

## Quick Facts
- arXiv ID: 2412.13952
- Source URL: https://arxiv.org/abs/2412.13952
- Reference count: 40
- One-line primary result: PC-SubQ improves LLM performance on causal inference by decomposing tasks into algorithmic steps with few-shot examples

## Executive Summary
This work introduces PC-SubQ, a prompting strategy that significantly improves large language models' ability to infer causal relationships from correlation statements. The approach guides LLMs through the PC algorithm's steps by sequentially prompting with subquestions, each accompanied by few-shot Chain-of-Thought examples. Evaluated on the Corr2Cause benchmark across five LLMs, PC-SubQ consistently outperforms baseline prompting strategies in F1-score and accuracy, demonstrating robustness to variable renaming and natural language variations.

## Method Summary
PC-SubQ decomposes causal discovery into 8 sequential subquestions mirroring the PC algorithm's steps. For each subquestion, the LLM receives few-shot Chain-of-Thought examples demonstrating how to solve that specific subtask. The method uses selective information propagation, passing only the answer from the previous subquestion to the next prompt rather than full reasoning history. This structured approach enables LLMs to follow algorithmic procedures while maintaining transparency through intermediate reasoning traces. The method requires 16 LLM calls per query (two per subquestion) and is evaluated on the Corr2Cause benchmark using five different LLMs.

## Key Results
- PC-SubQ consistently outperforms baseline strategies (zero-shot, zero-shot COT, few-shot, few-shot COT) across all five evaluated LLMs
- The method demonstrates robustness to variable renaming and paraphrasing, correctly handling natural story scenarios despite few-shot examples containing only symbolic variables
- Final errors are isolated to the last subquestion while earlier steps produce correct causal graphs, enabling transparent error tracing
- Performance degrades with increased problem complexity (5-6 variables) but shows promise for natural language scenarios with fewer variables

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Decomposing causal discovery into algorithmic steps improves LLM performance by aligning task structure with known statistical procedures
- Mechanism: PC-SubQ breaks NL-CD into 8 sequential subquestions that mirror the PC algorithm's steps, allowing LLMs to follow a structured reasoning path rather than inferring the entire graph in one step
- Core assumption: LLMs can effectively follow algorithmic procedures when explicitly guided through fixed subquestions with appropriate few-shot examples
- Evidence anchors: [abstract] "The proposed prompting strategy, PC-SubQ, guides the LLM to follow these algorithmic steps, by sequentially prompting it with one subquestion at a time"; [section] "PC-SubQ sequentially prompts the LLM with one of these 8 subquestions at a time"

### Mechanism 2
- Claim: Sequential prompting with selective information propagation reduces cognitive load and improves focus on relevant task components
- Mechanism: PC-SubQ only passes the answer to the previous subquestion (not full reasoning history) to the next prompt, reducing context length and helping the LLM focus on the current subtask
- Core assumption: LLMs can solve complex tasks more effectively when context is minimized to only the most relevant information for each step
- Evidence anchors: [abstract] "by sequentially prompting it with one subquestion at a time, augmenting the next subquestion's prompt with the answer to the previous one(s)"; [section] "Notice that the rest of the history, for instance the reasoning steps, is not used in the prompt construction of following subquestions"

### Mechanism 3
- Claim: Few-shot Chain-of-Thought examples for each subquestion provide targeted demonstrations that improve step-specific reasoning
- Mechanism: PC-SubQ provides 1-4 shots per subquestion (11 for the final question) that demonstrate how to solve that specific subtask, allowing the LLM to learn task decomposition patterns
- Core assumption: LLMs benefit from task-specific demonstrations that show intermediate reasoning steps rather than just final answers
- Evidence anchors: [abstract] "by sequentially prompting it with one subquestion at a time, augmenting the next subquestion's prompt with the answer to the previous one(s)"; [section] "We prepend few-shot examples, each with a COT reasoning, to each subquestion"

## Foundational Learning

- Concept: PC algorithm steps for causal discovery
  - Why needed here: PC-SubQ directly maps its 8 subquestions to specific steps of the PC algorithm, so understanding these steps is essential for modifying or extending the approach
  - Quick check question: What are the four main steps of the PC algorithm and what does each accomplish in causal graph discovery?

- Concept: Directed Acyclic Graph (DAG) and d-separation
  - Why needed here: The PC algorithm relies on d-separation criteria to determine conditional independencies, and the final output is a DAG or CPDAG
  - Quick check question: How does d-separation in a DAG relate to conditional independence in the joint distribution?

- Concept: Markov equivalence and CPDAG
  - Why needed here: The PC algorithm outputs a CPDAG representing a Markov equivalence class, which is important for understanding what the method can and cannot distinguish
  - Quick check question: What information is preserved versus lost when moving from individual DAGs to a CPDAG representation?

## Architecture Onboarding

- Component map: LLM prompt generation → Sequential subquestion prompting → Few-shot example injection → Context management → Output aggregation
- Critical path: Prompt construction → LLM call 1 (reasoning) → LLM call 2 (answer) → Next subquestion → Final hypothesis evaluation
- Design tradeoffs: 16 LLM calls per query vs. single call efficiency; Fixed subquestion structure vs. adaptive decomposition; Symbolic variable examples vs. natural language examples
- Failure signatures: Propagation of errors through sequential steps; Context overflow from too much information passing; Poor few-shot examples leading to incorrect step execution
- First 3 experiments:
  1. Test with 2-variable examples to verify basic functionality
  2. Test with 3-variable examples to verify v-structure orientation
  3. Test with paraphrased premises to verify robustness claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the PC-SubQ prompting strategy be extended to handle scenarios with unobserved confounders that violate the no-unobserved-confounders assumption of the PC algorithm?
- Basis in paper: [inferred] The paper mentions that the PC algorithm assumes no unobserved confounders and evaluates PC-SubQ on the Corr2Cause benchmark. It also discusses future research directions about combining formal causal reasoning with commonsense knowledge for natural scenarios.
- Why unresolved: The paper only evaluates PC-SubQ on scenarios without unobserved confounders and doesn't explore how the approach would handle such cases.
- What evidence would resolve it: Experimental results comparing PC-SubQ performance on datasets containing latent confounders versus datasets without them.

### Open Question 2
- Question: What is the computational complexity of PC-SubQ compared to fine-tuned models for causal inference tasks?
- Basis in paper: [explicit] The paper acknowledges that the current approach induces increased inference time due to 16 LLM calls (two per subquestion) compared to few-shot prompting requiring only one LLM call.
- Why unresolved: While the paper notes increased inference time, it doesn't provide a detailed computational complexity analysis comparing PC-SubQ to fine-tuned alternatives.
- What evidence would resolve it: A quantitative comparison of computational resources (time and cost) required for PC-SubQ versus fine-tuned models on the same causal inference tasks.

### Open Question 3
- Question: How does PC-SubQ performance scale when applied to causal discovery tasks with significantly more than 6 variables?
- Basis in paper: [explicit] The paper shows performance degradation when problem complexity increases (N = 5, 6 variables) and mentions this as a limitation.
- Why unresolved: The evaluation only goes up to 6 variables, and the paper suggests this limitation might be mitigated for natural stories which typically involve fewer variables.
- What evidence would resolve it: Experimental results evaluating PC-SubQ on benchmarks with larger numbers of variables (e.g., 10+ variables) to determine scaling behavior.

## Limitations
- The sequential prompting approach requires 16 LLM calls per query, which may not scale efficiently for practical applications despite the accuracy benefits
- The method relies heavily on few-shot examples for each subquestion, making it sensitive to the quality and representativeness of these demonstrations
- The PC-SubQ framework assumes that LLMs can effectively execute algorithmic steps when guided through sequential prompts, but this may not generalize to more complex causal structures beyond the Corr2Cause benchmark

## Confidence
- High confidence in the core mechanism (PC-SubQ improves causal inference accuracy): Strong empirical evidence across multiple LLMs and consistent improvements in F1-score and accuracy
- Medium confidence in scalability claims: While accuracy improvements are demonstrated, the computational overhead of 16 sequential calls per query is acknowledged but not fully characterized
- Medium confidence in robustness to variable renaming and paraphrasing: These claims are supported but could benefit from broader testing across more diverse natural language inputs
- Low confidence in generalizability beyond the specific benchmark: Limited evaluation scope to one dataset with controlled conditions

## Next Checks
1. **Scalability Test**: Measure end-to-end inference time and cost for PC-SubQ versus baseline methods across varying input sizes to quantify the practical overhead of 16 sequential LLM calls
2. **Cross-Dataset Validation**: Apply PC-SubQ to at least two additional causal inference benchmarks with different complexity levels to assess generalizability beyond Corr2Cause
3. **Robustness Expansion**: Test PC-SubQ with more diverse natural language variations including longer premises, multiple hypotheses, and ambiguous causal relationships to stress-test the variable renaming and paraphrasing claims