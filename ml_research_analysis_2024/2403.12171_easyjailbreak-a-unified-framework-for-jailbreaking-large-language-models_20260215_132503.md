---
ver: rpa2
title: 'EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models'
arxiv_id: '2403.12171'
source_url: https://arxiv.org/abs/2403.12171
tags:
- jailbreak
- attack
- arxiv
- language
- attacks
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'EasyJailbreak is a unified framework that standardizes jailbreak
  attack construction and evaluation against large language models. It decomposes
  attacks into four modular components: Selector, Mutator, Constraint, and Evaluator,
  enabling researchers to easily build attacks from novel and existing components.'
---

# EasyJailbreak: A Unified Framework for Jailbreaking Large Language Models

## Quick Facts
- arXiv ID: 2403.12171
- Source URL: https://arxiv.org/abs/2403.12171
- Authors: Weikang Zhou et al.
- Reference count: 10
- Primary result: 60% average breach probability across 10 LLMs, even GPT-4 shows 33% attack success rate

## Executive Summary
EasyJailbreak is a unified framework that standardizes jailbreak attack construction and evaluation against large language models. It decomposes attacks into four modular components: Selector, Mutator, Constraint, and Evaluator, enabling researchers to easily build attacks from novel and existing components. The framework supports 11 distinct jailbreak methods and facilitates security validation across diverse LLMs.

Evaluations across 10 models reveal significant vulnerabilities, with an average breach probability of 60% under various attacks. Notably, even advanced models like GPT-3.5-Turbo and GPT-4 exhibit average Attack Success Rates (ASR) of 57% and 33%, respectively. EasyJailbreak is released with comprehensive resources including a web platform, PyPI package, screencast video, and experimental outputs to support further research.

## Method Summary
EasyJailbreak provides a modular framework for constructing and evaluating jailbreak attacks on LLMs by decomposing attacks into four fundamental components: Selector (chooses promising seeds from candidate pool), Mutator (modifies rejected jailbreak inputs using generation, gradient-based, or rule-based methods), Constraint (filters ineffective inputs), and Evaluator (assesses attack success using classifier-based, generative model-based, or rule-based approaches). The framework supports 11 attack methods and enables systematic benchmarking across different LLMs by providing a standardized interface for attack construction and evaluation.

## Key Results
- 60% average breach probability across 10 distinct LLMs under various attacks
- GPT-3.5-Turbo shows 57% Attack Success Rate, GPT-4 shows 33% Attack Success Rate
- Increasing model parameter size does not necessarily enhance security (tested on Llama2 and Vicuna models)
- Open-source models show higher vulnerability than closed-source models like GPT-3.5-Turbo and GPT-4

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modular decomposition into Selector, Mutator, Constraint, and Evaluator components enables systematic jailbreak attack construction and evaluation.
- Mechanism: The framework abstracts jailbreak attacks into four interchangeable components, allowing researchers to assemble novel attacks by combining existing components or creating new ones. This modular design reduces development effort and enables standardized benchmarking across different methods.
- Core assumption: Jailbreak attacks can be decomposed into these four fundamental components without losing attack effectiveness or generality.
- Evidence anchors:
  - [abstract] "It builds jailbreak attacks using four components: Selector, Mutator, Constraint, and Evaluator."
  - [section 3] "EasyJailbreak aims to carry out jailbreak attacks on large-scale language models... It streamlines the process by decomposing jailbreak methods into four fundamental components"
- Break condition: If certain jailbreak techniques cannot be decomposed into these four components or if the decomposition loses critical attack capabilities.

### Mechanism 2
- Claim: The framework's flexibility enables both automated optimization-based attacks and rule-based attacks to be implemented within the same structure.
- Mechanism: By supporting different types of mutators (generation, gradient-based, rule-based) and selectors (random, EXP3, UCB, MCTS), the framework can accommodate both optimization-driven approaches like GPTFUZZER and rule-based approaches like JailBroken within a unified interface.
- Core assumption: Different jailbreak attack paradigms can be mapped to the same component structure without losing their distinctive advantages.
- Evidence anchors:
  - [section 3.3] "When a jailbreak input is rejected by a target model, users can leverage a mutator to modify this input"
  - [section 3.2] "In certain jailbreak methods, the number of alternative jailbreak inputs can exponentially increase due to the presence of productive mutators"
  - [Table 1] Shows multiple attack recipes using different combinations of components
- Break condition: If certain attack strategies require fundamentally different architectural approaches that cannot be expressed through the component abstraction.

### Mechanism 3
- Claim: Standardized evaluation across diverse models reveals pervasive vulnerabilities, with even advanced models showing significant breach probabilities.
- Mechanism: By using a unified Evaluator component (GenerativeJudge) and consistent evaluation methodology across 10 different LLMs, the framework reveals that 60% average breach probability is widespread, even among models like GPT-4 with 33% ASR.
- Core assumption: A single evaluation method can fairly assess jailbreak success across models with different architectures, training regimes, and safety alignments.
- Evidence anchors:
  - [abstract] "Our validation across 10 distinct LLMs reveals a significant vulnerability, with an average breach probability of 60%"
  - [section 5.1] "We use GenerativeJudge as a uniform evaluation method to judge jailbreak instances after the attack"
  - [Table 2] Shows consistent evaluation results across multiple models
- Break condition: If evaluation results vary significantly based on model-specific characteristics that the uniform evaluation cannot capture.

## Foundational Learning

- Concept: Modular software architecture and design patterns
  - Why needed here: The framework's effectiveness depends on understanding how to decompose complex systems into interchangeable components that can be mixed and matched.
  - Quick check question: Can you explain how the Strategy pattern differs from the Factory pattern, and which one is more relevant to the component-based design in EasyJailbreak?

- Concept: Adversarial machine learning and attack taxonomies
  - Why needed here: Understanding different categories of jailbreak attacks (human-design, long-tail encoding, prompt optimization) is essential for mapping attacks to framework components.
  - Quick check question: What distinguishes long-tail encoding attacks from prompt optimization attacks in terms of their underlying assumptions about model vulnerabilities?

- Concept: Evaluation methodology and benchmarking in AI security
  - Why needed here: The framework's claims about vulnerability rates depend on understanding how to design fair, consistent evaluation methods across diverse models.
  - Quick check question: What are the key considerations when designing an evaluation metric that can fairly compare jailbreak success rates across models with different safety training approaches?

## Architecture Onboarding

- Component map: Selector -> Mutator -> Constraint -> Evaluator
- Critical path: User configures queries/seeds/models → Attacker iteratively updates inputs using Selector→Mutator→Constraint pipeline → Evaluator assesses results → Report generated with metrics
- Design tradeoffs:
  - Flexibility vs. Performance: Modular design allows experimentation but may introduce overhead compared to specialized implementations
  - Uniformity vs. Model-specific optimization: Single evaluation method provides consistency but may miss model-specific nuances
  - Generality vs. Attack effectiveness: Component abstraction may not capture all attack strategies perfectly
- Failure signatures:
  - Selector consistently choosing poor candidates → low attack success rates
  - Mutator producing inputs that consistently fail constraints → wasted computation
  - Evaluator misclassifying successful attacks → underreported vulnerabilities
  - Component combinations that don't work well together → need for recipe refinement
- First 3 experiments:
  1. Test basic PAIR attack on Vicuna-13B with default parameters to verify framework functionality
  2. Compare RandomSelector vs EXP3SelectPolicy on the same attack to evaluate selector impact
  3. Test the same attack across multiple models (Vicuna, Llama2, GPT-3.5) to verify cross-model compatibility

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the impact of increasing parameter size on the security of large language models against jailbreak attacks?
- Basis in paper: [explicit] The paper states that "Increasing a model's parameter size does not necessarily lead to enhanced security" and provides evidence from Llama2 and Vicuna models.
- Why unresolved: The conclusion is based on models with parameters up to 13B, but the security of much larger models, such as Llama2-Chat-70B, is not tested.
- What evidence would resolve it: Testing and comparing the jailbreak success rates across a range of model sizes, including significantly larger models like Llama2-Chat-70B, would provide more comprehensive insights into the relationship between model size and security.

### Open Question 2
- Question: How does the efficiency of jailbreak attacks vary between closed-source and open-source models?
- Basis in paper: [explicit] The paper indicates that closed-source models like GPT-3.5-Turbo and GPT-4 have lower average Attack Success Rates (ASR) compared to open-source models.
- Why unresolved: While the paper provides ASR data, it does not delve into the time and resource efficiency of executing these attacks on different types of models.
- What evidence would resolve it: Conducting a comparative analysis of the time and computational resources required to successfully execute jailbreak attacks on both closed-source and open-source models would clarify the efficiency differences.

### Open Question 3
- Question: What are the trade-offs between accuracy and efficiency in evaluating jailbreak attacks?
- Basis in paper: [explicit] The paper compares different evaluation methods, noting that GPT-4 leads in accuracy but has longer processing times, while other methods like GPTFuzzer offer a balance between accuracy and efficiency.
- Why unresolved: The paper does not provide a detailed exploration of how these trade-offs affect the practical deployment of jailbreak detection systems.
- What evidence would resolve it: A comprehensive study that measures the real-world impact of using different evaluation methods in terms of both detection accuracy and operational efficiency would help in understanding the optimal balance for practical applications.

## Limitations

- The modular decomposition may not capture all jailbreak attack strategies effectively, as some techniques might not fit cleanly into the four-component framework
- The uniform evaluation methodology using GenerativeJudge could mask model-specific vulnerabilities that require tailored assessment approaches
- Reported success rates (60% average breach probability, 57% for GPT-3.5, 33% for GPT-4) may not generalize to future model versions or different evaluation settings

## Confidence

**High Confidence**: The framework's basic architecture and component design (Modular decomposition enabling systematic attack construction) - The component structure is clearly specified and demonstrated through multiple working attack recipes.

**Medium Confidence**: The effectiveness of the unified evaluation methodology (Standardized evaluation revealing pervasive vulnerabilities) - While the methodology is consistent, its fairness across diverse model architectures is not fully validated.

**Medium Confidence**: The generalizability of component combinations (Framework flexibility supporting diverse attack paradigms) - The paper shows successful implementation of multiple attack types, but doesn't exhaustively test all possible component combinations.

## Next Checks

1. **Cross-model evaluation consistency test**: Run the same set of attack recipes across all 10 evaluated models using identical parameters to verify that the 60% average breach probability holds consistently across different model architectures and training regimes.

2. **Component decomposition boundary test**: Attempt to implement at least two jailbreak attack methods that are not in the 11 tested methods to verify whether they can be decomposed into the four component framework without losing attack effectiveness.

3. **Evaluation methodology robustness test**: Compare GenerativeJudge evaluation results against alternative evaluation approaches (human evaluation, different LLM judges) on a subset of successful and unsuccessful attacks to assess consistency and potential biases in the uniform evaluation methodology.