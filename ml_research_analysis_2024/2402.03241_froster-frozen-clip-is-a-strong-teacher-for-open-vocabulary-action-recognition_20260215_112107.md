---
ver: rpa2
title: 'FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition'
arxiv_id: '2402.03241'
source_url: https://arxiv.org/abs/2402.03241
tags: []
core_contribution: This paper presents FROSTER, a framework for open-vocabulary action
  recognition that effectively learns feature representation that is both video-specific
  and generalizable. It treats the frozen CLIP model as a teacher to maintain the
  generalizability exhibited by the original CLIP and supervises the feature learning
  for the extraction of video-specific features to bridge the gap between images and
  videos.
---

# FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition

## Quick Facts
- **arXiv ID**: 2402.03241
- **Source URL**: https://arxiv.org/abs/2402.03241
- **Authors**: Xiaohu Huang, Hao Zhou, Kun Yao, Kai Han
- **Reference count**: 17
- **Primary result**: Achieves state-of-the-art performance on all datasets across base-to-novel and cross-dataset settings for open-vocabulary action recognition

## Executive Summary
FROSTER presents a framework for open-vocabulary action recognition that leverages frozen CLIP as a teacher to maintain generalizability while learning video-specific features. The approach uses residual feature distillation to balance two objectives: preserving CLIP's generalizable knowledge and extracting video-specific temporal information. By treating CLIP as a frozen teacher and applying a modified residual network for distillation, FROSTER consistently outperforms existing methods across multiple benchmarks and evaluation settings.

## Method Summary
FROSTER freezes the pretrained CLIP model and treats it as a teacher for knowledge distillation. The student model learns video-specific features while maintaining alignment with frozen CLIP features through a residual distillation network. The residual network combines identity mapping with a learnable projection to allow controlled deviation from frozen features when necessary for video-specific adaptation. Training uses both cross-entropy loss for classification and distillation loss to preserve generalizable representations, with the residual coefficient α=0.1 controlling the balance between video-specific and generalizable learning.

## Key Results
- Consistently achieves state-of-the-art performance across all tested datasets (Kinetics-400, UCF-101, HMDB-51, Something-to-Something V2)
- Excels in both base-to-novel settings (learning from seen categories, testing on novel ones) and cross-dataset transfer
- Demonstrates superior generalizability preservation compared to full fine-tuning baselines, particularly on Kinetics-600 unseen categories

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FROSTER preserves CLIP's generalization capability while enabling video-specific adaptation through residual feature distillation
- Mechanism: By treating frozen CLIP as a teacher and distilling knowledge via a residual network, FROSTER balances two objectives - maintaining pretrained generalizability while learning video-specific features. The residual design allows video-specific features to deviate from frozen features only as needed, preventing over-constraining while preserving the strong CLIP representation
- Core assumption: The pretrained CLIP features contain generalizable knowledge that transfers well to video tasks when properly distilled
- Evidence anchors:
  - [abstract]: "treats the frozen CLIP model as a teacher to maintain the generalizability exhibited by the original CLIP and supervises the feature learning for the extraction of video-specific features"
  - [section]: "inspired by knowledge distillation techniques... ensures the tuned features do not diverge too far from the frozen ones"
  - [corpus]: Weak - corpus contains unrelated distillation papers but none specifically addressing frozen CLIP teacher frameworks
- Break condition: If video tasks require fundamentally different feature representations than images, the frozen CLIP's generalizable knowledge may not transfer effectively

### Mechanism 2
- Claim: The residual feature distillation approach prevents overfitting while enabling video-specific learning
- Mechanism: The modified residual network in Eq. (6) combines identity mapping with a learnable projection. The identity path directly guides generalizable learning from frozen features, while the projection allows controlled deviation for video-specific adaptation. The α coefficient balances these objectives, with smaller values maintaining closer alignment to frozen features
- Core assumption: Video-specific features should be close to image features for effective transfer, but can deviate when necessary for temporal dynamics
- Evidence anchors:
  - [section]: "The intuition behind the design is to allow the tuned features to effectively receive supervision from generalized ones while also being video-specific"
  - [section]: "We apply a modified residual network h on zg to transform its representation with a two-layer MLP projector and an identity mapping"
  - [corpus]: Weak - corpus mentions various distillation methods but none specifically use residual designs for frozen-teacher frameworks
- Break condition: If video tasks require completely different feature spaces than images, the residual approach may be too constraining

### Mechanism 3
- Claim: Zero initialization of the second projection layer ensures stable training and proper knowledge transfer
- Mechanism: Initializing W2 as zeros means the residual path starts as identity mapping, allowing the model to begin from the pretrained CLIP state. This initialization prevents early catastrophic forgetting and ensures smooth adaptation to video tasks
- Core assumption: Starting from the pretrained CLIP representation provides better initialization than random weights for video adaptation
- Evidence anchors:
  - [section]: "to make sureˆzg is learned starting from the pretrained status, we initialize the parameters of the second fully connected layer W2 as zeros"
  - [section]: "at the beginning of fine-tuning, ˆzg only contains zg and gradually gets updated"
  - [corpus]: Weak - corpus contains initialization strategies but none specifically for residual distillation with frozen teachers
- Break condition: If the pretrained CLIP representation is not a good starting point for the specific video domain, this initialization may slow down adaptation

## Foundational Learning

- Concept: Knowledge distillation and teacher-student frameworks
  - Why needed here: FROSTER fundamentally relies on knowledge distillation from frozen CLIP to transfer generalizable knowledge while adapting to video tasks
  - Quick check question: How does feature-based distillation differ from logits-based distillation, and why is it preferred for maintaining generalizable representations?

- Concept: Residual network design and skip connections
  - Why needed here: The residual feature distillation uses modified residual connections to balance video-specific and generalizable learning objectives
  - Quick check question: Why does combining identity mapping with learnable projection help prevent over-constraining video-specific feature learning?

- Concept: Catastrophic forgetting and continual learning
  - Why needed here: FROSTER must prevent the tuned model from forgetting CLIP's generalizable knowledge while learning video-specific features
  - Quick check question: How does the residual distillation approach prevent catastrophic forgetting compared to full fine-tuning?

## Architecture Onboarding

- Component map: Frozen CLIP (teacher) → Video-specific model (student) → Residual feature distillation → Classification head. The residual network sits between student features and distillation loss
- Critical path: Input video → Student encoder → Residual transformation → Distillation loss (with frozen teacher) + Classification loss → Backpropagation through student only
- Design tradeoffs: Full fine-tuning offers more flexibility but risks losing generalization; adapter-based methods preserve more generalization but may limit video-specific adaptation; FROSTER balances both through residual distillation
- Failure signatures: If video performance improves but generalizability drops (poor novel set performance), the residual coefficient α may be too large; if both suffer, the distillation may be too weak
- First 3 experiments:
  1. Compare FROSTER with frozen CLIP baseline on Kinetics-400→UCF-101 transfer to verify video-specific gains while preserving generalization
  2. Test different α values (0.05, 0.1, 0.5, 1.0) to find optimal balance between video-specific and generalizable learning
  3. Evaluate on Kinetics-600 (unseen categories) to verify generalization preservation compared to full fine-tuning baselines

## Open Questions the Paper Calls Out

- How to improve the generalizability of video models beyond the CLIP's generalizability

## Limitations

- The residual feature distillation framework depends heavily on the assumption that frozen CLIP features contain transferable generalizable knowledge
- The choice of α=0.1 appears arbitrary without ablation studies showing sensitivity to this hyperparameter
- Limited cross-dataset diversity may not fully represent real-world novel class scenarios

## Confidence

- **High Confidence**: The core mechanism of using frozen CLIP as teacher and residual distillation for feature learning is well-supported by the experimental results and theoretical framework
- **Medium Confidence**: The generalizability preservation claim relies on specific evaluation settings that may not fully represent real-world novel class scenarios
- **Low Confidence**: The assertion that FROSTER works "across the board" may be overstated given limited cross-dataset diversity and potential domain-specific variations

## Next Checks

1. **Ablation study on residual coefficient α**: Systematically evaluate FROSTER performance across α ∈ [0.01, 0.1, 0.5, 1.0] to understand the sensitivity of generalization vs. video-specific adaptation tradeoff
2. **Cross-domain transfer evaluation**: Test FROSTER on non-action video tasks (e.g., surveillance, medical imaging) to verify generalization beyond action recognition
3. **Time efficiency analysis**: Measure training/inference computational overhead compared to full fine-tuning and adapter-based methods to assess practical deployment viability