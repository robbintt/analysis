---
ver: rpa2
title: 'AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer Models
  for Lateral Thinking Puzzles'
arxiv_id: '2404.01084'
source_url: https://arxiv.org/abs/2404.01084
tags:
- reasoning
- language
- task
- mistral-7b
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors evaluated various pre-trained transformer-based language
  models on two sub-tasks of the SemEval-2024 Task 9 competition involving lateral
  thinking puzzles. They fine-tuned models like BERT, RoBERTa, DeBERTa, and LLMs like
  Llama 2, Phi-2, and Mistral-7b on the datasets, transforming the multiple-choice
  questions into binary classification problems.
---

# AILS-NTUA at SemEval-2024 Task 9: Cracking Brain Teasers: Transformer Models for Lateral Thinking Puzzles

## Quick Facts
- arXiv ID: 2404.01084
- Source URL: https://arxiv.org/abs/2404.01084
- Reference count: 26
- Top-performing approaches secured competitive positions on the competition leaderboard, achieving average accuracy scores of 81.7% in the Sentence Puzzle and 85.4% in the Word Puzzle sub-tasks.

## Executive Summary
This paper presents the AILS-NTUA team's approach to the SemEval-2024 Task 9 competition on lateral thinking puzzles. The team evaluated various pre-trained transformer-based language models, including BERT, RoBERTa, DeBERTa, and large language models like Llama 2, Phi-2, and Mistral-7b, on two sub-tasks: Sentence Puzzle and Word Puzzle. By fine-tuning these models on the BrainTeaser datasets and transforming the multiple-choice questions into binary classification problems, the team achieved competitive performance on the competition leaderboard. Their best-performing approaches significantly outperformed the best neural baseline (ChatGPT) by more than 20% and 30% respectively.

## Method Summary
The authors fine-tuned pre-trained transformer-based language models on the BrainTeaser datasets for the SemEval-2024 Task 9 competition. They transformed the multiple-choice questions into binary classification problems by pairing each candidate answer with the question and assigning a label of 0 for incorrect choices and 1 for the correct choice. The team also explored pre-training encoders on commonsense reasoning datasets before fine-tuning on the BrainTeaser data. For large language models, they used a LoRA-based approach with a specific prompt format. The models were evaluated using instance-based and group-based accuracy metrics.

## Key Results
- The best-performing approaches achieved an average accuracy of 81.7% in the Sentence Puzzle sub-task and 85.4% in the Word Puzzle sub-task.
- The team's approaches significantly outperformed the best neural baseline (ChatGPT) by more than 20% and 30% respectively.
- Pre-training encoders on commonsense reasoning datasets resulted in substantial performance enhancements.
- The binary classification approach yielded lower performance compared to the multi-class classification approach.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning smaller encoder models and LLMs using in-domain data outperforms large-scale training and prompting.
- Mechanism: Leveraging transfer learning techniques starting from smaller models based on masked language modeling, such as BERT and BERT-based encoders, followed by similar techniques on LLMs.
- Core assumption: Large-scale training and prompting may not always serve as universally applicable solutions towards flexible reasoning.
- Evidence anchors:
  - [abstract] "Our top-performing approaches secured competitive positions on the competition leaderboard across both sub-tasks. In the evaluation phase, our best submission attained an average accuracy score of 81.7% in the Sentence Puzzle, and 85.4% in the Word Puzzle, significantly outperforming the best neural baseline (ChatGPT) by more than 20% and 30% respectively."
  - [section] "Thus, assuming that large-scale training and prompting may not always serve as universally applicable solutions towards flexible reasoning, we move one step back and leverage transfer learning techniques starting from smaller models based on masked language modelling, such as BERT (Devlin et al., 2019) and consequent BERT-based encoders."

### Mechanism 2
- Claim: Transforming the multiple-choice problem into a binary classification problem allows the model to explore diverging reasoning paths.
- Mechanism: Each sample underwent transformation by pairing each candidate answer (excluding "None of above") with the question, receiving label 0 if incorrect or 1 if correct.
- Core assumption: The diverse options provide crucial context to the models, enabling them to understand the reasoning paths better.
- Evidence anchors:
  - [section] "Binary Classification task Each sample originally consisting of multiple-choice QAs with four available options, underwent the following transformation: each candidate answer (excluding the 'None of above' option) was paired with the question receiving the label 0 if the choice was incorrect, or the label 1 for the opposite. In case all the 3 pairings returned 0, it is directly implied that 'None of above' is the correct answer."

### Mechanism 3
- Claim: Pre-training encoders across various commonsense reasoning datasets results in substantial performance enhancements.
- Mechanism: Using pre-trained models that have undergone additional pre-training using supplementary commonsense reasoning datasets before fine-tuning on BrainTeaser data.
- Core assumption: Pre-training on related datasets equips the model with the ability to understand reasoning complexities similar to those found in the target task.
- Evidence anchors:
  - [section] "Initially, it becomes apparent that pre-training encoders across various commonsense reasoning datasets results in substantial performance enhancements, as it enables the system to grasp domain-agnostic features which prove advantageous for the subsequent task."

## Foundational Learning

- Concept: Lateral thinking
  - Why needed here: The BrainTeaser task requires models to think "out-of-the-box" and defy default senses of concepts and common associations.
  - Quick check question: What is the key novelty of the BrainTeaser task that makes it challenging for models?

- Concept: Transfer learning
  - Why needed here: Transfer learning allows the models to leverage knowledge gained from pre-training on related tasks to improve performance on the target task.
  - Quick check question: How does transfer learning help in improving the performance of models on the BrainTeaser task?

- Concept: Binary classification vs. multi-class classification
  - Why needed here: The authors explore both binary and multi-class classification approaches to understand their impact on the model's performance.
  - Quick check question: Why does the binary classification approach yield lower performance compared to the multi-class classification approach in this task?

## Architecture Onboarding

- Component map: Pre-trained transformer models -> Fine-tuning pipeline with in-domain data -> Binary classification transformation (optional) -> Prompting mechanism for LLMs -> Evaluation metrics

- Critical path: Load pre-trained model -> Fine-tune model using in-domain data -> Transform multiple-choice questions into binary classification format (optional) -> Generate predictions using fine-tuned model -> Evaluate model performance using instance-based and group-based accuracy metrics

- Design tradeoffs:
  - Encoder models vs. LLMs: Encoder models offer faster inference and require less computational resources, while LLMs may capture more complex reasoning patterns but at the cost of increased computational requirements.
  - Binary classification vs. multi-class classification: Binary classification simplifies the problem but may lose important context, while multi-class classification preserves context but increases the complexity of the problem.

- Failure signatures:
  - Low performance on either sub-task: Indicates that the model is not capturing the reasoning patterns specific to that sub-task.
  - High variance in instance-based and group-based accuracy: Suggests that the model is inconsistent in its reasoning and may be overfitting to certain patterns.
  - Poor performance on semantic and context reconstruction: Indicates that the model is not generalizing well to variations in the input.

- First 3 experiments:
  1. Fine-tune a pre-trained BERT model on the BrainTeaser dataset using the multi-class classification approach.
  2. Fine-tune a pre-trained RoBERTa model on the BrainTeaser dataset using the binary classification approach.
  3. Fine-tune a pre-trained Llama 2 model on the BrainTeaser dataset using the multi-class classification approach with different LoRA hyperparameters (r and a).

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of pre-trained models on commonsense reasoning datasets compare to those trained specifically on brain teaser datasets?
- Basis in paper: [explicit] The authors mention that pre-training on various commonsense reasoning datasets results in substantial performance enhancements, but also note that other than WinoGrande, extra pre-training datasets do not significantly improve overall performance for encoders.
- Why unresolved: The paper does not provide a direct comparison between models pre-trained on commonsense reasoning datasets versus those trained specifically on brain teaser datasets.
- What evidence would resolve it: Conducting experiments with models pre-trained on brain teaser datasets and comparing their performance to those pre-trained on commonsense reasoning datasets would provide insights into the effectiveness of each approach.

### Open Question 2
- Question: What is the impact of model size on the reasoning abilities of language models in lateral thinking puzzles?
- Basis in paper: [explicit] The authors examine the effect of model size on their task, noting that Mistral-7b outperforms larger models like Llama 2 and Mixtral-8x7b in both sub-tasks.
- Why unresolved: The paper does not explore the full range of model sizes or provide a detailed analysis of the relationship between model size and reasoning abilities in lateral thinking puzzles.
- What evidence would resolve it: Conducting experiments with a wider range of model sizes and analyzing their performance on lateral thinking puzzles would provide a clearer understanding of the impact of model size on reasoning abilities.

### Open Question 3
- Question: How do different fine-tuning strategies, such as LoRA and QLoRA, affect the performance of language models on lateral thinking puzzles?
- Basis in paper: [explicit] The authors use LoRA and QLoRA for fine-tuning their models, but do not provide a detailed comparison of their effectiveness.
- Why unresolved: The paper does not explore the impact of different fine-tuning strategies on the performance of language models in lateral thinking puzzles.
- What evidence would resolve it: Conducting experiments with different fine-tuning strategies and comparing their performance on lateral thinking puzzles would provide insights into the effectiveness of each approach.

## Limitations

- Hyperparameter transparency: The paper does not provide complete details about the specific pre-trained model variants used or their exact hyperparameters.
- Data distribution: With only 169 Sentence Puzzle and 132 Word Puzzle training examples, the small sample size raises questions about whether the performance gains would scale to larger, more diverse datasets.
- Prompt formulation ambiguity: While the paper mentions using prompts for LLM fine-tuning, the exact prompt format and content are not fully specified.

## Confidence

- High Confidence: The claim that fine-tuning transformer models on the BrainTeaser task significantly outperforms the ChatGPT baseline is well-supported by the reported leaderboard results and the substantial performance gaps (>20-30%).
- Medium Confidence: The assertion that smaller encoder models outperform large-scale LLMs on this task through transfer learning is plausible given the results, but the small sample size and lack of detailed hyperparameter information limit confidence in replicating these specific outcomes.
- Low Confidence: The specific mechanisms by which the binary classification transformation and pre-training on commonsense reasoning datasets contribute to performance improvements are not rigorously validated through ablation studies.

## Next Checks

1. Conduct an ablation study removing the binary classification transformation and commonsense pre-training to isolate their individual contributions to performance gains.

2. Evaluate model performance on incrementally larger subsets of the BrainTeaser data (e.g., 50%, 75%, 100%) to assess how performance scales with training data size.

3. Test the fine-tuned models on related lateral thinking or commonsense reasoning datasets not seen during training to evaluate whether the learned reasoning patterns transfer beyond the specific BrainTeaser task.