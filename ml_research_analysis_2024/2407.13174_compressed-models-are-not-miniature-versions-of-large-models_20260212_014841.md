---
ver: rpa2
title: Compressed models are NOT miniature versions of large models
arxiv_id: '2407.13174'
source_url: https://arxiv.org/abs/2407.13174
tags:
- compressed
- data
- dataset
- bert
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper challenges the common assumption that compressed neural
  models are miniature versions of their larger counterparts. The authors compare
  BERT-large with five compressed variants (BERT-base, Distil-BERT, BERT-medium, BERT-mini,
  Tiny-BERT) across four characteristics: prediction errors, data representation,
  data distribution, and vulnerability to adversarial attacks.'
---

# Compressed models are NOT miniature versions of large models

## Quick Facts
- arXiv ID: 2407.13174
- Source URL: https://arxiv.org/abs/2407.13174
- Authors: Rohit Raj Rai; Rishant Pal; Amit Awekar
- Reference count: 25
- Compressed models show substantial differences from large models in prediction errors, data representation, and adversarial vulnerability

## Executive Summary
This paper challenges the common assumption that compressed neural models are simply scaled-down versions of their larger counterparts. The authors systematically compare BERT-large with five compressed variants (BERT-base, Distil-BERT, BERT-medium, BERT-mini, Tiny-BERT) across four key characteristics: prediction errors, data representation, data distribution, and vulnerability to adversarial attacks. Their comprehensive evaluation reveals significant differences across all dimensions, with compressed models exhibiting substantially lower similarity to BERT-large in terms of prediction patterns, internal representations, and robustness to adversarial attacks. The findings demonstrate that compressed models should not be assumed to behave like miniature versions of larger models and require independent evaluation before deployment.

## Method Summary
The study evaluates five compressed BERT variants against BERT-large using SQUAD2 for fine-tuning and IMDB reviews for adversarial testing. The comparison spans four characteristics: prediction errors (using Jaccard similarity coefficients), data representation (embedding space similarity), data distribution (OOD detection capability), and adversarial attack vulnerability (using TextBugger attacks). Each compressed model is assessed across these dimensions to quantify similarity to BERT-large and identify systematic differences in behavior.

## Key Results
- Compressed models show low similarity in prediction errors with BERT-large, with Jaccard coefficients ranging from 0.22-0.528
- Data representation similarity between compressed models and BERT-large is minimal, with mean Jaccard values of 0.03-0.05
- Compressed models are significantly more vulnerable to adversarial attacks, with Tiny-BERT showing 87% accuracy degradation versus 40% for BERT-large
- Out-of-distribution detection capability varies substantially across compressed models compared to BERT-large

## Why This Works (Mechanism)
None

## Foundational Learning
- **Model compression techniques** - Various methods to reduce model size while maintaining performance
  - Why needed: Understanding how different compression approaches affect model behavior
  - Quick check: Review pruning, quantization, and knowledge distillation methods
- **Jaccard similarity coefficient** - Statistical measure of similarity between finite sample sets
  - Why needed: Used to quantify similarity between prediction patterns and representations
  - Quick check: Understand J=0 (no similarity) to J=1 (perfect similarity) scale
- **Adversarial attacks in NLP** - Methods to generate malicious inputs that cause model misclassification
  - Why needed: Evaluating model robustness to security threats
  - Quick check: Review TextBugger and other common NLP attack methods

## Architecture Onboarding
- **Component map**: BERT-large → Compression Techniques → Compressed Models (BERT-base, Distil-BERT, BERT-medium, BERT-mini, Tiny-BERT)
- **Critical path**: Model training → Fine-tuning on SQUAD2 → Evaluation across four characteristics → Adversarial testing
- **Design tradeoffs**: Model size vs performance vs robustness; accuracy vs computational efficiency
- **Failure signatures**: Low Jaccard similarity in predictions, poor OOD detection, high vulnerability to adversarial attacks
- **First experiments**: 1) Compute prediction error similarity between compressed and large models 2) Measure embedding space similarity across layers 3) Test OOD detection performance

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis focuses on specific BERT compression techniques, limiting generalizability to other architectures
- Adversarial attack evaluation uses single attack type and dataset
- Comparison limited to English-language NLP tasks, leaving questions about performance across languages and domains

## Confidence
- High confidence: Compressed models exhibit different prediction error patterns from BERT-large
- High confidence: Data representation dissimilarity results are robust across all compressed models
- Medium confidence: Adversarial vulnerability results may be influenced by specific attack method used

## Next Checks
1. Replicate analysis using different compression techniques (pruning, quantization) and model architectures
2. Conduct adversarial robustness testing using multiple attack methods and datasets
3. Evaluate compressed models on cross-lingual and multi-domain datasets