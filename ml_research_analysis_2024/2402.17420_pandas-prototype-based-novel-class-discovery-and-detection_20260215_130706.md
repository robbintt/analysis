---
ver: rpa2
title: 'PANDAS: Prototype-based Novel Class Discovery and Detection'
arxiv_id: '2402.17420'
source_url: https://arxiv.org/abs/2402.17420
tags:
- novel
- classes
- pandas
- base
- discovery
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes PANDAS, a prototype-based method for novel
  class discovery and detection in object detection. PANDAS extends a detector trained
  on base classes to automatically discover and detect novel classes from unlabeled
  data.
---

# PANDAS: Prototype-based Novel Class Discovery and Detection

## Quick Facts
- **arXiv ID**: 2402.17420
- **Source URL**: https://arxiv.org/abs/2402.17420
- **Reference count**: 23
- **Primary result**: Achieves 12% higher mAP on novel classes than RNCDL on VOC while running 36x faster

## Executive Summary
PANDAS is a novel class discovery and detection method that extends object detectors trained on base classes to automatically discover and detect novel classes from unlabeled data. The method computes prototypes for base classes from labeled data, then discovers clusters and their associated prototypes to represent novel classes from unlabeled data. During inference, a distance-based classifier uses these prototypes to assign labels to detected objects. PANDAS demonstrates state-of-the-art performance on both VOC 2012 and COCO-to-LVIS benchmarks while being significantly more computationally efficient than existing methods.

## Method Summary
PANDAS operates in two phases: base phase and discovery phase. In the base phase, it trains a Faster R-CNN detector on base classes using a class-agnostic regression head. It then computes prototypes for each base class by extracting features from ground truth boxes and averaging them. In the discovery phase, it runs the frozen detector on unlabeled data, extracts RPN features, and applies k-means clustering to discover novel class prototypes. During inference, it computes similarity scores between detected object features and both base and novel class prototypes, using a background classifier for objects that don't match well with any prototype.

## Key Results
- Achieves 12% higher mAP on novel classes than RNCDL on VOC 2012 benchmark
- Runs 36x faster than RNCDL while maintaining superior performance
- Achieves 2.5% higher mAP on novel classes on COCO-to-LVIS benchmark
- Outperforms state-of-the-art methods without requiring privileged information like number of novel classes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: PANDAS achieves strong performance by using a distance-based classifier with prototypes that lie in the same feature space for both base and novel classes.
- **Mechanism**: The method computes prototypes for base classes from labeled data, then discovers clusters and their associated prototypes to represent novel classes from unlabeled data. During inference, it computes similarity scores between detected object features and these prototypes to assign labels.
- **Core assumption**: A simple distance-based classifier operating on prototypes can effectively distinguish between base and novel classes without requiring separate classifiers or complex architectures.
- **Evidence anchors**:
  - [abstract]: "During inference, a distance-based classifier uses these prototypes to assign a label to each detected object instance."
  - [section]: "PANDAS not only outperforms RNCDL across various numbers of novel clusters, but also requires much less time."
- **Break condition**: If the feature space doesn't adequately separate base and novel classes, or if the prototype computation fails to capture meaningful class representations.

### Mechanism 2
- **Claim**: PANDAS maintains performance on base classes while discovering novel classes by keeping base class prototypes fixed during discovery.
- **Mechanism**: Base class prototypes are computed once during the base phase and stored. During discovery, only novel class clusters are updated, while the base classifier continues to use the original base prototypes.
- **Core assumption**: Storing base prototypes and not updating them during discovery prevents catastrophic forgetting of base class knowledge.
- **Evidence anchors**:
  - [abstract]: "PANDAS, a method for novel class discovery and detection. It discovers clusters representing novel classes from unlabeled data, and represents old and new classes with prototypes."
  - [section]: "Note that our method stores only base class prototypes, and not the entire base dataset. This is in contrast to existing methods which require storing the entire base set, and is beneficial as it reduces memory requirements and potentially reduces privacy concerns."
- **Break condition**: If the base prototypes become outdated or if the discovery process significantly shifts the feature space representation.

### Mechanism 3
- **Claim**: PANDAS is computationally efficient because it doesn't require self-supervised training or storing the entire base dataset during discovery.
- **Mechanism**: The method uses k-means clustering for novel class discovery and only stores base class prototypes rather than the entire base dataset. It avoids the need for self-supervised training or memory modules.
- **Core assumption**: Simpler clustering-based approaches can achieve comparable or better performance than complex self-supervised methods while being more computationally efficient.
- **Evidence anchors**:
  - [abstract]: "The simplicity of our method makes it widely applicable."
  - [section]: "PANDAS not only outperforms RNCDL across various numbers of novel clusters, but also requires much less time."
- **Break condition**: If the clustering quality degrades significantly or if the prototype-based approach fails to capture necessary class distinctions.

## Foundational Learning

- **Object detection fundamentals**: Understanding R-CNN architecture, RPN, and RoI pooling is essential as PANDAS builds on Faster R-CNN.
  - *Why needed*: The paper modifies Faster R-CNN components for novel class discovery.
  - *Quick check*: What are the two main stages of Faster R-CNN and how do they work together?

- **Prototype-based classification**: Understanding how class prototypes are computed from feature representations.
  - *Why needed*: The method represents both base and novel classes with prototypes and uses distance-based classification.
  - *Quick check*: How are class prototypes computed from feature representations?

- **Clustering algorithms**: Understanding k-means clustering parameters and behavior.
  - *Why needed*: The method uses k-means to discover novel class clusters from unlabeled data.
  - *Quick check*: What are the key hyper-parameters of k-means clustering and how do they affect the results?

## Architecture Onboarding

- **Component map**: Base phase → Prototype computation → Discovery phase → Inference with distance-based classifier
- **Critical path**: Base phase → Prototype computation → Discovery phase → Inference with distance-based classifier
- **Design tradeoffs**:
  - Using fixed base prototypes vs. updating them during discovery
  - Number of clusters in k-means vs. computational cost
  - Choice of similarity metric (Euclidean distance vs. cosine similarity)
- **Failure signatures**:
  - Poor performance on novel classes suggests inadequate clustering or feature representation
  - Degradation on base classes indicates feature space shift or prototype quality issues
  - Computational inefficiency suggests sub-optimal clustering parameters
- **First 3 experiments**:
  1. Test prototype quality: Compute mAP on base classes using ground truth prototypes vs. k-means clusters for base classes
  2. Validate clustering: Vary number of clusters and measure impact on novel class discovery performance
  3. Assess background classifier: Compare performance with and without background classifier on small dataset

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: How does the performance of PANDAS scale with the number of base classes, particularly when the number of base classes is much larger than in the VOC benchmark?
- **Basis in paper**: [inferred] The paper primarily tests on VOC with 10 base classes, but does not explore performance with a significantly larger number of base classes.
- **Why unresolved**: The paper focuses on VOC and COCO-to-LVIS benchmarks but does not vary the number of base classes within these datasets to study scalability.
- **What evidence would resolve it**: Experiments on datasets with a larger number of base classes, such as LVIS, would provide insights into scalability and performance.

### Open Question 2
- **Question**: What is the impact of different backbone architectures on the performance of PANDAS, especially more advanced ones like Swin Transformer or ConvNeXt?
- **Basis in paper**: [inferred] The paper uses ResNet-50 as the backbone and does not explore the impact of using different backbone architectures.
- **Why unresolved**: The paper does not include experiments with alternative backbone architectures to assess their impact on performance.
- **What evidence would resolve it**: Comparative experiments using different backbone architectures would show how performance varies with backbone choice.

### Open Question 3
- **Question**: How does PANDAS handle the discovery of novel classes in highly imbalanced datasets, and what strategies could improve its performance in such scenarios?
- **Basis in paper**: [explicit] The paper mentions that LVIS is long-tailed and that RNCDL benefits from assuming a long-tailed prior, but PANDAS does not make such assumptions.
- **Why unresolved**: While the paper shows that PANDAS performs well without assuming a long-tailed prior, it does not explore strategies to improve performance on highly imbalanced datasets.
- **What evidence would resolve it**: Experiments that test PANDAS on various imbalanced datasets and incorporate strategies like class-balanced sampling or re-weighting would provide insights into handling imbalance.

## Limitations

- Computational efficiency claims lack detailed ablation studies showing the impact of individual components on both accuracy and speed
- The method assumes the feature space adequately separates base and novel classes, which may not hold in all scenarios
- Background classifier integration details and exact k-means hyperparameter choices are not fully specified

## Confidence

- **High**: The core mechanism of using prototypes for distance-based classification and the overall task formulation
- **Medium**: The computational efficiency claims and comparison to RNCDL, given limited ablation studies
- **Low**: The background classifier integration details and exact k-means hyperparameter choices

## Next Checks

1. Perform controlled experiments varying k-means cluster counts (5-20) to identify optimal cluster numbers for novel class discovery performance
2. Conduct feature space analysis comparing base and novel class distributions to validate the assumption that a single distance metric works across both class types
3. Implement ablation studies isolating the impact of the background classifier by comparing full PANDAS against a variant without background handling on a subset of classes