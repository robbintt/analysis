---
ver: rpa2
title: 'HGTDR: Advancing Drug Repurposing with Heterogeneous Graph Transformers'
arxiv_id: '2405.08031'
source_url: https://arxiv.org/abs/2405.08031
tags:
- drug
- protein
- graph
- disease
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: HGTDR is an end-to-end framework for drug repurposing using heterogeneous
  graph transformers. It constructs a comprehensive knowledge graph from multiple
  sources, applies HGT to extract node features while preserving heterogeneity, and
  uses a fully connected network to predict drug-disease indication scores.
---

# HGTDR: Advancing Drug Repurposing with Heterogeneous Graph Transformers

## Quick Facts
- arXiv ID: 2405.08031
- Source URL: https://arxiv.org/abs/2405.08031
- Authors: Ali Gharizadeh; Karim Abbasi; Amin Ghareyazi; Mohammad R. K. Mofrad; Hamid R. Rabiee
- Reference count: 19
- Primary result: HGTDR achieves AUROC of 0.944 and AUPR of 0.946 on drug repurposing task

## Executive Summary
HGTDR is an end-to-end framework for drug repurposing that leverages heterogeneous graph transformers to process multi-relational biological knowledge graphs. The method constructs a comprehensive knowledge graph from multiple sources, applies HGT to extract node features while preserving heterogeneity, and uses a fully connected network to predict drug-disease indication scores. Evaluated via 5-fold cross-validation, HGTDR achieves state-of-the-art performance with AUROC of 0.944 and AUPR of 0.946, demonstrating its effectiveness for drug repurposing and other biological relation prediction tasks.

## Method Summary
HGTDR constructs a heterogeneous knowledge graph from multiple biological data sources, then applies a heterogeneous graph transformer network to extract node features. The model uses a masking strategy where 80% of indication edges are kept in the graph while 20% are masked and used as positive samples during training, along with randomly sampled negative edges. An end-to-end architecture allows joint learning of feature extraction and link prediction, with the HGT layers producing node embeddings that are directly fed into a fully connected network for prediction. The entire model is trained using binary cross-entropy loss on the masked indications and negative samples.

## Key Results
- HGTDR achieves AUROC of 0.944 and AUPR of 0.946 on drug repurposing task
- Successfully predicts other biological relations including drug-protein and disease-protein interactions
- Shows robustness to input variations and maintains strong performance across different relation types
- Experimental validation supports several novel drug repurposing predictions

## Why This Works (Mechanism)

### Mechanism 1
- Claim: HGTDR's use of heterogeneous graph transformers (HGT) allows it to capture complex, multi-relational patterns in biological networks without requiring manual feature engineering.
- Mechanism: HGT uses node- and edge-specific attention weights to differentiate how each type of node and edge contributes to the representation, enabling it to learn both common and unique patterns across different biological relationships.
- Core assumption: The heterogeneity of the graph contains predictive information about drug-disease indications.
- Evidence anchors: The abstract describes HGTDR as utilizing heterogeneous graph transformer networks, and the methodology section explains the heterogeneous mutual attention mechanism that calculates attention grounded by meta-relations.

### Mechanism 2
- Claim: HGTDR's end-to-end architecture allows the model to jointly learn feature extraction and link prediction, avoiding manual intermediate steps.
- Mechanism: The HGT layers produce node embeddings that are directly fed into a fully connected network for link prediction, all trained together using a binary cross-entropy loss.
- Core assumption: Joint training improves feature quality for the downstream task compared to separate steps.
- Evidence anchors: The abstract explicitly states HGTDR is an end-to-end framework, and the methodology confirms the HGT network and link-predicting network are trained as an end-to-end model.

### Mechanism 3
- Claim: Masking a subset of indication edges during training simulates the drug repurposing task and encourages the model to generalize to novel indications.
- Mechanism: 80% of indication edges are kept in the graph as input, while 20% are masked and used as positive samples in the loss function along with randomly sampled negative edges.
- Core assumption: The model can learn to predict indications not present in the input graph, mimicking the real-world task of discovering new drug uses.
- Evidence anchors: The methodology section describes the masking strategy where 20 percent of indications are put in the graph and the rest are used for prediction, and the results section reports strong performance metrics.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs)
  - Why needed here: GNNs are the backbone of HGTDR, enabling it to process graph-structured biological data and extract meaningful node features.
  - Quick check question: Can you explain the difference between a graph convolutional network and a graph attention network?

- Concept: Heterogeneous Graphs
  - Why needed here: HGTDR operates on knowledge graphs with multiple node and edge types, requiring an understanding of how to handle heterogeneity in graph neural networks.
  - Quick check question: What is a meta-relation in a heterogeneous graph, and why is it important for HGTDR?

- Concept: Attention Mechanisms
  - Why needed here: Attention mechanisms in HGT allow the model to weigh the importance of different neighbors and edge types when aggregating information for each node.
  - Quick check question: How does multi-head attention in HGT differ from single-head attention in standard Transformers?

## Architecture Onboarding

- Component map: PrimeKG knowledge graph -> BioBERT/ChemBERTa embeddings -> HGT layers (3 layers, 64 dimensions, 8 attention heads) -> Fully connected network (128→64→1) -> Link prediction scores

- Critical path:
  1. Construct heterogeneous knowledge graph from multiple biological data sources
  2. Add initial embeddings using BioBERT for non-drug entities and ChemBERTa for drugs
  3. Sample subgraph with HGSampling (depth 3, batch size 164)
  4. Apply HGT layers to extract node features with heterogeneous attention
  5. Concatenate features and predict drug-disease scores using fully connected network
  6. Compute binary cross-entropy loss on masked indications and negative samples
  7. Update model parameters using AdamW optimizer with cosine annealing learning rate

- Design tradeoffs:
  - Using HGT instead of HAN: HGT does not require predefined meta-paths, making it more flexible but potentially less interpretable
  - Masking 20% of indications: Balances the need for training signal with the goal of simulating drug repurposing
  - Sampling depth of 3: Captures relevant neighborhood information while keeping computation manageable

- Failure signatures:
  - Low AUROC/AUPR: Could indicate poor feature extraction, insufficient training data, or an imbalanced loss function
  - High training loss, low validation loss: Possible overfitting, consider regularization or more data
  - High validation loss, low training loss: Possible underfitting, consider more model capacity or better initialization

- First 3 experiments:
  1. Replace HGT layers with HAN layers and compare performance to confirm the advantage of HGT
  2. Vary the proportion of masked indications (e.g., 10%, 50%, 80%) and measure the impact on AUROC/AUPR
  3. Remove one type of relation (e.g., drug-protein) from the input graph and evaluate robustness of the model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can the method be extended to incorporate edge-specific side information in addition to node embeddings?
- Basis in paper: The paper states that the current method only allows adding side information to nodes, not edges, and mentions that some information relates to edges rather than nodes.
- Why unresolved: The paper acknowledges this limitation but does not propose a solution for incorporating edge-specific information.
- What evidence would resolve it: A proposed method for adding edge-specific embeddings or features to the knowledge graph, along with experimental results showing improved performance.

### Open Question 2
- Question: What is the impact of different sampling strategies (beyond HGSampling) on the scalability and performance of HGTDR?
- Basis in paper: The paper uses HGSampling for scalability but does not compare it with other sampling methods or explore the impact of different sampling strategies.
- Why unresolved: The paper does not provide a comprehensive comparison of different sampling strategies.
- What evidence would resolve it: A comparison of HGTDR's performance using different sampling methods (e.g., node-wise sampling, layer-wise sampling) with varying sampling depths and batch sizes.

### Open Question 3
- Question: How can the interpretability of the end-to-end HGTDR model be improved to provide insights into the drug repurposing predictions?
- Basis in paper: The paper mentions that the method focuses on predicting relationships numerically without the capacity to substantiate these predictions with empirical evidence or elucidate the drug's mechanism of action.
- Why unresolved: The paper does not propose methods to improve the interpretability of the model's predictions.
- What evidence would resolve it: A proposed method for interpreting the model's predictions, such as attention visualization, feature importance analysis, or causal inference techniques, along with case studies demonstrating improved interpretability.

## Limitations

- The masking strategy and negative sampling approach, while reasonable, are not extensively validated across different proportions or sampling schemes
- The model focuses on predicting relationships numerically without the capacity to substantiate these predictions with empirical evidence or elucidate the drug's mechanism of action
- The method only allows adding side information to nodes, not edges, which limits the incorporation of edge-specific biological information

## Confidence

- **High confidence** in the mechanism that heterogeneous graph transformers can capture multi-relational patterns better than homogeneous approaches
- **Medium confidence** in the claim that end-to-end training improves feature quality
- **Low confidence** in the robustness claims without explicit ablation studies removing specific relation types

## Next Checks

1. Conduct ablation studies removing each relation type (e.g., drug-protein, disease-protein) from the input graph to quantify their contribution to performance
2. Compare HGTDR's masking strategy with alternative proportions (10%, 50%, 80%) and negative sampling approaches to assess sensitivity
3. Evaluate HGTDR on a held-out test set from a different knowledge graph or real-world drug repurposing scenario to validate generalization beyond PrimeKG