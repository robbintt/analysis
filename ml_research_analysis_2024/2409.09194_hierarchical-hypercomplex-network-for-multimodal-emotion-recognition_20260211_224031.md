---
ver: rpa2
title: Hierarchical Hypercomplex Network for Multimodal Emotion Recognition
arxiv_id: '2409.09194'
source_url: https://arxiv.org/abs/2409.09194
tags:
- hypercomplex
- layers
- emotion
- recognition
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses the challenge of multimodal emotion recognition
  using physiological signals such as EEG, ECG, GSR, and eye data, which offer more
  reliable emotion indicators than behavioral signals. The core idea is to develop
  a Hierarchical Hypercomplex (H2) network that leverages hypercomplex algebra to
  capture both intra-modal (within each modality) and inter-modal (across modalities)
  correlations.
---

# Hierarchical Hypercomplex Network for Multimodal Emotion Recognition

## Quick Facts
- arXiv ID: 2409.09194
- Source URL: https://arxiv.org/abs/2409.09194
- Authors: Eleonora Lopez; Aurelio Uncini; Danilo Comminiello
- Reference count: 0
- Primary result: Achieves state-of-the-art F1-scores (40.20% arousal, 57.11% valence) on MAHNOB-HCI dataset

## Executive Summary
This paper introduces a Hierarchical Hypercomplex (H2) network for multimodal emotion recognition using physiological signals (EEG, ECG, GSR, eye data). The model leverages hypercomplex algebra through parameterized hypercomplex convolutions (PHCs) to capture intra-modal channel relationships and parameterized hypercomplex multiplications (PHMs) for inter-modal fusion. The hierarchical structure first learns modality-specific embeddings before fusing them, achieving significant improvements over existing approaches on the MAHNOB-HCI dataset.

## Method Summary
The H2 network processes multimodal physiological signals through modality-specific encoders using PHCs to capture intra-channel relationships, followed by a hypercomplex fusion module with PHMs to combine embeddings across modalities. The model is trained on the MAHNOB-HCI dataset with 27 participants, using EEG (10 electrodes), ECG, GSR, and eye movement data segmented into 10-second trials. Training employs Adam optimizer with cross-entropy loss, once-cycle policy, and early stopping.

## Key Results
- Achieves 40.20% improvement in F1-score for arousal classification compared to existing models
- Achieves 57.11% improvement in F1-score for valence classification compared to existing models
- Demonstrates state-of-the-art performance on the MAHNOB-HCI dataset

## Why This Works (Mechanism)

### Mechanism 1
- Hypercomplex convolutions (PHCs) improve emotion recognition by explicitly modeling intra-channel relationships within each physiological modality through Kronecker product weight matrix construction.
- Core assumption: Physiological signals have structured inter-channel relationships carrying discriminative information.
- Break condition: No benefit if modalities are uncorrelated or have single channels.

### Mechanism 2
- Hierarchical learning structure separates intra-modal and inter-modal feature learning, leading to better generalization by preventing early mixing of different correlation types.
- Core assumption: Emotion-relevant information is structured hierarchically with some features within modalities and others across modalities.
- Break condition: No benefit if emotion recognition relies primarily on either intra-modal or inter-modal features alone.

### Mechanism 3
- Parameterized hypercomplex multiplications (PHMs) improve cross-modal fusion by modeling complex inter-modal relationships through learnable non-linear transformations.
- Core assumption: Different physiological modalities provide complementary information about emotional states.
- Break condition: No benefit if modalities are redundant or simple concatenation would suffice.

## Foundational Learning

- Hypercomplex algebra and parameterized hypercomplex neural networks (PHNNs):
  - Why needed here: Reduces parameters while capturing complex relationships within and across modalities
  - Quick check question: How does the Kronecker product construction in PHCs allow learning of inter-channel relationships?

- Multimodal emotion recognition and physiological signals:
  - Why needed here: Addresses emotion recognition using EEG, ECG, GSR, and eye data
  - Quick check question: What are the key differences between physiological signals and behavioral signals (like facial expressions) in the context of emotion recognition?

- Hierarchical neural network architectures:
  - Why needed here: H2 model has hierarchical structure with separate encoder and fusion stages
  - Quick check question: What are the advantages of separating intra-modal and inter-modal learning in a multimodal network?

## Architecture Onboarding

- Component map: Raw multimodal signals → modality-specific encoders → hypercomplex fusion module → dense layer → emotion prediction

- Critical path: Raw signals processed through modality-specific encoders with PHCs/PHMs → fusion module combines embeddings → dense layer produces arousal/valence prediction

- Design tradeoffs:
  - Parameter reduction vs. expressiveness: PHCs/PHMs reduce parameters by 1/n but may limit capacity; structured relationships compensate
  - Hierarchical vs. joint learning: Improves generalization but may miss early cross-modal interactions

- Failure signatures:
  - Overfitting: Large gap between training and validation performance
  - Underfitting: Poor performance on both training and test data
  - Poor cross-modal fusion: Failure to effectively combine modality information

- First 3 experiments:
  1. Ablation study: Replace PHCs with standard convolutions in encoders and compare performance
  2. Ablation study: Remove hierarchical structure and process raw modalities jointly, then compare
  3. Hyperparameter sensitivity analysis: Vary n parameter in PHCs/PHMs and observe impact on performance and model size

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed hierarchical structure compare to a single-modality approach in terms of computational efficiency and performance?
- Basis in paper: [inferred] Paper demonstrates hierarchical structure effectiveness but doesn't compare to single-modality approaches
- Why unresolved: Paper focuses on superiority over multimodal approaches, not single-modality comparisons
- What evidence would resolve it: Experiments comparing hierarchical structure with single-modality models on computational efficiency and performance metrics

### Open Question 2
- Question: Can the proposed model be extended to other types of physiological signals beyond EEG, ECG, GSR, and eye data?
- Basis in paper: [explicit] Mentions use of specific physiological signals but doesn't discuss application to other signals
- Why unresolved: Paper doesn't provide insights into generalizability to other physiological signals
- What evidence would resolve it: Testing model on different physiological signals and evaluating performance

### Open Question 3
- Question: How does the model perform on datasets with different demographic characteristics or cultural backgrounds?
- Basis in paper: [inferred] Evaluates on MAHNOB-HCI dataset but doesn't discuss performance across diverse populations
- Why unresolved: Paper doesn't provide information on model performance across different demographics
- What evidence would resolve it: Experiments on datasets with diverse demographic characteristics to assess performance and generalizability

### Open Question 4
- Question: What is the impact of the number of channels in each modality on the model's performance?
- Basis in paper: [inferred] Mentions specific channels but doesn't discuss impact of channel numbers on performance
- Why unresolved: Paper doesn't provide insights into how channel numbers affect performance
- What evidence would resolve it: Experimenting with different channel numbers for each modality and evaluating performance

## Limitations
- Limited evaluation to one dataset with 27 participants, potentially missing emotional response variability
- Complex hypercomplex algebra implementation lacks detailed specifications for exact reproduction
- Effectiveness depends heavily on specific physiological modalities used (EEG, ECG, GSR, eye data)

## Confidence

**High Confidence**: Hierarchical structure's separation of intra-modal and inter-modal learning is well-supported by architectural design and experimental results showing improved performance over non-hierarchical baselines.

**Medium Confidence**: Parameterized hypercomplex convolutions effectively capture intra-channel relationships is theoretically sound but lacks direct empirical validation through ablation studies.

**Low Confidence**: Specific contributions of hypercomplex fusion module versus simpler fusion methods are not adequately tested, making it difficult to isolate true benefit of PHMs.

## Next Checks
1. Ablation study replacing PHM-based fusion with standard concatenation + fully connected layers to isolate hypercomplex multiplication contributions
2. Cross-dataset validation evaluating H2 model on DEAP dataset to test generalizability beyond MAHNOB-HCI
3. Hyperparameter sensitivity analysis varying n parameter in PHCs/PHMs across range (n=2, 5, 10, 20) to measure trade-off between parameter reduction and performance