---
ver: rpa2
title: LLM-Based Section Identifiers Excel on Open Source but Stumble in Real World
  Applications
arxiv_id: '2404.16294'
source_url: https://arxiv.org/abs/2404.16294
tags:
- section
- sections
- gpt-4
- clinical
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study evaluates large language models (LLMs) for section identification
  in electronic health records (EHR). While GPT-4 achieved near-perfect accuracy (96%)
  on open-source benchmark datasets, its performance dropped significantly (37%) on
  real-world, OCR-derived data due to noisy text and inconsistent formatting.
---

# LLM-Based Section Identifiers Excel on Open Source but Stumble in Real World Applications

## Quick Facts
- arXiv ID: 2404.16294
- Source URL: https://arxiv.org/abs/2404.16294
- Reference count: 7
- Large language models (LLMs) like GPT-4 achieve near-perfect accuracy (96%) on open-source EHR benchmarks but drop to 37% on real-world, OCR-derived data due to noise and formatting inconsistencies.

## Executive Summary
This study evaluates the performance of large language models, particularly GPT-4, for section identification in electronic health records (EHR). While GPT-4 demonstrates exceptional accuracy on clean, open-source benchmark datasets (MedSecID and i2b2), its performance significantly degrades on real-world, OCR-derived data due to noisy text and inconsistent formatting. The authors also create a more challenging real-world dataset and develop a comprehensive section ontology, highlighting the limitations of existing benchmarks in capturing real-world complexity. This work underscores the potential of LLMs for EHR segmentation while emphasizing the need for more robust evaluation methods and datasets that reflect real-world conditions.

## Method Summary
The study evaluates GPT-4, LLaMa-2, and Mistral models for section identification in EHR using zero-shot, few-shot, and chain-of-thought prompting strategies. Performance is benchmarked on clean datasets (i2b2 2010 with 96 documents and MedSecID with 2,002 documents) and compared to an internal real-world dataset of 100 OCR-derived documents. The models are evaluated using exact match accuracy, precision, recall, and F1-score, with results converted to IOB tagging scheme for consistency. Baseline methods like keyword search, regex, and MedSpacy are also compared.

## Key Results
- GPT-4 achieves 96% accuracy on clean benchmark datasets (MedSecID and i2b2) but drops to 37% on real-world, OCR-derived data.
- Zero-shot and few-shot prompting strategies provide marginal improvements over baseline methods, but performance remains sensitive to input quality and formatting consistency.
- The study highlights the need for more robust evaluation methods and datasets that reflect real-world complexity, as existing benchmarks fail to capture the challenges of noisy, unstructured EHR data.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4's zero-shot performance on clean EHR datasets is driven by its ability to generalize from massive pretraining over diverse web text.
- Mechanism: The model leverages its learned representations of medical terminology, formatting patterns, and semantic coherence to accurately segment EHR sections without requiring task-specific fine-tuning.
- Core assumption: The pretraining corpus contains sufficient examples of medical text structure and EHR-like documents.
- Evidence anchors:
  - [abstract] "GPT-4 can effectively solve the task on both zero and few-shot settings as well as segment dramatically better than state-of-the-art methods."
  - [section] "Large language models (LLMs) on the other hand, have performed impressive feats in natural language processing (NLP), that too in a zero-shot manner, i.e. without any labeled data."
  - [corpus] Weak - the corpus contains papers related to EHR and LLMs but no direct evidence of pretraining content or domain overlap.
- Break condition: Performance degrades significantly when faced with noisy, OCR-derived text and inconsistent formatting that deviates from pretraining patterns.

### Mechanism 2
- Claim: The disparity between benchmark and real-world performance is due to the fundamentally different data characteristics.
- Mechanism: Benchmark datasets (MedSecID, i2b2) are derived from clean, structured EHR systems with consistent formatting, while real-world data contains handwritten notes, OCR errors, and unstructured variations that challenge GPT-4's pattern recognition.
- Core assumption: The model's performance is sensitive to input quality and formatting consistency.
- Evidence anchors:
  - [abstract] "GPT-4 achieved near-perfect accuracy (96%) on open-source benchmark datasets, its performance dropped significantly (37%) on real-world, OCR-derived data due to noisy text and inconsistent formatting."
  - [section] "These documents lack a standardized structure, with segments and titles that can vary significantly in length."
  - [corpus] Missing - no corpus evidence specifically addressing the formatting differences between datasets.
- Break condition: The performance gap disappears when real-world data is preprocessed to match the clean formatting of benchmark datasets.

### Mechanism 3
- Claim: Few-shot and chain-of-thought prompting provide marginal improvements over zero-shot prompting.
- Mechanism: Providing examples and reasoning steps helps the model better understand the task structure and handle edge cases, but the fundamental limitations in handling noisy data remain.
- Core assumption: Additional context through examples can guide the model to better section identification.
- Evidence anchors:
  - [abstract] "We find that GPT-4 can effectively solve the task on both zero and few-shot settings as well as segment dramatically better than state-of-the-art methods."
  - [section] "Wang et al. (2023); Bian et al. (2023); Ashok and Lipton (2023) have shown the efficacy of prompting the LLM to extract biomedical named entities from scientific articles."
  - [corpus] Weak - the corpus contains related prompting research but no specific evidence about few-shot improvements for EHR section identification.
- Break condition: The performance gains from few-shot and CoT prompting are negligible when the input data is extremely noisy and unstructured.

## Foundational Learning

- Concept: Section identification as a span-level entity recognition task
  - Why needed here: Understanding that section identification is essentially classifying text spans into predefined categories helps frame the problem in terms of established NLP techniques and evaluation metrics.
  - Quick check question: How does the Inside-Outside-Beginning (IOB) tagging scheme apply to section identification in EHRs?

- Concept: Zero-shot vs few-shot learning
  - Why needed here: Recognizing the difference between zero-shot (no examples) and few-shot (with examples) prompting is crucial for understanding GPT-4's performance and the experimental design.
  - Quick check question: What is the key distinction between traditional ML models and LLMs that enables zero-shot performance?

- Concept: OCR and its impact on text quality
  - Why needed here: Understanding how optical character recognition introduces errors and inconsistencies is essential for interpreting the performance gap between benchmark and real-world datasets.
  - Quick check question: What are the primary sources of OCR errors in medical documents that could affect section identification?

## Architecture Onboarding

- Component map:
  - Input layer: Raw clinical text (clean EHR or OCR-derived)
  - Prompt engineering module: Template-based prompt construction (zero-shot, few-shot, CoT)
  - LLM inference engine: GPT-4 API calls with specified parameters
  - Post-processing layer: JSON parsing and IOB scheme conversion
  - Evaluation module: Exact match, precision, recall, F1-score calculation
  - Dataset management: Loading and preprocessing of MedSecID, i2b2, and real-world datasets

- Critical path:
  1. Load and preprocess input text
  2. Construct appropriate prompt (zero-shot/few-shot/CoT)
  3. Send request to GPT-4 API
  4. Parse JSON response into section headers
  5. Convert to IOB scheme for evaluation
  6. Calculate evaluation metrics against ground truth

- Design tradeoffs:
  - Accuracy vs cost: GPT-4 provides superior accuracy but at higher computational cost compared to open-source models like LLaMa-2
  - Zero-shot simplicity vs few-shot precision: Zero-shot is easier to implement but may miss edge cases that few-shot could capture
  - Exact match strictness vs semantic similarity: Exact match is straightforward but may not capture semantically equivalent section headers

- Failure signatures:
  - High precision but low recall: Model is conservative in identifying sections, missing many true sections
  - Low precision but high recall: Model is over-inclusive, identifying many false sections
  - Consistent performance drop on real-world data: Indicates sensitivity to input quality and formatting
  - Model output in unexpected format: Suggests prompt engineering issues or model misunderstanding

- First 3 experiments:
  1. Replicate zero-shot performance on MedSecID dataset to establish baseline
  2. Test zero-shot performance on i2b2 dataset to verify generalizability
  3. Apply zero-shot approach to small subset of real-world data to quantify performance gap

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of LLMs on EHR section identification vary across different languages and medical systems internationally?
- Basis in paper: [inferred] The paper focuses on English EHRs from a specific healthcare system, with no mention of multilingual or cross-system evaluation.
- Why unresolved: The study's scope is limited to English EHRs from a single healthcare system, and there is no discussion of how the findings might generalize to other languages or healthcare systems.
- What evidence would resolve it: Comparative studies of LLM performance on EHR section identification across multiple languages and healthcare systems would provide insights into the generalizability of the findings.

### Open Question 2
- Question: What are the long-term effects of using LLMs for EHR section identification on healthcare professionals' workflow and decision-making?
- Basis in paper: [explicit] The paper discusses the potential benefits of LLMs for EHR section identification but does not explore the long-term impact on healthcare professionals.
- Why unresolved: The study focuses on the technical performance of LLMs and does not address the practical implications for healthcare professionals.
- What evidence would resolve it: Longitudinal studies tracking healthcare professionals' workflow and decision-making processes after implementing LLM-based EHR section identification would provide insights into the long-term effects.

### Open Question 3
- Question: How can the accuracy of LLMs in EHR section identification be improved for real-world, noisy data?
- Basis in paper: [explicit] The paper highlights the significant drop in LLM performance on real-world, noisy data compared to benchmark datasets.
- Why unresolved: The study identifies the problem but does not propose specific solutions for improving LLM accuracy on real-world data.
- What evidence would resolve it: Research into data preprocessing techniques, model fine-tuning strategies, or alternative LLM architectures tailored for noisy data would provide potential solutions for improving accuracy.

## Limitations
- Limited access to real-world data, with only 100 documents available for testing, which may not fully represent the diversity of real-world EHRs.
- The performance gap between benchmark and real-world datasets may be influenced by unmeasured factors such as domain specificity of pretraining data and prompt engineering variations.
- The study does not explore model calibration or confidence scoring, which could provide insights into prediction reliability.

## Confidence
- **High Confidence**: GPT-4's superior performance on clean benchmark datasets (96% accuracy) is well-established through multiple experiments and comparisons with baseline methods.
- **Medium Confidence**: The performance drop on real-world data (37% accuracy) is supported by experimental results, but the exact contribution of different noise factors (OCR errors, formatting inconsistencies) remains unclear.
- **Low Confidence**: The marginal benefits of few-shot and CoT prompting are reported but not thoroughly quantified, with limited evidence on how these approaches affect edge cases or specific types of noise.

## Next Checks
1. **Ablation study on noise types**: Systematically introduce different types of OCR errors and formatting inconsistencies to benchmark data to isolate their individual impact on LLM performance.
2. **Extended real-world evaluation**: Test on a larger, more diverse real-world dataset (minimum 500 documents) to verify if the 37% accuracy is consistent across different document types and noise levels.
3. **Confidence calibration analysis**: Implement and evaluate uncertainty quantification methods for GPT-4 predictions to identify when the model is uncertain and correlate this with actual performance on noisy data.