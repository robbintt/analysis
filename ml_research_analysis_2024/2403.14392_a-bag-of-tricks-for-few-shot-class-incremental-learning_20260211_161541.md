---
ver: rpa2
title: A Bag of Tricks for Few-Shot Class-Incremental Learning
arxiv_id: '2403.14392'
source_url: https://arxiv.org/abs/2403.14392
tags:
- classes
- learning
- tricks
- incremental
- stability
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a framework for few-shot class-incremental
  learning (FSCIL) that improves both stability and adaptability. The method combines
  six key techniques grouped into stability tricks (supervised contrastive loss, pre-assigning
  prototypes, pseudo-classes), adaptability tricks (incremental fine-tuning with subnet
  tuning), and training tricks (pre-training, additional learning signals).
---

# A Bag of Tricks for Few-Shot Class-Incremental Learning

## Quick Facts
- **arXiv ID:** 2403.14392
- **Source URL:** https://arxiv.org/abs/2403.14392
- **Reference count:** 32
- **Primary result:** Achieves state-of-the-art performance on FSCIL benchmarks with 3.22%, 1.1%, and 2.0% improvements on CIFAR-100, CUB-200, and miniImageNet respectively

## Executive Summary
This paper presents a comprehensive framework for few-shot class-incremental learning (FSCIL) that addresses the stability-adaptability dilemma. The method integrates six key techniques organized into stability tricks (supervised contrastive loss, pre-assigning prototypes, pseudo-classes), adaptability tricks (incremental fine-tuning with subnet tuning), and training tricks (pre-training, additional learning signals). These components are unified into a single framework that significantly outperforms existing approaches on standard FSCIL benchmarks.

## Method Summary
The proposed framework combines multiple techniques to tackle the challenges of FSCIL. Stability is enhanced through supervised contrastive loss for better feature separation, pre-assigned prototypes to maintain consistent class representations, and pseudo-classes for regularization. Adaptability is improved via incremental fine-tuning with subnet tuning to selectively update model parameters. Additional training tricks include careful pre-training strategies and incorporating extra learning signals. The framework integrates these components systematically, with extensive ablation studies demonstrating the contribution of each element to overall performance.

## Key Results
- Achieves state-of-the-art results on CIFAR-100, CUB-200, and miniImageNet datasets
- Demonstrates 3.22%, 1.1%, and 2.0% improvements over prior methods respectively
- Shows consistent performance across different model sizes and shot settings
- Provides comprehensive ablation studies validating the effectiveness of individual components

## Why This Works (Mechanism)
The framework addresses the fundamental trade-off in FSCIL between stability (retaining knowledge of old classes) and adaptability (learning new classes effectively). Supervised contrastive loss improves feature discriminability for both old and new classes. Pre-assigning prototypes provides stable reference points during incremental learning, preventing catastrophic forgetting. Pseudo-classes act as regularization to maintain feature space consistency. Subnet tuning enables selective parameter updates, preserving important features while adapting to new tasks. The combination of these techniques creates a synergistic effect that enhances both stability and adaptability simultaneously.

## Foundational Learning
- **Few-shot learning:** Training models to recognize new classes from very few examples (why needed: enables adaptation to new classes with limited data)
- **Incremental learning:** Learning new tasks without forgetting previous ones (why needed: models must evolve without catastrophic forgetting)
- **Contrastive learning:** Learning by comparing similar and dissimilar examples (why needed: improves feature discriminability)
- **Catastrophic forgetting:** The tendency of neural networks to forget previously learned information when trained on new tasks (why needed: central challenge in incremental learning)
- **Prototype-based classification:** Using class centroids as decision boundaries (why needed: provides stable reference points)
- **Subnet tuning:** Selectively updating subsets of model parameters (why needed: balances adaptation with preservation)

Quick check: Can the model recognize new classes with 1-5 examples while maintaining performance on previously seen classes?

## Architecture Onboarding

**Component map:** Input -> Feature Extractor -> Supervised Contrastive Loss -> Prototype Assignment -> Incremental Fine-tuning -> Subnet Tuning -> Output

**Critical path:** Feature extraction with supervised contrastive loss → prototype assignment → incremental fine-tuning with subnet tuning

**Design tradeoffs:** The framework trades increased computational complexity for improved performance. While individual techniques add overhead, the combined approach achieves significant gains. The use of subnet tuning reduces parameter updates compared to full fine-tuning, partially offsetting computational costs.

**Failure signatures:** Performance degradation on old classes indicates catastrophic forgetting; poor performance on new classes suggests insufficient adaptability; unstable training curves may indicate improper balance between stability and adaptability techniques.

**First experiments:**
1. Evaluate baseline performance without any tricks to establish performance floor
2. Add supervised contrastive loss to assess its individual contribution
3. Test incremental fine-tuning with different subnet tuning strategies

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Evaluation limited to standard benchmark datasets without real-world deployment scenarios
- Computational overhead of multiple techniques not thoroughly analyzed
- Scalability to larger, more complex architectures remains unexplored

## Confidence
- CIFAR-100 and miniImageNet improvements: High
- CUB-200 improvements: Medium (smaller margin)
- Overall framework design: High

## Next Checks
1. Evaluate the framework's performance under significant domain shift conditions to assess robustness beyond standard benchmarks
2. Conduct detailed computational complexity analysis to quantify the trade-offs between performance gains and resource requirements
3. Test scalability to larger, more complex architectures (e.g., Vision Transformers) to assess generalization beyond the ResNet-based models used in the study