---
ver: rpa2
title: 'VisScience: An Extensive Benchmark for Evaluating K12 Educational Multi-modal
  Scientific Reasoning'
arxiv_id: '2409.13730'
source_url: https://arxiv.org/abs/2409.13730
tags:
- figure
- visscience
- question
- point
- mass
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VisScience, a comprehensive benchmark designed
  to evaluate the multi-modal scientific reasoning capabilities of MLLMs across mathematics,
  physics, and chemistry disciplines. The benchmark comprises 3,000 K12 education
  questions with visual contexts, evenly distributed across the three disciplines
  and categorized into five difficulty levels and 21 subjects.
---

# VisScience: An Extensive Benchmark for Evaluating K12 Educational Multi-modal Scientific Reasoning
## Quick Facts
- **arXiv ID:** 2409.13730
- **Source URL:** https://arxiv.org/abs/2409.13730
- **Reference count:** 40
- **Key outcome:** Evaluates MLLM scientific reasoning across math, physics, and chemistry with 3,000 K12 questions, revealing reasoning errors as primary weakness

## Executive Summary
VisScience introduces a comprehensive benchmark for evaluating multi-modal large language models (MLLMs) in scientific reasoning tasks across K12 mathematics, physics, and chemistry. The benchmark comprises 3,000 questions with visual contexts, distributed evenly across three disciplines and categorized into five difficulty levels and 21 subjects. Experiments on 25 representative MLLMs demonstrate that closed-source models generally outperform open-source ones, with the best accuracy reaching 53.4% in mathematics, 38.2% in physics, and 47.0% in chemistry. The study reveals reasoning errors as the most prevalent challenge across all disciplines, highlighting significant room for improvement in complex scientific problem-solving with visual information.

## Method Summary
The VisScience benchmark was constructed through a systematic process involving domain experts to create 3,000 K12-level scientific questions across mathematics, physics, and chemistry. Each question includes visual contexts such as diagrams, graphs, or experimental setups. The benchmark features five difficulty levels and 21 distinct subjects, ensuring comprehensive coverage of K12 scientific curricula. The questions were designed to test various aspects of scientific reasoning, including problem interpretation, visual analysis, logical deduction, and multi-step problem solving. The benchmark was then applied to evaluate 25 representative MLLMs, including both closed-source and open-source models, using accuracy as the primary performance metric.

## Key Results
- Closed-source models significantly outperform open-source models in scientific reasoning tasks
- Best performances: 53.4% accuracy in mathematics (Claude3.5-Sonnet), 38.2% in physics (GPT-4o), and 47.0% in chemistry (Gemini-1.5-Pro)
- Reasoning errors identified as the most prevalent weakness across all three disciplines

## Why This Works (Mechanism)
The benchmark works by providing standardized, visually-rich scientific problems that require integrated multi-modal reasoning. The structured difficulty levels and subject categorization allow for granular performance assessment across different reasoning capabilities. The visual contexts challenge models to interpret and integrate textual and graphical information, which is essential for real-world scientific problem solving.

## Foundational Learning
- **Multi-modal integration**: Why needed: Scientific problems often combine text and visual data; Quick check: Can the model correlate textual descriptions with visual elements
- **Scientific reasoning**: Why needed: Requires logical deduction beyond pattern recognition; Quick check: Can the model derive conclusions from given premises
- **Visual comprehension**: Why needed: Scientific diagrams and graphs convey critical information; Quick check: Can the model accurately interpret graphical data
- **Multi-step problem solving**: Why needed: Complex scientific problems require sequential reasoning; Quick check: Can the model maintain consistency across solution steps
- **Subject-specific knowledge**: Why needed: Different scientific domains have unique principles and terminology; Quick check: Can the model apply domain-specific concepts correctly
- **Error analysis**: Why needed: Understanding failure modes guides model improvement; Quick check: Can the model's reasoning errors be systematically categorized

## Architecture Onboarding
**Component map:** Question -> Visual processing -> Text processing -> Reasoning engine -> Answer generation
**Critical path:** Visual context interpretation → Textual information extraction → Multi-modal reasoning → Final answer prediction
**Design tradeoffs:** Visual context complexity vs. computational efficiency; breadth of subject coverage vs. depth of domain-specific fine-tuning
**Failure signatures:** Inability to correlate visual and textual information; logical inconsistencies in multi-step reasoning; misinterpretation of scientific diagrams
**3 first experiments:**
1. Test model performance on single-modality questions (text-only) vs. multi-modal questions
2. Evaluate model accuracy across different visual complexity levels
3. Assess reasoning consistency by presenting modified versions of the same problem

## Open Questions the Paper Calls Out
None

## Limitations
- Performance gap between closed-source and open-source models may reflect availability of fine-tuning data rather than fundamental architectural differences
- K12-level questions may not represent complexity of real-world professional scientific reasoning
- Error analysis based on limited sample size may not capture full spectrum of model weaknesses

## Confidence
- **High confidence:** Benchmark construction methodology and basic performance metrics across 25 MLLMs are reliable
- **Medium confidence:** Cross-disciplinary performance comparisons and error analysis findings
- **Low confidence:** Absolute performance numbers as indicators of real-world scientific reasoning capabilities

## Next Checks
1. Conduct cross-validation with domain experts to verify accuracy and relevance of questions across all 21 subjects
2. Test additional open-source models with enhanced fine-tuning on scientific multi-modal data to establish whether performance gaps are fundamental or training-related
3. Expand the benchmark with real-world scientific problem scenarios beyond K12 education to assess practical applicability of MLLM capabilities