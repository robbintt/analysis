---
ver: rpa2
title: 'YuLan-Mini: An Open Data-efficient Language Model'
arxiv_id: '2412.17743'
source_url: https://arxiv.org/abs/2412.17743
tags:
- data
- training
- arxiv
- https
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents YuLan-Mini, a highly capable base language
  model with 2.42 billion parameters. The authors focus on improving training efficacy
  through three key technical contributions: an elaborate data pipeline combining
  data cleaning with scheduling strategies, a robust optimization method to mitigate
  training instability, and an effective annealing approach incorporating targeted
  data selection and long context training.'
---

# YuLan-Mini: An Open Data-efficient Language Model

## Quick Facts
- arXiv ID: 2412.17743
- Source URL: https://arxiv.org/abs/2412.17743
- Reference count: 40
- Model achieves 37.80 on MATH-500 (four-shot), 64.00 on HumanEval (zero-shot), and 49.10 on MMLU (five-shot)

## Executive Summary
YuLan-Mini is a 2.42 billion parameter base language model designed for data efficiency. The model achieves top-tier performance among models of similar scale by training on 1.08 trillion tokens using an elaborate data pipeline that combines data cleaning with scheduling strategies. The authors introduce a robust optimization method to mitigate training instability and an effective annealing approach incorporating targeted data selection and long context training. The model demonstrates strong performance on standard benchmarks including MATH-500, HumanEval, and MMLU.

## Method Summary
The authors present three key technical contributions to improve training efficacy. First, they implement an elaborate data pipeline that combines data cleaning with scheduling strategies to optimize the training data composition. Second, they develop a robust optimization method specifically designed to mitigate training instability during large-scale pretraining. Third, they employ an effective annealing approach that incorporates targeted data selection and long context training to enhance model capabilities. The model is trained on 1.08 trillion tokens with full details of data composition for each training phase released to facilitate reproduction.

## Key Results
- Achieves 37.80 score on MATH-500 benchmark (four-shot)
- Achieves 64.00 score on HumanEval benchmark (zero-shot)
- Achieves 49.10 score on MMLU benchmark (five-shot)

## Why This Works (Mechanism)
The data pipeline's effectiveness stems from its combination of rigorous data cleaning with intelligent scheduling strategies that optimize the sequence and composition of training data. The robust optimization method addresses the inherent instability that occurs during large-scale language model training by providing more stable convergence properties. The annealing approach allows the model to gradually adapt to more complex tasks and longer contexts, improving its ability to handle diverse inputs while maintaining performance on core language tasks.

## Foundational Learning
- Data cleaning and scheduling: Essential for removing noise and optimizing learning progression; quick check involves examining data quality metrics before and after cleaning
- Optimization stability: Critical for preventing training collapse during large-scale pretraining; quick check involves monitoring loss curves for instability patterns
- Annealing strategies: Necessary for progressive skill development and context handling; quick check involves tracking performance on long-context tasks over training time
- Token efficiency: Important for measuring computational resource utilization; quick check involves comparing performance per training token against baseline models
- Context window management: Crucial for handling diverse input lengths; quick check involves testing model performance across varying context lengths

## Architecture Onboarding

Component Map:
Data Pipeline -> Optimization Method -> Annealing Approach -> Model Performance

Critical Path:
Data cleaning and scheduling forms the foundation, feeding into the robust optimization method which stabilizes training, leading to the annealing approach that enhances capabilities, ultimately resulting in benchmark performance.

Design Tradeoffs:
The model prioritizes data efficiency over parameter count, accepting a smaller model size (2.42B parameters) in exchange for extensive training on 1.08T tokens. This approach balances computational costs with performance gains, though it may limit certain capabilities that larger models possess.

Failure Signatures:
Potential failure modes include overfitting to specific data subsets if scheduling is not properly balanced, optimization instability leading to training divergence, and inadequate annealing causing poor adaptation to long contexts or complex tasks.

First Experiments:
1. Verify data cleaning effectiveness by comparing model performance with and without cleaned datasets
2. Test optimization stability by introducing controlled perturbations during training
3. Evaluate annealing impact by measuring performance progression across different annealing schedules

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow evaluation scope focused primarily on standard benchmarks rather than real-world deployment scenarios
- Insufficient comparative analysis to demonstrate data efficiency gains against similar models trained on comparable data volumes
- Lack of ablation studies to isolate the contribution of individual technical components to final performance

## Confidence
- Model performance claims: High - Specific benchmark scores are verifiable though comparison context could be stronger
- Technical contributions: Medium - Three-pronged approach is clear but lacks empirical validation of individual component contributions
- Data efficiency claims: Low - Training claim presented without adequate comparative analysis to demonstrate efficiency gains

## Next Checks
1. Conduct ablation studies to quantify the individual impact of data scheduling, optimization method, and annealing approach on model performance
2. Compare training efficiency metrics (tokens per performance point) against similar-sized models trained on comparable data volumes
3. Evaluate the model on task-specific benchmarks beyond standard academic tests to assess practical utility