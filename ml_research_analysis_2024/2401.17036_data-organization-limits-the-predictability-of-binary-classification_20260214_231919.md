---
ver: rpa2
title: Data organization limits the predictability of binary classification
arxiv_id: '2401.17036'
source_url: https://arxiv.org/abs/2401.17036
tags:
- uni00000013
- uni00000011
- uni0000004c
- uni00000014
- uni00000015
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This study addresses the fundamental question of the maximum achievable\
  \ performance in binary classification by establishing theoretical upper bounds\
  \ for key evaluation metrics\u2014the ROC curve, the PR curve, and accuracy. The\
  \ core method idea involves treating the problem as a 0-1 knapsack optimization,\
  \ where the best classifier is determined by the dataset's inherent structure, specifically\
  \ the distribution of positive and negative samples across feature vectors."
---

# Data organization limits the predictability of binary classification

## Quick Facts
- arXiv ID: 2401.17036
- Source URL: https://arxiv.org/abs/2401.17036
- Authors: Fei Jing; Zi-Ke Zhang; Yi-Cheng Zhang; Qingpeng Zhang
- Reference count: 0
- Primary result: Theoretical upper bounds for binary classification metrics (ROC, PR, accuracy) are established and shown to be limited by data structure

## Executive Summary
This study establishes theoretical upper bounds for binary classification performance metrics—the ROC curve, the PR curve, and accuracy—by modeling the classification problem as a 0-1 knapsack optimization. The authors demonstrate that the maximum achievable performance is fundamentally constrained by the dataset's intrinsic structure, specifically the distribution of positive and negative samples across feature vectors. Empirical results across four real-world datasets confirm that no classifier can exceed these theoretical limits, and the work reveals a direct relationship between class overlap and the upper bound: greater overlap reduces the potential for high performance.

## Method Summary
The method treats binary classification as a 0-1 knapsack optimization problem, where the optimal classifier is determined by the dataset's inherent structure. The authors derive exact upper bounds (ARu, APu, ACu) for the ROC curve, PR curve, and accuracy metrics using mathematical formulations that depend only on the counts of positive and negative samples per unique feature vector. These bounds are validated empirically across multiple classifiers (XGBoost, MLP, SVM, Logistic Regression, Decision Tree, Random Forest, KNN, Naive Bayes) on four real-world datasets. The study also quantifies class overlap using Jensen-Shannon divergence and shows how feature selection can reduce overlap and increase the upper bound.

## Key Results
- Theoretical upper bounds for ROC, PR, and accuracy metrics are derived and shown to be attainable under specific conditions
- Empirical validation across four real-world datasets confirms no classifier exceeds these theoretical limits
- Greater class overlap directly reduces the achievable upper bound, approaching random guessing when overlap is complete
- Adding independent new features can reduce overlap and increase the upper bound, while feature transformations alone cannot

## Why This Works (Mechanism)

### Mechanism 1
The upper bound of binary classification performance is fundamentally limited by the data's intrinsic structure, specifically the distribution of positive and negative samples across feature vectors. The authors model the classification problem as a 0-1 knapsack optimization, where the optimal classifier is determined by the inherent distribution of samples. The key insight is that the maximum achievable performance metrics (ROC, PR, accuracy) can be derived directly from the dataset's characteristics without reference to any specific classifier.

### Mechanism 2
Greater overlap between positive and negative instances directly reduces the achievable upper bound of classification performance. The authors quantify overlap using Jensen-Shannon divergence between positive and negative sample distributions. As overlap increases, the theoretical maximum performance decreases, approaching random guessing when overlap is complete.

### Mechanism 3
Adding new, independent features can reduce class overlap and increase the upper bound of performance, while transformations of existing features cannot. New features that are statistically independent of existing ones can separate previously overlapping classes, effectively reducing DS and increasing ARu. However, feature extraction that merely transforms existing features cannot change the fundamental overlap structure.

## Foundational Learning

- **Concept**: 0-1 knapsack optimization
  - Why needed here: The authors use this classical optimization framework to determine the optimal classifier that maximizes the performance metric for each threshold
  - Quick check question: Can you explain why the optimal classifier for ROC/PR curves is simply the weight function f*(xi) = P(xi)/(P(xi)+N(xi))?

- **Concept**: Jensen-Shannon divergence as a measure of overlap
  - Why needed here: The authors use this symmetric, bounded measure to quantify the degree of overlap between positive and negative class distributions
  - Quick check question: What happens to the upper bound ARu when DS = 1 (complete overlap) versus DS = 0 (complete separation)?

- **Concept**: Sensitivity analysis in out-of-sample contexts
  - Why needed here: The authors extend their theoretical bounds to practical scenarios by analyzing how training-test distribution differences affect generalization
  - Quick check question: According to the paper, under what condition can the upper bound be reached in both training and testing sets?

## Architecture Onboarding

- **Component map**: Data preprocessing -> Calculate P(xi), N(xi), p+(xi) -> 0-1 knapsack optimization -> Compute ARu, APu, ACu -> Compare with empirical performance -> Analyze overlap (DS) -> Guide feature selection

- **Critical path**: Input dataset → Calculate class counts per feature vector → Apply 0-1 knapsack optimization → Determine optimal classifier weights → Compute upper bounds (ARu, APu, ACu) → Compare with empirical classifier performance → Analyze overlap (DS) → Guide feature selection decisions

- **Design tradeoffs**: Computational complexity vs. accuracy: The exact bound calculations require O(m²) operations for ARu, which may be prohibitive for large datasets; Discretization vs. continuity: The theoretical framework assumes discrete feature vectors, which may not capture continuous relationships; Feature independence assumption: The feature selection benefits assume new features are independent, which may not hold in practice

- **Failure signatures**: Bound calculations return values significantly higher than empirical performance → May indicate violation of discretization assumption; Sensitivity analysis shows large generalization gaps → Training and test distributions may differ substantially; Feature selection provides minimal improvement → New features may not be truly independent or may not reduce overlap

- **First 3 experiments**: Implement bound calculation for a simple synthetic dataset with known overlap characteristics; Compare theoretical bounds with empirical performance of multiple classifiers on a real-world dataset; Test feature selection algorithm on a dataset with varying degrees of class overlap

## Open Questions the Paper Calls Out

- How can the theoretical upper bounds of binary classification performance be practically approached in real-world scenarios with limited data and computational resources?
- How does the proposed boundary theory extend to multiclass classification problems, and what modifications are necessary?
- What is the impact of feature transformations (e.g., PCA, feature scaling) on the overlap between positive and negative samples and the performance upper bounds?

## Limitations

- The discretization assumption for feature vectors may not hold for continuous or high-dimensional data, potentially invalidating the 0-1 knapsack optimization framework
- Jensen-Shannon divergence as a sole measure of overlap may oversimplify complex class distribution relationships
- Feature independence assumption for new features may not hold in practical scenarios

## Confidence

- **High confidence**: The theoretical framework and mathematical derivations for upper bounds are sound within the stated assumptions
- **Medium confidence**: Empirical validation across four datasets supports the theoretical claims, but generalization to other domains requires further testing
- **Low confidence**: The sensitivity analysis for out-of-sample generalization needs more extensive validation across diverse distribution shifts

## Next Checks

1. Test the bound calculations on continuous feature spaces using kernel density estimation instead of discretization
2. Validate the overlap-performance relationship across datasets with varying class distribution complexities
3. Evaluate the feature selection algorithm's effectiveness when new features have partial correlation with existing ones