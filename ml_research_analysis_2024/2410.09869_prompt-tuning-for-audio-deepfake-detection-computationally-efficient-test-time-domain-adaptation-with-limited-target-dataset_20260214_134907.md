---
ver: rpa2
title: 'Prompt Tuning for Audio Deepfake Detection: Computationally Efficient Test-time
  Domain Adaptation with Limited Target Dataset'
arxiv_id: '2410.09869'
source_url: https://arxiv.org/abs/2410.09869
tags:
- prompt
- target
- domain
- tuning
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses test-time domain adaptation for audio deepfake
  detection, tackling challenges of domain gaps, limited target dataset size, and
  high computational costs. The authors propose a prompt-tuning method that adds trainable
  parameters (prompts) to pre-trained transformer models.
---

# Prompt Tuning for Audio Deepfake Detection: Computationally Efficient Test-time Domain Adaptation with Limited Target Dataset

## Quick Facts
- arXiv ID: 2410.09869
- Source URL: https://arxiv.org/abs/2410.09869
- Authors: Hideyuki Oiso; Yuto Matsunaga; Kazuya Kakizaki; Taiki Miyagawa
- Reference count: 0
- One-line primary result: Prompt tuning reduces equal error rates across seven target domains with as few as 10 samples while requiring only 0.0016% additional parameters

## Executive Summary
This paper addresses test-time domain adaptation for audio deepfake detection (ADD) by proposing a prompt-tuning method that adds trainable parameters to pre-trained transformer models. The approach tackles the challenges of domain gaps between source and target datasets, limited target dataset sizes, and high computational costs associated with traditional fine-tuning methods. The authors demonstrate that prompt tuning can effectively bridge domain gaps while maintaining computational efficiency and preventing overfitting on small target datasets.

## Method Summary
The proposed method inserts trainable prompt parameters into intermediate feature vectors of pre-trained transformer encoders (wav2vec 2.0 or Whisper). These prompts are fine-tuned on small labeled target datasets while keeping the base model parameters frozen. The method supports three variants: tuning prompts only, tuning prompts plus the last linear layer, or tuning prompts plus all linear layers. Class-balanced loss is used to handle imbalanced real/fake distributions. Hyperparameters are optimized using Optuna across seven target domains with varying sample sizes.

## Key Results
- Prompt tuning reduces EER across seven target domains compared to baseline without adaptation
- Superior performance over full fine-tuning on small target datasets (10-100 samples)
- Maintains computational efficiency with only 0.0016% additional parameters (5,120 for W2V vs 317M base parameters)
- Training durations are approximately 500 seconds for W2V and 100 seconds for Whisper models on target datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt tuning bridges domain gaps by adding trainable parameters that adapt to target domain characteristics without modifying the base model
- Mechanism: Prompts are inserted into intermediate feature vectors of transformer encoders and fine-tuned on target data, allowing the model to adjust its internal representations for the new domain while preserving pre-trained knowledge
- Core assumption: Small number of trainable parameters can effectively capture domain shift patterns without overfitting to limited target data
- Evidence anchors:
  - [abstract]: "It bridges domain gaps by integrating it seamlessly with state-of-the-art transformer models"
  - [section 2.3]: "The prompt θP is inserted to the intermediate feature vectors in Front-End"
  - [corpus]: No direct evidence in corpus papers about prompt tuning for domain adaptation

### Mechanism 2
- Claim: Prompt tuning prevents overfitting on small target datasets by using minimal additional parameters compared to full fine-tuning
- Mechanism: By tuning only the prompt parameters (5,120 parameters for W2V) rather than all model weights (317 million for W2V), the method maintains generalization while adapting to target domain
- Core assumption: Domain adaptation can be achieved with parameter-efficient methods when target data is scarce
- Evidence anchors:
  - [abstract]: "Our method can fit small target datasets because it does not require a large number of extra parameters"
  - [section 3.2]: "This is an advantage over full fine-tuning... which necessitates optimizing a huge number of parameters"
  - [table 3]: Shows ratio of trainable parameters is only 0.00161% for W2V

### Mechanism 3
- Claim: Prompt tuning maintains computational efficiency by requiring minimal additional resources compared to full fine-tuning
- Mechanism: The small parameter count (0.0016% of base model) translates to reduced memory consumption and faster training times during adaptation
- Core assumption: Computational efficiency scales with parameter count for fine-tuning operations
- Evidence anchors:
  - [abstract]: "This feature also contributes to computational efficiency, countering the high computational costs"
  - [section 3.1]: "The training durations for W2V and WSP models on the target datasets are approximately 500 and 100 seconds"
  - [table 3]: Shows GPU memory consumption differences between methods

## Foundational Learning

- Concept: Test-time domain adaptation vs domain generalization
  - Why needed here: The paper distinguishes between adapting to available labeled target data vs generalizing to unseen domains
  - Quick check question: What's the key difference between test-time domain adaptation and domain generalization in terms of data availability?

- Concept: Prompt tuning in transformer models
  - Why needed here: The method relies on inserting and tuning prompt parameters within transformer architectures
  - Quick check question: Where are prompts inserted in the transformer architecture and what dimension do they have?

- Concept: Class-balanced loss for imbalanced datasets
  - Why needed here: ADD datasets are inherently imbalanced between real and fake samples
  - Quick check question: Why is class-balanced loss particularly important for audio deepfake detection datasets?

## Architecture Onboarding

- Component map:
  - Front-End: wav2vec 2.0 (300M params) or Whisper (39M params) transformer models
  - Back-End: AASIST (297k params) or MesoNet (28k params) classifier
  - Prompt layer: d×NP dimensional trainable parameters inserted into Front-End
  - Loss function: Class-balanced loss with effective number parameter β

- Critical path:
  1. Load pre-trained ADD model
  2. Insert prompts into intermediate feature vectors
  3. Fine-tune prompts (and optionally linear layer) on target data
  4. Evaluate on target domain test set

- Design tradeoffs:
  - Prompt length vs performance: Longer prompts provide more adaptation capacity but risk overfitting
  - Which parameters to tune: Tuning prompts only vs prompts + linear layer vs full fine-tuning
  - Target dataset size vs method choice: Small datasets favor prompt tuning over full fine-tuning

- Failure signatures:
  - EER doesn't improve despite prompt tuning → domain gap too large or prompts insufficient
  - EER degrades on source domain → over-adaptation to target domain
  - Training instability → learning rate too high for small parameter set

- First 3 experiments:
  1. Run prompt tuning with NP=5 on W2V model with |DT|=50 samples on In-The-Wild dataset
  2. Compare EER with and without prompt tuning on development set during training
  3. Test different prompt lengths (1, 5, 10) to find saturation point

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal prompt length for different ADD models and domain gap types?
- Basis in paper: [explicit] The paper shows performance saturates rapidly with prompt length, suggesting NP ~ O(1) or O(10) is adequate, but doesn't determine if this varies by model or domain.
- Why unresolved: The ablation study only tested one target domain (In-The-Wild), leaving open whether optimal prompt lengths vary across different domain gaps or model architectures.
- What evidence would resolve it: Comparative experiments testing multiple prompt lengths across all seven target domains and both base models would reveal if optimal prompt lengths are domain-dependent or model-dependent.

### Open Question 2
- Question: How does prompt tuning performance scale with target dataset size beyond the tested range?
- Basis in paper: [inferred] The paper tested up to 1000 samples but didn't explore whether performance continues improving or plateaus at larger dataset sizes, which would inform practical deployment scenarios.
- Why unresolved: The experimental design capped at 1000 samples, leaving unclear whether prompt tuning maintains its advantage over full fine-tuning as dataset sizes increase substantially.
- What evidence would resolve it: Experiments testing prompt tuning effectiveness at much larger target dataset sizes (e.g., 10,000+ samples) would reveal if the computational efficiency advantage persists or if full fine-tuning becomes preferable.

### Open Question 3
- Question: Can prompt tuning be effectively combined with other domain adaptation techniques to further improve ADD performance?
- Basis in paper: [explicit] The paper mentions that prompt tuning could be combined with existing techniques that maintain source accuracy while enhancing target accuracy, but doesn't explore these combinations.
- Why unresolved: The study focuses solely on prompt tuning as a standalone technique, leaving open whether hybrid approaches could yield superior performance, particularly for challenging domain gaps.
- What evidence would resolve it: Experiments combining prompt tuning with techniques like adversarial training, meta-learning, or multi-task learning would demonstrate whether additive benefits exist and under what conditions.

## Limitations
- Evaluation primarily focuses on specific target datasets that may not represent all possible domain gaps in audio deepfake detection
- Hyperparameter optimization covers only a limited search space, and different optimization strategies might yield better results
- Study doesn't explore catastrophic forgetting on the source domain when fine-tuning on target domains

## Confidence

- **High Confidence**: The computational efficiency claims and parameter efficiency (0.0016% additional parameters) are well-supported by the experimental data and straightforward calculations.
- **Medium Confidence**: The domain adaptation effectiveness across seven target domains is demonstrated, but the general applicability to unseen domains requires further validation.
- **Medium Confidence**: The comparison with full fine-tuning on small datasets is convincing, though the optimal method choice may depend on specific dataset characteristics not fully explored.

## Next Checks

1. **Cross-source validation**: Test prompt tuning effectiveness when source dataset is switched from ASVspoof 2019 LA to an alternative source like ASVspoof 2021 LA to verify robustness to source domain choice.

2. **Source domain retention**: Measure performance degradation on the original source domain (ASVspoof 2019 LA) after prompt tuning on target domains to assess catastrophic forgetting.

3. **Alternative domain shift scenarios**: Evaluate the method on audio deepfake datasets with different types of domain gaps (e.g., different languages, recording conditions, or generation methods) not covered in the current evaluation.