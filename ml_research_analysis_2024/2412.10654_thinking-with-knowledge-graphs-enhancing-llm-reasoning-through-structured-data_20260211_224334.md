---
ver: rpa2
title: 'Thinking with Knowledge Graphs: Enhancing LLM Reasoning Through Structured
  Data'
arxiv_id: '2412.10654'
source_url: https://arxiv.org/abs/2412.10654
tags:
- reasoning
- llms
- language
- knowledge
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper addresses the challenge of improving Large Language\
  \ Models\u2019 (LLMs) reasoning capabilities, particularly for complex multi-hop\
  \ reasoning tasks where they often struggle and produce hallucinations. The core\
  \ method involves representing knowledge graphs (KGs) using programming languages\
  \ (specifically Python), leveraging the structured and unambiguous nature of code\
  \ to encode entity relationships."
---

# Thinking with Knowledge Graphs: Enhancing LLM Reasoning Through Structured Data

## Quick Facts
- arXiv ID: 2412.10654
- Source URL: https://arxiv.org/abs/2412.10654
- Reference count: 40
- Key outcome: Fine-tuning LLMs with Python-based KG representations improves multi-hop reasoning accuracy up to 78% over zero-shot prompting

## Executive Summary
This paper addresses the challenge of improving Large Language Models' (LLMs) reasoning capabilities for complex multi-hop reasoning tasks where they often struggle and produce hallucinations. The core innovation involves representing knowledge graphs (KGs) using Python code rather than natural language or JSON, leveraging the structured and unambiguous nature of code to encode entity relationships. The authors fine-tune LLMs (Llama-3.1-8B-Instruct) with these Python-based KG representations and evaluate performance on two-hop and three-hop reasoning tasks. Results show that fine-tuning with Python representations significantly improves multi-hop reasoning accuracy and generalizes well to longer reasoning paths.

## Method Summary
The method involves representing KG triples in three formats: natural language, JSON, and Python code. The Python representation uses dictionary structures for static facts and class-based representations for dynamic inference. LLMs are fine-tuned using LoRA on these representations, with the Python format showing superior performance. The approach leverages the model's pre-existing familiarity with code syntax from pre-training and explicitly encodes the logical inference process rather than just memorizing facts. The study uses Wikidata-derived datasets for two-hop and three-hop reasoning evaluation.

## Key Results
- Fine-tuning with Python KG representations improves multi-hop reasoning accuracy by up to 78% compared to zero-shot prompting
- Python representations outperform both natural language and JSON formats across all evaluation conditions
- Fine-tuned models generalize better to longer reasoning paths than models trained only on shorter paths
- Python representation fine-tuned models outperform much larger LLMs (Llama-3.1-70B-Instruct) in certain conditions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Representing KGs in Python code improves LLM reasoning by leveraging the model's existing familiarity with code syntax and semantics, reducing ambiguity compared to natural language.
- **Mechanism:** Python code provides a structured, unambiguous representation of KG relationships that aligns with how LLMs process programming language during pre-training.
- **Core assumption:** LLMs have sufficient exposure to Python code structures in pre-training to understand and utilize KG representations encoded as code.
- **Evidence anchors:** [abstract] "Programming code is part of the pre-training data for many LLMs"; [section 3.2] "Programming language code is another major data source for LLM pre-training"
- **Break condition:** If the LLM lacks sufficient code exposure during pre-training, or if the KG representation becomes too complex for the model to parse correctly

### Mechanism 2
- **Claim:** Fine-tuning LLMs with Python-based KG representations improves multi-hop reasoning performance more than fine-tuning with natural language or JSON representations.
- **Mechanism:** Fine-tuning with Python code explicitly teaches the model the logical inference process rather than just memorizing facts.
- **Core assumption:** The fine-tuning process can effectively teach the model to execute KG inference logic encoded in Python rather than just pattern-matching text.
- **Evidence anchors:** [section 4.7.1] "LLMs fine-tuned with Python representations perform better than the LLMs fine-tuned with either JSON representation or plain natural language text representations"
- **Break condition:** If the Python code becomes too verbose or complex relative to the model's context window

### Mechanism 3
- **Claim:** Python-based KG representations enable better generalization to longer reasoning paths than models trained only on shorter paths.
- **Mechanism:** The dynamic Python representation encodes the general reasoning algorithm rather than specific entity facts, allowing the model to apply the same logical steps to new entities and longer paths.
- **Core assumption:** The model learns the abstract inference procedure from the Python code rather than memorizing specific entity relationships.
- **Evidence anchors:** [section 4.7.2] "LLMs that were fine-tuned with the Python representation outperform those fine-tuned with either the plain natural language representation or the JSON representation" on three-hop reasoning
- **Break condition:** If the model memorizes specific code patterns instead of learning the abstract reasoning procedure

## Foundational Learning

- **Concept:** Knowledge Graph structure (entities, relations, triples)
  - **Why needed here:** The paper's core innovation is encoding these KG structures in Python code for LLM consumption. Understanding the triple format (e1, r1, e2) is essential for grasping how relationships are represented.
  - **Quick check question:** What are the three components of a KG triple, and how do they correspond to Python dictionary keys and values in the static representation?

- **Concept:** Fine-tuning vs. In-context learning
  - **Why needed here:** The paper compares both approaches. Fine-tuning permanently updates model weights with KG examples, while in-context learning provides examples in the prompt without weight updates.
  - **Quick check question:** How does the performance of fine-tuned models compare to one-shot prompted models in the experiments?

- **Concept:** Multi-hop reasoning evaluation metrics
  - **Why needed here:** The paper uses a specific metric measuring conditional accuracy (r = p(h|h1, h2, ..., hn)). Understanding this helps interpret why Python representations improve reasoning.
  - **Quick check question:** Why does the paper use conditional accuracy rather than overall accuracy when evaluating multi-hop reasoning?

## Architecture Onboarding

- **Component map:** Dataset preparation pipeline -> Representation generator -> LLM interface -> Evaluation module -> Training infrastructure
- **Critical path:** KG triple → Python representation generation → LoRA fine-tuning → zero-shot inference on longer paths
- **Design tradeoffs:**
  - Python representation offers structure but increases token count vs. natural language
  - Fine-tuning provides permanent improvements but requires computational resources vs. in-context learning
  - Using Wikidata ensures broad coverage but may introduce noise in entity/relation extraction
- **Failure signatures:**
  - Poor performance on zero-shot inference indicates the model didn't learn the inference procedure
  - Degradation on longer paths suggests overfitting to specific path lengths
  - JSON representation performing worse than natural language suggests the model doesn't naturally parse structured data
- **First 3 experiments:**
  1. Implement Python representation generator for KG triples and verify it produces valid, executable code
  2. Run one-shot prompting experiments comparing Python vs. natural language vs. JSON representations on Dataset 1
  3. Implement LoRA fine-tuning pipeline with Python representations and evaluate zero-shot performance on Dataset 3 (three-hop)

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** Does the integration of knowledge graphs with programming language representations generalize to more complex reasoning tasks beyond two-hop and three-hop compositional relationships?
- **Basis in paper:** [explicit] The authors mention that they only studied the simplest form of reasoning and plan to study the graph representation of entity relationships and its impact on more complex reasoning cases in future work.
- **Why unresolved:** The experiments conducted were limited to two-hop and three-hop reasoning tasks, and the paper acknowledges the need to explore more sophisticated reasoning scenarios.
- **What evidence would resolve it:** Conducting experiments with knowledge graphs involving more complex reasoning tasks, such as multi-step reasoning with non-compositional relationships or reasoning over larger subgraphs, would provide evidence of the generalizability of the approach.

### Open Question 2
- **Question:** How does the performance of fine-tuned models compare when using programming language representations at the pre-training stage versus the fine-tuning stage?
- **Basis in paper:** [inferred] The authors mention that they conducted experiments at the fine-tuning stage and plan to experiment at both the pre-training and fine-tuning stages in future work to evaluate the performance impact.
- **Why unresolved:** The current study only explores the impact of programming language representations during the fine-tuning stage, and the potential benefits of incorporating such representations during pre-training remain unexplored.
- **What evidence would resolve it:** Comparing the performance of models fine-tuned with programming language representations to those pre-trained with such representations on various reasoning tasks would provide insights into the optimal stage for integration.

### Open Question 3
- **Question:** What is the impact of incorporating programming language representations of knowledge graphs on the reasoning abilities of larger language models (LLMs) like Llama-3.1-70B-Instruct?
- **Basis in paper:** [explicit] The authors conducted experiments with Llama-3.1-8B-Instruct and mention that they didn't fine-tune the larger model (Llama-3.1-70B-Instruct) due to computation resource constraints.
- **Why unresolved:** The study focuses on the impact of programming language representations on a smaller LLM, and the potential benefits for larger models remain unexplored due to resource limitations.
- **What evidence would resolve it:** Conducting experiments with larger LLMs using programming language representations of knowledge graphs would provide insights into whether the approach scales and benefits models with more parameters.

## Limitations

- The performance advantage may not generalize to other knowledge graph sources or model architectures beyond the specific Wikidata-derived datasets and Llama-3.1-8B-Instruct used
- The mechanism explaining why Python code representations work better is plausible but not definitively proven through direct testing
- The significant computational cost of fine-tuning (1 epoch on A100 GPUs) versus performance gains may limit practical applicability

## Confidence

- **High confidence:** The experimental results showing Python representations outperform natural language and JSON in controlled settings (Datasets 1 and 2)
- **Medium confidence:** The claim that Python representations enable better generalization to longer reasoning paths
- **Medium confidence:** The assertion that fine-tuned models outperform much larger models (Llama-3.1-70B) in certain conditions

## Next Checks

1. **Cross-dataset validation:** Test the Python representation approach on KG datasets from different domains (e.g., biomedical or social network data) to assess generalizability beyond Wikidata
2. **Ablation study on code familiarity:** Compare performance using Python representations against other structured formats (e.g., XML or YAML) to isolate whether Python-specific syntax familiarity is the key factor
3. **Efficiency analysis:** Measure the trade-off between fine-tuning computational costs and performance gains by testing on multiple model sizes and comparing with retrieval-augmented alternatives