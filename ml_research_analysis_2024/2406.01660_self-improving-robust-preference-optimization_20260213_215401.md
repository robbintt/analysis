---
ver: rpa2
title: Self-Improving Robust Preference Optimization
arxiv_id: '2406.01660'
source_url: https://arxiv.org/abs/2406.01660
tags:
- policy
- preference
- srpo
- self-improvement
- optimization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper introduces Self-Improving Robust Preference Optimization
  (SRPO), a method to align large language models (LLMs) with human preferences. SRPO
  addresses two key limitations of existing RLHF methods: the lack of an innate self-improvement
  mechanism at inference time and the dependence of optimal solutions on the training
  task.'
---

# Self-Improving Robust Preference Optimization

## Quick Facts
- arXiv ID: 2406.01660
- Source URL: https://arxiv.org/abs/2406.01660
- Authors: Eugene Choi; Arash Ahmadian; Matthieu Geist; Oilvier Pietquin; Mohammad Gheshlaghi Azar
- Reference count: 39
- Primary result: SRPO achieves 90% win rate against human completions on XSum after 5 self-revisions

## Executive Summary
This paper introduces Self-Improving Robust Preference Optimization (SRPO), a method to align LLMs with human preferences that addresses two key limitations of existing RLHF methods: lack of self-improvement at inference time and dependence on training task. SRPO jointly optimizes a self-improvement policy and generative policy in an adversarial fashion, but the key innovation is that the solution is independent of the training task, making it robust to changes. The paper proves that preference probabilities can be expressed in terms of log-likelihoods of optimal policies, enabling efficient optimization using standard supervised learning techniques. Experiments on summarization tasks demonstrate that SRPO outperforms baselines like DPO and IPO, particularly in out-of-distribution settings.

## Method Summary
SRPO reformulates preference optimization as a self-improvement process where a policy learns to refine its own outputs iteratively. The method jointly optimizes a generative policy π and a self-improvement policy π† using a combined loss function that is a convex combination of both policies' objectives. The key innovation is that the optimal solution is independent of the behavior policy distribution used to generate preference data, making it robust to distribution shifts. The method is trained using standard supervised learning techniques by expressing the preference probability in terms of both policies' log-likelihoods. At inference time, SRPO can generate completions and then apply N iterations of self-improvement revisions to refine them.

## Key Results
- SRPO achieves 90% win rate against human completions on XSum dataset after 5 self-revisions
- On Arena-Hard prompts, SRPO achieves 56% win rate against Llama-3.1-8B-Instruct, outperforming baselines by 4-6% with a single revision
- SRPO shows superior performance on out-of-distribution tasks compared to baselines, particularly when using self-improvement revisions
- The method demonstrates robustness to changes in the training task, as evidenced by consistent performance across different summarization datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The SRPO solution is independent of the behavior policy distribution µ, making it robust to changes in the preference data distribution.
- Mechanism: By reformulating the preference optimization as a self-improvement process and solving the min-max objective analytically, the optimal self-improvement policy π*† and the robust generative policy π* are expressed solely in terms of the preference probabilities p(y' ≻ y|x), without dependence on µ.
- Core assumption: The preference probabilities p(y' ≻ y|x) are well-defined and accurately capture human preferences between pairs of completions.
- Evidence anchors:
  - [abstract]: "the solution for this optimization problem is independent of the training task, which makes it robust to its changes."
  - [section 4.1]: Theorem 1 proves that the preference probability p can be expressed in terms of the log-likelihoods of the optimal self-improvement policy π*† and the optimal generative policy π*, without reference to the behavior policy µ.
  - [corpus]: The related paper "Provably Mitigating Corruption, Overoptimization, and Verbosity Simultaneously in Offline and Online RLHF/DPO Alignment" (paper_id 6952) mentions robustness to corruption, supporting the general theme of robust preference optimization.
- Break condition: If the preference probabilities p(y' ≻ y|x) are not accurately estimated or if they are influenced by the behavior policy µ, the independence claim would break.

### Mechanism 2
- Claim: The self-improvement policy π† learned through SRPO enables effective iterative refinement of LLM outputs.
- Mechanism: The self-improvement policy is trained to maximize the preference probability p(y' ≻ y|x) for improving completion y to y', while staying close to a reference policy. This learned improvement mechanism can then be applied recursively to refine generations.
- Core assumption: The self-improvement policy can be effectively learned from pairwise preference data and generalizes to improve completions beyond the training distribution.
- Evidence anchors:
  - [section 3]: Defines the self-improvement model π(y'|y,x) and explains how it is trained to output improved completions.
  - [section 6]: Experiments show that SRPO with self-improvement revisions (1-rev to 5-rev) consistently outperforms baselines without self-improvement.
  - [corpus]: The related paper "SAIL: Self-Improving Efficient Online Alignment of Large Language Models" (paper_id 152552) directly supports the self-improvement mechanism.
- Break condition: If the self-improvement policy overfits to the training data or fails to generalize, the iterative refinement process would not improve performance.

### Mechanism 3
- Claim: The combined loss function LSRPO, which is a convex combination of the self-improvement loss and the generative policy loss, allows for joint optimization of both policies through a single supervised objective.
- Mechanism: By expressing the preference probability p in terms of both π*† and π*, and then minimizing the ℓ2 loss between the left and right sides of the equation, SRPO optimizes both policies simultaneously using standard supervised learning techniques.
- Core assumption: The loss function correctly captures the relationship between the preference probabilities and the policies, and standard supervised learning can effectively minimize this loss.
- Evidence anchors:
  - [section 4.2.3]: Derives the combined loss LSRPO as a convex combination of LL2 and Lself-improve, showing how it optimizes both policies.
  - [section 4.3]: Provides the sample loss bLα for SRPO, which is used in the training algorithm.
  - [section 6]: Describes the implementation details of training SRPO using the combined loss.
- Break condition: If the loss function is not correctly formulated or if the optimization landscape is too complex for standard supervised learning to navigate effectively.

## Foundational Learning

- Concept: KL-regularized reinforcement learning and its analytical solution.
  - Why needed here: The paper uses KL-regularization to constrain the self-improvement policy and derive its analytical form, which is crucial for expressing the preference probability in terms of the policies.
  - Quick check question: What is the analytical solution for the optimal policy in a KL-regularized RL problem, and how is it derived?

- Concept: Preference models and their relationship to human preferences.
  - Why needed here: The paper relies on a preference model p(y' ≻ y|x) that accurately represents human preferences between pairs of completions. Understanding how these models work and how they are estimated is essential.
  - Quick check question: How is the preference model p(y' ≻ y|x) typically estimated from pairwise comparison data, and what are the assumptions behind common preference models like Bradley-Terry?

- Concept: Chain of thought and self-improvement in LLMs.
  - Why needed here: The paper introduces a self-improvement policy that refines LLM outputs iteratively. Understanding how chain-of-thought prompting and self-improvement techniques work in LLMs is important for grasping the motivation and potential benefits of this approach.
  - Quick check question: How do self-improvement techniques like chain-of-thought prompting work in LLMs, and what are the challenges in training models to improve their own outputs?

## Architecture Onboarding

- Component map:
  - LLM base model (e.g., LLaMA-7B) -> generates initial completions
  - Self-improvement policy π† -> takes a completion and context as input and generates an improved completion
  - Generative policy π -> generates completions directly from context
  - Reference policy πref -> used for KL-regularization to keep the learned policies close to a reasonable baseline
  - Preference dataset -> contains pairs of completions (y, y') annotated with human preferences

- Critical path:
  1. Sample a batch of preference data (x, y, y', preference)
  2. Compute the SRPO loss using the current policies π and π†
  3. Update the policies using gradient descent
  4. Generate completions using π and refine them using π†

- Design tradeoffs:
  - Using a single LLM to represent both π and π† simplifies the architecture but may limit the expressiveness of the self-improvement policy
  - The choice of the reference policy πref affects the KL-regularization and the final learned policies
  - The combination coefficient α in the loss function balances the contribution of the self-improvement and generative policy terms

- Failure signatures:
  - If the self-improvement policy fails to improve completions, the win rates will not increase with revisions
  - If the learned policies overfit to the preference data, performance may degrade on out-of-distribution tasks
  - If the optimization is unstable, the win rates may fluctuate or fail to converge

- First 3 experiments:
  1. Train SRPO on a small preference dataset and evaluate the win rates with and without self-improvement revisions to verify the iterative refinement mechanism
  2. Compare the performance of SRPO with different values of the combination coefficient α to understand its impact on the learned policies
  3. Test the robustness of SRPO by training on preference data from one distribution and evaluating on data from a different distribution

## Open Questions the Paper Calls Out
- How does the performance of SRPO compare to other methods on more challenging multi-task benchmarks? The paper mentions this as a future work limitation, stating that SRPO should be more resilient in multi-task benchmarks where existing RLHF methods specialize to specific tasks.
- How sensitive is SRPO's performance to the choice of the combination coefficient α in the loss function? While the paper includes an ablation study on the effect of α, showing that different values affect performance, particularly in OOD settings, a more comprehensive analysis across various scenarios is needed.
- What is the impact of the number of self-improvement iterations (N-revision) on the quality of the final output? The paper shows that SRPO's performance improves with self-improvement iterations (up to 5 revisions), but doesn't explore the limits of this improvement or potential degradation with excessive iterations.

## Limitations
- The paper's claims about task-independent robustness hinge critically on the assumption that preference probabilities are accurately estimated and not influenced by the behavior policy distribution.
- The empirical results, while promising, are based on relatively modest sample sizes (e.g., 125 samples in Arena-Hard).
- The claim about robust generalization to arbitrary task changes is based on experiments testing only summarization variants rather than fundamentally different tasks.

## Confidence
- High confidence: The mathematical derivation of the SRPO objective and the claim that it can be optimized via standard supervised learning
- Medium confidence: The empirical results showing SRPO outperforms baselines, particularly on out-of-distribution tasks
- Low confidence: The claim about robust generalization to arbitrary task changes

## Next Checks
1. Train SRPO on summarization preference data and evaluate on a completely different task domain (e.g., dialogue or code generation) to test the claimed task-independence more rigorously.
2. Compare SRPO with and without the self-improvement mechanism on datasets where initial generations are already high quality to determine whether iterative refinement provides meaningful gains beyond simple generation.
3. Systematically vary the behavior policy used to generate preference data and measure how performance changes, directly testing the claimed robustness to training distribution shifts.