---
ver: rpa2
title: 'JustiLM: Few-shot Justification Generation for Explainable Fact-Checking of
  Real-world Claims'
arxiv_id: '2401.08026'
source_url: https://arxiv.org/abs/2401.08026
tags: []
core_contribution: This paper proposes a novel few-shot justification generation approach
  called JustiLM for explainable fact-checking of real-world claims. The authors address
  the challenge of generating justifications based on retrieved evidence documents
  rather than summarizing fact-check articles, which is more realistic for new claims.
---

# JustiLM: Few-shot Justification Generation for Explainable Fact-Checking of Real-world Claims

## Quick Facts
- arXiv ID: 2401.08026
- Source URL: https://arxiv.org/abs/2401.08026
- Reference count: 40
- Primary result: Novel few-shot justification generation approach for explainable fact-checking using fact-check articles as auxiliary training signals

## Executive Summary
JustiLM addresses the challenge of generating justifications for fact-checking real-world claims by leveraging fact-check articles as auxiliary resources during training. Unlike existing approaches that summarize fact-check articles, JustiLM generates justifications based on retrieved evidence documents, making it more realistic for new claims. The method employs a retrieval-augmented language model trained end-to-end with two types of distillation techniques (article-level and chunk-level) to guide the model using fact-check articles as supervision signals.

## Method Summary
JustiLM is a retrieval-augmented language model that uses fact-check articles as auxiliary information during training through distillation techniques. The model consists of a retriever (Contriever) for evidence retrieval and a T5 encoder-decoder with Fusion-in-Decoder for justification generation. It employs article-level and chunk-level distillation to align fact-check articles with retrieved documents, providing granular supervision signals. The retriever and language model are jointly trained within a single RAG framework, enabling the model to learn which documents are most useful for generating high-quality justifications.

## Key Results
- JustiLM outperforms strong baselines like ICL-enabled LMs and state-of-the-art few-shot RAG model Atlas on the ExClaim dataset
- The model achieves promising performance compared to GPT-4 in justification generation
- A straightforward extension of JustiLM for joint veracity prediction and justification generation improves veracity prediction task performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Distillation techniques enable learning to generate justifications using fact-check articles as auxiliary supervision during training
- Mechanism: Article-level distillation aligns entire fact-check articles with retrieved documents; chunk-level distillation provides granular supervision by aligning individual chunks
- Core assumption: Fact-check articles contain necessary information to guide justification generation based on retrieved evidence
- Evidence anchors:
  - [abstract]: "JustiLM utilizes fact-check articles as auxiliary information in its training only via fine-tuning a pre-trained Retrieval-Augmented Generation (RAG) model"
  - [section]: "distilling information from z as auxiliary supervisory signals for training phase only"
- Break condition: Weak or non-existent alignment between fact-check articles and retrieved documents would fail to provide meaningful supervision signals

### Mechanism 2
- Claim: Joint training of retriever and LM improves justification quality compared to training only the LM
- Mechanism: Joint training within RAG framework allows retriever to learn which documents are most useful for generating high-quality justifications based on LM's performance
- Core assumption: Joint training allows retriever to benefit from LM's supervision signals, leading to better retrieval of relevant documents
- Evidence anchors:
  - [abstract]: "JustiLM is based on a retrieval-augmented language model trained end-to-end"
  - [section]: "advantages of jointly training the retriever and the LM in an end-to-end manner"
- Break condition: If LM supervision signals are not informative or retriever cannot effectively learn from them, joint training would not improve justification generation

### Mechanism 3
- Claim: Few-shot learning enables comparable performance to state-of-the-art fully-trained models with limited training data
- Mechanism: Fine-tuning on small number of training instances (30 shots) while leveraging pre-trained knowledge of RAG model
- Core assumption: Few-shot learning is effective for justification generation task and pre-trained RAG knowledge is sufficient for high-quality justifications
- Evidence anchors:
  - [abstract]: "few-shot Justification generation based on retrieval-augmented Language Model"
  - [section]: "few-shot fine-tuning can mitigate the training resource requirements"
- Break condition: If pre-trained RAG knowledge is insufficient or few-shot learning is ineffective, JustiLM would not achieve comparable performance

## Foundational Learning

- Concept: Retrieval-Augmented Generation (RAG)
  - Why needed here: JustiLM uses RAG framework combining retriever for evidence retrieval and language model for justification generation
  - Quick check question: What are the two main components of a RAG framework and how do they interact during justification generation?

- Concept: Few-shot learning
  - Why needed here: JustiLM employs few-shot learning to achieve comparable performance with limited training data
  - Quick check question: How does few-shot learning differ from traditional fine-tuning, and what are the advantages for justification generation?

- Concept: Distillation techniques
  - Why needed here: JustiLM uses distillation to leverage fact-check articles as auxiliary supervision signals without requiring them during inference
  - Quick check question: What are the two types of distillation techniques employed by JustiLM, and how do they differ in granularity of supervision signals?

## Architecture Onboarding

- Component map: Claim → Retriever (Contriever) → Retrieved Documents → Language Model (T5 with FiD) → Justification
- Critical path: Claim retrieval evidence generation justification
- Design tradeoffs:
  - Joint training of retriever and LM vs. training only the LM: Joint training provides better supervision but requires more computational resources
  - Few-shot learning vs. traditional fine-tuning: Few-shot reduces training resource requirements but may yield lower performance
  - Distillation techniques vs. no distillation: Distillation leverages auxiliary supervision but adds computational overhead during training
- Failure signatures:
  - Poor retrieval performance: Retrieved documents are not relevant to the claim or lack necessary evidence
  - Low-quality justifications: Generated justifications are factually incorrect, incomplete, or incoherent with ground truth
  - High computational overhead: Distillation and joint training lead to excessive computational requirements
- First 3 experiments:
  1. Evaluate retriever performance on held-out test set to ensure relevant document retrieval for justification generation
  2. Compare justification quality using different combinations of distillation techniques to identify most effective approach
  3. Assess impact of varying training shots on JustiLM performance to determine optimal few-shot learning setting

## Open Questions the Paper Calls Out

- How would JustiLM perform under scenarios where gold evidence may be absent from the retrieval corpus?
  - Basis in paper: [explicit] Authors mention this as potential future direction
  - Why unresolved: Current experiments assume needed evidence exists in retrieval corpus
  - What evidence would resolve it: Experiments varying ratio of gold reference documents in retrieval corpus and measuring JustiLM's performance

- Would larger multi-task training dataset or independent veracity classifier jointly trained with retriever and LM improve both tasks simultaneously?
  - Basis in paper: [explicit] Authors suggest these potential solutions for joint veracity prediction
  - Why unresolved: Joint training with veracity prediction doesn't improve justification generation performance
  - What evidence would resolve it: Experiments comparing JustiLM with larger multi-task dataset or independent veracity classifier

- How would JustiLM perform compared to other LLM-based reasoning methods like Chain-of-Thought, Tree-of-Thought, and Graph-of-Thought?
  - Basis in paper: [explicit] Authors mention this as potential future direction
  - Why unresolved: Current experiments focus on JustiLM without incorporating advanced reasoning methods
  - What evidence would resolve it: Experiments comparing JustiLM with and without these reasoning methods

## Limitations
- Distillation technique details are not fully specified, making direct replication challenging
- Evaluation primarily relies on ROUGE-based metrics and MAUVE, which may not fully capture justification quality
- Comparison with GPT-4 lacks detailed performance breakdowns across evaluation dimensions

## Confidence
- High Confidence: Overall framework design and core concept of using fact-check articles as auxiliary training signals
- Medium Confidence: Effectiveness of few-shot learning for this specific task based on experimental results
- Low Confidence: Specific implementation details of distillation techniques and their relative contributions to final performance

## Next Checks
1. Request and verify exact mathematical formulations of article-level and chunk-level distillation losses to ensure faithful reproduction
2. Conduct ablation studies to measure individual contributions of article-level vs chunk-level distillation and assess necessity of both
3. Obtain detailed performance metrics comparing JustiLM against GPT-4 across all evaluation dimensions including factuality checks and computational efficiency trade-offs