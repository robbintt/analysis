---
ver: rpa2
title: Allowing humans to interactively guide machines where to look does not always
  improve human-AI team's classification accuracy
arxiv_id: '2404.05238'
source_url: https://arxiv.org/abs/2404.05238
tags:
- explanations
- users
- chm-corr
- image
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates whether allowing users to interactively
  guide an AI's attention improves human-AI team classification accuracy on CUB-200
  bird images. The researchers developed CHM-Corr++, an interactive interface built
  on CHM-Corr that enables users to edit the classifier's feature importance map and
  observe updated predictions.
---

# Allowing humans to interactively guide machines where to look does not always improve human-AI team's classification accuracy

## Quick Facts
- arXiv ID: 2404.05238
- Source URL: https://arxiv.org/abs/2404.05238
- Reference count: 40
- Allowing humans to interactively guide machines where to look does not always improve human-AI team classification accuracy

## Executive Summary
This study investigates whether allowing users to interactively guide an AI's attention improves human-AI team classification accuracy on CUB-200 bird images. The researchers developed CHM-Corr++, an interactive interface built on CHM-Corr that enables users to edit the classifier's feature importance map and observe updated predictions. In a study with 18 expert ML users performing 1,400 decisions, the interactive approach showed no statistically significant improvement in accuracy over static explanations (73.57% vs 72.68%). Users performed better on correct AI predictions (85-87%) than incorrect ones (59-60%), and the interactive tool was most helpful when it corrected wrong predictions. The findings challenge assumptions about interactivity's benefits and suggest that interactive XAI may not always improve human-AI collaboration. The code and data are open-sourced.

## Method Summary
The study compared static (CHM-Corr) and dynamic (CHM-Corr++) explanations for human-AI team classification on CUB-200 bird images. CHM-Corr++ allows users to edit feature importance maps through a 7×7 patch selection interface using Gradio with custom HTML components. The experiment involved 18 ML experts making 20 decisions each, with 300 correctly classified and 300 misclassified instances from the test set. The interactive tool enables users to control which image patches the model focuses on, with the model re-ranking candidates and updating predictions in real-time. Accuracy was measured across 1,400 total decisions to compare the effectiveness of static versus interactive explanations.

## Key Results
- Interactive explanations showed no statistically significant improvement over static explanations (73.57% vs 72.68%, p=0.749)
- Users achieved much higher accuracy on correct AI predictions (85-87%) than incorrect ones (59-60%)
- Interactivity was most helpful when it changed predictions from incorrect to correct, though this occurred infrequently

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Interactive attention editing does not improve accuracy because the model's underlying kNN structure limits the available candidate classes
- Mechanism: CHM-Corr uses kNN to find 50 nearest training examples, but if the ground truth class is not among these candidates, no amount of attention editing can produce the correct prediction
- Core assumption: The kNN candidate pool is fixed and independent of user attention edits
- Evidence anchors:
  - [section] "Moreover, the underlying nature of the classifier contributes to this issue. Given that the classifier employs a k-Nearest Neighbors (kNN) algorithm to retrieve a set of candidate samples, there is a possibility that the ground-truth class may not appear within the candidate pool."
  - [corpus] Weak evidence - no direct corpus support found for kNN candidate limitations in interactive XAI systems
- Break condition: If the kNN candidate pool could be expanded or updated based on user attention, the mechanism would break

### Mechanism 2
- Claim: Users struggle more with rejecting incorrect predictions than accepting correct ones, limiting the benefit of interactivity
- Mechanism: Cognitive bias leads users to trust AI predictions even when wrong, so interactivity that maintains incorrect predictions reinforces this bias
- Core assumption: Human-AI team accuracy is limited by user inability to detect and reject AI errors
- Evidence anchors:
  - [section] "We find participants' decision accuracy on correct instances is much higher than that on incorrect ones for both types of explanations: 85.21% vs. 60.13% with static, 86.79% vs. 59.39% with dynamic."
  - [section] "This result is consistent with prior findings that users tend to accept AI predictions as correct even when they are incorrect"
- Break condition: If users could be trained to better detect AI errors, or if the system could flag high-confidence incorrect predictions

### Mechanism 3
- Claim: Interactivity is most helpful when it changes predictions from incorrect to correct, but this occurs infrequently
- Mechanism: The interactive tool helps most when user attention edits cause the model to switch from wrong to right prediction, but this scenario is rare in the dataset
- Core assumption: The model's initial predictions are often correct, leaving few opportunities for user intervention to improve accuracy
- Evidence anchors:
  - [section] "When the model's new predictions are always incorrect, participants' decision accuracy is 62.11%. But when the model eventually becomes correct, participants' decision accuracy goes up to 65.43%."
  - [section] "As such, understanding when users can and cannot help the model be more accurate, and aiding users in the process, would be important directions for future research."
- Break condition: If the dataset had more instances where user attention could meaningfully correct model errors

## Foundational Learning

- Concept: k-Nearest Neighbors classification
  - Why needed here: The CHM-Corr classifier uses kNN to find similar training examples, which determines the candidate predictions available to the user
  - Quick check question: What happens to CHM-Corr's predictions if the ground truth class is not among the k nearest neighbors?

- Concept: Feature importance maps and attention mechanisms
  - Why needed here: The study tests whether allowing users to edit feature importance maps (attention) improves classification accuracy
  - Quick check question: How does CHM-Corr++ allow users to control which image patches the model focuses on?

- Concept: Statistical significance testing
  - Why needed here: The study reports no significant difference between static and dynamic explanations using t-tests
  - Quick check question: What does a p-value of 0.749 tell us about the difference between static and dynamic explanation accuracy?

## Architecture Onboarding

- Component map: User interface → CHM-Corr model → kNN candidate retrieval → Patch similarity computation → Prediction output → Updated UI display
- Critical path: User patch selection → Model re-ranking of candidates → Updated prediction and explanations → User decision
- Design tradeoffs: Interactive editing vs. model accuracy vs. user cognitive load
- Failure signatures: Model maintains incorrect predictions despite attention edits; ground truth class not in candidate pool; user bias toward accepting AI predictions
- First 3 experiments:
  1. Test on dataset where ground truth is frequently missing from kNN candidates to measure baseline limitation
  2. Compare accuracy when users can add custom examples to candidate pool vs. fixed kNN candidates
  3. Measure accuracy improvement when system flags high-confidence incorrect predictions for user rejection

## Open Questions the Paper Calls Out
None

## Limitations
- The kNN candidate pool limitation may not apply to all interactive XAI systems
- The study's findings may not generalize beyond the CUB-200 bird classification domain
- User bias toward accepting AI predictions is well-documented but may vary across domains

## Confidence

**Confidence: Medium** - The study demonstrates a null result but with limitations in generalizability. The kNN candidate pool limitation (Mechanism 1) represents a fundamental architectural constraint that may not apply to all interactive XAI systems. The user bias toward accepting AI predictions (Mechanism 2) is well-documented in prior work but may vary across domains and user expertise levels. The finding that interactivity is most helpful when correcting wrong predictions (Mechanism 3) is limited by the dataset composition and the specific CHM-Corr architecture.

**Sample Size Concerns**: With 18 participants making 20 decisions each (1,400 total), the study has reasonable statistical power for the main comparison, but the breakdown of correct vs incorrect AI predictions (600 each) may limit detection of subtle effects.

**Domain Specificity**: The CUB-200 bird classification task represents a specific domain where attention mechanisms are particularly relevant, but results may not generalize to other domains where different types of explanations might be more beneficial.

## Next Checks

1. **Expand Candidate Pool Test**: Conduct an experiment where users can add custom examples to the kNN candidate pool beyond the 50 nearest neighbors, to determine if the accuracy limitation is truly due to candidate pool constraints.

2. **Cross-Domain Replication**: Replicate the study on a different classification task (e.g., medical imaging or satellite imagery) to test generalizability of the null result across domains with different explanation requirements.

3. **User Training Intervention**: Implement a brief training module to help users better identify and reject incorrect AI predictions, then measure if this changes the accuracy patterns observed in the current study.