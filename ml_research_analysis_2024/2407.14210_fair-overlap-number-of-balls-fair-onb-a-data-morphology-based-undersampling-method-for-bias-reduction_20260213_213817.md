---
ver: rpa2
title: 'Fair Overlap Number of Balls (Fair-ONB): A Data-Morphology-based Undersampling
  Method for Bias Reduction'
arxiv_id: '2407.14210'
source_url: https://arxiv.org/abs/2407.14210
tags:
- data
- protected
- density
- class
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper proposes Fair-ONB, a preprocessing method for bias reduction
  in machine learning models with protected features. It uses data morphology via
  Overlap Number of Balls (ONB) coverage to identify and undersample instances in
  overlap areas between groups defined by class and protected feature combinations.
---

# Fair Overlap Number of Balls (Fair-ONB): A Data-Morphology-based Undersampling Method for Bias Reduction

## Quick Facts
- arXiv ID: 2407.14210
- Source URL: https://arxiv.org/abs/2407.14210
- Reference count: 33
- The method improves fairness (Disparate Impact) with minimal impact on predictive performance using data morphology-based undersampling.

## Executive Summary
Fair-ONB is a preprocessing method for bias reduction in machine learning models with protected features. It uses data morphology via Overlap Number of Balls (ONB) coverage to identify and undersample instances in overlap areas between groups defined by class and protected feature combinations. By applying percentile thresholds on coverage ball attributes like radius, number of covered instances, and density, it selectively removes instances that may contribute to bias. Experiments on COMPAS, Adult, German, and Ricci datasets show that Fair-ONB improves model fairness (measured via Disparate Impact) with minimal impact on predictive performance (AUC and accuracy), often outperforming the state-of-the-art FAWOS method.

## Method Summary
Fair-ONB is a preprocessing method that reduces bias in machine learning models by undersampling instances in overlap regions between protected groups. It uses Overlap Number of Balls (ONB) coverage to identify these regions and applies percentile thresholds on ball attributes (radius, number of covered instances, density) to guide undersampling. The method groups data by class and protected feature combinations, then removes instances from favored groups in high-overlap, low-density, or low-coverage areas. This approach aims to balance group treatment without removing informative boundaries, improving fairness metrics while maintaining predictive performance.

## Key Results
- Fair-ONB improves model fairness (Disparate Impact) while maintaining predictive performance (AUC and accuracy).
- The method outperforms the state-of-the-art FAWOS method on multiple datasets (COMPAS, Adult, German, Ricci).
- Percentile-based thresholds on ball attributes provide better adaptability to dataset-specific overlap patterns than fixed cutoffs.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fair-ONB improves fairness by selectively undersampling instances in overlap regions between protected groups, reducing bias without sacrificing predictive performance.
- Mechanism: Uses Overlap Number of Balls (ONB) coverage to identify overlap areas, then removes instances in high-overlap, low-density, or low-coverage regions from favored groups.
- Core assumption: Overlap regions contain the most bias-inducing samples, and removing them balances group treatment without removing informative boundaries.
- Evidence anchors:
  - [abstract] "employs attributes of the ball coverage of the groups, such as the radius, number of covered instances and density, to select the most suitable areas for undersampling and reduce bias."
  - [section 3.2] "The hypothesis of this work is that, to balance model performance on different data groups, the focus must be on the group overlap areas of the problem."
  - [corpus] Weak evidence; no direct citations or similar methods in neighbor papers.
- Break condition: If overlap areas also contain critical class boundaries, undersampling could harm accuracy; or if density thresholds misidentify noisy points as overlap, leading to overfitting.

### Mechanism 2
- Claim: Fair-ONB achieves better fairness than random undersampling by using data morphology to inform instance selection.
- Mechanism: Percentile thresholds on ball attributes (radius, number of instances covered, density) guide undersampling, adapting to dataset-specific overlap patterns.
- Core assumption: Percentile-based thresholds are more stable and adaptive than fixed cutoffs, and ball coverage morphology reliably identifies problematic regions.
- Evidence anchors:
  - [section 3.3] "Using percentiles instead of predefined values aims to have a better adaptability to each dataset."
  - [section 5.1] "Choosing the right number of covered instances threshold is always very important... the best result is obtained without group balance, which is what most other methods... would pursue."
  - [corpus] No direct support; neighbor papers focus on different sampling strategies.
- Break condition: If morphological metrics are noisy or do not correlate with bias, or if percentile thresholds are not robust to data variation.

### Mechanism 3
- Claim: Fair-ONB improves fairness by undersampling favored groups defined by combinations of class and protected feature values.
- Mechanism: Groups are formed by class-protected feature pairs, and undersampling is applied to those with favorable bias using union or intersection logic.
- Core assumption: Fairness metrics (like Disparate Impact) are most meaningfully applied to these group combinations, and balancing them improves model fairness.
- Evidence anchors:
  - [section 3.2] "The aim is to see how those groups are distributed, since, depending on their overlap and their degrees of imbalance, the classifier learned using the dataset can be biased."
  - [section 5.2] "As can be observed, the Fair-ONB method obtains the best results according to Global Adapted Disparate Impact... very often, also the best Adapted Disparate Impact on the individual protected features."
  - [corpus] No direct support; neighbor papers use different group definitions or metrics.
- Break condition: If fairness gains are achieved at the cost of significant accuracy loss, or if group definitions do not capture intersectional biases.

## Foundational Learning

- Concept: Overlap Number of Balls (ONB) coverage
  - Why needed here: Fair-ONB relies on ONB to characterize group overlap and guide undersampling; understanding its mechanics is essential to tuning and debugging.
  - Quick check question: How does ONB construct balls, and what does the ball radius signify in overlap estimation?

- Concept: Disparate Impact (DI) and Adapted Disparate Impact (ADI)
  - Why needed here: DI/ADI are the primary fairness metrics used to evaluate and guide Fair-ONB; correct interpretation is needed for parameter selection.
  - Quick check question: What DI value range is considered fair, and how is ADI derived from DI?

- Concept: Percentile-based thresholding for undersampling
  - Why needed here: Fair-ONB uses percentiles on ball attributes; understanding this avoids arbitrary cutoffs and improves adaptability.
  - Quick check question: Why use percentiles instead of fixed thresholds, and how do different percentile choices affect undersampling?

## Architecture Onboarding

- Component map: Data preprocessing pipeline -> ONB coverage generation -> Group definition (class Ã— protected feature) -> Percentile threshold selection -> Undersampling (union/intersection) -> Classifier training -> Fairness and accuracy evaluation
- Critical path:
  1. Compute ONB ball coverage for each group
  2. Select undersampling groups based on DI
  3. Apply percentile thresholds to ball attributes
  4. Remove instances in selected balls
  5. Train classifier and evaluate
- Design tradeoffs:
  - Union vs intersection undersampling: Union may oversample, intersection may undersample; tradeoff is bias reduction vs group representation.
  - Percentile choice: Low percentiles remove more instances but risk losing informative data; high percentiles are safer but may not reduce bias enough.
  - ONB vs other morphology methods: ONB is interpretable but may be slower; alternative methods may be faster but less explainable.
- Failure signatures:
  - Accuracy drops sharply: Possible over-undersampling or removal of key boundaries.
  - Fairness does not improve: Possible incorrect group definitions or thresholds not aligned with bias sources.
  - ONB coverage fails: Dataset too large, sparse, or high-dimensional for ball generation.
- First 3 experiments:
  1. Baseline: Train classifier on raw data, record DI and AUC.
  2. Fair-ONB with union undersampling, radius percentile=5, num_inst=1, density=2.4; compare to baseline.
  3. Fair-ONB with intersection undersampling, radius percentile=10, num_inst=2, density=4.8; compare to baseline and experiment 2.

## Open Questions the Paper Calls Out
None explicitly stated in the paper.

## Limitations
- Limited validation that overlap areas are the primary source of bias; no comparisons to methods targeting different regions.
- Percentile thresholds are presented as adaptive but not rigorously justified; no sensitivity analysis provided.
- Group definitions based on class-protected feature pairs may miss intersectional biases.

## Confidence
- Claim: Fair-ONB improves fairness with minimal accuracy loss. Confidence: Medium (experimental results promising but mechanism not fully validated, no ablation studies).
- Claim: Percentile thresholds provide better adaptability. Confidence: Low (rationale stated but not empirically supported).
- Claim: Group definition strategy captures meaningful fairness metrics. Confidence: Medium (aligns with standard definitions but may not capture all bias sources).

## Next Checks
1. Perform an ablation study removing the overlap-based undersampling and measure the impact on fairness and accuracy.
2. Conduct a sensitivity analysis on percentile thresholds to determine their effect on model performance and robustness.
3. Compare Fair-ONB to a method that undersamples based on alternative morphological or distance-based criteria to assess the unique contribution of the ONB approach.