---
ver: rpa2
title: Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks
arxiv_id: '2406.12066'
source_url: https://arxiv.org/abs/2406.12066
tags:
- names
- brand
- dataset
- medqa
- drug
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the robustness of large language models (LLMs)
  in medical contexts, specifically focusing on their ability to handle drug name
  variations. The authors created RABBITS, a dataset designed to evaluate LLM performance
  when drug names are swapped between brand and generic equivalents.
---

# Language Models are Surprisingly Fragile to Drug Names in Biomedical Benchmarks

## Quick Facts
- arXiv ID: 2406.12066
- Source URL: https://arxiv.org/abs/2406.12066
- Reference count: 31
- LLMs show 1-10% accuracy drop when drug names are swapped between brand and generic forms

## Executive Summary
This paper reveals a surprising fragility in large language models when handling drug name variations in biomedical contexts. The authors created RABBITS, a benchmark that tests LLM robustness by swapping brand and generic drug names in medical questions. Despite the semantic equivalence of these terms, models consistently show performance drops of 1-10% on established medical benchmarks. The study also identifies substantial dataset contamination as a likely source of this fragility, with test data appearing extensively in pre-training corpora. The findings highlight the importance of evaluating LLMs not just on raw accuracy but on their ability to handle nomenclature variations that commonly occur in clinical practice.

## Method Summary
The study created the RABBITS benchmark by extracting drug name pairs from RxNorm ontology and transforming MedQA and MedMCQA test datasets through systematic brand/generic name swaps. Expert physicians reviewed all transformations to ensure semantic equivalence. Models were evaluated zero-shot using the EleutherAI lm-evaluation harness, comparing performance on original versus transformed datasets. The authors also conducted extensive contamination analysis using n-gram overlap detection to measure how much benchmark test data appeared in pre-training corpora.

## Key Results
- LLMs show consistent 1-10% accuracy drops when drug names are swapped between brand and generic forms
- Larger models exhibit more severe drops (5-10%) compared to smaller models (1-5%), despite better overall performance
- Substantial dataset contamination found: 99.21% overlap in MedQA and 34.13% in MedMCQA test sets with pre-training data
- Generic drug names appear more frequently than brand names in pre-training datasets (564,151 vs 234,138 mentions in Dolma)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs rely more on memorization of exact terminology than semantic understanding of drug names
- Mechanism: When drug names are swapped between brand and generic forms, the exact tokens in the input change. If the model's knowledge is stored as memorized text patterns rather than abstract semantic representations, these token-level changes can disrupt the retrieval of relevant information, causing accuracy drops.
- Core assumption: The model stores brand and generic drug names as separate, non-overlapping representations rather than mapping them to a shared semantic concept
- Evidence anchors:
  - [abstract] "This is concerning given that patients commonly use brand names and are less likely to spot errors"
  - [section] "LLM performance may be driven by memorization and not reasoning ability"
  - [corpus] Found 25 related papers, average neighbor FMR=0.401, suggesting moderate corpus similarity but limited direct citations
- Break condition: The model would need to demonstrate consistent performance across drug name variations if it had true semantic understanding, which the 1-10% accuracy drop contradicts

### Mechanism 2
- Claim: Pre-training data contamination creates an implicit mapping between brand and generic names
- Mechanism: When benchmark test questions appear in pre-training data, the model learns to associate specific question-answer patterns with exact drug names. This creates an artificial correlation between brand/generic pairs that doesn't reflect true semantic understanding, leading to fragility when names are swapped.
- Core assumption: The model's performance on medical benchmarks is artificially inflated by memorization of contaminated data rather than genuine reasoning
- Evidence anchors:
  - [section] "We identify a potential source for this fragility: Open pretraining datasets contain substantial amounts of benchmark test data"
  - [section] "Dataset contamination are 99.21% and 34.13% in the MedQA and MedMCQA test datasets, respectively"
  - [corpus] No direct evidence of contamination patterns, but high FMR scores suggest related work exists
- Break condition: If contamination were removed, we would expect to see more consistent performance across brand/generic variations rather than the observed 4% average drop

### Mechanism 3
- Claim: Model architecture and training focus creates bias toward generic terminology
- Mechanism: Pre-training datasets contain more generic drug mentions than brand names (564,151 vs 234,138 in Dolma dataset). This imbalance means the model is exposed to generic terminology more frequently during training, creating an implicit preference that makes brand name substitutions more disruptive.
- Core assumption: The frequency of exposure during pre-training directly influences the model's confidence and accuracy with different terminology variants
- Evidence anchors:
  - [section] "Generic names are more common than brand names in these pre-training datasets"
  - [section] "Table 1 shows our overall dataset swapping statistics where we observe benchmark questions overwhelmingly use generic terms"
  - [corpus] Weak evidence - the corpus shows related work but doesn't directly address terminology frequency effects
- Break condition: If the model had been trained with balanced brand/generic exposure, we would expect smaller performance differences between terminology variants

## Foundational Learning

- Concept: Drug nomenclature mapping
  - Why needed here: Understanding how brand and generic drug names relate is crucial for evaluating whether LLMs can handle these variations correctly
  - Quick check question: Can you explain why "ibuprofen" and "Advil" refer to the same medication but might be treated differently by a language model?

- Concept: Dataset contamination detection
  - Why needed here: The study identifies contamination as a key factor in model fragility, requiring understanding of how to detect and measure it
  - Quick check question: How would you use n-gram overlap to determine if test data appears in training corpora?

- Concept: Robustness evaluation metrics
  - Why needed here: The RABBITS benchmark specifically measures performance differences when terminology is swapped, requiring understanding of appropriate evaluation methods
  - Quick check question: What does a 4% accuracy drop when swapping brand/generic names tell you about a model's robustness?

## Architecture Onboarding

- Component map: RxNorm drug pair extraction -> Expert physician review -> Regex-based dataset transformation -> EleutherAI lm-evaluation harness -> Zero-shot performance benchmarking
- Critical path: Accurate drug pair identification -> Careful dataset transformation -> Contamination detection -> Performance benchmarking
- Design tradeoffs: The study chose not to release the full transformed dataset to prevent further contamination, prioritizing research integrity over open access
- Failure signatures: Performance drops of 1-10% when drug names are swapped, larger drops in MedMCQA vs MedQA suggesting contamination effects, and scaling patterns showing larger models have more severe drops despite better overall performance
- First 3 experiments:
  1. Replicate the contamination analysis using Infini-gram API to measure n-gram overlap between pre-training data and benchmark datasets
  2. Test model performance on drug name mapping tasks (brand-to-generic and generic-to-brand) to establish baseline knowledge
  3. Run the RABBITS benchmark on a new model variant to compare performance patterns against established models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific strategies or methods could effectively mitigate the performance drop observed in LLMs when drug names are swapped between brand and generic equivalents?
- Basis in paper: [explicit] The authors identify a performance drop of 1-10% when drug names are swapped and note the importance of robustness evaluations.
- Why unresolved: The paper identifies the problem and its potential source (dataset contamination) but does not propose specific solutions to address the performance drop.
- What evidence would resolve it: Experiments testing various data augmentation techniques, improved fine-tuning strategies, or novel architectural approaches that demonstrate reduced performance degradation when drug names are swapped.

### Open Question 2
- Question: How does the contamination of test data in pre-training datasets specifically impact the reasoning abilities of LLMs in medical contexts, and can this be distinguished from memorization?
- Basis in paper: [explicit] The authors identify dataset contamination as a potential source of fragility and note that larger models show greater sensitivity to g2b swaps.
- Why unresolved: While contamination is identified as a problem, the paper does not provide a detailed analysis of how this contamination affects reasoning versus memorization, nor does it offer methods to distinguish between the two.
- What evidence would resolve it: Studies using controlled experiments to compare model performance on contaminated versus uncontaminated data, and analyses that differentiate between reasoning-based and memorization-based responses.

### Open Question 3
- Question: To what extent can the robustness of LLMs to drug name variations be generalized to other medical terminology and synonyms beyond pharmaceuticals?
- Basis in paper: [explicit] The authors limit their evaluation to biomedical datasets focusing only on pharmaceuticals and suggest future work to extend this approach.
- Why unresolved: The paper's findings are specific to drug names, and there is no exploration of how these results might apply to other types of medical terminology or synonyms.
- What evidence would resolve it: Extending the RABBITS dataset to include other medical terms (e.g., anatomical terms, disease names) and evaluating model performance across these new categories to assess generalizability.

## Limitations

- Evaluation limited to only two biomedical benchmarks (MedQA and MedMCQA) may not represent full diversity of medical knowledge domains
- 1-10% accuracy drop, while statistically significant, represents relatively small absolute change that could be influenced by noise
- Relies on RxNorm for drug name mappings, which may not capture all clinical terminology variations used in practice
- Contamination analysis may underestimate true contamination levels due to limitations of n-gram overlap detection methods

## Confidence

**High Confidence**: The observation that LLMs experience accuracy drops when drug names are swapped is well-supported by empirical results across multiple model sizes and datasets. The contamination analysis showing substantial overlap between pre-training data and benchmark test sets is also robust, with specific percentages (99.21% for MedQA, 34.13% for MedMCQA) providing concrete evidence.

**Medium Confidence**: The proposed mechanisms explaining why models are fragile to drug name variations are plausible but not definitively proven. The study shows correlation between contamination and performance drops, but cannot definitively prove causation. The hypothesis that memorization rather than semantic understanding drives performance is supported by the results but requires additional experimental validation.

**Low Confidence**: The claim that this fragility represents a fundamental limitation of LLM architecture rather than a training data artifact is not well-supported. The study does not test whether models trained on balanced brand/generic data would show different robustness patterns, leaving open the possibility that data curation rather than model design is the primary factor.

## Next Checks

1. **Contamination Impact Validation**: Conduct a controlled experiment where benchmark contamination is systematically removed from pre-training data, then measure whether the drug name robustness gap persists. This would directly test whether contamination explains the observed fragility.

2. **Semantic Understanding Test**: Design a drug mapping task where models must identify semantic equivalence between brand and generic names in isolation (without full medical context). This would help determine whether the models have the underlying knowledge but struggle with context transfer, or lack the knowledge entirely.

3. **Data Distribution Analysis**: Analyze the frequency and context of brand vs generic drug mentions in both pre-training data and benchmark datasets to quantify whether terminology bias could explain the observed patterns independently of contamination effects.