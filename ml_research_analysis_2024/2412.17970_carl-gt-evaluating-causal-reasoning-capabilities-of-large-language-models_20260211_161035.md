---
ver: rpa2
title: 'CARL-GT: Evaluating Causal Reasoning Capabilities of Large Language Models'
arxiv_id: '2412.17970'
source_url: https://arxiv.org/abs/2412.17970
tags:
- causal
- reasoning
- tasks
- llms
- graph
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper introduces CARL-GT, a benchmark designed to evaluate
  causal reasoning capabilities of large language models (LLMs) using graphs and tabular
  data. Unlike existing benchmarks that focus on conversational tasks, academic math
  tests, and coding tests, CARL-GT assesses LLMs'' abilities to solve real-world problems
  by testing three key aspects: causal graph reasoning, knowledge discovery, and decision-making.'
---

# CARL-GT: Evaluating Causal Reasoning Capabilities of Large Language Models

## Quick Facts
- arXiv ID: 2412.17970
- Source URL: https://arxiv.org/abs/2412.17970
- Reference count: 13
- This paper introduces CARL-GT, a benchmark designed to evaluate causal reasoning capabilities of large language models (LLMs) using graphs and tabular data.

## Executive Summary
This paper introduces CARL-GT, a benchmark designed to evaluate causal reasoning capabilities of large language models (LLMs) using graphs and tabular data. Unlike existing benchmarks that focus on conversational tasks, academic math tests, and coding tests, CARL-GT assesses LLMs' abilities to solve real-world problems by testing three key aspects: causal graph reasoning, knowledge discovery, and decision-making. The benchmark includes diverse tasks, such as estimating adjacency matrices, d-separations, causal directions, and performing intervention and counterfactual reasoning. Effective zero-shot learning prompts are developed for these tasks, and the benchmark is evaluated on open-source LLMs like Llama3-8B, Qwen2-7B, Gemma2-9B, Mistral-7B, and Mixtral-8x7B.

## Method Summary
CARL-GT evaluates causal reasoning in LLMs through three categories of tasks: causal graph reasoning (estimating adjacency matrices, d-separations, causal directions), knowledge discovery (causal feature importance, causal effects), and decision-making (intervention, counterfactual reasoning). The benchmark uses both graph-based and tabular data formats. Zero-shot prompting strategies were developed for each task type, and evaluations were conducted on five open-source LLM models ranging from 7B to 70B parameters.

## Key Results
- LLMs show significant weaknesses in causal reasoning, particularly with tabular data formats
- Model performance varies considerably across different task types, with some tasks proving more challenging than others
- Tasks in different categories (causal graph reasoning, knowledge discovery, decision-making) show stronger correlation than tasks within the same category

## Why This Works (Mechanism)
The benchmark's effectiveness stems from its comprehensive evaluation of causal reasoning through multiple task types and data formats. By testing both graph-based and tabular data, it captures different aspects of causal inference that LLMs must handle. The zero-shot prompting approach allows for standardized evaluation across models without requiring task-specific fine-tuning.

## Foundational Learning
1. **Causal Graphs** - Why needed: Core representation for modeling causal relationships; Quick check: Can model identify parent-child relationships in DAGs
2. **d-Separation** - Why needed: Determines conditional independence in causal graphs; Quick check: Can model correctly identify blocked paths
3. **Intervention vs Counterfactual** - Why needed: Different causal queries require different reasoning; Quick check: Can model distinguish do-calculus from observational queries
4. **Tabular Causal Inference** - Why needed: Real-world data often comes in tabular format; Quick check: Can model extract causal structure from raw data
5. **Causal Effect Estimation** - Why needed: Quantifying causal impact is crucial for decision-making; Quick check: Can model estimate treatment effects accurately
6. **Zero-shot Prompting** - Why needed: Enables standardized evaluation without task-specific tuning; Quick check: Does prompt consistently elicit correct reasoning patterns

## Architecture Onboarding

**Component Map**: Data Generation -> Task Formulation -> Zero-shot Prompting -> LLM Evaluation -> Performance Analysis

**Critical Path**: The evaluation pipeline flows from generating diverse causal scenarios, through task-specific prompt formulation, to LLM responses, and finally performance analysis across models and tasks.

**Design Tradeoffs**: Zero-shot prompting provides standardization but may underutilize model capabilities compared to fine-tuning. Graph-based tasks are more structured but less representative of real-world tabular data challenges.

**Failure Signatures**: Common failures include incorrect identification of causal directions, confusion between correlation and causation, difficulty with counterfactual reasoning, and poor performance on tabular data compared to graph-based tasks.

**First Experiments**: 
1. Test basic causal graph reasoning (adjacency matrix estimation) on simple DAGs
2. Evaluate d-separation identification in known graph structures
3. Compare model performance on graph-based versus tabular causal inference tasks

## Open Questions the Paper Calls Out
None

## Limitations
- Benchmark performance evaluations based on limited set of open-source LLMs
- Zero-shot prompting strategies may not represent full potential of fine-tuned models
- Focus on specific causal reasoning tasks may not capture full spectrum of real-world causal inference challenges

## Confidence
- **High confidence**: Core finding that LLMs demonstrate significant weaknesses in causal reasoning, particularly with tabular data
- **Medium confidence**: Benchmark design and ability to comprehensively evaluate causal reasoning capabilities
- **Low confidence**: Relative performance rankings between specific models

## Next Checks
1. Test the benchmark on additional LLM architectures and proprietary models to assess result generalizability
2. Evaluate the impact of different prompting strategies and fine-tuning approaches on model performance
3. Conduct a more granular analysis of task difficulty to distinguish between challenges stemming from causal reasoning versus other factors like mathematical computation or data format handling