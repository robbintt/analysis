---
ver: rpa2
title: Shared-unique Features and Task-aware Prioritized Sampling on Multi-task Reinforcement
  Learning
arxiv_id: '2406.00761'
source_url: https://arxiv.org/abs/2406.00761
tags:
- tasks
- learning
- performance
- stars
- task
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the performance imbalance issue in multi-task
  reinforcement learning (MTRL), where state-of-the-art methods achieve impressive
  average performance but perform poorly on individual tasks. The authors propose
  STARS, a method that combines a shared-unique feature extractor and task-aware prioritized
  sampling.
---

# Shared-unique Features and Task-aware Prioritized Sampling on Multi-task Reinforcement Learning

## Quick Facts
- arXiv ID: 2406.00761
- Source URL: https://arxiv.org/abs/2406.00761
- Reference count: 40
- This paper addresses the performance imbalance issue in multi-task reinforcement learning, where state-of-the-art methods achieve impressive average performance but perform poorly on individual tasks.

## Executive Summary
This paper addresses the performance imbalance issue in multi-task reinforcement learning (MTRL), where state-of-the-art methods achieve impressive average performance but perform poorly on individual tasks. The authors propose STARS, a method that combines a shared-unique feature extractor and task-aware prioritized sampling. The shared-unique feature extractor learns both shared and task-specific features to improve knowledge synergy, while the task-aware sampling strategy dynamically adjusts the number of samples from each task based on their current performance. Experiments on the Meta-World benchmark show that STARS statistically outperforms previous state-of-the-art methods, achieving a 6.5% improvement on the MT-10 track and demonstrating higher stability across tasks.

## Method Summary
STARS combines a shared-unique feature extractor and task-aware prioritized sampling to address performance imbalance in MTRL. The method uses a shared parameter set Φ to extract task-focused shared features and task-specific vectors to capture unique features. Task-aware sampling dynamically adjusts the number of transitions drawn from each task's replay buffer based on current performance, using both transition-level and task-level priorities. The method is built on SAC and evaluated on Meta-World's MT-10 and MT-50 tracks.

## Key Results
- STARS achieves 6.5% improvement over state-of-the-art methods on MT-10 track
- Statistically outperforms current SOTA methods on Meta-World benchmark
- Demonstrates higher stability across tasks with reduced performance imbalance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: STARS reduces performance imbalance by learning both shared and unique task features simultaneously.
- Mechanism: The shared-unique feature extractor splits representation learning into two streams—one for cross-task knowledge and one for task-specific details—allowing each task to receive the right mix of generalizable and specialized guidance.
- Core assumption: Task performance depends on having access to both shared knowledge (e.g., gripper control) and unique features (e.g., task-specific object geometry).
- Evidence anchors:
  - [abstract] "shared-unique feature extractor learns both shared and task-specific features to enable better synergy of knowledge between different tasks"
  - [section] "We develop a shared-unique feature extractor to extract two informative features from each state: task-focused shared features and task-unique features"
- Break condition: If tasks do not benefit from both shared and unique features, or if the shared pool dominates unique details, the feature extractor will fail to address imbalance.

### Mechanism 2
- Claim: STARS dynamically shifts training focus toward poorly performing tasks.
- Mechanism: The task-aware sampling strategy adjusts the number of transitions drawn from each task's replay buffer based on current performance, increasing sampling from tasks with low success rates.
- Core assumption: Tasks can be ranked by performance, and sampling proportionally to this rank will reduce variance across tasks.
- Evidence anchors:
  - [abstract] "task-aware sampling strategy dynamically adjusts the number of samples from each task based on their current performance"
  - [section] "we developed a multi-task variant of PER...includes two levels of priorities: transition priority and task priority"
- Break condition: If performance ranking is noisy or if sampling adjustments lag behind rapid changes in task difficulty, the imbalance will persist.

### Mechanism 3
- Claim: STARS improves stability by reducing variance in per-task performance.
- Mechanism: By combining shared and unique features with adaptive sampling, STARS ensures that no single task is ignored or under-resourced during training, leading to lower standard deviation across tasks.
- Core assumption: Uniform sampling and single-feature learning create a "rich-get-richer" dynamic where easy tasks dominate and hard tasks fall behind.
- Evidence anchors:
  - [abstract] "demonstrates higher stability across tasks" and "alleviates the performance imbalance issue"
  - [section] "From the results, our STARS statistically outperforms current SOTA methods and alleviates the performance imbalance issue"
- Break condition: If shared knowledge is insufficient or task performance metrics are unstable, variance reduction will not materialize.

## Foundational Learning

- Concept: Multi-task reinforcement learning and the performance imbalance problem.
  - Why needed here: STARS is designed to solve a specific failure mode in MTRL—some tasks perform much worse than others, even when average performance looks good.
  - Quick check question: Why might averaging performance across tasks hide poor performance on individual tasks?

- Concept: Prioritized experience replay and multi-level priority schemes.
  - Why needed here: STARS uses both transition-level and task-level priorities to guide sampling, so understanding PER is essential.
  - Quick check question: How does PER decide which transitions to sample more often, and why might this need adaptation for multi-task settings?

- Concept: Contrastive learning and triplet loss for representation learning.
  - Why needed here: The unique feature extractor uses triplet loss to encourage task-specific feature separation, so familiarity with metric learning helps.
  - Quick check question: What does triplet loss do, and why would it help keep task-specific features distinct?

## Architecture Onboarding

- Component map: Shared-unique feature extractor (PaCo-style shared pool + separate task-unique head) -> Task-aware sampling strategy (task-level + transition-level priorities) -> Multi-task prioritized replay buffer (one per task) -> SAC actor-critic backbone

- Critical path:
  1. Collect transitions from all tasks
  2. Store in per-task replay buffers
  3. Compute task priorities and allocate sampling quotas
  4. Sample transitions and compute features
  5. Train SAC policy with both shared and unique features
  6. Update priorities based on TD errors

- Design tradeoffs:
  - Using a fixed shared knowledge pool vs. task-specific modules (balance between generalization and specialization)
  - Sampling equally vs. performance-proportionally (stability vs. rapid correction of imbalance)
  - Extra computational cost of dual feature extraction vs. improved per-task performance

- Failure signatures:
  - High variance in task performance despite improved average
  - Task priorities converge to zero for certain tasks (they get ignored)
  - Feature extractor collapses to shared features only (unique features become uninformative)

- First 3 experiments:
  1. Run STARS with only shared features (disable unique features) to verify improvement over single-feature baselines.
  2. Run STARS with only task-unique features to see if shared features are actually necessary.
  3. Disable task-aware sampling and use uniform sampling to measure the contribution of dynamic prioritization.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the number of learnable parameters (P) and the number of knowledge units (K) in the shared parameter set Φ affect the performance of STARS on multi-task reinforcement learning?
- Basis in paper: [explicit] The paper mentions that Φ ∈ ℝ^(P×K) captures K knowledge, each characterized by P learnable parameters.
- Why unresolved: The paper does not explore the sensitivity of STARS' performance to variations in P and K, leaving the optimal configuration unclear.
- What evidence would resolve it: Conducting experiments with varying P and K values and analyzing the resulting performance metrics would provide insights into the optimal configuration for different task complexities.

### Open Question 2
- Question: How does STARS perform on multi-task reinforcement learning tasks with more than 50 tasks, and what are the limitations of its current architecture in handling larger task sets?
- Basis in paper: [inferred] The paper mentions that STARS achieves the best results on the MT-50 track but notes that the performance gap with other methods is not statistically significant, and that effectively utilizing shared and unique knowledge across more tasks is a future research direction.
- Why unresolved: The paper does not provide results or analysis for tasks with more than 50 tasks, and the limitations of the current architecture in handling larger task sets are not explicitly discussed.
- What evidence would resolve it: Extending STARS to handle tasks with more than 50 tasks and analyzing its performance, as well as identifying the architectural bottlenecks, would clarify its scalability and limitations.

### Open Question 3
- Question: What are the specific architectural improvements or alternative contrastive objectives that could enhance the knowledge synergy in STARS when dealing with a larger number of tasks?
- Basis in paper: [explicit] The paper suggests that designing more innovative architectures and developing different contrastive objectives to extract unique features are potential future research directions.
- Why unresolved: The paper does not propose or test specific architectural improvements or alternative contrastive objectives, leaving the potential enhancements unexplored.
- What evidence would resolve it: Proposing and testing various architectural modifications and contrastive objectives, followed by performance analysis, would identify the most effective strategies for improving knowledge synergy in STARS.

## Limitations
- The performance gains are based on a limited number of runs (5 runs) and may not generalize to other multi-task benchmarks or real-world robotics scenarios.
- The computational overhead of maintaining separate replay buffers and computing dual priorities per task is not discussed, leaving open questions about scalability to hundreds of tasks.
- The paper does not adequately address how the method would scale to very large numbers of tasks or how it would perform in non-simulation environments with partial observability and noisy rewards.

## Confidence

**High Confidence**: The core mechanism of combining shared and unique features for multi-task learning is well-supported by the theoretical framework and experimental results. The task-aware sampling strategy is clearly described and its implementation appears straightforward.

**Medium Confidence**: The performance improvement claims are statistically significant but may be sensitive to hyperparameter choices and specific benchmark conditions. The stability improvements across tasks are demonstrated but could be benchmark-dependent.

**Low Confidence**: The paper does not adequately address how the method would scale to very large numbers of tasks or how it would perform in non-simulation environments with partial observability and noisy rewards.

## Next Checks
1. **Ablation Studies**: Run experiments disabling either the shared features or unique features to quantify their individual contributions to performance improvements.
2. **Hyperparameter Sensitivity**: Test STARS across different values of the shared parameter pool size, priority clipping bounds, and sampling quotas to assess robustness.
3. **Cross-Benchmark Validation**: Evaluate STARS on additional multi-task benchmarks beyond Meta-World (e.g., RLBench, MultiWorld) to verify generalization of performance improvements.