---
ver: rpa2
title: Improving Transformers with Dynamically Composable Multi-Head Attention
arxiv_id: '2405.08553'
source_url: https://arxiv.org/abs/2405.08553
tags:
- attention
- arxiv
- composition
- dcmha
- transformer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Dynamically Composable Multi-Head Attention (DCMHA) is a novel
  attention architecture that improves upon Multi-Head Attention (MHA) in Transformers
  by introducing dynamic composition of attention heads. The core idea is to transform
  attention score and weight matrices in an input-dependent way using a Compose function,
  allowing heads to share information and collaborate more effectively.
---

# Improving Transformers with Dynamically Composable Multi-Head Attention

## Quick Facts
- arXiv ID: 2405.08553
- Source URL: https://arxiv.org/abs/2405.08553
- Authors: Da Xiao; Qingye Meng; Shengping Li; Xingyuan Yuan
- Reference count: 40
- DCFormer significantly outperforms Transformer on language modeling tasks across different model scales (405M to 6.9B parameters)

## Executive Summary
This paper introduces Dynamically Composable Multi-Head Attention (DCMHA), a novel attention architecture that improves upon standard Multi-Head Attention (MHA) in Transformers. DCMHA dynamically composes attention heads using an input-dependent Compose function, allowing heads to share information and collaborate more effectively. The method is parameter and computation efficient, with negligible overhead compared to standard MHA. Experimental results demonstrate that DCFormer, which replaces MHA with DCMHA, significantly outperforms Transformer on language modeling tasks across different model scales, matching the performance of models with ~1.7-2.0x more compute.

## Method Summary
DCMHA introduces a Compose function that transforms attention score and weight matrices in an input-dependent way. This function uses query/key-dependent weights and gates to dynamically combine the QK and OV circuits of existing heads. The composition is implemented efficiently using a two-level tensor decomposition approach: row+column decomposition allows applying Wq and Wk separately without materializing large 4-D tensors, while low-rank+diagonal decomposition reduces transformation matrix size. DCMHA can be applied to both attention scores (pre-compose) and weights (post-compose), with post-compose being more effective but both beneficial.

## Key Results
- DCFormer outperforms standard Transformer on language modeling tasks across model scales from 405M to 6.9B parameters
- DCPythia-6.9B outperforms open-source Pythia-12B on both pretraining perplexity and downstream task evaluation
- DCMHA improves performance on image classification tasks when applied to vision transformers
- The effectiveness is attributed to dynamically combining QK and OV circuits of existing heads according to input

## Why This Works (Mechanism)

### Mechanism 1
Dynamic composition of attention score and weight matrices enables input-dependent cross-head sharing. The Compose function transforms attention vectors using query/key-dependent weights and gates, allowing heads to dynamically share their QK and OV circuits based on context. Core assumption: Cross-head sharing patterns are sufficiently captured by low-rank projections (R=2) and gating. Break condition: If low-rank assumption fails (R needs to be much larger than 2), or if query/key-dependent weights don't capture meaningful sharing patterns.

### Mechanism 2
Composing attention matrices is equivalent to expanding QK and OV projections, increasing model expressiveness. Theorem 2.1 and 2.2 show that composing attention scores/weights is equivalent to concatenating original projections with a composition map C, effectively expanding head dimension by H-fold. Core assumption: The equivalence between attention matrix composition and projection composition holds for both static and dynamic cases. Break condition: If theoretical equivalence doesn't hold in practice, or if expanded projections introduce instability.

### Mechanism 3
Dynamic composition reduces head redundancy and increases head diversity by allowing heads to specialize and share contextually. By composing attention matrices dynamically, heads can specialize in different QK/OV circuit combinations based on input, reducing redundancy and increasing diversity. Core assumption: Head redundancy is a significant problem in standard MHA that limits expressiveness. Break condition: If head diversity doesn't correlate with performance, or if dynamic composition introduces new forms of redundancy.

## Foundational Learning

- Concept: Multi-Head Attention (MHA) and its limitations
  - Why needed here: Understanding MHA is crucial to grasp why DCMHA improves upon it and what problems it solves
  - Quick check question: What are the two main problems with standard MHA identified in the paper?
    - Answer: Low-rank bottleneck of attention score matrices and head redundancy

- Concept: Tensor decomposition and its efficiency benefits
  - Why needed here: DCMHA uses a two-level decomposition (row+column and low-rank+diagonal) to efficiently compute dynamic composition
  - Quick check question: What are the two levels of decomposition used in DCMHA and what do they achieve?
    - Answer: Row+column decomposition allows applying Wq and Wk separately without materializing large 4-D tensor; low-rank+diagonal decomposition reduces transformation matrix size from H^2 to 2HR+H

- Concept: Attention circuits (QK and OV)
  - Why needed here: DCMHA's effectiveness relies on dynamically composing these circuits according to input
  - Quick check question: What are the roles of QK and OV circuits in attention heads?
    - Answer: QK circuit (W_Q W_K^T) forms attention distribution; OV circuit (W_V W_O) transforms attended tokens

## Architecture Onboarding

- Component map: Q -> W_Q -> Attention Scores -> Compose Function -> Attention Weights -> Compose Function -> Attention Outputs -> W_O -> Final Output
- Critical path: 1) Compute attention scores: AS_i = Q W_Q_i (K W_K_i)^T / sqrt(D_h); 2) Apply pre-compose: AS = Compose(AS, Q, K; theta_pre); 3) Compute attention weights: AW = Softmax(AS); 4) Apply post-compose: AW = Compose(AW, Q, K; theta_post); 5) Compute attention outputs: O_i = AW_i (V W_V_i); 6) Combine outputs: O = Concat(O_1, ..., O_H) W_O
- Design tradeoffs:
  - Static vs dynamic composition: Dynamic composition significantly improves performance but adds computational overhead
  - Query-wise vs key-wise composition: Both can work independently; combining them may offer slight improvements
  - Pre-compose vs post-compose: Post-compose is more effective but pre-compose helps; both are beneficial
  - Rank R: Increasing R from 1 to 2 helps, but further increases don't improve performance
- Failure signatures:
  - Training instability: Check initialization of dynamic weights (should be small initially)
  - Performance degradation: Verify that Compose functions are correctly implemented and that low-rank projections capture meaningful patterns
  - Excessive memory usage: Monitor the size of dynamic weights and consider grouped composition for tensor parallel training
- First 3 experiments:
  1. Implement DCMHA with only static projection (equivalent to Talking-Heads Attention) and compare with standard MHA
  2. Add dynamic projection to static version and measure performance improvement
  3. Add dynamic gating to measure its contribution and compare query-wise vs key-wise effectiveness

## Open Questions the Paper Calls Out

### Open Question 1
How does DCMHA's dynamic head composition generalize to non-autoregressive tasks like sequence-to-sequence translation or masked language modeling? The paper only evaluates DCMHA on autoregressive language modeling and image classification. The mechanism's effectiveness for other transformer architectures (encoder-only, encoder-decoder) is not explored.

### Open Question 2
What is the impact of DCMHA on transformer models with very long sequences (e.g., 64K+ tokens) where the low-rank bottleneck becomes more severe? The paper mentions the low-rank bottleneck as a motivation for DCMHA, but experiments only go up to sequence length 8192. The scaling behavior for much longer sequences is unknown.

### Open Question 3
Can DCMHA be effectively combined with other transformer improvements like FlashAttention or linear attention mechanisms? The paper mentions in Section 4.4 that DCMHA could potentially benefit from FlashAttention-like kernel fusion, but doesn't implement it. It also discusses linear attention transformers in Appendix A.3 as a separate line of work.

## Limitations
- Theoretical foundations rely on assumptions about low-rank decompositions and equivalences that need more rigorous validation
- Efficiency claims of negligible overhead need more comprehensive benchmarking across hardware platforms
- Generalizability to other domains and architectures remains untested beyond language modeling and image classification
- Interpretability claims based on synthetic datasets may not fully capture real-world task complexity

## Confidence

- **High confidence**: Core claims about DCMHA's architecture and its ability to improve upon MHA in terms of expressiveness and performance
- **Medium confidence**: Theoretical claims about equivalence between attention matrix composition and projection composition
- **Low confidence**: Mechanistic interpretations of DCMHA's effectiveness based on synthetic datasets

## Next Checks

1. Validate low-rank assumption by conducting experiments with varying values of R (the rank of the low-rank decomposition) to determine the minimum value required for effective cross-head sharing
2. Benchmark efficiency by measuring the actual computational overhead of DCMHA, including memory usage and inference latency, on different hardware platforms and model scales
3. Test generalizability by applying DCMHA to different domains (e.g., speech, multimodal) and architectures (e.g., BERT, GPT) to assess its performance and effectiveness beyond language modeling and image classification