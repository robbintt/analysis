---
ver: rpa2
title: 'A Survey of Few-Shot Learning on Graphs: from Meta-Learning to Pre-Training
  and Prompt Learning'
arxiv_id: '2402.01440'
source_url: https://arxiv.org/abs/2402.01440
tags:
- graph
- graphs
- learning
- node
- few-shot
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a comprehensive survey of few-shot learning
  on graphs, addressing the challenge of data scarcity in graph-centric tasks. The
  authors propose two taxonomies: problem taxonomy (label scarcity and structure scarcity)
  and technique taxonomy (meta-learning, pre-training, and hybrid approaches).'
---

# A Survey of Few-Shot Learning on Graphs: from Meta-Learning to Pre-Training and Prompt Learning

## Quick Facts
- arXiv ID: 2402.01440
- Source URL: https://arxiv.org/abs/2402.01440
- Reference count: 40
- One-line primary result: Comprehensive survey of few-shot learning on graphs with dual taxonomy framework

## Executive Summary
This survey provides a systematic overview of few-shot learning approaches applied to graph-structured data, addressing the critical challenge of data scarcity in graph-centric tasks. The authors establish two taxonomies - one based on problem scarcity (label and structure) and another based on technique (meta-learning, pre-training, and hybrid approaches) - to organize and synthesize the rapidly growing literature. The work categorizes existing studies, analyzes their strengths and limitations, and identifies emerging research directions for the field.

## Method Summary
The survey employs a comprehensive literature review methodology, examining 40+ recent publications on few-shot learning for graphs. The authors systematically classify existing approaches using their dual taxonomy framework, comparing methodologies across multiple dimensions including problem formulation, algorithmic strategies, and performance characteristics. The analysis incorporates both qualitative assessments of methodological contributions and comparative evaluations of relative strengths and limitations across different approaches.

## Key Results
- Establishes dual taxonomy framework (problem-based and technique-based) for organizing few-shot learning on graphs
- Synthesizes recent developments and categorizes existing studies into coherent methodological groups
- Provides comparative analysis of strengths and limitations across different approaches
- Identifies promising future research directions for the field

## Why This Works (Mechanism)
The survey's effectiveness stems from its structured approach to organizing a complex and rapidly evolving research area. By establishing clear taxonomies based on both problem characteristics and solution approaches, the authors create a framework that helps researchers understand relationships between different methods and identify gaps in the literature. The systematic categorization enables meaningful comparisons between approaches and facilitates knowledge transfer across different problem domains.

## Foundational Learning

Graph Neural Networks (GNNs) - Why needed: Fundamental building blocks for graph-based few-shot learning methods. Quick check: Can represent node features, edge relationships, and graph-level patterns through message passing.

Meta-Learning - Why needed: Enables models to learn from limited examples by transferring knowledge across tasks. Quick check: Must demonstrate rapid adaptation capability through few gradient steps.

Graph Representation Learning - Why needed: Extracts meaningful features from graph structures for downstream few-shot tasks. Quick check: Should capture both local and global structural information effectively.

Transfer Learning - Why needed: Leverages knowledge from source domains to improve performance on target few-shot tasks. Quick check: Must show improved performance compared to training from scratch.

Prompt Learning - Why needed: Recent paradigm for adapting pre-trained models to few-shot scenarios without fine-tuning. Quick check: Should maintain competitive performance with minimal task-specific adaptation.

## Architecture Onboarding

Component Map: Problem Definition -> Taxonomy Framework -> Literature Survey -> Comparative Analysis -> Future Directions

Critical Path: Problem Scarcity Analysis → Technique Taxonomy Application → Method Classification → Comparative Evaluation → Research Gap Identification

Design Tradeoffs: Comprehensive coverage vs. recency bias; systematic categorization vs. emerging paradigm inclusion; qualitative assessment vs. quantitative benchmarking

Failure Signatures: Incomplete taxonomy coverage; misclassification of hybrid approaches; overemphasis on recent trends; insufficient consideration of fundamental limitations

First Experiments:
1. Test taxonomy classification on 10 recent pre-2024 papers to identify coverage gaps
2. Replicate literature search with alternative query terms to assess comprehensiveness
3. Compare citation impact of surveyed methods to validate relative importance claims

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Potential recency bias may overlook foundational earlier work in graph few-shot learning
- Classification framework might not fully capture emerging hybrid paradigms spanning multiple categories
- Comparative analysis relies on qualitative rather than quantitative benchmarking across standardized datasets

## Confidence
High confidence in taxonomy framework and literature synthesis methodology
Medium confidence in comparative analysis of strengths and limitations
Medium confidence in prospective research direction recommendations

## Next Checks
1. Verify taxonomy coverage by testing classification of recent pre-2024 papers against proposed framework to identify potential gaps or misclassifications
2. Conduct quantitative citation analysis comparing surveyed methods' impact and adoption rates to validate comparative strengths assessment
3. Replicate literature search methodology with alternative query terms to assess comprehensiveness and potential blind spots in survey coverage