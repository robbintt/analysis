---
ver: rpa2
title: Learning and Calibrating Heterogeneous Bounded Rational Market Behaviour with
  Multi-Agent Reinforcement Learning
arxiv_id: '2402.00787'
source_url: https://arxiv.org/abs/2402.00787
tags:
- agents
- learning
- agent
- rational
- these
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a multi-agent reinforcement learning (MARL)
  approach to learn heterogeneous bounded rational behaviors in agent-based models
  (ABMs). The key idea is to treat agents as constrained optimizers with varying degrees
  of strategic skills, learning policies through policy gradients while constraining
  deviations from prior beliefs using Kullback-Leibler (KL) divergence.
---

# Learning and Calibrating Heterogeneous Bounded Rational Market Behaviour with Multi-Agent Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2402.00787
- **Source URL**: https://arxiv.org/abs/2402.00787
- **Reference count**: 40
- **Primary result**: MARL approach with bounded rationality and agent heterogeneity outperforms standard RL and analytical equilibrium in predicting human behavior in economic systems

## Executive Summary
This paper introduces a multi-agent reinforcement learning (MARL) approach to learn heterogeneous bounded rational behaviors in agent-based models (ABMs). The method treats agents as constrained optimizers with varying degrees of strategic skills, learning policies through policy gradients while constraining deviations from prior beliefs using Kullback-Leibler (KL) divergence. To improve computational efficiency, the approach employs shared policy learning with agent supertypes, where agents' skill levels are sampled from a Gaussian distribution. The method is validated on three canonical multi-agent economic environments (supply chains, oligopolies, and cobweb markets) using real-world data.

The proposed approach significantly outperforms both analytically derived rational equilibrium solutions and standard MARL algorithms (PPO) in predicting human decision-making behavior, demonstrating the importance of incorporating bounded rationality and agent heterogeneity in MARL for modeling complex social systems. The results suggest that realistic modeling of economic systems requires moving beyond the assumption of perfectly rational agents to capture the diverse strategic capabilities observed in human behavior.

## Method Summary
The approach combines MARL with economic modeling of bounded rationality through constrained optimization. Agents are modeled as having varying skill levels sampled from a Gaussian distribution, with each agent's policy constrained by KL divergence from a prior belief to represent bounded rationality. The method uses shared policy learning across agent supertypes to improve computational efficiency while maintaining heterogeneity. Policy gradients are used to learn optimal behaviors within the rationality constraints. The framework is tested on three canonical economic environments using real-world data, comparing performance against both analytical rational equilibrium solutions and standard MARL baselines like PPO.

## Key Results
- The MARL approach with bounded rationality and heterogeneity significantly outperforms standard PPO and analytical equilibrium solutions in predicting human decision-making behavior
- Shared policy learning with agent supertypes provides computational efficiency while maintaining behavioral diversity
- The approach successfully captures realistic bounded rational behavior across supply chain, oligopoly, and cobweb market environments using real-world data
- KL divergence constraints effectively model the limited rationality observed in human economic decision-making

## Why This Works (Mechanism)
The approach works by combining MARL's ability to learn complex strategic behaviors with economic theory's understanding of bounded rationality. By constraining policy updates with KL divergence, the method ensures that learned behaviors remain within realistic bounds of human rationality. The heterogeneous skill levels across agents capture the diversity of strategic capabilities observed in real markets, while the supertype structure enables efficient learning without sacrificing behavioral diversity. The policy gradient optimization finds strategies that balance individual optimization with the constraints of bounded rationality and social interaction.

## Foundational Learning

**Multi-Agent Reinforcement Learning (MARL)**: Multiple agents learn simultaneously in a shared environment, requiring consideration of strategic interaction and non-stationarity. Needed to model complex economic systems with multiple interacting decision-makers. Quick check: Can the environment be modeled as a Markov game with observable states and rewards?

**Bounded Rationality**: Agents have limited cognitive capabilities and information processing, leading to suboptimal but realistic decision-making. Needed to model actual human behavior rather than theoretical optimal agents. Quick check: Does the model incorporate constraints on information processing or computational resources?

**KL Divergence Constraints**: Measures the difference between probability distributions, used here to constrain policy updates to remain close to prior beliefs. Needed to formalize the notion of bounded rationality in the optimization framework. Quick check: Is the KL constraint preventing unrealistic policy deviations while allowing learning?

**Agent Heterogeneity**: Different agents possess varying levels of strategic skill or rationality. Needed to capture the diversity of capabilities observed in real economic systems. Quick check: Are skill levels drawn from a distribution rather than being uniform across agents?

## Architecture Onboarding

**Component Map**: Environment -> Agent Supertype Pool -> Policy Network -> KL-Constrained Optimizer -> Action Selection -> Reward Feedback

**Critical Path**: Agent receives state → selects action via policy network → environment responds with reward → KL-constrained policy update → repeat

**Design Tradeoffs**: Heterogeneous skill levels vs. computational complexity; KL constraints vs. learning flexibility; shared policies vs. individual customization

**Failure Signatures**: 
- Poor performance: KL constraints too tight, preventing learning
- Non-convergence: Insufficient exploration or inappropriate skill level distribution
- Overfitting: Model captures noise rather than underlying behavioral patterns
- Computational inefficiency: Too many unique policies without supertype sharing

**3 First Experiments**:
1. Test KL-constrained learning on a single-agent environment to verify basic functionality
2. Validate supertype sharing by comparing performance with and without shared policies
3. Test different skill level distributions (Gaussian vs. uniform) to assess impact on learning

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Gaussian assumption for skill levels may not capture all forms of agent heterogeneity observed in real markets
- KL divergence constraint, while computationally convenient, may not perfectly represent all forms of bounded rationality
- The approach focuses on specific canonical environments and may not generalize to all types of multi-agent economic systems
- Performance comparisons rely on fitting to historical data rather than independent behavioral verification

## Confidence

**High**: Computational approach and experimental design are clearly defined with appropriate statistical comparisons; shared policy learning with supertypes is a practical innovation for efficiency

**Medium**: Broader economic implications and real-world applicability beyond tested environments; claim that this better captures bounded rationality relies on historical data fitting rather than independent verification

**Low**: None identified in the provided materials

## Next Checks

1. Test the approach on out-of-sample data or in environments not used during training to verify generalization capabilities

2. Compare performance against alternative bounded rationality models that don't use MARL, such as quantal response equilibrium approaches

3. Conduct ablation studies to determine which components (heterogeneity, KL constraints, supertypes) contribute most to the performance improvements