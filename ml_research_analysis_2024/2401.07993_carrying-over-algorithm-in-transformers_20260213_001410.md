---
ver: rpa2
title: Carrying over algorithm in transformers
arxiv_id: '2401.07993'
source_url: https://arxiv.org/abs/2401.07993
tags:
- attention
- head
- after
- layer
- position
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper investigates how transformer models implement the carrying
  over algorithm for integer addition. The author studies encoder-only and decoder-only
  transformer models trained on 3-digit addition tasks, finding that two-layer models
  implement the algorithm in a modular fashion, with each step allocated to specific
  parts of the network: the first layer adds digits in the same position, the second
  layer decides where to carry a one using attention, and the final MLP performs the
  carrying.'
---

# Carrying over algorithm in transformers

## Quick Facts
- arXiv ID: 2401.07993
- Source URL: https://arxiv.org/abs/2401.07993
- Authors: Jorrit Kruthoff
- Reference count: 40
- Primary result: Two-layer transformers implement integer addition using a modular carrying algorithm, with each layer specializing in different aspects of the computation

## Executive Summary
This paper investigates how transformer models implement the carrying over algorithm for integer addition. Through systematic analysis of encoder-only and decoder-only models trained on 3-digit addition tasks, the author demonstrates that two-layer models implement the algorithm in a modular fashion: the first layer adds digits in the same position using attention, the second layer decides where to carry a one, and the final MLP performs the carrying operation. This implementation generalizes to 4-digit addition and can be extended to larger numbers through priming or finetuning. The study also provides suggestive evidence that similar mechanisms exist in large language models like Alpaca 7B, Llemma 7B, and Zephyr 7B.

## Method Summary
The study trains encoder-only and decoder-only transformer models (1-3 layers, dmodel=128, dff=128, 2 attention heads) on 3-digit addition tasks (a + b < 1000) using RoFormer positional embeddings. Models are trained for 1000 epochs with AdamW optimizer (η=1.4×10⁻⁴, λ=0.2), batch size 1024, and evaluated on accuracy metrics including per-task performance (non-carry, C@1, C@2, C all, C all con). The analysis involves attention pattern visualization, residual stream analysis via PCA, and ablation studies to identify the modular implementation of the carrying algorithm.

## Key Results
- Two-layer models achieve perfect accuracy on 3-digit addition by implementing a modular carrying algorithm
- The first layer performs digit-wise addition while the second layer handles carry decisions and execution
- 1-layer models exhibit a non-grokking phase transition in their QK-circuit where attention suddenly becomes an adder
- The modular implementation generalizes to 4-digit addition and can be extended to larger numbers through priming or finetuning
- Suggestive evidence indicates similar mechanisms exist in large language models (Alpaca 7B, Llemma 7B, Zephyr 7B)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim:** Two-layer encoder-only models implement integer addition using a modular carrying over algorithm, with the first layer adding digits in the same position and the second layer determining where to carry a one.
- **Mechanism:** The first layer uses attention to add digit vectors at the same position (staircase attention patterns). The second layer uses attention to decide where a carried one is needed (transferring information from previous sums) and the final MLP adds the carried one.
- **Core assumption:** The transformer's self-attention mechanism can learn to route digit information appropriately and the residual connections enable addition.
- **Evidence anchors:**
  - [abstract] "The first layer is mostly responsible for adding digits in the same position. The second layer first decides, in the attention, which positions need a carried one or not, and then performs the carrying of the one in the final MLP."
  - [section] "The attention blocks are again adders and transfer information from the relevant digits to each other. The attention patterns are shown in Fig. 3... For the second layer, we see that head 1:0 transfers information from the previous sum to the current sum."
  - [corpus] Weak evidence: Related papers discuss transformer generalization on arithmetic but do not specifically address this modular algorithm implementation.
- **Break condition:** If the model has insufficient capacity or the training data lacks diversity in carrying patterns, the modular division may break down.

### Mechanism 2
- **Claim:** The final MLP in two-layer models is specialized for adding carried ones, not for general addition.
- **Mechanism:** Ablation studies show that removing the MLP drastically reduces accuracy on carrying-over sums but leaves non-carry sums largely intact. The MLP learns to "rotate" hidden representations of carry cases toward correct outputs.
- **Core assumption:** The MLP can specialize on a subset of tasks without harming others, enabled by the modular division of labor in the network.
- **Evidence anchors:**
  - [section] "The ablated model is very confident where to add a one when it indeed should... when it is not supposed to add a one, it either adds the one or it does not... This suggests the decision heads are necessary to make a proper decision on where a carried one is needed."
  - [section] "For the second experiment we study the action of the final MLP on embedding vectors for sums with the same outcome. The model should rotate those embedding vectors towards each other... Averaged over six runs, we find this to be the case."
  - [corpus] No direct evidence; generalization work focuses on positional encoding, not MLP specialization.
- **Break condition:** If the MLP is too small or the training distribution is too narrow, it may not learn this specialization.

### Mechanism 3
- **Claim:** One-layer models exhibit a non-grokking phase transition in their QK-circuit, where attention suddenly becomes an adder.
- **Mechanism:** Early in training, the QK-circuit changes abruptly, forming staircase attention patterns that perform digit-wise addition. This transition is driven by changes in attention weights, not weight norm growth.
- **Core assumption:** The architecture can spontaneously reorganize attention patterns to implement addition without explicit supervision.
- **Evidence anchors:**
  - [abstract] "For larger models this transition happens very early on in training."
  - [section] "These staircase patterns after the transitions are actually very intuitive... The self-attention mechanism makes that possible once the correct pattern has been learned."
  - [corpus] No direct evidence; most related work focuses on architectural modifications, not spontaneous circuit reorganization.
- **Break condition:** If dropout or weight decay is too high, the transition may be suppressed or delayed.

## Foundational Learning

- **Concept: Self-attention mechanism**
  - Why needed here: The carrying algorithm relies on routing digit information between positions; self-attention enables this dynamic routing.
  - Quick check question: Can you explain how attention scores determine which digits are combined at each step?
- **Concept: Residual connections**
  - Why needed here: Without skip connections, attention alone would just copy vectors; residuals enable the actual addition operation.
  - Quick check question: What happens to the model's performance if you remove residual connections in the first layer?
- **Concept: Positional encoding**
  - Why needed here: The model must know which digits belong to which position; positional embeddings provide this information.
  - Quick check question: How would the model behave if all positional embeddings were set to zero?

## Architecture Onboarding

- **Component map:** Input digits with positional embeddings -> Layer 0 attention (digit-wise addition) + MLP -> Layer 1 attention (carry decision) + MLP (carry execution) -> Output sum digits
- **Critical path:** Layer 0 attention → Layer 0 MLP → Layer 1 attention (carry decision) → Layer 1 MLP (carry execution)
- **Design tradeoffs:**
  - More layers: Could distribute tasks further but risk overfitting or losing modularity
  - Larger MLP hidden size: May improve carry execution but increase parameter count
  - Different positional encodings: Could affect how well the model tracks digit positions
- **Failure signatures:**
  - High accuracy on non-carry sums but poor on carry sums: MLP not learning carry execution
  - Poor accuracy on both: Attention not learning digit-wise addition
  - Phase transition absent in 1-layer models: Regularization too strong or architecture too shallow
- **First 3 experiments:**
  1. Train 2-layer model with attention ablations to confirm which heads do what
  2. Train 1-layer model and monitor QK-circuit evolution for phase transition
  3. Vary positional encoding schemes to test impact on length generalization

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How does the carrying over algorithm implementation in two-layer encoder-only models generalize to larger integers beyond 6 digits?
- **Basis in paper:** [explicit] The paper mentions that the implementation generalizes to 4-digit addition and can be extended to larger numbers through priming or finetuning, but doesn't provide detailed analysis beyond 6 digits.
- **Why unresolved:** The paper only tested up to 6 digits with priming and doesn't explore the limitations or scaling behavior of the algorithm implementation for much larger numbers.
- **What evidence would resolve it:** Testing the model on addition problems with 10+ digits and analyzing whether the modular implementation of the carrying over algorithm persists or breaks down.

### Open Question 2
- **Question:** What is the precise mechanism by which the decision heads in the second layer of two-layer models determine where to carry a one?
- **Basis in paper:** [explicit] The paper identifies decision heads but only provides suggestive evidence about their role in determining where to carry a one, without fully explaining the underlying mechanism.
- **Why unresolved:** While the paper shows that ablating decision heads affects accuracy, it doesn't provide a detailed mechanistic understanding of how these heads make their decisions.
- **What evidence would resolve it:** Detailed analysis of the decision head's attention patterns and their relationship to the residual stream, combined with ablation studies targeting specific components of the decision process.

### Open Question 3
- **Question:** How does the carrying over algorithm implementation in small transformer models compare to that in large language models like Alpaca 7B, Llemma 7B, and Zephyr 7B?
- **Basis in paper:** [explicit] The paper provides suggestive evidence that similar mechanisms exist in these LLMs but doesn't provide a comprehensive comparison of the implementations.
- **Why unresolved:** The paper only provides preliminary analysis of attention patterns and residual streams in LLMs without fully characterizing their implementation of the carrying over algorithm.
- **What evidence would resolve it:** Detailed analysis of attention patterns, residual streams, and ablation studies in LLMs to determine if they implement the carrying over algorithm in a modular fashion similar to small models.

## Limitations
- Analysis is confined to 3-digit and 4-digit addition tasks with specific model architectures (2-layer encoder-only/decoder-only, dmodel=128, dff=128, 2 heads)
- Evidence for similar mechanisms in large language models is only "suggestive" without detailed mechanistic analysis
- Does not explore how the algorithm scales to much larger numbers or different arithmetic operations beyond addition

## Confidence
- **High Confidence**: The modular implementation of the carrying algorithm in 2-layer transformer models for 3-digit addition
- **Medium Confidence**: The existence of a phase transition in 1-layer models where attention spontaneously learns to add digits
- **Low Confidence**: Claims about similar mechanisms existing in large language models

## Next Checks
1. **Architectural Robustness Test**: Vary model depth (1-4 layers) and width (dmodel=64, 256, 512) to determine the minimum architecture required for modular carrying implementation and whether the algorithm scales with model capacity.

2. **Cross-Operation Validation**: Test whether the same modular approach applies to subtraction, multiplication, and division tasks to assess whether this is a general arithmetic strategy or specific to addition.

3. **Large Model Mechanistic Analysis**: Conduct detailed mechanistic interpretability studies (attention pattern analysis, activation patching, causal tracing) on the claimed large language models to verify whether they implement carrying using the same modular approach identified in small models.