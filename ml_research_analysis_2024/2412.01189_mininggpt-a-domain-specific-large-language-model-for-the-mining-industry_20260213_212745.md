---
ver: rpa2
title: MiningGPT -- A Domain-Specific Large Language Model for the Mining Industry
arxiv_id: '2412.01189'
source_url: https://arxiv.org/abs/2412.01189
tags:
- mining
- domain-specific
- dataset
- domain
- mininggpt
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents MiningGPT, a domain-specific large language
  model (LLM) for the mining industry, developed by fine-tuning Mistral 7B Instruct
  on a 120-million-token mining corpus. The approach uses keyword filtering and sentence
  embedding methods to construct a high-quality mining dataset, followed by instruction-tuned
  fine-tuning.
---

# MiningGPT -- A Domain-Specific Large Language Model for the Mining Industry

## Quick Facts
- arXiv ID: 2412.01189
- Source URL: https://arxiv.org/abs/2412.01189
- Authors: Kurukulasooriya Fernando ana Gianluca Demartini
- Reference count: 9
- Key outcome: MiningGPT achieved 14% higher mining domain knowledge test scores than Mistral 7B Instruct while maintaining general capabilities

## Executive Summary
MiningGPT is a domain-specific large language model for the mining industry developed by fine-tuning Mistral 7B Instruct on a 120-million-token mining corpus. The model demonstrates that open data sources and efficient fine-tuning methods can effectively build domain-specific LLMs without requiring massive computing resources or proprietary data. The approach uses keyword filtering and sentence embedding methods to construct a high-quality mining dataset, followed by instruction-tuned fine-tuning that preserves both domain knowledge and general capabilities.

## Method Summary
The researchers constructed the MiningPile dataset by filtering C4 corpus and arXiv papers using 600 mining domain keywords and sentence embedding similarity (0.65 cutoff). They generated question-answer pairs from this corpus using a large general-domain LLM, then fine-tuned Mistral 7B Instruct using QLoRA with hyperparameters: LoRA rank 128, LoRA alpha 16, learning rate 1e-4, batch size 16. The fine-tuning process aimed to add mining domain knowledge while preserving the instruction-following capabilities of the base model.

## Key Results
- 14% higher mining domain knowledge test score compared to parent model Mistral 7B Instruct
- Maintained general capabilities in reasoning, math, and logic while adding domain expertise
- Demonstrated that smaller, high-quality datasets can be nearly as effective as larger datasets for domain adaptation

## Why This Works (Mechanism)

### Mechanism 1
Keyword filtering plus sentence embedding similarity filtering yields a high-quality mining domain corpus without needing proprietary data. The two-stage process improves corpus purity by first isolating candidate documents containing mining-related terms, then filtering out non-mining content that happens to contain mining keywords. This assumes mining keywords have sufficient discriminative power when combined with semantic similarity scoring.

### Mechanism 2
Fine-tuning an instruction-tuned model with domain-specific question-answer pairs preserves instruction-following capabilities while adding domain knowledge. Adapter-based QLoRA fine-tuning adds domain knowledge to the embedding space while keeping base model parameters frozen, theoretically preserving the instruction-following behavior learned during pre-training.

### Mechanism 3
A smaller, high-quality dataset can be nearly as effective as a larger dataset for building domain-specific models. The ablation study shows that a subset of 30,000 samples from either the thesis reports or open-source data categories achieved similar performance to the full dataset, suggesting that data quality and informativeness matter more than quantity.

## Foundational Learning

- **Domain-specific terminology and knowledge structure**: Understanding mining domain concepts and terminology is essential for evaluating model performance and designing effective filtering and evaluation approaches. Quick check: Can you explain the difference between open-pit mining and underground mining, and name three common mining equipment types?

- **Fine-tuning vs. pre-training tradeoffs**: The paper demonstrates that fine-tuning a suitable foundational model is sufficient for domain adaptation, avoiding the need for expensive pre-training. Quick check: What are the key differences between full fine-tuning, LoRA, and QLoRA, and when would you choose each approach?

- **Evaluation methodology for domain-specific models**: The paper uses cosine similarity with a high threshold (0.85) to evaluate domain-specific knowledge, requiring understanding of embedding-based similarity measures. Quick check: How does cosine similarity between sentence embeddings differ from exact string matching, and why is a high threshold appropriate for this evaluation?

## Architecture Onboarding

- **Component map**: Keyword filtering → Sentence embedding similarity filtering → Corpus compilation → QLoRA adapter training on instruction-tuned base model → Domain-specific knowledge testing + General capability retention testing

- **Critical path**: Data filtering and corpus creation → Model fine-tuning → Domain-specific evaluation → General capability evaluation

- **Design tradeoffs**: Model size (7B parameters) vs. computational resources and performance; Corpus size (120M tokens) vs. data quality and diversity; High evaluation threshold (0.85) vs. false positive rate; Adapter-based fine-tuning vs. full fine-tuning for parameter efficiency

- **Failure signatures**: Low domain-specific performance despite high-quality corpus (adapter not learning effectively or base model lacks sufficient domain-agnostic knowledge); Loss of general capabilities (adapter interference or insufficient instruction-following capability in base model); Poor data filtering results (inappropriate similarity threshold or unrepresentative reference knowledge dataset)

- **First 3 experiments**: 1) Test keyword filtering on a small sample dataset to verify mining content identification accuracy; 2) Run sentence embedding similarity filtering with different threshold values (0.6, 0.65, 0.7) to find optimal balance; 3) Perform small-scale fine-tuning with different learning rates (1e-4, 2e-4, 3e-4) to identify optimal training configuration

## Open Questions the Paper Calls Out

- How does the quality of MiningGPT's responses vary when tested on domain-specific tasks outside of mining operations and maintenance? The paper only evaluates MiningGPT's performance on mining operations and maintenance tasks, not other subdomains like finance, marketing, or human resources.

- What is the long-term performance stability of MiningGPT when deployed in real-world mining industry applications? The paper presents initial evaluation results but doesn't address model performance over extended periods or in production environments.

- How does MiningGPT's performance compare to domain-specific models built using proprietary mining company data? The paper discusses using only open data sources and hypothesizes about the potential of proprietary data.

- What is the minimum viable dataset size for building effective mining domain-specific LLMs? While the paper demonstrates effectiveness with 120M tokens, it doesn't systematically explore the relationship between dataset size and model performance.

## Limitations

- The exact 600 mining domain keywords used for initial dataset filtering are not provided, making direct reproduction difficult
- The specific prompt used to generate question-answer pairs from MiningPile with the large general-domain LLM is not disclosed
- Limited ablation studies on different filtering configurations or threshold values for the similarity cutoff
- Lack of standardized evaluation benchmarks for domain-specific knowledge makes it difficult to assess true model quality

## Confidence

- Effectiveness of two-stage filtering approach: Medium-Low (promising results but no direct validation of 0.65 threshold)
- Open data sources and efficient fine-tuning can build effective domain-specific LLMs: Medium-High (demonstrated 14% improvement and retained general capabilities)
- Smaller datasets being nearly as effective: Low-Medium (limited ablation testing with only one subset size tested)

## Next Checks

1. Test the keyword filtering approach on a manually annotated validation set to measure precision and recall of mining content identification
2. Evaluate the model on established domain-specific benchmarks like domain-specific variants of MMLU or specialized mining knowledge tests
3. Conduct a more comprehensive ablation study varying both corpus size and similarity thresholds to identify optimal filtering parameters