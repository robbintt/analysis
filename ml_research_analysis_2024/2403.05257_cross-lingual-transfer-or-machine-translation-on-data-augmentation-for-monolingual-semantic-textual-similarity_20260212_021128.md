---
ver: rpa2
title: Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual
  Semantic Textual Similarity
arxiv_id: '2403.05257'
source_url: https://arxiv.org/abs/2403.05257
tags:
- data
- language
- cross-lingual
- transfer
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study compared cross-lingual transfer and machine translation
  as data augmentation techniques for monolingual semantic textual similarity (STS)
  in Japanese and Korean. Using unsupervised multilingual SimCSE as the testbed, the
  authors found that cross-lingual transfer and machine translation achieved comparable
  performance.
---

# Cross-lingual Transfer or Machine Translation? On Data Augmentation for Monolingual Semantic Textual Similarity

## Quick Facts
- arXiv ID: 2403.05257
- Source URL: https://arxiv.org/abs/2403.05257
- Authors: Sho Hoshino; Akihiko Kato; Soichiro Murakami; Peinan Zhang
- Reference count: 0
- Primary result: Cross-lingual transfer and machine translation achieved comparable performance for Japanese and Korean monolingual STS

## Executive Summary
This study investigates data augmentation techniques for monolingual semantic textual similarity (STS) tasks in Japanese and Korean, two low-resource languages. The authors compare cross-lingual transfer and machine translation as methods to leverage high-resource language data. Using unsupervised multilingual SimCSE as the testbed, they find that both approaches yield similar performance levels. Interestingly, Wikipedia data proves more effective than NLI data for these languages, contrary to previous findings. The best results are achieved by combining cross-lingual transfer with Wikipedia data, surpassing or matching state-of-the-art multilingual models.

## Method Summary
The authors employ unsupervised multilingual SimCSE as their testbed to compare data augmentation techniques for monolingual STS. They experiment with cross-lingual transfer from English to Japanese and Korean, as well as machine translation of English data into these target languages. Two types of source data are used: Wikipedia articles and NLI datasets. The study evaluates the effectiveness of each approach by training sentence embeddings on the augmented data and measuring their performance on monolingual STS benchmarks. The authors also investigate the impact of using native Wikipedia data alongside the augmented data.

## Key Results
- Cross-lingual transfer and machine translation achieved comparable performance for Japanese and Korean STS tasks
- Wikipedia data outperformed NLI data as unlabeled training data for these languages
- Combining cross-lingual transfer of Wikipedia data achieved the best performance, surpassing or matching state-of-the-art multilingual models

## Why This Works (Mechanism)
The success of both cross-lingual transfer and machine translation stems from their ability to leverage the semantic richness of high-resource language data to improve low-resource language models. Cross-lingual transfer benefits from shared linguistic structures and semantic concepts across languages, allowing knowledge to be transferred from English to Japanese and Korean. Machine translation creates parallel data that preserves semantic relationships while adapting to the target language's syntax and vocabulary. The effectiveness of Wikipedia data likely results from its diverse, encyclopedic content that captures a wide range of semantic relationships, making it more suitable for general-purpose sentence embedding tasks compared to the more specialized NLI datasets.

## Foundational Learning
1. **Semantic Textual Similarity (STS)**
   - Why needed: Core task being addressed; measures semantic equivalence between sentence pairs
   - Quick check: Can you explain the difference between STS and paraphrase detection?

2. **Cross-lingual Transfer Learning**
   - Why needed: Primary technique for leveraging high-resource language data
   - Quick check: How does cross-lingual transfer differ from zero-shot learning?

3. **Unsupervised Sentence Embedding**
   - Why needed: Method used to train models without labeled STS data
   - Quick check: What's the key difference between supervised and unsupervised SimCSE?

4. **Data Augmentation for NLP**
   - Why needed: Central theme of the paper; techniques to expand training data
   - Quick check: Name three common data augmentation techniques for NLP tasks

5. **Low-resource Language Processing**
   - Why needed: Context for why data augmentation is necessary
   - Quick check: What are the main challenges in developing NLP models for low-resource languages?

6. **Multilingual Models**
   - Why needed: Framework used for cross-lingual experiments
   - Quick check: How do multilingual models differ from monolingual models in terms of architecture?

## Architecture Onboarding
**Component Map:**
Wikipedia/NLI data -> English model -> Cross-lingual transfer/Machine translation -> Japanese/Korean STS model

**Critical Path:**
1. Source data selection (Wikipedia or NLI)
2. Training English model
3. Applying cross-lingual transfer or machine translation
4. Fine-tuning on target language data
5. Evaluating on monolingual STS benchmarks

**Design Tradeoffs:**
- Cross-lingual transfer vs. machine translation: Transfer is more efficient but may lose some language-specific nuances; translation is more accurate but computationally expensive
- Wikipedia vs. NLI data: Wikipedia offers broader coverage but may lack the focused semantic relationships in NLI data
- Unsupervised vs. supervised training: Unsupervised is more scalable but may underperform supervised methods with labeled data

**Failure Signatures:**
- Poor performance on domain-specific STS tasks if Wikipedia data doesn't cover relevant topics
- Suboptimal results if cross-lingual transfer doesn't align well with target language structures
- Degradation in performance if machine translation introduces errors or loses semantic information

**First Experiments:**
1. Compare cross-lingual transfer and machine translation on a held-out STS test set
2. Ablation study removing Wikipedia data to isolate its contribution
3. Evaluate model performance on out-of-domain STS tasks to test generalization

## Open Questions the Paper Calls Out
None

## Limitations
- Findings primarily based on experiments with Japanese and Korean, limiting generalizability to other low-resource languages
- Comparison conducted using unsupervised multilingual SimCSE, which may not represent the full range of potential STS model architectures
- Reliance on Wikipedia data as an alternative to NLI data may not be universally applicable across all domains or languages

## Confidence
- High: Cross-lingual transfer and machine translation achieved comparable performance for Japanese and Korean STS tasks
- Medium: Wikipedia data outperforms NLI data for these specific languages, contradicting prior work
- Medium: Combining cross-lingual transfer with Wikipedia data achieves state-of-the-art performance, dependent on specific models and languages tested

## Next Checks
1. Replicate experiments with a broader set of low-resource languages to assess generalizability
2. Test the proposed approach with other STS model architectures beyond unsupervised multilingual SimCSE
3. Conduct ablation studies to isolate the contributions of Wikipedia data quality versus cross-lingual transfer technique