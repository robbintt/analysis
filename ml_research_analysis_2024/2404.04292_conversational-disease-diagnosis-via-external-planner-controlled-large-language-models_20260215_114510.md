---
ver: rpa2
title: Conversational Disease Diagnosis via External Planner-Controlled Large Language
  Models
arxiv_id: '2404.04292'
source_url: https://arxiv.org/abs/2404.04292
tags:
- diagnosis
- patient
- question
- medical
- disease
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a novel conversational disease diagnosis system
  that leverages large language models (LLMs) augmented with external planners to
  emulate the two-phase decision-making process of doctors: disease screening and
  differential diagnosis. The system employs a reinforcement learning-based planner
  for disease screening to identify high-risk diseases through symptom inquiry, and
  a planner guided by clinical guidelines for differential diagnosis to confirm or
  exclude suspected diseases.'
---

# Conversational Disease Diagnosis via External Planner-Controlled Large Language Models

## Quick Facts
- arXiv ID: 2404.04292
- Source URL: https://arxiv.org/abs/2404.04292
- Authors: Zhoujian Sun; Cheng Luo; Ziyi Liu; Zhengxing Huang
- Reference count: 37
- Key outcome: A two-phase conversational diagnosis system using RL for screening and guideline-based decision procedures for differential diagnosis, achieving top-1 hit rate of 0.341 in screening and F1 score of 0.914 for heart failure diagnosis

## Executive Summary
This paper presents a novel conversational disease diagnosis system that emulates the two-phase decision-making process of human doctors using large language models (LLMs) augmented with external planners. The system employs a reinforcement learning-based planner for disease screening to identify high-risk diseases through symptom inquiry, and a planner guided by clinical guidelines for differential diagnosis to confirm or exclude suspected diseases. Evaluated on the MIMIC-IV dataset, the system demonstrates superior performance compared to pure LLM-based approaches, showcasing the potential of integrating LLMs with structured planning to enhance the accuracy and accessibility of AI-assisted medical diagnostics.

## Method Summary
The system uses a two-phase approach for conversational disease diagnosis. First, a reinforcement learning-based planner conducts disease screening by iteratively asking symptom questions to identify high-risk diseases. Second, a planner guided by clinical guidelines performs differential diagnosis to confirm or exclude suspected diseases from the screening phase. The system uses the MIMIC-IV dataset with 98 prevalent diseases, converting patient admission records into symptom vectors using GPT-4 Turbo and Mayo Clinic symptom checker. Decision procedures for differential diagnosis are derived from clinical guidelines through a human-in-the-loop process.

## Key Results
- Achieved top-1 hit rate of 0.341 in disease screening, outperforming pure LLM approaches
- Obtained F1 score of 0.914 in differential diagnosis for heart failure
- Demonstrated the effectiveness of combining RL-based symptom questioning with structured guideline-based decision procedures

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The disease screening planner achieves high accuracy by iteratively asking symptom questions guided by reinforcement learning.
- Mechanism: The RL agent learns to maximize rewards based on whether the queried symptom is present in the patient. This approach allows the system to prioritize asking about the most informative symptoms first, reducing the number of questions needed to form a high-risk disease list.
- Core assumption: Symptom presence is a strong predictor of disease risk, and the RL agent can effectively learn an optimal questioning strategy within a limited number of turns.
- Evidence anchors:
  - [abstract] "The system employs a reinforcement learning-based planner for disease screening to identify high-risk diseases through symptom inquiry"
  - [section 3.1] "We employed a RL approach to train the inquiry strategy... The RL action space comprises inquiries about the existence of specific symptoms. We train the RL agent to maximize the likelihood that each queried symptom is present."
  - [corpus] Weak evidence; no related studies directly validate RL-based symptom questioning for disease screening.
- Break condition: If the patient cannot accurately report symptoms, or if the symptom-to-disease mapping is too noisy, the RL agent's learned strategy may become ineffective.

### Mechanism 2
- Claim: The differential diagnosis planner improves diagnostic accuracy by following structured decision procedures derived from clinical guidelines.
- Mechanism: Clinical guidelines are converted into a decision tree or flowchart. The LLM follows this structure, asking targeted questions to confirm or exclude specific diseases based on the high-risk list from the screening phase. This ensures that the diagnostic process is systematic and aligned with established medical knowledge.
- Core assumption: Clinical guidelines can be effectively translated into decision procedures that a LLM can follow, and these procedures are comprehensive enough to cover the diagnostic reasoning needed for the diseases in question.
- Evidence anchors:
  - [abstract] "The second planner uses LLMs to parse medical guidelines and conduct differential diagnoses"
  - [section 3.2] "We employ a human-in-the-loop methodology to derive reliable decision procedures... Each disease is assessed using its specific literature to either confirm or exclude it."
  - [corpus] Weak evidence; related studies focus on knowledge-infused diagnosis but not structured guideline-based decision procedures.
- Break condition: If the clinical guidelines are incomplete, ambiguous, or not well-suited for a decision tree format, the LLM may make incorrect diagnostic decisions.

### Mechanism 3
- Claim: The combination of the two planners creates a two-phase diagnostic process that mimics human doctors, leading to improved accuracy.
- Mechanism: The screening phase narrows down the list of possible diseases by asking about symptoms, while the differential diagnosis phase uses specific tests and questions to confirm or exclude the suspected diseases. This two-step approach is more efficient and accurate than a single-step diagnosis because it first reduces the search space and then focuses on the most likely candidates.
- Core assumption: The two-phase approach used by human doctors is effective, and the LLM-based system can accurately replicate this process.
- Evidence anchors:
  - [abstract] "Our system involves two external planners to handle planning tasks... The first focuses on collecting patient symptoms to identify potential diseases, while the second delves into specific inquiries to confirm or exclude these diseases."
  - [section 1] "human doctors usually follow a two-phase decision procedure in medical consultations... The first planner can enquire about the patientâ€™s history of present illness (HPI)... The second planner is permitted to inquire arbitrarily information which is beneficial to the diagnosis."
  - [corpus] Weak evidence; no direct studies compare two-phase LLM-based diagnosis to single-step approaches.
- Break condition: If the screening phase is inaccurate, the differential diagnosis phase may focus on the wrong diseases, leading to incorrect diagnoses.

## Foundational Learning

- Concept: Reinforcement Learning (RL) for decision-making
  - Why needed here: The disease screening planner needs to learn an optimal strategy for asking symptom questions, which is a sequential decision-making problem.
  - Quick check question: What is the difference between policy-based and value-based RL methods, and which one is used in this study?

- Concept: Natural Language Understanding (NLU) and Generation (NLG)
  - Why needed here: The LLM is responsible for understanding patient responses and generating questions in natural language.
  - Quick check question: How does the LLM parse the patient's response to determine if a symptom is confirmed or denied?

- Concept: Clinical Guidelines and Decision Trees
  - Why needed here: The differential diagnosis planner uses structured decision procedures derived from clinical guidelines to guide the diagnostic process.
  - Quick check question: What are the key elements of a clinical guideline that need to be converted into a decision tree?

## Architecture Onboarding

- Component map:
  Patient Simulator -> Disease Screening Planner -> Differential Diagnosis Planner -> LLM

- Critical path:
  1. Vectorize patient admission records into symptom vectors
  2. Train disease screening planner using RL on vectorized data
  3. Generate decision procedures for differential diagnosis from clinical guidelines
  4. Evaluate system performance on test data using simulated dialogues

- Design tradeoffs:
  - Using an open-source LLM (Llama2) vs. a commercial LLM (GPT-4) for the doctor simulator
  - Number of questions allowed in the screening phase (10 vs. 20)
  - Level of human involvement in refining the decision procedures

- Failure signatures:
  - Low accuracy in the screening phase leading to incorrect high-risk disease lists
  - Decision procedures that are too complex or incomplete, causing the LLM to make incorrect diagnostic decisions
  - LLM failing to accurately parse patient responses or generate appropriate questions

- First 3 experiments:
  1. Evaluate the disease screening planner's performance with different numbers of questions (e.g., 5, 10, 15) to find the optimal balance between accuracy and efficiency.
  2. Compare the performance of the system using GPT-4 vs. Llama2 as the LLM backbone to assess the impact of LLM choice.
  3. Test the system's performance on a different dataset or disease set to evaluate its generalizability.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed conversational disease diagnosis system compare to human doctors in real clinical settings?
- Basis in paper: [inferred] The paper mentions plans to conduct clinical trials to test the system's performance in real-world settings, but no results are provided.
- Why unresolved: The paper only evaluates the system using simulated dialogues with virtual patients based on the MIMIC-IV dataset. Real-world clinical trials are still planned.
- What evidence would resolve it: Conducting and publishing results from clinical trials comparing the system's diagnostic accuracy, efficiency, and patient satisfaction to human doctors in actual clinical settings.

### Open Question 2
- Question: How does the system handle cases where patients have multiple concurrent diseases, and how does it prioritize diagnoses?
- Basis in paper: [inferred] The paper mentions that patients could be afflicted with one of 98 prevalent diseases, but it does not discuss how the system handles multiple concurrent diseases or prioritizes diagnoses.
- Why unresolved: The paper does not provide information on how the system deals with complex cases involving multiple diseases or how it determines the primary diagnosis when multiple high-risk diseases are identified.
- What evidence would resolve it: Detailed analysis of the system's performance on cases with multiple concurrent diseases, including how it prioritizes and presents diagnoses to clinicians.

### Open Question 3
- Question: How does the system ensure patient privacy and data security, especially when using large language models that may process sensitive medical information?
- Basis in paper: [explicit] The paper mentions that transmitting medical data to external third parties is subject to restrictions and that the system must operate in an environment disconnected from the public internet.
- Why unresolved: While the paper acknowledges the importance of data privacy and security, it does not provide specific details on how the system ensures compliance with regulations like HIPAA or how it handles data encryption and access control.
- What evidence would resolve it: Detailed description of the system's data privacy and security measures, including compliance with relevant regulations, data encryption methods, and access control mechanisms.

### Open Question 4
- Question: How does the system adapt to different medical specialties and disease domains beyond the 98 prevalent diseases evaluated in the study?
- Basis in paper: [inferred] The paper focuses on evaluating the system's performance on 98 prevalent diseases but does not discuss its generalizability to other medical specialties or rare diseases.
- Why unresolved: The paper does not provide information on how the system can be adapted or extended to handle diseases outside the evaluated domain or how it performs on rare or complex medical conditions.
- What evidence would resolve it: Studies demonstrating the system's performance and adaptability across different medical specialties, including rare diseases and complex cases, as well as its ability to incorporate new medical knowledge and guidelines.

### Open Question 5
- Question: How does the system handle ambiguous or conflicting information provided by patients during the diagnostic conversation?
- Basis in paper: [inferred] The paper mentions that the system uses large language models for natural language understanding and generation but does not discuss how it handles ambiguity or conflicting information in patient responses.
- Why unresolved: The paper does not provide details on how the system manages situations where patients provide inconsistent or contradictory information, or how it deals with uncertainty in symptom descriptions.
- What evidence would resolve it: Analysis of the system's ability to handle ambiguous or conflicting patient information, including its strategies for clarification, dealing with uncertainty, and making diagnostic decisions in the face of incomplete or contradictory data.

## Limitations

- The effectiveness of RL-based symptom questioning relies heavily on the quality and completeness of symptom-to-disease mappings in the MIMIC-IV dataset.
- The system's performance on rare diseases and complex cases involving multiple concurrent diseases is not well-established.
- The generalizability of the approach to different medical specialties and real-world clinical settings remains to be demonstrated.

## Confidence

**High Confidence:** The system architecture and implementation details are well-specified, with clear descriptions of the data preparation, model training, and evaluation procedures. The use of MIMIC-IV dataset and the overall methodology are sound.

**Medium Confidence:** The effectiveness of the RL-based screening planner and the decision procedure approach for differential diagnosis. While the results are promising, the underlying assumptions about symptom-disease relationships and guideline conversion have not been thoroughly validated.

**Low Confidence:** The generalizability of the system to diseases beyond heart failure and the robustness of the approach across different patient populations and healthcare settings.

## Next Checks

1. **Symptom Verification Study:** Conduct a controlled study comparing the RL-based symptom questioning strategy against expert physician symptom selection on a held-out dataset. This would validate whether the learned strategy actually identifies the most informative symptoms for disease screening.

2. **Decision Procedure Completeness Audit:** Perform a systematic audit of the decision procedures across multiple disease categories to assess their completeness and accuracy in capturing clinical reasoning. This should include evaluation by multiple domain experts to identify potential gaps or ambiguities.

3. **Cross-Disease Performance Evaluation:** Test the system's performance on a broader range of diseases beyond heart failure, including both common and rare conditions, to evaluate its generalizability and identify potential limitations in the approach.