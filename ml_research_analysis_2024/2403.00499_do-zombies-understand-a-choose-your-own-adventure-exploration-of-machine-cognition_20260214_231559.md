---
ver: rpa2
title: Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of Machine Cognition
arxiv_id: '2403.00499'
source_url: https://arxiv.org/abs/2403.00499
tags:
- understanding
- consciousness
- language
- which
- understand
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper explores the debate on whether large language models
  truly understand text, arguing that opponents in this debate hold different definitions
  for understanding, particularly differing in their view on the role of consciousness.
  The authors propose a thought experiment involving an open-source chatbot Z which
  excels on every possible benchmark, seemingly without subjective experience.
---

# Do Zombies Understand? A Choose-Your-Own-Adventure Exploration of Machine Cognition

## Quick Facts
- arXiv ID: 2403.00499
- Source URL: https://arxiv.org/abs/2403.00499
- Reference count: 16
- One-line primary result: Opponents in the LLM understanding debate hold different definitions of understanding rooted in their views on consciousness.

## Executive Summary
This paper explores the debate on whether large language models truly understand text by exposing a fundamental terminological disagreement among researchers. The authors argue that the disagreement stems from different definitions of understanding, particularly regarding the role of consciousness. Through a thought experiment involving an open-source chatbot Z that excels on every benchmark without subjective experience, they demonstrate how different AI research traditions would answer differently about Z's understanding. The paper proposes two distinct working definitions for understanding - functional and conscious - and draws connections with philosophical, psychological, and neuroscience literature.

## Method Summary
The authors employ a conceptual analysis approach using a thought experiment (the Z chatbot) that satisfies all external benchmarks but lacks subjective experience. They trace historical patterns of AI benchmark adoption and abandonment, analyze seminal AI research traditions, and draw connections to consciousness research. The method involves philosophical reasoning about the mind-body problem and proposing two working definitions of understanding: functional (based on performance) and conscious (requiring both performance and subjective experience).

## Key Results
- Different AI research traditions implicitly use different definitions of understanding, revealed through their responses to the Z thought experiment
- Historical analysis shows a pattern of abandoning benchmarks once models achieve functional understanding of them
- The debate on machine understanding is fundamentally connected to questions about consciousness and the mind-body problem
- Two distinct working definitions of understanding are proposed, each with different implications for AI research agendas

## Why This Works (Mechanism)

### Mechanism 1
The paper works by exposing a terminological disagreement about "understanding" rooted in different assumptions about the role of consciousness. The authors construct a thought experiment (the Z chatbot) that satisfies all external benchmarks but lacks subjective experience, then show that two major AI research traditions would answer differently whether Z "understands." This reveals that the disagreement is not about facts but about definitions.

Core assumption: Researchers who disagree on whether LLMs understand are actually using different definitions, one based on functional performance and one requiring consciousness.

Evidence anchors:
- [abstract] "opponents in this debate hold different definitions for understanding, and particularly differ in their view on the role of consciousness"
- [section] "we propose two distinct working definitions for understanding which explicitly acknowledge the question of consciousness"
- [corpus] Weak corpus evidence - the neighbor papers discuss consciousness and understanding but don't directly address the definitional split the paper identifies.

### Mechanism 2
The paper works by tracing the historical pattern of adopting benchmarks until models functionally understand them, then moving to new benchmarks. By showing how tasks like chess and Go were abandoned as benchmarks once models functionally understood them, the paper demonstrates that the AI community implicitly accepts a functional definition of understanding.

Core assumption: The field's behavior (moving goalposts) reveals an implicit functional definition of understanding.

Evidence anchors:
- [section] "the recurring trend in the last 70 years has seen tasks adopted as benchmarks for understanding until automated models functionally understand them"
- [section] "chess served as a proxy task for understanding for nearly 50 years...chess engines now vastly outplay any human opponent"
- [corpus] No direct corpus evidence supporting this specific mechanism of benchmark abandonment.

### Mechanism 3
The paper works by connecting the machine cognition debate to the philosophical mind-body problem and consciousness research. By framing the question of machine understanding within the context of philosophical zombies and consciousness research, the paper provides a framework for understanding why the debate persists and how it might be resolved through neuroscience research on neural correlates of consciousness.

Core assumption: The machine cognition debate is fundamentally about the same questions as the mind-body problem and can be informed by consciousness research.

Evidence anchors:
- [abstract] "We contextualize the debate on machine cognition within the mind-body problem"
- [section] "We follow Chalmers (1995), who asks whether the quality of consciousness - the ability to have subjective experiences - is a strict requirement for understanding"
- [corpus] The corpus contains papers on consciousness and AI but doesn't directly support the specific connection made here.

## Foundational Learning

- Concept: Philosophical zombies and the hard problem of consciousness
  - Why needed here: The paper's thought experiment relies on the reader understanding what a philosophical zombie is and why the question of whether zombies can understand is philosophically interesting
  - Quick check question: What is the key difference between a philosophical zombie and a conscious human, according to the paper?

- Concept: Functional vs. conscious definitions of understanding
  - Why needed here: The paper's main contribution is distinguishing these two definitions and showing how they lead to different research agendas
  - Quick check question: According to the paper, what is the main difference between functional understanding and conscious understanding?

- Concept: Historical development of AI benchmarks
  - Why needed here: The paper uses historical examples (chess, Go, NLI) to show how the field implicitly adopts functional definitions of understanding
  - Quick check question: According to the paper, what happens to AI benchmarks once models achieve functional understanding of them?

## Architecture Onboarding

- Component map: The paper's argument has three main components: the thought experiment setup (Z chatbot), the two definitions of understanding (functional and conscious), and the historical analysis of benchmark adoption. These components feed into each other to build the overall argument.

- Critical path: The most important sequence is: (1) present the thought experiment with Z, (2) show how different research traditions would answer about Z's understanding, (3) reveal this reveals definitional disagreement, (4) trace historical pattern of benchmark adoption, (5) propose the two definitions and their implications.

- Design tradeoffs: The paper trades specificity for generality - it doesn't commit to one definition of understanding but instead shows why the debate exists. This makes the paper more widely applicable but less actionable for immediate research decisions.

- Failure signatures: If the historical analysis of benchmark adoption is incorrect, or if the thought experiment with Z doesn't clearly illustrate the definitional split, the paper's main argument would fail.

- First 3 experiments:
  1. Test whether researchers in different AI traditions actually have different implicit definitions of understanding by surveying them about the Z thought experiment
  2. Track benchmark saturation and abandonment over time in NLP to see if the historical pattern holds
  3. Survey the literature to see if the connection between consciousness and understanding is as widespread as the paper claims

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What empirical tests could distinguish between functional understanding and conscious understanding in LLMs?
- Basis in paper: [explicit] The paper explicitly proposes two distinct definitions of understanding and discusses their implications for AI research.
- Why unresolved: The paper highlights the fundamental distinction between functional and conscious understanding but does not propose concrete empirical tests to differentiate between them.
- What evidence would resolve it: Development of behavioral tests that can detect the presence of subjective experience in LLMs, beyond just measuring performance on benchmarks.

### Open Question 2
- Question: What are the computational or architectural requirements for achieving conscious understanding in AI systems?
- Basis in paper: [inferred] The paper draws connections between consciousness research and AI, suggesting that findings from neuroscience could inform AI architecture.
- Why unresolved: While the paper discusses the role of consciousness in understanding, it does not provide specific architectural requirements or computational models for implementing conscious understanding.
- What evidence would resolve it: Development of AI architectures that incorporate neural correlates of consciousness and demonstrate both high performance and signs of subjective experience.

### Open Question 3
- Question: How does the role of consciousness in understanding evolve as AI systems become more capable?
- Basis in paper: [explicit] The paper presents a thought experiment with a hypothetical AI system (Z) that excels on all benchmarks, raising questions about the necessity of consciousness for understanding.
- Why unresolved: The paper poses the question but does not explore how the relationship between consciousness and understanding might change as AI capabilities advance.
- What evidence would resolve it: Longitudinal studies tracking the correlation between AI performance and indicators of consciousness as AI systems become more sophisticated.

### Open Question 4
- Question: Can the concept of understanding in AI be meaningfully separated from the question of consciousness?
- Basis in paper: [explicit] The paper argues that current debates about AI understanding implicitly involve differing views on the role of consciousness.
- Why unresolved: The paper suggests that consciousness and understanding are intertwined but does not definitively answer whether they can be meaningfully separated.
- What evidence would resolve it: Philosophical arguments or empirical findings that either demonstrate a clear separation between understanding and consciousness, or show that they are inherently linked in all forms of intelligence.

## Limitations
- The central argument relies heavily on a thought experiment and conceptual analysis rather than empirical validation
- The historical analysis of benchmark adoption lacks direct corpus evidence showing explicit acceptance of functional understanding
- The connection to consciousness research remains speculative about practical implications for resolving the debate

## Confidence

*High Confidence*: The observation that researchers disagree about whether LLMs understand, and that this disagreement maps onto different assumptions about consciousness, appears well-supported by the literature review and thought experiment.

*Medium Confidence*: The historical pattern of benchmark adoption and abandonment as evidence for an implicit functional definition of understanding is plausible but would benefit from more systematic empirical tracking.

*Medium Confidence*: The connection between machine cognition and the philosophical mind-body problem is conceptually sound, but the practical implications remain unclear without further neuroscience research.

## Next Checks
1. Survey researchers from different AI traditions (e.g., cognitive science vs. engineering approaches) with the Z thought experiment to empirically verify whether they do indeed answer differently about understanding, and whether these differences map to explicit vs. implicit definitions.

2. Conduct a systematic analysis of NLP benchmark papers over the past 20 years to track when and how the field abandons benchmarks, looking for explicit statements about functional understanding versus other criteria.

3. Review the consciousness literature to identify specific neural correlates that have been empirically established, and assess whether these could serve as testable criteria for machine consciousness if we accept the paper's framework.