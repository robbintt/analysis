---
ver: rpa2
title: Don't Use LLMs to Make Relevance Judgments
arxiv_id: '2409.15133'
source_url: https://arxiv.org/abs/2409.15133
tags:
- relevance
- judgments
- documents
- trec
- relevant
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "The core argument is that using LLMs to generate relevance judgments\
  \ for TREC-style test collections creates a ceiling effect: systems cannot be measured\
  \ as performing better than the LLM that generated the ground truth, because the\
  \ evaluation is only comparing against the model\u2019s own judgments. This is demonstrated\
  \ by showing that if the LLM also serves as part of the retrieval system, improvements\
  \ beyond the LLM's capabilities will not be detected."
---

# Don't Use LLMs to Make Relevance Judgments

## Quick Facts
- arXiv ID: 2409.15133
- Source URL: https://arxiv.org/abs/2409.15133
- Authors: Ian Soboroff
- Reference count: 23
- Primary result: Using LLMs to generate relevance judgments creates a ceiling effect that prevents measuring systems better than the LLM itself

## Executive Summary
The paper argues that using Large Language Models (LLMs) to generate relevance judgments for TREC-style test collections creates fundamental measurement problems. When LLMs serve as both the evaluation model and potentially as components of retrieval systems being measured, improvements beyond the LLM's capabilities cannot be detected. This ceiling effect means systems cannot be measured as performing better than the LLM that generated the ground truth, fundamentally undermining the purpose of evaluation. The paper concludes that LLMs should not be used to generate relevance judgments when they are part of the systems being evaluated.

## Method Summary
The paper presents a theoretical argument against using LLMs for relevance judgment creation in information retrieval evaluation. Rather than describing a specific experimental methodology, it analyzes the implications of using LLM-generated judgments through mathematical reasoning based on the ideal ranking theorem. The core claim is that evaluation systems can only measure systems that perform worse than the state of the art at the time the relevance judgments were created, making future improvements impossible to detect.

## Key Results
- Systems cannot be measured as performing better than the LLM that generated the ground truth
- LLMs used for judgments will appear to perform worse than human assessors when measured against human-created ground truth
- Using LLM-generated judgments prevents future-proofing of evaluation because newer models will appear worse than older ones when measured against older judgments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using LLMs to generate relevance judgments creates a ceiling effect because the evaluation can only measure performance relative to the LLM's own judgments
- Mechanism: The LLM serves as both the evaluation model and potentially a component of the retrieval systems being measured. This means any system improvement beyond the LLM's capabilities cannot be detected
- Core assumption: The LLM's judgments are treated as ground truth, making it impossible to measure systems that perform better than the LLM
- Evidence anchors:
  - [abstract] "systems cannot be measured as performing better than the LLM that generated the ground truth, because the evaluation is only comparing against the model's own judgments"
  - [section] "The bottom-line-up-front message is, don't use LLMs to create relevance judgments for TREC-style evaluations"
  - [corpus] Weak evidence - corpus contains papers discussing LLM relevance assessment but lacks direct evidence about ceiling effects
- Break condition: If the LLM is not used as part of any evaluated system, or if the LLM is fine-tuned on manual judgments that provide privileged information beyond what the systems have access to

### Mechanism 2
- Claim: LLMs used for relevance judgments will appear to perform worse than human assessors when measured against human-created ground truth
- Mechanism: Human assessors have world knowledge and context that LLMs lack, leading to systematic differences in relevance judgments that make LLMs look inferior when compared to human truth data
- Core assumption: Human relevance judgments represent a different (and often higher) standard than LLM-generated judgments
- Evidence anchors:
  - [abstract] "letting the LLM write your truth data handicaps the evaluation by setting that LLM as a ceiling on performance"
  - [section] "When the next shiny model comes out, it will measure as performing less well than the old model, because it necessarily must retrieve unjudged documents or ones judged incorrectly as not relevant"
  - [corpus] Weak evidence - corpus papers discuss LLM vs human agreement but don't directly address measurement inferiority
- Break condition: If the LLM is trained or fine-tuned on the same human judgments being used as ground truth, creating privileged information

### Mechanism 3
- Claim: Using LLM-generated judgments prevents future-proofing of evaluation because newer models will appear worse than older ones when measured against older judgments
- Mechanism: Newer retrieval systems will inevitably find relevant documents that older LLM judgments missed or incorrectly labeled, making the newer systems appear to perform worse when measured against the older judgments
- Core assumption: Retrieval systems and LLMs improve over time, finding new relevant documents that were missed in earlier judgments
- Evidence anchors:
  - [section] "When the next shiny model comes out, it will measure as performing less well than the old model, because it necessarily must retrieve unjudged documents or ones judged incorrectly as not relevant"
  - [section] "And so the relevance judgments can only measure systems that perform worse than the state of the art at the time the relevance judgments were created"
  - [corpus] No direct evidence in corpus about temporal measurement degradation
- Break condition: If evaluation uses continuous updating of judgments or if systems are only compared to contemporaneous models rather than historical ones

## Foundational Learning

- Concept: Cranfield paradigm and test collections
  - Why needed here: Understanding how TREC-style evaluations work is crucial for grasping why LLM-generated judgments are problematic
  - Quick check question: What are the three components of a TREC-style test collection and how are relevance judgments typically created?

- Concept: Ideal ranking theorem
  - Why needed here: The paper's core argument relies on the mathematical proof that no ranking can be measured as better than the truth data
  - Quick check question: According to the ideal ranking theorem, what happens when a system retrieves documents not present in the relevance judgments?

- Concept: Kendall's tau and Spearman's rho
  - Why needed here: These correlation measures are mentioned as ways to quantify how well automatic evaluation methods match human rankings
  - Quick check question: What do Kendall's tau and Spearman's rho measure in the context of comparing system rankings?

## Architecture Onboarding

- Component map: Document collection -> Topics -> Relevance judgments creation -> System runs -> Pooling process -> Judgment assignment -> Metric computation -> System ranking
- Critical path: Document collection → Topics → Relevance judgments creation → System runs → Pooling process → Judgment assignment → Metric computation → System ranking
- Design tradeoffs: Manual judgment creation is expensive but accurate vs. LLM generation is cheap but creates ceiling effects; complete judgments are thorough but impractical vs. pooling is efficient but incomplete
- Failure signatures: Systems appear to plateau in performance improvement; newer models measure worse than older ones; evaluation fails to distinguish between genuinely better systems
- First 3 experiments:
  1. Run a small-scale test comparing LLM-generated judgments vs human judgments on the same system runs and measure the difference in system rankings
  2. Create synthetic data where a "better" system finds documents not in the ground truth and measure how the evaluation scores this system
  3. Implement a hybrid approach where LLM judgments are used but supplemented with manual verification of high-scoring documents

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can LLM-based relevance judgments be used for evaluation if the LLM is not part of the retrieval system being tested?
- Basis in paper: [explicit] The paper explicitly states that LLMs can be used if they have more knowledge about relevance than the systems being measured, such as being fine-tuned on manual relevance judgments
- Why unresolved: While the paper provides conditions under which LLM-based judgments might be acceptable, it doesn't empirically demonstrate the practical effectiveness or limitations of this approach
- What evidence would resolve it: Empirical studies comparing system rankings using LLM judgments (fine-tuned on human data) versus human judgments, showing whether the rankings remain consistent and meaningful

### Open Question 2
- Question: What specific quality control mechanisms can effectively detect and correct LLM-generated relevance judgment errors?
- Basis in paper: [explicit] The paper suggests using LLMs to follow assessors and look for mistakes, but notes that simple questioning won't work due to people trusting computers too much
- Why unresolved: The paper identifies the problem but doesn't propose specific solutions or validate any quality control approaches
- What evidence would resolve it: Development and testing of automated quality control systems that can identify and flag potentially incorrect LLM judgments with measurable accuracy

### Open Question 3
- Question: Under what conditions might LLM-generated relevance judgments provide value despite their limitations?
- Basis in paper: [explicit] The paper mentions that Thomas et al. found LLM judgments to be as useful as crowdsourced judgments but not better than curated judgments, suggesting potential value in certain contexts
- Why unresolved: The paper doesn't explore specific scenarios or thresholds where LLM judgments might be acceptable or beneficial
- What evidence would resolve it: Systematic evaluation of LLM judgments across different types of collections, topics, and evaluation purposes to identify when they provide sufficient quality for specific use cases

## Limitations
- The paper relies primarily on theoretical reasoning rather than empirical validation of the ceiling effect
- The argument assumes LLMs are used as components of the retrieval systems being evaluated, which may not be the common case
- Limited evidence exists about the practical impact of temporal degradation on real-world evaluation scenarios

## Confidence

**High confidence**: Core claim that LLM-generated judgments create ceiling effects preventing measurement of better systems - well-supported by theoretical reasoning and ideal ranking theorem

**Medium confidence**: Claims about LLM inferiority to human judgments - theoretical argument is sound but corpus provides weak direct evidence

**Low confidence**: Claims about temporal degradation and measurement of newer models - no direct evidence exists in the corpus about this effect

## Next Checks

1. **Empirical Ceiling Effect Test**: Create a controlled experiment where a retrieval system demonstrably finds relevant documents missed by LLM judgments, then measure whether the evaluation system correctly identifies this improvement

2. **Hybrid Judgment Validation**: Implement and test a hybrid approach where LLM judgments are used for initial assessment but supplemented with targeted human verification of high-scoring documents, measuring the impact on ceiling effects

3. **Cross-Temporal Comparison**: Compare system rankings when evaluated against contemporaneous human judgments versus older LLM-generated judgments to quantify the temporal degradation effect claimed in the paper