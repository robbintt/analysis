---
ver: rpa2
title: 'Bosch Street Dataset: A Multi-Modal Dataset with Imaging Radar for Automated
  Driving'
arxiv_id: '2407.12803'
source_url: https://arxiv.org/abs/2407.12803
tags:
- radar
- dataset
- lidar
- sensor
- sensors
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the Bosch street dataset (BSD), a novel multi-modal
  large-scale dataset for highly automated driving (HAD) and advanced driver-assistance
  systems (ADAS) research. Unlike existing datasets, BSD offers a unique integration
  of high-resolution imaging radar, lidar, and camera sensors, providing unprecedented
  360-degree coverage to bridge the current gap in high-resolution radar data availability.
---

# Bosch Street Dataset: A Multi-Modal Dataset with Imaging Radar for Automated Driving

## Quick Facts
- arXiv ID: 2407.12803
- Source URL: https://arxiv.org/abs/2407.12803
- Reference count: 32
- Primary result: Novel multi-modal dataset with imaging radar, lidar, and cameras for automated driving research

## Executive Summary
The Bosch Street Dataset (BSD) introduces a large-scale, multi-modal dataset for automated driving research featuring high-resolution imaging radar, lidar, and camera sensors with 360-degree coverage. Unlike existing datasets, BSD specifically addresses the gap in high-resolution radar data availability, spanning urban, rural, and highway environments. The dataset includes 13.6k sequences and over 36 hours of measurements, all annotated with bounding box labels to support advanced driver-assistance system (ADAS) and highly automated driving (HAD) research.

## Method Summary
The dataset was collected using a sensor suite combining imaging radar, lidar, and camera systems mounted on test vehicles operating in Germany and neighboring countries. Data collection covered diverse driving scenarios across urban, rural, and highway environments. The imaging radar provides high-resolution measurements complementary to lidar and camera modalities, enabling enhanced object detection and sensor fusion research. All data sequences are timestamped and synchronized across modalities, with comprehensive bounding box annotations for objects in the scene.

## Key Results
- 13.6k sequences and over 36 hours of synchronized multi-modal measurements
- Integration of high-resolution imaging radar with lidar and camera sensors
- Comprehensive 360-degree coverage across diverse driving environments
- Initial benchmarks provided for sensor modalities and a development kit for data analysis

## Why This Works (Mechanism)
The dataset's effectiveness stems from combining high-resolution imaging radar with traditional lidar and camera sensors, creating a rich multi-modal representation of the driving environment. The imaging radar's unique ability to penetrate adverse weather conditions while maintaining high spatial resolution addresses critical limitations of optical sensors. The 360-degree coverage ensures complete environmental awareness, while the diverse geographic and environmental sampling enables robust algorithm development across various driving scenarios.

## Foundational Learning
- **Sensor Fusion Principles**: Why needed - To combine complementary information from multiple sensor modalities for improved perception accuracy. Quick check - Verify temporal synchronization between sensor streams.
- **Imaging Radar Technology**: Why needed - To understand high-resolution radar capabilities and limitations compared to conventional radar. Quick check - Review radar resolution specifications and angular coverage.
- **Bounding Box Annotation**: Why needed - To comprehend object detection labeling methodology and its impact on algorithm training. Quick check - Examine annotation consistency across different sensor modalities.
- **Temporal Alignment**: Why needed - To ensure accurate cross-modal data association for fusion algorithms. Quick check - Validate timestamp synchronization between sensors.
- **Environmental Diversity**: Why needed - To assess dataset representativeness across different driving conditions. Quick check - Analyze geographic distribution and weather conditions in collected data.

## Architecture Onboarding
- **Component Map**: Vehicles equipped with imaging radar -> lidar -> cameras -> central data logging system -> annotation pipeline
- **Critical Path**: Sensor data acquisition → synchronization → storage → annotation → benchmarking
- **Design Tradeoffs**: High-resolution radar provides weather robustness but adds complexity vs. simpler camera-only systems; comprehensive coverage vs. data volume management
- **Failure Signatures**: Missing sensor data due to environmental interference, synchronization errors between modalities, incomplete object annotations
- **First 3 Experiments**: 1) Benchmark single-modality object detection performance, 2) Evaluate radar-camera fusion accuracy, 3) Test cross-weather condition robustness

## Open Questions the Paper Calls Out
None

## Limitations
- Unclear sensor specifications including horizontal field of view and radar angular resolution
- Potential synchronization uncertainties between sensor modalities
- Geographic coverage limited to Germany and neighboring countries
- Missing detailed technical specifications for camera resolution and lidar performance parameters

## Confidence
- Dataset novelty and comprehensiveness: Medium
- Technical specifications and coverage claims: Low

## Next Checks
1. Request detailed sensor specification sheets from the authors to verify claimed resolutions and field-of-view parameters
2. Analyze a sample subset to assess temporal synchronization quality between modalities
3. Compare geographic and environmental diversity with established benchmarks to validate coverage claims