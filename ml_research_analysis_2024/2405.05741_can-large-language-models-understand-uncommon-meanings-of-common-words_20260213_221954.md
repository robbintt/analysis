---
ver: rpa2
title: Can large language models understand uncommon meanings of common words?
arxiv_id: '2405.05741'
source_url: https://arxiv.org/abs/2405.05741
tags:
- llms
- language
- performance
- arxiv
- which
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces LeSC, a novel benchmark for evaluating fine-grained
  lexical semantic understanding in large language models (LLMs). LeSC is the first
  benchmark to assess both fine-grained and cross-lingual dimensions of semantic comprehension.
---

# Can large language models understand uncommon meanings of common words?

## Quick Facts
- arXiv ID: 2405.05741
- Source URL: https://arxiv.org/abs/2405.05741
- Reference count: 14
- Key outcome: GPT-4 underperforms 16-year-olds by 3.9% on lexical semantic understanding task

## Executive Summary
This paper introduces LeSC, a novel benchmark for evaluating fine-grained lexical semantic understanding in large language models (LLMs). LeSC is the first benchmark to assess both fine-grained and cross-lingual dimensions of semantic comprehension, comprising 600 high-quality samples derived from standardized tests focusing on uncommon meanings of common words. Extensive experiments reveal that even state-of-the-art LLMs like GPT-4 and GPT-3.5 underperform compared to 16-year-old humans by 3.9% and 22.3%, respectively, on the LSU task.

## Method Summary
The LeSC benchmark evaluates LLMs on their ability to comprehend uncommon meanings of common words through a dataset of 600 multiple-choice questions derived from standardized tests (GAOKAO and CET). The study employs two evaluation metrics: absolute accuracy and a novel weighted accuracy that accounts for option order sensitivity. Experiments test various LLMs (GPT-4, GPT-3.5, Vicuna, Llama2, Qwen, Baichuan2, ChatGLM3) using role-oriented and task-oriented prompts, with additional tests using Chain-of-Thought prompting and retrieval-augmented generation techniques.

## Key Results
- GPT-4 underperforms 16-year-old humans by 3.9% on LSU task
- GPT-3.5 underperforms by 22.3% on LSU task
- Advanced prompting techniques and RAG partially help but benefits diminish or become counterproductive on very large LLMs

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs struggle with fine-grained lexical semantic understanding because they rely on co-occurrence patterns and imitation rather than true semantic comprehension
- Mechanism: During pretraining, LLMs internalize statistical relationships between words, leading to coarse-grained representations and defaulting to common meanings even when context demands uncommon interpretations
- Core assumption: Pre-training corpus biases and large parameter counts lead to overconfidence in memorized patterns rather than contextual reasoning
- Evidence anchors:
  - [abstract] "Large language models (LLMs) like ChatGPT have shown significant advancements across diverse natural language understanding (NLU) tasks... Yet, lacking widely acknowledged testing mechanisms, answering `whether LLMs are stochastic parrots or genuinely comprehend the world' remains unclear"
  - [section 3.3.1] "However, introducing translation steps fundamentally exposes the model to a more familiar and proﬁcient task paradigm, thereby capitalizing on its advantages in co-occurrence contrast and imitation"
- Break condition: When models encounter truly novel contexts where common co-occurrence patterns fail to provide correct answers

### Mechanism 2
- Claim: Model scale paradoxically reduces effectiveness of advanced prompting techniques due to increased overconfidence and adherence to pre-trained knowledge
- Mechanism: Larger models internalize more knowledge from training data, leading to stronger biases and less flexibility in following novel instructions or reasoning chains
- Core assumption: Instruction fine-tuning and human preference alignment increase model adherence to learned patterns rather than promoting genuine reasoning
- Evidence anchors:
  - [abstract] "Results indicate that advancing prompting techniques and retrieval-augmented generation partially help mitigate this challenge, however, their beneﬁts tend to diminish or even become counterproductive on very large language models"
  - [section 3.2.3] "Our observations reveal a consistent inverse scaling law for methods like CoT and few-shot in-context learning, diminishing their eﬃcacy in improving LLMs' performance"
- Break condition: When prompting techniques introduce conflicts with pre-trained knowledge or when models become too confident in their existing understanding

### Mechanism 3
- Claim: LLMs prioritize misleading information over corrective instructions due to their fundamental reliance on pattern matching rather than logical reasoning
- Mechanism: Models treat all input text as potential patterns to match against training data, making them susceptible to accepting false information when it aligns with familiar patterns
- Core assumption: LLMs lack true logical reasoning capabilities and instead rely on pattern matching and statistical correlations
- Evidence anchors:
  - [abstract] "Additionally, we observe an intriguing ﬁnding that, in lexical comprehension tasks, LLMs signiﬁcantly prioritize misleading information over valuable instructions"
  - [section 3.2.4] "Our results reveal a noteworthy impact of these instructions on LLMs, reaching up to 96% e ﬀectiveness" and "LLMs signiﬁcantly prioritize misleading information over valuable instructions"
- Break condition: When models encounter clear logical contradictions or when external knowledge explicitly contradicts internal patterns

## Foundational Learning

- Concept: Fine-grained lexical semantics
  - Why needed here: The core task requires understanding uncommon meanings of common words, which demands precise semantic differentiation beyond general language understanding
  - Quick check question: Can you explain the difference between semantic understanding at sentence level versus word level?

- Concept: Cross-lingual transfer
  - Why needed here: The benchmark tests both English questions and Chinese answers, requiring models to transfer understanding across languages
  - Quick check question: How does a model's performance typically change when tested on a language different from its training data?

- Concept: Model calibration and overconfidence
  - Why needed here: Understanding why larger models perform worse with advanced techniques requires knowledge of how model confidence relates to performance
  - Quick check question: What relationship typically exists between model size and confidence in predictions?

## Architecture Onboarding

- Component map: Data collection (GAOKAO/CET sources) → dataset construction (polysemy dictionary → sentence filtering → multiple-choice formatting) → prompt design (role-oriented/task-oriented) → model evaluation (absolute and weighted accuracy) → result analysis
- Critical path: Data collection → dataset construction → prompt design → model evaluation → result analysis
- Design tradeoffs: High-quality human-curated data vs. scalability, English questions with Chinese answers vs. pure English testing, multiple-choice format vs. open-ended responses
- Failure signatures: Models performing at or below random baseline, large performance gaps between absolute and weighted accuracy, inverse scaling laws for prompting techniques
- First 3 experiments:
  1. Run baseline evaluation on Vicuna-7B using LeSC dataset to verify random baseline comparison
  2. Test GPT-4 with both role-oriented and task-oriented prompts to measure prompt sensitivity
  3. Apply CoT prompting to Llama2-13B with varying shot counts to observe scaling effects

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does model size affect the diminishing returns of advanced prompting techniques like Chain-of-Thought (CoT) in large language models?
- Basis in paper: [explicit] The paper explicitly states that advanced prompting techniques, such as CoT, effectively enhance overall performance on the LeSC dataset. However, on very large LLMs, these strategies might be ineffective or even counterproductive, and an inverse scaling law for methods like CoT and few-shot in-context learning is observed.
- Why unresolved: While the paper observes the diminishing returns, it does not provide a detailed quantitative analysis or a clear threshold where these techniques become counterproductive.
- What evidence would resolve it: Empirical studies quantifying the relationship between model size and the effectiveness of CoT and similar techniques, identifying the specific model size threshold where these techniques start to diminish in effectiveness.

### Open Question 2
- Question: What is the underlying reason for LLMs' preference for misleading information over corrective instructions in lexical semantic tasks?
- Basis in paper: [explicit] The paper observes that LLMs significantly prioritize misleading information over valuable instructions, even when provided with explicit instructions to ignore incorrect information.
- Why unresolved: The paper does not delve into the cognitive or training-related reasons behind this preference, leaving the underlying mechanism unclear.
- What evidence would resolve it: Detailed cognitive and training-related studies exploring why LLMs prioritize misleading information, potentially involving ablation studies or controlled experiments to isolate the contributing factors.

### Open Question 3
- Question: How does the pre-training corpus composition influence the cross-lingual transfer capabilities of LLMs?
- Basis in paper: [explicit] The paper finds that disparities in pre-training corpora may account for substantial divergences in cross-lingual transfer among different models, as evidenced by the comparison between Vicuna and Baichuan models.
- Why unresolved: While the paper identifies a correlation, it does not provide a comprehensive analysis of how different types and proportions of languages in the pre-training corpus specifically impact cross-lingual transfer.
- What evidence would resolve it: Comparative studies analyzing the cross-lingual transfer performance of LLMs with varied pre-training corpus compositions, quantifying the impact of different language proportions and types on transfer capabilities.

## Limitations

- Benchmark Generalizability: The LeSC benchmark focuses specifically on uncommon meanings of common words, representing a narrow slice of lexical semantic understanding that may not generalize to broader semantic comprehension tasks
- Human Baseline Comparability: The methodology for human evaluation is not fully specified, making it difficult to assess the robustness of the 16-year-old benchmark comparison
- Prompt Sensitivity & Reproducibility: The exact formulations of role-oriented and task-oriented prompts are not provided, making it difficult to assess whether results are reproducible or sensitive to prompt engineering variations

## Confidence

**High Confidence**: LLMs struggle with fine-grained lexical semantic understanding, as evidenced by performance below human baselines and the novel observation of prioritizing misleading information over corrective instructions.

**Medium Confidence**: The claim that larger models show inverse scaling for advanced prompting techniques is supported by experimental results but may be sensitive to specific prompting formulations.

**Low Confidence**: The mechanism explanations for why LLMs struggle (co-occurrence reliance vs. true semantic comprehension) remain speculative without direct evidence linking training data patterns to evaluation performance.

## Next Checks

1. **Prompt Ablation Study**: Systematically vary the role-oriented and task-oriented prompts across different formulations while keeping the LeSC dataset constant to determine whether the observed inverse scaling law for prompting techniques is robust or an artifact of specific prompt choices.

2. **Cross-Domain Transfer Test**: Apply the LeSC evaluation framework to non-standardized test contexts (e.g., real-world usage examples, creative writing, technical documentation) to assess whether the observed limitations generalize beyond educational testing scenarios.

3. **Human Expert Comparison**: Replicate the human evaluation with domain experts (linguists, language teachers) rather than just 16-year-olds to establish whether the human baseline represents typical performance or whether expert-level understanding would show different patterns of LLM performance gaps.