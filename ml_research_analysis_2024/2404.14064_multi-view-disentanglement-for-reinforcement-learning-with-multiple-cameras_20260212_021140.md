---
ver: rpa2
title: Multi-view Disentanglement for Reinforcement Learning with Multiple Cameras
arxiv_id: '2404.14064'
source_url: https://arxiv.org/abs/2404.14064
tags:
- cameras
- camera
- learning
- representation
- shared
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of learning robust image-based
  policies in reinforcement learning when only a single camera may be available at
  deployment, despite using multiple cameras during training. The core method, Multi-View
  Disentanglement (MVD), learns a disentangled representation consisting of shared
  (across cameras) and private (camera-specific) components using contrastive learning.
---

# Multi-view Disentanglement for Reinforcement Learning with Multiple Cameras

## Quick Facts
- arXiv ID: 2404.14064
- Source URL: https://arxiv.org/abs/2404.14064
- Authors: Mhairi Dunion; Stefano V. Albrecht
- Reference count: 11
- Primary result: Enables zero-shot generalization to any single camera from multi-camera training in RL

## Executive Summary
This paper addresses the challenge of deploying image-based reinforcement learning policies when only a single camera is available at test time, despite training with multiple cameras. The proposed Multi-View Disentanglement (MVD) method learns a shared representation across all camera views while maintaining camera-specific private components through contrastive learning. This disentangled representation enables zero-shot generalization to any individual camera view, including third-person perspectives where single-camera baselines fail entirely. MVD outperforms baselines that combine multiple cameras during training and requires no additional data collection or teacher policies.

## Method Summary
Multi-View Disentanglement (MVD) learns a shared representation across multiple camera views by leveraging known camera extrinsics to create positive and negative pairs for contrastive learning. The method disentangles observations into shared components (relevant to task completion) and private components (view-specific details like textures or backgrounds). During training, the agent experiences multiple camera views simultaneously, learning to extract task-relevant information that transfers across views. At deployment, the policy can operate with any single camera from the training set by using only the shared representation, enabling robust performance even with views never seen during single-camera training.

## Key Results
- Successfully learns optimal policies using any individual camera from the training set
- Enables zero-shot generalization to single cameras, including third-person views
- Outperforms baselines that combine multiple cameras during training on Panda and MetaWorld tasks
- Eliminates need for additional data collection or separate teacher policies

## Why This Works (Mechanism)
The method works by explicitly separating task-relevant information from view-specific details through contrastive learning. By using known camera extrinsics, MVD creates positive pairs (observations of the same state from different views) and negative pairs (observations of different states), forcing the shared representation to capture only information invariant across views. This shared representation contains the essential task information while discarding view-dependent noise, enabling the policy to generalize across different camera perspectives without additional training.

## Foundational Learning

**Contrastive Learning**
*Why needed*: To learn representations that capture similarities across different views of the same state
*Quick check*: Verify that positive pairs (same state, different views) are closer in representation space than negative pairs

**Disentanglement**
*Why needed*: To separate task-relevant information from view-specific details
*Quick check*: Confirm that private components contain view-dependent features while shared components remain view-invariant

**Camera Extrinsics**
*Why needed*: To determine which observations correspond to the same underlying state across views
*Quick check*: Validate that known camera poses correctly align observations from different viewpoints

## Architecture Onboarding

**Component Map**
Observation Encoder -> Shared/Private Disentangler -> Shared Encoder -> Policy Network
                          |
                          -> Private Encoder

**Critical Path**
Observations → Encoder → Shared/Private Separation → Shared Representation → Policy → Action

**Design Tradeoffs**
- Requires known camera extrinsics (limitation) vs. strong generalization performance (benefit)
- Maintains separate shared and private representations (memory overhead) vs. view-invariant task learning (performance gain)
- Uses contrastive learning (requires positive/negative pairs) vs. simpler reconstruction-based approaches

**Failure Signatures**
- Poor performance indicates insufficient disentanglement between shared and private components
- Failure to generalize suggests shared representation doesn't capture task-relevant information adequately
- Inconsistent performance across views may indicate camera extrinsics are incorrect or noisy

**First Experiments**
1. Verify contrastive loss decreases when training with known camera extrinsics
2. Test whether shared representation remains consistent across different views of the same state
3. Evaluate policy performance when using only shared representation versus full observation

## Open Questions the Paper Calls Out
None specified in the provided information.

## Limitations
- Requires known camera extrinsics for contrastive learning, limiting practical applicability
- Performance untested in environments with significant visual distractors or dynamic backgrounds
- Method's computational overhead for maintaining separate representations not thoroughly analyzed

## Confidence

**High confidence**: The core claim that MVD enables zero-shot generalization to single cameras from multi-camera training is well-supported by the experimental results

**Medium confidence**: The assertion that MVD outperforms baselines that combine multiple cameras is valid within the tested domains but may not generalize to all RL tasks

**Medium confidence**: The claim about no additional data collection being required is accurate, though the need for known camera extrinsics could be considered an implicit requirement

## Next Checks

1. Test MVD on environments with significant visual distractors or dynamic backgrounds to evaluate robustness beyond controlled robotic settings

2. Evaluate performance when camera extrinsics are unknown or must be estimated, assessing the method's practical applicability

3. Compare MVD against other representation learning approaches like autoencoders or VAEs that don't require camera pose information