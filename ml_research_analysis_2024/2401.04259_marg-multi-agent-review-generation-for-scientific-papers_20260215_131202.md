---
ver: rpa2
title: 'MARG: Multi-Agent Review Generation for Scientific Papers'
arxiv_id: '2401.04259'
source_url: https://arxiv.org/abs/2401.04259
tags:
- agent
- comments
- information
- comment
- more
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces MARG, a multi-agent LLM approach for generating
  scientific peer review feedback. By splitting paper text across specialized agents
  and enabling inter-agent communication, MARG overcomes input length limitations
  and produces more specific, helpful comments than baseline methods.
---

# MARG: Multi-Agent Review Generation for Scientific Papers

## Quick Facts
- arXiv ID: 2401.04259
- Source URL: https://arxiv.org/abs/2401.04259
- Authors: Mike D'Arcy; Tom Hope; Larry Birnbaum; Doug Downey
- Reference count: 40
- Key outcome: MARG achieves 2.2x improvement in generating helpful peer review comments (3.7 vs 1.7 per paper) while reducing generic comments from 60% to 29%

## Executive Summary
This paper introduces MARG, a multi-agent LLM approach for generating scientific peer review feedback. By splitting paper text across specialized agents and enabling inter-agent communication, MARG overcomes input length limitations and produces more specific, helpful comments than baseline methods. In a user study, MARG generated 3.7 good comments per paper compared to 1.7 for the best baseline, while reducing generic comment rates from 60% to 29%. The approach outperforms recent methods in automated metrics and demonstrates strong potential for assisting with scientific review tasks.

## Method Summary
MARG employs a multi-agent system where specialized agents are assigned different sections of a scientific paper. These agents communicate with each other to synthesize comprehensive review feedback. The approach addresses the LLM context window limitations by distributing the review task across multiple agents, each focusing on specific aspects of the paper. The agents collaborate through structured communication protocols to generate cohesive and detailed review comments.

## Key Results
- MARG generates 3.7 good comments per paper compared to 1.7 for the best baseline (2.2x improvement)
- Generic comment rate reduced from 60% to 29% compared to baseline methods
- Automated metrics show MARG outperforming recent state-of-the-art methods for review generation

## Why This Works (Mechanism)
The multi-agent architecture enables parallel processing of different paper sections while maintaining coherence through agent communication. By distributing the cognitive load across specialized agents, MARG can analyze papers more comprehensively than single LLM approaches constrained by context windows. The inter-agent communication allows for cross-validation of insights and synthesis of section-specific observations into holistic review feedback.

## Foundational Learning
- **Context window limitations**: LLMs cannot process entire scientific papers in one pass, necessitating text splitting strategies
- **Multi-agent collaboration**: Multiple specialized agents can collectively outperform single agents on complex tasks through information sharing
- **Peer review quality metrics**: Good reviews require specificity, constructive feedback, and relevance to paper content
- **LLM prompt engineering**: Effective agent communication protocols are crucial for coordinating multi-agent systems
- **Scientific paper structure**: Understanding standard sections (abstract, intro, methods, results) enables specialized agent design
- **Human evaluation methodology**: User studies are essential for validating AI-generated review quality beyond automated metrics

## Architecture Onboarding

**Component Map**: Paper text -> Section agents -> Inter-agent communication -> Review synthesis -> Output

**Critical Path**: Input paper → Text segmentation → Agent assignment → Section analysis → Agent communication → Review generation → Quality filtering

**Design Tradeoffs**: 
- Agent specialization vs. communication overhead
- Granularity of text splitting vs. review coherence
- Quality filtering vs. output volume
- Automated metrics vs. human evaluation

**Failure Signatures**: 
- Generic comments when agent communication breaks down
- Section inconsistencies when text splitting is too aggressive
- Missed key points when agents lack proper context sharing
- Review incoherence when synthesis fails

**3 First Experiments**:
1. Single-agent baseline using entire paper (context window limited)
2. Non-communicating multi-agent baseline (no inter-agent coordination)
3. Human-written review comparison set for quality benchmarking

## Open Questions the Paper Calls Out
None provided in the source material.

## Limitations
- Study methodology and sample size details not fully provided
- Automated metrics may not fully capture nuanced review quality
- Results may not generalize across all paper domains and quality levels
- Longitudinal performance as LLM capabilities evolve not evaluated

## Confidence
- Multi-agent architecture innovation: High
- 2.2x improvement claim: Medium
- Generic comment reduction: Medium
- Automated metric superiority: Medium

## Next Checks
1. Conduct larger-scale user study with diverse paper types and reviewer expertise levels to confirm 2.2x improvement generalizability
2. Perform longitudinal studies to assess whether MARG maintains quality advantages as LLM capabilities evolve
3. Implement cross-validation with independent reviewer panels to verify reduced generic comment rates represent genuine quality improvement