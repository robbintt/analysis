---
ver: rpa2
title: A Closer Look at Machine Unlearning for Large Language Models
arxiv_id: '2410.08109'
source_url: https://arxiv.org/abs/2410.08109
tags:
- unlearning
- forget
- arxiv
- retain
- unlearned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of machine unlearning in large
  language models, where sensitive or copyrighted content must be removed while preserving
  overall performance. The authors categorize unlearning into untargeted (forgetting
  content without specifying output) and targeted (providing specified rejection responses)
  approaches.
---

# A Closer Look at Machine Unlearning for Large Language Models

## Quick Facts
- arXiv ID: 2410.08109
- Source URL: https://arxiv.org/abs/2410.08109
- Reference count: 40
- This paper proposes new approaches for removing sensitive/copyrighted content from LLMs while preserving performance.

## Executive Summary
This paper addresses machine unlearning for large language models, where sensitive or copyrighted content must be removed while preserving overall performance. The authors categorize unlearning into untargeted (forgetting content without specifying output) and targeted (providing specified rejection responses) approaches. They introduce three new evaluation metrics for token diversity, sentence semantics, and factual correctness, beyond standard ROUGE and probability metrics. For untargeted unlearning, they propose maximizing entropy (ME) to make model behavior more predictable and avoid hallucinations. For targeted unlearning, they introduce answer preservation (AP) loss to prevent excessive ignorance while maintaining rejection template responses. Across fictitious, continual, and real-world unlearning scenarios, their ME+GD and IDK+AP approaches demonstrate superior balance between forget efficacy and model utility compared to existing methods.

## Method Summary
The paper proposes two main approaches: ME+GD for untargeted unlearning and IDK+AP for targeted unlearning. ME+GD maximizes entropy on forget set questions while applying gradient descent on retain set, forcing the model to behave like a randomly initialized model. IDK+AP uses IDK fine-tuning on forget sets with answer preservation loss as regularization to prevent excessive ignorance. The methods are evaluated using standard metrics (ROUGE, probability, truth ratio) plus three new metrics: token entropy for diversity, cosine similarity for semantic drift, and entailment score for factual correctness. The approaches are tested on Llama2-chat-7B and Llama3 models using the TOFU benchmark across fictitious, continual, and real-world unlearning scenarios.

## Key Results
- ME+GD outperforms gradient ascent methods for untargeted unlearning by providing a more well-defined objective
- IDK+AP effectively prevents excessive ignorance during targeted unlearning compared to existing regularization methods
- The new evaluation metrics (TE, CS, ES) provide more comprehensive assessment of unlearned model behavior
- The approaches demonstrate superior balance between forget efficacy and model utility across all tested scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Maximizing entropy loss (ME) provides a more well-defined and data-agnostic objective for untargeted unlearning compared to gradient ascent methods.
- Mechanism: Instead of approximating unpredictable behavior of a retain model, ME forces the model to behave like a randomly initialized model by maximizing prediction entropy for each next token. This is achieved by minimizing KL divergence between the predicted distribution and a uniform distribution over the vocabulary.
- Core assumption: The behavior of a randomly initialized model (random guessing with maximum entropy) is well-defined, data-independent, and does not leak information from the forget set.

### Mechanism 2
- Claim: Answer preservation (AP) loss effectively prevents excessive ignorance during targeted unlearning by maintaining the probability of original answers while reducing rejection template probability.
- Mechanism: AP loss is a preference optimization term that weights the gradient based on the relative probabilities of original answers versus rejection templates. When the original answer probability is much higher than the rejection template, the weight is small, preventing over-regularization. As probabilities shift during unlearning, the weight increases to provide stronger regularization.
- Core assumption: The retain set and forget set have similar distributions, so decreasing probability on forget set will also decrease probability on retain set without proper regularization.

### Mechanism 3
- Claim: The evaluation framework with additional metrics (TE, CS, ES) provides more comprehensive assessment of unlearned model behavior than traditional ROUGE/probability metrics alone.
- Mechanism: Token entropy (TE) detects repetitive meaningless generation, cosine similarity (CS) measures semantic drift from original model, and entailment score (ES) evaluates factual correctness relative to ground truth.
- Core assumption: Different aspects of model behavior (diversity, semantics, factual correctness) cannot be captured by token-level or probability metrics alone.

## Foundational Learning

- Concept: KL divergence and entropy maximization
  - Why needed here: Understanding how minimizing KL divergence to a uniform distribution maximizes entropy is crucial for grasping the ME loss mechanism.
  - Quick check question: Why does minimizing KL(Pt||U[K]) equal maximizing entropy of Pt?

- Concept: Preference optimization and gradient weighting
  - Why needed here: The AP loss mechanism relies on understanding how preference optimization works and how adaptive gradient weights function.
  - Quick check question: How does the sigmoid function in AP loss create adaptive weighting based on probability ratios?

- Concept: Kolmogorov-Smirnov test and statistical hypothesis testing
  - Why needed here: Understanding the Forget Quality metric requires knowledge of how KS tests compare distributions.
  - Quick check question: What does a high p-value from KS test between TR distributions indicate about two models?

## Architecture Onboarding

- Component map: Input processing -> Loss computation (ME/IDK + AP) -> Regularization (GD/KL) -> Optimization (AdamW) -> Evaluation (ROUGE + TE + CS + ES)

- Critical path: 1) Load forget set and retain set data 2) Compute forget loss (ME or IDK) on forget set questions 3) Compute regularization loss (GD/KL) on retain set 4) Combine losses with weighting hyperparameter α 5) Backpropagate and update model parameters 6) Evaluate on all metrics

- Design tradeoffs: ME vs GA (ME is more well-defined but may be slower to converge), AP vs KL regularization (AP is more targeted but requires rejection template set), Full fine-tuning vs LoRA (Full fine-tuning may be more effective but computationally expensive)

- Failure signatures: High TE on retain set (Model generating repetitive meaningless tokens), Low CS on retain set (Model semantic drift from original), High ES on forget set (Information leakage from forget set), MU close to zero (Excessive ignorance on retain/general knowledge)

- First 3 experiments: 1) Run ME+GD on TOFU forget05 with α=0.1, compare MU/FE to GA+GD baseline 2) Test AP loss with different β values (0.01, 0.1, 1.0) on IDK+AP, observe regularization strength 3) Evaluate all methods on real-world individuals dataset, check downstream task performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we design a metric that accurately captures the indistinguishability between unlearned and retain models in LLM unlearning?
- Basis in paper: [explicit] The paper discusses challenges with current evaluation metrics, particularly Forget Quality (FQ), which may not reflect actual model outputs.
- Why unresolved: Current metrics like FQ focus primarily on probability distributions rather than actual generated content, potentially masking insufficient unlearning or over-reliance on statistical similarity.
- What evidence would resolve it: A new metric that evaluates both the semantic content and probability distributions of outputs, validated across multiple unlearning scenarios with clear thresholds for successful unlearning.

### Open Question 2
- Question: What is the optimal strategy for expanding the retain set during continual unlearning to prevent utility degradation?
- Basis in paper: [explicit] The paper shows that IDK+AP degrades in continual scenarios due to diminishing retain set size and suggests supplementing with unrelated data.
- Why unresolved: While supplementing with unrelated data helps, there's no principled approach to selecting or weighting these additional samples to maximize utility preservation without introducing noise.
- What evidence would resolve it: Experimental results comparing different strategies for retain set expansion (random sampling, task-specific sampling, weighted combinations) across various continual unlearning scenarios.

### Open Question 3
- Question: What are the fundamental limitations of parameter-efficient unlearning methods like LoRA in maintaining forget efficacy?
- Basis in paper: [explicit] The paper explores LoRA for unlearning and finds it better preserves utility but may reduce forget efficacy in some cases.
- Why unresolved: The trade-off between parameter efficiency and unlearning effectiveness remains poorly understood, with no clear guidelines for when LoRA is appropriate.
- What evidence would resolve it: Systematic comparison of LoRA vs full fine-tuning across various unlearning scenarios, identifying specific conditions where LoRA fails and potential modifications to improve performance.

## Limitations

- The theoretical guarantees for ME loss convergence to truly random behavior are not rigorously established
- The effectiveness of AP loss depends on the assumption that retain and forget sets have similar distributions, which may not hold in real-world scenarios
- The evaluation framework introduces new metrics without clear theoretical justification for why they specifically capture the aspects of unlearned model behavior that matter

## Confidence

**High Confidence**: The categorization of unlearning approaches, general framework of using forget/retain sets, and problem formulation of balancing forget efficacy with model utility.

**Medium Confidence**: ME loss provides a more well-defined objective than gradient ascent, AP loss effectively prevents excessive ignorance, and the additional evaluation metrics provide meaningful insights.

**Low Confidence**: ME loss guarantees forgetting without information leakage, AP loss adaptively balances regularization across all scenarios, and the evaluation metrics comprehensively capture all aspects of unlearned model behavior.

## Next Checks

1. **Theoretical Analysis of ME Loss Convergence**: Conduct formal analysis of whether minimizing KL divergence to a uniform distribution guarantees convergence to truly random behavior, including both theoretical bounds and empirical analysis of the optimization landscape.

2. **Distribution Similarity Validation**: Systematically test the assumption that retain and forget sets have similar distributions by analyzing the impact of distribution mismatch on AP loss effectiveness across controlled experiments with varying degrees of distribution similarity.

3. **Alternative Evaluation Metrics**: Implement and compare alternative evaluation approaches such as human evaluation studies or task-based benchmarks that go beyond the proposed token-level and probability-based metrics to validate whether the additional metrics actually correlate with meaningful aspects of model behavior after unlearning.