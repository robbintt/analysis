---
ver: rpa2
title: Convergence Analysis of Natural Gradient Descent for Over-parameterized Physics-Informed
  Neural Networks
arxiv_id: '2408.00573'
source_url: https://arxiv.org/abs/2408.00573
tags:
- have
- rate
- convergence
- gradient
- neural
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper analyzes the convergence of natural gradient descent\
  \ (NGD) for training over-parameterized two-layer Physics-Informed Neural Networks\
  \ (PINNs). The authors improve the learning rate for gradient descent from O(\u03BB\
  \u2080/n\xB2) to O(1/\u2225H^\u221E\u2225\xB2) for both L2 regression and PINN problems,\
  \ where \u03BB\u2080 is the least eigenvalue of the Gram matrix H^\u221E."
---

# Convergence Analysis of Natural Gradient Descent for Over-parameterized Physics-Informed Neural Networks

## Quick Facts
- arXiv ID: 2408.00573
- Source URL: https://arxiv.org/abs/2408.00573
- Authors: Xianliang Xu; Ting Du; Wang Kong; Bin Shan; Ye Li; Zhongyi Huang
- Reference count: 40
- Primary result: NGD achieves O(1) learning rate independent of Gram matrix for PINN training, with quadratic convergence for smooth activations

## Executive Summary
This paper analyzes the convergence of natural gradient descent (NGD) for training over-parameterized two-layer Physics-Informed Neural Networks (PINNs). The authors improve the learning rate for gradient descent from O(λ₀/n²) to O(1/∥H^∞∥²) for both L2 regression and PINN problems. They show that NGD achieves a learning rate of O(1) that is independent of the Gram matrix, leading to faster convergence. For smooth activation functions, NGD enjoys quadratic convergence. The main theoretical results are supported by numerical experiments.

## Method Summary
The paper analyzes convergence of gradient descent and natural gradient descent for training two-layer PINNs. The key insight is using a refined recursion formula that avoids standard decomposition, allowing improved learning rates. NGD uses Jacobian-based updates that are stable under small perturbations, enabling O(1) learning rates independent of the Gram matrix. The analysis covers both ReLU³ and smooth activation functions, with quadratic convergence established for smooth cases.

## Key Results
- Improved gradient descent learning rate from O(λ₀/n²) to O(1/∥H^∞∥²)
- NGD achieves O(1) learning rate independent of Gram matrix
- Quadratic convergence for smooth activation functions
- Theoretical bounds supported by numerical experiments

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The improved learning rate from O(λ₀/n²) to O(1/∥H^∞∥²) for gradient descent is achieved through a refined recursion formula that avoids the standard decomposition.
- Mechanism: Instead of decomposing the residual as y - u(k+1) = (y - u(k)) - (u(k+1) - u(k)), the authors use y - u(k+1) = (I - ηH(k))(y - u(k)) - I₁(k), where I₁(k) captures the error between the actual and predicted change. This avoids the conservative requirement that η = O(λ₀/n²) and instead allows η = O(1/∥H^∞∥²).
- Core assumption: The Gram matrix H(k) stays close to H^∞ throughout training, and λ₀ > 0 due to the no-parallel-inputs assumption.
- Evidence anchors:
  - [abstract] states the learning rate improves from O(λ₀/n²) to O(1/∥H^∞∥²).
  - [section 2.2] provides the recursion formula and proves the convergence under the improved rate.
- Break condition: If the Gram matrix deviates significantly from H^∞, the recursion formula no longer bounds the error, and convergence may fail.

### Mechanism 2
- Claim: Natural gradient descent (NGD) achieves a learning rate of O(1) independent of the Gram matrix, enabling faster convergence than gradient descent.
- Mechanism: NGD uses the update w(k+1) = w(k) - ηJ(k)ᵀ(H(k))⁻¹[s(k); h(k)], where J(k) is the Jacobian and H(k) = J(k)J(k)ᵀ. The key insight is that J(k) is stable under small perturbations of w, so (H(k))⁻¹ exists and the step size can be large without dependency on λ₀.
- Core assumption: The Jacobian matrix J(w) is stable for small perturbations of w, which is proven for both ReLU³ and smooth activation functions.
- Evidence anchors:
  - [abstract] claims NGD achieves O(1) learning rate independent of the Gram matrix.
  - [section 4.1] and Lemma 7 provide the stability bounds on J(w).
- Break condition: If the Jacobian becomes unstable (e.g., due to ill-conditioning or large perturbations), the inverse (H(k))⁻¹ may not exist or may amplify noise, breaking convergence.

### Mechanism 3
- Claim: For smooth activation functions, NGD enjoys quadratic convergence.
- Mechanism: The smoothness of the activation function ensures higher-order differentiability of the loss, allowing the NGD update to capture curvature information more effectively. This leads to faster local convergence.
- Core assumption: The activation function satisfies Assumption 5 (Lipschitz derivatives up to order 3 and analyticity).
- Evidence anchors:
  - [abstract] mentions quadratic convergence for smooth activation functions.
  - [section 4.1] and Corollary 1 provide the quadratic bound ∥u(t+1)∥ ≤ C B⁴ √m λ₃₀ ∥u(t)∥².
- Break condition: If the activation function is not smooth enough (e.g., ReLU), the higher-order terms vanish, and the convergence degrades to linear.

## Foundational Learning

- Concept: Gram matrix stability in over-parameterized neural networks
  - Why needed here: The convergence analysis relies on the Gram matrix H(k) staying close to H^∞, which ensures the optimization dynamics are well-behaved.
  - Quick check question: Why does λ₀ > 0 under the no-parallel-inputs assumption?

- Concept: Natural gradient and its relation to Gauss-Newton method
  - Why needed here: NGD is presented as a variant of Gauss-Newton that scales better in practice. Understanding the Jacobian-based update is key to grasping the improved convergence.
  - Quick check question: How does NGD differ from standard gradient descent in terms of the update direction?

- Concept: Mean value theorem and its role in bounding gradient changes
  - Why needed here: The proof uses the mean value theorem to bound the difference between function values at different iterates, which is crucial for the error recursion.
  - Quick check question: In what way does the mean value theorem help bound |σ'(ζ(k)) - σ'(wr(k)ᵀxp)| in the proof?

## Architecture Onboarding

- Component map: Two-layer neural network with ReLU³ or smooth activation -> Gram matrix H(k) tracking inner products of gradients -> Jacobian matrix J(k) for NGD update -> Training data split into interior and boundary samples -> Loss function combining PDE residual and boundary condition

- Critical path:
  1. Initialize weights randomly
  2. Compute predictions and residuals
  3. Form Gram matrix or Jacobian
  4. Update weights via GD or NGD
  5. Check convergence criteria

- Design tradeoffs:
  - GD: Simpler, cheaper per iteration, but requires small learning rate and slower convergence
  - NGD: Faster convergence, O(1) learning rate, but higher memory and computational cost
  - Smooth vs ReLU activation: Smooth allows quadratic convergence, ReLU is simpler but only linear

- Failure signatures:
  - GD: Divergence or extremely slow convergence if learning rate too large or λ₀ too small
  - NGD: Numerical instability if Jacobian ill-conditioned or inverse computation fails
  - Both: Poor generalization if training error goes to zero but test error remains high

- First 3 experiments:
  1. Verify Gram matrix stability by computing H(0) and H^∞ for a small synthetic dataset
  2. Compare convergence of GD with learning rates η = O(λ₀/n²) vs η = O(1/∥H^∞∥²) on a simple regression task
  3. Implement NGD and measure convergence speed vs GD on a 1D Poisson equation using PINNs

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis assumes over-parameterization (n > m), which may not hold for all PINN applications
- Gram matrix stability H(k) ≈ H^∞ is critical but not extensively validated empirically
- Quadratic convergence for smooth activations depends on high-order smoothness conditions that may be restrictive

## Confidence
- Improved GD learning rate (O(1/∥H^∞∥²)): High
- NGD O(1) learning rate: Medium
- Quadratic convergence for smooth activations: Medium

## Next Checks
1. Empirical Gram matrix stability: Track H(k) vs H^∞ throughout training for various PINN problems to verify the stability assumption holds in practice.

2. NGD numerical stability: Implement NGD with different preconditioning strategies and measure sensitivity to ill-conditioning in the Jacobian matrix across multiple test problems.

3. Smoothness requirement verification: Test convergence rates with activation functions of varying smoothness (e.g., C² vs C³) to empirically validate the quadratic convergence claim and identify the minimum smoothness threshold.