---
ver: rpa2
title: Using LLMs to Aid Annotation and Collection of Clinically-Enriched Data in
  Bipolar Disorder and Schizophrenia
arxiv_id: '2406.12687'
source_url: https://arxiv.org/abs/2406.12687
tags:
- scene
- data
- task
- sspa
- scores
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents a method for using fine-tuned small language
  models to aid annotation and data collection in psychiatric research, specifically
  for bipolar disorder and schizophrenia. The authors demonstrate that their models
  can effectively assist in administering standardized mental health instruments and
  annotating clinical variables with high accuracy.
---

# Using LLMs to Aid Annotation and Collection of Clinically-Enriched Data in Bipolar Disorder and Schizophrenia

## Quick Facts
- arXiv ID: 2406.12687
- Source URL: https://arxiv.org/abs/2406.12687
- Authors: Ankit Aich; Avery Quynh; Pamela Osseyi; Amy Pinkham; Philip Harvey; Brenda Curtis; Colin Depp; Natalie Parde
- Reference count: 15
- Primary result: Fine-tuned small language models can effectively annotate clinical variables in bipolar disorder and schizophrenia research with high accuracy

## Executive Summary
This paper presents a method for using fine-tuned small language models to aid annotation and data collection in psychiatric research, specifically for bipolar disorder and schizophrenia. The authors demonstrate that their models can effectively assist in administering standardized mental health instruments and annotating clinical variables with high accuracy. The key findings include: small models trained on clinical data achieve low error rates in predicting SSPA (Social Skills Performance Assessment) scores for five clinical variables; the models outperform commercial large language models like GPT-4 and GPT-4o in capturing domain-specific clinical nuances; and a chained model setup combining interview generation and score prediction shows results comparable to standalone models.

## Method Summary
The study uses a sequence-to-sequence approach with a pre-trained T5-base model fine-tuned on 75% of a dataset containing 644 participants with bipolar disorder, schizophrenia, and healthy controls. The model predicts both clinical variable labels and scores (e.g., "Interest = 3") rather than scores alone. For the chained pipeline, an interview generation model creates SSPA transcripts from patient input, which are then scored by a prediction model. The approach addresses challenges in mental health research including high clinician caseloads and the need for scalable data collection and annotation methods.

## Key Results
- Small models trained on clinical data achieve low error rates in predicting SSPA scores for five clinical variables: Interest, Fluency, Clarity, Focus, and Social Appropriateness
- The models outperform commercial large language models like GPT-4 and GPT-4o in capturing domain-specific clinical nuances
- A chained model setup, combining interview generation and score prediction, shows results comparable to standalone models
- The approach addresses challenges in mental health research, such as high clinician caseloads and the need for scalable data collection and annotation methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuned small language models outperform large commercial models on domain-specific clinical tasks.
- Mechanism: Small models are trained on task-specific clinical data, allowing them to capture nuanced clinical variables that large general-purpose models like GPT-4 cannot.
- Core assumption: Domain-specific fine-tuning yields better performance than general-purpose large models for niche tasks.
- Evidence anchors:
  - [abstract] "We show that small models are capable of annotation for domain-specific clinical variables, data collection for mental-health instruments, and perform better then commercial large models."
  - [section 6] "We find that for domain-specific clinical variables, GPT and GPT-4o show high errors in generating appropriate scores and show a high degree of error. We can see that when compared to our model we get statistically significant low errors showing the need for appropriate fine-tuning."
  - [corpus] Weak - corpus contains related work on social media mental health detection but lacks direct comparison of fine-tuned small models vs large models on clinical tasks.
- Break condition: If the clinical domain becomes less specialized or if large models are fine-tuned on the same clinical data, the performance advantage may diminish.

### Mechanism 2
- Claim: Chaining interview generation and score prediction models maintains accuracy while enabling end-to-end automation.
- Mechanism: A two-stage pipeline where an interview generation model creates transcripts that are then scored by a prediction model, with minimal error increase compared to standalone scoring.
- Core assumption: The output of the interview generation model is sufficiently similar to human-generated transcripts to allow accurate scoring.
- Evidence anchors:
  - [section 5] "Our first observation is the acute closeness to the stand-alone model scores... This shows that even when LLM-based assistants are adapted in a chained end-to-end fashion, the results are similar to those observed using standalone models."
  - [section 4.1] "The results indicate generally low error, with improved predictive performance in Scene 2 compared to Scene 1."
  - [corpus] Weak - corpus focuses on single-model approaches for mental health detection rather than chained pipelines.
- Break condition: If the interview generation model produces transcripts with significantly different structure or content from human interviews, the scoring accuracy will degrade.

### Mechanism 3
- Claim: Sequence-to-sequence modeling with variable labels improves interpretability and performance for clinical annotation.
- Mechanism: Instead of predicting scores alone, the model predicts both the clinical variable name and its score, creating a more structured and interpretable output format.
- Core assumption: Including variable labels in the output sequence provides better context for the model to learn the task.
- Evidence anchors:
  - [section 4] "Rather than simply predicting a sequence of scores, we also predict the label for which the score is being generated... Therefore, the model predicts a sequence Interest = XX, Fluency = YY rather than a simple distribution 4, 5, 3..."
  - [section 4.1] "Overall, this standalone model demonstrates effective prediction capabilities."
  - [corpus] Weak - corpus does not discuss sequence-to-sequence approaches with labeled outputs for mental health annotation.
- Break condition: If the additional complexity of including labels does not improve performance or makes training more difficult, a simpler approach may be preferable.

## Foundational Learning

- Concept: Sequence-to-sequence modeling
  - Why needed here: The task involves generating structured outputs (clinical scores and variable labels) from input sequences (transcripts or dialogue histories), which is naturally suited to seq2seq architectures.
  - Quick check question: What type of neural network architecture is typically used for tasks that involve generating sequences from sequences?

- Concept: Fine-tuning vs. in-context learning
  - Why needed here: The paper demonstrates that fine-tuning small models on clinical data is more effective than relying on large models' general knowledge or in-context learning capabilities.
  - Quick check question: What is the key difference between fine-tuning a model on specific data and using in-context learning with prompts?

- Concept: Evaluation metrics for generation tasks
  - Why needed here: The paper uses multiple metrics (RMSE, ROUGE, BERTScore, cosine similarity) to evaluate different aspects of model performance, requiring understanding of when each metric is appropriate.
  - Quick check question: Which metric would you use to evaluate the semantic similarity between generated text and reference text?

## Architecture Onboarding

- Component map: The system consists of two main components - an interview generation model that creates SSPA transcripts from patient input, and a score prediction model that annotates clinical variables from transcripts. These are connected in a chained pipeline for end-to-end automation.

- Critical path: For the chained model, the critical path is: patient input → interview generation model → generated transcript → score prediction model → clinical scores. Any failure in the generation model directly impacts the quality of the final scores.

- Design tradeoffs: Using smaller models (T5-base) instead of large models (GPT-4) reduces computational requirements and enables effective fine-tuning on limited clinical data, but may limit generalization to tasks outside the training domain.

- Failure signatures: If the interview generation model hallucinates information or produces transcripts that deviate significantly from human interviews, the score prediction model will show increased RMSE. If the score prediction model shows high variance across different clinical variables, it may indicate insufficient training data for certain variables.

- First 3 experiments:
  1. Evaluate the standalone score prediction model on the original dataset to establish baseline performance.
  2. Test the interview generation model with various prompt engineering approaches to optimize dialogue quality.
  3. Implement and test the chained pipeline, comparing RMSE scores to the standalone model to verify minimal performance degradation.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but based on the discussion section and limitations, several questions emerge:

1. How do hallucinations in the interview generation model affect the validity of the generated transcripts for clinical annotation purposes?
2. What is the optimal model size and architecture for this clinical annotation task, considering the trade-off between performance and computational resources?
3. How generalizable are the findings to other psychiatric disorders or clinical variables beyond the five SSPA variables studied?
4. What is the impact of incorporating multimodal data (e.g., video, audio) on the performance of the clinical annotation models?
5. How does the performance of the fine-tuned models compare to human annotators in terms of inter-rater reliability and agreement?

## Limitations
- The evaluation focuses primarily on RMSE metrics, which may not fully capture the clinical utility of the predictions
- The comparison with large models uses commercial GPT-4 variants rather than fine-tuned large models, making it unclear whether the advantage stems from model size or fine-tuning approach
- The corpus search revealed limited directly comparable work on chained pipeline approaches in mental health annotation, suggesting the evaluation framework is relatively untested

## Confidence

- High confidence: The standalone model's ability to predict clinical scores with low RMSE (target <1.2) on the SSPA dataset
- Medium confidence: The superiority of small fine-tuned models over commercial large models for domain-specific clinical tasks
- Medium confidence: The effectiveness of the chained pipeline approach

## Next Checks

1. Reproduce the standalone model using the exact T5-base fine-tuning setup and evaluate whether RMSE scores of <1.2 can be consistently achieved across all five clinical variables.

2. Compare against fine-tuned large models by taking a GPT-4 variant and fine-tuning it on the same clinical data to determine if the performance advantage is due to model size or fine-tuning approach.

3. Conduct clinical utility validation by having domain experts evaluate whether the model predictions would lead to correct clinical decisions, particularly for borderline cases where RMSE differences might mask important clinical distinctions.