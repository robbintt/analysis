---
ver: rpa2
title: Finding Lottery Tickets in Vision Models via Data-driven Spectral Foresight
  Pruning
arxiv_id: '2406.01820'
source_url: https://arxiv.org/abs/2406.01820
tags:
- pruning
- network
- training
- sparsity
- each
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Path eXclusion (PX), a pruning-at-initialization
  method that leverages the Neural Tangent Kernel (NTK) theory to identify and preserve
  parameters that mostly influence the NTK's trace. Unlike prior approaches that neglect
  the data-dependent component of the NTK spectrum, PX introduces an analytical upper
  bound on the NTK's trace by decomposing neural networks into individual paths.
---

# Finding Lottery Tickets in Vision Models via Data-driven Spectral Foresight Pruning

## Quick Facts
- **arXiv ID**: 2406.01820
- **Source URL**: https://arxiv.org/abs/2406.01820
- **Reference count**: 40
- **Primary result**: PX achieves 93.12% sparsity on CIFAR-10 with ResNet-20, matching dense model accuracy

## Executive Summary
This paper introduces Path eXclusion (PX), a pruning-at-initialization method that leverages Neural Tangent Kernel (NTK) theory to identify and preserve parameters that maximally influence the NTK's trace. Unlike prior approaches, PX incorporates data-dependent information and uses an iterative process to avoid layer collapse. The method demonstrates superior performance across various architectures and tasks, achieving high sparsity levels while maintaining accuracy comparable to dense models. PX shows particular effectiveness in both classification and semantic segmentation tasks, with the ability to extract high-performing subnetworks from pre-trained models.

## Method Summary
PX is a pruning-at-initialization method that decomposes neural networks into individual paths to construct an analytical upper bound on the NTK's trace. It uses two auxiliary networks to compute saliency scores for each parameter, considering both the Path Kernel and Path Activation Matrix. The method is iterative and assigns only positive saliency scores, preventing layer collapse. PX can be applied to pre-trained models and is effective on various architectures including ResNet and VGG. The pruning process preserves parameters that most significantly influence the NTK's training dynamics while incorporating data-dependent information for better performance.

## Key Results
- Achieves 93.12% sparsity on CIFAR-10 with ResNet-20, matching dense model accuracy
- Extracts subnetworks from pre-trained models with performance comparable to dense counterparts
- Demonstrates superior results in classification and semantic segmentation tasks at high sparsity levels
- Shows effectiveness across multiple architectures (ResNet, VGG) and datasets (CIFAR-10, CIFAR-100, Tiny-ImageNet, ImageNet)

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: PX preserves parameters that maximally influence the NTK's trace, thereby maintaining the original network's training dynamics.
- **Mechanism**: PX constructs an analytical upper bound on the NTK's trace by decomposing the network into individual paths. It then prunes parameters that minimally change this trace, retaining only those paths that most significantly contribute to the NTK's spectral properties.
- **Core assumption**: The NTK's trace and its largest eigenvalues are strong indicators of training convergence speed and predictive potential.
- **Evidence anchors**: [abstract] states PX is "designed to preserve the parameters that mostly influence the NTK's trace."
- **Break condition**: If the relationship between NTK spectral properties and training dynamics is not as strong as assumed, or if the path decomposition upper bound does not accurately reflect the true NTK trace.

### Mechanism 2
- **Claim**: PX incorporates data-dependent information into the pruning process, leading to better performance than purely data-agnostic methods.
- **Mechanism**: PX uses a data-dependent term in its upper bound on the NTK's trace. This term, represented by the Path Activation Matrix, captures how the training data influences the network's output through active paths.
- **Core assumption**: Data statistics significantly influence the relevance of network parameters, especially in architectures with components like batch normalization.
- **Evidence anchors**: [abstract] mentions PX "considers both the Path Kernel and the Path Activation Matrix."
- **Break condition**: If the data-dependent component does not significantly impact the NTK's trace or if the additional computational cost outweighs the performance benefits.

### Mechanism 3
- **Claim**: PX's iterative nature and positive saliency scores prevent layer collapse, ensuring the pruned network remains trainable.
- **Mechanism**: PX assigns only positive saliency scores to parameters, meaning that the importance of parameters is preserved across layers. Combined with its iterative pruning approach, this satisfies the hypotheses of the Theorem of Maximal Critical Compression.
- **Core assumption**: Maintaining a minimum width in each layer is crucial for the network to remain trainable.
- **Evidence anchors**: [abstract] states PX "guarantees that the network parameters have only positive scores" and that "this provides guarantees on avoiding layer collapse."
- **Break condition**: If the assumptions of the Theorem of Maximal Critical Compression are violated, or if the iterative pruning process does not adequately preserve layer widths.

## Foundational Learning

- **Concept**: Neural Tangent Kernel (NTK) theory
  - Why needed here: PX leverages NTK theory to estimate the training dynamics of the network and identify parameters that maximally influence these dynamics.
  - Quick check question: What is the relationship between the NTK and the training dynamics of a neural network, and how does this relationship change for wide networks?

- **Concept**: Path decomposition of neural networks
  - Why needed here: PX decomposes the network into individual paths to construct an analytical upper bound on the NTK's trace.
  - Quick check question: How does the path decomposition of a neural network relate to its output, and how can this decomposition be used to estimate the NTK's trace?

- **Concept**: Pruning at initialization (PaI)
  - Why needed here: PX is a PaI method, meaning it prunes the network before training.
  - Quick check question: What are the key differences between PaI methods and traditional pruning methods that prune after training, and what are the advantages and disadvantages of each approach?

## Architecture Onboarding

- **Component map**: Dense network (f) -> Auxiliary networks (g, h) -> Parameter mask (M) -> Saliency function (SPX)
- **Critical path**:
  1. Initialize the dense network (f) and create two copies (g, h)
  2. For each pruning round: Compute saliency scores, update parameter mask (M)
  3. Use final parameter mask (M) to prune the dense network
- **Design tradeoffs**:
  - Data dependence vs. data independence: PX incorporates data-dependent information, which can lead to better performance but also increases computational cost
  - Iterative vs. single-shot pruning: PX uses an iterative approach, which can be more computationally expensive but also more effective at preserving layer widths
  - Global vs. local masking: PX uses global masking, which considers the entire network when assigning saliency scores
- **Failure signatures**:
  - Layer collapse: Some layers may be completely pruned, rendering the network untrainable
  - Poor performance: The pruned network may perform poorly if the data-dependent component does not significantly impact the NTK's trace
- **First 3 experiments**:
  1. Reproduce CIFAR-10 results with ResNet-20 at 93.12% sparsity
  2. Compare PX's performance with SynFlow-L2 on CIFAR-10 with ResNet-20 at 93.12% sparsity
  3. Test PX's ability to prevent layer collapse by applying it to ResNet-50 at 98.20% sparsity

## Open Questions the Paper Calls Out

- **Open Question 1**: How does PX perform on more complex vision tasks beyond classification and segmentation, such as object detection or instance segmentation?
- **Open Question 2**: What is the impact of different initialization strategies (e.g., Xavier, orthogonal) on PX's performance compared to Kaiming normal initialization?
- **Open Question 3**: How does PX scale to extremely large models (e.g., GPT-3, ViT-Huge) in terms of both computational efficiency and effectiveness?

## Limitations

- Performance claims at extreme sparsity levels (93.12% and higher) lack comparison with other state-of-the-art PaI methods at those specific sparsity thresholds
- The ablation study comparing data-dependent vs. data-independent variants is limited to CIFAR-10 only
- The computational overhead of PX, particularly the data-dependent component, is not explicitly quantified relative to baseline methods
- The method's effectiveness on architectures beyond standard CNNs and vision transformers remains unexplored

## Confidence

- **High Confidence**: PX's core mechanism of preserving NTK trace-influencing parameters is well-supported by theoretical foundations and experimental results across multiple architectures and datasets
- **Medium Confidence**: The claim that data-dependent information significantly improves pruning decisions is supported by CIFAR-10 results but requires broader validation across datasets and architectures
- **Medium Confidence**: The assertion that PX prevents layer collapse is theoretically justified through the Theorem of Maximal Critical Compression, but empirical validation across extreme sparsity levels would strengthen this claim

## Next Checks

1. Conduct ablation studies on CIFAR-100, Tiny-ImageNet, and Pascal VOC2012 to verify the importance of the data-dependent component across diverse datasets and tasks
2. Compare PX's computational overhead with SynFlow-L2 at various sparsity levels to quantify the trade-off between performance gains and computational costs
3. Test PX's layer collapse prevention capability on ResNet-50 at 98.20% sparsity and monitor layer widths throughout pruning iterations to empirically validate the theoretical guarantees