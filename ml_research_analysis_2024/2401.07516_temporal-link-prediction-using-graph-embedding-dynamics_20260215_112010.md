---
ver: rpa2
title: Temporal Link Prediction Using Graph Embedding Dynamics
arxiv_id: '2401.07516'
source_url: https://arxiv.org/abs/2401.07516
tags:
- prediction
- link
- embedding
- temporal
- node
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study proposes a novel approach to temporal link prediction
  by modeling nodes as Newtonian objects with velocity vectors in embedding space.
  The method computes node-specific dynamics through temporal embedding and velocity
  prediction using LSTMs, rather than aggregating overall network dynamics.
---

# Temporal Link Prediction Using Graph Embedding Dynamics

## Quick Facts
- arXiv ID: 2401.07516
- Source URL: https://arxiv.org/abs/2401.07516
- Reference count: 40
- 17.34% AUROC improvement over baseline model

## Executive Summary
This paper introduces a novel temporal link prediction approach that models nodes as Newtonian objects with velocity vectors in embedding space. The method predicts future node positions by first forecasting velocity vectors using LSTM networks, then applying vertical and horizontal aggregation to compute final node locations. The approach was validated on 17 years of PubMed co-authorship data, achieving significant improvements over baseline methods while demonstrating robustness to hyperparameter changes and efficiency with limited historical data.

## Method Summary
The approach treats nodes as Newtonian objects in embedding space, computing velocity vectors from consecutive time-step embeddings. These velocities are predicted using an LSTM time-series model, with results aggregated vertically (over time) and horizontally (over one-hop neighbors). The method separates trajectory learning from static embedding similarity, allowing the system to focus on dynamic patterns rather than proximity alone. The framework was implemented using DynamicGem's AERNN for embedding generation and a 3-layer LSTM network for velocity prediction.

## Key Results
- 17.34% AUROC improvement over baseline model on PubMed co-authorship data
- Robust performance across different hyperparameter settings
- High accuracy achieved with shorter time-series and history lengths
- Efficient performance even with limited historical data availability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modeling nodes as Newtonian objects with explicit velocity vectors captures node dynamics more precisely than aggregating network-level dynamics.
- Mechanism: The approach computes the difference in node embeddings between consecutive time-steps to derive velocity vectors, then predicts future velocities using an LSTM time-series model.
- Core assumption: Velocity vectors in embedding space meaningfully encode directional trends in node behavior that are predictive of future links.
- Evidence anchors: [abstract] "incorporating the concept of velocity to predict network dynamics" and [section] "the novelty of our work is an extension of Temporal Embedding Approaches that considers the embedded nodes as Newtonian Objects characterized by their position, as well as their Velocity in the embedding space"
- Break condition: If embedding trajectories are noisy or non-directional, velocity vectors may not reliably predict future positions.

### Mechanism 2
- Claim: Vertical and horizontal aggregation layers enrich node representations by combining temporal trends with local neighborhood influence.
- Mechanism: Vertical aggregation computes a weighted sum of recent velocities to capture historical movement patterns. Horizontal aggregation averages the predicted locations of a node's one-hop neighbors.
- Core assumption: Recent temporal patterns and immediate neighborhood context together provide a better predictor of future location than either alone.
- Evidence anchors: [section] "we did a 2-dimensional aggregation for each node: first, aggregation over time-steps (vertical) and second, neighbor aggregation in the last time-step (horizontal)" and [section] "Zhang et al. [38] proved that the local network of a node provides enough information on node dynamics in order to predict its behavior"
- Break condition: If the network has sparse connectivity or rapidly changing topology, neighbor aggregation may introduce noise rather than useful signal.

### Mechanism 3
- Claim: Predicting velocities before final positions decouples trajectory learning from static embedding similarity, improving link prediction accuracy.
- Mechanism: By predicting future velocity vectors using an LSTM trained on historical velocities, the model can generate plausible future locations independent of the embedding encoder.
- Core assumption: Velocity prediction is a more stable intermediate task than direct link prediction, especially when historical data is limited.
- Evidence anchors: [section] "the goal is predicting p's velocity moving towards st+1 in the future" and "we initialize the location of nodes at time-step t + 1 as we can see in (3)" and [section] "our approach offers an interpretable layer over traditional approaches"
- Break condition: If velocity patterns are highly erratic or non-Markovian, the LSTM may fail to learn useful dynamics.

## Foundational Learning

- Concept: Newtonian mechanics applied to embedding spaces (position, velocity, trajectory prediction)
  - Why needed here: The model treats nodes as moving objects whose future positions depend on past movement direction and speed
  - Quick check question: If a node's embedding moved 0.5 units in year t-1→t and 0.8 units in t→t+1, what is the predicted velocity for t+1→t+2 assuming linear acceleration?

- Concept: Graph embedding techniques and temporal dependency modeling (e.g., AERNN, LSTM)
  - Why needed here: Embeddings encode node features; LSTMs model sequential velocity changes over time
  - Quick check question: How does the AERNN encoder handle temporal dependencies differently from a standard static graph embedding method?

- Concept: Graph-based link prediction evaluation metrics (AUROC, AUPRC)
  - Why needed here: The system outputs similarity scores used for binary classification of link existence
  - Quick check question: If the model outputs a similarity of 0.85 for a node pair that has no link, what effect does this have on AUROC vs AUPRC?

## Architecture Onboarding

- Component map:
  AERNN encoder -> embedding generator -> velocity calculator -> LSTM velocity predictor -> vertical aggregation -> horizontal aggregation -> Euclidean similarity

- Critical path:
  1. Generate yearly embeddings with AERNN
  2. Compute velocity sequences
  3. Predict next-step velocities with LSTM
  4. Aggregate vertically and horizontally
  5. Compute final positions and similarities

- Design tradeoffs:
  - Short vs long history length: shorter histories reduce computation but may miss long-term trends
  - Embedding dimension: larger dimensions capture more detail but increase computational cost
  - Neighbor aggregation: including more hops captures broader context but may dilute local dynamics

- Failure signatures:
  - Low AUROC/AUPRC despite high embedding quality → velocity prediction may be unstable
  - Model sensitive to embedding dimension → embedding space may not be well-structured
  - Performance drops with sparse networks → neighbor aggregation may introduce noise

- First 3 experiments:
  1. Verify embedding generation: run AERNN on a small synthetic dynamic graph and visualize embeddings over time
  2. Test velocity computation: compute velocities between consecutive years and check if they follow expected movement patterns
  3. Validate LSTM velocity prediction: train LSTM on synthetic velocity sequences and evaluate next-step prediction accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of the proposed approach change when considering multi-step predictions in the future (beyond t+1)?
- Basis in paper: [explicit] The paper mentions that since the method predicts a velocity curve, more than one step in the future may be predictable with high accuracy.
- Why unresolved: The current experiments only evaluate the model's performance for predicting the next time step (t+1). The accuracy and robustness of multi-step predictions remain unexplored.
- What evidence would resolve it: Experimental results comparing single-step and multi-step prediction accuracy, including error propagation analysis over multiple future time steps.

### Open Question 2
- Question: What is the impact of incorporating node acceleration alongside velocity in the prediction framework?
- Basis in paper: [explicit] The conclusion section mentions that future research will consider acceleration in addition to velocity to capture node movement dynamics more precisely.
- Why unresolved: The current model only accounts for velocity, potentially missing higher-order dynamics that could improve prediction accuracy.
- What evidence would resolve it: Comparative experiments showing prediction performance with and without acceleration, demonstrating whether the additional complexity improves results.

### Open Question 3
- Question: How does the proposed approach perform on dynamic networks with different topological structures (e.g., scale-free vs. small-world networks)?
- Basis in paper: [inferred] The experiments were conducted on co-authorship networks and a single temporal contact network dataset, but the approach's generalizability to different network structures is not evaluated.
- Why unresolved: The paper does not test the model across diverse network topologies, which is important for understanding its limitations and applicability.
- What evidence would resolve it: Experimental results across multiple datasets with varying topological properties, showing how performance scales with different network structures.

### Open Question 4
- Question: What is the computational complexity of the proposed approach compared to other state-of-the-art methods, and how does it scale with network size?
- Basis in paper: [inferred] While the paper mentions that shorter time-series and history lengths make the approach efficient, it does not provide a detailed complexity analysis or scaling behavior.
- Why unresolved: The efficiency claims are not quantified, and there is no comparison of computational requirements against competing methods.
- What evidence would resolve it: A formal complexity analysis and empirical runtime comparisons across different network sizes, showing how the approach scales relative to baselines.

## Limitations

- Performance claims based on single co-authorship dataset may not generalize to other network types
- Velocity-based approach assumes predictable movement patterns that may not hold for bursty or non-directional dynamics
- Neighbor aggregation effectiveness depends heavily on local network density and connectivity patterns

## Confidence

- **High confidence**: The velocity-based prediction mechanism is well-grounded in the methodology and supported by the mathematical formulation
- **Medium confidence**: The aggregation approach is reasonable but specific weighting schemes and their impact are not fully explored
- **Low confidence**: The generalizability of the 17.34% improvement claim to other datasets and domains remains uncertain

## Next Checks

1. **Ablation study on aggregation mechanisms**: Test the model with only vertical aggregation, only horizontal aggregation, and the full combined approach to quantify each component's contribution to performance.

2. **Cross-dataset validation**: Apply the same methodology to Hypertext2009 and other dynamic graph datasets to assess whether the velocity-based approach consistently outperforms baselines across domains.

3. **Sensitivity analysis on embedding dimensions**: Systematically vary embedding dimensions (e.g., 64, 128, 256) to determine if the reported performance is robust to this key hyperparameter or if it's optimized specifically for 128 dimensions.