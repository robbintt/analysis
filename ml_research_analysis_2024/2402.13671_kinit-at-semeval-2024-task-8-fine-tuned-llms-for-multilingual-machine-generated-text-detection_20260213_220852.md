---
ver: rpa2
title: 'KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated
  Text Detection'
arxiv_id: '2402.13671'
source_url: https://arxiv.org/abs/2402.13671
tags:
- have
- system
- detection
- used
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work addressed multilingual machine-generated text detection
  by combining fine-tuned 7B-parameter LLMs with statistical detection metrics. The
  approach used a two-step majority voting ensemble of Falcon-7B and Mistral-7B fine-tuned
  models, combined with Entropy, Rank, and Binoculars statistical metrics.
---

# KInIT at SemEval-2024 Task 8: Fine-tuned LLMs for Multilingual Machine-Generated Text Detection

## Quick Facts
- arXiv ID: 2402.13671
- Source URL: https://arxiv.org/abs/2402.13671
- Reference count: 9
- Primary result: 95% accuracy on multilingual machine-generated text detection test set

## Executive Summary
This paper presents a multilingual machine-generated text detection system for SemEval-2024 Task 8, combining fine-tuned 7B-parameter LLMs with statistical detection metrics. The approach uses Falcon-7B and Mistral-7B fine-tuned models, combined with Entropy, Rank, and Binoculars statistical metrics through a two-step majority voting ensemble. Per-language threshold calibration is applied to enhance generalization across different languages. The system achieved 95% accuracy on the test set, ranking fourth in the competition, just under 1 percentage point behind the winning system.

## Method Summary
The system employs QLoRA fine-tuning of 7B-parameter LLMs (Falcon-7B and Mistral-7B) on multilingual machine-generated text detection data. These fine-tuned models are combined with three statistical metrics (Entropy, Rank, Binoculars) through a two-step majority voting ensemble. Per-language threshold calibration is performed using ROC curve analysis on the training and development sets, with FastText language identification used to select appropriate thresholds for test predictions. The ensemble approach aims to leverage both learned model features and statistical signal patterns to improve detection accuracy across multiple languages.

## Key Results
- Achieved 95% accuracy on test set, ranking 4th in SemEval-2024 Task 8
- Mistral-7B fine-tuned alone achieved 0.97 AUC ROC, demonstrating strong performance of 7B-parameter LLMs
- Per-language threshold calibration significantly improved multilingual generalization
- Two-step majority voting ensemble combining LLMs and statistical metrics outperformed single-model approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Per-language threshold calibration improves multilingual detection accuracy.
- Mechanism: Separate thresholds are learned for each language in the training/dev set plus one for unknown languages, calibrated to maximize TPR-FPR difference on dev data.
- Core assumption: Machine-generated text patterns differ by language, so a single global threshold is suboptimal.
- Evidence anchors:
  - [abstract] "We have further used the per-language classification-threshold calibration to uniquely combine fine-tuned models predictions with statistical detection metrics to improve generalization of the system detection performance."
  - [section 3] "These thresholds are calibrated in a per-language way, meaning that separate thresholds are used for each language officially present in the train and dev sets, plus an additional threshold for unknown languages"
  - [corpus] Weak evidence - corpus shows related works on RoBERTa and mDeBERTa but no explicit mention of per-language calibration.
- Break condition: If FastText language identification confidence < 0.5, text is treated as unknown and uses the generic threshold, which may hurt accuracy for low-resource languages.

### Mechanism 2
- Claim: Ensemble of fine-tuned LLMs with statistical metrics via two-step majority voting outperforms single models.
- Mechanism: First, majority vote among Entropy, Rank, Binoculars metrics; then combine that result with Falcon-7B and Mistral-7B predictions via majority vote.
- Core assumption: Statistical metrics capture different signal than fine-tuned models, and combining both reduces false positives/negatives.
- Evidence anchors:
  - [abstract] "Our submitted method achieved competitive results, ranking at the fourth place, just under 1 percentage point behind the winner."
  - [section 3] "Our best system (see Figure 1) combines the predictions of two fine-tuned LLMs (Falcon-7B and Mistral-7B) with the selected statistical metrics (Entropy, Rank, Binoculars) by using a two-step majority voting."
  - [corpus] Weak evidence - corpus lists related submissions but no explicit description of two-step majority voting ensembles.
- Break condition: If one model is overfitted, its confident wrong predictions may dominate the majority vote and reduce overall accuracy.

### Mechanism 3
- Claim: Fine-tuning 7B-parameter LLMs (Falcon-7B, Mistral-7B) is effective for multilingual text classification.
- Mechanism: Parameter-efficient fine-tuning (QLoRA) adapts large pre-trained models to binary classification of human vs. machine-generated text.
- Core assumption: Large pre-trained LLMs capture rich multilingual features that can be leveraged with modest fine-tuning.
- Evidence anchors:
  - [abstract] "We have experienced a remarkably good performance of fine-tuned LLMs of 7B parameters in this task."
  - [section 2] "We have further combined these LLMs with statistical detectors to ensure better generalization of the system, which is described in the following sections."
  - [corpus] Weak evidence - corpus lists related submissions but no explicit evaluation of 7B-parameter LLM fine-tuning.
- Break condition: If the base model is pre-trained on too few languages (e.g., Falcon on English/French only), cross-lingual performance may degrade for low-resource languages.

## Foundational Learning

- Concept: Language identification and its confidence thresholds
  - Why needed here: Determines which per-language threshold to apply; unknown languages fallback to generic threshold
  - Quick check question: What happens when FastText confidence is below 0.5 for a text?

- Concept: Parameter-efficient fine-tuning (QLoRA) and LoRA configuration
  - Why needed here: Enables fine-tuning large LLMs with limited GPU memory while preserving model quality
  - Quick check question: What LoRA parameters were used (alpha, r, dropout)?

- Concept: Statistical detection metrics (Entropy, Rank, Binoculars) and their calculation
  - Why needed here: Provides zero-shot detection capability and complements fine-tuned model predictions
  - Quick check question: Which base model was used to compute these metrics?

## Architecture Onboarding

- Component map: Language ID (FastText) → Threshold selector → Per-language threshold lookup → Fine-tuned Falcon-7B → Prediction probabilities → Fine-tuned Mistral-7B → Prediction probabilities → Statistical metrics (Entropy, Rank, Binoculars) → Metric values → Two-step majority voting → Final binary prediction

- Critical path: Language ID → Threshold selection → All three prediction sources → Two-step majority voting → Output

- Design tradeoffs: Per-language calibration increases accuracy but adds complexity and dependency on language ID quality; ensemble improves robustness but increases inference cost

- Failure signatures: Low FastText confidence → generic threshold used → possible accuracy drop; overfitted fine-tuned model → majority vote skewed → reduced generalization

- First 3 experiments:
  1. Test accuracy impact of FastText confidence threshold (0.3, 0.5, 0.7) on dev set
  2. Compare ensemble accuracy vs. best single model on dev set
  3. Measure effect of removing one statistical metric from the ensemble on dev set accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific linguistic features or stylistic patterns do large language models fail to replicate, leading to their detectability in machine-generated text?
- Basis in paper: [inferred] The paper mentions stylometric detection as a method for differentiating human and machine writing styles, suggesting there are identifiable differences.
- Why unresolved: The paper does not explicitly analyze which specific linguistic features or patterns make machine-generated text detectable, focusing instead on ensemble methods combining fine-tuned models and statistical metrics.
- What evidence would resolve it: Detailed analysis comparing linguistic features (syntax, vocabulary, coherence, etc.) between human and machine-generated texts across multiple languages and domains.

### Open Question 2
- Question: How does the performance of ensemble systems combining fine-tuned LLMs with statistical detection methods generalize to completely unseen languages or domains not present in the training data?
- Basis in paper: [explicit] The paper mentions using FastText language identification for unknown languages and per-language threshold calibration, indicating uncertainty about performance on unseen languages.
- Why unresolved: The paper only tested on 8 known languages and mentions "surprise languages" in the test set without detailed analysis of performance on truly unseen languages or domains.
- What evidence would resolve it: Testing the system on a completely new language or domain not present in any training or development data, with comprehensive performance metrics.

### Open Question 3
- Question: What is the optimal threshold value for classifying machine-generated text when using fine-tuned LLMs, and how does this vary across different languages and domains?
- Basis in paper: [explicit] The paper mentions that optimal thresholds for fine-tuned LLMs are often set to 1.0, and explores per-language threshold calibration, but questions why this works so well.
- Why unresolved: The paper notes that using a threshold of 1.0 (only 100% confident predictions marked as machine-generated) performed surprisingly well but doesn't fully explain why this threshold is optimal or how it varies across contexts.
- What evidence would resolve it: Systematic experimentation with different threshold values across multiple languages and domains, with statistical analysis of why certain thresholds perform better than others.

## Limitations

- Limited evaluation on truly unseen languages or domains not present in training data
- No detailed analysis of which specific linguistic features make machine-generated text detectable
- Uncertainty about generalizability of 7B-parameter LLM fine-tuning approach to other tasks

## Confidence

- High Confidence: Effectiveness of per-language threshold calibration in improving multilingual detection accuracy
- Medium Confidence: Superiority of ensemble approach combining fine-tuned LLMs with statistical metrics
- Low Confidence: Generalizability of 7B-parameter LLM fine-tuning approach to other tasks or domains

## Next Checks

1. Test accuracy impact of FastText confidence threshold (0.3, 0.5, 0.7) on dev set and analyze precision-recall trade-off
2. Perform component-wise ensemble analysis by measuring accuracy when each statistical metric (Entropy, Rank, Binoculars) is removed individually
3. Evaluate model performance on languages not seen during training to assess true multilingual capabilities using external validation sets