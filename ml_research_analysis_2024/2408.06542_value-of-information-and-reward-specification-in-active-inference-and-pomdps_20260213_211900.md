---
ver: rpa2
title: Value of Information and Reward Specification in Active Inference and POMDPs
arxiv_id: '2408.06542'
source_url: https://arxiv.org/abs/2408.06542
tags:
- policy
- value
- information
- reward
- inference
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper analyzes the theoretical connection between active inference
  and reinforcement learning by comparing their respective policies in partially observable
  Markov decision processes (POMDPs). The key insight is that the expected free energy
  (EFE) objective in active inference can be reformulated as a belief MDP with open-loop
  belief dynamics and a reward function that includes an epistemic value term.
---

# Value of Information and Reward Specification in Active Inference and POMDPs

## Quick Facts
- arXiv ID: 2408.06542
- Source URL: https://arxiv.org/abs/2408.06542
- Authors: Ran Wei
- Reference count: 40
- Key outcome: EFE policy approximates Bayes optimal policy by adding epistemic value to close performance gap from open-loop belief dynamics

## Executive Summary
This paper establishes a theoretical connection between active inference and reinforcement learning by comparing their policies in partially observable Markov decision processes (POMDPs). The key insight is that the expected free energy (EFE) objective in active inference can be reformulated as a belief MDP with open-loop belief dynamics and a reward function that includes an epistemic value term. By comparing this policy to the Bayes optimal RL policy, the authors show that EFE policy approximates Bayes optimal behavior by leveraging epistemic value to compensate for the loss of information value due to open-loop dynamics. The performance gap is shown to be linear in policy advantage and quadratic in information gain, providing a quantitative understanding of when and why EFE achieves near-optimal behavior.

## Method Summary
The paper casts both active inference (via EFE) and reinforcement learning (via Bayes optimal policy) into a belief MDP framework for direct comparison. The EFE objective is reformulated as a reward function containing pragmatic value (expected log likelihood of observations) and epistemic value (information gain). The Bayes optimal policy uses closed-loop belief dynamics with standard reward maximization, while EFE uses open-loop dynamics with the augmented reward. The authors then apply the performance difference lemma to bound the performance gap between these policies, showing that the epistemic value term in EFE compensates for the information value lost due to open-loop dynamics, achieving approximation to Bayes optimal behavior.

## Key Results
- EFE policy approximates Bayes optimal RL policy by adding epistemic value to close performance gap from open-loop belief dynamics
- Performance gap between EFE and Bayes optimal policies is linear in policy advantage and quadratic in information gain
- EFE reward function is concave in beliefs due to linear pragmatic value plus concave information gain term
- The specification of EFE objective requires balancing reward with information gain via temperature parameter λ

## Why This Works (Mechanism)

### Mechanism 1
- Claim: EFE approximates Bayes optimal RL policy by adding epistemic value to close the performance gap caused by open-loop belief dynamics
- Mechanism: The open-loop belief dynamics in EFE lose the value of information compared to Bayes optimal closed-loop dynamics. The epistemic value term in EFE compensates for this loss by encouraging information-seeking actions
- Core assumption: The pragmatic value difference after observation exceeds the epistemic value loss (Assumption 5.3)
- Evidence anchors:
  - [abstract] "EFE approximates the Bayes optimal RL policy via information value"
  - [section 5.2] "The main insight of this work is that EFE closes the optimality gap between open and closed-loop policies by augmenting the reward of the open-loop policy with the epistemic value term"
- Break condition: If the environment doesn't satisfy Assumption 5.3 (pragmatic value gain < epistemic value loss), the EFE policy may be distracted by information seeking

### Mechanism 2
- Claim: The performance gap between EFE and Bayes optimal policies is linear in policy advantage and quadratic in information gain
- Mechanism: Using performance difference lemma, the gap decomposes into policy advantage (linear in horizon) and reward-model advantage (quadratic in information gain due to KL divergence bounds)
- Core assumption: Policies have bounded state-action marginal density ratio with Bayes optimal policy (C < ∞)
- Evidence anchors:
  - [section 5.2] "The performance gap of both policies are linear (w.r.t. planning horizon) in the policy advantage and quadratic in the information gain"
  - [section 4] Lemma 4.2 shows performance gap is linear in policy advantage and reward difference, quadratic in model difference
- Break condition: If the bound C is very large or information gain is extremely small, the approximation breaks down

### Mechanism 3
- Claim: EFE reward function is concave in beliefs due to linear pragmatic value plus concave information gain
- Mechanism: The epistemic value term (information gain) is concave in beliefs due to entropy concavity, making the combined reward concave and enabling better optimization
- Core assumption: Information gain is concave in beliefs (proven in Proposition 3.2)
- Evidence anchors:
  - [section 3] "The EFE reward function contains an information gain term which corresponds to epistemic value. The first term pragmatic value is defined as the expected log likelihood of the next observation... The addition of information gain, however, makes the EFE reward no longer convex"
  - [section 3] Proposition 3.2 proves EFE reward is concave in beliefs
- Break condition: If information gain were not concave (e.g., with non-standard formulations), the optimization properties would change

## Foundational Learning

- Concept: Partially Observable Markov Decision Processes (POMDPs)
  - Why needed here: The entire analysis compares EFE policy to Bayes optimal policy in POMDPs
  - Quick check question: What makes POMDPs different from regular MDPs, and why can't we just use state-based policies?

- Concept: Belief MDPs and their equivalence to POMDPs
  - Why needed here: The paper casts both EFE and Bayes optimal policies in belief MDP framework for comparison
  - Quick check question: How do belief MDPs represent POMDPs, and what's the difference between open-loop and closed-loop belief dynamics?

- Concept: Expected Free Energy (EFE) and its decomposition
  - Why needed here: EFE is the central quantity being analyzed and compared to Bayes optimal policy
  - Quick check question: What are the pragmatic and epistemic components of EFE, and how do they relate to exploration vs exploitation?

## Architecture Onboarding

- Component map: Belief state → EFE computation → Action selection → Environment interaction → Belief update → Repeat
- Critical path: Belief state → EFE computation → Action selection → Environment interaction → Belief update → Repeat
- Design tradeoffs:
  - Open-loop vs closed-loop belief dynamics: Open-loop is computationally cheaper but loses information value
  - Information gain weighting: Too much weight distracts from task rewards, too little loses exploration benefits
  - Temperature parameter λ: Controls balance between reward maximization and information gain
- Failure signatures:
  - Agent gets stuck in information-gathering loops without achieving rewards
  - Performance gap remains large despite high information gain
  - Belief entropy doesn't decrease even with observations
- First 3 experiments:
  1. Implement simple POMDP (e.g., Tiger problem) with both EFE and Bayes optimal policies, measure performance gap
  2. Vary the temperature parameter λ and observe the tradeoff between exploration and exploitation
  3. Compare open-loop EFE policy vs closed-loop belief dynamics EFE policy on information-seeking tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise relationship between the temperature parameter λ in the preference distribution and the performance gap between EFE and Bayes optimal policies?
- Basis in paper: [explicit] The paper discusses how λ interpolates between pure reward maximization (λ→∞) and distribution matching (λ→0), and mentions achieving Bayes optimal behavior requires setting λ appropriately.
- Why unresolved: The paper only provides theoretical bounds on performance gaps but doesn't specify how to practically choose λ for a given environment to minimize the gap.
- What evidence would resolve it: Empirical studies showing the relationship between λ values and actual performance gaps across different POMDP environments, along with theoretical characterization of optimal λ choices.

### Open Question 2
- Question: How does the performance of sophisticated EFE (with closed-loop belief dynamics) compare to the Bayes optimal policy in practice?
- Basis in paper: [explicit] The paper mentions that sophisticated EFE uses closed-loop belief dynamics but still includes an information gain term, breaking the exact equivalence with Bayes optimal policy.
- Why unresolved: While the paper provides theoretical bounds for vanilla EFE, it doesn't analyze the sophisticated variant's performance gap.
- What evidence would resolve it: Comparative experiments measuring the performance gap between sophisticated EFE, vanilla EFE, and Bayes optimal policies across various POMDP benchmarks.

### Open Question 3
- Question: What are the computational trade-offs between using EFE with open-loop dynamics versus closed-loop belief updates in large-scale POMDPs?
- Basis in paper: [inferred] The paper discusses how EFE with open-loop dynamics is computationally cheaper but approximates the Bayes optimal policy, while closed-loop methods are more expensive but potentially more accurate.
- Why unresolved: The paper focuses on theoretical performance bounds but doesn't address practical computational considerations.
- What evidence would resolve it: Empirical comparisons of computation time and scalability between open-loop and closed-loop implementations across POMDPs of varying complexity.

## Limitations
- The theoretical bounds rely heavily on Assumption 5.3, which may not hold in all POMDP environments
- The analysis assumes bounded policy advantage and state-action marginal density ratios, which may not be satisfied in complex environments
- The quadratic relationship between performance gap and information gain assumes relatively small information gains

## Confidence

- **High Confidence**: The mathematical derivations of performance bounds using the performance difference lemma and the decomposition of EFE into pragmatic and epistemic components are rigorously proven and mathematically sound.
- **Medium Confidence**: The claim that EFE approximates Bayes optimal policy via epistemic value addition is theoretically supported, but empirical validation across diverse POMDP environments would strengthen this claim.
- **Medium Confidence**: The recommendation to balance reward and information gain via temperature parameter λ is theoretically justified, though the optimal tuning strategy for specific environments remains heuristic.

## Next Checks

1. **Empirical validation**: Implement the EFE policy and Bayes optimal policy in standard POMDP benchmarks (Tiger problem, Light-Dark, etc.) to measure actual performance gaps and verify theoretical bounds hold empirically.
2. **Assumption relaxation analysis**: Systematically test environments where Assumption 5.3 is violated to quantify how performance degrades when epistemic value doesn't compensate for information loss.
3. **Temperature sensitivity study**: Conduct a comprehensive parameter sweep across different λ values to map the exploration-exploitation tradeoff and identify regimes where EFE policy behavior aligns with theoretical predictions.