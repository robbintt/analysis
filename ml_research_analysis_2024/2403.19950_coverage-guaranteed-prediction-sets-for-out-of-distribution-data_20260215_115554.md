---
ver: rpa2
title: Coverage-Guaranteed Prediction Sets for Out-of-Distribution Data
arxiv_id: '2403.19950'
source_url: https://arxiv.org/abs/2403.19950
tags:
- fmin
- then
- lemma
- have
- where
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of constructing valid confidence
  sets for out-of-distribution (OOD) data, where traditional split conformal prediction
  (SCP) fails due to distributional shifts between source and target domains. The
  authors propose a method based on f-divergence to bound the distributional difference
  between the target domain and the convex hull of source domains.
---

# Coverage-Guaranteed Prediction Sets for Out-of-Distribution Data

## Quick Facts
- arXiv ID: 2403.19950
- Source URL: https://arxiv.org/abs/2403.19950
- Authors: Xin Zou; Weiwei Liu
- Reference count: 40
- Key outcome: Proposes f-divergence based method that maintains marginal coverage under OOD settings where traditional SCP fails

## Executive Summary
This paper addresses the challenge of constructing valid confidence sets for out-of-distribution (OOD) data, where traditional split conformal prediction (SCP) fails due to distributional shifts between source and target domains. The authors propose a method based on f-divergence to bound the distributional difference between the target domain and the convex hull of source domains. They theoretically prove that their approach maintains marginal coverage under OOD settings and demonstrate its effectiveness through simulation experiments, showing that their method outperforms standard SCP in maintaining coverage while providing reasonable set lengths.

## Method Summary
The method constructs prediction sets by bounding the f-divergence between the target domain and the convex hull of source domains. It uses empirical distributions from source domains to estimate the worst-case quantile function that guarantees coverage for any target distribution within the f-divergence ball. The approach corrects the significance level based on the empirical distributions and the f-divergence bound to achieve the desired marginal coverage level. The nonconformity score is defined as the absolute difference between predicted and actual values, and prediction sets are constructed using the corrected quantile threshold.

## Key Results
- OOD-SCP maintains marginal coverage around 0.9 even under large distributional shifts, while standard SCP coverage drops significantly
- The method demonstrates that the f-divergence bound effectively captures the distributional difference between source and target domains
- Set lengths remain reasonable while maintaining the coverage guarantee

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: The method guarantees marginal coverage by bounding the f-divergence between the target domain and the convex hull of source domains.
- **Mechanism**: By constraining the f-divergence, the method ensures that the target distribution lies within a set of distributions close to the source domains. This allows for constructing prediction sets that maintain coverage even under distributional shifts.
- **Core assumption**: The f-divergence between the target domain and the convex hull of source domains is bounded by a predefined threshold ρ.
- **Evidence anchors**:
  - [abstract]: "We develop a method for forming confident prediction sets in the OOD setting and theoretically prove the validity of our method."
  - [section 5.1]: "We define a set of distributions T and construct prediction sets that satisfy marginal coverage when (Xn+1, Yn+1) is drawn from any target domain T ∈ T."
- **Break condition**: If the f-divergence between the target domain and the convex hull of source domains exceeds the predefined threshold ρ, the method may fail to maintain coverage.

### Mechanism 2
- **Claim**: The method corrects the prediction set to achieve the desired marginal coverage level.
- **Mechanism**: By adjusting the significance level α to α' based on the empirical distributions and the f-divergence bound, the method ensures that the prediction set maintains the desired coverage level.
- **Core assumption**: The empirical distributions of the source domains are representative of the true distributions.
- **Evidence anchors**:
  - [section 5.2]: "For arbitrary ϵ > 0, if we set t = eQ(1 − α′; ˆPf,ρ), where α′ = 1 − gf,ρ(ϵ + g−1f,ρ(1 − α)/(1 − 2Pd i=1 e−2miϵ2)), then we obtain the following marginal coverage guarantee: P(Yn+1 ∈ eC(Xn+1)) ≥ 1 − α."
  - [section 6]: "The white point represents the median, while the two endpoints of the thick line are the 0.25 quantile and the 0.75 quantile."
- **Break condition**: If the empirical distributions are not representative of the true distributions, the correction may not achieve the desired coverage level.

### Mechanism 3
- **Claim**: The method uses the worst-case quantile function to construct prediction sets that are valid for any target distribution within the f-divergence ball.
- **Mechanism**: By computing the worst-case quantile function eQ(α; Pf,ρ) = supP∈Pf,ρ Q(α; P), the method ensures that the prediction set contains the correct label with high probability for any target distribution within the f-divergence ball.
- **Core assumption**: The f-divergence ball contains all possible target distributions.
- **Evidence anchors**:
  - [section 5.1]: "The following lemma provides a proper choice of t. Lemma 3. For any unknown target distribution T ∈ T, assume that (Xn+1, Yn+1) is drawn from T. If we set t ≥ maxP∈P Q(1 − α; P), then: P(Yn+1 ∈ eC(Xn+1)) ≥ 1 − α."
  - [section 5.2]: "By Lemma 14 in the Appendix, gf,ρ(β) is non-increasing in ρ and non-decreasing in β, so gf,ρ⋆(g−1f,ρ(1 − α) − ϵ) ≥ gf,ρ(g−1f,ρ(1 − α) − ϵ)."
- **Break condition**: If the f-divergence ball does not contain all possible target distributions, the method may fail to maintain coverage for some target distributions.

## Foundational Learning

- **Concept**: Exchangeability of examples
  - **Why needed here**: The validity of Split Conformal Prediction (SCP) relies on the exchangeability of examples. In the OOD setting, this assumption is violated, leading to the failure of SCP.
  - **Quick check question**: What is the key assumption of SCP that is violated in the OOD setting?

- **Concept**: f-divergence
  - **Why needed here**: The method uses f-divergence to bound the distributional difference between the target domain and the convex hull of source domains, ensuring coverage under distributional shifts.
  - **Quick check question**: How does f-divergence help in maintaining coverage under distributional shifts?

- **Concept**: Quantile function
  - **Why needed here**: The method uses the quantile function to construct prediction sets that contain the correct label with high probability.
  - **Quick check question**: What role does the quantile function play in constructing valid prediction sets?

## Architecture Onboarding

- **Component map**: Source domains -> Empirical distributions -> f-divergence bound -> Worst-case quantile function -> Prediction set correction -> Coverage guarantee

- **Critical path**:
  1. Estimate the empirical distributions of the source domains.
  2. Compute the f-divergence between the target domain and the convex hull of source domains.
  3. Construct the prediction set using the worst-case quantile function.
  4. Correct the prediction set to achieve the desired coverage level.

- **Design tradeoffs**:
  - Choosing the f-divergence threshold ρ: A smaller ρ ensures tighter bounds but may exclude some target distributions, while a larger ρ includes more distributions but may lead to looser bounds.
  - Computational complexity: Computing the worst-case quantile function and correcting the prediction set can be computationally intensive, especially for large datasets.

- **Failure signatures**:
  - Coverage below the desired level: Indicates that the f-divergence threshold ρ is too small or the empirical distributions are not representative.
  - Overly conservative prediction sets: Indicates that the f-divergence threshold ρ is too large, leading to looser bounds.

- **First 3 experiments**:
  1. Verify the coverage of the prediction sets under different f-divergence thresholds ρ.
  2. Compare the coverage of the proposed method with standard SCP under distributional shifts.
  3. Evaluate the impact of the sample size on the coverage and length of the prediction sets.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How should the parameter ρ be selected in practice for real-world datasets when using f-divergence based OOD prediction sets?
- Basis in paper: Explicit - The paper acknowledges that the issue of choosing ρ is not resolved and states "In many fields, we face the same problem" but doesn't provide a solution.
- Why unresolved: The paper demonstrates theoretical validity under the assumption that the target distribution satisfies T ∈ T f,ρ, but doesn't address how to set ρ in practice when the true target distribution is unknown.
- What evidence would resolve it: Experimental results showing a method to select ρ (e.g., cross-validation, domain adaptation metrics) that maintains coverage guarantees on real-world OOD datasets, or theoretical bounds on how ρ selection affects coverage.

### Open Question 2
- Question: How does the performance of OOD-SCP scale with the number of source domains d?
- Basis in paper: Explicit - The paper mentions that "The problem gets worse if the source domain number d becomes larger" when discussing why simply mixing source domains and applying Cauchois et al.'s method doesn't work.
- Why unresolved: While the paper extends Cauchois et al.'s method to multiple sources, it doesn't provide empirical or theoretical analysis of how performance changes as d increases.
- What evidence would resolve it: Experiments varying d on simulated data showing coverage/length trade-offs, or theoretical analysis of computational complexity and sample complexity requirements as d increases.

### Open Question 3
- Question: What is the relationship between the nonconformity score function s(·,·) and the choice of f-divergence function f?
- Basis in paper: Explicit - The paper discusses different f-divergences (χ², TV, KL) but doesn't analyze how the choice of f relates to properties of the nonconformity score.
- Why unresolved: The paper shows examples of computing g⁻¹f,ρ for different f-divergences, but doesn't provide guidance on how to match f to the characteristics of s(·,·).
- What evidence would resolve it: Theoretical analysis showing conditions under which certain f-divergences are more appropriate for specific classes of nonconformity scores, or empirical studies comparing different f-s combinations on benchmark OOD datasets.

## Limitations
- The method relies on the assumption that f-divergence between target and source domains is bounded, which may not hold for complex real-world distributional shifts
- Computational complexity of computing worst-case quantile function and prediction set correction may be prohibitive for large-scale applications
- Limited empirical validation on real-world datasets with known distributional shifts

## Confidence
- **High Confidence**: The theoretical framework for using f-divergence to bound distributional differences is sound, and the simulation results demonstrate improved coverage over standard SCP under controlled distributional shifts.
- **Medium Confidence**: The practical applicability of the method depends on the validity of the f-divergence assumption in real-world settings, which is not thoroughly validated in the paper. The computational feasibility for large-scale problems is also uncertain.
- **Low Confidence**: The paper does not provide extensive empirical validation on real-world datasets, making it difficult to assess the method's performance in practical applications.

## Next Checks
1. **Empirical Validation on Real Data**: Test the method on real-world datasets with known distributional shifts to assess its practical effectiveness and validate the f-divergence assumption.
2. **Sensitivity Analysis**: Conduct a thorough sensitivity analysis to understand how the choice of f-divergence threshold ρ affects coverage and set length, and identify the optimal range for different types of distributional shifts.
3. **Scalability Assessment**: Evaluate the computational scalability of the method for large-scale datasets and compare its performance with alternative approaches in terms of both accuracy and computational efficiency.