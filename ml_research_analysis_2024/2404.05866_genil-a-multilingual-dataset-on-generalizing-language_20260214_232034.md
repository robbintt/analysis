---
ver: rpa2
title: 'GeniL: A Multilingual Dataset on Generalizing Language'
arxiv_id: '2404.05866'
source_url: https://arxiv.org/abs/2404.05866
tags:
- language
- generalization
- association
- languages
- stereotypes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GeniL, a multilingual dataset for detecting
  generalizations in language across 9 languages (English, Arabic, Bengali, Spanish,
  French, Hindi, Indonesian, Malay, Portuguese). The dataset contains over 50K sentences
  annotated for instances of generalization and whether they are promoting or merely
  mentioning the generalization.
---

# GeniL: A Multilingual Dataset on Generalizing Language

## Quick Facts
- arXiv ID: 2404.05866
- Source URL: https://arxiv.org/abs/2404.05866
- Authors: Aida Mostafazadeh Davani; Sagar Gubbi; Sunipa Dev; Shachi Dave; Vinodkumar Prabhakaran
- Reference count: 24
- Primary result: Introduces GeniL dataset with 50K+ sentences across 9 languages annotated for generalization detection

## Executive Summary
This paper introduces GeniL, a multilingual dataset for detecting generalizations in language across 9 languages (English, Arabic, Bengali, Spanish, French, Hindi, Indonesian, Malay, Portuguese). The dataset contains over 50K sentences annotated for instances of generalization and whether they are promoting or merely mentioning the generalization. The key findings are that only 5.9% of sentences containing identity terms and attributes are generalizing, and this varies across languages, identity groups, and attributes. The authors also develop classifiers to detect generalization in language, achieving an overall PR-AUC of 58.7. The dataset and classifiers enable more nuanced evaluation and mitigation of stereotypical language in NLP systems.

## Method Summary
The GeniL dataset is constructed by filtering sentences containing identity-attribute pairs from the mC4 corpus across 9 languages. Each sentence is annotated by three native speakers for generalization (G/NG) and type (promoting/generalizing vs mentioning/generalization). Annotations are aggregated via majority vote, and models are fine-tuned on mT5-XXL and PaLM-2-S base models with a 70/10/20 train/val/test split. The evaluation metric is PR-AUC, which is more appropriate for the imbalanced classification task.

## Key Results
- Only 5.9% of sentences containing identity terms and attributes are generalizing, varying across languages and identity groups
- Multilingual annotations significantly improve generalization detection accuracy compared to single-language training
- The dataset enables nuanced evaluation of stereotypical language, distinguishing between promoting and mentioning generalizations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Generalization detection accuracy improves significantly when using multilingual GeniL annotations versus only English annotations.
- Mechanism: Multilingual annotations capture language-specific linguistic patterns of generalization, enabling models to learn context-dependent cues that are not transferable via translation.
- Core assumption: Generalization linguistic cues are not language-agnostic; translation cannot preserve all subtle context differences.
- Evidence anchors:
  - [abstract] "We build classifiers that can detect generalization in language with an overall PR-AUC of 58.7, with varying degrees of performance across languages."
  - [section] "Training on annotated data in multiple languages (multilingual) remedies these issues and obtains the best performance across all languages."
  - [corpus] Weak: No direct corpus evidence comparing translated vs native annotations.
- Break condition: If translation quality improves to near-perfect preservation of context, the multilingual advantage may diminish.

### Mechanism 2
- Claim: Co-occurrence of identity terms and attributes is a poor proxy for generalization, with only ~5.9% of such co-occurrences being generalizing.
- Mechanism: Generalization requires specific linguistic contexts (promoting vs mentioning) that simple co-occurrence does not capture; most sentences containing stereotypes do not express generalizations.
- Core assumption: The linguistic context determines whether a stereotype is being generalized, not just the presence of stereotype-related terms.
- Evidence anchors:
  - [abstract] "We demonstrate that the likelihood of a co-occurrence being an instance of generalization is usually low, and varies across different languages, identity groups, and attributes."
  - [section] "On average 5.9% (SD = 2.4%) of our sentences across all languages are labeled as G."
  - [corpus] Strong: Table 2 shows low generalizing rates across all languages.
- Break condition: If the dataset over-represents sentences likely to contain generalizations, the co-occurrence baseline may appear artificially low.

### Mechanism 3
- Claim: Distinguishing between promoting and mentioning generalizations is critical for different use cases in stereotype mitigation.
- Mechanism: Promoting generalizations endorse and spread stereotypes, requiring intervention, while mentioning them (e.g., in neutral or critical contexts) may be acceptable for certain applications.
- Core assumption: The functional intent behind language use (to influence vs. to inform) determines whether generalization is harmful.
- Evidence anchors:
  - [abstract] "We distinguish two types of generalizations: (1) language that merely mentions the presence of a generalization... and (2) language that reinforces such a generalization..."
  - [section] "This distinction is especially important if the model creator intends to mitigate stereotyping by filtering stereotypes from training set vs. filtering outputs through safeguards."
  - [corpus] Weak: No direct corpus evidence showing effectiveness of this distinction in downstream applications.
- Break condition: If downstream models cannot reliably distinguish promoting vs mentioning, the distinction becomes less actionable.

## Foundational Learning

- Concept: Functional linguistic theory (Halliday, 1973)
  - Why needed here: Provides theoretical framework for distinguishing language that expresses ideas vs. influences people, which maps to the promoting vs mentioning distinction.
  - Quick check question: What are the two main purposes of language according to functional linguistic theory?

- Concept: Inter-annotator agreement and its interpretation
  - Why needed here: Understanding agreement levels (Kappa values) is crucial for assessing annotation quality and task difficulty across languages.
  - Quick check question: What does a Kappa value of 0.44 in Arabic annotations indicate about annotator consensus?

- Concept: Precision-Recall trade-offs in imbalanced classification
  - Why needed here: The generalization detection task is highly imbalanced (few generalizing instances), making PR-AUC a more appropriate metric than accuracy.
  - Quick check question: Why is PR-AUC preferred over accuracy for evaluating classifiers on imbalanced datasets?

## Architecture Onboarding

- Component map:
  - mC4 corpus → filter by identity-attribute pairs → annotation interface → aggregated labels
  - base multilingual model (mT5-XXL/PaLM-2-S) → fine-tuning on GeniL → evaluation on test split
  - aggregation of annotations → computation of generalization likelihood → comparison across languages/associations

- Critical path:
  1. Curate sentences containing identity-attribute pairs from mC4
  2. Annotate with 3 native speakers per sentence for G/NG and PG/MG distinction
  3. Aggregate annotations via majority vote
  4. Fine-tune multilingual model on annotated data
  5. Evaluate performance using PR-AUC

- Design tradeoffs:
  - Manual annotation vs. automated labeling: Manual ensures quality but limits scale; automated would enable larger coverage but risks noise
  - Translation augmentation vs. native annotations: Translation is cheaper but loses context; native is expensive but more accurate
  - Simple co-occurrence vs. context-aware detection: Co-occurrence is fast but inaccurate; context-aware is slower but more precise

- Failure signatures:
  - Low inter-annotator agreement suggests ambiguous task definition or cultural differences in generalization perception
  - High false positive rates indicate model over-generalizes from identity-attribute co-occurrences
  - Performance degradation on certain languages suggests insufficient training data or linguistic feature mismatch

- First 3 experiments:
  1. Train mT5-XXL on English GeniL data only, evaluate on all languages to establish baseline degradation
  2. Train mT5-XXL on English + machine-translated data for target language, compare to native annotations
  3. Train PaLM-2-S on multilingual GeniL, compare PR-AUC to mT5-XXL to assess base model impact

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different cultures perceive the line between mentioning and promoting generalizations?
- Basis in paper: [explicit] The authors distinguish between promoting and mentioning generalizations, but note that the ratio varies across languages (e.g., English has 78.72% promoting vs Arabic at 45.12%).
- Why unresolved: The paper doesn't explore the cultural or linguistic reasons behind these differences in perception.
- What evidence would resolve it: Cross-cultural studies on how speakers of different languages interpret the intent behind generalizations could shed light on these differences.

### Open Question 2
- Question: Does the quality of GeniL annotations vary based on the annotator's socio-demographic background?
- Basis in paper: [inferred] The authors acknowledge that while they balanced gender, there could be skews along other axes, and they release individual annotations for future studies.
- Why unresolved: The paper doesn't analyze the impact of annotator demographics on the annotation quality or consistency.
- What evidence would resolve it: Analyzing the annotations for patterns based on annotator demographics could reveal if certain groups are more or less likely to label sentences as generalizing.

### Open Question 3
- Question: Can the GeniL dataset be used to detect implicit generalizations that span multiple sentences?
- Basis in paper: [explicit] The authors mention that GeniL focuses on explicit generalizations within the same sentence and doesn't capture implicit ones that occur across sentences.
- Why unresolved: The paper doesn't explore methods to extend the dataset or task to detect implicit generalizations.
- What evidence would resolve it: Developing models that can analyze discourse-level context and identify implicit generalizations would address this limitation.

## Limitations

- Dataset representativeness may be biased due to filtering from mC4 corpus, potentially oversampling stereotype-relevant content
- Annotation quality varies across languages, with lower inter-annotator agreement in Bengali, Arabic, and Indonesian
- Model performance generalization to unseen contexts and low-resource languages remains unclear

## Confidence

**High Confidence**: The claim that co-occurrence of identity terms and attributes is a poor proxy for generalization (only 5.9% of such co-occurrences are generalizing) is well-supported by direct corpus evidence and aligns with theoretical understanding.

**Medium Confidence**: The claim that multilingual annotations significantly improve generalization detection accuracy is supported by experimental results but relies on assumptions about language-specific linguistic cues.

**Low Confidence**: The distinction between promoting and mentioning generalizations, while theoretically sound, lacks strong empirical validation for downstream applications.

## Next Checks

1. **Dataset Sampling Bias Validation**: Conduct a stratified random sampling of sentences from mC4 that contain identity-attribute pairs versus those that don't, then annotate a subset to determine if the 5.9% generalization rate holds outside the filtered dataset.

2. **Cross-Cultural Annotation Consistency Study**: Replicate the annotation task with culturally diverse annotator groups for a subset of sentences across multiple languages, measuring inter-annotator agreement within and across cultural groups.

3. **Generalization Transfer to Unseen Contexts**: Test the trained models on out-of-distribution sentences that contain identity-attribute pairs in novel contexts (e.g., literary language, technical documentation, social media) to evaluate whether the models overfit to the specific linguistic patterns in the GeniL dataset.