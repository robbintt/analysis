---
ver: rpa2
title: 'PEMMA: Parameter-Efficient Multi-Modal Adaptation for Medical Image Segmentation'
arxiv_id: '2404.13704'
source_url: https://arxiv.org/abs/2404.13704
tags:
- adaptation
- scans
- parameters
- only
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the problem of adapting a pre-trained transformer-based
  medical image segmentation model to efficiently incorporate additional imaging modalities
  like PET scans. The proposed method, PEMMA, leverages the modularity of transformer
  architecture and applies Low-Rank Adaptation (LoRA) to attention weights for parameter-efficient
  adaptation.
---

# PEMMA: Parameter-Efficient Multi-Modal Adaptation for Medical Image Segmentation

## Quick Facts
- arXiv ID: 2404.13704
- Source URL: https://arxiv.org/abs/2404.13704
- Authors: Nada Saadi; Numan Saeed; Mohammad Yaqub; Karthik Nandakumar
- Reference count: 15
- Key outcome: Achieves comparable performance to early fusion techniques with just 8% of trainable parameters, showing +28% improvement in average Dice score on PET scans when trained on a single modality

## Executive Summary
PEMMA addresses the challenge of efficiently adapting pre-trained transformer-based medical image segmentation models to incorporate additional imaging modalities like PET scans. The method leverages the modularity of transformer architecture by applying Low-Rank Adaptation (LoRA) to attention weights while minimizing cross-modal entanglement through architectural separation. This approach enables flexible fine-tuning with single modalities without catastrophic forgetting, achieving significant parameter efficiency (92% reduction) while maintaining segmentation accuracy.

## Method Summary
PEMMA builds on pre-trained transformer-based segmentation models by introducing PET-specific patch embedding and skip connection layers in parallel to existing CT pathways. The method applies LoRA to attention weights, updating only low-rank matrices while keeping pre-trained weights frozen. This architecture minimizes cross-modal entanglement by maintaining separate modality paths that are linearly combined, allowing the model to be fine-tuned on one modality while preserving knowledge from others. The approach achieves parameter efficiency by limiting trainable parameters to LoRA matrices and newly added PET-specific layers.

## Key Results
- Achieves comparable performance to early fusion techniques with just 8% of trainable parameters
- Shows +28% improvement in average Dice score on PET scans when trained on a single modality
- Demonstrates 92% reduction in trainable parameters while maintaining segmentation accuracy

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LoRA updates to attention weights enable parameter-efficient multimodal adaptation while keeping pre-trained weights frozen
- Mechanism: Low-rank matrices injected in parallel to query and value weight matrices capture modality-specific information without full fine-tuning
- Core assumption: Weight updates during adaptation have low intrinsic rank
- Evidence anchors: Abstract mentions leveraging LoRA for parameter-efficient adaptation; section describes LoRA injecting trainable low-rank matrices in parallel to attention layers
- Break condition: If adaptation updates have high intrinsic rank, LoRA will fail to capture necessary information

### Mechanism 2
- Claim: Separating CT and PET paths minimizes cross-modal entanglement, enabling flexible single-modality fine-tuning without catastrophic forgetting
- Mechanism: New PET-specific layers added in parallel rather than replacing existing layers, with outputs linearly combined
- Core assumption: Cross-modal entanglement can be minimized through architectural separation
- Evidence anchors: Abstract mentions minimizing cross-modal entanglement for flexible fine-tuning; section describes adding new PET patch embedding and skip layers in parallel
- Break condition: If linear combination fails to capture necessary modality interactions, accuracy will suffer

### Mechanism 3
- Claim: PEMMA achieves comparable performance to early fusion with only 8% trainable parameters through LoRA and architectural separation
- Mechanism: Combination of low-rank attention adaptation and separate modality paths efficiently incorporates PET information while maintaining CT performance
- Core assumption: Synergy between LoRA and architectural separation provides sufficient capacity with minimal parameters
- Evidence anchors: Abstract states comparable results with early fusion using 8% trainable parameters; section confirms achieving comparable results with minimal trainable parameters
- Break condition: If combined approach cannot capture necessary modality interactions, performance will fall below early fusion benchmarks

## Foundational Learning

- Concept: Low-Rank Adaptation (LoRA)
  - Why needed here: Enables parameter-efficient adaptation of large pre-trained models by decomposing weight updates into low-rank matrices
  - Quick check question: What is the key hypothesis behind LoRA that makes it parameter-efficient?

- Concept: Cross-modal entanglement
  - Why needed here: Understanding how different imaging modalities interact in neural networks is crucial for designing flexible multimodal architectures
  - Quick check question: What problem does minimizing cross-modal entanglement solve in multimodal adaptation?

- Concept: Vision Transformers (ViT) for medical image segmentation
  - Why needed here: PEMMA builds on transformer-based segmentation models like UNETR, requiring understanding of transformer processing of medical images
  - Quick check question: How does a transformer-based segmentation model convert an image into patch tokens?

## Architecture Onboarding

- Component map: Input → CT patch embedding + PET patch embedding → Transformer encoder with LoRA → CT tokens to decoder → Skip connections (CT + weighted PET) → Output segmentation

- Critical path: CT and PET inputs processed through separate patch embedding layers, combined in transformer encoder with LoRA matrices, then processed through decoder with combined skip connections

- Design tradeoffs:
  - Parameter efficiency vs. performance: Using only 8% trainable parameters may limit maximum achievable accuracy
  - Architectural complexity vs. flexibility: Separate modality paths increase complexity but enable single-modality fine-tuning
  - Knowledge preservation vs. adaptation: Freezing pre-trained weights preserves CT knowledge but may limit PET-specific adaptation

- Failure signatures:
  - Poor PET segmentation despite good CT performance
  - Catastrophic forgetting when fine-tuning on single modality
  - Unexpected degradation in both modalities when adapting to new data

- First 3 experiments:
  1. Compare baseline UNETR (CT only) performance with PEMMA (CT only) to verify knowledge preservation
  2. Test PEMMA with both modalities available to measure improvement over baseline
  3. Fine-tune PEMMA on CT only and evaluate PET performance to verify no catastrophic forgetting

## Open Questions the Paper Calls Out

- Question: How does PEMMA perform when adapting to modalities beyond CT and PET, such as MRI or ultrasound?
  - Basis in paper: [inferred] Paper mentions testing with other medical imaging types like MRI as a future direction
  - Why unresolved: Current study only evaluates CT and PET modalities
  - What evidence would resolve it: Experimental results on MRI or ultrasound data comparing PEMMA to early and late fusion approaches

- Question: What is the impact of varying the rank (r) in LoRA matrices on PEMMA's performance and parameter efficiency?
  - Basis in paper: [explicit] Paper discusses using LoRA with low-rank matrices but does not explore different rank values
  - Why unresolved: Optimal rank for LoRA matrices not investigated
  - What evidence would resolve it: Study varying rank parameter and analyzing impact on segmentation accuracy and trainable parameters

- Question: How does PEMMA handle scenarios with more than two modalities, such as integrating CT, PET, and MRI simultaneously?
  - Basis in paper: [inferred] Framework's scalability to more than two modalities not tested
  - Why unresolved: Framework's ability to handle multi-modal scenarios beyond two modalities unexplored
  - What evidence would resolve it: Experimental results showing PEMMA's performance with three or more modalities

## Limitations
- Implementation details for LoRA matrices and their integration into transformer architecture are not specified
- Optimal hyperparameters for LoRA adaptation (rank, scaling factors) are not investigated
- Experimental validation limited to specific dataset (HECKTOR) and may not generalize to other scenarios

## Confidence

- High confidence in conceptual framework and architectural innovations (PEMMA design, LoRA integration, cross-modal entanglement minimization)
- Medium confidence in parameter efficiency claims due to lack of detailed implementation specifications
- Medium confidence in performance improvements, pending broader validation across different datasets and modalities

## Next Checks
1. Reproduce PEMMA framework on a different multi-modal medical imaging dataset (e.g., MMWHS) to test generalizability and compare parameter efficiency across implementations
2. Conduct ablation studies varying LoRA rank and scaling parameters to determine optimal configurations and assess sensitivity to hyperparameter choices
3. Evaluate catastrophic forgetting explicitly by fine-tuning on one modality and measuring performance degradation on the other modality across multiple fine-tuning iterations