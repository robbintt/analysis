---
ver: rpa2
title: On the Universal Truthfulness Hyperplane Inside LLMs
arxiv_id: '2407.08582'
source_url: https://arxiv.org/abs/2407.08582
tags:
- answer
- dataset
- data
- question
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates whether a universal truthfulness hyperplane
  exists within large language models (LLMs) to distinguish correct and incorrect
  outputs. Previous works have attempted to probe hidden states for truthfulness but
  often fail to generalize to out-of-distribution data.
---

# On the Universal Truthfulness Hyperplane Inside LLMs

## Quick Facts
- arXiv ID: 2407.08582
- Source URL: https://arxiv.org/abs/2407.08582
- Reference count: 40
- Key outcome: Universal truthfulness hyperplane exists in LLMs with 70% cross-task accuracy

## Executive Summary
This paper investigates whether a universal truthfulness hyperplane exists within large language models (LLMs) to distinguish correct and incorrect outputs. Previous works have attempted to probe hidden states for truthfulness but often fail to generalize to out-of-distribution data. To address this, the authors scale up the diversity of training datasets by curating over 40 datasets across 17 task categories. They train probes using logistic regression and mass mean methods, and evaluate generalization in cross-task, cross-domain, and in-domain settings. Results show that increasing dataset diversity significantly improves probe performance across all scenarios, while the number of samples per dataset is less critical. The probe achieves around 70% cross-task accuracy, outperforming baselines and suggesting the existence of a universal truthfulness hyperplane. This finding supports the hypothesis that LLMs encode truthfulness in a generalizable way, offering promising directions for future research.

## Method Summary
The authors scale up the diversity of training datasets by curating over 40 datasets across 17 task categories to investigate the existence of a universal truthfulness hyperplane in LLMs. They train probes using logistic regression and mass mean methods on these diverse datasets and evaluate generalization in cross-task, cross-domain, and in-domain settings. The study systematically examines how dataset diversity affects probe performance, comparing scenarios with varying numbers of datasets and samples per dataset. The evaluation framework tests whether truthfulness directions discovered in one context can reliably identify correct outputs in completely different contexts.

## Key Results
- 70% cross-task accuracy achieved, outperforming baseline approaches
- Dataset diversity significantly improves probe generalization across all evaluation scenarios
- Number of samples per dataset is less critical than overall dataset diversity for probe performance

## Why This Works (Mechanism)
The paper hypothesizes that LLMs encode truthfulness information in their hidden representations in a way that can be extracted through linear probes. By increasing dataset diversity, the probe learns a more robust direction that captures fundamental patterns of truthfulness rather than dataset-specific artifacts. The mass mean method aggregates hidden states to find the truthfulness direction, while logistic regression learns a hyperplane that separates truthful from untruthful outputs. The improved generalization suggests that truthfulness information is embedded in a relatively stable and universal manner across different tasks and domains within LLMs.

## Foundational Learning

**Hidden State Representations**: LLM internal states that encode semantic information
*Why needed*: The truthfulness hyperplane operates on these representations
*Quick check*: Verify hidden states capture factual information through ablation studies

**Dataset Diversity**: Range of task categories and domains in training data
*Why needed*: Critical for learning generalizable truthfulness patterns
*Quick check*: Measure performance gains as new dataset categories are added

**Probe Methods**: Logistic regression and mass mean techniques for extracting truthfulness
*Why needed*: Different methods may capture different aspects of truthfulness encoding
*Quick check*: Compare probe performance across methods on held-out data

## Architecture Onboarding

**Component Map**: Datasets (40+ across 17 categories) -> Probe Training (logistic regression + mass mean) -> Evaluation (cross-task, cross-domain, in-domain)

**Critical Path**: Dataset curation → Probe training → Generalization evaluation

**Design Tradeoffs**: 
- Diversity vs. sample size per dataset
- Simple linear probes vs. more complex methods
- Task-specific vs. universal truthfulness directions

**Failure Signatures**: 
- Poor cross-task generalization indicates dataset bias
- Low accuracy suggests truthfulness not encoded linearly
- Performance drops in domain shifts reveal limited universality

**First 3 Experiments**:
1. Train probe on single task, test on same task (baseline)
2. Train on diverse datasets, test on held-out tasks
3. Vary number of samples per dataset while holding diversity constant

## Open Questions the Paper Calls Out
None

## Limitations
- 70% cross-task accuracy leaves significant room for error
- Focus on logistic regression and mass mean probes without exploring more sophisticated methods
- Dataset curation may contain implicit biases affecting probe performance

## Confidence

High: Dataset diversity significantly improves probe generalization
Medium: Universal truthfulness hyperplane exists with current performance metrics
Low: The truthfulness direction represents a fundamental property rather than experimental artifact

## Next Checks
1. Test probe performance on adversarial examples and counterfactual scenarios specifically designed to break truthfulness directions
2. Evaluate whether the discovered hyperplane generalizes across different LLM architectures and training regimes
3. Conduct ablation studies to determine which dataset categories contribute most to probe generalization and whether removing any would break the universality claim