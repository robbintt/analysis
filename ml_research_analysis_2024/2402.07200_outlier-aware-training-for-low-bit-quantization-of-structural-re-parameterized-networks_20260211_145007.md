---
ver: rpa2
title: Outlier-Aware Training for Low-Bit Quantization of Structural Re-Parameterized
  Networks
arxiv_id: '2402.07200'
source_url: https://arxiv.org/abs/2402.07200
tags:
- quantization
- oabn
- repvgg
- weights
- training
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors identify a quantization bottleneck in structural re-parameterized
  (SR) networks like RepVGG, caused by outliers introduced during the merging of training
  and inference structures. They propose Outlier-Aware Batch Normalization (OABN),
  which clips batch normalization parameters during training to suppress these outliers,
  making the network compatible with post-training quantization (PTQ) and quantization-aware
  training (QAT).
---

# Outlier-Aware Training for Low-Bit Quantization of Structural Re-Parameterized Networks

## Quick Facts
- arXiv ID: 2402.07200
- Source URL: https://arxiv.org/abs/2402.07200
- Reference count: 1
- The authors propose OABN and ClusterQAT to address quantization bottlenecks in SR networks like RepVGG, achieving near full-precision accuracy at 3-6 bits.

## Executive Summary
This paper addresses a critical quantization challenge in structural re-parameterized networks, particularly RepVGG, where merging training-time multi-branch structures into inference-time monolithic layers creates outlier weights that severely degrade low-bit quantization performance. The authors introduce Outlier-Aware Batch Normalization (OABN) to suppress these outliers during training and ClusterQAT, a dynamic non-uniform quantization framework that better preserves weight distributions. Their approach enables successful post-training quantization and quantization-aware training for SR networks, achieving accuracy close to full-precision models where previous methods failed.

## Method Summary
The authors identify that structural re-parameterization in networks like RepVGG creates mismatched weight distributions between training and inference phases, introducing outliers that become particularly problematic during low-bit quantization. To address this, they propose OABN, which clips batch normalization parameters during training to suppress outlier formation. Additionally, they introduce ClusterQAT, a clustering-based non-uniform quantization framework that dynamically adjusts quantization intervals during training to better preserve the weight distribution under low-bit conditions. The combination of these two techniques significantly improves quantized accuracy for RepVGG networks at 3-6 bits.

## Key Results
- OABN enables successful post-training quantization for RepVGG networks where standard methods fail
- ClusterQAT improves quantization-aware training accuracy by dynamically adjusting quantization intervals
- Combined approach achieves accuracy close to full-precision models at 3-6 bits, a significant improvement over baseline methods

## Why This Works (Mechanism)
The paper demonstrates that structural re-parameterization in SR networks like RepVGG creates weight distributions during training that differ significantly from inference, introducing outliers that become amplified during quantization. By clipping batch normalization parameters during training (OABN), these outliers are suppressed early, making the network more quantization-friendly. The ClusterQAT framework further improves performance by using clustering algorithms to determine optimal non-uniform quantization intervals that better preserve the weight distribution under low-bit constraints, rather than using uniform quantization which poorly represents skewed distributions with outliers.

## Foundational Learning

1. **Structural Re-Parameterization** - Technique that merges training-time multi-branch structures into inference-time monolithic layers for efficiency. Needed to understand why SR networks like RepVGG have different weight distributions between training and inference. Quick check: Verify that the 3×3 and 1×1 convolution weights are properly merged during re-parameterization.

2. **Batch Normalization Clipping** - Process of limiting BN parameter values during training. Needed to understand how OABN suppresses outlier formation. Quick check: Confirm that the clipping operation doesn't overly constrain the learning process.

3. **Non-uniform Quantization** - Quantization method that uses variable interval sizes rather than fixed intervals. Needed to understand how ClusterQAT better preserves weight distributions. Quick check: Verify that the clustering algorithm properly identifies optimal quantization intervals.

## Architecture Onboarding

**Component Map**: Training Structure -> Structural Re-parameterization -> Inference Structure -> Quantization (PTQ/QAT) -> OABN/ClusterQAT Enhancement

**Critical Path**: The critical path involves the merging of multi-branch structures during re-parameterization, which creates the outlier issue that OABN and ClusterQAT address. The quantization stage is where the improvements from these methods become most apparent.

**Design Tradeoffs**: The main tradeoff is between accuracy preservation and quantization efficiency. OABN adds training-time clipping overhead but improves post-training quantization capability. ClusterQAT increases training complexity with dynamic interval adjustment but achieves better accuracy at low bit-widths.

**Failure Signatures**: Without OABN, SR networks show significant accuracy degradation during PTQ, particularly at lower bit-widths. Without ClusterQAT, QAT may fail to fully recover accuracy due to poor representation of outlier-heavy weight distributions with uniform quantization.

**First Experiments**:
1. Verify outlier formation in standard RepVGG during structural re-parameterization by comparing training vs inference weight distributions
2. Test OABN clipping parameters to find optimal balance between outlier suppression and learning capability
3. Compare uniform vs non-uniform quantization performance on outlier-heavy weight distributions to validate ClusterQAT approach

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- The approach's generalizability beyond RepVGG to other SR architectures remains unclear
- Effectiveness at extremely low bit-widths (below 3 bits) is not thoroughly explored
- Computational overhead from dynamic interval adjustment in ClusterQAT during training is not fully characterized

## Confidence

**High confidence**: Identification of outliers as quantization bottleneck in SR networks, basic premise of mismatched weight distributions, general improvement in quantized accuracy with OABN+PTQ/QAT

**Medium confidence**: Specific OABN design choices (parameter clipping during training), ClusterQAT's dynamic interval adjustment mechanism, accuracy improvements across different bit-widths

**Low confidence**: Absolute numerical improvements reported, comparative advantages over all baseline methods, long-term stability across different datasets and tasks

## Next Checks

1. Test OABN and ClusterQAT on diverse SR architectures beyond RepVGG (e.g., ResNet variants with structural re-parameterization) to verify generalizability across the SR network family.

2. Evaluate computational overhead of ClusterQAT by measuring additional training time and memory usage compared to standard QAT, and assess whether accuracy gains justify increased resource requirements.

3. Conduct experiments at ultra-low bit-widths (1-2 bits) to determine practical limits of the proposed methods and identify scenarios where approach may fail or require modification.