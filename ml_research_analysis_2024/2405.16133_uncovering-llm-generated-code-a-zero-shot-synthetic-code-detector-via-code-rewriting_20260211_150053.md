---
ver: rpa2
title: 'Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via Code
  Rewriting'
arxiv_id: '2405.16133'
source_url: https://arxiv.org/abs/2405.16133
tags:
- code
- synthetic
- detection
- similarity
- llms
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of detecting code generated
  by Large Language Models (LLMs) by proposing a novel zero-shot synthetic code detector
  based on code rewriting and similarity measurement. The method leverages the observation
  that differences between LLM-rewritten and original code tend to be smaller when
  the original code is synthetic.
---

# Uncovering LLM-Generated Code: A Zero-Shot Synthetic Code Detector via Code Rewriting

## Quick Facts
- arXiv ID: 2405.16133
- Source URL: https://arxiv.org/abs/2405.16133
- Reference count: 8
- Primary result: Novel zero-shot synthetic code detector achieves 20.5% and 29.1% improvements in AUROC scores on APPS and MBPP benchmarks respectively

## Executive Summary
This paper introduces a novel zero-shot approach for detecting Large Language Model (LLM)-generated code by leveraging code rewriting and similarity measurement. The method is based on the observation that when LLM-rewritten code is compared to original code, the differences tend to be smaller when the original code is synthetic. The approach involves rewriting input code using an LLM, measuring similarity between original and rewritten versions using contrastive learning, and estimating expected similarity through multiple rewrites. Experimental results demonstrate significant improvements over existing synthetic content detectors on two major benchmarks.

## Method Summary
The proposed approach works by first rewriting the input code using an LLM to generate multiple versions of the rewritten code. It then measures the similarity between the original code and each rewritten version using a self-supervised contrastive learning model. By analyzing the distribution of similarity scores across multiple rewrites, the method estimates whether the original code was likely generated by an LLM. The key insight is that synthetic code tends to have smaller differences when rewritten compared to human-written code, creating a distinctive pattern that can be detected without requiring labeled training data.

## Key Results
- Achieved 20.5% improvement in AUROC scores on the APPS benchmark compared to existing state-of-the-art synthetic content detectors
- Achieved 29.1% improvement in AUROC scores on the MBPP benchmark compared to existing methods
- Demonstrated effective zero-shot detection capability without requiring labeled training data
- Showed consistent performance improvements across both benchmark datasets

## Why This Works (Mechanism)
The method exploits the inherent characteristics of LLM-generated code and how it responds to rewriting. When an LLM rewrites code that it originally generated, the changes tend to be minimal because the rewritten version closely matches the original synthetic structure and patterns. In contrast, when an LLM rewrites human-written code, the differences are typically more substantial due to the distinct coding styles and patterns between human and machine generation. By measuring these differences through similarity metrics and analyzing the distribution across multiple rewrites, the approach can effectively distinguish between synthetic and human-written code.

## Foundational Learning
- **Code Rewriting**: The process of generating modified versions of existing code using LLMs; needed to create comparative samples for similarity measurement; quick check: verify LLM can successfully rewrite diverse code samples
- **Similarity Measurement**: Using contrastive learning models to quantify code similarity; needed to establish a numerical basis for comparison; quick check: ensure similarity scores are consistent across repeated measurements
- **Zero-Shot Detection**: Identifying synthetic content without requiring labeled training data; needed to enable practical deployment without extensive dataset preparation; quick check: validate detection performance on unseen code samples
- **Contrastive Learning**: Self-supervised learning approach for measuring semantic similarity; needed to capture meaningful code similarities beyond surface-level syntax; quick check: confirm contrastive model generalizes across different programming languages
- **Multiple Rewrite Sampling**: Generating several rewritten versions to estimate similarity distributions; needed to account for variability in rewriting and improve detection reliability; quick check: verify statistical significance of similarity score distributions
- **AUROC Evaluation**: Area Under the Receiver Operating Characteristic curve for performance measurement; needed to provide standardized comparison with existing methods; quick check: ensure proper calculation and interpretation of AUROC scores

## Architecture Onboarding
- **Component Map**: Input Code -> LLM Rewriting Engine -> Multiple Rewritten Codes -> Contrastive Similarity Model -> Similarity Score Distribution -> Detection Decision
- **Critical Path**: The sequence from code input through rewriting to similarity measurement and final detection decision forms the essential processing pipeline
- **Design Tradeoffs**: Balances detection accuracy against computational cost of multiple rewrites; chooses zero-shot approach over supervised learning to avoid labeled data requirements
- **Failure Signatures**: Poor performance on code from domains not represented in training data; degraded accuracy when faced with sophisticated adversarial generation techniques; computational inefficiency in resource-constrained environments
- **First Experiments**:
  1. Test detection accuracy on code samples from diverse programming domains (embedded systems, scientific computing, web development)
  2. Evaluate robustness against adversarial code generation techniques designed to evade detection
  3. Measure computational efficiency and resource requirements compared to existing detectors

## Open Questions the Paper Calls Out
None

## Limitations
- Generalizability to diverse code generation scenarios beyond APPS and MBPP benchmarks remains uncertain
- Dependence on access to capable LLMs for rewriting may limit deployment in resource-constrained environments
- Effectiveness against sophisticated adversarial code generation techniques that specifically target rewriting-based detection is unclear
- Computational overhead from multiple rewrites may impact real-world deployment efficiency

## Confidence
- **High confidence**: Experimental results showing improved AUROC scores on APPS and MBPP benchmarks are well-documented and methodologically sound
- **Medium confidence**: Theoretical foundation that LLM-rewritten code exhibits smaller differences with original synthetic code is intuitively reasonable but lacks extensive empirical validation
- **Medium confidence**: Practical utility in real-world deployment scenarios, particularly regarding computational efficiency and adaptability to different programming domains

## Next Checks
1. Test the detector's performance on code samples from diverse programming domains (e.g., embedded systems, scientific computing, web development) not represented in the current benchmarks
2. Evaluate the method's robustness against adversarial code generation techniques specifically designed to evade rewriting-based detection
3. Conduct a comprehensive computational efficiency analysis comparing the proposed method's runtime and resource requirements against existing detectors in real-world deployment scenarios