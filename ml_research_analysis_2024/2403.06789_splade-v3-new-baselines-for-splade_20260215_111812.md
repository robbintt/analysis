---
ver: rpa2
title: 'SPLADE-v3: New baselines for SPLADE'
arxiv_id: '2403.06789'
source_url: https://arxiv.org/abs/2403.06789
tags:
- splade
- splade-v3
- query
- sets
- retrieval
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This technical report introduces SPLADE-v3, a new series of sparse
  neural retrieval models that push the state-of-the-art in effectiveness. The improvements
  stem from better training procedures including using multiple hard negatives per
  batch (100 negatives), distilling from ensembles of cross-encoder re-rankers instead
  of single models, combining both KL-Div and MarginMSE distillation losses, and fine-tuning
  from SPLADE++SelfDistil checkpoints.
---

# SPLADE-v3: New baselines for SPLADE

## Quick Facts
- arXiv ID: 2403.06789
- Source URL: https://arxiv.org/abs/2403.06789
- Reference count: 26
- SPLADE-v3 achieves 40.2 MRR@10 on MS MARCO dev set with significant gains over both BM25 and SPLADE++ across 44 query sets

## Executive Summary
This technical report introduces SPLADE-v3, a new series of sparse neural retrieval models that push the state-of-the-art in effectiveness. The improvements stem from better training procedures including using multiple hard negatives per batch (100 negatives), distilling from ensembles of cross-encoder re-rankers instead of single models, combining both KL-Div and MarginMSE distillation losses, and fine-tuning from SPLADE++SelfDistil checkpoints. SPLADE-v3 achieves 40.2 MRR@10 on MS MARCO dev set, a 2% improvement on BEIR out-of-domain results, and statistically significant gains over both BM25 and SPLADE++ across 44 query sets. The models are also compared to re-rankers, showing competitive performance. Four variants are released: SPLADE-v3, SPLADE-v3-DistilBERT, SPLADE-v3-Lexical, and SPLADE-v3-Doc, each offering different efficiency-effectiveness trade-offs.

## Method Summary
SPLADE-v3 improves sparse neural retrieval through a combination of training innovations. The models are initialized from SPLADE++SelfDistil checkpoints and trained using KL-Div and MarginMSE distillation losses combined with specific weights (λKL=1, λMSE=0.05). A key innovation is the use of 100 hard negatives per batch (50 from top-50 and 50 from top-1k SPLADE++), which forces the model to distinguish between closely related but non-relevant items. Instead of single-model distillation, SPLADE-v3 uses an ensemble of 5 cross-encoder re-rankers to generate richer supervision signals. The models are evaluated on MS MARCO passages, TREC Deep Learning tasks, BEIR, and several other benchmarks using MRR@10, nDCG@10, and FLOPS efficiency measures.

## Key Results
- SPLADE-v3 achieves 40.2 MRR@10 on MS MARCO dev set
- Statistically significant gains over both BM25 and SPLADE++ across 44 query sets
- 2% improvement on BEIR out-of-domain results
- Competitive performance compared to re-rankers despite being a sparse retrieval model

## Why This Works (Mechanism)

### Mechanism 1: Multiple Hard Negatives
Using multiple hard negatives per batch (100 negatives) improves in-domain effectiveness by forcing the model to distinguish between closely related but non-relevant items. The model receives 100 hard negatives per query in a batch (50 from top-50 and 50 from top-1k SPLADE++), increasing the difficulty of the contrastive learning task and sharpening the embedding space. Core assumption: More negatives lead to better discrimination between relevant and non-relevant items.

### Mechanism 2: Ensemble Distillation
Distillation from an ensemble of cross-encoder re-rankers (rather than a single model) provides richer supervision signals and leads to better sparse representations. The model is trained using scores from 5 different cross-encoder re-rankers (MiniLM, TREC DL 2022 models) that generate both raw ensemble and "rescored" scores via affine transformation, creating a more robust distillation target. Core assumption: Ensemble scores capture more diverse and reliable relevance judgments than single-model scores.

### Mechanism 3: Combined Distillation Losses
Combining KL-Div and MarginMSE distillation losses with tuned weights (λKL=1, λMSE=0.05) balances precision and recall, leading to better overall effectiveness. KL-Div loss emphasizes precision (top-ranked relevance), while MarginMSE loss emphasizes recall (coverage of relevant items). Combining them creates a balanced objective. Core assumption: Different distillation losses capture complementary aspects of ranking quality.

## Foundational Learning

- Concept: Contrastive learning with hard negatives
  - Why needed here: The model needs to learn fine-grained distinctions between similar but non-relevant items; hard negatives make this distinction harder and thus more informative.
  - Quick check question: Why might using random negatives instead of hard negatives hurt effectiveness?

- Concept: Knowledge distillation from ensemble teachers
  - Why needed here: Single teacher models may have biases or blind spots; an ensemble provides more diverse and robust supervision signals.
  - Quick check question: What could go wrong if the ensemble teachers disagree significantly on relevance scores?

- Concept: Multi-loss optimization balancing precision and recall
  - Why needed here: Different loss functions emphasize different ranking objectives; combining them aims for a balanced retrieval effectiveness.
  - Quick check question: How would you decide the relative weights between KL-Div and MarginMSE?

## Architecture Onboarding

- Component map: Query and document texts -> SPLADE model (BERT-based) generating sparse representations -> Cross-encoder re-rankers producing relevance scores -> Combined KL-Div and MarginMSE loss over hard negatives -> Sparse term weights for retrieval

- Critical path: 1. Encode query and documents to sparse vectors, 2. Retrieve hard negatives (top-50 + random from top-1k), 3. Generate distillation scores from ensemble re-rankers, 4. Compute combined KL-Div and MarginMSE loss, 5. Update model parameters via backpropagation

- Design tradeoffs:
  - Hard negatives vs random negatives: Hard negatives improve discrimination but may introduce noise if not truly relevant.
  - Ensemble re-rankers vs single re-ranker: Ensembles are more robust but require more computation and coordination.
  - Combined losses vs single loss: Combined losses balance objectives but require careful tuning of weights.

- Failure signatures:
  - Model overfits to training negatives: Check if performance degrades on out-of-domain datasets.
  - Distillation scores are poorly calibrated: Check if re-rankers disagree widely or have skewed distributions.
  - Loss weights are suboptimal: Check if one loss dominates training dynamics.

- First 3 experiments:
  1. Train SPLADE-v3 with 10 negatives instead of 100 to see if diminishing returns exist.
  2. Replace ensemble distillation with a single strong re-ranker to test if ensemble benefit is real.
  3. Use only KL-Div loss (λMSE=0) and only MarginMSE loss (λKL=0) to confirm complementary effects.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Why does SPLADE-v3-Doc struggle so significantly in zero-shot settings despite being competitive with dense bi-encoders?
- Basis in paper: The paper notes that SPLADE-v3-Doc is "the less effective approach overall, especially in 'zero-shot', showing that (even) a minimal amount of computation on the query side is important."
- Why unresolved: The paper doesn't investigate the specific reasons behind this poor zero-shot performance or explore whether different architectural modifications could improve it.
- What evidence would resolve it: Detailed analysis of the zero-shot performance across different datasets, comparison with other zero-shot retrieval methods, and ablation studies to identify which components of the query processing contribute most to effectiveness.

### Open Question 2
- Question: What causes the curriculum learning effect observed when fine-tuning from SPLADE++SelfDistil checkpoints?
- Basis in paper: The paper mentions "we are still not sure about the cause(s) of this effect, but we believe that a sort of curriculum learning... could happen and lead to the observed improvements, but it still needs to be better investigated."
- Why unresolved: The paper acknowledges this as a hypothesis but doesn't provide empirical evidence or theoretical explanation for why this curriculum learning might occur.
- What evidence would resolve it: Controlled experiments comparing training from different initialization points, analysis of the learning dynamics during training, and investigation of whether the effect persists with different model architectures.

### Open Question 3
- Question: Why does the affine transformation of distillation scores (to match statistics of previous distillation settings) improve MarginMSE distillation but not necessarily KL-Div?
- Basis in paper: The paper notes "we notice empirically that changing the distribution helps when using distillation – especially in the case of MarginMSE... but we didn't investigate further into why this happens."
- Why unresolved: The paper observes this empirical phenomenon but doesn't provide theoretical justification or explore the interaction between score distributions and different distillation losses.
- What evidence would resolve it: Theoretical analysis of how score distributions affect different distillation losses, controlled experiments with varying score statistics, and investigation of whether this effect generalizes to other distillation approaches.

## Limitations
- The paper relies on unpublished internal experiments for several claims, particularly around the ensemble distillation setup
- Limited external validation with only MS MARCO and BEIR datasets reported
- Absence of detailed ablations showing the individual contribution of each improvement
- Comparison to re-rankers conflates different evaluation paradigms (sparse retrieval vs re-ranking)

## Confidence
- Claims about effectiveness improvements over SPLADE++: **Medium**
- Claims about ensemble distillation superiority: **Medium**
- Claims about the 100 negatives mechanism: **Medium**
- Claims about loss combination benefits: **Medium**

## Next Checks
1. Conduct ablation studies removing each of the three main innovations (100 negatives, ensemble distillation, loss combination) to quantify individual contributions
2. Perform statistical significance testing across all reported datasets to verify the claimed improvements are not due to chance
3. Compare SPLADE-v3 against modern dense retrievers (e.g., ANCE, Contriever) to establish its position in the broader retrieval landscape