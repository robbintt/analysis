---
ver: rpa2
title: How do Large Language Models Handle Multilingualism?
arxiv_id: '2402.18815'
source_url: https://arxiv.org/abs/2402.18815
tags:
- neurons
- language-specific
- multilingual
- language
- languages
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study investigates how large language models (LLMs) handle
  multilingualism. The authors hypothesize that LLMs follow a three-stage workflow:
  understanding multilingual queries by converting them to English, reasoning in English
  while incorporating multilingual knowledge, and generating responses in the original
  language.'
---

# How do Large Language Models Handle Multilingualism?

## Quick Facts
- arXiv ID: 2402.18815
- Source URL: https://arxiv.org/abs/2402.18815
- Reference count: 16
- Primary result: Proposes PLND method to identify language-specific neurons and validate three-stage multilingual workflow in LLMs

## Executive Summary
This paper investigates how large language models (LLMs) process multilingual inputs through a three-stage workflow: understanding (converting to English), reasoning (task-solving in English while incorporating multilingual knowledge), and generation (producing output in original language). The authors introduce Parallel Language-specific Neuron Detection (PLND), a novel method to identify activated neurons for different languages without labeled data. By selectively deactivating these neurons, they demonstrate that LLMs indeed follow this hypothesized workflow and show that fine-tuning language-specific neurons with small datasets can significantly enhance multilingual capabilities.

## Method Summary
The authors propose PLND, which identifies language-specific neurons by analyzing hidden embeddings when neurons are activated versus deactivated using a corpus of text in the target language. They validate their three-stage workflow hypothesis by systematically deactivating language-specific neurons in different layers and measuring performance impacts. For fine-tuning experiments, they adjust only the identified language-specific neurons with small datasets (400 documents) and evaluate improvements across multiple tasks and languages. The approach is tested on LLaMA and PaLM-2 models across various high-resource and low-resource languages.

## Key Results
- Deactivating language-specific neurons in specific layers significantly reduces performance in non-English languages while minimally affecting English performance
- Fine-tuning language-specific neurons with just 400 documents improves multilingual capabilities by 3.6% for high-resource languages and 2.3% for low-resource languages
- Language-specific neurons comprise only around 0.1% of all parameters, enabling efficient multilingual enhancement

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs convert multilingual queries into English for reasoning and then generate responses in the original language
- Mechanism: The model processes input through a three-stage workflow: understanding (converting to English), task-solving (reasoning in English while incorporating multilingual knowledge), and generation (producing output in original language)
- Core assumption: Language-specific neurons exist and can be isolated without labeled data
- Evidence anchors:
  - [abstract] "LLMs initially understand the query, converting multilingual inputs into English for task-solving."
  - [section] "We observe that non-English queries initially generate non-English embeddings... as queries progress through the middle layers, the representations surprisingly become English-centric."
  - [corpus] Strong - the paper provides experimental evidence showing performance drops when language-specific neurons are deactivated in specific layers
- Break condition: If language-specific neurons cannot be reliably detected or if the model architecture changes significantly (e.g., different attention mechanisms), this mechanism may fail

### Mechanism 2
- Claim: Language-specific neurons can be detected using Parallel Language-specific Neuron Detection (PLND) without labeled data
- Mechanism: PLND measures the importance of individual neurons by observing the difference in hidden embeddings when neurons are activated versus deactivated, using a corpus of text in the target language
- Core assumption: Neurons that are consistently activated across multiple inputs in a language are language-specific
- Evidence anchors:
  - [abstract] "We introduce Parallel Language-specific Neuron Detection (PLND) to identify activated neurons for inputs in different languages without any labeled data."
  - [section] "Using PLND, we identify language-specific neurons by inputting a free text corpus of that language and isolating consistently activated neurons."
  - [corpus] Strong - the paper demonstrates that deactivating these detected neurons significantly reduces performance in the target language
- Break condition: If the neuron activation patterns are not consistent across inputs or if the model uses different architectures where neuron importance is not easily measurable

### Mechanism 3
- Claim: Fine-tuning language-specific neurons with a small dataset enhances multilingual capabilities without compromising other languages
- Mechanism: By adjusting only the language-specific neurons for a particular language, the model improves performance in that language while maintaining performance in others
- Core assumption: Language-specific neurons are largely independent and adjusting them for one language doesn't significantly affect other languages
- Evidence anchors:
  - [abstract] "fine-tuning language-specific neurons with a small dataset, enhancing multilingual abilities in a specific language without compromising others."
  - [section] "With language-specific neurons comprising only around 0.1% of all parameters, the need for training documents to improve multilingual capabilities can be significantly reduced to just a few hundred."
  - [corpus] Strong - the paper reports average improvements of 3.6% for high-resource languages and 2.3% for low-resource languages with just 400 documents
- Break condition: If language-specific neurons are not as independent as assumed or if the model's architecture leads to interference between languages during fine-tuning

## Foundational Learning

- Concept: Neuron importance measurement
  - Why needed here: To identify which neurons are responsible for handling specific languages without labeled data
  - Quick check question: How does the importance of a neuron get quantified in PLND?

- Concept: Transformer architecture layers
  - Why needed here: Understanding how different layers (understanding, task-solving, generation) contribute to multilingual processing
  - Quick check question: What is the role of self-attention versus feed-forward structures in multilingual reasoning?

- Concept: Language detection and tokenization
  - Why needed here: To analyze the language of tokens at different layers and observe the shift from multilingual to English-centric representations
  - Quick check question: How does the model's vocabulary handle tokens from different languages?

## Architecture Onboarding

- Component map: PLND algorithm for detecting language-specific neurons -> Layer-wise analysis of neuron activation patterns -> Fine-tuning pipeline for language-specific enhancement

- Critical path:
  1. Run PLND to identify language-specific neurons
  2. Validate by deactivating neurons and observing performance drops
  3. Fine-tune identified neurons with small dataset
  4. Measure performance improvements

- Design tradeoffs:
  - Using a small dataset for fine-tuning vs. comprehensive multilingual training
  - Focusing on specific languages vs. general multilingual improvement
  - Computational cost of PLND vs. traditional fine-tuning methods

- Failure signatures:
  - No significant performance drop when deactivating language-specific neurons
  - Overlap between language-specific neurons across languages is too high
  - Fine-tuning causes degradation in other languages

- First 3 experiments:
  1. Run PLND on a multilingual model and visualize language-specific neuron distribution across layers
  2. Deactivate language-specific neurons in different layers and measure performance impact on various tasks
  3. Fine-tune language-specific neurons for a low-resource language and evaluate improvements across all tasks

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of MWork enhancement scale with corpus size beyond 800 documents, and what is the optimal trade-off between performance gains and computational costs?
- Basis in paper: [explicit] The paper notes that fine-tuning with 800 documents leads to performance deterioration compared to 400 documents, attributed to overfitting and knowledge distribution disruption
- Why unresolved: The study only explores corpus sizes up to 800 documents. Larger corpus sizes and their impact on performance and computational efficiency remain untested
- What evidence would resolve it: Experiments with varying corpus sizes (e.g., 1000, 1500, 2000) to identify the point of diminishing returns and optimal performance

### Open Question 2
- Question: To what extent does the degree of overlap among language-specific neurons correlate with the linguistic similarity of languages, and how does this impact the effectiveness of MWork?
- Basis in paper: [explicit] The paper observes that languages within the same family (e.g., Spanish, French, English) demonstrate more overlap in language-specific neurons compared to languages from different families
- Why unresolved: While the paper identifies overlap patterns, it does not quantify the relationship between overlap degree and linguistic similarity, nor does it explore how this affects MWork's performance
- What evidence would resolve it: A systematic analysis of overlap ratios across diverse language pairs and their corresponding MWork performance metrics

### Open Question 3
- Question: How does MWork perform on low-resource languages with non-Latin scripts, and what additional challenges arise in these cases?
- Basis in paper: [inferred] The paper tests MWork on four low-resource languages (Vietnamese, Thai, Arabic, Swahili) but does not explicitly address the challenges posed by non-Latin scripts
- Why unresolved: The paper does not discuss the unique challenges of handling non-Latin scripts, such as tokenization and embedding issues, which could impact MWork's effectiveness
- What evidence would resolve it: Detailed experiments on low-resource languages with non-Latin scripts, focusing on tokenization accuracy and embedding quality

### Open Question 4
- Question: What is the impact of fine-tuning language-specific neurons on the model's ability to handle code-switching or mixed-language inputs?
- Basis in paper: [inferred] The paper focuses on enhancing monolingual capabilities but does not address the model's performance on mixed-language inputs
- Why unresolved: The study does not explore how fine-tuning affects the model's ability to process code-switching, which is a common real-world scenario
- What evidence would resolve it: Experiments evaluating MWork's performance on code-switching tasks before and after fine-tuning

## Limitations

- Findings are based on decoder-only models (LLaMA, PaLM-2) and may not generalize to encoder-decoder architectures
- PLND relies on consistent neuron activation patterns which may not hold for all languages or model architectures
- Long-term stability and robustness of fine-tuning improvements require further validation

## Confidence

- High confidence in the three-stage workflow hypothesis and PLND method's ability to identify language-specific neurons
- Medium confidence in the independence of language-specific neurons during fine-tuning
- Medium confidence in the generalizability across different model architectures

## Next Checks

1. Test PLND and the three-stage workflow hypothesis on encoder-decoder models (like mBERT or mT5) to assess architectural generalizability
2. Conduct long-term stability tests by measuring performance retention after fine-tuning language-specific neurons over extended periods and with varying dataset sizes
3. Investigate interference effects by fine-tuning neurons for multiple languages sequentially and measuring cross-language performance degradation