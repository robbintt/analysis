---
ver: rpa2
title: Explaining Predictions by Characteristic Rules
arxiv_id: '2405.21003'
source_url: https://arxiv.org/abs/2405.21003
tags:
- rules
- characteristic
- cega
- rule
- local
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CEGA, a method that aggregates local explanations
  into general characteristic rules using association rule mining. Unlike previous
  approaches that focus on discriminative rules, CEGA generates characteristic rules
  that capture common properties of instances within a class.
---

# Explaining Predictions by Characteristic Rules

## Quick Facts
- arXiv ID: 2405.21003
- Source URL: https://arxiv.org/abs/2405.21003
- Reference count: 34
- The paper introduces CEGA, a method that aggregates local explanations into general characteristic rules using association rule mining.

## Executive Summary
The paper presents CEGA, a novel approach for generating characteristic rules that explain predictions by capturing common properties of instances within a class. Unlike traditional discriminative rules that focus on differences between classes, CEGA leverages local explanation techniques (SHAP, LIME, Anchors) and transforms them into itemsets for association rule mining. This enables the extraction of general, interpretable rules that maintain high fidelity while being more concise than existing methods.

## Method Summary
CEGA operates by first collecting local explanations for individual instances using any local explanation technique. These explanations are converted into itemsets where each item represents a feature-value pair and its contribution to the prediction. Association rule mining is then applied to these itemsets to extract characteristic rules that are both general (covering multiple instances) and faithful to the local explanations. The method accepts any local explanation technique as input and outputs a small set of interpretable rules that explain the model's behavior for an entire class of instances.

## Key Results
- CEGA significantly outperforms GLocalX in fidelity metrics (accuracy, AUC, F1-score) across 20 datasets
- CEGA reduces rule count from hundreds to just a handful compared to Anchors
- Characteristic rules consistently show higher fidelity than discriminative rules with fewer rules
- CEGA with SHAP or Anchors as local explanation techniques produces higher fidelity than when using LIME

## Why This Works (Mechanism)
CEGA works by bridging local and global interpretability through a principled aggregation framework. By converting local explanations to itemsets and applying association rule mining, it identifies patterns that are statistically significant across multiple instances. This approach captures the underlying structure of the model's decision-making process while filtering out noise from individual explanations.

## Foundational Learning
- Association rule mining: why needed - identifies frequent patterns and relationships in data; quick check - supports confidence and lift metrics
- Local explanation techniques (SHAP/LIME/Anchors): why needed - provide instance-level feature contributions; quick check - produces additive feature attributions
- Itemsets and transactions: why needed - data format for association mining; quick check - each instance becomes a transaction of feature-value pairs
- Fidelity measurement: why needed - evaluates how well rules represent model behavior; quick check - compares rule predictions to model predictions

## Architecture Onboarding
Component map: Local explanations -> Itemsets -> Association rule mining -> Characteristic rules
Critical path: The transformation from local explanations to itemsets is critical, as it determines the quality of the association rules mined.
Design tradeoffs: Uses characteristic rules for interpretability versus discriminative rules for decision boundaries; accepts any local explanation technique versus being tied to specific methods.
Failure signatures: Poor local explanations lead to uninformative itemsets; overly general rules may lose fidelity; excessive rule mining may produce noise.
First experiments: 1) Run CEGA with SHAP on a simple dataset and inspect the generated rules; 2) Compare rule counts between CEGA and Anchors on a medium-sized dataset; 3) Test fidelity differences between characteristic and discriminative rules on a binary classification problem.

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation relies entirely on synthetic data from local explanation techniques
- Fidelity metric assumes aggregated characteristic rules should maintain local explanation accuracy
- Does not evaluate semantic quality or actionability of generated rules for end-users

## Confidence
- CEGA's superiority over existing methods: Medium
- Characteristic rules are inherently more interpretable: Low
- CEGA can accept any local explanation technique: Medium

## Next Checks
1. Validate CEGA on real-world datasets where ground truth local explanations are available through expert annotation or controlled experiments
2. Conduct user studies comparing the interpretability and usefulness of characteristic versus discriminative rules for different stakeholder groups
3. Test CEGA's performance with additional local explanation techniques beyond SHAP, LIME, and Anchors to confirm the claimed generality