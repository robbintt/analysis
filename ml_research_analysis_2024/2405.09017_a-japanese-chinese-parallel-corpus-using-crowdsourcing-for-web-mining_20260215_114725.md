---
ver: rpa2
title: A Japanese-Chinese Parallel Corpus Using Crowdsourcing for Web Mining
arxiv_id: '2405.09017'
source_url: https://arxiv.org/abs/2405.09017
tags: []
core_contribution: Using crowdsourcing, the authors collected more than 10,000 URL
  pairs of bilingual Japanese-Chinese websites and created a parallel corpus of 4.6
  million sentence pairs. They used document and sentence alignment based on a Japanese-Chinese
  bilingual dictionary and applied a parallel corpus filter to improve quality.
---

# A Japanese-Chinese Parallel Corpus Using Crowdsourcing for Web Mining

## Quick Facts
- arXiv ID: 2405.09017
- Source URL: https://arxiv.org/abs/2405.09017
- Reference count: 23
- Key result: Created 4.6 million Japanese-Chinese sentence pairs using crowdsourcing, achieving translation accuracy comparable to much larger corpora

## Executive Summary
This paper presents a novel approach to creating Japanese-Chinese parallel corpora using crowdsourcing to identify bilingual websites, followed by dictionary-based document and sentence alignment. The authors collected over 10,000 URL pairs through crowd workers and extracted 4.6 million sentence pairs. Their translation models trained on this corpus achieved comparable accuracy to those trained on the much larger CCMatrix corpus, demonstrating that crowdsourcing can effectively identify high-quality parallel content for web mining applications.

## Method Summary
The authors employed a multi-stage pipeline: first, crowd workers manually identified bilingual Japanese-Chinese websites; second, Heritrix web crawler extracted parallel documents; third, dictionary-based alignment using a 160K word pair bilingual dictionary identified parallel documents and sentences; finally, Bicleaner filtering removed noisy pairs using statistical language models. The approach deliberately minimized external resource requirements while maintaining quality through human-in-the-loop verification at the URL collection stage.

## Key Results
- Created 4.6 million Japanese-Chinese parallel sentence pairs from crowdsourced URL pairs
- Translation accuracy comparable to models trained on CCMatrix (13M pairs) despite using only one-third the data
- Resource-efficient approach requiring minimal external resources beyond a bilingual dictionary

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Crowdsourcing improves parallel data quality by leveraging human judgment to identify genuine bilingual websites
- Mechanism: Crowd workers manually verify and select websites containing parallel content, reducing noise compared to automated web crawling
- Core assumption: Human evaluators can more accurately distinguish high-quality parallel websites than automated methods based on language detection alone
- Evidence anchors:
  - [abstract] "we collected more than 10,000 URL pairs (parallel top page pairs) of bilingual websites that contain parallel documents"
  - [section 3.1] "We asked crowd workers to collect websites containing parallel pages, specifying only the language pairs"
  - [corpus] Limited evidence - paper does not provide detailed analysis of crowdsourced selection criteria
- Break condition: If crowd workers cannot effectively identify parallel content or if quality control measures fail, the advantage over automated methods diminishes

### Mechanism 2
- Claim: Bilingual dictionary-based alignment produces high-quality sentence pairs with minimal external resources
- Mechanism: Using a Japanese-Chinese dictionary and word alignment to compute semantic equivalence between documents and sentences
- Core assumption: Dictionary coverage and word alignment accuracy are sufficient for reliable document and sentence alignment
- Evidence anchors:
  - [section 3.2] "We used a Japanese-Chinese bilingual dictionary of 160K word pairs for document and sentence alignment"
  - [section 3.2] "The bilingual dictionary-based sentence alignment uses Hunalign"
  - [corpus] Dictionary coverage is explicitly stated (160K word pairs) but alignment accuracy metrics are not provided
- Break condition: If dictionary coverage is insufficient or word alignment introduces significant errors, parallel corpus quality degrades

### Mechanism 3
- Claim: Parallel corpus filtering with Bicleaner effectively removes noisy sentence pairs
- Mechanism: Training a classifier on high-quality in-house data to score and filter sentence pairs based on word translation probabilities and statistical language models
- Core assumption: The in-house training data (1.2M sentence pairs) adequately represents high-quality parallel content for the filter to learn from
- Evidence anchors:
  - [section 3.3] "We used high-quality 1.2M Japanese-Chinese sentence pairs to train a parallel corpus filter based on statistical language models and word translation probabilities"
  - [section 3.3] "We used Bicleaner to minimize the use of external resources and improve computational efficiency"
  - [corpus] Training data source and composition are described, but filter performance metrics are not detailed
- Break condition: If training data quality is poor or filter parameters are suboptimal, useful sentence pairs may be incorrectly filtered out

## Foundational Learning

- Concept: Document alignment using bilingual dictionaries
  - Why needed here: Critical for identifying parallel documents before sentence alignment
  - Quick check question: How does dictionary-based document alignment differ from embedding-based methods in terms of resource requirements?

- Concept: Sentence alignment algorithms (Hunalign)
  - Why needed here: Converts aligned documents into parallel sentence pairs
  - Quick check question: What are the key assumptions behind dictionary-based sentence alignment algorithms?

- Concept: Parallel corpus filtering techniques
  - Why needed here: Essential for removing noise from automatically extracted parallel data
  - Quick check question: What features do statistical language models use to determine sentence pair quality?

## Architecture Onboarding

- Component map:
  Crowdsourcing layer → URL collection → Web crawler (Heritrix) → Website content extraction → Document alignment module → Parallel document identification → Sentence alignment module (Hunalign) → Sentence pair extraction → Corpus filtering (Bicleaner) → Quality control → Translation model training → Evaluation

- Critical path: Crowdsourcing → URL collection → Web crawling → Document alignment → Sentence alignment → Corpus filtering → Translation evaluation

- Design tradeoffs:
  - Crowdsourcing vs. automated collection: Higher quality vs. scalability
  - Dictionary-based vs. embedding-based alignment: Lower resource requirements vs. potentially lower accuracy
  - Filter threshold selection: Balance between recall and precision

- Failure signatures:
  - Low extraction rates from crawled websites
  - Translation model performance significantly below baseline
  - Corpus quality metrics showing high noise levels

- First 3 experiments:
  1. Compare extraction rates from crowdsourced vs. Common Crawl websites
  2. Evaluate dictionary-based alignment accuracy against a small manually aligned sample
  3. Test different Bicleaner threshold values on a validation set to optimize filter performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would the translation quality change if the parallel corpus filtering threshold was adjusted?
- Basis in paper: [explicit] The paper used a threshold of 0.5 or higher for Bicleaner and 0.7 for LaBSE, but these values were not systematically evaluated.
- Why unresolved: The paper fixed these thresholds without exploring the impact of different values on translation quality.
- What evidence would resolve it: Systematic experiments varying the Bicleaner and LaBSE thresholds and measuring the resulting translation accuracy on standard test sets.

### Open Question 2
- Question: Would using machine translation-based document and sentence alignment improve quality over the bilingual dictionary approach?
- Basis in paper: [explicit] The authors state they plan to "train a machine translation model using the sentence pairs created with bilingual dictionary-based document and sentence alignment to perform machine translation-based document and sentence alignment, which could improve the quality of parallel sentence pairs."
- Why unresolved: The authors only used bilingual dictionary-based alignment in this work and did not evaluate machine translation-based alignment.
- What evidence would resolve it: Creating a parallel corpus using machine translation-based alignment and comparing its translation quality to the current dictionary-based approach.

### Open Question 3
- Question: How would adding the Common Crawl parallel data (2.8M sentence pairs) to the crowdsourced data affect translation quality, especially for Chinese-to-Japanese?
- Basis in paper: [explicit] The authors state they expect "that adding parallel sentence pairs collected using Common Crawl will increase diversity and improve translation accuracy from Chinese to Japanese."
- Why unresolved: The paper only evaluated the crowdsourced corpus alone, not combined with the Common Crawl data.
- What evidence would resolve it: Creating a combined corpus from both sources and evaluating its translation accuracy, particularly focusing on Chinese-to-Japanese direction.

## Limitations

- Dictionary-based alignment may have coverage limitations for specialized domains or less common vocabulary
- Translation experiments lack ablation studies to isolate the impact of each pipeline component
- Crowdsourcing methodology's reliability depends heavily on worker quality control, which is not extensively discussed

## Confidence

- High confidence: The feasibility of using crowdsourcing for parallel corpus collection
- Medium confidence: The translation performance claims given the lack of intermediate validation
- Medium confidence: The resource efficiency claims due to limited implementation details

## Next Checks

1. Measure dictionary coverage and alignment accuracy on a manually annotated test set
2. Compare crowdsourced URL quality against automated Common Crawl extraction rates
3. Perform ablation studies varying corpus filtering thresholds and dictionary sizes