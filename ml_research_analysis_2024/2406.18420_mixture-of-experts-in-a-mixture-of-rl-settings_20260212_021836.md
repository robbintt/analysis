---
ver: rpa2
title: Mixture of Experts in a Mixture of RL settings
arxiv_id: '2406.18420'
source_url: https://arxiv.org/abs/2406.18420
tags:
- learning
- baseline
- performance
- figure
- moes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of Mixture of Experts
  (MoE) architectures in deep reinforcement learning (DRL) under high non-stationarity
  settings. The authors focus on multi-task reinforcement learning (MTRL) and continual
  reinforcement learning (CRL) to "amplify" non-stationarity beyond single-task RL.
---

# Mixture of Experts in a Mixture of RL settings

## Quick Facts
- arXiv ID: 2406.18420
- Source URL: https://arxiv.org/abs/2406.18420
- Reference count: 40
- Primary result: Big architecture (single MoE module with full-network experts) outperforms other MoE variants and baseline in CRL, with SoftMoE routing showing better performance than TopK routing

## Executive Summary
This paper investigates the effectiveness of Mixture of Experts (MoE) architectures in deep reinforcement learning (DRL) under high non-stationarity settings. The authors focus on multi-task reinforcement learning (MTRL) and continual reinforcement learning (CRL) to amplify non-stationarity beyond single-task RL. They evaluate various MoE architectures, including replacing different layers of the network with MoE modules or using a single MoE module with experts comprising the full network. The primary results show that the Big architecture, which uses a single MoE module with full-network experts, outperforms other MoE variants and the baseline in CRL. In MTRL, only the Big architecture significantly outperforms the baseline. The authors also find that SoftMoE routing is more effective than TopK routing in handling high non-stationarity. Additionally, they observe that MoE variants reduce dormant neurons compared to the baseline, suggesting improved network plasticity.

## Method Summary
The authors evaluate MoE architectures in DRL by implementing various MoE configurations within the DQN framework. They test different architectures: replacing individual layers (ReLU, Linear, Head) with MoE modules, and a Big architecture using a single MoE module with full-network experts. Two routing strategies are compared: TopK (selecting top-k experts) and SoftMoE (weighted combination of all experts). The experiments are conducted on CartPole and MountainCar environments under MTRL and CRL settings. The evaluation includes measuring final performance, learning curves, and neuron activation patterns to assess network plasticity.

## Key Results
- Big architecture (single MoE module with full-network experts) outperforms other MoE variants and baseline in CRL
- Only Big architecture significantly outperforms baseline in MTRL settings
- SoftMoE routing is more effective than TopK routing in handling high non-stationarity
- MoE variants reduce dormant neurons compared to baseline, suggesting improved network plasticity

## Why This Works (Mechanism)
MoE architectures work in DRL under high non-stationarity by allowing specialized experts to handle different tasks or environment states. The routing mechanism (especially SoftMoE) enables smooth transitions between experts as the environment changes, maintaining performance across task switches. This specialization reduces interference between tasks and prevents catastrophic forgetting in continual learning scenarios. The reduction in dormant neurons indicates that MoE architectures maintain better feature utilization across changing conditions, which is crucial for adapting to non-stationary environments.

## Foundational Learning
- **Mixture of Experts (MoE)**: A neural network architecture with multiple specialized sub-networks (experts) and a gating network that routes inputs to appropriate experts. Why needed: Allows specialization for different tasks/states in non-stationary environments. Quick check: Verify that experts have distinct activation patterns for different inputs.
- **Non-stationarity in RL**: The changing nature of the environment or task over time. Why needed: Real-world RL applications often face changing conditions requiring adaptive policies. Quick check: Measure performance degradation when task/environment changes abruptly.
- **Catastrophic Forgetting**: The tendency of neural networks to forget previously learned tasks when learning new ones. Why needed: Critical challenge in continual learning that MoE architectures aim to address. Quick check: Compare performance on old vs new tasks after training sequence.
- **Dormant Neurons**: Neurons that remain inactive (near-zero activation) across many inputs. Why needed: Indicator of network efficiency and plasticity; reducing dormancy suggests better utilization. Quick check: Measure activation frequency distribution across neurons.
- **SoftMoE vs TopK Routing**: SoftMoE uses weighted combination of all experts, while TopK selects only top-k experts. Why needed: Different routing strategies affect how smoothly the network can adapt to changes. Quick check: Analyze performance stability during task transitions.

## Architecture Onboarding

Component Map:
Input -> Observation Preprocessing -> MoE Module (Experts + Router) -> Value Estimation -> Action Selection

Critical Path:
Observation → Router → Expert Selection → Value Estimation → Action

Design Tradeoffs:
- **Router Complexity**: SoftMoE routing provides smoother transitions but may be computationally heavier than TopK
- **Expert Specialization**: More experts allow finer specialization but increase model complexity and training difficulty
- **Layer Placement**: Placing MoE at different network depths affects how specialization is distributed across the feature hierarchy

Failure Signatures:
- **Poor Performance in CRL**: Indicates routing mechanism failing to adapt to task changes
- **Catastrophic Forgetting**: Suggests experts aren't properly specialized or routing isn't maintaining task-specific pathways
- **Excessive Dormant Neurons**: May indicate routing mechanism is too selective or experts aren't being utilized effectively

First 3 Experiments:
1. Compare Big architecture performance against baseline on simple non-stationary task with abrupt changes
2. Test SoftMoE vs TopK routing performance during task transition periods
3. Analyze neuron activation patterns to verify reduction in dormant neurons across MoE variants

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation limited to simple benchmark tasks (CartPole, MountainCar) which may not represent real-world complexity
- Analysis of dormant neurons provides only partial view of network plasticity without direct measurement of practical benefits
- Comparison between routing strategies doesn't explore adaptive or hybrid routing approaches
- Computational overhead (training time, inference efficiency) of MoE architectures not investigated

## Confidence

High Confidence:
- Big architecture outperforms other MoE variants and baseline in CRL
- MoE variants reduce dormant neurons compared to baseline

Medium Confidence:
- SoftMoE routing is more effective than TopK routing in handling high non-stationarity

Low Confidence:
- General statement that MoE architectures are beneficial for DRL under high non-stationarity

## Next Checks

1. Expand task diversity: Validate findings on broader set of DRL benchmark tasks with varying degrees of non-stationarity
2. Investigate computational efficiency: Analyze training times, inference speeds, and memory usage of MoE architectures vs baseline
3. Explore adaptive routing strategies: Experiment with dynamic routing that can switch between SoftMoE and TopK based on environment state or task requirements