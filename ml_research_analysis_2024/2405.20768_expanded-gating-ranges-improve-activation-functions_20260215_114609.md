---
ver: rpa2
title: Expanded Gating Ranges Improve Activation Functions
arxiv_id: '2405.20768'
source_url: https://arxiv.org/abs/2405.20768
tags:
- activation
- gating
- functions
- function
- gelu
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the effectiveness of expanding the gating
  ranges of activation functions in deep learning architectures. The authors propose
  using arctan as a gating mechanism and introduce Expanded ArcTan Linear Unit (xATLU),
  Expanded GELU (xGELU), and Expanded SiLU (xSiLU).
---

# Expanded Gating Ranges Improve Activation Functions

## Quick Facts
- arXiv ID: 2405.20768
- Source URL: https://arxiv.org/abs/2405.20768
- Authors: Allen Hao Huang
- Reference count: 6
- Primary result: Expanding gating ranges of activation functions with arctan gating mechanism improves performance in transformer architectures

## Executive Summary
This paper investigates the effectiveness of expanding the gating ranges of activation functions in deep learning architectures. The authors propose using arctan as a gating mechanism and introduce Expanded ArcTan Linear Unit (xATLU), Expanded GELU (xGELU), and Expanded SiLU (xSiLU). They demonstrate that these expanded gating ranges can improve the performance of self-gated activation functions and first-order Gated Linear Units (GLU). The results show that xATLU, xGELU, and xSiLU outperform their respective baseline activation functions, and expanded gating ranges narrow the performance gap between first-order and second-order GLU. The authors also identify favorable properties of activation functions and challenge the conventional belief that ReLU-like properties are necessary for good performance.

## Method Summary
The authors introduce a trainable parameter α to expand the gating range of activation functions from (0,1) to (-α,1+α). They use arctan as the gating function and apply this expansion to create xATLU, xGELU, and xSiLU. The method is evaluated on transformer-based autoregressive language modeling tasks using the OpenWebText2 dataset with a modified nanoGPT implementation. Models are trained on A100 GPUs with depth 12/24/48 and model dimension 768, using 3 different random seeds for each experiment.

## Key Results
- xATLU, xGELU, and xSiLU outperform their respective baseline activation functions (ATLU, GELU, SiLU)
- Expanded gating ranges narrow the performance gap between first-order and second-order GLU
- The performance improvements suggest that ReLU-like properties are not necessary for good activation function performance
- xATLU, xGELU, and xSiLU consistently achieve lower perplexity scores compared to their baseline counterparts

## Why This Works (Mechanism)

### Mechanism 1
Expanding gating ranges allows gradients to flow through both positive and negative input regions, enabling better information propagation during training. The trainable parameter α expands the gating range from (0,1) to (-α,1+α), allowing the gating function to take on negative values and values above one. This enables the first derivative of the activation function to be negative or greater than one, which can preserve more information in the backward pass and prevent information loss in regions where traditional gating functions saturate.

### Mechanism 2
Monotonically increasing gating functions with continuously differentiable properties lead to better gradient flow and training stability. The arctan function is continuously differentiable and monotonically increasing, ensuring smooth gradient flow during backpropagation. This avoids the vanishing gradient problem that can occur with non-differentiable activation functions like ReLU, while also providing more information than simple binary thresholding.

### Mechanism 3
The specific choice of gating range expansion (-α, 1+α) provides the optimal balance between negative and positive gradient flow. The symmetric expansion around the original range (0,1) ensures that the gating function can take negative values while still maintaining positive values above one. This balanced approach allows for controlled negative gradient flow while preserving the positive gradient flow that ReLU-like functions provide.

## Foundational Learning

- Concept: Self-gated activation functions and their mathematical formulation
  - Why needed here: Understanding how self-gated functions work is crucial for grasping why expanding the gating range improves performance. The paper's core innovation builds directly on this concept.
  - Quick check question: Can you write the general form of a self-gated activation function and explain what each component represents?

- Concept: The relationship between activation function properties and gradient flow
  - Why needed here: The paper argues that certain properties (like negative gradient flow and gradients above one) are beneficial. Understanding this relationship is key to understanding the proposed mechanisms.
  - Quick check question: How does the range of an activation function's first derivative affect the flow of gradients during backpropagation?

- Concept: Gated Linear Units (GLUs) and their relationship to self-gated activation functions
  - Why needed here: The paper extends the gating range expansion idea to GLUs, showing that the same principles apply. Understanding GLUs is necessary to follow this extension.
  - Quick check question: What is the key difference between self-gated activation functions and GLUs in terms of their mathematical formulation?

## Architecture Onboarding

- Component map: Transformer -> MLP blocks -> Activation functions (xATLU/xGELU/xSiLU) with trainable α parameters
- Critical path: 1) Initialize α to zero, 2) During forward pass, compute the gating function with expanded range, 3) During backward pass, update α along with other model parameters, 4) Monitor perplexity on validation set to ensure the expanded gating is beneficial
- Design tradeoffs: The main tradeoff is between the increased expressivity from expanded gating ranges and the potential for instability from allowing negative gradients. Another tradeoff is between the simplicity of a single α per MLP block versus per-channel or per-neuron parameters.
- Failure signatures: If perplexity increases or training becomes unstable, it may indicate that the gating range expansion is too aggressive. If α remains near zero throughout training, it suggests that expanded gating ranges aren't beneficial for the given task.
- First 3 experiments:
  1. Run a small-scale experiment comparing ATLU with and without gating range expansion to verify that expanded ranges improve performance.
  2. Test different fixed values of α to find the optimal expansion before implementing trainable α.
  3. Compare xATLU, xGELU, and xSiLU on a standard benchmark to verify the general applicability of the gating range expansion approach.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal gating range expansion factor (α) for different activation functions and architectures?
- Basis in paper: [explicit] The paper shows that expanded gating ranges improve activation functions but does not determine the optimal range for each function.
- Why unresolved: The paper only explores fixed values of α in preliminary experiments and uses a trainable α in main experiments, but does not systematically determine optimal ranges.
- What evidence would resolve it: Systematic experiments testing different α ranges for each activation function across various architectures and datasets.

### Open Question 2
- Question: How does the expanded gating range technique affect activation sparsity in large language models?
- Basis in paper: [inferred] The paper mentions activation sparsity as a limitation but does not investigate how expanded gating ranges impact it.
- Why unresolved: The paper does not measure or analyze activation sparsity patterns when using expanded gating ranges.
- What evidence would resolve it: Experiments measuring activation sparsity and computational efficiency when using xATLU, xGELU, and xSiLU compared to baseline functions.

### Open Question 3
- Question: Are there alternative gating functions to arctan that could benefit from expanded gating ranges?
- Basis in paper: [explicit] The paper suggests that expanded gating ranges increase the search space of viable gating functions but does not explore alternatives to arctan.
- Why unresolved: The paper only demonstrates the technique with arctan and does not systematically search for or test other potential gating functions.
- What evidence would resolve it: Experiments testing various bounded functions (beyond arctan) with expanded gating ranges to identify other effective gating mechanisms.

## Limitations

- Limited exploration of alternative gating range expansion strategies beyond the symmetric (-α, 1+α) parameterization
- Experiments conducted exclusively on transformer-based language modeling tasks, limiting generalizability to other domains
- Lack of detailed analysis on computational overhead introduced by the additional trainable parameter α

## Confidence

**High Confidence**: The empirical results demonstrating that expanded gating ranges improve perplexity scores for xATLU, xGELU, and xSiLU compared to their baseline counterparts. The experimental setup is well-defined, and the improvements are consistent across multiple model depths and random seeds.

**Medium Confidence**: The claim that negative gradient flow and gradients greater than one are beneficial for learning. While the empirical evidence supports this assertion, the theoretical foundation is not fully developed, and the mechanisms could vary depending on the specific architecture and task.

**Low Confidence**: The assertion that ReLU-like properties are not necessary for good performance. This conclusion is based primarily on the success of xATLU, xGELU, and xSiLU, but doesn't consider a broader range of activation functions or architectural variations that might still benefit from ReLU-like characteristics.

## Next Checks

1. **Ablation Study on Gating Range Parameters**: Systematically test different gating range expansions (e.g., (-α, 1), (0, 1+α), (-α, 1+β) with different α and β values) across multiple tasks and architectures to determine if the symmetric (-α, 1+α) expansion is truly optimal or if task-specific ranges perform better.

2. **Cross-Domain Performance Evaluation**: Evaluate xATLU, xGELU, and xSiLU on non-language tasks such as computer vision (image classification, object detection) and reinforcement learning to assess the generalizability of expanded gating ranges beyond autoregressive language modeling.

3. **Theoretical Analysis of Gradient Flow Properties**: Conduct a formal mathematical analysis of how expanded gating ranges affect gradient flow properties, including second-order effects on optimization dynamics, and develop theoretical bounds on the benefits of negative gradient flow and gradients greater than one.