---
ver: rpa2
title: Learning Sequence Attractors in Recurrent Networks with Hidden Neurons
arxiv_id: '2404.02729'
source_url: https://arxiv.org/abs/2404.02729
tags: []
core_contribution: This paper investigates how recurrent networks of binary neurons
  can learn to store and retrieve temporal sequence patterns robustly, an ability
  that standard recurrent models without hidden neurons lack for arbitrary sequences.
  The authors introduce hidden neurons connected to visible ones and propose a local
  learning algorithm inspired by feedback alignment and target propagation.
---

# Learning Sequence Attractors in Recurrent Networks with Hidden Neurons

## Quick Facts
- arXiv ID: 2404.02729
- Source URL: https://arxiv.org/abs/2404.02729
- Authors: Yao Lu; Si Wu
- Reference count: 21
- This paper investigates how recurrent networks of binary neurons can learn to store and retrieve temporal sequence patterns robustly, an ability that standard recurrent models without hidden neurons lack for arbitrary sequences.

## Executive Summary
This paper addresses the challenge of learning and retrieving arbitrary temporal sequences in recurrent neural networks of binary neurons. While standard recurrent networks struggle with arbitrary sequences, the authors introduce hidden neurons connected to visible ones and propose a biologically plausible learning algorithm. The method combines local three-factor learning rules with theoretical guarantees for convergence and attractor formation, demonstrating successful learning on both synthetic random sequences and real-world datasets like gait recognition and moving MNIST.

## Method Summary
The authors introduce hidden neurons into recurrent networks of binary neurons to enable learning of arbitrary temporal sequences. They propose a local learning algorithm inspired by feedback alignment and target propagation, where weight updates follow a three-factor rule involving pre-synaptic activation, post-synaptic activation, and an error-modulated term. The algorithm learns both the visible-to-hidden and hidden-to-visible weight matrices simultaneously, with a robustness hyperparameter κ controlling the convergence properties. Theoretical analysis proves that the algorithm converges and ensures sequence attractors, while experiments demonstrate successful learning and retrieval under noise conditions.

## Key Results
- The proposed algorithm successfully learns and retrieves arbitrary temporal sequences that standard recurrent networks cannot handle
- Experiments on synthetic random sequences and real-world datasets (OU-ISIR gait and Moving MNIST) show robust performance under noise
- Joint learning of both weight matrices outperforms methods that fix hidden-to-visible weights, while sparsity does not significantly improve capacity

## Why This Works (Mechanism)
The mechanism works by introducing hidden neurons that create an intermediate representation space, allowing the network to transform arbitrary input sequences into a form that can be stored as attractors. The three-factor learning rule enables local weight updates that approximate the error gradient without requiring symmetric feedback connections, making it more biologically plausible than traditional backpropagation through time. The robustness hyperparameter κ controls the trade-off between stability and flexibility in the attractor landscape, allowing the network to maintain sequence patterns while tolerating perturbations.

## Foundational Learning
- **Recurrent neural networks with binary neurons**: Why needed - to model biological plausibility and computational simplicity; Quick check - verify binary activation function and recurrent connections
- **Sequence attractors**: Why needed - to understand how temporal patterns can be stored as stable states; Quick check - confirm attractor dynamics through state space visualization
- **Three-factor learning rules**: Why needed - to enable local weight updates with error modulation; Quick check - verify the pre-synaptic, post-synaptic, and error term components
- **Feedback alignment and target propagation**: Why needed - to understand biologically plausible alternatives to backpropagation; Quick check - compare weight update rules with standard backpropagation
- **Robustness hyperparameters in learning algorithms**: Why needed - to understand trade-offs between stability and flexibility; Quick check - analyze how different κ values affect convergence

## Architecture Onboarding

**Component Map**: Input sequence -> Visible neurons -> Hidden neurons -> Output reconstruction -> Error signal -> Weight updates

**Critical Path**: Input sequence → visible neurons → hidden neurons → output reconstruction → error computation → weight updates (both V→H and H→V matrices)

**Design Tradeoffs**: 
- Hidden neurons enable arbitrary sequence learning but increase computational complexity
- Local learning rules improve biological plausibility but may sacrifice convergence speed compared to global optimization
- Robustness hyperparameter κ provides stability control but requires careful tuning

**Failure Signatures**:
- Network fails to learn sequences beyond certain length regardless of hidden neurons
- Weight updates oscillate or diverge during training
- Retrieved sequences show high noise sensitivity despite successful training

**First Experiments**:
1. Test the algorithm on short synthetic sequences (T=5-10) with varying hidden neuron counts
2. Measure retrieval accuracy under different noise levels to verify robustness claims
3. Compare learning dynamics with and without hidden neurons on the same sequences

## Open Questions the Paper Calls Out
### Open Question 1
- Question: How does the robustness hyperparameter κ affect the convergence speed and final accuracy of the learning algorithm?
- Basis in paper: The authors mention that κ acts as a robustness hyperparameter controlling the level of perturbation for inequality (12) to hold, but do not explore its impact on convergence or accuracy.
- Why unresolved: The paper sets κ = 1 for all experiments without exploring how different values affect performance. The theoretical analysis (Theorem 4) shows κ controls robustness but not convergence dynamics.
- What evidence would resolve it: Experiments varying κ across a range of values and measuring convergence speed, final error rates, and retrieval accuracy under different noise levels.

### Open Question 2
- Question: What is the maximum sequence length T that can be reliably learned and retrieved by the network, and how does this depend on the number of hidden neurons M?
- Basis in paper: The authors show that the algorithm fails for large T even with increased M in Figure 5, but do not characterize the theoretical limit or provide a precise relationship between T and M.
- Why unresolved: The paper demonstrates limitations empirically but does not provide theoretical bounds on the maximum learnable sequence length or how it scales with network size.
- What evidence would resolve it: Theoretical analysis deriving bounds on maximum T as a function of M, N, and other parameters, validated by extensive empirical tests.

### Open Question 3
- Question: How does the proposed learning algorithm compare to other biologically plausible sequence learning methods like predictive coding or modern Hopfield networks?
- Basis in paper: The authors briefly mention modern Hopfield networks and predictive coding as related work but do not directly compare their algorithm's performance to these methods.
- Why unresolved: While the paper establishes the effectiveness of their algorithm, it does not benchmark against other biologically plausible approaches to sequence learning.
- What evidence would resolve it: Direct empirical comparisons of the proposed algorithm with predictive coding networks and modern Hopfield networks on the same sequence learning tasks, measuring accuracy, convergence speed, and biological plausibility.

## Limitations
- Theoretical analysis relies on idealized assumptions that may not hold under stochastic gradient updates in finite networks
- Biological plausibility claims are speculative, as the proposed learning rule still requires error-modulation signals not clearly mapped to known neural mechanisms
- Performance gains are shown against limited baselines, leaving open whether other architectures might perform similarly

## Confidence
- **High**: The algorithm can learn to store and retrieve sequences in synthetic and real-world tasks
- **Medium**: The theoretical guarantees of convergence and attractor formation under the proposed learning rule
- **Low**: Biological plausibility and mapping to neural substrates

## Next Checks
1. Test the algorithm's performance on longer sequences and measure how robustness hyperparameter scales with sequence length
2. Compare against other recurrent architectures with hidden units (e.g., LSTM, GRU) on the same sequence learning tasks
3. Analyze the learned hidden representations to assess whether they exhibit biologically plausible coding properties (e.g., population coding, temporal integration)