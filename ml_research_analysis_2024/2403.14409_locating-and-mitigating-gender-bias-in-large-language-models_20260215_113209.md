---
ver: rpa2
title: Locating and Mitigating Gender Bias in Large Language Models
arxiv_id: '2403.14409'
source_url: https://arxiv.org/abs/2403.14409
tags:
- bias
- gender
- lsdm
- language
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper addresses gender bias in large language models (LLMs)
  by integrating location and mitigation approaches. The authors employ causal mediation
  analysis to trace the flow of gender bias within LLMs, identifying the bottom MLP
  modules acting on the last token of occupational pronouns and the top attention
  module acting on the final word in the sentence as primary contributors.
---

# Locating and Mitigating Gender Bias in Large Language Models

## Quick Facts
- arXiv ID: 2403.14409
- Source URL: https://arxiv.org/abs/2403.14409
- Reference count: 40
- Authors identify bottom MLP modules on last token of occupational pronouns and top attention module on final word as primary gender bias carriers

## Executive Summary
This paper addresses gender bias in large language models by integrating both localization and mitigation approaches. The authors employ causal mediation analysis to trace the flow of gender bias within LLMs, identifying specific transformer components responsible for propagating bias. They propose the Least Square Debias Method (LSDM), a knowledge-editing based approach that modifies parameters to reduce gender bias while preserving the model's overall capabilities. The approach is evaluated on three gender bias datasets and seven knowledge competency test datasets, demonstrating superior performance in reducing gender bias compared to baseline methods.

## Method Summary
The authors integrate causal mediation analysis with parameter editing to address gender bias in LLMs. They first use causal mediation analysis to identify the flow of gender bias through transformer components, discovering that the bottom MLP modules acting on the last token of occupational pronouns and the top attention module acting on the final word are primary bias carriers. Based on these findings, they propose the Least Square Debias Method (LSDM), which modifies specific parameters to reduce gender bias while maintaining model performance on other tasks. The approach combines structural causal modeling with knowledge editing techniques to achieve targeted bias reduction.

## Key Results
- LSDM achieves an average reduction of 71.4% in gender bias across evaluation datasets
- The approach maintains model performance on seven knowledge competency test datasets while reducing bias
- Outperforms two baseline methods in both bias reduction effectiveness and capability preservation

## Why This Works (Mechanism)
The method works by first using causal mediation analysis to identify specific transformer components that propagate gender bias through the model. By understanding the causal pathways through which bias flows (bottom MLP modules for pronoun tokens and top attention for final words), the authors can target interventions precisely rather than applying broad, potentially disruptive modifications. The Least Square Debias Method then modifies parameters in these identified components using knowledge editing techniques, allowing for bias reduction while preserving the model's learned knowledge and capabilities on other tasks.

## Foundational Learning
- **Causal Mediation Analysis**: Why needed - To identify which transformer components actually carry and propagate gender bias rather than applying blanket interventions. Quick check - Verify that identified components show statistical significance in bias propagation across multiple datasets.
- **Transformer Architecture**: Why needed - Understanding MLP and attention mechanisms is crucial for identifying where bias manifests. Quick check - Confirm understanding of how residual connections and layer normalization affect bias propagation.
- **Knowledge Editing**: Why needed - To modify bias-carrying parameters without disrupting overall model performance. Quick check - Ensure edits preserve model outputs on non-bias-related tasks.
- **Bias Quantification Metrics**: Why needed - To measure effectiveness of bias reduction interventions. Quick check - Validate that chosen metrics capture meaningful aspects of gender bias in language models.

## Architecture Onboarding

**Component Map:**
Input -> Token Embeddings -> Transformer Layers (MLP + Attention) -> Output

**Critical Path:**
Occupational pronoun tokens → Bottom MLP modules → Attention modules → Final output tokens

**Design Tradeoffs:**
- Targeted vs. broad parameter modification: Targeted modification preserves capabilities but may miss subtle bias propagation
- Synthetic vs. naturalistic evaluation: Synthetic datasets allow controlled measurement but may not reflect real-world complexity
- Single-bias vs. multi-bias consideration: Focusing on gender bias may create blind spots for other bias types

**Failure Signatures:**
- Over-correction leading to performance degradation on non-bias tasks
- Bias shifting to different linguistic contexts or transformer components
- Unintended amplification of other forms of bias during gender bias reduction

**3 First Experiments:**
1. Apply LSDM to a small subset of transformer parameters to verify targeted intervention effectiveness
2. Test bias reduction on a single occupational pronoun category before full implementation
3. Measure performance impact on knowledge competency tasks before and after parameter modification

## Open Questions the Paper Calls Out
None

## Limitations
- Focus on English-only occupational pronouns limits generalizability to multilingual contexts
- Reliance on synthetic datasets rather than naturalistic text samples raises questions about real-world applicability
- Does not examine potential trade-offs with other forms of bias (racial, cultural) or unintended consequences

## Confidence

**High confidence:**
- The causal mediation analysis methodology is technically sound and the identification of specific transformer components as bias carriers is methodologically defensible

**Medium confidence:**
- The LSDM algorithm implementation and its reported effectiveness, though limited by synthetic evaluation contexts

**Low confidence:**
- Generalizability to non-occupational contexts, multilingual applications, and claims about preserving "all" model capabilities

## Next Checks
1. Evaluate LSDM on naturalistic text corpora (news articles, social media) rather than synthetic datasets to assess real-world effectiveness
2. Test the approach on multilingual models (e.g., mBERT, XLM-R) to verify cross-linguistic applicability
3. Conduct ablation studies examining whether bias reduction in one dimension (gender) creates increases in other bias types (race, age, nationality)