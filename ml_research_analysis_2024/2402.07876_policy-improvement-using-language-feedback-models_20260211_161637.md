---
ver: rpa2
title: Policy Improvement using Language Feedback Models
arxiv_id: '2402.07876'
source_url: https://arxiv.org/abs/2402.07876
tags:
- feedback
- language
- policy
- learning
- action
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Language Feedback Models (LFMs) to improve
  policy learning in instruction following tasks. The core idea is to use LLMs to
  provide feedback on whether agent actions are productive in achieving task goals,
  then train a small feedback model to identify desirable behavior for imitation learning.
---

# Policy Improvement using Language Feedback Models

## Quick Facts
- arXiv ID: 2402.07876
- Source URL: https://arxiv.org/abs/2402.07876
- Reference count: 40
- Key outcome: Language Feedback Models improve task completion rates by 3.5-12.0% over behavioral cloning baselines

## Executive Summary
This paper introduces Language Feedback Models (LFMs) to enhance policy learning in instruction-following tasks. The core innovation uses LLMs to generate feedback on agent actions, determining whether they contribute to task goals. This feedback is then used to train a compact feedback model that identifies desirable behaviors for imitation learning. The approach demonstrates significant improvements over traditional behavioral cloning and direct LLM action prediction across three benchmarks.

## Method Summary
The method employs LLMs to evaluate whether agent actions are productive toward achieving task goals in instruction-following scenarios. A feedback model is trained on this LLM-generated feedback to distinguish between desirable and undesirable actions. This feedback model then guides imitation learning, allowing agents to learn from both positive and negative examples. The approach addresses the challenge of acquiring high-quality training data by leveraging LLM capabilities to generate structured feedback rather than relying solely on expensive human annotations or imperfect reward functions.

## Key Results
- 3.5-12.0% improvement in task completion rates over behavioral cloning baselines
- Outperforms direct LLM action prediction on ALFWorld, ScienceWorld, and Touchdown benchmarks
- Demonstrates generalization to unseen environments not present in training data
- Provides human-interpretable feedback for verification of imitation learning data quality

## Why This Works (Mechanism)
The approach works by leveraging the reasoning capabilities of LLMs to provide structured feedback on agent behavior, rather than requiring LLMs to directly generate actions. This two-stage process - first generating feedback, then learning from it - allows the system to benefit from LLM judgment while maintaining the efficiency of a compact feedback model for deployment. The feedback model learns to identify productive actions, enabling more effective imitation learning by focusing on quality over quantity of training examples.

## Foundational Learning
- **Instruction following**: Agents must interpret and execute natural language instructions; needed for aligning agent behavior with human intent
- **Imitation learning**: Learning from expert demonstrations rather than trial-and-error; provides sample-efficient training
- **Feedback modeling**: Using structured feedback to guide learning; enables learning from both positive and negative examples
- **Environment generalization**: Ability to perform in unseen environments; critical for real-world deployment
- **LLM-based evaluation**: Using language models to assess task progress; provides scalable feedback generation
- **Action filtering**: Selecting productive actions from possible options; improves learning efficiency

## Architecture Onboarding

**Component Map**: LLM -> Feedback Generator -> Feedback Model -> Imitation Learner -> Agent

**Critical Path**: Instruction → Agent Action → LLM Evaluation → Feedback Model Training → Improved Agent Policy

**Design Tradeoffs**: 
- Uses LLM for high-quality feedback generation but maintains compact feedback model for efficiency
- Balances between direct action prediction (fast but potentially less accurate) and feedback-based learning (slower but more targeted)
- Tradeoff between feedback model complexity and generalization capability

**Failure Signatures**: 
- Poor generalization when training environments don't adequately represent test environments
- Feedback model overfitting to specific LLM feedback patterns
- Insufficient diversity in training examples leading to brittle policies

**3 First Experiments**:
1. Compare task completion rates between LFMs and behavioral cloning on held-out environments
2. Evaluate feedback model performance on distinguishing desirable vs undesirable actions
3. Test human interpretability of generated feedback by having humans verify selected training examples

## Open Questions the Paper Calls Out
None

## Limitations
- Feedback model trained on limited data (10 environments) may not scale well
- Computational efficiency gains not thoroughly validated
- Potential for LLM feedback biases to propagate through training
- Limited investigation of how feedback quality affects final performance

## Confidence
- High confidence: Outperformance of baselines on ALFWorld and Touchdown
- Medium confidence: Generalization claims due to limited environment diversity in training data
- Medium confidence: Human-interpretability benefits, as verification was not systematically evaluated

## Next Checks
1. Evaluate scalability by training feedback models on varying numbers of environments (5, 25, 50) to assess performance degradation
2. Conduct ablation studies on LLM feedback quality by introducing controlled noise or biases to test robustness
3. Measure end-to-end computational efficiency including training time, model size, and inference latency compared to baseline methods