---
ver: rpa2
title: 'SeMOPO: Learning High-quality Model and Policy from Low-quality Offline Visual
  Datasets'
arxiv_id: '2406.09486'
source_url: https://arxiv.org/abs/2406.09486
tags:
- offline
- learning
- policy
- semopo
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of learning high-quality policies
  from low-quality offline visual datasets, which often contain noisy observations
  with moving distractors and are collected by non-expert or random policies. The
  authors propose SeMOPO (Separated Model-based Offline Policy Optimization), which
  decomposes latent states into endogenous (task-relevant) and exogenous (task-irrelevant)
  components using conservative sampling.
---

# SeMOPO: Learning High-quality Model and Policy from Low-quality Offline Visual Datasets

## Quick Facts
- arXiv ID: 2406.09486
- Source URL: https://arxiv.org/abs/2406.09486
- Authors: Shenghua Wan; Ziyuan Chen; Le Gan; Shuai Feng; De-Chuan Zhan
- Reference count: 40
- Primary result: SeMOPO achieves normalized returns up to 0.87 on LQV-D4RL benchmark, substantially outperforming baselines ranging from 0.04-0.73

## Executive Summary
This paper addresses the challenge of learning high-quality policies from low-quality offline visual datasets containing noisy observations with moving distractors. The authors propose SeMOPO (Separated Model-based Offline Policy Optimization), which decomposes latent states into endogenous (task-relevant) and exogenous (task-irrelevant) components using conservative sampling. By estimating uncertainty only on endogenous states and theoretically analyzing the performance bound under the Exogenous Block MDP assumption, SeMOPO achieves significantly better policy performance than existing methods across multiple tasks.

## Method Summary
SeMOPO decomposes latent states into endogenous and exogenous components using conservative sampling, where trajectories are sampled from individual policies rather than mixing all trajectories. The method estimates model uncertainty only on endogenous states, which theoretically leads to tighter performance bounds under the Exogenous Block MDP assumption. The policy and value model are trained on imagined endogenous states with uncertainty-penalized rewards. The method uses an ensemble of Gaussian dynamics models and trains for 25000 epochs before optimizing the policy for 100000 steps with an imagination horizon of 5.

## Key Results
- SeMOPO achieves normalized returns of 0.87 on average across LQV-D4RL tasks, substantially outperforming baselines ranging from 0.04-0.73
- The method shows particular strength on medium-quality datasets where other methods struggle, achieving up to 0.87 normalized return versus 0.05-0.67 for baselines
- Conservative sampling consistently results in lower action entropy compared to random sampling across different environments and datasets
- SeMOPO exhibits lower model uncertainty than Offline DV2 across nine datasets

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Separating latent states into endogenous and exogenous components reduces model uncertainty estimation bias from distractors.
- Mechanism: The method decomposes latent states z into endogenous (task-relevant) s+ and exogenous (task-irrelevant) s− components, allowing uncertainty estimation only on s+ which excludes irrelevant distractor dynamics.
- Core assumption: The environment follows an Exogenous Block MDP (EX-BMDP) structure where latent states can be decoupled into task-relevant and irrelevant parts.
- Evidence anchors:
  - [abstract]: "decomposing latent states into endogenous (task-relevant) and exogenous (task-irrelevant) components using conservative sampling"
  - [section 3.1]: "we propose a new approach - Separated Model-based Offline Policy Optimization (SeMOPO) - decomposing latent states into endogenous and exogenous parts via conservative sampling and estimating model uncertainty on the endogenous states only"
  - [corpus]: Weak - only 1 related paper mentions uncertainty in model-based RL, no direct connection to exogenous/endogenous decomposition
- Break condition: The EX-BMDP assumption fails if endogenous and exogenous states are not independent or if task-irrelevant components cannot be cleanly separated.

### Mechanism 2
- Claim: Conservative sampling of trajectories improves the model's ability to distinguish task-relevant from irrelevant dynamics.
- Mechanism: The method samples trajectories from individual policies rather than mixing all trajectories, which reduces action entropy and helps the model learn better separation between endogenous and exogenous transitions.
- Core assumption: Lower action entropy in sampled trajectories helps the model identify which dynamics are controlled by the agent versus being exogenous.
- Evidence anchors:
  - [section 3.2]: "we design a simple but effective implementation... the SeMOPO's model is only trained on the sampled trajectory τj generated by a certain policy"
  - [section 4.3]: "Figure 6, the CS method consistently results in lower action entropy compared to RS across different environments and datasets"
  - [corpus]: Weak - no direct evidence about sampling strategies affecting state decomposition in model-based RL
- Break condition: If the dataset contains policies with similar action distributions, conservative sampling provides no advantage over random sampling.

### Mechanism 3
- Claim: Uncertainty estimation on endogenous states provides tighter performance bounds than POMDP-based methods.
- Mechanism: By estimating uncertainty only on task-relevant endogenous states rather than full latent belief states, the method achieves lower model uncertainty which translates to better performance bounds.
- Core assumption: POMDP-based uncertainty estimation conflates task-relevant and irrelevant uncertainty, leading to overestimation.
- Evidence anchors:
  - [section 3.1]: "we theoretically analyze the lower performance bound under the Exogenous Block MDP assumption... We find that such a lower bound is empirically tighter than that under the POMDP assumption"
  - [section 4.2]: "SeMOPO exhibits lower model uncertainty than Offline DV2 across nine datasets"
  - [corpus]: Weak - no direct evidence comparing POMDP vs EX-BMDP uncertainty bounds in offline RL literature
- Break condition: If the endogenous state space becomes too complex or if the separation is imperfect, uncertainty estimation may become as biased as POMDP methods.

## Foundational Learning

- Concept: Exogenous Block MDP (EX-BMDP)
  - Why needed here: The method relies on the EX-BMDP assumption to decompose latent states and achieve better uncertainty estimation
  - Quick check question: What distinguishes an EX-BMDP from a standard POMDP, and why is this distinction important for SeMOPO?

- Concept: Conservative sampling strategy
  - Why needed here: The method uses conservative sampling to reduce action entropy and improve state decomposition during model training
  - Quick check question: How does sampling trajectories from individual policies (rather than mixing) affect the model's ability to separate endogenous and exogenous dynamics?

- Concept: Uncertainty estimation in model-based RL
  - Why needed here: The method uses model uncertainty as a penalty term, but only estimates it on endogenous states rather than full latent states
  - Quick check question: Why would estimating uncertainty on the full latent state (as in POMDP methods) lead to worse performance than estimating only on endogenous states?

## Architecture Onboarding

- Component map: Observation encoder → Endogenous state inference (˜qθ) → Exogenous state inference (¯qθ) → Endogenous dynamics model (eTθ) → Exogenous dynamics model (¯Tθ) → Reward predictor (Rθ) → Policy (πθ) + Value model (Vθ)
- Critical path: 
  1. Conservative sampling of trajectories
  2. Endogenous/exogenous state decomposition during model training
  3. Uncertainty estimation on endogenous states only
  4. Policy optimization with uncertainty-penalized rewards
- Design tradeoffs:
  - Conservative sampling vs. random sampling: better state decomposition vs. broader transition coverage
  - Ensemble size for uncertainty estimation: more accurate uncertainty vs. computational cost
  - Separation of endogenous/exogenous states: cleaner uncertainty vs. potential information loss
- Failure signatures:
  - High model uncertainty on normal states suggests poor endogenous state inference
  - Policy performance worse than behavioral cloning indicates overconfident uncertainty estimation
  - Training instability with high gradient norms suggests conservative sampling is too restrictive
- First 3 experiments:
  1. Implement conservative sampling and compare action entropy distributions vs. random sampling on a simple task
  2. Test state decomposition quality by visualizing endogenous state reconstructions vs. observations
  3. Compare uncertainty estimation on endogenous vs. full latent states using the same computational method

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can SeMOPO be extended to handle potential interactions between endogenous and exogenous states in real-world tasks?
- Basis in paper: [explicit] The paper mentions that the current independence assumption under EX-BMDP may not hold in real-world scenarios and suggests this as a future research direction.
- Why unresolved: The paper only proposes this as a future direction without providing any concrete approach or theoretical analysis for handling state interactions.
- What evidence would resolve it: Developing and evaluating a model that can capture interactions between endogenous and exogenous states, with theoretical guarantees on performance bounds and empirical validation on real-world datasets.

### Open Question 2
- Question: What is the optimal sampling strategy for conservative sampling in SeMOPO to maximize the separation of task-relevant and irrelevant information?
- Basis in paper: [inferred] The paper proposes a conservative sampling method but acknowledges that Theorem 3.4 does not guarantee maximum likelihood via this method, and the current implementation is described as "simple but effective."
- What evidence would resolve it: Systematic analysis of different sampling strategies, including adaptive methods that adjust sampling based on model uncertainty or other metrics, with empirical comparisons showing improved performance over the current approach.

### Open Question 3
- Question: How can SeMOPO be adapted to handle environments with dynamic distractors that change over time?
- Basis in paper: [explicit] The paper mentions that generalization experiments show SeMOPO's ability to handle variations between online and offline environmental disturbances, but does not explore dynamic distractors.
- Why unresolved: The current implementation and experiments focus on static distractors, and there is no discussion of how the method would perform with time-varying noise patterns.
- What evidence would resolve it: Evaluating SeMOPO on datasets with time-varying distractors and developing modifications to the model to explicitly account for temporal changes in the exogenous components.

## Limitations
- The EX-BMDP assumption requires strong independence between endogenous and exogenous states that may not hold in real-world scenarios
- Conservative sampling's effectiveness depends heavily on dataset diversity - if behavioral policies have similar action distributions, the method may not outperform random sampling
- The claimed superiority over POMDP-based uncertainty estimation is theoretical and lacks direct empirical validation

## Confidence

**Confidence labels:**
- SeMOPO's performance advantage over baselines: **Medium** (strong empirical results but limited baseline comparison)
- Conservative sampling improving state decomposition: **Medium** (supported by entropy analysis but lacking ablation on sampling methods)
- EX-BMDP theoretical bounds being tighter than POMDP: **Low** (theoretical claim without direct empirical comparison)

## Next Checks

1. **Ablation study on sampling strategies**: Compare conservative sampling against alternative trajectory selection methods (e.g., entropy-based filtering, cluster-based sampling) to isolate the specific mechanism by which CS improves state decomposition.

2. **Controlled experiment on state independence**: Generate synthetic datasets with varying degrees of endogenous-exogenous correlation to test the method's robustness when the EX-BMDP assumption is violated.

3. **Direct POMDP comparison**: Implement a comparable POMDP-based uncertainty estimation method using the same computational budget and ensemble size to empirically validate whether endogenous-only uncertainty estimation provides measurable advantages.