---
ver: rpa2
title: 'Spirit LM: Interleaved Spoken and Written Language Model'
arxiv_id: '2402.05755'
source_url: https://arxiv.org/abs/2402.05755
tags:
- speech
- text
- tokens
- pirit
- spirit
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SPIRIT LM, a multimodal language model that
  can generate both text and speech in a cross-modal manner. The model is based on
  a 7B pretrained text language model (LLaMA 2) that is continuously trained on text
  and speech units.
---

# Spirit LM: Interleaved Spoken and Written Language Model

## Quick Facts
- arXiv ID: 2402.05755
- Source URL: https://arxiv.org/abs/2402.05755
- Reference count: 35
- Primary result: Introduces a multimodal language model that generates both text and speech with cross-modal capabilities

## Executive Summary
Spirit LM is a novel multimodal language model that bridges spoken and written language generation. Built by extending LLaMA 2 with continuous training on interleaved text and speech units, it achieves cross-modal generation capabilities where speech can be converted to text and vice versa. The model comes in two variants - BASE using phonetic units and EXPRESSIVE incorporating pitch and style information. A key innovation is the introduction of the Speech-Text Sentiment Preservation (STSP) benchmark for evaluating cross-modal sentiment transfer.

## Method Summary
The model architecture extends a pretrained 7B text language model (LLaMA 2) through continuous training on interleaved sequences of text and speech units. Speech is processed using HuBERT for phonetic representations, with the EXPRESSIVE variant additionally incorporating pitch and style units. Training involves word-level interleaving of speech and text sequences, allowing the model to learn aligned representations across modalities. The model supports few-shot learning across ASR, TTS, and speech classification tasks, with performance evaluated using both traditional metrics and the newly proposed STSP benchmark.

## Key Results
- Cross-modal generation capability between speech and text with preservation of semantic content
- Two model variants: BASE (phonetic units) and EXPRESSIVE (phonetic + pitch/style units)
- Introduction of STSP benchmark for measuring sentiment preservation across modalities
- Demonstrated few-shot learning abilities across speech recognition, synthesis, and classification tasks

## Why This Works (Mechanism)
The interleaved training approach allows the model to learn aligned representations between speech and text at the word level. By continuously training a strong text baseline (LLaMA 2) on combined modalities, the model inherits semantic capabilities while acquiring speech generation abilities. The word-level interleaving ensures fine-grained alignment between modalities, while the continuous training on additional data helps adapt the model to multimodal generation.

## Foundational Learning
- Text language modeling: Required for semantic understanding and generation; quick check: evaluate perplexity on text-only tasks
- Speech representation learning: Needed for capturing phonetic and expressive features; quick check: compare HuBERT vs other speech encoders
- Cross-modal alignment: Essential for preserving meaning across speech-text conversion; quick check: measure semantic similarity before/after conversion
- Few-shot learning: Critical for practical deployment; quick check: test performance with varying shot counts
- Expressivity modeling: Important for capturing pitch and style; quick check: subjective evaluation of generated speech naturalness
- Sentiment preservation: Key metric for cross-modal quality; quick check: compare sentiment scores across modalities

## Architecture Onboarding

Component Map: Input Text/Speech -> Tokenization -> Interleaved Sequence -> Continuous Training -> Multimodal Generation

Critical Path: Text/Speech Input → Tokenization (HuBERT for speech) → Interleaving → Cross-modal training → Generation

Design Tradeoffs: The continuous training approach vs. full multimodal pretraining, phonetic vs. expressive representations, word-level vs. other granularities for interleaving

Failure Signatures: Loss of expressivity in speech generation, sentiment misalignment during cross-modal conversion, degradation in text-only performance

First Experiments:
1. Ablation study comparing continued pretraining vs. interleaved training
2. Sentiment preservation evaluation across different expressiveness levels
3. Few-shot learning performance comparison across ASR, TTS, and classification tasks

## Open Questions the Paper Calls Out
None specified in the source material.

## Limitations
- Heavy reliance on continuous training without full ablation studies
- Newly introduced STSP benchmark lacks standardization and independent validation
- Limited rigorous quantification of expressivity preservation beyond sentiment
- No analysis of potential biases introduced by the interleaving approach

## Confidence

Model architecture and training approach: High confidence
Cross-modal generation capabilities: Medium confidence
Few-shot learning across modalities: Medium confidence
Expressivity preservation claims: Low confidence
Evaluation benchmark validity: Low confidence

## Next Checks

1. Conduct ablation studies comparing SPIRIT LM performance against a baseline that undergoes the same amount of continued pretraining without interleaved speech-text training to isolate the contribution of the interleaving approach.

2. Implement third-party validation of the STSP benchmark using independent datasets to verify its reliability and sensitivity in measuring cross-modal sentiment preservation.

3. Perform comprehensive error analysis on failure cases across different modalities and tasks, including analysis of expressivity loss, to better understand the model's limitations and failure modes.