---
ver: rpa2
title: 'CoXQL: A Dataset for Parsing Explanation Requests in Conversational XAI Systems'
arxiv_id: '2406.08101'
source_url: https://arxiv.org/abs/2406.08101
tags:
- table
- filter
- parsing
- operations
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces CoXQL, the first dataset for parsing explanation
  requests in conversational XAI systems, containing 1,179 user questions and SQL-like
  parses for 31 intents, seven with multiple slots. The authors enhance the multi-prompt
  parsing approach with template checks (MP+), improving parsing accuracy significantly.
---

# CoXQL: A Dataset for Parsing Explanation Requests in Conversational XAI Systems

## Quick Facts
- arXiv ID: 2406.08101
- Source URL: https://arxiv.org/abs/2406.08101
- Reference count: 12
- Llama3-70B achieves best parsing accuracy on CoXQL dataset

## Executive Summary
This paper introduces CoXQL, the first dataset for parsing explanation requests in conversational XAI systems. The dataset contains 1,179 user questions mapped to SQL-like parses across 31 intents, with seven intents requiring multiple slots. The authors propose an enhanced multi-prompt parsing approach with template checks (MP+) that significantly improves parsing accuracy compared to previous methods. Evaluations on seven LLMs demonstrate that MP+ outperforms both guided decoding and vanilla multi-prompt parsing, with Llama3-70B achieving the best results. However, operations requiring multiple slots remain highly challenging for LLMs, highlighting the dataset's value as a benchmark for future research.

## Method Summary
The paper addresses parsing explanation requests in conversational XAI systems by framing it as a text-to-SQL-like task. The authors propose MP+, an enhanced multi-prompt parsing approach that first determines the main operation through coarse-grained demonstrations, then applies fine-grained operation-specific prompts with template validation. This template checking ensures generated SQL-like queries adhere to expected grammar and slot ordering. The approach is evaluated on seven LLMs (Falcon 1B, Pythia 2.8B, Mistral 7B, Llama3 8B, Llama3 70B, CodeQwen1.5 7B, sqlcoder 7B) using exact match parsing accuracy on the CoXQL test set.

## Key Results
- MP+ improves parsing accuracy significantly compared to guided decoding and vanilla multi-prompt parsing
- Llama3-70B achieves the best results among evaluated LLMs
- Operations requiring multiple slots remain highly challenging for LLMs, creating opportunities for specialized parsing approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multi-prompt parsing with template checks (MP+) outperforms both guided decoding and vanilla multi-prompt parsing on parsing accuracy for XAI explanation requests.
- Mechanism: MP+ first determines the main operation through coarse-grained demonstrations, then applies fine-grained operation-specific prompts with additional template validation to ensure generated SQL-like queries adhere to expected grammar and slot ordering.
- Core assumption: Template checking compensates for the lack of grammar constraints in MP, ensuring correct slot naming and ordering while maintaining flexibility in slot generation.
- Evidence anchors:
  - [abstract]: "We enhance the multi-prompt parsing approach with template checks (MP+), improving parsing accuracy significantly."
  - [section]: "Thus, we improve MP and introduce MP+ that uses additional template checking."
  - [corpus]: Weak evidence - no directly relevant citations found in corpus.
- Break condition: If template checking becomes too restrictive and prevents valid alternative phrasings from being recognized, or if the grammar is too complex for simple template validation to handle effectively.

### Mechanism 2
- Claim: Operations requiring multiple slots remain highly challenging for LLMs, creating opportunities for specialized parsing approaches.
- Mechanism: LLMs struggle to simultaneously recognize the main operation intent and extract multiple associated slots (e.g., topk values, method names) from user questions, leading to incomplete or incorrect parses.
- Core assumption: The difficulty increases exponentially with the number of slots that need to be extracted, as LLMs must balance multiple extraction tasks simultaneously.
- Evidence anchors:
  - [abstract]: "However, operations requiring multiple slots remain highly challenging for LLMs, highlighting the dataset's value as a benchmark for future research."
  - [section]: "Table 10 reveals that when additional slots are available for operations, LLMs exhibit limitations in fully accurately recognizing every slot."
  - [corpus]: Weak evidence - no directly relevant citations found in corpus.
- Break condition: If new LLMs with improved multi-task capabilities emerge, or if specialized slot extraction techniques are developed that can handle multiple slots more effectively.

### Mechanism 3
- Claim: CoXQL serves as a benchmark for future research by providing a comprehensive dataset covering 31 intents, seven of which require filling multiple slots.
- Mechanism: The dataset's diversity in intents and slot complexity creates a challenging test bed that reveals current LLM limitations and drives improvements in explanation request parsing.
- Core assumption: Having a diverse, well-annotated dataset with varying complexity levels enables systematic evaluation and comparison of different parsing approaches.
- Evidence anchors:
  - [abstract]: "CoXQL, the first dataset in the NLP domain for user intent recognition in ConvXAI, covering 31 intents, seven of which require filling multiple slots."
  - [section]: "CoXQL comprises 1179 pairs of user questions and corresponding SQL-like queries over full SQL parses, 82 of which were post-processed manually."
  - [corpus]: Weak evidence - no directly relevant citations found in corpus.
- Break condition: If the dataset becomes outdated due to changes in XAI methods or if new datasets emerge that better capture emerging XAI interaction patterns.

## Foundational Learning

- Concept: Text-to-SQL-like task framing for XAI intent recognition
  - Why needed here: This framing allows leveraging established techniques from semantic parsing and natural language interfaces to databases for the XAI domain.
  - Quick check question: What is the primary advantage of treating XAI intent recognition as a text-to-SQL-like task rather than a traditional intent classification problem?

- Concept: Semantic similarity for demonstration selection in guided decoding
  - Why needed here: Selecting relevant demonstrations based on semantic similarity ensures the LLM receives contextually appropriate examples for generating accurate parses.
  - Quick check question: How does semantic similarity-based demonstration selection differ from random or sequential demonstration selection in guided decoding?

- Concept: Template validation for generated parses
  - Why needed here: Template validation ensures the generated SQL-like queries follow the expected structure and slot ordering, improving practical usability.
  - Quick check question: What are the potential risks of implementing overly strict template validation for generated parses?

## Architecture Onboarding

- Component map: User Question Input → LLMs (multiple models with different parsing strategies) → Generated SQL-like Queries → Template Validation (MP+ only) → Parsed Output
- Critical path: User Question → Operation Recognition → Slot Extraction → SQL-like Query Generation → Template Validation (MP+) → Parsed Output
- Design tradeoffs: Flexibility vs. accuracy (MP+ vs. GD), parameter efficiency vs. performance (smaller vs. larger LLMs), comprehensive coverage vs. practical usability (number of operations and slots)
- Failure signatures: Incorrect operation mapping, missing or incorrect slots, template validation failures, semantic mismatch between user intent and generated parse
- First 3 experiments:
  1. Compare MP+ performance across different LLM sizes (1B, 7B, 70B) to identify performance scaling patterns
  2. Test template validation robustness by introducing intentional slot ordering variations in gold parses
  3. Evaluate error patterns for operations with different numbers of slots to quantify the slot complexity impact

## Foundational Learning (continued)

- Concept: Intent recognition in conversational XAI systems
  - Why needed here: Understanding the specific challenges of XAI intent recognition (domain specificity, multiple XAI methods) is crucial for developing effective parsing approaches.
  - Quick check question: How does the specificity of the XAI domain affect the generalizability of intent recognition approaches compared to more general conversational AI systems?

- Concept: Few-shot prompting for LLM-based parsing
  - Why needed here: Few-shot prompting enables effective parsing without extensive fine-tuning, making the approach more practical and adaptable.
  - Quick check question: What factors determine the optimal number of demonstrations to include in few-shot prompts for parsing tasks?

- Concept: Error analysis at category and instance levels
  - Why needed here: Detailed error analysis helps identify systematic weaknesses in parsing approaches and guides targeted improvements.
  - Quick check question: How does error analysis at the category level differ from instance-level analysis in terms of insights gained and practical implications?

## Open Questions the Paper Calls Out

- **Question**: How does CoXQL's performance compare to multilingual datasets when adapted for non-English languages?
  - Basis in paper: [inferred] The paper states CoXQL currently supports only English and mentions feasibility of adapting it through translation.
  - Why unresolved: The paper does not provide any experimental results or analysis on multilingual adaptation of CoXQL.
  - What evidence would resolve it: Comparative evaluation of CoXQL's parsing accuracy when translated to other languages versus existing multilingual text-to-SQL datasets.

- **Question**: What is the impact of increasing operation complexity (e.g., JOINs, aggregations) on parsing accuracy in CoXQL?
  - Basis in paper: [explicit] The paper notes that CoXQL's complexity might be lower compared to other text-to-SQL datasets that involve complex SQL grammar.
  - Why unresolved: The paper does not explore how CoXQL's performance would change with more complex SQL operations.
  - What evidence would resolve it: Experimental results showing parsing accuracy trends as more complex SQL operations are introduced to CoXQL.

- **Question**: How would active learning techniques on smaller-sized LMs compare to larger models like Llama3-70B in terms of parsing accuracy?
  - Basis in paper: [explicit] The paper mentions that while Llama3-70B achieves good results, its deployment may not always be feasible due to resource limitations, suggesting active learning on smaller LMs as a potential solution.
  - Why unresolved: The paper does not provide any experimental comparison between active learning approaches and larger models.
  - What evidence would resolve it: Head-to-head comparison of parsing accuracy between active learning-enhanced smaller LMs and larger pre-trained models on CoXQL.

## Limitations

- The dataset's coverage of 31 intents may not generalize well to emerging or more complex explanation paradigms
- Template checking implementation details remain underspecified, making it difficult to assess whether the approach could be over-constraining
- Performance claims rely heavily on exact match accuracy without considering semantic equivalence of parses

## Confidence

- **High Confidence:** The existence of CoXQL as the first dataset for XAI explanation request parsing, and the general observation that multi-slot operations remain challenging for LLMs
- **Medium Confidence:** The specific performance improvements of MP+ over baseline methods, given the lack of detailed template checking specifications
- **Medium Confidence:** The claim that MP+ outperforms guided decoding, as the comparison doesn't account for potential semantic equivalence in parses

## Next Checks

1. Conduct a semantic equivalence analysis comparing MP+ parses with gold parses to determine if exact match accuracy underestimates actual parsing performance
2. Implement and test multiple template checking strategies with varying strictness levels to identify optimal balance between constraint and flexibility
3. Evaluate CoXQL's generalizability by testing whether parsing approaches trained on this dataset transfer effectively to newer XAI methods not represented in the original data