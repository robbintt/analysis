---
ver: rpa2
title: 'SkillTree: Explainable Skill-Based Deep Reinforcement Learning for Long-Horizon
  Control Tasks'
arxiv_id: '2411.12173'
source_url: https://arxiv.org/abs/2411.12173
tags:
- learning
- skill
- decision
- policy
- tree
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: SkillTree introduces a hierarchical framework that addresses the
  explainability challenge in deep reinforcement learning by reducing complex continuous
  action spaces into discrete skill spaces. The approach integrates a differentiable
  decision tree within a high-level policy to generate skill embeddings that guide
  low-level policy execution, enabling skill-level explainability.
---

# SkillTree: Explainable Skill-Based Deep Reinforcement Learning for Long-Horizon Control Tasks

## Quick Facts
- arXiv ID: 2411.12173
- Source URL: https://arxiv.org/abs/2411.12173
- Authors: Yongyan Wen; Siyuan Li; Rongchang Zuo; Lei Yuan; Hangyu Mao; Peng Liu
- Reference count: 13
- One-line primary result: SkillTree achieves 3.06 average completed subtasks in Kitchen MKBL with 64 leaf nodes while providing skill-level explainability

## Executive Summary
SkillTree addresses the explainability challenge in deep reinforcement learning for long-horizon continuous control tasks by introducing a hierarchical framework that reduces continuous action spaces to discrete skill spaces. The method integrates a differentiable decision tree within a high-level policy to generate skill embeddings that guide low-level policy execution, enabling skill-level explainability while maintaining competitive performance. Through experimental evaluation on robotic manipulation tasks, SkillTree demonstrates effectiveness in sparse reward environments with clear skill-level insights into the decision-making process.

## Method Summary
SkillTree employs a hierarchical reinforcement learning approach that learns discrete skill embeddings from offline trajectory data using a VQ-VAE architecture. A differentiable decision tree serves as the high-level policy, selecting skill indices that index into a learned codebook. The low-level policy executes actions conditioned on both state observations and the selected skill embedding. After training, the soft decision tree is distilled into a hard decision tree to improve interpretability while preserving performance. The method is evaluated on Kitchen MKBL/MLSH, CALVIN, and Office robotic manipulation tasks.

## Key Results
- Achieves 3.06 average completed subtasks in Kitchen MKBL with only 64 leaf nodes in the distilled decision tree
- Matches neural network based methods' performance while providing skill-level explainability
- Demonstrates effectiveness in long-horizon sparse reward tasks with clear decision path visualization

## Why This Works (Mechanism)

### Mechanism 1
- Discrete skill representation reduces continuous action space complexity while maintaining sufficient expressiveness for long-horizon tasks
- VQ-VAE-based skill learning learns a fixed codebook of D-dimensional skill embeddings from offline trajectory segments
- During downstream policy learning, the high-level decision tree selects skill indices that constrain the policy to choose from discrete skill options rather than continuous actions

### Mechanism 2
- Decision tree regularization provides skill-level explainability while maintaining competitive performance
- Differentiable soft decision tree operates as high-level policy, making probabilistic decisions based on observation features
- Tree structure provides clear decision paths that map observations to specific skills, creating transparent skill selection logic

### Mechanism 3
- Distillation of soft decision tree to hard decision tree maintains performance while improving interpretability
- After training soft decision tree policy, trajectories are sampled to train CART decision tree that mimics soft tree's behavior
- Converts probabilistic decisions to deterministic splits while preserving underlying skill selection logic

## Foundational Learning

- Concept: Hierarchical reinforcement learning with temporal abstraction
  - Why needed here: Long-horizon tasks require planning at multiple time scales; skills provide temporal abstraction needed to break down complex tasks
  - Quick check question: How does skill-based RL differ from standard RL in terms of action space and decision frequency?

- Concept: Differentiable decision trees and soft decision boundaries
  - Why needed here: Standard decision trees are non-differentiable and cannot be trained end-to-end with neural networks; soft decision trees enable gradient-based optimization
  - Quick check question: What mathematical modification allows decision trees to become differentiable?

- Concept: Vector quantization for discrete representation learning
  - Why needed here: VQ-VAE provides mechanism to learn discrete skill embeddings from continuous trajectory data while maintaining reconstruction capability
  - Quick check question: How does the commitment loss in VQ-VAE prevent the codebook from becoming unstable during training?

## Architecture Onboarding

- Component map: State → High-level DT → Skill index → Codebook lookup → Skill embedding → Low-level policy → Action
- Critical path: The high-level decision tree is the critical component for explainability
- Design tradeoffs:
  - Skill granularity vs. codebook size: More skills provide finer control but increase tree complexity
  - Tree depth vs. interpretability: Deeper trees can model complex logic but become harder to understand
  - Discrete vs. continuous skills: Discrete skills enable decision tree usage but may lose some expressiveness
- Failure signatures:
  - Low performance with simple tree structure → Skills not capturing essential temporal patterns
  - High variance in skill selection → Decision tree not learning stable decision boundaries
  - Poor distillation results → Soft and hard tree decision boundaries diverge significantly
- First 3 experiments:
  1. Validate skill learning: Train VQ-VAE on Kitchen dataset, visualize skill embeddings and reconstruction quality
  2. Test high-level policy: Train soft decision tree policy on simple task, analyze decision paths
  3. Evaluate distillation: Compare soft and hard tree performance on validation set, check if decision boundaries align

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does SkillTree's performance scale with increasing task complexity and longer horizons compared to neural network based approaches?
- Basis in paper: The paper mentions that SkillTree is designed to handle long-horizon tasks and achieves performance comparable to neural network based methods, but does not provide detailed comparative results for varying task complexities.
- Why unresolved: The experiments focus on specific tasks without systematically varying task complexity or horizon length.
- What evidence would resolve it: Comprehensive experiments testing SkillTree on tasks with varying complexity and horizon lengths, comparing performance against neural network baselines.

### Open Question 2
- Question: What is the impact of the discrete skill representation on the overall interpretability of the decision-making process, especially in scenarios with overlapping or ambiguous skills?
- Basis in paper: The paper discusses the benefits of discrete skill representation for interpretability but does not address scenarios where skills might overlap or be ambiguous.
- Why unresolved: The analysis of skill-level interpretability is limited to specific tasks without exploring edge cases or ambiguous skill scenarios.
- What evidence would resolve it: Empirical studies showing how SkillTree handles overlapping or ambiguous skills, including visualizations and explanations of decision paths in such cases.

### Open Question 3
- Question: How does the choice of hyperparameters, such as the depth of the decision tree and the size of the codebook, affect the performance and interpretability of SkillTree?
- Basis in paper: The paper mentions setting the depth of the tree to 6 and the codebook size K to 16 or 8, but does not explore the impact of these choices on performance or interpretability.
- Why unresolved: The experiments use fixed hyperparameters without exploring their sensitivity or impact on the model's behavior.
- What evidence would resolve it: Ablation studies varying the depth of the decision tree and the size of the codebook, analyzing the resulting changes in performance and interpretability.

## Limitations
- Performance improvements demonstrated only on specific robotic manipulation tasks without broader domain validation
- Decision tree distillation process lacks detailed validation metrics showing quality of approximation
- Method depends on availability of high-quality task-agnostic datasets, limiting real-world applicability

## Confidence
- **Confidence: Medium** - Claims about general applicability to long-horizon tasks require broader validation across diverse domains
- **Confidence: Low** - Decision tree distillation quality needs quantitative validation metrics
- **Confidence: Medium** - Skill learning module's dependence on high-quality datasets introduces uncertainty about real-world applicability

## Next Checks
1. Measure distributional similarity between soft and hard decision tree predictions using KL divergence to quantify distillation quality
2. Visualize and cluster learned skill embeddings using t-SNE or UMAP to verify distinct, interpretable behaviors
3. Evaluate SkillTree on non-robotic tasks (e.g., Atari games, navigation) to test domain generalization beyond manipulation tasks