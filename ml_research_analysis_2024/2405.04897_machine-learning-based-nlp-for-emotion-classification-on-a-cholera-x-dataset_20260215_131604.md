---
ver: rpa2
title: Machine Learning-based NLP for Emotion Classification on a Cholera X Dataset
arxiv_id: '2405.04897'
source_url: https://arxiv.org/abs/2405.04897
tags:
- sentiment
- https
- cholera
- bert
- public
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study applied ML-based NLP techniques to analyze public sentiment
  expressed in social media posts about the Cholera outbreak in Hammanskraal, South
  Africa. A dataset of 23,000 tweets was collected and pre-processed.
---

# Machine Learning-based NLP for Emotion Classification on a Cholera X Dataset

## Quick Facts
- arXiv ID: 2405.04897
- Source URL: https://arxiv.org/abs/2405.04897
- Reference count: 0
- Primary result: LSTM model achieved 75% accuracy in classifying six emotions in tweets about cholera outbreak

## Executive Summary
This study applies machine learning and natural language processing techniques to analyze public sentiment expressed in social media posts about the Cholera outbreak in Hammanskraal, South Africa. A dataset of 23,000 tweets was collected and processed using pysentimiento library to label emotions based on Ekman's basic wheel of emotions. Four ML models - LSTM, Logistic Regression, Decision Tree, and BERT - were trained and evaluated for emotion classification, with LSTM achieving the highest accuracy of 75%. The research demonstrates the potential of ML-based NLP for understanding public emotions during health crises, which can inform public health strategies and interventions.

## Method Summary
The research collected 23,000 tweets related to the Cholera outbreak in Hammanskraal using specific hashtags from April to July 2023. Tweets were preprocessed, tokenized, and padded before being labeled with six emotions (disgust, joy, fear, sadness, anger, surprise) using the pysentimiento library. The dataset was split into 80% training and 20% test sets. Four ML models - LSTM, Logistic Regression, Decision Tree, and BERT - were trained on the balanced dataset and evaluated using accuracy, precision, recall, and F1-score metrics.

## Key Results
- LSTM model achieved highest accuracy of 75% for emotion classification
- Dataset labeled with six emotions using pysentimiento library based on Ekman's basic wheel
- Logistic Regression, Decision Tree, and BERT models showed lower performance than LSTM
- Results demonstrate potential of ML-based NLP for public health sentiment analysis

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Emotion labeling via pysentimiento using Ekman's basic wheel enables scalable emotion extraction from noisy Twitter text.
- Mechanism: The pysentimiento library applies a pre-trained sentiment analyzer to each tweet, returning a probability distribution over Ekman's six emotions. The emotion with the highest probability (excluding "others") is selected as the label, converting unstructured text into structured emotion classes.
- Core assumption: Tweet text contains sufficient emotional cues for pysentimiento to assign meaningful emotion probabilities.
- Evidence anchors:
  - [abstract] "The Python Natural Language Toolkit (NLTK) sentiment analyzer library was applied to determine the emotional significance of each text."
  - [section] "The pysentimiento library was used to analyze emotion within tweets. Utilizing Ekman's basic wheel of emotions, the objective was to discern six specific emotions within a corpus of tweets."
  - [corpus] Weak - related papers discuss emotion detection from social media, but none mention pysentimiento or this specific labeling approach.
- Break condition: If tweets are too short, ambiguous, or contain sarcasm/irony, the probability distribution may be unreliable, leading to noisy labels.

### Mechanism 2
- Claim: LSTM's gating architecture captures long-term dependencies in tweet sequences, enabling better emotion classification than simpler models.
- Mechanism: LSTM layers use forget, input, and output gates to selectively retain or discard information across time steps. This allows the model to remember relevant context from earlier in a tweet while ignoring noise, improving emotion classification accuracy.
- Core assumption: Emotion cues in tweets are distributed across multiple tokens and require memory of prior context to interpret correctly.
- Evidence anchors:
  - [abstract] "LSTM achieved the highest accuracy of 75%."
  - [section] "LSTMs address the vanishing gradient problem in traditional RNNs by introducing gating mechanisms. These mechanisms allow LSTMs to selectively retain and forget information over long periods."
  - [corpus] Moderate - several related papers use RNNs/LSTMs for emotion detection, suggesting this approach is common but not specific to cholera tweets.
- Break condition: If emotion cues are localized to single words or short phrases, the memory capacity of LSTM may be unnecessary and add computational overhead.

### Mechanism 3
- Claim: Dataset balancing via oversampling the minority class prevents model bias toward majority emotion classes.
- Mechanism: The minority class is duplicated using resampling until its size matches the majority class. This creates a balanced training set where the model cannot simply predict the majority class to achieve high accuracy.
- Core assumption: The original class distribution is imbalanced and would cause the model to favor majority classes during training.
- Evidence anchors:
  - [section] "The data was balanced for sentiment analysis using a two-step approach. Initially, the dataset was split into training and test sets. Within the training set, both majority and minority classes were identified based on the target variable. The minority class underwent an oversampling process to address the class imbalance."
  - [abstract] No direct mention of balancing, but the balanced performance across classes in results suggests it was effective.
  - [corpus] Weak - related papers mention class imbalance in sentiment analysis but don't describe the specific oversampling technique used here.
- Break condition: If the original distribution is already balanced or if oversampling creates unrealistic duplicates that the model learns to overfit.

## Foundational Learning

- Concept: Text preprocessing and tokenization
  - Why needed here: Raw Twitter text contains noise (URLs, hashtags, emojis, slang) that must be cleaned and converted to numerical sequences for ML models to process.
  - Quick check question: What preprocessing steps were applied to the tweets before emotion labeling?

- Concept: One-hot encoding of categorical labels
  - Why needed here: Machine learning models require numerical input, so emotion categories must be converted to binary vectors for training.
  - Quick check question: How were the emotion labels transformed for model training?

- Concept: Performance metrics for classification
  - Why needed here: Accuracy alone doesn't reveal class-specific performance; precision, recall, and F1-score provide a complete picture of model effectiveness across all emotion classes.
  - Quick check question: Which metrics were used to evaluate the emotion classification models?

## Architecture Onboarding

- Component map: Data collection (Twitter API + Tweepy) → Preprocessing (cleaning, tokenization) → Emotion labeling (pysentimiento) → Dataset balancing (oversampling) → Model training (LSTM, LR, DT, BERT) → Evaluation (accuracy, precision, recall, F1-score)
- Critical path: Data collection → preprocessing → emotion labeling → model training → evaluation
- Design tradeoffs: LSTM provides highest accuracy but requires more computational resources than LR or DT. BERT offers strong performance but is pre-trained and may not capture cholera-specific language patterns as well as LSTM fine-tuned on the dataset.
- Failure signatures: Low precision indicates many false positives; low recall indicates missed true positives. Poor performance on minority classes suggests class imbalance issues.
- First 3 experiments:
  1. Train and evaluate each model (LSTM, LR, DT, BERT) on the unbalanced dataset to establish baseline performance.
  2. Apply oversampling to balance the training data and retrain all models to measure improvement.
  3. Perform ablation study by removing preprocessing steps (e.g., stopword removal) to identify which steps contribute most to model performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do sentiment dynamics evolve over time during different stages of cholera outbreaks, and can temporal patterns inform targeted public health interventions?
- Basis in paper: [inferred] The authors mention that future research could conduct longitudinal studies over multiple cholera outbreaks to reveal temporal patterns of emotions and sentiments.
- Why unresolved: The current study only analyzed a single cholera outbreak dataset, providing a snapshot of public sentiment rather than temporal dynamics.
- What evidence would resolve it: A longitudinal study tracking sentiment changes across multiple cholera outbreaks, correlating emotional trends with outbreak stages, public health measures, and media coverage.

### Open Question 2
- Question: How does the inclusion of multimodal data (images, videos, user engagement metrics) alongside text improve emotion classification accuracy for health crisis sentiment analysis?
- Basis in paper: [inferred] The authors suggest exploring integration of different data modalities like images, videos, and user engagement metrics with text-based sentiment analysis in future research.
- Why unresolved: The current study only analyzed text data from tweets, not considering other potential indicators of emotional response.
- What evidence would resolve it: Comparative analysis of emotion classification models using only text data versus multimodal data, measuring improvements in accuracy, recall, and F1-scores.

### Open Question 3
- Question: How do cultural, socioeconomic, and geographic factors influence the expression and detection of emotions in social media responses to cholera outbreaks?
- Basis in paper: [inferred] The authors cite research on cultural influences in cholera transmission and local perceptions, suggesting contextual factors affect public response.
- Why unresolved: The study did not analyze how demographic or regional differences impact emotion expression patterns in the dataset.
- What evidence would resolve it: Comparative sentiment analysis across different geographic regions, cultural contexts, and socioeconomic groups affected by cholera outbreaks, identifying significant variations in emotional responses.

## Limitations
- Emotion labeling using pysentimiento may not accurately capture nuanced emotional expressions in context-specific cholera-related tweets
- Small dataset size (23,000 tweets) for six-class classification may limit generalizability of results
- Absence of hyperparameter optimization for all models makes it difficult to determine true architectural advantages

## Confidence
- Emotion labeling accuracy: Low - pysentimiento's performance on cholera-specific text is uncertain
- Model performance comparison: Medium - 75% accuracy is reasonable but lacks statistical significance testing
- Generalizability: Medium - results may not extend to different health crises or cultural contexts

## Next Checks
1. Conduct a small-scale manual validation of 200 randomly sampled tweets to assess the accuracy of pysentimiento emotion labels and calculate inter-rater reliability scores.
2. Perform 5-fold cross-validation with statistical significance testing to determine whether the LSTM's 75% accuracy is significantly different from the other models.
3. Analyze temporal patterns in emotion distributions throughout the data collection period to identify whether public sentiment shifted during different phases of the cholera outbreak.