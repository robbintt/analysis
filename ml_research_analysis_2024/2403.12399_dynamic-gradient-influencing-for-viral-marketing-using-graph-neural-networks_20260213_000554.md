---
ver: rpa2
title: Dynamic Gradient Influencing for Viral Marketing Using Graph Neural Networks
arxiv_id: '2403.12399'
source_url: https://arxiv.org/abs/2403.12399
tags:
- node
- influence
- meta
- budget
- nodes
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Dynamic Viral Marketing (DVM), a data-driven
  optimization problem for maximizing product adoption in attributed social networks
  using Graph Neural Networks (GNNs) as the propagation model. The problem seeks to
  find minimum budget and minimal dynamic topological/attribute changes to reach a
  spread goal under referral and co-marketing constraints.
---

# Dynamic Gradient Influencing for Viral Marketing Using Graph Neural Networks

## Quick Facts
- arXiv ID: 2403.12399
- Source URL: https://arxiv.org/abs/2403.12399
- Reference count: 40
- This paper introduces Dynamic Viral Marketing (DVM), a data-driven optimization problem for maximizing product adoption in attributed social networks using Graph Neural Networks (GNNs) as the propagation model.

## Executive Summary
This paper introduces Dynamic Viral Marketing (DVM), a data-driven optimization problem for maximizing product adoption in attributed social networks using Graph Neural Networks (GNNs) as the propagation model. The problem seeks to find minimum budget and minimal dynamic topological/attribute changes to reach a spread goal under referral and co-marketing constraints. The authors prove DVM is NP-Hard and relate it to influence maximization. They develop Dynamic Gradient Influencing (DGI), a framework that uses gradient ranking to target low-budget, high-influence non-adopters in discrete steps. DGI employs efficient budget computation via bisection search and a novel Meta Influence heuristic with Meta Attribute Flips to enhance node potency.

## Method Summary
The Dynamic Gradient Influencing (DGI) framework addresses the DVM problem by using gradient information to guide discrete edge and feature perturbations. It computes gradients of a loss function with respect to edge and feature matrices, ranks these gradients, and uses bisection search to find the minimal set of perturbations needed to flip a non-adopter node. The framework also introduces Meta Influence, which uses Meta Attribute Flips to temporarily enhance an adopter's features, recompute gradients, and measure downstream influence on non-adopters. Node selection is based on minimum budget and maximum Meta Influence.

## Key Results
- DGI achieves average gains of 24% on budget compared to multiple baselines
- DGI achieves average gains of 37% on AUC compared to multiple baselines
- DGI is evaluated on three real-world attributed networks (Flixster, Epinions, Ciao) with both GCN and GraphSAGE backbones

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Gradient-guided node flipping selects perturbations that most efficiently flip non-adopters.
- Mechanism: The framework computes non-negative gradients on edge and feature perturbations with respect to a loss that measures the gap between current and target labels. It ranks these perturbations and applies the top ones to minimize the budget required for flipping a node.
- Core assumption: First-order gradients provide a good approximation for the minimal set of discrete perturbations needed to cross the decision boundary.
- Evidence anchors:
  - [abstract]: "DGI uses gradient ranking to find optimal perturbations and targets low-budget and high influence non-adopters in discrete steps."
  - [section]: "While it is an NP-Hard combinatorial optimization problem to find the minimal perturbation that flips a node, given that the adjacency and feature matrices are both discrete, first-order gradients work well enough in practice to find the required perturbations."
- Break condition: If the GNN decision boundary is highly non-linear or the perturbation space is very sparse, gradients may not point toward feasible discrete flips.

### Mechanism 2
- Claim: Meta Influence captures long-range effects of perturbations and guides selection of high-impact non-adopters.
- Mechanism: Meta Attribute Flips temporarily enhance an adopter's features, recompute gradients, and measure the resulting outgoing edge influence on non-adopters. Nodes with high normalized gradient scores on these outgoing edges are considered to have high Meta Influence.
- Core assumption: Feature changes that align with the GNN's classifier weights increase the dynamic outgoing edge influence, thus increasing the likelihood of downstream cascades.
- Evidence anchors:
  - [abstract]: "develops the 'Meta-Influence' heuristic for assessing a node's downstream influence."
  - [section]: "Meta Influence uses Meta Attribute Flips which are feature perturbations that increase the potency of outgoing edge perturbations at an adopter node."
- Break condition: If the GNN's attention weights are uniform or the graph structure is very sparse, Meta Influence may not differentiate effectively between nodes.

### Mechanism 3
- Claim: Budget computation via bisection search efficiently identifies the minimal set of perturbations needed to flip a node.
- Mechanism: For each candidate node, gradients are computed once and sorted. A bisection search over the sorted list finds the smallest prefix of perturbations that flips the node, avoiding repeated gradient computations.
- Core assumption: The order of gradient magnitudes correlates with the order of perturbation importance for crossing the decision boundary.
- Evidence anchors:
  - [section]: "we use the bisection method... we compute the sorted gradients, Ë†ð‘ƒ in Eq. 15 once and run bisection search over these gradients to find the minimal set of perturbations required to convert ð‘£."
- Break condition: If gradient magnitudes are very similar or if the decision boundary is not monotonic in gradient space, bisection search may not find the minimal set efficiently.

## Foundational Learning

- Concept: Graph Neural Networks (GNNs) as non-linear propagation models
  - Why needed here: DVM requires a data-driven model that can capture both topological and attribute information to simulate product adoption dynamics.
  - Quick check question: How does a 2-layer GCN differ from a 1-layer GCN in terms of receptive field and oversmoothing risk?

- Concept: Gradient-based adversarial attacks on discrete structures
  - Why needed here: DGI uses gradient information to guide discrete edge and feature flips under referral and co-marketing constraints.
  - Quick check question: What are the challenges of applying continuous gradient methods to discrete graph perturbations, and how does DGI address them?

- Concept: Influence maximization and cascade modeling
  - Why needed here: DVM is related to influence maximization but seeks dynamic perturbations instead of static seed sets; understanding this connection motivates the DGI approach.
  - Quick check question: In the linear threshold model, what condition must be met for a node to flip, and how does DVM's criterion differ?

## Architecture Onboarding

- Component map: Input graph -> Gradient Computation -> Budget Computation -> Meta Influence -> Node Selection -> Network Update -> Repeat

- Critical path:
  1. Compute gradients for all non-adopters
  2. Use bisection search to find minimal budgets
  3. Select node with lowest budget and highest Meta Influence
  4. Apply perturbations and update network
  5. Repeat until spread goal reached

- Design tradeoffs:
  - Gradient recomputation vs. single-pass sorting: DGI opts for single-pass to save time but may miss dynamic changes.
  - Meta Attribute Flips cost vs. cascade benefit: Thresholding on Meta Influence avoids overspending on low-impact nodes.
  - Bisection search depth vs. accuracy: Setting max bound to max degree balances speed and correctness.

- Failure signatures:
  - Budget grows unbounded or plateaus: Likely Meta Influence thresholding too high or gradient signals too weak.
  - Spread stalls early: Possible GNN overfitting or very sparse graph structure.
  - Excessive recomputation time: Budget hashing not effective due to frequent logit changes.

- First 3 experiments:
  1. Run DGI on a small synthetic graph with known optimal flips; verify budget matches expectation.
  2. Compare DGI with and without Meta Attribute Flips on a medium graph; measure cascade depth and budget.
  3. Vary the Meta Influence threshold (Î²) on a real dataset; plot budget vs. spread to find optimal setting.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different network structures (e.g., tree-like vs. dense) impact the efficiency of DVM and DGI?
- Basis in paper: [explicit] The paper notes that larger-scale datasets are more tree-like while smaller-scale datasets are denser, and observes that propagation happens faster in dense networks with lesser budgets.
- Why unresolved: The paper only qualitatively mentions the difference in network structure but does not provide a detailed quantitative analysis of how structure impacts the spread dynamics or DGI performance.
- What evidence would resolve it: A systematic study varying network density and structure, measuring budget efficiency and spread speed across different topologies.

### Open Question 2
- Question: What is the theoretical bound on the number of Meta Attribute Flips required to maximize downstream influence in DGI?
- Basis in paper: [inferred] The paper uses Meta Attribute Flips to enhance node potency and Meta Influence, but does not provide a theoretical analysis of how many flips are optimal or necessary.
- Why unresolved: The paper uses grid search to find optimal values for the number of Meta Attribute Flips (k) and the influence threshold (Î²), but does not derive theoretical bounds or conditions for their effectiveness.
- What evidence would resolve it: A mathematical analysis or empirical study establishing the relationship between the number of Meta Attribute Flips, the network structure, and the downstream influence.

### Open Question 3
- Question: How does the choice of GNN architecture (e.g., GCN vs. GraphSAGE) affect the performance of DGI in different types of attributed networks?
- Basis in paper: [explicit] The paper compares GCN and GraphSAGE backbones and reports different performance results, but does not provide a deep analysis of why one performs better than the other in specific scenarios.
- Why unresolved: While the paper shows that both GCN and GraphSAGE can be used, it does not analyze the underlying reasons for performance differences or provide guidance on which architecture is better suited for different network characteristics.
- What evidence would resolve it: An analysis of how different GNN architectures interact with network structure and attributes, and how this impacts the propagation model and DGI's effectiveness.

## Limitations
- The framework relies on first-order gradients to guide discrete perturbations, but gradients may not always point toward valid minimal perturbations for complex decision boundaries.
- The effectiveness of Meta Attribute Flips depends heavily on the Meta Influence threshold Î², for which the paper does not provide clear guidance on optimal selection.
- While bisection search is presented as efficient, it still requires multiple forward passes per candidate node, and the claimed efficiency gains need empirical validation.

## Confidence
- **High Confidence**: The problem formulation as an NP-Hard optimization problem and its relationship to influence maximization are well-established theoretical claims.
- **Medium Confidence**: The experimental results showing DGI outperforming baselines are convincing, but lack of implementation details for key components introduces uncertainty.
- **Low Confidence**: The mechanism by which gradients guide discrete perturbations and the effectiveness of Meta Attribute Flips in practice are less certain, relying on empirical observations rather than theoretical guarantees.

## Next Checks
1. **Gradient-to-flip mapping validation**: Create a synthetic graph with known minimal perturbation sets and verify that DGI's gradient-guided approach consistently identifies these minimal sets. Measure the gap between gradient-based and optimal solutions across different graph structures.

2. **Meta Influence threshold sensitivity**: Implement a systematic grid search over Î² values on the Flixster dataset and plot budget vs. spread curves. Identify the optimal threshold range and test whether reported results fall within this range.

3. **Budget computation overhead measurement**: Instrument the DGI implementation to track the number of forward passes required per node selection step. Compare this with the alternative of recomputing gradients for each perturbation to validate the claimed efficiency gains.