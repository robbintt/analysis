---
ver: rpa2
title: 'Grass: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients'
arxiv_id: '2406.17660'
source_url: https://arxiv.org/abs/2406.17660
tags:
- grass
- memory
- update
- gradient
- matrix
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces GRASS, a memory-efficient method for training
  large language models (LLMs) that leverages sparse gradient projections. The key
  idea is to use sparse projection matrices to transform gradients into structured
  sparse updates, significantly reducing memory usage for optimizer states, gradient
  memory footprint, computation, and communication costs.
---

# Grass: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients

## Quick Facts
- arXiv ID: 2406.17660
- Source URL: https://arxiv.org/abs/2406.17660
- Reference count: 40
- Key result: Enables 13B parameter LLaMA pretraining on a single 40GB A100 GPU with 2.5× memory reduction and up to 2× throughput improvement

## Executive Summary
GRASS is a memory-efficient training method for large language models that leverages sparse gradient projections to dramatically reduce memory usage for optimizer states, gradient storage, and communication costs. The approach transforms dense gradients into structured sparse updates using sparse projection matrices, enabling half-precision pretraining of a 13B parameter LLaMA model on a single 40GB A100 GPU—previously infeasible with conventional methods. GRASS achieves competitive performance to full-rank training with minimal perplexity degradation (<0.1) while delivering up to 2× throughput improvement on 8-GPU systems.

## Method Summary
GRASS introduces a novel approach to memory-efficient LLM training by applying sparse projection matrices to gradients before optimization. Unlike traditional gradient sparsification methods that introduce randomness, GRASS uses deterministic, structured sparsity patterns derived from sparse random matrices. This allows for efficient encoding and decoding of gradients while maintaining computational efficiency. The method integrates seamlessly with existing optimizers like Adam and momentum SGD, transforming dense gradients into structured sparse updates that reduce memory footprint for both optimizer states and gradient storage. By maintaining the sparsity pattern across iterations, GRASS achieves consistent memory savings without the overhead of frequent mask updates.

## Key Results
- Enables half-precision pretraining of 13B parameter LLaMA on a single 40GB A100 GPU
- Achieves up to 2× throughput improvement on 8-GPU systems
- Maintains <0.1 perplexity gap compared to full-rank training on 1B parameter LLaMA with 2.5× memory reduction

## Why This Works (Mechanism)
GRASS exploits the observation that gradient updates in LLM training often exhibit redundancy and can be effectively compressed without significant performance loss. By projecting dense gradients through sparse random matrices, GRASS creates structured sparse representations that preserve essential information while eliminating redundancy. The deterministic nature of the sparsity pattern enables efficient encoding/decoding operations and reduces memory overhead for maintaining dynamic sparsity masks. The structured approach also improves computational efficiency by allowing sparse matrix operations that are more cache-friendly than dense operations.

## Foundational Learning

1. **Gradient Sparsification**
   - Why needed: Traditional dense gradient storage becomes prohibitive for large models
   - Quick check: Verify that key gradient information is preserved after sparsification

2. **Random Projection Theory**
   - Why needed: Provides mathematical foundation for preserving gradient information in lower dimensions
   - Quick check: Confirm Johnson-Lindenstrauss type properties hold for the chosen projection matrices

3. **Structured Sparsity Patterns**
   - Why needed: Enables efficient encoding/decoding and reduces computational overhead
   - Quick check: Validate that sparsity pattern remains effective across training iterations

4. **Memory-Computation Tradeoffs**
   - Why needed: Critical for understanding when GRASS provides benefits
   - Quick check: Measure memory savings vs. computational overhead across different model sizes

## Architecture Onboarding

Component Map: Model Parameters -> Dense Gradients -> Sparse Projection -> Structured Sparse Gradients -> Optimizer -> Parameter Update

Critical Path: Forward pass → Backward pass (dense gradients) → Sparse projection → Sparse gradient update → Parameter update

Design Tradeoffs:
- Sparsity level vs. model accuracy: Higher sparsity reduces memory but may impact convergence
- Projection matrix density: Affects computational efficiency of encoding/decoding operations
- Integration with existing optimizers: Must balance simplicity with performance

Failure Signatures:
- Training instability or divergence when sparsity is too aggressive
- Convergence slowdown due to information loss in projection
- Communication bottlenecks in distributed settings if projection is not well-optimized

First Experiments:
1. Compare convergence curves of GRASS vs. full-rank training on a small model
2. Measure memory usage and throughput on different sparsity levels
3. Test gradient reconstruction quality across various projection matrices

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions, focusing instead on demonstrating the effectiveness and efficiency of the GRASS method across various training scenarios.

## Limitations

- Primarily validated on LLaMA architecture; performance on other model architectures (OPT, GPT-2) remains untested
- Results based on 13B parameter model; scalability to 70B+ parameter models uncertain
- Limited evaluation of mixed-precision training beyond half-precision

## Confidence

- Memory savings claim: High (well-validated across experiments)
- Throughput improvement: Medium (primarily tested on 8-GPU setup)
- Generalization to larger models: Low (limited testing on larger scales)
- Performance degradation: High (well-characterized on tested models)

## Next Checks

1. Evaluate GRASS on larger models (70B+ parameters) and verify if the structured sparsity and memory benefits scale proportionally

2. Test GRASS with mixed-precision training (FP16/FP32) and assess its impact on training stability and convergence speed

3. Benchmark GRASS on multi-node GPU clusters (16+ GPUs) to confirm the 2× throughput improvement and identify any communication bottlenecks