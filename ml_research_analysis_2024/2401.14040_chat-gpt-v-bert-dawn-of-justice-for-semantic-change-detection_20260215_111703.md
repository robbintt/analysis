---
ver: rpa2
title: '(Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection'
arxiv_id: '2401.14040'
source_url: https://arxiv.org/abs/2401.14040
tags:
- chat
- chatgpt
- bert
- semantic
- histowic
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This study evaluates ChatGPT (and GPT) 3.5 for detecting semantic
  change, comparing it to BERT models. Two diachronic Word-in-Context (WiC) tasks
  are used: TempoWiC for short-term changes in social media, and HistoWiC for long-term
  changes in historical text.'
---

# (Chat)GPT v BERT: Dawn of Justice for Semantic Change Detection

## Quick Facts
- arXiv ID: 2401.14040
- Source URL: https://arxiv.org/abs/2401.14040
- Authors: Francesco Periti; Haim Dubossarsky; Nina Tahmasebi
- Reference count: 16
- Primary result: ChatGPT performs worse than GPT-3.5-turbo and both underperform BERT for semantic change detection, especially for short-term changes

## Executive Summary
This study evaluates ChatGPT and GPT-3.5-turbo for detecting semantic change using diachronic Word-in-Context (WiC) tasks. Two datasets are used: TempoWiC for short-term changes in social media and HistoWiC for long-term changes in historical text. The research tests various prompting strategies (zero-shot, few-shot, many-shot) and temperature settings across ChatGPT Web interface and GPT API. Results consistently show that BERT models outperform both ChatGPT and GPT-3.5-turbo, with ChatGPT performing particularly poorly on short-term semantic change detection tasks.

## Method Summary
The study compares ChatGPT, GPT-3.5-turbo, and BERT on diachronic WiC tasks using TempoWiC (short-term social media changes) and HistoWiC (long-term historical changes) datasets. Multiple prompting strategies are tested: zero-shot (no examples), few-shot (3 examples), and many-shot (full task examples). Temperature values range from 0.0 to 2.0 in 0.2 increments to assess the impact of randomness on performance. The BERT model uses bert-base-uncased with threshold-based classifiers on different layers. Due to API limitations, subsets of the datasets are used for (Chat)GPT evaluation.

## Key Results
- BERT outperforms both ChatGPT and GPT-3.5-turbo on both short-term and long-term semantic change detection
- ChatGPT performs significantly worse than GPT-3.5-turbo on all tasks
- ChatGPT shows slightly better performance on long-term changes (HistoWiC) compared to short-term changes (TempoWiC)
- Lower temperature values (less creativity) improve GPT performance on WiC tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT models perform better on long-term semantic change (HistoWiC) than short-term semantic change (TempoWiC)
- Mechanism: The model's training corpus likely covers historical meanings more comprehensively than rapidly evolving contemporary slang and abbreviations
- Core assumption: GPT's pretraining data includes sufficient historical text to model long-term semantic shifts but lacks coverage of fast-evolving social media language
- Evidence anchors:
  - [abstract] "ChatGPT performs slightly better on long-term changes than short-term, but still underperforms BERT."
  - [section] "This suggests that (Chat)GPT is significantly more effective for long-term change detection than for short-term change detection."
  - [corpus] Weak evidence - no direct citation about training data composition
- Break condition: If training data includes extensive contemporary social media or if the model is fine-tuned on short-term change examples

### Mechanism 2
- Claim: Lower temperature values improve GPT performance on semantic change detection tasks
- Mechanism: Lower temperature reduces randomness, making the model's outputs more consistent and aligned with learned patterns
- Core assumption: The model's pretraining has established stable patterns for semantic understanding that are disrupted by high randomness
- Evidence anchors:
  - [abstract] "Our results indicate that GPT used with low temperature values (i.e., less creativity) is better at handling WiC tasks."
  - [section] "Figures 3 and 4 show that, on average, higher performance is associated with lower temperatures for both TempoWiC and HistoWiC."
  - [corpus] No direct corpus evidence about temperature effects on semantic tasks
- Break condition: If task requires creative interpretation or if high temperature better captures ambiguous semantic boundaries

### Mechanism 3
- Claim: BERT outperforms GPT on semantic change detection because BERT's architecture is better suited for this task
- Mechanism: BERT's bidirectional attention and static embeddings capture semantic relationships more effectively than GPT's autoregressive generation for classification tasks
- Core assumption: The architectural differences between BERT and GPT make BERT more suitable for understanding semantic relationships than generating text
- Evidence anchors:
  - [abstract] "BERT represents a family of models that currently stand as the state-of-the-art for modeling semantic change."
  - [section] "BERT's contextualized embeddings consistently provide a more effective and robust solution for capturing both short- and long-term changes in word meanings."
  - [corpus] Weak evidence - no direct comparison of architectural mechanisms
- Break condition: If task requires generation or if GPT is fine-tuned specifically for semantic change detection

## Foundational Learning

- Concept: Semantic change detection
  - Why needed here: The entire paper evaluates models' ability to detect semantic shifts over time
  - Quick check question: What distinguishes short-term from long-term semantic change in computational terms?

- Concept: Word-in-Context (WiC) task
  - Why needed here: Both TempoWiC and HistoWiC are extensions of the WiC task framework
  - Quick check question: How does WiC binary classification relate to semantic change detection?

- Concept: Temperature hyperparameter in language models
  - Why needed here: The study systematically varies temperature to assess its impact on model performance
  - Quick check question: What range of temperature values is typically used and how does it affect output randomness?

## Architecture Onboarding

- Component map: TempoWiC/HistoWiC datasets -> (Chat)GPT models (Web/API) with prompting strategies -> BERT models -> Macro-F1 evaluation
- Critical path:
  1. Load dataset (TempoWiC/HistoWiC)
  2. Generate predictions using (Chat)GPT with specified prompt strategy and temperature
  3. Format predictions to match WiC label format
  4. Calculate Macro-F1 score
  5. Compare results across models and conditions

- Design tradeoffs:
  - Using subsets of datasets due to API limitations vs. full dataset evaluation
  - Web interface convenience vs. API control over parameters
  - Few-shot prompting for learning vs. zero-shot for generalization

- Failure signatures:
  - Incorrect answer format from (Chat)GPT
  - Temperature settings causing excessive randomness
  - Message history exceeding token limits in API

- First 3 experiments:
  1. Evaluate GPT API on TempoWiC with zero-shot prompting at temperature 0.0
  2. Evaluate ChatGPT Web on HistoWiC with few-shot prompting using full message history
  3. Compare BERT layer 12 performance on both datasets using threshold-based classifier

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What are the specific mechanisms through which ChatGPT's message history impacts its performance in WiC tasks, and how does this compare to the fixed message history limit in the GPT API?
- Basis in paper: [explicit] The paper discusses the differences in message history handling between ChatGPT Web and GPT API, noting that ChatGPT Web seems to handle an unlimited message history, while GPT API is limited to a fixed number of tokens
- Why unresolved: The paper mentions that the influence of message history is significant in shaping the quality of conversations with ChatGPT, but it does not delve into the specific mechanisms or provide a detailed comparison of how these differences impact performance
- What evidence would resolve it: Experiments comparing the performance of ChatGPT Web and GPT API under varying message history conditions, along with a detailed analysis of how message history affects the model's responses in WiC tasks

### Open Question 2
- Question: How does the performance of ChatGPT and BERT in detecting semantic change vary across different languages, and what factors contribute to these variations?
- Basis in paper: [inferred] The paper acknowledges that the results are limited to English and suggests that the behavior of ChatGPT may differ for other languages, but does not explore this aspect
- Why unresolved: The paper does not provide any data or analysis on the performance of ChatGPT and BERT in languages other than English, leaving a gap in understanding how these models generalize across linguistic contexts
- What evidence would resolve it: Comparative studies evaluating ChatGPT and BERT on WiC and LSC tasks in multiple languages, along with an analysis of the factors that contribute to performance differences across languages

### Open Question 3
- Question: What are the potential benefits and limitations of using ChatGPT for detecting semantic change in domains with highly specialized or rapidly evolving language, such as medical or olfactory domains?
- Basis in paper: [explicit] The paper mentions that the computational modeling of semantic change is relevant for out-of-domain texts like medical and olfactory domains, but does not explore the performance of ChatGPT in these specific contexts
- Why unresolved: The paper does not provide any empirical evidence or analysis of ChatGPT's performance in specialized or rapidly evolving language domains, leaving questions about its applicability and effectiveness in these areas
- What evidence would resolve it: Experiments evaluating ChatGPT's performance on WiC and LSC tasks in specialized domains, along with a comparative analysis of its effectiveness relative to domain-specific models or approaches

## Limitations

- Results are limited to English language evaluation without cross-linguistic validation
- Uses dataset subsets due to API constraints, potentially missing full complexity of semantic change detection
- Does not evaluate model's ability to provide interpretable explanations for detected semantic changes

## Confidence

- High confidence: BERT's superior performance on both short-term and long-term semantic change detection
- Medium confidence: The hypothesis that training data coverage explains better long-term vs short-term performance
- Medium confidence: Temperature effects on performance

## Next Checks

1. Test (Chat)GPT models on full dataset versions once API limitations are resolved to verify if performance patterns hold at scale
2. Conduct ablation studies comparing (Chat)GPT with different pretraining data compositions to isolate the impact of training corpus on semantic change detection
3. Evaluate additional prompting strategies and fine-tuning approaches to determine if (Chat)GPT can be adapted for competitive semantic change detection performance