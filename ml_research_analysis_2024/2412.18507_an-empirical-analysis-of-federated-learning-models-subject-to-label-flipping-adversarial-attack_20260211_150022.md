---
ver: rpa2
title: An Empirical Analysis of Federated Learning Models Subject to Label-Flipping
  Adversarial Attack
arxiv_id: '2412.18507'
source_url: https://arxiv.org/abs/2412.18507
tags:
- clients
- adversarial
- accuracy
- each
- federated
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper empirically analyzes label-flipping attacks on federated
  learning models using MNIST dataset. The study compares seven models (MLR, SVC,
  MLP, CNN, Random Forest, XGBoost, LSTM) across 10 and 100 clients, varying adversarial
  client percentages (10-100%) and label-flipping rates (10-100%).
---

# An Empirical Analysis of Federated Learning Models Subject to Label-Flipping Adversarial Attack

## Quick Facts
- **arXiv ID**: 2412.18507
- **Source URL**: https://arxiv.org/abs/2412.18507
- **Reference count**: 40
- **Primary result**: Model accuracy decreases with increased client count; MLR most stable; different models show varying robustness patterns against label-flipping attacks.

## Executive Summary
This paper presents an empirical analysis of label-flipping attacks on federated learning models using the MNIST dataset. The study evaluates seven different models (MLR, SVC, MLP, CNN, Random Forest, XGBoost, LSTM) across two client configurations (10 and 100 clients) while varying adversarial client percentages and label-flipping rates. The research reveals that model accuracy consistently decreases as the number of clients increases, with Multinomial Logistic Regression (MLR) demonstrating the most stability across attack scenarios. The analysis shows that different models exhibit distinct resilience patterns - some are more robust when few clients flip many labels, while others perform better when many clients flip few labels.

## Method Summary
The researchers conducted controlled experiments using the MNIST dataset to evaluate seven federated learning models under various label-flipping attack scenarios. They tested models across 10 and 100 client configurations with adversarial client percentages ranging from 10% to 100%, and label-flipping rates from 10% to 100%. The study systematically varied these parameters to assess model performance degradation under different attack intensities. Results were measured in terms of classification accuracy, with comparisons made across model types to identify which architectures demonstrated greater robustness to label-flipping attacks.

## Key Results
- Model accuracy consistently decreases as the number of clients increases
- MLR (Multinomial Logistic Regression) shows the most stability across attack scenarios
- Different models exhibit distinct resilience patterns based on attack configuration (few clients flipping many labels vs. many clients flipping few labels)

## Why This Works (Mechanism)
This empirical approach works by systematically isolating the impact of label-flipping attacks on federated learning models through controlled parameter variation. The mechanism reveals how different model architectures respond to adversarial behavior in distributed learning environments. By maintaining consistent experimental conditions while varying only the attack parameters, the study effectively demonstrates the relative robustness of each model type and identifies patterns in how models fail under different adversarial scenarios.

## Foundational Learning
- **Federated Learning**: Decentralized machine learning where multiple clients train models collaboratively without sharing raw data. Why needed: Provides context for distributed training scenarios and attack vectors.
- **Label-Flipping Attacks**: Adversarial strategy where malicious clients intentionally corrupt training data by flipping correct labels to incorrect ones. Why needed: Defines the specific attack vector being studied and its impact mechanism.
- **Model Robustness**: The ability of a model to maintain performance under adversarial conditions or data corruption. Why needed: Establishes the key performance metric being evaluated across different architectures.
- **Client Heterogeneity**: Variation in data distribution, model updates, and behavior across different participating clients. Why needed: Explains why different models respond differently to the same attack scenario.
- **Attack Scalability**: How the impact of adversarial behavior changes with the number of attacking clients and intensity of label flipping. Why needed: Demonstrates the relationship between attack scale and model degradation.

## Architecture Onboarding

**Component Map:**
Data Distribution -> Model Training -> Aggregation -> Model Evaluation -> Attack Analysis

**Critical Path:**
The critical path follows data distribution to training to aggregation to evaluation, with attack analysis occurring as a feedback mechanism to assess model robustness at each stage.

**Design Tradeoffs:**
- Simple models (MLR) vs. complex models (CNN, LSTM): Simpler models show more stability but potentially lower accuracy on clean data
- Number of clients vs. model accuracy: More clients increase attack surface but may improve generalization
- Attack intensity vs. detection capability: Higher label-flipping rates are easier to detect but cause more damage

**Failure Signatures:**
- Accuracy degradation proportional to number of adversarial clients
- Different degradation patterns based on which models are used
- Resilience patterns vary between "few clients flipping many labels" vs. "many clients flipping few labels"

**First Experiments:**
1. Test MLR and CNN models on MNIST with 10% adversarial clients flipping 50% of labels
2. Compare accuracy degradation between 10-client and 100-client configurations with identical attack parameters
3. Evaluate Random Forest and XGBoost performance under 25% adversarial clients with 25% label-flipping rate

## Open Questions the Paper Calls Out
None

## Limitations
- Analysis limited to MNIST dataset, restricting generalizability to other domains
- Fixed client counts (10 and 100) may not reflect realistic federated learning deployments
- Assumes uniform data distribution across clients, ignoring typical heterogeneity in federated environments

## Confidence
- **High Confidence**: Model accuracy decreases with increased client count is well-supported by experimental results
- **Medium Confidence**: Comparative robustness rankings of different models across attack scenarios are supported but need validation
- **Low Confidence**: Specific patterns of model resilience (few vs. many clients flipping labels) may be dataset-dependent

## Next Checks
1. Replicate experiments across diverse datasets (CIFAR-10, ImageNet subsets, non-image domains) to assess model performance consistency
2. Test attack scenarios with heterogeneous data distributions and non-uniform client participation patterns
3. Implement and evaluate proposed model selection guidelines against real-world federated learning deployments with actual adversarial behavior patterns