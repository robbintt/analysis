---
ver: rpa2
title: 'The Neutrality Fallacy: When Algorithmic Fairness Interventions are (Not)
  Positive Action'
arxiv_id: '2404.12143'
source_url: https://arxiv.org/abs/2404.12143
tags:
- positive
- action
- discrimination
- algorithmic
- fairness
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper explores when algorithmic fairness interventions in
  machine learning should be interpreted as positive action under EU non-discrimination
  law, rather than measures to avoid discrimination. The authors argue that categorizing
  fair-ml interventions as positive action relies on faulty assumptions about the
  neutrality of algorithmic decision-making, which they term "neutrality fallacies."
  They identify three such fallacies: assuming "data is neutral," "predictive models
  are neutral," and "algorithmic decision-making is neutral." The paper concludes
  that fair-ml interventions aimed at mitigating measurement bias and disparities
  in predictive performance should not necessarily be considered positive action,
  as neither fairness-aware nor unconstrained algorithmic decision-making is value-neutral.'
---

# The Neutrality Fallacy: When Algorithmic Fairness Interventions are (Not) Positive Action

## Quick Facts
- arXiv ID: 2404.12143
- Source URL: https://arxiv.org/abs/2404.12143
- Reference count: 40
- Primary result: Fairness interventions should often be classified as avoiding discrimination rather than positive action due to neutrality fallacies in algorithmic decision-making

## Executive Summary
This paper examines when algorithmic fairness interventions in machine learning should be interpreted as positive action under EU non-discrimination law, rather than measures to avoid discrimination. The authors argue that categorizing fair-ml interventions as positive action relies on faulty assumptions about the neutrality of algorithmic decision-making, which they term "neutrality fallacies." They identify three such fallacies: assuming "data is neutral," "predictive models are neutral," and "algorithmic decision-making is neutral." The paper concludes that fair-ml interventions aimed at mitigating measurement bias and disparities in predictive performance should not necessarily be considered a form of positive action, as neither fairness-aware nor unconstrained algorithmic decision-making is value-neutral.

## Method Summary
The paper employs legal and conceptual analysis to examine the classification of algorithmic fairness interventions under EU non-discrimination law. Through identification of three "neutrality fallacies" (data is neutral, predictive models are neutral, algorithmic decision-making is neutral), the authors analyze how these assumptions impact the legal categorization of fairness interventions. The analysis draws on EU case law, particularly from the Court of Justice of the European Union, and considers the distinction between interventions to avoid discrimination versus those constituting positive action. The paper proposes a shift from negative obligations to avoid harm toward positive obligations to actively prevent harm as a more adequate framework for algorithmic decision-making.

## Key Results
- Fairness interventions addressing measurement bias and performance disparities should not automatically be classified as positive action
- Three neutrality fallacies underlie faulty assumptions about algorithmic fairness: data neutrality, model neutrality, and decision-making neutrality
- Current legal frameworks inadequately address algorithmic discrimination, requiring a shift from negative to positive obligations

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Algorithmic fairness interventions often avoid discrimination rather than constitute positive action because they address measurement bias, not structural inequalities.
- **Mechanism**: When target variables encode implicit bias (e.g., past hiring decisions affected by gender stereotypes), fairness interventions that correct for this bias are seen as neutral measures to avoid discrimination rather than preferential treatment.
- **Core assumption**: The data used to train models is not neutral and contains measurement bias that systematically disadvantages protected groups.
- **Evidence anchors**:
  - [abstract] "fairness interventions aimed at mitigating measurement bias and disparities in predictive performance should not necessarily be considered a form of positive action"
  - [section] "The legal question then becomes whether measurement bias has occurred and to what extent a fairness intervention is an effective and justifiable measure to avoid discrimination arising from that bias"
  - [corpus] Weak - corpus neighbors focus on bias profiles and audit studies but don't directly address measurement bias vs. positive action distinction
- **Break condition**: If measurement bias cannot be empirically distinguished from structural inequalities in the data, interventions may be misclassified as positive action.

### Mechanism 2
- **Claim**: Fair ML interventions targeting predictive performance disparities are justified as non-discrimination measures when they address legitimate technical disparities, not preferential treatment.
- **Mechanism**: When machine learning models systematically underperform for certain protected groups due to underfitting or data distribution differences, interventions like oversampling or group-specific thresholds are technical corrections rather than preferential treatment.
- **Core assumption**: Disparities in predictive performance arise from technical limitations rather than intentional discrimination.
- **Evidence anchors**:
  - [abstract] "neither fairness-aware nor unconstrained algorithmic decision-making is value-neutral"
  - [section] "Even including a protected characteristic explicitly as a feature in a machine learning model could be lawful in some cases"
  - [corpus] Weak - corpus neighbors discuss fairness feedback loops but don't address legal justification for performance-based interventions
- **Break condition**: If performance disparities are used to justify differential treatment that systematically advantages one group over another beyond technical correction.

### Mechanism 3
- **Claim**: Algorithmic decision-making policies are value-laden, so interventions to prevent discriminatory outcomes are necessary rather than optional.
- **Mechanism**: Decision thresholds and optimization objectives encode normative choices about how to weigh different types of errors, making "neutral" algorithmic decisions impossible without intentional intervention.
- **Core assumption**: The status quo decision-making policy is not neutral and may systematically disadvantage protected groups.
- **Evidence anchors**:
  - [abstract] "moving away from a duty to 'not do harm' towards a positive obligation to actively 'do no harm'"
  - [section] "even when patients' no-show risk estimates are accurate and valid, the decision-making policy that informs how we act upon the predictions encodes specific value judgements"
  - [corpus] Weak - corpus neighbors discuss algorithmic fairness but don't address the normative implications of decision-making policies
- **Break condition**: If the decision-making policy is explicitly designed to maintain existing inequalities rather than prevent discrimination.

## Foundational Learning

- **Concept**: Measurement bias in machine learning
  - Why needed here: Understanding how target variables can encode implicit bias is crucial for distinguishing between interventions that avoid discrimination versus those that constitute positive action
  - Quick check question: What is the difference between measurement bias and structural inequality in the context of algorithmic fairness?

- **Concept**: EU non-discrimination law framework
  - Why needed here: The legal distinction between direct discrimination, indirect discrimination, and positive action determines whether fairness interventions are lawful
  - Quick check question: How does the EU legal framework distinguish between avoiding discrimination and positive action?

- **Concept**: Algorithmic decision-making policies
  - Why needed here: Understanding how decision thresholds and optimization objectives encode value judgments is essential for recognizing when interventions are necessary
  - Quick check question: Why is it problematic to assume that algorithmic decision-making policies are neutral?

## Architecture Onboarding

- **Component map**: Data preprocessing (addressing measurement bias) → Model training (handling performance disparities) → Decision policy (encoding value judgments)
- **Critical path**: Data → Model → Decision Policy. Interventions can target any stage, but their legal classification depends on the underlying assumptions about neutrality.
- **Design tradeoffs**: Stricter fairness constraints may reduce overall model performance but increase fairness. Legal compliance requires documenting assumptions and justifications for each intervention.
- **Failure signatures**: Misclassification of interventions as positive action when they should be avoiding discrimination, or vice versa. Failure to document measurement bias assumptions. Overcorrection that disadvantages non-protected groups.
- **First 3 experiments**:
  1. Analyze a dataset for measurement bias by comparing target variable distributions across protected groups and testing for proxy discrimination
  2. Implement and compare different fairness interventions (demographic parity vs. equalized odds) on a model with known performance disparities
  3. Document the value judgments encoded in a decision-making policy and test alternative policies that might reduce discriminatory outcomes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Under what specific conditions should algorithmic fairness interventions be legally required rather than merely permitted under EU non-discrimination law?
- Basis in paper: [explicit] The paper argues for moving from a negative obligation to avoid discrimination to a positive obligation to actively prevent it, citing the inadequacy of current legal frameworks in addressing algorithmic discrimination.
- Why unresolved: The paper identifies this as a necessary shift but does not specify the precise legal or practical conditions that would trigger such requirements, nor does it outline how these would be implemented and enforced.
- What evidence would resolve it: Empirical studies on the effectiveness of different fairness interventions in preventing algorithmic discrimination, legal analyses of potential implementation mechanisms, and case studies of jurisdictions that have adopted similar positive obligations.

### Open Question 2
- Question: How can we develop a standardized framework for assessing measurement bias in machine learning datasets that accounts for both statistical properties and social context?
- Basis in paper: [explicit] The paper identifies measurement bias as a key source of algorithmic discrimination and argues that interventions to address it should not be considered positive action, but acknowledges the difficulty in distinguishing between materialized inequalities and measurement artifacts.
- Why unresolved: The paper suggests that empirical assumptions are needed to assess measurement bias but does not provide a concrete methodology for developing such assumptions or standardizing their application across different contexts and domains.
- What evidence would resolve it: Development and validation of assessment frameworks that integrate statistical testing with qualitative analysis of data collection processes and social context, along with case studies demonstrating their application in real-world scenarios.

### Open Question 3
- Question: What are the unintended consequences of implementing fairness-aware machine learning algorithms that optimize for group fairness metrics, and how can these be mitigated?
- Basis in paper: [explicit] The paper discusses how group fairness metrics like demographic parity can require differential treatment and potentially violate equal treatment principles, and notes that interventions to address performance disparities must be carefully implemented to avoid creating new disadvantages.
- Why unresolved: While the paper identifies potential issues with group fairness metrics and the need for careful implementation, it does not provide a comprehensive analysis of the range of possible unintended consequences or specific strategies for mitigating them.
- What evidence would resolve it: Empirical studies examining the real-world impacts of different fairness interventions, including their effects on various stakeholders and potential ripple effects in related systems, along with the development and testing of mitigation strategies.

## Limitations

- The analysis relies heavily on theoretical distinctions between measurement bias and structural inequalities without providing concrete empirical methods for distinguishing between these categories
- The paper does not address how different types of protected characteristics (race, gender, age) might require different analytical approaches or how intersectional discrimination would be handled
- Practical implementation guidance for determining when fairness interventions constitute positive action versus non-discrimination measures is limited

## Confidence

**High Confidence**: The identification of three neutrality fallacies (data neutrality, model neutrality, decision-making neutrality) is well-supported by existing literature on algorithmic bias and provides a solid conceptual foundation for the paper's arguments.

**Medium Confidence**: The legal interpretation of EU non-discrimination law regarding positive action and the distinction between avoiding discrimination versus implementing preferential treatment is reasonable, though it depends on assumptions about how courts would interpret algorithmic fairness interventions.

**Low Confidence**: The practical implementation guidance for determining when fairness interventions constitute positive action versus non-discrimination measures is limited. The paper lacks concrete decision trees or criteria for practitioners to apply this framework in real-world scenarios.

## Next Checks

1. **Empirical Validation of Measurement Bias Distinction**: Develop and test empirical methods for distinguishing between measurement bias and structural inequalities in datasets across multiple domains (hiring, lending, healthcare) to validate the paper's theoretical framework.

2. **Legal Precedent Analysis**: Conduct a systematic review of recent CJEU cases involving algorithmic decision-making to assess whether the court's reasoning aligns with the paper's interpretation of positive action requirements and neutrality fallacies.

3. **Practitioner Survey and Case Studies**: Survey AI ethics practitioners and conduct case studies of organizations implementing fairness interventions to determine how they currently classify these interventions and whether the paper's framework would change their approach.