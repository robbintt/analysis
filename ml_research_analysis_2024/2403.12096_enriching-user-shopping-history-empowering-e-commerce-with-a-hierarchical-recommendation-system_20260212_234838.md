---
ver: rpa2
title: 'Enriching User Shopping History: Empowering E-commerce with a Hierarchical
  Recommendation System'
arxiv_id: '2403.12096'
source_url: https://arxiv.org/abs/2403.12096
tags:
- shopping
- history
- user
- recommendation
- item
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study proposes a hierarchical recommendation system to enrich
  user shopping histories for improved next-item predictions in e-commerce. The method
  uses a bidirectional encoder (BERT) to fill in gaps in user shopping histories by
  predicting likely missing purchases at various points.
---

# Enriching User Shopping History: Empowering E-commerce with a Hierarchical Recommendation System

## Quick Facts
- **arXiv ID:** 2403.12096
- **Source URL:** https://arxiv.org/abs/2403.12096
- **Reference count:** 3
- **Primary result:** Hierarchical system using BERT for history enrichment and SASRec for recommendations significantly improves next-item prediction accuracy

## Executive Summary
This study proposes a hierarchical recommendation system to enrich user shopping histories for improved next-item predictions in e-commerce. The method uses a bidirectional encoder (BERT) to fill in gaps in user shopping histories by predicting likely missing purchases at various points. This enriched history is then fed into a sequential recommendation model (SASRec) to recommend the next item. The system was evaluated on three Amazon datasets (Movies & TV, Video Games, Beauty) and showed significant improvements in both NDCG@10 and HR@10 compared to baseline models.

## Method Summary
The hierarchical system first preprocesses user shopping histories to identify shopping sessions based on timestamps. A BERT model is then trained to predict missing items in these histories by placing "imaginary masks" at various positions. The enriched histories are subsequently used to train a SASRec model for next-item recommendations. The system evaluates performance using HR@10 and NDCG@10 metrics on test sets containing the last item in each user's history and 99 randomly selected negative items.

## Key Results
- Significant improvements in both NDCG@10 and HR@10 compared to baseline models
- Placing imaginary masks between shopping sessions (rather than random positions) yielded the best results
- The approach of history enrichment before sequential recommendation outperforms direct next-item prediction methods

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Enriched user shopping history improves next-item prediction accuracy.
- Mechanism: The hierarchical system first uses a BERT model to predict missing items in user shopping histories by placing "imaginary masks" at various positions. This enriched history is then fed into a sequential recommendation model (SASRec) to recommend the next item. By filling gaps in user histories, the system has more complete information to base recommendations on.
- Core assumption: Users shop across multiple e-commerce platforms, and no single platform has access to the complete user shopping history. If a platform could predict the missing parts of a user's shopping history, it could make more accurate recommendations.
- Evidence anchors:
  - [abstract] "A richer user history results in more accurate recommendations... If a recommendation system is able to predict the missing parts first and enrich the user's shopping history properly, it will be possible to recommend the next item more accurately."
  - [section] "In this study, we assume that no e-commerce platform has a complete record of the user's shopping history but only accesses some parts of it. If the recommendation system predicts the missing parts and can enrich the user's shopping history properly, it will be possible to recommend the next item more accurately."
  - [corpus] Weak evidence. The corpus neighbors focus on recommendation systems and e-commerce but do not specifically address the mechanism of enriching user shopping histories using BERT for missing item prediction.

### Mechanism 2
- Claim: Placing imaginary masks between shopping sessions yields better results than random placement.
- Mechanism: The system places imaginary masks (positions where items might be missing) between different shopping sessions identified by timestamps, rather than randomly throughout the history. This approach is more realistic as it assumes users are more likely to have made purchases from other e-commerce platforms between shopping sessions.
- Core assumption: Purchases made on different days represent different shopping sessions, and users are more likely to have made purchases from other e-commerce platforms between these sessions.
- Evidence anchors:
  - [section] "In 'End-to-end test scenario 8', an imaginary mask is placed between different sessions in the shopping history... we define the shopping sessions of the users. We consider purchases made on different days are different shopping sessions."
  - [section] "Table 4 shows that 'test scenario 8' had the least number of imaginary masks added, but yielded the best results... placing imaginary masks between shopping sessions where the user is more likely to have made a purchase from another e-commerce platform rather than choosing random positions, which is a more realistic approach."
  - [corpus] Weak evidence. The corpus neighbors do not specifically discuss the placement of imaginary masks or the distinction between shopping sessions for history enrichment.

### Mechanism 3
- Claim: Adding more imaginary masks does not necessarily improve performance; placement quality matters more than quantity.
- Mechanism: The system analyzes the relationship between the number of imaginary masks added and the recommendation performance. It finds that increasing the number of masks beyond a certain point does not improve performance, and that placing masks in the "right" positions (between shopping sessions) is more important than the total number of masks.
- Core assumption: There is a limited number of optimal positions for imaginary masks in a user's shopping history, and adding masks beyond these positions does not provide additional useful information for prediction.
- Evidence anchors:
  - [section] "The analysis for the RQ3... can be seen in Fig. 6 and Fig. 7... we see that the ratio of imaginary masks with the best results is different for each dataset... it is more important to put the imaginary masks in the right places than the number of them."
  - [section] "Another point is that considering a real-world e-commerce platform, the fewer imaginary masks there are, the fewer imaginary items will be predicted, so it is possible to improve recommendation system performance both in less time and with fewer transactions."
  - [corpus] Weak evidence. The corpus neighbors do not discuss the relationship between the number of imaginary masks and recommendation performance.

## Foundational Learning

- **Concept: Bidirectional Encoder Representations from Transformers (BERT)**
  - Why needed here: BERT is used to predict the most likely items to fill in the imaginary masks in the user's shopping history. Understanding BERT's architecture and how it processes sequential data is crucial for understanding how the system enriches user histories.
  - Quick check question: How does BERT's bidirectional processing of input sequences differ from traditional autoregressive models, and why is this beneficial for predicting missing items in a user's shopping history?

- **Concept: Self-Attention Mechanisms**
  - Why needed here: Both the BERT model (for history enrichment) and the SASRec model (for next-item recommendation) rely on self-attention mechanisms to process sequential data. Understanding how self-attention works is essential for grasping how these models capture relationships between items in a user's history.
  - Quick check question: In the context of the SASRec model, how does the self-attention mechanism help the model focus on relevant items in a user's enriched shopping history when predicting the next item?

- **Concept: Sequential Recommendation Systems**
  - Why needed here: The overall goal of the system is to recommend the next item a user is likely to interact with based on their shopping history. Understanding the principles and challenges of sequential recommendation systems, such as how they handle evolving user preferences and sparse data, is important for understanding the context and significance of the proposed hierarchical approach.
  - Quick check question: What are the key differences between traditional collaborative filtering approaches and sequential recommendation systems that consider the order of user interactions?

## Architecture Onboarding

- **Component map:** User History Preprocessing -> BERT History Enrichment -> SASRec Next-Item Recommendation -> Evaluation
- **Critical path:**
  1. Preprocess user shopping histories to identify shopping sessions.
  2. Train BERT model on masked histories to predict missing items.
  3. Use trained BERT model to enrich user histories by placing imaginary masks and predicting items.
  4. Train SASRec model on enriched histories to recommend next items.
  5. Evaluate system performance on test datasets.

- **Design tradeoffs:**
  - Number and placement of imaginary masks: More masks provide more enrichment but increase computational cost. Placement between sessions is more realistic but may miss some missing items.
  - Choice of models: BERT and SASRec are state-of-the-art but computationally expensive. Simpler models might be faster but less accurate.
  - Training data: Using only user-item interactions simplifies the problem but ignores potentially useful information from reviews or item metadata.

- **Failure signatures:**
  - Poor prediction accuracy: If the BERT model fails to accurately predict missing items, the enrichment process will not provide meaningful improvement.
  - Overfitting: If the models are too complex relative to the amount of training data, they may overfit and perform poorly on unseen data.
  - Slow inference: If the enrichment and recommendation process is too slow, it may not be practical for real-world e-commerce platforms.

- **First 3 experiments:**
  1. Train and evaluate the BERT model on a small subset of the data to verify that it can accurately predict missing items.
  2. Train and evaluate the SASRec model on the original (un-enriched) user histories to establish a baseline performance.
  3. Combine the trained BERT and SASRec models to enrich user histories and make recommendations, then evaluate the performance improvement over the baseline.

## Open Questions the Paper Calls Out
- How does the hierarchical recommendation system perform when applied to cross-domain recommendation problems?
- What is the optimal balance between the number of imaginary masks added and their placement strategy (random vs. session-based)?
- How does the system perform on e-commerce datasets with more diverse product categories and longer user histories?

## Limitations
- The study relies on review timestamps to identify shopping sessions, which may not accurately reflect true shopping behavior
- The evaluation uses only Amazon datasets, limiting generalizability to other e-commerce platforms
- The computational cost of the two-stage approach (BERT + SASRec) may be prohibitive for real-time recommendations

## Confidence
- High confidence in the core methodology and experimental results showing improved recommendation performance with history enrichment
- Medium confidence in the mechanism explaining why placing masks between sessions works best, as this relies on assumptions about user behavior patterns
- Low confidence in the generalizability of the findings beyond the Amazon datasets used, particularly for different e-commerce domains

## Next Checks
1. Validate the shopping session identification method on a different e-commerce dataset with more granular user behavior data
2. Conduct an ablation study to quantify the individual contributions of history enrichment vs. sequential modeling to overall performance
3. Test the system's performance on datasets with different characteristics (e.g., higher/lower sparsity, different item categories) to assess generalizability