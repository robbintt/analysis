---
ver: rpa2
title: 'AI Assistants for Spaceflight Procedures: Combining Generative Pre-Trained
  Transformer and Retrieval-Augmented Generation on Knowledge Graphs With Augmented
  Reality Cues'
arxiv_id: '2409.14206'
source_url: https://arxiv.org/abs/2409.14206
tags:
- procedure
- space
- information
- astronauts
- procedures
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents CORE, an intelligent personal assistant designed
  to support astronauts during spaceflight procedures on the ISS, Lunar Gateway, and
  beyond. CORE addresses key limitations of existing assistants by combining Knowledge
  Graphs, Retrieval-Augmented Generation with GPT models, and Augmented Reality to
  ensure reliability, offline availability, flexibility, and intuitive 3D visual support.
---

# AI Assistants for Spaceflight Procedures: Combining Generative Pre-Trained Transformer and Retrieval-Augmented Generation on Knowledge Graphs With Augmented Reality Cues

## Quick Facts
- arXiv ID: 2409.14206
- Source URL: https://arxiv.org/abs/2409.14206
- Reference count: 40
- Presents CORE system combining Knowledge Graphs, RAG, GPT models, and AR for spaceflight procedure assistance

## Executive Summary
This paper introduces CORE, an intelligent personal assistant designed to support astronauts during complex spaceflight procedures on the ISS, Lunar Gateway, and beyond. The system addresses critical limitations of existing assistants by integrating Knowledge Graphs for structured data management, Retrieval-Augmented Generation with GPT models for natural language processing, and Augmented Reality for intuitive 3D visual guidance. Initial experiments demonstrate successful procedure understanding and question answering capabilities, with the system correctly interpreting procedures and providing step-by-step guidance with visual cues.

The CORE system achieves accurate verbatim step retrieval and appropriate handling of related images while ignoring unrelated queries. A second offline version based on Llama 3.1 with integrated AR HUD features is planned for user testing at ESA's European Astronaut Centre. The architecture aims to ensure reliability, offline availability, flexibility, and intuitive support for astronauts working in challenging space environments where traditional documentation methods may be insufficient.

## Method Summary
The CORE system architecture combines multiple AI technologies to create an intelligent assistant for spaceflight procedures. The core components include a Knowledge Graph for structured representation of procedural knowledge, Retrieval-Augmented Generation (RAG) with GPT models for natural language understanding and generation, and Augmented Reality for visual guidance. The system processes procedure documents, extracts steps and visual cues, and enables interactive question-answering capabilities. A prototype version using GPT-4 demonstrated successful procedure interpretation, while an offline version using Llama 3.1 is under development for deployment in environments with limited connectivity.

## Key Results
- GPT-4 correctly interpreted spaceflight procedures and provided step-by-step guidance with visual cues in controlled experiments
- System achieved accurate verbatim step retrieval and appropriate handling of related images while ignoring unrelated queries
- Planned offline version using Llama 3.1 with integrated AR HUD features scheduled for user testing at ESA EAC

## Why This Works (Mechanism)
The CORE system works by combining structured knowledge representation with flexible language understanding and intuitive visual guidance. Knowledge Graphs provide a reliable foundation for organizing procedural information, ensuring consistency and accuracy in critical space operations. RAG technology bridges the gap between structured data and natural language queries, enabling astronauts to ask questions in conversational terms while maintaining access to precise procedural information. AR overlays translate abstract instructions into concrete visual cues, reducing cognitive load and potential for error during complex tasks.

## Foundational Learning
**Knowledge Graphs**: Structured representation of procedural knowledge; needed for reliable information organization in safety-critical environments; quick check: verify graph consistency and completeness of procedural steps
**Retrieval-Augmented Generation**: Combines information retrieval with language model generation; needed to enable natural language interaction while maintaining factual accuracy; quick check: measure retrieval precision and generation relevance
**Augmented Reality**: Overlays digital information onto physical environment; needed for intuitive spatial guidance during hands-on procedures; quick check: validate AR alignment accuracy and visual clarity under various lighting conditions
**GPT Models**: Large language models for natural language understanding and generation; needed for flexible question answering and procedure interpretation; quick check: evaluate response accuracy and hallucination rates
**Offline AI Systems**: Local processing without internet connectivity; needed for operation in space environments with limited communication; quick check: measure inference latency and memory usage on target hardware

## Architecture Onboarding
**Component Map**: Knowledge Graph <- RAG <- GPT-4 -> AR Overlay
**Critical Path**: Procedure document → Knowledge Graph extraction → RAG retrieval → GPT generation → AR visualization
**Design Tradeoffs**: Cloud-based GPT-4 vs offline Llama 3.1 (accuracy vs autonomy), complex Knowledge Graph structure vs simple text storage (reliability vs flexibility), high-bandwidth AR streaming vs local processing (quality vs latency)
**Failure Signatures**: Incorrect procedure interpretation (Knowledge Graph errors), irrelevant responses (RAG retrieval failures), AR misalignment (calibration issues), system crashes (memory constraints)
**First 3 Experiments**:
1. Procedure document parsing accuracy test with varying complexity levels
2. Question-answering evaluation comparing GPT-4 responses to ground truth
3. AR overlay precision measurement under different lighting and viewing conditions

## Open Questions the Paper Calls Out
None identified in the provided content.

## Limitations
- Evaluation relies on controlled experiments rather than real astronaut testing, limiting generalizability to operational conditions
- Performance metrics focus on interpretation accuracy without measuring critical factors like response time, cognitive load, or workflow integration
- Offline Llama 3.1 version is planned but not yet tested, making performance claims speculative
- Does not address integration challenges with existing space mission systems or certification requirements

## Confidence
- Technical implementation of CORE: High - Architecture is clearly described and technically sound
- Performance claims for procedure understanding: Medium - Results show correct interpretation in controlled tests but lack real-world validation
- Offline capability assertions: Low - Offline version planned but not yet tested

## Next Checks
1. Conduct usability testing with astronauts or astronaut trainers using realistic ISS procedure scenarios to evaluate cognitive load and workflow integration
2. Perform latency and reliability testing of the AR overlay system under simulated mission conditions, including bandwidth constraints and equipment failures
3. Implement a comparative study between CORE and existing procedure documentation systems to quantify improvements in task completion time and error rates