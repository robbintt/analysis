---
ver: rpa2
title: Linear Time Complexity Conformers with SummaryMixing for Streaming Speech Recognition
arxiv_id: '2409.07165'
source_url: https://arxiv.org/abs/2409.07165
tags:
- summarymixing
- streaming
- speech
- mhsa
- time
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SummaryMixing, a linear-time complexity alternative
  to self-attention, for streaming and non-streaming automatic speech recognition.
  The core idea is to summarize the entire input sequence into a single vector, which
  is then concatenated back to each time step of the sequence.
---

# Linear Time Complexity Conformers with SummaryMixing for Streaming Speech Recognition

## Quick Facts
- **arXiv ID**: 2409.07165
- **Source URL**: https://arxiv.org/abs/2409.07165
- **Reference count**: 33
- **Primary result**: SummaryMixing-equipped conformer transducers achieve equivalent or better word error rates than self-attention models while reducing peak VRAM usage by 16% and training time by 15.5%

## Executive Summary
This paper introduces SummaryMixing, a linear-time complexity alternative to self-attention for automatic speech recognition. The approach summarizes entire input sequences into a single vector that is concatenated back to each time step, reducing complexity from quadratic to linear. Experiments on Librispeech and Voxpopuli datasets demonstrate that SummaryMixing-equipped conformer transducers achieve equivalent or better word error rates compared to self-attention models while requiring significantly less compute and memory during training and decoding. The method is validated for both streaming and non-streaming scenarios through dynamic chunk training.

## Method Summary
SummaryMixing is a linear-time complexity alternative to self-attention that summarizes the entire input sequence into a single vector, which is then concatenated back to each time step of the sequence. This approach reduces the time complexity from quadratic to linear, making it more efficient for long sequences. The method is applied to conformer transducers, which are trained with dynamic chunk training (DCT) and dynamic chunk convolution (DCCONV) on two ASR datasets: Librispeech and Voxpopuli. The models are evaluated for both streaming and non-streaming scenarios, with varying chunk sizes and left contexts.

## Key Results
- SummaryMixing reduces peak VRAM usage by 16% compared to self-attention models
- Training time is reduced by 15.5% with SummaryMixing
- SummaryMixing-equipped conformer transducers achieve equivalent or better word error rates than self-attention models on Librispeech and Voxpopuli datasets
- Models trained with DCT and DCCONV operate in both streaming and non-streaming modes without architectural changes

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: SummaryMixing reduces the time complexity of self-attention from quadratic to linear by summarizing the entire input sequence into a single vector.
- **Mechanism**: The input sequence is transformed through a summary function that computes a mean vector over time. This single vector is then concatenated back to each time step of the sequence, replacing the need for pairwise attention computations.
- **Core assumption**: The global context can be effectively captured by a single summary vector without significant loss of accuracy.
- **Evidence anchors**:
  - [abstract]: "The core idea is to summarize the entire input sequence into a single vector, which is then concatenated back to each time step of the sequence. This approach reduces the time complexity from quadratic to linear"
  - [section]: "SummaryMixing summarises the whole sequence in a single vector modelling global relations between frames. This summary vector is then concatenated back to each time step of the input sequence."
  - [corpus]: Weak evidence - only 1 related paper (Linear-Complexity Self-Supervised Learning for Speech Processing) mentions linear complexity alternatives, but no specific evidence about this mechanism.

### Mechanism 2
- **Claim**: Dynamic Chunk Training (DCT) enables a single model to operate in both streaming and non-streaming modes by exposing it to various context lengths during training.
- **Mechanism**: The model is trained with randomly varying chunk sizes and left contexts. A mask controls which frames are visible at each time step, allowing the model to learn to handle different latency requirements.
- **Core assumption**: The model can learn to generalize across different chunk sizes and contexts without architectural changes.
- **Evidence anchors**:
  - [section]: "Dynamic chunk training (DCT) [21] makes an ASR system capable of operating both in a streaming and offline settings. This is achieved by exposing the ASR model to a variety of context lengths, or dynamic chunks, during training."
  - [section]: "ASR systems trained with DCT and DCCONV are capable of both streaming and non-streaming ASR without architectural change or fine-tuning"
  - [corpus]: No direct evidence in corpus - related papers focus on other streaming/non-streaming unification methods.

### Mechanism 3
- **Claim**: Dynamic Chunk Convolution (DCCONV) enables convolutional layers to work with dynamic chunks without data leakage from future frames.
- **Mechanism**: Standard convolutional kernels are kept centered on the current frame but are masked according to chunk boundaries, allowing within-chunk future information while preventing cross-chunk leakage.
- **Core assumption**: Standard convolutional kernels can be adapted with masking rather than requiring causal convolutions.
- **Evidence anchors**:
  - [section]: "Dynamic chunk convolution [7] (DCCONV) addresses all these issues by keeping standard convolutional kernels centered on the current frame, hence allowing within-chunk future information, but with masking according to the boundaries of the chunk."
  - [section]: "Dynamic chunk convolution [7] (DCCONV) addresses all these issues by keeping standard convolutional kernels centered on the current frame, hence allowing within-chunk future information, but with masking according to the boundaries of the chunk."
  - [corpus]: No evidence in corpus - this appears to be a novel contribution not mentioned in related papers.

## Foundational Learning

- **Concept**: Self-attention mechanism
  - **Why needed here**: Understanding the quadratic complexity problem that SummaryMixing solves requires knowledge of how self-attention works.
  - **Quick check question**: How does the computational complexity of self-attention scale with sequence length, and why does this create problems for long sequences?

- **Concept**: Conformer architecture
  - **Why needed here**: The paper extends SummaryMixing to conformer transducers, so understanding the conformer's components (self-attention and convolution) is essential.
  - **Quick check question**: What are the two main components of a conformer block, and how do they complement each other?

- **Concept**: Streaming vs non-streaming ASR
  - **Why needed here**: The paper addresses both scenarios, so understanding the latency and architectural differences is crucial.
  - **Quick check question**: What is the fundamental difference in how streaming and non-streaming ASR models process input sequences?

## Architecture Onboarding

- **Component map**: Input acoustic features → Conformable encoder (with SummaryMixing blocks) → LSTM predictor → Joiner → Output vocabulary
- **Critical path**: Acoustic features → Conformable encoder → Joiner → Output predictions
- **Design tradeoffs**:
  - Linear complexity vs potential accuracy loss (mitigated by experiments showing equivalent or better performance)
  - Unified streaming/non-streaming training vs potential suboptimal specialization
  - Half-precision computation vs numerical stability concerns
- **Failure signatures**:
  - Degradation in WER as sequence length increases (should be constant with SummaryMixing)
  - Memory usage scaling with sequence length (should remain constant)
  - Training instability with half-precision transducer loss
  - Poor performance in streaming mode if DCT range is insufficient
- **First 3 experiments**:
  1. Compare WER and inference time of SummaryMixing vs self-attention on Librispeech with varying sequence lengths
  2. Test streaming performance with different chunk sizes (320ms, 640ms, 1280ms) to verify latency-accuracy tradeoff
  3. Measure memory usage during inference on long sequences to confirm linear complexity benefits

## Open Questions the Paper Calls Out

- **Open Question 1**: How does SummaryMixing perform with extremely long sequences (e.g., 10+ minutes) compared to self-attention models in terms of both accuracy and memory efficiency?
  - **Basis in paper**: [inferred] The paper discusses performance on utterances up to 120 seconds, but mentions the potential for SummaryMixing to handle even longer sequences more efficiently than self-attention.
  - **Why unresolved**: The experiments conducted only covered sequences up to 120 seconds, leaving the performance on significantly longer sequences untested.
  - **What evidence would resolve it**: Experiments comparing SummaryMixing and self-attention on sequences longer than 10 minutes, measuring both word error rates and memory usage.

- **Open Question 2**: Can SummaryMixing be effectively applied to other sequence modeling tasks beyond speech recognition, such as natural language processing or computer vision?
  - **Basis in paper**: [inferred] The paper focuses on speech recognition but mentions the general nature of the SummaryMixing approach, suggesting potential applicability to other domains.
  - **Why unresolved**: The paper does not explore applications of SummaryMixing outside of speech recognition.
  - **What evidence would resolve it**: Experiments applying SummaryMixing to tasks like machine translation, text summarization, or image classification, comparing its performance to traditional attention mechanisms.

- **Open Question 3**: How does the choice of the summary function and local transformation function in SummaryMixing affect its performance?
  - **Basis in paper**: [explicit] The paper mentions that these functions are used in the SummaryMixing formulation but does not explore different choices or their impact.
  - **Why unresolved**: The paper uses specific functions but does not investigate how different choices might affect the model's performance.
  - **What evidence would resolve it**: Experiments testing different summary and local transformation functions, comparing their impact on word error rates and computational efficiency.

- **Open Question 4**: What is the impact of varying the chunk size and left context in dynamic chunk training on the performance of SummaryMixing-equipped models?
  - **Basis in paper**: [explicit] The paper mentions dynamic chunk training but does not provide an in-depth analysis of how different chunk sizes and left context values affect the model's performance.
  - **Why unresolved**: While the paper uses dynamic chunk training, it does not explore the sensitivity of the model to these hyperparameters.
  - **What evidence would resolve it**: Experiments varying the chunk sizes and left context values in dynamic chunk training, measuring their impact on word error rates and real-time factors for both streaming and non-streaming scenarios.

## Limitations

- Exact implementation details of the streaming variant of SummaryMixing are not fully specified, making exact reproduction challenging
- The paper reports results on only two datasets (Librispeech and Voxpopuli) without exploring diverse acoustic conditions or languages
- Half-precision transducer loss training may introduce numerical stability concerns not fully explored

## Confidence

**High Confidence (90-100%)**:
- Linear time complexity claim for SummaryMixing - mathematically sound given the mechanism of summarizing to a single vector
- Memory usage reduction - the 16% reduction is well-documented and directly follows from the complexity improvement
- WER results on Librispeech - extensive experiments with multiple chunk sizes provide robust validation

**Medium Confidence (70-90%)**:
- Generalization to other speech recognition tasks and languages beyond the tested datasets
- Long-term stability of half-precision transducer loss training
- Performance in extremely long sequences (>10 minutes) where streaming assumptions may break down

**Low Confidence (0-70%)**:
- Scalability to multilingual or low-resource language scenarios
- Robustness to domain shifts (noisy environments, different accents)
- Computational efficiency on edge devices with limited memory

## Next Checks

1. **Streaming Latency Validation**: Test SummaryMixing conformers with chunk sizes ranging from 160ms to 2560ms in 160ms increments to precisely map the latency-accuracy tradeoff curve and verify the claimed streaming capabilities.

2. **Cross-dataset Generalization**: Train and evaluate the SummaryMixing conformer on a third dataset (e.g., TED-LIUM or Common Voice) to assess performance beyond Librispeech and Voxpopuli, particularly in different acoustic conditions.

3. **Long Sequence Robustness**: Evaluate performance on sequences exceeding 30 minutes to verify that linear complexity benefits hold at extreme lengths and identify any hidden computational bottlenecks.