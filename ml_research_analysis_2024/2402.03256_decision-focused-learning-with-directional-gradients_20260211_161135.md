---
ver: rpa2
title: Decision-Focused Learning with Directional Gradients
arxiv_id: '2402.03256'
source_url: https://arxiv.org/abs/2402.03256
tags:
- should
- losses
- loss
- error
- regret
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces a new family of decision-aware surrogate
  losses, called Perturbation Gradient (PG) losses, for the predict-then-optimize
  framework. The core idea is to connect the expected downstream decision loss with
  the directional derivative of a plug-in objective and approximate this derivative
  using zeroth-order gradient techniques.
---

# Decision-Focused Learning with Directional Gradients

## Quick Facts
- arXiv ID: 2402.03256
- Source URL: https://arxiv.org/abs/2402.03256
- Reference count: 40
- Key outcome: Introduces Perturbation Gradient (PG) losses that connect expected downstream decision loss with directional derivatives of plug-in objectives, proving asymptotic optimality even under model misspecification

## Executive Summary
This paper introduces Perturbation Gradient (PG) losses for decision-focused learning, addressing a key limitation in predict-then-optimize frameworks where the original decision loss is typically piecewise constant and discontinuous. The PG losses approximate the directional derivative of the plug-in objective using zeroth-order gradient techniques, resulting in Lipschitz continuous surrogate losses that can be optimized using standard gradient-based methods. The authors prove that the approximation error of PG losses vanishes as sample size grows, yielding asymptotically optimal policies even in misspecified settings.

## Method Summary
The paper presents a new family of decision-aware surrogate losses called Perturbation Gradient (PG) losses. These losses connect the expected downstream decision loss with the directional derivative of a plug-in objective, which is then approximated using zeroth-order gradient techniques. Unlike the original decision loss which is typically piecewise constant and discontinuous, the PG losses are Lipschitz continuous and can be optimized using off-the-shelf gradient-based methods. The authors prove that the approximation error of PG losses vanishes as the number of samples grows, yielding asymptotically optimal policies even when the underlying model is misspecified.

## Key Results
- PG losses outperform existing proposals when the underlying model is misspecified
- The approximation error of PG losses vanishes as the number of samples grows, yielding asymptotically optimal policies
- PG losses are Lipschitz continuous and can be optimized using standard gradient-based methods, unlike the original piecewise constant decision loss

## Why This Works (Mechanism)
The PG losses work by approximating the directional derivative of the plug-in objective, which connects to the expected downstream decision loss. This approximation uses zeroth-order gradient techniques, which provide a way to estimate gradients without requiring explicit knowledge of the derivative. The resulting loss function is smooth and continuous, enabling the use of standard gradient-based optimization methods that would fail on the original discontinuous decision loss.

## Foundational Learning
1. **Predict-then-optimize framework**: Connects prediction errors to downstream decision quality. Why needed: This is the foundational problem setting being addressed.
   Quick check: Does the surrogate loss account for how prediction errors propagate through the optimization?

2. **Zeroth-order gradient estimation**: Techniques for approximating gradients when only function evaluations are available. Why needed: Enables gradient-based optimization of the PG losses without requiring explicit derivatives.
   Quick check: Does the variance of the gradient estimator affect convergence?

3. **Directional derivatives**: Measures of how a function changes in a specific direction. Why needed: The PG losses are built on connecting expected decision loss to directional derivatives of the plug-in objective.
   Quick check: Is the direction of perturbation chosen appropriately for the optimization problem?

4. **Lipschitz continuity**: Property ensuring bounded rate of change. Why needed: Guarantees that small changes in predictions lead to small changes in the loss, enabling stable optimization.
   Quick check: Does the Lipschitz constant affect the choice of learning rate?

5. **Misspecified model settings**: Scenarios where the assumed model family doesn't contain the true data-generating process. Why needed: The paper proves PG losses perform well even when the model is misspecified.
   Quick check: Does the performance degrade gracefully as misspecification increases?

## Architecture Onboarding

**Component Map**: Input data -> Feature extraction -> PG loss computation -> Zeroth-order gradient estimation -> Model parameter updates

**Critical Path**: Data → PG loss → Gradient estimate → Parameter update → Decision optimization

**Design Tradeoffs**: The PG losses balance between bias and computational complexity through the perturbation parameter h. Smaller h reduces bias but increases computational complexity, while larger h has the opposite effect.

**Failure Signatures**: 
- Poor performance may indicate inappropriate choice of perturbation parameter h
- Optimization instability could suggest variance in the zeroth-order gradient estimates is too high
- Failure to converge might indicate the PG loss is not well-suited for the specific optimization problem structure

**3 First Experiments**:
1. Compare PG loss performance against standard methods on a simple linear optimization problem with known ground truth
2. Vary the perturbation parameter h systematically to identify the optimal value for a specific problem instance
3. Test PG losses on a misspecified model to verify the theoretical guarantees about asymptotic optimality

## Open Questions the Paper Calls Out
### Open Question 1
What is the optimal choice of the perturbation parameter h for the PG losses in practice, and how does it depend on the specific problem instance (e.g., data size, hypothesis class complexity, noise level)?

### Open Question 2
How does the performance of the PG losses compare to other decision-aware methods in settings with non-linear hypothesis classes (e.g., neural networks) and non-polyhedral feasible regions Z?

### Open Question 3
Can the PG losses be extended to handle settings with more complex decision-making scenarios, such as multi-stage optimization problems or problems with multiple objectives?

## Limitations
- Theoretical guarantees rely on assumptions about continuity and differentiability of the plug-in objective that may not hold in all practical settings
- Zeroth-order gradient estimation introduces variance that could affect optimization stability, particularly in high-dimensional problems
- Computational overhead of PG losses compared to standard methods requires further investigation

## Confidence
- High confidence in the theoretical framework and asymptotic guarantees
- Medium confidence in empirical performance claims due to limited experimental scope
- Medium confidence in the practical applicability across diverse optimization problems

## Next Checks
1. Conduct extensive ablation studies varying the number of gradient samples to quantify the variance-performance tradeoff
2. Test the PG loss framework on larger-scale problems (e.g., with >10,000 variables) to assess scalability
3. Evaluate robustness to different types of model misspecification beyond the linear case studied in experiments