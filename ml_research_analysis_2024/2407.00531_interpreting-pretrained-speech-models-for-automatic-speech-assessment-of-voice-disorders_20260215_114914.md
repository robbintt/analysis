---
ver: rpa2
title: Interpreting Pretrained Speech Models for Automatic Speech Assessment of Voice
  Disorders
arxiv_id: '2407.00531'
source_url: https://arxiv.org/abs/2407.00531
tags:
- speech
- voice
- spectrogram
- relevance
- finetuned
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper explores the interpretability of deep learning models\
  \ for automatic speech assessment of voice disorders. The authors train and compare\
  \ two configurations of the Audio Spectrogram Transformer (AST) model on the Saarbr\xFC\
  cken Voice Database, which contains recordings from individuals with various voice\
  \ disorders and healthy controls."
---

# Interpreting Pretrained Speech Models for Automatic Speech Assessment of Voice Disorders

## Quick Facts
- arXiv ID: 2407.00531
- Source URL: https://arxiv.org/abs/2407.00531
- Reference count: 32
- Primary result: Fine-tuning improves AST model performance and concentrates attention on relevant phoneme regions for voice disorder detection

## Executive Summary
This paper explores the interpretability of deep learning models for automatic speech assessment of voice disorders using the Saarbrücken Voice Database. The authors compare two configurations of the Audio Spectrogram Transformer (AST): one with a frozen pre-trained base and one with full fine-tuning. Using attention rollout methods, they generate relevance maps showing which spectrogram regions influence predictions. The analysis reveals that fine-tuning improves performance and concentrates the model's attention on specific phoneme regions (/ɔ/ and /e/s/i/n/), demonstrating the importance of phoneme features for automatic speech assessment and the potential of interpretable AI tools in guiding model design.

## Method Summary
The study trains two AST configurations on voice disorder detection: ast_freeze with a frozen pre-trained base and ast_finetuned with full model fine-tuning. Both models process 128-dimensional log Mel filterbank features extracted from recordings of specific vowels and a German phrase. The authors apply attention rollout to generate relevance maps visualizing which spectrogram regions contribute most to predictions. Performance is evaluated using Unweighted Average Recall (UAR) and Area Under the ROC Curve (AUC), while interpretability is assessed through t-SNE visualizations and attention map analysis comparing frozen versus fine-tuned models.

## Key Results
- Fine-tuned AST model achieves higher UAR and AUC scores compared to frozen model
- Attention rollout reveals both models focus on specific phonemes, particularly /ɔ/ and /e/s/i/n/ segments
- Fine-tuning reduces attention spread and concentrates focus on relevant phoneme regions
- Frozen model shows speaker gender separation in t-SNE rather than pathology separation

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Fine-tuning improves model performance by concentrating attention on relevant phoneme regions
- Mechanism: Pretrained AST captures general speech features; fine-tuning adjusts attention weights to focus on discriminative phoneme-level features for disorder detection
- Core assumption: Voice disorders manifest through distinct phoneme-level acoustic features
- Evidence anchors:
  - [abstract] "the spread of attention is reduced as a model is finetuned, and the model attention is concentrated on specific phoneme regions"
  - [section 3.2] "When the model was fine-tuned, we found more concentration, and the location often changed/shifted"
- Break condition: If phoneme-level features are not discriminative or disorders manifest at suprasegmental levels

### Mechanism 2
- Claim: Relevance maps reveal which spectrogram regions influence predictions
- Mechanism: Attention rollout aggregates attention weights across layers to highlight input regions contributing to output
- Core assumption: Attention weights correlate with feature importance for classification
- Evidence anchors:
  - [section 2.3] "The method uses the model's attention layers to produce relevancy maps that visualise the computed relevancy of the spectrogram regions"
  - [section 3.2] "From the available visualizations, we can see the highest relevance scores are not necessarily assigned to the highest-intensity region such as fundamental frequency and the formats"
- Break condition: If attention weights don't correlate with feature importance or other mechanisms drive decisions

### Mechanism 3
- Claim: Pretrained models capture speaker identity that interferes with pathology detection
- Mechanism: t-SNE visualization shows frozen AST separates by gender rather than pathological status, indicating speaker features dominate
- Core assumption: Pretrained models on general speech data learn speaker-specific features irrelevant to pathology
- Evidence anchors:
  - [section 3.2] "when the based AST model is not fully trained (A, ast_freeze), the representations show separations between genders rather than pathological status"
- Break condition: If pretrained model doesn't capture speaker identity or it doesn't interfere with pathology detection

## Foundational Learning

- Transformer attention mechanisms
  - Why needed here: Understanding how attention rollout generates relevance maps requires knowledge of self-attention in transformers
  - Quick check question: How does multi-head attention aggregate information from different representation subspaces?

- Spectrogram interpretation
  - Why needed here: Analyzing relevance maps requires understanding spectrogram features like formants and phoneme boundaries
  - Quick check question: What acoustic features distinguish /ɔ/ from other vowels in a spectrogram?

- Voice disorder acoustic markers
  - Why needed here: Interpreting model decisions requires knowing which acoustic features are clinically relevant for different disorders
  - Quick check question: Which acoustic parameters (jitter, shimmer, formant frequencies) are most affected in organic vs inorganic voice disorders?

## Architecture Onboarding

- Component map: Audio → Spectrogram → AST patches → Attention layers → CLS token → Classification
- Critical path: Log Mel filterbank features → Patch embedding → Positional embedding → Transformer layers → CLS token embedding → Linear classification layer
- Design tradeoffs:
  - Fixed vs trainable AST: Freeze preserves general features but may retain irrelevant speaker info; fine-tune adapts to pathology but needs disorder-specific data
  - Spectrogram resolution: Higher resolution captures more detail but increases computation and may overfit
  - Attention visualization method: Attention rollout is model-specific; perturbation methods are more general but computationally expensive
- Failure signatures:
  - Poor UAR/AUC scores: Model fails to learn pathology-relevant features
  - Relevance maps concentrated on non-phonetic regions: Model relies on spurious correlations
  - t-SNE shows speaker separation instead of pathology separation: Pretrained features dominate
- First 3 experiments:
  1. Compare frozen vs fine-tuned AST on held-out validation set to verify performance improvement
  2. Generate relevance maps for samples where models disagree to understand decision differences
  3. Test model on samples with varying recording quality to assess robustness

## Open Questions the Paper Calls Out

- How do different pre-trained speech models trained with phonetic objectives compare to AST in terms of interpretability and performance for automatic speech assessment of voice disorders?
- What specific acoustic features in the /ɔ/ phoneme and /e/s/i/n/ segment make them particularly relevant for voice disorder detection, and how do these features vary across different voice pathologies?
- How does recording environment quality affect model decision-making and attention patterns, and what methods can improve robustness to suboptimal recording conditions?

## Limitations
- Interpretability findings rely on attention rollout, which has limitations in correlating attention weights with feature importance
- Corpus size (1853 samples) may limit generalizability given heterogeneity of voice disorders
- Analysis focuses on two specific phoneme regions without systematic exploration of other potentially relevant acoustic features

## Confidence
- Medium: Fine-tuning improves performance by concentrating attention on relevant phoneme regions
- Low: Attention rollout relevance maps accurately identify discriminative spectrogram regions
- Medium: Pretrained models capture speaker identity that interferes with pathology detection

## Next Checks
1. Validate relevance maps with ground truth phoneme annotations by manually annotating phoneme boundaries and verifying attention rollout consistency
2. Compare attention rollout with perturbation-based interpretability using feature ablation to assess correlation with actual feature importance
3. Test model generalization across recording conditions by evaluating models on recordings with varying background noise, microphone qualities, and recording distances