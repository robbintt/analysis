---
ver: rpa2
title: 'Baba Is AI: Break the Rules to Beat the Benchmark'
arxiv_id: '2407.13729'
source_url: https://arxiv.org/abs/2407.13729
tags:
- rule
- object
- environment
- rules
- plan
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The authors develop a new benchmark based on the game Baba Is You
  to test large language models' abilities to identify, manipulate, and combine rules
  in dynamic environments. They test GPT-4o, Gemini-1.5-Pro, and Gemini-1.5-Flash
  on various tasks involving distractors and rule manipulation.
---

# Baba Is AI: Break the Rules to Beat the Benchmark

## Quick Facts
- **arXiv ID**: 2407.13729
- **Source URL**: https://arxiv.org/abs/2407.13729
- **Reference count**: 7
- **Primary result**: LLMs show near-perfect accuracy on simple rule-following tasks but drop to ~20% accuracy when tasks require manipulating and combining rules in novel ways

## Executive Summary
This paper introduces a novel benchmark based on the game Baba Is You to test large language models' abilities to identify, manipulate, and combine rules in dynamic environments. The benchmark evaluates GPT-4o, Gemini-1.5-Pro, and Gemini-1.5-Flash on tasks involving varying levels of distractors and rule manipulation complexity. While models perform well on simple rule-following tasks, they show dramatic performance drops when required to compose previously learned rule transformations in novel ways, highlighting fundamental limitations in compositional generalization for current LLMs.

## Method Summary
The benchmark uses visual game environments where players manipulate textual rules to change game mechanics. LLMs receive game instructions and 10 in-context example images with winning plans, then must generate high-level textual plans for novel test environments. Performance is measured by exact match accuracy between generated plans and ground truth winning plans across five test environments with increasing distractor complexity (no distractor, object distractor, rule distractor, object & rule distractor, win rule distractor).

## Key Results
- LLMs achieve near-perfect accuracy on simple rule-following tasks without distractors
- Accuracy drops to ~20% when environments contain both object and rule distractors
- All three tested models (GPT-4o, Gemini-1.5-Pro, Gemini-1.5-Flash) show similar performance degradation patterns
- Visual input modality does not overcome fundamental limitations in compositional reasoning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LLMs fail when generalization requires both rule manipulation and compositional combination of rules.
- Mechanism: The benchmark isolates two critical cognitive abilities: (1) identifying relevant vs. irrelevant rules/objects in a dynamic environment, and (2) composing previously seen rule transformations in novel ways. The authors systematically increase distractor complexity, showing accuracy drops from near-perfect to ~20% when both object and rule distractors are present.
- Core assumption: Performance degradation correlates with the need to manipulate environment rules rather than simply follow them.
- Evidence anchors:
  - [abstract]: "All models show substantial performance drops when generalization requires rule manipulation and combination. Accuracy drops from near-perfect in simpler tasks to around 20% when both object and rule distractors are present"
  - [section]: "We show results for three large language models...and find that they fail dramatically when generalization requires that the rules of the game must be manipulated and combined."

### Mechanism 2
- Claim: Multi-modal visual inputs allow direct evaluation of LLMs without text conversion, but this doesn't overcome compositional generalization limitations.
- Mechanism: The benchmark uses visual inputs of the game environment directly, leveraging the multi-modal capabilities of GPT-4o and Gemini models. This bypasses potential loss of spatial information that might occur with text-only descriptions.
- Core assumption: Visual input processing doesn't compensate for fundamental limitations in compositional reasoning and rule manipulation.
- Evidence anchors:
  - [section]: "Here we leverage the multi-modal ability of these models to evaluate them directly on visual inputs of the game" and "this is while receiving visual and not textual inputs about the game"
  - [section]: Despite visual input, "GPT-4o performs with perfect accuracy on the first four environments" but fails on compositional tasks

### Mechanism 3
- Claim: In-context learning with 10 examples is insufficient for teaching compositional generalization in dynamic rule environments.
- Mechanism: The benchmark uses in-context learning where models see 10 example environments with corresponding winning plans, then must generalize to novel environments. The authors show that while models perform well on simple generalization tasks, they fail when required to compose multiple rule manipulations.
- Core assumption: The number and diversity of in-context examples limits the model's ability to learn the underlying compositional principles.
- Evidence anchors:
  - [section]: "Following previous work on LLM-based agents and planners...we ask LLMs to produce high-level textual plans" and "LLMs are evaluated on 5 samples for each test environment"
  - [section]: "The accuracy for all three LLMs is low" when asked to compose previously learned rules in novel ways

## Foundational Learning

- Concept: Rule-based reasoning and active vs. inactive rule states
  - Why needed here: The entire benchmark hinges on understanding that rules are only active when text blocks are horizontally aligned in the specific "<object> is <property>" format, and that manipulating this alignment changes the game dynamics
  - Quick check question: If the blocks "wall", "is", "stop" are arranged vertically instead of horizontally, is the rule active? Why or why not?

- Concept: Compositional generalization and rule manipulation sequences
  - Why needed here: Success requires not just identifying winning conditions but planning sequences like "break [wall is stop], make [door is win], goto[door]" which combine multiple rule transformations
  - Quick check question: If you've learned to break a rule then go to an object, and to make a rule then go to an object, can you automatically solve a task requiring break-then-make-then-go? Why or why not?

- Concept: Distractor identification and filtering
  - Why needed here: Complex environments contain irrelevant objects and rules that must be ignored to identify the correct winning strategy
  - Quick check question: In an environment with both "[door is win]" and "[ball is win]" rules, but only a ball object present, which rule should the agent follow? How do you know?

## Architecture Onboarding

- Component map: Visual input → LLM prompt → In-context examples + instructions → Plan generation → Plan execution → Win condition verification
- Critical path: Visual input → Plan generation → Plan execution → Win condition verification
  The most critical step is the plan generation phase, where models must correctly identify relevant rules and compose appropriate action sequences.
- Design tradeoffs:
  - Visual vs. text input: Visual preserves spatial relationships but may introduce noise; text is more structured but loses spatial information
  - In-context examples vs. fine-tuning: In-context is more flexible but may require more examples for complex compositional tasks
  - High-level vs. low-level planning: High-level abstracts away implementation details but requires accurate environment understanding
- Failure signatures:
  - Grounding mistakes: Referring to objects that don't exist in the environment
  - Path planning mistakes: Incorrectly asserting that paths are blocked when they're not
  - Rule confusion: Failing to distinguish between active and inactive rules
  - Compositional failure: Unable to combine learned rule manipulations in novel sequences
- First 3 experiments:
  1. Test model performance on environments with only one active rule and no distractors to establish baseline capability
  2. Add object distractors (multiple objects but only one relevant to the active win rule) to test distractor filtering
  3. Add rule distractors (active win rules pointing to non-existent objects) to test rule relevance identification

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different model architectures (transformers vs. other architectures) perform on the Baba Is AI benchmark compared to LLMs?
- Basis in paper: [inferred] The paper tests only three LLMs and does not explore other model architectures that might perform differently on compositional generalization tasks.
- Why unresolved: The paper focuses exclusively on LLMs without comparing to alternative architectures that might handle rule manipulation and compositionality better.
- What evidence would resolve it: Systematic testing of various model architectures (e.g., neuro-symbolic systems, graph neural networks, or hybrid approaches) on the same benchmark tasks.

### Open Question 2
- Question: What specific aspects of rule manipulation (breaking vs. making rules) are most challenging for LLMs, and why?
- Basis in paper: [explicit] The paper notes that "the errors that LLMs make in solving the Baba Is AI environments are instructive" and mentions grounding mistakes and path planning mistakes, but doesn't systematically analyze which rule manipulation operations are most difficult.
- Why unresolved: While the paper identifies error types, it doesn't perform a detailed analysis of which specific rule manipulation operations (breaking, making, or combining rules) pose the greatest challenges.
- What evidence would resolve it: Detailed error analysis categorizing failures by the type of rule manipulation operation attempted, with statistical analysis of success rates for each operation.

### Open Question 3
- Question: How does the complexity of the environment (number of objects, rules, and distractors) affect LLM performance in a quantifiable way?
- Basis in paper: [explicit] The paper shows accuracy drops as distractors are added but doesn't quantify the relationship between environment complexity and performance degradation.
- Why unresolved: The paper demonstrates that performance decreases with added distractors but doesn't establish a precise relationship between the number of distractors/objects and accuracy.
- What evidence would resolve it: Controlled experiments varying the number of objects, rules, and distractors independently, with performance curves showing how accuracy scales with each dimension of complexity.

## Limitations

- The benchmark focuses on a specific game mechanic that may not generalize to all types of rule-based reasoning tasks
- In-context learning with only 10 examples may be insufficient for teaching the underlying compositional principles
- The visual input modality, while innovative, doesn't appear to compensate for fundamental reasoning limitations

## Confidence

- **High confidence**: Models fail dramatically on compositional generalization tasks requiring rule manipulation and combination
- **Medium confidence**: Visual input modality doesn't overcome compositional reasoning limitations
- **Medium confidence**: In-context learning with 10 examples is insufficient for teaching compositional principles

## Next Checks

1. **Test varying in-context example sizes**: Systematically evaluate model performance with 5, 10, 20, and 50 in-context examples to determine the minimum number needed for successful compositional generalization
2. **Compare visual vs. text input performance**: Re-run the benchmark with text descriptions of game states instead of visual inputs to isolate whether visual processing provides any advantage for rule manipulation tasks
3. **Analyze failure mode distributions**: Categorize and quantify the different types of errors (grounding mistakes, path planning errors, rule confusion, compositional failures) to identify which specific reasoning capabilities need improvement