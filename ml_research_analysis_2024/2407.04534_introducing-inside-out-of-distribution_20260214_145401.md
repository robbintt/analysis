---
ver: rpa2
title: Introducing 'Inside' Out of Distribution
arxiv_id: '2407.04534'
source_url: https://arxiv.org/abs/2407.04534
tags:
- inside
- data
- outside
- learning
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces the concept of "inside" OOD, distinguishing
  it from traditional "outside" OOD. It defines inside OOD as samples that fall within
  the convex hull of the training data but are still considered out-of-distribution
  by a predictor, while outside OOD refers to samples beyond the training data range.
---

# Introducing 'Inside' Out of Distribution

## Quick Facts
- arXiv ID: 2407.04534
- Source URL: https://arxiv.org/abs/2407.04534
- Authors: Teddy Lazebnik
- Reference count: 40
- Key outcome: Introduces inside vs outside OOD taxonomy, showing outside OOD causes greater performance degradation than inside OOD, with impact increasing non-linearly with dimensionality

## Executive Summary
This paper introduces a novel taxonomy of out-of-distribution (OOD) samples, distinguishing between "inside" OOD (samples within the convex hull of training data) and "outside" OOD (samples beyond the training data range). The study uses synthetic datasets to systematically analyze how these two types of OOD samples impact machine learning model performance. Results demonstrate that outside OOD generally causes more severe performance degradation than inside OOD, with the effect intensifying as dimensionality increases. The findings highlight the importance of distinguishing between inside and outside OOD for developing more effective counter-OOD methods and improving model robustness.

## Method Summary
The study employs synthetic datasets to create controlled scenarios for comparing inside and outside OOD samples. Performance is measured using normalized Root Mean Squared Error (RMSE) and F1 score across different dimensional configurations. The experimental design systematically varies the number of dimensions and feature distribution complexity to assess their impact on OOD detection and model performance. Sensitivity analysis is conducted to understand how different factors contribute to performance decline, with particular focus on the relative impact of inside versus outside OOD samples.

## Key Results
- Outside OOD causes significantly greater performance degradation than inside OOD
- Performance degradation increases non-linearly with dimensionality
- Both the number of dimensions and feature distribution complexity contribute to performance decline
- Outside OOD has a more pronounced negative impact compared to inside OOD

## Why This Works (Mechanism)
The mechanism behind the inside-outside OOD distinction relates to how models extrapolate beyond their training data. Outside OOD samples fall beyond the training data range, making extrapolation more challenging and error-prone. Inside OOD samples, while technically within the convex hull, may still exhibit distributional characteristics that differ from training data, leading to prediction uncertainty. The non-linear increase in performance degradation with dimensionality suggests that higher-dimensional spaces amplify the challenges of both types of OOD detection.

## Foundational Learning
- Convex hull geometry: Understanding the mathematical boundary of training data distribution is crucial for defining inside OOD
- Why needed: Provides the theoretical foundation for distinguishing inside from outside OOD
- Quick check: Can you identify the convex hull of a simple 2D dataset?

- Dimensionality effects: Higher dimensions exponentially increase the complexity of OOD detection
- Why needed: Explains why performance degradation scales non-linearly with dimensions
- Quick check: How does volume scale with dimension in high-dimensional spaces?

- Distributional shift: Even samples within the convex hull can exhibit distributional differences from training data
- Why needed: Clarifies why inside OOD samples can still be problematic
- Quick check: Can you identify subtle distributional shifts in a simple dataset?

## Architecture Onboarding
Component map: Data Generation -> OOD Classification -> Performance Evaluation -> Sensitivity Analysis
Critical path: Synthetic Data Creation → Inside/Outside OOD Labeling → Model Training → Performance Measurement → Analysis
Design tradeoffs: Synthetic data enables controlled experiments but limits real-world applicability; simple metrics provide clarity but may miss nuanced failure modes
Failure signatures: Outside OOD shows consistent large-scale performance drops; inside OOD shows more variable, context-dependent failures
First experiments:
1. Generate 2D synthetic dataset and manually classify inside vs outside OOD samples
2. Train a simple regression model and measure performance on both OOD types
3. Vary dimensionality from 2D to 10D and observe performance degradation patterns

## Open Questions the Paper Calls Out
None

## Limitations
- Entirely synthetic datasets may not capture real-world data complexity
- Controlled experimental setup limits generalizability to practical applications
- Performance metrics may not fully capture practical implications of OOD detection failures

## Confidence
High: The conceptual framework distinguishing inside and outside OOD is well-founded
Medium: Comparative analysis showing outside OOD causes greater degradation (based on synthetic data)
Medium: Relationship between dimensionality and OOD impact (limited to controlled scenarios)
Low: Specific quantitative thresholds and patterns may not translate to real-world applications

## Next Checks
1. Replicate the analysis using real-world datasets across multiple domains (medical imaging, NLP, financial data) to validate the framework in practical contexts

2. Test findings with state-of-the-art OOD detection methods beyond simple distance-based approaches, including deep learning-based detectors and conformal prediction methods

3. Conduct ablation studies to determine specific characteristics of inside OOD samples that lead to detection failures, examining patterns across different model architectures and training regimes