---
ver: rpa2
title: 'Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model'
arxiv_id: '2404.04167'
source_url: https://arxiv.org/abs/2404.04167
tags:
- chinese
- language
- data
- arxiv
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study introduces CT-LLM, a 2-billion parameter Chinese-centric
  large language model trained from scratch on a 1,200-billion-token corpus with 800
  billion Chinese tokens, 300 billion English tokens, and 100 billion code tokens.
  Unlike prior models that adapt English-trained LLMs to Chinese, CT-LLM is pre-trained
  primarily on Chinese data, achieving superior Chinese language understanding while
  maintaining strong English proficiency through supervised fine-tuning.
---

# Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model

## Quick Facts
- arXiv ID: 2404.04167
- Source URL: https://arxiv.org/abs/2404.04167
- Reference count: 40
- Key outcome: Introduces CT-LLM, a 2B parameter Chinese-centric model pre-trained from scratch on 1,200B tokens (800B Chinese, 300B English, 100B code), achieving superior Chinese language understanding while maintaining strong English proficiency through supervised fine-tuning and preference optimization.

## Executive Summary
This paper presents CT-LLM, a 2-billion parameter Chinese-centric large language model trained from scratch on a carefully curated corpus with 800 billion Chinese tokens. Unlike existing approaches that adapt English-trained models to Chinese, CT-LLM is pre-trained primarily on Chinese data, achieving superior Chinese language understanding while maintaining strong English proficiency. The model is aligned using preference optimization techniques and evaluated on a newly developed Chinese Hard Case Benchmark (CHC-Bench), where it demonstrates exceptional performance in Chinese language tasks. The research also introduces a high-quality Chinese pretraining dataset (MAP-CC) and challenges the prevailing English-centric LLM training paradigm.

## Method Summary
The authors develop CT-LLM by pre-training a 2B parameter model from scratch on a 1,200-billion-token corpus heavily weighted toward Chinese content (800B Chinese, 300B English, 100B code). The training process involves careful data curation and preprocessing, followed by supervised fine-tuning and preference optimization for alignment. A novel Chinese Hard Case Benchmark (CHC-Bench) is introduced to evaluate Chinese language understanding capabilities. The MAP-CC dataset serves as the foundation for pretraining, emphasizing high-quality Chinese web data. The model architecture follows standard transformer design but is optimized for Chinese language processing through its training approach.

## Key Results
- CT-LLM achieves superior Chinese language understanding compared to models adapted from English training
- Maintains strong English proficiency despite being pre-trained primarily on Chinese data
- Demonstrates exceptional performance on the newly developed Chinese Hard Case Benchmark (CHC-Bench)
- Challenges the prevailing English-centric LLM training paradigm by showing the benefits of Chinese-first pretraining

## Why This Works (Mechanism)
The success of CT-LLM stems from its fundamental departure from the English-centric pretraining paradigm. By training from scratch on a corpus dominated by Chinese content (800B out of 1,200B tokens), the model develops native Chinese language understanding capabilities rather than adapting from English foundations. This approach allows the model to capture the unique linguistic structures, idioms, and cultural contexts inherent to the Chinese language more effectively. The substantial Chinese token count (67% of total) ensures deep exposure to Chinese language patterns during the critical pretraining phase, while the inclusion of English (25%) and code (8%) tokens maintains multilingual and programming capabilities. The preference optimization alignment further refines the model's responses to better match human preferences in Chinese contexts.

## Foundational Learning
**Chinese Language Processing**: Understanding the unique characteristics of Chinese (logographic writing, lack of word boundaries, tonal aspects) - needed for proper tokenization and contextual understanding; quick check: examine tokenization strategies and handling of polysemy.
**Transformer Architecture**: The foundational neural network design for LLMs - needed for understanding model capacity and scaling; quick check: verify attention mechanisms and positional encoding implementations.
**Pretraining Paradigms**: Understanding the differences between English-centric and language-specific pretraining - needed to contextualize the innovation; quick check: compare token distribution and its impact on language representation.
**Preference Optimization**: Advanced alignment techniques that refine model behavior based on human feedback - needed for achieving high-quality outputs; quick check: evaluate alignment metrics and preference datasets.

## Architecture Onboarding
**Component Map**: Data Pipeline -> Pretraining Phase -> Supervised Fine-tuning -> Preference Optimization -> Inference
**Critical Path**: The most critical sequence is Data Pipeline -> Pretraining Phase, as the quality and composition of training data directly determines the model's language understanding capabilities. Without proper Chinese-dominated pretraining, subsequent fine-tuning and alignment cannot compensate for fundamental language representation gaps.
**Design Tradeoffs**: The authors chose a 2B parameter model size to balance computational efficiency with capability, accepting limitations in maximum performance potential for faster iteration and deployment. This contrasts with larger models that require more resources but may achieve higher absolute performance.
**Failure Signatures**: Models trained primarily on English data and then adapted to Chinese typically show poor handling of Chinese idioms, struggle with classical Chinese references, and exhibit unnatural phrasing in Chinese contexts. CT-LLM's approach specifically addresses these failure modes.
**First Experiments**: 
1. Evaluate Chinese idiom comprehension and generation compared to English-adapted models
2. Test classical Chinese text understanding and modern Chinese text generation quality
3. Assess cross-lingual performance to verify maintained English capabilities despite Chinese-dominated pretraining

## Open Questions the Paper Calls Out
The paper identifies several open questions regarding the scalability of the Chinese-centric pretraining approach to larger model sizes, the optimal balance between Chinese and other language tokens for multilingual proficiency, and the generalizability of the MAP-CC dataset curation methodology to other languages and domains.

## Limitations
- The Chinese Hard Case Benchmark (CHC-Bench) is newly developed and lacks external validation
- The 2B parameter model size limits direct comparisons with larger state-of-the-art models
- The exact quality distribution and potential redundancy within the 1,200-billion-token training corpus remain unclear

## Confidence
The confidence level in the reported performance improvements is **Medium**. While the paper presents extensive training details and a new benchmark, several key limitations affect the overall confidence: the newly developed benchmark lacks external validation, the model size limits comparison with larger models, and the training corpus composition details are incomplete.

## Next Checks
1. Independent evaluation of CHC-Bench on multiple Chinese language models to establish benchmark reliability and difficulty calibration
2. Controlled experiments comparing CT-LLM with models trained on different Chinese/English token ratios to isolate the impact of corpus composition
3. Release of the MAP-CC dataset and associated preprocessing scripts to enable reproducibility and independent assessment of data quality