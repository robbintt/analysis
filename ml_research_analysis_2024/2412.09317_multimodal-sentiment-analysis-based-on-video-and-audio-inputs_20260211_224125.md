---
ver: rpa2
title: Multimodal Sentiment Analysis based on Video and Audio Inputs
arxiv_id: '2412.09317'
source_url: https://arxiv.org/abs/2412.09317
tags:
- video
- audio
- dataset
- sentiment
- emotion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper tackles the challenge of building accurate multimodal
  sentiment analysis models using video and audio inputs. The authors fine-tuned separate
  transformer-based models for each modality: Facebook/wav2vec2-large for audio and
  Google/vivit-b-16x2-kinetics400 for video, using CREMA-D and RAVDESS datasets respectively.'
---

# Multimodal Sentiment Analysis based on Video and Audio Inputs

## Quick Facts
- arXiv ID: 2412.09317
- Source URL: https://arxiv.org/abs/2412.09317
- Authors: Antonio Fernandez; Suzan Awinat
- Reference count: 26
- Primary result: Combined video and audio models achieve better sentiment classification accuracy than single-modality approaches

## Executive Summary
This paper explores multimodal sentiment analysis by combining video and audio inputs using transformer-based models. The authors fine-tune separate models for each modality - wav2vec2-large for audio and ViViT-b-16x2-kinetics400 for video - achieving 72.59% and 88% accuracy respectively. They implement and test four fusion strategies to combine predictions: weighted averaging, confidence-based thresholding, dynamic weighting, and rule-based logic. The study demonstrates that multimodal fusion can improve sentiment classification accuracy, particularly when models have similar performance levels or when using weighted averaging for significantly different accuracies.

## Method Summary
The authors fine-tuned two pre-trained transformer models on emotion classification datasets - wav2vec2-large on CREMA-D for audio and ViViT-b-16x2-kinetics400 on RAVDESS for video. After achieving baseline accuracies of 72.59% and 88% respectively, they implemented four fusion frameworks to combine predictions: simple averaging of probabilities, weighted averaging based on model accuracy, confidence-based dynamic weighting, and rule-based logic. The fusion approaches were evaluated by comparing combined performance against individual modality baselines, with particular attention to scenarios where one modality significantly outperformed the other.

## Key Results
- Fine-tuned wav2vec2-large audio model achieved 72.59% accuracy on CREMA-D test set
- Fine-tuned ViViT video model achieved 88% accuracy on RAVDESS test set
- When model accuracies were similar, averaging and rule-based methods performed best
- When one model was significantly more accurate, weighted averaging provided superior results
- Multimodal fusion demonstrated improved sentiment classification over single-modality approaches

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Multimodal fusion of video and audio features improves sentiment classification accuracy over single-modality models.
- Mechanism: The model extracts visual features from video frames using a transformer-based architecture and acoustic features from audio using a wav2vec2 model. These features are processed independently and then combined using probabilistic averaging or rule-based logic to make final predictions.
- Core assumption: Visual and audio modalities contain complementary information that enhances sentiment classification when properly integrated.
- Evidence anchors:
  - [abstract] "The main objective of this paper is to prove the usability of emotion recognition models that take video and audio inputs."
  - [section] "The avarage of the probabilities for each emotion generated by the two previous models is utilized in the decision making framework."
  - [corpus] Weak evidence - corpus contains related multimodal sentiment analysis papers but none specifically address this exact averaging mechanism.
- Break condition: If one modality is significantly less accurate than the other, simple averaging may reduce overall performance rather than improve it.

### Mechanism 2
- Claim: Using pre-trained transformer models (wav2vec2 and ViViT) provides better feature extraction for multimodal sentiment analysis than training from scratch.
- Mechanism: The wav2vec2 model extracts high-level audio representations and ViViT extracts visual features from video frames. These pre-trained models are fine-tuned on domain-specific datasets (CREMA-D for audio, RAVDESS for video) to adapt to sentiment classification tasks.
- Core assumption: Pre-trained transformer models capture generalizable features that can be adapted to sentiment analysis with relatively small domain-specific datasets.
- Evidence anchors:
  - [section] "The fine-tuned models that been used are: Facebook/wav2vec2-large for audio and the Google/vivit-b-16x2-kinetics400 for video."
  - [section] "After roughly 1 hour and 15 minutes of training on our data, the model reached around 72.59% accuracy on our test set."
  - [corpus] Weak evidence - corpus contains related transformer-based sentiment analysis papers but doesn't specifically address the use of wav2vec2 and ViViT for this task.
- Break condition: If the pre-trained features don't align well with sentiment-relevant information, fine-tuning may not significantly improve performance.

### Mechanism 3
- Claim: Dynamic weighting based on confidence levels improves multimodal fusion performance compared to static averaging.
- Mechanism: When combining predictions from video and audio models, weights are assigned based on each model's confidence score for its prediction. Higher confidence predictions receive greater weight in the final decision.
- Core assumption: Model confidence is a reliable indicator of prediction quality, and confidence-weighted averaging produces better results than equal weighting.
- Evidence anchors:
  - [section] "The Dynamic Weighting Based on Confidence method (Fig. 5) involves using a dynamic weighting method based on the confidence of the predictions."
  - [section] "The confidence value for each prediction is calculated by dividing the probability of the predicted label by the total of all predicted probabilities."
  - [corpus] Weak evidence - corpus contains related confidence-based fusion methods but doesn't specifically address dynamic weighting for multimodal sentiment analysis.
- Break condition: If confidence scores don't correlate well with actual prediction accuracy, this method may perform worse than static averaging.

## Foundational Learning

- Concept: Transformer architecture and self-attention mechanisms
  - Why needed here: Both wav2vec2 and ViViT are transformer-based models that use self-attention to capture long-range dependencies in audio and video sequences respectively.
  - Quick check question: How does self-attention in transformers differ from traditional convolutional approaches for sequence modeling?

- Concept: Multimodal feature fusion strategies
  - Why needed here: The paper implements multiple fusion approaches (averaging, weighted averaging, confidence-based, rule-based) to combine predictions from different modalities.
  - Quick check question: What are the key differences between early fusion, late fusion, and hybrid fusion approaches in multimodal learning?

- Concept: Confidence score calculation and interpretation
  - Why needed here: The dynamic weighting method relies on calculating and interpreting confidence scores from each model's probability distribution over emotion classes.
  - Quick check question: How can you calculate a meaningful confidence score from a model's output probability distribution?

## Architecture Onboarding

- Component map: Raw video/audio → Feature extraction → Independent classification → Fusion → Final prediction
- Critical path: Raw video/audio → Feature extraction → Independent classification → Fusion → Final prediction
- Design tradeoffs:
  - Model complexity vs. training efficiency: Using pre-trained transformers reduces training time but requires significant GPU memory
  - Fusion strategy choice: Simple averaging is computationally efficient but may underperform when modality accuracies differ significantly
  - Dataset size: Fine-tuning on smaller domain-specific datasets vs. training from scratch on larger combined datasets
- Failure signatures:
  - Poor performance when one modality consistently outperforms the other
  - Degradation when modality data quality varies significantly
  - Overfitting on training datasets with limited speaker diversity
- First 3 experiments:
  1. Train and evaluate audio-only and video-only models separately to establish baseline performance
  2. Implement simple averaging fusion and compare combined performance against individual modality baselines
  3. Test dynamic weighting based on confidence scores and measure improvement over static averaging

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed multimodal sentiment analysis model perform when tested on datasets collected in real-world environments with varying lighting, background noise, and cultural contexts?
- Basis in paper: [inferred] The paper acknowledges limitations including controlled environments, single-culture data, and lack of real-world noise robustness.
- Why unresolved: The current model was trained and tested only on CREMA-D and RAVDESS datasets, which use controlled environments and actors from specific demographics.
- What evidence would resolve it: Testing the model on datasets collected in diverse real-world settings with varied lighting, background noise, and cultural contexts, and comparing performance metrics.

### Open Question 2
- Question: What is the optimal combination strategy for multimodal sentiment analysis when one modality (video or audio) is significantly more accurate than the other?
- Basis in paper: [explicit] The paper found that when one model was significantly more accurate, weighted averaging gave better results compared to other methods like averaging or rule-based logic.
- Why unresolved: While weighted averaging showed promise in limited testing, the paper notes that further testing would be required to see the consistency of these results across different scenarios.
- What evidence would resolve it: Extensive testing across various datasets and scenarios to determine the most reliable combination strategy when there is a significant accuracy gap between modalities.

### Open Question 3
- Question: How does adding a third modality (text) to the multimodal sentiment analysis framework affect overall performance compared to the current bimodal approach?
- Basis in paper: [explicit] The paper suggests future work could include adding a third model for text-based sentiment analysis using NLP techniques on audio transcriptions.
- Why unresolved: The authors did not implement or test a three-modality approach, focusing only on video and audio inputs.
- What evidence would resolve it: Implementation and testing of a three-modality framework, comparing performance metrics with the current bimodal approach across multiple datasets.

## Limitations

- Limited generalizability due to evaluation only on two specific datasets (CREMA-D and RAVDESS)
- Fusion framework validation lacks rigorous testing across diverse datasets and scenarios
- Decision frameworks, particularly rule-based logic, are not fully specified, making reproducibility challenging

## Confidence

- **High Confidence**: The individual model training methodology and reported baseline accuracies (72.59% for audio, 88% for video) are methodologically sound and well-documented.
- **Medium Confidence**: The general premise that multimodal fusion improves sentiment analysis is supported by existing literature, though specific fusion strategies need more rigorous testing.
- **Low Confidence**: The superiority claims for specific fusion frameworks (particularly rule-based logic and confidence-weighted methods) are not sufficiently validated across multiple test conditions.

## Next Checks

1. **Cross-dataset validation**: Test the combined framework on at least two additional multimodal sentiment datasets to assess generalizability and robustness to different emotional distributions.
2. **Modality imbalance testing**: Systematically evaluate performance when one modality has significantly lower accuracy than the other (e.g., 60% vs 90%) to validate the dynamic weighting approach claims.
3. **Real-time feasibility assessment**: Benchmark inference time and resource requirements for each fusion strategy to determine practical deployment constraints, particularly for the rule-based logic framework.