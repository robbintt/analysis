---
ver: rpa2
title: 'Label Convergence: Defining an Upper Performance Bound in Object Recognition
  through Contradictory Annotations'
arxiv_id: '2409.09412'
source_url: https://arxiv.org/abs/2409.09412
tags:
- label
- annotation
- convergence
- data
- datasets
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces "label convergence" as a method to estimate
  the maximum achievable performance for object detection models given inherent annotation
  inconsistencies. The authors propose a modified mean Average Precision (mAP) metric
  that works with two independent annotators by setting confidence scores to 0.99
  and randomly switching ground truth/prediction roles.
---

# Label Convergence: Defining an Upper Performance Bound in Object Recognition through Contradictory Annotations

## Quick Facts
- arXiv ID: 2409.09412
- Source URL: https://arxiv.org/abs/2409.09412
- Reference count: 40
- Current SOTA models (Co-DETR) approach label convergence bounds, suggesting future efforts should focus on cleaner test data and better annotation guidelines

## Executive Summary
This paper introduces "label convergence" as a method to estimate the maximum achievable performance for object detection models given inherent annotation inconsistencies. The authors propose a modified mean Average Precision (mAP) metric that works with two independent annotators by setting confidence scores to 0.99 and randomly switching ground truth/prediction roles. Using bootstrapping on five real-world datasets (LVIS, TexBiG, VinDr-CXR, plus problematic COCO/Open Images reannotated versions), they find label convergence for LVIS is between 62.63-67.52 mAP@[0.5:0.95:0.05] with 95% confidence. The study also extends this to multiple annotators using Krippendorff's Alpha, showing a strong correlation (R²=0.85) between annotation consistency and mAP.

## Method Summary
The method estimates label convergence by treating human annotations as model detections with confidence 0.99, then randomly switching ground truth/prediction roles to compute modified mAP. Bootstrapping (1,000 subsets of 10% each) provides confidence intervals for convergence thresholds. For multiple annotators, Krippendorff's Alpha measures inter-annotator agreement, with linear regression against mAP values enabling convergence estimation. The approach analyzes variation types through IoU-based matching to distinguish merged, unmerged, and split instances across annotations.

## Key Results
- LVIS dataset label convergence: 62.63-67.52 mAP@[0.5:0.95:0.05] with 95% confidence
- Strong correlation (R²=0.85) between Krippendorff's Alpha and mAP enables convergence estimation for multiple annotators
- Current SOTA models (Co-DETR at 66.8 mAP) approach label convergence bounds, suggesting model architectures are nearing practical limits given annotation noise

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Modified mAP can estimate label convergence by treating human annotations as model detections with confidence 0.99 and randomly switching roles
- Mechanism: By setting confidence scores to 0.99 (since human annotations don't produce overlapping detections) and swapping ground truth/prediction roles, the modified mAP measures agreement between annotators without requiring explicit confidence scores
- Core assumption: Human annotators produce consistent, non-overlapping detections for the same object
- Evidence anchors:
  - [abstract] "We propose a modified mean Average Precision (mAP) metric that works with two independent annotators by setting confidence scores to 0.99 and randomly switching ground truth/prediction roles"
  - [section 4.1] "We set the confidence score of all annotations to 0.99, as human annotations are unlikely to produce severely overlapping detections of the same class"
- Break condition: If human annotations frequently produce overlapping detections or if confidence scores matter for evaluation

### Mechanism 2
- Claim: Krippendorff's Alpha correlates with mAP, enabling convergence estimation for multiple annotators
- Mechanism: Linear regression between K-α (inter-annotator agreement) and mAP (modified for two annotators) allows interpolation of convergence bounds for datasets with 3+ annotators
- Core assumption: The relationship between annotation consistency and mAP is linear across different datasets
- Evidence anchors:
  - [section 4.2] "The regression shows a Pearson correlation of ρ = 0.92 and an R-squared value R2 = 0.85, indicating a good model fit"
  - [abstract] "The study also extends this to multiple annotators using Krippendorff's Alpha, showing a strong correlation (R²=0.85) between annotation consistency and mAP"
- Break condition: If the relationship between K-α and mAP becomes non-linear with more annotators or different annotation types

### Mechanism 3
- Claim: Bootstrapping with 10% sampling provides reliable confidence intervals for label convergence
- Mechanism: Repeated sampling of subsets (1,000 samples of 10% each) captures variability in annotation consistency across the dataset
- Core assumption: 10% subset sampling is representative of the full dataset's annotation variation
- Evidence anchors:
  - [section 4.1] "we use bootstrapping, sampling 1,000 subsets, each with 10% of the available images of the respective dataset and evaluate the modified mAP on each of these subsets"
  - [section 4.1] "By evaluating the 95% confidence interval of the sampling distribution, we capture the variability of the data and infer the convergence threshold for the entire dataset"
- Break condition: If annotation variation is highly non-uniform across the dataset or if 10% samples don't capture rare annotation patterns

## Foundational Learning

- Concept: Inter-annotator agreement metrics (Cohen's Kappa, Krippendorff's Alpha)
  - Why needed here: Understanding how to measure consistency between multiple annotators is fundamental to estimating label convergence
  - Quick check question: What's the difference between Cohen's Kappa and Krippendorff's Alpha in handling missing data?

- Concept: Mean Average Precision (mAP) calculation
  - Why needed here: The modified mAP is the core metric for estimating label convergence, requiring understanding of precision-recall curves and IoU thresholds
  - Quick check question: How does mAP handle multiple detections of the same object?

- Concept: Bootstrapping methodology
  - Why needed here: Estimating confidence intervals for convergence thresholds requires understanding sampling distributions and statistical inference
  - Quick check question: Why might 10% subset sampling be chosen over larger or smaller percentages?

## Architecture Onboarding

- Component map: Data preprocessing → Modified mAP calculation → Bootstrapping → Confidence interval estimation → K-α correlation (for >2 annotators) → Convergence threshold reporting
- Critical path: The most critical components are the modified mAP calculation and bootstrapping, as errors here propagate to all downstream estimates
- Design tradeoffs: Modified mAP trades accuracy (assuming perfect human confidence) for practicality (avoiding need for confidence scores)
- Failure signatures: Wide confidence intervals (>10 mAP points) suggest inconsistent annotation patterns; poor K-α correlation (<0.7) indicates non-linear relationship
- First 3 experiments:
  1. Verify modified mAP produces reasonable values on a simple two-annotator subset
  2. Test bootstrapping stability by comparing confidence intervals from different sample sizes
  3. Validate K-α to mAP correlation on a dataset with known annotation variation patterns

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of state-of-the-art models on the LVIS dataset change when evaluated with a stricter IoU threshold (e.g., 0.75 instead of the standard 0.5)?
- Basis in paper: [explicit] The paper analyzes LVIS convergence at different IoU thresholds (0.5:0.95:0.05) and finds the upper bound is 62.63-67.52 mAP, with SOTA models approaching this limit.
- Why unresolved: The paper only provides convergence bounds for LVIS at the standard IoU range used in COCO evaluation, but doesn't test how model performance behaves at stricter thresholds.
- What evidence would resolve it: Empirical evaluation of current SOTA models (like Co-DETR) on LVIS using stricter IoU thresholds (0.75 or 0.8) to determine if performance drops below the convergence threshold or if models maintain their proximity to the upper bound.

### Open Question 2
- Question: Does the linear relationship between Krippendorff's Alpha and mAP (mAP = 0.836·α + 0.197) hold for datasets outside of object detection, such as semantic segmentation or instance segmentation?
- Basis in paper: [explicit] The authors establish this linear correlation for object detection datasets (LVIS, COCO, Open Images) and use it to extrapolate convergence thresholds for TexBiG and VinDr-CXR.
- Why unresolved: The paper only validates this relationship within object detection tasks and two specific datasets with multiple annotators, leaving uncertainty about its generalizability to other computer vision tasks.
- What evidence would resolve it: Empirical testing of the linear relationship across diverse datasets including semantic segmentation (e.g., Cityscapes, ADE20K) and other multi-annotator datasets from different computer vision domains.

### Open Question 3
- Question: What is the optimal ratio of multi-annotated data to single-annotated data needed in a dataset to reliably estimate label convergence and improve model robustness to annotation noise?
- Basis in paper: [inferred] The paper suggests including multi-annotated data to investigate annotation variation and make issues visible from the outset, but doesn't specify how much of the dataset should be multi-annotated.
- Why unresolved: The authors propose multi-annotation as a solution but don't provide quantitative guidance on the proportion of data that needs repeated annotation to achieve meaningful convergence analysis.
- What evidence would resolve it: Systematic experiments varying the percentage of multi-annotated data (e.g., 5%, 10%, 25%, 50%) in a dataset and measuring the stability and reliability of convergence threshold estimates across these different ratios.

## Limitations
- The modified mAP assumes perfect human annotation confidence (0.99), which may overestimate the true upper bound
- The linear correlation between Krippendorff's Alpha and mAP (R²=0.85) shows good fit but doesn't prove causation or guarantee extrapolation accuracy
- Bootstrapping with 10% sampling may miss rare annotation patterns or systematic biases present in smaller subsets

## Confidence

- **High Confidence**: The methodology for modified mAP calculation and bootstrapping implementation is well-defined and reproducible. The observation that current SOTA models approach label convergence bounds is empirically verifiable.
- **Medium Confidence**: The correlation between K-α and mAP suggests a relationship but requires more datasets to validate generalizability. The claim that future improvements should focus on test data quality rather than model architecture is reasonable but not definitively proven.
- **Low Confidence**: The exact impact of annotation variations (merged/unmerged instances) on convergence thresholds requires more granular analysis. The assumption that 10% bootstrapping samples are representative across all dataset types hasn't been fully validated.

## Next Checks
1. **Sensitivity Analysis**: Test how convergence thresholds vary with different confidence score assignments (0.95, 0.99, 1.0) to understand the impact of the perfect confidence assumption on upper bound estimates.

2. **Dataset Generalization**: Apply the methodology to additional object detection datasets with repeated annotations to verify whether the K-α to mAP correlation remains consistent across different annotation styles and object categories.

3. **Error Type Decomposition**: Analyze how different types of annotation variations (missed objects, wrong class, merged instances) contribute to the convergence threshold to identify which error types are most limiting for current models.