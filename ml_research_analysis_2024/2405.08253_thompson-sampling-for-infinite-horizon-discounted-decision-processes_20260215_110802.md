---
ver: rpa2
title: Thompson Sampling for Infinite-Horizon Discounted Decision Processes
arxiv_id: '2405.08253'
source_url: https://arxiv.org/abs/2405.08253
tags:
- regret
- expected
- policy
- state
- period
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies Thompson sampling for Markov decision processes
  with unknown parameters. The authors show that the standard definition of regret
  can grow super-linearly in general state settings and fail to capture learning.
---

# Thompson Sampling for Infinite-Horizon Discounted Decision Processes

## Quick Facts
- arXiv ID: 2405.08253
- Source URL: https://arxiv.org/abs/2405.08253
- Reference count: 3
- Primary result: Thompson sampling achieves exponential convergence of expected residual regret in general MDP settings

## Executive Summary
This paper studies Thompson sampling for Markov decision processes with unknown parameters, addressing a fundamental limitation in the standard regret definition that can grow super-linearly in general state settings. The authors introduce a new metric called expected residual regret that measures regret against the optimal reward moving forward from the current period, rather than accumulating sunk costs. They prove that under appropriate assumptions, the expected residual regret of Thompson sampling converges exponentially fast to zero, and the posterior sampling error converges to zero almost surely. This provides a viable concept of learning for sampling algorithms in broader settings than previously considered.

## Method Summary
The paper analyzes Thompson sampling for θ-MDPs where rewards and transitions depend on an unknown parameter θ. The algorithm maintains a Bayesian posterior over parameters and, at each period, samples θt from the posterior and chooses actions using the optimal policy for that sampled parameter. The posterior is updated using Bayes' theorem based on observed rewards and transitions. The analysis focuses on expected residual regret, which measures the difference between the algorithm's value and the optimal value moving forward from the current state, decomposed into finite-time regret, state regret, and residual regret components.

## Key Results
- Expected residual regret of Thompson sampling converges exponentially fast to zero in general state settings
- Posterior sampling error converges to zero almost surely under appropriate assumptions
- Standard expected regret can grow super-linearly in MDPs with absorbing states, failing to capture learning progress

## Why This Works (Mechanism)

### Mechanism 1
Thompson sampling's expected residual regret converges exponentially fast to zero because the residual regret is decomposed into three parts: finite-time regret (sunk), state regret (unavoidable future consequences), and residual regret (actionable future performance). The third component decays exponentially because the posterior belief concentrates on the true parameter, making the temporal difference error vanish. This requires transition densities bounded away from zero to ensure sufficient exploration.

### Mechanism 2
Thompson sampling achieves complete learning (posterior concentrates on true parameter) almost surely because the posterior sampling error decays exponentially in expectation, and under the existence of limits, the Dominated Convergence Theorem ensures convergence almost surely. This requires the relative entropy between parameter models to be bounded below, making parameters distinguishable.

### Mechanism 3
The standard expected regret can grow super-linearly in general MDP settings because it accumulates sunk costs that cannot be recovered, masking actual learning progress. The residual regret metric ignores these sunk costs and measures only future performance. This occurs when the MDP has absorbing states or non-trivial state evolution that depends on actions.

## Foundational Learning

- **Markov Decision Processes with unknown parameters (θ-MDP)**: Extends standard MDPs where rewards and transitions depend on unknown parameter θ. Needed because the paper analyzes Thompson sampling in this more general setting.
  - Quick check: What distinguishes a θ-MDP from a standard MDP? (Answer: rewards and transitions depend on unknown parameter θ)

- **Bayesian posterior updating and Thompson sampling decision rule**: The algorithm samples parameters from posterior and acts optimally assuming the sample is true. Needed to understand how the algorithm operates and posterior convergence.
  - Quick check: How does Thompson sampling choose actions in period t? (Answer: samples θt from posterior and chooses optimal action for that θt)

- **Asymptotic Discount Optimality (ADO) and its equivalence to vanishing residual regret**: Provides theoretical framework connecting residual regret to established optimality concepts in adaptive control. Needed to interpret residual regret convergence.
  - Quick check: What is the relationship between θ-ADO and residual regret? (Answer: They are equivalent by Lemma 6.1)

## Architecture Onboarding

- **Component map**: Parameter space P (finite) -> State space X and control space U (Borel) -> Reward density fθ and transition density qθ -> Posterior update πt(·|Ht) using Bayes' theorem -> Thompson sampling policy τt sampling from posterior -> Value functions νθ(x) for optimal policy -> Residual regret calculation

- **Critical path**: 1) Initialize prior π0 over parameters, 2) For each period t: observe state Xt, sample θt from πt, choose action Ut using µθt, observe reward Rt and next state Xt+1, 3) Update posterior πt+1 using Bayes' rule, 4) Compute residual regret using νθ(Xn) - Vτ,θ(x0)(n), 5) Analyze convergence properties

- **Design tradeoffs**: General state spaces vs. computational tractability, exponential convergence vs. slower rates with weaker assumptions, complete learning requirement vs. partial learning scenarios, finite parameter space vs. continuous parameter extensions

- **Failure signatures**: Posterior not concentrating (violation of Assumption 1), residual regret not decaying (non-identifiable parameters), linear/super-linear growth in standard regret (absorbing states), oscillating limits (violation of Assumption 2)

- **First 3 experiments**: 1) Implement Thompson sampling on a simple 3-state MDP with known optimal policy to verify residual regret calculation, 2) Test exponential decay of posterior error on a single-state MDP with two parameter candidates, 3) Demonstrate super-linear growth of standard regret in an absorbing MDP while residual regret decays

## Open Questions the Paper Calls Out

### Open Question 1
How do the assumptions of complete learning and vanishing probabilistic residual regret relate to each other? Are they equivalent, or does one imply the other under certain conditions? The paper shows that under Assumptions 1 and 2, the posterior distribution converges to a point mass at the true parameter, and under Assumptions 1 and 3, the probabilistic residual regret vanishes, but does not explicitly establish a connection between these two concepts.

### Open Question 2
How does the performance of Thompson sampling in terms of expected residual regret compare to other adaptive control algorithms, such as minimum contrast estimators or posterior sampling algorithms with different exploration strategies? The paper focuses on Thompson sampling and establishes its performance guarantees, but does not compare its performance to other algorithms.

### Open Question 3
How does the choice of the discount factor affect the performance of Thompson sampling in terms of expected residual regret? Are there any specific values of the discount factor that lead to optimal performance? The paper studies Thompson sampling under the discounted reward criterion with discount factor β ∈ [0, 1), showing exponential convergence for any β, but does not investigate the impact of the discount factor on performance.

## Limitations

- The analysis relies heavily on Assumption 1 (bounded densities and relative entropy conditions) which may not hold in many practical applications
- Results are established for finite parameter spaces, with extension to continuous spaces requiring additional technical work
- The paper focuses on theoretical guarantees without extensive empirical validation on complex MDPs

## Confidence

- **Exponential convergence claim**: Medium confidence - theoretical proof provided but depends on technical assumptions that require careful verification
- **Complete learning result**: Medium confidence - proof relies on Dominated Convergence Theorem and existence of limits
- **Super-linear regret claim**: High confidence - clear counterexamples provided demonstrating the phenomenon

## Next Checks

1. **Implementation Test**: Implement Thompson sampling on a simple MDP with two parameters and verify that residual regret decays exponentially while standard regret grows linearly when the wrong initial parameter guess leads to absorption.

2. **Assumption Verification**: Test Assumption 1 (bounded densities and relative entropy) on a continuous-state MDP to determine how restrictive these conditions are in practice and identify when they might fail.

3. **Posterior Concentration**: Simulate Thompson sampling over many runs on a parameterized MDP and empirically measure the convergence rate of the posterior to the true parameter, comparing observed rates to the theoretical exponential bound.