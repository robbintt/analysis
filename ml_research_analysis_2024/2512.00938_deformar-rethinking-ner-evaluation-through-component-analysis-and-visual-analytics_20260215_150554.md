---
ver: rpa2
title: 'DeformAr: Rethinking NER Evaluation through Component Analysis and Visual
  Analytics'
arxiv_id: '2512.00938'
source_url: https://arxiv.org/abs/2512.00938
tags:
- entity
- arabic
- these
- across
- english
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'Problem: ArabiNER performance lags English despite larger pretraining
  data; causes include tokenisation bias, dataset quality (annotation inconsistencies),
  and evaluation effects. Core method: DeformAr framework decomposes NER into data
  and model components, combining cross-component metrics (F1, loss, confidence, silhouette,
  OOV, ambiguity) with behavioural interpretability (attention, representation space,
  clustering) via interactive dashboard.'
---

# DeformAr: Rethinking NER Evaluation through Component Analysis and Visual Analytics

## Quick Facts
- **arXiv ID**: 2512.00938
- **Source URL**: https://arxiv.org/abs/2512.00938
- **Reference count**: 0
- **Primary result**: Cross-lingual NER performance gaps are driven by interacting data, model, and evaluation factors, not model capability alone.

## Executive Summary
DeformAr is a framework that decomposes Named Entity Recognition into data, model, and evaluation components to diagnose cross-lingual performance gaps. It integrates quantitative metrics (tokenization, ambiguity, loss, silhouette, OOV) with behavioral interpretability (attention, representation space, clustering) via an interactive dashboard. Applied to Arabic/English NER, it reveals that Arabic model underperformance stems from higher lexical sparsity, annotation noise, and evaluation artifacts rather than intrinsic model weakness.

## Method Summary
The framework fine-tunes language-specific BERT models on CoNLL-2003 (English) and ANERCorp (Arabic), extracting token-level behavioral metrics and representation spaces. These are visualized via an interactive dashboard that enables cross-component comparison. The pipeline includes data preprocessing, model training, output extraction (hidden states, attention, probabilities), metric computation (silhouette, ambiguity, consistency), and visualization (UMAP, error taxonomies).

## Key Results
- BERT-base on CoNLL-2003 achieved token-F1 0.905 vs AraBERTv02 on ANERCorp 0.821; entity-F1 0.913 vs 0.833.
- Arabic model exhibited higher loss, lower silhouette, greater annotation inconsistency, and wider precision-recall gaps.
- Tokenization reduced type diversity more in English; Arabic showed higher ambiguity and OOV, especially for MISC.
- Error patterns diverged: Arabic dominated by boundary and exclusion errors; English by inclusion and cross-entity confusion.

## Why This Works (Mechanism)

### Mechanism 1: Cross-Component Diagnostics via Metric Decomposition
- **Claim:** Quantifying discrepancies across data, model, and evaluation subcomponents reveals root causes of cross-lingual performance gaps more effectively than aggregate F1 alone.
- **Mechanism:** The framework decomposes NER into measurable subcomponents (tokenization rate, annotation ambiguity, loss distributions, silhouette scores, error taxonomies). By isolating and comparing these across Arabic/English, it attributes gaps to specific interacting factors (e.g., Arabic's higher lexical sparsity → higher loss, lower silhouette).
- **Core assumption:** Aggregate metrics mask component-level imbalances; measuring each subcomponent exposes causal chains (data → model → performance).
- **Evidence anchors:**
  - [abstract] "DeformAr integrates ... cross-component analysis ... addressing the 'what,' 'how,' and 'why' behind observed discrepancies."
  - [section 4.3-4.5] Quantitative tables comparing Arabic/English across ambiguity, OOV, loss, confidence, and error distributions.
  - [corpus] Weak – Corpus neighbors focus on model robustness or applications, not component-level decomposition.
- **Break condition:** If all subcomponent metrics show no significant cross-lingual variation, the performance gap may stem from unmeasured factors (e.g., external domain shift).

### Mechanism 2: Behavioral Linking via Token-Level Metrics
- **Claim:** Interactive visual analytics that link token-level behavioral metrics (loss, confidence, ambiguity) to representation space structures explain *how* data properties influence model decisions.
- **Mechanism:** The dashboard layers UMAP projections (global structure) with token behavioral scores (e.g., high-tokenization-rate tokens map to low-confidence, high-uncertainty regions). This connects dataset characteristics (e.g., tokenization) to model instability.
- **Core assumption:** Fine-grained, instance-level inspection can identify systematic error patterns invisible in aggregate views.
- **Evidence anchors:**
  - [section 5.2-5.3] Describes interactive tabs that dynamically color UMAP plots by behavioral metrics (loss, token confidence, token ambiguity).
  - [section 5.5] Shows concrete patterns: Arabic O-tokens with high confidence/high loss cluster separately, indicating systematic errors.
  - [corpus] Moderate – Corpus includes interactive NLP visualization tools (e.g., T3-Vis, LIT), but none integrate cross-lingual behavioral metrics as DeformAr does.
- **Break condition:** If behavioral metrics show no correlation with representation clusters, the model's spatial organization may be random or task-irrelevant.

### Mechanism 3: Annotation-Scheme-Aware Evaluation
- **Claim:** Understanding how annotation conventions (IOB1 vs. IOB2) and scorer repair strategies interact with data noise is essential for fair cross-lingual comparison.
- **Mechanism:** The framework extracts span-level outcomes (TP/FP/FN) under multiple schemes, revealing that IOB2 improvements in Arabic are partly due to correcting gold-standard IOB violations, not just better modeling.
- **Core assumption:** Evaluation metrics are not neutral; they reflect scorer logic as much as model quality.
- **Evidence anchors:**
  - [section 4.5] Detailed error taxonomy showing how IOB1 vs. IOB2 changes FP/FN counts differently for Arabic vs. English.
  - [section 4.5.4] Quantifies "false-positive false positives": correct predictions marked wrong due to IOB2 strictness and gold noise.
  - [corpus] Good – Corpus includes papers on NER reproducibility (e.g., seqeval, SeqScore) that address scorer mechanics, but DeformAr integrates this into component analysis.
- **Break condition:** If all annotation schemes produce identical error distributions, annotation conventions may not be a primary factor in observed gaps.

## Foundational Learning

**Concept: Token-Level Behavioral Metrics**
- **Why needed here:** These metrics (loss, confidence, uncertainty, ambiguity) convert abstract model uncertainty into inspectable signals that correlate with data properties (e.g., OOV, tokenization rate).
- **Quick check question:** If a token has high loss and low confidence but appears frequently in training, what does that suggest about the annotation consistency or morphological complexity?

**Concept: Representation Space Analysis (UMAP + Silhouette)**
- **Why needed here:** Visualizing fine-tuned token embeddings reveals whether the model organizes entities by type/boundary. Silhouette scores quantify separability. Together, they show if representational quality aligns with performance gaps.
- **Quick check question:** Would you interpret high silhouette for PER but low for MISC as a data quality issue or model capacity limitation?

**Concept: Annotation Scheme Mechanics (IOB1/IOB2, Repair vs. Discard)**
- **Why needed here:** Scorer logic determines what counts as an error. Without controlling for this, cross-lingual F1 differences may reflect annotation noise rather than model capability.
- **Quick check question:** If switching from IOB1 to IOB2 improves Arabic F1 significantly more than English, what is the most plausible explanation?

## Architecture Onboarding

**Component map:**
- Data Extraction -> Model Fine-tuning -> Output Extraction -> Metric Calculation -> Dashboard Visualization

**Critical path:**
1. Data preparation (format → tokenization with label alignment).
2. Model fine-tuning (same architecture/hyperparams for both langs).
3. Output extraction during evaluation (probabilities, loss, attentions, hidden states).
4. Metric calculation (silhouette, ambiguity, consistency, correlation).
5. Output consolidation into unified format loaded by dashboard.

**Design tradeoffs:**
- **Granularity vs. usability:** Token-level metrics provide fine-grained signals but require careful labeling and storage (millions of tokens). Dashboard balances per-tag aggregation with instance drill-down.
- **Cross-lingual comparability vs. language specificity:** Framework uses same metrics for both langs, but some (e.g., tokenization rate) reflect morphological differences inherently.
- **Interactive analysis vs. automation:** Manual exploration is necessary for anomaly detection, but limits scalability; future could add automated clustering of error patterns.

**Failure signatures:**
- **Annotation scheme mismatch:** High FP/FN shifts between IOB1/IOB2 in Arabic indicate gold-standard IOB violations (Section 4.5). Check dataset documentation and run scheme integrity validator.
- **Representation collapse:** If silhouette scores are near 0 for all tags, model failed to learn separable representations – check training convergence, learning rates.
- **Tokenization distortion:** Extreme OOV rates post-tokenization or inflated type diversity may signal tokenizer-lang mismatch (Section 4.3).
- **Metric divergence:** If token-level macro-F1 gaps are large but span-level gaps shrink, suspect averaging artifacts and scheme sensitivity.

**First 3 experiments:**
1. **Data sanity check:** Load ANERCorp and CoNLL-2003. Verify token counts, entity tag distributions. Confirm IOB2 compliance in gold labels (run scheme validator).
2. **Tokenization impact:** Compute type/token ratios pre/post tokenization for both languages. Expect English reduction > Arabic. Validate WordPiece vocab sizes differ (BERT ~30k vs AraBERT ~64k).
3. **Model baseline:** Fine-tune BERT-base on CoNLL, AraBERTv02 on ANERCorp with default first-subword labeling. Report token-level macro-F1 and span-level F1 (IOB1 vs IOB2) to establish baseline gaps.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question:** How do tokenization strategies differently reshape lexical structures and ambiguity rates in Arabic vs. English NER datasets, and how do these changes propagate to impact model performance and error distributions?
- **Basis in paper:** [explicit] The paper's data-component analysis explicitly identifies tokenization as a key factor altering vocabulary statistics (e.g., type reduction, OOV rates) and overlap patterns, noting these effects diverge between Arabic and English.
- **Why unresolved:** The analysis documents *what* changes occur (e.g., English sees stronger type reduction, Arabic retains more diversity) but does not fully trace *how* these lexical shifts causally influence specific model behaviors like boundary errors or tag confusion. The mechanism linking tokenization-induced variance to downstream prediction inconsistencies remains inferred but not quantified.
- **What evidence would resolve it:** Controlled experiments where tokenization algorithms are swapped (e.g., BPE vs. WordPiece) while holding all other factors constant, combined with metrics that track how token-level alterations affect loss, confidence, and representation coherence across entity tags in both languages.

### Open Question 2
- **Question:** What specific internal model behaviors (e.g., attention head specialization, layer-wise representational stability) diverge between Arabic and English NER models during fine-tuning, and how do these structural differences correlate with precision-recall trade-offs and calibration errors?
- **Basis in paper:** [explicit] The Training Impact section explicitly compares attention similarity matrices and representation-space evolution, finding more distributed parameter updates and cleaner clustering in English, versus more localized and unstable changes in Arabic.
- **Why unresolved:** While the paper identifies *that* Arabic representations are noisier and attention adaptations are more fragmented, it does not establish a direct causal link from these internal patterns to observed metric-level discrepancies (e.g., higher loss, lower silhouette in Arabic). The extent to which instability in attention or representation coherence *drives* poor calibration or boundary errors remains to be tested.
- **What evidence would resolve it:** Regression analyses or mediation models that use quantifiable attention metrics (e.g., head entropy, stability scores) or representation metrics (e.g., silhouette, cluster purity) as predictors of token-level loss or boundary error rates, controlling for data characteristics.

### Open Question 3
- **Question:** How do evaluation scheme choices (e.g., IOB1 vs. IOB2, repair vs. discard) and aggregation strategies (macro vs. micro averaging) structurally alter the perceived magnitude and nature of the Arabic-English performance gap, and to what extent can these artifacts be separated from true model deficiencies?
- **Basis in paper:** [explicit] The evaluation-component analysis explicitly demonstrates that switching from IOB1 to IOB2 improves AraBERTv02's precision-recall balance (especially for MISC/ORG), while BERT shows minor changes, and that macro averaging exaggerates differences due to class imbalance.
- **Why unresolved:** The paper shows *that* these evaluation artifacts matter but does not quantify how much of the total cross-lingual gap is attributable to evaluation design versus intrinsic model capability. For instance, it is unclear how much of AraBERTv02's macro-F1 deficit persists after correcting for scheme inconsistencies or using a balanced aggregation.
- **What evidence would resolve it:** Re-evaluating both models on a standardized, cleaned dataset using a consistent annotation scheme and multiple averaging strategies, then decomposing the total performance difference into components attributable to (a) data noise, (b) scheme sensitivity, and (c) aggregator bias via statistical decomposition.

### Open Question 4
- **Question:** Why does the Arabic model exhibit greater misalignment between its last hidden state representations and output layer predictions than English, despite English having a higher tokenization rate and ambiguity? What data or model factors (e.g., annotation inconsistency density, morphological sparsity) explain this representational-instability asymmetry?
- **Basis in paper:** [inferred] The behavioral analysis finds that Arabic representations show lower true silhouette (poorer separability) and that tokens with high loss often have high confidence (poor calibration). The paper infers this relates to annotation noise and sentence-structure anomalies, but does not measure the relative contribution of these factors.
- **Why unresolved:** The observed subcomponent misalignment (e.g., a token embedded in an entity cluster but predicted as O) is documented anecdotally, but its prevalence across entity types and its relationship to specific data properties (e.g., OOV rate, I-tag inconsistency) are not systematically quantified. The causal path from data irregularity to internal-external divergence remains speculative.
- **What evidence would resolve it:** A meta-analysis correlating token-level measures of data instability (e.g., training ambiguity, schema violation frequency) with metrics of subcomponent divergence (e.g., difference between true and predicted silhouette) across a stratified sample of tokens in both languages. Controlled data augmentation experiments could also test whether injecting synthetic consistency or inconsistency changes this divergence.

### Open Question 5
- **Question:** Can the DeformAr framework's component-based diagnostic approach be generalized to Large Language Models (LLMs) for generative NER tasks, and what modifications are necessary to adapt its extraction pipeline, behavioral metrics, and visual analytics to autoregressive decoding and instruction-tuning contexts?
- **Basis in paper:** [inferred] The Future Work section explicitly suggests extending DeformAr to LLMs, noting the need to adapt components for generative settings, but provides no implementation details or validation.
- **Why unresolved:** DeformAr currently relies on properties like token-level loss and attention similarity that assume a classification head and bidirectional context. LLMs use decoder-only architectures, generate tokens sequentially, and may not produce direct loss per entity token in the same way. The framework's decomposition logic and metric suite are untested in this regime.
- **What evidence would resolve it:** A port of the extraction pipeline to an LLM-based NER system (e.g., prompting GPT or fine-tuning T5 for span extraction), followed by a comparative study that redefines the "output layer" and "representation" subcomponents for generation. Behavioral metrics would need rethinking (e.g., generation perplexity, token-by-token confidence) and the dashboard would require new visualizations for sequence generation errors.

## Limitations
- Analysis confined to two languages and two datasets, limiting typological and dataset diversity.
- Causal attribution of performance gaps to specific factors remains correlational; unmeasured confounders could contribute.
- Behavioral analysis depends on UMAP projections and clustering, which are sensitive to hyperparameters and initialization.
- Framework assumes span-level F1 differences are meaningful only after controlling for annotation scheme mechanics, complicating real-world deployment.

## Confidence
- **High**: Claims about measurable cross-lingual differences in tokenization impact, annotation ambiguity, and error type distributions (Sections 4.3-4.5).
- **Medium**: Claims that component-level diagnostics reveal root causes of performance gaps (Mechanism 1), as these rely on interpretation of correlational patterns.
- **Medium**: Claims about behavioral linking (Mechanism 2), given dependence on UMAP/visualization stability and lack of statistical significance testing.
- **Medium**: Claims about annotation-scheme-aware evaluation (Mechanism 3), as they hinge on specific scorer logic and gold-standard noise, which may not generalize.

## Next Checks
1. **Dataset Integrity and Scheme Validation**: Run a systematic IOB scheme validator on both ANERCorp and CoNLL-2003, and quantify annotation inconsistencies (e.g., missing B- tags, illegal transitions). Report the proportion of sentences requiring IOB1-to-IOB2 repair and how this differs by language.
2. **Parameter Sensitivity Analysis**: For UMAP and KMeans, conduct a grid search over n_neighbors (5–30), min_dist (0.0–0.99), and random_state seeds (1–10). Report silhouette stability and cluster coherence (e.g., ARI) across settings; identify thresholds where results become unreliable.
3. **Cross-Dataset Generalization**: Apply DeformAr to a third language (e.g., French from NoDaLiDa corpus) with the same model and pipeline. Compare whether the observed Arabic/English patterns (e.g., tokenization impact, OOV rates, error taxonomy) replicate or diverge, and assess if DeformAr can diagnose dataset-specific issues beyond the original two.