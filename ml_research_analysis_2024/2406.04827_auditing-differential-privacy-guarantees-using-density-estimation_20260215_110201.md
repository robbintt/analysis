---
ver: rpa2
title: Auditing Differential Privacy Guarantees Using Density Estimation
arxiv_id: '2406.04827'
source_url: https://arxiv.org/abs/2406.04827
tags: []
core_contribution: "This paper introduces a practical method for auditing differential\
  \ privacy (DP) guarantees in machine learning models using only black-box access\
  \ to the model and a small hold-out dataset. The core idea is to estimate the total\
  \ variation distance between score distributions computed on auditing training and\
  \ test sets, which can be converted to (\u03B5, \u03B4)-DP guarantees."
---

# Auditing Differential Privacy Guarantees Using Density Estimation

## Quick Facts
- arXiv ID: 2406.04827
- Source URL: https://arxiv.org/abs/2406.04827
- Reference count: 40
- Introduces practical method for black-box DP auditing using density estimation and total variation distance

## Executive Summary
This paper presents a novel approach to auditing differential privacy (DP) guarantees in machine learning models using only black-box access. The method estimates total variation distance between score distributions computed on auditing training and test sets, which can be converted to (ε, δ)-DP guarantees. By generalizing threshold membership inference attacks through multiple histogram bins for density estimation, the approach provides more accurate estimates than previous black-box auditing methods.

## Method Summary
The proposed auditing method works by estimating the total variation distance between score distributions from auditing training and test sets. These distributions are obtained by running a small hold-out dataset through the black-box model and computing scores (e.g., prediction probabilities). The total variation distance serves as a proxy for membership inference risk, which can be converted to (ε, δ)-DP guarantees. The method uses multiple histogram bins rather than a single threshold, providing more robust density estimation and tighter bounds compared to traditional threshold-based membership inference attacks.

## Key Results
- Generalizes threshold membership inference attacks using multiple histogram bins for density estimation
- Proves that total variation distance estimation is near-optimal for robust estimation
- Experiments on MNIST and CIFAR-10 show significantly more accurate ε-estimates compared to baseline methods, particularly when theoretical ε-value is large

## Why This Works (Mechanism)
The method leverages the fundamental connection between membership inference risk and differential privacy. When a model leaks information about its training data, this manifests as differences in score distributions between training and test examples. By accurately estimating these distributional differences through total variation distance, the method can bound the privacy loss. The multi-bin histogram approach captures more nuanced patterns in the score distributions compared to single-threshold methods, leading to more accurate privacy estimates.

## Foundational Learning
- **Total Variation Distance**: Measures the difference between two probability distributions; needed to quantify distributional differences between training and test scores; quick check: verify it satisfies properties of a metric
- **Differential Privacy (ε, δ)**: Mathematical framework for quantifying privacy guarantees; needed as the target metric for auditing; quick check: confirm understanding of privacy budget interpretation
- **Membership Inference**: Attack that determines whether a data point was in a model's training set; needed as the underlying threat model; quick check: understand how it relates to privacy leakage
- **Score Distributions**: Probability distributions of model outputs (e.g., softmax scores); needed as the observable quantities for auditing; quick check: verify they capture relevant information about training influence
- **Density Estimation**: Statistical technique for estimating probability distributions from samples; needed to estimate score distributions from finite samples; quick check: understand bias-variance tradeoff in estimation
- **Robust Estimation**: Statistical approach that provides guarantees even under worst-case distributions; needed to ensure the auditing method works across different data distributions; quick check: verify the near-optimality proof conditions

## Architecture Onboarding

**Component Map**: Auditing dataset -> Score computation -> Histogram construction -> Total variation estimation -> (ε, δ) conversion

**Critical Path**: The critical path involves computing scores for auditing data, constructing histograms, estimating total variation distance, and converting this to privacy guarantees. Each step depends on the previous one, with the total variation estimation being the most computationally intensive part.

**Design Tradeoffs**: The method trades computational complexity for accuracy by using multiple histogram bins instead of a single threshold. While this increases computation, it provides tighter privacy bounds. The choice of histogram bin size represents another tradeoff between bias and variance in the density estimation.

**Failure Signatures**: The method may fail when score distributions are too similar (underestimating privacy loss) or when the auditing dataset is too small (high variance in estimates). It may also struggle with models that produce scores with very different scales or distributions across classes.

**First Experiments**:
1. Verify total variation estimation accuracy on synthetic score distributions with known differences
2. Test the method on a simple DP-SGD trained logistic regression model
3. Compare results with theoretical (ε, δ) values for known DP mechanisms

## Open Questions the Paper Calls Out
None specified in the provided content.

## Limitations
- Limited to small image classification models (MNIST, CIFAR-10) and may not generalize to complex architectures
- Requires access to training data for auditing, which could be problematic in privacy-sensitive contexts
- Comparison primarily focuses on one baseline method (shadow models), potentially missing other relevant approaches
- Empirical validation is constrained by the specific experimental setup and model architectures used

## Confidence
- Theoretical guarantees: Medium
- Empirical results: Low-Medium
- Practical applicability: Low-Medium

## Next Checks
1. Test the method across diverse DP mechanisms beyond DP-SGD, including Gaussian mechanisms and other composition strategies
2. Evaluate performance on larger, more complex datasets and architectures (e.g., ImageNet, transformers)
3. Investigate the method's robustness to realistic constraints, such as limited access to training data or partial model access