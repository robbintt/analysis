---
ver: rpa2
title: 'Open Source Language Models Can Provide Feedback: Evaluating LLMs'' Ability
  to Help Students Using GPT-4-As-A-Judge'
arxiv_id: '2405.05253'
source_url: https://arxiv.org/abs/2405.05253
tags: []
core_contribution: Large language models (LLMs) can be used to generate feedback on
  student code, but concerns exist about accuracy and privacy. This study investigates
  whether GPT-4 can serve as an automated judge for evaluating the quality of LLM-generated
  feedback, and whether open-source models can match proprietary ones.
---

# Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge

## Quick Facts
- arXiv ID: 2405.05253
- Source URL: https://arxiv.org/abs/2405.05253
- Reference count: 40
- Key outcome: Open-source models like Zephyr-7B can generate educational feedback competitively with GPT-3.5 while GPT-4 shows positive bias but reliable low-quality detection

## Executive Summary
This study evaluates whether large language models can effectively provide feedback on student programming code, focusing on the viability of open-source alternatives to proprietary models. Using GPT-4 as an automated judge, the researchers compare feedback quality from various models (GPT-3.5, Code Llama, Zephyr) against human-annotated ground truth from student help requests. The results reveal that while GPT-4 exhibits positive bias in general, it reliably identifies low-quality feedback with high recall. Importantly, smaller open-source models, particularly Zephyr-7B variants, achieve competitive performance with GPT-3.5 despite being significantly smaller, suggesting they are becoming viable alternatives for educational feedback tools.

## Method Summary
The researchers used an existing dataset of 100 student programming help requests with human-annotated feedback quality labels. They generated feedback using various language models (GPT-3.5, Code Llama variants, Zephyr variants) and evaluated these using GPT-4 as a judge with three binary classification criteria: completeness (comprehensive vs incomplete), perceptivity (relevant vs irrelevant), and selectivity (appropriate vs inappropriate). The study compared GPT-4's judgments against human annotations to assess reliability and bias. Multiple performance metrics were calculated including precision, recall, F-scores, and Cohen's kappa for inter-rater reliability.

## Key Results
- GPT-4 exhibits positive bias but achieves moderate agreement with human raters (kappa 0.40-0.48) and high recall (0.94-1.00) across evaluation criteria
- Zephyr-7B models outperform larger CodeLlama-34B despite being 5x smaller, achieving competitive performance with GPT-3.5
- Open-source models can generate 70-75% comprehensive feedback compared to GPT-4's 99%, making them viable alternatives for cost-sensitive educational contexts

## Why This Works (Mechanism)

### Mechanism 1
- Claim: GPT-4 can reliably identify low-quality feedback despite showing positive bias
- Mechanism: GPT-4 achieves moderate agreement with human raters (kappa 0.40-0.48) while maintaining high recall (0.94-1.00) across evaluation criteria
- Core assumption: GPT-4's evaluation performance remains stable when judging feedback quality, even if it tends toward positive ratings
- Evidence anchors:
  - [abstract] "GPT-4 exhibits a positive bias but achieves moderate agreement with human raters (kappa 0.40-0.48) and high recall (0.94-1.00) across criteria"
  - [section] "GPT-4 can reliably identify low-quality feedback, but might be overly optimistic in general"
- Break Condition: When GPT-4's positive bias consistently masks severe deficiencies in feedback quality, or when its agreement with human raters drops below acceptable thresholds for the application context

### Mechanism 2
- Claim: Smaller open-source models can outperform larger ones in feedback generation
- Mechanism: Zephyr-7B models (7B parameters) outperform CodeLlama-34B (34B parameters) despite being 5x smaller, achieving competitive performance with GPT-3.5
- Core assumption: Model architecture and fine-tuning approach matter more than parameter count for this specific task
- Evidence anchors:
  - [abstract] "Some open-source models (especially Zephyr-7B variants) achieve competitive performance with GPT-3.5 in generating high-quality feedback"
  - [section] "we notice that the more recent Zephyr-7B models (released October 2023) outperform even the largest CodeLlama model (34B) despite the latter being almost 5 times larger"
- Break Condition: When task complexity exceeds the reasoning capacity of smaller models, or when the fine-tuning approach that benefits Zephyr doesn't generalize to other open-source models

### Mechanism 3
- Claim: Open-source models provide viable alternatives to proprietary models for educational feedback tools
- Mechanism: Zephyr-7B models achieve 70-75% comprehensive feedback generation compared to GPT-4's 99%, while being significantly more cost-effective and privacy-preserving
- Core assumption: The performance gap between open-source and proprietary models is acceptable for educational contexts where privacy and cost are major concerns
- Evidence anchors:
  - [abstract] "open-source models are becoming viable alternatives for educational feedback tools"
  - [section] "Zephyr-7B-α, a 7 billion parameters open-source language model performed as well as GPT-3.5"
- Break Condition: When the performance gap exceeds what educators consider acceptable for student learning outcomes, or when privacy concerns diminish as proprietary models implement better safeguards

## Foundational Learning

- Concept: LLM-as-a-Judge methodology
  - Why needed here: The study relies on using GPT-4 to evaluate feedback quality from other models, making understanding this paradigm essential
  - Quick check question: What are the three variations of LLM-as-judge paradigm proposed by Zheng et al. [44]?

- Concept: Cohen's kappa for inter-rater reliability
  - Why needed here: The study uses kappa scores (0.40-0.48 for completeness/selectivity, 0.21 for perceptivity) to measure agreement between GPT-4 and human raters
  - Quick check question: How does Cohen's kappa differ from simple percentage agreement in evaluating model performance?

- Concept: Precision, recall, and F-score tradeoffs
  - Why needed here: The study prioritizes precision and F0.5-score to minimize false positives (misleading feedback) while reporting other metrics
  - Quick check question: Why might an educational application prioritize precision over recall when evaluating feedback quality?

## Architecture Onboarding

- Component map: Student code → Model-generated feedback → GPT-4 evaluation → Human expert comparison
- Critical path:
  1. Generate feedback using open-source models on student help requests
  2. Evaluate feedback quality using GPT-4 as judge with specified rubric
  3. Compare GPT-4 judgments against human expert annotations
  4. Analyze performance metrics (precision, recall, F-scores, kappa)
- Design tradeoffs:
  - Single vs. multiple GPT-4 generations per feedback: Single generation reduces cost but increases variance; multiple generations improve reliability but increase cost
  - Zero-shot vs. few-shot prompting: Zero-shot reduces prompt engineering complexity but may miss task-specific nuances
  - Open-source vs. proprietary models: Trade cost/privacy benefits against performance gaps
- Failure signatures:
  - High precision but low recall indicates model is conservative but missing issues
  - Low kappa scores suggest poor agreement between judge and human raters
  - Performance gaps between open-source and proprietary models exceeding acceptable thresholds
- First 3 experiments:
  1. Reproduce GPT-4 vs human expert comparison using a subset of the dataset to validate methodology
  2. Compare single vs. multiple GPT-4 generations on feedback evaluation consistency
  3. Test different prompting strategies (zero-shot vs. few-shot) on GPT-4 evaluation performance

## Open Questions the Paper Calls Out
None

## Limitations
- GPT-4 shows positive bias in feedback evaluation, potentially overestimating quality of good feedback
- Study relies on single human annotator, limiting understanding of inter-rater reliability among humans
- Dataset comes from existing programming help resource, may not represent full diversity of student programming challenges

## Confidence
- **High confidence**: GPT-4's ability to identify low-quality feedback (high recall 0.94-1.00 across criteria)
- **Medium confidence**: GPT-4's moderate agreement with human raters for completeness and selectivity criteria (kappa 0.40-0.48)
- **Low confidence**: GPT-4's perceptivity evaluation (kappa 0.21) and generalizability of open-source model performance across different educational contexts

## Next Checks
1. Multi-annotator reliability study: Re-evaluate a subset of feedback using multiple human annotators to establish baseline inter-rater reliability and better understand the human-human agreement before comparing to GPT-4-human agreement.

2. Longitudinal student outcomes study: Track actual student learning outcomes when receiving feedback from GPT-4, GPT-3.5, and top-performing open-source models to determine if performance differences in evaluation metrics translate to meaningful differences in student comprehension and code improvement.

3. Cross-domain generalizability test: Apply the same evaluation methodology to non-programming educational contexts (e.g., essay writing, mathematics problem-solving) to determine whether the observed performance patterns of open-source models hold across different subject areas and feedback types.