---
ver: rpa2
title: Stochastic Optimization Algorithms for Instrumental Variable Regression with
  Streaming Data
arxiv_id: '2405.19463'
source_url: https://arxiv.org/abs/2405.19463
tags:
- page
- stochastic
- cited
- have
- algorithm
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper develops streaming algorithms for instrumental variable
  regression by reformulating it as a conditional stochastic optimization problem.
  The key insight is that, for the least-squares loss, one can construct unbiased
  gradient estimators using either two samples from the conditional distribution of
  X given Z or one sample with an online estimator for the conditional mean.
---

# Stochastic Optimization Algorithms for Instrumental Variable Regression with Streaming Data

## Quick Facts
- arXiv ID: 2405.19463
- Source URL: https://arxiv.org/abs/2405.19463
- Reference count: 40
- Primary result: Develops streaming algorithms for IV regression with O(log T/T) and O(1/T^(1-ι)) convergence rates

## Executive Summary
This paper addresses instrumental variable regression in streaming settings by reformulating it as a conditional stochastic optimization problem. The authors propose two algorithms: TOSG-IVaR that uses two independent samples of X given Z to construct unbiased gradient estimators, achieving O(log T/T) convergence; and OTSG-IVaR that works with a single sample while maintaining an online estimator of E[X|Z], achieving O(1/T^(1-ι)) convergence. Both methods avoid matrix inversions and mini-batches, making them suitable for streaming applications. The key insight is leveraging the quadratic loss structure to construct unbiased gradient estimators without explicitly modeling the confounder-instrument relationship.

## Method Summary
The paper develops streaming algorithms for IV regression by minimizing F(θ) = EZEY|Z[(Y - EX|Z[g(X)])²] directly. TOSG-IVaR uses two independent samples Xt, X't from PX|Zt to construct the unbiased gradient estimator v(θ) = (g(θ; Xt) − Yt)∇θg(θ; X't), enabling updates θt+1 = θt - αt+1v(θt). OTSG-IVaR maintains an estimator γt of E[X|Z] and performs coupled updates: θt+1 = θt - αt+1γ⊤t Zt(Z⊤t γtθt − Yt) and γt+1 = γt - βt+1Zt(Z⊤t γt - X⊤t). Both algorithms use carefully chosen step-size schedules (αt = O(t^(-1+ι/2)), βt = O(t^(-1+ι/2))) to ensure convergence.

## Key Results
- TOSG-IVaR achieves O(log T/T) convergence rate when two independent X samples are available per iteration
- OTSG-IVaR achieves O(1/T^(1-ι)) convergence rate for any ι > 0 when only one X sample is available
- Both algorithms avoid matrix inversions and explicit modeling of the confounder-instrument relationship
- Methods work in streaming settings without requiring mini-batches or offline data storage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unbiased gradient estimator can be constructed using two independent X samples given Z
- Mechanism: When X and X' are independently sampled from P(X|Z), the term (g(θ; X) - Y)∇θg(θ; X') forms an unbiased estimator of the gradient by leveraging the quadratic loss structure
- Core assumption: Two independent samples of X given each Z are available, and the conditional distribution is continuous
- Evidence anchors: Paper establishes O(log T/T) rate for linear models under two-sample oracle; weak corpus evidence
- Break condition: If samples are not independent given Z or conditional distribution is not continuous

### Mechanism 2
- Claim: OTSG-IVaR converges through coupled parameter updates for θ and γ in a two-time-scale scheme
- Mechanism: The algorithm couples updates θt+1 = θt - αt+1γ⊤t Zt(Z⊤t γtθt − Yt) and γt+1 = γt - βt+1Zt(Z⊤t γt - X⊤t) with carefully balanced step-sizes
- Core assumption: Linear dependence between X and Z, with αt = O(t^(-1+ι/2)) and βt = O(t^(-1+ι/2))
- Evidence anchors: Paper establishes O(1/T^(1-ι)) rate and discusses coupling challenges; weak corpus evidence
- Break condition: If linear dependence fails or step-sizes are not properly balanced

### Mechanism 3
- Claim: Reformulating IV regression as conditional stochastic optimization avoids forbidden regression
- Mechanism: Direct minimization of F(θ) = EZEY|Z[(Y - EX|Z[g(X)])²] sidesteps need to estimate nuisance parameters or approximate dual variables
- Core assumption: Function class G includes the true underlying relationship between X and Y
- Evidence anchors: Paper contrasts with minimax approaches and emphasizes direct solution; moderate corpus evidence
- Break condition: If function class G is misspecified or problem cannot be correctly formulated

## Foundational Learning

- Concept: Conditional stochastic optimization
  - Why needed here: IV regression naturally fits this framework because the objective involves nested expectations E[EY|Z[...]] that depend on the conditional distribution of X given Z
  - Quick check question: Can you write the IV regression objective as minimizing EZ[EY|Z[(Y - EX|Z[g(X)])²]] and identify why this differs from standard stochastic optimization?

- Concept: Two-sample oracle and its unbiasedness property
  - Why needed here: The two-sample gradient estimator (g(θ; X) - Y)∇θg(θ; X') is unbiased only when X and X' are independent given Z, crucial for Algorithm 1's convergence
  - Quick check question: If X and X' are both sampled from PX|Z for the same Z, what property ensures that (g(θ; X) - Y)∇θg(θ; X') is an unbiased estimator of the true gradient?

- Concept: Two-time-scale stochastic approximation
  - Why needed here: Algorithm 2 requires different step-size schedules for θ and γ updates to ensure convergence while maintaining correct coupling
  - Quick check question: Why must βt decay faster than αt in the two-time-scale setting, and what would happen if they decayed at the same rate?

## Architecture Onboarding

- Component map: Streaming data (Zt, Xt, Yt) or (Zt, Xt, Xt', Yt) -> Gradient computation using unbiased estimators -> Parameter estimate θT
- Critical path:
  - TOSG-IVaR: Zt → sample Xt, Xt' → compute (g(θt; Xt) - Yt)∇θg(θt; X't) → update θt+1 = θt - αt+1 * gradient
  - OTSG-IVaR: Zt → sample Xt → compute γ⊤t Zt(Z⊤t γtθt - Yt) → update θt+1 and γt+1 simultaneously
- Design tradeoffs:
  - TOSG-IVaR: Simpler analysis, better O(log T/T) rate, requires two X samples per Z
  - OTSG-IVaR: Works with single sample, rate O(1/T^(1-ι)), more complex coupling analysis
  - Memory: TOSG-IVaR O(dx), OTSG-IVaR O(dxdz + dx)
- Failure signatures:
  - Divergence in early iterations: Likely step-size too large or γt not close enough to γ* in OTSG-IVaR
  - Plateau at non-zero error: Possible model misspecification or insufficient iterations
  - High variance in estimates: Data quality issues or insufficient samples per Z value
- First 3 experiments:
  1. Implement TOSG-IVaR on synthetic data with known θ* and linear g, verify O(log T/T) convergence rate empirically
  2. Compare TOSG-IVaR vs OTSG-IVaR on same data, measure trade-off between data requirements and convergence speed
  3. Test sensitivity to step-size choices by running multiple configurations and plotting error vs iterations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can the convergence rate of TOSG-IVaR be improved beyond O(log T/T) for linear models under the two-sample oracle setting?
- Basis in paper: The paper establishes O(log T/T) rate for linear models but doesn't explore whether tighter bounds are possible
- Why unresolved: Paper only establishes this rate for linear models without investigating optimality
- What evidence would resolve it: A proof showing O(log T/T) is optimal, or an algorithm achieving better rate with two samples

### Open Question 2
- Question: What are the theoretical guarantees for TOSG-IVaR when the model g(θ;X) is non-linear (e.g., neural networks)?
- Basis in paper: Paper mentions local convergence guarantees for non-linear models require additional assumptions but doesn't specify them
- Why unresolved: Paper only provides convergence guarantees for linear models and mentions additional assumptions are needed
- What evidence would resolve it: Complete characterization of additional assumptions and corresponding convergence rates for non-linear models

### Open Question 3
- Question: Can OTSG-IVaR achieve the same O(log T/T) convergence rate as TOSG-IVaR when only one sample per iteration is available?
- Basis in paper: Paper establishes O(log T/T) for TOSG-IVaR with two samples and O(1/T^(1-ι)) for OTSG-IVaR with one sample
- Why unresolved: Paper attributes rate difference to need for estimating conditional expectation in OTSG-IVaR but doesn't explore algorithmic modifications
- What evidence would resolve it: Modified OTSG-IVaR achieving O(log T/T) or proof that this rate is impossible with one sample

## Limitations
- Requires either two independent samples per iteration (TOSG-IVaR) or careful step-size tuning (OTSG-IVaR)
- OTSG-IVaR convergence depends critically on linear dependence structure between X and Z
- Theoretical guarantees are established for linear models but only local convergence is mentioned for non-linear cases

## Confidence
- TOSG-IVaR convergence theory: **Medium** - well-established for two-sample case but relies on strong data assumptions
- OTSG-IVaR convergence theory: **Medium-Low** - complex coupling analysis with potential sensitivity to initialization and step-size choices
- Avoidance of forbidden regression: **High** - clear theoretical distinction from minimax formulations

## Next Checks
1. Test the sensitivity of OTSG-IVaR to initialization by running experiments with different starting values for γ0 and measuring convergence stability
2. Verify the unbiasedness of the two-sample gradient estimator empirically by checking that E[v(θ)] ≈ ∇F(θ) through Monte Carlo sampling
3. Evaluate performance under model misspecification by testing on data where the true relationship between X and Z is non-linear