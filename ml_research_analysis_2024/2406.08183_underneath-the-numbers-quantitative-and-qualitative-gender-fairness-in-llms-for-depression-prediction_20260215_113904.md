---
ver: rpa2
title: 'Underneath the Numbers: Quantitative and Qualitative Gender Fairness in LLMs
  for Depression Prediction'
arxiv_id: '2406.08183'
source_url: https://arxiv.org/abs/2406.08183
tags:
- fairness
- llms
- gender
- llama
- bias
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work investigates gender fairness in large language models
  (LLMs) for depression prediction, a high-stakes application. The authors evaluate
  ChatGPT, LLaMA 2, and Bard using both quantitative fairness metrics and a novel
  qualitative fairness approach based on explanations.
---

# Underneath the Numbers: Quantitative and Qualitative Gender Fairness in LLMs for Depression Prediction

## Quick Facts
- arXiv ID: 2406.08183
- Source URL: https://arxiv.org/abs/2406.08183
- Reference count: 23
- This work investigates gender fairness in large language models (LLMs) for depression prediction using both quantitative metrics and novel qualitative analysis of explanations.

## Executive Summary
This paper addresses gender fairness in large language models (LLMs) for depression prediction, a critical high-stakes application. The authors evaluate ChatGPT, LLaMA 2, and Bard using quantitative fairness metrics and introduce a novel qualitative fairness approach based on analyzing LLM-generated explanations. Their findings reveal that while LLaMA 2 achieves the fairest quantitative results across datasets, ChatGPT consistently provides more comprehensive and well-reasoned explanations. The study identifies a fundamental trade-off between quantitative and qualitative capabilities, with LLaMA 2 excelling at numerical tasks and ChatGPT at qualitative evaluation. The authors also identify key themes in LLM responses related to fairness evaluation, including assumptions about gender, language use, and suggestions for improvement.

## Method Summary
The study evaluates three LLMs (ChatGPT, LLaMA 2, and Bard) on two depression detection datasets (DAIC-WOZ and E-DAIC) using both quantitative and qualitative approaches. For quantitative evaluation, the researchers employed standard classification metrics (precision, recall, F1, accuracy) and fairness metrics including statistical parity, equal opportunity, equalized odds, and equal accuracy. They tested three prompt formulations: baseline, gender-explicit, and gender-implicit. For qualitative evaluation, they conducted thematic analysis of LLM-generated explanations for fairness, coding responses to identify patterns in how models approach fairness evaluation. The study also included sentiment analysis of LLM responses to assess the quality of explanations provided.

## Key Results
- LLaMA 2 achieved the fairest quantitative results across most metrics on both datasets tested
- ChatGPT had the best classification performance but LLaMA 2 was more fair quantitatively
- ChatGPT consistently provided more comprehensive and well-reasoned explanations compared to LLaMA 2
- Both models converged on avoiding assumptions and using gender-neutral language in their qualitative fairness evaluations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Different LLMs have distinct strengths in quantitative vs qualitative fairness evaluation.
- Mechanism: LLaMA 2's architecture is optimized for numerical outputs (e.g., explicit scoring, rating scales), while ChatGPT's design favors context-aware, explanatory responses.
- Core assumption: Model architecture and training objectives influence task-specific performance.
- Evidence anchors:
  - [abstract]: "We found that ChatGPT consistently provides a more comprehensive, well-reasoned explanation for its prediction compared to LLaMA 2."
  - [section]: "Our findings indicate the presence of bias within existing LLMs. Most of the fairness scores are within the acceptable threshold range. LLaMA 2 is quantitatively fairest of all for both datasets."
  - [corpus]: Weak - related works focus on general LLM fairness but not specific model architectural impacts on depression detection.

### Mechanism 2
- Claim: Prompt formulation significantly affects fairness outcomes.
- Mechanism: Explicitly mentioning gender in prompts introduces bias signals that models use to adjust predictions, while implicit gender cues allow models to rely on contextual inference.
- Core assumption: LLMs are sensitive to explicit vs implicit demographic cues in prompts.
- Evidence anchors:
  - [section]: "We decided to explore different prompt formulations which included gender information: gender-explicit and gender-implicit."
  - [section]: "We compared the three LLMs' detection using the prompts defined in Section 3.3.1 to ground truth annotations and computed the F1 score of detection in the baseline scenario."
  - [corpus]: Weak - related works discuss prompt sensitivity but not specifically in depression detection fairness contexts.

### Mechanism 3
- Claim: Thematic analysis of qualitative fairness reveals convergent and divergent model behaviors.
- Mechanism: Both LLMs converge on avoiding assumptions and using gender-neutral language, but diverge in depth of explanation and consistency of responses.
- Core assumption: Qualitative evaluation can surface nuanced fairness behaviors not captured by quantitative metrics.
- Evidence anchors:
  - [section]: "We have also identified several themes adopted by LLMs to qualitatively evaluate gender fairness."
  - [section]: "Our results show that LLMs defined fairness according to the capability of the model to avoid assumptions, used gender-neutral language, in line with previous fairness literature."
  - [corpus]: Moderate - related works on qualitative fairness exist but not specifically for mental health applications.

## Foundational Learning

- Concept: Depression detection using LLMs
  - Why needed here: Understanding how LLMs process mental health signals is crucial for interpreting fairness results.
  - Quick check question: What are the typical input-output formats for LLM-based depression detection?

- Concept: Fairness metrics in ML
  - Why needed here: Quantitative fairness evaluation requires understanding statistical parity, equal opportunity, etc.
  - Quick check question: How does statistical parity differ from equal opportunity in fairness evaluation?

- Concept: Thematic analysis methodology
  - Why needed here: Qualitative fairness evaluation relies on systematic coding of LLM explanations.
  - Quick check question: What are the key steps in conducting thematic analysis on qualitative data?

## Architecture Onboarding

- Component map: Data preprocessing → Prompt generation → LLM inference → Performance evaluation → Fairness analysis → Qualitative analysis
- Critical path: Prompt → LLM → Evaluation metrics → Fairness assessment
- Design tradeoffs: Explicit gender prompts may introduce bias vs implicit prompts may miss important signals
- Failure signatures: Inconsistent qualitative responses, contradictory fairness ratings, poor performance on specific demographics
- First 3 experiments:
  1. Compare baseline, gender-explicit, and gender-implicit prompts on a small sample to observe prompt effects
  2. Evaluate quantitative fairness metrics across different demographic splits
  3. Conduct thematic analysis on a subset of LLM explanations to identify emerging fairness themes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can qualitative fairness be systematically and reliably measured across different LLMs and tasks beyond depression detection?
- Basis in paper: [explicit] The authors introduce a novel approach to qualitative fairness evaluation using thematic analysis of LLM-generated explanations, but acknowledge this is an open research question.
- Why unresolved: The paper presents a first attempt at defining qualitative fairness but doesn't provide a standardized, validated methodology that could be widely adopted.
- What evidence would resolve it: A validated framework for qualitative fairness measurement with clear metrics, inter-rater reliability, and demonstrated applicability across multiple domains and LLM architectures.

### Open Question 2
- Question: What are the specific mechanisms by which dataset composition influences LLM fairness outcomes in depression prediction?
- Basis in paper: [explicit] The authors observe differences in fairness scores between DAIC-WOZ and E-DAIC datasets but don't explore why these differences occur.
- Why unresolved: The paper identifies dataset effects but doesn't investigate the underlying causes (e.g., demographic representation, labeling practices, conversation styles).
- What evidence would resolve it: Comparative analysis of dataset characteristics (demographic distributions, annotation procedures, linguistic features) correlated with fairness metric variations.

### Open Question 3
- Question: How can we effectively combine the quantitative fairness strengths of LLaMA 2 with the qualitative evaluation capabilities of ChatGPT for optimal bias mitigation?
- Basis in paper: [inferred] The authors identify a trade-off where LLaMA 2 excels at quantitative fairness while ChatGPT performs better on qualitative evaluation, but don't propose integration strategies.
- Why unresolved: The paper highlights complementary strengths but doesn't explore hybrid approaches or ensemble methods that could leverage both capabilities.
- What evidence would resolve it: Experimental results comparing hybrid systems (e.g., using LLaMA 2 for numerical assessment and ChatGPT for explanation generation) against individual LLM performance.

## Limitations

- Dataset sizes are relatively small (237 subjects for DAIC-WOZ, 161 for E-DAIC), limiting generalizability
- Potential data leakage risks from using transcripts containing explicit gender information
- Qualitative analysis relies on subjective interpretation of LLM-generated explanations

## Confidence

- High confidence: The quantitative fairness metrics and their comparative analysis across LLMs
- Medium confidence: The thematic analysis findings regarding LLM fairness evaluation approaches
- Medium confidence: The conclusions about the trade-off between quantitative and qualitative capabilities

## Next Checks

1. **Dataset size validation**: Test the observed model behaviors and fairness patterns on a larger, more diverse dataset to confirm generalizability of findings
2. **Prompt sensitivity analysis**: Systematically vary prompt formulations and evaluate how this affects both quantitative fairness metrics and qualitative explanation quality across all three LLMs
3. **Cross-cultural validation**: Apply the same evaluation framework to depression detection datasets from different cultural contexts to assess whether the identified fairness patterns hold across diverse populations