---
ver: rpa2
title: Online Learning with Unknown Constraints
arxiv_id: '2403.04033'
source_url: https://arxiv.org/abs/2403.04033
tags:
- algorithm
- online
- regret
- learning
- constraint
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper studies online learning under unknown safety constraints,\
  \ where the learner must pick actions satisfying constraints f(a, x) \u2264 0 while\
  \ minimizing regret. The key challenge is that f is unknown, and the learner only\
  \ observes noisy feedback."
---

# Online Learning with Unknown Constraints

## Quick Facts
- arXiv ID: 2403.04033
- Source URL: https://arxiv.org/abs/2403.04033
- Reference count: 40
- One-line primary result: Achieves O(κT + RegOR(T,δ,F0)E(F0,α)/α + RegOL(T,δ)) regret for online learning with unknown safety constraints

## Executive Summary
This paper studies online learning under unknown safety constraints where the learner must pick actions satisfying constraints f*(a, x) ≤ 0 while minimizing regret. The key challenge is that f* is unknown, and the learner only observes noisy feedback. The authors propose a meta-algorithm using two oracles: an online regression oracle to estimate the unknown constraint function f*, and an online learning oracle to optimize performance given estimated constraints. The algorithm maintains a version space of possible constraint functions and constructs optimistic and pessimistic action sets to balance exploration and exploitation while ensuring safety.

## Method Summary
The paper proposes a meta-algorithm that maintains a version space Ft of possible constraint functions consistent with observations. It uses an online regression oracle to update Ft based on noisy feedback and constructs optimistic (Ot) and pessimistic (Pt) action sets. A mapping M converts recommendations from optimistic sets to pessimistic actions while balancing exploration and exploitation. The algorithm achieves safety by sampling only from Pt, ensuring f*(at, xt) ≤ 0 for all t with high probability.

## Key Results
- Achieves regret bound O(κT + RegOR(T,δ,F0)E(F0,α)/α + RegOL(T,δ)) where κ balances constraint satisfaction and information gain
- For linear constraints with known initial safe set, provides O(√T) regret algorithm using scaling transformation
- Shows κ is necessary for learning - if too large, no algorithm can achieve sublinear regret
- Establishes lower bounds showing eluder dimension and complexity measure are necessary for safe learning

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The algorithm achieves safety by maintaining a version space Ft of constraint functions consistent with observations and constructing pessimistic action sets Pt that guarantee constraint satisfaction.
- Mechanism: On each round, the algorithm uses an online regression oracle to update Ft based on past observations, then defines Pt as the set of actions guaranteed to satisfy the constraint for any f in Ft. By sampling actions only from Pt, the algorithm ensures f*(at, xt) ≤ 0 for all t with high probability.
- Core assumption: The online regression oracle provides predictions with bounded squared error, ensuring f* remains in Ft with high probability.
- Evidence anchors:
  - [abstract]: "The algorithm maintains a version space Ft of possible constraint functions consistent with observations"
  - [section]: "We also construct a pessimistic set of actions Pt of actions that for that rounds are guaranteed to satisfy the constraint for that round"
  - [corpus]: Weak - the corpus focuses on related but distinct settings (multi-point feedback, distributed optimization)
- Break condition: If the regression oracle's error grows unbounded or the initial safe set A0 is too restrictive, the algorithm cannot expand its feasible region.

### Mechanism 2
- Claim: The algorithm balances exploration and exploitation through a mapping M that converts optimistic recommendations to pessimistic actions while minimizing regret.
- Mechanism: The algorithm uses an online learning oracle to get recommendations from optimistic sets Ot (which may contain unsafe actions), then applies mapping M to convert these to actions in Pt. The mapping balances exploration of uncertain regions (maximizing ∆Ft) with exploitation of known safe regions.
- Core assumption: There exists a mapping M that can convert optimistic distributions to pessimistic ones while keeping the regret increase bounded by κ.
- Evidence anchors:
  - [abstract]: "converts the predictions of an online learning oracle to predictions that adhere to the unknown safety constraint"
  - [section]: "To this end, we introduce a mapping M that takes in a distribution over Ot along with the pessimistic set Pt"
  - [corpus]: Weak - corpus papers don't explicitly discuss this mapping mechanism
- Break condition: If κ becomes too large (high complexity of the constraint class), the regret bound becomes linear in T, making learning impossible.

### Mechanism 3
- Claim: The eluder dimension of the constraint function class provides a measure of how much information can be gained from observations.
- Mechanism: The algorithm uses the eluder dimension E(F,α) to bound how quickly the version space Ft shrinks as more observations are collected. This dimension captures the "information gain" potential of the constraint class.
- Core assumption: The eluder dimension of F is finite and grows reasonably with the scale parameter α.
- Evidence anchors:
  - [abstract]: "the eluder dimension of the model class containing the unknown safety constraint"
  - [section]: "The eluder dimension E(F, α ) is the length of the longest sequence of pairs in A×X such that for some ǫ′ > ǫ, each pair is ǫ′-independent of its predecessors"
  - [corpus]: Weak - eluder dimension isn't explicitly mentioned in related papers
- Break condition: If the eluder dimension grows too quickly (e.g., exponentially with dimension), the regret bound becomes intractable.

## Foundational Learning

- Concept: Online convex optimization with long-term constraints
  - Why needed here: This paper builds on the distinction between per-round constraint satisfaction (strict safety) vs. long-term average constraint violation. Understanding this distinction is crucial for grasping why the algorithm maintains pessimistic sets.
  - Quick check question: What's the key difference between per-round constraint satisfaction and long-term constraint violation in terms of algorithm design?

- Concept: Eluder dimension and information gain
  - Why needed here: The eluder dimension quantifies how much information can be gained from observations about the unknown constraint function. This directly impacts the regret bound and determines how quickly the algorithm can learn.
  - Quick check question: How does the eluder dimension of linear functions scale with dimension d and accuracy ǫ?

- Concept: Version space maintenance and optimistic/pessimistic sets
  - Why needed here: The algorithm maintains a version space Ft of possible constraint functions and uses this to construct optimistic (Ot) and pessimistic (Pt) action sets. Understanding this mechanism is key to grasping how the algorithm balances safety and learning.
  - Quick check question: Why does the algorithm need both optimistic and pessimistic action sets, and what role does each play?

## Architecture Onboarding

- Component map:
  - Online regression oracle (OracleOR) -> Maintains version space Ft by providing bounded-error predictions of constraint values
  - Online learning oracle (OracleOL) -> Provides recommendations from optimistic sets Ot
  - Mapping M -> Converts optimistic recommendations to pessimistic actions while minimizing regret increase
  - Constraint satisfaction checker -> Ensures all actions satisfy f*(at, xt) ≤ 0
  - Regret calculator -> Tracks cumulative regret against best safe action in hindsight

- Critical path:
  1. Receive context xt
  2. Update version space Ft using OracleOR
  3. Construct optimistic set Ot and pessimistic set Pt
  4. Get recommendation ˜pt from OracleOL using Ot
  5. Apply mapping M to get pt from ˜pt and Pt
  6. Sample action at ~ pt and observe feedback
  7. Update OracleOR with new observation

- Design tradeoffs:
  - Exploration vs. exploitation: The mapping M must balance exploring uncertain regions (maximizing ∆Ft) with exploiting known safe regions
  - Computational complexity: Maintaining version spaces and constructing action sets can be expensive for large function classes
  - Initial safe set size: Larger A0 enables faster learning but may be harder to verify

- Failure signatures:
  - Linear regret growth: Indicates κ is too large or eluder dimension is growing too quickly
  - Constraint violations: Suggests OracleOR is providing poor predictions or Pt is being constructed incorrectly
  - Slow learning: May indicate A0 is too restrictive or the function class F has high eluder dimension

- First 3 experiments:
  1. Linear constraints with known initial safe set: Test the explicit O(√T) regret algorithm with A0 = {a : ||a|| ≤ b}
  2. Finite action spaces: Verify κ* ≤ 1/∆0 when actions are separated by ∆0 in F0
  3. Multiple linear constraints: Test the extension to polytopic constraints with vector feedback

## Open Questions the Paper Calls Out
None

## Limitations
- The computational complexity of maintaining version spaces Ft for general constraint function classes remains unclear
- The exact construction of mapping M for general settings is not fully specified
- The practical performance depends heavily on the quality of the online regression oracle

## Confidence
- High Confidence: The O(√T) regret bound for linear constraints with known initial safe set (Theorem 4.1)
- Medium Confidence: The general regret bound involving κ, RegOR, and E(F0,α) (Theorem 3.1)
- Medium Confidence: The necessity of the complexity measure κ for learning (Theorem 5.1)

## Next Checks
1. Implement the explicit algorithm for linear constraints and verify the O(√T) regret bound empirically across different problem dimensions and initial safe set sizes.
2. Test OracleOR with different regression algorithms (linear regression, kernel methods, neural networks) to evaluate how regression error impacts overall regret.
3. For simple constraint classes, implement and analyze different strategies for the mapping M to understand the exploration-exploitation tradeoff and identify optimal mapping strategies.