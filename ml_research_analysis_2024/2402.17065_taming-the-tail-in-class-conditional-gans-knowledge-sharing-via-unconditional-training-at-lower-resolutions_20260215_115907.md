---
ver: rpa2
title: 'Taming the Tail in Class-Conditional GANs: Knowledge Sharing via Unconditional
  Training at Lower Resolutions'
arxiv_id: '2402.17065'
source_url: https://arxiv.org/abs/2402.17065
tags:
- training
- classes
- images
- tail
- data
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper addresses the challenge of training class-conditional
  GANs on long-tailed datasets, where imbalanced data causes mode collapse in tail
  classes. The authors propose a method called Unconditional Training at Lower Resolutions
  (UTLO) that splits the generator into two parts: an unconditional low-resolution
  part and a conditional high-resolution part.'
---

# Taming the Tail in Class-Conditional GANs: Knowledge Sharing via Unconditional Training at Lower Resolutions

## Quick Facts
- arXiv ID: 2402.17065
- Source URL: https://arxiv.org/abs/2402.17065
- Authors: Saeed Khorram; Mingqi Jiang; Mohamad Shahbazi; Mohamad H. Danesh; Li Fuxin
- Reference count: 40
- One-line primary result: Unconditional Training at Lower Resolutions (UTLO) improves tail class generation in long-tailed cGANs by splitting the generator into unconditional low-res and conditional high-res parts.

## Executive Summary
This paper addresses the challenge of training class-conditional GANs on long-tailed datasets, where imbalanced data causes mode collapse in tail classes. The authors propose a method called Unconditional Training at Lower Resolutions (UTLO) that splits the generator into two parts: an unconditional low-resolution part and a conditional high-resolution part. This allows the model to learn shared features across classes at low resolutions and then refine them conditionally at higher resolutions. Experiments across multiple benchmarks (CIFAR, LSUN, Flowers, iNaturalist, AnimalFaces) and architectures (StyleGAN2, Projected GAN) show consistent improvements in FID and KID metrics, especially for tail classes. The method outperforms baselines like GSR and NoisyTwins, and a proposed few-shot metric (FID-FS/KID-FS) better captures tail class performance.

## Method Summary
The method splits the generator into unconditional low-resolution (Gl) and conditional high-resolution (Gh) parts, with the split occurring at a specified resolution (e.g., 8×8). The generator produces two style vectors: wz for unconditional low-res generation and wz,y for conditional high-res generation. The discriminator is similarly split into Dl (shared for both paths) and Dh (conditional high-res only). Training combines unconditional loss for low-res images and conditional loss for high-res images, weighted by λ. This enables head classes to inject universal features into tail classes at low resolutions while preserving class-specific details at high resolutions.

## Key Results
- UTLO improves FID and KID metrics by 10-50% across multiple long-tailed datasets compared to baseline cGANs
- Tail class generation quality shows significant improvement, as measured by proposed FID-FS and KID-FS metrics
- The method outperforms GSR and NoisyTwins baselines while maintaining stable training across architectures
- Ablation studies show 8×8 and 16×16 split resolutions perform similarly, while 32×32 degrades performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Unconditional training at low resolutions enables head classes to inject universal features (like backgrounds, object layouts) into tail classes.
- Mechanism: The generator is split into two sub-networks. The lower-resolution part (e.g., 8×8) is trained unconditionally, learning class-independent features shared across head and tail classes. These features are then passed to the higher-resolution conditional part, which applies class-specific details.
- Core assumption: Low-resolution features are more class-independent and can be effectively shared between head and tail classes.
- Evidence anchors:
  - [abstract] "we propose modifications to existing class-conditional GAN architectures to ensure that the lower-resolution layers of the generator are trained entirely unconditionally while reserving class-conditional generation for the higher-resolution layers."
  - [section] "We make a key observation that the similarity between head and tail instances is often higher at lower resolutions: discriminative features such as shape or specific texture are usually unveiled at higher resolutions."
  - [corpus] Weak or missing direct comparison to diffusion models or other generative architectures.
- Break condition: If low-resolution features are not sufficiently class-independent or if the mapping between low-res and high-res features becomes too nonlinear to preserve useful information.

### Mechanism 2
- Claim: Splitting the discriminator into conditional and unconditional branches prevents class imbalance from biasing low-resolution feature learning.
- Mechanism: The discriminator is divided into two sub-networks: one for unconditional low-res images and another for conditional high-res images. Each branch uses separate fully-connected layers for predictions, ensuring balanced learning at both resolutions.
- Core assumption: Separate discriminators for unconditional and conditional paths prevent mode collapse in tail classes by providing balanced gradient signals.
- Evidence anchors:
  - [section] "The discriminator does not see enough examples from the tail of the distribution during training, which can lead to poor discriminative signals for the generator."
  - [section] "To overcome this, we add a 1 × 1 convolutional layer, referred to as 'fromRGB', that increases the channels of the RGB images to match C, the number of input channels of Dl."
  - [corpus] No explicit ablation on removing the unconditional discriminator branch.
- Break condition: If the unconditional discriminator becomes too strong relative to the conditional one, it may dominate training and prevent meaningful class-specific learning.

### Mechanism 3
- Claim: Using few-shot metrics (FID-FS/KID-FS) provides a more accurate evaluation of tail class performance than standard metrics.
- Mechanism: When calculating FID and KID, the method samples an equal number of images from each class for both real and generated data, preventing head classes from dominating the metric.
- Core assumption: Standard FID/KID metrics are misleading in long-tailed settings because they assume balanced class distributions.
- Evidence anchors:
  - [abstract] "Due to the strong class imbalance in the long-tailed data, naive usage of existing GAN metrics can be misleading. To mitigate this issue, we propose a few practices to adapt commonly-used GAN metrics for long-tail setups."
  - [section] "Contrasting with the standard FID/KID, we maintain an equal number of real images across all classes during our FID-FS/KID-FS calculation."
  - [corpus] No direct comparison showing performance differences when using standard vs few-shot metrics.
- Break condition: If the few-shot subset is too small or unrepresentative, the metrics may become unstable or noisy.

## Foundational Learning

- Concept: Long-tail data distribution and its effects on GAN training
  - Why needed here: Understanding why standard GANs fail on long-tailed data is crucial for grasping the motivation behind UTLO.
  - Quick check question: What happens to GAN performance when training data follows a power-law distribution with few head classes and many tail classes?

- Concept: Conditional vs unconditional GAN objectives
  - Why needed here: The core innovation involves switching between conditional and unconditional training at different resolutions.
  - Quick check question: How does an unconditional GAN objective differ from a conditional one, and what are the implications for feature learning?

- Concept: Spectral normalization and mode collapse
  - Why needed here: Previous work identified spectral norm explosion in class-conditional BatchNorms as a cause of mode collapse in tail classes.
  - Quick check question: What is spectral normalization, and how can it contribute to mode collapse in class-conditional GANs?

## Architecture Onboarding

- Component map:
  - Generator: Gl (unconditional low-res) -> Gh (conditional high-res)
  - Discriminator: Dl (shared for both paths) -> Dh (conditional high-res only)
  - Style mapping: Shared mapping network produces wz (unconditional) and wz,y (conditional)
  - fromRGB layers: Convert RGB to discriminator input channels

- Critical path:
  1. Generate wz and wz,y from latent z using shared style mapping
  2. Pass wz through Gl to generate unconditional low-res image
  3. Pass wz,y through Gh to generate conditional high-res image
  4. Pass both images through Dl for unconditional and conditional losses
  5. Pass high-res image through Dh for conditional loss only

- Design tradeoffs:
  - Lower resuc (e.g., 8×8) provides more shared features but less class-specific control
  - Higher resuc (e.g., 32×32) provides more class-specific control but less shared feature injection
  - Weight λ balances unconditional vs conditional objectives

- Failure signatures:
  - Overfitting to head classes: FID improves but FID-FS/KID-FS degrade
  - Mode collapse in tail classes: Low diversity in generated tail class images
  - Training instability: Oscillating losses or NaN values

- First 3 experiments:
  1. Train StyleGAN2-ADA baseline on CIFAR10-LT (ρ=100) and observe FID-FS degradation
  2. Implement UTLO with resuc=8×8 and λ=1, compare FID-FS improvement
  3. Vary resuc (8×8, 16×16, 32×32) and observe impact on tail class diversity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of UTLO vary with different intermediate low-resolution choices (resuc) across various datasets and GAN architectures?
- Basis in paper: [explicit] The paper mentions an ablation study on resuc for AnimalFaces-LT and suggests that 8x8 and 16x16 resolutions perform similarly, while 32x32 degrades performance. However, the impact of different resuc choices across diverse datasets and architectures remains unexplored.
- Why unresolved: The ablation study is limited to a single dataset and architecture. The generalizability of the findings to other datasets and GAN architectures is unclear.
- What evidence would resolve it: Conducting extensive experiments on multiple datasets (e.g., CIFAR10-LT, CIFAR100-LT, LSUN5-LT) with various GAN architectures (e.g., StyleGAN2, Projected GAN, FastGAN) and different resuc values would provide a comprehensive understanding of the optimal resuc choice for different scenarios.

### Open Question 2
- Question: What is the impact of incorporating other regularization techniques (e.g., spectral normalization, gradient penalty) alongside UTLO on the performance of cGANs in long-tailed setups?
- Basis in paper: [inferred] The paper mentions that regularization techniques like GSR and NoisyTwins can help mitigate mode collapse in tail classes. However, it does not explore the combination of UTLO with other regularization methods.
- Why unresolved: The potential synergistic effects or conflicts between UTLO and other regularization techniques remain unexplored. It is unclear whether combining these methods would lead to further improvements or hinder performance.
- What evidence would resolve it: Conducting experiments that integrate UTLO with various regularization techniques (e.g., spectral normalization, gradient penalty) on long-tailed datasets and evaluating the performance using standard and few-shot metrics would provide insights into the effectiveness of such combinations.

### Open Question 3
- Question: How does the choice of unconditional training objective weight (λ) impact the performance of UTLO across different datasets and architectures?
- Basis in paper: [explicit] The paper mentions an ablation study on λ values for AnimalFaces-LT, suggesting that λ = 1 achieves the best performance for both 8x8 and 16x16 resolutions. However, the impact of different λ values across diverse datasets and architectures remains unexplored.
- Why unresolved: The optimal λ value may vary depending on the dataset characteristics, GAN architecture, and the degree of imbalance. The findings from the ablation study may not generalize to other scenarios.
- What evidence would resolve it: Conducting extensive experiments on multiple datasets with various GAN architectures and different λ values would provide a comprehensive understanding of the optimal λ choice for different scenarios. Analyzing the performance trends across datasets and architectures would help identify the factors influencing the optimal λ value.

## Limitations
- Limited theoretical justification for why unconditional low-resolution training works
- No ablation studies on the impact of different unconditional training weights λ
- No comparison to alternative approaches like diffusion models or other long-tail GAN methods

## Confidence
- Mechanism 1 (feature sharing at low resolution): Medium
- Mechanism 2 (discriminator splitting): Medium
- Mechanism 3 (few-shot metrics): Low

## Next Checks
1. **Ablation study on split resolution**: Systematically vary the unconditional training resolution (8×8, 16×16, 32×32) and measure the impact on tail class diversity and overall FID-FS performance to identify the optimal tradeoff point.

2. **Theoretical analysis of feature sharing**: Quantify the class-independency of low-resolution features through class-conditional entropy measurements or clustering analysis to provide empirical support for the core assumption about feature sharing.

3. **Comparison to alternative long-tail GAN approaches**: Implement and compare against other recent methods like GSR, NoisyTwins, and conditional diffusion models on the same benchmarks to establish UTLO's relative effectiveness and identify potential complementary strategies.