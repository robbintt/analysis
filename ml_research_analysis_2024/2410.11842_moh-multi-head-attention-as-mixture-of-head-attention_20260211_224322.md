---
ver: rpa2
title: 'MoH: Multi-Head Attention as Mixture-of-Head Attention'
arxiv_id: '2410.11842'
source_url: https://arxiv.org/abs/2410.11842
tags:
- attention
- heads
- multi-head
- training
- head
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Mixture-of-Head attention (MoH), a new attention
  mechanism that treats attention heads as experts in the Mixture-of-Experts (MoE)
  framework. By allowing each token to dynamically select the most relevant attention
  heads and replacing standard summation with weighted summation, MoH improves inference
  efficiency and model performance without increasing parameters.
---

# MoH: Multi-Head Attention as Mixture-of-Head Attention

## Quick Facts
- arXiv ID: 2410.11842
- Source URL: https://arxiv.org/abs/2410.11842
- Authors: Peng Jin; Bo Zhu; Li Yuan; Shuicheng Yan
- Reference count: 40
- Multi-head attention reimagined as Mixture-of-Experts with dynamic head selection

## Executive Summary
This paper introduces Mixture-of-Head Attention (MoH), a novel attention mechanism that treats attention heads as experts within a Mixture-of-Experts (MoE) framework. By allowing each token to dynamically select the most relevant attention heads and replacing standard summation with weighted summation, MoH improves inference efficiency and model performance without increasing parameters. The approach is validated across vision transformers (ViT), diffusion transformers (DiT), and large language models (LLMs).

## Method Summary
MoH introduces a routing mechanism that enables tokens to dynamically select relevant attention heads from the available pool. Unlike traditional multi-head attention where all heads are equally weighted and summed, MoH uses a gating function to compute importance weights for each head. The final attention output is computed as a weighted sum of head outputs rather than a simple summation. This framework allows models to use only 50%-90% of attention heads while maintaining or improving performance, effectively reducing computational overhead during inference.

## Key Results
- MoH achieves better performance than standard multi-head attention while using only 50%-90% of attention heads
- MoH-LLaMA3-8B achieves 64.0% average accuracy across 14 benchmarks, outperforming baseline LLaMA3-8B by 2.4%
- The mechanism maintains parameter efficiency while improving inference efficiency

## Why This Works (Mechanism)
MoH works by treating attention heads as specialized experts in a mixture-of-experts framework. The routing mechanism learns to identify which heads are most relevant for each token, allowing the model to focus computational resources on the most informative attention patterns. The weighted summation instead of simple addition enables soft selection of heads, preserving information from multiple relevant heads while suppressing less useful ones. This dynamic selection reduces redundancy across heads and improves the signal-to-noise ratio in attention computations.

## Foundational Learning
- **Attention Mechanisms**: Core component of transformer architectures that enables context-aware representations. Why needed: Foundation for understanding how MoH modifies traditional attention. Quick check: Can explain scaled dot-product attention.
- **Mixture-of-Experts (MoE)**: A model architecture where different "experts" are specialized for different inputs. Why needed: Provides conceptual framework for MoH's approach to attention heads. Quick check: Understands gating mechanisms in MoE.
- **Dynamic Routing**: Process of selecting which components to use based on input characteristics. Why needed: Key to MoH's selective head utilization. Quick check: Can describe routing in other MoE systems.
- **Weighted Summation**: Computing outputs as weighted combinations rather than simple addition. Why needed: Alternative to standard head summation in attention. Quick check: Understands how weights affect information aggregation.

## Architecture Onboarding
**Component Map**: Input -> Token Embedding -> MoH Routing -> Selected Head Outputs -> Weighted Sum -> Output
**Critical Path**: The routing mechanism (gating function) is the critical path, as it determines which heads contribute to each token's representation and introduces computational overhead that must be minimized.
**Design Tradeoffs**: Reduced head utilization improves efficiency but may limit the model's ability to capture diverse attention patterns. The routing mechanism adds computation but enables selective processing. Weighted summation preserves information but requires careful weight normalization.
**Failure Signatures**: Poor routing decisions leading to consistently underutilized heads, collapse to using only one or two heads, or weights becoming uniform (losing the benefit of selection). Performance degradation on tasks requiring diverse attention patterns.
**First Experiments**: 1) Ablation study varying the percentage of heads used (50%, 60%, 70%, 80%, 90%) to find optimal tradeoff. 2) Comparison of routing strategies (softmax vs sparsemax vs top-k). 3) Analysis of head utilization patterns across different layers and tasks.

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- The generalizability of MoH across diverse attention-based architectures beyond ViT, DiT, and LLMs remains uncertain
- The computational overhead of the routing mechanism for head selection is not fully characterized
- Performance gains may be sensitive to specific training configurations or hyperparameter choices

## Confidence
- **High**: MoH framework implementation and experimental methodology on tested models
- **Medium**: Claims about inference efficiency improvements and parameter efficiency
- **Medium**: Performance comparisons against LLaMA3-8B baseline

## Next Checks
1. Evaluate MoH on additional attention-based architectures (e.g., BERT, GPT variants) to test generalizability across model families
2. Conduct ablation studies isolating the contribution of the routing mechanism versus the weighted summation to quantify computational overhead
3. Test model robustness under domain shift conditions and adversarial attacks to assess reliability with reduced head utilization