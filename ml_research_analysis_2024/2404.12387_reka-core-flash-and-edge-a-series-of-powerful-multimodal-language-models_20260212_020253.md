---
ver: rpa2
title: 'Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models'
arxiv_id: '2404.12387'
source_url: https://arxiv.org/abs/2404.12387
tags:
- reka
- core
- flash
- evaluation
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "Reka introduces a series of multimodal language models\u2014Reka\
  \ Core, Flash, and Edge\u2014trained from scratch to handle text, images, video,\
  \ and audio inputs. The models are designed to be competitive with leading frontier\
  \ models while offering strong performance relative to their compute class."
---

# Reka Core, Flash, and Edge: A Series of Powerful Multimodal Language Models

## Quick Facts
- arXiv ID: 2404.12387
- Source URL: https://arxiv.org/abs/2404.12387
- Reference count: 8
- Models achieve competitive performance on MMMU, Perception-Test, and human evaluations

## Executive Summary
Reka introduces a series of multimodal language models—Reka Core, Flash, and Edge—trained from scratch to handle text, images, video, and audio inputs. The models are designed to be competitive with leading frontier models while offering strong performance relative to their compute class. Reka Core approaches top-tier models like GPT-4V and Gemini Ultra on benchmarks such as MMMU and Perception-Test, and ranks second in blind human evaluations for multimodal chat. Reka Flash and Edge deliver state-of-the-art performance for their respective parameter sizes (21B and 7B), outperforming larger models like Llama 2 70B and Gemini Pro on various tasks.

## Method Summary
The models were trained on a mix of public and proprietary datasets, with extensive deduplication and filtering, and support multilingual capabilities across 32 languages. Training involved aggressive learning rates and large-scale infrastructure, with models achieving strong results in both language and multimodal evaluations. The models use a transformer-based architecture with modifications for multimodal input processing, including specialized tokenization for non-text modalities.

## Key Results
- Reka Core approaches GPT-4V and Gemini Ultra on MMMU and Perception-Test benchmarks
- Reka Flash and Edge achieve SOTA performance for their parameter classes, outperforming larger models
- Reka Core ranks second in blind human evaluations for multimodal chat capabilities

## Why This Works (Mechanism)
The models leverage large-scale training on diverse multimodal datasets with careful filtering and deduplication. The transformer architecture is extended with modality-specific processing layers that allow efficient fusion of different input types. Aggressive learning rate schedules and large batch training enable rapid convergence while maintaining performance.

## Foundational Learning
- **Multimodal training**: Combining text, images, video, and audio requires understanding cross-modal relationships - quick check: test with mixed input types
- **Large-scale data processing**: Handling billions of tokens requires efficient filtering and deduplication pipelines - quick check: measure data quality metrics
- **Transformer architecture**: Standard transformer blocks modified for multimodal inputs - quick check: compare with unimodal baselines
- **Aggressive optimization**: High learning rates and large batch sizes accelerate training - quick check: monitor loss curves for stability
- **Multilingual support**: Training across 32 languages requires careful tokenization and representation - quick check: evaluate on cross-lingual tasks
- **Human evaluation protocols**: Blind comparisons ensure unbiased assessment of capabilities - quick check: verify inter-rater reliability

## Architecture Onboarding

**Component Map**: Input Tokenization -> Modality Encoders -> Cross-Attention Fusion -> Transformer Blocks -> Output Generation

**Critical Path**: The cross-attention fusion layers between modality encoders and transformer blocks are critical for performance. These layers must efficiently combine information from different modalities while maintaining computational efficiency.

**Design Tradeoffs**: The models balance parameter efficiency with capability by using modality-specific processing only where needed. Smaller models (Flash, Edge) sacrifice some cross-modal reasoning depth for inference speed and deployment efficiency.

**Failure Signatures**: Common failure modes include poor handling of rare languages, confusion between similar visual concepts, and degraded performance on tasks requiring long temporal reasoning in videos.

**3 First Experiments**:
1. Test single-modality performance to establish baselines for each input type
2. Evaluate cross-modal reasoning with controlled mixed-input tasks
3. Measure inference latency and memory usage across different model sizes

## Open Questions the Paper Calls Out
None

## Limitations
- No details provided about training data composition or licensing, making reproducibility difficult
- Human evaluation methodology lacks specification of sample sizes and evaluation protocols
- Some comparisons are made against models with different parameter counts without accounting for this factor

## Confidence
- **High confidence**: Reka Core's strong performance on standard benchmarks (MMMU, Perception-Test) - results are verifiable against published baselines
- **Medium confidence**: Claims about model capabilities and multilingual support - limited technical detail on implementation
- **Medium confidence**: Comparative performance claims against other models - some comparisons lack apples-to-apples matching

## Next Checks
1. Conduct ablation studies to determine the contribution of individual architectural components to performance gains
2. Verify benchmark results on a held-out test set not used during any training or tuning phases
3. Replicate key performance metrics using the publicly available models through the provided interfaces to confirm reported capabilities