---
ver: rpa2
title: 'Exploring ReAct Prompting for Task-Oriented Dialogue: Insights and Shortcomings'
arxiv_id: '2412.01262'
source_url: https://arxiv.org/abs/2412.01262
tags:
- user
- system
- dialogue
- domain
- train
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: ReAct prompting guides LLMs through task-oriented dialogue by combining
  reasoning with external tool use, but shows limited success in simulation compared
  to state-of-the-art baselines (e.g., success rate 28.2% for GPT-3.5 vs. 97.3% for
  HDC).
---

# Exploring ReAct Prompting for Task-Oriented Dialogue: Insights and Shortcomings

## Quick Facts
- arXiv ID: 2412.01262
- Source URL: https://arxiv.org/abs/2412.01262
- Authors: Michelle Elizabeth; Morgan Veyret; Miguel Couceiro; Ondrej Dusek; Lina M. Rojas-Barahona
- Reference count: 26
- One-line primary result: ReAct-LLM shows lower task completion (28.2% for GPT-3.5) compared to specialized baselines (97.3%) but receives higher user satisfaction for natural responses

## Executive Summary
This paper evaluates ReAct prompting for task-oriented dialogue systems, comparing GPT-3.5 and GPT-4 performance against state-of-the-art baselines. The ReAct-LLM approach combines reasoning with external tool use through few-shot examples, guiding LLMs through sequential thought-action-observation patterns. While ReAct-LLM underperforms specialized dialogue systems in task completion metrics, human evaluators prefer its natural and confident responses. GPT-4 significantly outperforms GPT-3.5 in format compliance, clarification skills, and politeness, though at 60x higher cost per token. The study reveals fundamental challenges with consistent reasoning, dialogue state tracking, and tool usage that limit ReAct-LLM's practical deployment despite its conversational advantages.

## Method Summary
The authors implement ReAct prompting with GPT-3.5 and GPT-4 on the MultiWOZ dataset using CONVLAB 3 benchmark for simulation and human evaluation. The system employs few-shot examples to teach LLMs a thought-action-observation pattern for task-oriented dialogue, using external tools (list_domains, list_slots, db_query, generate_booking_reference) for database interaction. They evaluate performance through automated metrics (success rate, book rate, inform rate, complete rate, average turns) and human satisfaction scores across 1000 simulated dialogues and 95 human-evaluated dialogues. The implementation uses LangChain for LLM integration and Langfuse for debugging and tracking.

## Key Results
- GPT-3.5 success rate: 28.2% vs HDC baseline: 97.3% on MultiWOZ simulation
- GPT-4 generates more format-compliant, clarifying, and polite responses than GPT-3.5
- Human evaluators prefer ReAct-LLM's natural responses despite lower task completion rates
- ReAct-LLM struggles with consistent reasoning, dialogue state tracking, and tool usage

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ReAct prompting guides LLMs through task-oriented dialogue by decomposing complex tasks into sequential reasoning and external tool calls.
- Mechanism: The ReAct-LLM system uses few-shot examples to teach the LLM a pattern of thoughts → actions → observations, where each thought plans the next step, actions call database functions, and observations analyze the results to update the belief state.
- Core assumption: LLMs can generalize the demonstrated reasoning pattern to novel dialogue contexts and correctly identify when to use each tool.
- Evidence anchors:
  - [abstract] "ReAct employs few-shot LLM prompting with a sequence of thoughts, actions, and observations."
  - [section 3] "The reasoning process to be followed by the SA is outlined below: Step 1: The SA should try to understand the user input. Its first thought should be explicitly planning out the next course of action..."
  - [corpus] Weak - no direct citations, but related work exists on ReAct prompting for other tasks.
- Break condition: The LLM fails to follow the prescribed thought-action-observation sequence, leading to invalid tool calls or inconsistent reasoning traces.

### Mechanism 2
- Claim: ReAct-LLM systems can produce more natural and fluent responses compared to rule-based baselines, even when task completion is lower.
- Mechanism: The LLM generates full natural language responses instead of delexicalized templates, allowing for more conversational and context-aware replies that users find more satisfying.
- Core assumption: Users prefer natural language fluency over strict task accuracy, and the LLM can maintain coherence across multiple dialogue turns.
- Evidence anchors:
  - [abstract] "humans report higher subjective satisfaction with ReAct-LLM despite its lower success rate, most likely thanks to its natural and confidently phrased responses."
  - [section 5.2] "Overall, users are more satisfied with ReAct-LLM than with HDC, despite the better success rate of HDC, because ReAct-LLM is self-confident and generates well structured, polite, fluent and natural sentences..."
  - [corpus] Weak - satisfaction is measured subjectively; no objective fluency metrics provided.
- Break condition: The LLM produces responses that are fluent but incorrect or misleading, causing user frustration despite natural phrasing.

### Mechanism 3
- Claim: GPT-4 outperforms GPT-3.5 in ReAct-based TOD systems due to better format compliance, clarification skills, and politeness.
- Mechanism: GPT-4 generates outputs that more closely follow the prescribed response format, asks clarifying questions when user input is ambiguous, and produces more verbose and polite replies compared to GPT-3.5.
- Core assumption: GPT-4's larger model size and training data provide better instruction-following capabilities and social awareness in conversational contexts.
- Evidence anchors:
  - [section 5.3] "Compared to the GPT-3.5 agent, the GPT-4 agent is more consistent with respect to the desired reply format, is better at clarifying, and produces more verbose and polite replies."
  - [section D] "We saw that the performance of GPT-4 is superior to GPT-3 when we consider the quality of the reasoning and generated texts."
  - [corpus] Moderate - supported by direct comparison, but limited sample size (50 goals analyzed).
- Break condition: The improvement from GPT-4 over GPT-3.5 does not justify the substantial cost increase (60x more expensive per token).

## Foundational Learning

- Concept: Task-oriented dialogue (TOD) pipeline components
  - Why needed here: Understanding the standard TOD architecture (NLU, DST, DM, NLG) is essential for appreciating how ReAct-LLM differs from traditional approaches and where it might struggle.
  - Quick check question: What are the four main components of a traditional TOD system, and how does ReAct-LLM attempt to replace them?

- Concept: External tool integration with LLMs
  - Why needed here: The ReAct approach relies on the LLM's ability to call external functions (list_domains, db_query, etc.) based on reasoning, which requires understanding function calling patterns and parameter handling.
  - Quick check question: How does the ReAct system decide which tool to call and with what parameters at each step of the dialogue?

- Concept: Evaluation metrics for dialogue systems
  - Why needed here: Success rate, book rate, inform rate, and complete rate are used to measure performance; understanding these metrics is crucial for interpreting the results.
  - Quick check question: What is the difference between success rate and complete rate in the context of task-oriented dialogue evaluation?

## Architecture Onboarding

- Component map: User Input → ReAct-LLM System Agent → External Tools → Database → ReAct-LLM System Agent → Natural Language Response → User

- Critical path:
  1. User input → SA
  2. SA thought: Identify domain and slots needed
  3. SA action: Call list_domains
  4. SA observation: Analyze available domains
  5. SA thought: Select appropriate domain
  6. SA action: Call list_slots(domain)
  7. SA observation: Identify relevant slots
  8. SA thought: Formulate belief state
  9. SA action: Call db_query(domain, state)
  10. SA observation: Retrieve matching entities
  11. SA thought: Generate final response
  12. SA action: Call generate_booking_reference if needed
  13. SA output: Natural language response to user

- Design tradeoffs:
  - Flexibility vs. control: ReAct-LLM offers more natural responses but loses fine-grained control over each pipeline step
  - Cost vs. performance: GPT-4 significantly outperforms GPT-3.5 but at 60x higher cost
  - User satisfaction vs. task completion: Users prefer ReAct-LLM's natural responses despite lower success rates
  - Zero-shot learning vs. specialized systems: ReAct-LLM requires no task-specific training but underperforms specialized baselines

- Failure signatures:
  - Invalid tool calls or parameters
  - Inconsistent reasoning traces that deviate from instructions
  - Hallucinations producing invalid dialogue states or slot values
  - Failure to ask clarifying questions when user input is ambiguous
  - Excessive turns due to repeated misunderstandings

- First 3 experiments:
  1. Run 10 simulated dialogues with GPT-3.5 using generic examples; analyze failure modes in tool usage and reasoning consistency
  2. Run 10 simulated dialogues with GPT-4 using generic examples; compare format compliance and clarification behavior against GPT-3.5
  3. Run 5 human evaluation sessions with both GPT-3.5 and GPT-4; collect qualitative feedback on response naturalness and task completion

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can ReAct-LLM's reasoning consistency be improved to reduce deviations from instructions and hallucinations?
- Basis in paper: Explicit - The paper identifies that reasoning traces may just imitate examples, reasoning is inconsistent, and the LLM struggles to stay within bounds set by instructions, often producing invalid dialogue states or not sticking to the set of external tools given.
- Why unresolved: While the paper identifies these issues, it doesn't propose specific solutions for improving the LLM's reasoning consistency or preventing it from deviating from instructions and hallucinating invalid states.
- What evidence would resolve it: Experiments showing improved reasoning consistency through techniques like enhanced prompt engineering, additional fine-tuning, or architectural modifications that constrain the LLM's reasoning process.

### Open Question 2
- Question: Would incorporating clarifying questions into ReAct-LLM's dialogue flow improve success rates in complex user requests?
- Basis in paper: Explicit - The paper notes that ReAct-LLM does not ask clarifying questions when user input might be incomplete, and that GPT-4 (which does ask clarifying questions) performs better in this regard.
- Why unresolved: The paper observes that GPT-4's ability to ask clarifying questions is a positive trait, but doesn't experiment with explicitly incorporating such questions into the ReAct-LLM framework to measure the impact on success rates.
- What evidence would resolve it: Comparative evaluation showing whether adding clarifying question capabilities to ReAct-LLM improves task completion success rates, particularly for complex goals requiring multiple slots.

### Open Question 3
- Question: How does the performance gap between ReAct-LLM and specialized dialogue systems change with domain complexity and goal length?
- Basis in paper: Explicit - The paper notes that ReAct-LLMs underperform state-of-the-art baselines in simulation but the difference becomes less pronounced in human evaluation, and that shorter goals are easier to achieve while longer, more complex goals pose challenges.
- Why unresolved: While the paper observes these patterns, it doesn't systematically analyze how the performance gap varies across different domain complexities or goal lengths, or identify at what point ReAct-LLM's performance deteriorates significantly.
- What evidence would resolve it: Detailed analysis correlating performance metrics with goal complexity metrics (number of domains, slots, turns required) to identify thresholds where ReAct-LLM's performance significantly degrades relative to specialized systems.

## Limitations

- Significant performance gap between ReAct-LLM and specialized baselines (28.2% vs 97.3% success rate)
- Fundamental difficulties with consistent reasoning and dialogue state tracking
- High cost of GPT-4 implementation (60x more expensive than GPT-3.5 per token)

## Confidence

**High confidence**: The comparative performance between GPT-3.5 and GPT-4 in format compliance, clarification, and politeness is well-supported by direct evidence from the evaluation section. The observation that users prefer ReAct-LLM's natural responses despite lower task completion rates is also strongly supported.

**Medium confidence**: The characterization of failure modes (invalid tool calls, inconsistent reasoning, hallucinations) is supported by the qualitative analysis, but the paper could provide more systematic categorization of these failures across the evaluation corpus.

**Low confidence**: The claim that ReAct-LLM demonstrates "potential for more natural interactions" is aspirational and not strongly supported by the quantitative results, which show significantly lower task completion rates than specialized systems.

## Next Checks

1. **Systematic failure mode analysis**: Conduct a detailed categorization of failure types across the entire evaluation set (not just qualitative examples) to identify the most common failure modes and their relative frequency.

2. **Cost-benefit analysis**: Calculate the exact cost per successful dialogue for both GPT-3.5 and GPT-4 implementations, and determine the break-even point where GPT-4's improved performance justifies its 60x higher cost.

3. **Long-term user satisfaction study**: Conduct a longitudinal study where users interact with both ReAct-LLM and HDC systems over multiple sessions to determine if initial satisfaction with natural responses persists or diminishes as users encounter repeated task completion failures.