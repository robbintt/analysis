---
ver: rpa2
title: Improve Fidelity and Utility of Synthetic Credit Card Transaction Time Series
  from Data-centric Perspective
arxiv_id: '2401.00965'
source_url: https://arxiv.org/abs/2401.00965
tags:
- data
- synthetic
- dataset
- fraud
- schema
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper tackles the challenge of generating high-fidelity synthetic
  credit card transaction data using the Conditional Probabilistic Auto-Regressive
  (CPAR) model. The authors propose five preprocessing schemas to improve data quality
  before training CPAR.
---

# Improve Fidelity and Utility of Synthetic Credit Card Transaction Time Series from Data-centric Perspective

## Quick Facts
- arXiv ID: 2401.00965
- Source URL: https://arxiv.org/abs/2401.00965
- Authors: Din-Yin Hsieh; Chi-Hua Wang; Guang Cheng
- Reference count: 40
- Key outcome: This paper tackles the challenge of generating high-fidelity synthetic credit card transaction data using the Conditional Probabilistic Auto-Regressive (CPAR) model. The authors propose five preprocessing schemas to improve data quality before training CPAR. These schemas incrementally enhance the fidelity of synthetic data, particularly for categorical variables like Location and MCC, and continuous variables like Amount. The best schema (Schema 5) produces synthetic data that closely matches the original data distribution. The utility of the synthetic data is evaluated by training fraud detection models (Catboost, XGBoost, LGBM) on datasets with varying fraud-to-non-fraud ratios. Catboost and LGBM achieve near-zero false positive and false negative rates, outperforming XGBoost. The study demonstrates that with appropriate preprocessing, synthetic data can serve as a high-fidelity alternative to real data for training machine learning models in fraud detection.

## Executive Summary
This paper addresses the challenge of generating high-fidelity synthetic credit card transaction data for fraud detection. The authors introduce five preprocessing schemas to enhance the training of the Conditional Probabilistic Auto-Regressive (CPAR) model, demonstrating incremental improvements in synthetic data's fidelity and utility. The best schema, Schema 5, applies cubic root transformation to mitigate non-Gaussianity in the Amount column, resulting in synthetic data that closely matches the original distribution. The utility of the synthetic data is evaluated by training fraud detection models on datasets with varying fraud-to-non-fraud ratios, with Catboost and LGBM achieving near-zero false positive and false negative rates.

## Method Summary
The paper proposes a method to generate high-fidelity synthetic credit card transaction data using the CPAR model and five preprocessing schemas. The authors preprocess the original dataset by applying various transformations, such as standardization, cubic root transformation, and label encoding, to improve the model's performance. The CPAR model is then trained on the preprocessed data to generate synthetic datasets for 3 users. The utility of the synthetic data is evaluated by training fraud detection models (Catboost, XGBoost, and LGBM) on datasets with varying fraud-to-non-fraud ratios and comparing their False Positive Rate (FPR) and False Negative Rate (FNR) to the original data.

## Key Results
- Schema 5 preprocessing significantly improves the fidelity of synthetic data, particularly for continuous variables like Amount.
- Synthetic data generated using Schema 5 closely matches the original data distribution.
- Catboost and LGBM fraud detection models trained on synthetic data achieve near-zero FPR and FNR, outperforming XGBoost.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Preprocessing schemas (especially Schema 5) improve synthetic data fidelity by aligning continuous variable distributions to Gaussian-like shapes before model training.
- Mechanism: The CPAR model assumes Gaussian-distributed continuous variables (Amount). Schema 5 applies zero-mean, unit-variance standardization followed by cubic root transformation to mitigate non-Gaussianity. This reduces model bias and improves synthetic output matching.
- Core assumption: The transformation improves distributional similarity without destroying the transactional structure.
- Evidence anchors:
  - [abstract] "five pre-processing schemas to enhance the training of the Conditional Probabilistic Auto-Regressive Model (CPAR), demonstrating incremental improvements in the synthetic data’s fidelity and utility."
  - [section] "Schema 5: Mitigating Non-Gaussianity with Cubic Root Transformation. Building on Schema 3, this final schema standardizes the Amount column to zero mean and unit variance, followed by a cubic root transformation."
  - [corpus] Weak evidence. No direct mention of preprocessing in related papers.

### Mechanism 2
- Claim: Encoding high-cardinality categorical features as strings before CPAR training prevents out-of-category synthetic generation.
- Mechanism: Label encoding followed by string conversion ensures CPAR treats categories as discrete entities rather than continuous integers, avoiding impossible synthetic combinations.
- Core assumption: CPAR's categorical handling is sensitive to data type representation, not just value semantics.
- Evidence anchors:
  - [section] "Building upon prior schemas, we apply Scikit-Learn’s Label Encoder to categorical columns (Use Chip, MCC, Errors?, Location, and Is Fraud?), maintaining their 'string' data types..."
  - [abstract] "We introduce five pre-processing schemas to enhance the training of the Conditional Probabilistic Auto-Regressive Model (CPAR)..."
  - [corpus] Weak evidence. No related work directly addresses categorical encoding for CPAR.

### Mechanism 3
- Claim: Combining location fields into a single string column reduces synthetic data artifacts by preserving coherent location contexts.
- Mechanism: Separating Merchant Name, City, State, and Zip allows CPAR to generate mismatched tuples (e.g., correct city but wrong state). Merging them ensures consistency.
- Core assumption: Location coherence is preserved in the original data and should be maintained synthetically.
- Evidence anchors:
  - [section] "Building on the previous schema, we integrate Merchant Name, Merchant City, Merchant State, and Zip into a single 'string' data type Location column to prevent the creation of non-existent locations..."
  - [abstract] "incremental improvements in the synthetic data’s fidelity..."
  - [corpus] Weak evidence. No corpus papers discuss location field merging.

## Foundational Learning

- Concept: Gaussian distribution assumptions in probabilistic generative models
  - Why needed here: CPAR models continuous variables (Amount) as Gaussian; preprocessing must align data to this assumption.
  - Quick check question: What transformation would you apply if the Amount distribution is heavily right-skewed before standardization?

- Concept: Label encoding and categorical data handling in ML models
  - Why needed here: CPAR expects categorical features to be encoded consistently; incorrect encoding leads to invalid synthetic outputs.
  - Quick check question: If you encode Use Chip as integers instead of strings, what kind of synthetic errors might CPAR produce?

- Concept: Data cardinality and its impact on model performance
  - Why needed here: High-cardinality fields (Merchant Name, Zip) can overwhelm CPAR's categorical handling; preprocessing must manage this.
  - Quick check question: How would you test whether CPAR is generating out-of-category values for a high-cardinality feature?

## Architecture Onboarding

- Component map: Raw credit card transaction CSV -> Preprocessor (5 schema transformers) -> CPAR (neural network with GRU + Dense layers) -> Synthetic transaction dataset -> Evaluator (fidelity metrics + fraud detection model performance)
- Critical path: 1. Load and parse raw data 2. Apply Schema 5 preprocessing 3. Train CPAR on preprocessed data 4. Generate synthetic dataset 5. Evaluate fidelity (marginal distributions, Wasserstein distance) 6. Train fraud detection models on synthetic data 7. Validate against original fraud labels
- Design tradeoffs: Schema complexity vs. fidelity gain, encoding overhead vs. model accuracy, transformation reversibility vs. distribution alignment
- Failure signatures: Synthetic data shows impossible combinations (e.g., invalid locations), continuous variable distribution mismatch after generation, fraud detection models fail to generalize from synthetic to real data
- First 3 experiments: 1. Apply Schema 1 only; measure marginal distribution drift vs. original 2. Apply Schema 5; compare synthetic vs. original Amount distribution using Wasserstein distance 3. Train Catboost on Schema 5 synthetic data; measure FPR/FNR vs. training on real data

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different preprocessing schemas impact the fidelity of synthetic data for other continuous variables in credit card transaction data, beyond the Amount column?
- Basis in paper: [explicit] The paper mentions that Schema 5 closely matches the original transaction amount distribution, but does not provide detailed fidelity analysis for other continuous variables.
- Why unresolved: The paper focuses on the Amount column for fidelity evaluation of continuous variables, leaving the impact on other continuous variables unexplored.
- What evidence would resolve it: Detailed fidelity evaluation results for other continuous variables in credit card transaction data, comparing different preprocessing schemas.

### Open Question 2
- Question: What are the effects of preprocessing schemas on the utility of synthetic data for fraud detection models other than Catboost, XGBoost, and LGBM?
- Basis in paper: [explicit] The paper evaluates the utility of synthetic data for fraud detection using Catboost, XGBoost, and LGBM, but does not explore other potential fraud detection models.
- Why unresolved: The study limits the evaluation to three specific fraud detection models, leaving the potential utility of synthetic data for other models unexplored.
- What evidence would resolve it: Utility evaluation results for synthetic data trained fraud detection models other than Catboost, XGBoost, and LGBM.

### Open Question 3
- Question: How does the preprocessing schema affect the synthetic data's ability to generalize to unseen fraud patterns in real-world scenarios?
- Basis in paper: [inferred] The paper focuses on the fidelity and utility of synthetic data for fraud detection within the context of the provided dataset, but does not address the generalization capability to unseen fraud patterns.
- Why unresolved: The study does not investigate the synthetic data's performance in detecting fraud patterns not present in the original dataset.
- What evidence would resolve it: Evaluation results showing the synthetic data's performance in detecting unseen fraud patterns in real-world scenarios.

## Limitations
- The exact dataset structure and content remain unspecified, limiting reproducibility without access to the original credit card transaction data.
- No explicit evaluation of synthetic data privacy guarantees (e.g., membership inference resistance).
- The study focuses on a single CPAR model variant without ablation studies across different generative architectures.

## Confidence
- **High Confidence**: The mechanism of preprocessing schemas improving synthetic data fidelity (particularly Schema 5's cubic root transformation for Gaussian alignment).
- **Medium Confidence**: The claim that string-encoded categorical features prevent out-of-category synthetic generation in CPAR.
- **Low Confidence**: The assertion that merged location fields universally improve synthetic data coherence across all transaction datasets.

## Next Checks
1. Replicate Schema 5 preprocessing and CPAR training on an independently sourced credit card transaction dataset to verify fidelity improvements.
2. Conduct a sensitivity analysis of fraud detection model performance across different synthetic data generation runs to assess stability.
3. Implement a privacy evaluation (e.g., membership inference attack) to verify synthetic data does not leak information about the original dataset.