---
ver: rpa2
title: Progressive Mixed-Precision Decoding for Efficient LLM Inference
arxiv_id: '2410.13461'
source_url: https://arxiv.org/abs/2410.13461
tags:
- precision
- decoding
- pmpd
- scheduler
- quantization
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Progressive Mixed-Precision Decoding (PMPD)
  to improve LLM inference efficiency on resource-constrained devices. PMPD addresses
  the memory-bound nature of LLM decoding by dynamically lowering numerical precision
  deeper into the generated sequence, leveraging the observation that later tokens
  are more resilient to approximation errors.
---

# Progressive Mixed-Precision Decoding for Efficient LLM Inference

## Quick Facts
- arXiv ID: 2410.13461
- Source URL: https://arxiv.org/abs/2410.13461
- Reference count: 22
- Primary result: 3.8-8.0× throughput gains on NPU and 1.4-12.2× speedup on GPU linear layers over fp16 models while preserving output quality

## Executive Summary
This paper introduces Progressive Mixed-Precision Decoding (PMPD) to improve LLM inference efficiency on resource-constrained devices. PMPD addresses the memory-bound nature of LLM decoding by dynamically lowering numerical precision deeper into the generated sequence, leveraging the observation that later tokens are more resilient to approximation errors. The method combines phase-aware precision allocation (using high precision during prefill, low precision during decoding) with progressive mixed-precision decoding, guided by either static or learned precision-switching schedulers. Evaluated across multiple models (Vicuna-7B, MobileLLaMA-1.4B, StableLM Zephyr-3B, Phi-1.5) and tasks, PMPD achieves significant throughput gains while maintaining output quality, making it particularly effective for edge deployment scenarios.

## Method Summary
PMPD is a two-stage process combining offline calibration and deployment. During calibration, the method evaluates all combinations of prefill/decoding precisions on calibration datasets, selecting the lowest pair meeting quality targets. For deployment, PMPD employs progressive mixed-precision decoding with static or learned precision-switching schedulers. The static scheduler uses exhaustive search on validation sets, while the learned scheduler is trained on task-agnostic data using KV cache features from the last attention block. The approach leverages Any-Precision LLM quantization to generate variably quantized models (2-4 bits) and applies phase-aware precision allocation to balance context extraction during prefill with efficient memory bandwidth utilization during decoding.

## Key Results
- Achieves 3.8-8.0× throughput gains on NPU and 1.4-12.2× speedup on GPU linear layers over fp16 models
- Maintains output quality (BLEU, Rouge-L, BERTScore) within acceptable bounds across all tested models
- Outperforms KV cache quantization baselines in both algorithmic and hardware performance
- Learned scheduler performs close to or better than static scheduler in various scenarios

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Tokens generated later in the decoding sequence are more resilient to quantization errors.
- Mechanism: The model uses high precision for earlier tokens (which are more sensitive) and progressively reduces precision for later tokens, balancing output quality with computational efficiency.
- Core assumption: Later tokens are less critical for overall output quality and have accumulated context that makes them more robust to approximation.
- Evidence anchors:
  - [abstract] "leveraging the observation that later tokens are more resilient to approximation errors"
  - [section] "tokens deeper in the decoded sequence are more resilient to approximations"
  - [corpus] "MixPE: Quantization and Hardware Co-design for Efficient LLM Inference" suggests similar mixed-precision approaches
- Break condition: If the context window is extremely short, there may not be enough tokens for progressive lowering to be effective.

### Mechanism 2
- Claim: Different phases of LLM inference (prefill vs decoding) have distinct error resilience profiles.
- Mechanism: High precision is used during prefill for strong context extraction, while lower precision is used during decoding to improve memory bandwidth utilization.
- Core assumption: The prefill phase is compute-bound and thus can afford higher precision without significant latency impact, while decoding is memory-bound and benefits from precision reduction.
- Evidence anchors:
  - [abstract] "selectively allocates precision during different phases of LLM inference, achieving both strong context extraction during prefill and efficient memory bandwidth utilization during decoding"
  - [section] "prefilling was performed by 2- (top) or 3-bit (bottom) Vicuna-7B, while decoding employed the 2-bit model for both cases"
  - [corpus] "AlignedKV: Reducing Memory Access of KV-Cache with Precision-Aligned Quantization" addresses similar phase-aware precision considerations
- Break condition: If the prefill phase becomes memory-bound (e.g., with extremely long prompts), the latency benefit may diminish.

### Mechanism 3
- Claim: Dynamic precision scheduling based on task and prompt characteristics improves performance.
- Mechanism: Two scheduler types - static (task-specific) and learned (prompt-adaptive) - determine optimal precision-switching points.
- Core assumption: The optimal precision-switching schedule varies based on task characteristics and prompt difficulty, which can be learned or statically optimized.
- Evidence anchors:
  - [abstract] "a spectrum of precision-switching schedulers that dynamically drive the precision-lowering decisions in either task-adaptive or prompt-adaptive manner"
  - [section] "we introduce two types of runtime schedulers that drive precision-switching decisions in a prompt- and task-agnostic manner"
  - [corpus] "FGMP: Fine-Grained Mixed-Precision Weight and Activation Quantization" explores similar adaptive precision strategies
- Break condition: If the scheduler model is too simple or undertrained, it may not capture the nuanced patterns needed for optimal scheduling.

## Foundational Learning

- Concept: Quantization in neural networks
  - Why needed here: PMPD fundamentally relies on weight quantization as the approximation mechanism for precision reduction
  - Quick check question: What is the primary trade-off when reducing bitwidth in quantization?

- Concept: Transformer inference phases
  - Why needed here: Understanding the distinction between prefill and decoding phases is crucial for implementing phase-aware precision allocation
  - Quick check question: Which phase is typically memory-bound in transformer inference?

- Concept: KV cache and attention mechanism
  - Why needed here: The KV cache plays a key role in the prefill phase and influences the learned scheduler's input features
  - Quick check question: What information does the KV cache store during transformer inference?

## Architecture Onboarding

- Component map:
  - Input: Raw prompt text
  - Prefill engine: High-precision model processing
  - KV cache: Context storage from prefill
  - Decoding engine: Progressive mixed-precision model
  - Scheduler: Determines precision-switching points
  - Output: Generated text sequence

- Critical path: Prompt → Prefill (high precision) → KV cache → Decoding (progressive precision) → Output

- Design tradeoffs:
  - Precision vs. quality: Lower precision improves efficiency but risks quality degradation
  - Static vs. learned scheduler: Static offers predictability, learned offers adaptability
  - Scheduler complexity vs. runtime overhead: More complex schedulers may introduce latency

- Failure signatures:
  - Quality degradation: Indicates aggressive quantization or poor scheduler decisions
  - Memory bandwidth saturation: Suggests insufficient precision reduction
  - CPU-side bottlenecks: May occur during GPU kernel launching for quantized operations

- First 3 experiments:
  1. Implement phase-aware precision allocation with fixed precisions (e.g., 3-bit prefill, 2-bit decode) and measure quality impact
  2. Add progressive mixed-precision decoding with a simple static scheduler (e.g., switch precision at midpoint)
  3. Implement learned scheduler with KV cache features and compare against static scheduler performance

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the traditional sense. However, several implicit questions emerge from the evaluation scope and methodology:

1. How does PMPD's learned scheduler generalize across different model architectures (e.g., transformer variants, Mamba, RWKV) and modalities (vision, audio)?
2. What is the impact of PMPD on task-specific fine-tuned models versus base models, and does the progressive precision lowering strategy affect specialized capabilities?
3. How does PMPD interact with other inference optimization techniques like speculative decoding, continuous batching, or KV cache compression, and what are the compound effects?

## Limitations

- The assumption about token resilience to quantization errors is primarily theoretical, with limited direct empirical validation across diverse architectures.
- The learned scheduler's performance is evaluated only on a limited set of task-agnostic datasets, with unexplored robustness to domain shifts or adversarial prompts.
- The method's effectiveness may be constrained by the specific quantization scheme (Any-Precision LLM) and its compatibility with different hardware platforms beyond the tested NPU and GPU configurations.

## Confidence

- **High Confidence**: The core mechanism of progressive precision reduction during decoding is well-supported by the ablation studies and achieves the claimed throughput improvements (3.8-8.0× on NPU, 1.4-12.2× on GPU).
- **Medium Confidence**: The learned scheduler's ability to generalize across tasks is supported by the results, but the limited evaluation scope and lack of robustness testing reduce confidence in its real-world applicability.
- **Low Confidence**: The assumption about token resilience to quantization errors is primarily theoretical, with limited direct empirical validation.

## Next Checks

1. **Robustness Testing**: Evaluate the learned scheduler's performance on out-of-distribution prompts and adversarial inputs to assess its generalization capabilities. This includes testing with prompts that contain rare words, unusual syntax, or require complex reasoning.

2. **Architecture Generalization**: Apply PMPD to a diverse set of LLM architectures (e.g., OPT, LLaMA, BLOOM) and compare the quality preservation and throughput gains across models of varying sizes and training objectives. This will help determine the method's applicability beyond the tested models.

3. **Extreme Scenario Evaluation**: Test PMPD's performance on extremely long sequences (e.g., >10,000 tokens) and in memory-constrained environments (e.g., mobile devices with <4GB RAM) to identify potential limitations and edge cases. This includes measuring the impact on latency and quality when the KV cache size becomes a bottleneck.