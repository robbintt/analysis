---
ver: rpa2
title: 'Human-AI Safety: A Descendant of Generative AI and Control Systems Safety'
arxiv_id: '2405.09794'
source_url: https://arxiv.org/abs/2405.09794
tags:
- safety
- human
- systems
- control
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The paper identifies a gap in AI safety: current approaches focus
  on fine-tuning model outputs in isolation, ignoring how human responses create dynamic
  feedback loops that affect safety outcomes. The authors propose a new framework
  combining control systems safety theory with generative AI''s predictive capabilities.'
---

# Human-AI Safety: A Descendant of Generative AI and Control Systems Safety

## Quick Facts
- arXiv ID: 2405.09794
- Source URL: https://arxiv.org/abs/2405.09794
- Reference count: 26
- Primary result: Proposes a framework combining control systems safety theory with generative AI to address the gap in current human-AI safety approaches that ignore feedback loop dynamics.

## Executive Summary
This paper identifies a critical gap in current AI safety approaches: they focus on fine-tuning model outputs in isolation, ignoring how human responses create dynamic feedback loops that affect safety outcomes. The authors propose a new framework that combines control systems safety theory with generative AI's predictive capabilities. They model human-AI interaction as a closed-loop dynamical game where both agents' internal states evolve over time, and safety is defined as avoiding critical need violations throughout the interaction. The framework introduces a Human-AI Safety Filter that uses a learned safety policy and monitor to detect unsafe candidate actions and minimally modify them to maintain safety while allowing the AI's task policy to function with minimal intervention.

## Method Summary
The framework formulates human-AI interaction as a closed-loop dynamical system where both agents' internal states evolve over time. Safety is defined through failure specification mechanisms that encode critical human needs. The core approach uses a safety-critical human-AI game formulation to compute a maximal safe set, then implements a runtime safety filter that monitors and intervenes when necessary. The filter uses adversarial reach-avoid reinforcement learning to learn safety policies and employs statistical generalization theory to provide safety guarantees under uncertainty about human behavior.

## Key Results
- Identifies feedback loop dynamics as a critical gap in current human-AI safety approaches
- Proposes a framework combining control-theoretic safety with generative AI's context-aware representations
- Introduces a Human-AI Safety Filter that can operate at runtime to ensure safety while allowing task performance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The framework combines AI's generative capabilities with control theory's safety assurances to address the gap in current human-AI safety approaches.
- Mechanism: By modeling human-AI interaction as a closed-loop dynamical game where both agents' internal states evolve over time, the framework enables systematic risk mitigation through interaction-aware reasoning.
- Core assumption: Generative AI models can provide generalizable context-aware representations of human interaction that can be integrated into control-theoretic safety frameworks.
- Evidence anchors:
  - [abstract] The authors propose a new framework combining control systems safety theory with generative AI's predictive capabilities.
  - [section] We see a major opportunity to advance these rigorous safety frameworks to the implicit representations and context-aware models of interactive generative AI systems.
- Break condition: If generative AI models cannot accurately capture and predict human behavior in safety-critical contexts, the integration with control-theoretic safety frameworks will fail.

### Mechanism 2
- Claim: The Human-AI Safety Filter can detect unsafe candidate actions and minimally modify them to maintain safety while allowing the AI's task policy to function with minimal intervention.
- Mechanism: The filter operates at runtime, using a learned safety policy and monitor to detect potential safety violations and intervene only when necessary.
- Core assumption: It is possible to learn a safety policy that can accurately predict and prevent safety violations in human-AI interactions.
- Evidence anchors:
  - [abstract] A key contribution is the Human-AI Safety Filter, which uses a learned safety policy and monitor to detect unsafe candidate actions and minimally modify them to maintain safety.
  - [section] The filter automatically detects candidate actions that could lead to future constraint violations and suitably modifies them to preserve safety.
- Break condition: If the learned safety policy cannot accurately predict all potential safety violations or if the intervention scheme disrupts the AI's task performance too frequently, the filter will not be effective.

### Mechanism 3
- Claim: The proposed technical roadmap provides a concrete path towards next-generation human-centered AI safety systems that can anticipate and avoid potential hazards through interaction-aware reasoning.
- Mechanism: By formulating human-AI interaction as a zero-sum dynamic game and computing a maximal safe set, the framework enables AI systems to reason about and avoid potential future hazards.
- Core assumption: The safety-critical human-AI game formulation can accurately capture the dynamics of human-AI interaction and the potential for safety violations.
- Evidence anchors:
  - [abstract] The framework provides theoretical foundations for next-generation AI systems that can anticipate and avoid potential hazards through interaction-aware reasoning.
  - [section] We cast the computation of Î©* as a zero-sum dynamic game between the AI and a virtual adversary that chooses the worst-case realization of the human's behavior allowed by the AI's uncertainty.
- Break condition: If the game formulation does not accurately capture the complexity of human-AI interaction or if the computational methods for solving the game are not scalable, the roadmap will not lead to practical safety systems.

## Foundational Learning

- Concept: Control systems safety theory
  - Why needed here: Provides rigorous mathematical and algorithmic frameworks for certifying the safety of interactive systems, which is essential for addressing the safety gap in current human-AI approaches.
  - Quick check question: What is the main challenge in control systems safety that the paper's framework aims to address?

- Concept: Generative AI capabilities
  - Why needed here: Offers flexible, highly general representations and context-aware models of interactive human behavior, which are crucial for capturing the complexity of human-AI interaction.
  - Quick check question: How do the learned internal representations of generative AI models contribute to the proposed human-AI systems theory?

- Concept: Dynamic game theory
  - Why needed here: Enables the formulation of human-AI interaction as a multi-agent, dynamic interaction process, allowing for the analysis of feedback loops and their safety outcomes.
  - Quick check question: What role does the virtual adversary play in the safety-critical human-AI game formulation?

## Architecture Onboarding

- Component map: Human-AI dynamical system -> Failure specification mechanisms -> Safety-critical human-AI game -> Human-AI safety filter
- Critical path:
  1. Define the human-AI dynamical system and failure specification mechanisms.
  2. Formulate the safety-critical human-AI game and compute the maximal safe set.
  3. Implement the human-AI safety filter using the learned safety policy and monitor.
  4. Deploy the filter to monitor and intervene in human-AI interactions as needed.

- Design tradeoffs:
  - Conservatism vs. permissiveness: More conservative predictive action bounds lead to stronger safety assurances but may restrict the AI's ability to assist the human.
  - Accuracy vs. scalability: More accurate safety policies and monitors may be computationally expensive and not scale to high-dimensional human-AI systems.
  - Autonomy vs. control: Greater autonomy for the AI's task policy may lead to more effective assistance but also increases the risk of safety violations.

- Failure signatures:
  - Frequent intervention by the safety filter, indicating that the learned safety policy cannot accurately predict safety violations.
  - Inability to maintain safety under certain human behaviors, suggesting that the game formulation does not capture the full complexity of human-AI interaction.
  - Poor task performance due to overly conservative safety measures, indicating a need to balance safety and assistance.

- First 3 experiments:
  1. Implement a simple human-AI dynamical system with a well-defined failure set and compute the maximal safe set using the safety-critical game formulation.
  2. Train a safety policy and monitor using adversarial self-play reinforcement learning on a simulated human-AI interaction task.
  3. Deploy the human-AI safety filter on a real human-AI interaction task and evaluate its ability to maintain safety while allowing effective task performance.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Can generative AI models be effectively used as simulators to generate diverse and realistic human behavior for safety testing of human-AI systems?
- Basis in paper: [explicit] The paper discusses using generative AI as agent simulators for motion prediction, traffic simulation, and simulating human-like conversations in games.
- Why unresolved: The paper highlights this as a synergy between AI and control systems, but notes that obtaining assurances under generalizable and context-aware predictive models of human interaction is still an open problem.
- What evidence would resolve it: Demonstrating that generative AI models can consistently produce realistic and diverse human behavior across various interaction scenarios, and that these simulations can be effectively integrated into control systems safety frameworks.

### Open Question 2
- Question: How can we balance the need for robust safety assurances with the desire to avoid overly conservative behavior in human-AI systems?
- Basis in paper: [explicit] The paper mentions that the safest ODD has traditionally been a rigidly pessimistic one, often yielding overly conservative automation behavior even in nominal interactions.
- Why unresolved: The paper identifies this as an open challenge, noting that obtaining assurances under generalizable and context-aware predictive models of human interaction is still an open problem.
- What evidence would resolve it: Developing methods to adjust the level of pessimism in predictive human models, allowing for more nuanced safety assurances that are robust but not unduly conservative.

### Open Question 3
- Question: How can we develop effective mechanisms for specifying and encoding human needs in AI systems?
- Basis in paper: [explicit] The paper discusses various mechanisms for specifying requirements on AI system operation, including factory rules, common sense, direct feedback, and need reading.
- Why unresolved: The paper identifies this as a key consideration in human-AI systems theory, noting that meaningful human-AI safety requires enabling a diversity of human stakeholders to encode their needs.
- What evidence would resolve it: Demonstrating that AI systems can effectively learn and incorporate diverse human needs, leading to improved safety outcomes and user satisfaction across various interaction scenarios.

## Limitations
- The framework assumes generative AI models can provide generalizable context-aware representations of human behavior, but this has not been empirically validated in safety-critical contexts.
- The safety filter's effectiveness relies on accurately predicting future constraint violations, but the paper does not provide concrete evaluation metrics or thresholds for determining when intervention is needed.
- Limited discussion of practical implementation challenges such as computational complexity, scalability, and handling of long-horizon safety constraints.

## Confidence

**High confidence**: The control-theoretic foundation is well-established, and the identification of feedback loop dynamics is a sound insight.

**Medium confidence**: The integration of generative AI capabilities with control theory is conceptually promising but requires empirical validation.

**Low confidence**: The practical effectiveness of the Human-AI Safety Filter in real-world scenarios remains unproven.

## Next Checks
1. Implement a simulation-based evaluation of the safety filter on a concrete human-AI interaction task to measure intervention rates, safety assurance, and task performance trade-offs.
2. Conduct ablation studies to determine the sensitivity of safety outcomes to different components of the framework, particularly the quality of generative AI representations and the conservatism of the safety monitor.
3. Design a human-subject study to evaluate whether the framework can maintain safety in real-time human-AI interactions while preserving user trust and task effectiveness, measuring both objective safety metrics and subjective user experience.