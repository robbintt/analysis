---
ver: rpa2
title: 'Public-data Assisted Private Stochastic Optimization: Power and Limitations'
arxiv_id: '2403.03856'
source_url: https://arxiv.org/abs/2403.03856
tags:
- npriv
- npub
- public
- data
- spriv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper investigates the power and limitations of public data
  in differentially private (DP) stochastic optimization. The authors establish fundamental
  lower bounds showing that for complete/labeled public data, no rate improvement
  is possible beyond simply discarding private data or treating all data as private.
---

# Public-data Assisted Private Stochastic Optimization: Power and Limitations
## Quick Facts
- arXiv ID: 2403.03856
- Source URL: https://arxiv.org/abs/2403.03.03856
- Reference count: 40
- The paper establishes fundamental limits on using public data for differentially private stochastic optimization, showing that unlabeled public data can achieve dimension-independent rates while labeled data provides no improvement.

## Executive Summary
This paper provides a comprehensive analysis of the power and limitations of using public data to improve differentially private (DP) stochastic optimization. The authors establish fundamental lower bounds showing that for complete/labeled public data, no rate improvement is possible beyond simply discarding private data or treating all data as private. However, they demonstrate that unlabeled public data in supervised learning settings can yield significant improvements, achieving dimension-independent rates through novel algorithmic approaches. The work provides both theoretical guarantees and matching lower bounds, characterizing the optimal achievable performance.

## Method Summary
The authors analyze private-data assisted (PA-DP) algorithms that use both private and public data for stochastic optimization under differential privacy constraints. They establish lower bounds using techniques from information theory and private learning theory, proving that for complete/labeled public data, the excess risk is bounded below by Ω(min{1/√n_pub, 1/√n + √d/(nε)}), showing no improvement over baseline approaches. For unlabeled public data in GLMs, they develop an efficient algorithm using the exponential mechanism and uniform convergence arguments to achieve the rate Õ(1/√n_priv + 1/√(n_priv·ε)). The analysis extends to general hypothesis classes using fat-shattering dimension arguments.

## Key Results
- Lower bounds show no rate improvement for complete/labeled public data beyond discarding private data or treating all data as private
- For GLMs with unlabeled public data, achieves dimension-independent rate Õ(1/√n_priv + 1/√(n_priv·ε)) using Õ(n_priv/ε) unlabeled samples
- Matching lower bounds prove optimality of the unlabeled public data approach
- Extension to general hypothesis classes with finite fat-shattering dimension, applicable to neural networks

## Why This Works (Mechanism)
The key insight is that unlabeled public data can be used to construct appropriate covers for the hypothesis class, enabling dimensionality reduction and improved rates. By leveraging the structure of GLMs and using uniform convergence arguments, the algorithm can identify a small set of candidate hypotheses that likely contain the optimal solution. The exponential mechanism then privately selects from this set, achieving the improved rate. For general hypothesis classes, the fat-shattering dimension provides a measure of complexity that enables similar dimensionality reduction arguments.

## Foundational Learning
- Differential Privacy (ε,δ): A framework ensuring individual data points cannot be easily inferred from the output, needed to protect sensitive information while enabling useful analysis. Quick check: Verify privacy budget is properly allocated across algorithm components.
- Excess Risk: The difference between expected loss of an algorithm and the optimal possible loss, used as the performance metric. Quick check: Ensure excess risk bounds are properly derived and match theoretical expectations.
- Uniform Convergence: A property ensuring that empirical risk concentrates around true risk uniformly over a hypothesis class, needed for generalization guarantees. Quick check: Verify covering numbers and uniform convergence bounds are correctly computed.
- Fat-shattering Dimension: A complexity measure for hypothesis classes that generalizes VC dimension, used to characterize learnability. Quick check: Confirm fat-shattering dimension calculations for specific hypothesis classes.
- Exponential Mechanism: A fundamental DP mechanism that selects outcomes with probability proportional to their utility, weighted by the privacy parameter. Quick check: Validate that the mechanism satisfies (ε,δ)-DP and achieves the claimed utility.

## Architecture Onboarding
The PA-DP algorithm architecture consists of:
Public Data Preprocessing -> Hypothesis Class Covering -> Private Selection -> Final Hypothesis
Critical path: Public data is used to construct a cover of the hypothesis class, which is then used by the exponential mechanism to select a hypothesis privately from the cover. The final hypothesis is output with DP guarantees.
Design tradeoffs: Using more public data for covering enables better dimensionality reduction but requires more samples. The choice of covering strategy (uniform vs adaptive) affects both computational efficiency and statistical performance.
Failure signatures: If public data is contaminated or mislabeled, the covering step may produce a poor cover, leading to suboptimal performance. Insufficient public data may result in covers that are too coarse to enable dimensionality reduction.
First experiments: 1) Verify the covering algorithm produces the claimed covering number on synthetic data. 2) Test the exponential mechanism's selection accuracy with varying privacy parameters. 3) Compare the full PA-DP algorithm's performance against baselines on a simple GLM problem.

## Open Questions the Paper Calls Out
None

## Limitations
- Results assume public data is clean and unlabeled; contamination or partial labeling would invalidate guarantees
- Focus on excess risk as the performance metric ignores potential benefits like convergence speed or robustness
- Extension to complex models like neural networks requires additional empirical validation
- Analysis relies on specific assumptions about hypothesis class structure and properties of the exponential mechanism

## Confidence
- Lower bounds for labeled public data: High
- Upper bounds for unlabeled public data in GLMs: High
- Extension to general hypothesis classes: Medium
- Practical implications for neural networks: Medium

## Next Checks
1. Implement and test the unlabeled public data algorithm on real-world datasets to verify the theoretical rate improvements hold empirically
2. Analyze the sensitivity of results to different levels of public data contamination or partial labeling
3. Compare the proposed approach against alternative methods for leveraging public data in DP learning, such as knowledge distillation or pretraining on public data