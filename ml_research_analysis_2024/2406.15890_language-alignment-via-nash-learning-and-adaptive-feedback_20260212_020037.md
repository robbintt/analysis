---
ver: rpa2
title: Language Alignment via Nash-learning and Adaptive feedback
arxiv_id: '2406.15890'
source_url: https://arxiv.org/abs/2406.15890
tags:
- alignment
- feedback
- language
- preference
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Language Alignment via Nash-learning and
  Adaptive feedback (LANA), a method for aligning large language models without requiring
  human-annotated preference datasets or learned preference models. The approach casts
  alignment as a mirror descent algorithm against the adaptive feedback of an improved
  opponent, effectively creating a self-aligning process.
---

# Language Alignment via Nash-learning and Adaptive feedback

## Quick Facts
- arXiv ID: 2406.15890
- Source URL: https://arxiv.org/abs/2406.15890
- Reference count: 11
- Primary result: 1.66% improvement in AlpacaEval win rate using only 3K prompts

## Executive Summary
This paper introduces Language Alignment via Nash-learning and Adaptive feedback (LANA), a novel method for aligning large language models without requiring human-annotated preference datasets or learned preference models. LANA casts alignment as a mirror descent optimization problem where two identical models compete against each other's outputs, with each model providing adaptive feedback to improve the other. The approach is mathematically derived from game theory and mirror descent optimization, and experimentally validated on a small instruction-tuned model (Phi-3-mini-4k-instruct).

## Method Summary
LANA implements a two-player symmetric game where each player generates responses to prompts and evaluates the other player's responses using a self-evaluation mechanism. The models are updated via stochastic gradient descent on the log probability ratio of preferred responses, with KL divergence regularization to maintain policy stability. The method uses LoRA adapters for efficient parameter updates and operates on a small dataset of 3K prompts, achieving alignment without human preferences or learned reward models.

## Key Results
- 1.66% improvement in AlpacaEval win rate compared to base model
- No degradation in reasoning performance (GSM8K maintained/improved)
- Consistent improvements across task categories: riddles (+5.08%), theory of mind (+4.48%), planning (+3.53%)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: LANA achieves self-alignment without a learned preference model by using adaptive feedback from an improved opponent
- Mechanism: The algorithm casts alignment as a mirror descent optimization where each player's reward is computed based on the log probability ratio between their policy and the improved opponent's policy
- Core assumption: The self-evaluation mechanism provides sufficiently accurate feedback for meaningful policy improvement
- Evidence anchors:
  - [abstract] "LANA is capable of self-alignment without the need for a human-annotated preference dataset"
  - [section] "With the correct choice of proxy reward and slower learning dynamics, the game converges to a better policy"
  - [corpus] Weak evidence - neighboring papers focus on convergence guarantees but don't validate the self-evaluation mechanism specifically

### Mechanism 2
- Claim: The mirror descent formulation with KL divergence regularization enables stable policy updates while maintaining proximity to the previous policy
- Mechanism: By using mirror descent with KL divergence as the Bregman divergence, the algorithm performs gradient updates in the space of policy log-probabilities rather than directly in parameter space
- Core assumption: The KL regularization term effectively controls policy updates to prevent reward hacking
- Evidence anchors:
  - [section] "It defines a sequence of policies... according to the following updates... [with] Dϕ(πi, πi,t)"
  - [section] "we end up with Algorithm 1 which we refer to as LANA"
  - [corpus] Moderate evidence - neighboring papers discuss mirror descent variants but focus more on convergence rates

### Mechanism 3
- Claim: The two-player symmetric game structure enables efficient exploration of the policy space through competition between identical initial models
- Mechanism: By initializing two identical models and having them compete against each other's outputs, the algorithm creates a natural curriculum where each model is exposed to increasingly challenging examples
- Core assumption: The symmetric game structure with identical initial models provides sufficient diversity in the exploration process
- Evidence anchors:
  - [section] "Both players parameterized their base models using LoRA with a rank of 16"
  - [section] "Unlike other Self-rewarding LM, LANA don't face a significant drop in reasoning tasks"
  - [corpus] Weak evidence - while self-play is mentioned in related works, the specific symmetric initialization benefits are not well-documented

## Foundational Learning

- Concept: Mirror descent and Bregman divergences
  - Why needed here: The algorithm's mathematical foundation relies on mirror descent optimization with KL divergence as the Bregman divergence
  - Quick check question: What is the relationship between mirror descent and natural gradient descent, and how does the choice of Bregman divergence affect the optimization dynamics?

- Concept: Game theory and Nash equilibria
  - Why needed here: LANA frames alignment as a two-player zero-sum game where policies converge to a Nash equilibrium
  - Quick check question: How does the Nash equilibrium concept apply to the two-player policy optimization in LANA, and what guarantees does it provide about convergence?

- Concept: Preference modeling and Bradley-Terry models
  - Why needed here: Understanding traditional preference modeling approaches is crucial for appreciating why LANA's self-evaluation mechanism is innovative
  - Quick check question: What are the key limitations of Bradley-Terry preference models that LANA's adaptive feedback mechanism addresses?

## Architecture Onboarding

- Component map: Base model -> LoRA adapters -> Response generator -> Self-evaluation mechanism -> Mirror descent optimizer -> Updated policy

- Critical path:
  1. Sample prompt from distribution
  2. Generate two responses from each player's current policy
  3. Evaluate responses using opponent's self-evaluation mechanism
  4. Compute loss based on log probability ratios
  5. Update policies using SGD with LoRA adapters
  6. Repeat until convergence

- Design tradeoffs:
  - LoRA rank 16 vs full fine-tuning: LoRA provides parameter efficiency but may limit the expressiveness of policy updates
  - Temperature 0.1 for response generation: Low temperature ensures deterministic outputs for evaluation but may reduce diversity
  - 3K prompts vs larger datasets: Small dataset enables rapid experimentation but may limit generalization

- Failure signatures:
  - Win rate plateauing below baseline: Indicates the self-evaluation mechanism may be too noisy or biased
  - Reasoning task performance degradation: Suggests the policy updates are introducing artifacts or reward hacking
  - Training instability or divergence: Could indicate learning rate issues or insufficient KL regularization

- First 3 experiments:
  1. Baseline comparison: Run LANA with Phi-3-mini-4k-instruct vs the same model without LANA training on the 3K prompt dataset
  2. Self-evaluation noise sensitivity: Vary the temperature parameter for response generation (0.1, 0.5, 1.0) and measure impact on convergence
  3. Model size scaling: Test LANA with different base models (Mistral-7B, Gemma-2b) to validate the base model quality hypothesis

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of base model affect the performance of LANA?
- Basis in paper: [explicit] The authors mention that LANA provided no benefit when using Mistral-7B-Instruct-v0.1 as the foundation model
- Why unresolved: The paper only tested two base models, and the exact reasons for the difference in performance are not fully explored
- What evidence would resolve it: Testing LANA with a wider variety of base models, including larger and more diverse models

### Open Question 2
- Question: How does the amount of training data impact the effectiveness of LANA?
- Basis in paper: [explicit] The authors mention that they only had enough compute to train on 3K prompts and suggest that more improvement might be possible with continued training
- Why unresolved: The paper only tested LANA on a relatively small dataset (3K prompts)
- What evidence would resolve it: Conducting experiments with LANA using varying amounts of training data, from small to large datasets

### Open Question 3
- Question: Can the training efficiency of LANA be improved?
- Basis in paper: [explicit] The authors mention that sampling during training leads to highly inefficient training and that tricks such as PagedAttention are not applicable during training
- Why unresolved: The paper does not explore any specific methods for improving LANA's training efficiency
- What evidence would resolve it: Developing and testing methods to improve LANA's training efficiency, such as optimizing the sampling process

## Limitations

- Highly sensitive to base model quality - LANA showed no benefit with Mistral-7B-Instruct-v0.1 but improved with Phi-3-mini-4k-instruct
- Limited experimental validation on only one small model and 3K training prompts
- Training efficiency concerns due to sampling preventing use of optimizations like PagedAttention

## Confidence

- Mathematical framework: High - properly derived from mirror descent theory
- Experimental results: Medium - limited to single small model and modest datasets
- Generalizability claims: Low - insufficient testing across different model sizes and domains

## Next Checks

1. Test LANA with progressively worse base models to quantify the sensitivity to initial model quality and identify failure thresholds
2. Scale up to larger models (Mistral-7B, Llama-3-8B) and measure whether the 1.66% AlpacaEval improvement scales proportionally
3. Conduct a comprehensive ablation study on the mirror descent hyperparameters (learning rate, KL regularization strength) to understand their impact on convergence and final performance