---
ver: rpa2
title: 'Open RL Benchmark: Comprehensive Tracked Experiments for Reinforcement Learning'
arxiv_id: '2402.03046'
source_url: https://arxiv.org/abs/2402.03046
tags:
- learning
- benchmark
- open
- data
- research
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Open RL Benchmark, a community-driven repository
  containing over 25,000 tracked reinforcement learning experiments spanning 8+ years
  of cumulative training. The benchmark addresses reproducibility challenges by providing
  complete experimental data including raw learning curves, algorithm-specific metrics,
  system measurements, and precise dependency versions for exact reproduction.
---

# Open RL Benchmark: Comprehensive Tracked Experiments for Reinforcement Learning

## Quick Facts
- arXiv ID: 2402.03046
- Source URL: https://arxiv.org/abs/2402.03046
- Reference count: 40
- This paper introduces Open RL Benchmark, a community-driven repository containing over 25,000 tracked reinforcement learning experiments spanning 8+ years of cumulative training.

## Executive Summary
This paper introduces Open RL Benchmark, a community-driven repository containing over 25,000 tracked reinforcement learning experiments spanning 8+ years of cumulative training. The benchmark addresses reproducibility challenges by providing complete experimental data including raw learning curves, algorithm-specific metrics, system measurements, and precise dependency versions for exact reproduction. A command-line interface enables easy data access and figure generation. The authors demonstrate practical utility through two case studies: comparing PPO variants with different value estimation methods, and showcasing Cleanba's distributed RL implementation advantages. The work represents a significant step toward improving reproducibility and comparison standards in RL research.

## Method Summary
Open RL Benchmark provides a comprehensive tracking system for reinforcement learning experiments, storing raw learning curves, algorithm-specific metrics, system measurements, and precise dependency versions. The benchmark uses Weights and Biases as its backend for data storage and tracking, with a CLI tool enabling easy data access and visualization. Multiple RL library integrations are supported, including CleanRL, Stable Baselines3, and others. The system tracks training-related metrics, method-specific metrics, configuration details, and system metrics, with experiments contributed by the community and linked to their respective publications.

## Key Results
- Open RL Benchmark contains over 25,000 tracked experiments spanning 8+ years of cumulative training
- The CLI enables single-command generation of metrics-related figures for RL research
- Case study comparing PPO with TD(λ) versus Monte Carlo value estimation shows environment-dependent performance differences
- Cleanba implementation demonstrates reduced variability across different hardware configurations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Providing complete experimental data including raw learning curves, algorithm-specific metrics, system measurements, and precise dependency versions enables exact reproduction of RL experiments.
- Mechanism: By tracking all metrics consistently linked with global step and absolute time, and providing complete configuration with frozen dependencies and exact commands, researchers can reproduce experiments without implementation guesswork.
- Core assumption: Complete tracking of all metrics and dependencies is sufficient to reproduce experimental conditions.
- Evidence anchors:
  - [abstract] "Complete experimental data including raw learning curves, algorithm-specific metrics, system measurements, and precise dependency versions for exact reproduction"
  - [section 2.2] "Each experiment includes a complete configuration with all hyperparameters, frozen versions of dependencies, and the exact command, including the necessary random seed"
- Break condition: If any tracked metric or dependency is missing or corrupted, exact reproduction becomes impossible.

### Mechanism 2
- Claim: The CLI enables easy data access and figure generation, lowering barriers to analysis and comparison.
- Mechanism: Single-command generation of metrics-related figures, integration with RLiable for statistical analysis, and filtering capabilities allow researchers to quickly extract and visualize relevant results.
- Core assumption: The CLI interface correctly interprets user parameters and reliably fetches the appropriate data from the benchmark.
- Evidence anchors:
  - [abstract] "A command-line interface enables easy data access and figure generation"
  - [section 2.3] "The CLI is a powerful tool for generating most metrics-related figures for RL research and notably, all figures in this document were generated using the CLI"
- Break condition: If the CLI fails to correctly filter or process data, or if the underlying data structure changes, figure generation will break.

### Mechanism 3
- Claim: Community-driven contributions enhance result reliability through diverse runs and conditions.
- Mechanism: Multiple researchers contributing runs under different conditions and with different implementations increases statistical power and reduces bias from single-implementation results.
- Core assumption: Diverse contributors will provide sufficiently varied and high-quality data to meaningfully improve reliability.
- Evidence anchors:
  - [abstract] "Open RL Benchmark is community-driven: anyone can download, use, and contribute to the data"
  - [section 4.2] "RLLib maintains an intermediate stance in data sharing, hosting run data in a dedicated repository. However, this data is specific to select experiments and often presented in non-standard, undocumented formats"
- Break condition: If community contributions are limited in scope or quality, or if contributions become inconsistent in format, reliability benefits diminish.

## Foundational Learning

- Concept: Reinforcement Learning basics (policies, value functions, exploration vs exploitation)
  - Why needed here: Understanding RL fundamentals is essential to interpret the tracked metrics and experimental results in the benchmark
  - Quick check question: What is the difference between on-policy and off-policy RL algorithms?

- Concept: Experiment tracking and reproducibility practices
  - Why needed here: The benchmark's value depends on understanding how to properly track experiments and ensure reproducibility
  - Quick check question: What information is essential to include in an experiment configuration to ensure reproducibility?

- Concept: Statistical analysis of learning curves
  - Why needed here: Interpreting aggregated results and confidence intervals requires understanding statistical concepts
  - Quick check question: What is the difference between mean, median, and IQM (Interquartile Mean) for aggregating performance?

## Architecture Onboarding

- Component map: Weights and Biases backend -> CLI tool -> Multiple RL library integrations (CleanRL, Stable Baselines3, etc.) -> Metric tracking system (training-related, method-specific, configuration, system metrics) -> Community contribution system

- Critical path: 1. User specifies filters (library, algorithm, environment, metric) 2. CLI queries Weights and Biases API for matching runs 3. Data is fetched and processed according to user specifications 4. Results are aggregated and visualized 5. Output is generated in requested format

- Design tradeoffs:
  - Comprehensive metric tracking vs. storage overhead
  - Community-driven contributions vs. quality control
  - CLI convenience vs. flexibility for custom analysis
  - Fixed dependency versions vs. ability to run on newer systems

- Failure signatures:
  - Missing or incomplete runs indicate tracking failures
  - Inconsistent metric naming across libraries suggests integration issues
  - CLI errors when filtering suggest API or query problems
  - Reproducibility failures indicate missing dependencies or configuration details

- First 3 experiments:
  1. Compare PPO implementations from two different libraries on a single Atari environment
  2. Generate sample efficiency curves for SAC on MuJoCo environments using RLiable
  3. Create multi-metric plots for MORL algorithms on deterministic environments

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the variability in system metrics (GPU memory usage, CPU usage, etc.) across different RL implementations affect algorithm performance and reproducibility?
- Basis in paper: [explicit] The paper tracks system metrics alongside algorithm-specific metrics and mentions Cleanba's claim about reducing variability across hardware configurations.
- Why unresolved: The paper demonstrates Cleanba's reduced variability but doesn't provide a systematic analysis of how system metrics correlate with algorithm performance across different implementations.
- What evidence would resolve it: A comprehensive study analyzing system metric distributions across multiple implementations and their correlation with performance metrics like sample efficiency and final returns.

### Open Question 2
- Question: What is the optimal balance between TD(λ) and Monte Carlo estimation for value functions in PPO across different environment types and complexity levels?
- Basis in paper: [explicit] The paper presents a case study comparing PPO with TD(λ) versus Monte Carlo for value estimation, showing environment-dependent performance differences.
- Why unresolved: The study only compares two extreme cases (pure TD(λ) vs pure MC) without exploring intermediate λ values or analyzing the relationship between environment complexity and optimal estimation method.

## Limitations

- The effectiveness of the benchmark depends heavily on community adoption and consistent contribution practices
- While the tracking system captures extensive metrics, the paper does not validate whether this comprehensive data collection actually enables successful reproduction in practice
- The claim that "exact reproduction" is possible assumes that all necessary information is captured, but this has not been empirically verified through third-party reproduction attempts

## Confidence

- **High confidence**: The technical implementation of the CLI tool and its integration with Weights and Biases for data access
- **Medium confidence**: The claim that comprehensive metric tracking enables exact reproduction (based on described methodology but lacking empirical validation)
- **Medium confidence**: The practical utility demonstrated through case studies (limited to two examples without broader validation)

## Next Checks

1. Conduct a third-party reproduction attempt using the benchmark data to verify whether exact reproduction is achievable as claimed
2. Perform a systematic analysis of contribution consistency across different community members to assess reliability benefits
3. Compare the benchmark's tracking completeness against established experiment tracking standards to identify potential gaps in coverage