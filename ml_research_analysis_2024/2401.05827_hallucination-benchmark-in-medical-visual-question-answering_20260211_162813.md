---
ver: rpa2
title: Hallucination Benchmark in Medical Visual Question Answering
arxiv_id: '2401.05827'
source_url: https://arxiv.org/abs/2401.05827
tags:
- llav
- question
- option
- answer
- medical
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The study addresses the challenge of hallucination in medical
  visual question answering (Med-VQA), where models may produce factually incorrect
  responses. To tackle this, the authors created a hallucination benchmark dataset
  by modifying three publicly available VQA datasets (PMC-VQA, PathVQA, and VQA-RAD)
  with three scenarios: FAKE questions, None of the Above (NOTA), and Image SWAP.'
---

# Hallucination Benchmark in Medical Visual Question Answering

## Quick Facts
- arXiv ID: 2401.05827
- Source URL: https://arxiv.org/abs/2401.05827
- Authors: Jinge Wu; Yunsoo Kim; Honghan Wu
- Reference count: 4
- One-line primary result: LLaVA-v1.5-13B with L+D0 prompting achieves best hallucination detection in medical VQA

## Executive Summary
This study addresses the critical challenge of hallucination in medical visual question answering (Med-VQA), where models may produce factually incorrect responses. The authors created a hallucination benchmark dataset by modifying three publicly available VQA datasets (PMC-VQA, PathVQA, and VQA-RAD) with three challenging scenarios: FAKE questions, None of the Above (NOTA), and Image SWAP. Through comprehensive evaluation of state-of-the-art models including LLaVA variants and GPT-4-turbo-vision, the study identifies LLaVA-v1.5-13B with the L+D0 prompting strategy as the best performer, achieving 55.44% average accuracy with no irrelevant predictions. The research provides important insights into current models' capabilities and limitations in medical settings and highlights the importance of proper prompting strategies for hallucination mitigation.

## Method Summary
The authors created a hallucination benchmark dataset by modifying three existing VQA datasets with three scenarios: FAKE questions (semantically incoherent questions), NOTA (no correct answer among options), and Image SWAP (mismatched image-question pairs). They evaluated state-of-the-art models including LLaVA variants and GPT-4-turbo-vision using various prompting strategies, with an ablation study identifying L+D0 as the optimal approach. The evaluation measured classification accuracy and counted irrelevant predictions to assess hallucination rates. The methodology involved systematic testing across different model configurations, prompting strategies, and hallucination scenarios to provide comprehensive insights into current Med-VQA capabilities.

## Key Results
- LLaVA-v1.5-13B with L+D0 prompting achieved 55.44% average accuracy with zero irrelevant predictions
- GPT-4-turbo-vision outperformed LLaVA-v1.5-13B on average accuracy but produced 15 irrelevant predictions
- Fine-tuning on domain-specific data did not guarantee better hallucination resistance (LLaVA-Med performed worse than LLaVA-v0-7B)
- NOTA scenario proved most challenging for all models, indicating difficulty in recognizing when no correct answer exists

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The L+D0 prompt strategy reduces hallucination by explicitly instructing the model not to share false information when uncertain.
- Mechanism: The D0 component creates a decision boundary where the model is encouraged to abstain rather than fabricate answers.
- Core assumption: Models can reliably detect their own uncertainty and will follow the explicit instruction to refrain from answering when unsure.
- Evidence anchors: L+D0 prompt description and LLaVA-v1.5-13B achieving 55.44% accuracy with no irrelevant predictions.

### Mechanism 2
- Claim: Fine-tuning on domain-specific data does not guarantee better hallucination resistance in medical VQA.
- Mechanism: Domain-specific fine-tuning may increase model confidence in hallucinated responses, reducing likelihood of abstaining when uncertain.
- Core assumption: Generalist models with proper prompting strategies can outperform specialized models in hallucination detection.
- Evidence anchors: LLaVA-Med performs worse than LLaVA-v0-7B in hallucination evaluation.

### Mechanism 3
- Claim: FAKE and SWAP scenarios are more challenging than NOTA, requiring better understanding of semantic coherence.
- Mechanism: These scenarios test model's ability to detect semantic mismatches between question content and visual information.
- Core assumption: Models can leverage visual understanding to identify when questions are semantically inconsistent with image content.
- Evidence anchors: LLaVA-v1.5-13B performs better in FAKE and SWAP scenarios relative to GPT-4-turbo-vision.

## Foundational Learning

- Concept: Vision-Language Model Architecture
  - Why needed here: Understanding how LLaVA and similar models process and integrate visual and textual information is crucial for diagnosing hallucination mechanisms.
  - Quick check question: What are the key architectural differences between LLaVA-v0 and LLaVA-v1.5 that might explain performance differences in hallucination detection?

- Concept: Prompt Engineering Strategies
  - Why needed here: The study demonstrates that different prompting strategies significantly impact hallucination detection performance.
  - Quick check question: How does the L+D0 prompt differ structurally from other prompt strategies, and why might this difference lead to better hallucination resistance?

- Concept: Medical Domain Knowledge
  - Why needed here: Understanding medical context is essential for evaluating whether model responses are hallucinatory or accurate.
  - Quick check question: What specific medical knowledge would be required to distinguish between a genuine medical question and a "fake patient description" scenario?

## Architecture Onboarding

- Component map: Dataset creation module -> Model selection and configuration -> Prompt strategy implementation -> Evaluation execution -> Result analysis
- Critical path: Dataset creation → Model selection and configuration → Prompt strategy implementation → Evaluation execution → Result analysis. The most critical path element is the prompt strategy selection.
- Design tradeoffs: Multiple-choice questions for easier evaluation limit question types; using existing datasets provides domain relevance but may limit scenario diversity; temperature=0 and output length=1 focus on model capabilities but may not reflect real-world usage.
- Failure signatures: High rates of irrelevant predictions indicate hallucination; low accuracy on FAKE questions suggests inability to detect semantically incoherent questions; poor performance on SWAP scenarios indicates failure to align image content with question semantics.
- First 3 experiments:
  1. Replicate the ablation study with LLaVA-v1.5-13B using the seven prompt strategies to verify L+D0 superiority claim.
  2. Test the same models on a subset of the benchmark with temperature=1 to assess whether stochasticity affects hallucination rates.
  3. Implement a variant of the L+D0 prompt that adds a confidence threshold requirement to test if explicit uncertainty thresholds further reduce hallucination.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: Does fine-tuning on domain-specific data improve hallucination evaluation performance for Med-VQA models?
- Basis in paper: The authors found that fine-tuning in domain-specific data does not guarantee a performance boost in hallucination evaluation, as LLaVA-Med performs worse than LLaVA-v0-7B.
- Why unresolved: The study only compared a few models and their variants. More extensive comparisons across different fine-tuning approaches and datasets are needed to draw definitive conclusions.
- What evidence would resolve it: Conduct a comprehensive study comparing the hallucination evaluation performance of various Med-VQA models with different fine-tuning strategies.

### Open Question 2
- Question: How do different prompting strategies affect the hallucination evaluation performance of Med-VQA models?
- Basis in paper: The authors conducted an ablation study with various prompt styles and found that the L+D0 prompt was the best strategy for hallucination evaluation.
- Why unresolved: The study only explored a limited set of prompt styles. Further research is needed to investigate the impact of a wider range of prompt variations on hallucination evaluation.
- What evidence would resolve it: Perform a comprehensive ablation study with a diverse set of prompt styles and variations.

### Open Question 3
- Question: How do different hallucination scenarios (FAKE, NOTA, Image SWAP) impact the performance of Med-VQA models?
- Basis in paper: The authors created a benchmark dataset with three hallucination scenarios and evaluated the performance of Med-VQA models on each scenario.
- Why unresolved: The study only focused on three specific hallucination scenarios. Further research is needed to explore the impact of additional hallucination scenarios on model performance.
- What evidence would resolve it: Develop and evaluate Med-VQA models on a wider range of hallucination scenarios.

## Limitations
- The benchmark dataset is relatively small (approximately 1,800 questions), which may limit statistical power for detecting subtle performance differences.
- Evaluation focuses primarily on multiple-choice questions, which may not capture the full complexity of real-world medical VQA tasks with open-ended responses.
- The study does not investigate temporal stability of model performance or how results might change with different random seeds.

## Confidence
- **High Confidence**: LLaVA-v1.5-13B with L+D0 prompting achieves best hallucination detection performance among evaluated models, supported by clear distinction in irrelevant prediction rates.
- **Medium Confidence**: Domain-specific fine-tuning does not guarantee better hallucination resistance, based on comparison between LLaVA-v0-7B and LLaVA-Med.
- **Medium Confidence**: GPT-4-turbo-vision outperforms LLaVA-v1.5-13B on average accuracy but produces more irrelevant predictions, well-documented though clinical significance requires further investigation.

## Next Checks
1. **Cross-domain generalization test**: Evaluate the L+D0 prompt strategy on non-medical VQA datasets (e.g., VQA-v2, GQA) to determine whether hallucination detection benefits generalize beyond the medical domain.
2. **Open-ended response evaluation**: Modify the benchmark to include open-ended questions and implement a systematic evaluation framework for measuring hallucination in free-text responses.
3. **Confidence calibration analysis**: Conduct a detailed analysis of the relationship between model confidence scores and hallucination rates across different prompting strategies to determine whether explicit uncertainty thresholds could further improve performance.