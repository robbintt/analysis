---
ver: rpa2
title: 'Zero-shot prompt-based classification: topic labeling in times of foundation
  models in German Tweets'
arxiv_id: '2406.18239'
source_url: https://arxiv.org/abs/2406.18239
tags:
- prompt
- task
- language
- data
- classification
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study evaluates the use of zero-shot prompt-based classification
  for annotating German Twitter data on European crises. The approach leverages large
  language models (LLMs) to classify tweets based on written guidelines without requiring
  labeled training data.
---

# Zero-shot prompt-based classification: topic labeling in times of foundation models in German Tweets
## Quick Facts
- arXiv ID: 2406.18239
- Source URL: https://arxiv.org/abs/2406.18239
- Reference count: 15
- Primary result: Zero-shot prompt-based classification achieves 86% F1 score, comparable to fine-tuned BERT models

## Executive Summary
This study evaluates zero-shot prompt-based classification for annotating German Twitter data on European crises. The approach uses large language models to classify tweets based on written guidelines without requiring labeled training data. Results show prompt-based classification performs comparably to fine-tuned BERT models (86% F1) and significantly outperforms a Naive Bayes baseline (66% F1). The study finds that performance improves with more detailed prompts, but smaller models struggle with multilingual tasks. Challenges include LLM hallucinations and inconsistent outputs, highlighting both the potential and limitations of this scalable annotation approach.

## Method Summary
The study employs zero-shot prompt-based classification to label German Twitter data about European crises. The method uses large language models with written guidelines to classify tweets without labeled training data. Performance is evaluated against fine-tuned BERT models and a Naive Bayes baseline using F1 score as the primary metric. The experimental design systematically varies prompt detail levels to assess their impact on classification performance.

## Key Results
- Zero-shot prompt-based classification achieves 86% F1 score, matching fine-tuned BERT performance
- Outperforms Naive Bayes baseline by 20 percentage points (86% vs 66% F1)
- Performance improves with more detailed prompts
- Smaller models struggle with multilingual classification tasks

## Why This Works (Mechanism)
The approach leverages the emergent reasoning capabilities of large language models to perform classification without task-specific training. By framing classification as a natural language understanding task through prompts, the models can generalize from their pre-training on diverse text corpora. The written guidelines provide sufficient context for the models to infer the classification schema, while the zero-shot nature eliminates the need for resource-intensive annotation processes.

## Foundational Learning
- **Zero-shot learning**: Classification without task-specific training data; needed because traditional annotation is expensive and time-consuming; quick check: verify no labeled examples are used in the prompt
- **Prompt engineering**: Crafting effective instructions for LLMs; needed to guide model behavior and improve performance; quick check: test multiple prompt formulations systematically
- **Foundation models**: Large pre-trained models adaptable to various tasks; needed for their broad knowledge and reasoning capabilities; quick check: confirm model size and pre-training corpus
- **F1 score**: Harmonic mean of precision and recall; needed for balanced evaluation of classification performance; quick check: calculate both precision and recall components
- **Multilingual classification**: Handling multiple languages in a single model; needed for real-world social media data diversity; quick check: test with mixed-language input examples

## Architecture Onboarding
Component map: Written guidelines -> Prompt formulation -> LLM inference -> Classification output
Critical path: Prompt creation → Model inference → Output validation → Performance evaluation
Design tradeoffs: Prompt detail vs. model size vs. classification accuracy
Failure signatures: Hallucinations, inconsistent outputs, multilingual performance degradation
First experiments:
1. Compare baseline performance across different prompt detail levels
2. Test classification accuracy on single vs. mixed language inputs
3. Evaluate hallucination frequency across different model families

## Open Questions the Paper Calls Out
None

## Limitations
- Results limited to single domain (European crises on Twitter) and language (German)
- Limited model diversity tested for multilingual performance claims
- Hallucination issues lack systematic quantification across model families
- Evaluation timeframe may not reflect current model capabilities

## Confidence
High for BERT comparison results, Medium for multilingual performance claims, Low for hallucination characterization

## Next Checks
1. Replicate experiments across additional crisis domains and languages
2. Conduct systematic testing of hallucination rates across multiple LLM families
3. Evaluate performance decay over time with updated model versions