---
ver: rpa2
title: 'DoRA: Weight-Decomposed Low-Rank Adaptation'
arxiv_id: '2402.09353'
source_url: https://arxiv.org/abs/2402.09353
tags: []
core_contribution: This work introduces a novel weight decomposition analysis that
  reparameterizes model weights into magnitude and directional components to investigate
  the inherent differences between full fine-tuning (FT) and LoRA. Based on the findings,
  a new parameter-efficient fine-tuning (PEFT) method called DoRA is proposed.
---

# DoRA: Weight-Decomposed Low-Rank Adaptation

## Quick Facts
- arXiv ID: 2402.09353
- Source URL: https://arxiv.org/abs/2402.09353
- Reference count: 40
- Key result: Proposes DoRA method that outperforms LoRA on multiple benchmarks while maintaining parameter efficiency

## Executive Summary
DoRA introduces a novel weight decomposition analysis that separates model weights into magnitude and directional components to better understand differences between full fine-tuning and LoRA. Based on this analysis, DoRA is proposed as a parameter-efficient fine-tuning method that decomposes pre-trained weights and fine-tunes both magnitude and direction while using LoRA specifically for directional updates. This approach enhances LoRA's learning capacity and training stability while maintaining inference efficiency.

## Method Summary
DoRA reparameterizes model weights into magnitude and directional components, then fine-tunes both components while using LoRA specifically for directional updates. The method decomposes pre-trained weights and optimizes both the magnitude and direction components during training, with LoRA applied only to the directional component to minimize trainable parameters. This approach aims to enhance LoRA's learning capacity and training stability while maintaining the efficiency benefits of parameter-efficient fine-tuning.

## Key Results
- Outperforms LoRA by +3.4/+1.0 on commonsense reasoning tasks
- Improves visual instruction tuning by +0.6
- Achieves gains of +0.9/+1.9 on image/video-text understanding tasks
- Demonstrates consistent improvements across various tasks and model backbones including LLaMA, LLaVA, and VL-BART

## Why This Works (Mechanism)
DoRA works by decomposing weights into magnitude and directional components, allowing for more effective learning by separating these two aspects of weight representation. By fine-tuning both components while using LoRA specifically for directional updates, the method captures richer information than standard LoRA while maintaining parameter efficiency. This decomposition enables better optimization of the learning process by addressing both the scale (magnitude) and orientation (direction) of weight updates separately.

## Foundational Learning

**Low-Rank Adaptation (LoRA)**: A parameter-efficient fine-tuning method that freezes pre-trained weights and injects trainable low-rank matrices into attention layers. Needed to understand the baseline method being improved. Quick check: Verify understanding of how LoRA modifies attention weights through low-rank decomposition.

**Weight Decomposition**: The mathematical separation of weight matrices into distinct components (magnitude and direction in DoRA). Needed to grasp the core innovation of DoRA. Quick check: Confirm understanding of how weight matrices can be factored into scale and orientation components.

**Parameter-Efficient Fine-Tuning**: Methods that reduce the number of trainable parameters during adaptation while maintaining or improving performance. Needed to contextualize DoRA within the broader PEFT landscape. Quick check: Understand the trade-offs between parameter efficiency and model performance.

## Architecture Onboarding

**Component Map**: Pre-trained weights -> Weight Decomposition (magnitude + direction) -> Magnitude Fine-tuning + LoRA Directional Updates -> Output Weights

**Critical Path**: The decomposition and subsequent fine-tuning of both magnitude and directional components represents the critical path, as both components must be properly learned for optimal performance.

**Design Tradeoffs**: DoRA trades increased training complexity for improved learning capacity compared to standard LoRA. The decomposition adds computational overhead during training but maintains inference efficiency.

**Failure Signatures**: Poor decomposition quality could lead to suboptimal fine-tuning results. If the magnitude component is not properly learned, the model may fail to scale features appropriately.

**First Experiments**: 1) Compare DoRA vs LoRA performance on a single task to verify baseline improvement claims, 2) Test weight decomposition quality by examining learned magnitude and direction components, 3) Evaluate training stability by monitoring loss curves during fine-tuning.

## Open Questions the Paper Calls Out
None identified in the provided information.

## Limitations
- The general applicability of the weight decomposition across different model architectures is not fully established
- Computational overhead during training is not extensively explored
- Long-term stability and generalization across diverse datasets are not thoroughly examined
- The paper lacks detailed ablation studies on the directional component's specific contribution

## Confidence
- Technical novelty: High
- Performance consistency: Medium
- Generalizability: Medium
- Training stability analysis: Low

## Next Checks
1. Conduct ablation studies to isolate the impact of directional component updates versus magnitude updates
2. Test DoRA's effectiveness on larger-scale models and more diverse task distributions
3. Evaluate training stability and convergence speed compared to LoRA across different learning rate schedules