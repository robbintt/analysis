---
ver: rpa2
title: 'Machine Translation in the Covid domain: an English-Irish case study for LoResMT
  2021'
arxiv_id: '2403.01196'
source_url: https://arxiv.org/abs/2403.01196
tags:
- translation
- covid
- dataset
- transformer
- were
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study developed English-to-Irish translation models for the
  Covid domain as part of the LoResMT 2021 shared task. The approach combined domain
  adaptation techniques using a 55k-line generic corpus from the Directorate General
  of Translation with fine-tuning, mixed fine-tuning, and combined dataset approaches.
---

# Machine Translation in the Covid domain: an English-Irish case study for LoResMT 2021

## Quick Facts
- arXiv ID: 2403.01196
- Source URL: https://arxiv.org/abs/2403.01196
- Reference count: 4
- Key outcome: Extending an 8k in-domain baseline dataset by 5k lines improved BLEU score by 27 points

## Executive Summary
This study presents English-to-Irish translation models for the Covid domain as part of the LoResMT 2021 shared task. The authors explored domain adaptation techniques including fine-tuning, mixed fine-tuning, and combined dataset approaches using a 55k-line generic corpus from the Directorate General of Translation alongside an 8k-line Covid baseline dataset. The highest-performing model used a Transformer architecture trained on an extended 13k-line in-domain Covid dataset, achieving a BLEU score of 37.1. The research demonstrates that in low-resource settings, extending in-domain datasets significantly outperforms domain adaptation through fine-tuning with mixed domain data.

## Method Summary
The study employed Transformer architecture with optimized hyperparameters for low-resource English-Irish translation. Models were trained using various dataset compositions: a 52k-line general corpus from DGT, an 8k-line Covid baseline from MT Summit 2021, and a 5k-line Covid dataset from DCU. The authors experimented with fine-tuning, mixed fine-tuning, combined datasets, and extended in-domain approaches. A 16k BPE SentencePiece subword model was used for tokenization. Key hyperparameter optimizations included reducing hidden layer neurons to 256, increasing dropout to 0.3, using 2 attention heads, and training with batch size 2048.

## Key Results
- Extended 13k-line in-domain Covid dataset achieved 37.1 BLEU, outperforming the 8k baseline by 27 points
- Out-of-domain 52k DGT dataset achieved only 13.9 BLEU, demonstrating domain mismatch penalty
- Mixed fine-tuning (8k in-domain + 52k out-of-domain) achieved 22.9 BLEU, showing domain adaptation limitations

## Why This Works (Mechanism)

### Mechanism 1
- Claim: In-domain data augmentation significantly outperforms domain adaptation via fine-tuning when the in-domain dataset is small.
- Mechanism: The Transformer model learns specialized domain features more effectively from in-domain data than from mixing in-domain and out-of-domain data, even when the latter is much larger.
- Core assumption: The in-domain data captures domain-specific terminology and phrasing that cannot be effectively recovered through fine-tuning on mixed data.
- Evidence anchors:
  - [abstract]: "extending an 8k in-domain baseline dataset by just 5k lines improved the BLEU score by 27 points"
  - [section]: "An in-domain dataset of 13k lines (Covid extended), trained for just 35k steps outperformed by 22.1 BLEU points the corresponding out-of-domain 52k dataset"
- Break condition: If the in-domain data quality is poor or if the domain-specific terminology is not sufficiently represented in the augmented dataset.

### Mechanism 2
- Claim: Transformer architecture with reduced hidden layer neurons and increased dropout regularization performs better in low-resource settings.
- Mechanism: Smaller model capacity with stronger regularization prevents overfitting on limited training data while maintaining enough representational power for domain-specific translation.
- Core assumption: The reduced model complexity is sufficient to capture the essential translation patterns in low-resource domains.
- Evidence anchors:
  - [section]: "Reducing the number of hidden layer neurons and increasing dropout led to significantly better performance"
  - [abstract]: "careful selection of Transformer hyperparameters, and using a 16k BPE SentencePiece submodel, enabled rapid development of high performing translation models"
- Break condition: If the dataset size increases substantially or if the domain complexity requires more model capacity.

### Mechanism 3
- Claim: SentencePiece subword modeling with a 16k vocabulary size is optimal for low-resource English-Irish translation.
- Mechanism: The subword segmentation balances vocabulary coverage with generalization, preventing data sparsity issues in a low-resource language pair.
- Core assumption: The 16k vocabulary size provides an optimal trade-off between granularity and generalization for the English-Irish language pair.
- Evidence anchors:
  - [section]: "using a 16k BPE submodel resulted in the highest performing models"
  - [abstract]: "using a 16k BPE SentencePiece submodel, enabled rapid development of high performing translation models"
- Break condition: If the language pair characteristics change significantly or if the dataset size increases dramatically.

## Foundational Learning

- Concept: Domain adaptation in neural machine translation
  - Why needed here: Understanding why fine-tuning doesn't always improve performance when in-domain data is limited
  - Quick check question: Why might training on pure in-domain data outperform fine-tuning with mixed domain data when the in-domain dataset is small?

- Concept: Transformer architecture components and their roles
  - Why needed here: Understanding how attention mechanisms and layer configurations affect low-resource translation performance
  - Quick check question: How does reducing the number of hidden layer neurons help prevent overfitting in low-resource scenarios?

- Concept: Subword tokenization strategies (BPE vs unigram)
  - Why needed here: Understanding how vocabulary size and tokenization approach impact translation quality in low-resource settings
  - Quick check question: What are the trade-offs between using a smaller vs larger vocabulary size in SentencePiece models for low-resource languages?

## Architecture Onboarding

- Component map: English source text -> 6-layer Transformer encoder (256 embedding, 2 attention heads) -> SentencePiece with 16k BPE vocabulary -> 6-layer Transformer decoder (256 embedding, 2 attention heads) -> Irish target text

- Critical path:
  1. Data preprocessing and tokenization using SentencePiece
  2. Model initialization with optimized hyperparameters
  3. Training on in-domain dataset
  4. Early stopping based on validation accuracy
  5. Evaluation using BLEU, TER, and ChrF metrics

- Design tradeoffs:
  - Model size vs. performance: Smaller models with increased regularization work better for low-resource settings
  - Vocabulary size: 16k balances coverage and generalization
  - Attention heads: 2 heads perform similarly to 8 heads in this low-resource context

- Failure signatures:
  - BLEU score plateaus below 10: Likely overfitting or insufficient in-domain data
  - Large gap between training and validation scores: Overfitting, increase dropout or reduce model complexity
  - TER score increases during training: Model is learning to copy rather than translate

- First 3 experiments:
  1. Train baseline model on 8k in-domain dataset with default hyperparameters
  2. Train extended model on 13k in-domain dataset with optimized hyperparameters
  3. Train fine-tuned model on combined 65k dataset (8k in-domain + 52k out-of-domain) with optimized hyperparameters

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How would domain adaptation techniques perform if applied to the English-Irish translation models for Covid data using larger in-domain datasets (e.g., 20k+ lines)?
- Basis in paper: [explicit] The authors demonstrated that extending an 8k in-domain baseline dataset by 5k lines improved BLEU score by 27 points, suggesting further improvements might be possible with larger datasets.
- Why unresolved: The study only tested with 13k lines of in-domain data. The authors mention plans to develop models using a larger health domain dataset in future work, but did not test this approach.
- What evidence would resolve it: Experimental results comparing Transformer models trained on varying sizes of in-domain Covid datasets (e.g., 13k, 20k, 30k lines) using the same architecture and evaluation metrics.

### Open Question 2
- Question: Would using separate source and target subword models (instead of concatenated shared vocabulary) improve translation quality for low-resource English-Irish Covid domain translation?
- Basis in paper: [inferred] The authors used a shared vocabulary and SentencePiece subword model for all experiments but noted that "The impact of using separate source and target subword models was not explored."
- Why unresolved: The study did not investigate the potential benefits of using separate subword models for source and target languages, which is a common practice in high-resource language pairs.
- What evidence would resolve it: Comparative experiments training identical models with shared vocabulary vs. separate source/target subword models, measuring differences in BLEU, TER, and ChrF3 scores.

### Open Question 3
- Question: How would the translation quality change if the Transformer models were trained with curriculum learning or other data ordering strategies for the Covid domain?
- Basis in paper: [inferred] The authors experimented with different dataset compositions (baseline, extended, fine-tuned, etc.) but did not explore how the order of training data presentation affects model performance.
- Why unresolved: The study used fixed dataset combinations and training orders without investigating adaptive data ordering strategies that might benefit low-resource domain adaptation.
- What evidence would resolve it: Experiments comparing standard training order vs. curriculum learning approaches, measuring performance differences across the three evaluation metrics.

## Limitations

- Results are specific to the Covid domain and may not generalize to other specialized domains or language pairs
- Lack of comprehensive ablation study makes it difficult to isolate the exact contribution of each optimization component
- Insufficient evidence provided to support the 16k BPE subword model size as optimal for other low-resource language pairs

## Confidence

- High Confidence: The dataset extension mechanism (8k to 13k lines improving BLEU by 27 points)
- Medium Confidence: The hyperparameter optimization recommendations (reduced model size, increased dropout)
- Low Confidence: The generalizability of the 16k BPE subword model size recommendation

## Next Checks

1. **Ablation Study**: Systematically test each optimization component (dataset extension, hyperparameter tuning, subword model size) in isolation to quantify their individual contributions to performance improvements, particularly to validate whether the 27-point BLEU improvement is primarily driven by dataset size or other factors.

2. **Cross-Domain Transfer**: Evaluate the trained models on out-of-domain test sets (non-Covid data) to assess the trade-off between domain specialization and general translation capability, and determine if the significant performance gains in-domain come at the cost of reduced general translation quality.

3. **Scaling Analysis**: Test the optimal hyperparameter settings (reduced model size, increased dropout) on progressively larger datasets (10k, 20k, 50k lines) to determine the breaking point where these low-resource optimizations become suboptimal and more standard configurations would perform better.