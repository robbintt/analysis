---
ver: rpa2
title: 'SemScore: Automated Evaluation of Instruction-Tuned LLMs based on Semantic
  Textual Similarity'
arxiv_id: '2401.17072'
source_url: https://arxiv.org/abs/2401.17072
tags:
- evaluation
- human
- metrics
- semscore
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces SemScore, a straightforward evaluation metric
  for instruction-tuned large language models based on semantic textual similarity.
  The authors compare 12 prominent LLMs using 8 evaluation metrics and find that SemScore
  outperforms all other metrics, including more complex ones, in terms of correlation
  with human evaluation.
---

# SemScore: Automated Evaluation of Instruction-Tuned LLMs based on Semantic Textual Similarity

## Quick Facts
- arXiv ID: 2401.17072
- Source URL: https://arxiv.org/abs/2401.17072
- Authors: Ansar Aynetdinov; Alan Akbik
- Reference count: 17
- Primary result: SemScore outperforms all other metrics in correlation with human evaluation for instruction-tuned LLMs

## Executive Summary
This paper introduces SemScore, a simple yet effective evaluation metric for instruction-tuned large language models based on semantic textual similarity. The authors compare 12 prominent LLMs using 8 evaluation metrics and find that SemScore achieves the strongest correlation with human evaluation scores, even outperforming more complex metrics like G-Eval that rely on GPT-4. SemScore computes the cosine similarity between sentence embeddings of model and target responses, making it a straightforward tool that doesn't require special access or incur additional costs.

## Method Summary
The authors manually evaluate four additional LLMs (GPT-4, GPT-3.5, LLaMA, and Alpaca) using the same dataset and compare their performance using various evaluation metrics. They calculate correlations between automated metrics and human scores using Kendall τ and Pearson r coefficients, and perform an ablation study using different transformer models for SemScore.

## Key Results
- SemScore achieves 0.756 Kendall τ and 0.890 Pearson r correlation with human evaluation, outperforming all other metrics
- SemScore does not require special access or incur additional costs compared to LLM-based metrics like G-Eval
- The choice of transformer model significantly impacts SemScore performance, with ablation studies showing variation across different STS models

## Why This Works (Mechanism)

### Mechanism 1
- Claim: SemScore correlates strongly with human evaluation because semantic textual similarity captures meaning overlap rather than surface-level lexical overlap.
- Mechanism: SemScore computes the cosine similarity between sentence embeddings of model and target responses using a pre-trained transformer fine-tuned for sentence similarity.
- Core assumption: Semantic embeddings trained on large-scale contrastive data preserve semantic similarity in the instruction-tuning domain.
- Evidence anchors: The abstract states direct comparison using semantic textual similarity, and the method section describes embedding both responses and computing cosine similarity.

### Mechanism 2
- Claim: Using sentence-level embeddings rather than token-level scores improves evaluation robustness for open-ended instruction tasks.
- Mechanism: SemScore aggregates the embedding of the whole response via mean pooling over all tokens, avoiding reliance on exact token matches or BLEU-style n-gram overlap.
- Core assumption: Mean pooling preserves semantic content better than a single [CLS] token for open-ended, varied-length responses.
- Evidence anchors: The abstract describes computing cosine similarity between sentence embeddings, and the method section details embedding both responses separately.

### Mechanism 3
- Claim: SemScore's simplicity and reproducibility make it more reliable than LLM-based metrics for automated evaluation.
- Mechanism: Unlike G-Eval, which depends on GPT-4 and prompt engineering, SemScore uses a static, publicly available sentence transformer and cosine similarity.
- Core assumption: A fixed embedding model produces stable scores across runs and is not subject to LLM bias or model updates.
- Evidence anchors: The abstract emphasizes SemScore as straightforward and effective, and the results section shows it outperforms LLM-based metrics without requiring special access.

## Foundational Learning

- Concept: Semantic textual similarity and sentence embeddings
  - Why needed here: SemScore relies on STS to measure meaning overlap rather than exact word matching; understanding this ensures proper usage and interpretation.
  - Quick check question: What is the range of cosine similarity scores used in SemScore, and what does a score near 1 indicate?

- Concept: Transformer-based sentence embeddings and pooling strategies
  - Why needed here: The quality of SemScore depends on how sentence embeddings are computed; mean pooling vs. CLS token selection affects semantic capture.
  - Quick check question: Which pooling method does SemScore use for sentence representation in this paper?

- Concept: Evaluation metric correlation with human judgment
  - Why needed here: Comparing automated metrics requires understanding how correlation coefficients measure alignment with human scores.
  - Quick check question: What correlation coefficient values indicate the strongest alignment between SemScore and human evaluation in the study?

## Architecture Onboarding

- Component map: Input responses -> STS model (all-mpnet-base-v2) -> Mean pooling -> Cosine similarity -> Output score
- Critical path:
  1. Load pre-trained STS transformer
  2. Encode model and target responses into embeddings
  3. Apply mean pooling over token embeddings
  4. Compute cosine similarity
  5. Return similarity as evaluation score
- Design tradeoffs:
  - Simplicity and reproducibility vs. potential loss of nuanced context vs. LLM-based metrics
  - Use of fixed pre-trained model vs. risk of domain mismatch
  - No need for gold reference multiplicity vs. sensitivity to single target quality
- Failure signatures:
  - Consistently low scores for coding outputs if embedding model not trained on code semantics
  - Unexpected high/low scores if model embeddings biased toward certain lexical patterns
  - Score drift if embedding model updates or different STS model used
- First 3 experiments:
  1. Verify score range and consistency: run SemScore on identical model/target pairs multiple times.
  2. Compare SemScore vs. BERTScore using same transformer backbone to isolate embedding effect.
  3. Evaluate SemScore on a small set of instruction-tuning examples with known human scores to check correlation direction.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the choice of underlying transformer model for semantic textual similarity (STS) affect the performance of SemScore?
- Basis in paper: explicit
- Why unresolved: The paper uses all-mpnet-base-v2 but acknowledges that future research might yield improved models and shows that transformer choice influences results through ablation experiments.
- What evidence would resolve it: Comparative studies using different STS models with SemScore on the same evaluation dataset would provide insights into the impact of transformer choice on SemScore's performance.

### Open Question 2
- Question: Can SemScore be effectively applied to evaluate instruction-tuned LLMs on traditional NLP tasks?
- Basis in paper: inferred
- Why unresolved: The paper evaluates SemScore on a dataset covering various tasks beyond traditional NLP tasks but doesn't explicitly address its performance on tasks like text summarization or classification.
- What evidence would resolve it: Testing SemScore on a dataset specifically composed of traditional NLP tasks and comparing its performance with other evaluation metrics would clarify its applicability to such tasks.

### Open Question 3
- Question: How does SemScore handle coding-related instructions where the target answer is a code snippet?
- Basis in paper: explicit
- Why unresolved: The paper mentions that instruction-tuning datasets often contain coding questions, which are ill-suited for evaluation metrics based on N-gram overlaps or word-level embeddings, and presents an example where SemScore scores surprisingly badly.
- What evidence would resolve it: A detailed analysis of SemScore's performance on a larger set of coding-related instructions and exploration of potential improvements or adaptations for better handling of code snippets would be beneficial.

### Open Question 4
- Question: What are the implications of the limitations of LLM-based metrics like G-Eval for the automated evaluation of instruction-tuned LLMs?
- Basis in paper: explicit
- Why unresolved: The paper discusses limitations of LLM-based metrics including reliance on access to proprietary models like GPT-4 and potential biases but doesn't fully explore the implications for automated evaluation.
- What evidence would resolve it: Further research into alternative LLM-based evaluation methods that mitigate these limitations or a comprehensive study on the impact of these limitations on evaluation outcomes would provide clarity.

### Open Question 5
- Question: How does the size and diversity of the evaluation dataset affect the reliability and generalizability of SemScore?
- Basis in paper: inferred
- Why unresolved: The paper acknowledges the small size and lack of focus on traditional NLP tasks in the evaluation dataset, suggesting these factors may limit generalizability of findings.
- What evidence would resolve it: Expanding the evaluation to include a larger and more diverse dataset, including a balanced representation of traditional NLP tasks, would help assess the robustness and generalizability of SemScore.

## Limitations

- Domain coverage uncertainty: The paper demonstrates SemScore's effectiveness on instruction-tuning tasks but doesn't establish performance across other NLP domains such as code generation or long-form document summarization.
- Embedding model dependence: SemScore's performance is fundamentally tied to the quality of the underlying sentence transformer, and there's no guarantee that this particular model's semantic understanding transfers well to all instruction domains.
- Reference quality sensitivity: The paper doesn't investigate how reference quality affects SemScore scores, which could impact evaluation reliability if target responses contain errors or are suboptimal.

## Confidence

**High Confidence**: The claim that SemScore achieves strong correlation with human evaluation on the tested instruction-tuning dataset (correlation coefficients reported as 0.756 Kendall τ and 0.890 Pearson r). The methodology is clearly specified and reproducible.

**Medium Confidence**: The claim that SemScore is "straightforward but remarkably effective" compared to more complex metrics. While the correlation results are strong, the paper lacks ablation studies on different embedding models or pooling strategies that would strengthen this claim.

**Low Confidence**: The claim that SemScore will generalize to all instruction-tuning tasks and other NLP domains. The evaluation is limited to a specific dataset and task type, and the paper doesn't provide evidence for broader applicability.

## Next Checks

1. **Cross-Domain Validation**: Test SemScore on a diverse set of NLP tasks beyond instruction-tuning, including code generation, long-form summarization, and domain-specific technical writing. Compare correlation with human judgment across these domains to identify where SemScore performs well versus where it may fail.

2. **Embedding Model Ablation**: Implement SemScore using multiple different STS models (e.g., paraphrase-mpnet-base-v2, stsb-roberta-base) and compare correlation results. This will reveal whether SemScore's effectiveness is tied to the specific all-mpnet-base-v2 model or if the approach generalizes across different STS architectures.

3. **Reference Quality Analysis**: Create controlled experiments with intentionally flawed, incomplete, or suboptimal target references while keeping model responses constant. Measure how SemScore scores change when reference quality varies, and compare this sensitivity to other metrics like BERTScore or BLEURT.