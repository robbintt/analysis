---
ver: rpa2
title: Model Composition for Multimodal Large Language Models
arxiv_id: '2402.12750'
source_url: https://arxiv.org/abs/2402.12750
tags:
- mllms
- modalities
- inputs
- arxiv
- composition
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces model composition for multimodal large language
  models (MLLMs), proposing a training-free method to combine existing MLLMs into
  a single model that can handle multiple modalities. The core method, DAMC, employs
  parameter decoupling and adaptive adjustment to mitigate interference during the
  composition process.
---

# Model Composition for Multimodal Large Language Models

## Quick Facts
- arXiv ID: 2402.12750
- Source URL: https://arxiv.org/abs/2402.12750
- Reference count: 16
- One-line primary result: Model composition enables zero-shot multi-modality expansion for MLLMs without training

## Executive Summary
This paper introduces a novel approach to composing multiple pretrained multimodal large language models (MLLMs) into a single unified model capable of handling multiple modalities. The proposed method, called Model Composition, addresses the challenge of combining MLLMs without requiring additional training data or model fine-tuning. The core innovation is DAMC (Decoupled Adaptive Model Composition), which employs parameter decoupling and adaptive adjustment to mitigate interference during the composition process. The framework demonstrates significant performance improvements over baseline methods when combining image, audio, video, and point cloud modalities.

## Method Summary
The paper proposes a training-free method for composing multiple MLLMs into a single model that can handle multiple modalities. The approach involves reusing modality-specific encoders from different MLLMs while merging their underlying LLM parameters. The basic implementation, NaiveMC, averages LLM parameters when models share the same base. DAMC enhances this by introducing parameter decoupling (separating modality-specific from language model parameters during initial training) and adaptive parameter adjustment (using validation-based coefficients to weight parameter contributions). This allows for effective merging while reducing cross-modal interference.

## Key Results
- DAMC outperforms NaiveMC and single-modality models on audio-visual question answering tasks
- Composite models show significant improvements on 3D object classification tasks with point cloud inputs
- The new Multimodal Commonality Understanding Benchmark (MCUB) demonstrates DAMC's ability to understand shared attributes across different modalities
- Model composition achieves robust performance when integrating multiple modalities simultaneously

## Why This Works (Mechanism)

### Mechanism 1
NaiveMC allows zero-shot multi-modality expansion by reusing modality-specific encoders and averaging LLM parameters when initialized from the same base model. This creates a composite model that can process multiple modalities without additional training, assuming parameter compatibility through shared LLM initialization.

### Mechanism 2
DAMC reduces parameter interference by decoupling modality-specific parameters from language model parameters during MLLM training. This separation allows selective merging of only text-related parameters during composition, preventing cross-modal interference.

### Mechanism 3
DAMC uses adaptive parameter adjustment with adjustment coefficients (λi) that weight each model's parameter contribution based on validation performance. This optimization ensures optimal parameter merging by accounting for varying performance levels across different MLLMs.

## Foundational Learning

- Concept: Parameter averaging for model merging
  - Why needed here: Core of NaiveMC relies on averaging LLM parameters from different MLLMs
  - Quick check question: What happens to model performance if we average parameters from models trained on completely different tasks?

- Concept: Parameter decoupling in neural networks
  - Why needed here: DAMC requires separating modality-specific processing from general language processing
  - Quick check question: How does parameter decoupling affect the number of trainable parameters in the composite model?

- Concept: Multimodal alignment techniques
  - Why needed here: Understanding how different modalities are aligned to a common embedding space
  - Quick check question: What are the key differences between aligning modalities through a unified encoder versus composing separate modality encoders?

## Architecture Onboarding

- Component map:
  - Modality-specific encoders (image, audio, video, point cloud) -> Connectors (MLP projections, Q-Former) -> LLM backbone (Vicuna-7B-v1.5) -> Parameter merging logic (averaging, weighted averaging) -> DAMC-specific components (decoupled parameter sets, adjustment coefficients)

- Critical path:
  1. Load pre-trained MLLMs for each modality
  2. Apply parameter decoupling if DAMC is used
  3. Merge LLM parameters (average or weighted)
  4. Connect all modality encoders to merged LLM
  5. Apply adjustment coefficients to optimize performance
  6. Validate on target tasks

- Design tradeoffs:
  - NaiveMC: Simpler implementation but susceptible to parameter interference
  - DAMC: More complex with parameter decoupling and adjustment, but better performance
  - Tradeoff between training complexity (parameter decoupling) and inference performance

- Failure signatures:
  - Performance degradation when adding more modalities (parameter interference)
  - Inconsistent results across different task types
  - Failure to follow open-ended generation instructions (as seen with point cloud inputs)
  - Poor performance on single-modality tasks after composition

- First 3 experiments:
  1. Compose two MLLMs (image + audio) and test on a simple audio-visual question answering task
  2. Compare NaiveMC vs DAMC performance on the same task to observe interference effects
  3. Test composite model on single-modality tasks to verify retained capabilities

## Open Questions the Paper Calls Out

### Open Question 1
How does the performance of DAMC scale with the number of modalities beyond four (image, audio, video, point cloud)? The paper demonstrates DAMC's effectiveness on four modalities but does not explore beyond this limit.

### Open Question 2
How does the size of the underlying LLM affect the performance and efficiency of model composition methods like NaiveMC and DAMC? The study is limited to 7B parameter models, with applicability on larger-scale MLLMs noted as future research.

### Open Question 3
What is the impact of parameter decoupling on single-modality task performance compared to traditional fine-tuning approaches? While DAMC improves single-modality performance over NaiveMC, comparison to traditional fine-tuning is not explored.

### Open Question 4
How sensitive is the DAMC approach to the choice of parameter adjustment coefficients (λi), and is there an optimal strategy for selecting these coefficients? The paper uses a simple search strategy but does not investigate sensitivity or provide an optimal approach.

## Limitations

- Assumes all MLLMs are initialized from the same LLM base for NaiveMC to work effectively, limiting practical applicability
- Parameter decoupling strategy lacks detailed implementation specifications critical for reproducibility
- Limited task diversity in evaluation may not fully capture the robustness of composed models across diverse scenarios

## Confidence

- **High Confidence**: Core mechanism of parameter averaging for model merging is well-established; experimental results on MCUB benchmark are robust
- **Medium Confidence**: DAMC's parameter decoupling and adaptive adjustment mechanisms are empirically demonstrated but could benefit from more rigorous theoretical underpinnings
- **Low Confidence**: Claims about robust performance with multiple modalities are based on limited task diversity; point cloud failure modes suggest potential brittleness

## Next Checks

1. **Cross-Base Model Composition Test**: Evaluate composition using MLLMs initialized from different LLM bases to assess NaiveMC limitations and DAMC's necessity

2. **Long-Tail Modality Performance**: Test composed model on less common modalities (e.g., hyperspectral imagery, thermal imaging) to evaluate generalizability beyond the four studied modalities

3. **Ablation Study on Parameter Adjustment**: Conduct systematic ablation varying adjustment coefficients λi to quantify impact on performance and validate claimed benefits across task types and model combinations