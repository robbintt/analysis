---
ver: rpa2
title: 'SnakModel: Lessons Learned from Training an Open Danish Large Language Model'
arxiv_id: '2412.12956'
source_url: https://arxiv.org/abs/2412.12956
tags:
- language
- danish
- data
- training
- computational
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper presents SnakModel, a Danish large language model based
  on Llama2-7B, which was continuously pre-trained on 13.6B Danish words and instruction-tuned
  on 3.7M Danish instruction-answer pairs. The authors examined the effects of modeling
  and training decisions on downstream performance across eight Danish-specific tasks,
  achieving the highest overall performance compared to contemporary Llama2-7B-based
  models.
---

# SnakModel: Lessons Learned from Training an Open Danish Large Language Model

## Quick Facts
- arXiv ID: 2412.12956
- Source URL: https://arxiv.org/abs/2412.12956
- Reference count: 29
- This paper presents SnakModel, a Danish large language model based on Llama2-7B, which was continuously pre-trained on 13.6B Danish words and instruction-tuned on 3.7M Danish instruction-answer pairs.

## Executive Summary
This paper presents SnakModel, a Danish large language model based on Llama2-7B, which was continuously pre-trained on 13.6B Danish words and instruction-tuned on 3.7M Danish instruction-answer pairs. The authors examined the effects of modeling and training decisions on downstream performance across eight Danish-specific tasks, achieving the highest overall performance compared to contemporary Llama2-7B-based models. The model's training dynamics were analyzed, showing that instruction tuning after 2,000-5,000 steps of Danish pre-training may be sufficient to obtain close-to-final performance. Weight divergence analysis revealed that most parameter updates were concentrated in embeddings, feed-forward up-projections, and language modeling head.

## Method Summary
The authors created SnakModel by first collecting and preprocessing Danish text data from diverse sources including Bookshop, CC-100, CulturaX, DaNewsroom, Dawiki, FTSpeech, Gigaword, OpenSubtitles, Reddit, and Twitter, resulting in 13.6B Danish words after deduplication. They continuously pre-trained Llama2-7Bbase on this curated corpus using Megatron-LLM with a batch size of 512, learning rate of 1.5×10^-5, and 12,500 training steps on 4×NVIDIA A100 GPUs. The pre-trained model was then instruction-tuned using LoRA (rank 128) on 3.7M Danish instruction-answer pairs with the CHAT template format, AdamW optimizer (lr=2×10^-4), and batch size 64 for one epoch.

## Key Results
- SnakModel achieved the highest overall performance on the ScandEval benchmark (65.7) compared to DaLLM (63.5) and KIRT (63.3), representing a statistically significant improvement of approximately 2.2 points.
- Weight divergence analysis showed that parameter updates were concentrated in embeddings, feed-forward up-projections, and language modeling head during training.
- The study found that instruction tuning after 2,000-5,000 steps of Danish pre-training may be sufficient to obtain close-to-final performance.

## Why This Works (Mechanism)
The success of SnakModel stems from several key factors: first, the use of a diverse Danish corpus with 13.6B words provides comprehensive language coverage across multiple domains and registers; second, the instruction-tuning on 3.7M Danish-specific instruction-answer pairs ensures task-specific performance; third, the careful analysis of training dynamics revealed that most learning occurs in specific layers (embeddings, feed-forward layers, and LM head), allowing for targeted optimization; and fourth, the CHAT template format for instruction data proved more effective than alternative formats for Danish language generation tasks.

## Foundational Learning
- **Continuous pre-training vs. full fine-tuning**: Continuous pre-training adapts an existing model to a new language while preserving general capabilities, which is more efficient than training from scratch for resource-constrained languages. Quick check: Compare training time and parameter updates between continuous pre-training and full fine-tuning on the same dataset.
- **Weight divergence analysis**: This technique tracks how parameters change during training to identify which components learn most effectively, revealing that embeddings and feed-forward layers are primary sites of adaptation. Quick check: Monitor L2 norm of parameter differences across training steps to identify divergence patterns.
- **Instruction-tuning formats**: Different template formats (CHAT, CONCAT, ALPACA) affect how models process and respond to instructions, with CHAT format showing superior performance for Danish generation tasks. Quick check: Evaluate model performance across different template formats on the same instruction dataset.

## Architecture Onboarding
**Component Map:** Raw text data → Language identification and filtering → Deduplication → Continuous pre-training (Megatron-LLM) → LoRA instruction-tuning → Evaluation on ScandEval

**Critical Path:** Data collection and preprocessing → Continuous pre-training → Instruction-tuning with LoRA → Evaluation and analysis

**Design Tradeoffs:** The choice between continuous pre-training and full fine-tuning balances computational efficiency against potential catastrophic forgetting; the selection of LoRA for instruction-tuning reduces parameter updates compared to full fine-tuning while maintaining task performance; the decision to use CHAT template format over alternatives optimizes for Danish language generation quality.

**Failure Signatures:** Gradient explosion during pre-training (indicated by rapidly increasing loss values); poor Danish language generation despite instruction tuning (evidenced by low scores on Danish-specific tasks); weight divergence patterns that don't align with expected learning hierarchies.

**3 First Experiments:** 1) Run language identification and deduplication on raw Danish text sources to verify corpus quality; 2) Execute a small-scale continuous pre-training run with gradient clipping to test stability; 3) Perform instruction-tuning on a subset of the Danish instruction data using different template formats to compare performance.

## Open Questions the Paper Calls Out
None

## Limitations
- The evaluation is limited to Danish-specific tasks on the ScandEval benchmark, which may not capture broader language capabilities.
- The weight divergence analysis focuses on parameter updates without examining the functional impact on model outputs.
- The optimal instruction-tuning timing recommendation (2,000-5,000 steps) is based on a narrow range of experiments and may not apply to different model scales or data conditions.

## Confidence
- **High confidence** in the overall performance claims and benchmark results, as these are based on direct measurements against established evaluation protocols.
- **Medium confidence** in the training dynamics analysis and weight divergence findings, as these provide plausible explanations but may be sensitive to specific implementation details.
- **Low confidence** in the generalizability of the "optimal" instruction-tuning timing recommendation without further validation across different model sizes and data distributions.

## Next Checks
1. Replicate the weight divergence analysis across multiple training runs with different random seeds to assess the stability of the observed parameter update patterns.
2. Evaluate SnakModel on additional Danish NLP benchmarks beyond ScandEval to verify the consistency of performance improvements across different task types.
3. Test the instruction-tuning timing hypothesis (2,000-5,000 steps) with varying model sizes (e.g., 1B, 13B parameters) to determine if the finding generalizes across scales.