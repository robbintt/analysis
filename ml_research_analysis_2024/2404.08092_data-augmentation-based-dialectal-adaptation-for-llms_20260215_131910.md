---
ver: rpa2
title: Data-Augmentation-Based Dialectal Adaptation for LLMs
arxiv_id: '2404.08092'
source_url: https://arxiv.org/abs/2404.08092
tags: []
core_contribution: 'This work addresses the challenge of evaluating commonsense reasoning
  in non-standard South Slavic dialects using large language models. The authors propose
  a data augmentation approach that leverages multiple types of language models to
  generate synthetic training data for three dialects: Chakavian, Cherkano, and Torlak.'
---

# Data-Augmentation-Based Dialectal Adaptation for LLMs

## Quick Facts
- arXiv ID: 2404.08092
- Source URL: https://arxiv.org/abs/2404.08092
- Reference count: 4
- Primary result: Data augmentation approach achieves highest scores across three South Slavic dialects in open-source model category

## Executive Summary
This work addresses the challenge of evaluating commonsense reasoning in non-standard South Slavic dialects using large language models. The authors propose a data augmentation approach that leverages multiple types of language models to generate synthetic training data for three dialects: Chakavian, Cherkano, and Torlak. Their method combines fine-tuning of encoder-based models (BERTić) with instruction tuning of multilingual models (AYA-101) using augmented datasets. The proposed approach achieves the highest scores across all three test dialects in the open-source model category, outperforming other submissions. Notably, the BERTić model saw substantial performance gains when combined with the data augmentation strategy, demonstrating the effectiveness of this approach for low-resource dialectal settings.

## Method Summary
The authors employ a multi-pronged approach combining data augmentation techniques with different model architectures. They use reverse augmentation (swapping premise and correct answer to double training instances), cross-lingual translation (Macedonian Cyrillic to Latin, and English to other languages), and synthetic data generation (Chakavian from Croatian using Claude-3). The augmented data is used to fine-tune two types of models: BERTić (a language-specific encoder model) and Aya-101 (a multilingual instruction-tuned model). The best-performing configuration uses Aya-101 with LoRA adapter tuning on a combination of all available training data, achieving superior results across all three dialects.

## Key Results
- Aya-101 with LoRA adapter tuning and comprehensive data augmentation (otrsl setting) achieved the highest scores across all three test dialects
- BERTić model showed substantial performance gains when combined with the data augmentation strategy, though still underperformed compared to Aya-101
- The Chakavian dialect, which had no original training data, showed improved performance through synthetic data generation despite limited translation accuracy
- Upsampling Chakavian-related data improved scores on the Cerkno test set, suggesting complex language relatedness effects

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Data augmentation using reverse-augmentation (swapping premise and correct answer) doubles the training instances while preserving the logical structure of cause-effect relationships.
- Mechanism: By inverting cause-effect pairs, the model learns bidirectional reasoning patterns, improving generalization to unseen dialects.
- Core assumption: The underlying commonsense reasoning patterns are symmetric enough that reversing premise-answer pairs maintains task validity.
- Evidence anchors:
  - [abstract]: "We propose an approach that combines the strengths of different types of language models and leverages data augmentation techniques to improve task performance on three South Slavic dialects."
  - [section]: "For each instance in the training data, we swap the premise and the correct answer choice, effectively transforming cause examples into effect examples and vice versa, thereby doubling the number of training instances."
- Break condition: If the commonsense reasoning task is inherently asymmetric (e.g., certain causes always precede effects), the reverse pairs would introduce noise rather than useful signal.

### Mechanism 2
- Claim: Instruction tuning of Aya-101 with augmented datasets provides superior performance compared to full fine-tuning of smaller BERTić models.
- Mechanism: Instruction-tuned models have stronger cross-lingual reasoning capabilities and benefit more from data augmentation than monolingual encoder models.
- Core assumption: Instruction tuning creates more flexible reasoning patterns that transfer better across dialects than encoder-only fine-tuning.
- Evidence anchors:
  - [abstract]: "Our results demonstrate that the proposed data augmentation techniques lead to substantial performance gains across all three test datasets in the open-source model category."
  - [section]: "Full fine-tuning of the comparatively smaller, non-instruction-tuned, but language-specific BERTić model cannot surpass the performance of the multilingual, instruction-tuned Aya-101 model."
- Break condition: If the test dialects are too divergent from the model's pretraining languages, even instruction tuning may not overcome the gap.

### Mechanism 3
- Claim: Synthetic Chakavian data generated via Croatian-to-Chakavian conversion rules and Claude-3 translation partially bridges the data scarcity gap for low-resource dialects.
- Mechanism: Even imperfect synthetic data provides dialect-specific training signals that improve model performance on previously unseen dialectal patterns.
- Core assumption: Partial translation coverage is sufficient to create useful training examples, even when full accuracy is not achieved.
- Evidence anchors:
  - [section]: "Despite the limited accuracy of the translation, this synthetic translated dataset enables us to train and evaluate models on the Chakavian dialect, despite the absence of original training data for this specific dialect."
  - [corpus]: "Found 25 related papers (using 8). Average neighbor FMR=0.441, average citations=0.0." (Weak corpus evidence for dialect translation techniques)
- Break condition: If the dialectal divergence is too large for rule-based conversion to capture meaningful patterns, synthetic data may introduce more noise than signal.

## Foundational Learning

- Concept: Data augmentation strategies for low-resource NLP
  - Why needed here: The original dataset contains only 400 instances per language, insufficient for robust dialectal adaptation.
  - Quick check question: What are the three data augmentation approaches used in this work?

- Concept: Instruction tuning vs. full fine-tuning tradeoffs
  - Why needed here: The work compares instruction-tuned Aya-101 with fine-tuned BERTić to determine optimal adaptation strategy.
  - Quick check question: Why does instruction tuning provide better performance on dialectal commonsense reasoning tasks?

- Concept: Cross-lingual transfer in multilingual models
  - Why needed here: The models must generalize from standard languages to micro-dialects with limited training data.
  - Quick check question: How does combining data from multiple languages affect performance on individual dialects?

## Architecture Onboarding

- Component map: Aya-101 (13B parameter instruction-tuned multilingual model) -> LoRA adapters -> data augmentation pipeline -> BERTić (Bosnian-Croatian-Montenegrin-Serbian encoder model)
- Critical path: Data augmentation → Data combination selection → LoRA adapter tuning → 4-shot inference
- Design tradeoffs: Aya-101 provides better cross-lingual generalization but requires more compute; BERTić is smaller but less adaptable to unseen dialects
- Failure signatures: Performance degradation on Chakavian despite data augmentation suggests synthetic translation quality issues; poor Cerkno performance despite upsampling indicates language relatedness complexity
- First 3 experiments:
  1. Compare 4-shot prompting vs. 4-shot same-class prompting on validation set to verify prompt design effectiveness
  2. Test data augmentation impact by comparing o vs otrsl settings on Aya-101
  3. Evaluate script consistency by comparing otrsl vs otrslc settings to determine if Cyrillic script inclusion helps or hurts

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific characteristics of the Chakavian dialect make it more challenging for language models compared to Cerkno and Torlak dialects?
- Basis in paper: [explicit] The paper mentions that Chakavian is "intentionally held out from the training and validation splits and is exclusively encountered during the test phase"
- Why unresolved: The paper identifies Chakavian as a unique case but doesn't analyze why it presents particular challenges for the models
- What evidence would resolve it: Comparative analysis of model performance metrics and error patterns across all three dialects, specifically highlighting differences in Chakavian's linguistic features

### Open Question 2
- Question: How does the performance of the data augmentation approach vary across different dialect pairs, and what factors influence its effectiveness?
- Basis in paper: [explicit] The paper mentions that "upsampling the Chakavian dialect-related data using the otrslmk-hr-ckm setting leads to better scores on the Cerkno test set"
- Why unresolved: The paper observes this phenomenon but doesn't explain the underlying reasons for varying effectiveness across dialect pairs
- What evidence would resolve it: Detailed analysis of augmentation effectiveness metrics across all dialect pairs, identifying specific linguistic features that influence success rates

### Open Question 3
- Question: What are the limitations of using Claude-3 for generating synthetic Chakavian training data, and how could these limitations be addressed?
- Basis in paper: [explicit] The paper notes that "only a small number of words, specifically three in this instance, are correctly translated from Croatian to Chakavian"
- Why unresolved: While the paper acknowledges the limited accuracy, it doesn't explore potential improvements or alternative approaches
- What evidence would resolve it: Comparative analysis of synthetic data quality using different language models, along with quantitative measures of translation accuracy improvements

## Limitations
- Limited accuracy of synthetic Chakavian data generation despite performance improvements
- Unclear relative contributions of different data augmentation strategies (reverse augmentation vs. cross-lingual translation vs. synthetic generation)
- Potential confounding between model scale differences and architectural choices when comparing instruction-tuned vs. encoder-based models

## Confidence

**High Confidence**: The overall approach of combining data augmentation with dialectal adaptation improves performance over baseline settings. The effectiveness of reverse augmentation in doubling training data is well-established.

**Medium Confidence**: The superiority of instruction-tuned Aya-101 over fine-tuned BERTić for this task, while demonstrated, may depend heavily on specific model sizes and training configurations.

**Low Confidence**: The quality and impact of synthetic Chakavian data generation, given the acknowledged "limited accuracy" of the translation process.

## Next Checks

1. **Ablation study of augmentation techniques**: Systematically disable each augmentation method (reverse, translation, synthetic generation) to quantify their individual contributions to performance gains.

2. **Cross-dialect generalization test**: Evaluate whether models trained on one dialect can effectively transfer to unseen dialects without fine-tuning, measuring the limits of cross-lingual reasoning capabilities.

3. **Script normalization impact analysis**: Conduct controlled experiments comparing Cyrillic vs. Latin script training data to isolate the effect of script consistency on model performance across dialects.