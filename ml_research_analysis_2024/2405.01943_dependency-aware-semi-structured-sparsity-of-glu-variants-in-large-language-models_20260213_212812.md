---
ver: rpa2
title: Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language
  Models
arxiv_id: '2405.01943'
source_url: https://arxiv.org/abs/2405.01943
tags:
- pruning
- sparsity
- dass
- wanda
- weight
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of efficiently compressing large
  language models (LLMs) with GLU-based architectures while maintaining performance
  and achieving hardware-friendly sparsity patterns. The proposed Dependency-aware
  Semi-structured Sparsity (DaSS) method incorporates structural dependency into weight
  magnitude-based unstructured pruning by using intermediate activation norms as group
  importance indicators.
---

# Dependency-Aware Semi-Structured Sparsity of GLU Variants in Large Language Models

## Quick Facts
- **arXiv ID**: 2405.01943
- **Source URL**: https://arxiv.org/abs/2405.01943
- **Reference count**: 19
- **Primary result**: DaSS outperforms SparseGPT at high sparsity without weight updates while maintaining hardware-friendly N:M sparsity patterns

## Executive Summary
This paper addresses the challenge of efficiently compressing large language models with GLU-based architectures while maintaining performance and achieving hardware-friendly sparsity patterns. The proposed Dependency-aware Semi-structured Sparsity (DaSS) method incorporates structural dependency into weight magnitude-based unstructured pruning by using intermediate activation norms as group importance indicators. This approach balances the flexibility of unstructured pruning with the structural coherence of dependency-based structured pruning, enabling more precise weight removal.

## Method Summary
The Dependency-aware Semi-structured Sparsity (DaSS) method introduces an MLP-specific pruning metric that evaluates weight importance by jointly considering magnitude and corresponding intermediate activation norms. For Gate-Proj and Up-Proj layers, DaSS uses input-balanced pruning with importance scores calculated as |weight| × (activation norm)^α. For Down-Proj, it employs output-balanced pruning with importance scores of |weight| × activation norm. The method achieves N:M sparsity patterns (4:8 and 2:4) across various GLU variants and model families while maintaining computational efficiency.

## Key Results
- DaSS consistently outperforms existing methods like SparseGPT and Wanda in achieving N:M sparsity patterns across various GLU variants (SwiGLU, GeGLU, ReGLU)
- Achieves superior performance without weight updates, even at high sparsity levels
- Demonstrates robustness to calibration sample variations
- Shows consistent effectiveness across multiple model families (LLaMA2, Mistral, Gemma)

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Using intermediate activation norms as group importance indicators improves pruning precision by aligning weight removal with structural dependencies inherent in GLU-based MLP layers.
- Core assumption: The importance of a weight in a GLU-based MLP can be meaningfully captured by considering both its magnitude and the activation norm of the intermediate neuron it feeds into.
- Evidence anchors:
  - [abstract] "We introduce an MLP-specific pruning metric that evaluates the importance of each weight by jointly considering its magnitude and its corresponding MLP intermediate activation norms."
  - [section] "To emphasize the importance of weights corresponding to large intermediate activations, we present a new MLP pruning metric that assesses each importance of each weight based on the product of its magnitude and the norm of the corresponding MLP intermediate activations."
- Break condition: If intermediate activations are uniformly distributed or do not correlate with model performance, the importance metric would fail to capture true weight significance.

### Mechanism 2
- Claim: Input-balanced pruning granularity for Gate-Proj and Up-Proj, combined with output-balanced pruning for Down-Proj, enables incorporation of intermediate activations into importance scoring while maintaining hardware-friendly N:M sparsity.
- Core assumption: The choice of pruning granularity directly affects the ability to incorporate intermediate activations into the importance metric, and input-balanced granularity is necessary to differentiate importance scores within a group based on varying intermediate activations.
- Evidence anchors:
  - [section] "To incorporate intermediate activations into GLU-based MLP pruning, each weight in the same comparison group should correspond to different intermediate activations. Therefore, we choose to use input-balanced pruning for Gate-Proj and Up-Proj pruning..."
- Break condition: If hardware cannot support the required granularity or if granularity choice does not align with sparsity pattern constraints.

### Mechanism 3
- Claim: DaSS achieves performance comparable to SparseGPT without weight updates by more accurately identifying sparse subnetworks through its dependency-aware importance metric.
- Core assumption: The dependency-aware importance metric can identify a sparse subnetwork that is as performant as the one found by SparseGPT with weight updates, without requiring the additional computational step.
- Evidence anchors:
  - [abstract] "DaSS demonstrates consistent effectiveness in all the prevalent GLU variants... Impressively, DaSS outperforms SparseGPT at high sparsity even without weight update."
- Break condition: If the importance metric fails to accurately capture weight significance or if structural dependencies are not as critical as assumed.

## Foundational Learning

- **GLU variants (SwiGLU, GeGLU, ReGLU) and their role in modern LLMs**
  - Why needed: Understanding the architecture of GLU-based MLPs is essential to grasp why DaSS's dependency-aware approach is effective.
  - Quick check: In a SwiGLU layer, how are the intermediate activations computed, and why does this create structural dependencies between weights?

- **N:M sparsity patterns and hardware acceleration**
  - Why needed: DaSS is designed to achieve hardware-friendly N:M sparsity patterns.
  - Quick check: What is the difference between unstructured sparsity and N:M sparsity, and why is N:M sparsity more suitable for hardware acceleration?

- **Pruning granularity (input-balanced vs. output-balanced)**
  - Why needed: The choice of pruning granularity directly affects the ability to incorporate intermediate activations into the importance metric.
  - Quick check: How does input-balanced pruning differ from output-balanced pruning, and why does DaSS use input-balanced pruning for Gate-Proj and Up-Proj but output-balanced pruning for Down-Proj?

## Architecture Onboarding

- **Component map**: Input -> Gate-Proj (input-balanced) -> Intermediate Activations -> Up-Proj (input-balanced) -> Intermediate Activations -> Down-Proj (output-balanced) -> Output

- **Critical path**:
  1. Forward pass through the model to compute intermediate activations
  2. Calculate importance scores for each weight using the DaSS metric
  3. Sort weights within each comparison group based on importance scores
  4. Prune the lowest-scoring weights to achieve the desired sparsity ratio
  5. Convert to N:M sparsity pattern if necessary

- **Design tradeoffs**:
  - Accuracy vs. Efficiency: Higher accuracy but small computational overhead due to sorting operations
  - Flexibility vs. Structure: Balances unstructured pruning flexibility with dependency-based structural coherence
  - Granularity vs. Compatibility: Input-balanced pruning allows richer importance signals while maintaining N:M sparsity compatibility

- **Failure signatures**:
  - Performance degradation if importance metric fails to capture true weight significance
  - Structural mismatch if pruning granularity doesn't align with sparsity pattern constraints
  - Calibration sensitivity if method is highly sensitive to calibration sample variations

- **First 3 experiments**:
  1. Ablation study on hyperparameter α: Test values 0.25, 0.5, 0.75, 1.0 on LLaMA2-7B
  2. Granularity sensitivity analysis: Compare input-balanced vs. output-balanced pruning on a small model
  3. Robustness to calibration samples: Test performance using 32, 64, 128, 256 calibration samples on a small model

## Open Questions the Paper Calls Out

- **Open Question 1**: How does DaSS perform on LLMs with architectures significantly different from the tested GLU variants?
  - Basis: Paper demonstrates effectiveness across multiple GLU variants but doesn't explore non-GLU architectures
  - Why unresolved: Focus on GLU-based models without evidence for applicability to other architectural designs
  - What evidence would resolve it: Experiments applying DaSS to attention-only models or models using different MLP variants

- **Open Question 2**: What is the impact of different activation functions on DaSS's pruning effectiveness beyond tested SwiGLU, GeGLU, and ReGLU variants?
  - Basis: Paper tests DaSS on SwiGLU, GeGLU, and ReGLU but doesn't explore other activation functions
  - Why unresolved: No investigation of how other activation functions might affect group importance indicator
  - What evidence would resolve it: Comparative experiments with various activation functions (GELU, ReLU6, custom activations)

- **Open Question 3**: How does DaSS scale to extremely large models (e.g., models larger than LLaMA2-70B) in terms of both performance and computational efficiency?
  - Basis: Paper tests DaSS on LLaMA2-70B but doesn't explore scalability to significantly larger models
  - Why unresolved: No data on how DaSS performs or scales when applied to models with hundreds of billions or trillions of parameters
  - What evidence would resolve it: Experiments applying DaSS to ultra-large models analyzing accuracy retention and computational overhead

## Limitations

- Method's effectiveness depends on accurate computation of intermediate activation norms, which may be computationally expensive for very large models
- Choice of α = 0.5 as default group importance strength may not be optimal across all model architectures and tasks
- Study focuses primarily on N:M sparsity patterns compatible with current hardware, potentially limiting applicability to emerging sparsity paradigms

## Confidence

- **High Confidence**: Core mechanism of using intermediate activation norms as importance indicators is well-supported by ablation studies and performance comparisons
- **Medium Confidence**: Claim of achieving hardware-friendly N:M sparsity patterns is supported by implementation details but would benefit from real-world hardware performance metrics
- **Medium Confidence**: Robustness to calibration sample variations is demonstrated empirically but could benefit from testing on broader range of datasets and model sizes

## Next Checks

1. **Hardware Acceleration Validation**: Implement DaSS on actual GPU hardware to measure practical speedup compared to unstructured pruning and verify claimed N:M sparsity pattern efficiency
2. **Cross-Architecture Generalization**: Apply DaSS to non-GLU architectures (e.g., GPT-style models) to test method's broader applicability beyond current scope
3. **Long-term Stability Analysis**: Evaluate pruned models' performance over extended training periods to assess whether structural dependencies captured by DaSS remain relevant as model continues to adapt