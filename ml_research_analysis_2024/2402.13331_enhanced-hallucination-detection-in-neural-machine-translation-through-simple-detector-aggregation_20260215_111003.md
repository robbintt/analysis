---
ver: rpa2
title: Enhanced Hallucination Detection in Neural Machine Translation through Simple
  Detector Aggregation
arxiv_id: '2402.13331'
source_url: https://arxiv.org/abs/2402.13331
tags:
- detectors
- translation
- detection
- aggregation
- hallucinations
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: "This paper proposes a simple unsupervised aggregation method,\
  \ STARE, to combine multiple hallucination detectors in neural machine translation\
  \ (NMT) systems. By leveraging the complementary strengths of different detectors\u2014\
  such as cross-lingual similarity models and internal model features\u2014STARE achieves\
  \ state-of-the-art performance on two widely-used hallucination detection benchmarks."
---

# Enhanced Hallucination Detection in Neural Machine Translation through Simple Detector Aggregation

## Quick Facts
- arXiv ID: 2402.13331
- Source URL: https://arxiv.org/abs/2402.13331
- Reference count: 16
- One-line primary result: STARE aggregation method achieves state-of-the-art hallucination detection performance by combining complementary detectors

## Executive Summary
This paper addresses the challenge of detecting hallucinations in neural machine translation (NMT) systems. The authors propose STARE (Simple Detectors Aggregation), a straightforward unsupervised method that combines multiple hallucination detectors to leverage their complementary strengths. By normalizing and aggregating scores from various external and internal detection models, STARE consistently outperforms individual detectors on two major benchmarks, achieving up to 2.4 percentage points improvement in AUROC and reducing FPR by as much as 9.8 points.

## Method Summary
STARE is a simple aggregation method that combines multiple hallucination detectors in NMT. The approach uses min-max normalization on individual detector scores based on a reference set, then applies weighted averaging to create a consolidated detection score. The method works with both external detectors (like COMET-QE, CometKiwi, and multilingual encoders) and internal model features (attention maps, sequence log-probability). The normalization weights are specific to each detector and designed based on the entire training dataset, allowing effective combination of scores measured on different scales.

## Key Results
- STARE achieves up to 2.4 percentage points improvement in Area Under the ROC curve (AUROC) compared to individual detectors
- False Positive Rate (FPR) is reduced by as much as 9.8 percentage points at 90% True Positive Rate
- Internal model-based detectors, when aggregated, can outperform computationally expensive external models
- Performance improvements plateau when the reference set exceeds 1,000 samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Aggregation leverages complementary strengths of different detectors to improve overall hallucination detection.
- Mechanism: Different detectors excel at detecting different types of hallucinations. By combining them, the aggregation method can capture a broader range of hallucinatory patterns.
- Core assumption: The individual detectors have complementary performance characteristics.
- Evidence anchors:
  - [abstract] "Previous research works have identified that detectors exhibit complementary performance — different detectors excel at detecting different types of hallucinations."
  - [section 1] "Most importantly, different detectors exhibit complementary properties. For instance, oscillatory hallucinations — translations with anomalous repetitions of phrases or n-grams (Raunak et al., 2021) — are readily identified by CometKiwi, while detectors based on low source contribution or sentence dissimilarity struggle in this regard."

### Mechanism 2
- Claim: STARE's normalization weights allow effective aggregation of detectors measured on different scales.
- Mechanism: The min-max normalization creates detector-specific weights that standardize scores before aggregation, enabling meaningful combination.
- Core assumption: Normalization based on the entire training dataset provides appropriate scaling for aggregation.
- Evidence anchors:
  - [section 2.2] "The standardization weights we propose, wk, are specific to each detection score. Using the min-max normalization, they are designed based on the whole training dataset Dn = {x1, ..., xn}."
  - [section 3.2] "The calibration of scores relies on a reference set."

### Mechanism 3
- Claim: Internal model-based detectors can outperform external detectors when effectively aggregated.
- Mechanism: Internal features like attention maps and sequence log-probability capture unique patterns of hallucinations that, when combined, can surpass the performance of computationally expensive external models.
- Core assumption: Internal model features contain valuable information for hallucination detection that is not captured by external models.
- Evidence anchors:
  - [section 3.2] "Interestingly, we also observe that the best overall performance obtained exclusively with external models lags behind that of the overall aggregation. This suggests that internal models features — directly obtained via the generation process — contribute with complementary information to that captured by external models."
  - [section 3.2] "Aggregation of internal detectors, can achieve higher AUROC scores than the best single external detector on HALOMI."

## Foundational Learning

- Concept: Understanding of hallucination types in machine translation (oscillatory, fluent but detached, etc.)
  - Why needed here: Different detectors excel at different hallucination types, so understanding these distinctions is crucial for appreciating the aggregation approach.
  - Quick check question: What are the main categories of hallucinations in neural machine translation?

- Concept: Min-max normalization and weighted averaging
  - Why needed here: STARE uses these techniques to standardize and combine detector scores from different scales.
  - Quick check question: How does min-max normalization work, and why is it useful for combining scores from different detectors?

- Concept: Area Under the ROC Curve (AUROC) and False Positive Rate (FPR)
  - Why needed here: These are the primary evaluation metrics used to assess the performance of hallucination detection methods.
  - Quick check question: What do AUROC and FPR measure, and why are they important for evaluating hallucination detection?

## Architecture Onboarding

- Component map:
  - Individual hallucination detectors (external and internal) -> Reference set for normalization -> STARE aggregation method -> Evaluation metrics (AUROC, FPR)

- Critical path:
  1. Collect detection scores from individual detectors
  2. Normalize scores using reference set
  3. Aggregate normalized scores using weighted averaging
  4. Evaluate aggregated detector performance

- Design tradeoffs:
  - Using more detectors vs. computational cost
  - Size of reference set vs. normalization accuracy
  - Weighted averaging vs. other aggregation methods (e.g., max, isolation forest)

- Failure signatures:
  - Poor performance on specific hallucination types
  - High variance in results across different runs
  - Degradation in performance when adding new detectors

- First 3 experiments:
  1. Test STARE aggregation on a small subset of the data with 2-3 detectors
  2. Compare STARE performance with individual detectors on the full dataset
  3. Evaluate the impact of reference set size on STARE performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal reference set size for STARE calibration in different domains?
- Basis in paper: [explicit] The paper shows that performance improvement plateaus when the reference set exceeds 1,000 samples.
- Why unresolved: The paper only tests on one dataset (LFAN-H ALL) with 100k sentences, and doesn't explore how optimal size varies across different domains or hallucination detection tasks.
- What evidence would resolve it: Experiments testing STARE performance with varying reference set sizes on multiple datasets with different characteristics (e.g., different languages, different NMT models, different hallucination types).

### Open Question 2
- Question: How do STARE's performance gains compare to domain-adaptive versions of individual detectors?
- Basis in paper: [inferred] The paper shows STARE consistently outperforms individual detectors, but doesn't compare against detectors fine-tuned on the target domain.
- Why unresolved: The paper only tests on two datasets, and doesn't explore whether detectors specifically trained on the target domain could match or exceed STARE's performance.
- What evidence would resolve it: Experiments comparing STARE against domain-specific versions of individual detectors (e.g., CometKiwi fine-tuned on the target domain) on the same datasets.

### Open Question 3
- Question: What is the computational cost trade-off between STARE and more complex aggregation methods?
- Basis in paper: [explicit] The paper states STARE is "simple" and "straightforward," but doesn't compare its computational efficiency to more complex aggregation methods.
- Why unresolved: The paper doesn't provide runtime comparisons between STARE and more complex methods (e.g., Isolation Forest).
- What evidence would resolve it: Experiments measuring the time and memory requirements of STARE versus more complex aggregation methods on the same hardware.

## Limitations

- Limited ablation studies showing which specific detectors contribute most to performance gains
- No experiments across multiple language pairs to verify generalizability
- No analysis of computational efficiency trade-offs when adding more detectors

## Confidence

- STARE improves AUROC by up to 2.4 percentage points (High confidence)
- Internal model-based detectors can outperform external models when aggregated (Medium confidence)
- STARE's performance gains generalize across different domains (Low confidence)

## Next Checks

1. Conduct ablation studies to identify which specific detectors contribute most to performance improvements and test whether removing certain detectors significantly impacts results.

2. Evaluate STARE's performance across multiple language pairs and domains to verify generalizability beyond the English-German/French focus in the current experiments.

3. Test the sensitivity of STARE to different reference set sizes and distributions to establish robustness requirements for the normalization step.