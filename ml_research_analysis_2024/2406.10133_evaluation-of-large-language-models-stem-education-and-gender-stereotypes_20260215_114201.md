---
ver: rpa2
title: 'Evaluation of Large Language Models: STEM education and Gender Stereotypes'
arxiv_id: '2406.10133'
source_url: https://arxiv.org/abs/2406.10133
tags: []
core_contribution: The study investigates gender bias in Large Language Models (LLMs)
  by analyzing the STEM-related job suggestions made by ChatGPT when prompted with
  different names typically associated with boys and girls. The experiment involved
  four different languages (English, Danish, Catalan, and Hindi) and two age groups
  (10-15 years) to capture educational transitions.
---

# Evaluation of Large Language Models: STEM education and Gender Stereotypes

## Quick Facts
- arXiv ID: 2406.10133
- Source URL: https://arxiv.org/abs/2406.10133
- Reference count: 40
- Primary result: ChatGPT consistently provided more STEM job suggestions to names typically associated with boys than to names typically associated with girls across four languages

## Executive Summary
This study investigates gender bias in Large Language Models (LLMs) by analyzing STEM-related job recommendations made by ChatGPT when prompted with gendered names. The researchers examined how different names typically associated with boys and girls (across English, Danish, Catalan, and Hindi) influenced the model's suggestions for career paths. The experiment covered two age groups (10-15 years) to capture educational transition periods. Results revealed a consistent pattern where boys received significantly more STEM-related job suggestions than girls across all tested languages and age groups, highlighting how LLMs can perpetuate gender stereotypes in educational guidance.

## Method Summary
The study employed a name-based prompt methodology where researchers presented ChatGPT with fictional child profiles containing gendered names and asked for career suggestions. The experiment tested 16 names per age group (8 typically male, 8 typically female) across four languages: English, Danish, Catalan, and Hindi. For each prompt, researchers counted the number of STEM-related suggestions provided by the model. The analysis compared STEM suggestion counts between male-associated and female-associated names, examining patterns across languages and age groups to identify systematic gender bias in the model's educational recommendations.

## Key Results
- ChatGPT provided significantly more STEM-related job suggestions to names typically associated with boys than to names typically associated with girls across all four tested languages
- The gender bias in STEM recommendations was consistent across both age groups (10-15 years), suggesting the bias affects career guidance during critical educational transition periods
- The bias pattern persisted regardless of the language used in prompts, indicating a robust tendency in the model to associate boys with STEM fields more strongly than girls

## Why This Works (Mechanism)
Not applicable - the study focuses on documenting bias rather than explaining underlying mechanisms.

## Foundational Learning

**Gender bias measurement in AI systems**: Understanding how to detect and quantify bias in machine learning models is essential for evaluating fairness in automated decision-making. Quick check: Can identify common metrics used to measure bias in language models.

**Cross-cultural naming conventions**: Knowledge of how names vary across cultures and their typical gender associations is crucial for designing valid bias experiments. Quick check: Can explain why name-based gender proxies are used in bias research and their limitations.

**STEM education pipeline**: Understanding the relationship between early career guidance and later educational choices helps contextualize the significance of biased recommendations. Quick check: Can describe how early exposure to career options influences educational trajectories.

## Architecture Onboarding

**Component map**: ChatGPT (LLM) -> Name-based prompts -> Career suggestion generation -> STEM count analysis -> Bias quantification

**Critical path**: The experiment's critical path involves the interaction between gendered names in prompts and the model's response generation, where the model's training data influences how it associates names with career domains.

**Design tradeoffs**: The study uses names as gender proxies for experimental control, trading off the nuance of individual identity for systematic bias measurement across populations.

**Failure signatures**: Bias manifests as systematic over-representation of STEM suggestions for male-associated names and under-representation for female-associated names, regardless of language or age context.

**First experiments**: 1) Test prompts with gender-neutral names to establish baseline STEM suggestion rates; 2) Vary the specific wording of career guidance prompts to check for prompt sensitivity; 3) Analyze non-STEM suggestions to understand if bias extends beyond STEM fields.

## Open Questions the Paper Calls Out
None identified in the source material.

## Limitations
- The study uses a limited sample of 16 names per age group, which may not fully represent naming diversity across cultures
- Name-based gender proxies introduce uncertainty as name-gender associations can vary by region and time period
- The analysis focuses exclusively on STEM-related suggestions, potentially missing biases in other educational domains

## Confidence

**Claims about the existence and magnitude of gender bias in STEM recommendations**: Medium confidence - consistent patterns observed across languages and age groups, but limited sample size and single model constrain generalizability.

**Claims about the impact on children's educational choices**: Low confidence - supported by related literature but not directly tested in this study.

## Next Checks

1. Replicate the study using a larger, more diverse set of names (minimum 50 per age group) across the same four languages
2. Test additional LLM models beyond ChatGPT to assess whether the observed bias patterns are model-specific or widespread
3. Conduct qualitative analysis of the specific job suggestions to understand whether bias manifests through job type selection, prestige associations, or other linguistic patterns