---
ver: rpa2
title: 'Monkey See, Monkey Do: Harnessing Self-attention in Motion Diffusion for Zero-shot
  Motion Transfer'
arxiv_id: '2406.06508'
source_url: https://arxiv.org/abs/2406.06508
tags:
- motion
- follower
- leader
- motions
- diffusion
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces MoMo, a zero-shot motion transfer method that
  leverages self-attention in pre-trained motion diffusion models. It transfers the
  outline of a leader motion to a follower while preserving the follower's subtle
  motifs, enabling tasks like style transfer, action transfer, and out-of-distribution
  synthesis.
---

# Monkey See, Monkey Do: Harnessing Self-attention in Motion Diffusion for Zero-shot Motion Transfer

## Quick Facts
- arXiv ID: 2406.06508
- Source URL: https://arxiv.org/abs/2406.06508
- Authors: Sigal Raab; Inbar Gat; Nathan Sala; Guy Tevet; Rotem Shalev-Arkushin; Ohad Fried; Amit H. Bermano; Daniel Cohen-Or
- Reference count: 17
- Primary result: Zero-shot motion transfer method that transfers leader motion outline to follower while preserving follower motifs

## Executive Summary
MoMo introduces a novel zero-shot motion transfer approach that leverages self-attention in pre-trained motion diffusion models. By manipulating attention layers to combine leader motion queries with follower motion keys and values, MoMo transfers the outline of a leader motion to a follower while preserving the follower's subtle motifs. The method operates entirely at inference time without requiring additional training, and supports both real and generated motions through motion inversion. Experiments on a new benchmark demonstrate MoMo outperforms state-of-the-art methods across multiple transfer tasks.

## Method Summary
MoMo uses self-attention manipulation in pre-trained motion diffusion models to transfer motion patterns between leader and follower sequences. The method extracts queries (Q) from the leader motion and keys (K) and values (V) from the follower motion, then applies mixed-attention where leader queries attend to follower keys and values. This transfers the leader's motion outline while preserving the follower's motifs. The approach uses DDIM inversion to enable editing of both real and generated motions by transforming them into the latent space of the diffusion model. Motion transfer is achieved through nearest-neighbor matching in feature space, where each leader frame is replaced by its semantically similar follower frame based on attention scores.

## Key Results
- Achieves FID score of 2.33 on motion transfer benchmark
- Achieves R-precision of 0.439, outperforming state-of-the-art methods
- Successfully transfers motion outline while preserving follower motifs across style transfer, action transfer, and out-of-distribution synthesis tasks
- Works with both real and generated motions through motion inversion

## Why This Works (Mechanism)

### Mechanism 1
- Q vectors capture motion outline (temporal structure and locomotion phases) while K vectors capture motion motifs (subtle nuances and gestures)
- K-Means clustering of Q features shows periodic sub-motions like steps, while K clustering groups similar motifs
- Assumes Q and K encode complementary semantic aspects of motion
- Evidence: Clustering visualizations show Q dominated by outline and K by motifs
- Break condition: If Q and K encode overlapping information rather than complementary aspects

### Mechanism 2
- Attention mechanism creates implicit semantic correspondence between leader and follower motions
- For each leader frame, highest attention score identifies most semantically similar follower frame
- Assumes self-attention learns to associate similar motion patterns across different motions
- Evidence: Attention-based nearest neighbor matching shows consistent semantic alignment
- Break condition: If attention mechanism doesn't learn semantic correspondences

### Mechanism 3
- Motion inversion enables editing of both real and generated motions
- DDIM inversion converts real motions into noise space used by diffusion models
- Assumes inversion preserves sufficient information while enabling diffusion editing
- Evidence: Method description and experimental results with real motions
- Break condition: If inversion loses critical motion information or introduces artifacts

## Foundational Learning

- **Self-attention mechanism in transformers**: Why needed - MoMo relies on manipulating self-attention layers to transfer motion patterns. Quick check - How does self-attention compute relevance scores between query and key vectors?

- **Diffusion probabilistic models and DDIM**: Why needed - Framework uses pre-trained diffusion models and DDIM for inversion and deterministic inference. Quick check - What's the difference between DDPM and DDIM in terms of noise scheduling and determinism?

- **Motion representation in HumanML3D**: Why needed - Understanding feature structure (joint positions, velocities, rotations, foot contacts) is crucial for implementing MoMo correctly. Quick check - How are the 3D skeletal joint features structured and concatenated in the HumanML3D motion representation?

## Architecture Onboarding

- **Component map**: Inversion → Mixed-attention block → Denoising → Output
- **Critical path**: Inversion → Mixed-attention application → Denoising → Output
- **Design tradeoffs**:
  - DDIM (deterministic) vs DDPM (stochastic) for inversion: DDIM enables precise reconstruction but may reduce output diversity
  - Layer selection (2-12) and timestep selection (90-10): Earlier layers capture more abstract features; later timesteps refine details
  - Text prompt strategy: Using follower prompt vs leader prompt vs none affects output adherence to follower motifs
- **Failure signatures**:
  - Poor FID scores: Mixed-attention not properly aligned or insufficient training of backbone
  - Jittery output: Nearest neighbor approaches in feature space without attention weighting
  - Loss of follower motifs: Incorrect layer/timestep selection for mixed-attention application
  - Incorrect locomotion direction: Followers not properly rotated or diversified before concatenation
- **First 3 experiments**:
  1. Verify self-attention Q/K separation by running K-Means clustering on extracted features and checking if Q clusters by outline and K by motifs
  2. Test attention-based nearest neighbor matching by computing Q ldr · K flw^T and verifying semantic alignment visually
  3. Implement mixed-attention on a single layer/timestep and check if outline transfers while preserving follower characteristics on a simple motion pair

## Open Questions the Paper Calls Out

- **Open Question 1**: What specific latent feature layers, beyond self-attention, would be most beneficial to explore for improving motion transfer? The authors mention exploring cross-attention and feedforward layers as future direction, but don't experimentally compare them to self-attention manipulation.

- **Open Question 2**: How does the performance of MoMo compare when using non-deterministic diffusion models instead of the deterministic DDIM approach? The current implementation uses DDIM limiting output diversity, but the impact of switching to DDPM is unexplored.

- **Open Question 3**: Can MoMo be extended to handle motion transfer between motions with significantly different structures or ontologies? The method struggles when basic outline elements of the leader motion are missing in the follower, but extreme cases aren't tested.

- **Open Question 4**: How does the choice of attention layer and diffusion step range affect the quality of motion transfer? While an ablation study provides some insights, the optimal configuration may depend on specific motion characteristics.

## Limitations
- Experimental validation relies heavily on authors' newly created evaluation dataset, raising questions about generalizability
- Ablation studies don't fully explore sensitivity to different motion diffusion model architectures or inversion methods
- Computational efficiency and runtime implications of attention-based nearest neighbor search aren't addressed

## Confidence
- **Mechanism 1**: Medium - Theoretical framework is well-articulated with clustering visualizations, but could benefit from more rigorous quantitative validation
- **Mechanism 2**: Medium - Semantic correspondence through attention scores is supported by qualitative examples but needs quantitative metrics
- **Mechanism 3**: Medium - Motion inversion component is clearly described but fidelity to original motions isn't thoroughly characterized

## Next Checks
1. Conduct K-Means clustering experiments on Q and K features from multiple motion pairs to verify consistent separation of outline vs motif encoding across diverse motion types
2. Implement quantitative metrics for semantic alignment quality beyond visual inspection, such as cross-reconstruction error between attention-matched frames
3. Test MoMo's performance on established motion datasets (e.g., AMASS, HumanAct12) to validate generalizability beyond the proposed benchmark