---
ver: rpa2
title: Language Guided Skill Discovery
arxiv_id: '2406.06615'
source_url: https://arxiv.org/abs/2406.06615
tags: []
core_contribution: The paper introduces Language Guided Skill Discovery (LGSD), a
  method that uses large language models (LLMs) to discover semantically diverse skills
  in unsupervised reinforcement learning. The core idea is to employ LLMs to generate
  natural language descriptions of agent states, measure semantic differences using
  language embeddings, and maximize these differences to encourage diverse behaviors.
---

# Language Guided Skill Discovery

## Quick Facts
- arXiv ID: 2406.06615
- Source URL: https://arxiv.org/abs/2406.06615
- Reference count: 40
- Primary result: Language-guided skill discovery using LLMs outperforms five baselines on locomotion and manipulation tasks in terms of diversity and sample efficiency

## Executive Summary
Language Guided Skill Discovery (LGSD) introduces a novel approach to unsupervised skill discovery in reinforcement learning by leveraging large language models (LLMs) to generate semantically meaningful state descriptions. The method uses language embeddings to measure semantic differences between states, guiding the agent to explore diverse behaviors within semantically constrained subspaces. Experiments demonstrate that LGSD discovers more diverse skills compared to existing methods while enabling zero-shot goal following through natural language instructions.

## Method Summary
LGSD uses LLMs to generate natural language descriptions of agent states, then employs Sentence-Transformer embeddings to measure semantic differences between these descriptions. The method trains a skill-conditioned policy using the Wasserstein Dependency Measure framework, where the language-distance metric replaces traditional state-distance metrics. A representation function is constrained to be 1-Lipschitz under the language-distance metric, and a separate skill inference network enables zero-shot goal following by mapping language instructions to skill vectors. The approach is evaluated on locomotion and manipulation tasks, demonstrating superior skill diversity and sample efficiency compared to five baseline methods.

## Key Results
- LGSD outperforms five baseline skill discovery methods on both locomotion and manipulation tasks
- Achieves better state coverage and object manipulation distance compared to baselines
- Enables zero-shot goal following through natural language instructions
- Demonstrates improved sample efficiency with language guidance

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Language embeddings from LLMs can serve as a valid proxy for semantic distance between states
- Mechanism: LGSD uses Sentence-Transformer to map LLM-generated descriptions into vectors, using cosine similarity as the "language-distance" metric. This metric enforces 1-Lipschitz continuity on the representation function, preserving semantic differences in the latent space.
- Core assumption: LLM descriptions capture semantically meaningful aspects of states, and Sentence-Transformer embeddings preserve these semantics
- Evidence anchors: [abstract] mentions using Sentence-Transformer for mapping descriptions to vectors; [section 4.3] defines language distance using cosine similarity
- Break condition: If LLM fails to generate meaningful descriptions or embeddings don't preserve semantic similarity, language-distance won't be valid

### Mechanism 2
- Claim: Constraining skill space with prompts enables targeted exploration of semantically meaningful behaviors
- Mechanism: Language prompts guide LLM descriptions to focus on specific semantic aspects, constraining exploration to desired subspaces. The agent learns to maximize language-distance within these constrained spaces.
- Core assumption: LLMs can be reliably controlled through prompts, and agents can learn to maximize language-distance in constrained spaces
- Evidence anchors: [abstract] discusses prompts constraining search space; [section 4.2] mentions adapting focus through different prompt types
- Break condition: If LLM responses aren't controllable through prompts or agent can't maximize language-distance in constrained space

### Mechanism 3
- Claim: Language-guided skill discovery leads to more diverse and semantically meaningful skills
- Mechanism: Using language-distance in WDM framework encourages exploration of semantically diverse states, resulting in more meaningful skills than methods using Euclidean distance or mutual information.
- Core assumption: Semantic diversity better measures skill diversity than state coverage or discriminator distinguishability
- Evidence anchors: [abstract] claims LGSD outperforms baselines in diversity and sample efficiency; [section 5.2] shows improved object manipulation
- Break condition: If semantic diversity doesn't correlate with task performance or LLM descriptions don't capture relevant semantics

## Foundational Learning

- Concept: Markov Decision Processes (MDPs)
  - Why needed here: Skill discovery formalized within MDP framework where agent learns policy to maximize intrinsic reward without explicit task rewards
  - Quick check question: What are the components of an MDP and how do they relate to skill discovery problem?

- Concept: Mutual Information and Wasserstein Dependency Measure
  - Why needed here: Existing skill discovery uses mutual information or WDM to encourage diversity; LGSD adapts WDM framework using language-distance metric
  - Quick check question: How do mutual information and WDM differ in approach to encouraging skill diversity?

- Concept: Large Language Models (LLMs) and Natural Language Embeddings
  - Why needed here: LLMs generate semantic descriptions of states, and embeddings measure semantic distance between these descriptions
  - Quick check question: How do LLMs generate semantic descriptions of states, and how do embeddings preserve semantic similarity?

## Architecture Onboarding

- Component map: LLM -> Sentence-Transformer -> dlang -> ϕ constraint -> RL reward -> policy update -> skill discovery
- Critical path: LLM generates descriptions → Sentence-Transformer creates embeddings → language-distance computed → representation function constrained → RL reward formulation → policy optimization → skill discovery
- Design tradeoffs:
  - LLM choice: More powerful LLM may generate better descriptions but increases computational cost
  - Embedding model: Different models may capture semantics differently, affecting language-distance validity
  - Prompt engineering: Crucial for constraining skill space but requires domain knowledge
- Failure signatures:
  - Inconsistent or irrelevant LLM descriptions → invalid language-distance metric
  - Policy fails to maximize language-distance → non-diverse skills
  - Improperly trained skill inference network → failed zero-shot goal following
- First 3 experiments:
  1. Validate language-distance as proxy for semantic distance by comparing to human judgment on state sets
  2. Test effect of different prompts on skill space by training with various prompts and analyzing resulting skills
  3. Compare LGSD to existing methods on simple manipulation task to verify discovery of diverse, meaningful skills

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How robust is LGSD performance to variations in prompts used to guide LLM?
- Basis in paper: [inferred] Paper mentions using different prompts but doesn't explore impact of prompt variations on skill diversity and quality
- Why unresolved: Experiments use specific prompts without systematic exploration of prompt sensitivity
- What evidence would resolve it: Systematic experiments varying prompt wording, structure, and detail while measuring skill diversity and task performance

### Open Question 2
- Question: How does LGSD performance scale with complexity and dimensionality of state space?
- Basis in paper: [inferred] Paper demonstrates on relatively simple environments; unclear how well approach generalizes to more complex tasks
- Why unresolved: Doesn't explore performance on tasks with significantly more complex state representations
- What evidence would resolve it: Experiments applying LGSD to more challenging environments with higher-dimensional state spaces

### Open Question 3
- Question: How does choice of language embedding model affect quality of learned skills?
- Basis in paper: [explicit] Mentions using Sentence-Transformer but doesn't explore impact of different embedding models
- Why unresolved: Doesn't compare performance using various language embedding models or analyze impact on semantic diversity
- What evidence would resolve it: Experiments comparing LGSD using various embedding models and analyzing resulting skill diversity

## Limitations

- Reliance on LLM-generated descriptions introduces uncertainty in semantic quality and consistency
- Performance sensitivity to prompt engineering and LLM capabilities not thoroughly explored
- Limited evaluation of zero-shot instruction following across diverse instruction types

## Confidence

- **High Confidence:** Core algorithmic framework combining WDM with language-distance metrics is technically sound
- **Medium Confidence:** Superior skill diversity claim supported by experiments but relies on specific evaluation metrics
- **Low Confidence:** Zero-shot language instruction following demonstrated but not thoroughly evaluated against established methods

## Next Checks

1. **Semantic Distance Validation:** Conduct human evaluation study comparing language-distance rankings against human judgments of state similarity
2. **Prompt Sensitivity Analysis:** Systematically vary prompt templates and LLM parameters to quantify impact on skill diversity
3. **Cross-Domain Generalization:** Test LGSD on environments with substantially different state spaces to assess robustness