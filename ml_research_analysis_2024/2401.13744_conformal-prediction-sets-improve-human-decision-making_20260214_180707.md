---
ver: rpa2
title: Conformal Prediction Sets Improve Human Decision Making
arxiv_id: '2401.13744'
source_url: https://arxiv.org/abs/2401.13744
tags:
- conformal
- sets
- prediction
- human
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study scientifically evaluates whether conformal prediction
  sets improve human decision-making accuracy compared to traditional top-k sets.
  Through a pre-registered randomized controlled trial with 600 participants across
  three classification tasks (Image Classification, Sentiment Analysis, and Named
  Entity Recognition), the research demonstrates that humans achieve statistically
  significant higher accuracy when provided with conformal prediction sets compared
  to both control and top-k sets (p < 0.05).
---

# Conformal Prediction Sets Improve Human Decision Making

## Quick Facts
- arXiv ID: 2401.13744
- Source URL: https://arxiv.org/abs/2401.13744
- Reference count: 40
- Primary result: Conformal prediction sets improve human decision-making accuracy compared to traditional top-k sets

## Executive Summary
This study provides the first empirical evidence that conformal prediction sets improve human decision-making accuracy compared to traditional top-k sets. Through a pre-registered randomized controlled trial with 600 crowdworker participants across three classification tasks (Image Classification, Sentiment Analysis, and Named Entity Recognition), the research demonstrates that humans achieve statistically significant higher accuracy when provided with conformal prediction sets. The key insight is that conformal sets' variable sizes better quantify model uncertainty, helping humans make more informed decisions.

## Method Summary
The study employed a pre-registered randomized controlled trial design with 600 crowdworker participants. Participants were randomly assigned to view either conformal prediction sets, top-k prediction sets, or no sets (control) across three classification tasks. The conformal prediction sets were generated using split-conformal methods to achieve 90% coverage, while top-k sets were fixed-size. Human accuracy was measured across Image Classification (cats vs dogs), Sentiment Analysis, and Named Entity Recognition tasks. The study used statistical analysis to compare performance between conditions and examined how set size and model certainty influenced human decision-making.

## Key Results
- Humans achieved statistically significant higher accuracy with conformal prediction sets compared to both control and top-k sets (p < 0.05)
- Conformal sets most improve human decision-making when models express high certainty through singleton sets
- Humans adopt answers from prediction sets at rates matching the stated coverage guarantees

## Why This Works (Mechanism)
Conformal prediction sets work by providing a variable-size set of plausible answers that maintains a guaranteed coverage rate (e.g., 90% of the time the correct answer is in the set). Unlike fixed-size top-k sets, conformal sets can be smaller when the model is confident and larger when uncertain. This variability provides better information about the model's certainty level, allowing humans to make more informed decisions. When models are highly confident, conformal sets often contain only the correct answer, enabling humans to trust the prediction. When uncertain, larger sets signal humans to be more cautious.

## Foundational Learning
1. **Conformal Prediction**: A framework for quantifying uncertainty in predictions by producing sets that contain the correct answer with guaranteed probability. Needed to understand how conformal sets differ from traditional predictions. Quick check: Can you explain how split-conformal methods work?
2. **Top-k Predictions**: Fixed-size sets of k most likely predictions, commonly used in classification tasks. Needed as the baseline comparison for conformal sets. Quick check: What's the difference between top-1, top-3, and top-5 predictions?
3. **Coverage Guarantee**: The probability that the correct answer is contained in the prediction set (e.g., 90% coverage means the correct answer is in the set 90% of the time). Needed to understand the reliability of prediction sets. Quick check: How is coverage guarantee different from accuracy?

## Architecture Onboarding

Component Map:
Pre-trained Model -> Conformal Prediction Set Generator -> Human Decision Maker

Critical Path:
Pre-trained model makes predictions → Conformal set generation algorithm creates prediction set → Human observes set and makes decision → Decision accuracy is measured

Design Tradeoffs:
- Fixed-size vs variable-size prediction sets: Conformal sets provide better uncertainty quantification but may be larger on average
- Coverage rate selection: Higher coverage provides more reliability but produces larger sets
- Set presentation: How prediction sets are displayed to humans can significantly impact decision-making

Failure Signatures:
- Conformal sets consistently larger than top-k sets may indicate model uncertainty or poor calibration
- Humans ignoring prediction sets entirely suggests poor presentation or lack of trust
- Coverage guarantees not matching actual performance indicates issues with the conformal method

First Experiments:
1. Compare human accuracy using conformal vs top-k sets on a simple binary classification task
2. Measure the average size of conformal sets vs top-k sets across different confidence levels
3. Test different presentation formats for prediction sets (list vs grid vs other visualizations)

## Open Questions the Paper Calls Out
The study acknowledges several open questions, including whether these findings generalize to domain experts making consequential decisions in high-stakes domains like healthcare or finance. The effectiveness of conformal sets for non-classification tasks such as regression or structured prediction remains unexplored. Additionally, the study raises questions about how different presentation formats might further improve human decision-making with prediction sets.

## Limitations
- Findings based on crowdworker participants rather than domain experts, limiting generalizability to expert decision-making
- Tasks were relatively straightforward classification problems, potentially limiting applicability to more complex decision contexts
- Study focused exclusively on classification tasks, leaving effectiveness for other problem types unexplored

## Confidence
- Conformal sets improve human accuracy vs top-k: High confidence (supported by pre-registered RCT with 600 participants, p < 0.05 significance)
- Conformal sets better quantify uncertainty: Medium confidence (theoretical property well-established, but human perception evidence limited)
- Improvement most pronounced when models express high certainty: Medium confidence (observed pattern needs replication in different contexts)

## Next Checks
1. Replicate findings with domain experts making consequential decisions in high-stakes domains like healthcare or finance
2. Test effectiveness across diverse task types beyond simple classification, including regression and structured prediction problems
3. Investigate how different presentation formats of conformal sets affect human decision-making and whether alternative visualizations could further improve outcomes