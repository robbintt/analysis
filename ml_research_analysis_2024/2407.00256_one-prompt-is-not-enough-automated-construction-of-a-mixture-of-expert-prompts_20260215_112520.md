---
ver: rpa2
title: 'One Prompt is not Enough: Automated Construction of a Mixture-of-Expert Prompts'
arxiv_id: '2407.00256'
source_url: https://arxiv.org/abs/2407.00256
tags:
- demos
- each
- prompt
- instruction
- expert
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes Mixture-of-Prompts (MoP), a method that extends
  automatic prompt optimization from a single demo-free instruction to a mixture of
  experts, where each expert is a prompt containing both an instruction and a set
  of demos. Inspired by the theoretical connection between in-context learning and
  kernel regression, MoP clusters demos into semantically similar groups and jointly
  optimizes an instruction for each cluster.
---

# One Prompt is not Enough: Automated Construction of a Mixture-of-Expert Prompts

## Quick Facts
- **arXiv ID**: 2407.00256
- **Source URL**: https://arxiv.org/abs/2407.00256
- **Reference count**: 40
- **One-line primary result**: MoP achieves 81% average win rate against six prior prompt optimization methods across three major benchmarks

## Executive Summary
This paper introduces Mixture-of-Prompts (MoP), a method that extends automatic prompt optimization from single demo-free instructions to a mixture of expert prompts. Each expert contains both an instruction and a set of semantically similar demos, clustered using kernel regression theory. The method jointly optimizes instructions to complement demo clusters, with routing based on semantic similarity in embedding space. MoP demonstrates substantial improvements by covering blind spots through instruction-complementary expert regions.

## Method Summary
MoP clusters training demos into C experts based on semantic similarity using K-means in embedding space, then searches for instructions complementary to each cluster using region-based joint search (RBJS). The RBJS algorithm generates candidate instructions using demos from other experts to compensate for local information gaps, evaluating them on region-specific validation subsets rather than the full set. At inference, test inputs are routed to the nearest expert in embedding space and executed with the assigned prompt. The method leverages the theoretical connection between in-context learning and kernel regression to justify demo clustering and routing strategies.

## Key Results
- MoP achieves an average win rate of 81% against six prior prompt optimization methods
- The method shows consistent improvement across three major benchmarks: Instruction Induction, Super Natural Instructions, and BIG-Bench-Hard
- Ablation studies confirm both instructions and demos are necessary for optimal performance

## Why This Works (Mechanism)

### Mechanism 1
- Demo assignment based on semantic clustering improves routing accuracy
- Demos are embedded and clustered into experts using K-means; test inputs routed to expert with closest demos in embedding space
- Core assumption: Demos semantically similar to test input contribute more to correct prediction per kernel regression theory
- Evidence anchors: Theoretical connection between in-context learning and kernel regression; FMR score 0.59 for HMVLM indicates corpus relevance to MoE paradigms
- Break condition: If embedding space poorly reflects semantic similarity, routing will fail

### Mechanism 2
- Instructions complement demos to cover blind spots in each expert region
- For each demo cluster, instructions generated using demos from other experts compensate for local information gaps
- Core assumption: Each expert develops blind spots based on local demos requiring distinct instructions for compensation
- Evidence anchors: Instructions provide general abilities and high-level explanations; survey on prompt optimization shows active research on instruction-focused search algorithms
- Break condition: If instructions generated without considering demo-blind spots, complementarity fails

### Mechanism 3
- Region-based joint search outperforms global evaluation for prompt selection
- Validation performance evaluated per expert region rather than full validation set, ensuring instructions match local demo distributions
- Core assumption: Expert performance on local vs. full data distributions are misaligned, requiring region-specific validation
- Evidence anchors: Correlation analysis shows misalignment between global and region-based validation rankings; sequential optimal learning research suggests optimal evaluation strategies
- Break condition: If local and global validation rankings align, region-based evaluation provides no advantage

## Foundational Learning

- **Concept**: Kernel Regression connection to In-Context Learning
  - Why needed here: Provides theoretical justification for demo clustering based on semantic similarity
  - Quick check question: Why would demos semantically closer to a test query contribute more to its prediction under kernel regression view?

- **Concept**: Mixture of Experts (MoE) paradigm
  - Why needed here: Framework for partitioning problem space into specialized regions governed by expert prompts
  - Quick check question: How does treating prompts as experts differ from traditional MoE with distinct model parameters?

- **Concept**: Region-based evaluation vs global evaluation
  - Why needed here: Ensures instructions are optimized for local expert regions rather than entire problem space
  - Quick check question: What misalignment occurs when evaluating instructions on full validation vs region-specific subsets?

## Architecture Onboarding

- **Component map**: Demo embedding → K-means clustering → Demo clusters → Candidate instruction generation → Region-based joint search → Instruction selection → Expert prompt (instruction + demos) → Test input embedding → Nearest expert routing → Prompt execution → Output

- **Critical path**: 1) Cluster training demos into C experts using embedding space K-means; 2) For each expert: generate instructions using other experts' demos; 3) Jointly evaluate instructions on region-specific validation subsets; 4) At inference: route query to nearest expert and execute its prompt

- **Design tradeoffs**: Number of experts (more provide finer specialization but increase search complexity); embedding model choice (stronger encoders improve clustering but add cost); demo-to-instruction ratio (more demos reduce instruction importance but increase routing accuracy)

- **Failure signatures**: Poor routing accuracy (demos poorly clustered); similar expert performance (instructions not compensating for demo blind spots); inconsistent validation (region-based vs global evaluation misalignment)

- **First 3 experiments**: 1) Visualize demo clustering in embedding space to verify semantic grouping patterns; 2) Measure hit ratio distribution across experts to confirm different specialties; 3) Compare instruction rankings between global and region-based validation to validate RBJS necessity

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal number of experts for MoP in different task domains?
- Basis in paper: The paper mentions that "more experts are not necessarily better: there exists an optimal number of partitions for the problem space"
- Why unresolved: The paper does not provide a systematic method to determine the optimal number of experts for different task domains or problem complexities
- What evidence would resolve it: A comprehensive study across diverse task domains, varying the number of experts systematically and measuring performance to identify patterns or rules for determining optimal expert counts

### Open Question 2
- Question: How does MoP perform with different embedding models beyond text-embedding-ada-002?
- Basis in paper: The paper mentions that "MoP operates reasonably well with paired with GPT2-Large" but does not extensively explore other embedding models
- Why unresolved: The paper only briefly touches on the impact of different embedding models in the ablation study, without a comprehensive comparison
- What evidence would resolve it: A thorough comparison of MoP's performance using various state-of-the-art embedding models, measuring the impact on task performance and expert assignment quality

### Open Question 3
- Question: Can MoP be extended to handle continuous or non-discrete problem spaces?
- Basis in paper: The current MoP framework relies on clustering demos in a discrete embedding space, suggesting potential limitations for continuous problem spaces
- Why unresolved: The paper focuses on discrete clustering and routing, without addressing how MoP might adapt to continuous or more fluid problem spaces
- What evidence would resolve it: Development and testing of MoP variants that can handle continuous input spaces, potentially using techniques like kernel density estimation or Gaussian mixture models for expert assignment

## Limitations
- Empirical validation scope limited to instruction-following and reasoning tasks, with uncertainty about specialized domains like code generation
- Semantic clustering assumes embeddings capture task-relevant similarity, which may fail for cross-domain knowledge transfer
- RBJS algorithm computational overhead scales poorly with number of experts and candidate instructions

## Confidence

- **High confidence**: Theoretical connection between kernel regression and demo clustering (supported by Theorem 4.1 and explicit mathematical derivation)
- **Medium confidence**: Complementarity mechanism between instructions and demos (supported by ablation studies showing performance drops when removing either component)
- **Medium confidence**: Region-based joint search advantage (supported by correlation analysis showing misalignment between global and local validation rankings)

## Next Checks

1. **Embedding space validation**: Test whether MoP's routing accuracy degrades when using sentence transformers vs. task-specific embeddings, confirming whether semantic clustering depends on embedding quality
2. **Scaling analysis**: Measure MoP's runtime and memory usage as the number of experts increases from 2 to 16, quantifying the computational tradeoff against performance gains
3. **Cross-domain robustness**: Apply MoP to a code generation benchmark (e.g., HumanEval) to test whether the instruction-demo complementarity extends beyond natural language tasks