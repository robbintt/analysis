---
ver: rpa2
title: 'Conformal Alignment: Knowing When to Trust Foundation Models with Guarantees'
arxiv_id: '2405.10301'
source_url: https://arxiv.org/abs/2405.10301
tags:
- power
- alignment
- conformal
- level
- target
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: Conformal Alignment addresses the challenge of ensuring foundation
  model outputs meet alignment criteria with statistical guarantees. The method leverages
  reference data to train an alignment predictor, then selects outputs whose predicted
  alignment scores exceed a calibrated threshold.
---

# Conformal Alignment: Knowing When to Trust Foundation Models with Guarantees

## Quick Facts
- **arXiv ID**: 2405.10301
- **Source URL**: https://arxiv.org/abs/2405.10301
- **Reference count**: 40
- **Primary result**: Guarantees FDR control for foundation model output selection using conformalized selection with minimal reference data

## Executive Summary
Conformal Alignment is a statistical framework that ensures foundation model outputs meet alignment criteria with provable guarantees. The method addresses the challenge of deploying trustworthy outputs from large-scale pre-trained models in high-stakes applications by selecting outputs whose predicted alignment scores exceed a calibrated threshold. By leveraging conformal prediction theory and the Benjamini-Hochberg procedure, it guarantees that a prescribed fraction of selected units are truly aligned, regardless of the foundation model or data distribution. The approach is demonstrated on question answering and radiology report generation tasks, showing effective FDR control while maintaining high power with only a few hundred high-quality reference samples.

## Method Summary
The method works by first training an alignment predictor on a reference dataset with ground truth alignment labels. For test units, it computes predicted alignment scores and uses conformal p-value computation with the calibration set to determine statistical confidence. The Benjamini-Hochberg procedure then selects units whose p-values fall below the target FDR threshold. This approach differs from standard conformal prediction by selecting among already-generated outputs rather than modifying them, preserving the informativeness of original outputs while providing statistical guarantees. The framework requires splitting reference data into training and calibration sets, training the alignment predictor, and applying the selection procedure to achieve finite-sample, distribution-free FDR control.

## Key Results
- Achieves FDR control at target levels (α = 0.2) on both QA and radiology tasks
- Maintains high power (up to 0.7) with moderate reference data (few hundred samples)
- Requires minimal adaptation across different foundation models and tasks
- Demonstrates tight control of false discoveries while preserving output informativeness

## Why This Works (Mechanism)

### Mechanism 1: Conformalized Selection Framework
- Claim: Conformal alignment achieves finite-sample, distribution-free FDR control for selecting trustworthy foundation model outputs
- Mechanism: Uses conformalized selection framework with hypothesis testing to compute conformal p-values, then applies Benjamini-Hochberg procedure to control FDR at target level
- Core assumption: Exchangeability between calibration and test units, and no ties in predicted alignment scores
- Evidence anchors:
  - [abstract]: "It is guaranteed that on average, a prescribed fraction of selected units indeed meet the alignment criterion, regardless of the foundation model or the data distribution"
  - [section 3]: "Theorem 3.1. Suppose that for any j ∈ [m], {Zn+j} ∪ { Zi}i∈Dcal are exchangeable conditional on {Zn+ℓ}ℓ̸=j... Then for any α ∈ (0, 1), the outputS from Algorithm 1 satisfies FDR≤ α"
  - [corpus]: Weak - no direct corpus evidence for FDR control in foundation model alignment, though conformal prediction is well-established in other domains
- Break condition: Exchangeability assumption fails (e.g., calibration and test data come from different distributions) or ties occur in predicted alignment scores

### Mechanism 2: Alignment Predictor Power
- Claim: The alignment predictor's power determines the fraction of truly aligned outputs that can be selected
- Mechanism: Alignment predictor learns to discriminate between aligned and non-aligned outputs; better predictors yield higher power for given FDR constraint
- Core assumption: Alignment predictor can be trained effectively with moderate reference data to distinguish aligned from non-aligned outputs
- Evidence anchors:
  - [abstract]: "we demonstrate that our method is able to accurately identify units with trustworthy outputs via lightweight training over a moderate amount of reference data"
  - [section 3]: "The area of the blue part on the right of the cutoff is thus the asymptotic fraction of selected units, whose proportion among the entire blue area is the asymptotic power"
  - [section 4]: "What feature is informative for alignment?" section shows different features yield different power levels
- Break condition: Insufficient signal in features to distinguish aligned from non-aligned outputs, or inadequate training data to learn predictor

### Mechanism 3: Output Selection vs. Modification
- Claim: The method preserves informativeness of original outputs while providing statistical guarantees
- Mechanism: Instead of modifying outputs to ensure alignment, the method selects which outputs to deploy based on statistical confidence
- Core assumption: Users can accept abstaining from deploying some outputs rather than modifying them
- Evidence anchors:
  - [abstract]: "by selecting rather than modifying outputs, our approach preserves the (3) informativeness of the original outputs"
  - [introduction]: "Unlike standard CP that searches the sampling space to construct a prediction set that contains a desired output, we leverage the quantified confidence as an instrument to select units whose already-generated outputs are trusted"
  - [corpus]: Weak - this is a novel contribution distinguishing this work from prior approaches like [32] that modify outputs
- Break condition: Users require all outputs to be deployed regardless of alignment confidence, or cannot tolerate abstaining from deployment

## Foundational Learning

- **Concept**: Conformal prediction and its variants
  - Why needed here: The method builds directly on conformalized selection framework to achieve FDR control
  - Quick check question: What is the key difference between standard conformal prediction and conformalized selection?

- **Concept**: False Discovery Rate (FDR) control
  - Why needed here: FDR is the primary error metric being controlled to ensure statistical guarantees on selected outputs
  - Quick check question: How does FDR differ from family-wise error rate (FWER) in the context of multiple testing?

- **Concept**: Exchangeability assumption
  - Why needed here: Required for validity of conformal p-values and resulting FDR control
  - Quick check question: Under what conditions does exchangeability hold between calibration and test data?

## Architecture Onboarding

- **Component map**: Foundation model -> Alignment score function -> Reference dataset -> Alignment predictor -> Conformal p-value computation -> Benjamini-Hochberg selection -> Output selection

- **Critical path**:
  1. Train alignment predictor on reference data
  2. Generate model outputs for test units
  3. Compute predicted alignment scores
  4. Calculate conformal p-values
  5. Apply BH procedure to select units
  6. Deploy selected outputs

- **Design tradeoffs**:
  - Predictor complexity vs. sample efficiency (simpler models require less data)
  - Number of reference samples vs. power (more samples improve power)
  - Strict FDR control vs. maximum power (tighter FDR control reduces power)

- **Failure signatures**:
  - FDR exceeds target level (suggests exchangeability violation or implementation error)
  - Very low power (suggests weak predictor or insufficient signal in features)
  - Unstable selection across runs (suggests insufficient reference data or poor predictor)

- **First 3 experiments**:
  1. Verify FDR control on synthetic data with known alignment status
  2. Test power with varying predictor quality on same synthetic data
  3. Validate exchangeability assumption by testing on i.i.d. data from same distribution

## Open Questions the Paper Calls Out

- **Open Question 1**: What is the optimal sample size and splitting ratio (γ1, γ2) for the alignment predictor and calibration set?
  - Basis in paper: [inferred] The paper presents ablation studies varying γ1 and γ2 but recommends general ranges rather than specific optimal values
  - Why unresolved: The optimal values likely depend on the specific task, model, and available data
  - What evidence would resolve it: Systematic experiments across diverse tasks and models, comparing performance with different (γ1, γ2) values

- **Open Question 2**: How can the alignment score function be designed to better reflect human judgment in complex tasks like radiology report generation?
  - Basis in paper: [explicit] The paper uses CheXbert for CXR report alignment, but acknowledges potential discrepancies with human evaluation
  - Why unresolved: Current alignment scores (like CheXbert) may not fully capture the nuances of human judgment
  - What evidence would resolve it: Comparative studies of different alignment scoring methods, including human evaluation as ground truth

- **Open Question 3**: How does Conformal Alignment perform in scenarios with severe data scarcity or domain shift?
  - Basis in paper: [inferred] The paper demonstrates effectiveness with hundreds of reference samples but doesn't extensively explore extreme data scarcity or domain shift scenarios
  - Why unresolved: While the paper shows good performance with moderate data, it's unclear how well the method scales to very limited data or when test data distribution differs significantly from reference data
  - What evidence would resolve it: Experiments with extremely limited reference data and in domain-shifted scenarios

## Limitations

- **Exchangeability assumption**: The theoretical guarantees critically depend on exchangeability between calibration and test units, which may fail with domain shift
- **Reference data quality**: The method's performance depends heavily on the quality of reference alignment labels, which is assumed but not empirically studied
- **Feature engineering dependency**: Success depends on identifying informative features for alignment prediction, requiring significant user effort without clear guidance

## Confidence

- **FDR Control Guarantee**: High confidence - theoretical foundation is well-established with rigorous proofs
- **Power-Performance Tradeoff**: Medium confidence - empirical relationship demonstrated but not theoretically characterized
- **Application Versatility**: Low confidence - successful demonstrations exist but generalizability to new domains remains unproven

## Next Checks

1. **Exchangeability Stress Test**: Systematically evaluate FDR control under various degrees of distribution shift between calibration and test data to quantify breakdown points

2. **Sample Efficiency Analysis**: Conduct controlled experiments varying reference data size from 50 to 1000 samples to characterize the relationship between reference data quantity, predictor quality, and resulting power

3. **Feature Transferability Study**: Test the same alignment predictor features across multiple diverse tasks to assess feature reusability and identify which feature types are most consistently informative