---
ver: rpa2
title: 'S3: A Simple Strong Sample-effective Multimodal Dialog System'
arxiv_id: '2406.18305'
source_url: https://arxiv.org/abs/2406.18305
tags:
- multimodal
- modality
- image
- dialog
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work introduces a simple yet effective multimodal dialog system,
  S3, achieving near state-of-the-art results on the MMMU and AI Journey Contest 2023
  leaderboards. The system combines a pre-trained large language model, pre-trained
  modality encoders for image and audio, and a trainable modality projector.
---

# S3: A Simple Strong Sample-effective Multimodal Dialog System

## Quick Facts
- arXiv ID: 2406.18305
- Source URL: https://arxiv.org/abs/2406.18305
- Authors: Elisei Rykov; Egor Malkershin; Alexander Panchenko
- Reference count: 33
- Primary result: 4th place out of 30 teams in AI Journey Contest 2023 with less than 150K training samples on a single A100-80GB GPU

## Executive Summary
This work introduces S3, a multimodal dialog system that achieves near state-of-the-art results on the MMMU and AI Journey Contest 2023 leaderboards using minimal resources. The system combines a pre-trained large language model (Mistral-7B) with pre-trained modality encoders for image (CLIP) and audio (ImageBind), connected through a trainable modality projector. A compact training corpus of less than 150,000 multimodal samples is used, demonstrating that powerful multimodal performance is achievable with minimal data and computational resources.

## Method Summary
S3 uses a frozen pre-trained LLM with frozen modality encoders (CLIP for images, ImageBind for audio) and a trainable MLP projector that maps modality features into 4 token embeddings compatible with the language model. The system is trained on a custom mixture of less than 150K multimodal samples across 10 tasks using cross-entropy loss with a standard decoder-only transformer architecture. Training is performed on a single A100-80GB GPU using DeepSpeed optimization level 2 with AdamW optimizer and cosine annealing scheduler.

## Key Results
- Ranks 4th out of 30 teams in AI Journey Contest 2023
- Achieves competitive score on MMMU benchmark compared to larger models
- Demonstrates effective multimodal performance using only 4 tokens per modality
- Requires less than 150,000 multimodal training samples

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Mapping the whole image into just 4 textual tokens is sufficient for multimodal dialog performance
- Mechanism: The modality projector transforms image embeddings into a small number of tokens (4) that are compatible with the language model, reducing sequence length while preserving essential visual information
- Core assumption: Visual information can be effectively compressed into 4 tokens without significant loss of task-relevant features
- Evidence anchors:
  - [abstract]: "mapping the whole image into 4 textual tokens is sufficient for the task"
  - [section]: "We employed the same architectural design for the modality projector across both image and speech modalities... we map the modality object into 4 tokens, regardless of the number of output patches within the modality encoder"
  - [corpus]: Weak evidence - no directly comparable studies found in corpus
- Break condition: Tasks requiring detailed visual analysis or fine-grained visual distinctions may fail with only 4 tokens

### Mechanism 2
- Claim: A frozen pre-trained modality encoder plus trainable projector is sufficient for multimodal integration
- Mechanism: Pre-trained encoders (CLIP for images, ImageBind for audio) provide robust feature extraction, while the projector adapts these features to the language model's embedding space
- Core assumption: Pre-trained encoders capture sufficient modality-specific information that can be linearly/projected into the LLM space
- Evidence anchors:
  - [section]: "We used CLIP as the encoder of choice" and "we experimented with the ImageBind multimodal encoder"
  - [section]: "The role of the modality projector is to adjust the embeddings of various modality objects... to ensure they are compatible with the language model"
  - [corpus]: Weak evidence - no directly comparable studies found in corpus
- Break condition: Encoder limitations or domain shift between pre-training and target task data

### Mechanism 3
- Claim: Small amounts of carefully curated multimodal data can train an effective system
- Mechanism: Data mixture includes diverse tasks (OCR, VQA, AQA, captioning, conversation) that provide comprehensive training signal for multimodal reasoning
- Core assumption: Task diversity and quality matters more than quantity for multimodal learning
- Evidence anchors:
  - [abstract]: "a compact corpus of less than 150,000 multimodal samples"
  - [section]: "We introduced a unique mixture of data specifically designed for our task" and details 145,250 samples across 10 tasks
  - [corpus]: Weak evidence - no directly comparable studies found in corpus
- Break condition: Insufficient coverage of critical task types or distribution shift between training and evaluation data

## Foundational Learning

- Concept: Modality alignment through learned projection
  - Why needed here: Different modalities (text, image, audio) have incompatible feature spaces that must be mapped to a common embedding space
  - Quick check question: What is the dimensionality relationship between the modality encoder output and the LLM token embeddings in the projector?

- Concept: Cross-entropy loss for multimodal sequence generation
  - Why needed here: The model generates text responses conditioned on multimodal inputs, requiring standard language modeling training
  - Quick check question: How does the cross-entropy loss handle special tokens like [img], [audio], and role indicators?

- Concept: LoRA adapters for efficient fine-tuning
  - Why needed here: Enables task-specific adaptation of the frozen LLM without full parameter updates, crucial for resource efficiency
  - Quick check question: What is the rank of the LoRA decomposition used in this system?

## Architecture Onboarding

- Component map: LLM (Mistral-7B) → Modality Encoders (CLIP for images, ImageBind for audio) → Modality Projector (MLP) → Special Token Integration → Text Generation
- Critical path: Image/Audio → Encoder → Projector → LLM → Response generation
- Design tradeoffs: 4 tokens vs. more tokens (sequence length vs. information capacity), frozen encoders vs. fine-tuning (speed vs. adaptation), small dataset vs. large dataset (efficiency vs. coverage)
- Failure signatures:
  - Visual understanding failures → Check projector output dimensionality and token splitting
  - Audio processing issues → Verify ImageBind audio processing pipeline
  - Training instability → Check learning rate and batch size configurations
- First 3 experiments:
  1. Verify image projector: Pass test image through CLIP → projector → check token embeddings dimensionality and values
  2. Validate data pipeline: Process sample from each dataset type through preprocessing → confirm special tokens and embeddings are correctly generated
  3. Test projector capacity: Train with varying numbers of output tokens (2, 4, 8) and measure impact on validation performance

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the minimum dataset size required for effective multimodal dialog system training?
- Basis in paper: [explicit] The authors demonstrate success with less than 150,000 multimodal samples
- Why unresolved: The paper shows success with this dataset size but doesn't explore the lower bounds of data requirements
- What evidence would resolve it: Systematic experiments varying dataset sizes to identify the minimum effective training data threshold

### Open Question 2
- Question: How does the performance of S3 scale with increasing model size and complexity?
- Basis in paper: [inferred] The authors use a 7B language model and simple MLP projector, but don't explore scaling effects
- Why unresolved: The paper focuses on minimal resources but doesn't investigate how performance changes with larger models
- What evidence would resolve it: Comparative experiments with different model sizes and architectures

### Open Question 3
- Question: What is the impact of different modality token embedding dimensions on system performance?
- Basis in paper: [explicit] The authors use 4 tokens for image representation but don't explore other configurations
- Why unresolved: The paper shows 4 tokens are sufficient but doesn't investigate optimal token counts or dimensions
- What evidence would resolve it: Systematic testing of different token counts and embedding dimensions

### Open Question 4
- Question: How does the system perform with different types of audio data beyond the current dataset?
- Basis in paper: [inferred] The authors use ImageBind for audio but don't extensively test with diverse audio datasets
- Why unresolved: The paper mentions audio modality but focuses more on visual and text data
- What evidence would resolve it: Testing with various audio datasets covering different domains and audio characteristics

## Limitations

- Lack of detailed implementation specifications for data mixture composition and projector architecture
- No ablation studies testing different token counts beyond the 4-token configuration
- Limited exploration of audio modality capabilities compared to visual and text data

## Confidence

- **High confidence**: The basic architecture design (frozen encoders + trainable projector + LLM) is technically sound and well-established in multimodal literature. The use of CLIP and ImageBind as modality encoders is standard practice with proven effectiveness.
- **Medium confidence**: The empirical results showing competitive performance with minimal resources appear valid based on leaderboard rankings, though the exact performance gap to larger systems is not fully quantified. The data efficiency claim is supported but could benefit from more rigorous ablation studies.
- **Low confidence**: The specific claim that exactly 4 tokens are sufficient for all multimodal dialog tasks lacks sufficient justification. The mechanism by which this compression preserves task-relevant information is not explained, and no ablation studies test different token counts.

## Next Checks

1. **Token count ablation study**: Systematically vary the number of output tokens from the projector (2, 4, 8, 16) and measure the impact on validation performance across different task types to determine if 4 is truly optimal or just sufficient.

2. **Encoder fine-tuning comparison**: Train an identical model where CLIP and ImageBind encoders are fine-tuned rather than frozen, and compare performance to quantify the cost-benefit tradeoff of the frozen approach.

3. **Data efficiency analysis**: Train models on progressively smaller subsets of the training data (10%, 25%, 50%, 100%) to determine the actual minimum dataset size required for competitive performance, validating the "compact corpus" claim.