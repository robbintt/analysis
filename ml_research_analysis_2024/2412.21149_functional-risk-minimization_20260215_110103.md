---
ver: rpa2
title: Functional Risk Minimization
arxiv_id: '2412.21149'
source_url: https://arxiv.org/abs/2412.21149
tags:
- function
- loss
- functional
- noise
- distribution
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces Functional Risk Minimization (FRM), a framework
  that replaces output-space comparisons in ERM with function-space comparisons. Instead
  of assuming one true function, FRM models each data point as coming from its own
  function, sampled from a distribution, capturing structured noise better.
---

# Functional Risk Minimization

## Quick Facts
- arXiv ID: 2412.21149
- Source URL: https://arxiv.org/abs/2412.21149
- Reference count: 22
- Key outcome: FRM outperforms ERM in supervised, unsupervised, and RL tasks—e.g., >20% lower RMSE in RL, large gains in VAE representations, and improved generalization under heteroskedastic noise in linear regression.

## Executive Summary
This paper introduces Functional Risk Minimization (FRM), a framework that replaces output-space comparisons in ERM with function-space comparisons. Instead of assuming one true function, FRM models each data point as coming from its own function, sampled from a distribution, capturing structured noise better. FRM includes ERM as a special case for common losses like MSE and cross-entropy, but offers more flexibility for realistic noise. Empirically, FRM outperforms ERM in supervised, unsupervised, and RL tasks—e.g., >20% lower RMSE in RL, large gains in VAE representations, and improved generalization under heteroskedastic noise in linear regression. The approach leverages over-parameterization and Taylor approximations for scalability.

## Method Summary
FRM replaces ERM's output-space loss comparisons with function-space comparisons, treating each data point as coming from its own function sampled from a distribution. This allows FRM to capture structured noise better than ERM. The framework uses Taylor approximations and Hessian-vector products to make the optimization scalable. FRM can be seen as finding the simplest hyper-model that fits the data, leveraging over-parameterization to implicitly assign each data point its own latent model.

## Key Results
- FRM outperforms ERM in supervised, unsupervised, and RL tasks
- >20% lower RMSE in RL value function estimation
- Large gains in VAE representations compared to ERM
- Improved generalization under heteroskedastic noise in linear regression

## Why This Works (Mechanism)

### Mechanism 1
- Claim: FRM improves generalization by modeling structured noise in function space rather than output space.
- Mechanism: Instead of assuming a single true function with output noise, FRM treats each data point as coming from its own function sampled from a distribution, capturing structured variability within the data.
- Core assumption: The function class f_θ is a good model of the mapping x→y, and uncertainty in this mapping can be modeled within the same function class.
- Evidence anchors:
  - [abstract] "This allows FRM to subsume ERM for many common loss functions and to capture more realistic noise processes."
  - [section 3.2] "ERM with losses such as cross-entropy or MSE implicitly models all the particularities of each training example through noise in output space... However, the data remains the same whether separated into multiple datasets or kept together into a single one."
  - [corpus] Weak connection - no direct evidence found for this specific mechanism in related papers.
- Break condition: If the function class cannot represent the structured variability in the data, or if the noise is truly unstructured (e.g., pure salt-and-pepper noise), FRM may not provide benefits over ERM.

### Mechanism 2
- Claim: FRM leverages over-parameterization to find the simplest hyper-model that fits the data.
- Mechanism: In over-parameterized regimes, FRM can be seen as finding the minimal functional adaptations needed for each data point, encouraging all functions to be close to each other according to a metric that captures the relationship between the function class and the loss.
- Core assumption: The system (θ*, θ1,..., θn) is over-parameterized for the n independent constraints of fitting a single data point with the entire parameter set.
- Evidence anchors:
  - [section 4.4] "FRM implicitly assigns to every datapoint(xi,yi) its own latent model f_θi which fits it: f_θi(xi) = yi. In this way, we can turn a model f_θ into an over-parameterized hyper-model."
  - [section 5.1] "For FRM linear regression with MSE, the approximations in section 4.3 are exact and both Hessian and gradients are independent of the parameters."
  - [corpus] Weak connection - no direct evidence found for this specific mechanism in related papers.
- Break condition: If the model is not sufficiently over-parameterized, or if the data cannot be explained by a simple hyper-model, this mechanism may not provide benefits.

### Mechanism 3
- Claim: FRM provides an avenue towards understanding generalization in the modern over-parameterized regime.
- Mechanism: By framing the objective as finding the simplest model that fits the training data, FRM connects to principles like Occam's Razor and provides insights into why over-parameterized models can generalize despite memorizing training data.
- Core assumption: The simplest model that fits the data is also the one that generalizes best.
- Evidence anchors:
  - [abstract] "We also show that FRM provides an avenue towards understanding generalization in the modern over-parameterized regime, as its objective can be framed as finding the simplest model that fits the training data."
  - [section 4.4] "This can be seen as finding the simplest hyper-model{θ1,...,θn} that fits the data. Simplicity is measured as the distance of parameters being close to a central parameter given a metric that captures the relationship between the function class f_θ and the loss L."
  - [corpus] Weak connection - no direct evidence found for this specific mechanism in related papers.
- Break condition: If the simplest model that fits the data is not the one that generalizes best (e.g., in cases of severe overfitting), this mechanism may not hold.

## Foundational Learning

- Concept: Functional Risk Minimization (FRM)
  - Why needed here: FRM is the core framework proposed in this paper as an alternative to Empirical Risk Minimization (ERM). Understanding FRM is crucial for grasping the paper's contributions and experimental results.
  - Quick check question: How does FRM differ from ERM in terms of the space where losses are measured?

- Concept: Functional Generative Models (FGMs)
  - Why needed here: FGMs are introduced as a class of generative models that assign a function to each data point, forming the basis for deriving the FRM framework.
  - Quick check question: How do FGMs capture structured noise in datasets compared to traditional generative models?

- Concept: Over-parameterization and generalization
  - Why needed here: The paper leverages the over-parameterized regime of modern neural networks to propose mechanisms for understanding generalization, which is a key aspect of FRM's theoretical contributions.
  - Quick check question: How does FRM's approach to finding the simplest model that fits the data relate to the double descent phenomenon observed in over-parameterized models?

## Architecture Onboarding

- Component map:
  - Data points (xi, yi)
  - Function class f_θ
  - Loss function L
  - Parameter distribution P(θ)
  - Training objective (FRM objective)
  - Approximation methods (variational approach, Taylor approximation)

- Critical path:
  1. Define the function class f_θ and loss function L
  2. Implement the FRM objective based on the chosen approximation method
  3. Optimize the FRM objective to find the best parameter distribution P(θ)
  4. Use the learned distribution for prediction on new data

- Design tradeoffs:
  - Accuracy vs. computational cost: FRM can provide better generalization but is more computationally expensive than ERM due to the need to evaluate Hessian-vector products.
  - Flexibility vs. simplicity: FRM offers more flexibility in modeling structured noise but requires a more complex optimization procedure compared to ERM.
  - Over-parameterization vs. under-parameterization: FRM's benefits are more pronounced in over-parameterized regimes, but it may not provide significant advantages in under-parameterized settings.

- Failure signatures:
  - No improvement over ERM: If the function class cannot represent the structured variability in the data, or if the noise is truly unstructured, FRM may not provide benefits over ERM.
  - Increased computational cost without performance gains: If the model is not sufficiently over-parameterized, or if the data cannot be explained by a simple hyper-model, FRM may increase computational cost without providing significant performance improvements.
  - Difficulty in optimization: If the FRM objective is too complex or the approximations are not accurate enough, optimization may become challenging or unstable.

- First 3 experiments:
  1. Linear regression with heteroscedastic noise: Compare FRM and ERM on a synthetic dataset with varying noise levels across different input regions to demonstrate FRM's ability to handle structured noise.
  2. Image classification with structured variations: Use a dataset like colored MNIST or translated MNIST to show FRM's advantages in capturing structured variations in image data compared to ERM.
  3. Value function estimation in reinforcement learning: Apply FRM to a reinforcement learning task like mountain car to demonstrate its benefits in handling complex noise in temporal difference errors.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the precise mathematical relationship between Functional Risk Minimization (FRM) and over-parameterized ERM under gradient descent, as suggested in section 4.4?
- Basis in paper: [explicit] The paper states "FRM may do explicitly what over-parameterized ERM does implicitly" and conjectures that ERM under gradient descent may implicitly find a function with two components (stable + spiky) where the spiky component has negligible norm but allows overfitting.
- Why unresolved: The paper only provides a high-level intuition and does not prove this relationship mathematically or provide empirical evidence comparing FRM and ERM under identical conditions.
- What evidence would resolve it: A rigorous mathematical proof showing the equivalence between FRM and a specific form of over-parameterized ERM under gradient descent, or empirical results demonstrating that FRM finds the same stable + spiky decomposition as over-parameterized ERM.

### Open Question 2
- Question: How does FRM scale to extremely large datasets and models where the Hessian becomes intractable, and what are the trade-offs between different approximation methods?
- Basis in paper: [explicit] The paper acknowledges that "the main limitation of FRM in its general form is its compute cost" and discusses Taylor approximations but notes that "that requires inverting a Hessian, often too big to even instantiate in memory."
- Why unresolved: While the paper proposes Taylor approximations and mentions using iterative solvers and Hessian-vector products, it does not provide a comprehensive analysis of scalability limitations or compare different approximation methods.
- What evidence would resolve it: Benchmark results comparing FRM with different approximation methods (Taylor, variational, etc.) on large-scale datasets and models, analysis of computational complexity and memory requirements, and identification of the breaking point where FRM becomes impractical.

### Open Question 3
- Question: What is the theoretical foundation for why FRM performs better than ERM under heteroskedastic noise, and can this be generalized to other types of structured noise?
- Basis in paper: [explicit] The paper shows empirical results (Figure 7) that FRM performs significantly better than ERM under heteroskedastic noise in linear regression, with "ERM has up to 40% higher test error" compared to FRM.
- Why unresolved: The paper provides empirical evidence but does not offer a theoretical explanation for why FRM specifically handles heteroskedastic noise better than ERM, or whether this advantage extends to other structured noise patterns.
- What evidence would resolve it: A theoretical analysis proving that FRM is optimal for heteroskedastic noise, or empirical studies testing FRM against ERM on various structured noise patterns (correlated noise, non-Gaussian noise, etc.) to identify the types of noise where FRM provides the most benefit.

## Limitations

- The paper's strongest empirical claims rely on relatively small-scale experiments (e.g., 100-parameter linear models, 1-layer neural networks).
- The extension to deeper architectures and larger datasets remains untested.
- The theoretical analysis focuses on approximation properties without rigorous bounds on generalization error.

## Confidence

- **High confidence**: The core mathematical framework (FRM objective formulation, connection to ERM) is sound and well-defined.
- **Medium confidence**: The approximation methods (Taylor expansion, variational approach) are reasonable but their accuracy for complex function classes needs validation.
- **Low confidence**: The claimed advantages in over-parameterized regimes and the specific mechanism of finding "simplest hyper-models" lack rigorous theoretical justification.

## Next Checks

1. **Scalability test**: Implement FRM on a deeper architecture (e.g., ResNet-18) and compare performance on CIFAR-10 against ERM with various regularizations.
2. **Theoretical bounds**: Derive generalization bounds for FRM that explicitly account for the approximation error introduced by the Taylor expansion or variational methods.
3. **Ablation study**: Systematically vary the amount of over-parameterization and noise structure to identify the precise conditions under which FRM outperforms ERM.