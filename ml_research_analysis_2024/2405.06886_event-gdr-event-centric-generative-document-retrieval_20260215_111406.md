---
ver: rpa2
title: 'Event GDR: Event-Centric Generative Document Retrieval'
arxiv_id: '2405.06886'
source_url: https://arxiv.org/abs/2405.06886
tags: []
core_contribution: The paper proposes Event GDR, an event-centric generative document
  retrieval model that leverages enriched event relations and well-defined taxonomy
  to address challenges in document representation and identifier construction. The
  core method uses an exchange-then-reflection procedure based on multi-agents for
  event knowledge extraction, employs events and relations to model documents for
  comprehensiveness and inner-content correlation, and maps events to taxonomy for
  constructing identifiers with explicit semantic structure.
---

# Event GDR: Event-Centric Generative Document Retrieval

## Quick Facts
- arXiv ID: 2405.06886
- Source URL: https://arxiv.org/abs/2405.06886
- Reference count: 15
- Key outcome: Event GDR achieves 7.7% improvement in document representation and optimal performance on NQ and DuReader datasets by leveraging event-centric modeling and taxonomy-based identifiers.

## Executive Summary
This paper introduces Event GDR, a generative document retrieval model that addresses two key challenges in GDR: inner-content correlation modeling and explicit semantic identifier construction. The approach extracts events and relations using a multi-agent exchange-then-reflection procedure, represents documents through enriched event modeling, and constructs identifiers by mapping events to a hierarchical event taxonomy. Experiments on Natural Questions and DuReader datasets demonstrate significant improvements over baselines, with the model showing good generalization across languages and taxonomy settings.

## Method Summary
Event GDR employs a multi-agent exchange-then-reflection procedure for event extraction, uses events and their relations to model document semantics for comprehensiveness and inner-content correlation, and maps extracted events to a well-defined event taxonomy to construct semantically structured identifiers. The model is trained with a multi-task loss combining indexing and retrieval objectives, using T5 and mT5 architectures on Natural Questions (English) and DuReader_retrieval (Chinese) datasets.

## Key Results
- Achieves 7.7% improvement in document representation over baselines
- Optimal retrieval performance on both Natural Questions and DuReader datasets
- Demonstrates strong generalization across datasets, languages, and taxonomy settings

## Why This Works (Mechanism)

### Mechanism 1
- Event-centric representation captures inner-content correlation better than substring or chunked baselines by modeling dependencies between document contents through events and their relations.
- Core assumption: Document semantics are sufficiently captured by event triggers and relations, and event extraction is accurate.
- Evidence anchors: [abstract] and [section] claims about using events and relations for comprehensiveness and inner-content correlation; weak corpus support.
- Break condition: If event extraction fails to identify relevant events or relations, the inner-content correlation benefit disappears.

### Mechanism 2
- Event taxonomy provides explicit semantic structure for identifiers, improving indexing and retrieval by mapping events to hierarchical taxonomy nodes with semantic meaning.
- Core assumption: Taxonomy structure aligns with user query intent and covers the event space adequately.
- Evidence anchors: [abstract] and [section] claims about mapping events to taxonomy for explicit semantic structure; weak corpus support.
- Break condition: If taxonomy coverage is poor or event-to-taxonomy mapping is noisy, the semantic benefit degrades.

### Mechanism 3
- Exchange-then-reflection multi-agent extraction yields richer event and relation sets than single-agent extraction by having multiple agents exchange insights and iteratively refine outputs.
- Core assumption: Multi-agent communication surfaces complementary knowledge that a single agent misses.
- Evidence anchors: [section] and [abstract] claims about ExR procedure based on multiple agents; weak corpus support.
- Break condition: If agents converge to similar outputs, the diversity advantage vanishes.

## Foundational Learning

- Concept: Event extraction and relation modeling
  - Why needed here: Core to representing documents with enriched semantics and capturing inner-content correlation
  - Quick check question: How would you extract events and causal relations from a short news paragraph using a language model?

- Concept: Taxonomy mapping and hierarchical identifier construction
  - Why needed here: Provides semantic structure to identifiers, bridging the gap between raw event instances and human-understandable labels
  - Quick check question: Given a taxonomy and an event, how would you map the event to the most relevant taxonomy node?

- Concept: Multi-agent exchange-then-reflection procedure
  - Why needed here: Improves quality of extracted event knowledge by leveraging collective intelligence and iterative refinement
  - Quick check question: What prompts would you give two agents to exchange insights on event extraction before reflection?

## Architecture Onboarding

- Component map: Event extraction (multi-agent ExR) -> Document representation (Event Rep, Event Relation Rep) -> Identifier construction (Event Taxonomy Mapping) -> Indexing -> Retrieval
- Critical path: Extraction → Representation → Identifier Construction → Indexing → Retrieval
- Design tradeoffs:
  - More detailed event extraction improves representation quality but increases computation
  - Using a shared taxonomy simplifies identifier construction but may reduce granularity if taxonomy is coarse
  - Multi-agent extraction improves recall but adds latency and complexity
- Failure signatures:
  - Poor retrieval scores suggest extraction or taxonomy mapping is noisy
  - Training instability indicates multi-task balance issues
  - High memory usage hints at overly large event sets per document
- First 3 experiments:
  1. Replace multi-agent ExR with a single LLM extraction; measure impact on Hits@20
  2. Swap taxonomy-based identifiers with numeric cluster IDs; compare retrieval performance
  3. Vary number of extracted events per document; analyze trade-off between comprehensiveness and efficiency

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How do different event taxonomies (e.g., domain-specific vs. general) affect the performance of Event GDR?
- Basis in paper: [explicit] The paper states that Event GDR uses MAVEN's taxonomy and mentions testing generalizability across different taxonomies
- Why unresolved: The paper only tested one general taxonomy (MAVEN) and its Chinese translation. It did not explore how domain-specific taxonomies might impact performance
- What evidence would resolve it: Comparative experiments using multiple domain-specific event taxonomies (e.g., medical, legal, scientific) on various datasets, measuring retrieval performance differences

### Open Question 2
- Question: What is the optimal balance between event representation and traditional text representation in GDR?
- Basis in paper: [inferred] The paper shows event representation improves performance, but doesn't explore hybrid approaches combining events with traditional text representations
- Why unresolved: The paper only tests pure event-based representations versus traditional text-based baselines, not hybrid approaches
- What evidence would resolve it: Ablation studies comparing models using: (1) only events, (2) only text, (3) weighted combinations of both, measuring performance trade-offs

### Open Question 3
- Question: How does the exchange-then-reflection procedure scale with larger agent populations and more complex documents?
- Basis in paper: [explicit] The paper describes the ExR procedure but only uses a small number of agents (ChatGPT and GLM-130B) for event extraction
- Why unresolved: The paper doesn't explore how increasing the number of agents or handling more complex documents affects extraction quality and computational efficiency
- What evidence would resolve it: Experiments scaling agent numbers (3, 5, 10 agents) and testing on progressively more complex documents, measuring both quality improvements and computational overhead

## Limitations
- Specific prompts for multi-agent extraction are not disclosed, making reproduction difficult
- Neural model details for taxonomy mapping are underspecified
- 7.7% improvement claim lacks detailed methodology and baseline comparison specifics
- Limited ablation study evidence showing isolated impact of each mechanism

## Confidence
- High Confidence: The conceptual framework of using events and relations to capture inner-content correlation is well-motivated and aligns with existing IR literature
- Medium Confidence: The taxonomy-based identifier construction provides explicit semantic structure, but impact depends heavily on taxonomy quality
- Low Confidence: The multi-agent exchange-then-reflection procedure's effectiveness is asserted but not empirically isolated

## Next Checks
1. Ablation on Extraction Method: Replace multi-agent ExR with a single strong LLM extraction baseline and measure impact on Hits@20
2. Taxonomy Mapping Evaluation: Manually annotate a sample of documents with their mapped taxonomy nodes and evaluate precision/recall
3. Event Coverage Analysis: Analyze distribution of extracted events per document and their overlap with query-relevant events