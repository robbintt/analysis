---
ver: rpa2
title: What to align in multimodal contrastive learning?
arxiv_id: '2409.07402'
source_url: https://arxiv.org/abs/2409.07402
tags:
- multimodal
- comm
- learning
- information
- modalities
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces CoMM, a contrastive multimodal learning strategy
  that enables the communication between modalities in a single multimodal space.
  Unlike existing methods that align cross-modal features, CoMM aligns multimodal
  features by maximizing the mutual information between augmented versions of these
  features.
---

# What to align in multimodal contrastive learning?

## Quick Facts
- arXiv ID: 2409.07402
- Source URL: https://arxiv.org/abs/2409.07402
- Reference count: 40
- Key outcome: CoMM achieves state-of-the-art results on seven multimodal tasks by aligning multimodal features through augmented versions rather than cross-modal features.

## Executive Summary
This paper introduces CoMM, a novel contrastive multimodal learning strategy that enables communication between modalities in a single multimodal space. Unlike existing methods that align cross-modal features, CoMM aligns augmented versions of multimodal features by maximizing mutual information. The theoretical analysis shows that shared, synergistic, and unique terms of information naturally emerge from this formulation, allowing estimation of multimodal interactions beyond redundancy.

## Method Summary
CoMM is a contrastive multimodal learning strategy that fuses multiple modalities into a shared representation space and aligns augmented versions of this multimodal feature using contrastive objectives. The method uses modality-specific encoders, latent converters, and a transformer-based fusion module to create a joint multimodal representation. Training maximizes mutual information between augmented versions of this representation using InfoNCE, with additional terms ensuring individual modality representations are preserved. The approach is evaluated across seven multimodal tasks including both synthetic and real-world datasets.

## Key Results
- CoMM achieves state-of-the-art results on seven multimodal tasks including MIMIC, MOSI, UR-FUNNY, MUsTARD, Vision&Touch, and MM-IMDb
- The method successfully captures redundant, unique, and synergistic information between modalities in controlled experiments on Trifeature
- CoMM outperforms existing contrastive methods like Cross, Cross+Self, and FactorCL across multiple evaluation protocols

## Why This Works (Mechanism)

### Mechanism 1
- Claim: CoMM learns redundant, unique, and synergistic information by maximizing mutual information between augmented versions of multimodal features.
- Mechanism: Instead of aligning cross-modal features, CoMM fuses modalities into a shared representation and aligns augmented versions of this multimodal feature using contrastive objectives.
- Core assumption: There exist label-preserving multimodal augmentations such that I(X, t(X)) = I(X, Y) for any t in the augmentation set.
- Evidence anchors:
  - [abstract]: "CoMM aligns multimodal features by maximizing the mutual information between augmented versions of these multimodal features."
  - [section]: "Given a multimodal input X = ( X1, ..., Xn) and a set of label-preserving multimodal transformations T ⋆, two augmentations t′, t′′ are drawn from T ⋆ and applied to X"
  - [corpus]: Weak - corpus papers focus on alignment but don't directly address the augmentation-based approach
- Break condition: If the augmentation set doesn't preserve task-relevant information or if the mutual information estimator fails to capture complex interactions

### Mechanism 2
- Claim: The fusion transformer learns non-linear interactions between modalities that cannot be captured by linear methods.
- Mechanism: A transformer block with self-attention layers fuses modality-specific embeddings, enabling the model to learn complex interactions including synergy.
- Core assumption: Non-linear transformations are necessary to capture synergistic interactions between modalities.
- Evidence anchors:
  - [section]: "We define X ′ = t(X) with t ∈ T a stochastic mapping... of X and Z ′ θ = fθ(X ′). Given data processing inequalities for Markov chains X → X′ → Z′ θ and Z′ θ → X → Zθ"
  - [section]: "Unlike previous multimodal models (Cross) that align cross-modal features, CoMM aligns multimodal features in a shared representation space"
  - [corpus]: Weak - corpus papers discuss alignment but not the specific transformer-based fusion approach
- Break condition: If the transformer cannot learn meaningful attention patterns or if the modality-specific features are too dissimilar to fuse effectively

### Mechanism 3
- Claim: The InfoNCE-based contrastive objective naturally decomposes into components capturing redundancy, uniqueness, and synergy.
- Mechanism: The loss function LCoMM = L + ΣLi optimizes both the mutual information between augmented multimodal features and between each modality-specific feature and the augmented multimodal feature.
- Core assumption: The InfoNCE estimator can effectively approximate the mutual information terms needed to capture all three types of interactions.
- Evidence anchors:
  - [abstract]: "Our theoretical analysis shows that shared, synergistic and unique terms of information naturally emerge from this formulation"
  - [section]: "We use the InfoNCE (Oord et al., 2018) estimator of mutual information due to its simplicity and strong results in the self-supervised learning literature"
  - [section]: "Given this estimator, our final training loss can be written as: LCoMM = − ˆINCE(Z ′, Z′′) − Σ Li"
  - [corpus]: Weak - corpus papers discuss contrastive learning but not the specific decomposition into R, S, and U terms
- Break condition: If the InfoNCE estimator becomes unstable or if the contrastive objective fails to distinguish between the different types of information

## Foundational Learning

- Concept: Partial Information Decomposition (PID)
  - Why needed here: PID provides the theoretical framework for decomposing multivariate mutual information into redundancy, uniqueness, and synergy terms
  - Quick check question: What are the three terms in the PID decomposition of I(X1, X2; Y)?

- Concept: Data processing inequalities
  - Why needed here: These inequalities establish the theoretical foundation for why maximizing I(Zθ; Z′θ) can recover I(X, X′)
  - Quick check question: If Z is a function of X, what is the relationship between I(Z; Y) and I(X; Y)?

- Concept: Mutual information estimation with InfoNCE
  - Why needed here: InfoNCE provides a practical way to optimize mutual information in the contrastive learning framework
  - Quick check question: What is the key difference between InfoNCE and other mutual information estimators like MINE?

## Architecture Onboarding

- Component map: Modality encoders -> Latent converters -> Concatenation -> Fusion transformer -> Multimodal representation -> Projection heads -> InfoNCE loss
- Critical path: Encoder → Latent converter → Fusion transformer → Projection head → InfoNCE loss
- Design tradeoffs:
  - Transformer depth vs. computational cost
  - Augmentation strength vs. information preservation
  - Number of projection heads vs. loss stability
  - Modality-specific vs. shared encoders
- Failure signatures:
  - Random chance performance on tasks requiring synergy
  - Overfitting to training augmentations
  - Degraded performance when adding modalities
  - Unstable training with negative cosine similarity
- First 3 experiments:
  1. Train on bimodal Trifeature with shape redundancy task - should achieve near-perfect accuracy
  2. Test on bimodal Trifeature with texture uniqueness task - should capture unique information
  3. Evaluate on bimodal Trifeature with synergy task - should outperform cross-modal contrastive methods

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the theoretical framework for modeling multimodal interactions with more than two modalities using CoMM?
- Basis in paper: [inferred] The paper states that the theoretical analysis is currently limited to n = 2 modalities and that the number of interactions grows exponentially with n.
- Why unresolved: The complexity of interactions increases exponentially with the number of modalities, making theoretical analysis challenging.
- What evidence would resolve it: A theoretical framework extending CoMM's analysis to n > 2 modalities, potentially using advanced information theory or graph-based approaches.

### Open Question 2
- Question: How can CoMM's interpretability be improved to disentangle the contributions of unique, redundant, and synergistic information in the representation space?
- Basis in paper: [explicit] The paper acknowledges that it seems difficult to disentangle the contributions of these three interactions in the representation space.
- Why unresolved: The current formulation does not provide a clear mechanism for attributing specific information to each interaction type.
- What evidence would resolve it: A method for visualizing or quantifying the contribution of each interaction type (unique, redundant, synergistic) to the final representation.

### Open Question 3
- Question: What are the optimal multimodal augmentations for CoMM beyond unimodal augmentations?
- Basis in paper: [explicit] The paper suggests that crafting label-preserving multimodal augmentations not limited to unimodal augmentations is an open avenue for future research.
- Why unresolved: The effectiveness of multimodal augmentations is not fully explored, and their impact on learning different types of interactions is unclear.
- What evidence would resolve it: Empirical studies comparing the performance of CoMM with different sets of multimodal augmentations, including those that are not simply compositions of unimodal augmentations.

## Limitations
- Theoretical grounding for synergy capture relies on strong augmentation assumptions requiring empirical validation
- Claims about synergy in real-world settings lack direct measurement of R, U, and S components
- Limited exploration of multimodal augmentations beyond unimodal compositions

## Confidence
- **High confidence**: CoMM outperforms baselines on standard multimodal benchmarks; the architecture and training procedure are well-specified.
- **Medium confidence**: The controlled experiments demonstrate the mechanism for capturing different types of information in synthetic data.
- **Low confidence**: Claims about capturing synergistic information in real-world settings without direct measurement of these components.

## Next Checks
1. Apply partial information decomposition (PID) analysis to CoMM representations on Trifeature and real datasets to directly measure R, U, and S components rather than relying on downstream task performance.

2. Systematically vary augmentation strength and diversity across modalities to determine the minimum requirements for capturing synergy, testing the assumption that I(X, t(X)) = I(X, Y) holds empirically.

3. Compare CoMM's ability to capture synergy against ablations that use different fusion strategies (e.g., early, late, or cross-modal alignment) while controlling for model capacity and training data.