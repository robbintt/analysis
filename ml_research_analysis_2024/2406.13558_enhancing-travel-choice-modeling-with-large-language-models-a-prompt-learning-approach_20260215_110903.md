---
ver: rpa2
title: 'Enhancing Travel Choice Modeling with Large Language Models: A Prompt-Learning
  Approach'
arxiv_id: '2406.13558'
source_url: https://arxiv.org/abs/2406.13558
tags: []
core_contribution: 'This paper introduces a novel Large Language Model (LLM) framework
  for travel choice modeling, addressing two key challenges: limited survey data and
  the trade-off between model explainability and accuracy. The framework leverages
  prompt-learning techniques to transform input variables into textual form and applies
  them to a well-trained LLM.'
---

# Enhancing Travel Choice Modeling with Large Language Models: A Prompt-Learning Approach

## Quick Facts
- arXiv ID: 2406.13558
- Source URL: https://arxiv.org/abs/2406.13558
- Reference count: 36
- Primary result: LLM framework outperforms state-of-the-art models with accuracy of 0.772 on LPMC and 0.734 on Optima datasets

## Executive Summary
This paper introduces a novel LLM framework for travel choice modeling that addresses the dual challenges of limited survey data and the trade-off between model explainability and accuracy. The framework leverages prompt-learning techniques to transform structured travel data into textual descriptions that LLMs can process directly. Tested on London Passenger Mode Choice (LPMC) and Optima-Mode datasets, the LLM significantly outperformed traditional discrete choice models and deep learning approaches, achieving accuracy scores of 0.772 and 0.734 respectively. The framework also provides explicit natural language explanations for individual predictions, enhancing interpretability compared to traditional "black box" models.

## Method Summary
The method transforms tabular travel choice data into textual descriptions that LLMs can process through prompt learning. Input variables (gender, age, trip purpose, cost, etc.) are formatted into readable strings that preserve semantic richness. The framework uses three prompt paradigms: zero-shot inference, similar demonstrations (SD), and panel demonstrations (PD) that provide historical context. LLaMA3 and Gemma models are deployed via Ollama to predict travel modes. The approach eliminates complex feature engineering while maintaining interpretability through generated explanations.

## Key Results
- LLM framework achieved accuracy of 0.772 on LPMC dataset and 0.734 on Optima dataset
- Performance significantly exceeded state-of-the-art models including MNL (0.611 LPMC), WDL (0.728 LPMC), and DeepFM (0.720 LPMC)
- Generated explicit natural language explanations for individual predictions, enhancing model interpretability

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Prompt-learning with LLMs reduces feature engineering complexity by converting tabular inputs directly into textual descriptions.
- Mechanism: Instead of manual feature extraction and encoding, input variables are formatted into descriptive text that the LLM interprets directly. This preserves semantic richness and reduces preprocessing overhead.
- Core assumption: LLMs can accurately process and reason over semi-structured textual descriptions of behavioral variables.
- Evidence anchors:
  - [abstract] "transforming input variables into textual form"
  - [section] "For each instance, variables such as gender, driving license status, car availability, trip purpose, and others are compiled into a readable string format"
- Break condition: If textual descriptions lose critical numerical relationships or if the LLM misinterprets semi-structured data, performance degrades.

### Mechanism 2
- Claim: In-context learning with demonstrations improves LLM prediction accuracy for travel mode choice.
- Mechanism: Providing similar or historical travel instances as demonstrations within the prompt allows the LLM to adapt without extensive retraining, leveraging learned reasoning from pretraining.
- Core assumption: LLMs retain generalizable reasoning capabilities that can be transferred to new domains with few in-context examples.
- Evidence anchors:
  - [abstract] "building of demonstrations similar to the object, and applying these to a well-trained LLM"
  - [section] "For historical context, we retrieve past instances from the same user with a simple check"
- Break condition: If demonstrations are not representative or if the LLM fails to generalize from provided examples.

### Mechanism 3
- Claim: Explicit textual explanations from LLMs enhance interpretability compared to traditional "black box" models.
- Mechanism: LLMs generate natural language explanations for predictions, making reasoning transparent to non-technical stakeholders without requiring specialized knowledge.
- Core assumption: LLMs can articulate reasoning processes in understandable natural language.
- Evidence anchors:
  - [abstract] "provides explicit explanations for individual predictions"
  - [section] "we present a case of explanation illustrating how the LLM framework generates understandable and explicit explanations"
- Break condition: If generated explanations are vague, inconsistent, or misleading, trust and usability decrease.

## Foundational Learning

- Concept: Prompt engineering and in-context learning
  - Why needed here: Critical for structuring input to LLMs and providing demonstrations without retraining
  - Quick check question: How does providing demonstrations in the prompt affect LLM performance compared to zero-shot inference?

- Concept: Travel choice modeling and discrete choice theory
  - Why needed here: Understanding the problem domain and how LLMs can replace or augment traditional models like MNL
  - Quick check question: What are the key differences between multinomial logit models and LLM-based prediction approaches?

- Concept: Natural language processing and semantic understanding
  - Why needed here: LLMs process textual descriptions of travel data; understanding semantic extraction is key to framing inputs correctly
  - Quick check question: How does converting structured travel data into textual descriptions affect the information available to the model?

## Architecture Onboarding

- Component map: Data preprocessing -> Prompt assembly -> LLM inference -> Post-processing

- Critical path:
  1. Load and preprocess dataset
  2. Format input features into textual descriptions
  3. Assemble prompt with demonstrations (similar or historical)
  4. Send prompt to LLM via Ollama
  5. Parse LLM JSON output to obtain predicted travel mode

- Design tradeoffs:
  - Textual description length vs. LLM context window limits
  - Number of demonstrations vs. prompt token constraints
  - Granularity of input features vs. clarity of generated text
  - Model choice (Llama vs. Gemma) vs. dataset compatibility

- Failure signatures:
  - Low accuracy with zero-shot inference indicates need for demonstrations
  - Poor performance on Optima vs. LPMC may indicate missing value handling issues
  - Inconsistent explanations suggest problems with prompt formulation

- First 3 experiments:
  1. Zero-shot inference on LPMC dataset to establish baseline performance
  2. Similar demonstrations (SD) prompt on LPMC to measure improvement from in-context learning
  3. Panel demonstrations (PD) prompt on Optima to test historical context effectiveness and handle missing values

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can LLM-based travel choice models be adapted to provide aggregated-level policy recommendations while maintaining the explainability demonstrated at the individual level?
- Basis in paper: The authors note that while their work offers clear explainability at the individual level, it does not address aggregated-level issues such as modifying elements to achieve desired macro-level outcomes, suggesting this as future work.
- Why unresolved: Aggregated-level analysis requires different methodologies and validation approaches compared to individual-level predictions, and the paper does not provide a framework for scaling up the LLM approach to this context.
- What evidence would resolve it: Development and testing of a modified LLM framework that can generate aggregated policy recommendations, validated against real-world policy outcomes and compared with traditional aggregated choice models.

### Open Question 2
- Question: What is the impact of dataset characteristics (such as missing values and feature distribution) on LLM performance in travel choice modeling compared to traditional models?
- Basis in paper: The authors observe that the Optima dataset, which contains many missing values (marked as -1), led to lower LLM performance compared to the LPMC dataset, suggesting that dataset quality significantly affects LLM efficacy.
- Why unresolved: While the paper identifies this issue, it does not provide a systematic analysis of how different dataset characteristics specifically impact LLM versus traditional model performance.
- What evidence would resolve it: A controlled study comparing LLM and traditional model performance across datasets with varying levels of missing data, feature distributions, and other characteristics, identifying specific dataset features that most impact LLM performance.

### Open Question 3
- Question: How does the choice of LLM architecture (e.g., Llama vs. Gemma) influence performance in travel choice modeling across different dataset types and sizes?
- Basis in paper: The authors found that Llama3 significantly outperformed Gemma in their experiments, but note that LLM performance depends more on specific model configuration than on the volume of data trained.
- Why unresolved: The paper only compares two specific LLM models and does not explore how different architectures might perform across various dataset characteristics or sizes.
- What evidence would resolve it: A comprehensive comparison of multiple LLM architectures across diverse travel choice datasets with varying sizes and characteristics, identifying which architectures perform best under specific conditions.

## Limitations

- Performance gap between LPMC (0.772 accuracy) and Optima (0.734 accuracy) datasets suggests sensitivity to data quality and preprocessing methods
- Framework's reliance on textual prompt engineering introduces variability that may not be consistent across different LLMs or problem domains
- Computational cost of LLM inference compared to traditional models raises scalability questions for real-time applications

## Confidence

- High Confidence: The LLM framework outperforms traditional discrete choice models (MNL, WDL) on both datasets. The accuracy improvements over baseline models are statistically significant and consistent across evaluation metrics.
- Medium Confidence: The explanation generation capability genuinely enhances interpretability. While the framework produces explicit textual explanations, their consistency and practical utility for non-technical stakeholders require further validation across diverse cases.
- Low Confidence: The framework's generalizability to other choice modeling contexts beyond travel mode selection. The specific prompt engineering techniques and demonstration selection strategies may not transfer effectively to domains with different data structures or decision processes.

## Next Checks

1. Cross-dataset generalization test: Apply the exact same framework to a completely different choice modeling dataset (e.g., consumer product choice or healthcare treatment selection) to assess transferability of prompt engineering strategies.

2. Ablation study on demonstration quality: Systematically vary the similarity threshold and number of demonstrations to quantify their impact on prediction accuracy, isolating the contribution of in-context learning from the base LLM capabilities.

3. Human evaluation of explanations: Conduct a user study with transportation planners and policy makers to assess whether the generated explanations improve decision-making understanding compared to traditional model coefficients or feature importance scores.