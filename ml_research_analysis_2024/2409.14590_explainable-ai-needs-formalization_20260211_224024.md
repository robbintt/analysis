---
ver: rpa2
title: Explainable AI needs formalization
arxiv_id: '2409.14590'
source_url: https://arxiv.org/abs/2409.14590
tags:
- methods
- data
- explanations
- such
- feature
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The field of explainable AI (XAI) aims to make machine learning
  decisions understandable, but current feature attribution methods systematically
  fail to identify features that are statistically associated with prediction targets.
  The paper demonstrates that popular methods like SHAP, LIME, and gradient-based
  approaches attribute importance to "suppressor variables" - features that improve
  predictions without being predictive themselves.
---

# Explainable AI needs formalization

## Quick Facts
- arXiv ID: 2409.14590
- Source URL: https://arxiv.org/abs/2409.14590
- Reference count: 40
- Current feature attribution methods systematically fail to identify features statistically associated with prediction targets

## Executive Summary
The field of explainable AI (XAI) aims to make machine learning decisions understandable, but current feature attribution methods systematically fail to identify features that are statistically associated with prediction targets. The paper demonstrates that popular methods like SHAP, LIME, and gradient-based approaches attribute importance to "suppressor variables" - features that improve predictions without being predictive themselves. Using simple synthetic examples, the authors show that these methods assign non-zero importance to features uncorrelated with the target, preventing reliable use for model diagnostics, scientific discovery, or identifying intervention targets.

## Method Summary
The authors use synthetic datasets with known ground-truth explanations to demonstrate systematic failures of popular XAI methods. They generate controlled examples where suppressor variables are present and show that methods like SHAP, LIME, and gradient-based approaches attribute non-zero importance to features statistically independent of the target. The evaluation framework tests methods against the Statistical Association Property (SAP) criterion and ground-truth benchmarks. The paper also advocates for a systematic, requirement-driven approach to XAI development that includes defining stakeholder needs, formalizing XAI problems, developing targeted algorithms, and validating methods using ground-truth benchmarks.

## Key Results
- Current XAI methods attribute importance to suppressor variables - features that improve predictions without being statistically associated with targets
- Popular methods (SHAP, LIME, gradient-based) systematically fail the Statistical Association Property (SAP) criterion
- Feature attribution methods cannot reliably identify intervention targets or confounding variables due to lack of formal problem definitions
- Current evaluation frameworks relying on faithfulness metrics are insufficient for validating explanation correctness

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Feature attribution methods systematically assign importance to suppressor variables that have no statistical association with the prediction target.
- Mechanism: Current XAI methods implicitly assume feature independence, but suppressor variables improve predictions by removing target-irrelevant variance from other features. This creates a mathematical necessity for non-zero weights on these suppressors in optimal models.
- Core assumption: The presence of suppressor variables in real-world datasets is common and their statistical structure differs from that assumed by XAI methods.
- Evidence anchors:
  - [abstract] "current feature attribution methods do not address well-defined problems and are not evaluated against targeted criteria of explanation correctness"
  - [section] "Wilming et al. [14, 15] introduce the statistical association property (SAP) for feature attribution methods" and "existing feature attribution methods attribute importance to variables unrelated to the target"
  - [corpus] Weak - no direct mention of suppressor variables in neighbor papers
- Break condition: If XAI methods explicitly model feature dependencies and correlation structures, the suppressor attribution problem would be mitigated.

### Mechanism 2
- Claim: XAI methods cannot reliably identify intervention targets or confounding variables due to lack of formal problem definitions.
- Mechanism: Without formal specifications of what constitutes "explanation correctness," methods optimize for algorithmic performance rather than addressing stakeholder needs. This leads to explanations that appear useful but fail to provide actionable insights.
- Core assumption: Different stakeholders have distinct information needs that cannot be served by a one-size-fits-all explanation approach.
- Evidence anchors:
  - [abstract] "Researchers should formally define the problems they intend to solve and design methods accordingly"
  - [section] "It is unreasonable to call a mapping from input features to real numbers an explanation without endowing these numbers with a well-defined formal interpretation"
  - [corpus] Weak - neighbor papers focus on XAI design but don't address formal problem definitions
- Break condition: If formal XAI problems are defined and methods are validated against these specifications, the reliability of identifying intervention targets would improve.

### Mechanism 3
- Claim: Current XAI evaluation frameworks are insufficient because they rely on faithfulness metrics that don't account for data-generating processes.
- Mechanism: Faithfulness metrics measure whether removing important features decreases performance, but this fails when suppressor variables are present. The performance drop occurs regardless of whether the feature is actually informative about the target.
- Core assumption: The data-generating process, including causal structure and correlations, is essential for interpreting model behavior and feature importance.
- Evidence anchors:
  - [abstract] "Popular methods cannot reliably answer relevant questions about ML models, their training data, or test inputs"
  - [section] "XAI methods ignore data distribution and causal structure" and "most XAI methods explicitly or implicitly assume statistically independent features"
  - [corpus] Weak - neighbor papers discuss XAI evaluation but don't address faithfulness limitations
- Break condition: If evaluation frameworks incorporate ground-truth benchmarks with known data-generating processes, the assessment of explanation correctness would become more reliable.

## Foundational Learning

- Concept: Statistical Association Property (SAP)
  - Why needed here: SAP provides a formal criterion for determining whether feature attribution methods correctly identify features associated with the prediction target
  - Quick check question: If a feature attribution method assigns high importance to a feature that is statistically independent of the target, does it possess the SAP?

- Concept: Suppressor variables and causal structure
  - Why needed here: Understanding how suppressor variables work and their role in causal diagrams is crucial for recognizing why current XAI methods fail
  - Quick check question: In a causal diagram where a suppressor variable influences an informative feature, can the suppressor itself be statistically associated with the target?

- Concept: Formal problem specification in XAI
  - Why needed here: Without formal definitions of what constitutes a correct explanation for different stakeholder needs, XAI methods cannot be systematically evaluated or improved
  - Quick check question: What are the key differences between explanations needed for model diagnostics versus those needed for scientific discovery?

## Architecture Onboarding

- Component map:
  - Data generation module -> Model training module -> XAI method implementation -> Evaluation framework -> Visualization tools

- Critical path:
  1. Generate synthetic data with known statistical properties
  2. Train ML model on this data
  3. Apply XAI methods to generate explanations
  4. Evaluate explanations against ground-truth and formal criteria
  5. Analyze failure modes and identify suppressor attributions

- Design tradeoffs:
  - Synthetic vs. real data: Synthetic data provides ground-truth but may lack real-world complexity
  - Model simplicity vs. realism: Simple models are easier to analyze but may not capture complex interactions
  - Computational cost vs. thoroughness: Comprehensive benchmarking requires significant resources

- Failure signatures:
  - High importance assigned to features uncorrelated with target
  - Inconsistent explanations across different XAI methods
  - Explanations that change significantly with minor input perturbations
  - Poor performance on ground-truth benchmarks with known suppressor variables

- First 3 experiments:
  1. Implement Examples A and B from the paper and verify that linear models assign non-zero weights to suppressor variables
  2. Test SHAP, LIME, and gradient-based methods on these examples to confirm they attribute importance to suppressors
  3. Create a ground-truth benchmark with known suppressor variables and evaluate multiple XAI methods against SAP criterion

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What specific stakeholder information needs can be formally specified for XAI methods across different domains and use cases?
- Basis in paper: [explicit] The paper identifies that different stakeholders (ML developers, physicians, regulators) have different information needs and calls for formalizing these needs into concrete XAI problems
- Why unresolved: While the paper acknowledges this as a necessary step, it does not provide concrete examples of how to translate informal stakeholder needs into formal specifications across various domains
- What evidence would resolve it: Case studies demonstrating the process of translating stakeholder needs into formal XAI problem specifications across multiple domains (healthcare, finance, hiring)

### Open Question 2
- Question: Can formal verification methods be developed to prove that XAI algorithms satisfy their specified requirements?
- Basis in paper: [explicit] The paper notes that formal verification of algorithmic solutions may sometimes be infeasible or considered insufficient, but suggests it could be possible for certain XAI problems
- Why unresolved: Current XAI evaluation relies heavily on empirical benchmarking rather than formal verification, and the paper doesn't explore what formal verification approaches might look like for XAI
- What evidence would resolve it: Development of formal verification frameworks that can mathematically prove whether specific XAI algorithms satisfy their stated requirements

### Open Question 3
- Question: How can ground-truth datasets for XAI validation be extended to capture more realistic aspects of real-world data while maintaining known ground-truth explanations?
- Basis in paper: [explicit] The paper acknowledges that empirical benchmarking using ground-truth data can only provide partial validation due to the impossibility of covering all realistic aspects of real-world settings
- Why unresolved: Current synthetic benchmarks are limited in their ability to capture the complexity and interdependencies present in real-world data
- What evidence would resolve it: Novel synthetic dataset generation methods that balance realism with known ground-truth explanations, validated against real-world data properties

## Limitations
- Evidence is primarily based on synthetic examples rather than comprehensive real-world validation
- The suppressor variable examples represent specific mathematical structures that may not generalize to all practical scenarios
- Current formal problem specifications are limited in scope and haven't been validated across diverse real-world applications

## Confidence
- Claim that current XAI methods cannot reliably identify intervention targets or confounding variables: Medium
- Claim that evaluation frameworks are fundamentally insufficient: Medium-High
- Claim that formalization is essential for XAI development: High

## Next Checks
1. Replicate the suppressor variable experiments across multiple real-world datasets with known causal structures to verify the systematic nature of the failure
2. Test the proposed formalization framework by implementing formal problem specifications for different stakeholder needs and validating against these specifications
3. Develop and evaluate ground-truth benchmarks that include complex suppressor structures and multi-stakeholder explanation requirements to assess the practical impact of formalization