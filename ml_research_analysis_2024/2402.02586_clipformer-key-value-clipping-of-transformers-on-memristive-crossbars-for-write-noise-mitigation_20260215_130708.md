---
ver: rpa2
title: 'ClipFormer: Key-Value Clipping of Transformers on Memristive Crossbars for
  Write Noise Mitigation'
arxiv_id: '2402.02586'
source_url: https://arxiv.org/abs/2402.02586
tags:
- noise
- clipformer
- write
- crossbars
- deit-s
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This work proposes ClipFormer, a transformation applied during
  inference on the dynamically-generated Key (K) and Value (V) matrices in Vision
  Transformers (ViTs) to improve their non-ideal accuracies on memristive crossbars.
  Specifically, ClipFormer constricts the distribution of synaptic conductances corresponding
  to the K and V matrices towards zero to mitigate the impact of write noise during
  inference.
---

# ClipTransformer: Key-Value Clipping of Transformers on Memristive Crossbars for Write Noise Mitigation

## Quick Facts
- arXiv ID: 2402.02586
- Source URL: https://arxiv.org/abs/2402.02586
- Authors: Abhiroop Bhattacharjee; Abhishek Moitra; Priyadarshini Panda
- Reference count: 33
- Primary result: ClipTransformer improves non-ideal ViT accuracies by >40% in high write noise regimes without hardware or training overhead.

## Executive Summary
This paper proposes ClipTransformer, a transformation applied during inference on the dynamically-generated Key (K) and Value (V) matrices in Vision Transformers (ViTs) to improve their non-ideal accuracies on memristive crossbars. Specifically, ClipTransformer constricts the distribution of synaptic conductances corresponding to the K and V matrices towards zero to mitigate the impact of write noise during inference. Experiments using pre-trained DeiT-S and LV-ViT-S models on the Imagenet-1k dataset show that ClipTransformer boosts non-ideal accuracies by > 40% in high write noise regimes compared to untransformed models, without requiring additional hardware or training overhead.

## Method Summary
ClipTransformer applies a two-stage transformation to the K and V matrices during crossbar mapping: first shifting all conductances upward by a multiple of GMIN, then clipping values at a fraction of GMAX. This reduces the number of high-conductance elements, which are more susceptible to write noise, thereby improving signal-to-noise ratio (SNR) and final model accuracy under non-ideal crossbar conditions. The transformation is inference-only, requiring no retraining, and yields hardware efficiency gains by reducing the number of bit-slices needed to represent the matrices.

## Key Results
- ClipTransformer improves non-ideal ViT accuracies by >40% in high write noise regimes (γ=4,5) compared to untransformed models.
- The transformation reduces the number of bit-slices needed to represent K and V matrices, yielding ~7-8% reduction in total attention area and energy.
- Higher SNR in attention layers under transformed K and V distributions correlates with improved accuracy under high write noise.

## Why This Works (Mechanism)

### Mechanism 1
- Claim: ClipTransformer reduces write noise impact by biasing the conductance distribution of K and V matrices toward lower values.
- Mechanism: The transformation applies two stages—first shifting all conductances upward by a multiple of GMIN, then clipping values at a fraction of GMAX. This reduces the number of high-conductance elements, which are more susceptible to write noise.
- Core assumption: Lower conductances incur less write noise perturbation than higher ones in memristive crossbars.
- Evidence anchors:
  - [abstract] "Specifically, ClipTransformer constricts the distribution of synaptic conductances corresponding to the K and V matrices towards zero to mitigate the impact of write noise during inference."
  - [section] "We find that the transformation ClipTransformer (α = 1 & β = 1) constricts the distribution of K & V towards zero."
- Break condition: If write noise does not scale with conductance magnitude, or if the transformation excessively shrinks the effective range of K/V values, accuracy could degrade.

### Mechanism 2
- Claim: Clipping K and V values reduces the number of bit-slices needed in hardware, saving area and energy.
- Mechanism: By limiting the maximum conductance to a fraction of GMAX (e.g., 0.25 × GMAX), fewer RRAM cells are required to represent the matrices, reducing crossbar size and associated peripherals.
- Core assumption: The dynamic range of K/V values can be compressed without significantly harming model expressiveness.
- Evidence anchors:
  - [section] "Consequently with bit-slicing, three RRAM-based crossbars... can emulate an array with 6-bit conductances, thereby bringing in reduction in number of crossbars."
  - [section] "Using the ViT-X framework, a ~7 – 8% reduction in the total attention area and energy is seen..."
- Break condition: If the reduced conductance range causes significant information loss, accuracy may drop despite hardware gains.

### Mechanism 3
- Claim: ClipTransformer improves signal-to-noise ratio (SNR) in attention layers, especially under high write noise.
- Mechanism: By shifting more conductances to lower values, the relative noise contribution decreases, increasing SNR as measured in the S(QKT)V operations.
- Core assumption: SNR in crossbar-based attention layers is a good predictor of final model accuracy under noise.
- Evidence anchors:
  - [section] "Higher the SNR, greater the immunity of the deployed ViT model to hardware noise."
  - [section] "Fig. 6(right) plots the SNR averaged across all the 12 attention blocks... against different write noise factors (γ). For the higher device write noise regimes (γ = 4 & γ = 5), on increasing the rigor of transformation... there is a steady increase in <SNR>."
- Break condition: If the relationship between SNR and accuracy breaks down at very low or very high noise levels, SNR gains may not translate to accuracy gains.

## Foundational Learning

- Concept: In-memory computing with memristive crossbars
  - Why needed here: The paper’s core contribution is about transforming K and V matrices to mitigate noise in crossbar-based inference.
  - Quick check question: What is the difference between read noise and write noise in memristive crossbars, and how do they affect inference accuracy?

- Concept: Vision Transformer (ViT) attention mechanism
  - Why needed here: ClipTransformer targets the K and V matrices generated during ViT attention operations.
  - Quick check question: How are Query (Q), Key (K), and Value (V) matrices generated in a ViT encoder, and why are K and V dynamically generated per input?

- Concept: Signal-to-noise ratio (SNR) in analog computing
  - Why needed here: SNR is used as a metric to quantify the benefit of ClipTransformer under different write noise regimes.
  - Quick check question: How is SNR defined for crossbar-based matrix operations, and why does it correlate with inference accuracy?

## Architecture Onboarding

- Component map: Pre-trained ViT model -> K and V matrix generation -> ClipTransformer transformation -> Crossbar mapping -> ViT-X noise simulation -> Accuracy and hardware metrics
- Critical path:
  1. Load pre-trained ViT model weights
  2. For each input, generate K and V matrices in attention layers
  3. Apply ClipTransformer transformation to K and V conductances
  4. Map transformed matrices to crossbars
  5. Simulate read and write noise effects
  6. Compute final accuracy and hardware metrics
- Design tradeoffs:
  - Accuracy vs. hardware efficiency: More aggressive ClipTransformer (lower β) yields greater area/energy savings but may hurt accuracy
  - Training vs. inference: ClipTransformer is inference-only, avoiding retraining costs but potentially missing dataset-specific optimizations
  - Generality vs. specificity: Works on any memristive crossbar but may not be optimal for all noise profiles
- Failure signatures:
  - Accuracy drops sharply when β is too low or α is too high, especially in low noise regimes
  - Hardware savings diminish if K and V distributions are already concentrated at low conductances
  - Model becomes unstable if transformation disrupts critical attention dynamics
- First 3 experiments:
  1. Run a baseline ViT model (e.g., DeiT-S) on ViT-X without ClipTransformer, varying γ from 3 to 5, record accuracy and SNR.
  2. Apply ClipTransformer with (α=1, β=1) to the same model, repeat crossbar noise simulation, compare accuracy and SNR.
  3. Sweep α and β values (e.g., (1,1), (1,0.25), (2,1), (2,0.25)) and plot accuracy vs. γ to find optimal transformation settings.

## Open Questions the Paper Calls Out
None

## Limitations
- The exact relationship between write noise and conductance magnitude is assumed but not experimentally verified.
- The generalizability of ClipTransformer across different ViT architectures and datasets is not demonstrated.
- The long-term reliability of models under continuous inference with transformed K and V matrices is unknown.

## Confidence
- Medium: The proposed mechanism of biasing K and V matrices toward lower conductances is theoretically sound and supported by SNR improvements, but the evidence for accuracy gains under real-world noise is limited to simulations within the ViT-X framework.

## Next Checks
1. Validate the noise-conductance relationship experimentally using fabricated or emulated memristive crossbars.
2. Test ClipTransformer on a wider range of ViT models (e.g., DeiT-T, Swin) and datasets (e.g., CIFAR-100, VTAB).
3. Benchmark hardware efficiency gains against alternative crossbar mapping methods (e.g., MDM, weight sorting).