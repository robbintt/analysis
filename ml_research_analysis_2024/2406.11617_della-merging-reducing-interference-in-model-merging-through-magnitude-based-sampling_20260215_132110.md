---
ver: rpa2
title: 'DELLA-Merging: Reducing Interference in Model Merging through Magnitude-Based
  Sampling'
arxiv_id: '2406.11617'
source_url: https://arxiv.org/abs/2406.11617
tags:
- della
- parameters
- merging
- delta
- pruning
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces DELLA-Merging, a method for merging homologous
  models by reducing interference between their delta parameters. DELLA employs MAGPRUNE,
  a magnitude-based pruning technique that assigns higher dropout probabilities to
  lower-magnitude parameters and rescales survivors by 1/(1-p) to preserve embedding
  quality.
---

# DELLA-Merging: Reducing Interference in Model Merging through Magnitude-Based Sampling

## Quick Facts
- **arXiv ID**: 2406.11617
- **Source URL**: https://arxiv.org/abs/2406.11617
- **Reference count**: 36
- **Primary result**: DELLA-Merging achieves 2.4 points average improvement over baseline methods using delta parameter pruning

## Executive Summary
DELLA-Merging introduces MAGPRUNE, a magnitude-based pruning technique that reduces interference when merging homologous models by preferentially dropping low-magnitude delta parameters. The method ranks parameters by magnitude, assigns higher dropout probabilities to lower-ranked parameters, and rescales survivors by 1/(1-p) to preserve embedding quality. Through a three-step process (Drop, Elect, Fuse), DELLA demonstrates consistent improvements across LM, Math, and Code expert merging tasks, with specific gains of 3.6 points over TIES and 11.1 points over no-pruning baselines.

## Method Summary
DELLA-Merging operates on delta parameters (θt - θ) between fine-tuned and base models. The MAGPRUNE component ranks delta parameters by magnitude and assigns drop probabilities inversely proportional to rank, with survivors rescaled by 1/(1-p) to maintain expected embedding structure. The Elect step selects parameters with consistent signs across experts to minimize directional interference, and the Fuse step averages selected deltas and adds them to the base model. The approach requires homologous models with shared backbones and performs hyperparameter search for drop rate p and scaling factor λ.

## Key Results
- DELLA achieves 2.4 points average improvement over baseline methods using delta parameter pruning
- MAGPRUNE outperforms other pruning approaches, particularly at higher pruning ratios
- Specific gains of 3.6 points over TIES and 1.2 points over DARE
- 11.1 points improvement over no-pruning baseline
- Scaling factor validation shows 28.4-28.7 point improvement when rescaling is applied

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: MAGPRUNE reduces interference by preferentially dropping low-magnitude delta parameters, thereby removing parameters less essential to task-specific performance.
- **Mechanism**: Parameters are ranked by magnitude, and drop probabilities are assigned inversely proportional to this magnitude. Lower-magnitude parameters receive higher dropout probabilities, while higher-magnitude parameters are preserved. Survivors are rescaled by 1/(1-p) to maintain embedding fidelity.
- **Core assumption**: Low-magnitude delta parameters contribute less to task performance and are more likely to cause interference when merged.
- **Evidence anchors**:
  - [abstract] "MAGPRUNE first ranks the parameters in order of their magnitude and assigns higher dropout probabilities (p) to parameters with lower ranks corresponding to lower magnitudes."
  - [section 2.1] "For each node in the neural architecture, we map its delta parameters {δ1, . . . , δn} to drop probabilities Pd = {p1, . . . , pn} in the inverse order of their magnitudes."
  - [corpus] Weak: No direct experimental comparison showing low-magnitude parameters cause more interference, but inference supported by magnitude-based pruning literature.
- **Break condition**: If high-magnitude parameters are actually critical for task-specific fine details, pruning them would degrade performance more than expected.

### Mechanism 2
- **Claim**: Rescaling survivors by 1/(1-p) preserves the expected embedding structure lost during dropout.
- **Mechanism**: The rescaling compensates for the expected value reduction caused by random dropping, keeping the weighted sum of deltas consistent with the original model.
- **Core assumption**: The original embedding can be approximated by maintaining the expected value of the linear transformation after pruning.
- **Evidence anchors**:
  - [section 2.3] Theoretical analysis shows that with scaling γ = 1/(1-pi), E[ĥ] = h + ∆h, preserving the original embedding.
  - [section 2.2] "To compensate for the effect of pruning on the model's embedding, we rescale the remaining delta parameters by 1/(1 − pi)."
  - [section 4] Table 3 shows scaling improves performance by 28.4-28.7 points on average, empirically validating the theoretical claim.
- **Break condition**: If the distribution of delta parameters is highly skewed, the simple scaling factor may not fully preserve higher-order statistics.

### Mechanism 3
- **Claim**: The Elect step reduces directional interference by selecting only parameters with consistent signs across experts.
- **Mechanism**: The sign of the sum of delta parameters at each position determines the dominant direction. Only parameters matching this sign are retained for merging, minimizing opposing gradient effects.
- **Core assumption**: Parameters with opposite signs across experts represent conflicting updates that degrade merged model performance.
- **Evidence anchors**:
  - [section 2.1] "Elect the delta parameters that will take part in merging... minimizes directional discrepancy in delta parameters."
  - [section 4] Table 4 shows performance drops when Elect is omitted, indicating its importance in interference reduction.
  - [corpus] Weak: No explicit ablation on sign consistency, but sign-based selection is common in interference mitigation literature.
- **Break condition**: If the sum of delta parameters is near zero due to balanced positive/negative updates, the sign-based selection may arbitrarily exclude useful parameters.

## Foundational Learning

- **Concept**: Delta parameter arithmetic in model merging
  - Why needed here: DELLA operates on delta parameters (θt - θ), not raw model parameters. Understanding this representation is crucial for interpreting MAGPRUNE and the Elect step.
  - Quick check question: If a fine-tuned model has parameters θt and the base model has θ, what is the delta parameter δt? (Answer: δt = θt - θ)

- **Concept**: Magnitude-based pruning and its theoretical justification
  - Why needed here: MAGPRUNE's effectiveness relies on understanding why low-magnitude parameters are dropped and high-magnitude ones are preserved, including the scaling compensation.
  - Quick check question: Why does rescaling by 1/(1-p) preserve the expected embedding after dropout? (Answer: It compensates for the expected value reduction from random dropping)

- **Concept**: Interference in multi-task model merging
  - Why needed here: The core motivation for DELLA is reducing interference between task-specific models during merging, which affects both Elect and MAGPRUNE design choices.
  - Quick check question: What happens if two fine-tuned models have delta parameters with opposite signs at the same position? (Answer: They may cancel each other out during merging, causing performance degradation)

## Architecture Onboarding

- **Component map**: Input models → MAGPRUNE ranking and sampling → Elect sign-based selection → Fuse weighted averaging → Merged output
- **Critical path**: MAGPRUNE → Elect → Fuse (each step builds on the previous)
- **Design tradeoffs**:
  - Row-wise vs layer-wise ranking: Row-wise performed better (Figure 6), likely because it preserves local parameter relationships
  - Fixed vs adaptive λ: Constant λ performed better (Figure 5), simplifying hyperparameter tuning
  - Drop rate selection: Higher rates preserve more information but increase interference risk
- **Failure signatures**:
  - Performance degradation on specific tasks after merging (e.g., GSM8K in LM+Math+Code)
  - Over-pruning leading to loss of task-specific capabilities
  - Sign conflicts not fully resolved, causing residual interference
- **First 3 experiments**:
  1. Compare MAGPRUNE vs DARE on single expert pruning to verify magnitude-based selection advantage
  2. Test effect of scaling (with vs without) on expert performance to validate theoretical compensation
  3. Ablate Elect step to quantify its contribution to interference reduction in merged models

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does DELLA perform when merging models with different backbones or architectures?
- Basis in paper: [inferred] The paper mentions that "Similar to DARE, TIES and TA, our approach is only effective for models with the same backbone model" and suggests exploring merging models with different pre-trained backbones as future work.
- Why unresolved: The experiments only tested DELLA on homologous models with the same backbone. No experiments were conducted to validate performance on models with different architectures.
- What evidence would resolve it: Conducting experiments that merge models with different backbones (e.g., LLaMA-2 with CodeLlama) and comparing DELLA's performance against baseline methods would provide evidence of its generalizability.

### Open Question 2
- Question: What is the impact of the ranking method (row-wise vs. layer-wise) on DELLA's performance across different model architectures and tasks?
- Basis in paper: [explicit] The paper discusses that "row-wise ranking achieves a higher average score than the layer-wise approach across the 4 merge combinations using DELLA p∝r" but notes this was tested only on a specific setup.
- Why unresolved: The paper only tested the ranking method on Wizard models for specific tasks. It's unclear if this holds true for other model architectures or tasks.
- What evidence would resolve it: Systematically testing both ranking methods (row-wise and layer-wise) across various model architectures (e.g., different sizes, pre-training approaches) and diverse tasks would clarify the general effectiveness of each ranking strategy.

### Open Question 3
- Question: How does the choice of scaling factor (λ) affect the performance of merged models across different merge combinations and tasks?
- Basis in paper: [explicit] The paper mentions that "Using Constant λ scaling leads to a notable improvement across all merging approaches" and shows hyperparameter search results for λ, but only for specific merge combinations.
- Why unresolved: The optimal λ value may vary depending on the specific models being merged and the tasks involved. The paper only provides results for a limited set of merge combinations.
- What evidence would resolve it: Conducting a comprehensive hyperparameter search for λ across different merge combinations and tasks would provide insights into how the scaling factor affects performance in various scenarios.

### Open Question 4
- Question: How does the MAGPRUNE method compare to other magnitude-based pruning techniques in terms of parameter efficiency and task-specific performance?
- Basis in paper: [explicit] The paper compares MAGPRUNE to DARE and TIES but doesn't explore other magnitude-based pruning techniques or provide a detailed analysis of parameter efficiency.
- Why unresolved: While MAGPRUNE shows improvements over DARE and TIES, there might be other magnitude-based pruning methods that could perform better or offer different trade-offs between parameter efficiency and performance.
- What evidence would resolve it: Comparing MAGPRUNE to a wider range of magnitude-based pruning techniques (e.g., layer-adaptive sparsity, dynamic sparse training) and analyzing the trade-offs between parameter efficiency and task-specific performance would provide a more comprehensive understanding of its effectiveness.

## Limitations

- Magnitude-pruning correlation with interference lacks direct experimental validation; relies on general pruning literature
- Rescaling factor universality assumes linear relationships that may not hold for all delta parameter distributions
- Sign-based selection may arbitrarily exclude useful parameters when delta sums are near zero due to balanced updates

## Confidence

- **High confidence**: MAGPRUNE's empirical effectiveness (2.4 point average improvement, 11.1 point improvement over no-pruning baseline), and the theoretical basis for rescaling survivors
- **Medium confidence**: The claim that MAGPRUNE reduces interference through magnitude-based sampling, as this relies on indirect evidence and general pruning principles
- **Medium confidence**: The Elect step's role in reducing directional interference, supported by ablation results but without deeper analysis of sign consistency patterns

## Next Checks

1. **Ablation study on magnitude correlation**: Compare MAGPRUNE performance against random pruning at different drop rates to isolate the contribution of magnitude-based selection to interference reduction.

2. **Distribution analysis**: Characterize the distribution of delta parameters across experts to validate whether the linear scaling assumption holds, particularly examining skewness and higher-order moments.

3. **Sign consistency patterns**: Analyze the frequency and magnitude of sign conflicts across parameter positions to determine whether the Elect step's heuristic selection is optimal or if alternative approaches might perform better.