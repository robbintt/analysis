---
ver: rpa2
title: 'SCE-MAE: Selective Correspondence Enhancement with Masked Autoencoder for
  Self-Supervised Landmark Estimation'
arxiv_id: '2405.18322'
source_url: https://arxiv.org/abs/2405.18322
tags: []
core_contribution: This paper introduces SCE-MAE, a self-supervised framework for
  facial landmark estimation. The method uses MAE pretraining in the first stage to
  generate initial facial representations, then employs a Correspondence Approximation
  and Refinement Block (CARB) in the second stage to selectively refine important
  local correspondences using a novel Locality-Constrained Repellence Loss.
---

# SCE-MAE: Selective Correspondence Enhancement with Masked Autoencoder for Self-Supervised Landmark Estimation

## Quick Facts
- **arXiv ID**: 2405.18322
- **Source URL**: https://arxiv.org/abs/2405.18322
- **Reference count**: 40
- **Primary result**: Outperforms state-of-the-art by 20%-44% on landmark matching and 9%-15% on landmark detection

## Executive Summary
SCE-MAE introduces a two-stage self-supervised framework for facial landmark estimation that combines Masked Autoencoder (MAE) pretraining with selective correspondence refinement. The method first uses MAE to generate discriminative facial landmark representations through masked reconstruction, then employs a Correspondence Approximation and Refinement Block (CARB) with Locality-Constrained Repellence Loss to selectively refine important local correspondences. This approach achieves state-of-the-art performance across multiple facial landmark benchmarks while maintaining computational efficiency.

## Method Summary
SCE-MAE operates in two stages: (1) MAE pretraining on CelebA dataset to generate initial facial representations, and (2) CARB refinement using attentive-inattentive separation and clustering to selectively enhance landmark correspondences. The method uses density peak clustering to reduce non-landmark tokens to cluster centers, then applies LCR loss to refine only the most important correspondences. For landmark detection, features are expanded via cover-and-stride and fed to a projector and regressor.

## Key Results
- Achieves 20%-44% improvement over state-of-the-art methods on landmark matching
- Improves landmark detection by 9%-15% across multiple benchmarks
- Demonstrates superior performance with moderate-sized Vision Transformer architectures

## Why This Works (Mechanism)

### Mechanism 1
MAE pretraining generates discriminative landmark representations because facial landmarks are sparse and texture-rich, making their reconstruction dominate the masked reconstruction loss. This forces the encoder to form fine-grained local features well-suited for dense prediction tasks.

### Mechanism 2
CARB with LCR loss selectively refines important correspondences by identifying landmark vs. non-landmark tokens using CLS-token similarity, clustering non-landmark tokens to reduce redundancy, and applying locality-constrained penalties only to erroneous correspondence pairs.

### Mechanism 3
The locality constraint discourages erroneous correspondences between spatially distant patches by adding distance-based penalties weighted by correspondence type, forcing the network to strengthen only nearby, semantically consistent matches.

## Foundational Learning

- **Masked Image Modeling (MIM) and Masked Autoencoders (MAE)**: Essential for understanding why MAE pretraining generates better local features than instance-level SSL for landmark tasks. Quick check: Why does masking a high proportion of patches in MAE lead to better semantic features for dense prediction?

- **Vision Transformer (ViT) architecture and positional embeddings**: Critical for understanding patch token processing, CLS token aggregation, and attentive-inattentive separation. Quick check: How does the CLS token aggregate information from patch tokens for landmark region identification?

- **Clustering algorithms and density-based clustering**: Necessary to grasp how non-landmark tokens are grouped into cluster centers to reduce redundancy. Quick check: What criteria determine cluster center selection in density peak clustering for non-landmark tokens?

## Architecture Onboarding

- **Component map**: MAE pretraining -> Attentive-inattentive separation -> Density peak clustering -> CARB with LCR loss -> Projector/Regressor
- **Critical path**: 1) MAE pretraining â†’ frozen backbone 2) Attentive-inattentive separation using CLS-token similarity 3) Density peak clustering of inattentive tokens 4) CARB: substitute inattentive tokens with cluster centers, apply LCR loss 5) Inference: bypass clustering, use original features + cover-and-stride expansion
- **Design tradeoffs**: MAE vs. instance-level SSL (better local features vs. simpler training), selective vs. full correspondence (computation vs. completeness), clustering vs. no clustering (redundancy reduction vs. complexity)
- **Failure signatures**: Poor landmark matching if clustering merges landmark-adjacent regions, low detection accuracy if attentive rate mis-tuned, training instability if LCR hyperparameters poorly tuned
- **First 3 experiments**: 1) Replace MAE with MoCo backbone, keep CARB and LCR; measure performance drop 2) Remove clustering, apply LCR loss to all token pairs; measure computational cost and performance 3) Vary attentive rate (0.1, 0.25, 0.4) and cluster number (2, 4, 8); identify best settings

## Open Questions the Paper Calls Out

The paper acknowledges limitations including dependency on face crop quality and potential distraction from other salient objects, but does not propose specific solutions for these challenges.

## Limitations

- Effectiveness of MAE pretraining relies on unproven assumption about landmark reconstruction cost dominance
- Selective correspondence efficiency gains claimed but not explicitly measured against full correspondence baselines
- Locality constraint contribution not empirically isolated from other LCR loss components

## Confidence

- **MAE pretraining yields superior landmark representations (High)**: Strong empirical support from 20-44% improvement over MoCo/BYOL
- **Selective correspondence improves accuracy while reducing computation (Medium)**: Accuracy gains demonstrated, but computational savings unverified
- **Locality constraint meaningfully improves landmark disambiguation (Low)**: Design intuition-based claim lacking empirical isolation

## Next Checks

1. **Ablation of locality constraint**: Train SCE-MAE variants with and without locality penalty on MAFL landmark matching to quantify its contribution
2. **Computational complexity analysis**: Profile training and inference time for SCE-MAE versus full correspondence baseline, reporting FLOPs, memory, and wall-clock time
3. **Robustness to pose variation**: Evaluate SCE-MAE on AFLW-Full and challenging 300W subsets to test spatial locality assumption breakdown