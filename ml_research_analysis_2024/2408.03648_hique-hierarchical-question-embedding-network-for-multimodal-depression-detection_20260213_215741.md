---
ver: rpa2
title: 'HiQuE: Hierarchical Question Embedding Network for Multimodal Depression Detection'
arxiv_id: '2408.03648'
source_url: https://arxiv.org/abs/2408.03648
tags:
- depression
- primary
- detection
- question
- questions
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes HiQuE, a novel multimodal depression detection
  framework that leverages the hierarchical relationship between primary and follow-up
  questions in clinical interviews. HiQuE addresses the limitation of previous approaches
  that treat entire interview sequences as single entities, instead explicitly modeling
  the structured nature of clinical interviews.
---

# HiQuE: Hierarchical Question Embedding Network for Multimodal Depression Detection

## Quick Facts
- arXiv ID: 2408.03648
- Source URL: https://arxiv.org/abs/2408.03648
- Authors: Juho Jung; Chaewon Kang; Jeewoo Yoon; Seungbae Kim; Jinyoung Han
- Reference count: 40
- Primary result: Achieves state-of-the-art F1-score of 0.82 on DAIC-WOZ dataset for multimodal depression detection

## Executive Summary
This paper proposes HiQuE, a novel multimodal depression detection framework that leverages the hierarchical relationship between primary and follow-up questions in clinical interviews. HiQuE addresses the limitation of previous approaches that treat entire interview sequences as single entities, instead explicitly modeling the structured nature of clinical interviews. The model incorporates a hierarchical embedding structure and interview-specific attention modules to assess mutual information across multiple modalities (audio, visual, and text). HiQuE achieves state-of-the-art performance on the DAIC-WOZ dataset, outperforming other multimodal depression detection and emotion recognition models with a weighted average F1-score of 0.82 and G-mean score of 0.790.

## Method Summary
HiQuE processes clinical interviews by first segmenting them into primary and follow-up questions based on hierarchical positions. For each modality (audio, visual, text), the model extracts features (88 audio functionals, 68 facial landmarks, RoBERTa text embeddings) and applies hierarchical positional embeddings. A question-aware transformer encoder captures attention between questionnaire responses, while cross-modal attention layers with bidirectional attention between audio-visual, visual-text, and text-audio modalities learn complementary information. The depression detection layer uses global average pooling and classification. The model is trained for 100 epochs with batch size 8, learning rate 0.0002, and dropout 0.5, using data augmentation through random masking of 10 questions per interview.

## Key Results
- Achieves weighted average F1-score of 0.82 and G-mean score of 0.790 on DAIC-WOZ dataset
- Outperforms other multimodal depression detection and emotion recognition models including Transformer, ViVi, and MISA
- Demonstrates superior generalizability to unseen question scenarios on E-DAIC-WOZ (F1=0.64) and MIT Interview (F1=0.72) datasets

## Why This Works (Mechanism)

### Mechanism 1: Hierarchical Question Embedding
Hierarchical question embedding improves depression detection by explicitly modeling the structured nature of clinical interviews. The model segments interview sequences into primary and follow-up questions, allowing it to capture the importance of each question type in diagnosing depression. This mirrors the clinical interview strategy used by mental health professionals. The hierarchical relationship between primary and follow-up questions contains diagnostically relevant information that is lost when treating the entire interview as a single sequence.

### Mechanism 2: Cross-Modal Attention
Cross-modal attention between different modalities improves depression detection by capturing complementary information. The model uses bidirectional cross-attention between audio, visual, and text modalities to learn relevant information across modalities. This allows the model to capture both inter- and intra-modal representations. Different modalities contain complementary information about depression that can be better captured through cross-attention rather than simple concatenation or fusion.

### Mechanism 3: Question-Aware Self-Attention
Question-aware self-attention improves performance by focusing on important questions within the embedded representation. The transformer encoder with multi-heads captures attention between questionnaire responses, allowing the model to focus on important segments and relationships among question-embedded sequences. Not all questions contribute equally to depression detection, and the model can learn which questions are more diagnostically relevant.

## Foundational Learning

**Concept: Multimodal depression detection**
Why needed here: Depression manifests through multiple channels (speech patterns, facial expressions, language use), and combining these modalities provides more comprehensive diagnostic information than any single modality alone.
Quick check question: What are the three primary modalities used in this depression detection framework?

**Concept: Hierarchical modeling of clinical interviews**
Why needed here: Clinical interviews follow a structured format with primary questions and follow-up questions, and this structure contains diagnostically relevant information about depression that flat models miss.
Quick check question: How does the model distinguish between primary and follow-up questions in the interview structure?

**Concept: Cross-modal attention mechanisms**
Why needed here: Different modalities provide complementary information about depression, and cross-modal attention allows the model to learn which modality is most relevant for different types of questions.
Quick check question: What is the purpose of using bidirectional cross-attention between modalities?

## Architecture Onboarding

**Component map**: Hierarchical question embedding → feature extraction (audio/visual/text) → question-aware transformer encoder → cross-modal attention (audio-visual, visual-text, text-audio) → depression detection layer (global average pooling + classification)

**Critical path**: The most critical path is: hierarchical question embedding → feature extraction → question-aware self-attention → cross-modal attention → depression detection. Each layer builds on the previous one's representation.

**Design tradeoffs**: The model trades computational complexity (multiple attention layers, hierarchical processing) for improved performance by capturing interview structure. The question embedding adds preprocessing overhead but enables structured analysis.

**Failure signatures**: If the model fails, check: (1) incorrect hierarchical segmentation of questions, (2) poor feature extraction from one or more modalities, (3) attention weights collapsing to uniform distributions, (4) overfitting due to complex architecture on limited data.

**First 3 experiments**:
1. Ablation study: Remove hierarchical question embedding and compare performance to confirm its importance.
2. Modality ablation: Train with only audio, only visual, only text to quantify each modality's contribution.
3. Attention visualization: Plot attention weight distributions to verify the model is focusing on diagnostically relevant questions and modalities.

## Open Questions the Paper Calls Out
The paper does not explicitly call out open questions, but several implicit questions emerge from the work.

## Limitations
- Limited generalizability shown by performance drop on external datasets (F1=0.64 on E-DAIC-WOZ, F1=0.72 on MIT Interview) despite strong performance on DAIC-WOZ
- Small test set size (47 samples) raises concerns about statistical significance of the reported results
- Lack of direct ablation studies to isolate the contribution of hierarchical modeling versus increased model complexity

## Confidence

**High confidence**: The core methodology (hierarchical question segmentation, cross-modal attention architecture, and overall performance metrics) is well-documented and reproducible. The improvement over baseline models is statistically significant.

**Medium confidence**: The claim that hierarchical modeling specifically captures diagnostically relevant information is plausible but not directly validated through ablation studies. The performance gains could partly stem from increased model complexity rather than the hierarchical structure itself.

**Low confidence**: The generalizability claims are supported by transfer results but the performance drop on external datasets suggests the model may be overfitting to DAIC-WOZ's specific interview patterns and question structures.

## Next Checks

1. **Direct ablation study**: Implement a flat version of the model (treating entire interview as single sequence) and compare performance with the hierarchical version on the same dataset to isolate the contribution of hierarchical modeling.

2. **Modality contribution analysis**: Train three separate models using only audio, only visual, and only text modalities to quantify each modality's individual contribution and verify that cross-modal attention provides synergistic benefits beyond simple combination.

3. **Attention visualization validation**: Generate and analyze attention weight distributions across different modalities for key diagnostic questions to confirm the model is focusing on clinically meaningful patterns rather than artifacts of the training data.