---
ver: rpa2
title: Convolutional Kolmogorov-Arnold Networks
arxiv_id: '2406.13155'
source_url: https://arxiv.org/abs/2406.13155
tags:
- convolutional
- kans
- convolutions
- networks
- accuracy
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Convolutional Kolmogorov-Arnold Networks
  (CKANs), a novel architecture that integrates learnable spline-based activation
  functions from KANs into convolutional layers. Unlike traditional CNNs with fixed-weight
  kernels, CKANs employ kernels composed of learnable non-linear functions parameterized
  using B-splines, enabling greater expressiveness and parameter efficiency.
---

# Convolutional Kolmogorov-Arnold Networks

## Quick Facts
- arXiv ID: 2406.13155
- Source URL: https://arxiv.org/abs/2406.13155
- Reference count: 10
- Primary result: CKANs achieve competitive accuracy on Fashion-MNIST with up to 50% fewer parameters than CNNs

## Executive Summary
This paper introduces Convolutional Kolmogorov-Arnold Networks (CKANs), which integrate learnable spline-based activation functions from KANs into convolutional layers. Unlike traditional CNNs with fixed-weight kernels, CKANs employ kernels composed of learnable non-linear functions parameterized using B-splines, enabling greater expressiveness and parameter efficiency. The authors demonstrate that CKANs can capture complex spatial relationships with fewer resources, making them a promising direction for parameter-efficient deep learning models in computer vision.

## Method Summary
CKANs replace traditional fixed-weight convolutional kernels with learnable non-linear functions parameterized by B-splines. Each kernel element $\phi_{i,j}$ is defined as a weighted combination of a spline and a SiLU function, allowing each pixel to undergo a nonlinear transformation before summation. The architecture is evaluated on Fashion-MNIST using a grid search over learning rates, weight decay, batch sizes, and B-spline grid sizes. The study compares CKANs against classic CNNs across different model sizes to assess parameter efficiency and accuracy trade-offs.

## Key Results
- KANC MLP (Medium) achieves 88.99% accuracy with approximately 9K parameters, compared to CNN (Big) with 89.44% accuracy using 26K parameters
- CKAN convolutions learn more per kernel, outperforming classic convolutions in smaller models
- In larger models with more fully connected layers, classic CNNs show a slight advantage over CKANs

## Why This Works (Mechanism)

### Mechanism 1
- KAN convolutional layers replace fixed linear weights with learnable spline-based functions, allowing each pixel to undergo a nonlinear transformation before summation
- Each kernel element $\phi_{i,j}$ is defined as a weighted combination of a spline and a SiLU function, enabling outputs that can encode conditional or piecewise logic within a single pass
- Core assumption: Splines parameterized by B-splines can approximate complex nonlinear relationships well enough to capture the advantages claimed

### Mechanism 2
- The per-pixel nonlinearity in KAN kernels enables encoding of context-dependent behaviors that linear filters cannot represent without extra steps
- Each $\phi_{i,j}$ can apply different gains or thresholds depending on the pixel intensity, achieving conditional scaling within one convolution pass
- Core assumption: Different pixels in a neighborhood can be treated independently for the purpose of nonlinear scaling without losing spatial coherence

### Mechanism 3
- Fewer fully connected layers are needed after flattening because the convolutional stage already captures complex relationships
- KAN convolutions are more expressive per kernel, so subsequent dense layers can be shallower or smaller while maintaining accuracy
- Core assumption: The added expressibility of per-pixel splines compensates for the increased kernel parameter count

## Foundational Learning

- **B-spline basis functions and control point parametrization**: Why needed - The learnable functions in KAN layers are defined as weighted sums of B-spline basis functions; understanding their shape and continuity properties is essential to tuning grid size and degree. Quick check - What effect does increasing the grid size have on the number of learnable parameters per $\phi$?

- **Grid extension and update mechanism**: Why needed - During training, inputs can fall outside the default spline range; the grid must be expanded to keep the spline active rather than defaulting to SiLU. Quick check - When and how is the grid updated during training, and why is this critical for maintaining KAN behavior?

- **Difference between node-based activations (MLP) and edge-based learnable functions (KAN)**: Why needed - In MLPs, nonlinearities are applied after linear combinations; in KANs, the nonlinearity is intrinsic to the weight-like edges, changing how gradients flow and parameters are shared. Quick check - How does placing learnable functions on edges instead of nodes affect the number of parameters and the training dynamics?

## Architecture Onboarding

- **Component map**: Input → KAN Convolutional Layer(s) → Max Pooling → Flatten → (MLP or KAN dense layer) → Log Softmax
- **Critical path**: 1. Initialize spline grid with default range (e.g., [-1, 1]) 2. During forward pass, apply each $\phi_{i,j}$ to corresponding pixel, sum results 3. If any input falls outside grid range, trigger grid extension before next forward 4. Compute loss (cross-entropy + optional KAN regularization) 5. Backpropagate through spline control points and SiLU weights
- **Design tradeoffs**: Larger grid_size → more parameters, finer function approximation, slower training; Spline degree 3 (cubic) balances smoothness and expressiveness; BatchNorm can reduce need for grid updates but adds parameters
- **Failure signatures**: Accuracy plateaus early with large grid_size → overparameterization; Training diverges or slows dramatically → grid not updated, splines stuck in SiLU regime; No improvement over CNN despite higher parameter count → splines not learning useful functions
- **First 3 experiments**: 1. Implement KAN conv layer with small grid (size 4) and degree 3, run on Fashion-MNIST with 1 conv + 1 FC; compare params and accuracy to equivalent CNN. 2. Add grid update logic; train same setup but with varied input ranges; measure if accuracy improves vs static grid. 3. Replace dense layer with KAN dense; compare total params and accuracy vs CNN with 2 FC layers; analyze trade-off.

## Open Questions the Paper Calls Out

### Open Question 1
- **Question**: Do KAN convolutions consistently outperform classic convolutions in larger models with deeper architectures, or does the advantage of classic convolutions observed in the study persist?
- **Basis in paper**: The authors note that in larger models with more fully connected layers, classic CNNs show a slight advantage over KAN convolutions
- **Why unresolved**: The study only tested up to a certain model size, and the observed trend may not hold for even larger or deeper architectures
- **What evidence would resolve it**: Conducting experiments on larger models with deeper architectures and varying datasets would clarify whether KAN convolutions maintain their advantage

### Open Question 2
- **Question**: How does the choice of B-spline degree affect the performance and parameter efficiency of KAN convolutions?
- **Basis in paper**: The authors chose a B-spline degree of 3 as the default, following the original KAN paper's suggestion, but did not tune this hyperparameter
- **Why unresolved**: The impact of B-spline degree on model performance and parameter efficiency is not explored
- **What evidence would resolve it**: Systematic experimentation with different B-spline degrees (e.g., 2, 3, 4) across various model architectures and datasets would determine the optimal degree

### Open Question 3
- **Question**: Can KAN convolutions be effectively scaled to more complex datasets like CIFAR-10 or ImageNet, and how does their performance compare to CNNs on these datasets?
- **Basis in paper**: The authors acknowledge that experiments on more complex datasets are necessary to verify if the results hold at scale
- **Why unresolved**: The study's scope is limited to Fashion-MNIST, and the performance of KAN convolutions on more complex datasets is unknown
- **What evidence would resolve it**: Training and evaluating KAN convolutions on CIFAR-10, ImageNet, or similar complex datasets would reveal whether the observed advantages extend to more challenging tasks

## Limitations

- Exact implementation details of B-spline basis functions and control point initialization are underspecified, critical for reproducing parameter efficiency claims
- Performance on more complex datasets like CIFAR-10 or ImageNet is unknown, limiting generalizability of results
- The study does not explore the impact of B-spline degree hyperparameter on performance and efficiency

## Confidence

- **Parameter-efficiency claims**: Medium - The reported results show competitive accuracy with fewer parameters, but this depends heavily on implementation details that are underspecified
- **Mechanism explanations**: Medium - The per-pixel nonlinearity concept is sound, but practical benefits over standard convolutions depend on how well spline functions are learned during training
- **Implementation feasibility**: Low - Major uncertainties remain around B-spline basis functions and grid extension logic, which are critical for the claimed advantages

## Next Checks

1. Implement the KAN convolution layer with proper B-spline basis functions and grid extension logic, then verify that accuracy improves when inputs fall outside the default range versus a static grid
2. Compare training time and parameter counts for CKAN vs CNN across multiple random seeds to assess statistical significance of the reported efficiency gains
3. Test whether replacing only the final dense layer with KAN layers (as in KKAN) provides better parameter efficiency than KANC across different dataset complexities