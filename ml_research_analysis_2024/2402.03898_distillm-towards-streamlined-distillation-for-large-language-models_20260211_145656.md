---
ver: rpa2
title: 'DistiLLM: Towards Streamlined Distillation for Large Language Models'
arxiv_id: '2402.03898'
source_url: https://arxiv.org/abs/2402.03898
tags:
- disti
- training
- student
- language
- performance
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper introduces DISTI LLM, a streamlined distillation framework
  for compressing large language models (LLMs) into smaller student models. It addresses
  key limitations in existing methods, such as instability from Kullback-Leibler divergence
  (KLD) loss and inefficiency from using student-generated outputs (SGOs).
---

# DistiLLM: Towards Streamlined Distillation for Large Language Models

## Quick Facts
- **arXiv ID**: 2402.03898
- **Source URL**: https://arxiv.org/abs/2402.03898
- **Reference count**: 40
- **Primary result**: Introduces DISTI LLM, achieving up to 4.3× speedup with state-of-the-art performance on instruction-following, text summarization, and machine translation tasks

## Executive Summary
This paper presents DISTI LLM, a streamlined distillation framework for compressing large language models into smaller student models. The framework addresses two key limitations in existing methods: instability from Kullback-Leibler divergence (KLD) loss and inefficiency from using student-generated outputs. DISTI LLM combines a skew Kullback-Leibler divergence loss for stable gradients and faster convergence with an adaptive off-policy approach that efficiently leverages student-generated outputs by minimizing noisy feedback and improving sample efficiency.

## Method Summary
DISTI LLM is a two-component distillation framework designed to overcome limitations in existing knowledge distillation approaches for LLMs. The first component is a skew Kullback-Leibler divergence loss that ensures stable gradients and faster convergence compared to standard KLD loss. The second component is an adaptive off-policy approach that efficiently leverages student-generated outputs (SGOs) by minimizing noisy feedback and improving sample efficiency. This framework aims to streamline the distillation process while maintaining or improving performance across various NLP tasks.

## Key Results
- Achieves up to 4.3× speedup compared to recent knowledge distillation methods
- Demonstrates state-of-the-art performance on instruction-following, text summarization, and machine translation tasks
- Successfully addresses KLD instability and SGO inefficiency issues in LLM distillation

## Why This Works (Mechanism)
DISTI LLM works by addressing the fundamental challenges in knowledge distillation for large language models. The skew Kullback-Leibler divergence loss provides more stable gradient signals during training, preventing the vanishing gradient problems common with standard KLD loss. The adaptive off-policy approach intelligently samples from student-generated outputs, reducing the noise inherent in self-generated data while maintaining the benefits of using SGOs for more efficient training.

## Foundational Learning
- **Kullback-Leibler Divergence**: A measure of how one probability distribution diverges from another; needed to understand the core loss function in distillation methods
- **Knowledge Distillation**: The process of transferring knowledge from a larger model to a smaller one; essential for understanding the overall approach
- **Off-policy Learning**: A reinforcement learning concept adapted here to selectively use student-generated data; critical for understanding the adaptive sampling mechanism
- **Gradient Stability**: The consistency of gradient signals during training; important for understanding why skew KLD improves convergence
- **Sample Efficiency**: The ability to learn effectively from limited data; relevant for evaluating the adaptive off-policy approach

## Architecture Onboarding
**Component Map**: Student Model <- (Skew KLD Loss + Adaptive Off-Policy Sampling) <- Teacher Model
**Critical Path**: Teacher model output → Skew KLD loss computation → Student model update → Adaptive off-policy sampling → Student model output
**Design Tradeoffs**: The skew KLD loss trades computational complexity for stability and faster convergence, while the adaptive off-policy approach trades some potential information loss for improved sample efficiency and reduced noise.
**Failure Signatures**: Training instability or slow convergence would indicate issues with the skew KLD implementation; poor performance despite high sample efficiency might suggest problems with the adaptive sampling mechanism.
**First Experiments**:
1. Compare training stability and convergence speed between standard KLD and skew KLD loss
2. Evaluate sample efficiency of adaptive off-policy sampling versus random SGO sampling
3. Test the combined framework against baseline distillation methods on a simple text classification task

## Open Questions the Paper Calls Out
None

## Limitations
- Evaluation only benchmarks against a limited set of existing approaches with no comprehensive ablation studies
- Results may not generalize beyond the specific model sizes and configurations tested
- Limited validation on diverse LLM capabilities beyond instruction-following and text generation

## Confidence
- **High confidence**: The core framework architecture and general approach to addressing KLD instability and SGO inefficiency are sound and technically plausible
- **Medium confidence**: The reported 4.3× speedup is likely accurate within the tested configurations, but generalizability to other model pairs and tasks remains uncertain
- **Low confidence**: The claim of state-of-the-art performance across all evaluated tasks, given the limited comparison set and absence of ablation studies

## Next Checks
1. Conduct comprehensive ablation studies to isolate the contribution of skew KLD loss versus adaptive off-policy sampling versus their combination
2. Expand evaluation to include diverse LLM capabilities beyond instruction-following, such as logical reasoning, mathematical problem-solving, and code generation
3. Test DISTI LLM across a wider range of teacher-student size ratios and different model architectures (e.g., decoder-only vs. encoder-decoder) to assess generalizability