---
ver: rpa2
title: Learning to Verify Summary Facts with Fine-Grained LLM Feedback
arxiv_id: '2412.10689'
source_url: https://arxiv.org/abs/2412.10689
tags:
- error
- feedback
- sentence
- summary
- human
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: We address the challenge of training automatic summary fact verifiers
  by exploring the use of Large Language Model (LLM) generated feedback instead of
  human-labeled data. We introduce FineSumFact, a large-scale dataset containing fine-grained
  factual feedback on summaries, generated using 10 distinct LLMs for diverse summary
  generation and Llama-3-70B-Instruct for feedback.
---

# Learning to Verify Summary Facts with Fine-Grained LLM Feedback

## Quick Facts
- arXiv ID: 2412.10689
- Source URL: https://arxiv.org/abs/2412.10689
- Reference count: 20
- Key outcome: Training fact verification models on LLM-generated feedback outperforms models trained on smaller human-annotated datasets while being more cost-effective

## Executive Summary
This paper addresses the challenge of training automatic summary fact verifiers by exploring the use of Large Language Model (LLM) generated feedback instead of human-labeled data. The authors introduce FineSumFact, a large-scale dataset containing fine-grained factual feedback on summaries, generated using 10 distinct LLMs for diverse summary generation and Llama-3-70B-Instruct for feedback. They fine-tune the lightweight open-source model Llama-3-8B-Instruct on this dataset to optimize resource efficiency while maintaining high performance. Experimental results show that the model trained on extensive LLM-generated datasets outperforms that trained on smaller human-annotated datasets when evaluated using human-generated test sets, demonstrating that fine-tuning fact verification models with LLM feedback can be more effective and cost-efficient than using human feedback.

## Method Summary
The paper introduces FineSumFact, a dataset containing 10,877 documents from 7 domains with summaries generated by 10 different LLMs. Llama-3-70B-Instruct provides fine-grained feedback on these summaries, including binary factuality labels, reasoning, and error categorization. The authors fine-tune Llama-3-8B-Instruct on this feedback using QLoRA with a batch size of 32 for 8,000 iterations on 4 NVIDIA H100 GPUs. The fine-tuned model is then evaluated against human-labeled test sets to compare performance with models trained on human-annotated data.

## Key Results
- Models trained on extensive LLM-generated datasets surpass those trained on smaller human-annotated datasets when evaluated using human-generated test sets
- The fine-tuned Llama-3-8B-Instruct achieves performance close to 95% of the teacher model (Llama-3-70B-Instruct) while delivering over 3x faster inference time
- Adding explainable information (reasoning and error localization) to LLM feedback during fine-tuning improves agreement with human judgments

## Why This Works (Mechanism)

### Mechanism 1
Fine-tuning on a large amount of LLM-generated feedback improves fact verification performance more than using limited human-labeled data. The scaling law for LLM suggests that increasing the volume of training data enhances model performance. Since the quality difference between LLM-generated labels and human labels is not significant, increasing the amount of LLM feedback data provides more effective training.

### Mechanism 2
Adding explainable information to LLM feedback during fine-tuning improves agreement with human judgments. Fine-tuning with binary labels alone achieves high balanced accuracy but low system-level correlation. Adding reasoning and error localization to the feedback provides more detailed information that helps the model align better with human evaluation criteria.

### Mechanism 3
Fine-tuning a lightweight open-source model (Llama-3-8B-Instruct) on LLM feedback achieves near-teacher performance with significantly faster inference and lower cost. Knowledge distillation from the larger teacher model (Llama-3-70B-Instruct) to the smaller student model transfers the fact verification capabilities while reducing computational requirements.

## Foundational Learning

- **Concept: Fine-grained fact verification**
  - Why needed here: The paper focuses on training a model to detect and classify factual errors in summaries at the sentence level, which requires understanding the nuances of different error types.
  - Quick check question: Can you explain the difference between "out-of-context error" and "entity error" in the context of fact verification?

- **Concept: Knowledge distillation**
  - Why needed here: The paper uses a smaller model (Llama-3-8B-Instruct) trained on the feedback generated by a larger model (Llama-3-70B-Instruct) to achieve efficient fact verification.
  - Quick check question: How does knowledge distillation help in transferring the capabilities of a larger model to a smaller one?

- **Concept: Inter-annotator agreement**
  - Why needed here: The paper discusses the reliability of human-labeled data by mentioning low kappa values in existing fine-grained datasets, which supports the argument for using LLM-generated feedback.
  - Quick check question: What does a low kappa value indicate about the reliability of human annotations in a dataset?

## Architecture Onboarding

- **Component map**: Document → Summary Generation (10 LLMs) → Feedback Generation (Llama-3-70B-Instruct) → Fine-tuning (Llama-3-8B-Instruct) → Inference

- **Critical path**: Document → Summary Generation → Feedback Generation → Fine-tuning → Inference

- **Design tradeoffs**:
  - Using a larger teacher model for feedback generation provides more accurate feedback but is slower and more expensive
  - Using a smaller student model for fine-tuning and inference is faster and cheaper but may have slightly lower performance

- **Failure signatures**:
  - If the model fails to generalize to unseen data, it may indicate overfitting to the specific LLMs used for summary generation or the specific feedback style of Llama-3-70B-Instruct
  - If the model's performance is significantly worse than the teacher model, it may indicate that the student model is not effectively learning from the feedback

- **First 3 experiments**:
  1. Train the student model (Llama-3-8B-Instruct) on a small subset of LLM feedback and evaluate its performance on the test set
  2. Vary the granularity of the LLM feedback (binary labels only, binary labels + reasoning, binary labels + reasoning + error localization) and observe the impact on model performance
  3. Increase the volume of LLM feedback data used for training and observe the impact on model performance

## Open Questions the Paper Calls Out
- How does the performance of the fine-tuned Llama-3-8B-Instruct model vary across different error categories in error localization?
- What is the impact of using feedback from multiple LLMs on the performance of the fact verification model compared to using feedback from a single LLM?
- How does the size of the training dataset with LLM feedback correlate with the performance of the fact verification model?
- What are the specific challenges in handling dialogue summarization compared to non-dialogue summarization in terms of fact verification?

## Limitations
- Reliance on synthetic data generation through LLM feedback introduces potential distributional shifts between training and evaluation data
- The study focuses on document-summary pairs and may not generalize to other text generation tasks or domains not represented in the FineSumFact dataset
- The paper does not address potential biases introduced by using a single LLM (Llama-3-70B-Instruct) for feedback generation

## Confidence
- **High Confidence**: The core claim that fine-tuning on LLM-generated feedback outperforms training on smaller human-annotated datasets is well-supported by experimental results and aligns with scaling law principles
- **Medium Confidence**: The assertion that LLM-generated feedback has comparable quality to human-labeled data relies on indirect evidence (scaling laws and performance results) rather than direct comparison of feedback quality
- **Low Confidence**: The generalizability of these findings to other domains, languages, or text generation tasks beyond document summarization remains uncertain

## Next Checks
1. Conduct a direct comparison study where human evaluators assess the quality of LLM-generated feedback versus human-labeled feedback on a held-out test set, measuring inter-annotator agreement and identifying systematic differences in feedback patterns
2. Evaluate the trained fact verification model on datasets from domains not represented in FineSumFact (e.g., scientific literature, news articles, or social media content) to assess domain transferability and identify potential failure modes
3. Deploy the fine-tuned model in a real-world setting for a sustained period (e.g., 3-6 months) and monitor its performance degradation, error patterns, and adaptation needs compared to periodically re-trained models using fresh human annotations