---
ver: rpa2
title: 'DataAgent: Evaluating Large Language Models'' Ability to Answer Zero-Shot,
  Natural Language Queries'
arxiv_id: '2404.00188'
source_url: https://arxiv.org/abs/2404.00188
tags:
- data
- dataset
- datasets
- language
- large
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper evaluates the effectiveness of using large language
  models (LLMs) to perform data science tasks on datasets, in order to reduce the
  burden on human data scientists. The authors develop a "Language Data Scientist"
  (LDS) system that takes a dataset and natural language query, generates an action
  plan, and executes code to answer the query.
---

# DataAgent: Evaluating Large Language Models' Ability to Answer Zero-Shot, Natural Language Queries

## Quick Facts
- arXiv ID: 2404.00188
- Source URL: https://arxiv.org/abs/2404.00188
- Reference count: 9
- Achieved 32.89% accuracy on 225 total queries across 15 benchmark datasets

## Executive Summary
This paper evaluates a "Language Data Scientist" (LDS) system that uses large language models to perform data science tasks on datasets through natural language queries. The three-phase workflow (background info gathering, action plan generation, low-level execution) achieved 32.89% overall accuracy across 15 benchmark datasets with 225 queries. The system showed stable performance across dataset sizes, with best results on large datasets (36% accuracy). The authors identify several limitations and improvement opportunities, including better prompt engineering, handling multi-part queries, and using more capable models like GPT-4.

## Method Summary
The LDS system employs a three-stage approach to answer natural language queries on datasets. First, it gathers context about the dataset using Pandas functions like head(), info(), and describe(). Second, it generates an action plan using a GPT-based Action Plan Generator (AcPG) with Chain-of-Thought reasoning and SayCan prompt engineering. Finally, it executes the generated code through a low-level executor to produce answers. The system was tested on 15 benchmark datasets ranging from 50-300 rows, with 15 queries per dataset covering various data science tasks using libraries like NumPy, Pandas, Scikit-Learn, and TensorFlow.

## Key Results
- Overall accuracy of 32.89% across all datasets and queries
- Best performance on large datasets (>200 rows) at 36% accuracy
- Significant variation in accuracy between toy dataset sizes
- Demonstrated stable performance across different dataset sizes

## Why This Works (Mechanism)

### Mechanism 1
- Claim: The LDS system uses a three-phase workflow to break down complex data science tasks into manageable steps
- Mechanism: By first gathering context about the dataset using Pandas functions, the LDS provides necessary information to the LLM for generating accurate action plans
- Core assumption: Structured context and step-by-step breakdown improves LLM performance on data science queries
- Evidence anchors: [abstract] mentions the three-phase workflow; [section II.B] describes context gathering using Pandas functions

### Mechanism 2
- Claim: The SayCan framework improves action plan accuracy through structured prompts
- Mechanism: SayCan-formulated prompts guide the AcPG to produce action plans aligned with system capabilities and query needs
- Core assumption: Structured prompts that match system capabilities improve LLM performance
- Evidence anchors: [section II.C.2] describes SayCan framework implementation; [abstract] mentions SayCan prompt engineering

### Mechanism 3
- Claim: Dataset size influences accuracy, with better performance on larger datasets
- Mechanism: Larger datasets provide more comprehensive context for the LLM to understand structure and relationships
- Core assumption: More data points and context improve LLM performance on data science tasks
- Evidence anchors: [section III] shows best performance on large datasets; [abstract] mentions diverse benchmark testing

## Foundational Learning

- **Concept: Chain-of-Thought reasoning**
  - Why needed here: Breaking down complex tasks into smaller, explainable steps helps the LLM generate more accurate action plans
  - Quick check question: How does Chain-of-Thought reasoning improve the accuracy of the action plan generator?

- **Concept: Prompt engineering**
  - Why needed here: Structured prompts that align with system capabilities and query needs improve LLM performance
  - Quick check question: How does the SayCan framework structure prompts to improve the accuracy of the action plan generator?

- **Concept: Context gathering**
  - Why needed here: Providing the LLM with necessary context about the dataset improves its ability to generate accurate action plans
  - Quick check question: What information does the LDS gather about the dataset to provide context to the LLM?

## Architecture Onboarding

- **Component map**: Input Query & Dataset -> Context Gatherer -> Action Plan Generator (AcPG) -> Low-Level Executor -> Answer Output

- **Critical path**: 
  1. Input query and dataset
  2. Context gatherer retrieves basic information about the dataset
  3. AcPG generates action plan using Chain-of-Thought reasoning and SayCan prompts
  4. Action plan translated into executable code
  5. Low-level executor runs code to produce final answer
  6. Answer checker compares output to ground truth answers

- **Design tradeoffs**: 
  - Using GPT-3.5 instead of GPT-4 due to budget constraints
  - Breaking down tasks into smaller steps to improve accuracy but potentially increasing execution time
  - Gathering context about dataset to improve performance but potentially increasing token usage

- **Failure signatures**: 
  - Incorrect code generation due to insufficient context or misaligned prompts
  - Incomplete answers due to multi-part queries not being properly addressed
  - Edge case failures when queries don't fully apply to dataset context

- **First 3 experiments**:
  1. Test LDS on small dataset with simple query to verify basic three-phase workflow functionality
  2. Test on medium-sized dataset with multi-part query to evaluate complex task handling
  3. Test on large dataset with query requiring missing value handling to assess edge case robustness

## Open Questions the Paper Calls Out

- **Open Question 1**: How does accuracy change when using more advanced LLMs like GPT-4 instead of GPT-3.5?
  - Basis in paper: [explicit] Authors mention GPT-3.5 was used due to budget constraints and that GPT-4 could improve performance
  - Why unresolved: Current study only tested GPT-3.5
  - What evidence would resolve it: Running same experiments with GPT-4 and comparing accuracy results

- **Open Question 2**: How does performance scale with dataset size beyond 300 rows?
  - Basis in paper: [inferred] Authors tested up to 300 rows and suggest exploring larger datasets as future direction
  - Why unresolved: Study only tested datasets up to 300 rows
  - What evidence would resolve it: Testing on datasets with more than 300 rows and analyzing accuracy trends

- **Open Question 3**: How does the system handle queries requiring multiple outputs or complex reasoning?
  - Basis in paper: [explicit] Authors note LDS often failed on multi-output queries and suggest GPT-4's improved multi-part handling could help
  - Why unresolved: Current implementation struggles with multi-part queries
  - What evidence would resolve it: Evaluating performance on multi-part queries before and after improvements

## Limitations

- Reliance on GPT-3.5 rather than more capable models like GPT-4 due to budget constraints
- Manual calculation of ground truth answers introduces potential human bias
- Limited exploration of different prompt engineering strategies and their individual contributions

## Confidence

- **High Confidence**: Three-phase workflow architecture is well-specified and replicable
- **Medium Confidence**: Claim that dataset size affects performance is supported but lacks detailed analysis of why
- **Low Confidence**: Effectiveness of SayCan framework integration is mentioned but not rigorously validated

## Next Checks

1. **Ablation Study on Prompt Engineering**: Systematically test LDS with and without SayCan prompts and Chain-of-Thought reasoning to quantify individual contributions to accuracy improvements

2. **Model Comparison**: Re-run evaluation using GPT-4 or Claude to determine whether accuracy improvements are primarily due to model capability rather than architectural design

3. **Cross-Dataset Generalization**: Test system on datasets from domains not represented in original benchmark (e.g., medical imaging, financial time series) to assess performance across diverse data science applications