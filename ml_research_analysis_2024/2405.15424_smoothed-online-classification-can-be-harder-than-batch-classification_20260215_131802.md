---
ver: rpa2
title: Smoothed Online Classification can be Harder than Batch Classification
arxiv_id: '2405.15424'
source_url: https://arxiv.org/abs/2405.15424
tags: []
core_contribution: "This paper studies the relationship between smoothed online classification\
  \ and batch classification, revealing a fundamental separation between the two settings\
  \ when the label space is unbounded. The authors construct a hypothesis class that\
  \ is PAC learnable but not learnable under smoothed online adversaries with uniform\
  \ base measure and full smoothness (\u03C3 = 1)."
---

# Smoothed Online Classification can be Harder than Batch Classification

## Quick Facts
- arXiv ID: 2405.15424
- Source URL: https://arxiv.org/abs/2405.15424
- Authors: Vinod Raman; Unique Subedi; Ambuj Tewari
- Reference count: 40
- Key outcome: This paper studies the relationship between smoothed online classification and batch classification, revealing a fundamental separation between the two settings when the label space is unbounded. The authors construct a hypothesis class that is PAC learnable but not learnable under smoothed online adversaries with uniform base measure and full smoothness (σ = 1).

## Executive Summary
This paper investigates the relationship between smoothed online classification and batch classification, revealing a fundamental separation between the two settings. The authors construct a hypothesis class that is PAC learnable but not learnable under smoothed online adversaries with uniform base measure and full smoothness (σ = 1). This separation occurs because the adversary can exploit the large label space to create difficult sequences even when sampling from smooth distributions. The paper provides both theoretical insights and concrete counterexamples that advance our understanding of online learning under smoothed adversaries.

## Method Summary
The paper analyzes smoothed online learning by comparing it to batch PAC learning and constructing specific hypothesis classes to demonstrate fundamental differences. The authors establish a lower bound of T/2 for regret in certain classes, proving that some hypothesis classes that are learnable in the batch setting become unlearnable under smoothed online adversaries. They also develop a sufficient condition for online learnability based on uniformly bounded expected empirical metric entropy (UBEME), which recovers previous results for finite label spaces but extends to more general cases. The analysis involves constructing explicit counterexamples to show that neither finite metric entropy nor UBEME alone characterizes smoothed online learnability.

## Key Results
- Constructed a hypothesis class that is PAC learnable but not learnable under smoothed online adversaries with uniform base measure and full smoothness
- Proved a regret lower bound of T/2 for the constructed class
- Showed UBEME implies online learnability with regret bound O(√T log Cε,σ(H,µ))
- Demonstrated that neither finite metric entropy nor UBEME alone characterizes smoothed online learnability

## Why This Works (Mechanism)
The separation between smoothed online and batch learning occurs because the adversary can exploit the large label space to create difficult sequences even when sampling from smooth distributions. When the label space is unbounded, the adversary can generate label sequences that are hard to predict online, despite the smoothing mechanism. This is fundamentally different from batch learning where the learner has access to all data simultaneously. The UBEME condition works as a sufficient condition because it bounds the complexity of the hypothesis class in a way that prevents the adversary from creating these difficult sequences.

## Foundational Learning

**PAC Learnability**
- Why needed: Provides the baseline for batch learning performance
- Quick check: Verify that the constructed class is indeed PAC learnable using standard VC dimension arguments

**Smoothed Online Learning**
- Why needed: The primary setting being analyzed, where adversaries can only generate data from distributions with bounded smoothness
- Quick check: Confirm that the uniform base measure and σ = 1 specification are correctly implemented in the analysis

**Empirical Metric Entropy**
- Why needed: Used to characterize the complexity of hypothesis classes in online settings
- Quick check: Validate the metric entropy calculations for the constructed hypothesis class

**Regret Bounds**
- Why needed: The primary performance metric for online learning algorithms
- Quick check: Verify that the T/2 lower bound is tight and cannot be improved

## Architecture Onboarding

**Component Map**
H (hypothesis class) -> A (adversary) -> D (data distribution) -> L (learner) -> R (regret)

**Critical Path**
1. Construct hypothesis class H with unbounded label space
2. Define adversary A with uniform base measure and σ = 1
3. Analyze learner L's performance on sequences from A
4. Establish regret bound R = T/2 as lower bound
5. Verify H is PAC learnable but not online learnable

**Design Tradeoffs**
- Unbounded vs bounded label space: Enables separation but may limit practical applicability
- Uniform base measure: Simplifies analysis but may not capture all realistic scenarios
- Full smoothness (σ = 1): Provides strongest separation but weaker results may exist for partial smoothness

**Failure Signatures**
- If the hypothesis class is PAC learnable but online regret is o(T), the separation argument fails
- If UBEME condition is both necessary and sufficient, the paper's characterization is incomplete
- If similar separations don't exist for non-uniform base measures, the results may be limited in scope

**First Experiments**
1. Test the constructed hypothesis class on a synthetic dataset to empirically verify the T/2 regret lower bound
2. Apply the UBEME condition to benchmark datasets to assess practical learnability
3. Vary the smoothness parameter σ to determine the threshold where learnability changes

## Open Questions the Paper Calls Out
None

## Limitations
- The primary separation result relies on constructing a specific hypothesis class with unbounded label space, raising questions about whether this captures all scenarios where smoothed online learning might be fundamentally harder
- The sufficiency condition (UBEME) is proven but shown not to be necessary, leaving open the question of whether a complete characterization exists
- The paper focuses on the uniform base measure and full smoothness setting, which may not generalize to other distributions or smoothness parameters

## Confidence

| Claim | Confidence |
|-------|------------|
| Separation result between smoothed online and batch learning | High |
| UBEME sufficiency condition | High |
| Necessity of UBEME condition | High |
| Generalizability beyond uniform base measure | Low |

## Next Checks
1. Test the separation result on alternative hypothesis classes with different structural properties to assess robustness
2. Evaluate the UBEME condition on real-world datasets to determine practical applicability
3. Investigate whether similar separations exist for non-uniform base measures and partial smoothness parameters