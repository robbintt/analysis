---
ver: rpa2
title: Multi-modal Auto-regressive Modeling via Visual Words
arxiv_id: '2403.07720'
source_url: https://arxiv.org/abs/2403.07720
tags:
- visual
- multi-modal
- image
- information
- language
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Visual Tokens (VT-LMM), a novel approach
  that enables Large Multi-modal Models (LMMs) to perform auto-regressive modeling
  on both visual and language sequences. The core challenge addressed is that existing
  LMMs only optimize for language components during training, neglecting visual information
  supervision.
---

# Multi-modal Auto-regressive Modeling via Visual Words

## Quick Facts
- **arXiv ID**: 2403.07720
- **Source URL**: https://arxiv.org/abs/2403.07720
- **Authors**: Tianshuo Peng; Zuchao Li; Lefei Zhang; Hai Zhao; Ping Wang; Bo Du
- **Reference count**: 40
- **Primary result**: VT-LMM-Mistral-7B achieved best or second-best performance across all evaluated metrics, including 87.3% accuracy on adversarial object hallucination evaluation

## Executive Summary
This paper introduces Visual Tokens (VT-LMM), a novel approach that enables Large Multi-modal Models (LMMs) to perform auto-regressive modeling on both visual and language sequences. The core challenge addressed is that existing LMMs only optimize for language components during training, neglecting visual information supervision. VT-LMM proposes mapping visual features into probability distributions over the LLM's vocabulary, creating "visual tokens" that serve as supervised labels for visual modeling. The method demonstrates strong performance across 5 VQA benchmarks and 4 LMM evaluation toolkits, with the 7B parameter model achieving results competitive with or exceeding larger models (13B-33B).

## Method Summary
VT-LMM addresses the challenge of multi-modal auto-regressive modeling by introducing visual tokens that map visual features to probability distributions over the LLM's vocabulary. The method employs a four-stage training approach: (1) adapter training with language modeling loss on image captioning, (2) multi-modal decoder training with language modeling loss on complex instruction data, (3) VM head training with visual modeling loss to map visual information to language semantic space, and (4) joint training of multi-modal decoder and MM head with combined language and visual losses. This approach bridges the semantic gap between visual and language modalities while avoiding the need for additional training parameters or complex architectures.

## Key Results
- VT-LMM-Mistral-7B achieved best or second-best performance across all evaluated metrics
- 87.3% accuracy on adversarial object hallucination evaluation
- Competitive results with or exceeding larger models (13B-33B) despite using only 7B parameters
- Strong performance across 5 VQA benchmarks (VQAv2, GQA, VisWiz, SQAI, VQAT) and 4 LMM evaluation toolkits

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Visual Tokens enable LMMs to perform multi-modal auto-regressive modeling by mapping visual features into probability distributions over the LLM's vocabulary.
- Mechanism: The VM head transforms visual embeddings into probability distributions (visual tokens) over the pre-trained vocabulary, creating supervised labels for visual modeling. This bridges the semantic gap between visual and language modalities.
- Core assumption: The pre-trained embeddings weights of LLM can be approximated as a set of complete bases of the language semantic space, covering the whole semantic space.
- Evidence anchors:
  - [abstract] "We propose the concept of visual tokens, which maps the visual features to probability distributions over LLM's vocabulary, providing supervision information for visual modelling."
  - [section 2.2] "We propose using the linear projection to also map the visual features to the probability distribution over the model's vocabulary, which we called visual tokens, to further strengthen the correlation between visual features and text embeddings."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.181, average citations=0.0. Top related titles: HaploVL: A Single-Transformer Baseline for Multi-Modal Understanding, Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling, LMM-VQA: Advancing Video Quality Assessment with Large Multimodal Models.

### Mechanism 2
- Claim: The introduction of visual tokens as visual supervision information directly improves the vision-language comprehension performance of LMMs.
- Mechanism: By constructing visual tokens as supervision information, the model is guided to learn both language and visual modalities of the multi-modal sequences in an auto-regressive manner, thereby boosting the model's vision-language understanding capability.
- Core assumption: The model can effectively learn from the additional visual modeling task, which forces it to capture the distribution of image information in the current semantic space.
- Evidence anchors:
  - [abstract] "We propose the concept of visual tokens, which maps the visual features to probability distributions over LLM's vocabulary, providing supervision information for visual modelling."
  - [section 3.4] "From table 5, it can be observed that the introduction of visual tokens leads to evident performance enhancement in both different LLM settings, which confirms that VT-LMM, by constructing visual tokens as visual supervision information, guides the model to learn the rich semantics in multi-modal sequences through both language modelling and visual modelling, therefore boosting model's vision-language understanding capability."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.181, average citations=0.0. Top related titles: HaploVL: A Single-Transformer Baseline for Multi-Modal Understanding, Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling, LMM-VQA: Advancing Video Quality Assessment with Large Multimodal Models.

### Mechanism 3
- Claim: Visual tokens successfully implement the transfer of visual features to language semantic space, enabling the use of text embeddings to represent visual information.
- Mechanism: The text embeddings in the multi-modal decoder can be regarded as a set of base vectors within its semantic space. Visual tokens and text embeddings can be used to construct pseudo image features, exploring the manifestation of visual features in the semantic space of the LMM.
- Core assumption: The embedding layer of the pre-trained LLM essentially achieves full utilization of the semantic space, thus features from image modalities can be represented by linear combinations of text embeddings to some extent.
- Evidence anchors:
  - [abstract] "We further explored the distribution of visual features in the semantic space within the LMM and the possibility of using text embeddings to represent visual information."
  - [section 4.1] "The results are shown in table 5. From the experimental results, it is clear that the model that receives pseudo image features as visual input can still achieve vision-language understanding capability close to the original model. This result confirms that the embedding layer of the pre-trained LLM essentially achieves full utilisation of the semantic space, thus features from image modalities can be represented by linear combinations of text embeddings to some extent."
  - [corpus] Found 25 related papers (using 8). Average neighbor FMR=0.181, average citations=0.0. Top related titles: HaploVL: A Single-Transformer Baseline for Multi-Modal Understanding, Looking to Learn: Token-wise Dynamic Gating for Low-Resource Vision-Language Modelling, LMM-VQA: Advancing Video Quality Assessment with Large Multimodal Models.

## Foundational Learning

- Concept: Multi-modal learning and the integration of visual and textual information
  - Why needed here: The paper addresses the challenge of extending auto-regressive modeling to multi-modal scenarios, where image information is processed as continuous visual embeddings that cannot obtain discrete supervised labels for classification.
  - Quick check question: How do existing multi-modal learning methods of LMM typically transform visual features into the semantic space of LLM?

- Concept: Auto-regressive modeling and its application in language and multi-modal models
  - Why needed here: The paper proposes to perform multi-modal auto-regressive modeling with a unified objective, using visual tokens to construct representations of visual features in the language semantic space inside the LMM.
  - Quick check question: What is the main difference between the training objective of visual context method and the proposed multi-modal auto-regressive method?

- Concept: Visual tokens and their role in bridging the semantic gap between visual and language modalities
  - Why needed here: The paper introduces the concept of visual tokens, which maps visual features into probability distributions over the LLM's vocabulary, providing supervision information for visual modelling and enabling the model to perform auto-regressive modelling over multi-modal sequence.
  - Quick check question: How do visual tokens help in constructing pseudo image features using pre-trained embeddings of LLM?

## Architecture Onboarding

- Component map: Visual Encoder -> Adapter -> VM head -> Visual Tokens -> Multi-modal Decoder -> MM head -> Output

- Critical path: Image → Visual Encoder → Adapter → VM head → Visual Tokens → Multi-modal Decoder → MM head → Output

- Design tradeoffs:
  - Using visual tokens as supervision information avoids the need for additional training parameters or complex architectures, but may lead to a certain degree of information loss due to the mapping of continuous visual information into discrete language semantic space.
  - The linear projection used in VM head might have limitations for the projection of visual information into the language semantic space.

- Failure signatures:
  - Poor performance on visual question answering benchmarks or object hallucination evaluation
  - Instability during training due to the separation of optimization phases of VM head and multi-modal decoder
  - Significant performance difference between models using visual features and pseudo image features

- First 3 experiments:
  1. Implement the VM head and test its ability to map visual features to probability distributions over LLM's vocabulary
  2. Replace visual features in the input sequence with pseudo image features and evaluate the impact on model performance
  3. Conduct an ablation study to assess the contribution of visual tokens to the overall performance of the model

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the diversity of training images affect the learning effectiveness of the VM head in constructing visual tokens?
- Basis in paper: [inferred] The paper mentions "the effect of the diversity of training images on the learning effectiveness of the VM head was not explored" in the limitations section, indicating this remains an open research question.
- Why unresolved: The authors acknowledge this limitation but did not conduct experiments to systematically evaluate how different levels of image diversity in training data impact the quality and effectiveness of the learned visual tokens.
- What evidence would resolve it: Experiments training VT-LMM with datasets of varying image diversity levels (e.g., uniform vs diverse datasets) and measuring the impact on visual token quality metrics and downstream performance on VQA tasks would provide evidence.

### Open Question 2
- Question: What is the optimal architecture beyond linear projection for constructing visual tokens that minimizes information loss when mapping continuous visual features to discrete language semantic space?
- Basis in paper: [inferred] The paper notes that "the structure of the linear projection might have limitations for the projection of visual information into the language semantic space" and suggests this needs further investigation.
- Why unresolved: While the linear projection works reasonably well, the authors suggest there may be more effective architectures that could better preserve visual information during the mapping to language semantic space.
- What evidence would resolve it: Comparative experiments testing various projection architectures (e.g., multi-layer MLPs, attention-based mechanisms, or hybrid approaches) for visual token construction, with quantitative measurements of information preservation and downstream task performance.

### Open Question 3
- Question: How does the introduction of visual modeling tasks affect the text-side capabilities of different LLMs, and can this trade-off be optimized?
- Basis in paper: [explicit] The paper observes that "the introduction of the visual modelling task slightly diminishes the text-side capabilities of Vicuna-7B on both datasets" but notes that "its stronger baseline performance and pre-trained language semantic comprehension bridged this gap" with Mistral-7B.
- Why unresolved: The authors demonstrate that different LLMs respond differently to the introduction of visual modeling tasks, but the underlying reasons for this trade-off and potential optimization strategies remain unexplored.
- What evidence would resolve it: Systematic experiments varying the relative weighting of language and visual loss terms during training, along with analysis of how different pre-trained LLMs' architectures and training histories influence their ability to balance both modalities effectively.

## Limitations

- The linear projection used for visual token construction may have limitations for projecting visual information into the language semantic space, potentially leading to information loss
- The four-stage training approach introduces complexity in determining optimal stage transitions and weight initialization, with sensitivity to training schedule variations not thoroughly explored
- Evaluation focuses primarily on object recognition and basic visual reasoning tasks, with effectiveness for more complex visual reasoning and out-of-distribution visual content remaining untested

## Confidence

**High Confidence**: The technical implementation of visual tokens as probability distributions over LLM vocabulary is well-specified and reproducible. The four-stage training procedure is clearly defined, and the performance improvements over baseline LMMs are demonstrated across multiple benchmarks with statistical significance.

**Medium Confidence**: The mechanism claim that visual tokens bridge the semantic gap between visual and language modalities is supported by empirical results but relies on the assumption that language embeddings form a complete semantic basis. The exploration of visual features in language semantic space through pseudo image features is promising but requires further validation across diverse visual domains.

**Low Confidence**: The claim about full utilization of the semantic space by the embedding layer of pre-trained LLM is more speculative and would require more rigorous mathematical analysis or broader empirical validation across different LMM architectures.

## Next Checks

1. **Cross-modal Embedding Analysis**: Conduct a systematic analysis of the alignment between visual features and their mapped visual tokens by computing nearest-neighbor relationships in the joint embedding space. Measure the semantic coherence between original visual features and their top-k closest tokens to quantify the quality of the visual-to-language mapping.

2. **Stage Ablation Study**: Implement a controlled ablation study that removes individual training stages (particularly stages III and IV) to isolate the contribution of visual tokens to overall performance. This would clarify whether the performance gains come from the visual modeling task itself or from improved training dynamics.

3. **Out-of-Distribution Visual Testing**: Evaluate the model on out-of-distribution visual datasets (medical imaging, satellite imagery, abstract art) and complex visual reasoning tasks (spatial relationships, causal reasoning, temporal sequences) to test the limits of the visual token representation and identify failure modes.