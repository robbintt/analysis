---
ver: rpa2
title: 'VISA: Retrieval Augmented Generation with Visual Source Attribution'
arxiv_id: '2412.14457'
source_url: https://arxiv.org/abs/2412.14457
tags:
- bounding
- visa
- document
- answer
- source
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces VISA, a novel visual source attribution method
  for retrieval-augmented generation (RAG) systems. VISA leverages large vision-language
  models (VLMs) to generate answers and visually highlight supporting evidence within
  document screenshots using bounding boxes.
---

# VISA: Retrieval Augmented Generation with Visual Source Attribution
## Quick Facts
- arXiv ID: 2412.14457
- Source URL: https://arxiv.org/abs/2412.14457
- Authors: Xueguang Ma; Shengyao Zhuang; Bevan Koopman; Guido Zuccon; Wenhu Chen; Jimmy Lin
- Reference count: 12
- Primary result: VISA improves bounding box accuracy to 41.6% and answer accuracy to 51.1% for 7B model in multi-candidate settings

## Executive Summary
VISA introduces a novel approach for visual source attribution in retrieval-augmented generation systems by leveraging large vision-language models to generate answers while visually highlighting supporting evidence within document screenshots. The method uses bounding boxes to pinpoint the exact source of information, enhancing transparency and verifiability in RAG outputs. The authors created two datasets, Wiki-VISA and Paper-VISA, to evaluate their approach across different document types and domains.

## Method Summary
The VISA method fine-tunes vision-language models on curated datasets to improve their ability to both generate accurate answers and provide precise visual attribution through bounding boxes. The approach processes document screenshots, extracts relevant information, and generates answers while simultaneously highlighting the source regions. Two datasets were created: Wiki-VISA using Wikipedia webpages and Paper-VISA using PubLayNet for medical documents. The fine-tuning process significantly improves performance compared to zero-shot prompting, particularly in multi-candidate scenarios where the model must select from multiple potential sources.

## Key Results
- 7B model achieves 41.6% bounding box accuracy and 51.1% answer accuracy in multi-candidate settings
- Fine-tuning VLMs on VISA datasets significantly improves both visual attribution and answer quality
- Zero-shot prompting performs substantially worse than fine-tuned models, demonstrating the value of task-specific training

## Why This Works (Mechanism)
VISA works by combining the visual understanding capabilities of VLMs with fine-tuned generation abilities to create a system that can both comprehend document layouts and accurately attribute information sources. The method leverages the model's ability to process visual information alongside text, allowing it to identify relevant regions within documents and correlate them with generated answers. The fine-tuning process on domain-specific datasets helps the model learn the patterns and structures common to different document types, improving its ability to locate and highlight supporting evidence accurately.

## Foundational Learning
- Vision-Language Models: Why needed - to process both visual document layouts and textual content simultaneously; Quick check - model can accurately describe document contents
- Bounding Box Generation: Why needed - to provide precise visual attribution of information sources; Quick check - generated boxes align with relevant text regions
- Fine-tuning on Domain Data: Why needed - to adapt general VLMs to specific document types and layouts; Quick check - performance improvement over zero-shot baseline
- Multi-candidate Selection: Why needed - to handle scenarios where multiple sources could support an answer; Quick check - correct source selection among alternatives
- Document Layout Understanding: Why needed - to navigate complex document structures and identify relevant sections; Quick check - accurate section identification across document types

## Architecture Onboarding
Component Map: Document Screenshot -> VLM Processing -> Answer Generation -> Bounding Box Generation -> Visual Attribution Output

Critical Path: The system processes document screenshots through the VLM to generate answers while simultaneously identifying and highlighting source regions using bounding boxes. The critical path involves document preprocessing, VLM inference, and post-processing to extract and format the visual attributions.

Design Tradeoffs: The approach trades off computational efficiency for accuracy by using full document screenshots rather than text extraction alone. This provides better context for understanding document layouts but increases processing requirements. The fine-tuning approach requires domain-specific datasets but delivers superior performance compared to zero-shot methods.

Failure Signatures: Common failure modes include incorrect bounding box placement due to complex layouts, missed information in multi-page documents, and reduced accuracy on document types not well-represented in training data. The system may also struggle with non-standard layouts or heavily formatted documents.

First Experiments:
1. Test zero-shot performance on held-out documents from the same domain
2. Evaluate bounding box accuracy on simple, single-column documents
3. Measure answer quality degradation when visual attribution is disabled

## Open Questions the Paper Calls Out
The paper identifies challenges in handling multi-page documents and diverse layouts as open questions. It notes that the current approach may not scale well to real-world scenarios with significant layout variations and highlights the need for more extensive evaluation across different document types beyond the curated datasets.

## Limitations
- Evaluation limited to Wikipedia and medical documents, raising generalizability concerns
- Bounding box accuracy of 41.6% indicates substantial room for improvement in visual precision
- Challenges with multi-page document processing and complex layouts not fully addressed

## Confidence
- High confidence in methodology design and dataset creation process
- Medium confidence in reported performance improvements based on controlled experiments
- Medium confidence in practical applicability given limited real-world deployment validation

## Next Checks
1. Evaluate VISA on diverse document types including financial reports, legal documents, and technical manuals to assess cross-domain generalization
2. Conduct user studies with domain experts to validate visual source attribution usefulness in practical decision-making
3. Implement VISA in a real-time RAG system with continuous document updates to evaluate performance under dynamic conditions and measure system latency impact