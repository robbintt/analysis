---
ver: rpa2
title: Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback
arxiv_id: '2404.10271'
source_url: https://arxiv.org/abs/2404.10271
tags:
- social
- choice
- feedback
- preferences
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper argues that social choice theory should be applied to
  address the problem of aggregating diverse human feedback in AI alignment, particularly
  in reinforcement learning from human feedback (RLHF) and constitutional AI. The
  authors propose several methods for incorporating social choice principles into
  AI training pipelines, including using social welfare functions to aggregate individual
  preferences, simulating collective decisions, and treating ethical principles as
  voters.
---

# Social Choice Should Guide AI Alignment in Dealing with Diverse Human Feedback

## Quick Facts
- arXiv ID: 2404.10271
- Source URL: https://arxiv.org/abs/2404.10271
- Reference count: 31
- This paper proposes applying social choice theory to aggregate diverse human feedback for AI alignment, particularly in RLHF and constitutional AI.

## Executive Summary
This paper argues that social choice theory provides essential tools for addressing the challenge of aggregating diverse human feedback in AI alignment. The authors propose mapping AI alignment problems to established social choice problems, using methods like social welfare functions to aggregate individual preferences, and simulating collective decisions. They suggest that these approaches could lead to fairer, more representative AI systems that better reflect stakeholder values and have broader buy-in. The paper provides a framework for thinking about collective decision problems in AI alignment, discussing who should provide feedback, in what format, and how to account for behavioral aspects.

## Method Summary
The paper proposes applying social choice theory to AI alignment by treating preference aggregation challenges as social choice problems. The approach involves collecting diverse human feedback in various formats, extracting evaluator features, training individual preference models, and using social choice functions to aggregate these into collective preferences. The authors suggest two main methods: using cardinal social welfare functions to aggregate individual preference models into collective rewards for RLHF, and simulating collective decisions using social choice functions to select AI responses directly. They also discuss how to select representative stakeholders, account for behavioral effects, and navigate multiple AI systems.

## Key Results
- Social choice theory can be effectively applied to AI alignment problems, providing principled methods for aggregating diverse human feedback
- Using evaluator features to train individual preference models enables more representative preference aggregation through cardinal social welfare functions
- Simulated collective decisions using social choice functions can potentially replace traditional RLHF optimization, offering more transparent decision-making

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Social choice theory provides principled methods to aggregate diverse human preferences into consistent AI alignment decisions.
- Mechanism: The paper maps AI alignment challenges to established social choice problems, proposing concrete adaptations like using social welfare functions or simulated collective decisions.
- Core assumption: Diverse human feedback can be meaningfully modeled as preference data suitable for social choice aggregation methods.
- Evidence anchors: [abstract] - The authors propose using social choice principles to aggregate diverse human feedback into consistent data about "collective" preferences. [section 3] - The paper explicitly frames alternatives as probability distributions over responses. [corpus] - Weak evidence - corpus neighbors don't directly discuss aggregation mechanisms.
- Break condition: If human preferences are too heterogeneous or context-dependent to be meaningfully aggregated without losing essential information.

### Mechanism 2
- Claim: Incorporating evaluator features into reward models enables more representative preference aggregation.
- Mechanism: The paper proposes training individual preference models that incorporate evaluator features, then using cardinal social welfare functions to aggregate these into collective rewards for RLHF.
- Core assumption: Evaluator features can be meaningfully extracted and used to weight or adjust individual preferences in aggregation.
- Evidence anchors: [section 6.1] - The paper discusses using evaluator features to train individual preference models and then aggregate via cardinal social welfare functions. [section 4] - Discusses how to select a representative subset of the stakeholder population. [corpus] - No direct evidence in corpus about feature-based aggregation.
- Break condition: If evaluator features cannot be reliably extracted or if they don't capture the relevant dimensions of preference heterogeneity.

### Mechanism 3
- Claim: Simulated collective decisions can replace traditional RLHF optimization with more transparent decision-making.
- Mechanism: The paper proposes using single-winner or multi-winner social choice functions to select responses based on simulated collective decisions, potentially replacing the reinforcement learning step.
- Core assumption: Social choice functions can effectively select high-quality responses that reflect collective preferences without explicit reward modeling.
- Evidence anchors: [section 6.2] - The paper explicitly proposes "Simulated Collective Decisions" as an alternative to RLHF. [section 2.5] - Discusses how social choice rules can output probability distributions or multi-winner sets. [corpus] - No direct evidence in corpus about simulated collective decisions.
- Break condition: If social choice functions cannot scale to the complexity of AI response selection or if they introduce unacceptable computational overhead.

## Foundational Learning

- Concept: Preference aggregation and social choice theory
  - Why needed here: The entire paper is built on mapping AI alignment problems to social choice aggregation problems.
  - Quick check question: What is Arrow's Impossibility Theorem and why does it matter for AI alignment?

- Concept: Reinforcement Learning from Human Feedback (RLHF)
  - Why needed here: The paper critiques RLHF's limitations and proposes social choice-based alternatives.
  - Quick check question: What are the three main steps in the RLHF pipeline and where does the paper propose inserting social choice mechanisms?

- Concept: Behavioral economics and cognitive biases
  - Why needed here: The paper discusses how behavioral effects can distort preference elicitation.
  - Quick check question: What is the "overreporting" strategy mentioned in the paper and how might it affect preference aggregation?

## Architecture Onboarding

- Component map:
  Input collection -> Feature extraction -> Individual modeling -> Aggregation -> Output selection

- Critical path:
  1. Collect diverse human feedback in various formats
  2. Extract evaluator features and format feedback into common structure
  3. Train individual preference models using evaluator features
  4. Apply social choice aggregation to produce collective preferences
  5. Use aggregated preferences to guide AI system behavior

- Design tradeoffs:
  - Representativeness vs. feasibility: Broader stakeholder inclusion improves representation but increases complexity
  - Transparency vs. performance: Social choice methods may be more interpretable but potentially less performant than black-box RLHF
  - Flexibility vs. consistency: Multiple input formats increase flexibility but may complicate aggregation

- Failure signatures:
  - Aggregation paradoxes (cyclical preferences, inconsistent judgments)
  - Feature extraction failures leading to biased aggregation
  - Computational intractability with large stakeholder populations
  - Reduced performance compared to standard RLHF approaches

- First 3 experiments:
  1. Implement cardinal aggregation with synthetic evaluator features on a small preference dataset to validate the basic approach
  2. Compare different social choice functions (Borda, Instant Runoff, Ranked Pairs) on the same preference data to understand their effects
  3. Simulate collective decisions using a multi-winner social choice function and evaluate the quality of selected responses

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we effectively aggregate diverse human feedback in AI alignment while avoiding inconsistencies and paradoxes that arise from social choice theory?
- Basis in paper: [explicit] The paper discusses the challenge of aggregating diverse human feedback and mentions potential issues like cyclical preferences and logically inconsistent conclusions.
- Why unresolved: The paper acknowledges these issues but doesn't provide a definitive solution, instead suggesting that social choice theory provides tools for thinking about these problems.
- What evidence would resolve it: A concrete method for aggregating feedback that demonstrably avoids paradoxes while maintaining fairness and representativeness across diverse stakeholders.

### Open Question 2
- Question: What is the optimal format for collecting human feedback that balances naturalness of input, informativeness about preferences, and usefulness for AI alignment?
- Basis in paper: [explicit] The paper discusses various feedback formats (pairwise comparisons, rankings, ratings, free-form text) and their respective strengths and weaknesses.
- Why unresolved: Different formats have different trade-offs, and the paper suggests that the effectiveness of various formats depends on behavioral aspects and human cognitive structures.
- What evidence would resolve it: Comparative studies demonstrating which feedback formats produce the most consistent, representative, and useful data for AI alignment across different tasks and populations.

### Open Question 3
- Question: How should we account for behavioral effects and cognitive biases when interpreting human feedback for AI alignment?
- Basis in paper: [explicit] The paper discusses how behavioral aspects and human cognitive structures can affect feedback, mentioning issues like overreporting and strategic voting.
- Why unresolved: The paper raises these concerns but doesn't provide specific methods for correcting or accounting for these effects.
- What evidence would resolve it: A validated framework for identifying and correcting behavioral biases in human feedback that can be integrated into the AI alignment process.

## Limitations

- Limited empirical validation: The framework is largely conceptual with minimal real-world testing
- Scalability concerns: Unclear how social choice methods scale to complex AI response selection
- Practical implementation challenges: Significant unknowns about selecting representative stakeholders and handling irreconcilable preferences

## Confidence

- High confidence in the theoretical mapping between AI alignment problems and social choice theory
- Medium confidence in the proposed mechanisms, pending empirical validation
- Low confidence in specific implementation details and scalability claims

## Next Checks

1. Implement a proof-of-concept using synthetic preference data to test cardinal aggregation methods and verify that evaluator features meaningfully influence outcomes
2. Conduct controlled experiments comparing social choice-based aggregation against standard RLHF on a common preference dataset to measure performance differences
3. Analyze real-world preference data for aggregation paradoxes (cyclical preferences, inconsistencies) to understand failure modes and develop mitigation strategies