---
ver: rpa2
title: 'Breaking MLPerf Training: A Case Study on Optimizing BERT'
arxiv_id: '2402.02447'
source_url: https://arxiv.org/abs/2402.02447
tags: []
core_contribution: 'This work addresses the challenge of speeding up large-scale distributed
  training of BERT models by improving load balancing and communication/computation
  overlap. The authors propose two novel methods: (1) local presorting based on dataset
  stratification for load balancing, and (2) bucket-wise gradient clipping before
  allreduce to enable communication/computation overlap.'
---

# Breaking MLPerf Training: A Case Study on Optimizing BERT

## Quick Facts
- arXiv ID: 2402.02447
- Source URL: https://arxiv.org/abs/2402.02447
- Reference count: 40
- Fastest MLPerf BERT training time: 25.1 seconds on 1,024 NVIDIA A100 GPUs

## Executive Summary
This work addresses the challenge of speeding up large-scale distributed training of BERT models by improving load balancing and communication/computation overlap. The authors propose two novel methods: (1) local presorting based on dataset stratification for load balancing, and (2) bucket-wise gradient clipping before allreduce to enable communication/computation overlap. They also demonstrate that the conventional ADAM optimizer can be effective in large-scale distributed training under hyperparameter optimization. The proposed methods, when combined, achieve the fastest MLPerf BERT training time of 25.1 seconds on 1,024 NVIDIA A100 GPUs, which is 1.33x and 1.57x faster than the other top two submissions to MLPerf v1.1 and v2.0, respectively.

## Method Summary
The paper proposes two novel methods to optimize large-scale distributed training of BERT models. First, local presorting based on dataset stratification divides the training dataset into strata based on sequence length, then each GPU node performs local sorting within its node to minimize load imbalance while avoiding expensive inter-node communication. Second, bucket-wise gradient clipping before allreduce applies gradient clipping within each bucket as soon as they are computed, then synchronizes them via allreduce, allowing computation of subsequent buckets to overlap with communication of clipped gradients. These methods are combined with hyperparameter optimization for the ADAM optimizer to achieve optimal performance.

## Key Results
- Fastest MLPerf BERT training time: 25.1 seconds on 1,024 NVIDIA A100 GPUs
- 1.33x faster than top submission to MLPerf v1.1
- 1.57x faster than top submission to MLPerf v2.0

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Local presorting with dataset stratification reduces load imbalance without incurring expensive inter-node communication.
- Mechanism: The training dataset is divided into strata based on sequence length. Each GPU node performs local sorting within its node, then uses snake pattern scanning to assign sequences to GPUs, minimizing imbalance while avoiding global communication.
- Core assumption: The sequence length distribution within each stratum is sufficiently homogeneous that local sorting can achieve near-optimal load balance.
- Evidence anchors:
  - [abstract] "local presorting based on dataset stratification for load balancing"
  - [section] "Our proposed per-GPU node stratification enables us to presort samples only inside of GPU node, which we call local presorting, thereby avoiding expensive inter-node communication."
  - [corpus] Weak - corpus contains general load balancing papers but not this specific stratification technique.
- Break condition: If sequence length distribution within strata becomes too heterogeneous, local presorting will fail to achieve adequate load balance.

### Mechanism 2
- Claim: Bucket-wise gradient clipping before allreduce enables communication/computation overlap while maintaining sample efficiency.
- Mechanism: Gradients are clipped within each bucket as soon as they are computed, then synchronized via allreduce. This allows computation of subsequent buckets to overlap with communication of clipped gradients.
- Core assumption: Bucket size is large enough that clipping overhead is amortized but small enough that overlap is effective.
- Evidence anchors:
  - [abstract] "bucket-wise gradient clipping before allreduce which allows us to benefit from the overlap of gradient computation and synchronization"
  - [section] "gradients in each bucket are clipped as soon as they are ready and then immediately synchronized with allreduce"
  - [corpus] Weak - corpus has general distributed training papers but not this specific bucket-wise clipping approach.
- Break condition: If bucket size is too small, clipping overhead dominates; if too large, overlap opportunity decreases.

### Mechanism 3
- Claim: Re-evaluating ADAM under hyperparameter optimization enables larger batch sizes than LAMB while maintaining convergence.
- Mechanism: Automated hyperparameter optimization (SMAC) finds optimal learning rate schedules, momentum parameters, and weight decay for ADAM in large-scale distributed training, enabling batch sizes up to 16K.
- Core assumption: ADAM's standard formulation can be tuned to work effectively at batch sizes previously thought to require specialized optimizers like LAMB.
- Evidence anchors:
  - [abstract] "We also re-evaluate existing optimizers via hyperparameter optimization and utilize ADAM, which also contributes to fast training via larger batches than existing methods"
  - [section] "We use automatic hyperparameter optimization tool, Neural Network Intelligence [10], to compare optimizers and gradient clipping methods in a fair way"
  - [corpus] Weak - corpus contains general hyperparameter optimization papers but not this specific ADAM re-evaluation.
- Break condition: If batch size exceeds the range where ADAM's adaptive learning rates remain stable, convergence will degrade.

## Foundational Learning

- Concept: Data parallelism in distributed training
  - Why needed here: The entire approach relies on understanding how gradients are computed and synchronized across multiple GPUs
  - Quick check question: What is the key difference between gradient clipping before vs after allreduce in terms of communication/computation overlap?

- Concept: Allreduce communication patterns
  - Why needed here: Bucket-wise clipping requires understanding how gradient synchronization works at the bucket granularity
  - Quick check question: How does NCCL's dynamic selection of allreduce algorithms affect the effectiveness of communication/computation overlap?

- Concept: CUDA graph optimization
  - Why needed here: The performance benefits of packing vs other methods depend on whether CUDA graphs can be used
  - Quick check question: Under what conditions does CUDA graph acceleration become unavailable for FMHA kernels?

## Architecture Onboarding

- Component map: PyTorch distributed training stack -> NCCL backend -> Allreduce collectives -> Gradient computation -> Bucket clipping -> Parameter update
- Critical path: Forward pass -> Backward pass (with bucket clipping hooks) -> Allreduce (bucket-wise) -> Parameter update
- Design tradeoffs: Local presorting vs global presorting (communication overhead vs load balance quality), bucket size (clipping overhead vs overlap opportunity), batch size (GPU utilization vs convergence stability)
- Failure signatures: Load imbalance manifests as idle GPUs during training, poor gradient clipping shows as unstable training or poor sample efficiency, optimizer issues appear as slow convergence or divergence
- First 3 experiments:
  1. Measure per-GPU token count distribution with different load balancing methods on a small GPU cluster
  2. Profile latency breakdown of bucket-wise vs traditional gradient clipping methods
  3. Run hyperparameter optimization sweeps comparing ADAM vs LAMB at different batch sizes

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the proposed local presorting method perform with smaller batch sizes where stratification may not accurately reflect the dataset statistics?
- Basis in paper: [inferred] The paper mentions that local presorting may have limited utility with very small batch sizes (e.g., local batch size of 4) since stratification is based on probabilities of bins and may not accurately reflect the dataset statistics.
- Why unresolved: The paper does not provide empirical results or analysis of the proposed method's performance with small batch sizes.
- What evidence would resolve it: Experiments comparing the proposed method's performance with small batch sizes to other load balancing techniques.

### Open Question 2
- Question: How does the proposed bucket-wise gradient clipping method perform in terms of sample efficiency and training speed compared to other gradient clipping techniques for different model architectures and datasets?
- Basis in paper: [explicit] The paper claims that the proposed bucket-wise gradient clipping method enables communication/computation overlap while achieving sample efficiency comparable to gradient clipping before allreduce. However, the paper only evaluates the method on BERT models and MLPerf datasets.
- Why unresolved: The paper does not provide empirical results or analysis of the proposed method's performance on other model architectures or datasets.
- What evidence would resolve it: Experiments comparing the proposed method's performance to other gradient clipping techniques on various model architectures and datasets.

### Open Question 3
- Question: How does the proposed local presorting method scale with increasing numbers of GPUs and larger datasets?
- Basis in paper: [explicit] The paper mentions that the proposed local presorting method is expected to scale better than global presorting as the number of GPUs increases, since it avoids expensive inter-node communication. However, the paper only evaluates the method on 1,024 GPUs and does not provide analysis of its scalability with larger datasets.
- Why unresolved: The paper does not provide empirical results or analysis of the proposed method's performance with increasing numbers of GPUs and larger datasets.
- What evidence would resolve it: Experiments comparing the proposed method's performance with increasing numbers of GPUs and larger datasets to other load balancing techniques.

## Limitations

- Limited validation details and absence of ablation studies isolating each contribution
- Exact implementation details of local presorting and bucket-wise gradient clipping algorithms are not fully specified
- Claim that ADAM can match or exceed LAMB's performance at large batch sizes contradicts established literature on optimizer behavior at extreme scales

## Confidence

The claims about achieving the fastest MLPerf BERT training time (25.1 seconds on 1,024 GPUs) and the 1.33x/1.57x speedups over previous submissions are **High confidence** based on MLPerf benchmarking results. However, the novelty and effectiveness of the proposed mechanisms carry **Medium confidence** due to limited validation details and the absence of ablation studies isolating each contribution.

Major uncertainties include:
- The exact implementation details of local presorting and bucket-wise gradient clipping algorithms are not fully specified
- No ablation studies demonstrate the individual contribution of each proposed method
- The optimal bucket size for gradient clipping is not empirically validated
- Limited discussion of failure modes when sequence length distributions are highly heterogeneous

## Next Checks

1. Conduct ablation studies on MLPerf v2.0 benchmark to quantify the individual contributions of local presorting, bucket-wise gradient clipping, and ADAM optimizer tuning
2. Test the robustness of local presorting across different sequence length distributions to identify failure thresholds
3. Profile communication-computation overlap efficiency across different bucket sizes to determine optimal configuration parameters