---
ver: rpa2
title: Tensor Network-Constrained Kernel Machines as Gaussian Processes
arxiv_id: '2403.19500'
source_url: https://arxiv.org/abs/2403.19500
tags:
- uni00000013
- kernel
- tensor
- gaussian
- machines
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The paper establishes a connection between tensor network-constrained
  kernel machines and Gaussian processes by proving that when placing i.i.d. priors
  on the cores of these models, their outputs converge to a Gaussian process in the
  limit.
---

# Tensor Network-Constrained Kernel Machines as Gaussian Processes

## Quick Facts
- arXiv ID: 2403.19500
- Source URL: https://arxiv.org/abs/2403.19500
- Reference count: 34
- Primary result: Tensor network-constrained kernel machines converge to Gaussian processes when i.i.d. priors are placed on their cores

## Executive Summary
This paper establishes a fundamental connection between tensor network-constrained kernel machines and Gaussian processes. By placing i.i.d. priors on the cores of Canonical Polyadic Decomposition (CPD) and Tensor Train (TT)-constrained models, the authors prove that these models' outputs converge to Gaussian processes characterized by the basis functions and prior covariances. The key insight is that TT-constrained models exhibit faster convergence to Gaussian process behavior compared to CPD models for the same number of parameters, particularly as input dimensionality increases.

## Method Summary
The paper employs the multivariate Central Limit Theorem to prove convergence of tensor network-constrained kernel machines to Gaussian processes. The authors analyze both CPD and TT constraints, showing that when cores are independent and identically distributed with zero mean, the sum of intermediate functions converges to a multivariate normal distribution. They compare convergence rates between CPD and TT models, finding that TT models achieve faster convergence due to capturing an exponential range of model interactions versus CPD's linear interactions. The paper also examines MAP estimation implications, showing that regularization terms approximate the full prior regularization in the limit of large ranks.

## Key Results
- CPD and TT-constrained kernel machines converge to the same Gaussian process characterized by basis functions and prior covariances
- TT models achieve faster convergence to Gaussian process behavior than CPD for the same number of parameters when dimensionality D ≥ 3
- TT-constrained models capture exponential RD-1 range of interactions versus CPD's linear R interactions
- MAP estimation with Frobenius norm regularization approximates the true prior regularization in the limit

## Why This Works (Mechanism)

### Mechanism 1
Placing i.i.d. priors on TN cores causes outputs to converge to Gaussian processes via the multivariate Central Limit Theorem. The sum of intermediate functions from each rank converges to a multivariate normal distribution, fully specifying the GP.

### Mechanism 2
TT-constrained models converge faster than CPD for the same parameters when D ≥ 3 because TT captures exponential RD-1 interactions versus CPD's linear R interactions, while both use quadratic parameters in R.

### Mechanism 3
The regularization term in MAP estimation approximates full prior regularization as ranks grow because Frobenius norm regularization of tensorized weights approaches the true prior structure in the limit.

## Foundational Learning

- **Central Limit Theorem**: Why needed - main proof technique relies on sums of i.i.d. random variables converging to normal distributions. Quick check - what conditions must be satisfied for multivariate CLT to apply to intermediate functions?
- **Gaussian Processes**: Why needed - paper establishes TN models converge to GPs, so understanding GP definition is essential. Quick check - how is a GP fully specified and what are implications of degenerate GP convergence?
- **Tensor Networks (CPD/TT)**: Why needed - models constrain kernel weights using TNs, so understanding decomposition is crucial. Quick check - how do storage complexities of CPD and TT compare to full tensor?

## Architecture Onboarding

- **Component map**: Input x ∈ R^D → Basis function expansion φ(x) = ⊗_d φ(d)(x_d) → Tensorization → TN core multiplication → Output f_TN(x)
- **Critical path**: Input → Basis function expansion → Tensorization → TN core multiplication → Output
- **Design tradeoffs**: Rank vs. parameters (higher ranks improve GP approximation but increase cost); CPD vs. TT (TT converges faster for D ≥ 3 but has more hyperparameters); Basis functions (more improves expressiveness but increases memory)
- **Failure signatures**: Poor GP convergence (check i.i.d. assumption or scaling); Overfitting (insufficient parameters for GP behavior); Underfitting (too few basis functions or ranks)
- **First 3 experiments**: 1) Verify GP convergence by sampling and comparing to target GP using Cramér-von Mises statistic; 2) Compare CPD vs TT prediction performance as parameters increase on small UCI datasets; 3) Test MAP estimation with different regularization schemes and verify convergence to true prior

## Open Questions the Paper Calls Out

### Open Question 1
How does choice of tensor network architecture impact trade-off between model expressiveness and computational efficiency in high-dimensional settings? The paper discusses TT converging faster than CPD but doesn't empirically compare practical trade-offs.

### Open Question 2
What are implications of GP behavior for uncertainty quantification in ML tasks? The paper establishes TN models converge to GPs enabling uncertainty estimates but doesn't explore practical applications.

### Open Question 3
How do different initialization strategies for TN cores affect convergence rate and final performance? The paper mentions priors provide sensible initialization but doesn't compare with other methods.

### Open Question 4
Can insights be extended to other TN architectures or ML models? The paper focuses on CPD and TT but doesn't investigate generalizability to other architectures like MERA or models like transformers.

## Limitations

- Convergence proofs rely heavily on i.i.d. assumptions for TN cores which may not hold in practice during optimization
- Theoretical comparison assumes fixed basis functions and dimensionality, but practical implementations often involve adaptive basis selection
- MAP estimation analysis shows regularization approximates prior in limit, but finite-rank models may deviate significantly from true GP behavior

## Confidence

- **High Confidence**: Core CLT-based convergence proof - supported by direct theorem statements and standard probability theory
- **Medium Confidence**: TT vs CPD convergence rate comparison - theoretical analysis is sound but depends on specific parameter scaling assumptions
- **Low Confidence**: MAP estimation approximation quality - limited discussion of finite-rank behavior and practical implications

## Next Checks

1. **Empirical convergence verification**: Sample from trained CPD and TT models across increasing ranks, measuring empirical distribution convergence to target GP using Cramér-von Mises statistic and comparing against theoretical predictions

2. **Finite-rank MAP behavior**: Implement MAP estimation with different regularization schemes (Frobenius norm vs. true prior regularization) and measure KL divergence between resulting distributions as ranks vary

3. **Dimensionality scaling experiments**: Systematically vary input dimensionality D and basis function count M to empirically validate the claimed R^(D-1) vs R interaction scaling advantage of TT over CPD models