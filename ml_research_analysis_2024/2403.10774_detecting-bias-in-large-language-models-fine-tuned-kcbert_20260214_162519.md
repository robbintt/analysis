---
ver: rpa2
title: 'Detecting Bias in Large Language Models: Fine-tuned KcBERT'
arxiv_id: '2403.10774'
source_url: https://arxiv.org/abs/2403.10774
tags:
- bias
- language
- social
- data
- arxiv
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study investigates societal biases in large language models
  (LLMs) using a fine-tuned Korean BERT model (KcBERT) on Korean offensive language
  data (KOLD). The authors define societal bias as language that leads to discriminatory
  treatment based on ethnicity, gender, and race.
---

# Detecting Bias in Large Language Models: Fine-tuned KcBERT

## Quick Facts
- arXiv ID: 2403.10774
- Source URL: https://arxiv.org/abs/2403.10774
- Reference count: 35
- This study investigates societal biases in large language models (LLMs) using a fine-tuned Korean BERT model (KcBERT) on Korean offensive language data (KOLD).

## Executive Summary
This study investigates societal biases in large language models (LLMs) using a fine-tuned Korean BERT model (KcBERT) on Korean offensive language data (KOLD). The authors define societal bias as language that leads to discriminatory treatment based on ethnicity, gender, and race. They quantify bias using Categorical Bias Score (CBS) for multi-category ethnic bias and Log-Probability Bias Score (LPBS) for binary-category gender and racial biases. Compared to the original KcBERT, the fine-tuned model shows reduced ethnic bias but significant changes in gender and racial biases. To mitigate these biases, the authors propose two methods: data balancing during pre-training to adjust word distribution and Debiasing Regularization during training using dropout and L2 regularization. These methods effectively reduce bias metrics and training loss. The study demonstrates that societal bias exists in Korean language models and highlights the need for bias mitigation strategies tailored to language-dependent characteristics.

## Method Summary
The authors investigate societal bias in Korean language models by fine-tuning KcBERT on Korean offensive language data (KOLD). They define societal bias as language leading to discriminatory treatment based on ethnicity, gender, and race. Bias quantification is performed using two metrics: Categorical Bias Score (CBS) for multi-category ethnic bias and Log-Probability Bias Score (LPBS) for binary-category gender and racial biases. The study compares the original KcBERT with the fine-tuned model, revealing reduced ethnic bias but significant changes in gender and racial biases. To mitigate these biases, the authors propose two methods: data balancing during pre-training to adjust word distribution and Debiasing Regularization during training using dropout and L2 regularization. These methods effectively reduce bias metrics and training loss, demonstrating the presence of societal bias in Korean language models and the need for language-dependent bias mitigation strategies.

## Key Results
- Fine-tuned KcBERT shows reduced ethnic bias compared to the original model
- Significant changes observed in gender and racial biases after fine-tuning
- Proposed debiasing methods (data balancing and Debiasing Regularization) effectively reduce bias metrics and training loss

## Why This Works (Mechanism)
The study's approach works by leveraging fine-tuning on domain-specific data (KOLD) to adjust the model's understanding of offensive language and associated biases. The proposed debiasing methods operate at different stages: data balancing modifies the training corpus to reduce biased word distributions, while Debiasing Regularization applies regularization techniques during training to discourage biased representations. These mechanisms work synergistically to reduce measured bias scores while maintaining model performance on language tasks.

## Foundational Learning
- Korean BERT (KcBERT): A BERT model pre-trained on Korean language data. Why needed: Korean has unique linguistic characteristics requiring specialized language models. Quick check: Verify model architecture matches standard BERT with Korean-specific tokenization.
- Categorical Bias Score (CBS): Metric for quantifying multi-category ethnic bias. Why needed: Binary metrics insufficient for complex ethnic bias scenarios. Quick check: Ensure CBS calculation accounts for all relevant ethnic categories in Korean context.
- Log-Probability Bias Score (LPBS): Metric for binary-category gender and racial biases. Why needed: Provides interpretable measure of bias through probability differences. Quick check: Validate LPBS calculations against known biased/unbiased examples.
- KOLD (Korean Offensive Language Dataset): Domain-specific dataset for fine-tuning. Why needed: General pre-training insufficient for detecting offensive language nuances. Quick check: Confirm dataset covers diverse offensive language scenarios.
- Debiasing Regularization: Training-time regularization technique. Why needed: Addresses bias during model parameter updates. Quick check: Monitor training loss to ensure regularization doesn't harm model convergence.

## Architecture Onboarding

Component map: Korean text -> KcBERT tokenizer -> Encoder layers -> CLS token -> Bias detection metrics (CBS/LPBS) -> Bias mitigation methods

Critical path: Input text → Tokenization → Encoding → Bias detection → Mitigation → Output

Design tradeoffs:
- Language-specific vs. multilingual models: Korean-specific KcBERT vs. general multilingual BERT
- Fine-tuning vs. training from scratch: Resource efficiency vs. complete control over bias mitigation
- Metric selection: CBS for ethnic bias vs. LPBS for gender/racial bias - each captures different bias aspects

Failure signatures:
- Over-mitigation: Excessive bias reduction leading to unnatural language generation
- Under-mitigation: Insufficient bias reduction maintaining discriminatory outputs
- Metric misalignment: Bias metrics not correlating with actual societal impact

Three first experiments:
1. Baseline bias measurement on original KcBERT using CBS and LPBS
2. Fine-tuning KcBERT on KOLD and re-measuring bias metrics
3. Applying Debiasing Regularization during fine-tuning and comparing bias reduction

## Open Questions the Paper Calls Out
None

## Limitations
- Narrow focus on Korean language and culture limits generalizability
- Bias metrics rely on specific Korean datasets and may not capture full spectrum of biases
- Effectiveness of debiasing methods needs validation across different model architectures

## Confidence

High confidence:
- Existence of societal bias in Korean LLMs
- Effectiveness of CBS and LPBS metrics in quantifying biases

Medium confidence:
- Specific bias reduction achieved through fine-tuning
- Relative effectiveness of different debiasing approaches

Low confidence:
- Generalizability of findings to other languages and cultures
- Long-term stability of bias mitigation

## Next Checks

1. Replicate the study using multilingual BERT models to assess cross-linguistic generalizability of bias detection and mitigation methods.

2. Conduct longitudinal studies to evaluate the persistence of bias reduction over extended model usage and fine-tuning cycles.

3. Implement human evaluation studies to validate the relationship between automated bias metrics and perceived fairness in Korean language contexts.