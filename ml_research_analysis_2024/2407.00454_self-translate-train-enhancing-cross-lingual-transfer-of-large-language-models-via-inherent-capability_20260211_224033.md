---
ver: rpa2
title: 'Self-Translate-Train: Enhancing Cross-Lingual Transfer of Large Language Models
  via Inherent Capability'
arxiv_id: '2407.00454'
source_url: https://arxiv.org/abs/2407.00454
tags:
- data
- language
- cross-lingual
- translation
- transfer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper introduces Self-Translate-Train, a method that leverages
  large language models' translation capabilities to enhance cross-lingual transfer.
  The approach involves having the model translate training data into the target language
  and fine-tuning on the generated synthetic data.
---

# Self-Translate-Train: Enhancing Cross-Lingual Transfer of Large Language Models via Inherent Capability

## Quick Facts
- arXiv ID: 2407.00454
- Source URL: https://arxiv.org/abs/2407.00454
- Reference count: 40
- Primary result: Self-Translate-Train improves cross-lingual transfer by having LLMs translate training data into target languages, with effectiveness strongly correlated to translation quality

## Executive Summary
This paper introduces Self-Translate-Train, a method that leverages large language models' translation capabilities to enhance cross-lingual transfer performance. The approach involves generating synthetic training data in the target language by having the LLM translate source language data, then fine-tuning on this augmented dataset. Experiments across three tasks (math, QA, NLI) and four languages (German, Russian, Thai, Chinese) demonstrate consistent improvements over zero-shot transfer baselines, with gains ranging from 5.0-9.1% absolute accuracy. The method's effectiveness is particularly pronounced when translation quality is high, suggesting that leveraging a model's inherent cross-lingual capabilities can significantly boost performance in multilingual settings.

## Method Summary
Self-Translate-Train generates synthetic training data in the target language by using the LLM's own translation capabilities. The method employs few-shot prompting to elicit translation from the model, filters low-quality translations based on length ratios and repetition, and then fine-tunes the model on an augmented dataset combining original source language data with the generated target language data. The approach uses LoRA for parameter-efficient fine-tuning and includes an optional code-switched data augmentation where input and output languages differ.

## Key Results
- Self-Translate-Train consistently outperforms zero-shot transfer baselines across all tested tasks and languages
- Performance gains correlate strongly with translation quality, with improvements of 5.0-9.1% absolute accuracy
- Thai language presents challenges due to lower translation quality (BLEU scores 17.8-18.3), limiting method effectiveness
- Code-switched data augmentation shows limited additional benefit despite theoretical advantages

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Self-Translate-Train leverages the LLM's translation capability to generate synthetic training data in the target language, which helps align internal representations between languages.
- Mechanism: By having the LLM translate training data into the target language, the model can learn the correspondence between languages through the generated synthetic data, improving cross-lingual transfer.
- Core assumption: The LLM captures cross-lingual correspondence useful for transfer, even if it cannot effectively generalize across languages during fine-tuning.
- Evidence anchors:
  - [abstract] "We hypothesize that even when the model cannot generalize across languages effectively in fine-tuning, it still captures cross-lingual correspondence useful for cross-lingual transfer."
  - [section 1] "This process does not only rely on cross-lingual generalization that happens during fine-tuning, but also on the cross-lingual correspondence that the model captures and can be elicited through translation."
- Break condition: If the LLM's translation quality is poor for the target language, the generated synthetic data may not effectively align representations, reducing the method's effectiveness.

### Mechanism 2
- Claim: The method improves performance by providing more training data in the target language, allowing the model to learn task-specific patterns in that language.
- Mechanism: The synthetic data in the target language augments the original source language data, giving the model more examples to learn from in the target language context.
- Core assumption: More training data in the target language leads to better performance, even if that data is generated by the model itself.
- Evidence anchors:
  - [section 1] "The generated data can be added to Dsrc to achieve a better generalization to the target language."
  - [section 5.1] "Results show that fine-tuning on English (Dsrc) is more effective than few-shot learning in the target language (FewShot). This confirms that cross-lingual transfer from English is non-trivial and effective in this setting."
- Break condition: If the task is already well-learned in the source language, adding synthetic data may not provide significant improvement.

### Mechanism 3
- Claim: Code-switched synthetic data can provide additional information by exposing the model to different language combinations within the same task context.
- Mechanism: By creating pairs where input and output are in different languages, the model learns to handle cross-lingual mappings more flexibly.
- Core assumption: The model benefits from seeing different language combinations for the same semantic content.
- Evidence anchors:
  - [section 1] "We can exploit this to further synthesize data by generating code-switched instances where the input and output are in different languages."
  - [section 5.1] "However, the effectiveness of code-switching dataset is limited. When we add Dcs to Dtgt, there is no significant improvement from adding Dtgt only (p < 0.05 in Welch's t-test)."
- Break condition: If the model already effectively handles the cross-lingual mappings, code-switched data may not provide additional benefit.

## Foundational Learning

- Concept: Cross-lingual transfer
  - Why needed here: The paper's core focus is on improving cross-lingual transfer performance using LLMs.
  - Quick check question: What is the difference between zero-shot cross-lingual transfer and few-shot cross-lingual transfer?

- Concept: Translation quality and its impact on downstream tasks
  - Why needed here: The paper emphasizes the importance of translation quality for the effectiveness of Self-Translate-Train.
  - Quick check question: How does the BLEU score relate to the downstream task performance in this context?

- Concept: Parameter-efficient tuning techniques (e.g., LoRA)
  - Why needed here: The paper uses LoRA for fine-tuning the LLMs, which is crucial for the experimental setup.
  - Quick check question: What is the main advantage of using LoRA over full fine-tuning in this context?

## Architecture Onboarding

- Component map:
  Source language data -> LLM translation module -> Data filtering -> Fine-tuning module -> Target language task performance

- Critical path:
  1. Translate source data to target language using LLM
  2. Filter generated data for quality
  3. Augment original data with synthetic data
  4. Fine-tune LLM on augmented data
  5. Evaluate on target language test set

- Design tradeoffs:
  - Translation quality vs. computational cost: Higher quality translations may require more compute but improve performance
  - Amount of synthetic data vs. original data: More synthetic data may help but could also introduce noise
  - Code-switched data inclusion: May provide additional benefits but also increases complexity and computational cost

- Failure signatures:
  - No improvement over baseline: Could indicate poor translation quality or task already well-learned
  - Performance degradation: Might suggest issues with data filtering or synthetic data quality
  - Inconsistent results across runs: Could point to issues with fine-tuning stability or data filtering

- First 3 experiments:
  1. Baseline comparison: Few-shot learning vs. fine-tuning on source language data
  2. Self-Translate-Train with Dtgt only: Compare performance with and without synthetic data
  3. Self-Translate-Train with Dtgt and Dcs: Evaluate the impact of code-switched data

## Open Questions the Paper Calls Out
- How can we improve translation quality for low-resource languages like Thai to enhance cross-lingual transfer performance?
- What is the optimal balance between synthetic data and original source language data for fine-tuning to maximize cross-lingual transfer performance?
- How do different types of large language models (e.g., autoregressive, encoder-decoder, or other architectures) affect the effectiveness of Self-Translate-Train for cross-lingual transfer?

## Limitations
- Translation quality varies significantly across languages, with Thai showing notably lower performance
- Filtering mechanism for low-quality translations relies on simple heuristics that may miss quality issues
- Method's generalizability to other NLP tasks beyond math, QA, and NLI remains untested

## Confidence
**High Confidence**: Self-Translate-Train outperforms zero-shot transfer baselines across all tested tasks and languages, with consistent improvements of 5.0-9.1% absolute accuracy.

**Medium Confidence**: Translation quality is the primary driver of performance improvements, though the relationship is not perfectly linear and other factors may contribute.

**Low Confidence**: Code-switched data augmentation provides additional benefits, as the paper itself reports no significant improvement from this component.

## Next Checks
1. Apply Self-Translate-Train to additional NLP tasks beyond math, QA, and NLI to test generalizability across different input-output formats and reasoning requirements.

2. Conduct ablation studies isolating translation quality effects by using machine-translated data versus LLM-generated translations across all target languages, controlling for other variables.

3. Evaluate alternative filtering mechanisms (beyond length ratio and repetition) including semantic similarity checks or human evaluation to determine if current filtering misses significant quality issues.