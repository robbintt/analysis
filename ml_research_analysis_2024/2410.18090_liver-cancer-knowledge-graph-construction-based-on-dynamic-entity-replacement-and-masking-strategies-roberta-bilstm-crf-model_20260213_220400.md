---
ver: rpa2
title: Liver Cancer Knowledge Graph Construction based on dynamic entity replacement
  and masking strategies RoBERTa-BiLSTM-CRF model
arxiv_id: '2410.18090'
source_url: https://arxiv.org/abs/2410.18090
tags:
- entity
- knowledge
- medical
- graph
- cancer
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: The study develops a liver cancer knowledge graph from Chinese
  electronic medical records (EMRs) using a novel Dynamic Entity Replacement and Masking
  Strategy (DERM) integrated with a RoBERTa-BiLSTM-CRF model for named entity recognition.
  The approach addresses challenges of data source discrepancy and effective integration
  between EMRs and online medical knowledge bases.
---

# Liver Cancer Knowledge Graph Construction based on dynamic entity replacement and masking strategies RoBERTa-BiLSTM-CRF model

## Quick Facts
- arXiv ID: 2410.18090
- Source URL: https://arxiv.org/abs/2410.18090
- Authors: YiChi Zhang; HaiLing Wang; YongBin Gao; XiaoJun Hu; YingFang Fan; ZhiJun Fang
- Reference count: 30
- Key outcome: DERM-RoBERTa-BiLSTM-CRF model achieves 93.96% F1 score for liver cancer entity recognition from Chinese EMRs

## Executive Summary
This study develops a liver cancer knowledge graph from Chinese electronic medical records using a novel Dynamic Entity Replacement and Masking Strategy (DERM) integrated with a RoBERTa-BiLSTM-CRF model for named entity recognition. The approach addresses challenges of data source discrepancy and effective integration between EMRs and online medical knowledge bases. The constructed knowledge graph includes 7 entity types (disease, symptom, constitution, etc.) with 1495 entities. The system enables semantic querying and supports downstream applications like disease screening and complication detection. The research provides a framework for constructing knowledge graphs from EMRs that can be adapted for other medical domains.

## Method Summary
The study constructs a liver cancer knowledge graph using a multi-stage pipeline. First, 310 Chinese EMRs are preprocessed and annotated with 7 entity types. The DERM-RoBERTa-BiLSTM-CRF model is then trained on this data, where DERM dynamically replaces or masks entities during training to improve generalization. Entities are extracted from EMRs using the trained model, then aligned with external medical knowledge bases using TF-IDF similarity. The final knowledge graph is stored in Neo4j and supports semantic queries. The approach achieves 93.96% F1 score for entity recognition, outperforming baseline models by 4.3%.

## Key Results
- DERM-RoBERTa-BiLSTM-CRF model achieves 93.96% F1 score (vs. 89.66% baseline)
- Constructed knowledge graph contains 1495 entities across 7 types
- Model shows 94.69% recall and 93.23% precision
- Operation entity recognition achieves perfect 100% F1 score
- System successfully identifies liver cancer complications and supports semantic querying

## Why This Works (Mechanism)

### Mechanism 1
- Claim: DERM improves named entity recognition by increasing training data diversity through dynamic entity replacement and masking.
- Mechanism: During training, the model randomly replaces or masks entities in the input text with a 30% probability, exposing the model to varied representations of the same entities. This reduces overfitting to specific entity surface forms and improves generalization.
- Core assumption: Random replacement and masking of entities during training helps the model learn more robust entity representations.
- Evidence anchors:
  - [abstract] "A novel Dynamic Entity Replacement and Masking Strategy (DERM) for named entity recognition is proposed."
  - [section] "Firstly, a dictionary is constructed for each entity that has been manually annotated by categories such as diseases, symptoms, and treatment options, and then dynamic entity substitution and masking are performed on the text of the EMR during the training process..."
  - [corpus] Weak evidence - corpus neighbors focus on deep learning for liver cancer but do not discuss DERM specifically.
- Break condition: If the entity dictionary is incomplete or the replacement/masking introduces too much noise, the model may learn incorrect entity associations.

### Mechanism 2
- Claim: RoBERTa-wwm-large outperforms BERT due to whole word masking, which is better suited for Chinese medical NER.
- Mechanism: RoBERTa-wwm-large masks entire Chinese words instead of individual characters, allowing the model to learn word-level semantic representations. This is particularly effective for Chinese medical terminology which often consists of multi-character words.
- Core assumption: Word-level masking captures semantic relationships better than character-level masking for Chinese text.
- Evidence anchors:
  - [section] "RoBERTa-wwm-large... removes the NSP (Next Sentence Prediction) task during the pre-training process... uses the whole word masking strategy, which has a significant advantage in Chinese NER."
  - [section] "BERT may mask just one term of a character like '呕、泻'... In contrast, RoBERTa-wwm-large adopts a whole-terms masking strategy... RoBERTa-wwm-large can learn term-level semantic representations..."
  - [corpus] Weak evidence - corpus neighbors discuss deep learning but not the specific advantages of RoBERTa's masking strategy.
- Break condition: If the Chinese medical text contains many single-character entities or rare words, the word-level masking might miss important character-level information.

### Mechanism 3
- Claim: Knowledge fusion using TF-IDF improves the quality of the liver cancer knowledge graph by integrating EMR data with online medical knowledge bases.
- Mechanism: TF-IDF is used to calculate the similarity between entities in EMRs and online medical knowledge bases, allowing for entity alignment and integration. This addresses the challenge of inconsistent terminology between different data sources.
- Core assumption: TF-IDF similarity scores can effectively match entities across different medical knowledge bases.
- Evidence anchors:
  - [section] "We employ the Term Frequency-Inverse Document Frequency (TF-IDF) method for this purpose... This composite score effectively balances word frequency within documents against word distinctiveness across the corpus."
  - [section] "For disease entity recognition, the EMR-based knowledge graph is first utilized to pinpoint all disease entities. Subsequently, a specific disease entity is queried using the www.XYWY .com disease entity library, employing a TF-IDF vectorizer to determine the highest cosine similarity match..."
  - [corpus] Weak evidence - corpus neighbors do not discuss knowledge fusion techniques.
- Break condition: If the TF-IDF threshold is set too high or too low, it may either miss valid entity matches or incorrectly match unrelated entities.

## Foundational Learning

- Concept: Named Entity Recognition (NER) in Chinese medical text
  - Why needed here: The study relies on accurate entity recognition from Chinese EMRs to construct the knowledge graph.
  - Quick check question: What are the key challenges in performing NER on Chinese medical text compared to general text?

- Concept: Knowledge graph construction and entity alignment
  - Why needed here: The study integrates data from multiple sources (EMRs and online knowledge bases) into a unified knowledge graph, requiring entity alignment techniques.
  - Quick check question: How does TF-IDF help in aligning entities from different medical knowledge bases?

- Concept: Deep learning models for NER (BiLSTM-CRF, BERT, RoBERTa)
  - Why needed here: The study compares different deep learning architectures for NER and proposes improvements using DERM and RoBERTa.
  - Quick check question: What are the advantages of using BiLSTM-CRF over a simple BiLSTM for NER tasks?

## Architecture Onboarding

- Component map: EMR preprocessing -> DERM module -> RoBERTa-wwm-large -> BiLSTM -> CRF -> Entity extraction -> TF-IDF alignment -> Neo4j knowledge graph

- Critical path:
  1. Preprocess EMRs and online medical knowledge base data
  2. Annotate EMRs with entity labels
  3. Train DERM-RoBERTa-BiLSTM-CRF model on annotated data
  4. Extract entities from EMRs using the trained model
  5. Perform entity alignment using TF-IDF
  6. Integrate entities into Neo4j knowledge graph
  7. Perform semantic queries on the knowledge graph

- Design tradeoffs:
  - DERM increases training time but improves model generalization
  - RoBERTa-wwm-large requires more computational resources than BERT but provides better performance for Chinese text
  - TF-IDF is simple and effective but may struggle with synonyms or very rare entities

- Failure signatures:
  - Low precision: Model is overgenerating entity predictions
  - Low recall: Model is missing many actual entities in the text
  - Knowledge graph sparsity: Entity alignment is not working effectively
  - Long training times: DERM strategy may be introducing too much noise

- First 3 experiments:
  1. Train the baseline RoBERTa-BiLSTM-CRF model without DERM on a small subset of annotated data and evaluate F1 score
  2. Implement the DERM strategy and train the model again, comparing F1 scores and training times
  3. Perform entity alignment using TF-IDF on a small set of manually verified entity pairs to assess alignment accuracy

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the DERM module's performance scale with larger datasets beyond the current 310 EMRs?
- Basis in paper: [explicit] The paper notes that the model's training time is 20% slower due to DERM rules and mentions the current dataset size
- Why unresolved: The study only tested on 310 EMRs and did not explore performance with larger datasets
- What evidence would resolve it: Testing the DERM-RoBERTa-BiLSTM-CRF model on datasets 10x or 100x larger would reveal scalability limitations or advantages

### Open Question 2
- Question: How robust is the knowledge graph to concept drift in medical terminology over time?
- Basis in paper: [inferred] The paper mentions entity normalization challenges and integrating with external knowledge bases like XYWY.com
- Why unresolved: The study doesn't address how the knowledge graph adapts to evolving medical terminology and concepts
- What evidence would resolve it: Longitudinal studies tracking knowledge graph performance over 5+ years with periodic updates would demonstrate adaptation capabilities

### Open Question 3
- Question: Can the knowledge graph effectively handle rare or previously undocumented liver cancer subtypes?
- Basis in paper: [explicit] The study achieved 100% F1 score for operation entities but lower scores for symptom entities (86.06%)
- Why unresolved: The paper doesn't test the system's ability to recognize and integrate novel or rare disease presentations
- What evidence would resolve it: Testing the system on a curated dataset of rare liver cancer cases would reveal its ability to handle edge cases and novel entities

## Limitations

- DERM strategy parameters (30% replacement/masking probability) appear arbitrary without sensitivity analysis
- Model generalization to different liver cancer populations or other medical domains remains untested
- Knowledge graph integration relies on TF-IDF which may struggle with synonyms or very rare entities

## Confidence

- **High Confidence**: The overall knowledge graph construction pipeline and its components (RoBERTa-BiLSTM-CRF, Neo4j storage, TF-IDF alignment) are well-established methods with proven results
- **Medium Confidence**: The specific DERM strategy and its 30% probability parameters are novel but lack comprehensive ablation studies or sensitivity analysis
- **Medium Confidence**: The reported performance metrics are strong, but come from a single dataset without external validation

## Next Checks

1. Conduct ablation studies varying the DERM probability parameters (20%, 30%, 40%) to determine optimal values and test sensitivity
2. Validate the model on an external dataset from a different hospital or time period to assess generalization
3. Perform a head-to-head comparison with other state-of-the-art NER models (e.g., clinicalBERT, SciBERT) using the same dataset and evaluation metrics