---
ver: rpa2
title: 'BOK-VQA: Bilingual outside Knowledge-Based Visual Question Answering via Graph
  Representation Pretraining'
arxiv_id: '2401.06443'
source_url: https://arxiv.org/abs/2401.06443
tags:
- question
- knowledge
- information
- triple
- dataset
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper proposes a bilingual outside-knowledge VQA (BOK-VQA)
  dataset and a graph-embedded learning-based VQA (GEL-VQA) model to address the challenge
  of multilingual VQA by leveraging external knowledge. The BOK-VQA dataset contains
  17K Korean-English question-answer pairs and 280K knowledge triples, enabling multilingual
  extensibility.
---

# BOK-VQA: Bilingual outside Knowledge-Based Visual Question Answering via Graph Representation Pretraining

## Quick Facts
- arXiv ID: 2401.06443
- Source URL: https://arxiv.org/abs/2401.06443
- Reference count: 10
- Introduces BOK-VQA dataset with 17K Korean-English QA pairs and 280K knowledge triples

## Executive Summary
This paper introduces a novel bilingual outside-knowledge VQA dataset (BOK-VQA) and a graph-embedded learning-based VQA model (GEL-VQA) that addresses the challenge of multilingual VQA by leveraging external knowledge. The BOK-VQA dataset contains 17K Korean-English question-answer pairs and 280K knowledge triples, enabling multilingual extensibility. The GEL-VQA model employs a multitask learning approach, combining VQA and triple prediction tasks, and utilizes knowledge graph embeddings (KGEs) to incorporate external knowledge.

## Method Summary
The proposed approach introduces BOK-VQA, a bilingual dataset for knowledge-based visual question answering containing Korean and English question-answer pairs. The GEL-VQA model uses a multitask learning framework that simultaneously handles VQA tasks and knowledge triple prediction. The model leverages knowledge graph embeddings to incorporate external knowledge into the VQA process, enabling better performance in multilingual settings. The approach specifically addresses the challenge of less-resourced languages like Korean by utilizing shared knowledge representations across languages.

## Key Results
- GEL-VQA outperforms baseline by 25.97% in bilingual settings
- Korean performance is 3.7% higher than English performance
- Successfully demonstrates multilingual extensibility with 17K Korean-English QA pairs and 280K knowledge triples

## Why This Works (Mechanism)
The model leverages external knowledge through knowledge graph embeddings to enhance VQA performance. The multitask learning approach combines VQA and triple prediction tasks, allowing the model to simultaneously learn visual understanding and knowledge retrieval. The bilingual nature of the dataset enables knowledge sharing across languages, which particularly benefits less-resourced languages like Korean through cross-lingual transfer learning.

## Foundational Learning
- **Knowledge Graph Embeddings (KGEs)**: Used to represent external knowledge in a format suitable for neural networks. Needed to incorporate structured knowledge into the VQA pipeline. Quick check: Verify KGE quality by examining entity similarity scores.
- **Multitask Learning**: Combines VQA and knowledge triple prediction in a single model. Needed to jointly optimize visual understanding and knowledge retrieval. Quick check: Monitor task-specific loss components during training.
- **Bilingual Data Processing**: Handles both Korean and English question-answer pairs. Needed to enable cross-lingual knowledge transfer. Quick check: Validate translation quality and alignment between language pairs.

## Architecture Onboarding
**Component Map**: Input Image + Text -> Feature Extractor -> Knowledge Graph Embeddings -> Multitask Head (VQA + Triple Prediction) -> Output
**Critical Path**: Image encoding → Knowledge graph integration → Multitask prediction
**Design Tradeoffs**: Balance between VQA accuracy and knowledge retrieval precision; multilingual data quality vs. quantity
**Failure Signatures**: Performance degradation when knowledge triples are incomplete; language-specific bias in knowledge graph
**First Experiments**: 1) Ablation study removing KGEs 2) Single-language performance comparison 3) Knowledge graph sparsity analysis

## Open Questions the Paper Calls Out
None

## Limitations
- Claims of 25.97% improvement need verification of baseline comparisons and experimental conditions
- Korean vs. English performance difference requires investigation into dataset characteristics and evaluation methodology
- Knowledge graph quality and consistency across languages need validation to ensure unbiased performance

## Confidence
- 25.97% improvement over baseline: Medium confidence (requires detailed baseline comparison verification)
- 3.7% higher Korean performance: Medium confidence (needs analysis of underlying causes)
- Dataset quality claims: Medium confidence (requires validation of knowledge triple consistency)

## Next Checks
1. Conduct ablation studies to isolate the contribution of knowledge graph embeddings versus other model components to the reported performance gains
2. Test model performance on additional languages beyond Korean and English to verify true multilingual extensibility claims
3. Perform qualitative analysis of knowledge triples to assess their relevance, accuracy, and potential biases across both languages in the dataset