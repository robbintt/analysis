---
ver: rpa2
title: Hard-Thresholding Meets Evolution Strategies in Reinforcement Learning
arxiv_id: '2405.01615'
source_url: https://arxiv.org/abs/2405.01615
tags:
- features
- policy
- hard-thresholding
- gradient
- nesht
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of evolution strategies (ES)
  in reinforcement learning when dealing with task-irrelevant features, which can
  degrade performance. The authors propose NESHT, a novel approach that integrates
  Hard-Thresholding (HT) with Natural Evolution Strategies (NES) to champion sparsity,
  ensuring only pertinent features are employed.
---

# Hard-Thresholding Meets Evolution Strategies in Reinforcement Learning

## Quick Facts
- **arXiv ID**: 2405.01615
- **Source URL**: https://arxiv.org/abs/2405.01615
- **Reference count**: 40
- **Primary result**: NESHT consistently outperforms vanilla NES and other baselines in noisy Mujoco and Atari tasks

## Executive Summary
This paper addresses the challenge of evolution strategies (ES) in reinforcement learning when dealing with task-irrelevant features, which can degrade performance. The authors propose NESHT, a novel approach that integrates Hard-Thresholding (HT) with Natural Evolution Strategies (NES) to champion sparsity, ensuring only pertinent features are employed. NESHT leverages the Hard-Thresholding operator to truncate the parameter vector, retaining only a specified proportion of the most significant absolute magnitudes. The paper provides a comprehensive analysis of the convergence and complexity of NESHT, underpinning it with the canonical assumptions of sparse learning.

## Method Summary
NESHT integrates Hard-Thresholding (HT) with Natural Evolution Strategies (NES) to champion sparsity in reinforcement learning. The algorithm estimates gradients using perturbations and fitness evaluations, then applies the HT operator to truncate weights, retaining only the top k largest absolute magnitudes. This enforces an L0 sparsity constraint, eliminating task-irrelevant features. The method optimizes a smoothed version of the fitness function to enable convergence analysis under standard sparse learning assumptions. Empirical tests on noisy Mujoco and Atari tasks demonstrate effectiveness in mitigating the pitfalls of irrelevant features.

## Key Results
- NESHT consistently outperforms vanilla NES and other baselines in noisy Mujoco and Atari tasks
- Hard-thresholding effectively mitigates the impact of task-irrelevant features on gradient estimation
- The method provides convergence guarantees under canonical sparse learning assumptions

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Hard-thresholding operator filters irrelevant features by zeroing out small-magnitude weights
- **Mechanism**: During each parameter update, the hard-thresholding operator retains only the top k largest absolute weights and sets the rest to zero. This directly enforces an L0 sparsity constraint, ensuring only task-relevant features remain active
- **Core assumption**: The magnitude of learned weights correlates with feature relevance; irrelevant features naturally produce smaller weight values
- **Evidence anchors**:
  - [abstract]: "The modus operandi of the NESHT is straightforward: the parameters are truncated to retain only a specified proportion, upon each gradient descent/ascent update."
  - [section]: "With the HT operator, only the portion of the neuron corresponding to task-relevant features (the 0-th segment) is activated. Without HT, NES struggles with task-irrelevant features, leading to poor performance."
  - [corpus]: Weak; no direct mentions of hard-thresholding or feature selection in neighbor papers
- **Break condition**: If task-relevant features also have small magnitudes due to poor initialization or vanishing gradients, hard-thresholding may mistakenly eliminate them

### Mechanism 2
- **Claim**: Reducing noise variance via sparsity improves gradient estimation stability
- **Mechanism**: Task-irrelevant features increase the variance of the fitness function (constant C). Hard-thresholding reduces this variance by eliminating noisy contributions, leading to more stable gradient estimates and faster convergence
- **Core assumption**: Variance in the fitness function directly impacts gradient estimator variance, which affects convergence speed
- **Evidence anchors**:
  - [section]: "We present here a formal explanation for the superiority of NESHT over NES in the lens of constant C. Thanks to hard-thresholding, along training, θt and θt+1/2 remain in the space of k-sparse vectors (up to small perturbations σϵ), whereas they could live anywhere in Rd in the case of NES."
  - [abstract]: "NESHT consistently outperforms vanilla NES and other baselines in these challenging environments."
  - [corpus]: Weak; neighbor papers don't discuss variance reduction or gradient stability
- **Break condition**: If the noise is structured rather than random, hard-thresholding may not effectively reduce variance

### Mechanism 3
- **Claim**: Smoothing the discontinuous fitness function enables convergence analysis
- **Mechanism**: The original fitness function F is discontinuous due to environmental randomness. NESHT optimizes a smoothed version Fσ (expectation over Gaussian noise), which is Lipschitz-smooth with known constant L. This allows convergence proofs under standard assumptions
- **Core assumption**: The smoothed function Fσ is a good approximation of F when σ is small, and smoothness enables gradient-based optimization
- **Evidence anchors**:
  - [section]: "Since F can be discontinuous in general, maximizing F directly is impossible with evolutionary strategies. For instance if F is Dirac-like... we can instead analyze the convergence of a smoothed version of F, Fσ."
  - [abstract]: "We provide a comprehensive analysis of the convergence and complexity of NESHT, underpinning it with the canonical assumptions of sparse learning."
  - [corpus]: Weak; neighbor papers don't discuss function smoothing or Lipschitz continuity
- **Break condition**: If σ is too large, Fσ deviates significantly from F, leading to optimization of the wrong objective

## Foundational Learning

- **Concept**: Markov Decision Processes (MDPs)
  - **Why needed here**: The paper frames the reinforcement learning problem as maximizing expected cumulative reward over trajectories sampled from an MDP. Understanding state transitions, actions, and rewards is essential to grasp the fitness function definition
  - **Quick check question**: In the context of MDPs, what does the discount factor γ control, and why is it set to 1 in this paper?

- **Concept**: Evolution Strategies (ES) and gradient estimation
  - **Why needed here**: NESHT builds on NES, which estimates gradients using perturbations and fitness evaluations rather than backpropagation. Knowing how ES approximates gradients is crucial for understanding the algorithm's mechanics
  - **Quick check question**: How does the NES gradient estimator in Equation (3) approximate the true gradient of the expected fitness?

- **Concept**: Sparsity and L0-constrained optimization
  - **Why needed here**: NESHT explicitly enforces sparsity through hard-thresholding, which is an L0 constraint. Understanding the difference between L0 and L1 regularization is important for grasping why hard-thresholding is used
  - **Quick check question**: What is the key difference between L0 and L1 regularization in terms of sparsity enforcement?

## Architecture Onboarding

- **Component map**: Fitness function evaluator -> NES gradient estimator -> Hard-thresholding operator -> Policy parameter updater
- **Critical path**:
  1. Sample perturbations and rollouts
  2. Compute fitness scores
  3. Estimate gradient using NES
  4. Update parameters with gradient ascent
  5. Apply hard-thresholding to enforce sparsity
  6. Repeat until convergence
- **Design tradeoffs**:
  - Hard-thresholding ratio β vs. performance: Higher β (more sparsity) may improve robustness to noise but risks eliminating useful features
  - σ (smoothing radius) vs. convergence: Larger σ eases convergence but may cause optimization of Fσ instead of F
  - Population size n and rollouts N vs. sample efficiency: Larger values improve gradient estimates but increase computational cost
- **Failure signatures**:
  - Performance degrades with increasing task-irrelevant features (vanilla NES fails)
  - Too aggressive hard-thresholding (high β) eliminates relevant features
  - Insufficient smoothing (small σ) causes gradient estimator variance to dominate
  - Learning rate α too large causes divergence; too small causes slow convergence
- **First 3 experiments**:
  1. Run NESHT on Mujoco Hopper with ×5 Gaussian noise, varying β from 0.0 to 0.95 to observe sparsity-performance tradeoff
  2. Compare NESHT vs. vanilla NES on Atari games with pixel inputs to validate hard-thresholding effectiveness on visual tasks
  3. Analyze learned weight norms across observation segments to verify hard-thresholding selects relevant features

## Open Questions the Paper Calls Out

The paper does not explicitly call out open questions in the traditional sense, but the limitations section highlights several unresolved issues:

1. The paper acknowledges that the optimal hard-thresholding ratio β differs across environments but does not provide a systematic method for determining it
2. The theoretical analysis assumes bounded fitness functions and variance, which may not hold in all practical scenarios
3. The method's performance on high-dimensional continuous control tasks beyond the tested Mujoco and Atari environments remains unexplored

## Limitations
- Weak empirical evidence in the corpus supporting the core mechanisms of hard-thresholding and variance reduction
- No systematic study on how to determine the optimal hard-thresholding ratio β for different environments
- Limited evaluation to Mujoco and Atari tasks, without testing on high-dimensional continuous control scenarios

## Confidence

- **Mechanism 1 (Feature filtering via hard-thresholding)**: Medium - core assumption about weight magnitude correlating with feature relevance is plausible but untested
- **Mechanism 2 (Variance reduction)**: Low - theoretical claim lacks empirical support in the corpus
- **Mechanism 3 (Function smoothing)**: Medium - mathematically sound but practical effectiveness unclear

## Next Checks

1. Conduct ablation studies on NESHT with varying β values across different Mujoco tasks to empirically validate the sparsity-performance tradeoff
2. Measure gradient estimator variance with and without hard-thresholding to quantify the variance reduction claim
3. Test NESHT's robustness to different noise distributions (not just Gaussian) to validate the assumption about noise characteristics