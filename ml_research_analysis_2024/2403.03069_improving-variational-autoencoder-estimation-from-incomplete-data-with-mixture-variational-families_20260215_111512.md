---
ver: rpa2
title: Improving Variational Autoencoder Estimation from Incomplete Data with Mixture
  Variational Families
arxiv_id: '2403.03069'
source_url: https://arxiv.org/abs/2403.03069
tags:
- xobs
- data
- xmis
- variational
- incomplete
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of training variational autoencoders
  (VAEs) on incomplete data where some variables are missing. The authors show that
  missing data increases the complexity of the model's posterior distribution over
  the latent variables, which can lead to a mismatch between the variational and model
  posterior distributions and adversely affect model fitting.
---

# Improving Variational Autoencoder Estimation from Incomplete Data with Mixture Variational Families

## Quick Facts
- **arXiv ID**: 2403.03069
- **Source URL**: https://arxiv.org/abs/2403.03069
- **Reference count**: 40
- **Primary result**: Missing data increases posterior complexity in VAEs, and mixture variational families (finite mixtures and imputation-based approaches) improve estimation accuracy compared to single-component methods

## Executive Summary
This paper addresses the challenge of training variational autoencoders (VAEs) on incomplete data where some variables are missing. The authors demonstrate that missing data increases the complexity of the posterior distribution over latent variables, creating a mismatch between the variational and model posteriors that can harm model fitting. To address this issue, they propose two strategies based on variational-mixture distributions: finite variational-mixture approaches that use multiple mixture components, and an imputation-based variational-mixture approach that decouples model estimation from data imputation. Experiments on synthetic and real-world datasets show that these mixture approaches improve VAE estimation accuracy compared to existing methods that don't use variational mixtures.

## Method Summary
The paper proposes two main strategies to handle incomplete data in VAEs: finite variational-mixture distributions and an imputation-based variational-mixture approach. The finite mixture methods (MissVAE, MissSVAE, MissIWAE, MissSIWAE) increase the flexibility of the variational family by using multiple mixture components, with either ancestral or stratified sampling for reparameterization. The imputation-based method (DeMissVAE) approximates the posterior by marginalizing over imputed missing values, effectively decoupling model estimation from data imputation. All methods optimize variants of the ELBO, with the imputation-based method using a looser bound that averages over imputed samples rather than requiring direct modeling of pθ(xmis|xobs).

## Key Results
- Missing data increases posterior complexity in VAEs, leading to mismatch between variational and model posteriors
- Finite variational-mixture approaches (MissVAE/MissSVAE) improve accuracy by increasing variational family flexibility
- Imputation-based variational-mixture approach (DeMissVAE) achieves comparable performance to mixture methods despite using a looser ELBO bound
- Stratified sampling for mixtures often outperforms ancestral sampling in terms of gradient variance
- Methods show consistent improvements across synthetic mixture-of-Gaussians data and UCI repository datasets with varying missingness rates

## Why This Works (Mechanism)

### Mechanism 1
- **Claim**: Missing data increases posterior complexity in VAEs, leading to mismatch between variational and model posteriors
- **Mechanism**: When data is incomplete, the model's posterior over latent variables becomes multimodal and irregular, while the variational distribution (often Gaussian) remains unimodal and simple, creating a divergence that harms model fitting
- **Core assumption**: The increase in posterior complexity due to missingness is significant enough to cause practical estimation problems
- **Evidence anchors**:
  - [abstract]: "missing data increases the complexity of the model's posterior distribution over the latent variables compared to the fully-observed case"
  - [section 3]: "model posteriors...have become irregular multimodal distributionspθ(z|xobs) when evaluated with incomplete data"
  - [corpus]: Weak - no direct evidence, but "Normalising-flow variational inference" and "heterogeneous mixture of complementary flows" suggest related work on flexible variational families

### Mechanism 2
- **Claim**: Finite variational-mixture distributions can handle increased posterior complexity better than single-component distributions
- **Mechanism**: By using multiple mixture components with learned weights, the variational family becomes flexible enough to approximate multimodal posteriors, reducing the divergence from the true posterior
- **Core assumption**: The mixture components can be effectively learned using reparameterization tricks (ancestral or stratified sampling)
- **Evidence anchors**:
  - [abstract]: "finite variational-mixture approaches...increase the flexibility of the variational family"
  - [section 4.1]: "specifiyqϕ(z|xobs) as a finite-mixture distribution" with discussion of ancestral vs stratified sampling
  - [corpus]: Weak - mentions "Gaussian Mixture Vector Quantization" but not directly about variational inference with mixtures

### Mechanism 3
- **Claim**: Imputation-based variational-mixture approach decouples model estimation from data imputation, improving robustness
- **Mechanism**: By marginalizing over imputed values during training, the approach avoids the need to directly model pθ(xmis|xobs) and instead focuses on accurate approximation of the complete-data posterior, which is then averaged over imputations
- **Core assumption**: Approximate imputation methods (like pseudo-Gibbs or LAIR) can generate reasonable imputations without perfect accuracy
- **Evidence anchors**:
  - [abstract]: "imputation-based variational-mixture approach...decouples model estimation from data imputation problems"
  - [section 4.2]: Detailed derivation showing how the approach separates objectives for pθ and qϕ, and uses approximate sampling for imputation
  - [corpus]: Weak - no direct evidence, but "Adaptive Heterogeneous Mixtures" suggests interest in flexible imputation approaches

## Foundational Learning

- **Concept**: Evidence Lower Bound (ELBO) and its decomposition
  - Why needed here: All proposed methods optimize variants of the ELBO; understanding its decomposition (log-likelihood minus KL divergence) is essential to grasp why posterior approximation matters
  - Quick check question: What happens to the ELBO if the variational distribution is very far from the true posterior?

- **Concept**: Reparameterization trick and its limitations with mixture distributions
  - Why needed here: The paper discusses ancestral vs stratified sampling for mixture distributions because direct reparameterization is challenging; knowing why helps understand the proposed methods
  - Quick check question: Why can't we directly backpropagate through the sampling of mixture component indices?

- **Concept**: Importance-weighted ELBO (IWELBO) and its relationship to semi-implicit distributions
  - Why needed here: The paper compares standard ELBO methods with IWELBO methods; understanding that IWELBO uses multiple importance samples helps explain why it can handle some complexity without mixtures
  - Quick check question: How does increasing the number of importance samples in IWELBO affect the flexibility of the variational distribution?

## Architecture Onboarding

- **Component map**:
  - Encoder network: Maps input data to parameters of variational distribution (qϕ(z|xobs) for mixture methods, or qϕ(z|xobs,xmis) for imputation methods)
  - Decoder network: Maps latent variables to parameters of data distribution pθ(x|z)
  - Prior distribution: pθ(z), either fixed or learned
  - Mixture component networks (for finite mixtures): Additional encoder heads that output component probabilities qϕ(k|xobs) and component-specific variational parameters
  - Imputation sampler (for DeMissVAE): Iterative conditional sampling method to generate missing values
  - Training loop: Alternates between imputation updates and parameter updates using shared computation

- **Critical path**:
  1. Encode input with missing values to obtain variational parameters
  2. Sample latents and decode to compute reconstruction likelihood
  3. Compute ELBO or IWELBO objective
  4. Backpropagate gradients to update encoder and decoder
  5. For DeMissVAE, run imputation sampler and update imputation distribution

- **Design tradeoffs**:
  - Finite mixtures (MissVAE/MissIWAE): More flexible than single Gaussian but risk of mixture collapse; ancestral sampling is simpler but has higher gradient variance than stratified
  - Importance-weighted methods (MissIWAE/MissSIWAE): Can handle some complexity through importance sampling but require multiple latent samples per data point
  - DeMissVAE: Robust to imputation errors through marginalization but requires additional sampling step and two separate objectives

- **Failure signatures**:
  - Posterior collapse: All mixture components degenerate to one mode
  - High gradient variance: Training becomes unstable, especially with ancestral sampling
  - Poor imputation quality: DeMissVAE performance degrades if imputations are far from true conditionals
  - Mode dropping: Some posterior modes are not captured by the variational distribution

- **First 3 experiments**:
  1. Run MVAE (baseline) on a small synthetic dataset with missing data to establish baseline performance
  2. Implement MissVAE with K=3 components and compare log-likelihood to MVAE
  3. Try MissSVAE (stratified sampling) with same K=3 to see if gradient variance reduction improves results

## Open Questions the Paper Calls Out
None

## Limitations
- Theoretical justification for why missingness specifically increases posterior complexity is largely empirical rather than analytical
- Comparison focuses primarily on log-likelihood metrics without extensive ablation studies on hyperparameters like mixture component count
- Computational overhead of mixture methods versus standard VAEs is not thoroughly analyzed

## Confidence
- **High**: Missing data increases posterior complexity compared to fully-observed case (supported by empirical evidence across multiple datasets)
- **Medium**: Finite mixture distributions effectively capture this increased complexity (demonstrated empirically but theoretical justification is limited)
- **Medium**: DeMissVAE's decoupling approach provides robust performance despite looser ELBO bound (empirical results show this but mechanism is not fully explained)

## Next Checks
1. Run ablation study varying the number of mixture components (K) to identify optimal complexity trade-offs
2. Compare gradient variance between ancestral and stratified sampling methods quantitatively during training
3. Test DeMissVAE performance when imputation distribution is deliberately degraded to assess robustness limits