---
ver: rpa2
title: On Large Language Model Continual Unlearning
arxiv_id: '2407.10223'
source_url: https://arxiv.org/abs/2407.10223
tags:
- unlearning
- data
- request
- retained
- knowledge
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper addresses the challenge of continual unlearning in large
  language models (LLMs), where unlearning requests continuously emerge in real-world
  scenarios. Existing methods often fail to maintain model utility across such requests,
  especially when prior data access is restricted due to privacy or copyright concerns.
---

# On Large Language Model Continual Unlearning

## Quick Facts
- arXiv ID: 2407.10223
- Source URL: https://arxiv.org/abs/2407.10223
- Reference count: 40
- Key outcome: O3 framework achieves superior continual unlearning effectiveness and utility preservation without using retained data

## Executive Summary
This paper addresses the challenge of continual unlearning in large language models where unlearning requests continuously emerge in real-world scenarios. Existing methods often fail to maintain model utility across such requests, especially when prior data access is restricted due to privacy or copyright concerns. The authors propose the O3 framework, which combines an orthogonal low-rank adapter (LoRA) for continual unlearning and an out-of-distribution (OOD) detector to assess similarity between inputs and unlearning data. Experiments on three tasks and seven datasets show that O3 consistently outperforms state-of-the-art methods in both unlearning effectiveness and utility preservation.

## Method Summary
The O3 framework addresses continual unlearning through two key components: an orthogonal LoRA that disentangles parameter space across different unlearning requests to prevent interference, and an OOD detector trained with contrastive entropy loss and glocal-aware scoring to assess input similarity to unlearning data. During inference, soft-weighted loading of the unlearning LoRA is dynamically determined based on the OOD detector's similarity assessment. The method operates without using retained data, making it practical for real-world scenarios where prior data access is restricted. The framework is evaluated on LLaMA2-7b using three tasks: QA, fictitious knowledge generation, and intent classification across seven datasets.

## Key Results
- O3 consistently outperforms state-of-the-art methods on unlearning effectiveness (S.U., D.U.) and utility preservation (R.D., U.1., U.2.) metrics
- The framework achieves better Unlearning-Utility Ratio (U2R) across all experimental tasks
- O3 maintains effectiveness without using retained data, addressing practical privacy constraints

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Orthogonal LoRA disentangles parameter space across different unlearning requests, preventing interference and enabling effective continual unlearning.
- Mechanism: By initializing each LoRA with the latest parameters from the previous request and applying an orthogonal regularization loss (LOrth), the framework ensures that the parameter subspaces for different unlearning tasks are mutually orthogonal. This orthogonality means that updates for one unlearning request do not affect the effectiveness of previous unlearning requests.
- Core assumption: The assumption is that maintaining orthogonality in the LoRA parameter space is sufficient to prevent catastrophic forgetting and interference between different unlearning tasks.
- Evidence anchors:
  - [abstract]: "The orthogonal LoRA achieves parameter disentanglement among continual unlearning requests."
  - [section 3.1]: "To address such issues, inspired by Wang et al. (2023), we propose to adopt an orthogonal regularization loss to disentangle unlearning requests in the LoRA parameter space."
  - [corpus]: Weak evidence - related papers focus on continual unlearning but do not specifically mention orthogonal regularization in LoRA.
- Break condition: If the orthogonality constraint is too restrictive, it might hinder the model's ability to learn new unlearning tasks effectively, leading to poor performance.

### Mechanism 2
- Claim: The OOD detector uses a novel contrastive entropy loss and glocal-aware scoring to accurately assess the similarity between input data and unlearning data.
- Mechanism: The OOD detector is trained using a contrastive entropy loss (LCEL) that aligns positive pairs (augmented views of the same instance) while pushing negative pairs apart. This is supplemented with a glocal-aware scoring mechanism that combines Mahalanobis distance (global Gaussian assumption) and cosine similarity (local instance-wise comparison) to create a robust similarity score.
- Core assumption: The assumption is that the combination of contrastive entropy loss and glocal-aware scoring provides a more accurate and robust measure of OOD similarity than existing methods, especially in the absence of labeled data.
- Evidence anchors:
  - [abstract]: "The OOD detector is trained with a novel contrastive entropy loss and utilizes a glocal-aware scoring mechanism."
  - [section 3.2]: "Our contrastive entropy also starts with the augmentation view generation... Our contrastive entropy loss shares similar intuition with the standard contrastive learning, i.e., aligning positive pairs as close as possible while pushing negative pairs far away, but converges much faster owing to weighting by cosine similarity-based Softmax probability and being conducted at every model layer."
  - [corpus]: Weak evidence - related papers focus on OOD detection but do not specifically mention contrastive entropy loss or glocal-aware scoring.
- Break condition: If the assumptions about the Gaussian distribution of ID data are violated, or if the contrastive entropy loss does not converge effectively, the OOD detection accuracy may suffer.

### Mechanism 3
- Claim: Soft-weighted inference dynamically decides whether and to what extent to load the unlearning LoRA based on the OOD detector's predicted similarity, balancing unlearning effectiveness and utility preservation.
- Mechanism: During inference, the input is fed into all OOD detector backbones (one for each unlearning request). The OOD detector calculates a score vector for each request, which is then used to compute a distance to the boundary of a fitted hypersphere (using OCSVM). This distance is used to calculate a soft weight, which determines how much the unlearning LoRA should be loaded. If the input is similar to unlearning data, the LoRA is loaded more; otherwise, it is loaded less or not at all.
- Core assumption: The assumption is that the soft weighting mechanism, based on the OOD detector's similarity assessment, can effectively balance the need to unlearn specific knowledge while preserving general utility.
- Evidence anchors:
  - [abstract]: "The O3 framework can balance unlearning and utility because it smartly leverages the data similarity determined by the OOD detector to decide whether and to what extent to load the unlearning LoRA during inference."
  - [section 3.3]: "In the end, for each testing instance x, we have the following weight that reflects how much content of x belongs to the unlearning distribution of the t-th request... After getting the weights from all OOD detectors, we adopt the maximum weight w(x) = max {w(x)1, · · · , w(x)T } to load the unlearning LoRA by modifying Eq. 3 into h′ = W h+w(x)·ABh."
  - [corpus]: Weak evidence - related papers focus on inference strategies but do not specifically mention soft-weighted inference based on OOD detection.
- Break condition: If the OOD detector's similarity assessment is inaccurate, the soft weighting mechanism may load the unlearning LoRA inappropriately, leading to either insufficient unlearning or excessive utility loss.

## Foundational Learning

- Concept: LoRA (Low-Rank Adaptation)
  - Why needed here: LoRA is used to efficiently adapt large language models for unlearning without the computational cost of full fine-tuning. It reduces the number of trainable parameters by decomposing weight updates into low-rank matrices.
  - Quick check question: How does LoRA reduce the number of trainable parameters compared to full fine-tuning?

- Concept: Out-of-Distribution (OOD) Detection
  - Why needed here: OOD detection is crucial for identifying whether an input is similar to the data that needs to be unlearned. This allows the framework to dynamically decide whether to apply unlearning.
  - Quick check question: What are the key differences between OOD detection for classification tasks and OOD detection for unlearning in LLMs?

- Concept: Orthogonal Regularization
  - Why needed here: Orthogonal regularization is used to ensure that the parameter updates for different unlearning tasks do not interfere with each other, enabling effective continual unlearning.
  - Quick check question: How does orthogonal regularization prevent interference between different unlearning tasks in the LoRA parameter space?

## Architecture Onboarding

- Component map: Target LLM (frozen) -> Orthogonal LoRA adapter (trainable) -> OOD detector backbone (Roberta-large) -> OOD detector LoRA (trainable) -> One-class SVM (OCSVM)
- Critical path:
  1. Unlearning request arrives with unlearning data.
  2. Orthogonal LoRA is fine-tuned on the unlearning data with orthogonal regularization.
  3. OOD detector backbone is fine-tuned on the unlearning data using contrastive entropy loss.
  4. OCSVM is fitted using the glocal-aware scoring mechanism.
  5. During inference, input is passed through all OOD detectors to get similarity scores.
  6. Soft weights are calculated based on similarity scores.
  7. Unlearning LoRA is loaded with soft weights to generate the final output.
- Design tradeoffs:
  - Using LoRA reduces computational cost but may limit the model's ability to unlearn complex knowledge.
  - Relying on OOD detection without retained data improves practicality but may reduce accuracy.
  - Soft-weighted inference provides flexibility but adds complexity to the inference process.
- Failure signatures:
  - Poor unlearning effectiveness: OOD detector fails to accurately identify unlearning data, or orthogonal LoRA does not effectively disentangle parameter space.
  - Catastrophic utility loss: Soft-weighted inference loads unlearning LoRA inappropriately, or orthogonal regularization is too restrictive.
  - High computational overhead: Parallel processing of OOD detectors is not optimized, or LoRA rank is too high.
- First 3 experiments:
  1. Evaluate unlearning effectiveness on a single unlearning request using a simple dataset (e.g., TOFU with forget01).
  2. Test the impact of orthogonal regularization on preventing interference between two unlearning requests.
  3. Assess the accuracy of the OOD detector in distinguishing between unlearning data and retained data.

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How can we improve unlearning knowledge detection by replacing the Roberta backbone with the target LLM itself?
- Basis in paper: [explicit] The paper suggests using the target LLM instead of Roberta for OOD detection to better capture subtle text differences and contextual information, but notes that the scoring mechanism may need modification for decoder-only LLMs.
- Why unresolved: The paper identifies the potential benefits but does not explore the technical challenges of adapting the layer-wise token averaging scoring mechanism to the different architecture of LLMs.
- What evidence would resolve it: Experiments comparing OOD detection performance and inference efficiency using Roberta vs. LLM backbones, along with modifications to the scoring mechanism to handle decoder-only architectures.

### Open Question 2
- Question: What data selection strategies can be used to identify tasks and data distributions most susceptible to utility degradation during LLM unlearning?
- Basis in paper: [explicit] The paper discusses the challenge of identifying tasks and data distributions affected by unlearning, suggesting interpretable ML techniques for neuron localization but noting current limitations in granularity and consistency.
- Why unresolved: While the paper highlights the importance of data selection for utility preservation, it does not provide specific methods or algorithms for identifying susceptible tasks and distributions.
- What evidence would resolve it: Development and evaluation of data selection algorithms that can accurately identify tasks and distributions most impacted by unlearning, potentially using techniques like neuron activation analysis or gradient-based methods.

### Open Question 3
- Question: How can we extend the O3 framework to handle multimodal unlearning effectively?
- Basis in paper: [explicit] The paper mentions extending O3 to multimodal content as a promising direction for future research, but does not provide concrete methods for achieving this.
- Why unresolved: The paper acknowledges the potential benefits of multimodal unlearning but does not explore the technical challenges or specific approaches for adapting the framework to handle multiple modalities.
- What evidence would resolve it: Experiments demonstrating the effectiveness of the O3 framework on multimodal unlearning tasks, along with modifications to the OOD detection and orthogonal LoRA components to handle different data types.

## Limitations

- The orthogonal regularization mechanism assumes parameter space disentanglement alone is sufficient to prevent interference, lacking ablation studies on alternative approaches or regularization strengths
- Claims about contrastive entropy loss converging faster than standard contrastive learning are asserted without empirical runtime comparisons
- The soft-weighted inference mechanism assumes maximum weight aggregation is optimal without exploring alternative strategies
- The framework claims to work without retained data but doesn't thoroughly examine scenarios where limited retained data might improve performance

## Confidence

- **High confidence** in core claims about O3 outperforming state-of-the-art methods on reported metrics (S.U., D.U., R.D., U2R) across experimental tasks
- **Medium confidence** in mechanism claims about orthogonal LoRA preventing interference, as theoretical justification and experimental results support this but lack detailed ablation studies
- **Low confidence** in claims about contrastive entropy loss converging faster than standard contrastive learning, as this is asserted without empirical runtime comparisons

## Next Checks

1. **Ablation study on orthogonal regularization**: Remove the orthogonal regularization loss from the LoRA training and measure interference between unlearning requests across multiple sequential unlearning tasks. Compare parameter space overlap metrics and unlearning effectiveness degradation rates between O3 with and without orthogonal regularization.

2. **Runtime and convergence analysis**: Measure wall-clock training time and convergence speed for the OOD detector using contrastive entropy loss versus standard contrastive learning (MoCo, SimCLR) on the same unlearning datasets. Include learning curve plots showing accuracy vs. training steps for both approaches.

3. **Soft-weight aggregation alternatives**: Implement and compare alternative weight aggregation strategies (weighted sum, attention-based fusion, threshold-based binary selection) against the maximum weight approach. Evaluate impact on the Unlearning-Utility Ratio across different unlearning request sequences and input distributions.