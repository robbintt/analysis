---
ver: rpa2
title: Consolidating and Developing Benchmarking Datasets for the Nepali Natural Language
  Understanding Tasks
arxiv_id: '2411.19244'
source_url: https://arxiv.org/abs/2411.19244
tags:
- nepali
- dataset
- language
- tasks
- benchmark
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'The authors address the lack of comprehensive evaluation benchmarks
  for Nepali NLP by introducing the NLUE benchmark, which expands beyond the existing
  Nep-gLUE by adding twelve new datasets across four task categories: single-sentence
  classification, similarity and paraphrase detection, natural language inference,
  and a general masked evaluation task. The datasets were developed through a combination
  of automated translation using LLMs (GPT-4o-mini, Gemini-2.5-flash) and manual correction,
  with careful integration of existing Nepali data where available.'
---

# Consolidating and Developing Benchmarking Datasets for the Nepali Natural Language Understanding Tasks

## Quick Facts
- arXiv ID: 2411.19244
- Source URL: https://arxiv.org/abs/2411.19244
- Reference count: 20
- Primary result: Multilingual models generally outperform monolingual Nepali models on NLUE benchmark, with notable struggles on RTE, CR, and zero-shot GMET tasks

## Executive Summary
This paper addresses the critical gap in Nepali natural language understanding (NLU) benchmarking by introducing the NLUE benchmark, expanding beyond the existing Nep-gLUE with twelve new datasets across four task categories. The authors developed these datasets through a combination of automated translation using large language models and manual correction, while integrating existing Nepali data where available. The benchmark enables comprehensive evaluation of both multilingual and monolingual Nepali NLP models, revealing that multilingual models generally outperform monolingual ones on semantic understanding tasks, while monolingual models like RoBERTa-Nepali remain competitive on certain tasks. The study identifies key challenges including poor performance on small datasets and tokenization limitations affecting zero-shot tasks.

## Method Summary
The authors created the NLUE benchmark by translating existing English NLU datasets into Nepali using GPT-4o-mini and Gemini-2.5-flash with manual correction, supplemented by existing Nepali datasets. They evaluated ten models (67M-276M parameters) across twelve tasks using 5-fold cross-validation with hyperparameter tuning (learning rates 1e-5 to 2e-4, batch sizes 8-32, up to 15 epochs). The evaluation covered single-sentence classification, similarity and paraphrase detection, natural language inference, and masked language modeling. Models were tested on various fine-tuning configurations ranging from full fine-tuning to layer-specific updates.

## Key Results
- Multilingual models (m-deBERTa-v3, XLM-R) consistently outperform smaller monolingual Nepali models across most tasks
- All models struggle significantly with RTE and CR tasks due to small dataset sizes (1,000-2,000 examples)
- Zero-shot performance on GMET is suboptimal across all models, likely due to tokenization limitations with Devanagari script
- RoBERTa-Nepali remains competitive with multilingual models on certain tasks despite being monolingual

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Large multilingual models outperform smaller monolingual models on most Nepali NLU tasks due to their broader linguistic representation capacity.
- Mechanism: Multilingual pretraining exposes models to diverse linguistic structures, enabling better generalization across morphologically rich languages like Nepali, even without task-specific fine-tuning.
- Core assumption: The multilingual training data includes sufficient cross-linguistic patterns to benefit Nepali understanding.
- Evidence anchors:
  - [abstract] "We also find that the best multilingual model outperforms the best monolingual models across most tasks"
  - [section] "larger models with multilingual pretraining, such as m-debertav3 (276M parameters) and XLM-r-base (270M parameters), consistently outperform smaller Nepali-specific models"
- Break condition: If multilingual models lack sufficient Nepali-specific vocabulary or if task requires deep cultural/linguistic knowledge not captured in pretraining data.

### Mechanism 2
- Claim: Model performance degrades on tasks with smaller dataset sizes due to overfitting and insufficient generalization.
- Mechanism: When training data is limited, models cannot learn robust patterns and instead memorize training examples, leading to poor performance on unseen data.
- Core assumption: The dataset size is insufficient for the model complexity being used.
- Evidence anchors:
  - [section] "Tasks like RTE and CR remain challenging, due to smaller dataset sizes"
  - [section] "all models struggle with generalization from small datasets" for the coreference resolution task
- Break condition: If dataset size is increased or regularization techniques are applied effectively.

### Mechanism 3
- Claim: Zero-shot evaluation tasks are particularly challenging due to tokenization limitations and lack of task-specific fine-tuning.
- Mechanism: Without fine-tuning, models must rely entirely on their pretraining representations, and tokenization mismatches between Devanagari script and model vocabularies create performance barriers.
- Core assumption: Tokenization quality directly impacts model understanding in zero-shot settings.
- Evidence anchors:
  - [section] "all models demonstrated suboptimal performance on the GMET dataset"
  - [section] "multilingual models and those with larger parameter counts failed to achieve superior performance compared to their monolingual counterparts"
  - [section] "This performance gap may be attributed to tokenization limitations, as multilingual models typically contain fewer Devanagari tokens in their vocabularies"
- Break condition: If models are fine-tuned on the specific task or if tokenization is improved for Nepali script.

## Foundational Learning

- Concept: Fine-tuning strategies and hyperparameter optimization
  - Why needed here: Different tasks require different fine-tuning approaches based on dataset size and complexity
  - Quick check question: What fine-tuning configuration would you use for a dataset with 1,000 examples versus 80,000 examples?

- Concept: Cross-lingual transfer learning principles
  - Why needed here: Understanding why multilingual models work well on Nepali despite not being specifically trained on it
  - Quick check question: How does pretraining on multiple languages help a model perform better on a low-resource language like Nepali?

- Concept: Tokenization and its impact on model performance
  - Why needed here: Critical for understanding why zero-shot tasks fail and why multilingual models underperform in certain scenarios
  - Quick check question: What happens when a language's script is poorly represented in a model's vocabulary?

## Architecture Onboarding

- Component map: Dataset creation → Model selection → Fine-tuning → Evaluation → Analysis
- Critical path: Dataset creation/translation → Model fine-tuning → Performance evaluation → Analysis of results
- Design tradeoffs: Monolingual vs multilingual models (specialization vs generalization), dataset size vs model complexity, manual vs automated translation quality
- Failure signatures: Poor performance on small datasets (overfitting), zero-shot task failures (tokenization issues), multilingual models underperforming (insufficient cross-lingual patterns)
- First 3 experiments:
  1. Test different fine-tuning configurations on a small dataset to identify overfitting patterns
  2. Compare monolingual and multilingual model performance on a simple task to establish baseline differences
  3. Evaluate zero-shot performance on a masked language task to identify tokenization limitations

## Open Questions the Paper Calls Out

### Open Question 1
- Question: How does the performance of Nepali NLP models on NLUE compare to models trained on other low-resource languages with similar linguistic complexity?
- Basis in paper: [inferred] The paper discusses the challenges of Nepali NLP due to its complex script, morphology, and dialects, but does not compare performance to other low-resource languages.
- Why unresolved: The paper focuses exclusively on Nepali and does not provide a comparative analysis with other low-resource languages, making it difficult to contextualize Nepali's difficulty level.
- What evidence would resolve it: A cross-linguistic study evaluating NLUE-style benchmarks on multiple low-resource languages with varying linguistic complexities.

### Open Question 2
- Question: What specific aspects of Nepali morphology and script complexity most significantly impact model performance, and how can these be addressed through architectural improvements?
- Basis in paper: [explicit] The paper mentions Nepali's complex script (Devanagari), morphology, and dialects as challenges, and notes that multilingual models struggle with tokenization due to fewer Devanagari tokens.
- Why unresolved: While the paper identifies these challenges, it does not conduct detailed error analysis to pinpoint which morphological features or script-related issues most impact performance.
- What evidence would resolve it: Detailed ablation studies analyzing model errors on specific morphological phenomena (e.g., case marking, verb inflections) and script tokenization issues.

### Open Question 3
- Question: Would larger-scale pretraining on more diverse Nepali data significantly improve performance on the most challenging tasks (RTE, CR, GMET), or are these tasks inherently difficult regardless of model size?
- Basis in paper: [inferred] The paper shows that larger models perform better overall but all models struggle with RTE, CR, and GMET, suggesting these may be inherently difficult.
- Why unresolved: The paper does not experiment with larger models or additional pretraining data, leaving open whether scaling would overcome these specific challenges.
- What evidence would resolve it: Experiments with models significantly larger than 276M parameters, trained on substantially larger and more diverse Nepali corpora, evaluated on the same tasks.

## Limitations
- Dataset quality may vary due to automated translation approach, potentially introducing systematic biases
- Benchmark performance may not fully generalize to real-world Nepali NLP applications beyond controlled evaluation scenarios
- Tokenization impact is identified but not precisely quantified against other performance factors

## Confidence
- High Confidence: Claims about overall benchmarking results, dataset creation methodology, and comparative performance trends between model types
- Medium Confidence: Claims about specific mechanisms (tokenization limitations, dataset size effects) are supported by observations but lack comprehensive quantitative validation
- Low Confidence: Claims about real-world applicability and generalization beyond benchmark tasks require additional empirical validation

## Next Checks
1. Conduct ablation studies systematically varying tokenization approaches to isolate tokenization impact from other performance factors
2. Perform statistical significance testing on model performance comparisons to determine which differences are meaningful
3. Validate benchmark performance by testing best models on domain-specific Nepali NLP tasks (news summarization, customer service) to assess real-world utility