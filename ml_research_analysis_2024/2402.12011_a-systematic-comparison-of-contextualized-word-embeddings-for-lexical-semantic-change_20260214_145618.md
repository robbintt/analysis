---
ver: rpa2
title: A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic
  Change
arxiv_id: '2402.12011'
source_url: https://arxiv.org/abs/2402.12011
tags:
- word
- change
- semantic
- computational
- approaches
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This study systematically evaluates contextualized word embeddings
  for lexical semantic change (LSC) across eight languages. It compares form-based
  (APD, PRT) and sense-based (AP+JSD, WiDiD) approaches on the Graded Change Detection
  task under equal settings, revealing XL-LEXEME's superior performance over BERT,
  mBERT, and XLM-R.
---

# A Systematic Comparison of Contextualized Word Embeddings for Lexical Semantic Change

## Quick Facts
- arXiv ID: 2402.12011
- Source URL: https://arxiv.org/abs/2402.12011
- Authors: Francesco Periti; Nina Tahmasebi
- Reference count: 40
- Primary result: XL-LEXEME outperforms BERT, mBERT, and XLM-R for LSC detection across eight languages

## Executive Summary
This study provides a comprehensive evaluation of contextualized word embeddings for lexical semantic change (LSC) detection across eight languages. The research systematically compares form-based approaches (APD, PRT) with sense-based methods (AP+JSD, WiDiD) using XL-LEXEME, BERT, mBERT, and XLM-R models. XL-LEXEME emerges as the top performer, particularly when combined with the APD approach. The evaluation extends beyond LSC detection to include Word-in-Context and Word Sense Induction tasks, positioning models as computational annotators against human judgments. The findings reveal that XL-LEXEME and GPT-4 achieve near-human-level performance, with XL-LEXEME offering superior accessibility and cost-effectiveness.

## Method Summary
The study employs a rigorous comparative framework across multiple languages and tasks. For LSC detection, the research uses the Graded Change Detection task with equal settings for all approaches and models. Form-based methods (APD, PRT) and sense-based methods (AP+JSD, WiDiD) are evaluated using XL-LEXEME, BERT, mBERT, and XLM-R. The evaluation extends to Word-in-Context (WiC) and Word Sense Induction (WSI) tasks, where models serve as computational annotators compared against human judgments. Synthetic evaluation data is generated using GPT-4, with human verification for quality control. The methodology emphasizes controlled comparisons across approaches, models, and languages to identify optimal combinations for LSC detection.

## Key Results
- XL-LEXEME with APD approach achieves superior performance for LSC detection across all eight languages tested
- GPT-4 and XL-LEXEME reach near-human-level performance on Word-in-Context and Word Sense Induction tasks
- APD approach proves most effective for LSC detection, outperforming sense-based methods (AP+JSD, WiDiD)
- The study reveals that focusing on how, when, and why word meanings change is more valuable than measuring change extent alone

## Why This Works (Mechanism)
The superior performance of XL-LEXEME stems from its specialized training on lexical semantics and semantic change patterns. Unlike general-purpose models like BERT or XLM-R, XL-LEXEME incorporates explicit knowledge about how word meanings evolve over time through its pretraining objectives. The APD approach works effectively because it captures distributional differences between temporal contexts while maintaining semantic coherence. The combination of XL-LEXEME's semantic awareness with APD's distributional sensitivity creates a powerful synergy for detecting nuanced semantic shifts. GPT-4's strong performance reflects its broad linguistic knowledge and reasoning capabilities, though at significantly higher computational cost compared to XL-LEXEME.

## Foundational Learning
- **Lexical Semantic Change (LSC)**: The study of how word meanings evolve over time, essential for understanding language evolution and cultural shifts. Quick check: Can you identify examples of words whose meanings have changed significantly in your lifetime?
- **Contextualized Embeddings**: Word representations that vary based on surrounding context, crucial for capturing polysemy and semantic nuances. Quick check: How does "bank" differ when used in "river bank" versus "investment bank"?
- **Form-based vs Sense-based Approaches**: Form-based methods analyze surface-level distributional changes, while sense-based methods attempt to track specific meaning senses. Quick check: Which approach would better detect the shift in "tweet" from bird sounds to social media posts?
- **Cross-linguistic Evaluation**: Testing models across multiple languages reveals universal patterns and language-specific challenges in semantic change detection. Quick check: Can semantic change patterns observed in English be generalized to other language families?
- **Computational Annotation**: Using models as annotators to scale up evaluation and comparison against human judgments. Quick check: What are the trade-offs between human annotation accuracy and model scalability?

## Architecture Onboarding

**Component Map**: Data Preprocessing -> Model Selection -> Feature Extraction -> Change Detection -> Evaluation

**Critical Path**: The evaluation pipeline flows from data preparation through model application to final assessment. The critical path involves selecting appropriate embeddings, extracting temporal features using APD or sense-based methods, detecting change patterns, and validating against human judgments or synthetic data.

**Design Tradeoffs**: The study balances model complexity against interpretability, choosing between form-based (simpler, more interpretable) and sense-based (more nuanced, computationally intensive) approaches. XL-LEXEME offers better performance but requires specialized knowledge, while GPT-4 provides strong results but at higher cost. The synthetic data generation enables large-scale evaluation but may miss subtle human judgments.

**Failure Signatures**: Models may fail when semantic change involves cultural or domain-specific knowledge not captured in pretraining data. Performance degradation occurs with limited temporal data or when changes are gradual rather than abrupt. Cross-linguistic performance may vary significantly based on language family and available training data.

**3 First Experiments**:
1. Test XL-LEXEME with APD on a single language pair (e.g., English historical texts) to establish baseline performance
2. Compare form-based versus sense-based approaches on a controlled dataset with known semantic shifts
3. Evaluate model sensitivity to temporal granularity by testing different time window sizes on the same corpus

## Open Questions the Paper Calls Out
The paper highlights several open questions regarding the practical application of LSC detection models. Key questions include how to best integrate LSC detection into downstream NLP tasks, what the optimal temporal granularity is for different types of semantic change, and how to handle cases where multiple semantic changes occur simultaneously. The research also raises questions about the scalability of sense-based approaches versus form-based methods, and how to effectively combine computational detection with human expertise for real-world applications.

## Limitations
- The study relies heavily on synthetic evaluation data generated by GPT-4 rather than exclusively human-annotated gold standards
- Cross-linguistic evaluation covers eight languages but may not capture all language-specific semantic change patterns
- The focus on the Graded Change Detection task may limit generalizability to other LSC detection scenarios
- Computational cost comparisons may become outdated as language model technologies rapidly evolve

## Confidence
- XL-LEXEME performance claims: Medium
- Cross-linguistic generalizability: Medium
- APD approach effectiveness: High
- Computational cost comparisons: Medium

## Next Checks
1. Replicate findings using exclusively human-annotated gold standard datasets across all evaluation tasks to verify the GPT-4-generated synthetic data results
2. Conduct extended temporal validation by testing models on historical corpora spanning longer time periods (e.g., centuries rather than decades)
3. Implement ablation studies to isolate the specific architectural features of XL-LEXEME that contribute to its superior performance, separating model architecture effects from training data differences