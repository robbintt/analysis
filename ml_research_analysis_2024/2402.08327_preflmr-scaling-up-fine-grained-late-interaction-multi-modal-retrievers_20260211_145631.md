---
ver: rpa2
title: 'PreFLMR: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers'
arxiv_id: '2402.08327'
source_url: https://arxiv.org/abs/2402.08327
tags:
- image
- retrieval
- training
- question
- preflmr
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: This paper studies scaling laws for multi-modal retrievers in knowledge-based
  visual question answering. The authors create a multi-task benchmark M2KR from nine
  diverse datasets, and develop PreFLMR by pre-training a fine-grained late-interaction
  multi-modal retriever.
---

# PreFLMR: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers

## Quick Facts
- arXiv ID: 2402.08327
- Source URL: https://arxiv.org/abs/2402.08327
- Reference count: 40
- Multi-modal retrievers achieve state-of-the-art results on KB-VQA tasks through scaling analysis

## Executive Summary
This paper presents a comprehensive study of scaling laws for multi-modal retrievers in knowledge-based visual question answering. The authors introduce PreFLMR, a fine-grained late-interaction multi-modal retriever, and create M2KR, a multi-task benchmark from nine diverse datasets. Through systematic scaling analysis of vision encoders, text encoders, and intermediate pre-training, PreFLMR achieves state-of-the-art performance on KB-VQA tasks. The work provides valuable insights into how different components of multi-modal retrievers scale and contribute to overall performance.

## Method Summary
The authors develop PreFLMR by pre-training a fine-grained late-interaction multi-modal retriever architecture. The model is trained on a newly created multi-task benchmark M2KR, which aggregates nine diverse knowledge-based visual question answering datasets. The pre-training approach leverages late-interaction mechanisms that allow fine-grained matching between vision and text representations. The authors systematically study scaling behaviors across three dimensions: vision encoder size, text encoder size, and intermediate pre-training data. Through extensive experiments, they demonstrate how each scaling factor contributes to performance improvements and establish scaling laws that guide optimal resource allocation.

## Key Results
- PreFLMR achieves state-of-the-art results on the M2KR multi-task benchmark
- Systematic scaling analysis reveals optimal growth patterns for vision and text encoders
- Fine-grained late-interaction mechanism shows significant performance gains over traditional approaches
- PreFLMR demonstrates strong transfer capabilities on downstream KB-VQA tasks

## Why This Works (Mechanism)
The fine-grained late-interaction mechanism enables more precise matching between visual and textual representations by maintaining higher-dimensional interaction spaces throughout the retrieval process. This approach allows the model to capture subtle semantic relationships that coarse-grained interactions might miss. The late-interaction design defers the fusion of modalities until after individual encoding, preserving modality-specific information while enabling flexible matching patterns. The scaling analysis reveals that vision and text encoders contribute differently to performance, with vision encoders showing stronger scaling returns in the KB-VQA domain due to the visual complexity of knowledge-based queries.

## Foundational Learning

1. **Multi-modal Retrieval Basics** (why needed: understanding the core task of finding relevant information across modalities)
   - Quick check: Can retrieve relevant images given textual queries and vice versa

2. **Late-Interaction Mechanisms** (why needed: core architectural innovation enabling fine-grained matching)
   - Quick check: Understand how modality fusion is delayed until after encoding

3. **Scaling Laws in Neural Networks** (why needed: theoretical framework for understanding performance improvements)
   - Quick check: Know how model size and data scale affect performance

4. **Knowledge-Based Visual Question Answering** (why needed: target application domain)
   - Quick check: Understand the distinction between knowledge-based and non-knowledge-based VQA

5. **Multi-Task Learning** (why needed: framework for creating robust pre-training benchmarks)
   - Quick check: Can explain benefits and challenges of training on multiple tasks simultaneously

## Architecture Onboarding

**Component Map:** Vision Encoder -> Text Encoder -> Late-Interaction Layer -> Retrieval Head

**Critical Path:** The critical path flows from individual modality encoders through the fine-grained late-interaction layer to the final similarity scoring. The late-interaction layer is the most critical component as it determines the quality of cross-modal matching.

**Design Tradeoffs:** The architecture balances between early fusion (which may lose modality-specific information) and late fusion (which may miss early interaction opportunities). The fine-grained approach increases computational cost but enables more precise matching. The choice of encoder sizes involves a tradeoff between performance gains and computational efficiency.

**Failure Signatures:** Performance degradation typically manifests as either modality-specific weaknesses (poor vision or text encoding) or interaction weaknesses (the late-interaction layer failing to capture relevant semantic relationships). Computational bottlenecks occur primarily in the late-interaction layer due to the high dimensionality of fine-grained matching.

**First Experiments:**
1. Validate baseline late-interaction performance without fine-grained matching
2. Test scaling of vision encoder size while keeping text encoder fixed
3. Evaluate transfer performance on a single KB-VQA dataset before full M2KR training

## Open Questions the Paper Calls Out
None identified in the provided materials.

## Limitations
- Scaling laws discovered may not generalize beyond the KB-VQA domain or M2KR benchmark
- Substantial computational costs associated with scaling vision and text encoders may limit practical deployment
- Difficulty isolating the specific contribution of fine-grained late-interaction from other architectural improvements
- Focus on knowledge-based VQA may not translate to other multi-modal retrieval scenarios

## Confidence
- **High confidence** in the empirical methodology and experimental design
- **Medium confidence** in the scalability conclusions, pending validation on more diverse datasets
- **Medium confidence** in the architectural contributions, as the specific gains from fine-grained late-interaction versus other design choices are not fully disentangled

## Next Checks
1. Test PreFLMR on additional multi-modal retrieval tasks outside the KB-VQA domain to verify the generalizability of the scaling laws
2. Conduct ablation studies specifically isolating the contribution of the fine-grained late-interaction mechanism from other architectural components
3. Evaluate performance with more diverse and potentially more challenging datasets to assess robustness of the scaling relationships