---
ver: rpa2
title: 'SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical VQA
  Tasks'
arxiv_id: '2411.19688'
source_url: https://arxiv.org/abs/2411.19688
tags:
- robustness
- question
- shifts
- dataset
- shift
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This work introduces a novel framework for systematically evaluating
  the robustness of vision-language models (VLMs) in medical Visual Question Answering
  (VQA) tasks. The framework addresses key limitations in current robustness evaluation
  by focusing on three requirements: (1) evaluating on realistic shifts inherent to
  medical VQA data, (2) using large language models (LLMs) for semantic evaluation,
  and (3) including relevant sanity baselines to contextualize multimodal performance.'
---

# SURE-VQA: Systematic Understanding of Robustness Evaluation in Medical VQA Tasks

## Quick Facts
- arXiv ID: 2411.19688
- Source URL: https://arxiv.org/abs/2411.19688
- Reference count: 40
- Key outcome: This work introduces a novel framework for systematically evaluating the robustness of vision-language models (VLMs) in medical Visual Question Answering (VQA) tasks, addressing key limitations in current robustness evaluation.

## Executive Summary
This paper presents SURE-VQA, a comprehensive framework for evaluating the robustness of vision-language models in medical VQA tasks. The framework addresses three key requirements: evaluating on realistic shifts inherent to medical VQA data, using LLM-based semantic evaluation instead of traditional token-matching metrics, and including relevant sanity baselines to contextualize multimodal performance. Through extensive experiments across three medical VQA datasets with four types of distribution shifts, the study reveals that no single fine-tuning method consistently outperforms others in robustness, and that robustness trends are more stable across fine-tuning methods than across distribution shifts. The findings highlight the need for more elaborate datasets to minimize shortcut learning and provide valuable insights for practitioners selecting fine-tuning methods for medical VQA applications.

## Method Summary
The SURE-VQA framework evaluates VLM robustness in medical VQA tasks by comparing various fine-tuning methods across realistic distribution shifts. The approach uses LLaVA-Med 1.5 as the base model, fine-tuned using four methods: full fine-tuning, LoRA, prompt tuning, and (IA)3. Three medical VQA datasets (SLAKE, OVQA, MIMIC-CXR-VQA) are split into i.i.d. and OoD test sets, with four types of realistic shifts defined for each dataset. The framework employs LLM-based semantic evaluation using Mistral instead of traditional token-matching metrics, and includes sanity baselines that don't use image data. Robustness is measured using relative robustness (RR) as 1 - ΔP/P_I, where ΔP = (P_I - P_O).

## Key Results
- No single fine-tuning method consistently outperforms others in robustness across all distribution shifts
- Robustness trends are more stable across fine-tuning methods than across distribution shifts
- Simple sanity baselines that ignore image data can perform surprisingly well, indicating dataset shortcuts
- LLM-based semantic evaluation provides more accurate assessment than traditional token-matching metrics
- Artificial shifts fail to accurately capture the challenges presented by realistic shifts in medical VQA tasks

## Why This Works (Mechanism)

### Mechanism 1
- Claim: Traditional token-matching metrics fail to capture semantic meaning in medical VQA tasks, leading to inflated performance estimates.
- Mechanism: Token-matching metrics (BLEU, Exact Match, F1) evaluate answers based on n-gram overlap rather than semantic content. In medical VQA, answers often differ by single tokens that flip meaning (e.g., "nodule" vs "no nodule"), but token metrics treat these as similar since they share most words.
- Core assumption: Semantic meaning in medical VQA answers can be accurately captured by LLM-based evaluation that understands context and negation.
- Evidence anchors:
  - [abstract] "Traditional token-matching metrics often fail to capture underlying semantics, necessitating the use of large language models (LLMs) for more accurate semantic evaluation"
  - [section] "The primary limitation of traditional metrics is their inability to capture the underlying semantics of a sentence. They fail to recognize synonyms or account for negation, often misjudging sentences that differ from the ground truth by a single token, such as 'not.'"
  - [corpus] Weak evidence - only mentions "robustness" and "fine-tuning" without specific semantic evaluation details
- Break condition: If LLM evaluator is trained on non-medical data or fails to understand medical terminology, it may not accurately capture semantic meaning in medical VQA contexts.

### Mechanism 2
- Claim: Simple sanity baselines that ignore image data can perform surprisingly well, revealing dataset shortcuts.
- Mechanism: VQA datasets often contain language priors where question-answer pairs have strong correlations independent of image content. Models can exploit these correlations to achieve high performance without using visual information.
- Core assumption: Medical VQA datasets contain hidden patterns allowing models to answer questions based on text alone, and reporting sanity baselines reveals this shortcut learning.
- Evidence anchors:
  - [abstract] "simple sanity baselines that do not use the image data can perform surprisingly well, highlighting the need for more elaborate datasets to minimize shortcut learning"
  - [section] "The problem with many VQA datasets is that they tend to contain hidden patterns, allowing models to exploit shortcuts (Geirhos et al. (2020)) rather than using all available information, including the image content"
  - [corpus] Moderate evidence - related papers discuss robustness and fine-tuning but don't specifically address image-free baselines
- Break condition: If medical VQA datasets are carefully constructed to eliminate language priors and require visual information for every question, sanity baselines would perform at chance levels.

### Mechanism 3
- Claim: Realistic distribution shifts inherent to medical data are more challenging than artificial image corruptions.
- Mechanism: Artificial corruptions (blur, noise, brightness) are controlled and predictable, while realistic shifts (modality changes, demographic differences, question type variations) represent complex, multi-factor changes that better reflect real-world deployment scenarios.
- Core assumption: Robustness to synthetic shifts does not translate to robustness on real-world shifts, as demonstrated by controlled ablation studies.
- Evidence anchors:
  - [abstract] "Since robustness on synthetic shifts does not necessarily translate to real-world shifts, it should be measured on real-world shifts that are inherent to the VQA data"
  - [section] "In an ablation, we compared the model's performance on image corruptions (e.g., blur, noise, brightness) with the realistic shifts we defined in the SLAKE dataset. The results demonstrate that artificial shifts fail to accurately capture the challenges presented by realistic shifts"
  - [corpus] Weak evidence - corpus papers focus on fine-tuning and robustness but don't specifically compare synthetic vs realistic shifts
- Break condition: If artificial corruptions are designed to systematically target the same features that realistic shifts affect, they might provide equivalent robustness evaluation.

## Foundational Learning

- Concept: Domain shift in medical imaging
  - Why needed here: Medical VQA models must handle variations in acquisition modalities, patient demographics, and imaging protocols that differ from training data.
  - Quick check question: What types of distribution shifts are most relevant for medical VQA deployment in clinical settings?

- Concept: Parameter-efficient fine-tuning (PEFT) methods
  - Why needed here: Full fine-tuning of large VLMs is computationally expensive and may overfit on small medical datasets, while PEFT methods like LoRA offer efficient adaptation.
  - Quick check question: How do different PEFT methods (LoRA, prompt tuning, IA3) compare in terms of parameter efficiency and performance on medical VQA tasks?

- Concept: Multimodal learning evaluation metrics
  - Why needed here: Standard NLP metrics fail for VQA because they don't account for visual information; robust evaluation requires metrics that capture both semantic and multimodal aspects.
  - Quick check question: What are the key differences between traditional token-matching metrics and LLM-based semantic evaluation for VQA tasks?

## Architecture Onboarding

- Component map: Dataset preprocessing and shift definition -> Model fine-tuning (full FT or PEFT) -> Evaluation (traditional metrics + LLM evaluator + sanity baselines) -> Robustness analysis across shifts
- Critical path: Data preparation → Fine-tuning (full FT or PEFT) → Evaluation (traditional metrics + LLM evaluator + sanity baselines) → Robustness analysis across shifts
- Design tradeoffs: Using LLM evaluators provides semantic accuracy but increases computational cost and potential evaluation failures; sanity baselines reveal shortcuts but may be difficult to construct for complex medical questions
- Failure signatures: If LLM evaluator fails on medical terminology, traditional metrics will overestimate performance; if sanity baselines perform too well, the dataset contains exploitable shortcuts; if artificial shifts don't correlate with realistic shifts, robustness evaluation is misleading
- First 3 experiments:
  1. Compare traditional metrics (BLEU, F1) vs LLM evaluator on a small subset of medical VQA questions to demonstrate semantic evaluation differences
  2. Run full fine-tuning vs LoRA on SLAKE dataset with modality shift to observe computational efficiency and performance tradeoffs
  3. Evaluate no-image baseline on MIMIC dataset to reveal language priors in the dataset

## Open Questions the Paper Calls Out

### Open Question 1
- Question: What is the optimal fine-tuning method for medical VQA tasks that balances performance and robustness?
- Basis in paper: [explicit] The paper compares various fine-tuning methods including full fine-tuning, prompt tuning, LoRA, and (IA)3, finding no single method consistently outperforms others in robustness.
- Why unresolved: While LoRA performs best on in-distribution data, the study shows that robustness trends are more stable across fine-tuning methods than across distribution shifts, indicating that the choice of fine-tuning method is not the primary determinant of robustness.
- What evidence would resolve it: A comprehensive study comparing additional fine-tuning methods (e.g., prefix tuning, soft prompt tuning) across a wider range of medical VQA datasets and distribution shifts, with a focus on both in-distribution performance and robustness.

### Open Question 2
- Question: How can medical VQA datasets be designed to minimize shortcut learning and better evaluate the true multimodal capabilities of vision-language models?
- Basis in paper: [explicit] The paper highlights that simple baselines that do not use image data can perform surprisingly well, suggesting that current datasets may contain hidden patterns that allow models to exploit shortcuts rather than utilizing the full multimodal information.
- Why unresolved: While the paper identifies the need for more elaborate datasets, it does not provide specific guidelines or a framework for designing such datasets that minimize shortcut learning.
- What evidence would resolve it: The development and evaluation of new medical VQA datasets with increased linguistic variability, diverse question types, and semantic differences, coupled with rigorous testing to ensure that image information is essential for accurate answering.

### Open Question 3
- Question: How do population shifts (e.g., gender, ethnicity, age) impact the robustness of vision-language models in medical VQA tasks, and what strategies can mitigate these effects?
- Basis in paper: [explicit] The study investigates population shifts on the MIMIC dataset but finds that models generally exhibit robustness against such shifts. However, the performance on the MIMIC dataset is generally insufficient, raising questions about the true impact of these shifts.
- Why unresolved: The study's findings on population shifts are limited by the overall low performance on the MIMIC dataset, making it difficult to draw definitive conclusions about the models' robustness to these shifts.
- What evidence would resolve it: Further experiments on larger and more diverse medical VQA datasets, with a focus on population shifts, to determine whether the observed robustness is due to the nature of the shifts or simply a result of low in-distribution performance. Additionally, investigating methods to enhance robustness to population shifts, such as data augmentation or adversarial training.

## Limitations

- The framework relies on datasets that may still contain language priors, as evidenced by the strong performance of image-free baselines
- The use of LLM-based evaluation introduces computational overhead and potential evaluation failures, particularly with medical terminology
- The study's findings on population shifts are limited by the overall low performance on the MIMIC dataset

## Confidence

- High confidence: The observation that robustness trends are more stable across fine-tuning methods than across distribution shifts is well-supported by the experimental results
- Medium confidence: The claim that no single fine-tuning method consistently outperforms others is supported by the data, though limited to four specific methods
- Medium confidence: The recommendation for using LLM-based semantic evaluation over traditional metrics is supported, but depends on the specific LLM used
- Low confidence: The claim that artificial shifts fail to capture realistic shift challenges, while supported by ablation studies, is based on a limited comparison

## Next Checks

1. **Dataset shortcut validation**: Conduct a systematic analysis of the three medical VQA datasets to quantify the extent of language priors and develop more robust dataset construction guidelines that minimize shortcut learning opportunities.

2. **Evaluator generalization study**: Compare the performance and consistency of different LLM evaluators (Mistral, GPT-4, Claude) on the same medical VQA tasks to assess the stability and reliability of semantic evaluation across different evaluation models.

3. **Cross-dataset robustness analysis**: Test the fine-tuned models across different medical VQA datasets to determine whether robustness trends observed within datasets translate to cross-dataset generalization, which would better reflect real-world clinical deployment scenarios.