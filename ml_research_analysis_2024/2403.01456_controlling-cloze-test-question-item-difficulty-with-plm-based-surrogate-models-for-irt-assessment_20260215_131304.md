---
ver: rpa2
title: Controlling Cloze-test Question Item Difficulty with PLM-based Surrogate Models
  for IRT Assessment
arxiv_id: '2403.01456'
source_url: https://arxiv.org/abs/2403.01456
tags:
- control
- difficulty
- test
- distractors
- generation
- transformers
- self-attention
- retrieval-augmented-generation
- instruction-tuning
- parameter-efficient-finetuning
- mixture-of-experts
- chain-of-thought
core_contribution: 'This paper presents a novel evaluation framework for assessing
  control of item-level difficulty in multiple-choice cloze tests using pre-trained
  language models (PLMs) as surrogate test-takers. The key contributions are: Two
  strategies for difficulty control are proposed: manipulating the gap position and
  distractor selection, using entropy, semantic similarity, edit distance and validity
  rules to reduce invalid distractors.'
---

# Controlling Cloze-test Question Item Difficulty with PLM-based Surrogate Models for IRT Assessment

## Quick Facts
- arXiv ID: 2403.01456
- Source URL: https://arxiv.org/abs/2403.01456
- Reference count: 0
- Primary result: Novel PLM-based IRT framework for automated cloze test difficulty control

## Executive Summary
This paper introduces a framework for controlling cloze test item difficulty using pre-trained language models (PLMs) as surrogate test-takers. The approach fits Item Response Theory (IRT) models on PLM scores to quantify difficulty shifts, eliminating the need for human test subjects. Two difficulty control strategies are proposed: manipulating gap positions and selecting appropriate distractors. The framework is evaluated on the CLOTH dataset, demonstrating successful generation of both easier and harder test items through IRT parameter shifts.

## Method Summary
The framework employs PLMs to simulate test-taker behavior, generating scores that are used to fit IRT models and assess item difficulty. Two main strategies control difficulty: (1) gap position manipulation, which tests different locations in sentences for cloze items, and (2) distractor selection using entropy-based ranking, semantic similarity, and validity rules. The approach generates multiple item versions, compares their IRT parameters, and quantifies difficulty changes. GPT-2 and RoBERTa models serve as surrogate test-takers, with results validated against the CLOTH dataset's original items.

## Key Results
- Both gap position and distractor selection strategies successfully generated items with measurable IRT difficulty shifts compared to original CLOTH items
- Gap control showed limited effect on difficulty but increased variability for easy CLOTH-M generation
- 3-Factor Ranking Control method performed better for easy items, while Confidence Ranking Control excelled at hard item generation
- Entropy-based distractor selection with validity rules reduced but did not eliminate invalid distractors

## Why This Works (Mechanism)
The framework leverages PLM's ability to simulate human-like language understanding, providing a scalable alternative to human test subjects for IRT assessment. By fitting IRT models on PLM scores, the approach captures item characteristic curves and difficulty parameters that correlate with actual test-taker performance. The combination of entropy-based distractor selection and position manipulation creates systematic variation in item difficulty that the IRT framework can quantify and validate.

## Foundational Learning
- **Item Response Theory (IRT)**: A family of psychometric models that relate latent abilities to item responses; needed for quantifying item difficulty and discrimination parameters without human subjects
- **Cloze Tests**: Fill-in-the-blank assessment format where test-takers select correct answers from multiple choices; fundamental to the application domain
- **Entropy-based Ranking**: Statistical measure of uncertainty used to evaluate and select distractors; needed for automated distractor quality assessment
- **Surrogate Models**: AI systems that simulate human test-taker behavior; essential for eliminating the need for human subject studies
- **Validity Rules**: Constraints ensuring distractors are plausible but incorrect; critical for maintaining test quality
- **Semantic Similarity**: Measures how closely related options are to the correct answer; important for distractor selection quality

## Architecture Onboarding

**Component Map:**
PLM (GPT-2/RoBERTa) -> Score Generation -> IRT Model Fitting -> Difficulty Assessment -> Difficulty Control Strategy

**Critical Path:**
1. Generate cloze items with multiple distractor options
2. Run PLM simulations to obtain scores
3. Fit IRT models to PLM scores
4. Calculate difficulty parameters and compare to baseline
5. Apply difficulty control strategies and iterate

**Design Tradeoffs:**
- Using PLMs eliminates human subject requirements but requires validation against actual test-taker data
- Entropy-based distractor selection reduces invalid options but doesn't completely eliminate them
- Multiple PLM architectures provide robustness but increase computational costs

**Failure Signatures:**
- Poor IRT parameter shifts indicating ineffective difficulty control
- High proportion of invalid distractors suggesting inadequate validity rules
- Inconsistent difficulty levels across similar items indicating model instability

**First Experiments:**
1. Compare PLM-generated IRT parameters with human test-taker data to validate the surrogate approach
2. Test multiple distractor selection algorithms to optimize validity and difficulty control
3. Evaluate framework performance across different PLM architectures and languages

## Open Questions the Paper Calls Out
None

## Limitations
- PLM-based surrogate approach lacks validation against actual human test-taker performance
- Framework effectiveness may not generalize beyond English language and specific PLM architectures used
- Entropy-based distractor selection reduces but doesn't eliminate invalid options, potentially affecting test quality
- Effect sizes for difficulty manipulation appear modest and may not be practically significant in all educational contexts

## Confidence

**Major Claim Clusters and Confidence:**
- IRT model fitting using PLM scores: Medium confidence - Methodology is sound but lacks human performance validation
- Difficulty control effectiveness: Medium confidence - Statistical shifts demonstrated but practical significance unclear
- Distractor validity improvements: Low-Medium confidence - Rules reduce but don't eliminate invalid options

## Next Checks

1. Conduct human subject studies comparing PLM-generated difficulty levels with actual test-taker performance to validate IRT parameter translation
2. Test framework across multiple PLM architectures and languages to assess generalizability beyond current English-focused setup
3. Perform longitudinal analysis on generated items to evaluate stability of difficulty levels across different testing sessions and conditions